{
  "title": "Integrating large language models with human expertise for disease detection in electronic health records",
  "url": "https://openalex.org/W4409250979",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1996250324",
      "name": "Jie Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097647933",
      "name": "Seungwon Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306233075",
      "name": "Cheligeer Cheligeer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320799572",
      "name": "Elliot A. Martin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2029330601",
      "name": "Kiarash Riazi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2281826614",
      "name": "Hude Quan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1939676072",
      "name": "Na Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1522850626",
    "https://openalex.org/W2148554370",
    "https://openalex.org/W2131668595",
    "https://openalex.org/W2025349319",
    "https://openalex.org/W6760887161",
    "https://openalex.org/W4390070240",
    "https://openalex.org/W2950722229",
    "https://openalex.org/W6849341954",
    "https://openalex.org/W4386387286",
    "https://openalex.org/W6850716910",
    "https://openalex.org/W3000817905",
    "https://openalex.org/W2906311950",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W3087145289",
    "https://openalex.org/W2899915536",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W4386002582",
    "https://openalex.org/W4388608412",
    "https://openalex.org/W4384282814",
    "https://openalex.org/W4381480701",
    "https://openalex.org/W4388759569",
    "https://openalex.org/W6850312366",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W6854042620",
    "https://openalex.org/W4401432779",
    "https://openalex.org/W6859364697",
    "https://openalex.org/W4403151587",
    "https://openalex.org/W4403174382",
    "https://openalex.org/W4399280731",
    "https://openalex.org/W2775794168",
    "https://openalex.org/W3117466965",
    "https://openalex.org/W6718608068",
    "https://openalex.org/W6756393610",
    "https://openalex.org/W6636628491",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4233931937",
    "https://openalex.org/W4388005647",
    "https://openalex.org/W3023969949",
    "https://openalex.org/W3127166591",
    "https://openalex.org/W1899121568",
    "https://openalex.org/W1582456500",
    "https://openalex.org/W6743518184",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4323544013",
    "https://openalex.org/W2439007935",
    "https://openalex.org/W4322761615",
    "https://openalex.org/W2749759883",
    "https://openalex.org/W4402701735",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W2920206326",
    "https://openalex.org/W4382201147",
    "https://openalex.org/W4393160078",
    "https://openalex.org/W4320737442",
    "https://openalex.org/W2901139437",
    "https://openalex.org/W4241536761"
  ],
  "abstract": null,
  "full_text": "1 \n \nIntegrating Large Language Models with Human Expertise for Disease Detection in \nElectronic Health Records \n \n \nJie Pan1,2,4,#, PhD; Seungwon Lee 1,2,3, MPH PhD; Cheligeer Cheligeer 1,3, PhD; Elliot A. Martin 1,3, PhD; \nKiarash Riazi1,2,4, MD; Hude Quan1,2,4, MD PhD; Na Li1,2,4, PhD \n \n1 Centre for Health Informatics, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada \n2 Department of Community Health Sciences, Cumming School of Medicine, University of Calgary, \nCalgary, AB, Canada \n3 Provincial Research Data Services, Alberta Health Services, Calgary, AB, Canada \n4 Libin Cardiovascular Institute, University of Calgary, Calgary, AB, Canada \n \n# Corresponding author \nJie Pan, PhD \nDepartment of Community Health Sciences \nCumming School of Medicine  \nUniversity of Calgary \nCWPH 5E09 3280 Hospital Dr NW  \nCalgary, AB T2N 4Z6, Canada \nEmail: jie.pan@ucalgary.ca \n \n  \n2 \n \nABSTRACT \nObjective \nElectronic health records (EHR) are widely available to complement administrative data-based disease \nsurveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive \nand requires extensive manual labelling of disease outcomes. This study developed an efficient strategy \nbased on advanced large language models to identify multiple conditions from EHR clinical notes. \nMethods \nWe linked a cardiac registry cohort in 2015 with an EHR system in Alberta, Canada. We developed a \npipeline that leveraged a generative large language model (LLM) to analyze, understand, and interpret \nEHR notes by prompts based on specific diagnosis, treatment management, and clinical guidelines. The \npipeline was applied to detect acute myocardial infarction (AMI), diabetes, and hypertension. The \nperformance was compared against clinician-validated diagnoses as the reference standard and widely \nadopted International Classification of Diseases (ICD) codes-based methods.  \nResults \nThe study cohort accounted for 3,088 patients and 551,095 clinical notes. The prevalence was 55.4%, \n27.7%, 65.9% and for AMI, diabetes, and hypertension, respectively. The performance of the LLM-based \npipeline for detecting conditions varied: AMI had 88% sensitivity, 63% specificity, and 77% positive \npredictive value (PPV); diabetes had 91% sensitivity, 86% specificity, and 71% PPV; and hypertension \nhad 94% sensitivity, 32% specificity, and 72% PPV . Compared with ICD codes, the LLM-based method \ndemonstrated improved sensitivity and negative predictive value across all conditions. The monthly \npercentage trends from the detected cases by LLM and reference standard showed consistent patterns.  \nConclusion \nThe proposed LLM-based pipeline demonstrated reasonable accuracy and high efficiency in disease \ndetection for multiple conditions. Human expert knowledge can be integrated into the pipeline to guide \nEHR note analysis without manually curated labels. The method could enable comprehensive real-time \ndisease surveillance using EHRs.  \nKeywords: Natural language processing, disease phenotyping, public health surveillance, epidemiologic \nresearch design, cardiovascular diseases \n \n \n  \n3 \n \nIntroduction \nDisease detection is a foundation for numerous critical preventive medicine and public health operations. \nIt allows real-time monitoring of disease prevalence and incidence[1], [2], provides cohort identification \nfor observational and interventional research[3], and enables novel patient stratification for precision \nmedicine[4]. Detecting multiple diseases simultaneously from healthcare data is challenging since many \nconditions present with similar symptoms or have heterogeneous and ever-shifting presentations [5]. \nElectronic health records (EHRs) capture clinical decisions and orders by physicians, allied health \nproviders, and system information throughout patient admission encounters. With the availability of EHR \ndata, many artificial intelligence (AI)-based methods are increasingly being adopted to detect various \nconditions, including diabetes[6], [7], hypertension[8], cerebrovascular diseases[9], and adverse \nevents[10], [11].  \nRecent AI-based methods have shown promising abilities in detecting medical conditions with precision \n[12]. However, the path to integrating these technologies into healthcare systems highlights several areas \nrequiring improvement. For instance, due to its labour-intensive nature, manually labelling data for \ntraining models restricts the diversity of patient data and narrows model applications to limited cohorts \n[13], [14]. Besides, while effective for targeted diagnostics, the widespread focus on single-condition \nmodels risks oversimplifying the complex nature of patient health, where comorbidities are common. \nConsidering the vast number of recognized diseases (over 9,000[15]) and potentially many yet \nundiscovered, it is neither practical nor efficient to train models using labels for every condition [4]. There \nis a compelling need for innovative approaches that can analyze complex, multi-condition, diverse patient \ndata without relying on traditional labelling. This evolution requires not only technological innovation but \nalso a collaborative effort among AI researchers, clinicians, and data scientists to create more adaptable, \ncomprehensive, and effective healthcare solutions. Generative large language models (LLMs) provide a \nunique opportunity for developing such solutions[16].  \nGenerative LLMs represent a transformative advancement in AI, trained to understand and align with \nhuman instructions in text and language analysis[17]. LLMs offer a few key strengths: 1) clinical expert \nknowledge can be integrated into prompts for explainable outcomes; 2) they can be applied to multiple \nclinical tasks[17]; 3) they exhibit a strong reasoning capacity in understanding text and human \ninstructions; 4) they do not require reference labels, as in supervised learning models. If clinical expert \nknowledge is integrated correctly, LLMs hold the potential to support many medical applications, such as \nmedical diagnosis[18], clinical criteria extraction[19], cancer treatment response[20], clinical image \ninterpretation[21], treatment decision support[22], and medical education[23], [24]. \nMany existing studies have successfully applied generative LLMs to disease identification from EHR \nnotes, including cancer stages [25], Severe Acute Respiratory Syndrome Coronavirus 2 [26], Alzheimerâ€™s \ndisease [27], Parkinsonâ€™s disease [28], and rare diseases [29], [30]. Most of the work focuses on using \nspecific document types (e.g., pathology reports, patientâ€™s reported notes), cloud-based models (e.g., \nChatGPT), and detecting specific conditions. A holistic view of how to detect multiple conditions from \nreal-world EHR documents using localized generative LLMs is still lacking.  \nWe aimed to design a practical pipeline to analyze and process a full range of EHR clinical notes for \nmultiple condition detection. The pipeline combined LLMs with human clinical expertise to extract \nclinical diagnosis and disease management information. It was applied to the detection of acute \nmyocardial infarction (AMI), diabetes, and hypertension, conditions known for their widespread \nprevalence worldwide and substantial impact on healthcare [31]. This pilot project provided a holistic \nview of evaluating localized LLM for multi-condition detection without human labelling but with expert \nknowledge, offering a scalable solution across healthcare systems. \n4 \n \nMaterials and methods  \nPipeline architecture \nWe designed a pipeline integrating clinical expert knowledge and LLMs, as illustrated in Figure 1. The \npipeline was used to determine disease status by input text and prompts based on human instructions \nwithout needing gold standard disease labels. The pipeline consisted of four components: 1) an LLM-\nbased preprocessing of clinical notes to filter irrelevant information, 2) a prompt design for the LLM to \ninfer disease presence status, 3) a text inference by the LLM with optimal hyperparameters, and 4) a post-\nprocessing of model responses with clinical rules. Disease status, i.e., present or absent, was determined \nfor each patient through the pipeline. \nFigure 1 The architecture of the proposed pipeline for disease identification. The patientâ€™ s raw electronic health records (EHRs) \nclinical notes went through the pipeline: data preprocessing, prompt design, text inference by the large language model (LLM), \nand post-processing for LLMâ€™ s inference and information extraction results with clinical rules; the disease status was then \ndetermined. Human expert knowledge was applied to each component. \n \n \nData source and reference labels \nThe study cohort consisted of patients admitted to hospitals in 2015 from the Clinical Registry, \nAdminisTrative Data and Electronic Medical Records (CREATE) database system, which contained the \nAlberta Provincial Project for Outcome Assessment in Coronary Heart Disease (APPROACH) clinical \nregistry database linked to 3 other databases (SCM EHR, discharge abstract database, and national \nambulatory care reporting system)[32]. The databases comprised patient demographics data, EHR clinical \nnotes, and verified clinical diagnoses of the conditions of interest, including AMI, diabetes, and \nhypertension.  \nThe reference labels of the three conditions were derived from the APPROACH clinical registry. \nAPPROACH captures detailed procedures, demographic, and clinical information on all patients \nundergoing cardiac catheterization in Alberta. The data were collected by laboratory personnel through \ndirect inquiry into patients and the procedure physician and review of medical documentation on clinical \nrisk factors, including hypertension, diabetes, and a history of acute myocardial infarction [33]. The \ndiagnosis data was then combined with diagnosis results from administrative health data using the \nInternational Classification of Diseases 10th version Canadian modification (ICD-10-CA) [34]. This study \nwas approved by the Conjoint Health Research Ethics Board at the University of Calgary (REB19-0088 \nand REB-23-0535). \n\n5 \n \nPrompt-based data preprocessing \nIn the extracted EHR data, each patient could have over 60 types of documents, such as nursing notes, \ndischarge summaries, and clinical records, with some patients having multiple occurrences of many types. \nHowever, not all records contained information that was useful for condition identification. Including all \nclinical note types would add noise and consume extra computing resources, hampering model \nperformance[35].  \nWe designed a prompt-based data preprocessing method to extract the minimum sufficient text for LLM \nanalysis. It consisted of four steps: document type sampling, document type inference, document type \nfiltering, and document content selection, as shown in Figure 2. The first three steps involved analyzing \nthe document types and eliminating those unrelated to the conditions of interest, and the final step filtered \nthe notes within the selected types by extracting the most relevant sentences for each record.  \nFigure 2 Prompt-based preprocessing of electronic health records (EHRs) notes. This process consists of four steps: document \ntype sampling, document type inference, document type filtering, and document content selection. LLM stands for large language \nmodel. AMI stands for acute myocardial infarction. \n \nSampling a manageable subset of notes helped focus on a representative sample while maintaining \ncomputational feasibility. For document type sampling, we randomly checked out m records from each \ndocument type across all EHR notes. For document types with fewer than m records, we included all \navailable records. In documentation inference, each sampled record was sent to LLM for condition \ninference, where the prompt design was introduced in the following section. The results of LLM-based \ninference were not used for final disease detection but to calculate the Information Relevance (IR) score \nper document type in identifying cases. The IR is defined as the number of records having positive cases \nover m, as shown in Figure 2(3). During documentation type filtering, we set a threshold (larger than the \n25th percentile) to find important document types and keep only these types in the study notes. Patients \nlacking these document types were considered condition-free.  \nThe subsequent analyses, such as keyword-based matching and disease detection, were conducted on the \ndataset after removing unnecessary document types. Keywords-based matching is to select useful chunks \nof text within the remaining documents per patient, as shown in Figure 2(4). These keywords were \nselected by a clinical expert (KR) and consisted of medical terms, such as â€œsymptomsâ€, â€œtreatmentâ€, \nâ€œmedicationâ€, and specific terms for treatment management related to the targeted condition (see Table A. \n1). The keywords were automatically searched in each document; the sentences which contained them \nwere extracted and merged into a new document. The merged documents served as the final input for the \nsubsequent disease detection by LLMs.  \n\n6 \n \nPrompt design for disease identification \nPrompt design is the systematic practice of creating well-structured and effective instructions that guide \nLLMs to generate accurate, coherent, and contextually relevant outputs[36]. For disease identification, \nexpert knowledge can be combined with a prompt to guide LLM in analyzing text and reducing model \nuncertainty. In real-world healthcare systems, healthcare professionals are trained to review clinical notes \nby identifying specific diagnoses, disease management information, and key clinical indicators to code \nconditions in administrative data. We designed prompts to retrieve this information for each condition to \ninfer disease status.  \nTwo prompting techniques were available in practice: 1) Information extraction, creating prompts to \nextract different types of information (e.g., symptoms and laboratory tests) and comparing the extracted \nresults with clinical guidance for condition presence; or 2) Inferential reasoning, creating a prompt that \ninfers the condition presence status by finding diagnosis evidence internally. In this study, we mainly used \nthe second strategy for condition identification.  \nWe referred to specific diagnoses, treatment management, and relevant clinical guidelines per condition to \ndesign prompts and clinical rules. In Canada, the clinical diagnosis of diabetes includes hemoglobin A1c \nlevels of 6.5% or higher or glucose levels of 11.1 mmol/L [37]. AMI diagnosis involves characteristic \nelectrocardiogram changes and elevated cardiac biomarkers such as troponin levels above the 99th \npercentile of the upper reference limit[38]. Hypertension is diagnosed as persistently elevated blood \npressure readings, typically â‰¥140/90 mmHg, confirmed by multiple measurements[39]. \nThe specific prompt templates for the three conditions are shown in Table 1. To supplement detection, we \ncreated an information extraction type of prompt to extract a key laboratory test per condition. A specific \nkey laboratory test is often utilized in care systems in actual healthcare settings[40]. For example, we \nextracted the random blood sugar level for diabetes and compared it with a clinical standard of random \nplasma glucose â‰¥ 11.1â€¯mmol/L[37]. Blood pressure was extracted for hypertension. The prompt for AMI \nwas slightly different because many abbreviations were used in clinical notes. Though troponin levels \nwere measured for AMI, electrocardiography was the standard test in Canada. \nTable 1 Prompt template for the identification of acute myocardial infarction (AMI), diabetes, and \nhypertension. \nConditions Inferential reasoning type Information extraction \ntype \nAMI Analyze the clinical text: '{text}', and answer yes or no if you \nidentify acute myocardial infarction. Be careful with some \nabbreviations for acute myocardial infarction, including ami, mi, \nstemi, and non-stemi. \nFind all the key-value pairs \nof troponin level from the \ngiven text: {text}. \nDiabetes Analyze the clinical text: '{text}', answer yes or no if you identify \ndiabetes. Look for relevant information, including elevated blood \nglucose levels, mentions of diabetes diagnosis, or references to \nanti-diabetic medications. \nFind all the key-value pairs \nof blood sugar/glucose \nlevels from the given text: \n{text}.  \nHypertension Analyze the clinical text: '{text}', answer yes or no if you identify \nhypertension (high blood pressure). Look for relevant \ninformation, including high blood pressure readings or symptoms, \nmentions of hypertension diagnosis, or references to \nantihypertensive medications. \nFind all the key-value pairs \nof blood pressure from the \ngiven text: {text}. \nNotes: '{text}â€™ refers to the placeholder of the real clinical notes.  \n7 \n \n \nGenerative large language models for text inference  \nClinical notes contain a high degree of complexity and variations, including specialized medical \nterminologies, abbreviations, and jargon. They are also a hybrid of semi-structured (i.e., key-value pairs) \nand unstructured formats recorded by different care providers and institutions. With these difficulties, the \ngeneralizability of traditional natural language processing (NLP) methods is limited. It is common that a \npre-trained model may not perform well on another dataset without fine-tuning or retraining. With one \nmodel, the generative LLM could achieve robust generalizability across different tasks[19]. LLMs could \nparse, interpret, and summarize complex medical information in clinical notes data and transform \nunstructured text into structured knowledge that supports clinical decision-making[16], [19], [22], [24].  \nWe implemented a state-of-the-art generative LLM, Mistral-7B-OpenOrca[41], [42], which obtained the \nbest inference performance among models with 7 billion parameters. The model had already been pre-\ntrained on a vast amount of high-quality text data[42], enabling the generation of accurate answers with a \nprecise prompt. It was run in a secure and isolated health authority (i.e., Alberta Health Services) \napproved environment, using an NVIDIA Tesla V100 graphics processing unit (GPU). Its optimal \nhyperparameters were refined by grid search and given in Table A. 2.  \nThe model was then used to analyze the medical terminologies and context and extract medical history, \nsymptoms, laboratory results, diagnoses, and treatments from the notes. The inputs were each patientâ€™s \npreprocessed documents. The text within each document was inserted into the prompt template (see Table \n1), serving as LLM inputs.  \nRule-based classification  \nThe rule-based classification is the postprocess of LLM responses. For each disease identification task, \nthe LLM outputted two types of responses independently according to the prompts: a) inference response \nwas the directly inferred presence status containing â€œYesâ€, â€œNoâ€, or â€œNo mentionâ€, and b) information \nextraction response consisted of the key-value pairs of the laboratory test results, as shown in Table A. 3.  \nEach patientâ€™s clinical document was sent to LLM for disease inference. For type a) response, the rule \nwas to check if â€œYesâ€, â€œNoâ€, or â€œNo mentionâ€ was contained in the response and assigned the \ncorresponding results to the document. For type b) response, if key-value pairs of laboratory tests were \nextracted for this document, we compared the results with clinical guidelines. For diabetes detection, the \nkey was blood sugar level, and we checked if the value was larger than 11.1â€¯mmol/L [37] to assign the \nâ€œYesâ€ or â€œNoâ€ label to the document. Similarly, for hypertension, we compared the blood pressure (BP) \nwith a standard if the mean 24-hour systolic BP was â‰¥ 140 mm Hg or diastolic BP was â‰¥ 90 mm Hg [39]. \nFor AMI, we checked for a troponin level > 14 ng/L [38], [43]. If no test results were found in the \ndocument, a â€œNo mentionâ€ was assigned. Eventually, we combined the labels for each document and \nmerged these results as â€œYesâ€ or â€œNoâ€ per patient. \nStatistical analysis \nWe calculated the prevalences of AMI, diabetes, and hypertension in the study cohort. Sex and \ncomorbidities were calculated as ratios (%), and age and length of hospital stay were calculated as the \nmedian (interquartile range [IQR]). For document preprocessing, we calculated the percentage of records \nthat contained relevant text in detecting the three conditions across all document types, respectively. We \ncompared the inference results of the three conditions from the proposed pipeline against the reference \nstandards[33] and compared the performance with the widely adopted ICD-10 codes-based detection \nmethods[44]. The sensitivity, specificity, positive predictive value (PPV), and negative predictive value \n(NPV) per condition were reported. Their formula is given as follows, \nð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘† =\nð‘‡ð‘‡ð‘‡ð‘‡\nð‘‡ð‘‡ð‘‡ð‘‡+ð¹ð¹ð¹ð¹ ,                                                                 (1) \n8 \n \nð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘†ð‘† =\nð‘‡ð‘‡ð¹ð¹\nð¹ð¹ð‘‡ð‘‡+ð‘‡ð‘‡ð¹ð¹,                                                                  (2) \nð‘ƒð‘ƒð‘ƒð‘ƒð‘ƒð‘ƒ =\nð‘‡ð‘‡ð‘‡ð‘‡\nð‘‡ð‘‡ð‘‡ð‘‡+ð¹ð¹ð‘‡ð‘‡,                                                                       (3) \nð‘ð‘ð‘ƒð‘ƒð‘ƒð‘ƒâ€ˆ =\nð‘‡ð‘‡ð¹ð¹\nð‘‡ð‘‡ð¹ð¹+ð¹ð¹ð¹ð¹,                                                                       (4) \nwhere TP refers to true positive, FN refers to false negative, FP is false positive, and TN is true negative. \nTo visually assess the surveillance performance, the monthly percentage trends of the three conditions \nwere graphed using the pipeline detection results and reference standards.  \n \nResults  \nCharacteristics of the study participants \nWe obtained 3,088 patients from the CREATE database by selecting all the inpatients admitted in 2015. \nTheir characteristics are shown in Table 2. Hypertension had the highest prevalence (65.9%), followed by \nAMI (55.4%) and diabetes (27.7%). Most patients were male (70%), ranging from 68.8% to 70.7% for \nthree conditions. Patients with diabetes showed the highest prevalence of congestive heart failure (20.5%) \nand cerebrovascular diseases (7.0%). In contrast, the prevalence of chronic obstructive pulmonary disease \nwas distributed across the three groups. There were also patients with multiple conditions of interest: a \ntotal of 714 (23.1%) patients having both diabetes and hypertension, 415 (13.4%) patients having diabetes \nand AMI, 1047 (33.9%) patients having hypertension and AMI, and 339 (10.9%) patients having all three \nconditions.  \nTable 2  Characteristics of patient cohorts.  \n Total AMI Diabetes Hypertension \nNo. of patients (%) 3088 (100) 1710 (55.4) 854 (27.7) 2035 (65.9) \nMale sex (%) 2165 (70.1) 1209 (70.7) 592 (69.3) 1400 (68.8) \nMedian age (IQR), years 64.0 (55.8 - 72.8) 63.3 (54.8 - 72.3) 65.6 (58.5 - 73.2) 66.2 (58.5 - 74.6) \nLength of stay (IQR), days 7.1 (2.0 - 8.0) 6.3 (3.0 â€“ 7.0) 9.0 (3.0 - 9.2) 7.2 (2.0 - 8.0) \nComorbidities \nCongestive heart failure (%) 455 (14.7) 163 (9.5) 175 (20.5) 328 (16.1) \nChronic obstructive pulmonary disease (%) 269 (8.7) 117 (6.8) 81 (9.5) 191 (9.4) \nCerebrovascular disease (%) 148 (4.8) 72 (4.2) 60 (7.0) 126 (6.2) \nAMI: Acute Myocardial Infarction; IQR: interquartile range. \nConsolidation of clinical text \nWe gathered 551,095 EHR notes with 64 document types from the study cohort and then conducted \nprompt-based preprocessing to consolidate EHR notes. We sampled m=200 records per document type to \ndetermine the information relevance per condition. The calculated percentages of records containing \nrelevant texts detecting the three conditions are shown in Figure 3. The common document types for the \nthree conditions were â€œDischargeSummaryâ€, â€œCardiacDischargeâ€, and â€œTransferSummaryâ€. AMI was less \nfrequently documented in EHR notes than in the other two conditions. The detailed mapping between the \ndocument abbreviations and document names is reported in Table A. 4. Some documents were unique to \n9 \n \nthe specific condition, such as â€œPatientCareâ€ and â€œEDHandoverâ€ for AMI, â€œOutpatientConsultRâ€ and \nâ€œGoalFlowsheetâ€ for diabetes, and â€œGoalAssessmentâ€ and â€œAlcoholAssessmentâ€ for hypertension. There \nwere 20 document types that did not document any of these conditions. Figure 3 shows that many EHR \nnotes included unnecessary text per condition.  \nFigure 3 Information relevance of electronic health records (EHRs) document types for condition identification. Information \nrelevance was the percentage of inferred positive diagnoses by the large language model (LLM) per document type. It is \ncalculated for three conditions, such as acute myocardial infarction (AMI), diabetes, and hypertension. \n10 \n \n \n \nRunning a single prompt over all the clinical documents without filtering took around 20 hours in our \nlocal settings. Considering we had two prompts for each of the three conditions, it was computationally \ncostly. We had to preprocess the EHR notes to reduce the amount of input text sent to the pipeline. The \n25th percentile (Q1) threshold used in document-type filtering in Figure 2(3) controls the percentage of \nfiltered information. We had a comparative analysis of different levels of this threshold, such as 0, Q1, \nand Q2, as shown in Table A. 5. With an increase in the threshold level, the filtered document types and \n\n11 \n \nwords increased, and the positive cases dropped for all three conditions. We chose Q1 to maintain a \nbalanced filtering rate and the portion of positive cases retained.  \nAfter applying the 25th percentile threshold, there were 27 remaining document types for diabetes, 31 for \nhypertension, and 19 for AMI, as shown in Table 3. We then applied keyword searching to match \nsentences and merged them into a new document. Eventually, 70% of words in the entire cohort's notes \nwere excluded for diabetes, 62% were excluded for hypertension, and 93% were removed for AMI (see \nwords percentage in Table 3). The word percentage remaining is the ratio between the number of words in \nthe whole dataset after and before text preprocessing. To examine whether the process removed sensitive \ninformation necessary for detecting conditions, we calculated the positive cases in the preprocessed text \nand found that at least 93% of patients with conditions were retained, indicating that essential information \nwas preserved. We calculated the portion of positive cases retained by comparing patients with positive \ncases after and before text preprocessing. \nTable 3 Information consolidation after preprocessing for three conditions. \nCondition # document \ntypes \n(total N=64) \nWords percentage remaining \n(%) \nPortion of positive cases retained \n(%) \nAMI 19 6.9 93.3 \nDiabetes  27 29.7 98.5 \nHypertension 31 37.9 97.9 \nAMI: Acute Myocardial Infarction.  \nExplainability of LLM inference \nOur strategies for disease identification were to look for specific diagnoses, disease management \ninformation, and clinical indicators from EHR notes. To examine if the LLM could find this information \nwhen making decisions, we displayed three response examples of LLM in Error: Reference source not \nfound â€œResponse to inference promptâ€. We classified the LLM responses into two main categories: direct \nand inferred diagnosis based on clinical context. In the direct diagnosis category, the LLM identified \nconditions explicitly mentioned in the clinical text, such as its response for AMI: â€œYes, the text identifies \nacute myocardial infarction (AMI) as the patient has been diagnosed with AMIâ€. This demonstrated the \nLLMâ€™s ability to recognize diagnoses that were clearly stated in the clinical notes. \nIn the inferred diagnosis based on the clinical context category, the LLM inferred the presence of a \ncondition by interpreting relevant clinical information, such as medication use or test results, even when \nthe diagnosis was not directly mentioned. For instance, in the case of diabetes, the LLM states: â€œYes, you \ncan identify diabetes in this clinical text. The text mentions critically high blood sugars and the \nadministration of insulin (humulin r)â€. Conversely, the LLM also demonstrated caution by providing a \nnegative response when no clear evidence was present, as seen in the hypertension example: â€œNo, there is \nno clear mention of hypertension or high blood pressure in the given clinical textâ€. These two categories \nreflect the LLMâ€™s diagnostic reasoning capabilities. \nTo further evaluate the LLM's context understanding and ability to provide introspection into its decision-\nmaking, we introduced an instruction in the prompts to â€œhighlight all the original text that supports your \njudgementâ€. An example of AMI detection is shown in Figure 4, where the LLM produced a positive \ninference and highlighted key supporting sentences, including â€œacute onset of chest painâ€, â€œdiagnosed \nwith an acute ischemic strokeâ€, â€œtroponins escalatedâ€, and â€œchest pain resolvedâ€. These sentences cover \nthe symptoms, diagnosis, treatment, and lab tests for AMI, aligning with our identification strategies. This \nability to trace its conclusions back to specific text segments improves the transparency and explainability \nof LLM-driven disease detection. \n12 \n \nFigure 4 Large language model (LLM) inference for acute myocardial infarction (AMI) detection based on a clinical note. The \nmarked sentences were highlighted by the LLM in support of the presence of AMI. \n \nModel Selection  \nTo ensure the robustness of the selected LLM, Mistral-7B-OpenOrca, we evaluated several state-of-the-\nart LLMs for disease identification. We compared Mistral-7B-OpenOrca with three other modelsâ€”\nOpenBioLLM-8B, Phi3-mini, and BioMedLMâ€”on 10 predefined benchmarking questions (see Table A. \n6). The comparison results are shown in Table 4. OpenBioLLM-8B, a Llama-8B model fine-tuned on a \nmedical and life science dataset, demonstrates performance comparable to Mistral-7B-OpenOrca but \nrequires more processing time. Phi-3.5-mini-instruct achieves the best performance among the evaluated \nmodels; however, it is 17 times slower than Mistral-7B-OpenOrca in answering all benchmark questions. \nIn contrast, BioMedLM performs the least effectively, providing only one correct answer. Mistral-7B-\nOpenOrca demonstrated competitive accuracy while using significantly less computational resources. \nEventually, we selected Mistral-7B-OpenOrca as the final model for our pipeline due to its balanced \nperformance and our resource-constrained environments where data privacy and computational efficiency \nare critical. \nTable 4 Benchmarking results of different LLMs on 10 predefined disease detection questions. \n Correct answers (%) Time used (seconds) \nMistral-7BOpenOrca 80% 31s \nOpenBioLLM 70% 63s \nPhi-3.5-mini-instruct 90% 552s \nBioMedLM 10% 27s \n \nModel Performance \nThe detection performance of the three conditions is shown in Table 5. We compared the ICD-10 codes, \nthe pipeline using the first prompt, the pipeline using the second prompt, and the pipeline with merged \nresponses. The pipeline with merged responses performed similarly to the first prompt alone, with a slight \nimprovement in detecting hypertension. The sensitivity of the second prompt was lower, likely because \n\n13 \n \nlab results are typically not documented in clinical notes. The cases detected by the second prompt largely \noverlapped with those identified by the first prompt.  \nThe pipeline generally demonstrated high sensitivity (91%) and specificity (86%) for diabetes, although \nits positive predictive value (PPV) was lower (71%). For hypertension, the sensitivity was exceptional \n(94%), albeit with lower specificity (32%). Despite this, its PPV remains relatively high (72%). For AMI, \nthe method demonstrated good sensitivity (88%) and specificity (63%), with a higher PPV of 77%. \nThe ICD-10 codes were part of the reference standards since they were included in the APPROACH \nclinical registry database as per its data collection protocols. For AMI, ICD-10 method achieved a \nsensitivity of 84% and a specificity of 87%, indicating a reliable balance between detecting true positive \ncases and true negative cases. For Diabetes, ICD-10's sensitivity was 91% and specificity was 97%. \nHowever, for hypertension, while the specificity remained high at 92%, the sensitivity was lower at 80%.  \nCombining LLM and ICD-10 generally improved sensitivity but led to a decrease in specificity. For AMI, \nthe combined approach achieved the highest sensitivity at 95%, though specificity dropped to 60%. For \ndiabetes, the combined sensitivity reached 94%, an improvement over either method alone, with a \nspecificity of 85%. However, in the case of hypertension, while sensitivity was 96%, specificity was \nlower at 31%. \nTable 5 Comparison of detection performance for three conditions using physician labels as standard references. \n Condition Sensitivity (%) \n(95% CI) \nSpecificity (%) \n(95% CI) \nPPV (%) \n(95% CI) \nNPV (%) \n(95% CI) \nICD-10 (Part \nof reference \nstandard) \nAMI 84.0 (83.8-84.1) 87.0 (86.8-87.1) 89.4 (89.3-89.6) 80.6 (80.5-81.0) \nDiabetes 91.2 (91.0-91.3) 97.9 (97.8-98.0) 94.3 (94.2-94.5) 96.8 (96.7-96.9) \nHypertension 80.0 (79.1-81.2) 92.3 (92.0-92.5) 95.0 (94.9-95.1) 71.4 (71.1-72.0) \nPipeline \n(first \nprompt) \nAMI 88.3 (88.2-88.4) 63.4 (63.3-63.5) 76.5 (76.4-76.7) 80.0 (79.9-80.1) \nDiabetes 91.0 (90.0-91.1) 85.5 (85.3-85.6) 70.5 (70.4-70.6)  96.1 (96.0-96.2) \nHypertension 94.0 (93.9-94.1) 32.1 (32.0-32.2) 71.8 (71.7-71.9) 74.3 (74.1-74.4) \nPipeline \n(second \nprompt) \nAMI 0.1 (0.1-0.1) 99.9 (99.9-99.9) 50.0 (47.8-52.2) 41.5 (41.5-41.6) \nDiabetes 1.5 (1.4-1.5) 100.0 (99.9-100.0) 91.7 (91.2-92.2) 73.5 (73.5-73.6) \nHypertension 25.9 (25.9-25.9) 83.6 (83.5-83.7) 74.6 (74.5-74.7) 37.8 (37.8-37.9) \nPipeline \n(merged \nresponses) \nAMI 88.3 (88.2-88.4) 63.4 (63.3-63.5) 76.5 (76.4-76.7) 80.0 (79.9-80.1) \nDiabetes 91.0 (90.0-91.1) 85.5 (85.3-85.6) 70.5 (70.4-70.6)  96.1 (96.0-96.2) \nHypertension 94.3 (94.2-94.3) 32.6 (32.5-32.7) 72.2 (72.1-72.2) 75.5 (75.3-75.6) \nPipeline + \nICD-10 \nAMI 94.9 (94.8-95.0) 60.4 (60.2-60.5) 76.6 (76.4-76.6) 89.7 (89.5-89.7) \nDiabetes 93.6 (93.5-93.6) 85.2 (85.1-85.3) 70.1 (70.0-70.2) 97.3 (97.2-97.3) \nHypertension 96.2 (96.2-96.3) 32.2 (32.1-32.3) 72.4 (72.3-72.4) 81.9 (81.7-82.0) \nAMI: Acute Myocardial Infarction; CI: confidence interval. \nTo test the performance of the proposed method over time, we evaluated the monthly percentages of the \nthree conditions identified by the proposed method and physician diagnosis, as shown in Figure 5. For \nAMI, diabetes, and hypertension, the predicted numbers closely aligned with the changes in actual cases, \nindicating a consistent performance throughout the year with minor deviations that did not significantly \naffect the overall trend. \n14 \n \n \nFigure 5 The comparison of monthly percent of cohort with disease identified by the proposed pipeline and physician diagnosis. \nâ€œPredicted *â€ stands for the results by the pipeline, otherwise by physician diagnosis. \n \nDiscussion \nThis study proposed an innovative pipeline to detect multiple conditions by automatically analyzing \nlarge-scale EHR notes without the need for manual data labelling. The pipeline, which integrated an LLM \nwith human instructions from expert knowledge, demonstrated a reasonable performance in identifying \nconditions such as AMI, diabetes, and hypertension. The approach yielded high sensitivity and NPV \nacross the conditions compared to widely adopted ICD-10 methods. The monthly detected percentages \nclosely mirrored the pattern of actual diagnoses, exhibiting as an efficient real-time surveillance indicator \nfor healthcare outcomes. \nNotably, the ICD-10 codes used in our study are part of the reference standards and are merged with \nAPPROACH labels at the time of catheterization, so the high performance of ICD codes is expected here. \nSome studies reported the performance of ICD-10 in regular settings, where ICD codes are not involved \nin the reference standards. For instance, the sensitivity of ICD-10 was 47-78% for hypertension [8], [44], \n72% for AMI  [44], and 63-84% for diabetes [6], [44]. Higher sensitivities could be expected in this \ncohort as the patients were cardiac-specific, and these conditions would be germane to their visit. The \ncombined LLM and ICD approach demonstrated improved sensitivity and NPV across all conditions but \nshowed lower specificity and PPV. This approach might be suitable in scenarios where identifying true \ncases is more critical than minimizing false positives; for example, to manage chronic diseases, the \ncombined method or LLM alone could ensure that most cases with conditions are screened and identified, \nwith a follow-up test to rule out false positives. \nExisting disease detection methods rely either on administrative health data (e.g., ICD-10) algorithms or \nsupervised machine learning methods. In Canadian settings, the ICD-based method relies on the quality of \ncodes provided by nationally trained coders who review patient charts[45]. However, many conditions are \nunder-coded due to increased information overload for coders and the healthcare system[45], causing low \nsensitivity in detecting conditions. Therefore, supervised machine learning methods are being developed \non EHR data to enhance the detection performance. Supervised methods require manually labelled health \noutcomes, limiting their application to a large-scale population, diverse disease identification, and real-\n\n15 \n \ntime surveillance. With the wide adoption of EHR systems, unlabelled health data is exponentially \nincreasing globally, while labelled data can only grow linearly due to limited resources. The proposed \npipeline could scale to handle high volumes of EHR data and detect various conditions at a population \nlevel. \nAI methods are not systematically utilized in medical practice. One of the most important reasons is their \nunexplainable decision processes. As demonstrated in the proposed method, clinical expertise was \nincluded as part of the pipeline in plain text format, simplifying maintenance and troubleshooting. Unlike \ntraditional NLP methods, the pipeline was able to use pieces of evidence to support transparent decisions \n(Figure 4). In addition, the pipeline could be run entirely within a healthcare institutional firewall, \naligning with patient privacy and data security needs. The clear and transparent process and adherence to \ndata privacy standards will likely enhance utilization considerations within the health industries. \nGenerally, we observed three patterns from the experiment results. First, sensitivity was higher than PPV \n(mean 91% vs 73% across the three conditions). This indicates that besides accurately including true \npositive cases, the LLM model could include cases that were not diagnosed as positive. Second, the low \nspecificity was primarily due to false positive cases, which we analyzed across the three conditions. The \nfalse positives fall into several categories: \n1. Specific diagnoses were mentioned in clinical notes but not labelled in the reference standards \n(accounting for 25% of total cases). For example, discharge summaries often mentioned specific \ndiagnoses such as NSTEMI/STEMI for AMI, hypertension, and diabetes. The pipeline identified \nthese as positive cases, which was reasonable but not reflected in the reference labels. The reference \nlabels were collected by laboratory personnel through direct inquiry into patients and the \ncatheterization procedure physician, and review of medical documentation, and then combined with \ndiagnosis results from administrative health data using ICD-10 codes. However, the \ncomprehensiveness of data collection varied across personnel. \n2. Highly suspected conditions (40-50%): For AMI, clinical notes described chest pain and elevated or \npositive troponin levels. For hypertension, notes may document high blood pressure readings (e.g., \nâ€œsystolic 192, diastolic 102â€) and mention antihypertensive medications like Ramipril. For diabetes, \nnotes may include hemoglobin A1c levels and medications like metformin. These cases may represent \nborderline cases with similar symptoms requiring the same medications. It is possible that the models \nhad difficulty reliably distinguishing them.   \n3. History of the conditions (15%): Clinical notes may reference a patient's personal or family history of \nthe conditions in question, which led to false positives when active diagnoses were not present. \n4. Other cases (10-20%): For instance, the LLM sometimes classified acute coronary syndrome (ACS) \nas AMI, leading to misidentifications. In hypertension, the LLM occasionally misclassified normal \nblood pressure readings (e.g., 112/78) as positive hypertension, particularly when the notes contained \na large number of blood pressure readings. For diabetes, medications like Synthroid (not used for \ndiabetes treatment) were occasionally misinterpreted as indicators of the condition.   \nThe reasons behind these include some positive cases that were not captured in the reference labels from \nthe APPROACH clinical registry. The clinical registry's data collection process was more restrictive and \nsubject to physician confirmation. For example, diagnosing hypertension is more complex, requiring \nmultiple blood pressure measurements, whereas diabetes diagnosis is more straightforward and is often \nbased on hemoglobin A1c levels. Furthermore, the LLM struggled to distinguish subtle nuances between \nconditions (e.g., ACS vs. AMI) due to similarity of the  medical conditions and exhibited inconsistencies \nin mathematical comparisons, such as comparing the blood pressure values with diagnosis standards. \nNevertheless, identifying suspected cases remained valuable, as these patients may be at risk for \ndeveloping the condition, requiring earlier preventive interventions. \n16 \n \nLastly, the detection performance varied across conditions. Diabetes detection showcased superior results \n(91% sensitivity, 86% specificity, 71% PPV , and 96% NPV) compared to other conditions. This \ndiscrepancy might stem from varying documentation standards. In Alberta, diabetes coding in \nadministrative data and EHR systems is compulsory, likely leading to higher coding quality than for other \nconditions. Accordingly, the pipeline could serve as an indicator of documentation quality for various \nmedical conditions with varying detection performance if reference standards were provided. The higher \nperformance, characterized by both high sensitivity and specificity, could indicate better document \nquality.  \nThe performance of the pipeline varied with preprocessing, prompt design, LLM model, and post-\nprocessing. The EHR data included all clinical notes per patient during hospitalization, with an average of \n178 notes per patient in this study. Our designed preprocessing strategy using prompts significantly \nreduced the text volume (75% word reduction on average) for LLM analysis, thereby improving \nefficiency and accuracy by minimizing noise. The prompts for preprocessing and condition detection \nwere the same and were designed based on clinical diagnosis guidelines per disease. The LLM we \nimplemented was a small-scale model due to memory limitations (16 GB) with the available GPU, which \nlimited its capacity to process complex prompts. For example, you may need more specific prompts \nwhich include symptoms, laboratory tests, and medications instead of a broad and general one (e.g., \nâ€œDoes this patient have a diagnosis of diabetes given all the clinical notes?â€). Providing a few examples \nwith some annotated data would improve the retrieval of desired content. \nWe experimented with both overly detailed prompts that included clinical guidelines (e.g., troponin \nlevels >14 ng/L for AMI, and specific medications taken) and moderate instructions, as reported in our \nstudy. Interestingly, the moderate prompts yielded better performance in detecting these conditions, likely \nbecause overly complex prompts can overwhelm the model or introduce ambiguity in decision-making. \nPerformance is influenced by both the prompts and the specific model used. We believe that the optimal \nprompt may vary depending on the specific LLM, its training process, and the datasets being used. A few \nstrategies could always refine the prompt, including iterative refinement, where prompt wording and \nstructure are continuously tested and tweaked to improve model performance, and human-in-the-loop \nevaluation, which involves domain experts reviewing outputs to provide qualitative feedback for prompt \nadjustments. \nSince the inference capabilities of LLMs vary according to data and architectures, evaluating them before \nusage is often necessary. We applied post-processing in the pipeline to handle extracted laboratory test \nresults by comparing them with clinical guidelines. The post-processing could be further improved by \nconstructing a logistic regression model based on laboratory test trajectories to adapt to diverse tasks. \nWith the customization and optimization of these components, the pipeline could be extended to handle \nmore healthcare questions. \nAlthough we integrate human knowledge in prompt design to guide LLMâ€™s behaviours, it has several \ninherent limitations. One major challenge is that prompts are external instructions and cannot rectify \nfundamental limitations of LLMs, such as their baseline performance in reasoning and clinical knowledge \nunderstanding. Moreover, prompt design often requires extensive trial and error to create prompts that \nelicit accurate, reliable outputs, especially when dealing with complex or ambiguous medical data. This \nprocess can be labour-intensive and may not consistently yield improvements across all disease \nidentification tasks. Additionally, prompts are typically limited in their capacity to handle clinical \nnuances, as they may struggle to capture the full range of variability in clinical presentations, medical \nterminologies, and clinical abbreviations. This makes it challenging to design prompts that consistently \naccount for the ambiguity often present in medical data, potentially leading to disease detection or \nclassification errors. \nThe study has several limitations. First, further examination of false positive cases is needed to improve \nthe model performance for future implementations within clinical care settings. Since these cases could be \n17 \n \nhighly suspected patients, a specific LLM fine-tuned on relevant data is beneficial to distinguish them \nfrom firmly diagnosed cases. Second, our existing computing infrastructure limited us from utilizing \nlarger-scale LLMs with superior inference capacities. We plan to upgrade this infrastructure, enabling the \napplication and fine-tuning of advanced LLM models. However, recent studies [46], [47] show that fine-\ntuning LLMs on biomedical data without appropriate strategies does not always lead to improved \noutcomes. It can sometimes reduce performance on clinical tasks compared to general-purpose models. \nLastly, our pipeline was validated on a cardiac disease cohort in Calgary and has not been evaluated in \nexternal databases. We aim to collaborate with other institutions from multiple geographic regions in the \nfuture to strengthen the validity of the method. \nConclusion  \nOur proposed pipeline demonstrates the feasibility of integrating human expert knowledge with LLMs to \ninfer multiple diseases from EHR clinical notes at scale. By leveraging generative LLMs as foundational \nmodels, the proposed approach can detect multi-conditions and expand to a broader range of conditions \nwith customized data preprocessing and specific prompts. We are poised to expand beyond the limitations \nof detecting only one or a few cases, offering a scalable and adaptable solution for complex, multi-\ncondition diagnostics from real-time health data. \nAcknowledgement \nThe authors thank Dr. Alexander Leung for his invaluable suggestions and comments on the clinical \nknowledge usage in method development, improving the work's integrity. \n \nFunding sources \nThis work was supported by Canadian Institutes of Health Research Operating Project Grants \n(201809FDN-409926-FDN-CBBA-114817 for HQ and 202209PJT-486541-HS1-CBBA-68649 for NL). \n \nAvailability of data and materials \nThe patient data, which underpin the conclusions of this study, cannot be shared due to their potentially \nidentifiable contents and institutional data privacy policies. \n \nAppendices  \nTable A. 1 Keywords used to select sentences for input text. \nCondition Keywords \nAMI age|weight|wt|myocardial infarction|myocardial|heart|mi|acute \ncoronary|coronary|ischemic|cardiac|myocardium|infarct| \necg|troponin|artery|pci|stemi|nstemi|cardiogenic|aneurysm|medication \nDiabetes age|weight|wt|non-alcoholic fatty \nliver|dyslipidemia|sugar|dyslipidemia|hypertension|blood \npressure|glycemia|glucose|fasting|fpg|ogtt|hba1c|a1c| \nmmtt|hemoglobin|insulin|diabetes|diabetic|dm|tolerance|inhibitor|peptide | \ntzds|glp-1|inhibitors|dpp-4|metformin|medication \nHypertension age|weight|wt|hypertension|blood pressure|systolic|diastolic|htn| \ndash|hypertensive|medication \nAMI: Acute Myocardial Infarction. \n18 \n \n \nTable A. 2 Optimized hyperparameters for LLM after grid search. \nParameters Description Optimal value \n(range) \nTemperature The temperature parameter controls the randomness in the \nprobability distribution used to select the next word or token in the \ngenerated text. A higher value increases the randomness and \ndiversity; lower values produce more deterministic and predictable \ntext. \n0.5 (0-1) \nTop_p A cumulative probability threshold to select the next word from a \nsubset of the vocabulary. Higher values allow for a diverse but might \nless relevant selection; and vice versa.  \n0.9 (0-1) \nTop_k Top_k sampling restricts the word selection to the k most likely next \nwords, reducing the chance of selecting highly improbable words. \nHigher values introduce more variety but might bring less relevant \nselection, and vice versa. \n50 (>1) \n \nTable A. 3 Input and response examples of LLM for disease identification. \nCondition Response to inference \nprompt \nResponse to information \nextraction prompt \nInput text \nAMI Response:  Yes, the \ntext identifies acute \nmyocardial infarction \n(AMI) as the patient \nhas been diagnosed \nwith AMI). \nResponse:  troponin \nlevel: 1.16 ng/mL. \nA 68-year-old male presented to \nER after experiencing severe chest \npain during a morning jog. He was \ndiagnosis with acute myocardial \ninfarction (AMI), supported by \ntroponin I levels elevated at 1.16 \nng/mL. Past medical history \nincluded hypercholesterolemia and \nhypertension for which he was on \natorvastatin and lisinopril. After \nacute management, he was \nimmediately transferred to a \nspecialized cardiac care facility for \nfurther managementâ€¦ \nDiabetes Response:  Yes, you can \nidentify diabetes in this \nclinical text. The text \nmentions critically high \nblood sugars and the \nadministration of insulin \n(humulin r), which is an \nanti-diabetic \nmedication. \nResponse:  Key-value pairs:  \n1. glucose - mmol/l \nbreakfast: 24.8 mmol/l \n2. glucose - mmol/l \nbreakfast: 20.1 mmol/l \n3. glucose - mmol/l \nbreakfast: 16.6 mmol/l \n4. poct blood glucose - \nmmol/l lunch: 12 mmol/l \n5. glucose - mmol/l lunch: \n9.7 mmol/l \n6. glucose - mmol/l lunch: \n8.9 mmol/l \nglucose - mmol/l breakfast : mmol/l \ncomment : critically high blood sugars \nnow. insulin (humulin r), 21 units \ngiven now.  weight kg : 54.9 kg height \ncm : 158 cm. glucose - mmol/l \nbreakfast : 24.8 mmol/l. glucose - \nmmol/l breakfast : 20.1 mmol/l. \nglucose - mmol/l breakfast : 16.6 \nmmol/l poct blood glucose - mmol/l \nlunch : 12 mmol/l. glucose - mmol/l \nlunch : 9.7 mmol/l. glucose - mmol/l \nlunch : 8.9 mmol/l. glucose - mmol/l \nother : 6.5 mmol/l. glucose - mmol/l \n19 \n \n7. glucose - mmol/l other: \n6.5 mmol/l \n8. glucose - mmol/l other: \n5.8 mmol/l \n9. glucose - mmol/l other: \n6.6 mmol/l \n10. glucose - mmol/l other: \n8.7 mmol/l \n11. glucose - mmol/l other: \n7.4 mmol/l \n12. glucose - mmol/l other: \n6.1 mmol/l \n13. glucose - mmol/l other: \n6.8 mmol/l \n14. glucose - mmol/l other: \n93 mmol/l \n15. glucose - mmol/l other: \n83 mmol/l \n16. glucose - mmol/l other: \n17 mmol/l \n17. glucose - mmol/l other: \n16.7 mmol/l \n18. weight kg: 54.9 kg \n19. height cm: 158 cm \n20. insulin (humulin r): 20 \nunits given now \n21. blood pressure systolic: \n140 \n22. blood pressure diastolic: \n66 \n23. blood pressure mean: \n(not provided) \nother : 5.8 mmol/l. glucose - mmol/l \nother : 6.6 mmol/l. glucose - mmol/l \nother : 8.7 mmol/l. glucose - mmol/l \nother : 7.4 mmol/l. glucose - mmol/l \nother : 6.1 mmol/l. glucose - mmol/l \nother : 6.8 mmol/l. glucose - mmol/l \nother : 93 mmol/l. glucose - mmol/l \nother : 83 mmol/l. glucose - mmol/l \nother : 17 mmol/l. glucose - mmol/l \nother : 16.7 mmol/l. blood pressure \nblood pressure systolic : 140 \ncomment : after exertion of \ntransferring from wheelchair to bedâ€¦ \nHypertension Response:  No, there is \nno clear mention of \nhypertension or high \nblood pressure in the \ngiven clinical text. \nResponse:  There are no \nkey-value pairs of blood \npressure in the given text. \nAdmission Date: [DATE], Discharge \nDate: [DATE] from [HOSPITAL]  \nThe patient, a [AGE]-year-old male, \npresented to ER with acute onset of \nchest pain radiating to his left arm, \nsweating, and nauseaâ€¦ \nAn ECG revealed anterior wall ST-\nelevation, and troponins escalated \nfrom 12 ng/L to 240 ng/L within 4 \nhours. Diagnosed with acute \nmyocardial infarction.  \nImmediate management with IV \nmorphine [dose], aspirin [dose], and [a \nbeta-blocker [dose]â€¦. The patient was \npromptly moved to the cath lab. A \ncoronary angiography showed a 90% \nblockage in the left anterior \ndescending artery, which was \nsuccessfully treated with primary \npercutaneous coronary intervention \n(PCI) and a drug-eluting stent \nplacement. Post-PCI, chest pain \nresolved, and blood flow re-\nestablished ... \n20 \n \nAt discharge, the patient had stable \ncondition with no chest pain and \nnormal sinus cardiac rhythm. Vital \nsigns were within normal range. \nTreatment plan included managing \nhyperlipidemia, type 2 diabetes, and \noverweight ... \nAMI: Acute Myocardial Infarction. \n \nTable A. 4 Mapping between document names and abbreviations. \nDocument name Shorten names Content description  \nAcute Pain Summary PainSummary \nA summary of a patient's acute pain evaluation and \nmanagement. \n \nTrauma Admission Report TraumaReport \nDetails of the assessment and care provided during a \ntrauma patient's hospital admission. \n \nAdult Emergency Triage Note AdultTriage \nInitial assessment and prioritization of an adult \npatient in the emergency department. \n \nBlood Product Reaction Log BloodLog \nRecord of any reactions to blood product \ntransfusions. \n \nCardiac Diagnostic Report CardiacDiagnostic \nDiagnostic findings related to the patient's cardiac \nfunction and conditions. \n \nClinical Encounter Record ClinicalRecord \nDocumentation of a clinical visit or interaction \nbetween patient and provider. \n \nDay Surgery Record SurgeryRecord \nInformation on the patient's day surgery, including \nprocedure and outcomes. \n \nCardiac Discharge Summary CardiacDischarge \nSummary of a patient's condition and instructions \nupon discharge after cardiac care. \n \nGeneral Discharge Summary GeneralDischarge \nSummary of the patient's hospital stay and discharge \ninstructions. \n \nHospitalist Discharge \nSummary HospitalistSummary \nDischarge summary prepared by the hospitalist \noverseeing patient care. \n \nMedical Discharge Summary MedicalSummary \nSummary of a patient's medical condition and \ntreatment upon discharge. \n \nOrthopedic Surgery Discharge \nSummary OrthopedicSummary \nDischarge details for a patient who underwent \northopedic surgery. \n \nStroke Neurology Discharge \nSummary StrokeSummary \nDischarge report summarizing care for a patient \ntreated for a stroke. \n \nShort Surgery Discharge \nSummary ShortSummary \nSummary of discharge for short or minor surgical \nprocedures. \n \nThoracic Surgery Discharge \nSummary ThoracicSummary Discharge summary following thoracic surgery.  \nGeneral Discharge Summary \n(Duplicate) DischargeSummary \nSummary of the patient's hospital stay and \ndischarge. \n \nED Handover Report EDHandover \nInformation passed during handover from \nemergency to inpatient care. \n \n21 \n \nGoal Achievement Assessment GoalAssessment \nAssessment of a patient's progress towards health-\nrelated goals. \n \nGoal Achievement Flowsheet GoalFlowsheet \nFlowsheet tracking progress towards goals in patient \ncare. \n \nComprehensive History & \nPhysical Exam ComprehensiveExam \nDetailed history and physical exam conducted for \nthorough patient evaluation. \n \nHistory & Physical Summary HistorySummary \nA brief summary of the patient's history and \nphysical examination. \n \nInpatient Consultation Report InpatientConsultLog \nReport documenting the consultation for an inpatient \nby a specialist or consultant. \n \nInpatient Consultation InpatientConsult \nNotes from a consultant reviewing an inpatient's \ncondition. \n \nOperative/Procedure Detail \nReport OperativeReport \nDetailed report on the surgery or procedure \nperformed on the patient. \n \nPsychiatric Review (General) PsychiatricReview \nGeneral review of the patient's psychiatric status and \ntreatment. \n \nMed-Surg Outcome \nAssessment SurgOutcome \nAssessment of outcomes after medical or surgical \ntreatment. \n \nMed-Surg Outcome Flowsheet SurgFlowsheet \nFlowsheet tracking medical or surgical treatment \noutcomes. \n \nMental Health Outcome \nAssessment MentalOutcome \nEvaluation of patient outcomes in mental health \ntreatment. \n \nMental Health Outcome \nFlowsheet HealthFlowsheet \nFlowsheet monitoring outcomes in mental health \ncare. \n \nNeuro Diagnostic Report NeuroDiagnostic \nReport detailing neurological diagnostic tests and \nresults. \n \nED to Inpatient Transfer Note EDTransfer \nDocumentation of transfer from emergency \ndepartment to inpatient care. \n \nInpatient Transfer Note InpatientTransfer \nNotes documenting the patient's transfer between \ninpatient units. \n \nMental Health Transfer Note HealthTransfer \nReport on the transfer of a mental health patient to \nanother facility or care unit. \n \nPACU to Inpatient Transfer \nNote PACUTransfer \nTransfer note from post-anesthesia care unit \n(PACU) to inpatient unit. \n \nOutpatient Consultation Report OutpatientConsultR Detailed report of an outpatient consultation.  \nOutpatient Consult OutpatientConsult \nSummary of an outpatient consultation and care \nplan. \n \nOutpatient Procedure Note OutpatientProceLog \nNotes from an outpatient procedure including \noutcomes and follow-up. \n \nVascular Access Record VascularAccess \nDocumentation of vascular access procedures \nperformed on the patient. \n \nNeurological Patient \nAssessment NueroAssessment Assessment of the patient's neurological status.  \n22 \n \nPatient Care Summary PatientCare \nGeneral summary of patient care provided during \nhospitalization. \n \nPharmacy Treatment Plan PharmacyPlan \nPlan outlining the patient's medication therapy \nmanaged by the pharmacy. \n \nSocial Work Evaluation SocialWork \nAssessment conducted by a social worker regarding \nthe patient's social needs. \n \nPre-Op Nursing Assessment NursingAssessment \nPre-operative assessment completed by nursing \nstaff. \n \nTransfer Summary Note TransferSummary \nSummary of patient's condition and treatment during \ntransfer to another facility. \n \nAlcohol Withdrawal \nAssessment AlcoholAssessment \nEvaluation of the patient's symptoms and status \nrelated to alcohol withdrawal. \n \n \nTable A. 5 The comparison of different levels of threshold in document type filtering. \nCondition Information relevance \nthreshold \n#Document \ntypes \nWords percentage \nremaining \nPortion of positive \ncases retained \nDiabetes  > 0 36 0.42 0.986 \nHypertension > 0 42 0.571 0.979 \nAMI > 0 25 0.16 0.942 \n     \nDiabetes  > Q1 27 0.297 0.985 \nHypertension > Q1 31 0.379 0.979 \nAMI > Q1 19 0.069 0.933 \n     \nDiabetes  > Q2 18 0.065 0.974 \nHypertension > Q2 21 0.134 0.952 \nAMI > Q2 11 0.044 0.862 \nQ1: 25th percentile; \nQ2: 50th percentile. \n \nTable A. 6 Benchmark questions to test different localized LLMs \n Question Answer \nQ1 Imagine you are a physician, does the following text contain lab tests used to \ndetect sepsis: fasting plasma glucose (FPG) test, oral glucose tolerance test \n(OGTT), hemoglobin A1c (HbA1c) test, and random plasma glucose (RPG) \ntest? \nNo \nQ2 Imagine you are a physician, does the following text contain lab tests used to \ndetect diabetes: fasting plasma glucose (FPG) test, oral glucose tolerance test \nYes \n23 \n \n(OGTT), hemoglobin A1c (HbA1c) test, and random plasma glucose (RPG) \ntest? \nQ3 What is the systolic blood pressure from the given text: Temperature Degrees \nC 36.2 degrees CPulse Pulse bpm : 72 bpm Blood Pressure Blood Pressure \nSystolic : 119 Blood Pressure Diastolic : 71 Blood Pressure Mean : 87 \nmmHg Blood Pressure Patient Position? \nSystolic: 119 \nQ4 The clinical note states: â€˜The patient has a history of high blood sugar and is \ncurrently on insulin therapy.â€™ Can you identify if the patient has diabetes?  \nYes \n \nQ5 'The patient was diagnosed with hypertension 5 years ago and has been on \nlisinopril since. No signs of improvement. Can you extract the diagnosis of \nhypertension and recognize when it occurred? \nHypertension 5 years ago \nQ6 â€˜The patient was admitted with acute chest pain, later confirmed to be a \nmyocardial infarction. They also have a long-standing history of \nhypertension and are managing diabetes with metformin.â€™ Can you identify \nthe three conditions: myocardial infarction, hypertension, and diabetes? \nMyocardial infarction, \ndiabetes, and hypertension \nQ7 â€˜Patient reported severe chest pain radiating to the left arm, with nausea and \nshortness of breath. EKG confirmed ST elevation.â€™ Can you identify if this \npatient is likely suffering from an acute myocardial infarction based on the \nsymptoms and test results? \nYes \nQ8 â€˜The patient is obese, with a family history of diabetes and hypertension. \nFasting glucose levels are elevated, and blood pressure remains uncontrolled \ndespite medication.â€™ Based on the risk factors and medical history, can you \ninfer the likelihood of diabetes and hypertension in this patient? \nHighly likely that the \npatient has diabetes; \nalmost certainly has \nhypertension. \nQ9 â€˜The patient is currently on metformin, atorvastatin, and \nhydrochlorothiazide.â€™ Can you identify which conditions these medications \nare most likely treating? \nType 2 diabetes and \nhypertension \nQ10 â€˜The patient was evaluated for chest pain, but there is no evidence of \nmyocardial infarction. He has diabetes but no signs of hypertension.â€™ Can \nyou correctly identify the presence of diabetes while acknowledging that \nthere is no myocardial infarction or hypertension? \nYes, the patient has \ndiabetes but no MI or \nhypertension. \n \n \nReferences \n[1] S. H. Jain, B. W. Powers, J. B. Hawkins, and J. S. Brownstein, â€˜The digital phenotypeâ€™, Nat \nBiotechnol, vol. 33, no. 5, pp. 462â€“463, May 2015, doi: 10.1038/nbt.3223. \n[2] C. P. Friedman, A. K. Wong, and D. Blumenthal, â€˜Policy: Achieving a nationwide learning health \nsystemâ€™, Sci Transl Med, vol. 2, no. 57, Nov. 2010, doi: \n10.1126/SCITRANSLMED.3001456/ASSET/31DF6FBB-61EA-4899-8446-\n9F05C371B44A/ASSETS/GRAPHIC/257CM29-F1.JPEG. \n[3] R. L. Richesson et al., â€˜Electronic health records based phenotyping in next-generation clinical \ntrials: a perspective from the NIH health care systems collaboratoryâ€™, Journal of the American \nMedical Informatics Association, vol. 20, no. e2, p. e226, 2013, doi: 10.1136/AMIAJNL-2013-\n001926. \n[4] P. N. Robinson, â€˜Deep phenotyping for precision medicineâ€™, Hum Mutat, vol. 33, no. 5, pp. 777â€“\n780, May 2012, doi: 10.1002/HUMU.22080. \n[5] S. B. Cho, S. C. Kim, and M. G. Chung, â€˜Identification of novel population clusters with different \nsusceptibilities to type 2 diabetes and their impact on the prediction of diabetesâ€™, Sci Rep, vol. 9, \nno. 1, pp. 1â€“9, Mar. 2019, doi: 10.1038/s41598-019-40058-y. \n24 \n \n[6] S. Lee et al., â€˜Exploring the reliability of inpatient EMR algorithms for diabetes identificationâ€™, \nBMJ Health Care Inform, vol. 30, no. 1, p. e100894, Dec. 2023, doi: 10.1136/BMJHCI-2023-\n100894. \n[7] T. Zheng et al., â€˜A machine learning-based framework to identify type 2 diabetes through \nelectronic health recordsâ€™, Int J Med Inform, vol. 97, pp. 120â€“127, Jan. 2017, doi: \n10.1016/J.IJMEDINF.2016.09.014. \n[8] E. A. Martin, A. G. Dâ€™Souza, S. Lee, C. Doktorchik, C. A. Eastwood, and H. Quan, â€˜Hypertension \nidentification using inpatient clinical notes from electronic medical records: an explainable, data-\ndriven algorithm studyâ€™, Canadian Medical Association Open Access, vol. 11, no. 1, pp. E131â€“\nE139, Jan. 2023, doi: 10.9778/CMAJO.20210170. \n[9] J. Pan et al., â€˜Cerebrovascular disease case identification in inpatient electronic medical record \ndata using natural language processingâ€™, Brain Inform, vol. 10, p. 22, Dec. 2023, doi: \n10.1186/S40708-023-00203-W. \n[10] E. Nurmambetova et al., â€˜Developing an inpatient electronic medical record phenotype for \nhospital-acquired pressure injuries: case study using natural language processing modelsâ€™, JMIR \nAI, vol. 2, no. 1, p. e41264, Mar. 2023, doi: 10.2196/41264. \n[11] H. Nakatani, M. Nakao, H. Uchiyama, H. Toyoshiba, and C. Ochiai, â€˜Predicting inpatient falls \nusing natural language processing of nursing records obtained from Japanese electronic medical \nrecords: case-control study.â€™, JMIR Med Inform, vol. 8, no. 4, p. e16970, Apr. 2020, doi: \n10.2196/16970. \n[12] B. Norgeot, B. S. Glicksberg, and A. J. Butte, â€˜A call for deep-learning healthcareâ€™, Nat Med, vol. \n25, no. 1, pp. 14â€“15, Jan. 2019, doi: 10.1038/s41591-018-0320-3. \n[13] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan, â€˜Dissecting racial bias in an algorithm \nused to manage the health of populationsâ€™, Science (1979), vol. 366, no. 6464, pp. 447â€“453, Oct. \n2019, doi: 10.1126/SCIENCE.AAX2342/SUPPL_FILE/AAX2342_OBERMEYER_SM.PDF. \n[14] A. Aliabadi, A. Sheikhtaheri, and H. Ansari, â€˜Electronic health recordâ€“based disease surveillance \nsystems: A systematic literature review on challenges and solutionsâ€™, Journal of the American \nMedical Informatics Association, vol. 27, no. 12, pp. 1977â€“1986, Dec. 2020, doi: \n10.1093/JAMIA/OCAA186. \n[15] L. M. Schriml et al., â€˜Human disease ontology 2018 update: classification, content and workflow \nexpansionâ€™, Nucleic Acids Res, vol. 47, no. D1, pp. D955â€“D962, Jan. 2019, doi: \n10.1093/NAR/GKY1032. \n[16] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting, \nâ€˜Large language models in medicineâ€™, Nat Med, vol. 29, no. 8, pp. 1930â€“1940, Jul. 2023, doi: \n10.1038/s41591-023-02448-8. \n[17] L. Ouyang et al., â€˜Training language models to follow instructions with human feedbackâ€™, Adv \nNeural Inf Process Syst, vol. 35, pp. 27730â€“27744, Dec. 2022. \n[18] L. Caruccio, S. Cirillo, G. Polese, G. Solimando, S. Sundaramurthy, and G. Tortora, â€˜Can \nChatGPT provide intelligent diagnoses? A comparative study between predictive models and \nChatGPT to define a new medical diagnostic botâ€™, Expert Syst Appl, vol. 235, p. 121186, Jan. \n2024, doi: 10.1016/J.ESWA.2023.121186. \n[19] S. Datta et al., â€˜AutoCriteria: a generalizable clinical trial eligibility criteria extraction system \npowered by large language modelsâ€™, Journal of the American Medical Informatics Association, \nvol. 31, no. 2, pp. 375â€“385, Jan. 2024, doi: 10.1093/JAMIA/OCAD218. \n25 \n \n[20] R. S. Y . C. Tan et al., â€˜Inferring cancer disease response from radiology reports using large \nlanguage models with data augmentation and promptingâ€™, Journal of the American Medical \nInformatics Association, vol. 30, no. 10, pp. 1657â€“1664, Sep. 2023, doi: \n10.1093/JAMIA/OCAD133. \n[21] A. Rao et al., â€˜Evaluating GPT as an adjunct for radiologic decision making: GPT-4 versus GPT-\n3.5 in a breast imaging pilotâ€™, Journal of the American College of Radiology, vol. 20, no. 10, pp. \n990â€“997, Oct. 2023, doi: 10.1016/J.JACR.2023.05.003. \n[22] M. Benary et al., â€˜Leveraging large language models for decision support in personalized \noncologyâ€™, JAMA Netw Open, vol. 6, no. 11, pp. e2343689â€“e2343689, Nov. 2023, doi: \n10.1001/JAMANETWORKOPEN.2023.43689. \n[23] G. Eysenbach, â€˜The role of ChatGPT, generative language models, and artificial intelligence in \nmedical education: a conversation with ChatGPT and a call for papersâ€™, JMIR Med Educ, vol. 9, \nno. 1, p. e46885, Mar. 2023, doi: 10.2196/46885. \n[24] T. H. Kung et al., â€˜Performance of ChatGPT on USMLE: Potential for AI-assisted medical \neducation using large language modelsâ€™, PLOS Digital Health, vol. 2, no. 2, p. e0000198, Feb. \n2023, doi: 10.1371/JOURNAL.PDIG.0000198. \n[25] J. Huang et al., â€˜A critical assessment of using ChatGPT for extracting structured data from \nclinical notesâ€™, npj Digital Medicine 2024 7:1, vol. 7, no. 1, pp. 1â€“13, May 2024, doi: \n10.1038/s41746-024-01079-8. \n[26] B. Hao et al., â€˜A GPT-based EHR modeling system for unsupervised novel disease detectionâ€™, J \nBiomed Inform, vol. 157, p. 104706, Sep. 2024, doi: 10.1016/J.JBI.2024.104706. \n[27] T. Kwon et al., â€˜Large Language Models Are Clinical Reasoners: Reasoning-Aware Diagnosis \nFramework with Prompt-Generated Rationalesâ€™, Proceedings of the AAAI Conference on Artificial \nIntelligence, vol. 38, no. 16, pp. 18417â€“18425, Mar. 2024, doi: 10.1609/AAAI.V38I16.29802. \n[28] O. Stroganov et al., â€˜Unpacking unstructured data: A pilot study on extracting insights from \nneuropathological reports of Parkinsonâ€™s Disease patients using large language modelsâ€™, Biol \nMethods Protoc, vol. 9, no. 1, Jan. 2024, doi: 10.1093/BIOMETHODS/BPAE072. \n[29] J. Wu et al., â€˜A hybrid framework with large language models for rare disease phenotypingâ€™, BMC \nMed Inform Decis Mak, vol. 24, no. 1, p. 289, Dec. 2024, doi: 10.1186/S12911-024-02698-\n7/FIGURES/5. \n[30] A. Wang, C. Liu, J. Yang, and C. Weng, â€˜Fine-tuning large language models for rare disease \nconcept normalizationâ€™, Journal of the American Medical Informatics Association, vol. 31, no. 9, \npp. 2076â€“2083, Sep. 2024, doi: 10.1093/JAMIA/OCAE133. \n[31] J. R. Petrie, T. J. Guzik, and R. M. Touyz, â€˜Diabetes, hypertension, and cardiovascular disease: \nclinical insights and vascular mechanismsâ€™, Canadian Journal of Cardiology, vol. 34, no. 5, pp. \n575â€“584, May 2018, doi: 10.1016/J.CJCA.2017.12.005. \n[32] S. Lee et al., â€˜Create: a new data resource to support cardiac precision healthâ€™, CJC Open, vol. 3, \nno. 5, pp. 639â€“645, May 2021, doi: 10.1016/j.cjco.2020.12.019. \n[33] W. A. Ghali and M. L. Knudtson, â€˜Overview of the Alberta provincial project for outcome \nassessment in coronary heart disease: on behalf of the APPROACH investigatorsâ€™, Can J Cardiol, \nvol. 16, no. 10, pp. 1225â€“1230, Oct. 2000. \n[34] D. A. Southern et al., â€˜Expanding the impact of a longstanding Canadian cardiac registry through \ndata linkage: challenges and opportunitiesâ€™, Int J Popul Data Sci, vol. 3, no. 3, p. 441, Sep. 2018, \ndoi: 10.23889/IJPDS.V3I3.441. \n26 \n \n[35] G. H. John, R. Kohavi, and K. Pfleger, â€˜Irrelevant features and the subset selection problemâ€™, in \nMachine Learning Proceedings, Jan. 1994, pp. 121â€“129. \n[36] B. MeskÃ³, â€˜Prompt engineering as an important emerging skill for medical professionals: tutorialâ€™, \nJ Med Internet Res, vol. 25, no. 1, p. e50638, Jan. 2023, doi: 10.2196/50638. \n[37] Z. Punthakee, R. Goldenberg, and P. Katz, â€˜Definition, classification and diagnosis of diabetes, \nprediabetes and metabolic syndromeâ€™, Can J Diabetes, vol. 42, pp. S10â€“S15, Apr. 2018, doi: \n10.1016/j.jcjd.2017.10.003. \n[38] A. Kumar et al., â€˜The Canadian cardiovascular society classification of acute atherothrombotic \nmyocardial infarction based on stages of tissue injury severity: an expert consensus statementâ€™, \nCanadian Journal of Cardiology, vol. 40, no. 1, pp. 1â€“14, Jan. 2023, doi: \n10.1016/j.cjca.2023.09.020. \n[39] D. M. Rabi et al., â€˜Hypertension Canadaâ€™s 2020 comprehensive guidelines for the prevention, \ndiagnosis, risk assessment, and treatment of hypertension in adults and childrenâ€™, Canadian \nJournal of Cardiology, vol. 36, no. 5, pp. 596â€“624, May 2020, doi: \n10.1016/J.CJCA.2020.02.086/ATTACHMENT/AE66F1DE-F1C1-4030-B2D4-\n34E112F0947E/MMC1.DOCX. \n[40] S. Lee et al., â€˜Electronic medical recordâ€“based case phenotyping for the Charlson conditions: \nscoping reviewâ€™, JMIR Med Inform, vol. 9, no. 2, p. e23934, Feb. 2021, doi: 10.2196/23934. \n[41] A. Q. Jiang et al., â€˜Mistral 7Bâ€™, arXiv preprint, Oct. 2023. \n[42] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah, â€˜Orca: progressive \nlearning from complex explanation traces of GPT-4â€™, arXiv preprint, Jun. 2023. \n[43] K. R. Crowder et al., â€˜The impact of high-sensitivity troponin implementation on hospital \noperations and patient outcomes in 3 tertiary care centersâ€™, Am J Emerg Med, vol. 33, no. 12, pp. \n1790â€“1794, Dec. 2015, doi: 10.1016/J.AJEM.2015.08.041. \n[44] H. Quan et al., â€˜Assessing validity of ICD-9-CM and ICD-10 administrative data in recording \nclinical conditions in a unique dually coded databaseâ€™, Health Serv Res, vol. 43, no. 4, p. 1424, \nAug. 2008, doi: 10.1111/J.1475-6773.2007.00822.X. \n[45] K. L. Tang, K. Lucyk, and H. Quan, â€˜Coder perspectives on physician-related barriers to \nproducing high-quality administrative data: a qualitative studyâ€™, Canadian Medical Association \nOpen Access, vol. 5, no. 3, pp. E617â€“E622, Aug. 2017, doi: 10.9778/CMAJO.20170036. \n[46] J. Wu et al., â€˜A hybrid framework with large language models for rare disease phenotypingâ€™, BMC \nMed Inform Decis Mak, vol. 24, no. 1, p. 289, Dec. 2024, doi: 10.1186/S12911-024-02698-\n7/FIGURES/5. \n[47] F. J. Dorfner et al., â€˜Biomedical Large Languages Models Seem not to be Superior to Generalist \nModels on Unseen Medical Dataâ€™, Aug. 2024. \n  ",
  "topic": "Health records",
  "concepts": [
    {
      "name": "Health records",
      "score": 0.7529191970825195
    },
    {
      "name": "Computer science",
      "score": 0.6224743127822876
    },
    {
      "name": "Electronic health record",
      "score": 0.5851706266403198
    },
    {
      "name": "Data science",
      "score": 0.4516914188861847
    },
    {
      "name": "Natural language processing",
      "score": 0.3935835063457489
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33821290731430054
    },
    {
      "name": "Health care",
      "score": 0.1991387903690338
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}