{
    "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration",
    "url": "https://openalex.org/W4214686755",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2004206603",
            "name": "Zheng Qu",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A2109010745",
            "name": "Liu Liu",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A2296979454",
            "name": "Fengbin Tu",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A2176762750",
            "name": "Zhaodong Chen",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A2105644262",
            "name": "Yufei Ding",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A2095844698",
            "name": "Yuan Xie",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2053171205",
        "https://openalex.org/W2883542588",
        "https://openalex.org/W2044033049",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2009832130",
        "https://openalex.org/W2152839228",
        "https://openalex.org/W2048266589",
        "https://openalex.org/W3024621361",
        "https://openalex.org/W2067523571",
        "https://openalex.org/W2125203716",
        "https://openalex.org/W2605347906",
        "https://openalex.org/W3017024317",
        "https://openalex.org/W3189877953",
        "https://openalex.org/W3001665736",
        "https://openalex.org/W2963122961",
        "https://openalex.org/W4238341497",
        "https://openalex.org/W2949591530",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W2483966489",
        "https://openalex.org/W4239722617",
        "https://openalex.org/W2891680100",
        "https://openalex.org/W2000967104",
        "https://openalex.org/W3099513244",
        "https://openalex.org/W2886557986",
        "https://openalex.org/W2883920103",
        "https://openalex.org/W3037132819",
        "https://openalex.org/W2178628967",
        "https://openalex.org/W2518628311",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4245199738",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W2963367920",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W2094756095"
    ],
    "abstract": "Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-Attention. In this paper, we present DOTA, an algorithm-Architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-To-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively. Â© 2022 Owner/Author.",
    "full_text": null
}