{
  "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration",
  "url": "https://openalex.org/W4214686755",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2004206603",
      "name": "Zheng Qu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2109010745",
      "name": "Liu Liu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2296979454",
      "name": "Fengbin Tu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2176762750",
      "name": "Zhaodong Chen",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2105644262",
      "name": "Yufei Ding",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2095844698",
      "name": "Yuan Xie",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2053171205",
    "https://openalex.org/W2883542588",
    "https://openalex.org/W2044033049",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2009832130",
    "https://openalex.org/W2152839228",
    "https://openalex.org/W2048266589",
    "https://openalex.org/W3024621361",
    "https://openalex.org/W2067523571",
    "https://openalex.org/W2125203716",
    "https://openalex.org/W2605347906",
    "https://openalex.org/W3017024317",
    "https://openalex.org/W3189877953",
    "https://openalex.org/W3001665736",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W4238341497",
    "https://openalex.org/W2949591530",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W2483966489",
    "https://openalex.org/W4239722617",
    "https://openalex.org/W2891680100",
    "https://openalex.org/W2000967104",
    "https://openalex.org/W3099513244",
    "https://openalex.org/W2886557986",
    "https://openalex.org/W2883920103",
    "https://openalex.org/W3037132819",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W2518628311",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4245199738",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2963367920",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W2094756095"
  ],
  "abstract": "Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-Attention. In this paper, we present DOTA, an algorithm-Architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-To-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively. Â© 2022 Owner/Author.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7334669232368469
    },
    {
      "name": "Scalability",
      "score": 0.6921440958976746
    },
    {
      "name": "Speedup",
      "score": 0.6593723893165588
    },
    {
      "name": "Transformer",
      "score": 0.6072497367858887
    },
    {
      "name": "Inference",
      "score": 0.5575971007347107
    },
    {
      "name": "Computer engineering",
      "score": 0.4992802143096924
    },
    {
      "name": "Architecture",
      "score": 0.4348231256008148
    },
    {
      "name": "Detector",
      "score": 0.4246845841407776
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34556710720062256
    },
    {
      "name": "Computer architecture",
      "score": 0.32716408371925354
    },
    {
      "name": "Parallel computing",
      "score": 0.22122743725776672
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}