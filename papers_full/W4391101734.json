{
  "title": "Training microrobots to swim by a large language model",
  "url": "https://openalex.org/W4391101734",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2124642420",
      "name": "Zhuoqun Xu",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2099022078",
      "name": "Lailai Zhu",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385681450",
    "https://openalex.org/W4386874599",
    "https://openalex.org/W6685527089",
    "https://openalex.org/W1994923984",
    "https://openalex.org/W1751034982",
    "https://openalex.org/W2907537824",
    "https://openalex.org/W3010884024",
    "https://openalex.org/W3106409546",
    "https://openalex.org/W2584231952",
    "https://openalex.org/W2785318639",
    "https://openalex.org/W2972006933",
    "https://openalex.org/W3090319161",
    "https://openalex.org/W6780695629",
    "https://openalex.org/W2902780642",
    "https://openalex.org/W6783541019",
    "https://openalex.org/W4200365144",
    "https://openalex.org/W3106249418",
    "https://openalex.org/W2789741714",
    "https://openalex.org/W3216170918",
    "https://openalex.org/W3130787521",
    "https://openalex.org/W4221162387",
    "https://openalex.org/W4285102606",
    "https://openalex.org/W4308056791",
    "https://openalex.org/W4294192828",
    "https://openalex.org/W3167148272",
    "https://openalex.org/W4224306307",
    "https://openalex.org/W4283213366",
    "https://openalex.org/W4221160483",
    "https://openalex.org/W4283259621",
    "https://openalex.org/W4322733747",
    "https://openalex.org/W4320559411",
    "https://openalex.org/W6857299822",
    "https://openalex.org/W4318719422",
    "https://openalex.org/W4388444796",
    "https://openalex.org/W4388960802",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4306808908",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4294982692",
    "https://openalex.org/W4310831983",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4353007316",
    "https://openalex.org/W6664722883",
    "https://openalex.org/W2032101074",
    "https://openalex.org/W4383994288",
    "https://openalex.org/W2087534032",
    "https://openalex.org/W4285032497",
    "https://openalex.org/W4297256711",
    "https://openalex.org/W4362621331",
    "https://openalex.org/W1504362584",
    "https://openalex.org/W3089524014",
    "https://openalex.org/W4387880184",
    "https://openalex.org/W4385571264",
    "https://openalex.org/W2171900491",
    "https://openalex.org/W4284879444",
    "https://openalex.org/W3216544037",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W3137480660",
    "https://openalex.org/W4206082746",
    "https://openalex.org/W2056370235",
    "https://openalex.org/W4233205734",
    "https://openalex.org/W3104267187",
    "https://openalex.org/W3041588149",
    "https://openalex.org/W2491934829",
    "https://openalex.org/W2964027982"
  ],
  "abstract": "Abstract Machine learning and artificial intelligence have recently represented a popular paradigm for designing and optimizing robotic systems across various scales. Recent studies have showcased the innovative application of large language models (LLMs) in industrial control [1] and in directing legged walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two prototypical microrobots for swimming in viscous fluids. Adopting a few-shot learning approach, we develop a minimal, unified prompt composed of only five sentences. The same concise prompt successfully guides two distinct articulated microrobots—the three-link swimmer and the three-sphere swimmer—in mastering their signature strokes. These strokes, initially conceptualized by physicists, are now effectively interpreted and applied by the LLM, enabling the microrobots to circumvent the physical constraints inherent to micro-locomotion. Remarkably, our LLM-based decision-making strategy substantially surpasses a traditional reinforcement learning method in terms of training speed. We discuss the nuanced aspects of prompt design, particularly emphasizing the reduction of monetary expenses of using GPT-4.",
  "full_text": "Training microrobots to swim by a large language\nmodel\nZhuoqun Xu \nNational University of Singapore\nLailai Zhu  (  lailai_zhu@nus.edu.sg )\nNational University of Singapore\nArticle\nKeywords:\nPosted Date: January 22nd, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3853112/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nT raining microrobots to swim by a large language model\nZhuoqun Xu 1 and Lailai Zhu 1, ∗\n1Department of Mechanical Engineering, National University of Singapore, 117575, Singapore\n(Dated: January 12, 2024)\nMachine learning and artiﬁcial intelligence have recently represented a popular paradigm for de-\nsigning and optimizing robotic systems across various scales. Recent studies have showcased the\ninnovative application of large language models (LLMs) in industrial control [\n1] and in directing\nlegged walking robots [ 2]. In this study , we utilize an LLM, GPT-4, to train two prototypical micro-\nrobots for swimming in viscous ﬂuids. Adopting a few-shot learning approach, we develop a minimal,\nuniﬁed prompt composed of only ﬁve sentences. The same concise prompt successfully guides two\ndistinct articulated microrobots—the three-link swimmer and the three-sphere swimmer—in master-\ning their signature strokes. These strokes, initially conceptualized by physicists, are now eﬀectively\ninterpreted and applied by the LLM, enabling the microrobots to circumvent the physical constraints\ninherent to micro-locomotion. Remarkably , our LLM-based decision-making strategy substantially\nsurpasses a traditional reinforcement learning method in terms of training speed. W e discuss the\nnuanced aspects of prompt design, particularly emphasizing the reduction of monetary expenses of\nusing GPT-4.\nA fundamental characteristic of living organisms is\ntheir capacity for locomotory motion, encompassing a\nspectrum of movements such as running, crawling, ﬂying,\nslithering, and swimming. Having evolved through the\nlong process of natural selection and adaptation, these\nmotility patterns observed in biological realm inspire the\ndesign of bionic articulated robots. They achieve loco-\nmotion by actuating their movable components similar\nto natural joints (or hinges and links), e.g., walking legs,\nﬂapping wings, and undulating ﬁns.\nA natural question arises: How can we actuate these\njoints concertedly to enhance the robot’s locomotory per-\nformance, environmental adaptiveness, and perturbation\nresilience? It corresponds to identifying the eﬀective time\nsequence of robotic actions—forces (and torques) exerted\non the moving elements or their translational (and rota-\ntional) velocities.\nThis question has be addressed by various means. F or\nexample, biological strategies from evolution can be mim-\nicked with heuristic adjustment. Besides, diverse schemes\nin the frame of optimal control theory have been es-\ntablished for robotic movement [\n3– 5]. Moreover, ma-\nchine learning has expedited the development of more au-\ntonomous and intelligent robotic controllers. Notably , re-\ninforcement learning (RL) as a popular machine learning\ntechnique has been applied to walking [\n6– 8], ﬂying [ 9, 10],\nand swimming [ 11– 38] robots at diﬀerent scales.\nY et, the recent surge in large language models (LLMs)\nor foundation models in general has spawned a new ma-\nchine learning paradigm, in-context learning (ICL) [\n39],\nfor decision-making and robotic manipulation. Unlike\nRL or other machine learning techniques, ICL precludes\nupdating neural network weights, instead learns to solve\nnew tasks during the inference phase via receiving text\nprompts that incorporate exemplar task demonstrations.\n∗ lailai_zhu@nus.edu.sg\nOwing to their extensive pre-training on diverse and huge\ndatasets, LLMs exhibit the ICL capacity and thus can\nperform a variety of complex tasks via ICL including\nreasoning [\n40], logic deduction [ 41, 42], solving mathe-\nmatical problems [ 43], sentiment analysis [ 44], language\ntranslation [ 45], code generation [ 46], medical diagnos-\ntics [ 47], and so on. More recently , LLM has been uti-\nlized for industrial and robotic control. In particular,\nGPT-4, an LLM pre-rained on internet-scale datasets,\nhas been successfully utilized for controlling HV AC [\n1]\nand for enabling terrestrial robots to walk [ 2] in simu-\nlated environments. Importantly , Ref. [ 1] reveals that\nthis LLM approach attains a performance comparable to\nthat of RL, but requiring fewer samples and incurring\nlower technical debt.\nIn this work, we explore the ability of LLMs to de-\nduce physical principles underlying robotic locomotion\nand subsequently leverage the understanding to design\nlocomotory gaits. Our speciﬁc focus is on microrobotic\nswimming at a vanishing Reynolds (Re) number, a sce-\nnario physically constrained by the dominance of viscous\nﬂuid forces over inertia forces. This conﬁguration enables\nexamining whether LLMs can perceive Purcell’s “scallop\ntheorem”, introduced by Edward Purcell in his seminal\n1977 lecture “Life at low Reynolds number” [\n48]. The\ntheorem posits that in the inertialess (Stokesian) limit,\ni.e., Re = 0 , reciprocal motions or actuations, exempli-\nﬁed by a scallop’s opening and closing, cannot generate\nnet movement. Consequently , a one-hinged microswim-\nmer fails in eﬀective propulsion in viscous ﬂuids, because\nits single degree of freedom (DOF) leads to inherently\nreciprocal movement. T o circumvent this physical con-\nstraint, he then proposed a two-hinged ‘simplest animal’.\nLater known as Purcell’s three-link swimmer (see Fig\n1\nleft upper panel), the minimal model can eﬀectively pro-\npel through non-reciprocal motions, enabled by the ad-\ndition of more than one DOF.\nHere, we explore the LLM, GPT-4, to navigate this\nPurcell’s three-link swimmer and another prototypical\n2\na\nd\n1\n( t )\nd\n2\n( t )\nd\n1\n( t +∆ t )\nd\n2\n( t +∆ t )\nAction: [\n˙\nd\n1\n,\n˙\nd\n2\n]( t )\nAction: [\n˙\nd\n1\n,\n˙\nd\n2\n]( t )\nLLM\nTime \nViscous hydrodynamic \nenvironment\nPrompt:\n1. Assign the objective  \n2. Impose constraints  \n3. Show historical data\n5. Generate next actions\n∇ · u =0\nµ ∇\n2\nu = ∇ p\nd\n1\n( t )\nd\n2\n( t )\na\nd\nc\n( t )\nr\nc\n( t )\nr\nc\n( t )\nd\n1\n( t +∆ t )\nd\n2\n( t +∆ t )\nd\nc\n( t +∆ t )\nr\nc\n( t +∆ t )\nr\nc\n( t +∆ t )\nX ( t +∆ t ) − X ( t )\nX ( t +∆ t ) − X ( t )\nPurcell’s swimmer\nNG’s swimmer\ne\ny\ne\nx\nTime \n4. Indicate long-term \nimpacts of the actions \nFig. 1. Diagrammatic representation of utilizing an LLM, GPT-4 adopted here, for microrobotic locomotion. W e have\ndeveloped a minimal, uniﬁed minimal prompt capable of instructing two model microswimmers—Purcell’s swimmer [\n48] and NG’\nswimmer [ 49]—to propel through highly viscous ﬂuids. These microswimmers’ movements are subject to viscous hydrodynamics\ngoverned by the Stokes equations: ∇ · u = 0 and µ∇2u = ∇p, where u and p denote the velocity and pressure ﬁelds,\nrespectively , with µ indicating the ﬂuid’s dynamic viscosity . Comprising only ﬁve sentences, the prompt eﬀectively orchestrates\nthe interaction between the swimmer and the LLM, directing the former to swim along the horizontal axis ( ex or −ex) with\nmaximal speed. The swimmer’s displacement is X = rc · ex, where rc denotes its geometric centroid.\nmodel—Najaﬁ-Golestanian (NG)’s three-sphere swim-\nmer [\n49] in inertialess viscous ﬂuids. These swimmers\ncommunicate with the LLM by responding to its gen-\nerated actions and sending a few task demonstrations\nback through text prompts. W e design a uniﬁed mini-\nmal prompt framework to coordinate the interaction be-\ntween GPT-4 and both swimmers. Remarkably , despite\nits training lacking physical data, GPT-4 enables the mi-\ncroswimmers to learn eﬃcient strokes, thereby overcom-\ning the low-Reynolds-number physical constraints.\nVISCOUS HYDRODYNAMIC ENVIRONMENTS\nFOR MICROSWIMMERS\nThe Purcell’s swimmer comprises three slender cylin-\ndrical links of length a and radius b, connected in se-\nquence by two planar hinges. These hinges allow the links\nto rotate relative to their neighbor(s). Propulsion is at-\ntainable by varying over time ( t) the relative angles, d1(t)\nand d2(t), between every two adjacent links. In contrast,\nNG’s swimmer constitutes three spheres with radius a,\naligned colinearly along their common axis along ex. The\nspheres are linked sequentially by two extensible links,\nwith lengths d1(t) and d2(t), respectively . Time-varying\nthe two DOF s enables this swimmer to translate in the\nex direction. Notably , we have intentionally adopted the\nsame notation a as the characteristic length of both mod-\nels. The same intention applies to their DOF s, d1 and d2.\nW e characterize the locomotion of both swimmers by\nthe velocities of their centroid coordinated at rc. Namely ,\nNG’s swimmer translates horizontally at a velocity of\n˙rcex, with the overdot signifying a time derivative. In\ncontrast, the Purcell’s swimmer exhibits two-dimensional\nmotion, reﬂected by the translational velocity ˙rc of its\nmiddle link’s centroid and the link’s rotational velocity\n˙dcez. Below, we will brieﬂy describe how to calculate\nthe swimming velocity of Purcell’s swimmer. The corre-\nsponding calculation for NG’s swimmer is given in Ma-\nterials and Methods.\n3\n0 10 20 30 40 50 60\n0\n-0.5\n0.5\n1\n1.5\n2\n2.5\nA B\nLearning \noutcome\nLearning outcome  (signature gaits of Purcell’s swimmer)\n(a)\n(b)\n(c)\n(d)\nLearning outcome (signature gaits of NG’s swimmer)\n(a)\n(b)(d)\n(c)\nRL\nRL\nLLM\nLLM\nPurcell’s swimmer NG’s swimmer\n0 5 10 15 20\n0\n-0.2\n0.2\n0.4\n0.6\n0.8\n1\nLearning \noutcome\nFig. 2. A, displacement X/a of Purcell’s swimmer versus its execution step n, which is trained by the LLM (solid line) and\nQ-learning-based RL (dashed line). The lower panel demonstrates the cycle of signature gaits learned by the swimmer. B,\nsame as A, but for NG’s swimmer.\nIn this study of Purcell’s swimmer, we allow only one\nangular DOF, either d1 or d2, to vary at a time. The\nselected DOF is restricted to a discrete set of angles,\n[−dmax, dmax]. Hence, the swimming action as the DOF’\nrate of change (ROC) is also discrete and drawn from\n[−2dmax/T, 0, 2dmax/T ], where T represents the gear-\nchanging time.\nT o map the swimmer’ stroke to its kinematics, we as-\nsume that its links possess a small slenderness ϵ = b/a ≪\n1. This allows for using the local resistive force theory\nto account for the swimming hydrodynamics. F or a link\nparameterized by the arc-length s ∈ [0, a], the hydrody-\nnamic force per unit length f is:\nf(s) = −\n[\nξ∥tt + ξ⊥ (I − tt)\n]\n· ˙r(s), (1)\nwhere ξ∥ and ξ⊥ represent the resistive force coeﬃcients,\nand t and r(s) denote the link’s tangent vector and\nlocal coordinates, respectively . Here, I is the identity\ntensor. W e consider the limit ϵ → 0 here, leading to\nξ⊥/ξ∥ → 2. In the inertialess ﬂow, the swimmer’s trans-\nlational and rotational velocities can be derived using the\nconstraints of net zero hydrodynamic force and torque\nsummed over all links, respectively . F or an individual\nlink, the force is\n∫a\n0 f(s)ds, and the torque about the\ncentroid is\n∫a\n0 [r(s) − rc] × f(s)ds. Applying the con-\nstraints, we derive the swimming velocities analytically\nusing Mathematica, with their formula given in Materials\nand Methods.\nW e build the physical environment in a dimensionless\nframework. By choosing 2dmax/T as the characteristic\nangular velocity , the dimensionless ROCs fall within the\nset of discrete integers, [−1, 0, 1]. This integer nature\nsigniﬁcantly aids ICL via LLMs, as these models com-\nmonly have diﬃculty in accurately interpreting ﬂoating\nnumbers [\n50].\nMETHODOLOGY: LLM PROMPT\nW e employ the few-shot prompting method with stan-\ndard GPT-4, instructing it to directly generate actions\nfor a microswimmer without task-speciﬁc ﬁne-tuning.\n4\nThe swimmer subsequently executes the received action\nin the physical environment, with the latest few histor-\nical snapshots stored in a textual buﬀer. This buﬀer is\nthen provided to GPT-4, aiding its subsequent decision-\nmaking.\nW e have designed a uniﬁed prompt that can guide both\nswimmers eﬀectively , with a particular focus on minimiz-\ning the text length and associated monetary cost. Our\nprompt comprises ﬁve functional sections, each contain-\ning a single sentence, thereby highlighting the minimal\ntextual elements required. Firstly , it sets the objective:\nto determine the ROC sequence in both DOF s, target-\ning the swimmer’s fastest long-term movement in a pre-\nscribed direction. The second sentence deﬁne constraints\non the DOF s, e.g., d1 ∈ [−dmax, dmax]. Third, we demon-\nstrate the latest nht historical records of the interactions\nbetween the LLM and the physical environment, leaving\nnht as a tunable parameter. The following prompt alerts\nthe LLM to the potential for long-term impacts of the\naction. Finally , the last sentence instructs the LLM to\ngenerate the next action constrained within a speciﬁed\ngroup.\nA. History clearing scheme\nOccasionally , we observe that the swimmer cannot de-\nvelop an eﬀective strategy , becoming ensnared in a cy-\ncle of erroneous strokes. This trapping failure may be\nattributed to the limitations inherent in the few-shot\nprompting method: the LLM struggles to suggest ap-\npropriate actions based solely on a brief history of un-\nsuccessful attempts and their adverse outcomes.\nT o circumvent this issue, we propose and implement\na history clearing scheme. Speciﬁcally , when the swim-\nmer’s displacement within the last few (this number is\npredetermined) steps falls below a threshold, such as\n0.01, we erase and subsequently rebuild the entire record\nof prior few-shot demonstrations. In practice, this simple\napproach has proven to be successful in preventing the\naforementioned entrapment.\nB. A voiding ﬂoating-point or negative numbers\nLLMs are known to struggle with comprehending\nﬂoating-point or negative numbers [\n2]. T o address this\nissue, we implement a transformation to the swimmer’s\ndisplacement X/a. Its raw value produced by the the-\noretical model is a ﬂoating number truncated to three\ndecimal places, a step taken to curtail monetary ex-\npenses. By predeﬁning a lower boundary , Xmin/a, for\nX/a, we apply the following transformation (X/a)LLM =\n(X/a − Xmin/a) × 1000, upon which the LLM receives a\ntransformed value (X/a)LLM as a positive integer.\nC. Saving money by using aliases extensively\nThroughout our study , we have emphasized saving the\nmonetary costs of using GPT-4, primarily by minimiz-\ning the text length. W e ﬁnd that a simple approach—\nextensively employing aliases—signiﬁcantly aids in min-\nimization. F or example, we use the alias ‘DOF’ in place\nof ‘degree of freedom’, and ‘ROC’ for ‘rate of change’.\nGPT-4 seems to fully comprehend such abbreviations,\nthereby enabling the feasibility of this cheap strategy .\nD. T emperature (randomness) within GPT-4\nIn the GPT-4 architecture, the ‘temperature’ parame-\nter indicative of the level of randomness, can be adjusted\nwithin the range [0, 1]. This adjustment allows for a bal-\nance between consistent output and creative exploration\nby LLM. Throughout this study , we have set the temper-\nature parameter to zero, focusing on deterministic eval-\nuations.\nRESUL TS AND OBSER V A TIONS\nIn this section, we demonstrate using GPT-4 to guide\nthe swimming microrobots. In general, the performance\ndepends on their initial conﬁguration, the number of his-\ntorical demonstrations nht, and the target direction (pos-\nitive or negative ex here).\nLLM-prompted Swimming Strokes\nAs shown in Fig.\n2, GPT-4 eﬀectively directs both\nmodel microrobots to swim consistently in a speciﬁc\ndirection—positive ex here. Remarkably , using the LLM\nas the decision maker, both robots take only one exe-\ncution step to acquire the well-known signature cycle of\nstrokes (see the lower panels of Fig.\n2). Such strokes,\nnon-reciprocal in time, have originally been perceived by\nphysicists [\n48, 49] to overcome the low-Reynolds-number\nconstraints. Moreover, the LLM-based control dramat-\nically outperforms the RL approach in terms of the ac-\nquisition speed. T rained by a Q-learning scheme, the\nRL agents for Purcell’s and NG’s swimmers require, re-\nspectively , ≈ 12 and ≈ 40 steps before learning these\nstrokes. Though Fig.\n2 illustrates the training process\nfor one speciﬁc initial conﬁguration of both swimmers,\nwe assert that the same prompt eﬀectively function for\nall the other three initial conﬁgurations.\nIn our LLM prompt, the number of historical demon-\nstration, nht, is an important parameter inﬂuencing the\nlearning performance. A larger nht facilitates ICL, yet in-\ncreasing the information exchanged between the prompt\nand GPT-4, consequently raising the monetary cost. As\nillustrated in Fig.\n3A & B, Purcell’s swimmer, with\nnht = 1 , successfully learns eﬀective gaits when directed\n5\n400 10 20 30\n0\n1.5\n2\n0.5\n1\nA\n400 10 20 30\n1\n1.5B\n0.5\n0\n-0.5\n400 10 20 30\nC\n0\n-0.5\n0.5\n1\n1.5\nD\n0.5\n0\n  1\n1.5\n-0.5\n-1\n400 10 20 30\nT arget direction: e\nx\nT arget direction: − e\nx\n(chosen)\n(chosen)\n(chosen)\n(chosen)\nFig. 3. Inﬂuence of the length nht of historical records on the swimmers’ learning performance—the swimmer’s displacement\nin the target direction, X/a (positive ex, left column) or −X/a (negative ex, right column) versus the execution step n. The\nupper and lower rows correspond to Purcell’s swimmer and NG’s swimmer, respectively .\nto swim in both the positive and negative ex directions.\nHowever, we refrain from selecting nht = 1 for other\nparts of this study . This decision is based on observa-\ntions that the learning performance degenerates when nht\nis increased to 2 in both directions, suggesting that the\nsuccess at nht = 1 is mostly attributed to a fortuitous co-\nincidence rather than a systematic pattern. Likewise, as\nshown in Fig.\n3C & D, we opt for nht = 3 and nht = 6 for\nNG’s swimmer when navigating in the ex and −ex direc-\ntions, respectively . T o be rigorous, we do not designate\nthese selected nht as the minimum necessary number of\nhistorical shots.\nW e further examine the robustness of this LLM-based\ndecision making process under thermal noise in the swim-\nming environment. Speciﬁcally , we randomly perturb the\nswimmer’s incremental displacement ∆ X following each\nexecution step, yielding a noisy movement ∆ X (1 + ζY ).\nHere, ζ indicates the noise level, and Y is a random num-\nber uniformly distributed in [−1, 1].\nF or each noise level, we perform 10 tasks, evaluating\nthe average displacement ⟨X⟩/a and the overall success\nrate p. The number of tasks, 10, is selected to balance\nstatistical signiﬁcance with monetary expenses. A task\nis considered successful if the swimmer learns to execute\nthree consecutive cycles of its signature gaits within 50\nsteps, without subsequent failures. Fig.\n4 illustrates how\nthe noise ζ aﬀects the swimmer’s learning performance.\nWith increasing ζ, we generally observe a decrease in\nboth ⟨X⟩/a and p, although the decline in performance\nis relatively modest. Notably , the success rate remains\naround p ≈ 0.75 at the highest noise level of ζ = 3 , indi-\ncating the overall resilience of this LLM-based approach.\nPrompt Engineering\nW e note that our prompt is signiﬁcantly simpler and\nshorter than those in related works [\n1, 2], mostly ow-\ning to the simplicity of our physical environment. The\nmajor simpliﬁcation we have made is summarized below.\nFirst, unlike these works, we do not use a system log\nto provide an overview of the task. It turns out to be\nunnecessary and is thus removed for reducing monetary\ncost. Secondly , we do not employ a trained controller as\n6\n0.5 1 1.5 2 2.5 30\n0\n1\n2\n0.5\n0\n1\n1.5\n0\n0.5\n1\n0\n0.5\n1\nA\nB\nζ\n⟨ X ⟩ /a⟨ X ⟩ /a\nFig. 4. A verage displacement ⟨X⟩/a and overall success rate\np for Purcell’s swimmer (A) and NG’s swimmer (B) guided\nalong the ex direction, under varying noise levels ζ. F or each\nlevel, 10 individual runs are conducted to obtain the statistics.\nComplete \n  prompt\nS1\nS2\nS3\nS4\nS5\nComplete \n  prompt\nS1\nS2\nS3\nS4\nS5\nComplete \n  prompt\nS1\nS2\nS3\nS4\nS5\nComplete \n  prompt\nS1\nS2\nS3\nS4\nS5\nA B\nC D\n0\n1\n0.2\n1\n0\n2.34\n2.34\n1.07\n0.46\n0\n0\n0\n1\n1\n0\n0.2\n1.69\n1.72\n1.18\n-0.10\n0\n0\nDisplacement: ⟨ X ⟩ /aSuccess rate: p\nFig. 5. Criticality of the ﬁve sentences, labeled S1 to S5, in\nthe prompt. Left column: how the success rate p of Purcell’s\nswimmer (A) and NG’s swimmer (C) guided in the ex direc-\ntion is aﬀected when a single sentence is removed from the\nprompt, in comparison to the results obtained from the com-\nplete prompt (12 o’clock in the pie chart). The right column\nmirrors the left, but instead evaluates the impact on the av-\nerage displacement ⟨X⟩/a , rather than the success rate p.\nin Ref. [\n2] to collect the initial batch of historical demon-\nstrations fed to the LLM. Neither do we incorporate any\nexpert demonstrations dataset as in Ref. [\n1] that facili-\ntates the LLM to learn the working principles underlying\nexpert policies.\nF urthermore, we have investigated whether the de-\nsigned prompt is minimal by assessing the indispensabil-\nity of each of the ﬁve sentences, labeled S1 to S5. T o\nevaluate the necessity of an individual sentence, we con-\nducted 10 runs with the respective sentence omitted from\nthe prompt. W e then analyzed the overall success rate\np and average displacement ⟨X⟩/a of both swimmers,\nas shown in Fig.\n5. Our comparative analysis reveals\nthat sentences S2, S3, and S5 are critically indispensable;\nthe absence of any of these results in failure of all runs.\nConversely , sentences S1 and S4 demonstrate weaker in-\ndispensability , with occasional task success despite their\nremoval.\nCONCLUSIONS AND DISCUSSIONS\nIn this work, we have demonstrated the hitherto un-\nreported usage of LLMs for eﬀectuating low-Reynolds-\nnumber propulsion of model microswimmers. Employ-\ning a few-shot prompting approach, we have designed a\nﬁve-sentence prompt for two minimal swimmers: Pur-\ncell’s three-link swimmer and NG’s three-sphere swim-\nmer. This uniﬁed prompt enables GPT-4 to generate\nthrough ICL, the optimal stroke patterns for both swim-\nmers, despite their distinct propulsion mechanisms.\nBased on the current successful demonstration of us-\ning LLMs for microrobotic manipulation, we identify sev-\neral potentially promising research directions: 1) tran-\nsitioning from a discrete to a continuous action space;\n2) steering microrobots to navigate through complex en-\nvironments encompassing non-Newtonian ﬂuid rheology ,\ncomplex conﬁnements, and obstacles; 3) facilitating co-\noperative swimming behaviors among microrobots.\nW e admit that certain hyperparameters in this prelimi-\nnary study were selected arbitrarily . F or instance, we set\nthe temperature parameter of GPT-4 to zero. In our\nforthcoming research, we aim to examine the inﬂuence of\nthis temperature setting and explore whether increasing\nit could potentially improve the exploratory capabilities\nof the LLM.\n7\nMA TERIALS AND METHODS\nHydrodynamics of Purcell’s Swimmer\nF or Purcell’s swimmer, the middle link moves with a\ntranslational velocity ˙rc = uex + vey and rotates with an\nangular velocity ˙dcez about its center. The two veloc-\nities as a function of actions\n(\n˙d1, ˙d2\n)\nare calculated by\nMathematica and are given by Eq. S( 2), Eq. S( 3), and\nEq. S( 4). F or brevity , all the variables in these equations\nare dimensionless, where we have chosen the rod length\na as the characteristic length, T/2dmax the characteristic\ntime, and 2dmax/T the characteristic angular velocity .\nHydrodynamics of NG’s Swimmer\nF or NG’s swimmer, we allow only one DOF, ei-\nther d1 or d2, to vary at a time. The selected DOF\nis restricted to a discrete set of lengths, [dmin, dmax].\nHence, the action is also discrete and drawn from\n[−(dmax − dmin)/T, 0, (dmax − dmin)/T ], where T is the\ntime a rod takes to change its length. F ollowing Ref. [\n51],\nthe dimensionless velocity of the swimmer reads:\n˙rc = 1\n6\n[( ˙d2 − ˙d1\nd1 + d2\n)\n+ 2\n( ˙d1\nd2\n−\n˙d2\nd1\n)]\n, (6)\nwhere we have selected a as the characteristic length,\n(dmax − dmin) /T as the characteristic velocity , and\naT/(dmax − dmin) as the characteristic time for the non-\ndimensionalization.\nData A vailability\nThe datasets used and/or analysed during the current\nstudy available from the corresponding author on reason-\nable request.\nCode A vailability\nThe GPT-4 prompt will be open-sourced upon the ac-\nceptance of this manuscript.\nACKNOWLEDGMENTS\nZ.X. is supported by the research scholarship from\nthe National University of Singapore and the China\nScholarship Council. L.Z. acknowledges the partial sup-\nport from the A*Star AME-YIRG grant (A2084c0175).\nSome computation of the work was performed on re-\nsources of the National Supercomputing Centre, Singa-\npore (https://www.nscc.sg).\n8\nu = 1\n2∆\n(\n˙d1 (262 sin (dc − d1) − 26 sin (d1 + dc) − 2 sin (2d1 + dc) + 18 sin ( dc − 2d1) − 16 sin (dc − d2) + 78 sin ( −d1 − d2 + dc)\n−18 sin (d1 − d2 + dc) + 8 sin ( −2d1 − d2 + dc) + 104 sin ( d2 + dc) + 126 sin ( −d1 + d2 + dc) + 30 sin ( d1 + d2 + dc)\n+24 sin (2d2 + dc) + 21 sin ( −d1 + 2d2 + dc) + sin ( d1 + 2d2 + dc) − 2 sin (2d1 + 2d2 + dc) − 4 sin (dc − 2d2)\n+9 sin (−d1 − 2d2 + dc) − 3 sin (d1 − 2d2 + dc) + 2 sin ( −2d1 − 2d2 + dc) + 36 sin( dc)) + ˙d2 (104 sin (dc − d1) − 16 sin (d1 + dc)\n−4 sin (2d1 + dc) + 24 sin ( dc − 2d1) − 26 sin (dc − d2) + 30 sin ( −d1 − d2 + dc) − 18 sin (d1 − d2 + dc) − 3 sin (2d1 − d2 + dc)\n+ sin (−2d1 − d2 + dc) + 262 sin ( d2 + dc) + 126 sin ( −d1 + d2 + dc) + 78 sin ( d1 + d2 + dc) + 9 sin (2 d1 + d2 + dc)\n+21 sin (−2d1 + d2 + dc) + 18 sin (2 d2 + dc) + 8 sin ( d1 + 2d2 + dc) + 2 sin (2 d1 + 2d2 + dc) − 2 sin (dc − 2d2)\n−2 sin (−2d1 − 2d2 + dc) + 36 sin( dc))) .\n(2)\nv = − 1\n2∆\n(\n˙d1 (262 cos (dc − d1) − 26 cos (d1 + dc) − 2 cos (2d1 + dc) + 18 cos ( dc − 2d1) − 16 cos (dc − d2) + 78 cos ( −d1 − d2 + dc)\n−18 cos (d1 − d2 + dc) + 8 cos ( −2d1 − d2 + dc) + 104 cos ( d2 + dc) + 126 cos ( −d1 + d2 + dc) + 30 cos ( d1 + d2 + dc)\n+24 cos (2d2 + dc) + 21 cos ( −d1 + 2d2 + dc) + cos ( d1 + 2d2 + dc) − 2 cos (2d1 + 2d2 + dc) − 4 cos (dc − 2d2)\n+9 cos (−d1 − 2d2 + dc) − 3 cos (d1 − 2d2 + dc) + 2 cos ( −2d1 − 2d2 + dc) + 36 cos( dc)) + ˙d2 (104 cos (dc − d1) − 16 cos (d1 + dc)\n−4 cos (2d1 + dc) + 24 cos ( dc − 2d1) − 26 cos (dc − d2) + 30 cos ( −d1 − d2 + dc) − 18 cos (d1 − d2 + dc) − 3 cos (2d1 − d2 + dc)\n+ cos (−2d1 − d2 + dc) + 262 cos ( d2 + dc) + 126 cos ( −d1 + d2 + dc) + 78 cos ( d1 + d2 + dc) + 9 cos (2 d1 + d2 + dc)\n+21 cos (−2d1 + d2 + dc) + 18 cos (2 d2 + dc) + 8 cos ( d1 + 2d2 + dc) + 2 cos (2 d1 + 2d2 + dc) − 2 cos (dc − 2d2)\n−2 cos (−2d1 − 2d2 + dc) + 36 cos( dc))) .\n(3)\n˙dc = 2\n∆\n(\n˙d1 (102 cos (d1) + 2 cos (2 d1) − 6 cos (d1 − d2) − 4 cos (2d2) + 42 cos ( d1 + d2) + 2 cos (2 ( d1 + d2)) + 9 cos ( d1 + 2d2)\n−3 cos (d1 − 2d2) + 108) + ˙d2 (4 cos (2d1) + 6 cos ( d1 − d2) + 3 cos (2 d1 − d2) − 102 cos (d2) − 2 cos (2d2) − 42 cos (d1 + d2)\n−2 cos (2 ( d1 + d2)) − 9 cos (2d1 + d2) − 108)) .\n(4)\n∆ = 3 (136 cos ( d1) + 14 cos (2 d1) − 8 cos (d1 − d2) − cos (2 (d1 − d2)) − 4 cos (2d1 − d2) + 14 cos (2 d2) + 56 cos ( d1 + d2)\n+136 cos (d2) + 3 cos (2 ( d1 + d2)) + 12 cos (2 d1 + d2) + 12 cos ( d1 + 2d2) − 4 cos (d1 − 2d2) + 282) .\n(5)\n9\n[1] L. Song, C. Zhang, L. Zhao, and J. Bian, “Pre-trained\nlarge language models for industrial control,” arXiv\npreprint arXiv:2308.03028 , 2023.\n[2] Y.-J. W ang, B. Zhang, J. Chen, and K. Sreenath,\n“Prompt a robot to walk with large language models,”\narXiv preprint arXiv:2309.09969 , 2023.\n[3] C. Abdallah, D. M. Dawson, P . Dorato, and M. Jamshidi,\n“Survey of robust control for rigid robots,” IEEE Control\nSystems Magazine , vol. 11, no. 2, pp. 24–30, 1991.\n[4] P .-B. Wieber, R. T edrake, and S. Kuindersma, “Modeling\nand control of legged robots,” in Springer handbook of\nrobotics, pp. 1203–1234, Springer, 2016.\n[5] E. R. W estervelt, J. W. Grizzle, C. Chevallereau, J. H.\nChoi, and B. Morris, Feedback Control of Dynamic\nBipedal Robot Locomotion . CRC press, 2018.\n[6] H. Benbrahim and J. A. F ranklin, “Biped dynamic walk-\ning using reinforcement learning,” Robotics and Au-\ntonomous systems , vol. 22, no. 3-4, pp. 283–302, 1997.\n[7] H. Kimura, T. Y amashita, and S. Kobayashi, “Rein-\nforcement learning of walking behavior for a four-legged\nrobot,” IEEJ Transactions on Electronics, Information\nand Systems , vol. 122, no. 3, pp. 330–337, 2002.\n[8] T. Haarnoja, S. Ha, A. Zhou, J. T an, G. T ucker, and\nS. Levine, “Learning to walk via deep reinforcement\nlearning,” arXiv preprint arXiv:1812.11103 , 2018.\n[9] J. Xu, T. Du, M. F oshey , B. Li, B. Zhu, A. Schulz, and\nW. Matusik, “Learning to ﬂy: Computational controller\ndesign for hybrid UA Vs with reinforcement learning,”\nACM Trans. Graph. , vol. 38, no. 4, pp. 1–12, 2019.\n[10] P . Becker-Ehmck, M. Karl, J. Peters, and P . van der\nSmagt, “Learning to ﬂy via deep model-based reinforce-\nment learning,” arXiv preprint arXiv:2003.08876 , 2020.\n[11] M. Gazzola, A. A. T chieu, D. Alexeev, A. de Brauer, and\nP . Koumoutsakos, “Learning to school in the presence\nof hydrodynamic interactions,” J. Fluid Mech. , vol. 789,\npp. 726–749, 2016.\n[12] S. Colabrese, K. Gustavsson, A. Celani, and L. Biferale,\n“Flow navigation by smart microswimmers via rein-\nforcement learning,” Phys. Rev. Lett. , vol. 118, no. 15,\np. 158004, 2017.\n[13] S. V erma, G. Novati, and P . Koumoutsakos, “Eﬃcient\ncollective swimming by harnessing vortices through deep\nreinforcement learning,” Proc. Natl. Acad. Sci. USA ,\nvol. 115, no. 23, pp. 5849–5854, 2018.\n[14] E. Schneider and H. Stark, “Optimal steering of a smart\nactive particle,” EPL (Europhysics Letters) , vol. 127,\nno. 6, p. 64003, 2019.\n[15] M. Mirzakhanloo, S. Esmaeilzadeh, and M.-R. Alam,\n“Active cloaking in Stokes ﬂows via reinforcement learn-\ning,” J. Fluid Mech. , vol. 903, 2020.\n[16] A. C. H. T sang, P . W. T ong, S. Nallan, and O. S. Pak,\n“Self-learning how to swim at low Reynolds number,”\nPhys. Rev. Fluids , vol. 5, no. 7, p. 074101, 2020.\n[17] J. Qiu, W. Huang, C. Xu, and L. Zhao, “Swimming strat-\negy of settling elongated micro-swimmers by reinforce-\nment learning,” Sci. China: Phys. Mech. Astron. , vol. 63,\nno. 8, pp. 1–9, 2020.\n[18] Y. Jiao, F. Ling, S. Heydari, N. Heess, J. Merel, and\nE. Kanso, “Learning to swim in potential ﬂow,” Phys.\nRev. Fluids , vol. 6, no. 5, p. 050505, 2021.\n[19] H. Deng, P . Burke, D. Li, and B. Cheng, “Design and\nexperimental learning of swimming gaits for a magnetic,\nmodular, undulatory robot,” in 2021 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems\n(IROS), pp. 9562–9568, IEEE, 2021.\n[20] J. K. Alageshan, A. K. V erma, J. Bec, and R. Pan-\ndit, “Machine learning strategies for path-planning mi-\ncroswimmers in turbulent ﬂows,” Phys. Rev. E , vol. 101,\nno. 4, p. 043110, 2020.\n[21] S. Muiños-Landin, A. Fischer, V. Holubec, and F. Cichos,\n“Reinforcement learning with artiﬁcial microswimmers,”\nSci. Rob. , vol. 6, no. 52, 2021.\n[22] M. Gerhard, A. Jayaram, A. Fischer, and T. Speck,\n“Hunting active Brownian particles: Learning optimal\nbehavior,” Phys. Rev. E , vol. 104, no. 5, p. 054614, 2021.\n[23] P . Gunnarson, I. Mandralis, G. Novati, P . Koumoutsakos,\nand J. O. Dabiri, “Learning eﬃcient navigation in vortical\nﬂow ﬁelds,” Nat. Commun , vol. 12, no. 1, p. 7143, 2021.\n[24] M. R. Behrens and W. C. Ruder, “Smart magnetic micro-\nrobots learn to swim with deep reinforcement learning,”\nAdv. Intell. Syst. , vol. 4, no. 10, p. 2200023, 2022.\n[25] W. Liu, K. Bai, X. He, S. Song, C. Zheng, and X. Liu,\n“Fishgym: A high-performance physics-based simula-\ntion framework for underwater robot learning,” in 2022\nInternational Conference on Robotics and Automation\n(ICRA), pp. 6268–6275, IEEE, 2022.\n[26] Y. Zhu, J.-H. Pang, T. Gao, and F.-B. Tian, “Learning to\nschool in dense conﬁgurations with multi-agent deep re-\ninforcement learning,” Bioinspir. Biomim. , vol. 18, no. 1,\np. 015003, 2022.\n[27] Q. W ang, Z. Hong, and Y. Zhong, “Learn to swim: On-\nline motion control of an underactuated robotic eel based\non deep reinforcement learning,” BIROB, vol. 2, no. 4,\np. 100066, 2022.\n[28] F. Borra, L. Biferale, M. Cencini, and A. Celani, “Rein-\nforcement learning for pursuit and evasion of microswim-\nmers at low Reynolds number,” Phys. Rev. Fluids , vol. 7,\nno. 2, p. 023103, 2022.\n[29] G. Chen, Y. Lu, X. Y ang, and H. Hu, “Reinforcement\nlearning control for the swimming motions of a beaver-\nlike, single-legged robot based on biological inspiration,”\nRob Auton Syst , vol. 154, p. 104116, 2022.\n[30] Z. Zou, Y. Liu, Y.-N. Y oung, O. S. Pak, and A. C. H.\nT sang, “Gait switching and targeted navigation of mi-\ncroswimmers via deep reinforcement learning,” Commun.\nPhys., vol. 5, no. 1, p. 158, 2022.\n[31] M. Nasiri and B. Liebchen, “Reinforcement learning of\noptimal active particle navigation,” New J. Phys. , vol. 24,\nno. 7, p. 073042, 2022.\n[32] G. Zhu, W.-Z. F ang, and L. Zhu, “Optimizing low-\nReynolds-number predation via optimal control and re-\ninforcement learning,” J. Fluid Mech. , vol. 944, p. A3,\n2022.\n[33] K. Qin, Z. Zou, L. Zhu, and O. S. Pak, “Reinforcement\nlearning of a multi-link swimmer at low Reynolds num-\nbers,” Phys. Fluids , vol. 35, no. 3, 2023.\n[34] Z. E. Khiyati, R. Chesneaux, L. Giraldi, and J. Bec,\n“Steering undulatory micro-swimmers in a ﬂuid ﬂow\nthrough reinforcement learning,” Eur. Phys. J. E , vol. 46,\nno. 6, p. 43, 2023.\n10\n[35] C. Mo, Q. F u, and X. Bian, “Chemotaxis of an elastic\nﬂagellated microrobot,” Phys. Rev. E , vol. 108, no. 4,\np. 044408, 2023.\n[36] J. Hu and T. Dear, “Guided deep reinforcement learn-\ning for articulated swimming robots,” arXiv preprint\narXiv:2301.13072, 2023.\n[37] L.-S. Lin, K. Y asuda, K. Ishimoto, and S. Ko-\nmura, “Emergence of odd elasticity in a microswim-\nmer using deep reinforcement learning,” arXiv preprint\narXiv:2311.01973, 2023.\n[38] A. Xu, H.-L. W u, and H.-D. Xi, “Control the migration\nof self-propelling particles in thermal turbulence via rein-\nforcement learning algorithm,” in Proceedings of the IU-\nTAM Symposium on Turbulent Structure and Particles-\nTurbulence Interaction (X. Zheng and S. Balachandar.,\neds.), (Cham), pp. 313–325, Springer Nature Switzer-\nland, 2024.\n[39] Q. Dong, L. Li, D. Dai, C. Zheng, Z. W u, B. Chang,\nX. Sun, J. Xu, and Z. Sui, “A survey for in-context learn-\ning,” arXiv preprint arXiv:2301.00234 , 2022.\n[40] J. W ei, X. W ang, D. Schuurmans, M. Bosma, F. Xia,\nE. Chi, Q. V. Le, D. Zhou, et al. , “Chain-of-thought\nprompting elicits reasoning in large language models,”\nAdvances in Neural Information Processing Systems ,\nvol. 35, pp. 24824–24837, 2022.\n[41] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. T ay ,\nH. W. Chung, A. Chowdhery , Q. V. Le, E. H. Chi,\nD. Zhou, et al. , “Challenging BIG-Bench tasks and\nwhether chain-of-thought can solve them,” arXiv preprint\narXiv:2210.09261, 2022.\n[42] A. Creswell, M. Shanahan, and I. Higgins, “Selection-\ninference: Exploiting large language models for\ninterpretable logical reasoning,” arXiv preprint\narXiv:2205.09712, 2022.\n[43] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer,\nH. Michalewski, V. Ramasesh, A. Slone, C. Anil,\nI. Schlag, T. Gutman-Solo, et al. , “Solving quantita-\ntive reasoning problems with language models,” Advances\nin Neural Information Processing Systems , vol. 35,\npp. 3843–3857, 2022.\n[44] R. Mao, Q. Liu, K. He, W. Li, and E. Cambria, “The bi-\nases of pre-trained language models: An empirical study\non prompt-based sentiment analysis and emotion detec-\ntion,” IEEE Trans. Aﬀect. Comput. , 2022.\n[45] S. Agrawal, C. Zhou, M. Lewis, L. Zettlemoyer, and\nM. Ghazvininejad, “In-context examples selection for\nmachine translation,” arXiv preprint arXiv:2212.02437 ,\n2022.\n[46] P . V aithilingam, T. Zhang, and E. L. Glassman, “Expec-\ntation vs. experience: Evaluating the usability of code\ngeneration tools powered by large language models,” in\nCHI Ext. Abstr. , pp. 1–7, 2022.\n[47] Z. Liu, X. Y u, L. Zhang, Z. W u, C. Cao, H. Dai, L. Zhao,\nW. Liu, D. Shen, Q. Li, et al. , “Deid-GPT: Zero-shot\nmedical text de-identiﬁcation by GPT-4,” arXiv preprint\narXiv:2303.11032, 2023.\n[48] E. M. Purcell, “Life at low Reynolds number,” Am. J.\nPhys., vol. 45, no. 1, pp. 3–11, 1977.\n[49] A. Najaﬁ and R. Golestanian, “Simple swimmer at low\nReynolds number: Three linked spheres,” Phys. Rev. E ,\nvol. 69, no. 6, p. 062901, 2004.\n[50] S. Mirchandani, F. Xia, P . Florence, B. Ichter, D. Driess,\nM. G. Arenas, K. Rao, D. Sadigh, and A. Zeng, “Large\nlanguage models as general pattern machines,” arXiv\npreprint arXiv:2307.04721 , 2023.\n[51] R. Golestanian and A. Ajdari, “Analytic results for the\nthree-sphere swimmer at low Reynolds number,” Phys.\nRev. E , vol. 77, no. 3, p. 036308, 2008.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6281026601791382
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5559560656547546
    },
    {
      "name": "Reinforcement learning",
      "score": 0.4964514374732971
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4468046724796295
    },
    {
      "name": "Robot",
      "score": 0.4402046203613281
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33628571033477783
    },
    {
      "name": "Simulation",
      "score": 0.3301028609275818
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}