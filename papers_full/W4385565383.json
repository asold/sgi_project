{
  "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
  "url": "https://openalex.org/W4385565383",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2704968898",
      "name": "Victor Dibia",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2983681716",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3193539362",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W4313188278",
    "https://openalex.org/W4320351362",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4297262023",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W2888611489",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3081277912",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2964101465",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4302275696",
    "https://openalex.org/W4244888246",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W4307887575",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2342249984"
  ],
  "abstract": "Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 3: System Demonstrations, pages 113–126\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nLIDA: A Tool for Automatic Generation of Grammar-Agnostic\nVisualizations and Infographics using Large Language Models\nVictor Dibia\nMicrosoft Research\nvictordibia@microsoft.com\nAbstract\nSystems that support users in the automatic\ncreation of visualizations must address sev-\neral subtasks - understand the semantics of\ndata, enumerate relevant visualization goals\nand generate visualization specifications. In\nthis work, we pose visualization generation as\na multi-stage generation problem and argue that\nwell-orchestrated pipelines based on large lan-\nguage models ( LLM s) and image generation\nmodels (IGM s) are suitable to addressing these\ntasks. We present LIDA , a novel tool for gen-\nerating grammar-agnostic visua lizations and\ninfographics. LIDA comprises of 4 modules\n- A SUMMARIZER that converts data into a\nrich but compact natural language summary,\na GOAL EXPLORER that enumerates visualiza-\ntion goals given the data, a VIS GENERATOR\nthat generates, refines, executes and filters visu-\nalization code and an INFOGRAPHER module\nthat yields data-faithful stylized graphics using\nIGM s. LIDA provides a python api, and a hy-\nbrid USER INTERFACE (direct manipulation\nand multilingual natural language) for interac-\ntive chart, infographics and data story genera-\ntion. Code and demo are available at this url -\nhttps://microsoft.github.io/lida/\n1 Introduction\nVisualizations make data accessible by reducing\nthe cognitive burden associated with extracting in-\nsights from large tabular datasets. However, vi-\nsualization authoring is a complex creative task,\ninvolving multiple steps. First the user must build\nfamiliarity with the dataset (content and semantics)\nand enumerate a set of relevant goals or hypothe-\nses that can be addressed using the data. Next,\nusers must select the right visualization representa-\ntion (marks, transformations and layout) for each\ngoal. Finally, the user must implement the visu-\nalization either as code or using available direct\nmanipulation interfaces. Each of these steps re-\nquire expertise, and can be tedious as well as error\nprone for users with limited visualization experi-\nence (novices). Existing research has sought to\naddress these challenges by automating the visual-\nization (AUTOVIZ ) creation process, given a dataset\n(Podo et al., 2023). Automation may occur in two\nmodes: i.) fully automated - the system automati-\ncally generates visualizations relevant to the data\nii.) semi-automated - the user specifies their goals\nand the system generates visualizations that address\nthese goals. The former mode is valuable for users\nunfamiliar with the data and the latter is valuable\nfor users with some familiarity with the data and\nthe visualization task.\nConsequently, a successful AUTOVIZ tool must\nexcel at each of several subtasks - understand the\nsemantics of the data, enumerate relevant visual-\nization goals and generate visualization specifica-\ntions that meet syntax, design, task and perceptual\nrequirements of these goals (Podo et al., 2023).\nFurthermore, given the target demographic (novice\nusers), such a tool must support the user by offering\nNL (NL) interaction modalities (Mitra et al., 2022;\nNarechania et al., 2020; Chen et al., 2022), affor-\ndances to control system behavior and sense mak-\ning tools to understand and debug/verify system\nbehavior. While related work has addressed aspects\nof the AUTOVIZ task, there are several known limi-\ntations (Podo et al., 2023) such as they: (i) rely on\nheuristics that are limited in coverage, challenging\nto craft and tedious to maintain (Wongsuphasawat\net al., 2017). (ii) require significant user interac-\ntion to generate visualizations (Wongsuphasawat\net al., 2017; Moritz et al., 2018). (iii) implement\nautomated approaches that offer limited control\nover system input and output (Dibia and Demiralp,\n2019) (iv) require grammar (or chart type) specific\ntraining data and model architectures (Dibia and\nDemiralp, 2019; Luo et al., 2018) for each sub task,\n(v) do not consider alternative chart representation\nformats such as infographics.\nConcurrently, advances in large foundation mod-\n113\nGO AL EXPL ORER\n VIZ GENERA T OR\n INFOGRAPHER\nSUMM ARIZER \nConv er t datasets int o a rich but \ncompact natur al language \nr epr esentation (context).\nThe cars dataset contains \ntechnical specifications for \ncars and has 9 fields - Name, \nMiles_per_Gallon, Cylinders, \nDisplacement, Horsepower , \nW eight_in_lbs, Acceler ation, \nY ear , Origin ..\nHist ogr am of Miles per \ngallo\nPlot of miles per gallon vs \nhorse powe\nT r ends in miles per gallon \no v er time\nA v er age horsepower per \ncountr y\nGener ate a set of potential \n“goals*” giv en the dataset \ncontext.\nGener ate, e v aluate, r epair , filter \nex ecute\n  \nand  visualization code \nt o yield specifications* .\nGener ate styliz ed infogr aphics \nbased based on visualization \nand style pr ompts.\nOutput Output Output Output\n* goals ma y also be dir ectly pr o vided b y \nthe user . Suppor ts multi-lingual input. * specification ma y be in any \npr ogr amming language or gr ammar . * Style pr ompt :  line s k etch ar t, line \ndr awing\nCars.csv Gener ate code in visualization \nbased on context and goal\n“\n”\nR U L E S  +  L L M L L M I G ML L M\nFigure 1: LIDA generates visualizations and infographics across 4 modules - data summarization, goal exploration,\nvisualization generation and infographics generations. Example output from each module is shown.\nRef er ence Visualization Gener ated styliz ed infogr aphics\nunder water ar t, shells pastel ar t oil on canv as, impast o\nFigure 2: Example data-faithful infographics and associated style prompts generated with LIDA .\nels (Bommasani et al., 2021) have shown state of\nthe art performance on a variety of creative tasks\nsuch as multilingual text generation, code genera-\ntion, image captioning, image generation, and im-\nage editing. In this work, we argue that the vast\ncapabilities of these models can beassembled to ad-\ndress the AUTOVIZ task, whilst addressing the lim-\nitations of existing approaches. This work makes\nthe following contributions:\n• We present a novel multi-stage, modular ap-\nproach (Fig 1) for the automatic generation\nof data visualization and infographics using\nLLM s1. Specifically, we (i) Efficiently represent\ndatasets as NL summaries, suitable as ground-\ning context for an LLM to address visualization\ntasks. (ii) Generate a set of visualization goals\nusing LLM s. Importantly, we leverage prompt\nengineering to steer the model towards generat-\n1This work primarily utilizes the OpenAI gpt-3.5-turbo-x\nline of models for text and code generation.\ning correct visualization that follow best prac-\ntices (see Appendix C). (iii) Apply LLM s to\ngenerate grammar-agnostic visualization speci-\nfication based on generated (or human provided)\ngoals. (iv) Provide a hybrid interface that sup-\nports traditional direct manipulation controls\n(e.g., manually select which fields to explore)\nand a rich multilingual NL interface to sup-\nport user’s with varied skill/experience. (v) Ap-\nply text-conditioned image generation models\n(IGM ) models in generating stylized infograph-\nics that are both informative (generally faithful\nto data), aesthetically pleasing, memorable and\nengaging (see section 2.3).\n• We introduce metrics for evaluating LLM -\nenabled visualization tools, including a metric\nfor pipeline reliability (visualization error rate -\nVER ), and visualization quality (self-evaluated\nvisualization quality - SEVQ ) (see section 4).\n114\n• We implement our approach in an Open Source\nlibrary - LIDA 2. LIDA provides a python api,\na web api and a rich web interface useful for\nresearch and practical applications.\nCompared to existing AUTOVIZ approaches, LIDA\nproposes an implementation that is simplified\n(eliminates the need for subtask-specific mod-\nels), general (can be adapted to generate visual-\nizations in any programming language or gram-\nmar), flexible (individual modules can be opti-\nmized) and scalable (the system performance will\nimprove with advances in the underlying LLM ).\nTaken together, these contributions provide build-\ning blocks towards complex workflows such as\nvisualization translation, chart question answering\n(with applications in accessibility of charts), auto-\nmated data exploration and automated data stories.\nTo the best of our knowledge, LIDA is the first\ntool to formulate visualization/infographic genera-\ntion as a multi-step generation task and demonstrate\nan end-to-end pipeline that addresses a variety of\nsubtasks.\n2 Related Work\nLIDA is informed by research on large foundation\nmodels applied to creative tasks across modalities\nsuch as text and images, and advances in automated\ngeneration of visualizations and infographics.\n2.1 Foundation Models for Creative Tasks\nAdvances in large transformer-based (Vaswani\net al., 2017) models trained on massive amounts\nof data (terabytes of text and images) have led\nto a paradigm shift where a single model demon-\nstrates state of the art task performance across mul-\ntiple data modalities such as text, images, audio\nand video. These models, also known as founda-\ntion models (Bommasani et al., 2021), have been\nshown to be effective for a variety of human cre-\nativity tasks. LLM s like the GPT3 series (Brown\net al., 2020), OPT (Zhang et al., 2022), PALM\n(Chowdhery et al., 2022), LAMBDA (Cohen et al.,\n2022) learn complex semantics of language allow-\ning them to be effective in tasks such as text sum-\nmarization, question answering. Code LLM s such\nas Codex (Chen et al., 2021), AlphaCode (Li et al.,\n2022), InCoder (Fried et al., 2022) show state of\nthe art performance on a suite of code intelligence\ntasks. Finally, models such as CLIP (Radford et al.,\n2https://microsoft.github.io/lida/.\n2021), DALLE (Ramesh et al., 2022, 2021) and La-\ntent Diffusion (Rombach et al., 2022) have shown\nstate of the art capabilities on image generation\ntasks such as image captioning, image editing, and\nimage generation.\nIn this work, we adopt insights from Program-\nAided Language models (Gao et al., 2022) - a setup\nwhere LLM s generate programs as the intermedi-\nate reasoning steps, but offload the solution step to\na runtime such as a python interpreter. We lever-\nage the language modeling capabilities of LLM s in\ngenerating semantically meaningful visualization\ngoals, and their code writing capabilities in gener-\nating visualization code which is compiled to yield\nvisualizations. These visualizations (images) are\nthen used as input to image generation models in\ngenerating stylized infographics.\n2.2 Automated Visualization ( AUTOVIZ )\nExtant AUTOVIZ research have explored multiple\napproaches such as heuristics, task decomposition\nor learning based approaches. Heuristics-based ap-\nproaches explore properties of data in generating\na search space of potential visualizations (Wong-\nsuphasawat et al., 2017), ranking these visualiza-\ntions based on quality attributes (Luo et al., 2018;\nMoritz et al., 2018) and presenting them to the\nuser. For example, DeepEye (Luo et al., 2018)\nenumerates all possible visualizations and classi-\nfies/ranks them as “good” or “bad” using a binary\ndecision tree classifier while V oyager (Wongsupha-\nsawat et al., 2017) uses heuristics to enumerate the\nspace of visualizations. However, heuristics can\nbe tedious to maintain, may have poor coverage\nof the visualization space and does not leverage\ninformation encoded in existing datasets. More\nrecent work has explored a task decomposition\napproach where the AUTOVIZ process is decom-\nposed into multiple tasks that are solved individu-\nally via specialized tools and aggregated to yield\nvisualizations (Narechania et al., 2020; Chen et al.,\n2022; Wang et al., 2022b). For example NL4DV\n(Narechania et al., 2020) implements a custom\nquery engine that parses natural language queries,\nidentifies attributes/tasks and generates Vega-Lite\nspecifications. A limitation of task decomposition\napproaches is that they are bottlenecked by the\nimplementation performance for each step (e.g.,\nlimitations with models for disambiguating natural\nlanguage queries as seen in NL4DV (Narechania\net al., 2020)). Finally, end-to-end learning-based\n115\napproaches seek to automatically learn mappings\nfrom data directly to generated visualizations. For\nexample, Data2Vis (Dibia and Demiralp, 2019)\n(the most relevant work to this study) uses a se-\nquence to sequence model that implicitly addresses\nAUTOVIZ subtasks by learning a mapping from\nraw JSON data sampled from datasets to Vega-Lite\n(Satyanarayan et al., 2017) specifications. Some\nlimitations of current learning approaches is that\nthey are limited to a single grammar, require cus-\ntom models, custom paired training data and train-\ning objectives (Dibia and Demiralp, 2019; Luo\net al., 2018; Chen et al., 2022) for each supported\ngrammar, and do not provide a path to generating\ninfographics. Furthermore, they do not provide\nmechanisms for fine-grained control of visualiza-\ntion output or provide robust error detection and\nrecovery strategies.\nLIDA addresses these limitations in several ways:\n(i) Leverages patterns learned by LLM s from mas-\nsive language and code dataset, applying this\nknowledge to subtasks. (ii) Provides a single gram-\nmar-agnostic pipeline that generates visualization\nin multiple programming languages and visualiza-\ntion grammars. (iii) Supports natural language\nbased control of generated visualizations. (iv) lever-\nage emergent capabilities of large language models\nsuch chain of thought reasoning to improve reliabil-\nity of generated text/code (Kojima et al., 2022; Wei\net al., 2022; Shi et al., 2022a), model calibration\n(Kadavath et al., 2022) (predictions on correctness\nprobabilities of visualizations) as well as self-con-\nsistency (Wang et al., 2022a) in ranking/filtering\nresults. (v) provides a mechanism for generating\ninfographics that are data-faithful and aesthetically\npleasing. (vi) supports a fully automatic mode\nwhere an LLM is used to discover meaningful goal-\ns/hypotheses (fields to visualize, questions to ask)\nor a semi automatic mode where the user provides\na hypothesis and it generates a visualization.\nBy choosing to cast visualization/infographic gen-\neration as generation tasks that offloads core prob-\nlem solving to LLM s and IGM s, LIDA simplifies the\ndesign and maintenance of such systems.\n2.3 Infographics Generation\nInfographics (information graphics) are visual arti-\nfacts that seek to convey complex data-driven nar-\nratives using visual imagery and embellishments\n(Harrison et al., 2015). Existing research has shown\nthat infographics are aesthetically pleasing, engag-\ning and more memorable (Tyagi et al., 2021; Harri-\nson et al., 2015; Haroz et al., 2015), at no additional\ncost to the user (Haroz et al., 2015). These prop-\nerties have driven their applications in domains\nlike fashion, advertisemnt, business and general\ncommunications. However, the creation of info-\ngraphics that convey data insights can be a tedious\nprocess for content creators, often requiring skills\nacross multiple tools and domains. Research on\ninfographic generation have mainly explored the\ncreation of pictographs (Haroz et al., 2015) - replac-\ning the marks on traditional charts with generated\nimages and learning to extract/transfer styles from\nexisting pictographs (Shi et al., 2022b). In this\nwork, we extend this domain to exploring the gener-\nation of both visual marks as well as generating the\nentire infographic based on natural language style\ndescriptions using large image generation models\nsuch as DALLE (Ramesh et al., 2022, 2021) and\nLatent Diffusion (Rombach et al., 2022). This ap-\nproach also enables user-generated visual styles\nand personalization of visualizations to fit user pref-\nerences such as color palettes, visual styles, fonts\netc.\n3 The LIDA System\nLIDA comprises of 4 core modules - a SUMMA -\nRIZER , a GOAL EXPLORER , a VIS GENERATOR\nand an INFOGRAPHER (see Fig 1). Each module is\nimplemented in the LIDA github repo as a python li-\nbrary with an optional user interface (see Appendix\nA).\n3.1 SUMMARIZER\nA t omic type, field \nstatistics, samples ..\nLLM / User Enrichment \n(description, semantic type)\n{\" \":\" cars.json \", \" \":\" cars.json \", \" dataset_description \":\" A dataset \ncontaining information about cars. \", \" \":[{\" \":\"Name \", \" pr oper ties\":\n{\" \":\" string\", \" \":[\" amc concor d dl\", \" amc ambassador \ndpl\", \" plymouth crick et\"], \" \" : 311, \" \": \n\" \", \" \":\"The mak e and model of the car . \"}} ...\nname file_name\nfields column\ndtype samples\nnum_unique_v alues semantic_type\ncar_model description\nStage 1 Stage 2\nCars.csv\nFigure 3: The SUMMARIZER module constructs a NL\nsummary from extracted data properties (atomic types,\nfield statistics) and an optional LLM enrichment (pre-\ndicted field descriptions, semantic types).\nLLM s are capable zero shot predictors, able to solve\nmultiple tasks with little or no guiding examples.\nHowever, they can suffer from hallucination e.g.,\ngenerating text that is not grounded in training data\n116\nor the current task. One way to address this is to\naugment (Mialon et al., 2023) theLLM with ground-\ning context. Thus, the goal of the summarizer is to\nproduce an information dense but compact 3 sum-\nmary for a given dataset that is useful as grounding\ncontext for visualization tasks. A useful context is\ndefined as one that contains information an ana-\nlyst would need to understand the dataset and the\ntasks that can be performed on it. The summary is\nimplemented in two stages (see Fig 3)\nStage 1 - Base summary generation : We ap-\nply rules in extracting dataset properties includ-\ning atomic types (e.g., integer, string, boolean) us-\ning the pandas library (McKinney, 2010), general\nstatistics (min, max, # unique values) and a random\nnon-null list of n samples for each column.\nStage 2 - Summary enrichment: The base sum-\nmary is optionally enriched by an LLM or a user\nvia the LIDA ui to include semantic description of\nthe dataset (e.g., a dataset on the technical specifi-\ncation of cars), and fields (e.g., miles per gallon for\neach car) as well as field semantic type prediction\n(Zhang et al., 2019).\n3.2 GOAL EXPLORER\nThis module generates data exploration goals,\ngiven a summary generated by the SUMMARIZER .\nWe express goal generation as a multitask genera-\ntion problem where the LLM must generate a ques-\ntion (hypothesis), a visualization that addresses the\nquestion and rationale (see Fig 4). We find that\nrequiring the LLM to produce a rationale leads to\nmore semantically meaningful goals.\n{ \" \": \"What is the distribution of Miles_per_Gallon?\",\n\" \": \"Hist ogr am of Miles_per_Gallon \",\n\" \": \"This tells us about the fuel efficiency of the cars in the \ndataset and how it is distributed. \" }\nquestion\nvisualization\nr ationale\nFigure 4: A goal generated by LIDA is a JSON data\nstructure that contains a question, a visualization and a\nrationale.\n3.2.1 VIS GENERATOR\nThe VIS GENERATOR generates visualization speci-\nfications and is comprised of 3 submodules - acode\nscaffold constructor, a code generator and a code\nexecutor.\nCode scaffold constructor: Implements a library\nof code scaffolds that correspond to programming\n3Note: the summary must be compact in order to maximize\nthe limited context token budget of LLM s.\ncode scaffold construct or\n code gener at or\n code ex ecut or\nI m p l e m e n t  a  l i b r a r y  o f  “ c o d e  s c a f f o l d s ”  f o r  \nl a n g u a g e s  a n d  g r a m m a r s  e . g . ,  P y t h o n ,  V e g a - L i t e .\n1\n2\n3\n\n\n\n4\n5\nimport as\ndef\nreturn\n altair  alt \n (data):\n    chart = \nalt.Chart(data).mark_point()\n.encode(x=\n,y= )\n   chart\nchart = plot(data)\nplot\n'Miles_per_Gallon'\n'Cylinders'\n1\n2\n3\n4\n5\nimport as\ndef\nreturn\n altair  alt \n (data):\n  chart = \n   chart\nchart = plot(data)\nplot\n'''<stub>'''\nC o m p l e t e  c o d e  s c a f f o l d s  b a s e d  \no n    s u m m a r y  a n d   g o a l .\nE x e c u t e  g e n e r a t e d  c o d e ,  \np a r s e  r e s u l t s .\nFigure 5: The VIS GENERATOR module constructs vi-\nsualization code scaffolds, fills a constrained section\n(< stub >) and executes the scaffold.\nlanguages and visualization grammars e.g., python\nscaffolds support grammars such as Matplotlib,\nGGPlot, Plotly, Altair, Seaborn, and Bokeh. Each\nscaffold is an executable program that i.) imports\nrelevant dependencies ii.) defines an empty func-\ntion stub which returns a visualization specification\n(see Fig 5a).\nCode generator: Takes a scaffold, a dataset sum-\nmary, a visualization goal, and builds a prompt. An\nLLM (applied in fill-in-the-middle mode (Bavarian\net al., 2022)) is then used to generate n candidate\nvisualization code specifications.\nCode executor: Post-processes and executes4 the\ncode specifications as well as filters the results.\nLIDA implements several filtering mechanisms to\ndetect errors, each with latency tradeoffs: (i) gener-\nates a large sample forn with high temperature, dis-\ncard candidates that do not compile. (ii) apply self\nconsistency (Wang et al., 2022a) in LLM s where\nmultiple candidates are generated and the solution\nwith the highest consensus is selected. (iii) gener-\nate correctness probabilities (Kadavath et al., 2022)\nfor all candidates and selects the one with the high-\nest probability. Note that the last two approaches\nare computationally expensive (require multiple\nforward passes through an LLM ) and are not suit-\nable for real time applications. The final output\nis a list of visualization specifications (code) and\nassociated raster images.\n3.2.2 V IZOPS - Operations on Generated\nVisualizations\nGiven that LIDA represents visualizations as code,\nthe VIS GENERATOR also implements submodules\nto perform operations on this representation.\nNatural language based visualization refine-\nment: Provides a conversational api to iteratively\n4Execution in a sandbox environment is recommended.\n117\nrefine generated code (e.g., translate chart t hindi\n. . . zoom in by 50% etc) which can then be exe-\ncuted to generate new visualizations.\nVisualization explanations and accessibility :\nGenerates natural language explanations (valuable\nfor debugging and sensemaking) as well as acces-\nsibility descriptions (valuable for supporting users\nwith visual impairments).\nVisualization code self-evaluation and repair :\nApplies an LLM to self-evaluate generated code on\nmultiple dimensions (see section 4.1.2).\nVisualization recommendation: Given some con-\ntext (goals, or an existing visualization), recom-\nmend additional visualizations to the user (e.g., for\ncomparison, or to provide additional perspectives).\n3.3 INFOGRAPHER\nThis module is tasked with generating stylized\ngraphics based on output from theVIS GENERATOR\nmodule (see Fig 2). It implements a library of vi-\nsual styles described in NL that are applied directly\nto visualization images. Note that the style library\nis editable by the user. These styles are applied in\ngenerating infographics using the text-conditioned\nimage-to-image generation capabilities of diffusion\nmodels (Rombach et al., 2022), implemented using\nthe Peacasso library api (Dibia, 2022). An optional\npost processing step is then applied to improve the\nresulting image (e.g., replace axis with correct val-\nues from visualization, removing grid lines, and\nsharpening edges).\n3.4 U SER INTERFACE\nLIDA implements a user interface that communi-\ncates with the core modules over a REST and Web-\nsocket api. The user interface implements several\nviews.\nData upload and summarization: This view al-\nlows the user to upload a dataset and explore a\nsample of rows in the dataset via a table view. A\ndata upload event triggers a call to the SUMMA -\nRIZER and GOAL EXPLORER module and displays\na summary of the dataset and a list of potential\ngoals. This view also allows the user to option-\nally annotate and refine the generated summary or\ncurate fields used in the dataset.\nVisualization view: This view allows the user to\noptionally provide a visualization goal in NL (e.g.,\n\"what is the fuel efficiency per country?\") or se-\nlect a generated goal and then displays a generated\nvisualization . For each visualization, intermedi-\nate output from the models (underlying data sum-\nmary, visualization specification, code scaffold) are\nshown as explanations to aid in sensemaking, and\ndebugging(see Fig 9). This view also implements\nthe VIZOPS capabilities described in Section 3.2.2\n(e.g., See the interface for visualization evaluation\nin Fig 10). Note that the NL interface inherits the\nmultilingual language capabilities of the underly-\ning LLM , enabling multilingual NL interaction.\nOverall, the combination of these modules result\nin a system that is able to implicitly address an\narray of data visualization operations such as data\ntransformation, encoding, mark selection, styling,\nlayout, and annotation (Wang et al., 2022b).\n4 Evaluation\n4.1 Evaluation Metrics\nOur initial evaluation of LIDA focuses on two high\nlevel metrics - visualization error rates (VER ) to pro-\nvide signals on the reliability of the LIDA pipeline,\nand self-evaluated visualization quality (SEVQ ) to\nassess the quality of generated visualizations.\n4.1.1 Visualization Error Rate ( VER )\nVisualization error rate is computed as the percent-\nage of generated visualizations that result in code\ncompilation errors. This metric provides critical\ninsights into the reliability of the LIDA pipeline\nand impact of changes to the system (e.g., prompt\nengineering or scaffold update).\nVER = E\nT ∗ 100\nWhere: - E = Number of generated visualiza-\ntions with code compilation errors, and - T = Total\nnumber of generated visualizations.\n4.1.2 Self-Evaluated Visualization Quality\n(SEVQ )\nRecent work shows LLM s like GPT-4 encode broad\nworld knowledge (OpenAI, 2023), can assess the\nquality of their output (Kadavath et al., 2022; Lin\net al., 2022) and can approximate human judge-\nments for tasks such as summarization (Liu et al.,\n2023). Our observations applying GPT3.5/GPT-\n4 to visualization tasks suggest similar results.\nSpecifically, GPT-4 has learned to encode some\nvisualization best practices and can apply these in\ngenerating critiques of visualization code across\nmultiple dimensions. Thus, to evaluate visualiza-\ntion quality, we compute an SEVQ metric by ap-\nplying GPT-4 in assessing the quality of gener-\nated visualizations. Specifically, we task GPT-4\n118\nwith scoring generated visualization code (a nu-\nmeric value from 1-10 and a rationale) across 6\ndimensions - code accuracy, data transformation,\ngoal compliance, visualization type, data encoding,\nand aesthetics. These dimensions are informed by\nexisting literature on visualization generation/rec-\nommendation e.g., Wang et al. (2022b) outline 6\nvisualization tasks including data transformation,\nencoding, marks, styling, layout and annotation,\nwhile (Moritz et al., 2018) codify constraints for\nvisualization quality across expressivity (does it\nconvey the facts of the data) and effectiveness (is\nthe information more readily perceived compared\nto other visualizations) criteria. Additional details\non prompts used for each dimension are provided\nin Appendix B.\n4.2 Evaluation Benchmark Settings\nOur initial benchmark is based on 57 datasets\nsourced from the vega datasets repository 5. For\neach dataset, LIDA is tasked with generating 5 goals\nand 1 visualization per goal across multiple gram-\nmars6. For reproducibility, we settemperature =0\nand number of samples n = 1 for the LLM . A\ngallery of the generated evaluation visualizations\ncan be viewed on the LIDA project page.\n4.3 Evaluation and Ablation Study Results\nno_enrich enrich schema no_summary\nSummary T ype\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Error Rate\n5.61% 7.72% 7.02%\n95.79%\n3.51% 3.51%\n9.47%\n99.30%\nVisualization Error Rate | GPT-3.5,  n=2280\nlib\nmatplotlib\nseaborn\nFigure 6: Results from an ablation study on the impact\nof data summarization strategies on visualization error\nrate (VER ) metric.\nOverall, we find that LIDA is able to generate\nvisualizations with a low error rate (VER = 3.5%).\nWe also conduct an ablation study to inform on\nthe impact of the SUMMARIZER across the fol-\n5https://github.com/vega/vega-datasets\n6LIDA is given a single try for each step. In theory, the\nerror rates can be driven to zero, by recursively applying the\nvisualization self-evaluation and self-repair modules.\nlowing conditions - (i) no_enrich: a base sum-\nmary with no enrichment (see Section 3.1), (ii) en-\nrich: summary with LLM enrichment, (iii) schema:\nonly field names, i.e., schema as summary, and\n(iv) no_summary: no summary. Results show that\nincluding a summary leads to reduced error rate\ncompared to simply adding field names (schema)\nas summary. We also find that enriching the base\nsummary with an LLM has less of an effect on\nVER (with variations across visualization grammar),\nand an expressive, well-represented grammar like\nSeaborn having lower VER . These results are sum-\nmarized in Figure 6. We also find that the SEVQ\nmetric is valuable in identifying semantic quality\nissues with generated visualizations. For example,\nFig 10 shows an example where the user has re-\nquested a pie chart, and the LIDA self-evaluation\nmodule critiques this visualization using the SEVQ\nmetric, providing a rationale for why a bar chart\nis more effective (see Fig 10), with the option to\nautomatically repair the visualization.\n5 Conclusion\nIn this work, we formulate visualization generation\nas a multi-stage text (and code) generation problem\nthat can be addressed using large language mod-\nels. We present LIDA - a tool for the automatic\ngeneration of grammar-agnostic visualizations and\ninfographics. LIDA addresses limitations of cur-\nrent automatic visualization systems - automatic\ngeneration of hypothesis/goals given datasets, con-\nversational interface for controllable visualization\ngeneration and refinement, support for multiple vi-\nsualization grammars using the same pipeline and\nthe ability to generate infographics. LIDA is effec-\ntive compared to state of the art systems (see ex-\nample gallery of generated visualizations); it offers\na simplified system implementation and leverages\nthe immense language modeling and code genera-\ntion capabilities of LLM s in implicitly solving com-\nplex visualization subtasks. Finally, we introduce\nmetrics for assessing reliability (visualization error\nrate - VER ) and visualization quality (self-evaluated\nvisualization quality -SEVQ ) for LLM -enabled vi-\nsualization tools. We hope modules implemented\nin LIDA will serve as useful building blocks in en-\nabling complex creative workflows such as visual-\nization translation, chart question answering(with\napplications in accessibility of charts), automated\ndata exploration and automated storytelling.\n119\n6 Limitations\nWhile LIDA demonstrates clear advances in how\nwe can support users in authoring visualizations\nand infographics, there are several limitations that\noffer a natural avenue for future research.\nLow Resource Grammars: The problem formu-\nlation introduced in LIDA depends on the under-\nlying LLM s having some knowledge of visualiza-\ntion grammars as represented in text and cod e\nin its training dataset (e.g., examples of Altair,\nVega, Vega-Lite, GGPLot, Matplotlib,represented\nin Github, Stackoverflow, etc.). For visualization\ngrammars not well represented in these datasets\n(e.g., tools like Tableau, PowerBI, etc., that have\ngraphical user interfaces as opposed to code repre-\nsentations), the performance of LIDA may be lim-\nited without additional model fine-tuning or transla-\ntion. Furthermore, performance may be limited for\ncomplex tasks (e.g., tasks requiring complex data\ntransformations) beyond the expressive capabilities\nof specific grammars. Further research is needed\nto: i.) study effects of strategies like task disam-\nbiguation ii.) impact of task complexity and choice\nof programing language/grammar on performance.\nDeployment and Latency: Large language mod-\nels (e.g., GPT3.5 used in this work) are computa-\ntionally expensive and require significant compute\nresources to deploy at low latency. These costs can\nprove to be impractical for real-world application.\nIn addition, the current setup includes a code ex-\necution step which is valuable for verification but\nincreases deployment complexity (requires a sand-\nbox). Thus, there is opportunity to: i.) train smaller\ncapable LLM s (Touvron et al., 2023) finetuned on\na curated dataset of programming languages and\nvisualization grammars .ii) design vulnerability mit-\nigation approaches such as limiting program scope\nor generating only input parameters for visualiza-\ntion grammar compilers.\nExplaining System Behavior: The approach dis-\ncussed in this paper simplifies the design of vi-\nsualization authoring systems, but also inherits\ninterpretability challenges associated with large\nlanguage models. While LIDA offers intermedi-\nate outputs of the model (e.g., generated code and\nspecifications) as explanations, as well as post-hoc\nexplanations of generated code (see section 3.2.2),\nthere is a need for further research in explaining\nsystem behavior (conditions when they are needed)\nand providing actionable feedback to the user.\nSystem Evaluation: Benchmarking LLM ’s on cre-\nativity tasks can be challenging. While the current\nstudy introduces metrics for evaluating reliability\n(VER ) and visualization quality (SEVQ ) (see section\n4), there is a need for more comprehensive bench-\nmarks on a variety of datasets and visualization\ngrammars. Furthermore, there are research oppor-\ntunities to i.) study and quantify the capabilities of\nLLM s in encoding and applying visualization best\npractices ii.) conduct empirical studies that evalu-\nate model behavior, mapping out failure cases and\nproposing mitigations iii.) qualitatively study the\nimpact of tools like LIDA on user creativity while\nauthoring visualizations.\nAcknowledgements\nThis manuscript has benefited from comments\nand discussions with members of the HAX group\n(Saleema Amershi, Adam Fourney, Gagan Bansal),\nVIDA group (Steven Drucker, Dan Marshall),\nBongshing Lee, Rick Barraza and others at Mi-\ncrosoft Research.\nReferences\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak,\nJohn Schulman, Christine McLeavey, Jerry Tworek,\nand Mark Chen. 2022. Efficient training of lan-\nguage models to fill in the middle. arXiv preprint\narXiv:2207.14255.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nQiaochu Chen, Shankara Pailoor, Celeste Barnaby,\nAbby Criswell, Chenglong Wang, Greg Durrett, and\nI¸ sil Dillig. 2022. Type-directed synthesis of vi-\nsualizations from natural language queries. Pro-\nceedings of the ACM on Programming Languages ,\n6(OOPSLA2):532–559.\n120\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAaron Daniel Cohen, Adam Roberts, Alejandra Molina,\nAlena Butryna, Alicia Jin, Apoorv Kulshreshtha,\nBen Hutchinson, Ben Zevenbergen, Blaise Hilary\nAguera-Arcas, Chung ching Chang, Claire Cui,\nCosmo Du, Daniel De Freitas Adiwardana, De-\nhao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi,\nErin Hoffman-John, Heng-Tze Cheng, Hongrae Lee,\nIgor Krivokon, James Qin, Jamie Hall, Joe Fen-\nton, Johnny Soraker, Kathy Meier-Hellstern, Kris-\nten Olson, Lora Mois Aroyo, Maarten Paul Bosma,\nMarc Joseph Pickett, Marcelo Amorim Menegali,\nMarian Croak, Mark Díaz, Matthew Lamm, Maxim\nKrikun, Meredith Ringel Morris, Noam Shazeer,\nQuoc V . Le, Rachel Bernstein, Ravi Rajakumar, Ray\nKurzweil, Romal Thoppilan, Steven Zheng, Taylor\nBos, Toju Duke, Tulsee Doshi, Vincent Y . Zhao,\nVinodkumar Prabhakaran, Will Rusch, YaGuang Li,\nYanping Huang, Yanqi Zhou, Yuanzhong Xu, and\nZhifeng Chen. 2022. Lamda: Language models for\ndialog applications. In arXiv.\nVictor Dibia. 2022. Interaction design for systems that\nintegrate image generation models: A case study with\npeacasso.\nVictor Dibia and Ça ˘gatay Demiralp. 2019. Data2vis:\nAutomatic generation of data visualizations us-\ning sequence-to-sequence recurrent neural networks.\nIEEE computer graphics and applications, 39(5):33–\n46.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nSteve Haroz, Robert Kosara, and Steven L Franconeri.\n2015. Isotype visualization: Working memory, per-\nformance, and engagement with pictographs. In Pro-\nceedings of the 33rd annual ACM conference on hu-\nman factors in computing systems, pages 1191–1200.\nLane Harrison, Katharina Reinecke, and Remco Chang.\n2015. Infographic aesthetics: Designing for the first\nimpression. In Proceedings of the 33rd Annual ACM\nconference on human factors in computing systems,\npages 1187–1190.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nYuyu Luo, Xuedi Qin, Nan Tang, Guoliang Li, and\nXinran Wang. 2018. Deepeye: Creating good data\nvisualizations by keyword search. In Proceedings of\nthe 2018 International Conference on Management\nof Data, SIGMOD, pages 1733–1736.\nWes McKinney. 2010. Data structures for statistical\ncomputing in python. In Proceedings of the 9th\nPython in Science Conference, pages 51 – 56.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nRishab Mitra, Arpit Narechania, Alex Endert, and John\nStasko. 2022. Facilitating conversational interaction\nin natural language interfaces for visualization. In\n2022 IEEE Visualization and Visual Analytics (VIS),\npages 6–10. IEEE.\nDominik Moritz, Chenglong Wang, Greg L Nelson,\nHalden Lin, Adam M Smith, Bill Howe, and Jef-\nfrey Heer. 2018. Formalizing visualization design\nknowledge as constraints: Actionable and extensible\nmodels in draco. IEEE transactions on visualization\nand computer graphics, 25(1):438–448.\nArpit Narechania, Arjun Srinivasan, and John Stasko.\n2020. Nl4dv: A toolkit for generating analytic speci-\nfications for data visualization from natural language\nqueries. IEEE Transactions on Visualization and\nComputer Graphics, 27(2):369–379.\nOpenAI. 2023. Gpt-4 technical report.\nLuca Podo, Bardh Prenkaj, and Paola Velardi. 2023. Ma-\nchine learning for visualization recommendation sys-\ntems: Open challenges and future directions. arXiv\npreprint arXiv:2302.00569.\n121\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821–8831. PMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. 2022 ieee. In CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 10674–\n10685.\nArvind Satyanarayan, Dominik Moritz, Kanit Wong-\nsuphasawat, and Jeffrey Heer. 2017. Vega-lite: A\ngrammar of interactive graphics. IEEE TVCG (Proc.\nInfoVis).\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022a.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nYang Shi, Pei Liu, Siji Chen, Mengdi Sun, and Nan\nCao. 2022b. Supporting expressive and faithful pic-\ntorial visualization design with visual style transfer.\nIEEE Transactions on Visualization and Computer\nGraphics, 29(1):236–246.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAnjul Tyagi, Jian Zhao, Pushkar Patel, Swasti Khu-\nrana, and Klaus Mueller. 2021. User-centric semi-\nautomated infographics authoring and recommenda-\ntion. arXiv preprint arXiv:2108.11914.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022a. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nYun Wang, Zhitao Hou, Leixian Shen, Tongshuang Wu,\nJiaqi Wang, He Huang, Haidong Zhang, and Dong-\nmei Zhang. 2022b. Towards natural language-based\nvisualization authoring. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(1):1222–1232.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nKanit Wongsuphasawat, Zening Qu, Dominik Moritz,\nRiley Chang, Felix Ouk, Anushka Anand, Jock\nMackinlay, Bill Howe, and Jeffrey Heer. 2017. V oy-\nager 2: Augmenting visual analysis with partial view\nspecifications. In ACM CHI.\nDan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon\nHulsebos, Ça˘gatay Demiralp, and Wang-Chiew Tan.\n2019. Sato: Contextual semantic type detection in\ntables. arXiv preprint arXiv:1911.06311.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nA The LIDA Library\nLIDA is implemented as a python library with mod-\nules for each of the components described in Sec-\ntion 3. The library is available on github7 and can\nbe installed using pip - pip install lida. The library\nprovides a python api, web api for integration into\nother applications, and a command line interface. It\nalso provides a web-based user interface for users\nto interact with LIDA (Fig 10, 9).\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n10\n\n11\n# pip install lida \n\nfrom lida.modules  Manager \n\nlida = () \nsummary = lida. ( ) \ngoals = lida. (summary, n= )\n\nvis_specs = manager. ( summary=summary, \ngoal=goals[i]) \ncharts = manager. ( code_specs=vis_specs, \ndata=manager.data, summary=summary)\n(charts)\nimport\nManager\nsummarize\ngenerate_goals\ngenerate_viz\nexecute_viz\nprint\n\"data/cars.csv\"\n1\nFigure 7: Example usage ofLIDA shows how to generate\na summary, visualization goals, code specifications and\nexecute the code to generate visualizations.\nB Self-Evaluated Visualization Quality\n(SEVQ ) Prompts\nFor the SEVQ metric, we use GPT-4 to assess visu-\nalization quality by scoring generated visualization\n7https://github.com/microsoft/lida\n122\nFigure 8: In the data upload section of the LIDA UI, users can select a grammar of choice and upload a dataset. A\ndataset upload event triggers a goal generation as well as visualization generation tasks.\nFigure 9: The visualization generation section of the LIDA UI enables the user to i.) specify their overall goal in\nnatural language and generate visualizations ii.) inspect, edit and execute generated code iii.) view the generated\nvisualization. iv.) perform operations on generated code e.g., refine, explain, evaluate and recommend visualizations.\n123\nFigure 10: The self-evaluation module in LIDA is used to evaluate/critique a generated visualization, providing\nscores across 6 dimensions with rationale. In this case, the visualization contains a pie chart, and a bar chart is\nrecommended as an alternative.\ncode across the 6 task dimensions - code accuracy,\ndata transformation, goal compliance, visualization\ntype, data encoding, and aesthetics. These dimen-\nsions are implemented as prompts to an LLM 8,\nwhich then generates a score between 1-10 for each\ndimension. The final SEVQ score is the average of\nthe 6 scores. A sketch of the prompts used for each\ndimension are enumerated in table 1.\nC Design Reflections\nBuilding a system that leverages foundation models\n(text and images) involves engineering decisions\nacross a wide design space. In this section, we\nbriefly reflect on some of the design choices we\nmade for LIDA components and the tradeoffs we\nconsidered.\nC.1 Prompt Engineering\nWe explored multiple approaches to building\nprompts that maximized the probability of the LLM\nsolving each subtask.\n• SUMMARIZER : We found that improving the\nrichness of the summary (qualitative NL de-\nscription, including semantic types) was criti-\ncal to improved quality of generated goals and\n8Exact prompts can be found at the project repository\nhttps://github.com/microsoft/lida.\nDimension Prompt\nCode accu-\nracy\nDoes the code contain bugs, logic errors,\nsyntax error or typos? How serious are the\nbugs? How should it be fixed?\nData trans-\nformation\nIs the data transformed appropriately for\nthe visualization type?\nGoal com-\npliance\nHow well the code meets the specified visu-\nalization goals?\nVisualization\ntype\nConsidering best practices, is the visualiza-\ntion type appropriate for the data and intent?\nIs there a visualization type that would be\nmore effective in conveying insights?\nData encod-\ning\nIs the data encoded appropriately for the\nvisualization type?\nAesthetics Are the aesthetics of the visualization ap-\npropriate and effective for the visualization\ntype and the data?\nTable 1: Summary of the evaluation dimensions and the\ncorresponding prompt sketches.\nvisualization code. Implementation wise, we\nbegan with a manually crafted summary of the\ndata (see Section 3.1), and then enriched it via\ncalls to an LLM and optional user refinement\nof the summary.\n• GOAL EXPLORER : Providing few shot exam-\nples in the prompts where fields and rationale\n124\nare linked via symbols (e.g., plot a histogram\nof field X vs Y to show relationship between X\nand Y) nudges the model to use exact dataset\nfield names, and minimizes the occurrence of\nhallucinated fields. Prompt engineering also\nprovides mechanisms to bake in visualization\nbest practices e.g. avoid pie charts, apply vi-\nsualization best practices , Imagine you are\na highly experienced visualization specialist\nand data analyst.\n• VIS GENERATOR : Casting visualization code\ngeneration as a fill-in-the-middle problem (as\nopposed to free-from completion) ensures the\nmodel to generates executable code focused\non the task. For example, in Fig 5, the model\nis instructed to generate only the < stub >\nportion of the code scaffold. We also note that\nthe degrees of freedom alloted to the model\n(e.g., specifying how much of the scaffold\nto complete) can influence its ability to add\ntasks with varied complexity. For example, a\nscaffold that allows the model generate data\npreprocessing code (and includes libraries like\nstatsmodels etc) allows the model to address\ntasks that require steps such as data transfor-\nmation, sampling and statistical analysis be-\nfore generating visualizations etc.\n• Overall, we found that setting a low temper-\nature (t = 0; generating the most likely visu-\nalization) coupled with a per-grammar code\nscaffold provided the best results in terms\nof yielding code that correctly compiles into\nvisualization specifications and faithfully ad-\ndresses the subtask. We also explored prompt\nformulations that addressed multiple tasks to\nminimize costs (latency and compute). For\nexample, summary enrichment is a single call\nwhere the LLM must generate dataset descrip-\ntions, field descriptions and semantic types.\nC.2 Infographic Generation\nWe found that setting a low strength parameter\n(0.25 < strength < 0.45) for the latent diffusion\nmodel (image-to-image mode) and using parsimo-\nnious style prompts resulted in stylized images that\nwere faithful to the general structure of the origi-\nnal visualization, minimizing distorted or irrelevant\nimagery. This sort of controlled generation is nec-\nessary to avoid the distraction (Haroz et al., 2015)\nthat can arise from superfluous imagery in info-\ngraphics.\nC.3 Natural Language Interaction\n(i) HYBRID INTERFACE : Providing a hybrid in-\nterface that allows traditional direct manipulation\nsteps in creating visualizations (e.g., selecting\nwhich fields to use), paired with a NL interface\nallows users to leverage existing mental models\nwith traditional visualization tools as well as the\nNL affordances of LIDA . (ii) NL INTERACTION\nMODES : Beyond generating a base visualization,\nwe also enable operations on generated visualiza-\ntion code (e.g., refinement, explanation, evaluation,\nrecommendation). This builds on insights from\nMitra et al. (2022) who propose multi-turn dialog\ninterfaces for visualization authoring towards re-\nsolving ambiguities.\n125\nFigure 11: The LIDA infographer module supports the generation of data-faithful infographics. Each infographic is\nconditioned on a generated visualization as well as natural language style tags which can be used to customize the\nappearance of the chart.\n126",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8725255727767944
    },
    {
      "name": "Infographic",
      "score": 0.7505375742912292
    },
    {
      "name": "Visualization",
      "score": 0.6593541502952576
    },
    {
      "name": "Programming language",
      "score": 0.5412561297416687
    },
    {
      "name": "Data visualization",
      "score": 0.4988400936126709
    },
    {
      "name": "Python (programming language)",
      "score": 0.4605158567428589
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2482404112815857
    },
    {
      "name": "Data mining",
      "score": 0.13471642136573792
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 74
}