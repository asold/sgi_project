{
  "title": "Spatial Transformer for 3D Point Clouds",
  "url": "https://openalex.org/W2975950300",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2356198789",
      "name": "Wang Jiayun",
      "affiliations": [
        "International Computer Science Institute",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A4221442010",
      "name": "Chakraborty, Rudrasis",
      "affiliations": [
        "University of California, Berkeley",
        "International Computer Science Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4221782852",
      "name": "Yu, Stella X.",
      "affiliations": [
        "International Computer Science Institute",
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2811382168",
    "https://openalex.org/W2565829216",
    "https://openalex.org/W2963893349",
    "https://openalex.org/W2798297683",
    "https://openalex.org/W2963640720",
    "https://openalex.org/W2798873012",
    "https://openalex.org/W2811184430",
    "https://openalex.org/W2962912109",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W3186139152",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W2962731536",
    "https://openalex.org/W2565662353",
    "https://openalex.org/W2883753479",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W2888754481",
    "https://openalex.org/W2850910281",
    "https://openalex.org/W2278868814",
    "https://openalex.org/W2799162093",
    "https://openalex.org/W2890018557",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W2603429625",
    "https://openalex.org/W2962928871",
    "https://openalex.org/W2788158258",
    "https://openalex.org/W1964772475",
    "https://openalex.org/W2264432461",
    "https://openalex.org/W2963281829",
    "https://openalex.org/W2798297823",
    "https://openalex.org/W2797997528",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W2963719584",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W6687484953",
    "https://openalex.org/W2460657278",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2953007748",
    "https://openalex.org/W2902302021",
    "https://openalex.org/W3099258600",
    "https://openalex.org/W4394671432",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2737234477",
    "https://openalex.org/W3122159272",
    "https://openalex.org/W2950556479",
    "https://openalex.org/W2777356020",
    "https://openalex.org/W2772518193",
    "https://openalex.org/W2952263313",
    "https://openalex.org/W3104141662",
    "https://openalex.org/W2769473888",
    "https://openalex.org/W2163605009"
  ],
  "abstract": "Deep neural networks are widely used for understanding 3D point clouds. At each point convolution layer, features are computed from local neighbourhoods of 3D points and combined for subsequent processing in order to extract semantic information. Existing methods adopt the same individual point neighborhoods throughout the network layers, defined by the same metric on the fixed input point coordinates. This common practice is easy to implement but not necessarily optimal. Ideally, local neighborhoods should be different at different layers, as more latent information is extracted at deeper layers. We propose a novel end-to-end approach to learn different non-rigid transformations of the input point cloud so that optimal local neighborhoods can be adopted at each layer. We propose both linear (affine) and non-linear (projective and deformable) spatial transformers for 3D point clouds. With spatial transformers on the ShapeNet part segmentation dataset, the network achieves higher accuracy for all categories, with 8 percent gain on earphones and rockets in particular. Our method also outperforms the state-of-the-art on other point cloud tasks such as classification, detection, and semantic segmentation. Visualizations show that spatial transformers can learn features more efficiently by dynamically altering local neighborhoods according to the geometry and semantics of 3D shapes in spite of their within-category variations.",
  "full_text": "1\nSpatial Transformer for 3D Point Clouds\nJiayun Wang Rudrasis Chakraborty Stella X. Yu\nAbstract‚ÄîDeep neural networks are widely used for understanding 3D point clouds. At each point convolution layer, features are\ncomputed from local neighbourhoods of 3D points and combined for subsequent processing in order to extract semantic information.\nExisting methods adopt the same individual point neighborhoods throughout the network layers, deÔ¨Åned by the same metric on the Ô¨Åxed\ninput point coordinates. This common practice is easy to implement but not necessarily optimal. Ideally, local neighborhoods should be\ndifferent at different layers, as more latent information is extracted at deeper layers. We propose a novel end-to-end approach to learn\ndifferent non-rigid transformations of the input point cloud so that optimal local neighborhoods can be adopted at each layer. We propose\nboth linear (afÔ¨Åne) and non-linear (projective and deformable) spatial transformers for 3D point clouds. With spatial transformers on the\nShapeNet part segmentation dataset, the network achieves higher accuracy for all categories, with 8% gain on earphones and rockets in\nparticular. Our method also outperforms the state-of-the-art on other point cloud tasks such as classiÔ¨Åcation, detection, and semantic\nsegmentation. Visualizations show that spatial transformers can learn features more efÔ¨Åciently by dynamically altering local\nneighborhoods according to the geometry and semantics of 3D shapes in spite of their within-category variations.\nIndex Terms‚Äîpoint cloud, transformation, deformable, segmentation, 3D detection\n!\n1 I NTRODUCTION\n3D\nComputer vision has been on the rise with more\nadvanced 3D sensors and computational algorithms.\nDepth cameras and LiDAR sensors output 3D point clouds,\nwhich become key components in several 3D computer vision\ntasks including but not limited to virtual / augmented reality\n[1], [2], 3D scene understanding [3], [4], [5], and autonomous\ndriving [6], [7], [8].\nOn the algorithmic side, convolutional neural networks\n(CNNs) have achieved great success in many computer\nvision tasks [9], [10]. However, the concept of convolution\ncannot be directly applied to a point cloud, as 3D points are\nnot pixels on a regular grid with regular neighbourhoods.\nOne line of approaches is to convert the 3D point cloud into\na representation where CNNs are readily applicable, e.g.,\na regular voxel representation [ 11], [ 12], [ 13] or 2D view\nprojections [14], [15], [16], [17].\nAnother line of approaches is to develop network archi-\ntectures that can directly process point clouds [ 18], [19], [20],\n[21]. Analogous to convolution on 2D pixels, convolution on\n3D points needs to Ô¨Årst identify local neighborhoods around\nindividual input points. This step is achieved by computing\nthe so-called point afÔ¨Ånity matrix , i.e., the adjacency matrix\nof a dense graph constructed from the point cloud. These\nneighborhoods are then used for extracting features with\npoint-wise convolutions. By stacking basic point convolution\nlayers, a neural network can extract information from the\npoint cloud with an increasing level of abstraction.\nHowever, unlike images where 2D pixels are laid out on\na regular grid with simple and well-deÔ¨Åned local neighbor-\nhoods, local neighborhoods of 3D points are ill-deÔ¨Åned and\nsubject to various geometric transformations of 3D shapes.\nMost methods [18], [19], [22], [23] deÔ¨Åne local neighborhoods\n‚Ä¢ The authors are with UC Berkeley / ICSI, 2150 Shattuck Ave, Berkeley, CA,\n94704. E-mail: {peterwg, rudra, stellayu}@berkeley.edu. Corresponding\nauthor: Stella X. Yu.\n‚Ä¢ Our code us publicly available at http://pwang.pw/spn.html.\nas nearest neighbors in the Euclidean space of the input 3D\npoint coordinates.\nThis common practice of deÔ¨Åning a nearest neighbor\ngraph according to the Euclidean distances on the Ô¨Åxed input\n3D point coordinates may be simple but not optimal. First,\nsuch distances may not be able to efÔ¨Åciently encode semantics\nof 3D shapes, e.g., semantically or topologically far points\nmight be spatially close in terms of the Euclidean distances.\nSecondly, Ô¨Åxed neighborhoods throughout the network may\nreduce the model‚Äôs learning capacity as different layers\ncapture information at different levels of abstraction, e.g.,\nobjects have a natural hierarchy and in order to segment out\ntheir parts, it would be more efÔ¨Åcient to provide different\nlayers the ability to parse them at different spatial scales.\nWe propose to address these Ô¨Åxed point neighbourhood\nrestrictions by dynamically learning local neighborhoods and\ntransforming the input point cloud at different layers. We\nuse a parametric model that takes both point coordinates\nand learned features as inputs to learn the point afÔ¨Ånity\nmatrix. At different layers of the network, we learn several\ndifferent transformations (dubbed as spatial transformers\nor transformers hereafter, Fig.1) and corresponding point\nlocal neighborhoods (Fig.2). Spatial transformers allow the\nnetwork to adaptively learn point features covering different\nspatial extensions at each depth layer.\nTo spatially transform a point cloud, we learn a function,\nŒ¶, that generates transformed point coordinates from the\ninput point coordinates and feature maps. However, it is\nnontrivial to learn Œ¶ without smoothness constraints. Since\nany isometric (e.g. rigid) transformation cannot change the\ndistance metric, we consider non-rigid transformations, both\nlinear and non-linear families. That is, our spatial transformers\nare parameterized functions conditioned on the input point\ncoordinates P and feature map F; they are subsequently used\nto transform the point coordinates, resulting in a new point\nafÔ¨Ånity matrix for obtaining dynamic local neighborhoods.\nWe consider three families of spatial transformers.\narXiv:1906.10887v4  [cs.CV]  30 Mar 2021\n2\n...\n...\n...feature \nlearning\n...feature \nlearning\nMultiple transformers\nat layer 1Input point cloud\n(Table)\nOutput segmentation\ncaptures surface\ncaptures table base\ncaptures surface\ncaptures table base\nMultiple transformers\nat layer 2\n...\n...\n...feature \nlearning\n...feature \nlearning\nfully \nconnected\nfully \nconnected\n...\nFig. 1. Spatial transformers learn several global transformationsat each layer to obtain different local point neighborhoods. We show transformed\npoint clouds at different layers learned by spatial transformers for different instances of a category (e.g. tables). Compared with previous works\nadopting Ô¨Åxed local neighborhoods, dynamic point neighborhoods make the network more powerful in learning semantics from point clouds. For\nexample, corresponding geometric transformations capture similar semantic information even high intra-class spatial variations exist. The second\ntransformation at layer 1 deforms different tables to be more semantically similar, and makes parsing the part of table base easier. Furthermore, the\nproposed transformer is a stand-alone module and can be easily added to existing point cloud processing networks.\nTable\nFix graph\nTrans. 1\nTrans. 2\nTrans. 3\nTrans. 4\nTrans. 5\nTrans. 6\nEarphone Trans. 1 Trans. 3 Trans. 5\nFix graph Trans. 2 Trans. 4 Trans. 6\nFig. 2. At each layer, we apply multiple spatial transformers to deform\nthe input point cloud for learning different neighborhoods. We show local\nneighborhoods of input point cloud examples with and without deformable\ntransformers. Different colors indicate different neighborhoods, and inten-\nsities indicate distances to the central point. The dynamic neighborhood\nenhance the network capacity to learn from objects with large spatial\nvariations. Rotating table and earphone for better visualizations.\n1) AfÔ¨Åne transformation P ‚Ü¶‚ÜíAP, where A is an afÔ¨Åne\nmatrix. 2) Projective transformer ÀúP ‚Ü¶‚ÜíB ÀúP, where ÀúP is 3D\npoints expressed in homogeneous coordinates.3) Deformable\ntransformer P ‚Ü¶‚Üí CP + DF , where C, Dare respective\ntransformation matrices of point coordinates and features\nF. The transformation depends on both the input point\ncoordinates and the features the points assume.\nOur work makes the following contributions.\n‚Ä¢ We propose to learn linear (afÔ¨Åne) and non-linear (pro-\njective, deformable) spatial transformers for new point\nafÔ¨Ånity matrices and thus dynamic local neighborhoods\nthroughout the neural network.\n‚Ä¢ We demonstrate that our spatial transformers can be eas-\nily added to existing point cloud networks for a variety\nof tasks: classiÔ¨Åcation, detection, and segmentation.\n‚Ä¢ We apply spatial transformers to various point cloud\nprocessing networks, with point-based and sampling-\nbased metrics for point neighborhoods, and observe\nperformance gains of dynamic graphs over Ô¨Åxed graphs.\n2 R ELATED WORK\nWe discuss related works that motivate the necessity of our\nproposed spatial transformers.\nView-based methods project 3D shapes to 2D planes and\nuse images from multiple views as representations. Taking\nadvantages of the power of CNNs in image processing [ 24],\n[25], [15], [14], view-based methods achieve reasonable 3D\nprocessing performance. However, certain information about\n3D structures gets lost when 3D points are projected to 2D\nimage planes; occluded surfaces and density variations are\nthus often troublesome for these methods.\nVoxel-based methods represent 3D shapes as volumetric\ndata on a regular 3D grid, and proceed with 3D convolution\n[11], [ 26], [ 27]. Their caveates are quantization artifacts,\n3\ninefÔ¨Åcient usage of 3D voxels, and low spatial resolutions due\nto a large memory requirement. In addition, 3D convolutions\nare not biased towards surface property extraction and thus\ncannot capture geometrical and semantic information efÔ¨Å-\nciently. Recent works that apply different partition strategies\n[28], [12], [27], [13] relieve these issues but depend heavily\non bounding volume subdivision instead of local geometric\nshapes. In contract, our method works directly on the 3D\npoint cloud, minimizing geometric information loss and\nmaximizing processing efÔ¨Åciency.\nPoint cloud processing methods take a point cloud as the\ninput and extract semantic information by point convolutions.\nPointNet [ 18] directly learns the embedding of every 3D\npoint in isolation and gather that information by pooling\npoint features later on. Although it achieves good perfor-\nmance at the time, PointNet does not learn any 3D local\nshape information since each local neighborhood contains\nonly one point. PointNet++ [ 19] addresses this caveate by\nadopting a hierarchical application of isolated 3D point\nfeature learning to multiple subsets of a point cloud. Many\nother works also explore different strategies for leveraging\nlocal structure learning from point clouds [ 22], [23]. Instead\nof Ô¨Ånding neighbors of each point, SplatNet [ 29] encodes\nlocal structures from the sampling perspective: it groups\npoints based on permutohedral lattices [30], and then applies\nbilateral convolution [31] for feature learning. Super-point\ngraphs [ 32] partition a point cloud into super-points and\nlearn the 3D point geometric organization. Many works\nfocus on designing novel point convolutions given 3D point\nlocal neighborhoods [19], [29], [23], ignoring how the local\nneighborhoods should be formed.\nUnlike pixels in a 2D image, points in a 3D point\ncloud are un-ordered, with irregular and heterogeneous\nneighborhoods; regular convolution operations thus cannot\nbe applied. Many works [ 22], [23], [33], [34], [35], [36] aim to\ndesign point convolution operations that resemble regular\n2D convolutions. Fixed input point coordinates are used\nto deÔ¨Åne local neighborhoods for the point convolution,\nresulting in the same local neighbourhoods at different layers\nthat limit the model‚Äôs processing power. In contrast, our work\nuses spatial transformers at each layer to learn dynamic local\nneighborhoods in a more adaptive, Ô¨Çexible, and efÔ¨Åcient way.\nSpatial Transformations. The idea of enabling spatial trans-\nformation in neural networks has been explored for 2D image\nunderstanding [37]. It is natural to extend the idea to 3D\npoint clouds. PointNet [ 18] adopts a rigid transformation\nmodule on the input point cloud to factor out the object pose\nand improve classiÔ¨Åcation accuracy. KPConv [ 38] applies\nlocal deformation in the neighborhood of point convolution to\nenhance its learning capacity. In contrast, our work learns\nseveral different global transformations to apply on the input\npoint cloud at each layer for dynamic neighborhoods.\n3 M ETHODS\nWe Ô¨Årst brieÔ¨Çy review different geometric transformation\nmethods and their inÔ¨Çuence on the afÔ¨Ånity matrix of point\ncloud data, then describe the design of our three spatial trans-\nformers, namely, (a) afÔ¨Åne, (b) projective and (c) deformable.\nWe apply the spatial transformer block, consisting of multiple\nspatial transformers, to each layer of a network for altering\nDeformableRigidAffineProjective\nlinear transformationsnon-linear transformations\nFig. 3. Geometric transformations. We illustrate how a grey square\ntransforms after rigid, afÔ¨Åne, projective and deformable transformations.\nlocal neighborhoods for better point feature learning. We\nconclude the section by introducing how the transformers\ncan be added to existing point cloud processing networks\nand the relevance to other works.\n3.1 Geometric Transformations\nWe propose to learn transformations on the input point cloud\nto deform its geometric shape, and alter local neighborhoods\nwith new point afÔ¨Ånity matrices. The hypothesis behind the\nusage of geometric transformation is as follows:\nHypothesis 1. Let P = {pi}be the input point cloud and let Ni\nbe the local neighborhood around pi ‚ààR3 from which we extract\nlocal features. Let N = {Ni}be the set of local neighborhoods.\nAssume ÀúN =\n{\nÀúNi\n}\nbe the optimal neighborhood for learning\nlocal features, then ‚àÉ(smooth) Œ¶ : Ni ‚ÜíÀúNi for all pi.\nEssentially we are going to use different types of geo-\nmetric transformations to approximate Œ¶. The new learned\nafÔ¨Ånity matrix will dynamically alter local neighborhoods to\nallow better feature learning.\nIllustrated in Fig.3, transformations can be categorized\ninto rigid and non-rigid transformations, and the latter can\nbe further categorized into linear and non-linear transforma-\ntions. We now discuss different spatial transformations.\nRigid Transformations. The group of rigid transformations\nconsist of translations and rotations. However, rigid transfor-\nmations are isometric (in ‚Ñì2 distance) and therefore preserves\nthe point afÔ¨Ånity matrix. Thus, local neighborhoods are\ninvariant to rigid transformations in terms of k-NN graphs.\nHence, we do not consider rigid transformations.\nAfÔ¨Åne Transformations. AfÔ¨Åne transformations belong to\nnon-rigid linear transformations. Consider a 3D point cloud\nP = {pi}N\ni=1 ‚äÇ R3 consisting of N three-dimensional\nvectors pi ‚àà R3. Then, an afÔ¨Åne transformation can be\nparameterized by an invertible matrix A ‚àà R3√ó3 and a\ntranslation vector b ‚ààR3. Given A, b, the afÔ¨Åne transformed\ncoordinates pi can be written as pi ‚Ü¶‚ÜíApi + b. Note that\ntranslation b does not change the point afÔ¨Ånity matrix and\npoint neighborhoods. Recall that an afÔ¨Åne transformation\npreserves collinearity, parallelism, and convexity.\nProjective Transformations. Projective transformations are\nnon-rigid non-linear transformations. We Ô¨Årst map the 3D\npoint cloud P to the homogeneous space and get ÀúP, by\nappending one-vectors to the last dimension. The projective\ntransformation is parameterized by A ‚àà R4√ó4 and the\n4\ntransformed point Àúpi ‚Ü¶‚Üí AÀúpi. Compared to the afÔ¨Åne\ntransformations, projective transformations have more de-\ngrees of freedom but cannot preserve parallelism. Projective\ntransformations preserve collinearity and incidence, hence\nfail to capture all possible deformations. For example, points\nlying on the same line will always be mapped to a line,\nand this constraint may be overly restrictive. It is of interest\nto be able to break this constraint if these points are from\ndifferent semantic categories. A more general transformation\nthat covers various deformations may be more effective.\nDeformable Transformations. When all the points have the\nfreedom to move without much constraint, the 3D shape can\ndeform freely. We refer to this general spatial transformation\nas a deformable transformation. It has more degrees of freedom\nand does not necessarily preserve the topology.\nLearning new per-point offsets would be computationally\nhard and costly, we thus use a parametric offset model\ninstead. Taking both point coordinates and features as inputs,\nthe model would learn offsets dependent upon both spatial\nand feature representations of the input point cloud.\n3.2 Spatial Transformers for 3D Point Clouds\nOur so-called spatial transformer method applies a geometric\ntransformation to the input point cloud to obtain different\nlocal neighborhoods for feature learning. It can be applied\nto existing point cloud processing networks as spatial\ntransformers only alter local neighborhoods.\nSuppose at layer t, the spatial transformer block contains\nk(t) transformers. Each transformer learns a transformation\nto apply to the input point coordinates. We refer to the\ntransformed points as nodes of a sub-graph and their feature\non it the corresponding sub-feature. We then concatenate all\nsub-features from these transformers to form the Ô¨Ånal output\nof the learning block. Suppose that the ith spatial transformer\nat the tth layer takes as input the original point cloud P ‚àà\nR3√óN and previous feature map F(t‚àí1) ‚ààRf(t‚àí1)√óN .\nAfÔ¨Åne. We form k(t) new transformed point from pj as:\ng(t)\ni,j = A(t)\ni pj + b(t)\ni , i = 1, 2, ..., k(t). (1)\nSince the point afÔ¨Ånity matrix is invariant under uniform\nscaling and translation, we set ‚à•Ai‚à•F = 1, b= 0, for all i.\nThus, with G(t)\ni =\n{\ng(t)\ni,j\n}\nj\n, we simplify Equation 1 as:\nG(t)\ni = A(t)\ni P, i = 1, 2, ¬∑¬∑¬∑ , k(t). (2)\nWe compute thek nearest neighbours of each transformed\npoint G(t)\ni and obtain the point afÔ¨Ånity matrix S(t)\ni , based\non which we deÔ¨Åne local neighborhoods and apply point\nconvolutions on previous point cloud feature mapF(t‚àí1). We\nget the point cloud feature F(t)\ni ‚ààRf(t)\ni √óN of the sub-graph\nfrom i-th transformation and its altered neighborhoods:\nF(t)\ni = CONV(F(t‚àí1), S(t)\ni , k), i = 1, 2, ..., k(t), (3)\nwhere CONV denotes the point convolution: It takes (a) previ-\nous point cloud features, (b) the afÔ¨Ånity matrix (for deÔ¨Åning\nlocal neighborhoods of every point) and (c) the number of\nneighbors (for deÔ¨Åning the size of neighborhoods) as inputs.\nIn point convolutions such as [ 22], the point afÔ¨Ånity\nmatrix changes the input feature in a non-differentiable\nway. Therefore, we append the transformed point cloud\nP(t)\ni to the input feature for the sake of back-propagating\nthe transformation matrix A. In sampling-based convolu-\ntions such as bilateral convolution [ 29], the point afÔ¨Ånity\nmatrix changes the input feature in a differentiable way; no\nadditional operation is needed.\nFor all the k(t) sub-graph in layer/ block t, we learn k(t)\npoint cloud features F(t)\ni . The output of this module is the\nconcatenation of all the sub-graph point cloud features:\nF(t) = CONCAT(F(t)\n1 , F(t)\n2 , ..., F(t)\nk(t) ), (4)\nwhere F(t)\ni ‚ààRf(t)\ni √óN and f(t) = ‚àëk(t)\ni f(t)\ni , F(t) ‚ààRf(t)√óN .\nIn our implementation, we randomly initialize A from the\nstandard normal distribution N(0, 1). Before computing the\ncoordinates of the transformed point cloud, we normalize A\nby its norm ‚à•A‚à•F , as the point afÔ¨Ånity matrix is invariant\nunder uniform scaling.\nProjective. Analogous to the afÔ¨Åne spatial transformer, for\nthe ith graph at tth layer, we apply a projective transformation\nto the point cloud ÀúP in homogeneous coordinates and get\nthe transformed point cloud as:\nÀúG(t)\ni = B(t)\ni ÀúP, i = 1, 2, ¬∑¬∑¬∑ , k(t), (5)\nwhere B(t)\ni ‚ààR4√ó4 is the transformation matrix in homoge-\nneous coordinates. We then follow the same procedure as in\nEquations 3 and 4 to get the output feature Ft.\nDeformable. AfÔ¨Åne and projective transformations can trans-\nform the input point cloud, alter the point afÔ¨Ånity matrix, and\nprovide learnable local neighborhoods for point convolutions\nat different layers. However, they are limited as afÔ¨Åne\ntransformations are linear and projective transformations\nmap lines to lines only. We deÔ¨Åne a non-linear deformable\nspatial transformer at the tth layer and ith sub-graph as\nG(t)\ni = A(t)\ni P + D(t)\ni , (6)\nwhere A(t)\ni P is the afÔ¨Åne transformation component and\nD(t)\ni ‚ààR3√óN gives every point additional freedom to move,\nso the point cloud has the Ô¨Çexibility to deform its shape. Note\nthat the translation vector b in Equation 1 is a special case\nof the deformation matrix D(t)\ni . In general, the deformation\nmatrix D(t)\ni can signiÔ¨Åcantly change local neighborhoods.\nThe spatial transformer parameters are learned in an\nend-to-end fashion from both point cloud coordinates and\nfeatures. Since afÔ¨Åne transformation A(t)\ni P is dependent\non spatial locations, we let the deformation matrix D(t)\ni\ndepend on the features: D(t)\ni = C(t)\ni F(t‚àí1), where C(t)\ni ‚àà\nR3√óf transforms the previous layer feature F(t‚àí1) ‚ààRf√óN\nfrom Rf to R3. Hence, the deformable transformation in\nEquation 6 can thus be simpliÔ¨Åed as:\nG(t)\ni =\n[\nA(t)\ni C(t)\ni\n][ P\nF(t‚àí1)\n]\n= C(t)\ni\n[ P\nF(t‚àí1)\n]\n, (7)\nwhere C(t)\ni ‚ààR3√ó(3+f(t‚àí1)) is the concatenation of afÔ¨Åne\nand deformable transformation matrix that captures both\npoint cloud coordinates and features.\nAfter we compute the transformed point coordinates\nG(t), we follow Equations 3 and 4 to learn the feature of each\n5\ntransformed sub-graph and concatenate them as the Ô¨Ånal\noutput feature of layer t.\nOur deformable spatial transformer has two parts: A(t)\ni P\nand C(t)\ni F(t‚àí1), for a linear transformation of 3D spatial\ncoordinates and a nonlinear transformation of point features\n(which reÔ¨Çect semantics) respectively. In Section 4.5, we\nprovide empirical analysis of these two components.\n3.3 Spatial Transformer Networks\nWe spatially transform the input point cloud in order to\nobtain dynamic local neighborhoods for point convolutions.\nThe transformer can be easily added to existing point cloud\nprocessing networks. We Ô¨Årst describe the procedure and\nthen provide three applications with several networks.\nPoint Cloud Networks with Spatial Transformers. Con-\nsider segmenting N 3D points into C classes as an example.\nFig.4 depicts a general network architecture for point cloud\nsegmentation, where several spatial transformers are used\nat different layers. At layer t, we learn k(t) transformation\nmatrices {A(t)\ni }k(t)\ni=1, apply each to the input point coordinates\nP, and then compute the point afÔ¨Ånity matrices {S(t)\ni }k(t)\ni=1,\ne.g., based on k-NN graphs for the edge convolution [22].\nFor each sub-transformation, we learn a feature F(t)\ni of\ndimension N √óf(t)\ni . We then concatenate all k(t) features at\nthis layer to form an output feature Ft of dimension N √óf(t),\nwhere f(t) = ‚àëk(t)\ni f(t)\ni . The output feature serves as the\ninput to the next layer for further feature learning.\nNote that afÔ¨Åne or projective transformation matrices\nare applied to the original point cloud coordinates P, since\neach layer has not just one but multiple spatial transformers.\nHowever, the deformable transformation matrix C(t)\ni is ap-\nplied to the previous feature map, the feature transformation\ncomponent is thus progressively learned.\nBy stacking several such transformation learning blocks\nand Ô¨Ånally a fully connected layer of dimension C, we\ncan map the input point cloud to the segmentation map of\ndimension C √óN, or downsample to a vector of dimension\nC for classiÔ¨Åcation tasks. For the spatial transformer block in\na point cloud detection network (Fig.5), C is the dimension\nof the output feature. We train the network end-to-end.\nClassiÔ¨Åcation Networks. A point cloud classiÔ¨Åer [ 19], [23]\ntakes 3D points, learns features from their local neighbor-\nhoods, and outputs C classiÔ¨Åcation scores, where C is the\nnumber of classes. We add spatial transformers at each layer\nto obtain different local neighborhoods for feature learning.\nPoint-based Segmentation Networks. These networks [19],\n[18], [23], [22] take 3D points and compute their point afÔ¨Ånity\nmatrices and local neighborhoods from the point coordinates.\nFeatures are learned by applying convolution operators on\nthe points and their local neighborhoods.\nWe use the edge convolution in [22] as our baseline, which\ntakes relative point coordinates as inputs and achieves the\nstate-of-the-art performance. SpeciÔ¨Åcally, we retain their\nlearning settings and simply insert spatial transformers to\ngenerate new local neighborhoods for the edge convolutions.\nSampling-based Segmentation Networks. To demonstrate\nthe general applicability of our spatial transformers, we\nconsider point afÔ¨Ånity matrices on transformed point clouds\nas deÔ¨Åned in sampling-based networks such as SplatNet [ 29].\nSplatNet groups 3D points onto a permutohedral lattice\n[30] and applies bilateral Ô¨Ålters [ 31] on the grouped points\nto get features. The permutohedral lattice deÔ¨Ånes the local\nneighborhoods of every point and makes the bilateral\nconvolution possible. We add spatial transformers to deform\nthe point cloud and form various new lattices. The local\nneighborhoods can dynamically conÔ¨Ågure for learning point\ncloud semantics. We keep all the other settings of SplatNet.\nDetection Networks. Detecting objects in a 3D point cloud\ngenerated from e.g. LiDAR sensors is important for au-\ntonomous navigation, housekeeping robots, and AR/ VR.\nThese 3D points are often sparse and imbalanced across\nsemantic classes. Our spatial transformers can be added to\na detection network and improve feature learning efÔ¨Åciency\nand task performance with dynamic local neighborhoods.\nOur baseline is VoxelNet [ 39], the state-of-the-art 3D\nobject detector for autonomous driving data. We adopt all\nits settings, and add spatial transformers on the raw point\ncloud data, before point grouping (Fig.5). To demonstrate that\nspatial transformers enhance feature learning for point cloud\nprocessing, we let transformers only affect point features but\nnot point coordinates for grouping. With spatial transformers,\npoint coordinates could also be transformed at the grouping\nstage, which would lead to non-cuboid 3D detection boxes.\nAlthough interesting, we do not explore this variation and\ndeem it beyond the scope of this paper.\n3.4 Relevance to Other Works\nWe review related works on deformable convolutions [ 40],\n[38] and DGCNN [22].\nDeformable Convolutions. Deformable convolutional net-\nworks [40] learn dynamic local neighborhoods for 2D images.\nSpeciÔ¨Åcally, at each location p0 of the output feature map\nY , deformable convolutions modify the regular grid R with\noffsets {‚àÜpn}N\nn=1, where N = |R|. The output on input X\nby convolution with weight w becomes:\nY (p0) =\n‚àë\npn‚ààR3\nw(pn)X(p0 + pn + ‚àÜpn) (8)\nNote that KPConv [ 38] directly adapts this formula to\npoint clouds as deformable point convolutions. Although\nalso achieving dynamic local neighborhoods, our spatial\ntransformers alter neighborhoods differently:\n1) Deformable point convolutions learn to alter each neigh-\nborhood with an offset to the regular grid R. We learn\nglobal transformations on the input point cloud and the\nmetric of deÔ¨Åning local neighborhoods changes. Global\ntransformations like afÔ¨Åne transformations can retain\nthe global geometric properties such as collinearity and\nparallelism, while local transformations has no such\nconstraints as only local neighborhoods are available.\n2) Offsets of deformable point convolutions are dependent\nupon feature values, while transformation matrices of\nour spatial transformers are dependent upon point co-\nordinates for afÔ¨Åne and projective transformations, or\nboth point coordinates and feature values for deformable\ntransformations. Access to point coordinates provides\nadditional information and regularzation.\nDynamic Graph CNN. Dynamic local neighborhoods have\nalso been explored in DGCNN [22] for point cloud processing.\nIt has three main differences with our work.\n6\n!!!\npoint conv\npoint conv\npoint conv\n!!!!!!!!!\nùê¥#\n(#)ùëÉ\nùê¥'\n(#)ùëÉ\nùê¥(())\n(#) ùëÉ\nùëÅ\nùëì(#)\nùëì(#)\nùëì(#)\nùëÅ√ó\t(ùëò(#)√óùëì(#))\n!!!\npoint conv\npoint conv\npoint conv\n!!!!!!!!!\nùê¥#\n(')ùëÉ\nùê¥'\n(')ùëÉ\nùê¥((/)\n(') ùëÉ\nùëÅ\nùëì(')\nùëì(')\nùëì(')\nùëÅ√ó\t(ùëò(')√óùëì('))\nùëÅ√ó3\nInput Point Cloud ùëÉ\n!!!\nFully\tConnected\t\nùëÅ√óùê∂\nOutput Segmentation\naffinity matrix\naffinity matrix affinity matrix\naffinity matrix\naffinity matrix affinity matrix\nSpatial Transformer Block Spatial Transformer Block\nFig. 4. The point cloud segmentation network with spatial transformers. Our network consists of several spatial transformers. At each layer, we learn\nk transformation matrices A to apply to the input point cloud P, and compute the corresponding point afÔ¨Ånity matrices based on their k-NN graphs.\nFor each sub transformation, we can learn a sub-feature, and then concatenate all features to form an output feature of dimension f √ó N. The output\nfeature will be used for the next spatial transformer block for feature learning. By stacking several such transformation learning blocks and Ô¨Ånally a\nfully connected layer of dimension C (the number of class), we can map the input point cloud to the C √ó N segmentation map.\nSpatial \nTransformer \nBlocks\nV oxel Partition Grouping V oxel Feature Encoding 4D V oxel Feature\nConvolutional Middle Layers\nRegional Proposal Network\nFeature Learning Network\n‚Ä¢ ‚Ä¢\n‚Ä¢\n‚Ä¢‚Ä¢ ‚Ä¢‚Ä¢‚Ä¢ ‚Ä¢\n‚Ä¢\n‚Ä¢ ‚Ä¢‚Ä¢\nmaxpoolfc\nFig. 5. The object detection network. We add spatial transformers to\nthe the point feature learning network of [39] for obtaining dynamic local\nneighborhoods. Transformers only affect feature learning but not point\ncoordinates for grouping.\n1) How neighborhoods are deÔ¨Åned is different. DGCNN\nuses high-dimensional feature maps to construct the point\nafÔ¨Ånity matrix and generate local neighborhoods. Our\nlocal neighborhoods are from transformed point clouds.\nReusing point features for deÔ¨Åning neighborhoods may\nbe straightforward, but reduce the distinction between\nspatial and semantic information and hurt generalization.\n2) It is computationally costly to build dense nearest neigh-\nbor graphs in a high-dimensional feature space.\n3) DGCNN [ 22] uses only one nearest neighbor graph at\ndifferent layers, whereas we have multiple graphs at each\nlayer for capturing different geometric transformations.\nWith less computational cost and more Ô¨Çexibility in geometric\ntransformations, we achieve better empirical performance on\nsemantic segmentation (Table 2 and Table 3).\n4 E XPERIMENTS\nWe conduct comprehensive experiments to verify the effec-\ntiveness of our spatial transformers. We benchmark with\ntwo types of networks, point-based and sampling-based\nmetrics for deÔ¨Åning point neighborhoods, on four point cloud\nprocessing tasks: classiÔ¨Åcation, part segmentation, semantic\nsegmentation and detection. We conduct ablation studies\non deformable spatial transformers. We further provide\nvisualization, analysis and insights of our method.\n4.1 ClassiÔ¨Åcation\nWe benchmark on ModelNet40 3D shape classiÔ¨Åcation [ 11].\nWe add transformers to two baselines [ 22], [29] and adopt\nthe same network architecture, experimental setting and\nevaluation protocols. Table 1 and Fig.4.1 show that adding\nspatial transformers to point-based and sampling-based\nmethod gives 1% and 2% gain.\nIn addition, our performance gain over [22], which builds\none per-layer dynamic neighborhood graphs with high-\ndimensional point features, demonstrates the advantages\nof our method of building multiple dynamic neighborhood\ngraphs with transformed 3D point coordinates.\nFig.12 shows that spatial transformers align the 3D shape\nbetter according to its semantics. We augment training and\ntesting data with random rotations, and observe that spatial\ntransformers gain 3% over its Ô¨Åxed graph counterpart.\n4.2 Part Segmentation\nWe benchmark on ShapeNet part segmentation [41], where\nthe goal is to assign a part category label (e.g. chair leg, cup\nhandle) to each 3D point. The dataset contains 16, 881 shapes\nfrom 16 categories, annotated with 50 parts in total, and the\nnumber of parts per category ranges from 2 to 6.\n4.2.1 Point-based Method\nNetwork Architectures. Point-based methods construct neigh-\nborhoods based on point coordinate operations such as edge\nconvolution for our baseline DGCNN [ 22]. We follow the\nsame network architecture and evaluation protocols of [ 22].\nThe network has 3 convolutional layers; the output feature\ndimension is 64. To capture information at different levels, all\n7\nTABLE 1\nSpatial transformers improves ModelNet40 classiÔ¨Åcation accuracy. We report ModelNet40 classiÔ¨Åcation accuracy of different\nbaselines, without and with spatial transformers, with or without random rotations. Point-based refers to the baseline method\nadopting Euclidean-distance based afÔ¨Ånity matrices [22]. Sampling-based refers to the baseline method adopting\npermutohedral-lattice based afÔ¨Ånity matrices [29]. We observe accuracy gains for different baseline networks with spatial\ntransformers. Transformer gains are invariant to input rotations.\nPoint-based Point-based with rand. input rotations Samplin-based\nPointNet [18]DGCNN [22] [22] (Ô¨Åxed) AfÔ¨Åne Proj. Deformable [22] (Ô¨Åxed) Deformable SplatNet[29] AfÔ¨Åne Proj. Deformable\nAvg. 86.2 89.2 88.8 89.3 89.2 89.9 85.7 88.3 86.3 87.4 87.1 88.6\n11.2%10.1%14.3%11.7%13.7%11.4%\n0%\n4%\n8%\n12%\n16%\nFixDynamicFixDynamicFixDynamic\nModelNet40 Classification Error\nPoint-based methodPoint-based methodw/ random input rotationSampling-based method\nFig. 6. Spatial transformers lead to higher accuracy and more rotation\ninvariance on ModelNet40. We report classiÔ¨Åcation errors for different\nbaselines, without and with spatial transformers, with or without random\nrotations. Transformers consistently lead to the lower errors than Ô¨Åxed\ngraph baselines, and the improvement is larger upon random rotations.\nGround-truth Fix Our Dynamic\nFig. 7. Spatial transformers improve the part segmentation performance.\nWe show part segmentation results of different baselines, where different\nparts are marked with different colors. With spatial transformers, part\nsegmentation for objects with less rigid and more complicated structures\nimproves (1st and 2nd row, lamp). The segmentation consistency within\neach part also improves (3rd row, rocket).\nthe convolutional features are concatenated and fed through\nseveral fully connected layers to output the segmentation.\nAs a Ô¨Åxed graph baseline, we use the same input point\ncoordinates as the metric to deÔ¨Åne Ô¨Åxed local neighborhoods.\nWe insert spatial transformers to alter the metric for deÔ¨Ån-\ning point neighborhoods for edge convolutions. There are\npoint-based afÔ¨Åne, projective and deformable networks when\ninserting different spatial transformers (Section 3.2). As for\nclassiÔ¨Åcation, [ 22] directly uses learned features to build\npoint afÔ¨Ånity matrices for dynamic neighborhoods.\nWe follow [ 22] and use three edge convolution layers.\nAt each layer, we keep the number of graphs k and sub-\ngraph feature dimension f the same, and search for the\nbest architecture. We report results of afÔ¨Åne, projective\nand deformable networks with k = 4 , f =!32. For fair\ncomparisons, we increase the number of channels of baselines\nso all the methods have the same number of parameters.\nResults and Analyses . In Table 2, we report the instance\naverage mIOU (mean intersection over union), as well as\nthe mIOU of some representative categories in ShapeNet.\nCompared with the Ô¨Åxed graph baseline, the afÔ¨Åne, projective\nand deformable spatial transformers achieve 0.5%, 0.2%\nand 1.1% improvement respectively and beats the Ô¨Åxed\ngraph baseline methods in most categories. SpeciÔ¨Åcally, we\nobserve 8.0%, 8.3% and 4.7% performance boost with spatial\ntransformers over the Ô¨Åxed graph baseline. Our deformable\nspatial transformers gain 4.0% over [22].\nWe also beat other state-of-the-art methods [ 18], [ 19],\n[20] by a signiÔ¨Åcant margin. Adding deformable spatial\ntransformers to PointCNN [23] gains 6% (4%) on motorbike\n(bag) and 1% on average. We observe that categories with\nfewer samples are more likely to gain possibly due to\nregularization by transformers. Fig.7 shows that deformable\nspatial transformers make more smooth predictions and\nachieve better performance than the Ô¨Åxed graph baseline.\nFrom afÔ¨Åne to deformable transformations, the perfor-\nmance increases as the degree of freedom increases for\nthe transformer. Projective transformers, however, perform\nslightly worse than afÔ¨Åne transformers. The performance\ndrop could result from geometrical distortion caused by map-\nping 3D points with homogeneous coordinates. Furthermore,\nfor deformable transformers, when removing the constraint\nthat the transformed points should be similar to the input\npoint cloud (Fig.8, feature only G = CF ), the performance\nalso drops, indicating the necessity of the proposed similar-\nto-input constraint on spatial transformers.\n4.2.2 Sampling-based Method\nNetwork Architectures. Sampling-based methods construct\nneighborhoods are based on sampling operations on point\ncoordinates. SplatNet [29] groups points on permutohedral\nlattices and applies learned bilateral Ô¨Ålters [ 31] on naturally\ndeÔ¨Åned local neighbors to extract features. We follow the\nsame architecture as SplatNet [29]. The network starts with\na single 1 √ó1 regular convolutional layer, followed by 5\nbilateral convolution layers (BCL). The output of all BCL are\nconcatenated and fed to a Ô¨Ånal 1 √ó1 regular convolutional\n8\nTABLE 2\nSpatial transformers improve part segmentation performance. We report mIoU(%) on ShapeNet PartSeg dataset. Compared with several other\nmethods, deformable spatial transformers achieve the SOTA in average mIoU.\nAvg.aerobagcapcarchairearphoneguitarknifelamplaptopmotorbikemugpistolrocketskateboardtable# shapes 26907655898375869 7873921547451 202 18428366 152 52713DCNN [18] 79.475.1 72.8 73.3 70.0 87.2 63.5 88.4 79.6 74.4 93.9 58.7 91.8 76.4 51.2 65.3 77.1PointNet[18] 83.783.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6PointNet++ [19]85.082.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.482.6FCPN [20] 81.384.0 82.8 86.488.383.3 73.693.487.4 77.497.7 81.495.887.7 68.4 83.673.4DGCNN [22] 81.384.0 82.8 86.4 78.090.976.8 91.1 87.4 83.0 95.7 66.2 94.7 80.3 58.7 74.2 80.1Point-based [22] Ô¨Åxed graph84.283.7 82.4 84.0 78.290.969.9 91.3 86.6 82.5 95.8 66.5 94.0 80.8 56.0 73.8 79.8Point-based afÔ¨Åne84.784.1 83.5 86.9 79.690.972.5 91.6 88.2 83.3 96.1 68.9 95.3 83.3 60.9 75.2 79.7Point-based projective84.484.3 84.2 88.5 77.9 90.4 72.8 91.2 86.6 81.7 96.0 66.6 94.8 81.3 61.6 72.1 80.5Point-based deformable85.384.683.3 88.7 79.490.977.9 91.7 87.6 83.5 96.0 68.8 95.2 82.4 64.3 76.3 81.5Point-based deformable random84.784.3 84.4 83.2 78.9 90.8 75.6 91.4 87.1 83.0 95.9 66.8 94.8 82.1 62.3 75.7 80.4PointCNN [23]84.982.7 82.8 82.5 80.0 90.1 75.8 91.387.882.6 95.7 69.8 93.6 81.1 61.5 80.1 81.9PointCNN deformable85.883.486.685.5 79.1 90.378.591.687.884.2 95.8 75.3 94.6 83.3 65.0 80.7 81.7Sampling-based baseline [29]84.681.9 83.9 88.6 79.5 90.1 73.5 91.3 84.7 84.5 96.3 69.7 95.0 81.7 59.2 70.4 81.3Sampling-based projective84.482.1 84.089.177.9 89.6 73.7 91.1 83.3 83.0 96.3 67.2 94.5 79.8 60.0 68.8 82.1Sampling-based deformable85.282.9 83.8 87.6 79.6 90.6 73.0 92.2 86.185.796.3 72.795.883.1 65.1 76.5 81.3\n-1%\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\nAirplane\nBag Cap Car Chair\nEarphone\nGuitar Knife LampLaptop\nMotorbike\nMug PistolRocket\nSkateboard\nTable\nclass avg.\ninstance avg.\nPart Segmentation mIOU Improvement with Different \nComponent in Deformable Spatial Trasformer\nfeature only G = CF\naffine G = AP\ndeformable G = AP+CF\nFig. 8. Transforming both point cloud coordinates and features for\ndynamic local neighborhoods leads to the largest gain. We report different\nparts of deformable transformers‚Äô performance gain over Ô¨Åxed local\nneighborhood baseline on ShapeNet part segmentation. 0% means\nachieving the same accuracy as Ô¨Åxed local neighborhood baseline and\nnegative value means achieving worse accuracy than Ô¨Åxed neighborhood\nbaseline. Compared with preserving either afÔ¨Åne part AP or feature part\nCF , deformable spatial transformers (AP + CF ) achieves largest gains\non every category, speciÔ¨Åcally 8% gains on earphone and rocket.\nlayer to get the segmentation output. Since each BCL directly\ntakes raw point locations, we consider it as a Ô¨Åxed graph\nbaseline. We add deformable spatial transformers to the\nnetwork and feed transformed point graphs to BCL to\nconstruct permutohedral lattices. With gradients on the\npermutohedral lattice grid, we can make the transformation\nmatrix learned end-to-end. Note that we increase the channel\nof convolution layers for fair comparisons.\nResults and Analyses. Table 2 shows that our deformable\nspatial transformers (with k = 1 at all BCLs) gains over the\nsampling-based Ô¨Åxed graph baseline [ 29] in most categories\nwith 0.6% on average and 5.9% for the rocket category. It also\nbeats other state-of-the-art baselines.\n4.3 Semantic Segmentation\nWe benchmark on the Stanford 3D semantic parsing dataset\n[42]. It contains 3D scans by Matterport covering 6 areas and\n271 rooms. Each point is annotated into one of 13 categories\nsuch as chair, table, Ô¨Çoor, clutter. We follow the data processing\nprocedure of [ 18]: We Ô¨Årst split points by room, and then\nsample rooms into several 1m √ó1m blocks. When training,\n4096 points are sampled from the block on the Ô¨Çy. We train\nour network to predict the point class in each block, where\neach point is represented by 9 values: XYZ, RGB and its\n[0,1]-normalized location with respect to the room.\nTABLE 3\nSpatial transformers improve semantic segmentation performance. We\nreport mIoU(%) on S3DIS semantic segmentation dataset. Adding\nspatial transformers to [22] and [29] improves the performance.\nPointNet[18]DGCNN[22][22](FIXED)[22]+AFF[22]+DEFSplatNet [29][29]+DEF47.7 56.1 56.0 56.957.254.1 55.5ceilingÔ¨Çoorwallbeamcolumnwindowclutter[22](FIXED)92.5 93.1 76.1 51.041.7 49.646.8[22]+AFF92.7 93.6 76.7 52.6 41.2 48.7 47.8[22]+PROJ92.5 93.5 76.7 52.7 40.7 48.548.0[22]+DEF92.8 93.6 76.8 52.941.1 49.048.0doortablechairsofabookcaseboard[22](FIXED)63.4 61.8 43.1 23.342.043.5[22]+AFF63.7 63.4 45.1 27.0 41.3 44.8[22]+PROJ63.5 62.3 44.8 27.0 41.5 44.9[22]+DEF63.5 64.2 45.2 28.141.746.1\nTABLE 4\nSpatial transformers improve object detection performance. We report\ncar detection AP(%) on KITTI validation set. Adding spatial transformers\nleads to 2% performance gain.\nbirds‚Äô eye 3D\nEasy Medium Hard Easy Medium Hard\nVoxelNet[39] 77.3 59.6 51.6 43.8 32.6 27.9\nVoxelNet + Ô¨Åxed graph84.3 67.2 59.0 45.7 34.5 32.4\nVoxelNet + deformable85.3 69.1 60.9 46.1 35.9 34.0\nNetwork Architectures. We adopt DGCNN [22] as Section\n4.2, with C = 13, the number of semantic categories.\nResults and Analyses . In terms of average mIoU, Table 3\nshows that afÔ¨Åne and deformable spatial transformers gain\n0.9% and 1.2% respectively over the Ô¨Åxed graph baseline.\nDeformable transformers also gain 1.1% over [22] and beat all\nother state-of-the-art methods. Likewise for sampling-based\nmethods [29], we observe 1.4% gain.\nAs for part segmentation, semantic segmentation perfor-\nmance improves when point clouds are given more freedom\nto deform (from afÔ¨Åne to deformable spatial transformers)\nbased on transformation of original locations and feature\nprojections. Projective transformers give least performance\ngain, suggesting that mapping 3D points via homogeneous\ncoordinates may not be most efÔ¨Åcient.\nFig.9 shows that semantic segmentation results are\nsmoother and more robust to missing points and occlusions\nwith our deformable transformers.\n4.4 3D Object Detection\nWe benchmark on KITTI 3D object detection [ 43]. It contains\n7,481 training images / point clouds and 7,518 test images\n/ point clouds, covering three categories: Car, Pedestrian,\nand Cyclist. For each class, detection outcomes are evaluated\n9\nFig. 9. Spatial transformers improve semantic segmentation results. We show qualitative visualizations for semantic segmentation of deformable\nspatial transformers and the Ô¨Åxed local neighborhood baseline. The Ô¨Årst column is the input point cloud, the second and the third column shows the\nÔ¨Åxed graph and our spatial transformer results, and the last column is the ground truth. Points belonging to different semantic regions are colored\ndifferently. We observe better and more consistent segmentation result with our spatial transformer, speciÔ¨Åcally for the areas circled in red.\nbased on three difÔ¨Åculty levels: easy, moderate, and hard,\naccording to the object size, occlusion state and truncation\nlevel. We follow the evaluation protocol in VoxelNet [39] and\nreport car detection results on the validation set.\nNetwork Architectures.. Shown in Fig.5, the network Ô¨Årst\npartitions raw 3D points into voxels. We add deformable spa-\ntial transformers; points in each voxel are represented with\npoint features. There are two deformable feature learning\nlayers, each layer having 2 sub-graphs with 16-dimensional\noutputs. Note that the voxel partition is based on the input\npoint coordinates. As in VoxelNet, the point features in\neach voxel are fed to 2 voxel feature encoding layers with\nchannel 32 and 128 to get sparse 4D tensors representing the\nspace. The middle convolutional layers process 4D tensors to\nfurther aggregate spatial contexts. Finally a Region Proposal\nNetwork (RPN) generates the 3D detection.\nWe report the performance of 3 networks: (1) VoxelNet\nbaseline [39]; (2) the Ô¨Åxed graph baseline, where we used the\noriginal point cloud location to learn the point feature at the\nplace of spatial transformer blocks; (3) deformable spatial\ntransformer networks as discussed above.\nResults and Analyses. Table 4 reports car detection results\non KITTI validation set. 1 Compared with baseline, having\n1. The authors did not provide code. We use the implementation by\n[44] and obtain lower performance than the original paper.\nTABLE 5\nPerformance of different number of deformable transformation modules.\nMetric is average mIOU (%).\nÔ¨Åxed graph 1 graph 2 graphs 4 graphs\nf(t)\ni = 32 84.2 84.9 85.2 85.3\nf(t)\ni k(t)\ni = 64 84.2 85.3 85.2 83.5\nIn the Ô¨Årst row, the output feature of each sub-graph is of dim. 32, while\nthe number of subgraphs changes; the second row limits the multiplication of\nnumber of sub-graphs and sub-feature dim. to be 64.\na point feature learning module improves the performance\nby 7.3% and 2.8% for birds‚Äô eye view and 3D detection per-\nformance on average, respectively. The deformable module\nfurther improves 8.9% and 3.9% respectively over VoxelNet.\n4.5 Ablation Studies\nWe conduct ablation studies to understand how many\nspatial transformers may be sufÔ¨Åcient to achieve satisfac-\ntory performance. We also study transformations of point\ncoordinates and features of deformable spatial transformers.\nThe inÔ¨Çuences of updating transformation matrices and\ntransformers at different layers are investigated.\nThe Number of Transformers . Table 5 shows that for the\nÔ¨Åxed sub-feature dimension, the more graphs in each layer,\nthe higher the performance. With the Ô¨Åxed complexity, (i.e.,\n10\nFig. 10. Part segmentation performance (average mIOU) of deformable\ntransformers at different layers. When applying all transformers at three\nlayers, the performance is highest. Removing transformers at different\nlayer lead to performance drop. Removing transformers at layer 3 gives\nthe most performance drop.\nthe product of the number of sub-graphs and the sub-feature\ndimension Ô¨Åxed at 64), the best performance is achieved at\nk = 1, f= 64 and k = 2, f= 32 .\nTwo Components in Deformable transformers . A de-\nformable spatial transformer has two components (Equation\n7): afÔ¨Åne transformation on point coordinates, AP, and\nthree-dimensional projection of high-dimensional feature,\nCF . Fig.8 shows that both afÔ¨Åne and feature only spatial\ntransformers also improve performance, but the combination\nof both leads to the largest gain.\nUpdating Transformation Matrices. The transformation ma-\ntrices are updated in an end-to-end fashion with the ultimate\ngoal of increasing the task performance. It is of interest to\nunderstand if updating transformation matrix boosts the\nperformance. SpeciÔ¨Åcally, we randomly initialize transfor-\nmation matrices of deformable spatial transformers and\nkeep them not updated during training. The performance is\n0.5% better than Ô¨Åxed graphs, indicating that adding more\ntransformation graphs at different layers helps; however,\nit is 0.6% worse than updating transformation matrices,\nindicating learning to update transformation matrices in\nan end-to-end fashion is helpful.\nTransformers at Different Layers . We start with all de-\nformable transformers effective at three layers, and remove\ntransformers at one layer a time. Fig.10 shows that for part\nsegmentation, the performance is best with all transformers,\nwhereas removing transformers at layer 3 gives the largest\nperformance drop, suggesting that transformers at every\nlayer help and those at the last layer are most important.\n4.6 Time and Space Complexity\nWith spatial transformers, the model size changes little and\nthe inference takes slightly more time (Table 6). Note that\nfor fair comparisons, we increase the number of channels in\nthe Ô¨Åxed graph baseline model for all the experiments. Even\nwithout increasing the number of parameters of baselines\n(not shown in Table 6), adding spatial transformers only\nincreases the number of parameters by 0.1%, as the number\nof parameters of spatial transformers (only transformation\nmatrices) is very small.\nTABLE 6\nModel size and test time on ShapeNet part segmentation. Spatial\ntransformers slightly increase the inference time.\nSampling-based Point-based\n[29] [29] + transformer[22] (Ô¨Åxed)[22] + transformer\n# Params. 2,738K 2,738K 2,174K 2,174K\nInference time (s/shape)0.352 0.379 0.291 0.315\nTransformed coordinates\nEarphone\nInput coordinates\nTable\nTransformed shapePart label \nwith 2 query points\nNearest Neighbor Retrieval using\nFig. 11. Local neighborhoods of two query points (red and yellow)\nusing (transformed) 3D coordinates with nearest neighbor retrieval.\nNeighborhoods of transformed point clouds makes semantic information\nextraction more efÔ¨Åcient: the neighborhood inside the dashed circle\nadapts to table base part. View rotating version for better visualization.\n4.7 Visualization and Analysis\nWe visualize the change in local neighborhoods when apply-\ning spatial transformers. We also visualize the transformed\n3D points globally and locally.\nDynamic Neighborhood Visualization. To illustrate how\nour spatial transformers learn diverse neighborhoods for 3D\nshapes, we show the nearest neighbors of two query points\nand use corresponding colors to indicate corresponding\nneighborhoods. (1) Fig.11 shows that neighborhoods re-\ntrieved from deformed shapes encode additional semantic in-\nformation, compared to neighborhoods from 3D coordinates.\n(2) Fig.2 shows that for table and earphone, different graphs\nenable the network to learn from diverse neighborhoods\nwithout incurring additional computational cost.\nGlobal Visualization of Deformable Transformations .\nFig.12 depicts some examples of learned deformable trans-\nformations in ShapeNet part segmentation. Each graph at a\ncertain layer aligns the input 3D shape with similar semantic\ngeometric transformations. For example, regardless of the\nshape of the rocket, graph 2 at layer 2 always captures the\nrocket wing information.\nLocal Distributions after Deformable Transformations. 3D\nPoints often do not have balanced sampling, which makes\npoint convolution challenging, as the k-NN graph does\nnot accurately represents the exact neighborhood and 3D\nstructure information. Our deformable spatial transformer\ngives every point Ô¨Çexibility and Ô¨Ånds better neighborhoods.\nWe wonder if transformers make the point cloud closer\nto balanced sampling. We normalize the point coordinates\nfor fair comparisons. Fig.13 visualizes the local distribution\naround a sample point on skateboard: After deformable\ntransformation, the points are moved to a more uniform\ndistribution. We analyze the standard deviation of raw and\ntransformed point cloud coordinates in ShapeNet data. The\nstandard deviation of point coordinates decreases 50.2% over\nall categories after spatial transformations, indicating a more\nbalanced distribution of transformed points.\nWe check if the point coordinates are statistically different\nbefore and after the application of transformers. We perform\nt-test on the original and transformed point clouds. The t-\nscore is 7.15 over all categories with p-value smaller than 1e-9.\nThe transformed point cloud distribution is thus statistically\ndifferent from the input point cloud distribution.\n11\nGTprediction\ngraph\t1graph\t2\nlayer\t1 layer\t2 layer\t3\nGTprediction\ngraph\t1graph\t2\ngraph\t1graph\t2graph\t1graph\t2graph\t1graph\t2graph\t1graph\t2\nGTpredictionGTpredictionGTpredictionGTprediction\nFig. 12. Examples of learned deformable transformations in ShapeNet\npart segmentation. 3D shapes include rocket, table and earphone\n(from up to bottom). Every two rows depict an instance with learned\ntransformations. We observe that each transformation at certain layer\naligns input 3D shape with similar semantic geometric transformation,\ne.g., graph 2 at layer 2 in rocket examples captures rocket wings. Graph\n2 at layer 1 in table examples captures table surfaces.\n5 C ONCLUSION\nWe propose novel spatial transformers for 3D point clouds\nthat can be easily added onto to existing point cloud\nlayer 1layer 2layer 3\nstd1.03e-3     1.01e-3     1.01e-3\ninput point cloud\ndeformable transformation\nlocal distribution\nlocal distribution\nSkateboard\nFig. 13. Spatial transformers improve the point cloud processing\nefÔ¨Åciency by improving local distributions of points. We show local\ndistributions of a point cloud without and with transformers. The standard\ndeviation of the transformed point cloud is smaller, enhancing the local\nneighborhood grouping (e.g. when using k-NN for afÔ¨Ånity matrices, more\nbalanced point distributions make feature learning in each neighborhood\nsuffer less variations and outliers) and feature learning efÔ¨Åciency.\nprocessing networks. They can dynamically alter local point\nneighborhoods for better feature learning.\nWe study one linear (afÔ¨Åne) transformer and two non-\nlinear (projective and deformable) transformers. We bench-\nmark them on point-based [22], [23] and sampling-based [29]\npoint cloud networks and on three large-scale 3D point cloud\nprocessing tasks (part segmentation, semantic segmentation\nand object detection). Our spatial transformers outperform\nthe Ô¨Åx graph counterpart for state-of-the-art methods.\nThere are some limitations of our spatial transformers.\nFirst, there are not many constraints on deformable spatial\ntransformers to capture the geometry of the 3D point\nclouds. More complex non-linear spatial transformers may\nfurther improve the performance. On the other hand, spatial\ntransformers learn global transformations of 3D point clouds\nfor altering local neighborhoods. It is unclear if combining\nboth global and local transformations [40], [38] would further\nimprove the learning capacity and task performance.\nAcknowledgements. This research was supported, in\npart, by Berkeley Deep Drive and DARPA. The authors\nthank Utkarsh Singhal and Daniel Zeng for proofreading,\nand anonymous reviewers for their insightful comments.\nREFERENCES\n[1] C.-H. Lin, Y. Chung, B.-Y. Chou, H.-Y. Chen, and C.-Y. Tsai, ‚ÄúA\nnovel campus navigation app with augmented reality and deep\nlearning,‚Äù in Proc. Int. Conf. App. Sys. Invent. , 2018, pp. 1075‚Äì1077.\n[2] J. R. Rambach, A. Tewari, A. Pagani, and D. Stricker, ‚ÄúLearning\nto fuse: A deep learning approach to visual-inertial camera pose\nestimation,‚Äù in Proc. Int. Symp. Mix. & Aug. Real. , 2016, pp. 71‚Äì76.\n[3] S. Tulsiani, S. Gupta, D. Fouhey, A. A. Efros, and J. Malik, ‚ÄúFactoring\nshape, pose, and layout from the 2d image of a 3d scene,‚Äù in Proc.\nConf. Comput. Vis. Pat. Recog., 2018, pp. 302‚Äì310.\n[4] S. Vasu, M. M. MR, and A. Rajagopalan, ‚ÄúOcclusion-aware rolling\nshutter rectiÔ¨Åcation of 3d scenes,‚Äù in Proc. Conf. Comput. Vis. Pat.\nRecog., 2018, pp. 636‚Äì645.\n[5] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nie√üner,\n‚ÄúScancomplete: Large-scale scene completion and semantic segmen-\ntation for 3d scans,‚Äù in Proc. Conf. Comput. Vis. Pat. Recog. , vol. 1,\n2018, p. 2.\n[6] Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, and C. Wang, ‚ÄúLidar-\nvideo driving dataset: Learning driving policies effectively,‚Äù inProc.\nConf. Comput. Vis. Pat. Recog., 2018, pp. 5870‚Äì5878.\n[7] P . Li, T. Qin et al. , ‚ÄúStereo vision-based semantic 3d object and\nego-motion tracking for autonomous driving,‚Äù in Proc. EUR Conf.\nComput. Vis., 2018, pp. 646‚Äì661.\n[8] B. Wu, A. Wan, X. Yue, and K. Keutzer, ‚ÄúSqueezeseg: Convolutional\nneural nets with recurrent crf for real-time road-object segmentation\nfrom 3d lidar point cloud,‚Äù in Proc. Int. Conf. Rob. & Auto., 2018, pp.\n1887‚Äì1893.\n12\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\nA large-scale hierarchical image database,‚Äù in Proc. Conf. Comput.\nVis. Pat. Recog., 2009, pp. 248‚Äì255.\n[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classi-\nÔ¨Åcation with deep convolutional neural networks,‚Äù in Proc. Conf.\nNeruIPS, 2012, pp. 1097‚Äì1105.\n[11] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,\n‚Äú3d shapenets: A deep representation for volumetric shapes,‚Äù in\nProc. Conf. Comput. Vis. Pat. Recog., 2015, pp. 1912‚Äì1920.\n[12] G. Riegler, A. O. Ulusoy, and A. Geiger, ‚ÄúOctnet: Learning deep 3d\nrepresentations at high resolutions,‚Äù in Proc. Conf. Comput. Vis. Pat.\nRecog., vol. 3, 2017.\n[13] P .-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, ‚ÄúO-cnn:\nOctree-based convolutional neural networks for 3d shape analysis,‚Äù\nACM Transactions on Graphics, vol. 36, no. 4, p. 72, 2017.\n[14] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, ‚ÄúMulti-view\nconvolutional neural networks for 3d shape recognition,‚Äù in Proc.\nInt. Conf. Comput. Vis., 2015, pp. 945‚Äì953.\n[15] C. R. Qi, H. Su, M. Nie√üner, A. Dai, M. Yan, and L. J. Guibas,\n‚ÄúVolumetric and multi-view cnns for object classiÔ¨Åcation on 3d\ndata,‚Äù in Proc. Conf. Comput. Vis. Pat. Recog., 2016, pp. 5648‚Äì5656.\n[16] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri, ‚Äú3d shape\nsegmentation with projective convolutional networks,‚Äù in Proc.\nConf. Comput. Vis. Pat. Recog., vol. 1, no. 2, 2017, p. 8.\n[17] L. Zhou, S. Zhu, Z. Luo, T. Shen, R. Zhang, M. Zhen, T. Fang,\nand L. Quan, ‚ÄúLearning and matching multi-view descriptors for\nregistration of point clouds,‚Äù in Proc. EUR Conf. Comput. Vis., 2018,\npp. 505‚Äì522.\n[18] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, ‚ÄúPointnet: Deep learning\non point sets for 3d classiÔ¨Åcation and segmentation,‚Äù Proc. Conf.\nComput. Vis. Pat. Recog., vol. 1, no. 2, p. 4, 2017.\n[19] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, ‚ÄúPointnet++: Deep\nhierarchical feature learning on point sets in a metric space,‚Äù in\nProc. Conf. NeurIPS, 2017, pp. 5099‚Äì5108.\n[20] D. Rethage, J. Wald, J. Sturm, N. Navab, and F. Tombari, ‚ÄúFully-\nconvolutional point networks for large-scale point clouds,‚Äù in Proc.\nEUR Conf. Comput. Vis., 2018, pp. 596‚Äì611.\n[21] M. Gadelha, R. Wang, and S. Maji, ‚ÄúMultiresolution tree networks\nfor 3d point cloud processing,‚Äù in Proc. EUR Conf. Comput. Vis. ,\n2018, pp. 103‚Äì118.\n[22] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, ‚ÄúDynamic graph cnn for learning on point clouds,‚Äù ACM\nTransactions On Graphics, vol. 38, no. 5, pp. 1‚Äì12, 2019.\n[23] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, ‚ÄúPointcnn:\nConvolution on x-transformed points,‚Äù in Proc. Conf. NeurIPS, 2018,\npp. 820‚Äì830.\n[24] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao, ‚ÄúGvcnn: Group-view\nconvolutional neural networks for 3d shape recognition,‚Äù in Proc.\nConf. Comput. Vis. Pat. Recog., 2018, pp. 264‚Äì272.\n[25] Z. Han, M. Shang, Z. Liu, C.-M. Vong, Y.-S. Liu, M. Zwicker, J. Han,\nand C. P . Chen, ‚ÄúSeqviews2seqlabels: Learning 3d global features\nvia aggregating sequential views by rnn with attention,‚Äù IEEE\nTransactions on Image Processing, vol. 28, no. 2, pp. 658‚Äì672, 2019.\n[26] D. Maturana and S. Scherer, ‚ÄúVoxnet: A 3d convolutional neural\nnetwork for real-time object recognition,‚Äù in Proc. Int. Conf. Intell.\nRob. Sys., 2015, pp. 922‚Äì928.\n[27] M. Tatarchenko, A. Dosovitskiy, and T. Brox, ‚ÄúOctree generating\nnetworks: EfÔ¨Åcient convolutional architectures for high-resolution\n3d outputs,‚Äù in Proc. Int. Conf. Comput. Vis., 2017, pp. 2088‚Äì2096.\n[28] R. Klokov and V . Lempitsky, ‚ÄúEscape from cells: Deep kd-networks\nfor the recognition of 3d point cloud models,‚Äù in Proc. Int. Conf.\nComput. Vis., 2017, pp. 863‚Äì872.\n[29] H. Su, V . Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang,\nand J. Kautz, ‚ÄúSplatnet: Sparse lattice networks for point cloud\nprocessing,‚Äù in Proc. Conf. Comput. Vis. Pat. Recog., 2018, pp. 2530‚Äì\n2539.\n[30] A. Adams, J. Baek, and M. A. Davis, ‚ÄúFast high-dimensional\nÔ¨Åltering using the permutohedral lattice,‚Äù in Computer Graphics\nForum, vol. 29, no. 2, 2010, pp. 753‚Äì762.\n[31] V . Jampani, M. Kiefel, and P . V . Gehler, ‚ÄúLearning sparse high\ndimensional Ô¨Ålters: Image Ô¨Åltering, dense crfs and bilateral neural\nnetworks,‚Äù in Proc. Conf. Comput. Vis. Pat. Recog., 2016, pp. 4452‚Äì\n4461.\n[32] L. Landrieu and M. Simonovsky, ‚ÄúLarge-scale point cloud semantic\nsegmentation with superpoint graphs,‚Äù in Proc. Conf. Comput. Vis.\nPat. Recog., 2018, pp. 4558‚Äì4567.\n[33] S. Wang, S. Suo, W.-C. Ma, A. Pokrovsky, and R. Urtasun, ‚ÄúDeep\nparametric continuous convolutional neural networks,‚Äù in Proc.\nConf. Comput. Vis. Pat. Recog., 2018, pp. 2589‚Äì2597.\n[34] M. Tatarchenko, J. Park, V . Koltun, and Q.-Y. Zhou, ‚ÄúTangent\nconvolutions for dense prediction in 3d,‚Äù in Proc. Conf. Comput. Vis.\nPat. Recog., 2018, pp. 3887‚Äì3896.\n[35] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia, ‚ÄúPointweb: Enhancing local\nneighborhood features for point cloud processing,‚Äù in Proc. Conf.\nComput. Vis. Pat. Recog., 2019, pp. 5565‚Äì5573.\n[36] J. Li, B. M. Chen, and G. Hee Lee, ‚ÄúSo-net: Self-organizing network\nfor point cloud analysis,‚Äù in Proc. Conf. Comput. Vis. Pat. Recog. ,\n2018, pp. 9397‚Äì9406.\n[37] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n‚ÄúSpatial transformer networks,‚Äù in Proc. Conf. NeruIPS , 2015, pp.\n2017‚Äì2025.\n[38] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette,\nand L. J. Guibas, ‚ÄúKpconv: Flexible and deformable convolution for\npoint clouds,‚Äù in Proc. Int. Conf. Comput. Vis., 2019, pp. 6411‚Äì6420.\n[39] Y. Zhou and O. Tuzel, ‚ÄúVoxelnet: End-to-end learning for point\ncloud based 3d object detection,‚Äù in Proc. Conf. Comput. Vis. Pat.\nRecog., 2018, pp. 4490‚Äì4499.\n[40] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n‚ÄúDeformable convolutional networks,‚Äù in Proc. Int. Conf. Comput.\nVis., 2017, pp. 764‚Äì773.\n[41] A. X. Chang, T. Funkhouser, L. Guibas, P . Hanrahan, Q. Huang,\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su et al., ‚ÄúShapenet: An\ninformation-rich 3d model repository,‚Äù arXiv:1512.03012, 2015.\n[42] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer,\nand S. Savarese, ‚Äú3d semantic parsing of large-scale indoor spaces,‚Äù\nin Proc. Conf. Comput. Vis. Pat. Recog., 2016, pp. 1534‚Äì1543.\n[43] A. Geiger, P . Lenz, and R. Urtasun, ‚ÄúAre we ready for autonomous\ndriving? the kitti vision benchmark suite,‚Äù in Proc. Conf. Comput.\nVis. Pat. Recog., 2012, pp. 3354‚Äì3361.\n[44] Q. Huang, ‚ÄúVoxelnet: End-to-end learning for point cloud based 3d\nobject detection - code,‚Äù https://github.com/qianguih/voxelnet,\n2019.\nJiayun Wang is currently pursing the Ph.D. de-\ngree in Vision Science from University of Cali-\nfornia, Berkeley. He is also a graduate student\nresearcher at the International Computer Science\nInstitute. He received his B.E. degree in Elec-\ntronic Engineering from Xi‚Äôan Jiaotong University,\nChina. He is interested in self-supervised meth-\nods for understanding and modeling 3D vision.\nRudrasis Chakraborty received his doctorate\nfrom the University of Florida, where he studied\ncomputer and information science and engineer-\ning. He is a member of computer vision group\nat International Computer Science Institute. He\nis also a member of Berkeley Deep Drive at UC\nBerkeley. His research interest includes statistics,\ndifferential geometry and deep learning.\nStella X. Yu received her Ph.D. from Carnegie\nMellon University, where she studied robotics at\nthe Robotics Institute and vision science at the\nCenter for the Neural Basis of Cognition. She\ncontinued her computer vision research as a\npostdoctoral fellow at UC Berkeley, and then as\na Clare Booth Luce Professor at Boston College,\nduring which she received an NSF CAREER\naward. Dr. Yu is currently the Director of Vision\nGroup at the International Computer Science\nInstitute (ICSI), a Senior Fellow at the Berkeley\nInstitute for Data Science, and on the faculty of Computer Science,\nVision Science, Cognitive and Brain Sciences at UC Berkeley. She is\nalso afÔ¨Åliated faculty with the Department of Computer and Information\nScience at the University of Pennsylvania. Dr. Yu is interested not only\nin understanding visual perception from multiple perspectives, but also\nin using computer vision and machine learning to automate and exceed\nhuman expertise in practical applications.",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.8585678339004517
    },
    {
      "name": "Computer science",
      "score": 0.6644420623779297
    },
    {
      "name": "Segmentation",
      "score": 0.6528825163841248
    },
    {
      "name": "Affine transformation",
      "score": 0.6256306767463684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5946006774902344
    },
    {
      "name": "Transformer",
      "score": 0.49922823905944824
    },
    {
      "name": "Computer vision",
      "score": 0.4266264736652374
    },
    {
      "name": "Artificial neural network",
      "score": 0.4255959093570709
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4219260513782501
    },
    {
      "name": "Algorithm",
      "score": 0.3412304222583771
    },
    {
      "name": "Mathematics",
      "score": 0.1990201771259308
    },
    {
      "name": "Geometry",
      "score": 0.1576557755470276
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1297971548",
      "name": "International Computer Science Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 31
}