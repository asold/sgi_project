{
  "title": "Lessons on Parameter Sharing across Layers in Transformers",
  "url": "https://openalex.org/W3152530299",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2740354314",
      "name": "Takase, Sho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281574072",
      "name": "Kiyono, Shun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3167739156",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W3102816807",
    "https://openalex.org/W2965046076",
    "https://openalex.org/W2903810591",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W2963928591",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3118469444",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2325237720"
  ],
  "abstract": "We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.",
  "full_text": "Lessons on Parameter Sharing across Layers in Transformers\nSho Takase∗ Shun Kiyono\nLINE Corporation\n{sho.takase, shun.kiyono}@linecorp.com\nAbstract\nWe propose a novel parameter sharing method\nfor Transformers (Vaswani et al., 2017). The\nproposed approach relaxes a widely used tech-\nnique, which shares the parameters of one layer\nwith all layers such as Universal Transform-\ners (Dehghani et al., 2019), to improve the\nefficiency. We propose three strategies: SE-\nQUENCE , CYCLE , and CYCLE (REV ) to assign\nparameters to each layer. Experimental results\nshow that the proposed strategies are efficient\nin terms of the parameter size and computa-\ntional time in the machine translation task. We\nalso demonstrate that the proposed strategies\nare effective in the configuration where we use\nmany training data such as the recent WMT\ncompetition. Moreover, we indicate that the\nproposed strategies are also more efficient than\nthe previous approach (Dehghani et al., 2019)\non automatic speech recognition and language\nmodeling tasks.\n1 Introduction\nTransformer-based methods have achieved notable\nperformance in various NLP tasks (Vaswani et al.,\n2017; Devlin et al., 2019; Brown et al., 2020). In\nparticular, Brown et al. (2020) indicated that the\nlarger parameter size we prepare, the better perfor-\nmance the model achieves. However, the model\nwhich is composed of many parameters occupies\na large part of a GPU memory capacity. Thus, it\nis important to explore a parameter efficient way,\nwhich achieves better performance than a basic\nmodel with the same parameter size.\nParameter sharing is a widely used technique as\na parameter efficient way (Dehghani et al., 2019;\nDabre and Fujita, 2019; Lan et al., 2020). De-\nhghani et al. (2019) proposed Universal Trans-\nformer which consists of parameters for only one\nlayer of a Transformer-based encoder-decoder, and\nuses these parameters N times for an N-layered\n∗ A part of this work was done when the author was at\nTokyo Institute of Technology.\nInput1st layer2nd layerShare\nM=3, N=6の場合\n3rdlayer4thlayer5thlayer6thlayer\nShare\nShare\nSEQUENCEInput1st layer2nd layerShare\n3rdlayer4thlayer5thlayer6thlayer\nShare\nShare\nCYCLEInput1st layer2nd layerShare\n3rdlayer4thlayer5thlayer6thlayer\nShareShare\nCYCLE (REV)\nFigure 1: Examples of three parameter assignment\nstrategies proposed in this study when we set M = 3\nand N = 6.\nencoder-decoder. Dabre and Fujita (2019) and\nLan et al. (2020) also used such parameter shar-\ning across layers for their Transformers.\nDehghani et al. (2019) reported that Universal\nTransformer achieved better performance than the\nvanilla Transformer in machine translation if the\nparameter sizes of both models are (almost) the\nsame. However, when we prepare the same num-\nber of parameters for Universal Transformer and\nvanilla Transformer, the dimension sizes of each\nlayer in Universal Transformer are much larger\nthan ones in the vanilla Transformer. Thus, Univer-\nsal Transformer requires much more computational\ntime since its weight matrices are larger. For exam-\nple, Universal Transformer requires twice as much\ntraining time as the vanilla Transformer in WMT\nEnglish-to-German dataset, which is a widely used\nmachine translation dataset (see Table 1).\nIn this paper, we propose a new parameter shar-\ning method that is faster than using the same param-\neters for all layers such as Universal Transformers.\nUniversal Transformers raise their expressiveness\npower by increasing the size of weight matrices\nfor each layer. On the other hand, stacking (more)\nlayers is another promising approach to raise ex-\npressiveness power of neural methods (He et al.,\n2016). Thus, the most straight-forward way to\narXiv:2104.06022v4  [cs.CL]  2 Jun 2023\nmake Universal Transformers faster is stacking lay-\ners with smaller weight matrices for each layer.\nHowever, the approach using the same parameters\nfor all layers limits the improvement of stacking\nlayers (Dabre and Fujita, 2019). Therefore, in-\nstead of preparing parameters for only one layer,\nwe prepare parameters for M layers to construct an\nN-layered encoder-decoder, where 1 ≤ M ≤ N.\nIn other words, the proposed method relaxes the\nparameter sharing strategy in previous studies (De-\nhghani et al., 2019; Dabre and Fujita, 2019; Lan\net al., 2020). Because this relaxation addresses the\nabove limitation of improvement by stacking lay-\ners, the proposed method can be fast by stacking\nlayers with using small weight matrices for each\nlayer. For the actual parameter assignment strate-\ngies, we provide several simple examples (Figure 1)\nand investigate their performance empirically. The\nmain focus of this study is to demonstrate that such\nsimple strategies can be a better alternative to the\nexisting parameter sharing strategy used in Univer-\nsal Transformers.\nWe mainly conduct experiments on machine\ntranslation datasets. Experimental results show that\nthe proposed method achieves slightly better scores\nto the previous method, that assigns parameters of\none layer to all layers, with smaller computational\ntime. In addition, we indicate that the proposed\nmethod outperforms the previous parameter shar-\ning method when we spend almost the same train-\ning time. Moreover, we conduct experiments on au-\ntomatic speech recognition and language modeling\ntasks (Section 4 and Appendix A). Experimental re-\nsults on these tasks also indicate that the proposed\nmethod are also efficient in these situations.\n2 Proposed Method\nAs described in Section 1, we use parameters\nfor M layers in the construction of an N-layered\nTransformer-based encoder-decoder. We provide\nthree examples for the parameter assignment: SE-\nQUENCE , CYCLE , and CYCLE (REV ). This section\ndescribes these parameter assignment strategies.\nFigure 1 shows examples of three parameter as-\nsignment strategies for an encoder side when we\nset M = 3and N = 6. Let enci be the i-th layer\nof an encoder. Figure 2 describes the algorithm to\nassign each parameter to each layer of the encoder.\nFor the decoder side, we assign each parameter\nwith the same manner.\nAlgorithm Encoder Construction\nInput: the total number of layers N, number of\nindependent layers M, sharing strategy TYPE\n∈ {SEQUENCE , CYCLE , CYCLE (REV )}\nOutput: enc1, ..., encN\n1: for i in [1, ..., N] do\n2: if i == 1 then\n3: enci ← CreateNewLayer\n4: else if TYPE == SEQUENCE then\n5: if (i − 1) mod⌊N/M⌋ == 0then\n6: enci ← CreateNewLayer\n7: else\n8: enci ← enci−1\n9: else if TYPE == CYCLE then\n10: if i ≤ M then\n11: enci ← CreateNewLayer\n12: else\n13: enci ← enc((i−1) modM)+1\n14: else if TYPE == CYCLE (REV ) then\n15: if i ≤ M then\n16: enci ← CreateNewLayer\n17: else if i ≤ (M × (⌈N/M⌉ −1)) then\n18: enci ← enc((i−1) modM)+1\n19: else\n20: enci ← encM−((i−1) modM)\nFigure 2: Proposed parameter assignment strategies for\nencoder construction. CreateNewLayer is a function\nthat creates a new encoder layer.\n2.1 SEQUENCE\nThe simplest strategy is to assign the same param-\neters to sequential ⌊N/M⌋ layers. We name this\nstrategy SEQUENCE . For example, when we set\nM = 3 and N = 6, two sequential layers share\ntheir parameters as illustrated in Figure 1.\n2.2 CYCLE\nIn CYCLE , we stack M layers whose parameters\nare independent from each other. Then, we repeat\nstacking the M layers with the identical order to\nthe first M layers until the total number of layers\nreaches N. When we set M = 3and N = 6, we\nstack 3 layers twice as illustrated in Figure 1.\n2.3 CYCLE (REV )\nLiu et al. (2020) and Takase et al. (2022) reported\nthat higher decoder layers tends to obtain larger\ngradient norms1. Their report implies that higher\nlayers require more degrees of freedom than lower\nlayers for their expressiveness. In other words,\nlower layers probably have redundant parameters\ncompared to higher layers. Thus, we propose the\nCYCLE (REV ) strategy reusing parameters of lower\nlayers in higher layers.\nIn this strategy, we repeat stacking M layers in\nthe same manner as CYCLE until M ∗(⌈N/M⌉− 1)\nlayers. For the remaining layers, we stackM layers\nin the reverse order. When we set M = 3 and\nN = 6, we stack 3 layers and then stack the 3\nlayers in the reverse order as in Figure 1. Thus, the\nlowest layer and highest layer share parameters.\n3 Experiments on Machine Translation\nWe investigate the efficiency of the proposed pa-\nrameter sharing strategies. In detail, we indicate\nthat our proposed strategies are faster than Uni-\nversal Transformers while achieving comparable\n(or better) performance when we use the same pa-\nrameter size. In this section, we conduct experi-\nments on machine translation datasets. First, we\nfocus on the English-to-German translation task\nbecause this task is widely used in the previous\nstudies (Vaswani et al., 2017; Ott et al., 2018; De-\nhghani et al., 2019; Kiyono et al., 2020). We con-\nduct comparisons based on following aspects: (i)\ncomparison with Universal Transformers in terms\nof efficiency and (ii) comparison with models with-\nout parameter sharing across layers to investigate\nwhether our proposed strategies can achieve com-\nparable (or better) performance to the models with\nlarger memory footprint.\nIn addition to the widely used training data, we\nconduct experiments on a large amount of train-\ning dataset in the English-to-German translation\ntask. Then, we investigate if our findings are con-\nsistent in other language direction (i.e., German-\nto-English) and other language pair (i.e., English-\nto-French and French-to-English). We describe\ndetails in the following subsections.\n3.1 Standard Setting\n3.1.1 Datasets\nWe used the WMT 2016 training dataset, which\nis widely used in previous studies (Vaswani et al.,\n1In particular, this property is observed during warm-up\nwhen we use the post layer normalization (Post-LN) setting,\nwhich is originally used in Vaswani et al. (2017) and widely\nused in machine translation.\n2017; Ott et al., 2018; Takase and Kiyono, 2021).\nThis dataset contains 4.5M English-German sen-\ntence pairs. Following previous studies, we con-\nstructed a vocabulary set with BPE (Sennrich et al.,\n2016b) in the same manner. We set the number of\nBPE merge operations at 32K and shared the vocab-\nulary between the source and target languages. We\nmeasured case-sensitive detokenized BLEU with\nSacreBLEU (Post, 2018)2.\n3.1.2 Methods\nFor the proposed parameter assignment strategies,\nwe fixed M = 6 and set N = 12, 18 based on\nthe Vanilla configuration below. We compare the\nproposed strategies with the following baselines.\nVanilla: This is the original Transformer (base)\nsetting in (Vaswani et al., 2017). To stabilize the\ntraining, we applied Admin (Liu et al., 2020). See\nSection 5 for more details of Admin.\nUniversal: As the parameter sharing strategy\nin previous studies such as Universal Transform-\ners (Dehghani et al., 2019), we set M = 13. In\nthis setting, we increased the dimensions of each\nlayer for a fair comparison in terms of the num-\nber of parameters. This configuration corresponds\nto the Universal Transformer base setting in (De-\nhghani et al., 2019). Moreover, we prepared the\nmodel using twice as many layers to investigate the\neffect of stacking many layers in Universal Trans-\nformers. We call this setting Universal (deep). In\naddition, we prepared Universal (small) whose\ndimension sizes are the identical to ones of Trans-\nformer (base).\nFurthermore, we prepare two models that consist\nof a large number of parameters for reference.\nVanilla (big): This is the original Transformer (big)\nsetting in (Vaswani et al., 2017).\nVanilla (deep): We stacked layers until N = 18in\nthe Vanilla configuration.\n2The BLEU score computed by SacreBLEU is often lower\nthan the score obtained by the procedure of Vaswani et al.\n(2017) as reported in Ott et al. (2018). In fact, when we used\nthe same procedure as Vaswani et al. (2017), SEQUENCE of\nM = 6, N= 12 in Table 1 achieved 29.40 in the averaged\nBLEU score in newstest2014 and the best model in Table 2\nachieved 35.14 in the averaged BLEU score in newstest2014.\nHowever, since Post (2018) encouraged using SacreBLEU for\nthe compatibility of WMT results, we used SacreBLEU.\n3The original Universal Transformers (Dehghani et al.,\n2019) use the sinusoidal positional encoding for each layer\nand adaptive computation time technique (Graves, 2017) but\nwe omitted them in this study to focus on the difference among\nparameter sharing strategies.\nMethod M N #Params Speed 2010 2011 2012 2013 2014 2015 2016 Avg.\nVanilla 6 6 61M ×2.02 24.14 21.93 22.25 26.14 27.05 29.59 34.23 26.48\nUniversal 1 6 63M ×1.00 24.37 22.33 22.70 26.40 27.65 30.24 34.60 26.90\nUniversal (deep) 1 12 63M ×0.52 24.42 22.30 22.61 26.52 27.76 29.75 34.01 26.77\nUniversal (small) 1 6 24M ×2.52 22.89 21.11 21.29 24.75 24.71 28.16 32.81 25.10\nSEQUENCE 6 12 61M ×1.31 24.65 22.32 22.83 26.98 27.88 30.27 34.99 27.13\nCYCLE 6 12 61M ×1.31 24.51 22.43 22.69 26.61 27.91 30.37 34.77 27.04\nCYCLE (REV ) 6 12 61M ×1.31 24.66 22.47 22.87 26.68 27.72 30.37 34.81 27.08\nSEQUENCE 6 18 61M ×0.98 24.53 22.44 22.73 26.59 27.73 30.30 34.80 27.02\nCYCLE 6 18 61M ×0.98 24.74 22.60 23.04 26.89 28.14 30.54 34.79 27.25\nCYCLE (REV ) 6 18 61M ×0.98 24.93 22.77 23.09 26.88 28.09 30.60 34.84 27.31\nMethods consisting of a large number of parameters for reference\nVanilla (big) 6 6 210M ×0.81 24.31 22.21 22.75 26.39 28.28 30.35 33.40 26.81\nVanilla (deep) 18 18 149M ×0.96 24.54 22.30 22.75 26.57 28.03 30.24 34.19 26.94\nTable 1: The number of layers, number of parameters, computational speeds based on the Universal configuration,\nBLEU scores on newstest2010-2016, and averaged scores when we trained each method on widely used WMT 2016\nEnglish-to-German training dataset. Scores in bold denote the best results for each set. The results of our proposed\nstrategies are statistically significant (p <0.05) in comparison with Universal. The lowest part indicates results of\nmethods consisting of a large number of parameters for reference.\n3.1.3 Results\nTable 1 shows BLEU scores on newstest2010-2016\nfor each method. We trained three models with\ndifferent random seeds, and reported the averaged\nscores. Table 1 also shows the total number of\nparameters and computational speeds4. The com-\nputational speed is based on the speed of Universal.\n(i) Comparison with Universal in terms of effi-\nciency In the comparison between Universal and\nVanilla, Universal achieved better scores although\ntheir parameter sizes are almost the same. This\nresult is consistent with the report in (Dehghani\net al., 2019). However, the training time of Uni-\nversal is more than twice as much as the one of\nVanilla. In addition, Universal (deep) didn’t im-\nprove the performance from Universal, and thus\nstacking many layers have small effect on BLEU\nscores when the model shares parameters of one\nlayer with all layers.\nIn contrast, the proposed strategies (SEQUENCE ,\nCYCLE , and CYCLE (REV )) were faster and\nachieved slightly better scores than Universal when\nwe set M = 6and N = 12. Thus, our proposed\nparameter sharing strategies are more efficient than\nUniversal in terms of the parameter size and com-\nputational time.\nIn comparison among Universal (small) and the\nproposed strategies, Universal (small) was faster5\n4We regard processed tokens per second during the training\nas the computational speed.\n5We used the same dimension sizes for Vanilla and Uni-\nversal (small) but their training speeds are different from each\nother. Since Universal (small) consists of small parameters,\nthe computational time for updating is smaller than Vanilla.\nbut the configuration drastically sacrificed BLEU\nscores. These results imply that the strategy in\nUniversal Transformer, which shares parameters of\none layer with all layers, damages computational\ntime or the quality of output sequences. In com-\nparison with those Universal configurations, our\nproposed strategies improved both of the computa-\ntional speed and BLEU scores.\nFigure 3 illustrates the negative log-likelihood\n(NLL) values on newstest2013 for each training\nstep. In this figure, we used M = 6and N = 12\nfor our proposed strategies. This figure shows that\nUniversal achieved better NLL values in the be-\nginning of the training but the proposed strate-\ngies outperformed others when the training step\nis larger than 15,000. When we have finished train-\ning, the proposed strategies achieved better NLL\nvalues than Universal (and Vanilla). This result\nalso indicates that the proposed strategies achieved\nbetter performance. We emphasize that the pro-\nposed strategies reached this better performance\nwith small computational time in comparison with\nUniversal because the proposed strategies are faster\nas in Table 1.\n(ii) Comparison with models without parameter\nsharing across layers The lowest part of Table\n1 indicates results when we prepared more param-\neters. We trained these models to investigate the\nperformance of models without parameter sharing\nacross layers. In other words, the purpose of these\nsettings are comparison with models using larger\nmemory footprint. As shown in Table 1, the pro-\nposed strategies achieved better performance than\n0 10000 20000 30000 40000 50000\nThe number of updates\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5Valid loss (NLL)\nVanilla\nUniversal\nSequence\nCycle\nCycle (Rev)\nFigure 3: Negative log-likelihood (NLL) of each method\non newstest2013. For our proposed parameter sharing\nstrategies, we used M = 6and N = 12.\nmodels consisting of a large number of parame-\nters in the averaged BLEU scores of newstest2010-\n2016. This result implies that the proposed parame-\nter sharing strategies are not only efficient but also\neffective in constructing better encoder-decoder\nmodels.\n3.2 High Resource Setting\n3.2.1 Datasets\nIn the high resource setting, we constructed 44.2M\ntranslation sentence pairs as a training dataset with\nthe procedures of (Kiyono et al., 2020) which\nachieved the best result in the WMT 2020 news\ntranslation task. In addition, we augmented the\ntraining data by using the back-translation tech-\nnique (Sennrich et al., 2016a) in the same manner\nas (Kiyono et al., 2020). We obtained 284.3M\npairs as synthetic training data. For evaluation,\nwe add newstest2018 and 2019 to the set used in\nSection 3.1 to because (Kiyono et al., 2020) used\nthese two test sets. In the same as Section 3.1, we\nmeasured case-sensitive detokenized BLEU with\nSacreBLEU.\n3.2.2 Methods\nWe used the original Transformer (big) set-\nting (Vaswani et al., 2017) as our baseline in using\ngenuine training data. We call this setting Vanilla\nin this experiment. Moreover, we also prepared\nUniversal, which shares the parameters with all\nlayers, namely, M = 1, N= 6. We increased the\ndimensions of each layer in Universal to make their\nparameter size almost the same as others. For the\nproposed strategies, we used M = 6and N = 12.\nIn using both of the genuine and synthetic (back-\ntranslated) datasets, we applied CYCLE (REV ) to\nthe BASE setting in (Kiyono et al., 2020) because\nCYCLE (REV ) achieved the best BLEU scores on\nmost test sets in Table 1. We also used M = 6\nand N = 12in this configuration. We compare the\nreported scores of the best model in (Kiyono et al.,\n2020). Their model is composed of 9 layers (i.e.,\nM = 9and N = 9); thus, it contains considerably\nmore parameters than ours.\n3.2.3 Results\nTable 2 shows BLEU scores of each method on\neach test set. Similar to the experiments in Section\n3.1, we reported the averaged scores of three mod-\nels trained with different random seeds. Table 2\nalso shows the total number of parameters6.\nTable 2 shows that the proposed strategies\nachieved better BLEU scores than Vanilla and Uni-\nversal when we prepared almost the same number\nof parameters. This result indicates that the pro-\nposed strategies are also parameter efficient in the\nhigh resource setting. In addition, since we used\nM = 6and N = 12for proposed strategies, they\nare also more efficient than Universal in terms of\ncomputational time (see Table 1).\nWhen we used additional synthetic data for train-\ning in the same manner as (Kiyono et al., 2020),\nCYCLE (REV ) achieved comparable BLEU scores\nto the best system of (Kiyono et al., 2020) except\nfor newstest20197 even though the parameter size\nof CYCLE (REV ) was smaller than theirs. This re-\nsult indicates that CYCLE (REV ) is also efficient in\nthe construction of models for recent competitive\ntasks. In addition, this result implies that our pro-\nposed strategies can be used in the configuration\nwhere we train many parameters with a tremendous\namount of data such as recent pre-trained language\nmodels, e.g., GPT series (Brown et al., 2020). We\ninvestigate the effect of the proposed strategies on\nlanguage models in Appendix A.\n3.3 Other Direction and Language Pair\n3.3.1 Datasets\nWe conduct experiments on the other direction and\nlanguage pair. For the German-to-English training\ndataset, we used the identical data in Section 3.1.\nFor English-to-French and French-to-English, we\n6The parameter sizes of Vanilla (big) in Table 1 and Vanilla\nin Table 2 are different from each other due to the difference\nof sharing embeddings. Following (Kiyono et al., 2020), we\ndid not share embeddings in the high resource setting.\n7For newstest2019, synthetic data might harm the quality\nof a model because models trained with only genuine data\noutperformed those trained with both data.\nMethod #Params 2010 2011 2012 2013 2014 2015 2016 2018 2019 Avg.\nGenuine training data\nVanilla 242M 26.53 24.09 24.51 28.51 31.40 33.52 39.08 47.11 42.80 33.06\nUniversal 249M 27.00 24.20 24.96 28.94 31.73 33.53 39.38 47.54 43.11 33.38\nSEQUENCE 242M 27.31 24.24 24.86 29.15 31.90 33.84 39.93 48.15 43.12 33.61\nCYCLE 242M 27.23 24.45 25.13 29.12 32.10 34.04 39.82 48.11 43.19 33.69\nCYCLE (REV ) 242M 27.37 24.46 25.14 29.16 32.06 33.98 40.28 48.34 43.43 33.80\n+ Synthetic (back-translated) data\nKiyono et al. (2020) 514M - - - - 33.1 - - 49.6 42.7 -\nCYCLE (REV ) 343M 28.29 24.99 25.98 30.01 33.54 34.93 41.37 49.55 42.18 34.54\nTable 2: BLEU scores on newstest2010-2016, 2018, and 2019. We add newstest2018 and 2019 to the set in the\nstandard setting to compare the top system on WMT 2020 (Kiyono et al., 2020).\nGerman-to-English English-to-French French-to-English\nMethod M N 2013 2014 2013 2014 2013 2014\nVanilla 6 6 30.48 30.96 33.41 38.41 33.48 36.06\nUniversal 1 6 31.06 31.32 33.58 38.84 33.83 37.11\nSEQUENCE 6 18 31.31 31.97 34.49 40.18 34.26 37.45\nCYCLE 6 18 31.46 32.18 34.50 40.17 33.97 37.59\nCYCLE (REV ) 6 18 31.32 32.12 34.67 40.13 34.16 37.32\nTable 3: The number of layers and BLEU scores on each dataset. Each method is composed of almost the same\nnumber of parameters.\nused the WMT 2014 training dataset. We applied\nthe same pre-processing as in (Ott et al., 2018), and\nused 35.8M English-French sentence pairs. Each\nconfiguration, we used newstest2013 and new-\nstest2014 as valid and test sets, respectively. We\nalso measured case-sensitive detokenized BLEU\nwith SacreBLEU in these experiments.\n3.3.2 Methods\nWe compare our proposed strategies with baselines\nused in Section 3.1. We used the Transformer\n(base) setting with Admin as Vanilla and prepared\nUniversal which is M = 1, N= 6 with large\ndimension sizes for each internal layer. For the pro-\nposed strategies, we used M = 6and N = 18. In\nthese configurations, the training time of proposed\nstrategies are almost the same as one of Universal\nas described in Table 1.\n3.3.3 Results\nTable 3 shows BLEU scores of each method on\neach dataset. This table indicates that Universal\noutperformed Vanilla in all datasets. The proposed\nparameter sharing strategies (SEQUENCE , CYCLE ,\nand CYCLE (REV )) achieved better scores than Uni-\nversal in all datasets. These results are consistent\nwith results in Table 1. These results also indicate\nthat the proposed strategies are more efficient than\nUniversal, which shares parameters of one layer\nwith all layers, because they achieved better per-\nformance with almost the same parameter size and\ncomputational time.\nIn the comparison among the proposed strate-\ngies, CYCLE and CYCLE (REV ) outperformed SE-\nQUENCE on German-to-English but it is difficult\nto conclude that CYCLE and CYCLE (REV ) are\nsuperior to SEQUENCE on English-to-French and\nFrench-to-English. This result implies that the best\nstrategy might depend on a language pair8. How-\never, we emphasize that our proposed strategies out-\nperformed Universal. For applying our proposed\nparameter sharing strategies to other datasets, we\nrecommend using SEQUENCE as a first step be-\ncause it is the easiest to implement.\n4 Experiments on Automatic Speech\nRecognition\n4.1 Datasets\nTo investigate the effect of our proposed strate-\ngies on other modality, we conduct comparisons\non the automatic speech recognition (ASR) task.\nWe used the de-facto standard English ASR bench-\nmark dataset: LibriSpeech (Panayotov et al., 2015).\nThe dataset contains 1,000 hours of English speech\nfrom audiobooks. We used the standard splits of\nLibriSpeech; used all available training data for\ntraining and two configurations (clean and other)\nof development and test sets for evaluation. We\n8Section 4 and Appendix A imply that a sort of task and\nTransformer architectures also have an influence on the per-\nformance of proposed strategies.\nEnc Dec Dev Test\nMethod M N M N #Params Speed clean other clean other\nVanilla 6 6 6 6 52M ×2.94 3.98 9.06 4.18 9.18\nUniversal 1 6 1 6 54M ×1.00 3.73 8.85 4.14 8.80\nSEQUENCE 8 16 4 8 50M ×1.41 3.16 7.84 3.32 7.71\nCYCLE 8 16 4 8 50M ×1.41 3.28 7.86 3.57 7.97\nCYCLE (REV ) 8 16 4 8 50M ×1.41 3.11 8.10 3.60 8.11\nTable 4: The parameter sizes, computational speeds based on the Universal configuration, and word error rates of\neach method. For word error rates, lower is better. Scores in bold denote the best results for each set.\napplied the same pre-processing as in (Wang et al.,\n2020). We measured word error rate on each set.\n4.2 Methods\nWe also compare our proposed strategies with base-\nlines in Section 3. As the base architecture, we\nused Transformer based speech-to-text model (T-\nMd) described in (Wang et al., 2020). In contrast\nto the Post-LN architecture, which is the original\nTransformer architecture (Vaswani et al., 2017), the\nTransformer in T-Md consists of the Pre-LN config-\nuration. We prepared 6 layers for the encoder and\ndecoder in Vanilla and Universal. For proposed\nstrategies, we stacked more layers for the encoder\nside in the same as in (Wang et al., 2020). We pre-\npared N = 16 and M = 8 for the encoder side,\nand N = 8and M = 4for the decoder side.\n4.3 Results\nTable 4 shows word error rates of each method\non each dataset. This table indicates that Univer-\nsal outperformed Vanilla in all sets. The proposed\nparameter sharing strategies (SEQUENCE , CYCLE ,\nand CYCLE (REV )) achieved better scores than Uni-\nversal in all sets even though they are faster than\nUniversal. These results are consistent with results\nin machine translation experiments in Section 3.\nThus, the proposed strategies are also more effi-\ncient in the ASR task.\nIn contrast to machine translation experiments,\nSEQUENCE outperformed CYCLE and CYCLE\n(REV ) in the ASR task. We consider that this re-\nsult might be caused by the difference of tasks.\nIn addition, the cause might be the difference of\nlayer normalization positions in the Transformer\narchitecture. We used Post-LN based method (Ad-\nmin) (Liu et al., 2020) in machine translation exper-\niments, but Pre-LN based method in this ASR task.\nLiu et al. (2020) and Takase et al. (2022) demon-\nstrated that the position of the layer normalization\nhas a strong effect on the property of Transform-\ners. The experimental results in language modeling\n(Appendix A) also imply that SEQUENCE is more\nappropriate when we use the Pre-LN based Trans-\nformer. The main focus of this study is empirical\ncomparisons to the widely used parameter sharing\nstrategy, Universal (Dehghani et al., 2019), but we\nwill address theoretical analyses on the training\ndynamics in the future to understand the relation\nbetween parameter sharing strategies and Trans-\nformer architectures.\n5 Related Work\nParameter Sharing In the past decade, various\nstudies reported that a large amount of training data\nimprove the performance in NLP tasks (Suzuki and\nIsozaki, 2008; Brants et al., 2007; Mikolov et al.,\n2013; Sennrich et al., 2016a; Edunov et al., 2018).\nMoreover, recent studies indicated that the larger\nparameter size we prepare, the better performance\nthe model achieves when we have a large amount\nof training data (Devlin et al., 2019; Brown et al.,\n2020). In fact, the best system on the WMT 2020\nnews translation task is composed of about 10 times\nas many parameters as the widely used Transformer\n(base) setting (Kiyono et al., 2020). However, due\nto the limitation on a GPU memory capacity, we\nhave to explore a parameter efficient way, which\nachieves better performance while saving the pa-\nrameter size.\nParameter sharing is a widely used technique as\na parameter efficient way (Dehghani et al., 2019;\nDabre and Fujita, 2019; Xia et al., 2019; Lan et al.,\n2020). Dehghani et al. (2019) proposed Universal\nTransformer. Their method requires parameters\nfor only one layer (i.e., M = 1) of a Transformer-\nbased encoder-decoder, and shares these parame-\nters with N layers. Dabre and Fujita (2019) in-\nvestigated the effectiveness of Transformer sharing\nparameters of one layer across all layers on various\ntranslation datasets. Lan et al. (2020) used this pa-\nrameter sharing strategy to construct a parameter\nefficient model. As reported in these studies, we\ncan achieve better performance by the Transformer\nsharing parameters of one layer across all layers\nwhen we use the same parameter size as the original\nTransformer. However, this strategy requires much\nmore computational time as described in Table 1\nbecause weight matrices for each layer are much\nlarger. To solve this problem, we propose a new\nparameter sharing strategies that prepare parame-\nters for M layers and assign them into N layers,\nwhere 1 ≤ M ≤ N. Experimental results show\nthat our proposed strategies are more efficient than\nthe method sharing parameters of one layer with\nacross layers (Dehghani et al., 2019; Dabre and\nFujita, 2019; Lan et al., 2020). In addition, experi-\nmental results imply that the proposed parameter\nsharing strategies are effective to improve the per-\nformance. In fact, in language modeling, previous\nstudies demonstrated that the parameter sharing is\nuseful to improve the performance (Melis et al.,\n2018; Merity et al., 2018; Takase et al., 2018),\nXia et al. (2019) proposed an encoder-decoder\nwhich shares parameters of the encoder part and de-\ncoder part. Xiao et al. (2019) proposed the method\nto share the attention weights to make the compu-\ntation of Transformers fast. These techniques are\northogonal to our proposed method. Thus, we can\ncombine them to improve the efficiency of parame-\nters and computational time.\nTraining Acceleration In this study, we explore\na parameter efficient method. On the other hand,\nrecent studies proposed method to accelerate the\ntraining. Li et al. (2020) proposed a training strat-\negy for a deep Transformer. Their strategy trains a\nshallow model and then stacks layers to construct a\ndeep model. They repeat this procedure until the de-\nsired deep model. They indicated that their strategy\nwas faster than the training of whole parameters\nof a deep Transformer. Takase and Kiyono (2021)\ncompared regularization methods in terms of train-\ning time. Their experimental results show that the\nsimple regularizations such as word dropout are\nmore efficient than complex ones such as adver-\nsarial perturbations. We can use those findings to\naccelerate the training of our proposed strategies.\nDeep Transformers To raise expressiveness\npower of Transformers, we stack many layers\nin the proposed method. The stability of train-\ning deep Transformers depends on their architec-\ntures (Nguyen and Salazar, 2019; Xiong et al.,\n2020; Liu et al., 2020). Transformer architectures\ncan be categorized into two types based on the\nposition of layer normalizations: Post-LN and Pre-\nLN. Most of recent studies used the Pre-LN set-\nting when they stacked many layers (Wang et al.,\n2019; Brown et al., 2020) because Pre-LN makes\nthe training process more stable than the Post-\nLN setting, which is used in the original Trans-\nformer (Nguyen and Salazar, 2019; Xiong et al.,\n2020). On the other hand, several studies proposed\nmethods to stabilize the training of Post-LN based\nTransformers (Liu et al., 2020; Takase et al., 2022).\nIn this study, we used Admin (Liu et al., 2020) in\nmachine translation experiments because it stabi-\nlizes the training of Post-LN based Transformers\nwhile keeping the advantages of Post-LN in the ma-\nchine translation task. For other experiments, we\nused the Pre-LN configuration based on the imple-\nmentations of baselines. These experiments show\nthat our proposed strategies are effective in major\ntwo architectures: Post-LN and Pre-LN.\n6 Conclusion\nWe proposed three parameter sharing strategies:\nSEQUENCE , CYCLE , and CYCLE (REV ), for the\ninternal layers in Transformers. In contrast to the\nprevious strategy, which prepares parameters for\nonly one layer and shares them across layers such\nas Universal Transformers (Dehghani et al., 2019),\nthe proposed strategies prepare parameters for M\nlayers to construct N layers. The proposed strate-\ngies stack layers whose weight matrices are smaller\nthan ones of Universal Transformers to raise expres-\nsiveness power while saving computational time.\nExperimental results in the standard machine\ntranslation setting show that the proposed strate-\ngies achieved slightly better BLEU scores to those\nof Universal with a small computational time when\nwe prepared almost the same parameters for each\nmethod (M = 6 and N = 12). In addition, the\nproposed strategies outperformed Universal under\nthe same computational budgets ( M = 6 and\nN = 18). Thus, the proposed strategies are ef-\nficient in terms of the parameter size and compu-\ntational time. Through additional experiments, we\nindicated that the proposed strategies are also more\nefficient than Universal in the high resource set-\nting, other language pairs, and another modality\n(speech-to-text).\nLimitations\nAs described in Section 1, the purpose of this study\nis to relax the existing parameter sharing strategy\nwhich shares the parameters of one layer with all\nlayers (Dehghani et al., 2019; Dabre and Fujita,\n2019; Lan et al., 2020). Experimental results in-\ndicate that the proposed simple parameter sharing\nstrategies can be a better alternative to the existing\nmethod. As many studies on neural methods, this\nstudy also depend on empirical observations. In\nother words, this study lacks theoretical justifica-\ntions for proposed parameter sharing strategies.\nWe conducted experiments on various situations.\nWe mainly focused on sequence-to-sequence tasks\nand trained each model from scratch. Our con-\nducted experiments indicated the efficiency of the\nproposed strategies but we did not conduct experi-\nments on the pre-training and then fine-tuning con-\nfiguration such as comparison with BERT (Devlin\net al., 2019) due to the limitation of our computa-\ntional budgets. Thus, it is difficult to claim that the\nproposed strategies are also more efficient in such\nconfiguration. In addition, we have to investigate\nthe effectiveness in a more realistic situation. For\nexample, we will investigate the performance of\nthe combination of our proposed method, which is\nthe parameter efficient way for internal layers, and\na parameter efficient embedding such as Takase\nand Kobayashi (2020).\nThrough experiments in various configurations,\nit is difficult to conclude which strategy is the\nbest. Experimental results imply that the best strat-\negy depends on the task and Transformer architec-\nture (Post-LN or Pre-LN). Such phenomena are\nreported in previous studies (Press et al., 2020; Gu-\nlati et al., 2020). In fact, the architecture explored\nby Press et al. (2020) is better in the language mod-\neling task but ineffective in the machine transla-\ntion task. Since it is intractable to investigate a\ntremendous amount of possible parameter assign-\nment way due to the limitation of computational\nbudgets, there might be a superior way to three sim-\nple strategies proposed in this paper. However, we\nemphasize that all our proposed strategies are more\nefficient than the Universal configuration. Because\nthe purpose of our experiments is not to detect the\nbest parameter sharing strategy but to indicate that\nour proposed parameter sharing strategies are more\nefficient than the Universal configuration, we con-\nsider that our conducted experiments are sufficient\nto verify our claims.\nEthics Statement\nAs discussed in Strubell et al. (2019), recent neural\nmodels require substantial energy consumption. To\naddress this issue, we explore a parameter efficient\nway for Transformers in this study. We believe that\nour proposed strategies are effective to reduce the\nenergy consumption.\nOn the other hand, we spent a large amount of\ncomputational costs to investigate the usefulness of\nour proposed strategies in various situations. Ap-\npendix B indicates our used GPUs and the number\nof updates that correspond to the computational\ncosts.\nAcknowledgements\nWe thank the anonymous reviewers for their insight-\nful suggestions. A part of this work was supported\nby JSPS KAKENHI Grant Number JP21K17800\nand JST ACT-X Grant Number JPMJAX200I.\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In\nProceedings of ICLR.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language models\nin machine translation. In Proceedings of EMNLP-\nCoNLL, pages 858–867.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS,\npages 1877–1901.\nRaj Dabre and Atsushi Fujita. 2019. Recurrent stack-\ning of layers for compact neural machine translation\nmodels. Proceedings of AAAI, 33:6292–6299.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2019. Universal\ntransformers. In Proceedings of ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT, pages\n4171–4186.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of EMNLP, pages 489–500.\nAlex Graves. 2017. Adaptive computation time for\nrecurrent neural networks.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, and Ruom-\ning Pang. 2020. Conformer: Convolution-augmented\ntransformer for speech recognition. In Proceed-\nings of the 21st Annual Conference of the Interna-\ntional Speech Communication Association (INTER-\nSPEECH), pages 5036–5040.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In Proceedings of CVPR, pages 770–778.\nShun Kiyono, Takumi Ito, Ryuto Konno, Makoto Mor-\nishita, and Jun Suzuki. 2020. Tohoku-AIP-NTT at\nWMT 2020 news translation task. In Proceedings of\nWMT, pages 145–155.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nICLR.\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du,\nTong Xiao, Huizhen Wang, and Jingbo Zhu. 2020.\nShallow-to-deep training for neural machine transla-\ntion. In Proceedings of EMNLP, pages 995–1005.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen,\nand Jiawei Han. 2020. Understanding the difficulty\nof training transformers. In Proceedings of EMNLP,\npages 5747–5763.\nGábor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. Proceedings of ICLR.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and Optimizing LSTM\nLanguage Models. In Proceedings of ICLR.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of ICLR.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn NIPS, volume 26.\nToan Q. Nguyen and Julian Salazar. 2019. Transformers\nwithout tears: Improving the normalization of self-\nattention. In Proceedings of IWSLT.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of WMT, pages 1–9.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur. 2015. Librispeech: An asr corpus\nbased on public domain audio books. In ICASSP,\npages 5206–5210.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of WMT, pages 186–191.\nOfir Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\npages 2996–3005.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation models\nwith monolingual data. In Proceedings of ACL, pages\n86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of ACL, pages\n1715–1725.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 3645–3650.\nJun Suzuki and Hideki Isozaki. 2008. Semi-supervised\nsequential labeling and segmentation using giga-\nword scale unlabeled data. In Proceedings of ACL,\npages 665–673.\nSho Takase and Shun Kiyono. 2021. Rethinking per-\nturbations in encoder-decoders for fast training. In\nProceedings of NAACL-HLT, pages 5767–5780.\nSho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun\nSuzuki. 2022. B2t connection: Serving stability and\nperformance in deep transformers. arXiv preprint\narXiv:2206.00330.\nSho Takase and Sosuke Kobayashi. 2020. All word em-\nbeddings from one embedding. In Advances in Neu-\nral Information Processing Systems 33 (NeurIPS),\npages 3775–3785.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proceedings of EMNLP, pages 4599–4609.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nChanghan Wang, Yun Tang, Xutai Ma, Anne Wu,\nDmytro Okhonko, and Juan Pino. 2020. Fairseq\nS2T: Fast speech-to-text modeling with fairseq. In\nProceedings of AACL-IJCNLP, pages 33–39.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of ACL, pages\n1810–1822.\nYingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and\nTao Qin. 2019. Tied transformers: Neural machine\ntranslation with shared encoder and decoder. Pro-\nceedings of AAAI, 33(01):5466–5473.\nTong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and\nTongran Liu. 2019. Sharing attention weights for\nfast transformer. In Proceedings of IJCAI, pages\n5292–5298.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. In Pro-\nceedings of ICML.\nA Experiments on Language Modeling\nA.1 Dataset\nWe focused Transformer-based encoder-decoders\nin the main experiments of this paper. However, re-\ncent studies often employed the decoder side only\nas a pre-trained model. Thus, we conduct exper-\niments on the language modeling task to investi-\ngate the efficiency of our proposed strategies when\nwe use the decoder side only. We used Wikitext-\n103 (Merity et al., 2017) which contains a large\namount of training data. We measured perplexity\nof validation and test sets.\nA.2 Methods\nWe used the Transformer with adaptive in-\nputs (Baevski and Auli, 2019) as the base archi-\ntecture. In the same as in Baevski and Auli (2019),\nthe Transformer in the language modeling consists\nof the Pre-LN configuration. We set N = 6 for\nVanilla and Universal. For the proposed strategies,\nwe set N = 12and M = 6.\nA.3 Results\nTable 5 shows perplexities of each method. This\ntable indicates that Vanilla achieved better perfor-\nmance than Universal. Thus, the sharing param-\neters of one layer with all layers might not be\nsuitable for a large-scaled language modeling task.\nIn contrast, the proposed strategies outperformed\nVanilla. This result indicates that our proposed\nstrategies are also more efficient than Universal in\nthe language modeling.\nThrough the comparison among proposed strate-\ngies, SEQUENCE achieved the best perplexity. As\ndescribed in Section 4, SEQUENCE might be more\nappropriate to the Transformer with the Pre-LN\nconfiguration. To explore the reason, we believe\nthat we have to conduct the theoretical analysis of\nthe Transformer during its training. We address\nthis issue in the future study.\nThe lower part of Table 5 shows the reported\nscore of Baevski and Auli (2019), our reproduced\nscore, and SEQUENCE with more parameters. This\npart indicates that SEQUENCE achieved better per-\nplexities than others even though the parameter size\nof SEQUENCE is smaller. Therefore, SEQUENCE is\nalso efficient when we prepare a large amount of\nparameters for a language model.\nMethod #Params Valid Test\nVanilla 121M 20.39 21.13\nUniversal 121M 22.75 23.84\nSEQUENCE 121M 18.97 19.69\nCYCLE 121M 19.00 19.69\nCYCLE (REV ) 121M 19.60 20.24\nModels with more parameters\nBaevski and Auli (2019)† 247M 18.53 19.24\nBaevski and Auli (2019) 247M - 18.7\nSEQUENCE 234M 17.71 18.55\nTable 5: The parameter sizes and perplexities of each\nmethod. The lower part indicates scores reported in\nBaevski and Auli (2019) and the score of SEQUENCE\nwith more parameters. Scores in bold denote the best\nresults for each set. † represents our re-run of Baevski\nand Auli (2019).\nB Details of Experimental Settings\nWe used NVIDIATesla V100 GPUs for all exper-\niments. Table 6 shows the hyper-parameters for\ntraining in each task. The descriptions in our code\nalso help to understand configurations in this study.\nParams Machine Translation ASR Language Model\nLeaning rate 0.001 0.001 0.001\nScheduler inverse sqrt inverse sqrt inverse sqrt\nAdam β (0.9, 0.98) (0.9, 0.98) (0.9, 0.98)\nWarmup updates 4k 4k 2k\nMax updates 50k 150k 50k\nTable 6: Hyper-parameters used in our experiments.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6697954535484314
    },
    {
      "name": "Computer science",
      "score": 0.6174229383468628
    },
    {
      "name": "Computational complexity theory",
      "score": 0.4353301525115967
    },
    {
      "name": "Algorithm",
      "score": 0.36213192343711853
    },
    {
      "name": "Voltage",
      "score": 0.19500163197517395
    },
    {
      "name": "Electrical engineering",
      "score": 0.1615142822265625
    },
    {
      "name": "Engineering",
      "score": 0.15110084414482117
    }
  ]
}