{
    "title": "Image Transformer",
    "url": "https://openalex.org/W2950739196",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2625834147",
            "name": "Niki Parmar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2171687631",
            "name": "Ashish Vaswani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226984371",
            "name": "Jakob Uszkoreit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115925361",
            "name": "Łukasz Kaiser",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2496873187",
            "name": "Noam Shazeer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2946843700",
            "name": "Alexander Ku",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2497777409",
            "name": "Dustin Tran",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2963143316",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W2523714292",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W115742922",
        "https://openalex.org/W2097039814",
        "https://openalex.org/W2135181320",
        "https://openalex.org/W2963684088"
    ],
    "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.",
    "full_text": "Image Transformer\nNiki Parmar * 1 Ashish Vaswani *1 Jakob Uszkoreit1\nŁukasz Kaiser 1 Noam Shazeer 1 Alexander Ku 2 3 Dustin Tran4\nAbstract\nImage generation has been successfully cast as\nan autoregressive sequence generation or trans-\nformation problem. Recent work has shown that\nself-attention is an effective way of modeling tex-\ntual sequences. In this work, we generalize a\nrecently proposed model architecture based on\nself-attention, the Transformer, to a sequence\nmodeling formulation of image generation with\na tractable likelihood. By restricting the self-\nattention mechanism to attend to local neighbor-\nhoods we signiﬁcantly increase the size of im-\nages the model can process in practice, despite\nmaintaining signiﬁcantly larger receptive ﬁelds\nper layer than typical convolutional neural net-\nworks. While conceptually simple, our generative\nmodels signiﬁcantly outperform the current state\nof the art in image generation on ImageNet, im-\nproving the best published negative log-likelihood\non ImageNet from 3.83 to 3.77. We also present\nresults on image super-resolution with a large\nmagniﬁcation ratio, applying an encoder-decoder\nconﬁguration of our architecture. In a human eval-\nuation study, we ﬁnd that images generated by\nour super-resolution model fool human observers\nthree times more often than the previous state of\nthe art.\n1. Introduction\nRecent advances in modeling the distribution of natural\nimages with neural networks allow them to generate increas-\ningly natural-looking images. Some models, such as the\nPixelRNN and PixelCNN (van den Oord et al., 2016a), have\n*Equal contribution. Ordered by coin ﬂip. 1Google Brain,\nMountain View, USA2Department of Electrical Engineering and\nComputer Sciences, University of California, Berkeley 3Work\ndone during an internship at Google Brain 4Google AI, Mountain\nView, USA. Correspondence to: Ashish Vaswani, Niki Parmar,\nJakob Uszkoreit <avaswani@google.com, nikip@google.com,\nusz@google.com>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nTable 1.Three outputs of a CelebA super-resolution model fol-\nlowed by three image completions by a conditional CIFAR-10\nmodel, with input, model output and the original from left to right\na tractable likelihood. Beyond licensing the comparatively\nsimple and stable training regime of directly maximizing\nlog-likelihood, this enables the straightforward application\nof these models in problems such as image compression\n(van den Oord & Schrauwen, 2014) and probabilistic plan-\nning and exploration (Bellemare et al., 2016).\nThe likelihood is made tractable by modeling the joint dis-\ntribution of the pixels in the image as the product of condi-\ntional distributions (Larochelle & Murray, 2011; Theis &\nBethge, 2015). Thus turning the problem into a sequence\nmodeling problem, the state of the art approaches apply\nrecurrent or convolutional neural networks to predict each\nnext pixel given all previously generated pixels (van den\nOord et al., 2016a). Training recurrent neural networks\nto sequentially predict each pixel of even a small image\nis computationally very challenging. Thus, parallelizable\nmodels that use convolutional neural networks such as the\nPixelCNN have recently received much more attention, and\nhave now surpassed the PixelRNN in quality (van den Oord\net al., 2016b).\nOne disadvantage of CNNs compared to RNNs is their\ntypically fairly limited receptive ﬁeld. This can adversely\naffect their ability to model long-range phenomena common\nin images, such as symmetry and occlusion, especially with\na small number of layers. Growing the receptive ﬁeld has\nbeen shown to improve quality signiﬁcantly (Salimans et al.).\nDoing so, however, comes at a signiﬁcant cost in number\narXiv:1802.05751v3  [cs.CV]  15 Jun 2018\nImage Transformer\nof parameters and consequently computational performance\nand can make training such models more challenging.\nIn this work we show that self-attention (Cheng et al., 2016;\nParikh et al., 2016; Vaswani et al., 2017) can achieve a better\nbalance in the trade-off between the virtually unlimited\nreceptive ﬁeld of the necessarily sequential PixelRNN and\nthe limited receptive ﬁeld of the much more parallelizable\nPixelCNN and its various extensions.\nWe adopt similar factorizations of the joint pixel distribu-\ntion as previous work. Following recent work on model-\ning text (Vaswani et al., 2017), however, we propose es-\nchewing recurrent and convolutional networks in favor of\nthe Image Transformer, a model based entirely on a self-\nattention mechanism. The speciﬁc, locally restricted form\nof multi-head self-attention we propose can be interpreted\nas a sparsely parameterized form of gated convolution. By\ndecoupling the size of the receptive ﬁeld from the num-\nber of parameters, this allows us to use signiﬁcantly larger\nreceptive ﬁelds than the PixelCNN.\nDespite comparatively low resource requirements for train-\ning, the Image Transformer attains a new state of the art\nin modeling images from the standard ImageNet data set,\nas measured by log-likelihood. Our experiments indicate\nthat increasing the size of the receptive ﬁeld plays a sig-\nniﬁcant role in this improvement. We observe signiﬁcant\nimprovements up to effective receptive ﬁeld sizes of 256\npixels, while the PixelCNN (van den Oord et al., 2016b)\nwith 5x5 ﬁlters used 25.\nMany applications of image density models require condi-\ntioning on additional information of various kinds: from im-\nages in enhancement or reconstruction tasks such as super-\nresolution, in-painting and denoising to text when synthesiz-\ning images from natural language descriptions (Mansimov\net al., 2015). In visual planning tasks, conditional image\ngeneration models could predict future frames of video con-\nditioned on previous frames and taken actions.\nIn this work we hence also evaluate two different methods\nof performing conditional image generation with the Im-\nage Transformer. In image-class conditional generation we\ncondition on an embedding of one of a small number of\nimage classes. In super-resolution with high magniﬁcation\nratio (4x), we condition on a very low-resolution image,\nemploying the Image Transformer in an encoder-decoder\nconﬁguration (Kalchbrenner & Blunsom, 2013). In com-\nparison to recent work on autoregressive super-resolution\n(Dahl et al., 2017), a human evaluation study found im-\nages generated by our models to look convincingly natural\nsigniﬁcantly more often.\n2. Background\nThere is a broad variety of types of image generation mod-\nels in the literature. This work is strongly inspired by au-\ntoregressive models such as fully visible belief networks\nand NADE (Bengio & Bengio, 2000; Larochelle & Mur-\nray, 2011) in that we also factor the joint probability of\nthe image pixels into conditional distributions. Following\nPixelRNN (van den Oord et al., 2016a), we also model the\ncolor channels of the output pixels as discrete values gener-\nated from a multinomial distribution, implemented using a\nsimple softmax layer.\nThe current state of the art in modeling images on CIFAR-\n10 data set was achieved by PixelCNN++, which models the\noutput pixel distribution with a discretized logistic mixture\nlikelihood, conditioning on whole pixels instead of color\nchannels and changes to the architecture (Salimans et al.).\nThese modiﬁcations are readily applicable to our model,\nwhich we plan to evaluate in future work.\nAnother, popular direction of research in image generation\nis training models with an adversarial loss (Goodfellow\net al., 2014). Typically, in this regime a generator net-\nwork is trained in opposition to a discriminator network\ntrying to determine if a given image is real or generated. In\ncontrast to the often blurry images generated by networks\ntrained with likelihood-based losses, generative adversar-\nial networks (GANs) have been shown to produce sharper\nimages with realistic high-frequency detail in generation\nand image super-resolution tasks (Zhang et al., 2016; Ledig\net al., 2016).\nWhile very promising, GANs have various drawbacks. They\nare notoriously unstable (Radford et al., 2015), motivating\na large number of methods attempting to make their train-\ning more robust (Metz et al., 2016; Berthelot et al., 2017).\nAnother common issue is that of mode collapse, where gen-\nerated images fail to reﬂect the diversity in the training set\n(Metz et al., 2016).\nA related problem is that GANs do not have a density in\nclosed-form. This makes it challenging to measure the\ndegree to which the models capture diversity. This also\ncomplicates model design. Objectively evaluating and com-\nparing, say, different hyperparameter choices is typically\nmuch more difﬁcult in GANs than in models with a tractable\nlikelihood.\n3. Model Architecture\n3.1. Image Representation\nWe treat pixel intensities as either discrete categories or ordi-\nnal values; this setting depends on the distribution (Section\n3.4). For categories, each of the input pixels’ three color\nchannels is encoded using a channel-speciﬁc set of 256\nImage Transformer\nInput Gen Truth Input Gen Truth\nTable 2.On the left are image completions from our best conditional generation model, where we sample the second half. On the right are\nsamples from our four-fold super-resolution model trained on CIFAR-10. Our images look realistic and plausible, show good diversity\namong the completion samples and observe the outputs carry surprising details for coarse inputs in super-resolution.\nd-dimensional embedding vectors of the intensity values\n0 −255. For output intensities, we share a single, separate\nset of 256 d-dimensional embeddings across the channels.\nFor an image of width w and height h, we combine the\nwidth and channel dimensions yielding a 3-dimensional\ntensor with shape [h,w ·3,d].\nFor ordinal values, we run a 1x3 window size, 1x3 strided\nconvolution to combine the 3 channels per pixel to form an\ninput representation with shape [h,w,d ].\nTo each pixel representation, we add a d-dimensional en-\ncoding of coordinates of that pixel. We evaluated two dif-\nferent coordinate encodings: sine and cosine functions of\nthe coordinates, with different frequencies across different\ndimensions, following (Vaswani et al., 2017), and learned\nposition embeddings. Since we need to represent two co-\nordinates, we use d/2 of the dimensions to encode the row\nnumber and the other d/2 of the dimensions to encode the\nthe column and color channel.\n3.2. Self-Attention\nFor image-conditioned generation, as in our super-resolution\nmodels, we use an encoder-decoder architecture. The en-\ncoder generates a contextualized, per-pixel-channel repre-\nsentation of the source image. The decoder autoregressively\ngenerates an output image of pixel intensities, one channel\nper pixel at each time step. While doing so, it consumes the\npreviously generated pixels and the input image represen-\nsoftmax\n+ + + +\n \n \ncmp cmp\n \ncmp\n \nq’\nq m1 m2 m3\npq p1 p2 p3\nMatMul WvMatMul Wv\n·\nMatMul Wq MatMul Wk\n \n   \nMatMul Wv\n \nLayerNorm\nDropout\nLocal Self-Attention\n \nLayerNorm FFNN\nDropout\nFigure 1.A slice of one layer of the Image Transformer, recom-\nputing the representation q′ of a single channel of one pixel qby\nattending to a memory of previously generated pixels m1,m2,... .\nAfter performing local self-attention we apply a two-layer position-\nwise feed-forward neural network with the same parameters for\nall positions in a given layer. Self-attention and the feed-forward\nnetworks are followed by dropout and bypassed by a residual\nconnection with subsequent layer normalization. The position\nencodings pq,p1,... are added only in the ﬁrst layer.\nImage Transformer\ntation generated by the encoder. For both the encoder and\ndecoder, the Image Transformer uses stacks of self-attention\nand position-wise feed-forward layers, similar to (Vaswani\net al., 2017). In addition, the decoder uses an attention\nmechanism to consume the encoder representation. For un-\nconditional and class-conditional generation, we employ the\nImage Transformer in a decoder-only conﬁguration.\nBefore we describe how we scale self-attention to images\ncomprised of many more positions than typically found in\nsentences, we give a brief description of self-attention.\nEach self-attention layer computes a d-dimensional repre-\nsentation for each position, that is, each channel of each\npixel. To recompute the representation for a given posi-\ntion, it ﬁrst compares the position’s current representation\nto other positions’ representations, obtaining an attention\ndistribution over the other positions. This distribution is\nthen used to weight the contribution of the other positions’\nrepresentations to the next representation for the position at\nhand.\nEquations 1 and 2 outline the computation in our self-\nattention and fully-connected feed-forward layers; Figure\n1 depicts it. W1 and W2 are the parameters of the feed-\nforward layer, and are shared across all the positions in a\nlayer. These fully describe all operations performed in every\nlayer, independently for each position, with the exception of\nmulti-head attention. For details of multi-head self-attention,\nsee (Vaswani et al., 2017).\nqa = layernorm(q+ dropout(\nsoftmax\n(Wqq(MWk)T\n√\nd\n)\nMWv)) (1)\nq′= layernorm(qa + dropout(W1ReLu(W2qa))) (2)\nIn more detail, following previous work, we call the cur-\nrent representation of the pixel’s channel, or position, to be\nrecomputed the query q. The other positions whose repre-\nsentations will be used in computing a new representation\nfor qare m1,m2,... which together comprise the columns\nof the memory matrix M. Note that M can also contain q.\nWe ﬁrst transform qand M linearly by learned matrices Wq\nand Wk, respectively.\nThe self-attention mechanism then compares qto each of\nthe pixel’s channel representations in the memory with a dot-\nproduct, scaled by 1/\n√\nd. We apply the softmax function\nto the resulting compatibility scores, treating the obtained\nvector as attention distribution over the pixel channels in\nthe memory. After applying another linear transformation\nWv to the memory M, we compute a weighted average of\nthe transformed memory, weighted by the attention distribu-\ntion. In the decoders of our different models we mask the\noutputs of the comparisons appropriately so that the model\ncannot attend to positions in the memory that have not been\ngenerated, yet.\nTo the resulting vector we then apply a single-layer fully-\nconnected feed-forward neural network with rectiﬁed linear\nactivation followed by another linear transformation. The\nlearned parameters of these are shared across all positions\nbut different from layer to layer.\nAs illustrated in Figure1, we perform dropout, merge in\nresidual connections and perform layer normalization after\neach application of self-attention and the position-wise feed-\nforward networks (Ba et al., 2016; Srivastava et al., 2014).\nThe entire self-attention operation can be implemented using\nhighly optimized matrix multiplication code and executed\nin parallel for all pixels’ channels.\n3.3. Local Self-Attention\nThe number of positions included in the memory lm, or the\nnumber of columns of M, has tremendous impact on the\nscalability of the self-attention mechanism, which has a time\ncomplexity in O(h·w·lm ·d).\nThe encoders of our super-resolution models operate on8×8\npixel images and it is computationally feasible to attend to\nall of their 192 positions. The decoders in our experiments,\nhowever, produce32 ×32 pixel images with 3072 positions,\nrendering attending to all positions impractical.\nInspired by convolutional neural networks we address this\nby adopting a notion of locality, restricting the positions\nin the memory matrix M to a local neighborhood around\nthe query position. Changing this neighborhood per query\nposition, however, would prohibit packing most of the com-\nputation necessary for self-attention into two matrix multi-\nplications - one for computing the pairwise comparisons and\nanother for generating the weighted averages. To avoid this,\nwe partition the image into query blocks and associate each\nof these with a larger memory block that also contains the\nquery block. For all queries from a given query block, the\nmodel attends to the same memory matrix, comprised of all\npositions from the memory block. The self-attention is then\ncomputed for all query blocks in parallel. The feed-forward\nnetworks and layer normalizations are computed in parallel\nfor all positions.\nIn our experiments we use two different schemes for choos-\ning query blocks and their associated memory block neigh-\nborhoods, resulting in two different factorizations of the\njoint pixel distribution into conditional distributions. Both\nare illustrated in Figure 2.\nImage Transformer\n1D Local Attention For 1D local attention (Section 3.3)\nwe ﬁrst ﬂatten the input tensor with positional encodings in\nraster-scan order, similar to previous work (van den Oord\net al., 2016a). To compute self-attention on the resulting\nlinearized image, we then partition the length into non-\noverlapping query blocks Qof length lq, padding with ze-\nroes if necessary. While contiguous in the linearized image,\nthese blocks can be discontiguous in image coordinate space.\nFor each query block we build the memory block M from\nthe same positions as Qand an additional lm positions cor-\nresponding to pixels that have been generated before, which\ncan result in overlapping memory blocks.\n2D Local Attention In 2D local attention models, we\npartition the input tensor with positional encodings into\nrectangular query blocks contiguous in the original image\nspace. We generate the image one query block after another,\nordering the blocks in raster-scan order. Within each block,\nwe generate individual positions, or pixel channels, again in\nraster-scan order.\nAs illustrated in the right half of Figure 2, we generate the\nblocks outlined in grey lines left-to-right and top-to-bottom.\nWe use 2-dimensional query blocks of a size lq speciﬁed by\nheight and widthlq = wq·hq, and memory blocks extending\nthe query block to the top, left and right by hm, wm and\nagain wm pixels, respectively.\nIn both 1D and 2D local attention, we mask attention\nweights in the query and memory blocks such that posi-\ntions that have not yet been generated are ignored.\nAs can be seen in Figure 2, 2D local attention balances hori-\nzontal and vertical conditioning context much more evenly.\nWe believe this might have an increasingly positive effect on\nquality with growing image size as the conditioning informa-\ntion in 1D local attention becomes increasingly dominated\nby pixels next to a given position as opposed to above it.\n3.4. Loss Function\nWe perform maximum likelihood, in which we maximize\nlog p(x) = ∑h·w·3\nt=1 log p(xt |x<t) with respect to network\nparameters, and where the network outputs all parameters\nof the autoregressive distribution. We experiment with two\nsettings of the distribution: a categorical distribution across\neach channel (van den Oord et al., 2016a) and a mixture of\ndiscretized logistics over three channels (Salimans et al.).\nThe categorical distribution ( cat) captures each intensity\nvalue as a discrete outcome and factorizes across channels.\nIn total, there are256·3 = 768 parameters for each pixel; for\n32 ×32 images, the network outputs 786,432 dimensions.\nUnlike the categorical distribution, the discretized mixture\nof logistics ( DMOL) captures two important properties:\nthe ordinal nature of pixel intensities and simpler depen-\nMemory Block\nLocal 1D Attention\nMemory Block\nq\nq\nQuery Block\nQuery Block\nLocal 2D Attention\nMemory Block\nFigure 2.The two different conditional factorizations used in our\nexperiments, with 1D and 2D local attention on the left and right,\nrespectively. In both, the image is partitioned into non-overlapping\nquery blocks, each associated with a memory block covering a\nsuperset of the query block pixels. In every self-attention layer,\neach position in a query block attends to all positions in the memory\nblock. The pixel marked as qis the last that was generated. All\nchannels of pixels in the memory and query blocks shown in white\nhave masked attention weights and do not contribute to the next\nrepresentations of positions in the query block. While the effective\nreceptive ﬁeld size in this ﬁgure is the same for both schemes, in\n2D attention the memory block contains a more evenly balanced\nnumber of pixels next to and above the query block, respectively.\ndence across channels (Salimans et al.). For each pixel, the\nnumber of parameters is 10 times the number of mixture\ncomponents: 10 for one unnormalized mixture probability,\nthree means, three standard deviations, and three coefﬁcients\nwhich capture the linear dependence. For 10 mixtures, this\ntranslates to 100 parameters for each pixel; for 32 ×32 im-\nages, the network outputs 102,400 dimensions, which is a\n7x reduction enabling denser gradients and lower memory.\n4. Inference\nAcross all of the presented experiments, we use categorical\nsampling during decoding with a tempered softmax (Dahl\net al., 2017). We adjust the concentration of the distribution\nwe sample from with a temperature τ >0 by which we\ndivide the logits for the channel intensities.\nWe tuned τ between 0.8 and 1.0, observing the highest\nperceptual quality in unconditioned and class-conditional\nimage generation with τ = 1.0. For super-resolution we\npresent results for different temperatures in Table 5.\n5. Experiments\nAll code we used to develop, train, and evaluate our models\nis available in Tensor2Tensor (Vaswani et al., 2018).\nFor all experiments we optimize with Adam (Kingma & Ba,\n2015), and vary the learning rate as speciﬁed in (Vaswani\net al., 2017). We train our models on both p100 and k40\nGPUs, with batch sizes ranging from 1 to 8 per GPU.\nImage Transformer\nTable 3.Conditional image generations for all CIFAR-10 cate-\ngories. Images on the left are from a model that achieves 3.03\nbits/dim on the test set. Images on the right are from our best\nnon-averaged model with 2.99 bits/dim. Both models are able\nto generate convincing cars, trucks, and ships. Generated horses,\nplanes, and birds also look reasonable.\n5.1. Generative Image Modeling\nOur unconditioned and class-conditioned image generation\nmodels both use 1D local attention, withlq = 256 and a total\nmemory size of 512. On CIFAR-10 our best unconditional\nmodels achieve a perplexity of 2.90 bits/dim on the test set\nusing either DMOL or categorical. For categorical, we use\n12 layers with d= 512, heads=4, feed-forward dimension\n2048 with a dropout of 0.3. In DMOL, our best conﬁg uses\n14 layers, d= 256, heads=8, feed-forward dimension 512\nand a dropout of 0.2. This is a considerable improvement\nover two baselines: the PixelRNN (van den Oord et al.,\n2016a) and PixelCNN++ (Salimans et al.). Introduced after\nthe Image Transformer, the also self-attention based Pixel-\nSNAIL model reaches a signiﬁcantly lower perplexity of\n2.85 bits/dim on CIFAR-10 (Chen et al., 2017). On the\nmore challenging ImageNet data set, however, the Image\nTransformer performs signiﬁcantly better than PixelSNAIL.\nWe also train smaller 8 layer CIFAR-10 models which have\nd = 512 , 1024 dimensions in the feed-forward layers, 8\nattention heads and use dropout of 0.1, and achieve 3.03\nbits/dim, matching the PixelCNN model (van den Oord\net al., 2016a). Our best CIFAR-10 model with DMOL has d\nand feed-forward layer layer dimension of 256 and perform\nattention in 512 dimensions.\nTable 4.Bits/dim on CIFAR-10 test and ImageNet validation sets.\nThe Image Transformer outperforms all models and matches Pixel-\nCNN++, achieving a new state-of-the-art on ImageNet. Increasing\nmemory block size (bsize) signiﬁcantly improves performance.\nModel Type bsize NLL\nCIFAR-10 ImageNet\n(Test) (Validation)\nPixel CNN - 3.14 -\nRow Pixel RNN - 3.00 3 .86\nGated Pixel CNN - 3.03 3 .83\nPixel CNN++ - 2.92 -\nPixelSNAIL - 2.85 3.80\nOurs 1D local (8l, cat) 8 4.06 -\n16 3.47 -\n64 3.13 -\n256 2.99 -\nOurs 1D local (cat) 256 2.90 3.77\nOurs 1D local (dmol) 256 2.90 -\nImageNet is a much larger dataset, with many more cate-\ngories than CIFAR-10, requiring more parameters in a gener-\native model. Our ImageNet unconditioned generation model\nhas 12 self-attention and feed-forward layers, d = 512, 8\nattention heads, 2048 dimensions in the feed-forward lay-\ners, and dropout of 0.1. It signiﬁcantly outperforms the\nGated PixelCNN and establishes a new state-of-the-art of\n3.77 bits/dim with checkpoint averaging. We trained only\nunconditional generative models on ImageNet, since class\nlabels were not available in the dataset provided by (van den\nOord et al., 2016a).\nTable 4 shows that growing the receptive ﬁeld improves\nperplexity signiﬁcantly. We believe this to highlight a key\nadvantage of local self-attention over CNNs: namely that\nthe number of parameters used by local self-attention is\nindependent of the size of the receptive ﬁeld. Furthermore,\nwhile d> receptiveﬁeld, self-attention still requires fewer\nﬂoating-point operations.\nFor experiments with the categorical distribution we evalu-\nated both coordinate encoding schemes described in Section\n3.3 and found no difference in quality. For DMOL we only\nevaluated learned coordinate embeddings.\n5.2. Conditioning on Image Class\nWe represent the image classes as learned d-dimensional\nembeddings per class and simply add the respective em-\nbedding to the input representation of every input position\ntogether with the positional encodings.\nWe trained the class-conditioned Image Transformer on\nCIFAR-10, achieving very similar log-likelihoods as in un-\nconditioned generation. The perceptual quality of generated\nImage Transformer\nimages, however, is signiﬁcantly higher than that of our\nunconditioned models. The samples from our 8-layer class-\nconditioned models in Table 3, show that we can generate\nrealistic looking images for some categories, such as cars\nand trucks.\n5.3. Image Super-Resolution\nSuper-resolution is the process of recovering a high resolu-\ntion image from a low resolution image while generating\nrealistic and plausible details. Following (Dahl et al., 2017),\nin our experimental setup we enlarge an 8 ×8 pixel image\nfour-fold to 32 ×32, a process that is massively underspec-\niﬁed: the model has to generate aspects such as texture of\nhair, makeup, skin and sometimes even gender that cannot\npossibly be recovered from the source image.\nHere, we use the Image Transformer in an encoder-decoder\nconﬁguration, connecting the encoder and decoder through\nan attention mechanism (Vaswani et al., 2017). For the\nencoder, we use embeddings for RGB intensities for each\npixel in the 8×8 image and add 2 dimensional positional\nencodings for each row and width position. Since the input\nis small, we ﬂatten the whole image as a [h×w×3,d]\ntensor, where dis typically 512. We then feed this sequence\nto our stack of transformer encoder layers that uses repeated\nself-attention and feed forward layers. In the encoder we\ndon’t require masking, but allow any input pixel to attend\nto any other pixel. In the decoder, we use a stack of local\nself-attention, encoder-decoder-attention and feed-forward\nlayers. We found using two to three times fewer encoder\nthan decoder layers to be ideal for this task.\nWe perform end-to-end training of the encoder-decoder\nmodel for Super resolution using the log-likelihood ob-\njective function. Our method generates higher resolution\nimages that look plausible and realistic across two datasets.\nFor both of the following data sets, we resized the image to\n8 ×8 pixels for the input and 32 ×32 pixels for the label\nusing TensorFlow’sarea interpolation method.\nCelebA We trained both our 1D Local and 2D Local mod-\nels on the standard CelebA data set of celebrity faces with\ncropped boundaries. With the 1D Local, we achieve a nega-\ntive log likelihood (NLL) of 2.68 bits/dim on the dev set,\nusing lq = 128, memory size of 256, 12 self-attention and\nfeed-forward layers, d = 512, 8 attention heads, 2048 di-\nmensions in the feed-forward layers, and a dropout of 0.1.\nWith the 2D Local model, we only change the query and\nmemory to now represent a block of size 8 ×32 pixels and\n16 ×64 pixels respectively. This model achieves a NLL\nof 2.61 bits/dim. Existing automated metrics like pSNR,\nSSIM and MS-SSIM have been shown to not correlate with\nperceptual image quality (Dahl et al., 2017). Hence, we con-\nducted a human evaluation study on Amazon Mechanical\nTable 5.Negative log-likelihood and human eval performance for\nthe Image Transformer on CelebA. The fraction of humans fooled\nis signiﬁcantly better than the previous state of the art.\nModel Type τ %Fooled\nResNet n/a 4.0\nsrez GAN n/a 8.5\nPixelRecursive 1.0 11 .0\n(Dahl et al., 2017) 0.9 10 .4\n0.8 10 .2\n1D local 1.0 29 .6 ±4.0\nImage Transformer 0.9 33 .5 ±3.5\n0.8 35.94 ±3.0\n2D local 1.0 30 .64 ±4\nImage Transformer 0.9 34 ±3.5\n0.8 36.11 ±2.5\nTurk where each worker is required to make a binary choice\nwhen shown one generated and one real image. Following\nthe same procedure for the evaluation study as (Dahl et al.,\n2017), we show 50 pairs of images, selected randomly from\nthe validation set, to 50 workers each. Each generated and\noriginal image is upscaled to 128 ×128 pixels using the\nBilinear interpolation method. Each worker then has 1-2\nseconds to make a choice between these two images. In\nour method, workers choose images from our model up\nto 36.1% of the time, a signiﬁcant improvement over pre-\nvious models. Sampling temperature of 0.8 and 2D local\nattention maximized perceptual quality as measured by this\nevaluation.\nTo measure how well the high resolution samples correspond\nto the low resolution input, we calculate Consistency, the\nL2 distance between the low resolution input and a bicubic\ndownsampled version of the high resolution sample. We\nobserve a Consistency score of 0.01 which is on par with\nthe models in (Dahl et al., 2017).\nWe quantify that our models are more effective than exem-\nplar based Super Resolution techniques like Nearest Neigh-\nbors, which perform a naive look-up of the training data to\nﬁnd the high resolution output. We take a bicubic down-\nsampled version of our high resolution sample, ﬁnd the\nnearest low resolution input image in the training data for\nthat sample, and calculate the MS-SSIM score between the\nhigh resolution sample and the corresponding high reso-\nlution image in the training data. On average, we get a\nMS-SSIM score of 44.3, on 50 samples from the validation\nset, which shows that our models don’t merely learn to copy\ntraining images but generate high-quality images by adding\nsynthesized details on the low resolution input image.\nImage Transformer\nInput 1D Local Attention 2D Local Attention Original\nτ = 0.8 τ = 0.9 τ = 1.0 τ = 0.8 τ = 0.9 τ = 1.0\nTable 6.Images from our 1D and 2D local attention super-resolution models trained on CelebA, sampled with different temperatures. 2D\nlocal attention with τ = 0.9 scored highest in our human evaluation study.\nCIFAR-10 We also trained a super-resolution model on\nthe CIFAR-10 data set. Our model reached a negative log-\nlikelihood of 2.76 using 1D local attention and 2.78 using\n2D local attention on the test set. As seen in Figure 2, our\nmodel commonly generates plausible looking objects even\nthough the input images seem to barely show any discernible\nstructure beyond coarse shapes.\n6. Conclusion\nIn this work we demonstrate that models based on self-\nattention can operate effectively on modalities other than\ntext, and through local self-attention scale to signiﬁcantly\nlarger structures than sentences. With fewer layers, its larger\nreceptive ﬁelds allow the Image Transformer to signiﬁcantly\nimprove over the state of the art in unconditional, probabilis-\ntic image modeling of comparatively complex images from\nImageNet as well as super-resolution.\nWe further hope to have provided additional evidence\nthat even in the light of generative adversarial networks,\nlikelihood-based models of images is very much a promising\narea for further research - as is using network architectures\nsuch as the Image Transformer in GANs.\nIn future work we would like to explore a broader variety\nof conditioning information including free-form text, as\npreviously proposed (Mansimov et al., 2015), and tasks\ncombining modalities such as language-driven editing of\nimages.\nFundamentally, we aim to move beyond still images to\nvideo (Kalchbrenner et al., 2016) and towards applications\nin model-based reinforcement learning.\nReferences\nBa, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E.\nLayer normalization. arXiv preprint arXiv:1607.06450,\n2016.\nBellemare, Marc G., Srinivasan, Sriram, Ostrovski, Georg,\nSchaul, Tom, Saxton, David, and Munos, R´emi. Unifying\ncount-based exploration and intrinsic motivation. CoRR,\nabs/1606.01868, 2016. URL http://arxiv.org/\nabs/1606.01868.\nBengio, Yoshua and Bengio, Samy. Modeling high-\ndimensional discrete data with multi-layer neural net-\nworks. In Neural Information Processing Systems, pp.\n400–406. MIT Press, 2000.\nBerthelot, David, Schumm, Tom, and Metz, Luke. BEGAN:\nboundary equilibrium generative adversarial networks.\nImage Transformer\nCoRR, abs/1703.10717, 2017. URL http://arxiv.\norg/abs/1703.10717.\nChen, Xi, Mishra, Nikhil, Rohaninejad, Mostafa, and\nAbbeel, Pieter. Pixelsnail: An improved autoregres-\nsive generative model. arXiv preprint arXiv:1712.09763,\n2017.\nCheng, Jianpeng, Dong, Li, and Lapata, Mirella. Long\nshort-term memory-networks for machine reading. arXiv\npreprint arXiv:1601.06733, 2016.\nDahl, Ryan, Norouzi, Mohammad, and Shlens, Jonathan.\nPixel recursive super resolution. 2017. URL https:\n//arxiv.org/abs/1702.00783.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets,\n2014.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continuous\ntranslation models. In Proceedings EMNLP 2013, pp.\n1700–1709, 2013. URL http://nal.co/papers/\nKalchbrennerBlunsom_EMNLP13.\nKalchbrenner, Nal, van den Oord, A ¨aron, Simonyan,\nKaren, Danihelka, Ivo, Vinyals, Oriol, Graves, Alex,\nand Kavukcuoglu, Koray. Video pixel networks. CoRR,\nabs/1610.00527, 2016. URL http://arxiv.org/\nabs/1610.00527.\nKingma, Diederik and Ba, Jimmy. Adam: A method for\nstochastic optimization. In ICLR, 2015.\nLarochelle, Hugo and Murray, Iain. The neural autoregres-\nsive distribution estimator. InThe Proceedings of the 14th\nInternational Conference on Artiﬁcial Intelligence and\nStatistics, volume 15 of JMLR: W&CP, pp. 29–37, 2011.\nLedig, Christian, Theis, Lucas, Huszar, Ferenc, Caballero,\nJose, Aitken, Andrew, Tejani, Alykhan, Totz, Johannes,\nWang, Zehan, and Shi, Wenzhe. Photo-realistic single\nimage super-resolution using a generative adversarial net-\nwork. arXiv:1609.04802, 2016.\nMansimov, Elman, Parisotto, Emilio, Ba, Lei Jimmy, and\nSalakhutdinov, Ruslan. Generating images from cap-\ntions with attention. CoRR, abs/1511.02793, 2015. URL\nhttp://arxiv.org/abs/1511.02793.\nMetz, Luke, Poole, Ben, Pfau, David, and Sohl-Dickstein,\nJascha. Unrolled generative adversarial networks. CoRR,\nabs/1611.02163, 2016. URL http://arxiv.org/\nabs/1611.02163.\nParikh, Ankur, T ¨ackstr¨om, Oscar, Das, Dipanjan, and\nUszkoreit, Jakob. A decomposable attention model.\nIn Empirical Methods in Natural Language Process-\ning, 2016. URL https://arxiv.org/pdf/1606.\n01933.pdf.\nRadford, Alec, Metz, Luke, and Chintala, Soumith.\nUnsupervised representation learning with deep con-\nvolutional generative adversarial networks. CoRR,\nabs/1511.06434, 2015. URL http://arxiv.org/\nabs/1511.06434.\nSalimans, Tim, Karpathy, Andrej, Chen, Xi, Kingma,\nDiederik P., and Bulatov, Yaroslav. Pixelcnn++: A pix-\nelcnn implementation with discretized logistic mixture\nlikelihood and other modiﬁcations. In International Con-\nference on Learning Representations.\nSrivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a\nsimple way to prevent neural networks from overﬁtting.\nJournal of Machine Learning Research, 15(1):1929–1958,\n2014.\nTheis, Lucas and Bethge, Matthias. Generative image mod-\neling using spatial lstms. In Proceedings of the 28th Inter-\nnational Conference on Neural Information Processing\nSystems - Volume 2, NIPS’15, pp. 1927–1935, Cambridge,\nMA, USA, 2015. MIT Press. URL http://dl.acm.\norg/citation.cfm?id=2969442.2969455.\nvan den Oord, A ¨aron and Schrauwen, Benjamin. The\nstudent-t mixture as a natural image patch prior\nwith application to image compression. Jour-\nnal of Machine Learning Research , 15:2061–2086,\n2014. URL http://jmlr.org/papers/v15/\nvandenoord14a.html.\nvan den Oord, A¨aron, Kalchbrenner, Nal, and Kavukcuoglu,\nKoray. Pixel recurrent neural networks. ICML, 2016a.\nvan den Oord, A ¨aron, Kalchbrenner, Nal, Vinyals, Oriol,\nEspeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray.\nConditional image generation with pixelcnn decoders.\nNIPS, 2016b.\nVaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit,\nJakob, Jones, Llion, Gomez, Aidan N., Kaiser, Lukasz,\nand Polosukhin, Illia. Attention is all you need. 2017.\nURL http://arxiv.org/abs/1706.03762.\nVaswani, Ashish, Bengio, Samy, Brevdo, Eugene, Chol-\nlet, Francois, Gomez, Aidan N., Gouws, Stephan, Jones,\nLlion, Kaiser, Łukasz, Kalchbrenner, Nal, Parmar, Niki,\nSepassi, Ryan, Shazeer, Noam, and Uszkoreit, Jakob.\nTensor2tensor for neural machine translation. CoRR,\nabs/1803.07416, 2018. URL http://arxiv.org/\nabs/1803.07416.\nImage Transformer\nZhang, Han, Xu, Tao, Li, Hongsheng, Zhang, Shaoting,\nHuang, Xiaolei, Wang, Xiaogang, and Metaxas, Dim-\nitris N. Stackgan: Text to photo-realistic image synthe-\nsis with stacked generative adversarial networks. CoRR,\nabs/1612.03242, 2016. URL http://arxiv.org/\nabs/1612.03242."
}