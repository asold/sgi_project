{
  "title": "Multi-Scale Transformer Pyramid Networks for Multivariate Time Series Forecasting",
  "url": "https://openalex.org/W4391129999",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5102860798",
      "name": "Yifan Zhang",
      "affiliations": [
        "University of Nevada, Reno"
      ]
    },
    {
      "id": "https://openalex.org/A5082946134",
      "name": "Rui Wu",
      "affiliations": [
        "East Carolina University"
      ]
    },
    {
      "id": "https://openalex.org/A5002580372",
      "name": "Sergiu M. Dascalu",
      "affiliations": [
        "University of Nevada, Reno"
      ]
    },
    {
      "id": "https://openalex.org/A5101893887",
      "name": "J. S. Harris",
      "affiliations": [
        "University of Nevada, Reno"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205650351",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W4385763767",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W6797155008",
    "https://openalex.org/W6846825190",
    "https://openalex.org/W6797297377",
    "https://openalex.org/W6889955440",
    "https://openalex.org/W2797846142",
    "https://openalex.org/W4230410911",
    "https://openalex.org/W2770058355",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2890096158",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4376608362",
    "https://openalex.org/W4386385376",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6810637551",
    "https://openalex.org/W4382203079"
  ],
  "abstract": "Multivariate Time Series (MTS) forecasting entails the intricate process of modeling temporal dependencies within historical data records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation impedes their capacity to effectively capture diverse seasonalities. In our study, we present a dimension-invariant embedding technique designed to capture short-term temporal dependencies. This procedure projects MTS data into a higher-dimensional space while preserving the original time steps and variable dimensions. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to capture temporal dependencies at multiple unconstrained scales effectively. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods. This enhancement in performance is particularly pronounced in datasets rich in fine-scale information, as it enables MTPNet to effectively capture a wide spectrum of temporal dependencies, ranging from fine to coarse scales. This finding highlights MTPNet&#x2019;s notable potential in analyzing MTS data sampled at the minute level. Code is available at github.com/MTPNet.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nMulti-scale Transformer Pyramid Networks for\nMultivariate Time Series Forecasting\nYIFAN ZHANG1, RUI WU2, (Member, IEEE), SERGIU M. DASCALU1, and FREDERICK C. HARRIS,\nJR.1\n1Department of Computer Science and Engineering, University of Nevada, Reno, Reno, NV 89557 USA\n2Department of Computer Science, East Carolina University, Greenville, NC 27858 USA\nCorresponding author: Yifan Zhang (yfzhang@nevada.unr.edu)\nThis work was supported by the National Science Foundation under grant numbers NSF awards 2142428, 2142360, OIA-2019609, and\nOIA-2148788.\nABSTRACT Multivariate Time Series (MTS) forecasting entails the intricate process of modeling temporal\ndependencies within historical data records. Transformers have demonstrated remarkable performance in\nMTS forecasting due to their capability to capture long-term dependencies. However, prior work has been\nconfined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially\nincrease (most with base 2). This limitation impedes their capacity to effectively capture diverse seasonalities.\nIn our study, we present a dimension-invariant embedding technique designed to capture short-term temporal\ndependencies. This procedure projects MTS data into a higher-dimensional space while preserving the orig-\ninal time steps and variable dimensions. Furthermore, we present a novel Multi-scale Transformer Pyramid\nNetwork (MTPNet), specifically designed to capture temporal dependencies at multiple unconstrained scales\neffectively. The predictions are inferred from multi-scale latent representations obtained from transformers\nat various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet\noutperforms recent state-of-the-art methods. This enhancement in performance is particularly pronounced\nin datasets rich in fine-scale information, as it enables MTPNet to effectively capture a wide spectrum\nof temporal dependencies, ranging from fine to coarse scales. This finding highlights MTPNet’s notable\npotential in analyzing MTS data sampled at the minute level.\nINDEX TERMS time series forecasting, transformer, multi-scale feature pyramid, value embedding.\nI. INTRODUCTION\nMultivariate time series (MTS) data, which captures multi-\nple variables over time, is of critical importance in various\nfields including finance, climate, and energy. MTS forecast-\ning, a critical machine learning task, aims to predict the\nfuture values of multiple variables based on their historical\nrecords. The MTS data inherently demonstrates low semantic\ncharacteristics, necessitating its analysis as a collection of\nmultiple values. For instance, this involves analyzing values\nof multiple variables at a single time step or values of multiple\ntime steps for a single variable. This approach facilitates\nthe extraction of two types of information from the MTS\ndata: correlations among variables (spatial dependencies) and\ncorrelations across time steps (temporal dependencies).\nIn recent years, machine learning models [1], [2], notably\ntransformers [3], have significantly advanced the exploration\nof MTS forecasting problems. The pioneering work by [4]\nintroduced transformers to MTS forecasting, highlighting\ntheir potential in adeptly capturing temporal dependencies.\nConsequently, numerous transformer-based methods have\nbeen introduced for the task of MTS forecasting [5]–[7]. A\ncommon technique for extracting spatial dependencies uti-\nlizes a linear layer to project the MTS data into a higher-\ndimensional space along the spatial dimension. As a result,\nthe values of variables at a single time step are represented\nas a vector in this higher-dimensional space. As for temporal\ndependencies, their scales are critical in achieving accurate\nMTS forecasting. However, most existing methods are con-\nfined to capturing temporal dependencies solely at a single\nscale. For example, Informer [5] aims to model temporal\ndependency between individual time steps in the time se-\nries sequence. Autoformer [6] proposes an auto-correlation\nmechanism to capture temporal dependencies among sub-\nseries. PatchTST [7] and Crossformer [8] introduce a patch\nprocedure that divides each series within the MTS data into\npatches of a specific length, enabling the use of a canonical\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntransformer to model temporal dependencies at the sub-series\nlevel.\nFew methods have been proposed to model multi-scale\ntemporal dependencies. SCINet [9] leverages an SCI-Block,\nwhich employs downsampling techniques to divide the input\nsequence into two sub-sequences. This division enables the\nextraction of distinctive temporal relations at a coarser scale\nby utilizing two convolution neural network (CNN) kernels.\nBy arranging multiple SCI-Blocks in a hierarchical tree struc-\nture across multiple levels, SCINet models temporal relations\nat various scales. MICN [10] also incorporates downsampling\ntechniques, involving the reduction of the original MTS data’s\nresolution. Then, a multi-scale isometric convolution layer,\ncomprising multiple branches of the local-global module,\nprocesses downsampled MTS data of varying scale sizes.\nPyraformer [11] introduces a pyramidal graph for modeling\nMTS data at different resolutions. This pyramidal graph em-\nbodies a tree structure in which each parent node has sev-\neral child nodes. The parent nodes summarize the sub-series\nof all child nodes. Thus the scale increases exponentially\nat the base of the number of child nodes. Crossformer [8]\ncomprises three transformer encoder-decoder pairs that aim\nto capture temporal dependencies within MTS data at three\nscales. Crossformer merges the latent representations from\nthe lower level as input for the next level, leading to a doubling\nof the scale as the levels progress.\nScales: 1 2 3 4 5 Time\nTime\nValueValue\nMulti-Scales with base 2:\nUnconstrained multi-scales\nFIGURE 1. Illustration of the multi-scale mechanisms in baseline methods\nand MTPNet.\nHowever, all those methods suffer from a limitation\nwherein their multiple scales increase exponentially (most\nwith base 2) as shown in Figure 1. For instance, beginning\nwith a seasonality of 1 hour, current methods scale up in\nincrements of 2 hours, 4 hours, 8 hours, 16 hours, 32 hours,\nand so on. Consequently, this approach overlooks the crucial\ndaily seasonality and potentially other seasonalities that fall\nwithin the one to 24-hour range. Consequently, they may\nfail to capture certain scales of temporal dependencies in\nMTS data that are crucial for accurate forecasting tasks.\nThis limitation underscores the importance of developing\nmore flexible and adaptable approaches capable of effectively\nmodeling temporal dependencies across a wider range of\narbitrary scales within the MTS data. To address the afore-\nmentioned limitations, we propose Multi-scale Transformer\nPyramid Networks (MTPNet) that effectively model temporal\ndependencies at multiple unconstrained scales as shown in\nFigure 1. The MTPNet facilitates the setting of scales tailored\nto the specific characteristics of MTS data. The contributions\nof our work are summarized as follows:\n• We propose a dimension invariant (DI) embedding\nmechanism that captures short-term temporal dependen-\ncies and projects the MTS data into a high-dimensional\nspace. Notably, this DI embedding technique preserves\nboth the spatial and temporal dimensions of the MTS\ndata. The DI embedding technique further partitions the\nembedded feature maps along the temporal dimension.\nConsequently, it enables the transformer to model tem-\nporal dependencies at a designated scale.\n• We propose a multi-scale transformer-based pyramid\nthat effectively models temporal dependencies across\nmultiple unconstrained scales, thereby offering the ver-\nsatility to capture temporal patterns at various resolu-\ntions. The MTPNet comprises multiple transformers,\neach employing the DI embedding with varying patch\nsizes, to effectively model multi-scale temporal depen-\ndencies.\n• We evaluate the proposed MTPNet using nine real-world\ndatasets, and the experimental results demonstrate its\nsuperior performance compared to recent state-of-the-\nart methods.\nII. RELATED WORKS\nA. MTS FORECASTING\nThe primary objective of the MTS forecasting task is to es-\ntablish an accurate inference between historical observations\nX ∈ RI×D of D variables within a look-back window of I\ntime steps and future H time steps’ values of Xpred ∈ RH×D.\nTraditional statistical methods like ARIMA [12] and expo-\nnential smoothing [13] are confined to forecast univariate\ntime series. While V AR and V ARMA [14] can be extended\nto multivariate time series (MTS) data, their performance\ndiminishes as the prediction length H increases. Advances in\ndeep learning have greatly enhanced the development of MTS\nforecasting. LSTNet [15] and TPA-LSTM [16] combine CNN\nand RNN to capture short-term and long-term temporal de-\npendencies. MTGNN [17] introduces a graph neural network\nframework explicitly designed to model spatial dependencies\namong variables in MTS data. While these methods are based\non various neural network architectures, their common ob-\njective is to discover forecasting inferences through iterative\nweight adjustments that minimize the discrepancies between\nthe forecasts and the ground truth.\nB. TRANSFORMERS\nTransformers were first developed for natural language pro-\ncessing [18]–[20] and soon achieved great success in com-\nputer vision [21], [22] and MTS forecasting [4]–[8]. The\ncanonical transformer architecture includes a self-attention\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmechanism and a feed-forward network. To enhance the train-\ning process, it employs residual connections [23] and layer\nnormalization [24].\nEarly studies that applied transformers in MTS forecasting\nfocused on modeling temporal dependencies at individual\ntime step resolution. These approaches encountered quadratic\ntime complexity issues, which impose limitations on the input\nlength—a crucial factor for MTS forecasting. Informer [5]\nproposed ProbSparse self-attention reduced time complexity\nto O(n log n) by only calculating a subset of queries. Fed-\nformer [25] enhanced transformers with Fourier transforms\nand Wavelet transforms and achieved linear computational\ncomplexity and memory cost. Several recent studies [7], [8]\nhave adopted the patch mechanism introduced in ViT [21]\nto partition the MTS data into patches, thus facilitating the\ntransformer’s efficacy in managing extended input sequences\nand capturing temporal dependencies at the sub-series level.\nDespite the successful application of transformers in MTS\nforecasting, existing methods have limitations in capturing\ntemporal dependencies at various constrained scales. This can\nhinder their ability to effectively capture seasonality patterns\nat arbitrary scales.\nIII. METHOD\nA. DECOMPOSITION\nWe decompose the MTS data into seasonal and trend-cyclical\ncomponents following [6], [25], [26]. Given MTS input X ∈\nRI×D, the decomposition procedure is as follows:\nXt = mean\n nX\ni=1\nMovingAvg (Padding (X))i\n!\nXs = X − Xt\n(1)\nWhere Xs ∈ RI×D and Xt ∈ RI×D are seasonal and trend-\ncyclical components, respectively.\nFigure 2 illustrates our proposed framework, incorporating\nseasonal and trend models to learn and forecast the seasonal\nand trend-cyclical components, respectively. The MTPNet\nfunctions as the seasonal model, while a simple linear layer\nis employed as the trend model to infer predictions directly\nfrom historical records. In scenarios where the MTS data\nlacks distinct seasonality and trend, we use MTPNet as the\ntrend model to effectively learn intricate trend-cyclical com-\nponents. Finally, the predictions from both the seasonal and\ntrend models are summed elementwise to derive the final\nMTS predictions.\nB. TRANSFORMER FEATURE PYRAMID\nTo address the limitations of a transformer that captures\ntemporal dependencies solely at a single scale, we propose\na multi-scale transformer pyramid network, as depicted in\nFigure 4. The primary objective of the MTPNet is to capture\ntemporal dependencies across diverse unconstrained scales,\nranging from fine to coarse resolutions. Notably, the total\nnumber of levels, denoted by K, is not fixed but depends on\nthe array of available patch sizes, where k = 1, ··· , K. This\nhierarchical architecture empowers MTPNet to model multi-\nscale representations of the complex temporal dependencies\nwithin the input sequence.\nAs illustrated in Figure 4, transformers at all levels take\nthe MTS sequence as input, which is referred to as all-scale\ninputs. Note that the decoder inputs are omitted in Figure 4\nfor brevity and details are discussed later. The DI embedding\ncomponents are distinctive at each level, as they partition\ninput MTS data into patches of unconstrained lengths of\npk ∈ {p1, ··· , pK }. Consequently, the multi-level transform-\ners focus on capturing the temporal dependencies at scales\nfrom fine to coarse.\nThe inter-scale connections facilitate information flow be-\ntween transformers at different levels within the pyramid\narchitecture. Encoders and decoders are symmetrically struc-\ntured, with encoders adopting a bottom-up approach and\ndecoders following a top-down pattern. This design allows\nencoders to progressively learn latent representations from\nfine to coarse scales, while decoders generate fine-scale rep-\nresentations guided by coarse-scale levels. This yields K\nlatent representations from the feature pyramid. Finally, a 1-\nlayer CNN generates predictions from the concatenated K\nlatent representations.\nC. DIMENSION INVARIANT EMBEDDING\nThis section introduces the DI embedding technique and em-\nphasizes the significance of maintaining both spatial and tem-\nporal dimensions intact. Due to the inherent lack of seman-\ntic information in MTS data compared to words or images,\ntransformer-based methods for MTS forecasting commonly\ngroup values either along the spatial dimension (variables) or\nthe temporal dimension (time steps) for further analysis.\nFigure 3 shows the workflow of spatial, temporal, and DI\nembedding techniques. The spatial embedding [5] employs a\nlinear layer to project the values of all variables at a single\ntime step into an alternately dimensional space (e.g., 64,\n128) while preserving the temporal dimension invariant. The\ntemporal embedding [7], [8] preserves the spatial dimension’s\ninvariance while employing a linear layer to embed values\nof a variable at multiple time steps into a higher-dimensional\nspace. Both embedding techniques break one dimension of\nthe MTS data: spatial embedding mixes spatial information,\nwhile temporal embedding restricts the temporal scale.\nTo avoid these disadvantages, we introduce the DI embed-\nding technique which utilizes a 1-layer CNN with a kernel\nsize of 3 × 1 to embed the MTS data into feature maps while\npreserving both spatial and temporal dimensions invariant as\nfollows:\nXemb = Conv (Xinput ) (2)\nwhere the Xinput ∈ R1×I×D is either Xs (seasonal) or Xt\n(trend), Xemb ∈ Rc×I×D is the embedded feature maps. The\n3×1 CNN kernel captures local temporal dependencies while\nkeeping variables independent. The Conv also captures short-\nterm temporal dependencies.\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDecomposition\nInputs:\nMTPNet\nLinear\nSeasonal Model\nTrend Model\nFIGURE 2. Illustration of the overall framework: Decomposition of MTS data into seasonal and trend-cyclical components, employing Multi-scale\nTransformer Pyramid Networks (MTPNet) as the seasonal model and a linear layer as the trend model. The seasonal and trend predictions are summed to\nobtain the final predictions.\nPad\nTemporal Embedding.\nc\nConv\nSpatial Embedding. Dimension Invariant (DI) Embedding.\nLinear\nc\nPatch\nLinear\nPatch\nPad\nPatch\nFIGURE 3. Illustration of spatial, temporal, and dimension invariant embedding techniques.\nThe DI embedding then applies the Patch procedure to\nthe embedded inputs, generating patched inputs at scale p as\nfollows:\nXemb = Patch (Xemb, X0, p) (3)\nwhere X0 is zero-padding if the length of the time series\nis not divisible by the patch size p. The Patch procedure\ndivides the time series into N = ⌈I/p⌉ non-overlapping\npatches of size p, yielding Xemb ∈ Rc×N×p×D, as shown\nin Figure 3. In contrast to Vision Transformers [27], which\npartition an image across height and width dimensions, MTS\ndata require division solely along the time step dimension.\nConsequently, the embedded feature maps transformed to\nN × p along the time step dimension, diverging from the\noriginal total of I time steps. It is important to note that\nthe dimensions of the feature map and variables remain un-\nchanged. The DI embedding enhances forecasting accuracy\nby enabling the model to learn temporal dependencies at\nmultiple unconstrained sub-series levels. This advancement is\ndriven by two primary factors: firstly, it allows MTS data to be\nanalyzed independently of variables; secondly, it empowers\nthe model to capture dependencies among time series patches\nof varying sizes. In contrast, spatial embedding techniques\nlimit the model’s ability to capture temporal dependencies at\nthe individual time step level and require combining multiple\nvariables. Temporal embedding typically generates patches of\na fixed size, thus constraining the machine learning model to\noperate at either a single scale or at multiple scales that grow\nexponentially.\nD. TRANSFORMER ENCODER AND DECODER\nThe MTPNet comprises multiple transformer encoder-\ndecoder pairs, designed to learn temporal dependencies at\nmultiple unconstrained scales. Figure 4 illustrates the de-\ntailed computation procedure of one level of the MTPNet.\nThe multiple transformer pyramid architecture in our de-\nsign overcomes the constraints observed in Informer and\nPatchTST, where a single transformer is employed to model\na singular scale. Existing methods like Crossformer, MICN,\nand Pyraformer aim to model scales that grow exponentially\nthrough the concatenation of fine-scale patches to generate\ncoarse-scale patches. Our proposed MTPNet concatenates\nembeddings along the feature dimension, and then each trans-\nformer within the pyramid utilizes DI embedding to partition\nthe feature embeddings to a designated scale. This ensures\nthat the scale at each level remains independent, enhancing\nthe model’s versatility and effectiveness.\nWe take the k-th level of MTPNet as an illustrative exam-\nple to elaborate on the detailed computation process of the\ntransformer encoder-decoder pair.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nEncoder Decoder\nEncoder Decoder\nEncoder Decoder\nEncoder Decoder\n1\n2\nK\nK-1\nk\nInputs\nConcat\n1x1 Conv\n1x1 Conv\nTransformer\nEncoder\nConcat\nDI Embed pos\nDI Embed\n1x1 Conv\nTransformer\nDecoderpos\nConcat\nOutputs\nFIGURE 4. Left:The workflow of a single-level transformer-based encoder-decoder pair.Right: Illustration of the proposed multi-scale transformer\npyramid network (MTPNet).\n1) Encoder\nThe input Xenc ∈ R1×I×D are the same for the encoder at any\nlevel in the MTPNet. The DI embedding at k-th level takes\ninput MTS data and patch size pk as follows:\nXk\ndi = DI (Xenc, pk ) (4)\nwhere Xk\ndi ∈ Rc×Nk\nenc×pk ×D represents the embeded and\npatched k-th level encoder’s input. Then, the inter-scale con-\nnections concatenate and fuse input embedding Xk\ndi with\nlower level encoder’s output Hk−1\nenc as follows:\nXk\nemb =\n(\nXk\ndi if k = 1,\nConv\n\u0000\nConcat\n\u0000\nXk\ndi, Hk−1\nenc\n\u0001\u0001\nif k > 1. (5)\nwhere Concat denote concatenate process along the feature\nmap dimension ( c) and Conv represents 1-layer CNN with\na kernel size of 1 × 1 to fuse and reduce the concatenated\nfeature map dimension from 2c to c. Note that the numbers\nof patches Nk\nenc and Nk\nenc − 1 differ. To address this, we apply\nan inverse patch operation to reassemble the patches into a\ncomplete sequence before concatenating them. Subsequently,\nwe partition the concatenated embeddings into patches of size\npk . To incorporate positional information, we add a learnable\nposition embedding Wk\npos (denoted pos) to input embedding\nas follows:\nXk\nemb = Xk\nemb + Wk\npos (6)\nThis step is critical because the inherent nature of the Trans-\nformer architecture is order-agnostic. Therefore, positional\ninformation needs to be incorporated to capture the temporal\ndependencies within the input sequence. The input embed-\nding Xk\nemb is split into univariate embeddings. Therefore, we\nobtain the transformer encoder’s input Xk,d\nemb ∈ Rc×Nk\nenc×pk ×1\nwhich represents the d-th variable.\nWe employ the canonical transformer encoder [18], utiliz-\ning the scaled dot-product attention mechanism as follows:\nQ, K, V = Linear\n\u0010\nXk,d\nemb\n\u0011\nAttention (Q, K, V ) = Softmax\n\u0010\nQKT /\np\ndk\n\u0011\nV\n(7)\nwhere the Q, K, and V are query, keys, and values embedded\nfrom the input sequence of d-th series of Xk,d\np ∈ Rc×Nk\nenc×pk .\nNote that we flatten the feature map and patch size dimension\nof Xk,d\np so that Xk,d\np ∈ RNk\nenc×(c×pk ) represents the latent\nrepresentations of patches of size pk . We also utilize the\ncanonical multi-head attention as follows:\nQh, Kh, Vh = Linear (Q, K, V )h\nHk,d\nh = Attention (Qh, Kh, Vh)\nHk,d\nenc = Linear\n\u0000\nConcat\n\u0000\nHk\n1, ··· , Hk\nh, ···\n\u0001\u0001 (8)\nwhere the Hk,d\nenc ∈ Rc×Nk\nenc×pk is the output of transformer\nencoder at level k for d-th series, and the subscript h indicates\nthe h-th head of multi-head attention. By applying the encoder\nto all D series in the MTS data, we obtained the k-th level’s en-\ncoder output Hk\nenc ∈ Rc×Nk\nenc×pk ×D. The transformer encoder\nalso includes normalization layers, a feed-forward network,\nand residual connections, details are available in [18].\nThe last step of the encoder is skip-connection as follows:\nHk\nenc = Hk\nenc + Xk\ndi (9)\nwhere Hk\nenc ∈ Rc×Nk\nenc×pk ×D is the output of k-th level\nencoder.\n2) Decoder\nThe decoder’s input Xdec ∈ R(L+H)×D is the concatenation\nof the historical records ( L time steps) and zero-padding\n(H future time steps). Similar to the encoder’s workflow\npresented in Equations 4, 5, and 6, the decoder’s input\ngoes through DI embedding and inter-scale connections to\nobtain a patched embedding for the transformer decoder. It is\nworth mentioning that the decoder’s inter-scale connections\nfollow a top-down order, thus the decoder’s output latent\nrepresentations flow from coarse-scale to fine-scale. Then a\nlearnable position embedding (denoted pos) is added to the\ninput embedding. Consequently, we obtain the transformer\ndecoder’s input, denoted Xk\ndec ∈ Rc×Nk\ndec×pk ×D. The decoder’s\nnumber of patches is Nk\ndec = ⌈(L + H)/pk ⌉.\nWe also employ the canonical transformer decoder [18],\nutilizing the scaled dot-product attention and multi-head at-\ntention presented in Equations 7 and 8. The decoder also\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nincludes normalization layers, a feed-forward network, and\nresidual connections, as described in [18]. The last step of the\ndecoder is also a skip connection as presented in Equation 9.\nThe output of the decoder at k-th level is Hk\ndec ∈\nRc×Nk\ndec×pk ×D. To conduct the MTS forecasting task, we only\nneed the latent representations of future time steps. Therefore,\nwe only need the last Nk\npred = ⌈H/pk ⌉ patches that presenting\nfuture time steps, denoted Hk\ndec ∈ Rc×Nk\npred ×pk ×D. At each\nlevel, the latent representations represent the predicted value\nusing a vector of length c, containing temporal dependencies\nat a scale of patch size pk .\nE. MULTI-SCALE PREDICTION\nLastly, we concatenate latent representations of all K levels\nand generate predictions using a Conv layer as follows:\nH = Concat\n\u0000\nH1\ndec, ··· , HK\ndec\n\u0001\nXpred = Conv (H) (10)\nTo concatenate latent representations at different scales, we\napply an inverse patch operation to reassemble patches into\nthe complete sequence. Subsequently, all K latent representa-\ntions are concatenated, resulting in H ∈ R(K×c)×H×D. Each\npredicting future value is represented as a vector of length\nK × c, which captures the temporal dependencies at scales\nranging from p1 to pK . The Conv layer project each vector\nof length K × c into predicting value, generating the predic-\ntions Xpred ∈ RH×D. The process of inferring predictions\nfrom multi-scale latent representations enables the effective\nutilization of temporal dependencies across a wide range of\nunconstrained scales, ultimately enhancing forecasting accu-\nracy.\nIV. EXPERIMENTS\nA. EXPERIMENTAL SETTINGS\n1) Data\nIn our experiments, we employed nine benchmark datasets [6].\nThese datasets are ETTh1, ETTh2, ETTm1, ETTm2,\nWeather, Traffic, Electricity, Exchange-Rate, and ILI, all of\nwhich are publicly accessible. Each dataset encompasses a\nrange of variables, exhibiting diverse features. The sampling\nfrequency of these datasets varies substantially, ranging from\nevery 10 minutes to weekly intervals. This variation intro-\nduces differing degrees of temporal dependencies within each\ndataset. A summary of the characteristics of these nine bench-\nmark datasets, integral for our methodological evaluation, is\npresented below:\n• ETT1: The Electricity Transformer Temperature (ETT)\nis essential for long-term electric power infrastructure\nplanning. The Informer study [5] compiled data from\ntwo Chinese counties’ electricity transformers, focusing\non seven indicators such as oil temperature and load.\nThe ETTh1 and ETTh2 datasets feature 17,420 hourly\nsamples each, while ETTm1 and ETTm2 have 69,680\nsamples recorded every 15 minutes.\n1https://github.com/zhouhaoyi/ETDataset\n• Traffic2: This dataset, obtained from the California De-\npartment of Transportation, includes road occupancy\nrates (0 to 1) from 862 freeway sites in the San Francisco\nBay area, covering over a decade. Lai [28] compiled\nhourly data for 48 months (2015-2016), yielding 17,544\nsamples.\n• Electricity3: This dataset from the UCI Machine Learn-\ning Repository features hourly electricity consumption\ndata for 321 clients from 2012 to 2014, amounting to\n26,304 samples.\n• Weather4: This dataset includes 21 meteorological in-\ndicators recorded every 10 minutes over a year in Ger-\nmany, totaling 52,696 samples.\n• Exchange-Rate5: This dataset covers 27 years (1990-\n2016) and contains daily exchange rates for eight ma-\njor economies: Australia, Britain, Canada, Switzerland,\nChina, Japan, New Zealand, and Singapore, totaling\n7,588 samples.\n• ILI6: This dataset includes patient data with seven indi-\ncators from 2002 to 2021, sampled weekly, totaling 966\nsamples, and is distinguished by its unique forecasting\nhorizon setting.\nWe divided each dataset into training, validation, and test\nsubsets using a 0.6:0.2:0.2 ratio for the four ETT datasets and\na 0.7:0.1:0.2 ratio for the other five datasets.\n2) Implementation details\nFor training, we utilize the Adam optimizer and Cosine\nAnnealing scheduler with an initial learning rate ranging\nbetween 1e − 5 and 1e − 3 and set the batch size to\n32 while using L1 loss. The patch sizes of MTPNet are\nselected from {4, 6, 8, 12, 24, 32, 48, 96} via grid search.\nThe look-back window sizes of MTPNet are selected from\n{96, 192, 336, 720} through grid search, except for the ILI\ndataset, where it was set to 104. The transformers of MTPNet\nconsist of 2 encoder layers and 1 decoder layer. The multi-\nhead attention number is set to 4. The MTPNet is imple-\nmented with PyTorch and runs with an NVIDIA GeForce\nRTX 3090 GPU and an NVIDIA RTX A6000 GPU. The\nmain results are averaged across six runs with distinct seeds:\n1, 2022, 2023, 2024, 2025, and 2026. Additionally, ablation\nstudies were conducted using seed 1.\n3) Evaluation and baselines\nWe selected seven state-of-the-art (SOTA) baseline methods\nas follows:\n• Transformer-based methods: Informer [5], Pyraformer [11],\nFEDformer [25], Crossformer [8], PatchTST [7].\n• Linear methods: DLinear [26].\n• CNN methods: MICN [10].\n2http://pems.dot.ca.gov\n3https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\n4https://www.bgc-jena.mpg.de/wetter/\n5https://github.com/laiguokun/multivariate-time-series-data\n6https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWe obtained the results of DLinear, Pyraformer, Fed-\nformer, and Informer from Dlinear [26]. The results for\nPatchTST and Crossformer were obtained from the original\npaper. In cases where results are not available, we conducted\nexperiments using the optimal hyperparameters as presented\nin the original papers. In particular, for a fair comparison, we\ndid not fix the input length to 96, as recent studies [7], [8], [26]\nhave highlighted the significance of optimal input lengths for\nmethod performance.\nWe employed Mean Squared Error (MSE) and Mean Ab-\nsolute Error (MAE) as quantitative metrics for assessing\nforecasting accuracy. These measures are consistently used\nacross all baseline methods, providing a standardized basis\nfor comparison and evaluation. Consequently, the results of\nthe baseline methods presented in the experiments represent\ntheir respective best performances.\nB. MAIN RESULTS\nTable 1 shows the main experimental results of all methods for\nnine datasets on MSE and MAE, where the best and second-\nbest results for each case (dataset, horizon, and metric) are\nhighlighted in bold and underlined, respectively. The MTP-\nNet outperforms SOTA baseline methods, achieving 45 best\nresults and 19 second-best results out of 72 cases. MTPNet\nachieves a modest enhancement in accuracy when compared\nwith the best existing method, PatchTST. Compared to DLin-\near, which raised questions about the effectiveness of trans-\nformers in MTS forecasting, MTPNet demonstrates a reduc-\ntion of 7.04% in MSE and 8.56% in MAE. Notably, PatchTST\nand Dlinear are limited to modeling temporal dependencies\nat a fixed scale. In contrast, the implementation of a multi-\nscale transformer pyramid architecture in our proposed MTP-\nNet has enhanced forecasting accuracy. This highlights the\nefficacy of modeling multi-scale temporal dependencies in\npredictive tasks.\nCompared to methods that model temporal dependencies\nacross multiple scales that exponentially increase, MTPNet\nsignificantly enhances forecasting accuracy. In comparison\nwith MICN, MTPNet exhibited a performance enhancement\nof 19.53% in MSE and 16.72% in MAE. Against Cross-\nformer, it achieved noteworthy reductions in MSE and MAE,\naveraging 39.84% and 30.32%, respectively. Additionally,\nMTPNet demonstrates a substantial decrease in MSE and\nMAE by 64.35% and 51.63%, showcasing its efficacy. These\nfindings highlight the critical role of effectively modeling un-\nconstrained multi-scale temporal dependencies in forecasting\ntasks.\nThe MTPNet, designed to model temporal dependencies\nfrom fine to coarse scales, exhibits increased proficiency\nwith datasets containing high-frequency information (e.g.,\nsampled every 10 minutes). Consequently, MTPNet achieved\nthe best results in 22 out of 24 cases for datasets like ETTm1\n(15 minutes), ETTm2 (15 minutes), and Weather (10 min-\nutes). Conversely, MTPNet’s performance was marginally\nsurpassed by PatchTST and Dlinear in the Exchange (1 day)\nand ILI (1 week) datasets. This is attributed to the lack of fine-\nscale information in these particular datasets. We conclude\nthat MTPNet exhibits advantages in forecasting datasets that\nencompass rich information spanning from fine to coarse\nscales. It is worth noting that the average standard deviations\nof MTPNet across all cases, with six different random seeds,\nare 0.036 and 0.009 for MSE and MAE, respectively, demon-\nstrating its robustness against randomness.\nC. ABLATION STUDY\n1) How important is DI embedding?\nTable 2 presents a comparison of DI embedding with spatial\nembedding and temporal embedding. The mechanisms of\nspatial and temporal embedding are illustrated in Figure 3.\nThe DI embedding consistently outperformed the spatial em-\nbedding mechanism. For the horizon of 96, the MSE and\nMAE values of the spatial embedding were 9.09% and 5.6%\nhigher, respectively, compared to the DI embedding. The\nperformance gap increased further for the horizon of 720,\nwith the spatial embedding showing 32.11% higher MSE and\n12.95% higher MAE compared to the DI embedding. The\ntemporal embedding slightly degrades the MSE and MAE\nvalues for ETTh1, ETTm1, and Weather datasets by 3.9%\nand 3.8%, respectively. Notably, the temporal embedding\nachieved the best performance for the Exchange-Rate dataset.\nWe conjecture that this is because the Exchange-Rate dataset\ninherently exhibits less temporal dependence. In conclusion,\nour findings demonstrate that DI embedding outperforms\nboth spatial and temporal embeddings. Furthermore, breaking\nthe dimensionality of MTS data leads to a degradation in\nperformance.\n2) How important is multi-scale temporal dependency\nlearning?\nTo evaluate the effectiveness of multi-scale temporal depen-\ndency learning, we present experimental results of single-\nscale MTPNet of coarse (large patch size) and fine (small\npatch size) in Table 3. Both modifications performed worse\nthan the multi-scale design. Specifically, MTPNet-Fine ex-\nhibited a more substantial performance drop than MTPNet-\nCoarse, showing the challenge of capturing meaningful tem-\nporal dependencies from a small number of time steps due\nto time series data’s naturally low semantic characteristics.\nFurthermore, the multi-scale transformer pyramid architec-\nture consistently outperformed individual fixed scales. This\nobservation emphasizes the critical importance of the multi-\nscale transformer pyramid design in the context of MTS\nforecasting.\n3) How important are inter-scale connections?\nWe modified the MTPNet by removing the inter-scale con-\nnections, resulting in no information flow between multi-\nple levels. Each transformer level now receives input solely\nfrom the embedded input sequence patches of fixed scale.\nThe results are presented in Table 3 as ‘‘w/o inter-scale.\"\nSurprisingly, the performance of MTPNet without inter-scale\nconnections was even improved by a trivial amount. We\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Quantitative evaluation (MSE/MAE) of state-of-the-art multivariate time series forecasting methods on nine datasets. The forecasting horizons\ninclude 24, 36, 48, 96 for the ILI dataset, and 96, 192, 336, 720 for the others. Bold results indicate the best performance while underlined results\nrepresent the second-best performance.\nMethods MTPNet PatchTST/64 DLinear Crossformer MICN Pyraformer Fedformer Informer\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1 96 0.364 0.385 0.370 0.400 0.375 0.399 0.431 0.458 0.421 0.431 0.664 0.612 0.376 0.419 0.865 0.713\n192 0.404 0.410 0.413 0.429 0.405 0.416 0.420 0.448 0.474 0.487 0.790 0.681 0.420 0.448 1.008 0.792\n336 0.431 0.432 0.422 0.440 0.439 0.443 0.440 0.461 0.569 0.551 0.891 0.738 0.459 0.465 1.107 0.809\n720 0.453 0.463 0.447 0.468 0.472 0.490 0.519 0.524 0.770 0.672 0.963 0.782 0.506 0.507 1.181 0.865\nETTh2 96 0.278 0.335 0.274 0.337 0.289 0.353 1.177 0.757 0.299 0.364 0.645 0.597 0.358 0.397 3.755 1.525\n168 0.340 0.376 0.341 0.382 0.383 0.418 1.206 0.796 0.441 0.454 0.788 0.683 0.429 0.439 5.602 1.931\n336 0.365 0.403 0.329 0.384 0.448 0.465 1.452 0.883 0.654 0.567 0.907 0.747 0.496 0.487 4.721 1.835\n720 0.400 0.435 0.379 0.422 0.605 0.551 2.040 1.121 0.956 0.716 0.963 0.783 0.463 0.474 3.647 1.625\nETTm1 96 0.291 0.332 0.293 0.346 0.299 0.343 0.320 0.373 0.316 0.362 0.543 0.510 0.379 0.419 0.672 0.571\n192 0.332 0.355 0.333 0.370 0.335 0.365 0.400 0.432 0.363 0.390 0.557 0.537 0.426 0.441 0.795 0.669\n336 0.367 0.376 0.369 0.392 0.369 0.386 0.408 0.428 0.408 0.426 0.754 0.655 0.445 0.459 1.212 0.871\n720 0.425 0.410 0.416 0.420 0.425 0.421 0.582 0.537 0.481 0.476 0.908 0.724 0.543 0.490 1.166 0.823\nETTm2 96 0.164 0.248 0.166 0.256 0.167 0.260 0.444 0.463 0.179 0.275 0.435 0.507 0.203 0.287 0.365 0.453\n192 0.223 0.291 0.223 0.296 0.224 0.303 0.833 0.657 0.307 0.376 0.730 0.673 0.269 0.328 0.533 0.563\n336 0.273 0.325 0.274 0.329 0.281 0.342 0.766 0.620 0.325 0.388 1.201 0.845 0.325 0.366 1.363 0.887\n720 0.356 0.380 0.362 0.385 0.397 0.421 0.959 0.752 0.502 0.490 3.625 1.451 0.421 0.415 3.379 1.338\nWeather\n96 0.146 0.189 0.149 0.198 0.176 0.237 0.158 0.231 0.161 0.229 0.622 0.556 0.217 0.296 0.300 0.384\n192 0.188 0.230 0.194 0.241 0.220 0.282 0.194 0.262 0.220 0.281 0.739 0.624 0.276 0.336 0.598 0.544\n336 0.238 0.271 0.245 0.282 0.265 0.319 0.495 0.515 0.278 0.331 1.004 0.753 0.339 0.380 0.578 0.523\n720 0.310 0.322 0.314 0.334 0.323 0.362 0.526 0.542 0.311 0.356 1.420 0.934 0.403 0.428 1.059 0.741\nTraffic\n96 0.401 0.234 0.360 0.249 0.410 0.282 0.538 0.300 0.519 0.309 0.867 0.468 0.587 0.366 0.719 0.391\n192 0.431 0.247 0.379 0.256 0.423 0.287 0.515 0.288 0.537 0.315 0.869 0.467 0.604 0.373 0.696 0.379\n336 0.453 0.259 0.392 0.264 0.436 0.296 0.530 0.300 0.534 0.313 0.881 0.469 0.621 0.383 0.777 0.420\n720 0.491 0.284 0.432 0.286 0.466 0.315 0.573 0.313 0.577 0.325 0.896 0.473 0.626 0.382 0.864 0.472\nElectricity\n96 0.128 0.219 0.129 0.222 0.140 0.237 0.141 0.240 0.164 0.269 0.386 0.449 0.193 0.308 0.274 0.368\n192 0.146 0.237 0.147 0.240 0.153 0.249 0.166 0.265 0.177 0.177 0.378 0.443 0.201 0.315 0.296 0.386\n336 0.164 0.256 0.163 0.259 0.169 0.267 0.323 0.369 0.193 0.304 0.376 0.443 0.214 0.329 0.300 0.394\n720 0.203 0.293 0.197 0.290 0.203 0.301 0.404 0.423 0.212 0.321 0.376 0.445 0.246 0.355 0.373 0.439\nExchange\n96 0.091 0.215 0.896 0.209 0.081 0.203 0.323 0.425 0.102 0.235 1.748 1.105 0.148 0.278 0.847 0.752\n192 0.175 0.301 0.187 0.308 0.157 0.293 0.448 0.506 0.172 0.316 1.874 1.151 0.271 0.380 1.204 0.895\n336 0.280 0.389 0.349 0.432 0.305 0.414 0.840 0.718 0.272 0.407 1.943 1.172 0.460 0.500 1.672 1.036\n720 0.613 0.606 0.900 0.715 0.643 0.601 1.416 0.959 0.714 0.658 2.085 1.206 1.195 0.841 2.478 1.310\nILI\n24 1.602 0.837 1.319 0.754 2.215 1.081 3.041 1.186 2.684 1.112 7.394 2.012 3.228 1.260 5.764 1.677\n36 1.371 0.761 1.579 0.870 1.963 0.963 3.406 1.232 2.667 1.068 7.551 2.031 2.679 1.080 4.755 1.467\n48 1.371 0.822 1.553 0.815 2.130 1.024 3.459 1.221 2.558 1.052 7.662 2.057 2.622 1.078 4.763 1.469\n60 1.696 0.884 1.470 0.788 2.368 1.096 3.640 1.305 2.747 1.110 7.931 2.100 2.857 1.157 5.264 1.564\nTABLE 2. Multivariate time series forecasting results of MTPNet with\nthree embedding mechanisms: dimension invariant embedding, spatial\nembedding, and temporal embedding.\nMethods DI Spatial Temporal\nMetric MSE MAE MSE MAE MSE MAE\nETTh1\n96 0.365 0.384 0.371 0.396 0.369 0.391\n720 0.455 0.464 0.554 0.506 0.494 0.499\nETTm1\n96 0.292 0.333 0.302 0.339 0.302 0.342\n720 0.424 0.409 0.431 0.410 0.428 0.416\nWeather 96 0.145 0.187 0.149 0.193 0.155 0.200\n720 0.312 0.324 0.316 0.326 0.320 0.331\nExchange 96 0.090 0.214 0.116 0.244 0.085 0.206\n720 0.581 0.592 1.185 0.840 0.523 0.548\nconjecture that this improvement may be due to time series\ndata’s naturally low semantic characteristics. As a result,\nMTS forecasting doesn’t require latent representation flow\nbetween levels to extract high semantic information.\n4) How important are inputs for all scales?\nThe ‘‘w/o inter-scale\" column in Table 3 presents the results\nof MTPNet without the inputs for all levels. In this configu-\nration, transformers, except for the first level, only take the\nlatent representation from the previous level as input. The\nperformance of MTPNet without all scale inputs dropped for\nETTh1 and ETTm1 datasets when the forecasting horizon\nwas 720 and for both horizons of Weather and Exchange-\nRate datasets. The latent representations from the lower level\nare grounded in a different temporal scale. In contrast, the\ndirect MTS data input integrates a DI embedding component,\npatching input at the specific scale of the current level. As\nthe forecasting horizon increases, the task grows more chal-\nlenging, emphasizing the greater significance of direct input.\nFrom the experiments, we conclude that direct MTS data\ninput is critical for more challenging forecasting scenarios.\n5) Top-down vs. Bottom-up\nThe ‘‘bottom-up\" column in Table 3 shows the results of\nMTPNet with a modification where the top-down latent\nrepresentation flow in decoders is replaced with a bottom-\nup approach. This reversal in information flow means that\neach transformer decoder’s input is a fusion of the input\nsequence and the latent representation of the lower layer (finer\nscale). This modification only degrades the performance of\nthe Exchange-Rate dataset. It is worth noting that MTPNet\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3. The ablation study results of MTPNet’s pyramid structure.\nMethods MTPNet w/o inter-scale w/o all-scale bottom-up Fine Coarse\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n96 0.365 0.384 0.365 0.385 0.364 0.386 0.362 0.383 0.400 0.423 0.375 0.390\n720 0.455 0.464 0.450 0.460 0.462 0.462 0.449 0.462 0.515 0.509 0.461 0.462\nETTm1\n96 0.292 0.333 0.291 0.332 0.292 0.333 0.292 0.333 0.307 0.346 0.301 0.339\n720 0.424 0.409 0.423 0.409 0.428 0.411 0.424 0.409 0.432 0.417 0.428 0.414\nWeather 96 0.145 0.187 0.148 0.187 0.152 0.191 0.149 0.195 0.158 0.203 0.148 0.190\n720 0.312 0.324 0.309 0.321 0.315 0.329 0.311 0.324 0.322 0.331 0.313 0.326\nExchange 96 0.090 0.214 0.087 0.208 0.100 0.223 0.096 0.220 0.093 0.218 0.093 0.217\n720 0.581 0.592 0.579 0.597 0.648 0.619 0.601 0.596 0.635 0.612 0.648 0.617\ngenerates predictions by utilizing a Conv layer to project\nfrom the latent representations of all K levels. Thus, the in-\nformation from all temporal scales is utilized when generating\npredictions. This finding indicates that both information flow\nfrom coarse to fine and from fine to coarse can enhance\nforecasting accuracy.\n6) How does the look-back window length affect the\nperformance?\nThe size of the input sequence plays a crucial role in MTS\nforecasting as it determines the amount of historical infor-\nmation that can be utilized. Theoretically, an extended input\nsequence is expected to enhance forecasting accuracy as it\nencompasses a greater volume of information. However, re-\ncent studies [7], [26] have shown that this assumption does\nnot always hold. Figure 5 illustrates the effect of the input se-\nquence length on the forecasting accuracy. For both horizons,\nno method consistently benefits from a longer input sequence\nacross all four datasets. In most cases, a method reaches an\noptimal input sequence length, and its performance degrades\nwhen the input sequence becomes longer. Notably, for the\nExchange-Rate dataset, a shorter input sequence appears to be\noptimal for most methods. We attribute this to the inherently\nlow temporal dependency in this dataset. In conclusion, the\noptimal input sequence length varies depending on the dataset\nand forecasting horizon.\nV. CONCLUSION\nTime series data often exhibit various scales of seasonality,\nand these temporal dependencies are crucial for accurate\nforecasting tasks. In this study, we proposed a multi-scale\ntransformer-based pyramid network for MTS forecasting.\nThe proposed MTPNet tackles the complexity of modeling\ntemporal dependencies across either a fixed scale or con-\nstrained multi-scales. The overall framework initially decom-\nposes MTS data into seasonal and trend components. A linear\nlayer is employed to directly generate predictions for the\ntrend component from its historical data. In parallel, MTPNet\nis employed to model temporal dependencies and generate\npredictions for the seasonal component. The DI embedding\nprocedure is utilized to segment the time series sequence into\npatches, where the size of each patch varies according to the\nlevel. Subsequently, MTPNet leverages multiple transformers\nto capture temporal dependencies across a range of uncon-\nstrained scales. These multi-scale latent representations are\nsubsequently concatenated, followed by the application of a\nCNN layer to generate predictions for the seasonal compo-\nnent. The final predictions are derived by summing the pre-\ndictions of the seasonal and trend components in an element-\nwise manner. Extensive experimental results demonstrate that\nMTPNet outperforms existing state-of-the-art methods, par-\nticularly those that aim to address the multi-scale temporal de-\npendency issue. Moving forward, we intend to further develop\na sparse attention mechanism as a substitute for the canonical\nattention mechanism. We aim to decrease the computational\ncomplexity from quadratic to linear by enabling each query to\nattend to a limited number of highly correlated keys. We are\ncommitted to further developing an application that employs\nour proposed methods to visualize meteorological MTS data\nin the Lake Tahoe region. This application aims to forecast\ncritical events, including snowstorms, icy road conditions,\nand wildfires.\nACKNOWLEDGMENT\nThis material is based in part upon work supported by:\nThe National Science Foundation under grant number(s)\nNSF awards 2142428, 2142360, OIA-2019609, and OIA-\n2148788. Any opinions, findings, and conclusions or recom-\nmendations expressed in this material are those of the au-\nthor(s) and do not necessarily reflect the views of the National\nScience Foundation.\nREFERENCES\n[1] W. Zheng and J. Hu, ‘‘Multivariate time series prediction based on tem-\nporal change information learning method,’’ IEEE Transactions on Neural\nNetworks and Learning Systems , 2022.\n[2] T.-Y . Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n‘‘Feature pyramid networks for object detection,’’ in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n2117–2125.\n[3] Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, ‘‘Trans-\nformers in time series: A survey,’’ 2023.\n[4] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X. Yan, En-\nhancing the Locality and Breaking the Memory Bottleneck of Transformer\non Time Series Forecasting. Red Hook, NY , USA: Curran Associates Inc.,\n2019.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.36\n0.40\n0.44\n0.48\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.20\n0.25\n0.30\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.2\n0.4\n0.6\n0.8\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\n0.5\n0.6\n0.7\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.4\n0.8\n1.2\n1.6\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\nMethod MTPNet\nPatchTST\nDLinear\nCrossformer\nMICN\nFedformer\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.36\n0.40\n0.44\n0.48\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.20\n0.25\n0.30\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.2\n0.4\n0.6\n0.8\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\n0.5\n0.6\n0.7\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.4\n0.8\n1.2\n1.6\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\nMethod MTPNet\nPatchTST\nDLinear\nCrossformer\nMICN\nFedformer\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.36\n0.40\n0.44\n0.48\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.20\n0.25\n0.30\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.2\n0.4\n0.6\n0.8\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\n0.5\n0.6\n0.7\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.4\n0.8\n1.2\n1.6\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\nMethod MTPNet\nPatchTST\nDLinear\nCrossformer\nMICN\nFedformer\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.36\n0.40\n0.44\n0.48\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.20\n0.25\n0.30\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.2\n0.4\n0.6\n0.8\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\n0.5\n0.6\n0.7\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.4\n0.8\n1.2\n1.6\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\nMethod MTPNet\nPatchTST\nDLinear\nCrossformer\nMICN\nFedformer\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.36\n0.40\n0.44\n0.48\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.20\n0.25\n0.30\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.2\n0.4\n0.6\n0.8\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\n0.5\n0.6\n0.7\n96 192 336 720 1080 1440\nInput length\nMAE\nETTh1\n0.40\n0.45\n0.50\n0.55\n0.60\n96 192 336 720 1080 1440\nInput length\nMAE\nETTm1\n0.35\n0.40\n96 192 336 720 1080 1440\nInput length\nMAE\nWeather\n0.4\n0.8\n1.2\n1.6\n96 192 336 720 1080 1440\nInput length\nMAE\nExchange\nMethod MTPNet\nPatchTST\nDLinear\nCrossformer\nMICN\nFedformer\nHorizon: 96Horizon: 720\nFIGURE 5. The forecasting results in terms of MAE for different look-back window sizes at horizons 96 and 720.\n[5] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\n‘‘Informer: Beyond efficient transformer for long sequence time-series\nforecasting,’’ in The Thirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Virtual Conference, vol. 35, no. 12. AAAI Press, 2021,\npp. 11 106–11 115.\n[6] H. Wu, J. Xu, J. Wang, and M. Long, ‘‘Autoformer: Decomposition\ntransformers with Auto-Correlation for long-term series forecasting,’’ in\nAdvances in Neural Information Processing Systems , 2021.\n[7] Y . Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, ‘‘A time series\nis worth 64 words: Long-term forecasting with transformers,’’ in Interna-\ntional Conference on Learning Representations , 2023.\n[8] Y . Zhang and J. Yan, ‘‘Crossformer: Transformer utilizing cross-dimension\ndependency for multivariate time series forecasting,’’ in International Con-\nference on Learning Representations , 2023.\n[9] M. Liu, A. Zeng, M. Chen, Z. Xu, Q. Lai, L. Ma, and Q. Xu, ‘‘Scinet:\nTime series modeling and forecasting with sample convolution and interac-\ntion,’’Thirty-sixth Conference on Neural Information Processing Systems\n(NeurIPS), 2022, 2022.\n[10] H. Wang, J. Peng, F. Huang, J. Wang, J. Chen, and Y . Xiao, ‘‘Micn: Multi-\nscale local and global context modeling for long-term series forecasting,’’\n2023.\n[11] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar,\n‘‘Pyraformer: Low-complexity pyramidal attention for long-range time\nseries modeling and forecasting,’’ in International Conference on Learning\nRepresentations, 2022.\n[12] G. E. Box and G. M. Jenkins, ‘‘Some recent advances in forecasting\nand control,’’ Journal of the Royal Statistical Society. Series C (Applied\nStatistics), vol. 17, no. 2, pp. 91–109, 1968.\n[13] R. Hyndman, A. B. Koehler, J. K. Ord, and R. D. Snyder, Forecasting with\nexponential smoothing: the state space approach . Springer Science &\nBusiness Media, 2008.\n[14] L. Kilian and H. Lütkepohl, Structural vector autoregressive analysis .\nCambridge University Press, 2017.\n[15] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, ‘‘Modeling long-and short-\nterm temporal patterns with deep neural networks,’’ in The 41st interna-\ntional ACM SIGIR conference on research & development in information\nretrieval, 2018, pp. 95–104.\n[16] S.-Y . Shih, F.-K. Sun, and H.-y. Lee, ‘‘Temporal pattern attention for\nmultivariate time series forecasting,’’ Machine Learning , vol. 108, pp.\n1421–1441, 2019.\n[17] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, ‘‘Connecting\nthe dots: Multivariate time series forecasting with graph neural networks,’’\nin Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , 2020.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, ‘‘Attention is all you\nneed,’’ in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper_files/\npaper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[19] S. Bano and S. Khalid, ‘‘Bert-based extractive text summarization of\nscholarly articles: A novel architecture,’’ in 2022 International Conference\non Artificial Intelligence of Things (ICAIoT) . IEEE, 2022, pp. 1–5.\n[20] S. Bano, S. Khalid, N. M. Tairan, H. Shah, and H. A. Khattak, ‘‘Summa-\nrization of scholarly articles using bert and bigru: Deep learning-based\nextractive approach,’’ Journal of King Saud University-Computer and\nInformation Sciences, vol. 35, no. 9, p. 101739, 2023.\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, ‘‘An image is worth 16x16 words: Transformers for image\nrecognition at scale,’’ ICLR, 2021.\n[22] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, ‘‘Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,’’ in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , 2021, pp. 568–578.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , June 2016.\n[24] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‘‘Layer normalization,’’ 2016.\n[25] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, ‘‘FEDformer:\nFrequency enhanced decomposed transformer for long-term series fore-\ncasting,’’ in Proc. 39th International Conference on Machine Learning\n(ICML 2022), 2022.\n[26] A. Zeng, M. Chen, L. Zhang, and Q. Xu, ‘‘Are transformers effective for\ntime series forecasting?’’ 2023.\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. , ‘‘An\nimage is worth 16x16 words: Transformers for image recognition at scale,’’\narXiv preprint arXiv:2010.11929 , 2020.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[28] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, ‘‘Modeling long-and short-\nterm temporal patterns with deep neural networks,’’ in The 41st interna-\ntional ACM SIGIR conference on research & development in information\nretrieval, 2018, pp. 95–104.\nYIFAN ZHANG received the bachelor’s degree\nin automation from Huainan Normal University,\nChina, in 2016, and the master’s degree in control\nscience and engineering from Nanjing University\nof Aeronautics and Astronautics, China, in 2019.\nHe is now a Ph.D. student at the Department of\nComputer Science and Engineering, University of\nNevada, Reno, USA. His research interests include\nmachine learning, time series data analysis, and\ncomputer vision.\nRUI WU received the bachelor’s degree in com-\nputer science and technology from Jilin University,\nChina, in 2013, and the master’s and Ph.D. degrees\nin computer science and engineering from the Uni-\nversity of Nevada, Reno, in 2015 and 2018, respec-\ntively. He is now working as an Assistant Professor\nwith the Department of Computer Science, East\nCarolina University, and collaborates with geo-\nlogical and hydrological scientists to protect the\necological system. His main research interests are\nmachine learning and data visualization using AR/VR devices.\nSERGIU M. DASCALUis a Professor in the De-\npartment of Computer Science and Engineering at\nthe University of Nevada, Reno (UNR), which he\njoined in July 2002. He received his PhD degree in\nComputer Science (2001) from Dalhousie Univer-\nsity, Canada and a Master’s degree in Automatic\nControl and Computers (1982) from the Polytech-\nnic of Bucharest, Romania. At UNR he is also the\nDirector of the Software Engineering Laboratory\n(SOELA) and the Co-Director of the Cyberinfras-\ntructure Lab (CIL). Since joining UNR, he has worked on research projects\nfunded by federal agencies (NSF, NASA, DoD-ONR) as well as the industry.\nHe has advised 11 PhD and over 50 Master students. He received several\nawards, including the 2009 Nevada Center for Entrepreneurship Faculty\nAdvisor Award, the 2011 UNR Outstanding Undergraduate Research Faculty\nMentor Award, the 2011 UNR Donald Tibbitts Distinguished Teacher of the\nYear Award, the 2014 CoEN Faculty Excellence Award, and the 2019 UNR\nVada Trimble Outstanding Graduate Mentor Award. He is a Senior Member\nof the ACM.\nFREDERICK C. HARRIS, JR. received his BS\nand MS degrees in Mathematics and Educa-\ntional Administration from Bob Jones University,\nGreenville, SC, USA in 1986 and 1988 respec-\ntively. He then went on and received his MS and\nPh.D. degrees in Computer Science from Clemson\nUniversity, Clemson, SC, USA in 1991 and 1994\nrespectively.\nHe is currently the Associate Dean for Research\nin the College of Engineering, a Foundation Pro-\nfessor in the Department of Computer Science and Engineering, and the\nDirector of the High Performance Computation and Visualization Lab at the\nUniversity of Nevada, Reno. Since joining UNR, he has worked on research\nprojects funded by federal agencies (NSF, NASA, DARPA, ONR, DoD) as\nwell as industry. He is also the Nevada State EPSCoR Director and the Project\nDirector for Nevada NSF EPSCoR. He has published more than 300 peer-\nreviewed journal and conference papers along with several book chapters\nand has edited or co-edited 15 books. He has had 14 PhD students and 84\nMS Thesis students finish under his supervision. His research interests are in\nparallel computation, simulation, computer graphics, and virtual reality. He is\nalso a Senior Member of the ACM, and a Senior Member of the International\nSociety for Computers and their Applications (ISCA).\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357693\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7067750096321106
    },
    {
      "name": "Data mining",
      "score": 0.5964418649673462
    },
    {
      "name": "Transformer",
      "score": 0.5582651495933533
    },
    {
      "name": "Time series",
      "score": 0.4964800477027893
    },
    {
      "name": "Temporal database",
      "score": 0.48941770195961
    },
    {
      "name": "Multivariate statistics",
      "score": 0.46897825598716736
    },
    {
      "name": "Embedding",
      "score": 0.4520905911922455
    },
    {
      "name": "Grid",
      "score": 0.4283660352230072
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4250292479991913
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3637799620628357
    },
    {
      "name": "Machine learning",
      "score": 0.34095776081085205
    },
    {
      "name": "Mathematics",
      "score": 0.12441781163215637
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134113660",
      "name": "University of Nevada, Reno",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I186335123",
      "name": "East Carolina University",
      "country": "US"
    }
  ]
}