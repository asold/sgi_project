{
  "title": "Spatial Transformer Networks",
  "url": "https://openalex.org/W2951005624",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3175250689",
      "name": "Jaderberg, Max",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225334488",
      "name": "Simonyan, Karen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2930261228",
      "name": "Zisserman, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226541263",
      "name": "Kavukcuoglu, Koray",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1909952827",
    "https://openalex.org/W112688168",
    "https://openalex.org/W2729172879",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2964036520",
    "https://openalex.org/W2952390042",
    "https://openalex.org/W2952422028",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2949820118",
    "https://openalex.org/W1724369340",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1912570122",
    "https://openalex.org/W2953301748",
    "https://openalex.org/W2136026194",
    "https://openalex.org/W2128409098",
    "https://openalex.org/W1616462885",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2072072671",
    "https://openalex.org/W2952020226",
    "https://openalex.org/W252252322",
    "https://openalex.org/W2172010943",
    "https://openalex.org/W2068730032",
    "https://openalex.org/W1491389626",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2950209802",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2966661",
    "https://openalex.org/W2963829960",
    "https://openalex.org/W2952186347",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2096198554",
    "https://openalex.org/W2185466002"
  ],
  "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
  "full_text": "Spatial Transformer Networks\nMax Jaderberg Karen Simonyan Andrew Zisserman Koray Kavukcuoglu\nGoogle DeepMind, London, UK\n{jaderberg,simonyan,zisserman,korayk}@google.com\nAbstract\nConvolutional Neural Networks deﬁne an exceptionally powerful class of models,\nbut are still limited by the lack of ability to be spatially invariant to the input data\nin a computationally and parameter efﬁcient manner. In this work we introduce a\nnew learnable module, the Spatial Transformer, which explicitly allows the spa-\ntial manipulation of data within the network. This differentiable module can be\ninserted into existing convolutional architectures, giving neural networks the abil-\nity to actively spatially transform feature maps, conditional on the feature map\nitself, without any extra training supervision or modiﬁcation to the optimisation\nprocess. We show that the use of spatial transformers results in models which\nlearn invariance to translation, scale, rotation and more generic warping, result-\ning in state-of-the-art performance on several benchmarks, and for a number of\nclasses of transformations.\n1 Introduction\nOver recent years, the landscape of computer vision has been drastically altered and pushed forward\nthrough the adoption of a fast, scalable, end-to-end learning framework, the Convolutional Neural\nNetwork (CNN) [21]. Though not a recent invention, we now see a cornucopia of CNN-based\nmodels achieving state-of-the-art results in classiﬁcation [19, 28, 35], localisation [31, 37], semantic\nsegmentation [24], and action recognition [12, 32] tasks, amongst others.\nA desirable property of a system which is able to reason about images is to disentangle object\npose and part deformation from texture and shape. The introduction of local max-pooling layers in\nCNNs has helped to satisfy this property by allowing a network to be somewhat spatially invariant\nto the position of features. However, due to the typically small spatial support for max-pooling\n(e.g. 2 ×2 pixels) this spatial invariance is only realised over a deep hierarchy of max-pooling and\nconvolutions, and the intermediate feature maps (convolutional layer activations) in a CNN are not\nactually invariant to large transformations of the input data [6, 22]. This limitation of CNNs is due\nto having only a limited, pre-deﬁned pooling mechanism for dealing with variations in the spatial\narrangement of data.\nIn this work we introduce a Spatial Transformer module, that can be included into a standard neural\nnetwork architecture to provide spatial transformation capabilities. The action of the spatial trans-\nformer is conditioned on individual data samples, with the appropriate behaviour learnt during train-\ning for the task in question (without extra supervision). Unlike pooling layers, where the receptive\nﬁelds are ﬁxed and local, the spatial transformer module is a dynamic mechanism that can actively\nspatially transform an image (or a feature map) by producing an appropriate transformation for each\ninput sample. The transformation is then performed on the entire feature map (non-locally) and\ncan include scaling, cropping, rotations, as well as non-rigid deformations. This allows networks\nwhich include spatial transformers to not only select regions of an image that are most relevant (at-\ntention), but also to transform those regions to a canonical, expected pose to simplify recognition in\nthe following layers. Notably, spatial transformers can be trained with standard back-propagation,\nallowing for end-to-end training of the models they are injected in.\n1\narXiv:1506.02025v3  [cs.CV]  4 Feb 2016\n(a) (c)\n7\n(d)\n5\n6\n(b)\n9\n4\nFigure 1: The result of using a spatial transformer as the\nﬁrst layer of a fully-connected network trained for distorted\nMNIST digit classiﬁcation. (a) The input to the spatial trans-\nformer network is an image of an MNIST digit that is dis-\ntorted with random translation, scale, rotation, and clutter. (b)\nThe localisation network of the spatial transformer predicts a\ntransformation to apply to the input image. (c) The output\nof the spatial transformer, after applying the transformation.\n(d) The classiﬁcation prediction produced by the subsequent\nfully-connected network on the output of the spatial trans-\nformer. The spatial transformer network (a CNN including a\nspatial transformer module) is trained end-to-end with only\nclass labels – no knowledge of the groundtruth transforma-\ntions is given to the system.\nSpatial transformers can be incorporated into CNNs to beneﬁt multifarious tasks, for example:\n(i) image classiﬁcation: suppose a CNN is trained to perform multi-way classiﬁcation of images\naccording to whether they contain a particular digit – where the position and size of the digit may\nvary signiﬁcantly with each sample (and are uncorrelated with the class); a spatial transformer that\ncrops out and scale-normalizes the appropriate region can simplify the subsequent classiﬁcation\ntask, and lead to superior classiﬁcation performance, see Fig. 1; (ii) co-localisation: given a set of\nimages containing different instances of the same (but unknown) class, a spatial transformer can be\nused to localise them in each image; (iii) spatial attention: a spatial transformer can be used for\ntasks requiring an attention mechanism, such as in [14, 39], but is more ﬂexible and can be trained\npurely with backpropagation without reinforcement learning. A key beneﬁt of using attention is that\ntransformed (and so attended), lower resolution inputs can be used in favour of higher resolution\nraw inputs, resulting in increased computational efﬁciency.\nThe rest of the paper is organised as follows: Sect. 2 discusses some work related to our own, we\nintroduce the formulation and implementation of the spatial transformer in Sect. 3, and ﬁnally give\nthe results of experiments in Sect. 4. Additional experiments and implementation details are given\nin Appendix A.\n2 Related Work\nIn this section we discuss the prior work related to the paper, covering the central ideas of modelling\ntransformations with neural networks [15, 16, 36], learning and analysing transformation-invariant\nrepresentations [4, 6, 10, 20, 22, 33], as well as attention and detection mechanisms for feature\nselection [1, 7, 11, 14, 27, 29].\nEarly work by Hinton [15] looked at assigning canonical frames of reference to object parts, a theme\nwhich recurred in [16] where 2D afﬁne transformations were modeled to create a generative model\ncomposed of transformed parts. The targets of the generative training scheme are the transformed\ninput images, with the transformations between input images and targets given as an additional\ninput to the network. The result is a generative model which can learn to generate transformed\nimages of objects by composing parts. The notion of a composition of transformed parts is taken\nfurther by Tieleman [36], where learnt parts are explicitly afﬁne-transformed, with the transform\npredicted by the network. Such generative capsule models are able to learn discriminative features\nfor classiﬁcation from transformation supervision.\nThe invariance and equivariance of CNN representations to input image transformations are studied\nin [22] by estimating the linear relationships between representations of the original and transformed\nimages. Cohen & Welling [6] analyse this behaviour in relation to symmetry groups, which is also\nexploited in the architecture proposed by Gens & Domingos [10], resulting in feature maps that are\nmore invariant to symmetry groups. Other attempts to design transformation invariant representa-\ntions are scattering networks [4], and CNNs that construct ﬁlter banks of transformed ﬁlters [20, 33].\nStollenga et al. [34] use a policy based on a network’s activations to gate the responses of the net-\nwork’s ﬁlters for a subsequent forward pass of the same image and so can allow attention to speciﬁc\nfeatures. In this work, we aim to achieve invariant representations by manipulating the data rather\nthan the feature extractors, something that was done for clustering in [9].\nNeural networks with selective attention manipulate the data by taking crops, and so are able to learn\ntranslation invariance. Work such as [1, 29] are trained with reinforcement learning to avoid the\n2\n]\n] ]\n]\nU V\nLocalisation net\nSampler\nSpatial Transformer\nGrid !\ngenerator\n]\nT\n✓\n( G )✓\nFigure 2: The architecture of a spatial transformer module. The input feature mapUis passed to a localisation\nnetwork which regresses the transformation parameters θ. The regular spatial grid Gover V is transformed to\nthe sampling grid Tθ(G), which is applied to U as described in Sect. 3.3, producing the warped output feature\nmap V. The combination of the localisation network and sampling mechanism deﬁnes a spatial transformer.\nneed for a differentiable attention mechanism, while [14] use a differentiable attention mechansim\nby utilising Gaussian kernels in a generative model. The work by Girshick et al. [11] uses a region\nproposal algorithm as a form of attention, and [7] show that it is possible to regress salient regions\nwith a CNN. The framework we present in this paper can be seen as a generalisation of differentiable\nattention to any spatial transformation.\n3 Spatial Transformers\nIn this section we describe the formulation of a spatial transformer. This is a differentiable module\nwhich applies a spatial transformation to a feature map during a single forward pass, where the\ntransformation is conditioned on the particular input, producing a single output feature map. For\nmulti-channel inputs, the same warping is applied to each channel. For simplicity, in this section we\nconsider single transforms and single outputs per transformer, however we can generalise to multiple\ntransformations, as shown in experiments.\nThe spatial transformer mechanism is split into three parts, shown in Fig. 2. In order of computation,\nﬁrst a localisation network (Sect. 3.1) takes the input feature map, and through a number of hidden\nlayers outputs the parameters of the spatial transformation that should be applied to the feature map\n– this gives a transformation conditional on the input. Then, the predicted transformation parameters\nare used to create a sampling grid, which is a set of points where the input map should be sampled to\nproduce the transformed output. This is done by the grid generator, described in Sect. 3.2. Finally,\nthe feature map and the sampling grid are taken as inputs to the sampler, producing the output map\nsampled from the input at the grid points (Sect. 3.3).\nThe combination of these three components forms a spatial transformer and will now be described\nin more detail in the following sections.\n3.1 Localisation Network\nThe localisation network takes the input feature map U ∈RH×W×C with width W, height H and\nC channels and outputs θ, the parameters of the transformation Tθ to be applied to the feature map:\nθ = floc(U). The size of θ can vary depending on the transformation type that is parameterised,\ne.g. for an afﬁne transformation θis 6-dimensional as in (10).\nThe localisation network function floc() can take any form, such as a fully-connected network or\na convolutional network, but should include a ﬁnal regression layer to produce the transformation\nparameters θ.\n3.2 Parameterised Sampling Grid\nTo perform a warping of the input feature map, each output pixel is computed by applying a sampling\nkernel centered at a particular location in the input feature map (this is described fully in the next\nsection). By pixel we refer to an element of a generic feature map, not necessarily an image. In\ngeneral, the output pixels are deﬁned to lie on a regular grid G = {Gi}of pixels Gi = (xt\ni,yt\ni),\nforming an output feature map V ∈RH′×W′×C, where H′and W′are the height and width of the\ngrid, and Cis the number of channels, which is the same in the input and output.\n3\n(a) (b)\nFigure 3: Two examples of applying the parameterised sampling grid to an image U producing the output V.\n(a) The sampling grid is the regular grid G = TI(G), where I is the identity transformation parameters. (b)\nThe sampling grid is the result of warping the regular grid with an afﬁne transformation Tθ(G).\nFor clarity of exposition, assume for the moment that Tθ is a 2D afﬁne transformation Aθ. We will\ndiscuss other transformations below. In this afﬁne case, the pointwise transformation is\n( xs\ni\nys\ni\n)\n= Tθ(Gi) = Aθ\n\n\nxt\ni\nyt\ni\n1\n\n=\n[ θ11 θ12 θ13\nθ21 θ22 θ23\n]\n\nxt\ni\nyt\ni\n1\n\n (1)\nwhere (xt\ni,yt\ni) are the target coordinates of the regular grid in the output feature map, (xs\ni,ys\ni) are\nthe source coordinates in the input feature map that deﬁne the sample points, and Aθ is the afﬁne\ntransformation matrix. We use height and width normalised coordinates, such that −1 ≤xt\ni,yt\ni ≤1\nwhen within the spatial bounds of the output, and −1 ≤xs\ni,ys\ni ≤1 when within the spatial bounds\nof the input (and similarly for the ycoordinates). The source/target transformation and sampling is\nequivalent to the standard texture mapping and coordinates used in graphics [8].\nThe transform deﬁned in (10) allows cropping, translation, rotation, scale, and skew to be applied\nto the input feature map, and requires only 6 parameters (the 6 elements of Aθ) to be produced by\nthe localisation network. It allows cropping because if the transformation is a contraction (i.e. the\ndeterminant of the left 2 ×2 sub-matrix has magnitude less than unity) then the mapped regular grid\nwill lie in a parallelogram of area less than the range of xs\ni,ys\ni. The effect of this transformation on\nthe grid compared to the identity transform is shown in Fig. 3.\nThe class of transformations Tθ may be more constrained, such as that used for attention\nAθ =\n[\ns 0 tx\n0 s t y\n]\n(2)\nallowing cropping, translation, and isotropic scaling by varying s, tx, and ty. The transformation\nTθ can also be more general, such as a plane projective transformation with 8 parameters, piece-\nwise afﬁne, or a thin plate spline. Indeed, the transformation can have any parameterised form,\nprovided that it is differentiable with respect to the parameters – this crucially allows gradients to be\nbackpropagated through from the sample points Tθ(Gi) to the localisation network output θ. If the\ntransformation is parameterised in a structured, low-dimensional way, this reduces the complexity\nof the task assigned to the localisation network. For instance, a generic class of structured and dif-\nferentiable transformations, which is a superset of attention, afﬁne, projective, and thin plate spline\ntransformations, is Tθ = MθB, where B is a target grid representation ( e.g. in (10), B is the regu-\nlar grid Gin homogeneous coordinates), and Mθ is a matrix parameterised by θ. In this case it is\npossible to not only learn how to predict θfor a sample, but also to learn Bfor the task at hand.\n3.3 Differentiable Image Sampling\nTo perform a spatial transformation of the input feature map, a sampler must take the set of sampling\npoints Tθ(G), along with the input feature map U and produce the sampled output feature map V.\nEach (xs\ni,ys\ni) coordinate in Tθ(G) deﬁnes the spatial location in the input where a sampling kernel\nis applied to get the value at a particular pixel in the output V. This can be written as\nVc\ni =\nH∑\nn\nW∑\nm\nUc\nnmk(xs\ni −m; Φx)k(ys\ni −n; Φy) ∀i∈[1 ...H ′W′] ∀c∈[1 ...C ] (3)\n4\nwhere Φx and Φy are the parameters of a generic sampling kernel k() which deﬁnes the image\ninterpolation (e.g. bilinear), Uc\nnm is the value at location (n,m) in channel cof the input, and Vc\ni\nis the output value for pixel i at location (xt\ni,yt\ni) in channel c. Note that the sampling is done\nidentically for each channel of the input, so every channel is transformed in an identical way (this\npreserves spatial consistency between channels).\nIn theory, any sampling kernel can be used, as long as (sub-)gradients can be deﬁned with respect to\nxs\ni and ys\ni. For example, using the integer sampling kernel reduces (3) to\nVc\ni =\nH∑\nn\nW∑\nm\nUc\nnmδ(⌊xs\ni + 0.5⌋−m)δ(⌊ys\ni + 0.5⌋−n) (4)\nwhere ⌊x+ 0.5⌋rounds x to the nearest integer and δ() is the Kronecker delta function. This\nsampling kernel equates to just copying the value at the nearest pixel to(xs\ni,ys\ni) to the output location\n(xt\ni,yt\ni). Alternatively, a bilinear sampling kernel can be used, giving\nVc\ni =\nH∑\nn\nW∑\nm\nUc\nnmmax(0,1 −|xs\ni −m|) max(0,1 −|ys\ni −n|) (5)\nTo allow backpropagation of the loss through this sampling mechanism we can deﬁne the gradients\nwith respect to U and G. For bilinear sampling (5) the partial derivatives are\n∂Vc\ni\n∂Ucnm\n=\nH∑\nn\nW∑\nm\nmax(0,1 −|xs\ni −m|) max(0,1 −|ys\ni −n|) (6)\n∂Vc\ni\n∂xs\ni\n=\nH∑\nn\nW∑\nm\nUc\nnmmax(0,1 −|ys\ni −n|)\n\n\n\n0 if |m−xs\ni|≥ 1\n1 if m≥xs\ni\n−1 if m<x s\ni\n(7)\nand similarly to (7) for ∂Vc\ni\n∂ys\ni\n.\nThis gives us a (sub-)differentiable sampling mechanism, allowing loss gradients to ﬂow back not\nonly to the input feature map (6), but also to the sampling grid coordinates (7), and therefore back\nto the transformation parameters θand localisation network since ∂xs\ni\n∂θ and ∂xs\ni\n∂θ can be easily derived\nfrom (10) for example. Due to discontinuities in the sampling fuctions, sub-gradients must be used.\nThis sampling mechanism can be implemented very efﬁciently on GPU, by ignoring the sum over\nall input locations and instead just looking at the kernel support region for each output pixel.\n3.4 Spatial Transformer Networks\nThe combination of the localisation network, grid generator, and sampler form a spatial transformer\n(Fig. 2). This is a self-contained module which can be dropped into a CNN architecture at any point,\nand in any number, giving rise tospatial transformer networks. This module is computationally very\nfast and does not impair the training speed, causing very little time overhead when used naively, and\neven speedups in attentive models due to subsequent downsampling that can be applied to the output\nof the transformer.\nPlacing spatial transformers within a CNN allows the network to learn how to actively transform\nthe feature maps to help minimise the overall cost function of the network during training. The\nknowledge of how to transform each training sample is compressed and cached in the weights of\nthe localisation network (and also the weights of the layers previous to a spatial transformer) during\ntraining. For some tasks, it may also be useful to feed the output of the localisation network, θ,\nforward to the rest of the network, as it explicitly encodes the transformation, and hence the pose, of\na region or object.\nIt is also possible to use spatial transformers to downsample or oversample a feature map, as one can\ndeﬁne the output dimensions H′and W′to be different to the input dimensionsHand W. However,\nwith sampling kernels with a ﬁxed, small spatial support (such as the bilinear kernel), downsampling\nwith a spatial transformer can cause aliasing effects.\n5\nMNIST Distortion\nModel R RTS P E\nFCN 2.1 5.2 3.1 3.2\nCNN 1.2 0.8 1.5 1.4\nST-FCN\nAff 1.2 0.8 1.5 2.7\nProj 1.3 0.9 1.4 2.6\nTPS 1.1 0.8 1.4 2.4\nST-CNN\nAff 0.7 0.5 0.8 1.2\nProj 0.8 0.6 0.8 1.3\nTPS 0.7 0.5 0.8 1.1\n(a) (c)(b)\nR\nR\nRRTS\nE\nE\n(c)(b)\n58°\n(a)\n-65°\n93°\nTable 1: Left: The percentage errors for different models on different distorted MNIST datasets. The different\ndistorted MNIST datasets we test are TC: translated and cluttered, R: rotated, RTS: rotated, translated, and\nscaled, P: projective distortion, E: elastic distortion. All the models used for each experiment have the same\nnumber of parameters, and same base structure for all experiments. Right: Some example test images where\na spatial transformer network correctly classiﬁes the digit but a CNN fails. (a) The inputs to the networks. (b)\nThe transformations predicted by the spatial transformers, visualised by the grid Tθ(G). (c) The outputs of the\nspatial transformers. E and RTS examples use thin plate spline spatial transformers (ST-CNN TPS), while R\nexamples use afﬁne spatial transformers (ST-CNN Aff) with the angles of the afﬁne transformations given. For\nvideos showing animations of these experiments and more see https://goo.gl/qdEhUu.\nFinally, it is possible to have multiple spatial transformers in a CNN. Placing multiple spatial trans-\nformers at increasing depths of a network allow transformations of increasingly abstract representa-\ntions, and also gives the localisation networks potentially more informative representations to base\nthe predicted transformation parameters on. One can also use multiple spatial transformers in paral-\nlel – this can be useful if there are multiple objects or parts of interest in a feature map that should be\nfocussed on individually. A limitation of this architecture in a purely feed-forward network is that\nthe number of parallel spatial transformers limits the number of objects that the network can model.\n4 Experiments\nIn this section we explore the use of spatial transformer networks on a number of supervised learn-\ning tasks. In Sect. 4.1 we begin with experiments on distorted versions of the MNIST handwriting\ndataset, showing the ability of spatial transformers to improve classiﬁcation performance through\nactively transforming the input images. In Sect. 4.2 we test spatial transformer networks on a chal-\nlenging real-world dataset, Street View House Numbers [25], for number recognition, showing state-\nof-the-art results using multiple spatial transformers embedded in the convolutional stack of a CNN.\nFinally, in Sect. 4.3, we investigate the use of multiple parallel spatial transformers for ﬁne-grained\nclassiﬁcation, showing state-of-the-art performance on CUB-200-2011 birds dataset [38] by dis-\ncovering object parts and learning to attend to them. Further experiments of MNIST addition and\nco-localisation can be found in Appendix A.\n4.1 Distorted MNIST\nIn this section we use the MNIST handwriting dataset as a testbed for exploring the range of trans-\nformations to which a network can learn invariance to by using a spatial transformer.\nWe begin with experiments where we train different neural network models to classify MNIST data\nthat has been distorted in various ways: rotation (R), rotation, scale and translation (RTS), projective\ntransformation (P), and elastic warping (E) – note that elastic warping is destructive and can not be\ninverted in some cases. The full details of the distortions used to generate this data are given in\nAppendix A. We train baseline fully-connected (FCN) and convolutional (CNN) neural networks,\nas well as networks with spatial transformers acting on the input before the classiﬁcation network\n(ST-FCN and ST-CNN). The spatial transformer networks all use bilinear sampling, but variants use\ndifferent transformation functions: an afﬁne transformation (Aff), projective transformation (Proj),\nand a 16-point thin plate spline transformation (TPS) [2]. The CNN models include two max-pooling\nlayers. All networks have approximately the same number of parameters, are trained with identical\noptimisation schemes (backpropagation, SGD, scheduled learning rate decrease, with a multinomial\ncross entropy loss), and all with three weight layers in the classiﬁcation network.\nThe results of these experiments are shown in Table 1 (left). Looking at any particular type of dis-\ntortion of the data, it is clear that a spatial transformer enabled network outperforms its counterpart\nbase network. For the case of rotation, translation, and scale distortion (RTS), the ST-CNN achieves\n6\nSize\nModel 64px 128px\nMaxout CNN [13] 4.0 -\nCNN (ours) 4.0 5.6\nDRAM* [1] 3.9 4.5\nST-CNN Single 3.7 3.9\nMulti 3.6 3.9\nST conv ST conv ST conv ST … 2!\n6!\n0\n(a)\n(b)\n⇥\nTable 2: Left: The sequence error for SVHN multi-digit recognition on crops of 64 ×64 pixels (64px), and\ninﬂated crops of 128 ×128 (128px) which include more background. *The best reported result from [1] uses\nmodel averaging and Monte Carlo averaging, whereas the results from other models are from a single forward\npass of a single model. Right: (a) The schematic of the ST-CNN Multi model. The transformations applied by\neach spatial transformer (ST) is applied to the convolutional feature map produced by the previous layer. (b)\nThe result of multiplying out the afﬁne transformations predicted by the four spatial transformers in ST-CNN\nMulti, visualised on the input image.\n0.5% and 0.6% depending on the class of transform used for Tθ, whereas a CNN, with two max-\npooling layers to provide spatial invariance, achieves 0.8% error. This is in fact the same error that\nthe ST-FCN achieves, which is without a single convolution or max-pooling layer in its network,\nshowing that using a spatial transformer is an alternative way to achieve spatial invariance. ST-CNN\nmodels consistently perform better than ST-FCN models due to max-pooling layers in ST-CNN pro-\nviding even more spatial invariance, and convolutional layers better modelling local structure. We\nalso test our models in a noisy environment, on 60 ×60 images with translated MNIST digits and\nbackground clutter (see Fig. 1 third row for an example): an FCN gets 13.2% error, a CNN gets\n3.5% error, while an ST-FCN gets 2.0% error and an ST-CNN gets 1.7% error.\nLooking at the results between different classes of transformation, the thin plate spline transfor-\nmation (TPS) is the most powerful, being able to reduce error on elastically deformed digits by\nreshaping the input into a prototype instance of the digit, reducing the complexity of the task for the\nclassiﬁcation network, and does not over ﬁt on simpler data e.g. R. Interestingly, the transformation\nof inputs for all ST models leads to a “standard” upright posed digit – this is the mean pose found\nin the training data. In Table 1 (right), we show the transformations performed for some test cases\nwhere a CNN is unable to correctly classify the digit, but a spatial transformer network can. Further\ntest examples are visualised in an animation here https://goo.gl/qdEhUu.\n4.2 Street View House Numbers\nWe now test our spatial transformer networks on a challenging real-world dataset, Street View House\nNumbers (SVHN) [25]. This dataset contains around 200k real world images of house numbers, with\nthe task to recognise the sequence of numbers in each image. There are between 1 and 5 digits in\neach image, with a large variability in scale and spatial arrangement.\nWe follow the experimental setup as in [1, 13], where the data is preprocessed by taking 64 ×64\ncrops around each digit sequence. We also use an additional more loosely128×128 cropped dataset\nas in [1]. We train a baseline character sequence CNN model with 11 hidden layers leading to ﬁve\nindependent softmax classiﬁers, each one predicting the digit at a particular position in the sequence.\nThis is the character sequence model used in [19], where each classiﬁer includes a null-character\noutput to model variable length sequences. This model matches the results obtained in [13].\nWe extend this baseline CNN to include a spatial transformer immediately following the input (ST-\nCNN Single), where the localisation network is a four-layer CNN. We also deﬁne another extension\nwhere before each of the ﬁrst four convolutional layers of the baseline CNN, we insert a spatial\ntransformer (ST-CNN Multi), where the localisation networks are all two layer fully connected net-\nworks with 32 units per layer. In the ST-CNN Multi model, the spatial transformer before the ﬁrst\nconvolutional layer acts on the input image as with the previous experiments, however the subse-\nquent spatial transformers deeper in the network act on the convolutional feature maps, predicting a\ntransformation from them and transforming these feature maps (this is visualised in Table 2 (right)\n(a)). This allows deeper spatial transformers to predict a transformation based on richer features\nrather than the raw image. All networks are trained from scratch with SGD and dropout [17], with\nrandomly initialised weights, except for the regression layers of spatial transformers which are ini-\ntialised to predict the identity transform. Afﬁne transformations and bilinear sampling kernels are\nused for all spatial transformer networks in these experiments.\n7\nModel\nCimpoi ’15 [5] 66.7\nZhang ’14 [40] 74.9\nBranson ’14 [3] 75.7\nLin ’15 [23] 80.9\nSimon ’15 [30] 81.0\nCNN (ours) 224px 82.3\n2×ST-CNN 224px 83.1\n2×ST-CNN 448px 83.9\n4×ST-CNN 448px 84.1\nTable 3: Left: The accuracy on CUB-200-2011 bird classiﬁcation dataset. Spatial transformer networks with\ntwo spatial transformers ( 2×ST-CNN) and four spatial transformers ( 4×ST-CNN) in parallel achieve higher\naccuracy. 448px resolution images can be used with the ST-CNN without an increase in computational cost\ndue to downsampling to 224px after the transformers. Right: The transformation predicted by the spatial\ntransformers of 2×ST-CNN (top row) and 4×ST-CNN (bottom row) on the input image. Notably for the\n2×ST-CNN, one of the transformers (shown in red) learns to detect heads, while the other (shown in green)\ndetects the body, and similarly for the 4×ST-CNN.\nThe results of this experiment are shown in Table 2 (left) – the spatial transformer models obtain\nstate-of-the-art results, reaching 3.6% error on64×64 images compared to previous state-of-the-art\nof 3.9% error. Interestingly on 128 ×128 images, while other methods degrade in performance,\nan ST-CNN achieves 3.9% error while the previous state of the art at 4.5% error is with a recurrent\nattention model that uses an ensemble of models with Monte Carlo averaging – in contrast the ST-\nCNN models require only a single forward pass of a single model. This accuracy is achieved due to\nthe fact that the spatial transformers crop and rescale the parts of the feature maps that correspond\nto the digit, focussing resolution and network capacity only on these areas (see Table 2 (right) (b)\nfor some examples). In terms of computation speed, the ST-CNN Multi model is only 6% slower\n(forward and backward pass) than the CNN.\n4.3 Fine-Grained Classiﬁcation\nIn this section, we use a spatial transformer network with multiple transformers in parallel to perform\nﬁne-grained bird classiﬁcation. We evaluate our models on the CUB-200-2011 birds dataset [38],\ncontaining 6k training images and 5.8k test images, covering 200 species of birds. The birds appear\nat a range of scales and orientations, are not tightly cropped, and require detailed texture and shape\nanalysis to distinguish. In our experiments, we only use image class labels for training.\nWe consider a strong baseline CNN model – an Inception architecture with batch normalisation [18]\npre-trained on ImageNet [26] and ﬁne-tuned on CUB – which by itself achieves the state-of-the-\nart accuracy of 82.3% (previous best result is 81.0% [30]). We then train a spatial transformer\nnetwork, ST-CNN, which contains 2 or 4 parallel spatial transformers, parameterised for attention\nand acting on the input image. Discriminative image parts, captured by the transformers, are passed\nto the part description sub-nets (each of which is also initialised by Inception). The resulting part\nrepresentations are concatenated and classiﬁed with a single softmax layer. The whole architecture\nis trained on image class labels end-to-end with backpropagation (full details in Appendix A).\nThe results are shown in Table 3 (left). The ST-CNN achieves an accuracy of 84.1%, outperforming\nthe baseline by 1.8%. It should be noted that there is a small (22/5794) overlap between the Ima-\ngeNet training set and CUB-200-2011 test set1 – removing these images from the test set results in\n84.0% accuracy with the same ST-CNN. In the visualisations of the transforms predicted by 2×ST-\nCNN (Table 3 (right)) one can see interesting behaviour has been learnt: one spatial transformer\n(red) has learnt to become a head detector, while the other (green) ﬁxates on the central part of the\nbody of a bird. The resulting output from the spatial transformers for the classiﬁcation network is\na somewhat pose-normalised representation of a bird. While previous work such as [3] explicitly\ndeﬁne parts of the bird, training separate detectors for these parts with supplied keypoint training\ndata, the ST-CNN is able to discover and learn part detectors in a data-driven manner without any\nadditional supervision. In addition, the use of spatial transformers allows us to use 448px resolution\ninput images without any impact in performance, as the output of the transformed 448px images are\ndownsampled to 224px before being processed.\n1Thanks to the eagle-eyed Hugo Larochelle and Yin Zheng for spotting the birds nested in both the ImageNet\ntraining set and CUB test set.\n8\n5 Conclusion\nIn this paper we introduced a new self-contained module for neural networks – the spatial trans-\nformer. This module can be dropped into a network and perform explicit spatial transformations\nof features, opening up new ways for neural networks to model data, and is learnt in an end-to-\nend fashion, without making any changes to the loss function. While CNNs provide an incredibly\nstrong baseline, we see gains in accuracy using spatial transformers across multiple tasks, result-\ning in state-of-the-art performance. Furthermore, the regressed transformation parameters from the\nspatial transformer are available as an output and could be used for subsequent tasks. While we\nonly explore feed-forward networks in this work, early experiments show spatial transformers to be\npowerful in recurrent models, and useful for tasks requiring the disentangling of object reference\nframes, as well as easily extendable to 3D transformations (see Appendix A.3).\nReferences\n[1] J. Ba, V . Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. ICLR, 2015.\n[2] F. Bookstein. Principal warps : Thin-plate splines and the decomposition of deformations. IEEE PAMI,\n1989.\n[3] S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird species categorization using pose normalized\ndeep convolutional nets. BMVC., 2014.\n[4] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE PAMI, 35(8):1872–1886, 2013.\n[5] M. Cimpoi, S. Maji, and A. Vedaldi. Deep ﬁlter banks for texture recognition and segmentation. InCVPR,\n2015.\n[6] T. S. Cohen and M. Welling. Transformation properties of learned visual representations. ICLR, 2015.\n[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks.\nIn CVPR, 2014.\n[8] J. D. Foley, A. Van Dam, S. K. Feiner, J. F. Hughes, and R. L. Phillips.Introduction to computer graphics,\nvolume 55. Addison-Wesley Reading, 1994.\n[9] B. J. Frey and N. Jojic. Fast, large-scale transformation-invariant clustering. In NIPS, 2001.\n[10] R. Gens and P. M. Domingos. Deep symmetry networks. In NIPS, 2014.\n[11] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014.\n[12] G. Gkioxari, R. Girshick, and J. Malik. Contextual action recognition with r* cnn. arXiv:1505.01197,\n2015.\n[13] I. J. Goodfellow, Y . Bulatov, J. Ibarz, S. Arnoud, and V . Shet. Multi-digit number recognition from street\nview imagery using deep convolutional neural networks. arXiv:1312.6082, 2013.\n[14] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image\ngeneration. ICML, 2015.\n[15] G. E. Hinton. A parallel computation that assigns canonical object-based frames of reference. In IJCAI,\n1981.\n[16] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In ICANN. 2011.\n[17] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.\n[18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. ICML, 2015.\n[19] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Synthetic data and artiﬁcial neural networks\nfor natural scene text recognition. NIPS DLW, 2014.\n[20] A. Kanazawa, A. Sharma, and D. Jacobs. Locally scale-invariant convolutional neural networks. In NIPS,\n2014.\n[21] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, 1998.\n[22] K. Lenc and A. Vedaldi. Understanding image representations by measuring their equivariance and equiv-\nalence. CVPR, 2015.\n[23] T. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models for ﬁne-grained visual recognition.\narXiv:1504.07889, 2015.\n9\n[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. InCVPR,\n2015.\n[25] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with\nunsupervised feature learning. In NIPS DLW, 2011.\n[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.\n[27] J. Schmidhuber and R. Huber. Learning to generate artiﬁcial fovea trajectories for target detection. Inter-\nnational Journal of Neural Systems, 2(01n02):125–134, 1991.\n[28] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition and\nclustering. arXiv:1503.03832, 2015.\n[29] P. Sermanet, A. Frome, and E. Real. Attention for ﬁne-grained categorization. arXiv:1412.7054, 2014.\n[30] M. Simon and E. Rodner. Neural activation constellations: Unsupervised part model discovery with\nconvolutional networks. arXiv:1504.08289, 2015.\n[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\nICLR, 2015.\n[32] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in\nvideos. In NIPS, pages 568–576, 2014.\n[33] K. Sohn and H. Lee. Learning invariant representations with local transformations. arXiv:1206.6418,\n2012.\n[34] M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber. Deep networks with internal selective attention\nthrough feedback connections. In NIPS, 2014.\n[35] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. CVPR, 2015.\n[36] T. Tieleman. Optimizing Neural Networks that Generate Images. PhD thesis, University of Toronto, 2014.\n[37] J. J. Tompson, A. Jain, Y . LeCun, and C. Bregler. Joint training of a convolutional network and a graphical\nmodel for human pose estimation. In NIPS, pages 1799–1807, 2014.\n[38] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset.\n2011.\n[39] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y . Bengio. Show, attend\nand tell: Neural image caption generation with visual attention. ICML, 2015.\n[40] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based r-cnns for ﬁne-grained category\ndetection. In Computer Vision–ECCV 2014, pages 834–849. Springer, 2014.\nA Appendix\nIn this section we present the results of two further experiments – that of MNIST addition showing\nspatial transformers acting on multiple objects in Sect. A.1, and co-localisation in Sect. A.2 showing\nthe application to semi-supervised scenarios. In addition, we give an example of the extension to\n3D in Sect. A.3. We also expand upon the details of the experiments from Sect. 4.1 in Sect. A.4,\nSect. 4.2 in Sect. A.5, and Sect. 4.3 in Sect. A.6.\nA.1 MNIST Addition\nIn this section we demonstrate another use case for multiple spatial transformers in parallel: to\nmodel multiple objects. We deﬁne an MNIST addition task, where the network must output the sum\nof the two digits given in the input. Each digit is presented in a separate 42 ×42 input channel\n(giving 2-channel inputs), but each digit is transformed independently, with random rotation, scale,\nand translation (RTS).\nWe train fully connected (FCN), convolutional (CNN) and single spatial transformer fully connected\n(ST-FCN) networks, as well as spatial transformer fully connected networks with two parallel spa-\ntial transformers (2×ST-FCN) acting on the input image, each one taking both channels as input and\ntransforming both channels. The two 2-channel outputs of the two spatial transformers are concate-\nnated into a 4-channel feature map for the subsequent FCN. As in Sect. 4.1, all networks have the\nsame number of parameters, and are all trained with SGD to minimise the multinomial cross entropy\nloss for 19 classes (the possible addition results 0-18).\nThe results are given in Table 4 (left). Due to the complexity of this task, the FCN reaches a\nminimum error of 47.7%, however a CNN with max-pooling layers is far more accurate with 14.7%\n10\nModel RTS\nFCN 47.7\nCNN 14.7\nST-FCN\nAff 22.6\nProj 18.5\nTPS 19.1\n2×ST-FCN\nAff 9.0\nProj 5.9\nTPS 5.8\nST1\nST2\n13\nInput\nST1  \nout\nST2  \nout\nConcatenated\nFCN\nTable 4: Left: The percentage error for the two digit MNIST addition task, where each digit is transformed\nindependently in separate channels, trained by supplying only the label of the sum of the two digits. The\nuse of two spatial transformers in parallel, 2×ST-FCN, allows the fully-connected neural network to become\ninvariant to the transformations of each digit, giving the lowest error. All the models used for each column\nhave approximately the same number of parameters. Right: A test example showing the learnt behaviour of\neach spatial transformer (using a thin plate spline (TPS) transformation). The 2-channel input (the blue bar\ndenotes separation between channels) is fed to two independent spatial transformers, ST1 and ST2, each of\nwhich operate on both channels. The outputs of ST1 and ST2 and concatenated and used as a 4-channel input\nto a fully connected network (FCN) which predicts the addition of the two original digits. During training, the\ntwo spatial transformers co-adapt to focus on a single channel each.\nMNIST Distortion\nClass T TC\n0 100 81\n1 100 82\n2 100 88\n3 100 75\n4 100 94\n5 100 84\n6 100 93\n7 100 85\n8 100 89\n9 100 87\nST\nST\nrand\nw\nI n\nI m\nI n\nI\nT\nn\nI\nrand\nn\nI\nT\nm\ne ()\ne ()\ne ()\ne ( I\nT\nm\n)\ne ( I\nT\nn\n)\ne ( I\nrand\nn\n)\nsmall \ndistance\nlarge \ndistance\nTable 5: Left: The percent of correctly co-localised digits for different MNIST digit classes, for just translated\ndigits (T), and for translated digits with clutter added (TC). Right: The optimisation architecture. We use a\nhinge loss to enforce the distance between the two outputs of the spatial transformer (ST) to be less than the\ndistance to a random crop, hoping to encourage the spatial transformer to localise the common objects.\nerror. Adding a single spatial transformer improves the capability of an FCN by focussing on a\nsingle region of the input containing both digits, reaching 18.5% error. However, by using two\nspatial transformers, each transformer can learn to focus on transforming the digit in a single channel\n(though receiving both channels as input), visualised in Table 4 (right). The transformers co-adapt,\nproducing stable representations of the two digits in two of the four output channels of the spatial\ntransformers. This allows the 2×ST-FCN model to achieve 5.8% error, far exceeding that of other\nmodels.\nA.2 Co-localisation\nIn this experiment, we explore the use of spatial transformers in a semi-supervised scenario – co-\nlocalisation. The co-localisation task is as follows: given a set of images that are assumed to contain\ninstances of a common but unknown object class, localise (with a bounding box) the common object.\nNeither the object class labels, nor the object location ground truth is used for optimisation, only the\nset of images.\nTo achieve this, we adopt the supervision that the distance between the image crop corresponding to\ntwo correctly localised objects is smaller than to a randomly sampled image crop, in some embed-\nding space. For a dataset I= {In}of N images, this translates to a triplet loss, where we minimise\n11\nStep 0\n Step 180\nStep 10\n Step 90\n Step 120\n Step 150\nStep 60\nOptimisation\nFigure 4: A look at the optimisation dynamics for co-localisation. Here we show the localisation predicted\nby the spatial transformer for three of the 100 dataset images after the SGD step labelled below. By SGD\nstep 180 the model has process has correctly localised the three digits. A full animation is shown in the video\nhttps://goo.gl/qdEhUu\nthe hinge loss\nN∑\nn\nM∑\nm̸=n\nmax(0,∥e(IT\nn ) −e(IT\nm)∥2\n2 −∥e(IT\nn ) −e(Irand\nn )∥2\n2 + α) (8)\nwhere IT\nn is the image crop of In corresponding to the localised object, Irand\nn is a randomly sampled\npatch from In, e() is an encoding function and αis a margin. We can use a spatial transformer to\nact as the localiser, such that IT\nn = Tθ(In) where θ = floc(In), interpreting the parameters of the\ntransformation θas the bounding box of the object. We can minimise this with stochastic gradient\ndescent, randomly sampling image pairs (n,m).\nWe perform co-localisation on translated (T), and also translated and cluttered (TC) MNIST images.\nEach image, a 28 ×28 pixel MNIST digit, is placed in a uniform random location in a 84 ×84\nblack background image. For the cluttered dataset, we also then add 16 random 6 ×6 crops sam-\npled from the original MNIST training dataset, creating distractors. For a particular co-localisation\noptimisation, we pick a digit class and generate 100 distorted image samples as the dataset for the\nexperiment. We use a margin α= 1, and for the encoding function e() we use the CNN trained for\ndigit classiﬁcation from Sect. 4.1, concatenating the three layers of activations (two hidden layers\nand the classiﬁcation layer without softmax) to form a feature descriptor. We use a spatial trans-\nformer parameterised for attention (scale and translation) where the localisation network is a 100k\nparameter CNN consisting of a convolutional layer with eight 9 ×9 ﬁlters and a 4 pixel stride, fol-\nlowed by 2 ×2 max pooling with stride 2 and then two 8-unit fully-connected layers before the ﬁnal\n3-unit fully-connected layer.\nThe results are shown in Table 5. We measure a digit to be correctly localised if the overlap (area of\nintersection divided by area of union) between the predicted bounding box and groundtruth bounding\nbox is greater than 0.5. Our co-localisation framework is able to perfectly localise MNIST digits\nwithout any clutter with 100% accuracy, and correctly localises between 75-93% of digits when\nthere is clutter in the images. An example of the optimisation process on a subset of the dataset for\n“8” is shown in Fig. 4. This is surprisingly good performance for what is a simple loss function\nderived from simple intuition, and hints at potential further applications in tracking problems.\nA.3 Higher Dimensional Transformers\nThe framework described in this paper is not limited to 2D transformations and can be easily ex-\ntended to higher dimensions. To demonstrate this, we give the example of a spatial transformer\ncapable of performing 3D afﬁne transformations.\n12\n3D voxel input\n3D transformation applied\n6\n2D projection\nFigure 5: The behaviour of a trained 3D MNIST classiﬁer on a test example. The 3D voxel input contains\na random MNIST digit which has been extruded and randomly placed inside a 60 ×60 ×60 volume. A 3D\nspatial transformer performs a transformation of the input, producing an output volume whose depth is then\nﬂattened. This creates a 2D projection of the 3D space, which the subsequent layers of the network are able to\nclassify. The whole network is trained end-to-end with just classiﬁcation labels.\nWe extended the differentiable image sampling of Sect. 3.3 to perform 3D bilinear sampling. The\n3D equivalent of (5) becomes\nVc\ni =\nH∑\nn\nW∑\nm\nD∑\nl\nUc\nnmlmax(0,1 −|xs\ni −m|) max(0,1 −|ys\ni −n|) max(0,1 −|zs\ni −l|) (9)\nfor the 3D input U ∈RH×W×D×C and output V ∈RH′×W′×D′×C, where H′, W′, and D′are the\nheight, width and depth of the grid, and C is the number of channels. Similarly to the 2D sampling\ngrid in Sect. 3.2, the source coordinates that deﬁne the sampling points,(xs\ni,ys\ni,zs\ni) can be generated\nby the transformation of a regular 3D grid G = {Gi}of voxels Gi = (xt\ni,yt\ni,zt\ni). For a 3D afﬁne\ntransformation this is\n\n\nxs\ni\nys\ni\nzs\ni\n\n=\n\n\nθ11 θ12 θ13 θ14\nθ21 θ22 θ23 θ24\nθ31 θ32 θ33 θ34\n\n\n\n\nxt\ni\nyt\ni\nzt\ni\n1\n\n. (10)\nThe 3D spatial transformer can be used just like its 2D counterpart, being dropped into neural net-\nworks to provide a way to warp data in 3D space, where the third dimension could be space or\ntime.\nAnother interesting way to use the 3D transformer is to ﬂatten the 3D output across one dimension,\ncreating a 2D projection of the 3D space, e.g. Wc\nnm = ∑\nlVc\nnml such that W ∈RH′×W′×C. This\nallows the original 3D data to be intelligently projected to 2D, greatly reducing the dimensionality\nand complexity of the subsequent processing. We demonstrated this on the task of 3D object classi-\nﬁcation on a dataset of 3D, extruded MNIST digits. The task is to take a 3D voxel input of a digit\nwhich has been randomly translated and rotated in 3D space, and output the class of the digit. The\nresulting 3D spatial transformer network learns to create a 2D projection of the 3D space where the\ndigit is centered in the resulting 2D image, making it easy for the remaining layers to classify. An\nexample is shown in Fig. 5.\nA.4 Distorted MNIST Details\nIn this section we expand upon the details of the distorted MNIST experiments in Sect. 4.1.\nData. The rotated dataset (R) was generated from rotating MNIST training digits with a random\nrotation sampled uniformly between −90◦and +90◦. The rotated, translated, and scaled dataset\n(RTS) was generated by randomly rotating an MNIST digit by+45◦and −45◦, randomly scaling the\n13\ndigit by a factor of between 0.7 and 1.2, and placing the digit in a random location in a42×42 image,\nall with uniform distributions. The projected dataset (P) was generated by scaling a digit randomly\nbetween 0.75 and 1.0, and stretching each corner of an MNIST digit by an amount sampled from a\nnormal distribution with zero mean and 5 pixel standard deviation. The elasticly distorted dataset\n(E) was generated by scaling a digit randomly between 0.75 and 1.0, and then randomly peturbing\n16 control points of a thin plate spline arranged in a regular grid on the image by an amount sampled\nfrom a normal distribution with zero mean and 1.5 pixel standard deviation. The translated and\ncluttered dataset (TC) is generated by placing an MNIST digit in a random location in a 60 ×60\nblack canvas, and then inserting six randomly sampled 6 ×6 patches of other digit images into\nrandom locations in the image.\nNetworks. All networks use rectiﬁed linear non-linearities and softmax classiﬁers. All FCN net-\nworks have two hidden fully connected layers followed by a classiﬁcation layer. All CNN networks\nhave a 9 ×9 convolutional layer (stride 1, no padding), a 2 ×2 max-pooling layer with stride 2, a\nsubsequent 7 ×7 convolutional layer (stride 1, no padding), and another 2 ×2 max-pooling layer\nwith stride 2 before the ﬁnal classﬁcation layer. All spatial transformer (ST) enabled networks place\nthe ST modules at the beginning of the network, and have three hidden layers in their localisation\nnetworks with 32 unit fully connected layers for ST-FCN networks and two 20-ﬁlter 5 ×5 convo-\nlutional layers (stride 1, no padding) acting on a 2×downsampled input, with 2 ×2 max-pooling\nbetween convolutional layers, and a 20 unit fully connected layer following the convolutional layers.\nSpatial transformer networks for TC and RTS datasets have average pooling after the spatial trans-\nformer to downsample the output of the transformer by a factor of 2 for the classiﬁcation network.\nThe exact number of units in FCN and CNN based classiﬁcation models varies so as to always en-\nsure that all networks for a particular experiment contain the same number of learnable parameters\n(around 400k). This means that spatial transformer networks generally have less parameters in the\nclassiﬁcation networks due to the need for parameters in the localisation networks. The FCNs have\nbetween 128 and 256 units per layer, and the CNNs have between 32 and 64 ﬁlters per layer.\nTraining. All networks were trained with SGD for 150k iterations, the same hyperparameters (256\nbatch size, 0.01 base learning rate, no weight decay, no dropout), and same learning rate schedule\n(learning rate reduced by a factor of ten every 50k iterations). We initialise the network weights ran-\ndomly, except for the ﬁnal regression layer of localisation networks which are initialised to regress\nthe identity transform (zero weights, identity transform bias). We perform three complete training\nruns for all models with different random seeds and report average accuracy.\nA.5 Street View House Numbers Details\nFor the SVHN experiments in Sect. 4.2, we follow [1, 13] and select hyperparameters from a vali-\ndation set of 5k images from the training set. All networks are trained for 400k iterations with SGD\n(128 batch size), using a base learning rate of 0.01 decreased by a factor of ten every 80k iterations,\nweight decay set to 0.0005, and dropout at 0.5 for all layers except the ﬁrst convolutional layer and\nlocalisation networks. The learning rate for localisation networks of spatial transformer networks\nwas set to a tenth of the base learning rate.\nWe adopt the notation that conv[ N,w,s,p] denotes a convolutional layer with N ﬁlters of size\nw ×w, with stride s and p pixel padding, fc[ N] is a fully connected layer with N units, and\nmax[s] is a s×s max-pooling layer with stride s. The CNN model is: conv[48,5,1,2]-max[2]-\nconv[64,5,1,2]-conv[128,5,1,2]-max[2]-conv[160,5,1,2]-conv[192,5,1,2]-max[2]-conv[192,5,1,2]-\nconv[192,5,1,2]-max[2]-conv[192,5,1,2]-fc[3072]-fc[3072]-fc[3072], with rectiﬁed linear units\nfollowing each weight layer, followed by ﬁve parallel fc[11] and softmax layers for classiﬁcation\n(similar to that in [19]). The ST-CNN Single has a single spatial transformer (ST) before the\nﬁrst convolutional layer of the CNN model – the ST’s localisation network architecture is as\nfollows: conv[32,5,1,2]-max[2]-conv[32,5,1,2]-fc[32]-fc[32]. The ST-CNN Multi has four spatial\ntransformers, one before each of the ﬁrst four convolutional layers of the CNN model, and each\nwith a simple fc[32]-fc[32] localisation network.\nWe initialise the network weights randomly, except for the ﬁnal regression layer of localisation\nnetworks which are initialised to regress the identity transform (zero weights, identity transform\nbias). We performed two full training runs with different random seeds and report the average\naccuracy obtained by a single model.\n14\n]\n]\n✓\n1\n✓\n2\nT ✓ 1\n( G )\nT ✓ 2\n( G )\nInception1\n448px\n224px\n224px\n1k\n200\nf loc ()\nInception2 1k\nFigure 6: The architecture of the 2×ST-CNN 448px used for bird classiﬁcation. A single localisation network\nfloc predicts two transformation parameters θ1 and θ2, with the subsequent transforms Tθ1 and Tθ2 applied to\nthe original input image.\nA.6 Fine Grained Classiﬁcation Details\nIn this section we describe our ﬁne-grained image classiﬁcation architecture in more detail. For\nthis task, we utilise the spatial transformers as a differentiable attention mechanism, where each\ntransformer is expected to automatically learn to focus on discriminative object parts. Namely, each\ntransformer predicts the location (x,y) of the attention window, while the scale is ﬁxed to50% of the\nimage size. The transformers sample 224 ×224 crops from the input image, each of which is then\ndescribed each by its own CNN stream, thus forming a multi-stream architecture (shown in Fig. 6).\nThe outputs of the streams are 1024-D crop descriptors, which are concatenated and classiﬁed with\na 200-way softmax classiﬁer.\nAs the main building block of our network, we utilise the state-of-the-art Inception architecture with\nbatch normalisation [18], pre-trained on the ImageNet Challenge (ILSVRC) dataset. Our model\nachieves 27.1% top-1 error on the ILSVRC validation set using a single image crop (we only trained\non single-scale images, resized so that the smallest side is 256). The crop description networks\nemploy the Inception architecture with the last layer ( 1000-way ILSVRC classiﬁer) removed, so\nthat the output is a 1024-D descriptor.\nThe localisation network is shared across all the transformers, and was derived from Inception in\nthe following way. Apart from the ILSVRC classiﬁcation layer, we also removed the last pooling\nlayer to preserve the spatial information. The output of this truncated Inception net has7 ×7 spatial\nresolution and 1024 feature channels. On top of it, we added three weight layers to predict the\ntransformations: (i) 1 ×1 convolutional layer to reduce the number of feature channels from 1024\nto 128; (ii) fully-connected layer with 128-D output; (iii) fully-connected layer with 2N-D output,\nwhere N is the number of transformers (we experimented with N = 2 and N = 4).\nWe note that we did not strive to optimise the architecture in terms of the number of parameters\nand the computation time. Our aim was to investigate whether spatial transformer networks are\nable to automatically discover meaningful object parts when trained just on image labels, which we\nconﬁrmed both quantitatively and qualitatively (Sect. 4.3).\nThe model was trained for 30k iterations with SGD (batch size 256) with an initial learning rate\nof 0.1, reduced by a factor of 10 after 10k, 20k, and 25k iterations. For stability, the localisation\nnetwork’s learning rate is the base learning rate multiplied by 10−4. Weight decay was set at 10−5\nand dropout of 0.7 was used before the 200-way classiﬁcation layer.\nWe evaluated two input images sizes for the spatial transformers: 224 ×224 and 448 ×448. In the\nlatter case, we added a ﬁxed 2×downscaling layer before the localisation net, so that its input is\nstill 224 ×224. The difference between the two settings lies in the size of the image from which\nsampling is performed ( 224 vs 448), with 448 better suited for sampling small-scale crops. The\noutput of the transformers are 224 ×224 crops in both cases (so that they are compatible with crop\ndescription Inception nets). When training, we utilised conventional augmentation in the form of\nrandom sampling ( 224 ×224 from 256 ×S and 448 ×448 from 512 ×S where S is the largest\nimage side) and horizontal ﬂipping. The localisation net was initialised to tile the image plane with\nthe spatial transformer crops.\nWe also experimented with more complex transformations (location and scale, as well as afﬁne), but\nobserved similar results. This can be attributed to the very small size of the training set (6k images,\n200 classes), and we noticed severe over-ﬁtting in all training scenarios. The hyper-parameters were\nestimated by cross-validation on the training set.\n15",
  "topic": "Image warping",
  "concepts": [
    {
      "name": "Image warping",
      "score": 0.7536656856536865
    },
    {
      "name": "Transformer",
      "score": 0.6971244812011719
    },
    {
      "name": "Computer science",
      "score": 0.6807237863540649
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6665329933166504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5474319458007812
    },
    {
      "name": "Differentiable function",
      "score": 0.4475671052932739
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4372979998588562
    },
    {
      "name": "Algorithm",
      "score": 0.3790411949157715
    },
    {
      "name": "Mathematics",
      "score": 0.1425631046295166
    },
    {
      "name": "Voltage",
      "score": 0.10069981217384338
    },
    {
      "name": "Engineering",
      "score": 0.0951043963432312
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}