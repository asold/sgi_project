{
  "title": "Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models",
  "url": "https://openalex.org/W4385574440",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2520064248",
      "name": "Lifu Tu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2116467378",
      "name": "Yingbo Zhou",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4206529673",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3176209443",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3214173179",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4205523161",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4285201706",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W4385573782",
    "https://openalex.org/W4205991051"
  ],
  "abstract": "Pre-trained multilingual language models show significant performance gains for zero-shot cross-lingual model transfer on a wide range of natural language understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation, pre-trained models are only fine-tuned on English data and tested on a variety of target languages. In this paper, we do cross-lingualevaluation on various NLU tasks (sentence classification, sequence labeling, question answering) using prompt-tuning and compare it with fine-tuning. The results show that prompt tuning achieves much better cross-lingual transfer than fine-tuning across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we demonstrate through the analysis that prompt tuning can have better cross-lingual transfer-ability of representations on downstream tasks with better aligned decision boundaries.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5478–5485\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPrompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual\nUnderstanding With Multilingual Language Models\nLifu Tu and Caiming Xiong and Yingbo Zhou\nSalesforce AI Research\n{ltu,cxiong,yingbo.zhou}@salesforce.com\nAbstract\nPre-trained multilingual language models show\nsignificant performance gains for zero-shot\ncross-lingual model transfer on a wide range\nof natural language understanding (NLU) tasks.\nPreviously, for zero-shot cross-lingual evalua-\ntion, pre-trained models are only fine-tuned on\nEnglish data and tested on a variety of target\nlanguages. In this paper, we do cross-lingual\nevaluation on various NLU tasks (sentence clas-\nsification, sequence labeling, question answer-\ning) using prompt-tuning and compare it with\nfine-tuning. The results show that prompt tun-\ning achieves much better cross-lingual transfer\nthan fine-tuning across datasets, with only 0.1%\nto 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt\ntuning can have better cross-lingual transfer-\nability of representations on downstream tasks\nwith better aligned decision boundaries.\n1 Introduction\nLarge Multilingual language models (Pires et al.,\n2019; Wu and Dredze, 2019; Conneau et al.,\n2020) show surprisingly impressive zero-shot cross-\nlingual transfer on NLP tasks, even though they are\nonly trained from monolingual corpora. Recently,\nlarge-scale benchmarks such as XTREME (Hu\net al., 2020) and XGLUE (Liang et al., 2020) are\nintroduced for cross-lingual evaluation.\nIn a cross-lingual transfer setting, models are\nonly fine-tuned on the task-specific annotations\nin one language and evaluated in other languages.\nDuring fine-tuning, pre-trained language models\nare used for initialization and the entire model pa-\nrameters are tuned on downstream tasks. While\nfine-tuning obtains strong performance, it is inef-\nficient. Also as shown in (Hu et al., 2020), the\ncross-lingual transfer gap between the performance\non the English test set and all other languages is\nlarge even with the best baseline XLM-R (Conneau\net al., 2020).\nRecently, prompt tuning, where only a small\namount of additional parameters (i.e. prompts) is\nadded and tuned, but the original model is kept\nfrozen. Much fewer parameters or no parameters\nare tuned and thus the training is a lot more ef-\nficient. Prompt tuning still performs worse than\nfine-tuning in lots of NLP tasks(Brown et al., 2020;\nShin et al., 2020; Zhong et al., 2021). More re-\ncently, Li and Liang (2021); Lester et al. (2021);\nHambardzumyan et al. (2021) indicate prompt tun-\ning is competitive with fine tuning on some of the\nNLU tasks. Language model capacity (e.g., 10\nbillion parameters) is a key ingredient for these\napproaches to succeed. More recently, (Liu et al.,\n2022) shows prompt tuning can also be compara-\nble on several hard monolingual sequence labeling\ntasks such as extractive question answers.\nIn this paper, we aim to investigate the effect of\nprompt tuning in cross-lingual tasks.We freeze the\nentire multilingual language model and tune task\nprompts on the English training set for downstream\ntasks (sentence classification, structure prediction,\nquestion answering). Even with medium size mul-\ntilingual language model (less than 1 billion param-\neters), prompt tuning achieves much higher perfor-\nmance than fine-tuning on various NLU tasks.\nAccording to the analysis results, prompt tun-\ning does fewer changes to sentence representations\nthan fine-tuning and keeps good cross-lingual sen-\ntence representations. We also find that the decision\nboundaries of different language sentence represen-\ntations after prompt tuning on English data are al-\nmost aligned well. However, these decision bound-\naries of different languages after fine-tuning are a\nlarge difference. These aligned decision boundaries\ncan lead to stronger cross-lingual transfer.\nThis work sheds light on the strong cross-lingual\nability of prompt tuning. Our results suggest\nprompt tuning is better than fine-tuning on cross-\nlingual transfer. Our contributions are summarized\nas follows: we show that prompt tuning can per-\n5478\nform much better as compared to fine-tuning for\ncross-lingual transfer; we also show prompt tuning\nworks better in the case of the cross-lingual transfer\ndue to the relative small robust changes it brings to\nthe originally learned representations.\n2 Prompt-Tuning for Cross-Lingual Tasks\nMultilingual Language Models. In the past\nyears, lots of pre-trained multilingual language\nmodels come out: mBERT, XLM (CONNEAU and\nLample, 2019), XLM-R (Conneau et al., 2020),\netc. XLM-R (Conneau et al., 2020) significantly\noutperforms multilingual BERT (mBERT; Devlin\net al., 2019) on a variety of cross-lingual bench-\nmarks XTREME (Hu et al., 2020). In some pre-\nvious work (Luo et al., 2021; Zhang et al., 2019),\nXLM-R is also used for initialization to do another\nround of pretraining with parallel data to get the\nstronger cross-lingual ability. Previously, in the\ncross-lingual evaluation, models are fine-tuned on\nthe English training data but evaluated on all tar-\nget languages. As far as we know, we are the first\nto explore prompt tuning on several hard multilin-\ngual NLP tasks including structure prediction and\nquestion answering\nFigure 1: Two different approaches for cross-lingual\nevaluation when using large multilingual language\nmodel. Left: In fine-tuning, all model parameters are\ntuned on English task data. This setting is used in cross-\nlingual evaluation before. Right: In prompt tuning, only\nsmall ratio parameters are tuned. We use prefix prompts\nand use layer prompts in our experiments.\nPrompt Tuning. Fine-tuning on large pre-trained\nlanguage models leads to strong performance\non downstream tasks, however, it is memory-\nconsuming and lots of parameters need to save\nfor each task. In prompt tuning, only a small part\nof the parameters ( e.g., prompts or task classifier\n) are tuned during learning. However, it usually\nperforms not as good as compared to fine-tuning.\nRecently, Lester et al. (2021) find prompt tuning\ncan be better than fine-tuning when the model\nsize is not extremely large (10 billion parameters).\nPrefix-tuning (Li and Liang, 2021) obtains compa-\nrable performance for natural language generation\ntasks. Liu et al. (2022) shows prompt tuning can be\nmatched to fine-tuning on language understanding\ntasks even at hard sequence tagging tasks.\nWe investigate prompt tuning on cross-lingual\nunderstanding on a pre-trained multilingual lan-\nguage model. The framework is shown in Figure 1.\nOur setting is similar to Li and Liang (2021); Liu\net al. (2022). The continuous prompts are added\nas prefix tokens and tuned during learning. In the\nimplementation, the prompts are operated as past\nkeys and values in each transformer layer. Each\ntransformer layer has separated prompts. These\ncontinuous prompts are optimized, but multilingual\nlanguage model parameters are frozen.\n3 Experiments Setup\n3.1 Datasets.\nWe perform experiments on four datasets included\nin XTREME: cross-lingual natural language infer-\nence (XNLI; Conneau et al., 2018), cross-lingual\nadversarial dataset for paraphrase identification\n(PAWS-X; Yang et al., 2019), part-of-speech tag-\nging on the Universal Dependencies (UD-POS;\nNivre et al., 2018), cross-lingual question answer-\ning on XQuAD (Artetxe et al., 2020) and TyDiQA-\nGoldP (Clark et al., 2020). Three categories of\ndownstream tasks are included: (1) sentence clas-\nsification); (2) structure prediction; (3) question\nanswering.\n3.2 Training Details.\nOur frozen models are built on the top of the pre-\ntrained XLM-R checkpoint of LARGE size with\nabout 560M parameters. Previous work (Hu et al.,\n2020) shows it achieves stronger performance than\nmBERT1. All our experiments were run with Hug-\ngingface (Wolf et al., 2020). More details are in\nthe appendix.\nPrompt Length. Prompt length usually plays an\nimportant role in prompt tuning. In our experi-\nments, we treat this as a hyper-parameter. Longer\nprompt length often leads to have higher perfor-\nmance. In our experiments, prompt length is set to\n16 or 32 and tuned on the English validation set.\n1Some preliminary results are obtained with mBERT.\n5479\nModel Sentence Classification Structured Prediction Question Answering\nXNLI PAWS-X UD-POS XQuAD TyDiQA\nMetrics Acc. Acc. F1 F1 / EM F1 / EM\nFine Tuning\nMBERT* 65.4 81.9 70.3 64.5 / 49.4 59.7 / 43.9\nXLM-R-LARGE* 79.2 86.4 72.6 76.6 / 60.8 65.1 / 45.0\nXLM-R-LARGE+ 79.2 - 75.0 77.2 / 61.6 64.3 / 45.8\nXLM-R-LARGE(OUR) 78.8 (0.2) 87.9 (0.5) 74.4 (0.7) 77.3 (0.4) / 61.8(0.5) 70.1(0.6) / 51.7(2.7)\nPrompt Tuning\nXLM-R-LARGE 79.9(0.1) 88.4(0.3) 75.4(0.2) 79.0(0.2) /64.1(0.4) 71.5(0.4) /55.1(0.6)\nTable 1: Zero-shot cross-lingual transfer evaluation results (with standard deviation) on XTREME structured\nprediction, question answering, and sentence classification tasks. For both fine tuning and prompt tuning, models\nare only fine-tuned on the English training data but evaluated on all target languages. Baseline fine-tuning results\nwith “*” and “+” are taken from (Hu et al., 2020) and (Ruder et al., 2021) respectively. More results are shown in\nthe Appendix.\n4 Results\nTuned Parameter Sizes Comparison For the\nprompt tuning test results in Table 1, we did limited\ntuning on prompt length. The prompt length is 16,\nexcept prompt length for task XNLI is 32. With\nonly 0.1% to 0.3% additional prompt parameters\nas compared to the original model, the framework\nalready demonstrates strong cross-lingual results.\nOverall Results Table 1 shows the zero-shot\ncross-lingual results on four different tasks. Prompt\ntuning performs much better than fine-tuning, espe-\ncially for hard sequence task question answering.\nAnd prompt tuning is also with smaller variance.\nPreviously, although with parallel data or\nmore monolingual data, cross-lingual transfer re-\nsults (Zhang et al., 2019; Luo et al., 2021; Ruder\net al., 2021) on question answering and structured\nprediction tasks improved only slightly. With\nprompt tuning, there is larger performance gains\nfor question answering and structured prediction\ntasks. It suggests that prompt tuning is a better\ntuning method for cross-lingual transfer.\nCross-lingual Transfer Gap According to the\nabove result, on average, prompt tuning achieves\nbetter performance than fine tuning. Table 2 shows\nthe cross-lingual transfer gap of the two different\ntuning methods. Prompt tuning can also reduce the\ngap significantly.\nDiscussion In our preliminary experiments, for\nthe smaller size model (e.g., mBERT), prompt tun-\ning perform a little worse than fine tuning on En-\nglish, and match the performance of fine-tuning on\nall languages. The language model size still matter.\nThere is still some space for smaller size model.\nXNLI PAWS-X UD-POS XQuAD\nFine Tuning 10.2 12.4 24.3 16.3\nPrompt Tuning 9.7 8.7 20.7 14.5\nTable 2: Cross-lingual transfer gap of the two tuning\nmethods. The cross-lingual transfer gap is the perfor-\nmance difference between English test set and the aver-\nage of the other languages. The smaller is better.\nThis also indicates potential for future work with\nbetter prompt tuning method.\n5 Analysis\nIn order to perform some analysis on prompt tuning\nand fine tuning, we select 1000 samples for each\nlanguage (en, de, es, fr, ja, ko, zh ) from PAWS-\nX (Yang et al., 2019) dataset. For each English\nlanguage sample in our selections, there is a human\ntranslated sample from the other six languages. 2\nFigure 2 shows t-SNE visualization of sample\nrepresentations from frozen multilingual language\nmodel XLM-R. Samples’ representations are clus-\ntered well respect to languages, however, there is\nweak correlation with labels.\n5.1 Language Representation Changes\nFor each tuning method (fine-tuning and prompt-\ntuning), Table 3 shows the cosine similarity of rep-\nresentations from frozen language model and tuned\nmodel. According to the results, both of two tuned\nmethod make notable change on sentence represen-\ntations. However, the average cosine similarity of\nfine-tuning is much smaller. It indicates that fine-\ntuning leads much larger changes on sentence rep-\n2Each sample in PAWS-X dataset is a sentence pair. In the\nfollowing experiments, we treat the representations at CLS\ntoken as the sample sentence representations.\n5480\n(a) Visualization before fine tuning (FT).\n (b) Visualization before prompt tuning (PT).\n(c) Decision boundaries after fine tuning (FT).\n (d) Decision boundaries after prompt tuning (PT).\nFigure 2: T-SNE visualization of representations of four languages (en: English; de: German; ja: Japanese; zh:\nChinese) before and after two different tuning methods on English task data. The decision boundaries after prompt\ntuning is aligned much better.\nresentations than prompt tuning. We can also see\nrepresentation changes is larger when tuning is on\nMNLI, while prompt tuning still has less changes\non representations.\nen de es fr ja ko zh\nTraining on PA WS\nFT 25.2 26.5 24.5 25.2 18.9 15.0 22.6\nPT 57.6 56.8 57.2 57.7 58.7 59.4 59.5\nTraining on MNLI\nFT -16.9 -19.1 -16.3 -14.5 -16.7 -11.8 -14.9\nPT 32.2 32.1 31.2 32.1 33.8 36.0 35.8\nTable 3: Cosine similarity (%) of representations after\ntuning for each language. FT: fine-tuning; PT: prompt\ntuning. These checkpoints are on tuned on two English\ndatasets: PAWS and MNLI.3\n5.2 Cross-lingual Alignment After Tuning\nWe compute the averaged cosine similarity of all\nthe 1000 translation pairs for each language pair\n<en , xx>, where xx is de, es, fr, ja, ko or zh. We\nalso compute averaged cosine similarity of all the\n1000*999/2 non-translations for each language pair.\nAs shown in Table 3, both fine tuning and prompt\ntuning are doing well. Prompt tuning has the ad-\nvantage in the sense that they change the represen-\ntation more mildly, still have high cosine similarity\non translation pairs. This resulted in more robust\ntransfer and less overfitting.\n5.3 Decision Boundaries\nPrompt tuning keeps high cross-lingual alignment\nwith fewer changes in the previous subsections.\nThe general level of the learned representations’\nquality is still unknown, though. The learned repre-\nsentations quality are examined in this subsection.\nFigure 2 (a) and (b) show t-SNE visualization of\nrepresentations before two different tuning meth-\nods. Each dot in the two figures is a PAWS-X\nsample from four languages: German (de), zh (Chi-\nnese), en (English), ja (Japanese). The blue sam-\nple is a paraphrase, the orange sample is a non-\nparaphrase. Samples of the same language are\n5481\nen-de en-es en-fr en-ja en-ko en-zh\nTraining on MNLI\nFT 81.5 85.4 83.0 71.8 68.2 73.9\nFT-neg 52.6 53.1 52.8 51.5 50.6 50.0\nrel-diff (%) 54.8 60.8 57.2 39.4 34.8 47.8\nPT 96.4 97.3 96.6 94.8 93.8 95.0\nPT-neg 91.0 91.1 90.8 90.5 90.1 90.2\nrel-diff (%) 5.9 6.8 6.4 4.8 4.1 5.3\nTraining on PA WS\nFT 90.4 92.1 88.8 76.8 75.3 82.0\nFT-neg 13.3 13.2 13.4 14.3 14.4 13.6\nrel-diff (%) 580 598 563 437 423 503\nPT 98.4 98.6 98.3 96.3 96.0 96.7\nPT-neg 88.1 88.1 88.3 89.1 89.4 88.9\nrel-diff (%) 11.7 11.9 11.3 8.1 7.4 8.8\nTable 4: Cosine similarity (%) of translation pairs after\ntuning on two English dataset: MNLI and PAWS. “-neg“\nmeans the average cosine similarity of non-translations\nfor each language pair. “rel-diff“ means the relative dif-\nference between translation and non-translations. Two\ndifferent tuning method are shown, one is fine-tuning\n(FT), the other is prompt tuning (PT).\ngrouped together. However, label information is\nmissing from sample representations.\nFigure 2 (c) and (d) shows t-SNE (van der\nMaaten and Hinton, 2008) visualization after fine\ntuning (FT) and prompt tuning (PT). After tuning,\nboth have reasonable and nice separated represen-\ntations. For each language, we also plot logistic\nregression decision boundary for these t-SNE em-\nbeddings. The decision boundaries for various lan-\nguages vary significantly after fine tuning. The\nEnglish decision boundary can not separate well\non German samples. After prompt tuning, the deci-\nsion boundaries of the four languages are surpris-\ningly aligned well. This suggest that prompt tuning\nlearns better language-independent classifier than\nfine tuning, although the tuning is only on English\ntraining set.\n6 Related Work\nRecently, several previous works show prompt tun-\ning for multilingual language models. Winata\net al. (2021) shows the multilingual skills of large\npre-trained models with few examples. Zhao\nand Schütze (2021); Huang et al. (2022); Qi et al.\n(2022) shows new proposed prompt tuning meth-\nods. The goal of our work is different from theirs.\nWe show prompt tuning is better than fine-tuning\nfor cross-lingual evaluation. We have a conclu-\nsion that our prompt tuning achieves higher perfor-\nmance than fine-tuning consistently in the setting.\nPrevious work (Zhao and Schütze, 2021; Huang\net al., 2022; Qi et al., 2022) only experimented on\nthe sentence classification task. Hard sequence tag-\nging tasks and question answering is not explored\nor the settings are in low resource regimes. We\ninvestigate cross-lingual transfer ability on various\nNLU tasks from XTREME (Hu et al., 2020), which\nis one of the important cross-lingual transfer evalua-\ntion benchmarks. Sentence classification, sequence\nlabeling, and question answering are included.\n7 Conclusion\nIn this work, we compared prompt tuning and fine\ntuning on cross-lingual understanding with mul-\ntilingual languages models, finding that prompt\ntuning achieves a better performance. This sug-\ngest that it is promissing to use prompt tuning on\ncross-lingual transfer.\nLimitations\nIn this work, we investigate the effects of prompt\ntuning on cross-lingual understanding and empiri-\ncally demonstrate some promising outcomes. We\nneed a lot of GPU resources to complete our exper-\niments. The experiments on large size pretrained\nmultilingual language models are conducted on\nA100s with 40G memory. Training can be acceler-\nated by using large batches.\nThis is a preliminary exploration of prompt tun-\ning on cross-lingual transfer. In this work, encoder-\nonly models are explored on natural language un-\nderstanding tasks in the paper. Future work may\nalso involve encoder-decoder models and other\ntasks.\nAcknowledgements\nWe would like to thank Salesforce AI Research\nteam for helpful discussions, and the reviewers for\ninsightful comments.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\n5482\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis CONNEAU and Guillaume Lample. 2019.\nCross-lingual language model pretraining. In Ad-\nvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversarial\nReProgramming. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4921–4933, Online. Association for\nComputational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, volume 119 of\nProceedings of Machine Learning Research, pages\n4411–4421. PMLR.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022. Zero-shot cross-\nlingual transfer of prompt-based tuning with a unified\nmultilingual prompt. ArXiv, abs/2202.11451.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark datasetfor cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Song-\nfang Huang, Fei Huang, and Luo Si. 2021. VECO:\nVariable and flexible cross-lingual pre-training for\nlanguage understanding and generation. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3980–3994, Online.\nAssociation for Computational Linguistics.\nJoakim Nivre, Mitchell Abrams, Željko Agi ´c, Lars\nAhrenberg, Lene Antonsen, Maria Jesus Aranzabe,\nGashaw Arutie, Masayuki Asahara, Luma Ateyah,\nMohammed Attia, et al. 2018. Universal dependen-\ncies 2.2.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\n5483\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nKunxun Qi, Hai Wan, Jianfeng Du, and Haolan Chen.\n2022. Enhancing cross-lingual natural language in-\nference by prompt-learning from cross-lingual tem-\nplates. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1910–1923, Dublin,\nIreland. Association for Computational Linguistics.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: Towards more challenging\nand nuanced multilingual evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10215–10245,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting knowledge from language models with auto-\nmatically generated prompts. In Empirical Methods\nin Natural Language Processing (EMNLP).\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579–2605.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung. 2021.\nLanguage models are few-shot multilingual learners.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning, pages 1–15, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 833–844, Hong\nKong, China. Association for Computational Linguis-\ntics.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3687–3692, Hong\nKong, China. Association for Computational Linguis-\ntics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1441–1451, Florence, Italy. Association for Compu-\ntational Linguistics.\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and\nsoft prompting for multilingual models. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8547–8555,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\nA Appendix\nA.1 More Training Details\nFor prompt tuning, we train with the Adam opti-\nmizer (Kingma and Ba, 2015) with no warmup step.\nBatch size is 32 for tasks, and with the exception\nof answering questions, which has a batch size of 8.\nLinear learning rate scheduler is used. We tune the\nlearning rate in {5e−2, 1e−2, 5e−3, 1e−3, 5e−\n4, 1e−4}. We train all prompt tuning models for\n30 epochs. Finally, tuned prompt length for MNLI\nis 32. It is 16 for the other tasks. We use A100s\nwith 40G memory and all experiments can be done\nin few hours.\n5484\nMethod en ar bg de el es fr hi ru sw th tr ur vi zh avg\n88.2 77.4 82.3 82.6 81.1 83.7 82.0 75.2 79 71.0 76.7 77.5 71.4 79.1 78.6 79.1\n88.3 76.9 81.9 81.9 81.4 83.6 81.6 74.3 78.1 70.1 75.8 77.6 70.7 78.8 77.6 78.6\nFT 88.1 77.5 82.4 81.8 81.3 83.4 82.6 75.0 78.9 70.3 75.6 78.1 70.8 78.5 78.3 78.8\n88.4 77.4 81.7 82.0 81.5 83.3 82.3 75.4 79.0 70.2 75.5 78.1 71.2 79.1 77.9 78.9\n88.2 77.6 82.5 81.7 80.9 83.2 81.9 75.1 78.2 69.5 76.5 77.6 71.0 78.8 78.6 78.8\n88.5 78.3 82.8 82.2 82.5 84.2 83.0 76.1 80.4 71.0 77.6 79.2 72.5 80.0 78.3 79.8\n88.7 78.7 82.9 82.1 82.8 84.3 83.2 76.1 80.4 71.0 77.6 79.2 72.5 80.0 78.3 79.8\nPT 88.8 78.1 82.7 81.7 81.9 84.0 83.2 75.9 80.7 71.4 77.5 79.3 72.5 79.4 78.7 79.7\n89.1 79.2 83.2 82.1 82.4 84.1 83.0 76.2 80.8 70.7 77.7 79.5 72.5 79.9 78.4 79.9\n89.0 78.7 83.2 82.2 82.8 84.3 83.4 76.2 80.8 71.3 77.9 79.2 72.5 80.3 78.2 80.0\nTable 5: XNLI accuracy scores for each language with fine-tuning (FT) and prompt tuning (PT).\nMethod en de es fr ja ko zh avg\n95.6 90.8 81.4 91.3 82.7 81.8 84.5 88.3\n95.7 90.5 91.0 91.3 81.7 81.2 84.0 87.9\nFT 95.4 89.4 90.8 90.9 80.5 80.6 84.0 87.4\n95.4 90.2 90.6 90.5 80.6 80.4 83.4 87.2\n94.7 91.0 91.4 92.1 82.4 93.2 84.2 88.6\n96.2 92.3 91.4 92.1 81.3 83.2 84.8 88.8\n95.3 91.6 91.1 92.0 82.7 83.1 84.2 88.6\nPT 95.4 90.9 91.4 91.8 82.1 82.8 84.7 88.4\n95.9 90.7 90.7 91.6 81.4 81.6 84.6 88.1\n95.6 91.6 90.5 91.7 82.2 81.7 83.0 88.0\nTable 6: PAWS-X accuracy scores for each language with fine-tuning (FT) and prompt tuning (PT).\nMethod en es de el ru tr ar vi th zh hi avg\n75.2 / 87.2 61.2 / 80.7 62.7 / 82.5 60.2 / 78.7 63.5 / 80.1 57.8 / 74.3 58.6 / 75.5 59.9 / 79.4 59.9 / 73.2 58.7 / 68.5 56.6 / 74.761.3 / 77.775.0 / 86.8 61.6 / 79.8 61.9 / 80.0 59.6 / 78.6 62.7 / 79.6 57.6 / 73.3 56.6 / 74.4 57.8 / 78.6 61.3 / 72.4 60 / 67.5 58.2 / 74.661.1 / 76.9PT 75.5 / 87.0 64.0 / 81.3 64.8 / 80.9 62.4 / 80.0 63.8 / 80.1 57.7 / 73.8 55.9 / 72.8 60.2 / 79.5 62.3 / 73.4 61.4 / 69.6 59.8 / 76.162.5 / 77.775.8 / 87.0 63.0 / 81.4 62.4 / 79.4 62.1 / 79.9 62.9 / 79.8 56.9 / 73.6 57.4 / 74.6 59.6 / 78.5 62.8 / 74.7 60.5 / 70.5 57.5 / 74.261.9 / 77.676.0 / 87.4 62.8 / 80.8 65.0 / 80.2 61.2 / 78.3 63.1 / 79.5 56.3 / 72.3 57.3 / 73.9 57.6 / 77.5 62.9 / 71.6 61.0 / 68.7 58.4 / 74.262.0 / 76.8\n77.2 / 88.4 65.1 / 83.1 64.8 / 81.4 63.7 / 81.2 58.7 / 80.2 58.7 / 74.6 60.3 / 77.0 61.4 / 80.6 66.4 / 74.7 60.3 / 68.6 61.8 / 78.163.5 / 78.977.4 / 88.5 64.4 / 82.3 64.8 / 81.2 63.5 / 80.8 64.7 / 80.7 58.3 / 74.1 60.3 / 76.8 61.0 / 80.3 66.6 / 75.0 61.7 / 70.2 61.5 / 77.564.0 / 78.9PT 77.4 / 88.6 65.4 / 83.4 64.5 / 80.9 64.0 / 81.2 64.1 / 80.7 58.7 / 74.9 59.8 / 76.5 62.1 / 81.4 66.6 / 75.4 61.3 / 69.9 62.8 / 77.864.2 / 79.277.1 / 88.5 64.7 / 82.9 63.9 / 80.7 62.7 / 80.5 64.7 / 80.4 59.2 / 74.6 59.7 / 76.3 60.8 / 80.7 66.6 / 74.6 61.0 / 69.1 61.6 / 77.664.7 / 78.777.9 / 88.7 65.0 / 83.0 64.2 / 81.2 63.4 / 80.2 64.8 / 80.9 58.2 / 75.4 60.2 / 77.0 62.9 / 81.3 67.3 / 75.9 60.7 / 69.8 61.1 / 77.964.2 / 79.2\nTable 7: XQuAD results (EM / F1) for each language with fine-tuning (FT) and prompt tuning (PT).\nMethod en ar bn fi id ko ru sw te avg\n60.5 / 74.2 51.5 / 71.5 50.4 / 68.6 51.0 / 67.6 62.5 / 78.6 49.6 / 60.9 45.3 / 67.7 44.7 / 65.7 56.7 / 75.7 46.2 / 70.0\n57.7 / 71.8 51.5 / 71.0 50.4 / 70.4 53.5 / 70.3 61.4 / 77.1 53.6 / 64.5 47.8 / 68.6 50.3 / 70.3 57.0 / 75.7 53.7 / 71.1\nFT 57.7 / 73.2 51.9 / 72.5 48.7 / 66.4 53.6 / 69.7 59.8 / 77.0 50.7 / 59.6 50.7 / 68.0 49.1 / 69.1 58.0 / 77.8 53.4 / 70.4\n58.6 / 71.7 53.0 / 71.6 46.0 / 62.5 53.7 / 68.4 59.8 / 75.7 52.5 / 63.0 40.4 / 65.2 48.9 / 69.2 58.4 / 76.3 52.4 / 69.3\n59.8 / 72.3 48.3 / 70.6 52.2 / 68.6 49.7 / 67.5 60.4 / 77.7 55.1 / 65.5 38.9 / 65.2 45.4 / 66.6 56.8 / 75.4 51.8 / 69.9\n61.8 / 75.0 53.7 / 72.3 48.7 / 67.0 58.2 / 73.0 63.0 / 77.9 52.9 / 63.6 50.2 / 70.0 47.5 / 68.5 57.5 / 75.3 54.8 / 71.5\n60.7 / 74.0 53.1/72.2 45.1 / 64.5 55.9 / 71.8 63.5 / 78.3 51.8 / 61.9 52.3 / 71.0 48.9 / 68.9 58.4 / 76.2 54.4 / 71.0\nPT 60.2 / 73.6 54.8 / 73.9 52.2 / 70.0 56.6 / 71.4 64.8 / 78.7 52.5 / 62.3 53.1 / 71.4 51.1 / 70.7 61.6 / 79.1 56.3 / 72.3\n62.0 / 75.3 53.6 / 73.0 46.0 / 64.9 57.3 / 71.3 63.7 / 78.6 53.3 / 62.0 52.7 / 71.8 48.1 / 69.0 58.7 / 75.5 55.0 / 71.3\n61.4 / 74.5 54.9 / 72.8 46.9 / 66.3 56.8 / 71.4 63.2 / 77.6 54.3 / 63.0 53.1 / 71.1 47.9 / 68.4 58.6 / 76.4 55.2 / 71.3\nTable 8: TyDiQA-GoldP results (EM / F1) for each language with fine-tuning (FT) and prompt tuning (PT).\n5485",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.816063404083252
    },
    {
      "name": "Sentence",
      "score": 0.6384899616241455
    },
    {
      "name": "Fine-tuning",
      "score": 0.5654128789901733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5196208357810974
    },
    {
      "name": "Natural language processing",
      "score": 0.5006453990936279
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.46552082896232605
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4627414345741272
    },
    {
      "name": "Language model",
      "score": 0.4524100124835968
    },
    {
      "name": "Transfer (computing)",
      "score": 0.44863882660865784
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.4233390688896179
    },
    {
      "name": "Transfer of learning",
      "score": 0.4203481674194336
    },
    {
      "name": "Linguistics",
      "score": 0.09382873773574829
    },
    {
      "name": "Physics",
      "score": 0.07371458411216736
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ]
}