{
  "title": "HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image",
  "url": "https://openalex.org/W4382460188",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5067981329",
      "name": "Zhuchen Shao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5092354292",
      "name": "Yang Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5005359768",
      "name": "Hao Bian",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100409856",
      "name": "Jian Zhang",
      "affiliations": [
        "Peking University Shenzhen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100758892",
      "name": "Guojun Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101653272",
      "name": "Yongbing Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3041522598",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W3185545942",
    "https://openalex.org/W3203898052",
    "https://openalex.org/W3091730259",
    "https://openalex.org/W4225604175",
    "https://openalex.org/W3203263549",
    "https://openalex.org/W2998533832",
    "https://openalex.org/W4283801948",
    "https://openalex.org/W3204764952",
    "https://openalex.org/W3033382446",
    "https://openalex.org/W3015387539",
    "https://openalex.org/W3100722990",
    "https://openalex.org/W2890655214",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3018295606",
    "https://openalex.org/W2318810549",
    "https://openalex.org/W3202672547",
    "https://openalex.org/W4226181149",
    "https://openalex.org/W4283816523",
    "https://openalex.org/W3089090082",
    "https://openalex.org/W3100084586",
    "https://openalex.org/W3176016422",
    "https://openalex.org/W3126201322",
    "https://openalex.org/W3202214962",
    "https://openalex.org/W3186681406",
    "https://openalex.org/W2970165323",
    "https://openalex.org/W3043535018",
    "https://openalex.org/W3011132328",
    "https://openalex.org/W2745940724",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W4291021272",
    "https://openalex.org/W4295937529",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3093416812",
    "https://openalex.org/W3034447539",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W4283704078",
    "https://openalex.org/W2986297814",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3203838058",
    "https://openalex.org/W3135547872",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W3176719058",
    "https://openalex.org/W3093385833",
    "https://openalex.org/W4230920041"
  ],
  "abstract": "Survival prediction based on whole slide images (WSIs) is a challenging task for patient-level multiple instance learning (MIL). Due to the vast amount of data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped property of WSI, it is difficult to fully explore spatial, contextual, and hierarchical interaction in the patient-level bag. Many studies adopt random sampling pre-processing strategy and WSI-level aggregation models, which inevitably lose critical prognostic information in the patient-level bag. In this work, we propose a hierarchical vision Transformer framework named HVTSurv, which can encode the local-level relative spatial information, strengthen WSI-level context-aware communication, and establish patient-level hierarchical interaction. Firstly, we design a feature pre-processing strategy, including feature rearrangement and random window masking. Then, we devise three layers to progressively obtain patient-level representation, including a local-level interaction layer adopting Manhattan distance, a WSI-level interaction layer employing spatial shuffle, and a patient-level interaction layer using attention pooling. Moreover, the design of hierarchical network helps the model become more computationally efficient. Finally, we validate HVTSurv with 3,104 patients and 3,752 WSIs across 6 cancer types from The Cancer Genome Atlas (TCGA). The average C-Index is 2.50-11.30% higher than all the prior weakly supervised methods over 6 TCGA datasets. Ablation study and attention visualization further verify the superiority of the proposed HVTSurv. Implementation is available at: https://github.com/szc19990412/HVTSurv.",
  "full_text": "HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction\nfrom Whole Slide Image\nZhuchen Shao1, Yang Chen1, Hao Bian1, Jian Zhang2, Guojun Liu3, Yongbing Zhang4*\n1Tsinghua Shenzhen International Graduate School, Tsinghua University\n2School of Electronic and Computer Engineering, Peking University\n3Computer Science and Technology, Harbin Institute of Technology\n4Computer Science and Technology, Harbin Institute of Technology (Shenzhen)\nshaozc0412@gmail.com, ybzhang08@hit.edu.cn\nAbstract\nSurvival prediction based on whole slide images (WSIs) is a\nchallenging task for patient-level multiple instance learning\n(MIL). Due to the vast amount of data for a patient (one or\nmultiple gigapixels WSIs) and the irregularly shaped prop-\nerty of WSI, it is difﬁcult to fully explore spatial, contextual,\nand hierarchical interaction in the patient-level bag. Many\nstudies adopt random sampling pre-processing strategy and\nWSI-level aggregation models, which inevitably lose criti-\ncal prognostic information in the patient-level bag. In this\nwork, we propose a hierarchical vision Transformer frame-\nwork named HVTSurv, which can encode the local-level rel-\native spatial information, strengthen WSI-level context-aware\ncommunication, and establish patient-level hierarchical in-\nteraction. Firstly, we design a feature pre-processing strat-\negy, including feature rearrangement and random window\nmasking. Then, we devise three layers to progressively ob-\ntain patient-level representation, including a local-level in-\nteraction layer adopting Manhattan distance, a WSI-level\ninteraction layer employing spatial shufﬂe, and a patient-\nlevel interaction layer using attention pooling. Moreover,\nthe design of hierarchical network helps the model become\nmore computationally efﬁcient. Finally, we validate HVT-\nSurv with 3,104 patients and 3,752 WSIs across 6 can-\ncer types from The Cancer Genome Atlas (TCGA). The\naverage C-Index is 2.50-11.30% higher than all the prior\nweakly supervised methods over 6 TCGA datasets. Ablation\nstudy and attention visualization further verify the superior-\nity of the proposed HVTSurv. Implementation is available at:\nhttps://github.com/szc19990412/HVTSurv.\nIntroduction\nIn computational pathology, survival prediction based on gi-\ngapixels whole slide images (WSIs) is a weakly supervised\nlearning (WSL) task involving local-level tumor microenvi-\nronment interactions (Chen et al. 2021b), WSI-level tumor-\nrelated tissue interactions (Abbet et al. 2020) and patient-\nlevel heterogeneous tumor interactions (Carmichael et al.\n2022). Multiple instance learning (MIL) is usually adopted\nto tackle such a WSL problem (Shao et al. 2021b,a). How-\never, bag-based representation learning in MIL still remains\nan open and challenging problem.\n*Corresponding author: Yongbing Zhang.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Application of MIL in WSIs-based tasks. There is\nonly one WSI-level bag for the WSI-level MIL, where the\nWSI-level label is known, and the patch-level label is un-\nknown. For the patient-level MIL, there are one or more than\none WSI-level bags where the patient-level label is known,\nbut neither the WSI-level nor patch-level label is unknown.\nDifferent from natural images, WSIs have the property of\nhigh-resolution and wide ﬁeld of view (Srinidhi, Ciga, and\nMartel 2021), so the aggregation of bag-level representation\nwill impose a great demand on computational complexity.\nIn addition, different from the WSI-level MIL problem, as\nshown in Fig. 1, survival prediction based on WSIs is a\npatient-level MIL problem (Fan et al. 2021). Since the tu-\nmor may have a composite tissue structure, multiple WSIs\nare usually collected for patient diagnosis. Therefore, in sur-\nvival prediction, we have to face two dilemmas: 1) multi-\nple WSIs inevitably lead to linearly multiplied data volume;\n2) the aggregation of multiple WSI-level bags for a patient.\nFor the aggregation of the WSI-level bag, the risk infor-\nmation in survival prediction is often reﬂected in a series\nof histological patterns corresponding to disease progres-\nsion. For example, the local-level co-localization of tumors\nand lymphocytes (Shaban et al. 2019) and the WSI-level\nmetastatic distribution of sentinel lymph nodes (Kim et al.\n2020) have been shown to correlate with prognosis. There-\nfore, the spatial and contextual information must be fully\nconsidered in a WSI-level bag. Moreover, for the patient-\nlevel bag, intratumoral heterogeneity will inevitably lead to\ndiverse tumor microenvironments in different WSIs (Vitale\net al. 2021). So the patient-level contextual information be-\ntween instances in different WSI-level bags must be con-\nsidered, and these three-level interactions further constitute\nhierarchical information in a patient-level bag.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2209\nTo address the challenges mentioned above, numerous\nworks are proposed from two majority aspects: 1) compu-\ntational cost; 2) spatial, contextual and hierarchical informa-\ntion encoding. For the computational cost problem, random-\nsample-based methods (Huang et al. 2021b; Di et al. 2022)\nand cluster-based methods (Yao et al. 2020; Muhammad\net al. 2021; Shao et al. 2021a) are widely used. By ran-\ndomly selecting from different clusters, many cluster-based\nmethods try to include various richer tissue types. However,\na large number of tissue patches are discarded and usually\nlack structural information. For the spatial information en-\ncoding, some GNN-based methods (Wang et al. 2021; Li\net al. 2018; Chen et al. 2021a) adopt the topological struc-\nture to encode neighbor node information in WSI. In addi-\ntion, Transformer-based methods (Huang et al. 2021b; Shao\net al. 2021b) adopt trade-off strategies like using the sin-\ncos embedding or applying convolution to implicitly encode\nlocation information. However, it is still an unsolved prob-\nlem to efﬁciently encode 2D spatial information over such\nhigh resolution and irregularly shaped WSIs. For the con-\ntextual and hierarchical information encoding, patch-based\ngraph convolutional network (Chen et al. 2021a), Nystrom\nself-attention (Shao et al. 2021b) and non-local attention (Li,\nLi, and Eliceiri 2021) are used to encode WSI-level inter-\nactions. There are also other methods (Di et al. 2022; Fan\net al. 2021) hierarchically processing WSI and patient-level\nbags to encode hierarchical information. However, limited\nby the vast amount of data for patient-level survival predic-\ntion, randomly sampled patches are always used in the meth-\nods above, which inevitably lose potential risk information.\nIn this work, we propose a hierarchical vision Trans-\nformer for patient-level survival prediction (HVTSurv) that\nprogressively explores local-level spatial, WSI-level contex-\ntual and patient-level hierarchical interactions in the patient-\nlevel bag. The main contributions are as follows:\n1) To alleviate high computational complexity, we de-\nvise a local-to-global hierarchically processing framework.\nSpeciﬁcally, we leverage the window attention mechanism\nto design the local and shufﬂe window block, which can sig-\nniﬁcantly reduce the cost of Transformer. Therefore, we can\ntake advantage of all the patches in the patient-level bag.\n2) We propose a new feature generation method for spa-\ntial information embedding, which can fully reﬂect the local\ncharacteristics in both horizontal and vertical directions. In\naddition, we adopt Manhattan distance in the local window\nblock to measure the local relative position.\n3) For the contextual and hierarchical information encod-\ning, we design the local-level, WSI-level and patient-level\ninteraction layers to hierarchically deal with survival predic-\ntion. Besides, we adopt a random window masking strategy\nin feature generation to further exploit the advantages of our\nhierarchical processing framework.\n4) Our HVTSurv signiﬁcantly outperforms state-of-the-\nart (SOTA) methods over 6 public cancer datasets from\nThe Cancer Genome Atlas (TCGA) with less GPU Mem-\nory Costs. Besides, in the patient stratiﬁcation experiment,\nthere is a statistically signiﬁcant difference (P-Value<0.05)\nover all of the 6 cancer types. Attention visualization further\nveriﬁes our conclusion from experiments.\nRelated Work\nApplication of MIL in WSIs\nThe application of MIL in WSIs can be divided into two cat-\negories. As shown in Fig. 1, MIL tasks in WSIs include WSI\nand patient-level MIL. The WSI-level MIL is suitable for\nsome tasks such as tumor&non-tumor classiﬁcation. Com-\nmon solutions include instance-based methods (Campanella\net al. 2019; Xu et al. 2019; Kanavati et al. 2020; Lerousseau\net al. 2020; Chikontwe et al. 2020) and embedding-based\nmethods (Tomita et al. 2019; Hashimoto et al. 2020; Naik\net al. 2020; Lu et al. 2021; Hou et al. 2022).\nThe patient-level MIL is suitable for tasks like survival\nprediction that only has patient-level labels. Common solu-\ntions include simultaneous processing (Zhu et al. 2017; Yao\net al. 2020; Muhammad et al. 2021; Shao et al. 2021a; Ab-\nbet et al. 2020; Wang et al. 2021; Li et al. 2018; Huang et al.\n2021b) and hierarchical processing (Chen et al. 2021a; Di\net al. 2022; Fan et al. 2021) of all the WSIs from a patient.\nSimultaneous processing methods generally consider all the\nWSI-level bags in a patient as one bag. In contrast, the hier-\narchical processing methods ﬁrst aggregate features in the\nWSI-level bag and then further aggregate different WSI-\nlevel bags in the patient-level bag. In general, hierarchical\nprocessing methods have the potential to more effectively\nmodel the spatial information in the WSI-level bag and con-\ntextual and hierarchical information in the patient-level bag.\nApplication of WSIs for Survival Prediction\nFor the application of WSIs in survival prediction, a two-\nstage framework is widely used to predict patient hazard\nscores: 1) sampling and encoding patches; 2) patch features\naggregation. In the ﬁrst stage, constrained by limited com-\nputing resources, clustering and random sampling methods\n(Zhu et al. 2017; Yao et al. 2020; Muhammad et al. 2021;\nShao et al. 2021a) are widely used to select representative\ntissue phenotypes in WSI. However, these randomly sam-\npled patches are not context-aware and lost the interactions\nbetween cells and tissue types, which are prognostic for pa-\ntient survival prediction.\nIn the second stage, many CNN based, GNN based and\nTransformer based methods are used. Both Yao et al. (Yao\net al. 2020) and Shao et al. (Shao et al. 2021a) use a small\nCNN network to aggregate the sampled feature. However,\nCNN-based methods have inherent limitations in modeling\nglobal topological information. For the GNN-based method,\nChen et al. (Chen et al. 2021a) formulate WSIs as a graph-\nbased data structure to obtain hierarchical representations.\nWang et al. (Wang et al. 2021) emphasize the tumor mi-\ncroenvironment graph construction. Di et al. (Di et al. 2022)\npropose a big-hypergraph factorization neural network to\nobtain the high-order representations. However, the network\ndepth limitation brought by a large amount of data makes\nGNN more challenging to encode WSI-level information.\nFor the Transformer based method, Huang et al. (Huang\net al. 2021b) adopt 2D sin-cos position encoding and Trans-\nformer encoder blocks to obtain the bag-level feature. There\nis still room for improvement in position encoding and fea-\nture aggregation to this method.\n2210\nFigure 2: Overview of the proposed pipeline. For all WSIs in a patient, we ﬁrst segment and slice all the tissue patches and\nuse an ImageNet pre-trained ResNet50 to extract each patch as a 1024-dimensional feature vector. Then we adopt the feature\nrearrangement to ensure that, after window partition, patch features in the same window are adjacent in the rearranged feature\nsequence. Besides, we apply a random window masking strategy to split a WSI bag into m sub-WSI bags to increase the\nrobustness of the model for tumor heterogeneity. Next, we use the generated features to perform the aggregation. For each WSI,\nlocal-level interaction layer will ﬁrst encode local spatial information. And then, spatial shufﬂe is applied to make the model\ncarry out similarity computation for features in different local windows. Finally, all the WSI-level features will be concatenated\nto perform the attention pooling, and we use the patient-level representation to predict the patient’s hazard risk. The bottom is\nthe overview of the local window block and shufﬂe window block. In the local window block, we add relative position bias\nto the self-attention calculation, and the distance is measured using the Manhattan distance. In the shufﬂe window block, we\nshufﬂe the patient feature in the different local windows and then perform the window partition and self-attention calculation.\nMethod\nConsidering a set of N patients Xi, for i = 1;:::;N , each\npatient Xi has one or multiple WSIs, we have follow-up la-\nbel (Ti;Ci), where Ti stands for observation time and Ci\nstands for survival status. The binary status Ci 2f0; 1gin-\ndicates whether Ti is a survival time (C i = 0 ) or a right-\ncensored time (Ci = 1). Our task is to predict the survival\nprobability based on all the WSIs for each patient. The ac-\ncuracy is measured by the consistency between the sorted\nsurvival probability and sorted follow-up label sets.\nTo better perform survival prediction, the discrete-time\nsurvival model (Vale-Silva and Rohr 2021; Chen et al. 2022)\nis used in this paper. Brieﬂy, we subdivide the survival\ntime scale into n intervals: [t0;t1) ;:::; [tn\u00001;tn), where\nt1;:::;t n\u00001 deﬁne the evenly divided points of survival\ntimes for uncensored patients and t0 = 0 ;tn = 1. Each\npatient observation time will be attributed to an interval as:\nTi = kiff Ti 2[tk;tk+1): (1)\nTherefore, for each patient, the conditional hazard probabil-\nity h(k j Xi) can be deﬁned as its failure probability in\ninterval [tk;tk+1):\nh(kjXi) = P(Ti = kjTi \u0015k;Xi): (2)\nSurvival probability S(k jXi) can be deﬁned as its obser-\nvation probability at least to the end of interval [tk;tk+1):\nS(kjXi) = P(Ti >k jXi) =\nkY\ns=1\n(1 \u0000h(sjXi)): (3)\nSince each patient label is known, while neither WSIs la-\nbel nor patches label is unknown, survival prediction is a\n2211\nWSL problem, which can be solved by the MIL methods. To\nbetter predict h(kjXi) from the patient-level bag, as shown\nin Fig. 2, we propose a Transformer-based framework which\nis composed of feature generation and feature aggregation.\nFeature Generation\nWe ﬁrst convert patches to features in WSI processing. Then,\nwe adopt feature rearrangement to maintain the local 2D rel-\native position in rearranged features. Finally, we employ the\nrandom window masking to further strengthen the contex-\ntual and hierarchical interactions in feature aggregation.\nWSI processing WSIs have gigapixels and often contain\nmany blank regions. We follow the CLAM (Lu et al. 2021)\nprocessing steps to remove the background regions, and then\ncut out 256\u0002256 images at 20\u0002 resolution (0.5 \u0016m/pixel).\nA ResNet50 model pre-trained on ImageNet is employed to\nembed each patch in a 1024-dimensional feature vector.\nFeature rearrangement In patient-level survival predic-\ntion, a patient may correspond to multiple WSIs. To relieve\ncomputational cost, we employ the window attention mech-\nanism (Liu et al. 2021). Limited by irregularly shaped prop-\nerty of WSI, previous raster scanning method will inevitably\nlose correct 2D spatial information. To better reﬂect the lo-\ncal characteristics in both horizontal and vertical directions\nwithin a window, a feature rearrangement method is pro-\nposed to ensure the closeness in both directions of the 2D\nspace after the window partition. The speciﬁc implementa-\ntion is shown in Algorithm 1, and the Euclidean distance is\nused in the HNSW (Malkov and Yashunin 2018). We also\npresent qualitative and quantitative experimental results in\nAppendix Fig. 1 and Appendix Fig. 2, respectively.\nAlgorithm 1: Feature rearrangement\nInput A WSI-level bag Hi = fhi;1;:::; hi;bg, where\nhi;j 2Rd is the embedding of the jth instance, Hi 2\nRd\u0002b. Corresponding coordinates Zi = fzi;1;:::; zi;bg,\nwhere zi;j 2R2 is the original coordinate of the jth in-\nstance in WSI, Zi 2R2\u0002b. Window size w.\nOutput Rearranged features Hr 2Rd\u0002B.\nba  d b\nwe\u0002w\u0000b .Padding width\nB  b+ ba .Length after padding\nHs  Re\rectPadding\n\u0000\nHi;width =\n\u0000\nbba\n2 c;ba \u0000bba\n2 c\n\u0001\u0001\nZs  Re\rectPadding\n\u0000\nZi;width =\n\u0000\nbba\n2 c;ba \u0000bba\n2 c\n\u0001\u0001\nZs  Zs=256 .Scale the original coordinates\nZs  [Zs \u0000(xmin;ymin)] + 1 .(x;y) is the coordinate\nInitialize Hr as ;\nfor idx2[0 : B : w] do\n.Select the wfeatures closest to zs;1 in Zs, including\nzs;1 itself\nselect idx Hnsw:query (zs;1;topn = w)\n.Add the wclosest features to the new array\nHr  Hr + Hs[select idx]\nHs  Hs \u0000Hs[select idx] .Delete selected h\nZs  Zs \u0000Zs[select idx] .Delete selected z\nend for\nRandom window masking To increase the robustness of\nthe model for tumor heterogeneity and further exploit the ad-\nvantages of our hierarchical processing framework, we pro-\npose a random window masking strategy. A WSI bag will be\nfurther split into several sub-WSI bags. Inspired by the su-\nperpixel sampling strategy (Bian et al. 2022), we sample at\nthe window level to maintain 2D spatial information in each\nlocal window. Speciﬁcally, we perform mrandom window\nsampling for the rearranged feature sequence. A WSI feature\nis divided into msub-WSIs for subsequent feature aggrega-\ntion. To avoid adding additional computational burden, the\nfeature number of each sub-WSI is1=mof the original WSI.\nFeature Aggregation\nTo better encode the spatial, contextual and hierarchical in-\nformation in the patient-level bag, we propose a hierarchical\nvision Transformer named HVTSurv to perform feature ag-\ngregation in the patient-level bag. The HVTSurv is mainly\ncomposed of three layers, including the local-level, WSI-\nlevel and patient-level interaction layer. In our paper, local-\nlevel means patch features within the same window, WSI-\nlevel means patch features from different local windows\nwithin a sub-WSI, and patient-level means patch features\nfrom different sub-WSIs within a patient. The overview of\nproposed three interaction layers is shown in Fig. 2.\nLocal-level interaction layer To encode local spatial in-\nformation in each WSI, we design a local-level interaction\nlayer. Due to the irregularly shaped property of WSIs, the\nlocal windows usually appear irregularly shaped. In our intu-\nitive experience, the distance information in the local space\nalways contains more near range spatial structure informa-\ntion than the direction information in WSI. So in this pa-\nper, we use the Manhattan distance to encode the relative\nposition information between different patches in each win-\ndow. The 2D spatial information between different patches\nis consequently reduced to 1D distance information. Simi-\nlar to the relative position encoding method used in Swin-\nTransformer (Liu et al. 2021), a learnable matrix ^B is used\nto learn the embedding of different distances, which is com-\nbined with the self-attention (SA). In the local window\nblock, the self-attention (Liu et al. 2021) corresponding to\neach head in computing similarity can be deﬁned as:\nSAlocal = softmax\n\u0012QKT + B\np\nd\n\u0013\n; (4)\nwhere Q 2Rw\u0002d, K 2Rw\u0002d, B 2Rw\u0002w is the relative\nposition bias, and values in B are taken from ^B, with w\nbeing the number of patch features in a window.\nA segmented Manhattan distance is used to make the\nmodel more sensitive to short rather than long distances. In-\nspired by the method in (Wu et al. 2021), the expression of\nthe piecewise function is deﬁned as follows:\ng(x) =\n(\n[jxj]; jxj\u0014 \u000b\nmin\n\u0010\n\u0015;\nh\n\u000b+ ln(jxj=\u000b)\nln(\r=\u000b) (\f\u00002\u000b)\ni\u0011\n; jxj>\u000b\n(5)\n2212\nwhere [\u0001] is a round operation, \u000b, \f, \u0015, \r are all hyper-\nparameters and we parameterize a learnable matrix ^B 2\nR(2\u0015+1)\u0002head for all heads.\nWSI-level interaction layer To encode WSI-level long-\ndistance contextual information, we design a WSI-level in-\nteraction layer. We adopt the spatial shufﬂe method so the\npatch features from different regions in a WSI-level bag\ncan be used for similarity computation in the same window.\nSpeciﬁcally, for each WSI-level bag after the local-level in-\nteraction layer, we spatially shufﬂe the feature sequence be-\nfore dividing the window and calculating the window atten-\ntion. It should be noted that in the self-attention calculation,\nwe do not add spatial information. For the spatial shufﬂe\nalgorithm, we use the shufﬂe method noted in (Huang et al.\n2021a). In shufﬂe window block, the self-attention (Vaswani\net al. 2017) corresponding to each head in computing simi-\nlarity can be deﬁned as:\nSAshufﬂe = softmax\n\u0012QKT\np\nd\n\u0013\n; (6)\nwhere Q 2Rw\u0002d, K 2Rw\u0002d, with wbeing the number of\npatch features in a window.\nPatient-level interaction layer To further explore the hi-\nerarchical information from the WSI to the patient level, we\ndesign a patient-level interaction layer focusing on global\ncontextual interaction across the entire patient-level bag.\nSpeciﬁcally, we ﬁrst concatenate all sub-WSI features cor-\nresponding to a patient, and then an attention pooling layer\n(AttnPool) is used to obtain patient-level representation\nhpatient to estimate the patient’s hazard risk h(k j Xi).\nSpeciﬁcally, AttnPool can be deﬁned as:\nag = exp fU(tanh (Vhg))g\nPG\nj=1 exp fU(tanh (Vhj))g\n;\nhpatient =\nGX\ng=1\naghg;\n(7)\nwhere U 2R1\u0002dh , V 2Rdh\u0002d, with dh being the dimen-\nsion of hidden layer, Gis the number of patches in a patient.\nTo optimize the model parameters, we adopt the log like-\nlihood function (Zadeh and Schmid 2020; Chen et al. 2022)\nas loss function. For an uncensored patient (Ci = 0) with\nfailure in interval [tk;tk+1), the likelihood can be calculated\nas the survival probability in[t0;tk) multiplied by the failure\nprobability in [tk;tk+1):\nluncensored = h(kjXi)S(k\u00001 jXi): (8)\nFor a censored patient (Ci = 1) with censored in interval\n[tk;tk+1), the likelihood can be calculated as the survival\nprobability in [t0;tk+1):\nlcensored = S(kjXi): (9)\nFinally, the loss function can be deﬁned as:\nL= \u0000Cilog S(kjXi)\n\u0000(1 \u0000Ci) logS(k\u00001 jXi)\n\u0000(1 \u0000Ci) logh(kjXi):\n(10)\nCancer Type Patient WSI Censored 1 Time (month)2\nBLCA 373 437 0.547 163.2\nBRCA 956 1,022 0.864 282.7\nCO&RE 339 344 0.764 147.9\nGB&LG 519 895 0.690 211.0\nLUAD 452 514 0.650 238.1\nUCEC 465 540 0.837 225.5\n1 The ratio of censored patients in the dataset.\n2 The longest survival time of patients in the dataset.\nTable 1: Datasets Summary\nExperimental Results\nDatasets\nWe closely follow the data settings of PatchGCN (Chen\net al. 2021a). Five public cancer types from TCGA are\nadopted: Bladder Urothelial Carcinoma (BLCA), Breast\nInvasive Carcinoma (BRCA), Glioblastoma&Lower Grade\nGlioma (GB&LG), Lung Adenocarcinoma (LUAD), Uter-\nine Corpus Endometrial Carcinoma (UCEC). We take gas-\ntrointestinal tract cancer type into our experiment for\na comprehensive comparison: Colon&Rectal Adenocarci-\nnoma (CO&RE). Six public cancer datasets include 3,104\npatients and 3,752 H&E diagnostic WSIs, whose speciﬁc in-\nformation is summarized in Table 1.\nEvaluation Metric and Implementation Details\nThis paper uses Concordance Index (C-Index) and Ka-\nplan–Meier (KM) estimator with a Log-rank test for evalua-\ntion metrics. For the dataset partition, we adopt 4-fold cross-\nvalidation. For WSIs and follow-up labels, we follow the\nPatchGCN processing step. For the parameters and training\nof HVTSurv, the window size is 49, the number of sub-WSIs\nis 2, and the survival loss function Eq.(10) is adopted with\nthe training batch size being 1. More details are in Appendix.\nResults and Discussion\nPerformance comparisons for all methods are summarized\nin Table 2 and Appendix Fig. 3, including C-Index scores,\nP-Values and KM analysis. We also compare computa-\ntional efﬁciency in Appendix Table 1. For the 4-fold cross-\nvalidation C-Index results in Table 2, we present it as\n“average C-Indexstandard deviation”. “TCGA-Mean” represents\nthe average C-Index scores on the 6 TCGA datasets. Be-\nsides, we bold the best and underline the second best\n.\nCompared with WSI-level MIL methods such as DSMIL\nand TransMIL, the results of patient-level MIL methods in-\ncluding PatchGCN and HVTSurv show that hierarchically\naggregating the patient-level features can make a better sur-\nvival prediction. Compared with random sampling methods\nsuch as DeepAttnMISL and SeTranSurv, HVTSurv adopts\nthe hierarchical processing framework that can handle more\npatch features. Moreover, local spatially correlated windows\nobtained by the feature rearrangement can help to achieve\nsigniﬁcantly better results. Compared with Transformer-\nbased methods such as TransMIL, SeTranSurv, and ESAT,\nconvolutional based and sin-cos based position encoding\n2213\nBLCA BRCA CO&RE GB&LG LUAD UCEC Mean\nAMIL[1] 0:499:015 0:571:037 0:543:038 0:756y\n:117 0:548:063 0:561:069 0:580\nDSMIL[2] 0:530:064 0:575y\n:048 0:571:085 0:734y\n:133 0:562y\n:048 0:612y\n:091\n0:597\nTransMIL[3] 0:572:021 0:548:067 0:588y\n:051 0:748y\n:117 0:519:057 0:616y\n:051\n0:599\nESAT[4] 0:562y\n:027 0:516:035 0:562:097 0:489:039 0:533:031 0:463:036\n0:521\nDeepAttnMISL[5] 0:491:040 0:571y\n:046 0:536:031 0:697y\n:157 0:561:045 0:576y\n:079\n0:572\nSeTranSurv[6] 0:549:019 0:547:051 0:536:038 0:682y\n:138 0:560y\n:049 0:601y\n:066\n0:579\nDeepGraphSurv[7] 0:535:047 0:570y\n:076 0:585y\n:056 0:737y\n:138 0:569\n:041 0:580y\n:073\n0:596\nPatchGCN[8] 0:544:019 0:568y\n:040 0:599y\n:068 0:743y\n:107 0:567y\n:081 0:632\ny\n:030\n0:609\nHVTSurv 0.579y\n:019 0.614y\n:037 0.606y\n:084 0.779y\n:019 0.584y\n:015 0.643y\n:058\n0.634\nTable 2: Comparison of C-Index performance in TCGA. (“y” denotes P-Value <0.05) [1] (Chen et al. 2022), [2] (Li, Li, and\nEliceiri 2021), [3] (Shao et al. 2021b), [4] (Shen et al. 2022), [5] (Yao et al. 2020), [6] (Huang et al. 2021b), [7] (Li et al. 2018),\n[8] (Chen et al. 2021a)\nBLCA BRCA CO&RE GB&LG LUAD UCEC Mean\nw/o position encoding 0:579:012 0:603y\n:046 0:565y\n:076 0:767y\n:030 0:552:034 0:610y\n:026\n0:613\nw/o spatial shufﬂe 0.582y\n:014 0:599y\n:061 0:599\ny\n:076 0:776y\n:030 0:564y\n:017 0:634y\n:047\n0:626\nw/o local-level interaction layer 0:579:011 0:575:070 0:592y\n:098 0:765y\n:019 0:579y\n:008 0:638y\n:040\n0:621\nw/o WSI-level interaction layer 0:536:079 0:599y\n:046 0:578:067 0.785y\n:033 0:582\ny\n:009 0:624y\n:026\n0:617\nw/o patient-level interaction layer 0:573y\n:024 0:593y\n:049 0:591y\n:097 0:779\ny\n:014 0:556y\n:014 0.643y\n:037\n0:622\nHVTSurv 0:579y\n:019 0.614y\n:037 0.606y\n:084 0:779\ny\n:019 0.584y\n:015 0.643y\n:058\n0.634\nTable 3: Effect of different modules and major components in HVTSurv. (“y” denotes P-Value<0.05)\nBLCA BRCA CO&RE GB&LG LUAD UCECMean\n25 0:585 0:578y 0:601y 0:769y 0.588y0:638y0:626\n36 0:576y0:595y 0:604y 0:760y 0:572y0:642y0:625\n64 0:581y0:603y 0:571 0: 777y 0:574y0:635y0:624\n81 0.588y0:598y 0:569y 0:774y 0:553y0:627y0:618\n491 0:579y0.614y 0.606y 0.779y 0:584y0.643y 0.634\n1 In our paper, we use a window size of 49.\nTable 4: Effect of different window size in HVTSurv. (“y”\ndenotes P-Value <0.05)\nschemes pay more attention to global spatial information,\nwhich inevitably loses local prognostic information. HVT-\nSurv creatively adopts the Manhattan distance to represent\nthe relative position in the local window, which can correctly\nand effectively encode local prognostic information. Com-\npared with GNN-based models such as DeepGraphSurv and\nPatchGCN, different from simply increasing model depth,\nHVTSurv adopts spatial shufﬂe for all the local windows,\nwhich can encode WSI-level interaction more efﬁciently.\nCompared with other methods in computational efﬁciency,\nHVTSurv beneﬁts from the window attention method and\nhas more efﬁcient GPU Memory Costs in patient-level MIL\ntask. Compared with other methods in Log-rank test, binary\nexperiments show that low and high-risk patients have a sta-\ntistically signiﬁcant difference (P-Value <0.05) over 6 can-\ncer types. In summary, the average C-Index is 2.50-11.30%\nhigher than all competitive models over 6 TCGA datasets.\nBLCA BRCA CO&RE GB&LG LUAD UCECMean\n1 0:584y0:607y 0:581 0: 778y 0:575y0:631y 0:626\n3 0.585y 0:601y 0:586 0: 773y 0.586y 0:638y 0:628\n4 0:582y 0.618y 0:590 0: 774y 0:567y0:633y 0:627\n5 0:580 0:613y 0:597 0:775y 0:580y0:639y 0:631\n21 0:579y0:614y 0.606y 0.779y 0:584y 0.643y 0.634\n1 In our paper, we sample 2 sub-WSIs for each WSI fea-\nture. The masking ratio for each WSI feature is 0.5.\nTable 5: Effect of different sub-WSI numbers for the random\nwindow masking strategy. (“y” denotes P-Value<0.05)\nAblation and Effectiveness Analysis\nWe further conduct a series of ablation studies to determine\nthe contribution of different modules and major components\nin HVTSurv and test the parameters used in this paper. We\nuse the average C-Index score to measure the performance.\nIn Table 3 and Appendix Table 2, we test the effect of\ndifferent modules, major components and different model\nstructures in HVTSurv. The results show that both posi-\ntion encoding and spatial shufﬂe play a signiﬁcant role in\nimproving the performance of the model. WSI is a high-\nresolution and irregularly shaped image after background\nremoval, it is hard to directly encode accurate contextual\ninteraction in the WSI-level bag. So we devise a two-step\napproach, i.e., local position encoding and WSI-level spa-\ntial shufﬂe. Local accurate spatial information is an essen-\ntial basis for global information encoding, we ﬁnd it has\n2214\nFigure 3: Visual analysis of attention in three interaction layers. The doctor-annotated cancer areas are shown in dark red.\nFigure 4: Tissue classiﬁcation results of highly concerned\npatches in CO&RE. For clarity, we show the number\nof patches for tumor-associated tissues including cancer-\nassociated stroma (STR) and colorectal adenocarcinoma\nepithelium (TUM). “Low/High-local/shufﬂe/Attn” repre-\nsents the number of patches for three interaction layers in\nlow/high-risk patients, respectively.\na more signiﬁcant effect on the performance improvement.\nBesides, we also perform ablation experiments on the three\nmajor components. In most cancers, combining the three in-\nteraction layers can better encode the spatial, contextual and\nhierarchical information in the patient-level bag. Due to the\ndifferent prognostic features of various cancers, the impor-\ntance of each interaction layer is slightly different. BLCA\nand UCEC are two similar cancer types whose prognosis\ndepend more on global-level features such as the depth of\ntumor invasion in the myometrium and bladder wall, so the\nWSI-level interaction layer plays a more critical role in these\ncancer types. In Table 4, we test the effect of different win-\ndow sizes in HVTSurv, we ﬁnd that a moderate window size\ncan help the model to learn the spatial interaction within\nthe window more accurately and efﬁciently. Moreover, a\nrelatively small window size can fully utilize the window\nmethod’s high computational efﬁciency. In Table 5, we test\nthe effect of different sub-WSI numbers for the random win-\ndow masking strategy, it can be found that the training strat-\negy of random window masking can help the model better\nadapt to the heterogeneity of cancer. Moreover, dividing a\nWSI into multiple sub-WSIs can further exploit the advan-\ntages of our hierarchical processing framework.\nInterpretability and Attention Visualization\nWe further explore the interpretability of our HVTSurv\nmodel in the slide level and patch level, whose results are\nshown in Fig. 3 and Fig. 4, respectively. The details are in\nthe Appendix. In Fig. 3, the attention to the three major com-\nponents is gradually extended from local low-level informa-\ntion to global high-level information, such as cancer-related\nregions. Beneﬁting from the hierarchical network, the recep-\ntive ﬁeld of the model can gradually become larger, then the\nhierarchical information in the patient-level bag can be fully\nexplored. Besides, in Fig. 4, we show the number of tumor-\nrelated patches in the high attention score area, which fur-\nther explains from patch level statistical result for the whole\nCO&RE dataset. HVTSurv adopts a hierarchically designed\nnetwork structure encoding interactions from local-level to\nWSI-level and further to patient-level, which can gradually\ndiscover the critical prognostic tissues like tumor-related tis-\nsues STR and TUM. We can also ﬁnd that for high-risk pa-\ntients, the number of tumor-related patches has signiﬁcantly\nincreased, which has been medically proven to be related to\nthe prognosis of colorectal cancer (Abbet et al. 2020).\nConclusion\nIn this work, we propose a hierarchical Vision Trans-\nformer named HVTSurv that progressively explores local-\nlevel spatial interaction, WSI-level contextual interaction\nand patient-level hierarchical interaction in the patient-level\nsurvival prediction. Hierarchical processing framework ef-\nfectively reduces the computational cost, which is suitable\nfor patient-level MIL tasks. Inspired by this, we propose the\nlocal and shufﬂe window block to progressively obtain the\nWSI-level representation and an attention pooling layer to\nget patient-level hazard risk. Besides, we design feature pre-\nprocessing strategies, including feature rearrangement and\nrandom window masking to explore spatial, contextual, hier-\narchical information better. Compared to SOTA methods, we\nachieve a better average C-Index over the 6 TCGA datasets,\nwith a performance gain of 2.50%. In KM analysis and Log-\nrank test, the low and high-risk patients have a statistically\nsigniﬁcant difference over 6 TCGA cancer types. In the abla-\ntion study, we prove that adopting Manhattan distance based\nposition encoding and spatial shufﬂe based long-range in-\nteraction can obtain better representation. Moreover, the ab-\nlation results of three interaction layers demonstrate the ef-\nfectiveness of our hierarchical processing framework. The\nvisualization of attention further conﬁrms our conclusions.\n2215\nAcknowledgements\nThis work was supported in part by the National Natu-\nral Science Foundation of China (61922048&62031023),\nin part by the Shenzhen Science and Technology Project\n(JCYJ20200109142808034), and in part by Guangdong\nSpecial Support (2019TX05X187).\nReferences\nAbbet, C.; Zlobec, I.; Bozorgtabar, B.; and Thiran, J.-P.\n2020. Divide-and-rule: self-supervised learning for survival\nanalysis in colorectal cancer. In International Conference\non Medical Image Computing and Computer-Assisted Inter-\nvention, 480–489. Springer.\nBian, H.; Shao, Z.; Chen, Y .; Wang, Y .; Wang, H.; Zhang,\nJ.; and Zhang, Y . 2022. Multiple Instance Learning with\nMixed Supervision in Gleason Grading. arXiv preprint\narXiv:2206.12798.\nCampanella, G.; Hanna, M. G.; Geneslaw, L.; Miraﬂor, A.;\nSilva, V . W. K.; Busam, K. J.; Brogi, E.; Reuter, V . E.; Klim-\nstra, D. S.; and Fuchs, T. J. 2019. Clinical-grade computa-\ntional pathology using weakly supervised deep learning on\nwhole slide images. Nature medicine, 1301–1309.\nCarmichael, I.; Song, A. H.; Chen, R. J.; Williamson,\nD. F.; Chen, T. Y .; and Mahmood, F. 2022. Incorpo-\nrating intratumoral heterogeneity into weakly-supervised\ndeep learning models via variance pooling. arXiv preprint\narXiv:2206.08885.\nChen, R. J.; Lu, M. Y .; Shaban, M.; Chen, C.; Chen, T. Y .;\nWilliamson, D. F.; and Mahmood, F. 2021a. Whole Slide\nImages are 2D Point Clouds: Context-Aware Survival Pre-\ndiction using Patch-based Graph Convolutional Networks.\nIn International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 339–349. Springer.\nChen, R. J.; Lu, M. Y .; Weng, W.-H.; Chen, T. Y .;\nWilliamson, D. F.; Manz, T.; Shady, M.; and Mahmood, F.\n2021b. Multimodal Co-Attention Transformer for Survival\nPrediction in Gigapixel Whole Slide Images. InProceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 4015–4025.\nChen, R. J.; Lu, M. Y .; Williamson, D. F.; Chen, T. Y .; Lip-\nkova, J.; Shaban, M.; Shady, M.; Williams, M.; Joo, B.;\nNoor, Z.; et al. 2022. Pan-cancer integrative histology-\ngenomic analysis via multimodal deep learning. Cancer\nCell.\nChikontwe, P.; Kim, M.; Nam, S.; Go, H.; and Park, S.\n2020. Multiple Instance Learning with Center Embeddings\nfor Histopathology Classiﬁcation. In International Confer-\nence on Medical Image Computing and Computer-Assisted\nIntervention, 519–528.\nDi, D.; Zhang, J.; Lei, F.; Tian, Q.; and Gao, Y . 2022. Big-\nHypergraph Factorization Neural Network for Survival Pre-\ndiction from Whole Slide Image. IEEE Transactions on Im-\nage Processing.\nFan, L.; Sowmya, A.; Meijering, E.; and Song, Y .\n2021. Learning Visual Features by Colorization for Slide-\nConsistent Survival Prediction from Whole Slide Images. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 592–601. Springer.\nHashimoto, N.; Fukushima, D.; Koga, R.; Takagi, Y .; Ko, K.;\nKohno, K.; Nakaguro, M.; Nakamura, S.; Hontani, H.; and\nTakeuchi, I. 2020. Multi-scale domain-adversarial multiple-\ninstance CNN for cancer subtype classiﬁcation with unanno-\ntated histopathological images. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n3852–3861.\nHou, W.; Yu, L.; Lin, C.; Huang, H.; Yu, R.; Qin, J.; and\nWang, L. 2022. Hˆ 2-MIL: Exploring Hierarchical Rep-\nresentation with Heterogeneous Multiple Instance Learning\nfor Whole Slide Image Analysis. InProceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 36, 933–941.\nHuang, Z.; Ben, Y .; Luo, G.; Cheng, P.; Yu, G.; and Fu, B.\n2021a. Shufﬂe Transformer: Rethinking Spatial Shufﬂe for\nVision Transformer. arXiv preprint arXiv:2106.03650.\nHuang, Z.; Chai, H.; Wang, R.; Wang, H.; Yang, Y .; and\nWu, H. 2021b. Integration of patch features through self-\nsupervised learning and transformer for survival analysis on\nwhole slide images. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention ,\n561–570. Springer.\nKanavati, F.; Toyokawa, G.; Momosaki, S.; Rambeau, M.;\nKozuma, Y .; Shoji, F.; Yamazaki, K.; Takeo, S.; Iizuka, O.;\nand Tsuneki, M. 2020. Weakly-supervised learning for lung\ncarcinoma classiﬁcation using deep learning. Scientiﬁc re-\nports, 1–11.\nKim, Y .-G.; Song, I. H.; Lee, H.; Kim, S.; Yang, D. H.;\nKim, N.; Shin, D.; Yoo, Y .; Lee, K.; Kim, D.; et al. 2020.\nChallenge for diagnostic assessment of deep learning algo-\nrithm for metastases classiﬁcation in sentinel lymph nodes\non frozen tissue section digital slides in women with breast\ncancer. Cancer Research and Treatment: Ofﬁcial Journal of\nKorean Cancer Association, 52(4): 1103–1111.\nLerousseau, M.; Vakalopoulou, M.; Classe, M.; Adam, J.;\nBattistella, E.; Carr ´e, A.; Estienne, T.; Henry, T.; Deutsch,\nE.; and Paragios, N. 2020. Weakly supervised multiple in-\nstance learning histopathological tumor segmentation. InIn-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 470–479.\nLi, B.; Li, Y .; and Eliceiri, K. W. 2021. Dual-stream Multiple\nInstance Learning Network for Whole Slide Image Classiﬁ-\ncation with Self-supervised Contrastive Learning. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition.\nLi, R.; Yao, J.; Zhu, X.; Li, Y .; and Huang, J. 2018.\nGraph CNN for survival analysis on whole slide patholog-\nical images. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 174–182.\nSpringer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\n2216\nLu, M. Y .; Williamson, D. F.; Chen, T. Y .; Chen, R. J.; Bar-\nbieri, M.; and Mahmood, F. 2021. Data-efﬁcient and weakly\nsupervised computational pathology on whole-slide images.\nNature Biomedical Engineering, 5(6): 555–570.\nMalkov, Y . A.; and Yashunin, D. A. 2018. Efﬁcient and ro-\nbust approximate nearest neighbor search using hierarchical\nnavigable small world graphs. IEEE transactions on pattern\nanalysis and machine intelligence, 42(4): 824–836.\nMuhammad, H.; Xie, C.; Sigel, C. S.; Doukas, M.; Alpert,\nL.; Simpson, A. L.; and Fuchs, T. J. 2021. EPIC-Survival:\nEnd-to-end Part Inferred Clustering for Survival Analysis,\nwith Prognostic Stratiﬁcation Boosting. In Medical Imaging\nwith Deep Learning, 520–531. PMLR.\nNaik, N.; Madani, A.; Esteva, A.; Keskar, N. S.; Press, M. F.;\nRuderman, D.; Agus, D. B.; and Socher, R. 2020. Deep\nlearning-enabled breast cancer hormonal receptor status de-\ntermination from base-level H&E stains. Nature communi-\ncations, 1–8.\nShaban, M.; Khurram, S. A.; Fraz, M. M.; Alsubaie, N.; Ma-\nsood, I.; Mushtaq, S.; Hassan, M.; Loya, A.; and Rajpoot,\nN. M. 2019. A novel digital score for abundance of tumour\ninﬁltrating lymphocytes predicts disease free survival in oral\nsquamous cell carcinoma. Scientiﬁc reports, 9(1): 1–13.\nShao, W.; Wang, T.; Huang, Z.; Han, Z.; Zhang, J.; and\nHuang, K. 2021a. Weakly supervised deep ordinal cox\nmodel for survival prediction from whole-slide pathological\nimages. IEEE Transactions on Medical Imaging, 40(12):\n3739–3747.\nShao, Z.; Bian, H.; Chen, Y .; Wang, Y .; Zhang, J.; Ji, X.;\net al. 2021b. Transmil: Transformer based correlated mul-\ntiple instance learning for whole slide image classiﬁcation.\nAdvances in Neural Information Processing Systems, 34.\nShen, Y .; Liu, L.; Tang, Z.; Chen, Z.; Ma, G.; Dong, J.;\nZhang, X.; Yang, L.; and Zheng, Q. 2022. Explainable\nSurvival Analysis with Convolution-Involved Vision Trans-\nformer. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 36, 2207–2215.\nSrinidhi, C. L.; Ciga, O.; and Martel, A. L. 2021. Deep neu-\nral network models for computational histopathology: A sur-\nvey. Medical Image Analysis, 67: 101813.\nTomita, N.; Abdollahi, B.; Wei, J.; Ren, B.; Suriawinata,\nA.; and Hassanpour, S. 2019. Attention-Based Deep Neu-\nral Networks for Detection of Cancerous and Precancerous\nEsophagus Tissue on Histopathological Slides. JAMA Net-\nwork Open.\nVale-Silva, L. A.; and Rohr, K. 2021. Long-term cancer sur-\nvival prediction using multimodal deep learning. Scientiﬁc\nReports, 11(1): 1–12.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nVitale, I.; Shema, E.; Loi, S.; and Galluzzi, L. 2021. Intra-\ntumoral heterogeneity in cancer progression and response to\nimmunotherapy. Nature medicine, 27(2): 212–224.\nWang, Z.; Li, J.; Pan, Z.; Li, W.; Sisk, A.; Ye, H.; Speier,\nW.; and Arnold, C. W. 2021. Hierarchical Graph Pathomic\nNetwork for Progression Free Survival Prediction. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 227–237. Springer.\nWu, K.; Peng, H.; Chen, M.; Fu, J.; and Chao, H. 2021. Re-\nthinking and improving relative position encoding for vision\ntransformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 10033–10041.\nXu, G.; Song, Z.; Sun, Z.; Ku, C.; Yang, Z.; Liu, C.; Wang,\nS.; Ma, J.; and Xu, W. 2019. CAMEL: A Weakly Supervised\nLearning Framework for Histopathology Image Segmenta-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 10681–10690.\nYao, J.; Zhu, X.; Jonnagaddala, J.; Hawkins, N.; and Huang,\nJ. 2020. Whole slide images based cancer survival predic-\ntion using attention guided deep multiple instance learning\nnetworks. Medical Image Analysis, 65: 101789.\nZadeh, S. G.; and Schmid, M. 2020. Bias in cross-entropy-\nbased training of deep survival networks.IEEE Transactions\non Pattern Analysis and Machine Intelligence, 43(9): 3126–\n3137.\nZhu, X.; Yao, J.; Zhu, F.; and Huang, J. 2017. Wsisa: Mak-\ning survival prediction from whole slide histopathological\nimages. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 7234–7242.\n2217",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7572838068008423
    },
    {
      "name": "Pooling",
      "score": 0.6697440147399902
    },
    {
      "name": "Conditional random field",
      "score": 0.6278981566429138
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48015421628952026
    },
    {
      "name": "Hierarchical database model",
      "score": 0.4472801685333252
    },
    {
      "name": "Visualization",
      "score": 0.43725109100341797
    },
    {
      "name": "Machine learning",
      "score": 0.4283590316772461
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4190170466899872
    },
    {
      "name": "CRFS",
      "score": 0.4122156500816345
    },
    {
      "name": "ENCODE",
      "score": 0.4100019335746765
    },
    {
      "name": "Data mining",
      "score": 0.4063866138458252
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38977721333503723
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128628",
      "name": "Peking University Shenzhen Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}