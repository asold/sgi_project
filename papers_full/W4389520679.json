{
  "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
  "url": "https://openalex.org/W4389520679",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2107242005",
      "name": "Weishi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2002692889",
      "name": "Yue Wang",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3095952219",
      "name": "Steven Hoi",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1208744602",
      "name": "Shafiq Joty",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2145373440",
    "https://openalex.org/W2867448323",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3043761819",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3161027892",
    "https://openalex.org/W3100698844",
    "https://openalex.org/W4226174273",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W4226138290",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4315588908",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4307205456",
    "https://openalex.org/W4386737879",
    "https://openalex.org/W2964588180",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W4244452926",
    "https://openalex.org/W4293138061",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W2145680191",
    "https://openalex.org/W4385573988",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2978409868",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3166979522",
    "https://openalex.org/W4376652621",
    "https://openalex.org/W2924629359",
    "https://openalex.org/W4226305955",
    "https://openalex.org/W3208407575",
    "https://openalex.org/W4221166942",
    "https://openalex.org/W2063387237",
    "https://openalex.org/W4285189120",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W4294646197",
    "https://openalex.org/W2888541716",
    "https://openalex.org/W3177003157",
    "https://openalex.org/W2850616187",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3143822685",
    "https://openalex.org/W3193682477",
    "https://openalex.org/W4226474457",
    "https://openalex.org/W2889849786",
    "https://openalex.org/W4288080276",
    "https://openalex.org/W4312805641",
    "https://openalex.org/W1598377843",
    "https://openalex.org/W4225592887",
    "https://openalex.org/W4213019371",
    "https://openalex.org/W2139410856",
    "https://openalex.org/W3114290355",
    "https://openalex.org/W2344973853"
  ],
  "abstract": "Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers’ productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types. To address this limitation, we investigate a novel task of low-resource APR, and propose Meta-APR, a new meta-learning framework integrated with code pretrained language models to generate fixes for low-resource bugs with limited training samples. Our Meta-APR learns better error-specific knowledge from high-resource bugs through efficient first-order meta-learning optimization, which allows for a faster adaptation to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic framework that can be integrated with any neural models. Extensive experimental results on three benchmarks in various programming languages verify the superiority of our method over existing DL-based APR approaches.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6954–6968\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards Low-Resource Automatic Program Repair with Meta-Learning\nand Pretrained Language Models\nWeishi Wang12, Yue Wang1∗, Steven C.H. Hoi1, Shafiq Joty12\n1Salesforce AI Research\n2Nanyang Technological University, Singapore\n{weishi.wang,wang.y,sjoty,shoi}@salesforce.com\nAbstract\nAutomatic program repair (APR) has gained\nincreasing attention as an essential technique in\nsoftware development to reduce manual debug-\nging efforts and boost developers’ productivity.\nRecent advances in deep learning (DL) based\nmodels have demonstrated promising results\nby learning from large-scale bug-fix examples\nin a data-driven manner. However, in practical\nscenarios, software bugs have an imbalanced\ndistribution, and the fixing knowledge learned\nby APR models often only capture the patterns\nof frequent error types, making it inapplica-\nble to handle the rare error types. To address\nthis limitation, we investigate a novel task of\nlow-resource APR, and propose Meta-APR, a\nnew meta-learning framework integrated with\ncode pretrained language models to generate\nfixes for low-resource bugs with limited train-\ning samples. Our Meta-APR learns better error-\nspecific knowledge from high-resource bugs\nthrough efficient first-order meta-learning opti-\nmization, which allows for a faster adaptation\nto the target low-resource bugs. Besides, while\nwe adopt CodeT5, a pretrained code-aware\nencoder-decoder Transformer, as the backbone\nmodel for Meta-APR, it is a model-agnostic\nframework that can be integrated with any neu-\nral models. Extensive experimental results on\nthree benchmarks in various programming lan-\nguages verify the superiority of our method\nover existing DL-based APR approaches.\n1 Introduction\nProgram repair is critical to improving the pro-\nductivity and stability of software development.\nHowever, it is resource-consuming and cost-\nprohibitive (Weiß et al., 2007; Planning, 2002; Jør-\ngensen and Shepperd, 2007). A reliable automatic\nprogram repair (APR) system is thus crucial to re-\nduce manual debugging efforts and development\ntime (Gazzola et al., 2019; Winter et al., 2023).\n*Corresponding author: wang.y@salesforce.com.\nFirst-order Optimization\nθ′ θ\nθ = θ + β(θ′ - θ )\nSupport Set\n  \nEncoder Decoder\nθ\nHigh-Resource Meta-Training\nInner Loop\nOuter Loop\nLow-Resource Adaptation\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  θ\nθ\nθ\nLow-Resource APR\nchanges = payload.get(\"changes\", {})\nfor change in changes:\n    commits=target.get(\"commits\",[])\nBuggy code commits=change.get(\"commits\",[])\nFigure 1: Illustration of our Meta-APR framework with\nCodeT5 for low-resource error-specific automatic pro-\ngram repair. We first meta-train the CodeT5 with high-\nresource bugs ( ), where the backbone model is updated\nvia gradient descent with respect to θ. After that, Meta-\nAPR is finetuned on the target low-resource bugs (▲,▲)\nusing few-shot examples (10, 50, 100).\nWith the advances in deep learning (DL) mod-\nels (Vaswani et al., 2017) and accessibility to\nlarge code corpora (Tufano et al., 2019; Lu et al.,\n2021), neural approaches to APR have achieved\nremarkable performance via exploiting existing\ncode patches (Chen et al., 2021b; Zhu et al., 2021).\nThese models are typically trained and evaluated\non datasets that comprise a mix of various error\ntypes, which are diverse in nature: they vary in\nterms of the number of bug-fix pairs per error type,\nand are typically imbalanced. Moreover, the perfor-\nmance gaps across different error types are tremen-\ndous (Berabi et al., 2021), which can significantly\nimpair the APR models’ performance.\nThese observations motivate us to consider the\nidea: rather than training the model jointly on all\nerror types, could we train a model that is quickly\nadaptable to any low-resource error-specific APR\ntask? Inspired by the success of meta-learning\non low-resource NLP tasks like machine transla-\n6954\ntion (Gu et al., 2018; Park et al., 2021) and dialogue\ngeneration (Mi et al., 2019; Lin et al., 2021), in this\nwork, we drive pioneering efforts in formalizing\nlow-resource APR, and propose an effective meta-\nlearning framework that utilizes a code pretrained\nmodel to enhance APR performance.\nLow-resource APR formulation: Unlike tradi-\ntional APR approaches that jointly learn a model\nfrom a mix of error types, our formulation consid-\ners each rare error type as a low-resource target\ntask. Accordingly, we create datasets specifically\nto support the evaluation of low-resource error-\nspecific APR, based on three practical APR bench-\nmarks in various programming languages: TFix in\nJavaScript (Berabi et al., 2021), ManySStuBs4J in\nJava (Karampatsis and Sutton, 2020), and TSSB-\n3M in Python (Richter and Wehrheim, 2022).\nWe observe diverse and imbalanced error type\ndistributions in these benchmarks, e.g., TFix,\nManySStuBs4J, and TSSB-3M respectively have\n31, 8, and 9 error types that are of low-resource 1\nalong with 21, 6, and 14 high-resource error types.\nMeta learning for low-resource APR: To bet-\nter address the task distribution issue while adapt-\ning the model to low-resource (synonymously, few-\nshot) tasks, we propose a novel meta-learning ap-\nproach integrated with pretrained code language\nmodels. To the best of our knowledge, this is\nthe first work to study the low-resource error-\nspecific APR. We build Meta-APR with a code-\naware pretrained encoder-decoder Transformer\nmodel CodeT5 (Wang et al., 2021) and an efficient\nfirst-order meta-learning algorithm Reptile (Nichol\net al., 2018) for the challenging low-resource APR\ntasks. Fig. 1 illustrates the overview of our Meta-\nAPR approach. Specifically, we first meta-train a\nCodeT5-based APR model on high-resource bug-\nfix pairs to learn a better model initialization that\ncaptures error-specific knowledge, which enables\nfaster adaptation to the target low-resource bugs\nvia finetuning on few-shot examples. In our experi-\nments, we show that Meta-APR effectively aligns\nthe representations between high-resource and low-\nresource bugs so that they have a closer distance in\nthe representation vector space.\nWe extensively evaluate Meta-APR on three cu-\nrated low-resource multilingual APR benchmarks\nwith different degrees of low-resource settings, i.e.\ndifferent numbers of training samples (10, 50, 100).\n1We select low-resource scenarios based on the number of\nexamples per error type for each dataset.\nWe show that Meta-APR significantly outperforms\nthe standard transfer-learning method in all settings.\nAs our Meta-APR is a model-agnostic framework\nthat can be integrated with any other DL models,\nwe compare its performance when integrated with\nother pretrained models like UniXcoder (Guo et al.,\n2022). Our results demonstrate that Meta-APR con-\nsistently enhances performance. Further analysis\nconfirms that Meta-APR is a more robust and ef-\nfective approach in fixing bugs with various buggy\npatch lengths and error types.\nWe further compare with closed-sourced lan-\nguage models such as ChatGPT (OpenAI) in fix-\ning these low-resource bugs. We find Meta-APR\nachieves much better performance than ChatGPT\nunder zero-shot/few-shot settings. Besides, we ob-\nserve that ChatGPT often predicts “no bugs”, which\nis probably because it does not well capture the fix-\ning patterns of these low-resource bugs due to their\ndata scarcity issue.\n2 Related Work\nAutomatic Program Repair (APR) Recently,\nthere is a growing body of APR research that aims\nto automate the rectification of software defects\nwith less human intervention. In general, conven-\ntional APR approaches can be divided into three\ncategories (Zhang et al., 2023, 2022), which are 1)\nheuristic-based (Goues et al., 2012; Qi et al., 2014;\nJiang et al., 2018), 2) constraint-based (Martinez\nand Monperrus, 2018; Nguyen et al., 2013; Xuan\net al., 2017), 3) template-based approaches (Liu\net al., 2019; Koyuncu et al., 2020).\nBesides, learning-based approaches (Chen et al.,\n2021b; Lutellier et al., 2020; Jiang et al., 2021)\nhave shown to achieve promising results by learn-\ning the fix patterns from previous bug-fix pairs in\nan end-to-end manner. Motivated by the success\nof Neural Machine Translation (NMT) in the NLP\ndomain, one notable learning-based APR method\nis formulated as a sequence-to-sequence genera-\ntion problem (Tufano et al., 2019), which aims to\ntranslate a buggy code into its correct version. This\ntechnique is further enhanced by using pretrained\nmodels such as T5 (Raffel et al., 2020) in Berabi\net al. (2021). In this work, we propose to exploit a\npretrained code-aware CodeT5 (Wang et al., 2021)\nfollowing Bui et al. (2022); Wang et al. (2023a).\nMeta-Learning for Low-Resource Tasks Meta-\nlearning has been well studied for few-shot learning\nas a learning-to-learn approach, which attempts to\n6955\nlearn new concepts based on past experiences (Ben-\ngio et al., 2013; Vilalta and Drissi, 2002). Recently,\noptimization-based techniques yield substantial im-\nprovement in many low-resource NLP tasks (Zhao\net al., 2022). Among them, Model-Agnostic Meta-\nLearning (MAML) (Finn et al., 2017) has been\nwidely used to tackle low-resource NLP tasks such\nas machine translation (Gu et al., 2018; Park et al.,\n2021), dialogue generation (Mi et al., 2019; Lin\net al., 2021), and text-to-speech alignment (Lux\nand Vu, 2022). MAML has shown exceptional effi-\ncacy in learning a good parameter initialization for\na fast adaption with limited resources.\nRecently, meta-learning approaches have been\nadapted to solve low-resource software intelligence\ntasks such as code summarization (Rauf et al.,\n2022; Xie et al., 2022), and code search (Chai et al.,\n2022). To the best of our knowledge, we are the\nfirst to formulate the low-resource error-specific\nAPR task based on the error type distributions\nand investigate the effectiveness of meta-learning\nmethods. In addition, unlike prior approaches that\nmostly use the second-order meta-learning algo-\nrithm MAML, we exploit a more efficient first-\norder meta-learning method Reptile (Nichol et al.,\n2018). In the ablation studies, we show that it out-\nperforms MAML in various low-resource settings.\nProgramming Language Models Inspired by\nthe success of pretrained language models (LMs)\nsuch as BERT (Devlin et al., 2019), GPT (Rad-\nford et al., 2018), and T5 (Raffel et al., 2020)\nin NLP tasks, there are many recent attempts for\ncode pretrained models that can be classified as\nthree categories: 1) Encoder-only approaches like\nCodeBERT (Feng et al., 2020) and GraphCode-\nBERT (Guo et al., 2021); 2) Decoder-only methods\nsuch as CodeGPT (Lu et al., 2021); and 3) Encoder-\ndecoder models like CodeT5 (Wang et al., 2021;\nLe et al., 2022; Wang et al., 2023b). Besides, UniX-\ncoder (Guo et al., 2022) adopts a UniLM-like archi-\ntecture (Dong et al., 2019) with various attention\nmasks. Recent studies further explore the use of\nvery large LMs such as Codex (Chen et al., 2021a)\nfor APR tasks (Prenner and Robbes, 2021; Joshi\net al., 2022) in a zero-shot/few-shot setting, where\nthere is still a clear gap between Codex and the\ndomain-specific finetuning methods.\n3 Approach\nFig. 1 illustrates the overview of our proposed\nMeta-APR, a meta-learning framework that lever-\nAlgorithm 1: Meta-Training for APR\nRequire: A set of high-resource error types\nTh = {T1,T2,..., Tn}, ∀Ti ∈Th it pairs\nwith associated bug-fix pairs that\nDi = {(Bj,Fj)}|Di|\nj=1 , a APR model fθ,\ninner loop learning rate α, outer loop\nlearning rate β, meta update step size M\nInitialize: Initialize θfrom the APR model fθ\nOutput: Optimal meta-trained APR model fθ\nwhile not done do\nDh = ∅.\nforall T ∈Th do\nAppend the training dataset of Tinto Dh\nend\nRandomly divide the merged training dataset Dh\ninto batches Bs\nforall Bs do\nObtain Bsupport\ns and Bquerry\ns as in §3.1\nEvaluate the inner loop cross entropy loss\nLinner(fθ,Bsupport\ns )\nUpdate error-specific model parameters with\ngradient descent:\nθ′ = θ−α∇θLinner(fθ,Bsupport\ns )\nif current step i mod M= 0then\nUpdate global model parameters with\nestimation: θ←θ+ β(θ′ −θ)\nend\nEvaluate the inner loop cross entropy loss\nLinner(fθ,Bquerry\ns )\nend\nend\nages a code pretrained model for low-resource\nerror-specific APR. We first formulate the task of\nlow-resource error-specific APR in §3.1. Then,\nwe describe our error-specific meta-APR dataset\ncreation in §3.2 and our Meta-APR method in §3.3.\n3.1 Task Formulation\nAssume that we have a set of error types T =\n{T1,T2,..., Tn}. For each error type Ti, it as-\nsociates with a collection of bug-fix pairs Di =\n{(Bj,Fj)}|Di|\nj=1, where (Bj,Fj) denotes the j-th\nbug-fix pair. For the error types in T, we define\ntheir resourceness based on the total number of\nbug-fix pairs |Di|. Considering the actual data dis-\ntribution across three benchmarks, we select an em-\npirical cutoff value of 1000 instances. This thresh-\nold value is established to identify an error type\nas low-resource if it has less than 1000 samples.\nOtherwise, we treat it as high-resource.\nFormally, our proposed framework com-\nprises a neural sequence-to-sequence (seq2seq)\nmodel (Sutskever et al., 2014) fθ as a base APR\nmodel. Given a set of error-specific bug-fix pairs\nDi = {(Bj,Fj)}|Di|\nj=1, fθ generates Fj based on Bj\n6956\nreturn mapCb(err, memo);\n         -}.bind(this)\n      -}.bind(this), waterCb);\n +});\n  +}, waterCb);\n   }.bind(this),\nreturn mapCb(err, memo);\n         }.bind(this)\n      }.bind(this), waterCb);\n   }.bind(this),\nreturn mapCb(err, memo);\n });\n  }, waterCb);\n    }.bind(this),\nPatch differenceTFMeta-APR ET no-extra-bind \n(a) TFix (JavaScript)\nSWAP_BOOLEAN_LITERAL\ntry (LockedInodePath inodePath = \nmInodeTree.lockFullInodePath\n        (entry.getId(), \nInodeTree.LockMode.WRITE))\n{ \n- setAttributeInternal(inodePath,\nfalse, entry.getOpTimeMs(), options);\n+ setAttributeInternal(inodePath,\ntrue, entry.getOpTimeMs(), options);\n }}\nsetAttributeInternal(inodePath, \nfalse, entry.getOpTimeMs(), options);\nsetAttributeInternal(inodePath, true, \nentry.getOpTimeMs(), options); (b) ManySStuBs4J (Java)\nSAME_FUNCTION_WRONG_CALLER\nclass PushEventHook(BaseEventHook):\nchanges = \nself.payload.get(\"push\", \n{}).get('changes', [])\nfor change in filter(None, \nchanges):\n- commits =\ntarget.get(\"commits\", [])\n+ commits =\nchange.get(\"commits\", [])\nif not commits:\ncontinue\ncommits = target.get(\"commits\", [])\ncommits = change.get(\"commits\", []) (c) TSSB-3M (Python)\nFigure 2: Bug fix examples on three low-resource error-specific APR tasks from one particular error type (ET),\nwhere our Meta-APR successfully fixes bugs while the transfer-learning (TF) approach fails to do so.\nin an autoregressive manner. Formally,\nP(Fj|Bj,fθ) =\nN∏\nk=1\nP(Fj,k|Bj,Fj,1 : Fj,k−1,fθ)\nwhere Fj,1 : Fj,k−1 is the previous sequence atk-th\ntoken with N denoting the total number of tokens\nin the target sequence Fj.\nDuring the meta-training stage, we ran-\ndomly sample a batch of bug-fix pairs Bs =\n{(Bs,Fs)}|Bs|\ns=1 from high-resource error types.\nEach batch Bs is further divided into Bsupport\ns and\nBquery\ns equally. Then, we apply the first-order meta-\nlearning algorithm Reptile (Nichol et al., 2018)\nto update fθ via gradient descent. After that, the\nmodel fθ is finetuned on a low-resource error type\nwith few-shot examples. The underlying idea\nof Meta-APR is to meta-train a model on high-\nresource error types such that it is quickly adapt-\nable to low-resource types with few-shot examples.\n3.2 Low-resource APR Dataset Construction\nAs there are no available low-resource APR bench-\nmarks for evaluation, we curate three low-resource\nAPR datasets in various low-resource settings from\nthree existing APR benchmarks with error type an-\nnotations, which are TFix in JavaScript (Berabi\net al., 2021), ManySStuBs4J in Java (Karampat-\nsis and Sutton, 2020), and TSSB-3M in Python\n(Richter and Wehrheim, 2022). As mentioned\nin §3.1, we define the low-resource error types\nbased on the actual counts of its associated bug-\nfix pairs (<1000). To construct more challenging\nlow-resource scenarios, we randomly select 10, 50,\nand 100 samples from each low-resource error type.\nFollowing the common practice (Gao et al., 2021),\nwe repeat this few-shot sampling process with five\ndifferent random seeds (13, 21, 42, 87, and 100).\nFor evaluation, we report the averaged results over\nthe five seeds to rule out the random noises.\n3.3 Model-Agnostic Meta-APR Framework\nBase APR Model CodeT5 (Wang et al., 2021)\nis a unified code-aware encoder-decoder Trans-\nformer model pretrained from large-scale source\ncode corpus in eight different programming lan-\nguages. CodeT5 has been shown to achieve SoTA\nperformance in many code understanding and gen-\neration tasks such as defect detection and program\nrefinement. In this work, we propose to adapt\nCodeT5 as the base model of our Meta-APR to\nleverage its better code understanding capability.\nHigh-Resource APR Meta-Training During the\nmeta-training phase, each mini-batch of data simu-\nlates the low-resource scenarios. In our Meta-APR\napproach, we iterate through a set of high-resource\nerror types as a private training task to update\nfθ. We first merge all high-resource error-specific\ntraining dataset as Dtrain\nh , and randomly segment\nDtrain\nh into N batches {B1,B2,..., BN}equally.\nThen, each Bs is further split into Bsupport\ns and\nBquery\ns to form a local error-specific meta-learning\n6957\ntask to update the global APR model fθ using gra-\ndient descent:\nθ′= θ−α∇θL(fθ,Bsupport\ns )\nθ←θ+ β(θ′−θ)\nwhere θis the global model parameters, and θ′is\nthe local error-specific model parameters, α and\nβ denote the learning rate of the inner loop and\nouter loop respectively, Ldenotes the cross en-\ntropy loss function. The error-specific local gradi-\nents are grouped by every Msteps to update the\nglobal APR model parametersθ. The meta-training\nprocedure of our Meta-APR is summarized in Al-\ngorithm 1. In the low-resource setting, we set the\nsize of support and query sets to 10 and we lever-\nage support sets for the inner loop update. The\nquery sets are used to track the meta-loss and not\ninvolved in parameter updating.\nLow-Resource APR Adaptation After the meta-\ntraining, we adapt Meta-APR to the target low-\nresource APR tasks via directly finetuning the\nmeta-learned global APR model on few-shot train-\ning samples. Such meta-learned APR model is\nexpected to capture error-specific knowledge by\nproviding a better model initialization, which en-\nables faster adaptation to fix low-resource bugs.\nIn finetuning, the objective is to minimize the\ncross-entropy loss between model predictions and\nground-truth fixes.\n4 Experimental Setup\n4.1 Error-Specific APR Dataset\nManySStuBs4J (Karampatsis and Sutton, 2020)\nhas small and large versions comprising 10,231 and\n63,923 bug-fix pairs respectively in Java. It is orga-\nnized at the level of the single statement changes\nfor each bug-fix pair. We consider ManySStuBs4J\nlarge with 14 error types in this work.\nTFix (Berabi et al., 2021) is a large-scale pro-\ngram repair dataset that consists of a ground\ntruth repair code patch for each buggy patch in\nJavaScript. It focuses on syntax and stylistic errors\nfrom open-source GitHub commits, which com-\nprise 104,804 bug-fix pairs. Among them, 52 error\ntypes are detected by a static analyzer ESLint2 (Tó-\nmasdóttir et al., 2020).\n2https://eslint.org/\nBenchmark High-resource Low-resource\n#Error #Train #Error Few-shots #Test\nManySStuBs4J 6 20,225 8 (0,10,50,100) 569\nTFix 21 75,998 31 (0,10,50,100) 1,087\nTSSB-3M 14 66,384 9 (0,10,50,100) 538\nTable 1: Data statistics of 3 error-specific low-resource\nAPR benchmarks. During low-resource finetuning, we\nrandomly sample (10,50,100) shots for each error type\nto construct various low-resource settings.\nTSSB-3M (Richter and Wehrheim, 2022) is a\ndataset of over 3 million isolated single statement\nbug fixes across 23 error types. Each bug fix is as-\nsociated with a commit in an open-sourced Python\nproject that does not modify source code in other\nfiles or statements. We randomly down-sample by\n10% for each error type.\nTo facilitate future research in this new\nfield, we release our curated error-specific low-\nresource APR datasets at https://github.com/\nwang-weishi/Meta-APR. See Appendix A.1 for\nmore detailed statistics.\nData Preprocessing As discussed in §3.2, we\nprocess all three benchmarks to create high-\nresource and low-resource APR tasks based on the\nnumber of bug-fix in each error type. The data\nstatistics are reported in Table 1. We further pro-\nvide bug-fix examples for each benchmark in Fig. 2.\nTo prepare the source input to Meta-APR, we fol-\nlow Berabi et al. (2021) to combine error type, error\nmessage, and error context into a single piece of\ntext in the following format:\nfix {error type} {error message} {error context}\nwhere error context consists of the given localized\nerror line and its two neighboring code lines to\nform a buggy code patch. The corresponding fixed\nline is used as the target sequence.\n4.2 Metrics and Baselines\nMetrics Following the common practice (Berabi\net al., 2021), we use the Exact Match (EM) accu-\nracy to measure the APR performance. Specifically,\nEM requires the prediction to be identical to the\nground-truth fix, which can reflect how well model\npredictions are aligned with historic correct fixes\nfrom human developers. EM is commonly utilized\nto uphold correctness standards, especially in cases\nwhere static analyzers or unit tests are not available.\n6958\nBaselines We compare Meta-APR with three\nlearning settings: 1) only finetuning on low-\nresource bugs; 2) transfer-learning from high-\nresource to low-resource bugs; and 3) multi-task\nlearning on both high-resource and low-resource\nbugs with or without upsampling strategies. Specif-\nically, for the transfer-learning baseline, we first\nfinetune the model on the high-resource training\ndata and then have another stage of finetuning on\nthe low-resource training data. Under the multi-\ntask learning setting, we jointly finetune our models\non a mix of both high-resource and low-resource\ntraining data.\nBesides, we compare with other code pre-\ntrained models as the backbone model, which in-\nclude encoder-only CodeBERT (Feng et al., 2020),\ndecoder-only UniXcoder (Guo et al., 2022), and\nencoder-decoder CodeT5 (Wang et al., 2021). For\nour Meta-APR method, we perform two abla-\ntion studies by replacing either the Reptile meta-\nlearning approach to MAML or replacing the back-\nbone model CodeT5 into UniXcoder to verify the\neffectiveness of our design choices.\n4.3 Implementation Details\nWe implement Meta-APR based on the deep learn-\ning framework PyTorch 3. We employ CodeT5-\nbase4 with 220M parameters as our backbone\nmodel. All of our experiments are conducted on\na single NVIDIA A100-40GB GPU. We use the\nlibrary Higher (Grefenstette et al., 2019) to meta-\ntrain the model on high-resource error types for 50\nepochs with a batch size of 10, where the first 5\ninstances work as the support set and the remaining\n5 instances are query set. For inner loop gradient\nupdates, we use the SGD optimizer with an inner\nloop learning rate αof 1e-4. For the global gradi-\nent updates, we use the AdamW (Loshchilov and\nHutter, 2019) optimizer and set the outer loop learn-\ning rate βto 5e-5. Moreover, in the meta-training\nstage, we warm up the first 1000 steps with a linear\ndecay. The meta update step size Mis set to 150,\n20, 150 for TFix, ManySStuBs4J, and TSSB-3M\nrespectively. For low-resource APR adaptation, we\nfinetune the meta-trained model for 50 epochs on\nlow-resource error types with a batch size of 25 and\na learning rate of 5e-5. For testing, we select the\ncheckpoint which has the best EM on a held-out\nvalidation set.\n3https://pytorch.org/\n4https://github.com/salesforce/CodeT5/\nMethod Shot = 100 Shot = 50 Shot = 10 Shot = 0\nLow-resource finetuning\nCodeBERT 6.43 3.64 0.14 0.00\nUniXcoder 42.28 33.25 18.24 0.00\nCodeT5 42.74 36.10 9.24 0.00\nTransfer-learning\nCodeBERT 43.34 37.08 34.02 15.82\nUniXcoder 53.43 47.66 34.87 18.10\nCodeT5 55.22 49.91 38.49 18.28\nMulti-task learning\nCodeT5 53.78 46.75 35.85 -\nCodeT5 + upsampling 58.98 52.73 41.09 -\nMeta-learning\nMeta-APR 59.44 54.34 42.04 22.50\n→MAML 58.10 53.00 41.69 23.02\n→UniXcoder 54.48 48.22 36.77 19.68\nTable 2: Results on low-resource ManySStuBs4J.\nMethod Shot = 100 Shot = 50 Shot = 10 Shot = 0\nLow-resource finetuning\nCodeBERT 4.91 3.05 0.00 0.00\nUniXcoder 33.79 28.14 16.39 0.00\nCodeT5 35.61 29.07 13.05 0.00\nTransfer-learning\nCodeBERT 26.06 21.89 10.89 3.35\nUniXcoder 40.15 35.28 24.98 15.80\nCodeT5 46.91 43.05 31.23 13.57\nMulti-task learning\nCodeT5 46.47 41.26 30.30 -\nCodeT5 + upsampling 46.84 41.38 29.41 -\nMeta-learning\nMeta-APR 47.77 44.83 35.28 24.72\n→MAML 47.03 44.09 34.95 20.82\n→UniXcoder 40.85 35.58 25.58 15.61\nTable 3: Results on low-resource TSSB-3M.\n5 Experimental Results and Analysis\nIn this section, we compare Meta-APR with other\ncode pretrained models in different training settings\non a set of our curated low-resource error-specific\nAPR tasks from three benchmarks (§5.1), followed\nby a detailed analysis on the effects of different er-\nror types and token length (§5.2), and a pilot study\nto compare with a closed-sourced large language\nmodel such as ChatGPT in fixing these challenging\nlow-resource bugs (§5.3).\n5.1 Low-Resource APR Performance\nTables 2 to 4 present the results of exact match\n(EM) accuracies on ManySStuBs4J, TSSB-3M,\nand TFix benchmarks respectively at different low-\nresource settings. We can observe that Meta-APR\nconsistently outperforms other baselines in various\nfew-shot settings across 3 benchmarks in different\nprogramming languages. Among different mod-\nels, we find that CodeT5 achieves consistent per-\nformance gains over CodeBERT and UniXcoder\nin most cases, demonstrating that it can serve as\n6959\nMethod Shot = 100 Shot = 50 Shot = 10 Shot = 0\nLow-resource finetuning\nCodeBERT 13.15 1.75 0.13 0.00\nUniXcoder 45.11 41.53 27.51 0.00\nCodeT5 45.85 40.18 24.58 0.09\nTransfer-learning\nCodeBERT 42.80 38.86 26.66 16.38\nUniXcoder 46.64 44.89 32.75 18.31\nCodeT5 51.02 46.46 34.26 21.44\nMulti-task learning\nCodeT5 50.60 47.29 34.41 -\nCodeT5 + upsampling 51.39 47.58 36.28 -\nMeta-learning\nMeta-APR 51.63 48.06 39.50 24.38\n→MAML 50.56 47.38 37.09 21.71\n→UniXcoder 47.45 44.98 34.37 17.66\nTable 4: Results on low-resource TFix.\na better backbone model for APR tasks with an\nencoder-decoder architecture.\nAmong different learning paradigms, we find\nthat transfer-learning from high-resource to low-\nresource bugs and multi-task learning on both\nbugs yield much better results compared to di-\nrectly finetuning on low-resource bugs, validating\nour assumption that low-resource APR can benefit\nfrom the bug-fixing knowledge learned from high-\nresource bug-fix data. These two approaches gener-\nally exhibit comparable performance across differ-\nent benchmarks, and the upsampling strategy often\nproves to be helpful in multi-task learning. Over-\nall, our Meta-APR further improves the adaptation\nfrom high-resource to low-resource bugs, thereby\nleading to superior APR performance. Notably, the\nperformance gain of Meta-APR over other learning\nparadigms becomes more significant when there\nare fewer or even no low-resource training samples\navailable. This implies that Meta-APR is able to\nlearn a better model initialization that captures the\nerror-specific knowledge, thereby enabling faster\nadaptation to the target low-resource error types.\nAblation Study We consider two variants of\nMeta-APR to verify the design choices in our\nproposed framework, where “ →MAML” means\nthat we replace the first-order meta-learning algo-\nrithm with a second-order meta-learning approach\nMAML, and “→UniXcoder” means that we change\nthe backbone model CodeT5 to UniXcoder. From\nthe results, we find that both CodeT5 and the first-\norder meta-learning algorithm are important in en-\nhancing low-resource APR performance, observed\nby a consistent performance drop from these two\nvariants in most settings across 3 benchmarks. Note\nthat our Meta-APR’s first-order meta-learning is\n20 40 60 80 100 120 140 160\nNumber of tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Fraction\nExact Match\nMeta-APR Correct\nMeta-APR Wrong\n(a)\n<50 (36) 50~75 (161) 75~100 (159) 100~125 (119) 125~150 (56) >150 (38)\nNumber of tokens\n0\n10\n20\n30\n40\n50\n60\n70Exact Match Score (%)\nFine-tuning Transfer-learning Meta-APR\n(b)\nFigure 3: (a): cumulative fraction of programs by num-\nber of tokens in the source buggy patch, grouped by\nwhether Meta-APR can have a correct fix. (b): distri-\nbution of correct fix over number of tokens for low-\nresource finetuning and transfer-learning from high-\nresource to low-resource bugs and our Meta-APR.\nalso more efficient than MAML’s second-order\nmeta-learning approach.\n5.2 Further Analysis\nWe proceed to analyze the model predictions to\nbetter understand our Meta-APR behaves in fixing\nvarious bugs compared to other approaches. All\nresults in this section are under a 100-shot setting.\nEffect of Bug Sequence Length We analyze how\nMeta-APR performs in fixing low-resource bugs\nwith varying numbers of bug tokens. Fig. 3a shows\nthe cumulative fractions of bugs by their number of\ntokens, grouped based on the Meta-APR repair out-\ncome (EM). Comparing the blue and orange lines,\nwe observe that the blue one is consistently above\nthe orange one, and if we select a fixed cumulative\nfraction based on the y-axis, the blue line (correct\nfixes) will have fewer tokens (i.e. shorter) than\nthe orange one (wrong fixes), indicating the bugs\nsuccessfully fixed by Meta-APR tend to be shorter\nthan the ones that are incorrectly fixed. We further\ncompare Meta-APR with other training strategies,\nbased on the same backbone model of CodeT5, in\nfixing bugs with various lengths in Fig. 3b. We\n6960\nA (89) B (109) C (97) D (63) E (121) F (12) G (41) H (37)\nError Type\n0\n20\n40\n60\n80\n100Exact Match Score (%)\nFine-tuning Transfer-learning Meta-APR\nFigure 4: Distribution of correct fix on 9 low-resource\nerror types from ManySStuBs4J. Number of bugs for\neach type is included in the parentheses at the x-axis.\nThe details of error type A-H can be found in Table 5.\n(a) Transfer-learning 2-D embedding plot (b) Meta-APR 2-D embedding plot\nFigure 5: 2-D visualization of embeddings for high-\nresource bugs (blue) and low-resource bugs (red).\nobserve a monotonous performance decline when\nthe bug sequence length increases for all models,\nsuggesting that shorter bugs are easier to be fixed\nwhich might be due to their limited complexity.\nEffect of Error Type We further analyze how\nMeta-APR performs in addressing various error\ntypes. Fig. 4 presents a breakdown of results by\nerror type for ManySStuBs4J, where we can find\na notable variance in performance across different\nerror types. For instance, Meta-APR achieves more\nthan 50% EM score on types A, C, and E, while\nit achieves around 10% more EM score on type\nB. Overall, we observe that Meta-APR is a more\nrobust APR method, consistently outperforming\nother finetuning strategies. Interestingly, finetuning\nonly on low-resource bugs achieves comparable\nperformance to Meta-APR in fixing bug type F\n(add throws exception) and G (delete throws excep-\ntion). These bugs relate to the decision to add or\nremove a ‘throws’ clause in a function declaration,\nimplying that such error types comprise easy-to-fix\nbugs and require only a few training samples.\nRepresentation Visualization To understand\nhow Meta-APR learns a better model initializa-\nInput Prompt\n//code context:\nclass ClientTestCase(unittest.TestCase):\n   self.assertTrue(isinstance(models, dict))\n   keys = list(models.keys())\n   self.assertTrue(len(keys) > 3)\n   self.assertIn(keys, \"ak135f_5s\")\n   # Check random key.\n   self.assertEqual(models[\"ak135f_5s\"][\"item\"],\"components\")\n//buggy line:\n   self.assertIn(keys, \"ak135f_5s\")\n//fix the buggy line:\n    self.assertIn(\"ak135f_5s\" , keys)           \nChatGPT Output\nFigure 6: One example of low-resource TSSB-3M.\ntion through error-specific meta-training compared\nto the default transfer-learning approach, we visu-\nalize the embeddings of both high-resource and\nlow-resource bugs after the high-resource finetun-\ning from transfer-learning and Meta-APR in Fig. 5.\nWe observe that Meta-APR can better align the\nrepresentations between high-resource and low-\nresource bugs so that they are distributed in a\ncloser distance in the embedding vector space, en-\nabling faster adaptation from high-resource to low-\nresource bugs with limited training samples.\nCase Study We provide 3 qualitative examples\nfrom our multilingual low-resource APR bench-\nmarks in Fig. 2. We find that our Meta-APR is able\nto fix the bugs using various fix operations such as\ndeletions, boolean conversion, and identifier renam-\ning, while the standard transfer-learning approach\nfails to fix bugs by simply copying the buggy line\nas the fixed line. This indicates Meta-APR can\nenable faster and better adaptation to low-resource\nAPR scenarios.\n5.3 Comparison with ChatGPT\nRecent studies (Prenner and Robbes, 2021; Joshi\net al., 2022) have shown that large language models\n(LLMs) are capable of bug fixing in zero-shot/few-\nshot settings. In order to investigate their perfor-\nmance in fixing challenging low-resource bugs, we\nuse ChatGPT (GPT-3.5-Turbo5) and evaluate it on\n80 randomly sampled test bug-fix pairs for each\nbenchmark. As illustrated in Fig. 6, we construct\nthe zero-shot prompt to provide the code context\nand its buggy line, together with an instruction “fix\nthe buggy line:”. Besides, we randomly select one\nbug-fix pair from the same error type to design the\none-shot prompt for in-context learning.\nWe report the comparison results in Fig. 7. We\nobserve that Meta-APR significantly surpasses\nChatGPT in both zero-shot/one-shot settings across\n3 tasks. This shows that ChatGPT is still lim-\n5https://chat.openai.com/chat\n6961\nManySStuBs4J TFix TSSB-3M0\n10\n20\n30\n40\n50Number of correct fix\n13\n5\n16\n11\n19\n12\n51\n39 39\nChatGPT Zero-shot ChatGPT One-shot Meta-APR\nFigure 7: Evaluation results of correct fixes on a subset\nof 80 bugs from the test data across three benchmarks.\nited to handle the challenging low-resource bugs\nas it did not see much such bug-fix data during\ntraining due to the data scarcity issue. Addition-\nally, we find that the one-shot example is not al-\nways beneficial for low-resource APR and might\nintroduce some noises compared to zero-shot set-\nting. It substantially improves the performance\non TFix but leads to some performance degrades\non ManySStuBs4J and TSSB-3M. By inspecting\nthe predictions, we find that ChatGPT often pre-\ndicts “no bugs” as it might require more semantic\ninformation for decision-making. Besides, Chat-\nGPT performs pretty well in fixing bugs related to\nsyntax errors such as the error type of “no unsafe\nnegation”, which is to fix the bug by simply adding\nparentheses to an expression after a negation oper-\nator. This is probably due to the fact that ChatGPT\nhas been pretrained on a large-scale code corpus\nand can understand the program syntax well.\n6 Conclusion\nIn this work, we present Meta-APR, a simple yet\neffective framework that extends CodeT5 with\nmeta-learning for low-resource APR. It is a model-\nagnostic framework that can be integrated with any\nlearning-based models. To the best of our knowl-\nedge, we are the first to investigate APR in the low-\nresource setting and curate error-specific datasets\nin different low-resource degrees from three APR\nbenchmarks in Python, Java, and JavaScript. Com-\nprehensive experiments have verified the superior-\nity of Meta-APR over other learning strategies with\nvarious code pretrained models. More analysis\nshows that Meta-APR can better align the represen-\ntations of high-resource and low-resource bugs, and\nfix bugs with various sequence lengths and error\ntypes. A pilot comparison with ChatGPT further\nshows that our Meta-APR is still more capable of\nfixing these challenging low-resource bugs.\nLimitations\nAs we are the first to investigate the low-resource\nAPR tasks, we curated 3 datasets with different\nlow-resource degrees (i.e., shot=10/50/100) from\nexisting APR benchmarks to support our study.\nSuch data construction will have a data quality de-\npendency issue from those original APR datasets.\nBesides, the low-resource sub-sampling may in-\ntroduce some randomness issues. To mitigate this\nissue, we performed multiple rounds of random\nsampling with different seeds and reported the av-\nerage results. Furthermore, to evaluate the APR\nperformance, we employ exact match scores as\nthe metric to compare the predicted fixes with the\nground-truth fixes written by developers, which\nmight fail to capture other correct fixes with differ-\nent formats and styles.\nEthics Statement\nOur work complies with ACL Ethics Policy. In\nthis work, we construct our datasets using pub-\nlicly available APR benchmarks, which are widely\nused to examine the program repair performance.\nWe provide detailed procedures to create our low-\nresource APR datasets and provide proper citations\nto their source benchmarks. We will publicly re-\nlease our curated datasets with the same licenses as\ntheir source datasets. As an APR tool, one potential\nrisk of Meta-APR is that the predicted fixes from\nMeta-APR cannot be guaranteed to be correct, and\ndirectly adopting them without manual checking\ncould cause security risks to the software develop-\nment. We suggest that all the fixes should have a\nmanual check from experts before real adoption.\nReferences\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and\nJan Gescei. 2013. On the optimization of a synap-\ntic learning rule. In Optimality in Biological and\nArtificial Networks?, pages 281–303. Routledge.\nBerkay Berabi, Jingxuan He, Veselin Raychev, and Mar-\ntin T. Vechev. 2021. Tfix: Learning to fix coding\nerrors with a text-to-text transformer. In Proceed-\nings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 780–791. PMLR.\nNghi Bui, Yue Wang, and Steven C. H. Hoi. 2022.\nDetect-localize-repair: A unified framework for learn-\ning to debug with codet5. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\n6962\nAbu Dhabi, United Arab Emirates, December 7-11,\n2022, pages 812–823. Association for Computational\nLinguistics.\nYitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong\nGu. 2022. Cross-domain deep code search with few-\nshot meta learning. CoRR, abs/2201.00150.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nZimin Chen, Steve Kommrusch, Michele Tufano, Louis-\nNoël Pouchet, Denys Poshyvanyk, and Martin Mon-\nperrus. 2021b. Sequencer: Sequence-to-sequence\nlearning for end-to-end program repair. IEEE Trans.\nSoftware Eng., 47(9):1943–1959.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042–13054.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 1536–1547. Association\nfor Computational Linguistics.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1126–1135. PMLR.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3816–3830. Association for Computational Linguis-\ntics.\nLuca Gazzola, Daniela Micucci, and Leonardo Mariani.\n2019. Automatic software repair: A survey. IEEE\nTrans. Software Eng., 45(1):34–67.\nClaire Le Goues, ThanhVu Nguyen, Stephanie Forrest,\nand Westley Weimer. 2012. Genprog: A generic\nmethod for automatic software repair. IEEE Trans.\nSoftware Eng., 38(1):54–72.\nEdward Grefenstette, Brandon Amos, Denis Yarats,\nPhu Mon Htut, Artem Molchanov, Franziska Meier,\nDouwe Kiela, Kyunghyun Cho, and Soumith Chin-\ntala. 2019. Generalized inner loop meta-learning.\nCoRR, abs/1910.01727.\nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,\nand Kyunghyun Cho. 2018. Meta-learning for low-\nresource neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 3622–3631.\nAssociation for Computational Linguistics.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 7212–7225. Association for Computa-\ntional Linguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\natkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin B. Clement, Dawn Drain, Neel Sundare-\nsan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021.\nGraphcodebert: Pre-training code representations\nwith data flow. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nJiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao,\nand Xiangqun Chen. 2018. Shaping program repair\n6963\nspace with existing patches and similar code. In\nProceedings of the 27th ACM SIGSOFT International\nSymposium on Software Testing and Analysis, ISSTA\n2018, Amsterdam, The Netherlands, July 16-21, 2018,\npages 298–309. ACM.\nNan Jiang, Thibaud Lutellier, and Lin Tan. 2021.\nCURE: code-aware neural machine translation for\nautomatic program repair. In 43rd IEEE/ACM Inter-\nnational Conference on Software Engineering, ICSE\n2021, Madrid, Spain, 22-30 May 2021, pages 1161–\n1173. IEEE.\nMagne Jørgensen and Martin J. Shepperd. 2007. A\nsystematic review of software development cost esti-\nmation studies. IEEE Trans. Software Eng., 33(1):33–\n53.\nHarshit Joshi, José Pablo Cambronero Sánchez, Sumit\nGulwani, Vu Le, Ivan Radicek, and Gust Verbruggen.\n2022. Repair is nearly generation: Multilingual pro-\ngram repair with llms. CoRR, abs/2208.11640.\nRafael-Michael Karampatsis and Charles Sutton. 2020.\nHow often do single-statement bugs occur?: The\nmanysstubs4j dataset. In MSR ’20: 17th Interna-\ntional Conference on Mining Software Repositories,\nSeoul, Republic of Korea, 29-30 June, 2020, pages\n573–577. ACM.\nAnil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dong-\nsun Kim, Jacques Klein, Martin Monperrus, and\nYves Le Traon. 2020. Fixminer: Mining relevant\nfix patterns for automated program repair. Empir.\nSoftw. Eng., 25(3):1980–2024.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu-Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. In NeurIPS.\nShuai Lin, Pan Zhou, Xiaodan Liang, Jianheng Tang,\nRuihui Zhao, Ziliang Chen, and Liang Lin. 2021.\nGraph-evolving meta-learning for low-resource med-\nical dialogue generation. In Thirty-Fifth AAAI Con-\nference on Artificial Intelligence, AAAI 2021, Thirty-\nThird Conference on Innovative Applications of Arti-\nficial Intelligence, IAAI 2021, The Eleventh Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, EAAI 2021, Virtual Event, February 2-9, 2021,\npages 13362–13370. AAAI Press.\nKui Liu, Anil Koyuncu, Dongsun Kim, and\nTegawendé F. Bissyandé. 2019. Tbar: revisiting\ntemplate-based automated program repair. In Pro-\nceedings of the 28th ACM SIGSOFT International\nSymposium on Software Testing and Analysis, ISSTA\n2019, Beijing, China, July 15-19, 2019, pages 31–42.\nACM.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR (Poster). Open-\nReview.net.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie\nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nThibaud Lutellier, Hung Viet Pham, Lawrence Pang,\nYitong Li, Moshi Wei, and Lin Tan. 2020. Coconut:\ncombining context-aware neural translation models\nusing ensemble for program repair. In ISSTA ’20:\n29th ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis, Virtual Event, USA,\nJuly 18-22, 2020, pages 101–114. ACM.\nFlorian Lux and Ngoc Thang Vu. 2022. Language-\nagnostic meta-learning for low-resource text-to-\nspeech with articulatory features. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n6858–6868. Association for Computational Linguis-\ntics.\nMatias Martinez and Martin Monperrus. 2018. Ultra-\nlarge repair search space with automatically mined\ntemplates: The cardumen mode of astor. In\nSearch-Based Software Engineering - 10th Interna-\ntional Symposium, SSBSE 2018, Montpellier, France,\nSeptember 8-9, 2018, Proceedings , volume 11036\nof Lecture Notes in Computer Science, pages 65–86.\nSpringer.\nFei Mi, Minlie Huang, Jiyong Zhang, and Boi Falt-\nings. 2019. Meta-learning for low-resource natural\nlanguage generation in task-oriented dialogue sys-\ntems. In Proceedings of the Twenty-Eighth Interna-\ntional Joint Conference on Artificial Intelligence, IJ-\nCAI 2019, Macao, China, August 10-16, 2019, pages\n3151–3157. ijcai.org.\nHoang Duong Thien Nguyen, Dawei Qi, Abhik Roy-\nchoudhury, and Satish Chandra. 2013. Semfix: pro-\ngram repair via semantic analysis. In 35th Inter-\nnational Conference on Software Engineering, ICSE\n’13, San Francisco, CA, USA, May 18-26, 2013, pages\n772–781. IEEE Computer Society.\nAlex Nichol, Joshua Achiam, and John Schulman. 2018.\nOn first-order meta-learning algorithms. CoRR,\nabs/1803.02999.\nOpenAI. Chatgpt. 2022.\nCheonbok Park, Yunwon Tae, Taehee Kim, Soy-\noung Yang, Mohammad Azam Khan, Lucy Park,\nand Jaegul Choo. 2021. Unsupervised neural ma-\nchine translation for low-resource domains via meta-\nlearning. In Proceedings of the 59th Annual Meeting\n6964\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n2888–2901. Association for Computational Linguis-\ntics.\nStrategic Planning. 2002. The economic impacts of in-\nadequate infrastructure for software testing. National\nInstitute of Standards and Technology, page 1.\nJulian Aron Prenner and Romain Robbes. 2021. Auto-\nmatic program repair with openai’s codex: Evaluat-\ning quixbugs. CoRR, abs/2111.03922.\nYuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and\nChengsong Wang. 2014. The strength of random\nsearch on automated program repair. In 36th Inter-\nnational Conference on Software Engineering, ICSE\n’14, Hyderabad, India - May 31 - June 07, 2014 ,\npages 254–265. ACM.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nMoiz Rauf, Sebastian Padó, and Michael Pradel. 2022.\nMeta learning for code summarization. CoRR,\nabs/2201.08310.\nCedric Richter and Heike Wehrheim. 2022. TSSB-3M:\nmining single statement bugs at massive scale. In\n19th IEEE/ACM International Conference on Mining\nSoftware Repositories, MSR 2022, Pittsburgh, PA,\nUSA, May 23-24, 2022, pages 418–422. ACM.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nKristín Fjóla Tómasdóttir, Mauricio Finavaro Aniche,\nand Arie van Deursen. 2020. The adoption of\njavascript linters in practice: A case study on eslint.\nIEEE Trans. Software Eng., 46(8):863–891.\nMichele Tufano, Cody Watson, Gabriele Bavota, Massi-\nmiliano Di Penta, Martin White, and Denys Poshy-\nvanyk. 2019. An empirical study on learning bug-\nfixing patches in the wild via neural machine transla-\ntion. ACM Trans. Softw. Eng. Methodol., 28(4):19:1–\n19:29.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nRicardo Vilalta and Youssef Drissi. 2002. A perspective\nview and survey of meta-learning. Artif. Intell. Rev.,\n18(2):77–95.\nWeishi Wang, Yue Wang, Shafiq Joty, and Steven C. H.\nHoi. 2023a. Rap-gen: Retrieval-augmented patch\ngeneration with codet5 for automatic program repair.\nCoRR, abs/2309.06057.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi\nD. Q. Bui, Junnan Li, and Steven C. H. Hoi.\n2023b. Codet5+: Open code large language mod-\nels for code understanding and generation. CoRR,\nabs/2305.07922.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code under-\nstanding and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November,\n2021, pages 8696–8708. Association for Computa-\ntional Linguistics.\nCathrin Weiß, Rahul Premraj, Thomas Zimmermann,\nand Andreas Zeller. 2007. How long will it take\nto fix this bug? In Fourth International Workshop\non Mining Software Repositories, MSR 2007 (ICSE\nWorkshop), Minneapolis, MN, USA, May 19-20, 2007,\nProceedings, page 1. IEEE Computer Society.\nEmily Winter, Vesna Nowack, David Bowes, Steve\nCounsell, Tracy Hall, Sæmundur Óskar Haraldsson,\nand John R. Woodward. 2023. Let’s talk with devel-\nopers, not about developers: A review of automatic\nprogram repair research. IEEE Trans. Software Eng.,\n49(1):419–436.\nRui Xie, Tianxiang Hu, Wei Ye, and Shikun Zhang.\n2022. Low-resources project-specific code summa-\nrization. In 37th IEEE/ACM International Confer-\nence on Automated Software Engineering, ASE 2022,\nRochester, MI, USA, October 10-14, 2022 , pages\n68:1–68:12. ACM.\nJifeng Xuan, Matias Martinez, Favio Demarco, Maxime\nClement, Sebastian R. Lamelas Marcote, Thomas\nDurieux, Daniel Le Berre, and Martin Monperrus.\n2017. Nopol: Automatic repair of conditional state-\nment bugs in java programs. IEEE Trans. Software\nEng., 43(1):34–55.\nQuanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong\nSun, and Zhenyu Chen. 2023. A survey of\nlearning-based automated program repair. CoRR,\nabs/2301.03270.\nQuanjun Zhang, Yuan Zhao, Weisong Sun, Chunrong\nFang, Ziyuan Wang, and Lingming Zhang. 2022.\nProgram repair: Automated vs. manual. CoRR,\nabs/2203.05166.\nYingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng,\nDongkyu Lee, Yiping Song, Jian Sun, and Nevin L.\n6965\nZhang. 2022. Improving meta-learning for low-\nresource text classification and generation via mem-\nory imitation. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 583–595. Associa-\ntion for Computational Linguistics.\nQihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang,\nKang Yuan, Yingfei Xiong, and Lu Zhang. 2021.\nA syntax-guided edit decoder for neural program\nrepair. In ESEC/FSE ’21: 29th ACM Joint European\nSoftware Engineering Conference and Symposium\non the Foundations of Software Engineering, Athens,\nGreece, August 23-28, 2021, pages 341–353. ACM.\nA Appendix\nA.1 More Dataset Statistics\nWe provide the detailed statistics of our curated\nlow-resource APR benchmarks in Table 5, Table 6,\nand Table 7 for ManySStuBs4J, TSSB, and TFix\nrespectively. We can observe a very imbalanced\nerror type distribution across these benchmarks.\n6966\nError Type Train Valid Test All\nHigh-resource\nCHANGE_IDENTIFIER 10,175 1,250 1,250 12,675\nOVERLOAD_METHOD_MORE_ARGS 2,734 338 339 3,411\nCHANGE_NUMERAL 2,694 336 336 3,366\nCHANGE_MODIFIER 2,580 322 322 3,224\nMORE_SPECIFIC_IF 1,028 128 128 1,284\nCHANGE_OPERATOR 1,014 126 127 1,267\nLow-resource\nLESS_SPECIFIC_IF (E) 968 121 121 1,210\nSW AP_BOOLEAN_LITERAL (B) 871 109 109 1,089\nOVERLOAD_METHOD_DELETED_ARGS (C) 790 98 97 985\nCHANGE_CALLER_IN_FUNCTION_CALL (A) 712 89 89 890\nCHANGE_UNARY_OPERATOR (D) 511 64 63 638\nDELETE_THROWS_EXCEPTION (G) 328 41 41 410\nSW AP_ARGUMENTS (H) 304 38 37 379\nADD_THROWS_EXCEPTION (F) 98 12 12 122\nTable 5: Statistics of ManySStuBs4J benchmark.\nError Type Train Valid Test All\nHigh-resource\nSINGLE_STMT 24,346 3,044 3,043 30,433\nCHANGE_STRING_LITERAL 14,285 1,786 1,785 17,856\nCHANGE_IDENTIFIER_USED 5,035 630 629 6,294\nCHANGE_BINARY_OPERAND 3,434 430 429 4,293\nSAME_FUNCTION_MORE_ARGS 3,061 383 383 3,827\nWRONG_FUNCTION_NAME 2,813 352 351 3,516\nCHANGE_NUMERIC_LITERAL 2,480 310 310 3,100\nADD_FUNCTION_AROUND_EXPRESSION 2,318 290 290 2,898\nCHANGE_ATTRIBUTE_USED 2,206 276 276 2,758\nSINGLE_TOKEN 1,895 237 237 2,369\nADD_METHOD_CALL 1,290 161 161 1,612\nMORE_SPECIFIC_IF 1,101 138 137 1,376\nADD_ELEMENTS_TO_ITERABLE 1,070 134 133 1,337\nSAME_FUNCTION_LESS_ARGS 1,050 131 131 1,312\nLow-resource\nCHANGE_BOOLEAN_LITERAL 907 114 113 1,134\nADD_ATTRIBUTE_ACCESS 716 90 89 895\nCHANGE_BINARY_OPERATOR 681 85 85 851\nSAME_FUNCTION_WRONG_CALLER 558 70 70 698\nCHANGE_KEYWORD_ARGUMENT_USED 470 59 59 588\nLESS_SPECIFIC_IF 382 48 48 478\nCHANGE_UNARY_OPERATOR 318 40 40 398\nSAME_FUNCTION_SW AP_ARGS 150 18 19 187\nCHANGE_CONSTANT_TYPE 118 15 15 148\nTable 6: Statistics of TSSB benchmark.\n6967\nError Type Train Valid Test All\nHigh-resource\nno-invalid-this 13,101 1,456 1,609 16,166\nno-undef 8,614 958 1,064 10,636\nno-unused-vars 6,289 699 777 7,765\ncomma-style 5,180 576 639 6,395\nno-redeclare 5,167 575 639 6,381\nno-extra-semi 4,834 537 598 5,969\nno-unreachable 3,826 426 473 4,725\nprefer-rest-params 3,675 405 454 4,534\nno-debugger 3,372 375 417 4,164\nno-throw-literal 3,300 367 408 4,075\nguard-for-in 2,616 291 324 3,231\nno-console 2,484 276 307 3,067\nno-useless-escape 2,364 263 293 2,920\nprefer-spread 2,001 221 244 2,466\nno-dupe-keys 1,765 197 219 2,181\nno-empty 1,665 184 206 2,055\nno-process-exit 1,225 137 152 1,514\nno-cond-assign 1,194 132 146 1,472\nno-extra-boolean-cast 1,180 132 146 1,458\ngenerator-star-spacing 1,130 126 140 1,396\nno-constant-condition 1,016 112 123 1,251\nLow-resource\nno-array-constructor 793 89 98 980\nno-inner-declarations 671 75 84 830\nno-fallthrough 601 67 75 743\nno-case-declarations 584 66 73 723\nno-extra-bind 547 59 68 674\nno-self-assign 494 55 61 610\nvalid-typeof 436 49 54 539\nconstructor-super 375 42 47 464\nno-new-object 360 41 45 446\nno-caller 360 41 45 446\nno-extend-native 358 40 45 443\nrequire-yield 347 39 43 429\nno-unsafe-negation 342 38 43 423\nno-this-before-super 333 38 42 413\nno-new-wrappers 291 33 36 360\nno-global-assign 257 29 32 318\nno-const-assign 224 25 28 277\nno-sparse-arrays 191 22 24 237\ngetter-return 163 19 21 203\nno-duplicate-case 157 18 20 195\nno-unused-labels 151 17 19 187\nno-empty-pattern 144 16 18 178\nno-func-assign 118 14 15 147\nno-dupe-class-members 94 11 12 117\nno-class-assign 89 10 12 111\nuse-isnan 56 7 8 71\nno-unsafe-finally 50 6 7 63\nfor-direction 40 5 5 50\nno-ex-assign 32 4 4 40\nno-compare-neg-zero 9 2 2 13\nno-new-symbol 18 1 1 10\nTable 7: Statistics of TFix benchmark.\n6968",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8585641980171204
    },
    {
      "name": "Debugging",
      "score": 0.6938208937644958
    },
    {
      "name": "Machine learning",
      "score": 0.6101642847061157
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5778006315231323
    },
    {
      "name": "Encoder",
      "score": 0.5490401983261108
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5282147526741028
    },
    {
      "name": "Transformer",
      "score": 0.4799675941467285
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.4402189552783966
    },
    {
      "name": "Code (set theory)",
      "score": 0.4316948652267456
    },
    {
      "name": "Software",
      "score": 0.42891913652420044
    },
    {
      "name": "Source code",
      "score": 0.4264995753765106
    },
    {
      "name": "Software bug",
      "score": 0.4173800051212311
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.41718244552612305
    },
    {
      "name": "Language model",
      "score": 0.41126489639282227
    },
    {
      "name": "Task (project management)",
      "score": 0.2835308313369751
    },
    {
      "name": "Programming language",
      "score": 0.22445029020309448
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}