{
    "title": "OctFormer: Octree-based Transformers for 3D Point Clouds",
    "url": "https://openalex.org/W4372283849",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2975140934",
            "name": "Peng-Shuai Wang",
            "affiliations": [
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964253930",
        "https://openalex.org/W3034579518",
        "https://openalex.org/W3183392001",
        "https://openalex.org/W4312442876",
        "https://openalex.org/W2982649931",
        "https://openalex.org/W6635487051",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W4313007769",
        "https://openalex.org/W4312307873",
        "https://openalex.org/W2963182550",
        "https://openalex.org/W3166573884",
        "https://openalex.org/W2910628332",
        "https://openalex.org/W4313145913",
        "https://openalex.org/W2953399169",
        "https://openalex.org/W3034591723",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4214526701",
        "https://openalex.org/W2211722331",
        "https://openalex.org/W4214624153",
        "https://openalex.org/W3004300126",
        "https://openalex.org/W4206398307",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W6820303615",
        "https://openalex.org/W4312649925",
        "https://openalex.org/W3215207332",
        "https://openalex.org/W2606202972",
        "https://openalex.org/W1923184257",
        "https://openalex.org/W4312916565",
        "https://openalex.org/W4206645116",
        "https://openalex.org/W2737234477",
        "https://openalex.org/W2890556874",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6814250579",
        "https://openalex.org/W2142499637",
        "https://openalex.org/W1920022804",
        "https://openalex.org/W4214704706",
        "https://openalex.org/W3034428269",
        "https://openalex.org/W3034239841",
        "https://openalex.org/W3107479685",
        "https://openalex.org/W3096754345",
        "https://openalex.org/W3125010829",
        "https://openalex.org/W2795245768",
        "https://openalex.org/W3104141662",
        "https://openalex.org/W2150794573",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200 k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D semantic segmentation and 3D object detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.",
    "full_text": "OctFormer: Octree-based Transformers for 3D Point Clouds\nPENG-SHUAI WANG, Peking University, China\n(a) Window Attention (b) Octree Attention (c) Speed and mIoU\n72.2\n73.7\n74.3\n75.2\n75.4\n75.7\nOctFormer-Small\nOctFormer\nPoint Transformer V2\nMinkowskiNet\nStratified Transformer\n157 168 296 511 3430 ms\nMix3D\nmIoU\nFig. 1. Octree Attention and the superiority of OctFormer. (a): The window attention partitions the point cloud with cubic windows and constrains\nthe attention in each window to accelerate the global attention. Each window is encoded by a specific color, as indicated by the front and back view\nof the point cloud. The point number in each window is highly unbalanced, which incurs great computation cost. (b): Our octree attention partitions\nthe point cloud according to the sorted shuffled keys of the octree, ensuring an equal number of points in each window. (c): OctFormer built upon\nour octree attention achieves the best mIoU and efficiency on ScanNet compared with representative sparse-voxel-based CNNs and point cloud\ntransformers. The horizontal axis represents the time of one forward pass of each network on an Nvidia 3090 GPU, taking a batch of 250k points.\nWe propose octree-based transformers, named OctFormer, for 3D point cloud\nlearning. OctFormer can not only serve as a general and effective backbone\nfor 3D point cloud segmentation and object detection but also have linear\ncomplexity and is scalable for large-scale point clouds. The key challenge\nin applying transformers to point clouds is reducing the quadratic, thus over-\nwhelming, computation complexity of attentions. To combat this issue, several\nworks divide point clouds into non-overlapping windows and constrain atten-\ntions in each local window. However, the point number in each window varies\ngreatly, impeding the efficient execution on GPU. Observing that attentions\nare robust to the shapes of local windows, we propose a novel octree atten-\ntion, which leverages sorted shuffled keys of octrees to partition point clouds\ninto local windows containing a fixed number of points while permitting\nshapes of windows to change freely. And we also introduce dilated octree\nattention to expand the receptive field further. Our octree attention can be\nimplemented in 10 lines of code with open-sourced libraries and runs 17\ntimes faster than other point cloud attentions when the point number exceeds\n200ğ‘˜. Built upon the octree attention, OctFormer can be easily scaled up\nand achieves state-of-the-art performances on a series of 3D segmentation\nand detection benchmarks, surpassing previous sparse-voxel-based CNNs\nand point cloud transformers in terms of both efficiency and effectiveness.\nNotably, on the challenging ScanNet200 dataset, OctFormer outperforms\nsparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are\navailable at https://wang-ps.github.io/octformer .\nCCS Concepts: â€¢ Computing methodologies â†’Shape analysis; Point-\nbased models; Neural networks.\nAdditional Key Words and Phrases: Point Clouds, Transformers, Octree, 3D\nSemantic Segmentation, 3D Object Detection\nAuthorâ€™s address: Peng-Shuai Wang, Peking University, China, wangps@hotmail.com.\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not for\nredistribution. The definitive Version of Record was published inACM Transactions on\nGraphics, https://doi.org/10.1145/3592131.\nACM Reference Format:\nPeng-Shuai Wang. 2023. OctFormer: Octree-based Transformers for 3D\nPoint Clouds. ACM Trans. Graph. 42, 4, Article 1 (August 2023), 11 pages.\nhttps://doi.org/10.1145/3592131\n1 INTRODUCTION\n3D point cloud understanding is a fundamental task in computer\ngraphics and vision and has a broad range of applications, including\nrobotics, autonomous driving, and augmented reality. A variety of\ndeep learning methods have been proposed for it, such as voxel-based\nCNNs [Graham et al. 2018; Wang et al. 2017; Wu et al. 2015], view-\nbased CNNs [Su et al . 2015], and point-based networks [Li et al .\n2018; Qi et al. 2016, 2017b], and remarkable progress has been made.\nRecently, point cloud transformers have emerged [Guo et al. 2021;\nMisra et al. 2021; Zhao et al. 2021] as an effective alternative with\nthe potential for cross-multimodality training and general intelligent\nmodels [Radford et al. 2021; Ramesh et al. 2022].\nHowever, the efficiency of point cloud transformers is still much\nworse than their CNN counterparts [Choy et al . 2019; Nekrasov\net al. 2021; Wang et al . 2017], especially on scene-scale datasets\nlike ScanNet [Dai et al. 2017], and the performance of point cloud\ntransformers is also just comparable. Since it has been proven that\ntransformers are at least as expressive as CNNs [Cordonnier et al .\n2020], one of the key challenges of applying transformers to point\nclouds is to overcome the huge computational complexity of trans-\nformers, which is quadratic with the number of elements involved.\nSeveral methods [Guo et al. 2021; Pang et al. 2022; Yu et al. 2022]\ndirectly apply transformers to all points globally, thus limiting their\napplicability to large-scale point clouds. Following the progress in\nscaling up vision transformers [Dong et al. 2022; Liu et al. 2021a],\none effective strategy is to constrain point cloud transformers within\nnon-overlapping local windows [Fan et al. 2022; Lai et al. 2022; Mao\net al. 2021; Sun et al. 2022]. However, unlike images, the number\nof points across different local windows varies significantly due to\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\narXiv:2305.03045v2  [cs.CV]  8 May 2023\n1:2 â€¢ Peng-Shuai Wang\nthe sparsity of point clouds. To deal with this issue, sophisticated im-\nplementations like region batching [Fan et al. 2022; Sun et al. 2022]\nor customized GPU kernels [Lai et al . 2022] have to be adopted,\nwhich severely impedes massive parallelism on GPUs. Another strat-\negy to speed up point cloud transformers is to apply transformers in\ndownsampled feature maps [Cheng et al. 2021b; Park et al. 2022],\nwhich also weakens the network capability and incurs a decrease in\nperformance.\nIn this paper, we present a general and scalable octree-based trans-\nformer, abbreviated as OctFormer, for learning on 3D point clouds.\nThe key building block of OctFormer is a novel octree attention\nmechanism for point clouds. To retain linear complexity, we divide\neach point cloud into small groups when applying attentions. Our key\nobservation is that attentions are insensitive to the actual shape of\nunderlying local windows. Instead of using cubic windows as in pre-\nvious works, which incur variant point numbers in each window, we\ndivide point clouds into groups with irregular windows while keeping\nthe point number in each window the same. Consequently, we can\neasily implement our attention using standard operators provided by\ndeep learning frameworks like PyTorch [Paszke et al. 2019]. To gen-\nerate the required window partition, our second observation is that\nafter constructing an octree with the parallel algorithm in [Zhou et al.\n2011], the octree nodes are sorted in z-order by shuffled keys [Wil-\nhelms and Van Gelder 1992], which ensures that spatially-close\noctree nodes are contiguously stored in memory. We store features\nin tensors according to the order of octree nodes. After padding a\nfew zeros to make the spatial numbers of tensors divisible by the\nspecified point number in each window, we can efficiently generate\nthe window partition by simply reshaping the tensors at almost zero\ncost. An example is shown in Figure 1-(b), where the point number\nin each window is the same. To further increase the receptive fields\nof OctFormer, we introduce a dilated octree attention with dilated\npartitions along the spatial dimension of tensors, which can also be\nefficiently implemented with tensor reshaping and transposing.\nOur OctFormer challenges conventional wisdom in designing point\ncloud transformers from two aspects. First, instead of using fixed-\nsized local windows, we fix the point number in each window when\ndoing point cloud partition, enabling simple implementation and\neasy parallelization; second, instead of regarding point clouds as\nunordered and unstructured point sets, we actually sort the quantized\npoints with shuffled keys by building octrees, resulting in a conve-\nnient window partition. Our octree attention completely eliminates\nthe expensive neighborhood searching used in previous designs [Lai\net al. 2022; Wu et al. 2022], bypasses the sparsity of point clouds,\nand can be reduced to a standard multi-head self-attention [Vaswani\net al. 2017] on small groups of equal size. Consequently, our octree\nattention can be implemented in 10 lines of code with open-sourced\nlibraries freely available on the web. One single transformer block\non top of octree attention runs at least 17 times faster than previous\nstate-of-the-art point transformer blocks [Lai et al. 2022; Wu et al.\n2022] when the number of elements involved is 200ğ‘˜.\nWe also introduce feature hierarchies following the multiscale\nstructure of octrees, endowing OctFormer with the capability as a\ngeneral backbone for 3D segmentation and detection. We verify the\neffectiveness of OctFormer on a series of 3D benchmarks. Specifi-\ncally, our OctFormer achieves the best performance on the validation\nset of ScanNet segmentation [Dai et al. 2017], SUN RGB-D detec-\ntion [Song et al. 2015], and ScanNet200 segmentation [Rozenberszki\net al. 2022], surpassing all previous state-of-the-art sparse-voxel-\nbased CNNs [Choy et al. 2019; Graham et al. 2018; Wang et al. 2017]\nand point cloud transformers [Lai et al. 2022; Wu et al. 2022] by a\nlarge margin. Notably, on ScanNet200 segmentation, which contains\n200 semantic categories (ten times more than ScanNet), the mIoU of\nour OctFormer is higher than MinkowskiNet [Choy et al. 2019] by\n7.3 and even higher than the recently-proposed LGround [Rozenber-\nszki et al. 2022] by 5.4, which pretrains a sparse-voxel-based CNN\nwith a powerful CLIP model [Radford et al. 2021].\nIn summary, our main contributions are as follows:\n- We propose a novel octree attention and its dilated variant, which\nare easy to implement and significantly more efficient than previ-\nous point cloud attentions;\n- We propose OctFormer, which can serve as a general backbone\nfor 3D point cloud segmentation, detection, and classification;\n- OctFormer achieves state-of-the-art performances on a series of\n3D segmentation and detection benchmarks, and the computa-\ntional efficiency of OctFormer is much higher than previous point\ncloud transformers and even surpasses highly optimized sparse-\nvoxel-based CNNs.\n2 RELATED WORK\nVoxel-based CNNs. Full-voxel-based CNNs generalize 2D CNNs\nto 3D learning by representing 3D data with uniformly-sampled vox-\nels [Maturana and Scherer 2015; Qi et al . 2016; Wu et al . 2015].\nHowever, these methods can only take low-resolution voxels like\n323 as input due to their cubic computational and memory cost with\nregard to the voxel resolution. Sparse-voxel-based CNNs greatly im-\nprove the efficiency of full-voxel-based CNNs by constraining CNN\noperations into non-empty sparse voxels and adopting octrees [Wang\net al. 2017, 2018] or hash tables [Choy et al . 2019; Graham et al .\n2018; Shao et al. 2018] to facilitate neighborhood searching for con-\nvolutions. Several other networks also leverage octrees [Lei et al .\n2019; Riegler et al . 2017b,a] for improving the efficiency of full-\nvoxel-based CNNs. Sparse-voxel-based CNNs mainly use small con-\nvolution kernels, while it has been proven that large convolution\nkernels can greatly improve network performance [Chen et al. 2022]\nin practice. Our OctFormer can easily enlarge its window size and\nhas a much larger receptive field than sparse-voxel-based CNNs.\nPoint-based Networks. Instead of rasterizing 3D shapes into reg-\nular voxels, point-based networks directly take raw point clouds\nas input. Since point clouds are unordered and unstructured, these\nnetworks employ permutation-invariant operations [Li et al. 2018;\nQi et al . 2017a,b], continuous convolution kernals [Atzmon et al .\n2018; Fey et al. 2018; Thomas et al. 2019], or adaptive weights [Si-\nmonovsky and Komodakis 2017; Wu et al. 2019] to aggregate and\nupdate point features. To query neighboring points, point-based\nnetworks construct a k-nearest-neighbor graph from input point\nclouds [Fey et al. 2018; Li et al. 2018; Qi et al. 2017b; Simonovsky\nand Komodakis 2017; Xu et al . 2018]. In order to extract hierar-\nchical features, point-based networks often rely on farthest point\nsampling [Qi et al. 2017b] or grid-based sampling [Hu et al. 2020a;\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\nOctFormer: Octree-based Transformers for 3D Point Clouds â€¢ 1:3\nMLP\nOctree Attention\n \nLayerNorm\nLayerNorm\nMLP\nOctree Attention\n \nOctFormer\nBlock\nÃ—N1\nDownsampling\nDownsampling\nDownsampling\nS\n4 Ã— C\n S\n8 Ã— 2C\n S\n16 Ã— 4C\n S\n32 Ã— C\nOctFormer\nBlock\nÃ— N2\nOctFormer\nBlock\nÃ— N3\nOctFormer\nBlock\nÃ— N4\nEmbedding\nPoint Cloud\nLayerNorm\nLayerNorm\n(\na\n) OctFormer\n( b\n) Two Consecutive OctFromer Block\ns\n4C\nDilation = 1 Dilation =  4\nFig. 2. Overview. (a): The architecture of OctFormer. OctFormer consists of an Embedding module, a sequence of OctFormer blocks and\ndownsampling modules. ğ‘† and ğ¶ denote the spatial resolution and channel of features, ğ‘ğ‘– denotes the number of the corresponding OctFormer\nblocks. (b): Two consecutive OctFormer blocks. Each OctFormer block consists of an octree attention, an MLP , and two Layer Normalizations\n(LayerNorm). Two consecutive OctFormer blocks use dilations of 1 and 4 for their respective octree attentions.\nThomas et al. 2019] to downsample point clouds progressively. Our\nOctFormer also takes point cloud as input, but our novel octree atten-\ntion totally avoids expensive k-nearest-neighbour search and farthest\npoint sampling. As a result, OctFormer is much more efficient and\noutperforms previous point-based networks.\nVision Transformers. Inspired by the great success of transformers\nin natural language processing [Vaswani et al. 2017], ViT [Dosovit-\nskiy et al. 2021] uses transformer-based networks for visual recog-\nnition. ViT partitions the input image into non-overlapping regular\npatches, considers each patch as a token, and applies pure atten-\ntions to these tokens. To extend ViT for dense prediction tasks such\nas image segmentation and detection, PVT [Wang et al. 2021] and\nSwin Transformer [Liu et al. 2021a] introduce hierarchical network\narchitectures from CNNs to vision transformers. PVT proposes ap-\nplying attention modules on downsampled feature maps to improve\nthe efficiency of ViT when dealing with large images. On the other\nhand, Swin Transformer introduces shifted-window attentions to\nrestrict attentions in non-overlapping local windows. Many follow-\nup works [Chu et al . 2021a; Dong et al. 2022; Wang et al. 2022b;\nYang et al. 2021] further improve the attention designs with a similar\nnetwork architecture. OctFormer also has a hierarchical network ar-\nchitecture. Our key innovation is a novel octree attention mechanism\nfor point clouds, which utilizes variant local window shapes while\nmaintaining a fixed number of points in each window for efficiency.\nPoint Cloud Transformers. Following vision transformers, it is\nnatural to explore the extension of transformers for point cloud under-\nstanding. Point Cloud Transformer (PCT) [Guo et al. 2021] applies\noffset attentions to all point features for point cloud classification and\nsegmentation. Point-BERT [Yu et al. 2022] and Point-MAE [Pang\net al. 2022] utilize standard transformers trained on point clouds for\nunsupervised pre-training. 3DETR [Misra et al. 2021] proposes an\nend-to-end scheme for point cloud detection with standard transform-\ners. However, these methods are limited to point clouds containing\nonly a few thousand points due to the high computation and memory\ncosts incurred by global attentions.\nPoint Transformer (PT) [Zhao et al. 2021] applies vector attentions\nto a local neighborhood of each point. Although PT has lower mem-\nory costs than PCT, its computation cost is still high due to the usage\nof expensive farthest point sampling when doing pooling operations\nto downsample feature maps. Point Transformer V2 (PTv2) [Wu et al.\n2022] enhances PTâ€™s efficiency by substituting farthest point sam-\npling with grid-based sampling and improves its performance further\nby utilizing grouped vector attentions and additional positional encod-\ning multipliers. The attention modules of both PT and PTv2 are ap-\nplied independently to local neighborhoods of each point in a sliding\nwindow fashion. Since there is no computation sharing among over-\nlapping neighborhoods, significant computation resources are wasted.\nTo scale up point cloud transformers, SST [Fan et al . 2022],\nSWFormer [Sun et al. 2022], and Stratified Transformer [Lai et al.\n2022] extend Swin Transformer [Liu et al . 2021a] for image un-\nderstanding to point clouds by restricting attention modules to non-\noverlapping windows of point clouds. Since the number of points in\neach local window differs greatly, SST and SWFormer group local\nwindows with similar numbers of points together and process them\nin batch mode; while Stratified Transformer leverages sophisticated\nGPU programs to combat this issue. Similar to the scaling strategy of\nPVT [Wang et al. 2021], PatchFormer [Cheng et al. 2021b] applies\nattention modules to patch features instead of point features; Fast\nPoint Transformer [Park et al. 2022] downsamples the point cloud\ninto low-resolution voxels and restricts the attention modules to those\nvoxels. However, the performance of PatchFormer and Fast Point\nTransformer is worse than other contemporary point cloud transform-\ners. Unlike previous point cloud transformers, our OctFormer divides\nthe input point clouds into groups containing an equal number of\npoints, making it easy to parallelize and scale up. Our OctFormer\nalso achieves state-of-the-art performance on semantic segmentation\nand object detection on large-scale benchmarks.\n3 OCTREE-BASED TRANSFORMERS\nOverview. The overview of OctFormer is shown in Figure 2. Given\nan input point cloud, we first normalize it with a specified scale\nfactor and convert it to an octree. The initial features include average\npoint positions, colors, and point normals (if provided) stored in\nnon-empty octree leaf nodes. Then an embedding module is used to\ndownsample and project the initial features into a high-dimensional\nspace. Next, a sequence of OctFormer blocks and downsampling\nmodules are alternately applied to generate high-level hierarchical\nfeatures, which can be consumed by a lightweight Feature Pyramid\nNetwork (FPN) [Kirillov et al. 2019; Lin et al. 2017] for semantic\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\n1:4 â€¢ Peng-Shuai Wang\n(a) Octree\n (b) Z-Order Curve (c) Window Partition (d) Dilated Partition (e) Z-Order Curve (f) Window Partition (g) Dilated Partition\nFig. 3. Window partition for the octree attention. Here 2D images are shown for a better illustration. (a): An input point cloud sampled from a shape\nin red and the corresponding octree (quadtree). Non-empty octree nodes are highlighted in gray. (b): Z-order curve at depth 3 of the octree. (c): A\nwindow partition generated by tensor reshaping and transposing corresponding to (b), with a point number of 7. The features are stored in a tensor\nfollowing the order of non-empty octree nodes on the z-order curve. (d): A dilated partition with a point number of 7 and a dilation of 2. (e): Z-order\ncurve covering the whole space. (f): A window partition corresponding to (e) with a point number of 16. (g): A dilated partition corresponding to (e)\nwith a point number of 16 and a dilation of 4.\nsegmentation and object detection. The core of OctFormer is a novel\noctree attention module as shown in Figure 2-(b), which is elaborated\nin Section 3.1. Other network components, including the embedding\nand downsampling module, the OctFormer block, and the detailed\nnetwork configurations, are introduced in Section 3.2.\n3.1 Octree Attention\nAttention. Our octree attention is built upon the scaled dot-product\nattention proposed by [Vaswani et al. 2017], which is widely used in\ntransformers in NLP and vision. We first review the attention mod-\nule and introduce the observations motivating our octree attention.\nDenote an input feature map as ğ‘‹ âˆˆRğ‘Ã—ğ¶, where ğ‘ is the spatial\nnumber, and ğ¶ is the channel number. Three learnable weight matri-\nces ğ‘Šğ‘, ğ‘Šğ‘˜, and ğ‘Šğ‘£ in Rğ¶Ã—ğ· are used to map ğ‘‹ to queries, keys and\nvalues. Then the attention can be defined as follows:\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘‹)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥\n\u0010\n(ğ‘‹ğ‘Šğ‘)(ğ‘‹ğ‘Šğ‘˜)ğ‘‡/\nâˆš\nğ·\n\u0011\n(ğ‘‹ğ‘Šğ‘£). (1)\nIntuitively, the output can be regarded as a weighted sum of the\nvalues with weights dynamically computed from queries and the\ncorresponding keys. The multi-head attention can be implemented\nby concatenating ğ» independent attentions and merging the results\nwith a linear layer, where ğ» denotes the head number. In OctFormer,\nwe adopt the multi-head attention by default, and we directly use the\nterm attention to refer to the multi-head attention for simplicity.\nAccording to Equation 1, the computation complexity of the at-\ntention is O(ğ‘2), which is generally unaffordable when ğ‘ exceeds\nseveral thousand, whereas point clouds from large-scale scanned\ndatasets, such as ScanNet [Dai et al. 2017], often contain over 100ğ‘˜\npoints. To scale up the attention, we leverage the strategy of window\nattentions [Liu et al. 2021a] to constrain the computation within non-\noverlapping local windows. Denote the point number in each local\nwindow as ğ¾, then the computation complexity of window attentions\nis reduced to O(ğ¾2 Â·ğ‘\nğ¾), which is linear to ğ‘. A naive implemen-\ntation is to follow vision transformers [Liu et al. 2021a] to partition\npoint clouds into non-overlapping windows with 3D cubes [Fan et al.\n2022; Lai et al. 2022; Sun et al. 2022]. However, different from im-\nages, the point number in each window varies greatly. Concretely,\nwith a window size of 7 on ScanNet, the average point number is only\n48, whereas the maximum point number is 343. This issue severely\nhampers the efficient execution of attentions on GPUs.\nObserving that the attention aggregates features via weighted aver-\nages with weights normalized by a softmax function, we hypothesize\nthat the attention is robust to the change of underlying window shapes.\nAnd we verify this observation empirically via a pilot study. Specifi-\ncally, we use a ViT [Dosovitskiy et al. 2021] pretrained on a 16 Ã—16\npartition of images defined within a square window and alter the win-\ndow shape by randomly masking out 20% of image patches, resulting\nin highly irregular windows. We utilize the codebase and weights\nprovided by timm [Wightman 2019]. After the masking operation,\nthe accuracy of ViT on the validation set of ImageNet [Deng et al .\n2009] drops only slightly, from 85.1% to 84.2%. This observation\nmotivates us to keep the point number in each local window constant\nwhile allowing the shape of local windows to vary freely, as opposed\nto simply using cubic windows. By keeping the point numbers fixed\nin every window, the parallelized computation of GPU can be fully\nexploited, and the implementation can be greatly simplified.\nOctree. We adopt octrees to generate the required window partition\nand facilitate the hierarchical network architecture. For each point\ncloud, we employ the parallel algorithm proposed by [Zhou et al .\n2011] to build an octree on GPU. After building the octree, octree\nnodes of the same depth are sorted with the shuffled keys [Wilhelms\nand Van Gelder 1992]. Denote the integer coordinates of an octree\nnode as (ğ‘¥,ğ‘¦,ğ‘§ ), and the ğ‘–ğ‘¡â„ bit of each coordinate as ğ‘¥ğ‘–, ğ‘¦ğ‘–, and ğ‘§ğ‘–,\nthe shuffled key in binary expression is defined as follows:\nğ¾ğ‘’ğ‘¦(ğ‘¥,ğ‘¦,ğ‘§ )= ğ‘¥1ğ‘¦1ğ‘§1ğ‘¥2ğ‘¦2ğ‘§2 ...ğ‘¥ ğ‘‘ğ‘¦ğ‘‘ğ‘§ğ‘‘, (2)\nwhere ğ‘‘ is the depth of the octree. The value of a shuffled key repre-\nsents the position on the 3D z-order curve. We observe that octree\nnodes belong to one parent node, and more generally, octree nodes\nbelonging to one subtree are contiguously stored in memory accord-\ning to the z-order. This property of octrees is the key to our efficient\nwindow partition. A 2D illustration is shown in Figure 3. For an\noctree built from a point cloud marked in red color in Figure 3-(a),\nthe corresponding z-order curve in depth 3 of the octree is shown in\nFigure 3-(b). Since the octree nodes are sparse, the full z-order curve\nis shown in Figure 3-(e) for reference.\nOctree Attention. With sorted shuffled keys of octrees, we can\neasily generate window partitions through tensor reshaping and trans-\nposing. Note that only non-empty octree nodes contain feature vec-\ntors. We can trivially filter out empty nodes with the information\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\nOctFormer: Octree-based Transformers for 3D Point Clouds â€¢ 1:5\nprovided by octrees while preserving the original order. An example\nis shown in Figure 3-(a), where non-empty octree nodes are marked\nin gray color. We stack all features in non-empty octree nodes into\na 2D tensor following the order of sorted shuffled keys. Denote the\nresulting feature tensor asğ‘‹ with a shape of (ğ‘,ğ¶), we first pad zeros\nto create a new tensor Ë†ğ‘‹ with a shape of (Ë†ğ‘,ğ¶), so that Ë†ğ‘ is divisible\nby the specified point number ğ¾, which is typically set to 32 in our\nexperiments. Then we generate the window partition by reshaping\nË†ğ‘‹ to (ğµ,ğ¾,ğ¶ ), where ğµequals Ë†ğ‘/ğ¾ and denotes the total number of\nwindows. With this partition, we can implement our octree attention\nby applying the attention in Equation 1 to theseğµwindows in parallel\nwhile masking out the padded elements. An example is shown in\nFigure 3-(c), where ğ¾ and ğ‘ are equal to 7 and 28, respectively.\nAlthough window attentions speed up the computation, they come\nwith a reduced receptive field and a lack of information propagation\namong different windows. To alleviate these issues, we further pro-\npose a dilated octree attention. Denote the dilation as ğ·, which is\nset to 1 or 4 in our experiments. For the octree attention described\nabove, its dilation can be regarded as 1. When ğ· is larger than 1,\nwe pad ğ‘‹ to Ëœğ‘‹ so that the spatial number Ëœğ‘ of Ëœğ‘‹ is divisible by\nğ¾Ã—ğ·. Next, we reshape Ëœğ‘‹ to a tensor with shape (Ëœğµ,ğ¾,ğ·,ğ¶ )where\nËœğµ equals Ëœğ‘/(ğ¾ Ã—ğ·). Then we transpose it to a tensor with shape\n(Ëœğµ,ğ·,ğ¾,ğ¶ ), and flatten the first two dimensions to get a tensor with\nshape (ğµ,ğ¾,ğ¶ ), with which the attention in Equation 1 can also be\ndirectly applied. An example is shown in Figure 3-(c), where ğ¾ and\nğ· are equal to 7 and 2, respectively.\nAlbeit our octree attention is designed for sparse voxels, it degen-\nerates to the standard window attention [Liu et al. 2021a] and dilated\nattention [Hassani and Shi 2022; Tu et al. 2022] in vision transform-\ners when applied to full voxels or images with specific window size\nand dilation settings. An example is shown in Figure 3-(f)&(g), where\nthe point number is 16, and the dilation is 4.\nPositional Encoding. Positional encoding is essential for attentions\nto differentiate features defined at different positions. A widely-used\nstrategy is to add relative positional bias [Liu et al. 2021a; Raffel et al.\n2020] to the attention defined in Equation 1. Denote the maximum\nwindow size asğ‘Š; the relative positions of two arbitrary points within\nthe same window lie in [âˆ’ğ‘Š +1,ğ‘Š âˆ’1]. Naively extending relative\npositional bias to 3D requires ğ» Ã—(2ğ‘Š âˆ’1)3 trainable parameters,\nwhere ğ» is the head number of attention. With our octree attention,\nthe maximum window size is at least 50 when the dilation is larger\nthan 4, which greatly increases the parameters of our OctFormer.\nTherefore, we adopt the conditional positional encoding (CPE) [Chu\net al. 2021b; Dong et al. 2022; Wang et al. 2022b], which dynamically\ngenerates the positional encoding conditioned on current features\nwith a depthwise convolution. Specifically, we apply the octree-based\ndepthwise convolution provided by O-CNN [Wang et al. 2017] and\nthe Batch Normalization [Loffe and Szegedy 2015] to the input tensor\nğ‘‹ as positional encoding before each attention module:\nğ‘‹ = ğ‘‹ +ğ‘ğ‘ğ‘¡ğ‘â„_ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘‘ğ‘’ğ‘ğ‘¡â„_ğ‘¤ğ‘–ğ‘ ğ‘’_ğ‘ğ‘œğ‘›ğ‘£(ğ‘‹)) (3)\nWith CPE, the network performance improves significantly while\nusing fewer parameters than relative positional bias, as verified in\nour ablation study in Section 4.2.\nAlgorithm 1Pseudocode of Octree Attention in a PyTorch-like style.\n# x: input tensor with a shape of (N, C)\n# D: dilation for attention, set to 1 or 4 by default\n# P: point number in each window, set to 32 by default\n# attntion: an object of torch.nn.MultiheadAttention\n# apply conditional positional encoding\nx = x + batch_norm(depth_wise_conv(x))\n# window partition\nN, C = x.shape\nNz = (P * D) - N % (P * D) # number of zeros for padding\nx = torch.cat([x, x.new_zeros(Nz, C)]) # pad zeros\nx = x.reshape(-1, P, D, C).transpose(1, 2).flatten(0, 1)\n# attention mask\nm = torch.cat([x.new_zeros(N), x.new_ones(Nz)]).bool()\nm = m.reshape(-1, P, D).transpose(1, 2).flatten(0, 1)\n# apply attention\nx = attntion(query=x, key=x, value=x, key_padding_mask=m)\n# reverse window partition\nx = x.reshape(-1, D, P, C).transpose(1, 2).reshape(-1, C)\noutput = x[:N] # remove the padded elements\nSummary. Our octree attention is extremely easy to implement\nand much more efficient than previous point cloud attentions. With\nopen-sourced libraries on the web, we can implement our octree\nattention in 10 lines of code when the batch size is 1, as summa-\nrized in Algorithm 1. The core of our octree attention can be imple-\nmented with the multi-head attention module provided by PyTorch\nâ€“ torch.nn.MultiheadAttention [Paszke et al. 2019], which is highly\noptimized based on general matrix multiplication routines on GPU.\nAnd tensor reshaping and transposing operators for window parti-\ntion are also standard operators supported by PyTorch. The number\nof zeros for padding in our octree attention is (Ëœğ‘ âˆ’ğ‘), which is\nless than ğ¾ Ã—ğ· (128 in our settings); thus the computation cost of\npadding is negligible. In contrast, the implementation of other point\ncloud attentions [Fan et al . 2022; Lai et al. 2022; Sun et al. 2022]\nrequires complex engineering and customized GPU programming.\nWe demonstrate the efficiency of our octree attention in Section 4.2.\n3.2 Network Details\nIn this section, we introduce the remaining details of OctFormer,\nincluding the embedding module, the OctFormer block, and the\ndownsampling module, as shown in Figure 2. Denote the initial spa-\ntial resolution of octree as ğ‘†, the embedding module maps the input\nsignal into a high-dimensional feature space and downsamples the\nspatial resolution by a factor of 4. Following the embedding module\nare four network stages, and each network stage is composed of ğ‘ğ‘–\nOctFormer blocks and a downsampling module, where ğ‘– denotes the\nstage index. OctFormer blocks are used to process features; each\ndownsampling module reduces the spatial resolution of features by a\nfactor of 2 and increases the feature channel by a factor of 2 in each\nstage, except for the last stage, where the channel is kept as 4ğ¶ to\nreduce the total parameter number of the network.\nEmbedding. Instead of using a single convolution layer with a\nlarge kernel size and stride to instantiate the embedding module, as is\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\n1:6 â€¢ Peng-Shuai Wang\ndone in ViT [Dosovitskiy et al. 2021], we opt for several convolution\nlayers with small kernel sizes, which has been shown to be able to\nstabilize the training process of transformers [Xiao et al. 2021]. In\nOctFormer, we use a series of 5 octree-based convolution modules\nfor the embedding module. Each module consists of an octree con-\nvolution [Wang et al. 2017], a Batch Normalization layer [Loffe and\nSzegedy 2015], and a ReLU activation function. The kernel sizes and\nstrides of these octree convolutions are{3,2,3,2,3}and {1,2,1,2,1},\nrespectively. The convolutions with a stride of 2 downsample the\nspatial resolution of tensors by a factor of 2.\nOctFormer Block. Following the standard transformer block de-\nsign [Vaswani et al. 2017], an OctFormer block consists of an octree\nattention introduced in Section 3.1, a multilayer perceptron (MLP),\nand residual connections, as shown in Figure 2-(b). The MLP has two\nfully connected layers with a GELU activation function in between,\nand the expansion ratio of the hidden channel of MLP is set to 4.\nLayer Normalization (LayerNorm) [Ba et al. 2016] is employed prior\nto each attention module and each MLP to stabilize the training. The\ndilations of octree attention in two consecutive OctFormer blocks are\nset to 1 and 4, respectively.\nDownsampling. The downsampling module is implemented as an\noctree convolution with a kernel size of 2 and a stride of 2, followed\nby a Batch Normalization layer, which reduces the spatial resolution\nand increases the channel of a feature map by a factor of 2.\nNetwork Settings. By default, ğ¶ is set to 96, the block numbers\nare set to {2,2,18,2}, the head numbers of octree attentions are set\nto 1/16 of the channel numbers, and the point numbers for window\npartition are set to 32. The resulting OctFormer has a similar amount\nof trainable parameters (39M) as MinkowskiNet (38M) [Choy et al.\n2019]. The output of OctFormer is hierarchical features with four\nspatial resolutions, i.e., {ğ‘†/4,ğ‘†/8,ğ‘†/16,ğ‘†/32}, which can be conve-\nniently integrated with a feature pyramid network (FPN) [Lin et al.\n2017] for segmentation and detection. And the last feature map can\nbe averaged as the global feature for shape classification.\n4 EXPERIMENTS\nIn this section, we validate the effectiveness and efficiency of our\nOctFormer on 3D semantic segmentation and 3D object detection\ntasks. We also discuss our design choices in the ablation study. The\nexperiments were conducted using 4 Nvidia 3090 GPUs with 24GB\nof memory.\n4.1 3D Semantic Segmentation\nWe first verify the efficacy of our OctFormer on 3D semantic seg-\nmentation. The goal of this task is to predict the semantic label for\neach point in an input point cloud.\nDataset. The experiments are conducted on the ScanNet [Dai et al.\n2017] dataset and the recently-proposed ScanNet200 [Rozenberszki\net al. 2022] dataset. ScanNet contains 1513 large-scale 3D scans\nof indoor scenes and includes 20 semantic categories. The average\npoint number of scans in ScanNet is 148ğ‘˜. ScanNet200 shares the\nsame data as ScanNet, but has 200 semantic categories, making it\nmore challenging. We follow the standard data splits [Dai et al. 2017]\nTable 1. Semantic Segmentation on ScanNet. Val. and Test denote the\nmIoU on the validation and testing set, respectively. The best results\nare marked in bold. Our OctFormer achievs the best performance\non the validation set, surpassing all point cloud transformers, sparse-\nvoxel-based CNNs, and point-based networks. The mIoU of OctFormer\nwithout voting is shown in the parentheses for reference.\nMethod Val. Test\n3DMV [Dai and NieÃŸner 2018] - 48.4\nPanopticFusion [Narita et al. 2019] - 52.9\nPointNet++ [Qi et al. 2017b] 53.5 55.7\nSegGCN [Lei et al. 2020] - 58.9\nJointPoint [Chiang et al. 2019] 69.2 63.4\nRandLA-Net [Hu et al. 2020a] - 64.5\nPointConv [Wu et al. 2019] 61.0 66.6\nPointASNL [Yan et al. 2020] 63.5 66.6\nKPConv [Thomas et al. 2019] 69.2 68.6\nFusionNet [Zhang et al. 2020a] - 68.8\nJSENet [Hu et al. 2020b] - 69.9\nSparseConvNet [Graham et al. 2018] 69.3 72.5\nMinkowskiNet [Choy et al. 2019] 72.2 73.6\nLargeKernel [Chen et al. 2022] 73.2 73.9\nO-CNN [Wang et al. 2017] 74.5 76.2\nMix3D [Nekrasov et al. 2021] 73.6 78.2\nPoint Transformer [Zhao et al. 2021] 70.6 -\nFast Point Transformer [Zhao et al. 2021] 72.1 -\nStratified Transformer [Lai et al. 2022] 74.3 73.7\nPoint Transformer V2 [Wu et al. 2022] 75.4 75.2\nOctFormer (ours) 75.7 (74.5) 76.6\nfor training and evaluation, using 1201 scans for training, 312 scans\nfor validation, and 100 scans for testing. The testing labels are not\npublicly available, and testing results are obtained by submitting pre-\ndictions to the official ScanNet website. We use the mean intersection\nover union (mIoU) over all categories as the evaluation metric.\nSettings. OctFormer is used as the backbone to extract hierarchical\nfeatures, and a lightweight FPN [Lin et al . 2017] is used as the\nsegmentation head. The FPN first projects multiscale features with\na convolution layer with a kernel size of 1 to make the channels of\nfeatures equal to 168, then upsamples the features with the nearest\ninterpolation by a factor of 2 and merges consecutive features by\naddition. The final output feature is processed by a convolution with a\nkernel size of 3 and upsampled to each point with the nearest neighbor\ninterpolation, with which an MLP with one hidden layer is used\nto predict the point categories. For both ScanNet and ScanNet200,\nthe training settings are the same, except that the output channel is\n20 for ScanNet and 200 for ScanNet200. We employ an AdamW\noptimizer [Loshchilov and Hutter 2019] to train the network for 600\nepochs with a batch size of 16 and a weight decay of 0.05. The initial\nlearning rate is set as 0.006 and decreases by a factor of 10 after\n360 and 480 epochs, respectively. The input point clouds are first\nnormalized with a scale factor of 0.01m and then encoded by octrees\nwith a depth of 11. The initial features include colors, normals, and\npoint coordinates. The data augmentations include random rotation\nin [âˆ’180â—¦,180â—¦], random scaling in [0.75,1.25], random translation\nin [âˆ’0.1,0.1], random elastic deformations following [Choy et al .\n2019], and random spherical cropping following [Lai et al. 2022].\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\nOctFormer: Octree-based Transformers for 3D Point Clouds â€¢ 1:7\nTable 2. Semantic Segmentation on ScanNet200. Head, Common and\nTail denote mIoUs on three smaller groups containing 66, 68 and 66\ncategories of ScanNet200,All denotes mIoU on all 200 categories. The\nbest results are marked in bold. Our OctFormer trained from scratch\nacheives the best performance on all groups and are significantly better\nthan previous state-of-the-art methods, even those with pretraining.\nMethod Head Common Tail All\nCSC-Pretrain [Hou et al. 2021] 45.5 17.1 7.9 24.9\nMinkowskiNet [Choy et al. 2019] 46.3 15.4 10.2 25.3\nSupCon [Khosla et al. 2020] 48.6 19.2 10.3 26.0\nLGround [Rozenberszki et al. 2022] 48.5 18.4 10.6 27.2\nOctFormer (ours) 53.9 26.5 13.1 32.6\nResults on ScanNet. We compare our OctFormer with a series of\nprevious state-of-the-art methods on ScanNet and summarize the val-\nidation and testing mIoUs in Table 1. The training of our OctFormer\ntakes 15 hours and consumes 13GB of memory using 4 Nvidia 3090\nGPUs. Our OctFormer achieves a mIoU of 74.5 on the validation set\nwithout voting. Since Stratified Transformer [Lai et al. 2022] employs\nblock-wise prediction and Point Transformer V2 [Wu et al . 2022]\nemploys the voting strategy to improve the performance, we also\nadopt the voting strategy, which results in a mIoU of 75.7 as shown\nin Table 1. Clearly, our OctFormer achieves the best performance on\nthe validation set among all methods. Specifically, OctFormer out-\nperforms previous point cloud transformers with a validation mIoU\nhigher than Stratified Transformer [Lai et al. 2022] by 1.4 and Point\nTransformer [Zhao et al . 2021] by 5.1. OctFormer also surpasses\nsparse-voxel-based CNNs with a validation mIoU higher than Spar-\nseConvNet [Graham et al. 2018] by 6.4, MinkowskiNet [Choy et al.\n2019] by 3.5, and LargeKernel [Chen et al . 2022] by 2.5. Addi-\ntionally, OctFormer significantly outperforms point-based networks,\nincluding PointNet++ [Qi et al. 2017b] and KPConv [Thomas et al.\n2019]. On the testing set, our OctFormer achieves the second-best\nmIoU, as Mix3D currently ranks the first. However, our OctFormer\nachieves better mIoU than Mix3D on the validation set. One possible\nexplanation is that Mix3D employs an additional mixup data aug-\nmentation and post-processing to further improve the testing results,\nas mentioned in [Nekrasov et al. 2021]. A visual comparison with\nStratified Transformer is shown in Figure 5, which demonstrates that\nOctFormer can produce more faithful results in detailed regions.\nResults on ScanNet200. Since object categories in ScanNet200 are\nhighly imbalanced, the 200 categories of ScanNet200 are further split\ninto three smaller groups of 66, 68 and 66 categories [Rozenberszki\net al. 2022] according to the label frequency in the training set, which\nare denoted as Head, Common and Tail respectively. The mIoUs of\nthese small groups and all categories on the testing set are reported in\nTable 2. Among the listed methods in Table 2, MinkowskiNet and our\nOctformer are trained from scratch with random initialization, while\nCSC-Pretrain [Hou et al . 2021] and SupCon [Khosla et al . 2020]\nuse additional data to pretrain the network with contrastive learn-\ning, and LGround [Rozenberszki et al . 2022] is a newly-proposed\nlanguage-driven pre-training method based on a pretrained large\nvision-language CLIP model [Radford et al . 2021]. As shown in\nTable 2, our Octformer trained from scratch without additional data\n10k 20k 50k 100k 200k\n0.1\n0.2\n0.3\n0.4 OctFormer\nPoint Transformer v2\nStratified Transformer\nFig. 4. Efficiency comparisons. The horizontal axis represents the\nspatial number of an input tensor, and the vertical axis represents\nthe running time in seconds. Our OctFormer runs significantly faster\nand is over 17 times faster than Point Transformer v2 and Stratified\nTransformer when the spatial number is200ğ‘˜.\nGround Truth Stratified Transformer OctFormer\nFig. 5. Visual comparison on ScanNet. The results of OctFormer are\nmore faithful to the ground truth, as highlighted in the rectangle regions.\nis significantly better than these competitors and also consistently\nbetter in the three small groups, which verifies the effectiveness of\nour Octformer. Specifically, the mIoU of our OctFormer is higher\nthan MinkowskiNet by 7.3 and higher than LGround by 5.4, although\nLGround is pretrained with the help of a powerful CLIP model.\n4.2 Ablation Studies and Discussions\nIn this section, we justify key design choices of Octformer and com-\npare the efficiency of OctFormer with other point cloud transformers\non top of the semantic segmentation on ScanNet in Section 4.1.\nEfficiency. Global point cloud transformers [Guo et al. 2021; Pang\net al. 2022; Yu et al. 2022] can only process point clouds containing\nseveral thousand points, thus we omit the comparisons with them\nand focus on the comparisons with efficient point cloud transformers\nproposed recently, including Stratified Transformer [Lai et al. 2022]\nand Point Transformer v2 [Wu et al. 2022]. Stratified Transformer ex-\ntends window attentions [Liu et al. 2021a] to point clouds with cubic\nwindows [Fan et al. 2022; Sun et al. 2022] and leverages stratified\nsampling to improve its performance. Point Transformer v2 applies\nattentions to k nearest neighbors of each point in a sliding-window\nfashion. Since the network configurations vary greatly, we record\nthe running time of one single transformer block on an Nvidia 3090\nGPU to eliminate the influence of uncontrolled factors, We choose\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\n1:8 â€¢ Peng-Shuai Wang\nTable 3. Ablation studies on semantic segmentation on ScanNet. mIoU and Loss denote the validation mIoU and the training loss. The training loss\nis used as a reference of the expresiveness of networks, which is multiplied by 100 for better display. Best results are marked in bold.\n(a) Model size. A large model can steadily\nimprove the performance.\nName Model Size Loss mIoU\nSmall 18M 10.2 74.0\nBase 39M 8.12 74.5\nLarge 156M 7.22 74.6\n(b) Positional encoding. CPE is effective\nand essential for OctFormer.\nType Loss mIoU\nw/o 21.7 66.5\ncRPE 8.43 73.9\nCPE 8.12 74.5\n(c) Input voxel size. OctFormer is efficient\nenough to take fine voxels for better results.\nV oxel Size Loss mIoU\n4cm 12.6 70.7\n2cm 9.08 73.6\n1cm 8.12 74.5\n(d) Point number in each window. Large\npoint number increases network capacity.\nNum. Win. Size Loss mIoU\n16 7 8.24 73.8\n32 10 8.12 74.5\n48 12 8.10 74.3\n64 15 7.92 74.4\n(e) Dilation. Large dilation also increases\nnetwork capacity.\nDilation Loss mIoU\n1 8.29 74.2\n2 8.17 74.4\n4 8.12 74.5\n8 8.01 74.1\n(f) Data augmentations. Strong data aug-\nmentations are helpful for a good mIoU.\nAugmentation Loss mIoU\nw/o 0.58 53.8\n+Affine 5.59 72.7\n+Crop 6.27 73.6\n+Elastic 8.12 74.5\nthe input tensorâ€™s spatial number from {10ğ‘˜,20ğ‘˜,50ğ‘˜,100ğ‘˜,200ğ‘˜}\nand set the channel as 96. For the attention modules, we set the head\nnumber to 6, and set the point number and neighborhood number to\n32 for OctFormer and Point Transformer v2. Since the point num-\nber is variant in each window for Stratified Transformer, we set the\nwindow size to 6 so that the average point number is about 32.\nThe results are shown in Figure 4. It can be seen that although\nthe computation complexities of the three methods are all linear, our\nOctFormer runs significantly faster than Point Transformer v2 and\nStratified Transformer. OctFormer runs over 17 times faster than the\nother two methods when the spatial number of the input tensor is\n200ğ‘˜. The key reason for the efficiency of our Octformer is that our\nnovel octree attention mainly leverages standard operators supported\nby deep learning frameworks, which is further based on general ma-\ntrix multiplication routines on GPUs and has been optimized towards\nthe computation limit of GPUs. Conversely, the point number in each\nwindow of Stratified Transformer is highly unbalanced, making it\nchallenging for efficiency tuning even with hand-crafted GPU pro-\ngramming. Although the neighborhood number of Point Transformer\nv2 is fixed, the sliding window execution pattern wastes considerable\ncomputation that could have been shared.\nWe also compare the efficiency of the whole network as shown in\nFigure 1. We record the time of one forward pass of each network\non a Nivida 3090 GPU, taking a batch of 250ğ‘˜ points. The speed of\nour Octformer-Small is slightly faster than MinkowskiNet, and faster\nthan Point Transformer V2 by 3 times and Stratified Transformer\nby 20 times. It is worth mentioning that our OctFormer takes point\nclouds quantized by a voxel size of 1cm as input, whereas the other\nnetworks takes point clouds quantized by a voxel size of 2cm.\nModel Size. We denote the default OctFormer as OctFormer-Base\nand design two variants with different amounts of trainable param-\neters, including OctFormer-Small with half of the parameters of\nOctFormer-Base and OctFormer-Large with 4 times of parameters of\nOctFormer-Base. The detailed settings are listed as follows:\n- OctFormer-Small: ğ¶ = 96, block numbers = {2,2,6,2};\n- OctFormer-Large: ğ¶ = 192, block numbers = {2,2,18,2}.\nWe test the performances of these models and summarize the results\nin Table 3a. It can be seen that as the model size increases, the\nmIoU on the validation set and the training loss improve steadily.\nThe training loss can be used as a metric that directly reflects the\nnetworkâ€™s expressiveness. These results indicate that our OctFormer\nscales up well with increasing model sizes. Note that our OctFormer-\nSmall with only 18M parameters achieves a mIoU of 74.1 ( 75.2\nwith voting) on the validation set of ScanNet and already surpasses\nprevious sparse-voxel-based CNNs, like Mix3D [Nekrasov et al .\n2021] and MinkowskiNet [Choy et al. 2019] with 38M parameters,\nas shown in Table 1.\nPositional Encoding. Here we investigate the effect of conditional\npositional encoding (CPE) [Chu et al. 2021b] and report the results\nin Table 3b. After removing CPE, denoted as w/o in the second row\nof Table 3b, the mIoU decreases by 8.0, and the training loss also\nincreases greatly, which verifies that the positional encoding is crucial\nfor OctFormer to perceive positional information. Additionally, we\nalso test the contextual relative positional encoding (cRPE) proposed\nby Stratified Transformer [Lai et al. 2022] with OctFormer and get a\nmIoU of 73.9, which is much better than the model without positional\nencoding but still slightly worse than CPE. Although it is possible to\nfurther improve the mIoU by tuning the parameters of cRPE, like the\nquantization size of cRPE, cRPE is tightly coupled with the attention\nmodule, and its implementation is more complex than CPE.\nInput Voxel Size. Octrees are used to rasterize point clouds to\nsparse voxels, and it is known that finer voxels retain more infor-\nmation, which can help networks achieve better results. We train\nOctFormer on ScanNet using three different voxel sizes and report\nthe results in Table 3c. OctFormer achieve the best results with a\nvoxel size of 1cm, improving the mIoU by 0.9 compared with a voxel\nsize of 2cm. MinkowskiNet [Choy et al. 2019] and Stratified Trans-\nformer [Lai et al. 2022] are typically trained on ScanNet with a voxel\nsize of 2cm by default. However, they run out of memory on Nvidia\n3090 GPUs with 24GB memory when using a voxel size of 1cm,\nrequiring further parameter tuning to produce reasonable results. In\ncontrast, our OctFormer, even OctFormer-Large, is efficient enough\nto be trained with a voxel size of 1cm within 24GB GPU memory.\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\nOctFormer: Octree-based Transformers for 3D Point Clouds â€¢ 1:9\nPoint Number in Each Window. The point number ğ¾ in each win-\ndow is closely related to the window size and the receptive field of\noctree attentions. We train OctFormer with a point number chosen\nfrom {16,32,48,64}and report the results in Table 3d. With a fixed\npoint number, the window sizes are variable, and we calculated the\naverage window sizes in Table 3d. The window size in Stratified\nTransformer [Lai et al. 2022] is 5, and the average window size of\nOctFormer with a default point number of 32 is 10, which greatly in-\ncreases the receptive field of the network. It is evident from Table 3d\nthat the training loss progressively decreases as the point number\nincreases. Although the mIoU of OctFormer with a point number of\n64 is slightly worse than with a point number of 32, the training loss\nwith a point number of 64 is lower, which is a sign of overfitting.\nTherefore, we can conclude that the expressiveness or capacity of the\nnetwork increases with the point number of octree attention.\nDilation. Dilated octree attentions further increase the receptive\nfield by using dilated local windows, which are controlled by the\nhyperparameter dilation ğ·. We train OctFormer with a dilation cho-\nsen from {1,2,4,8}and report the results in Table 3e. Similarly, we\nobserve that the mIoU improves until the dilation reaches 4. And the\ntraining loss decreases as the dilation increases. When the dilation\nis 8, the training loss is the best, indicating that the capacity of the\nnetwork increases with the dilation of octree attention.\nData Augmentation. We inspect the influence of different data\naugmentations on the mIoU by progressively adding each of them\nand summarize the results in Table 3f. Without data augmentations,\nwe observe severe overfitting, leading to much lower mIoU. As\nwe add more data augmentations, the mIoU gradually increases.\nAlthough Mix3D [Nekrasov et al. 2021] leverages an additional 3D\nmixup augmentation to achieve the best performance on the testing\nset of ScanNet, we chose to use only the data augmentations in Point\nTransformer V2 [Wu et al . 2022] and Stratified Transformer [Lai\net al. 2022] for a fair comparison with point cloud transformers.\n4.3 3D Object Detection\nIn this section, we validate the effectiveness of our OctFormer on 3D\nobject detection. The goal is to predict 3D bounding boxes and the\ncorresponding categories of objects contained in an input point cloud.\nDataset. We perform 3D object detection on the SUN RGB-D\ndataset [Song et al . 2015], which contains about 10k single-view\nRGB-D scans of indoor scenes. The annotations of RGB-D scans\nconsist of oriented 3D bounding boxes of 37 categories. Following\nprevious settings [Qi et al. 2019; Rukhovich et al. 2022], we use 5285\nand 5050 RGB-D scans for training and validation, respectively, and\nreport the average precision (mAP) under IoU thresholds of 0.25 and\n0.5, denoted as mAP@0.25 and mAP@0.5, for the ten most common\ncategories for performance comparisons.\nSettings. We adapt FCAF3D [Rukhovich et al . 2022] with our\nOctFormer for object detection. FCAF3D currently achieves top per-\nformance on the SUN RGB-D detection benchmark. The backbone\nof FCAF3D is a ResNet with 34 sparse convolution layers built upon\nMinkowskiNet [Choy et al. 2019]. The detection head of FCAF3D\nTable 4. 3D Object detection on SUN RGB-D. The mAP@0.25 and\nmAP@0.5 denote the mean average precision under IoU threshold\nof 0.25 and 0.5, respectively. We run the experiments 5 times and re-\nport the maximum and average metric values; the average values are\nshown in brackets. By replacing the backbone of FACF3D with our Oct-\nFormer, we achieve the best performance on mAP@0.5, surpassing\nall pervious state-of-the-art methods.\nMethod mAP@0.25 mAP@0.5\nV oteNet [Qi et al. 2019] 57.7 -\nMLCVNet [Xie et al. 2020] 59.8 -\n3DETR [Misra et al. 2021] 59.1 32.7\nH3DNet [Zhang et al. 2020b] 60.1 39.0\nBRNet [Cheng et al. 2021a] 61.1 43.7\nHGNet [Chen et al. 2020] 61.6 -\nVENet [Xie et al. 2021] 62.5 39.2\nGroupFree [Liu et al. 2021b] 63.0 (62.6) 45.2 (44.4)\nCAGroup3D [Wang et al. 2022a] 66.8 (66.4) 50.2 (49.5)\nFCAF3D [Rukhovich et al. 2022] 64.2 (63.8) 48.9 (48.2)\nOctFormer (ours) 66.2 (65.7) 50.6 (50.2)\nis an FPN [Lin et al . 2017], which is widely used in object detec-\ntion. We replace the original backbone network of FCAF3D with our\nOctFormer while keeping other components fixed. To be compati-\nble with the configurations of FCAF3D, we further add two octree\nconvolution modules with kernel sizes {3,2}and stride {1,2}before\nthe input of OctFormer so that the resolutions of the resulting fea-\ntures are similar to the original backbone of FCAF3D. We perform\nexperiments with the MMDetection3D framework.We employ an\nAdamW optimizer [Loshchilov and Hutter 2019] to train the network\nfor 18 epochs with a batch size of 32 and a weight decay of 0.01. The\ninitial learning rate is set to 0.001 and decreases by a factor of 10\nafter 12 and 16 epochs, respectively. The input point clouds are first\nnormalized with a voxel size of 0.01m and then encoded by octrees\nwith a depth of 12, while the initial input features are point colors\nwith 3 channels. The data augmentations include random rotation in\n[âˆ’30â—¦,30â—¦]along the upright axis, random scaling in [0.85,1.15],\nand random translation in [âˆ’0.1,0.1], which are similar to FCAF3D.\nResults. We do comparisons with previous state-of-the-art meth-\nods on SUN RGB-D and summarize the results in Table 4. We run our\nOctFormer 5 times and report the average and maximum performance\nto reduce the effect of random fluctuations. Each training process\ntakes approximately 3.5 hours on four Nvidia 3090 GPUs, and the\nmaximum GPU memory consumption is less than 8GB. It can be\nseen from Table 4 that our method achieves the best results in terms\nof mAP@0.5. Compared with FCAF3D, the only difference is replac-\ning the original backbone of FCAF3D with our OctFormer, and the\nmetrics mAP@0.25 and mAP@0.5 directly increase by 1.9 and 2.0,\nrespectively, which clearly demonstrates the effectiveness of our Oct-\nFormer over sparse-voxel-based CNNs. Among the listed methods in\nTable 4, 3DETR [Misra et al. 2021] and GroupFree [Liu et al. 2021b]\nare based on point cloud transformers. Our OctFormer achieves sig-\nnificantly better performance than them. Specifically, the mAP@0.25\nof our OctFormer is higher than 3DETR and GroupFree by 6.6 and\n3.1, and mAP@0.5 is higher by 17.5 and 5.8, respectively. Lastly, it\nshould be noted that the contributions of CAGroup3D [Wang et al.\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\n1:10 â€¢ Peng-Shuai Wang\n2022a] are orthogonal to our OctFormer. CAGroup3D mainly im-\nproves FCAF3D by designing the detection head, whereas our goal\nis to demonstrate the advantages of OctFormer over other backbones\nby replacing the backbone of FACF3D. Nevertheless, the average\nmAP@0.5 of our method is still higher than CAGroup3D by 0.7.\n5 CONCLUSION\nWe propose OctFormer, a general and effective backbone for 3D point\ncloud understanding. The core of OctFormer contains a novel octree\nattention and its dilated variant. Our octree attention is extremely\neasy to implement and runs significantly faster than other point cloud\nattentions. OctFormer demonstrates great efficiency and achieves\nstate-of-the-art performance on several benchmarks, including se-\nmantic segmentation on ScanNet and ScanNet200 and 3D object\ndetection on SUN RGB-D.\nThe field of 3D deep learning is rapidly evolving, and it is in-\nevitable that the performance of OctFormer on the leaderboards will\nbe surpassed by future works sooner or later. However, our novel\noctree attentions, with their highly simplified implementation and\nsuper efficiency, along with our unified network design, will make\n3D transformers more accessible to a broader audience and open up\nmany exciting possibilities for the future, including pretraining of\nlarge-scale general 3D models, cross-modality training with images\nor languages, and more.\nThe limitations and future works are discussed as follows:\nSmall-Scale Dataset. One limitation of OctFormer is that it is\nprone to overfit on small-scale datasets. We test our OctFormer on\nthe ModelNet40 classification, which only contains about 9k shapes\nfor training and 2k for testing. We use the average features produced\nby the last stage of OctFormer as the shape features for classifica-\ntion and get an accuracy of 92.7% on the testing set without voting.\nAlthough OctFormer is superior to the point transformer in Point-\nBERT [Yu et al. 2022] with an accuracy of 91.2%, it is still worse\nthan PointMLP [Ma et al. 2022] with an accuracy of 94.1%. Point-\nBERT [Yu et al. 2022] and Point-MAE [Pang et al. 2022] improve the\naccuracy of their point transformer to 93.8% with unsupervised pre-\ntraining. We believe similar unsupervised pretraining techniques can\nalso help OctFormer to combat the overfitting issue to achieve better\nperformance on small-scale datasets, which is left as future work.\nPositional Encoding. As verified in our ablation studies, posi-\ntional encoding is essential for transformers, and we currently adopt\nCPE [Chu et al . 2021b] as the positional encoding of OctFormer.\nAlthough CPE is effective, it hurts the flexibility of OctFormer due\nto the dependency on octree-based depth-wise convolutions. In the\nfuture, we will explore the possibility of other positional encodings,\ne.g., MLP-based positional encoding that encodes the relative posi-\ntional information with MLPs.\nCross Attention. The proposed octree attention is essentially a\nself-attention, which is mainly used by OctFormer as a building\nblock of an encoder network. Additionally, the cross attention is also\nindispensable for learning complex relationships between queries and\nkeys and has been successfully used in the decoder of 3DETR [Misra\net al. 2021] for 3D object detection. We regard the extension of octree\nattention to the cross-attention as another future work.\n3D Generation. We focus on using OctFormer for point cloud\nunderstanding tasks in this paper. It is interesting to apply our Oct-\nFormer, or other network architectures designed with our octree\nattention, for 3D content creation with cross-modality training, e.g.,\n3D shape generation conditioned on images, sketches, or texts.\nACKNOWLEDGMENTS\nThis work was supported in part by National Key R&D Program of\nChina 2022ZD0160801. We also thank the anonymous reviewers for\ntheir valuable feedback.\nREFERENCES\nMatan Atzmon, Haggai Maron, and Yaron Lipman. 2018. Point Convolutional Neural\nNetworks by Extension Operators. ACM Trans. Graph. (SIGGRAPH) 37, 4 (2018).\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization.\narXiv preprint arXiv:1607.06450 (2016).\nJintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. 2020.\nA hierarchical graph network for 3D object detection on point clouds. In CVPR.\nYukang Chen, Jianhui Liu, Xiaojuan Qi, X. Zhang, Jian Sun, and Jiaya Jia. 2022. Scaling\nup Kernels in 3D CNNs. In NeurIPS.\nBowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and Dong Xu. 2021a. Back-\nTracing Representative Points for V oting-based 3D Object Detection in Point Clouds.\nIn CVPR.\nZhang Cheng, Haocheng Wan, Xinyi Shen, and Zizhao Wu. 2021b. PatchFormer: A\nversatile 3D transformer based on patch attention. In CVPR.\nHung-Yueh Chiang, Yen-Liang Lin, Yueh-Cheng Liu, and Winston H Hsu. 2019. A\nunified point-based framework for 3D segmentation. In 3DV.\nChristopher Choy, JunYoung Gwak, and Silvio Savarese. 2019. 4D spatio-temporal\nconvnets: Minkowski convolutional neural networks. InCVPR.\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia\nXia, and Chunhua Shen. 2021a. Twins: Revisiting the Design of Spatial Attention in\nVision Transformers. In NeurIPS.\nXiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. 2021b. Conditional positional encodings for vision transformers.\narXiv preprint arXiv:2102.10882 (2021).\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. 2020. On the relationship\nbetween self-attention and convolutional layers. In ICLR.\nAngela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and\nMatthias NieÃŸner. 2017. ScanNet: Richly-annotated 3D Reconstructions of Indoor\nScenes. In CVPR.\nAngela Dai and Matthias NieÃŸner. 2018. 3DMV: Joint 3D-multi-view prediction for 3D\nsemantic scene segmentation. In ECCV.\nJia Deng, Wei Dong, Richard Socher, Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A\nlarge-scale hierarchical image database. In CVPR.\nXiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan,\nDong Chen, and Baining Guo. 2022. CSWin Transformer: A general vision trans-\nformer backbone with cross-shaped windows. In CVPR.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. 2021. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR.\nLue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan\nWang, and Zhaoxiang Zhang. 2022. Embracing single stride 3D object detector with\nsparse transformer. In CVPR.\nMatthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich MÃ¼ller. 2018. SplineCNN:\nFast Geometric Deep Learning with Continuous B-Spline Kernels. In CVPR.\nBenjamin Graham, Martin Engelcke, and Laurens van der Maaten. 2018. 3D semantic\nsegmentation with submanifold sparse convolutional networks. In CVPR.\nMeng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and\nShi-Min Hu. 2021. PCT: Point cloud transformer. Comput. Vis. Media 7, 2 (2021).\nAli Hassani and Humphrey Shi. 2022. Dilated neighborhood attention transformer.arXiv\npreprint arXiv:2209.15001 (2022).\nJi Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. 2021. Exploring Data-\nEfficient 3D Scene Understanding with Contrastive Scene Contexts. In CVPR.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki\nTrigoni, and Andrew Markham. 2020a. RandLA-Net: Efficient Semantic Segmenta-\ntion of Large-Scale Point Clouds. CVPR.\nZeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and Chiew-lan Tai. 2020b. JSENet:\nJoint semantic segmentation and edge detection network for 3d point clouds. In\nECCV.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\nAaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning.\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023.\nOctFormer: Octree-based Transformers for 3D Point Clouds â€¢ 1:11\nAdvances in Neural Information Processing Systems 33 (2020).\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2019. Panoptic feature\npyramid networks. In CVPR.\nXin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,\nand Jiaya Jia. 2022. Stratified Transformer for 3D Point Cloud Segmentation. In\nCVPR.\nHuan Lei, Naveed Akhtar, and Ajmal Mian. 2019. Octree guided CNN with spherical\nkernels for 3D point clouds. In CVPR.\nHuan Lei, Naveed Akhtar, and Ajmal Mian. 2020. SegGCN: Efficient 3D point cloud\nsegmentation with fuzzy spherical kernel. In CVPR.\nYangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. 2018.\nPointCNN: Convolution on X-transformed points. In NeurIPS.\nTsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge\nBelongie. 2017. Feature pyramid networks for object detection. In CVPR.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. 2021a. Swin transformer: Hierarchical vision transformer using shifted\nwindows. In ICCV.\nZe Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. 2021b. Group-Free 3D Object\nDetection via Transformers. In ICCV.\nSergey Loffe and Christian Szegedy. 2015. Batch Normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In\nICLR.\nXu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. 2022. Rethinking network\ndesign and local geometry in point cloud: A simple residual MLP framework. In\nICLR.\nJiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang\nXu, and Chunjing Xu. 2021. V oxel Transformer for 3D Object Detection. InICCV.\nDaniel Maturana and Sebastian Scherer. 2015. V oxNet: A 3D convolutional neural\nnetwork for real-time object recognition. In IROS.\nIshan Misra, Rohit Girdhar, and Armand Joulin. 2021. An End-to-End Transformer\nModel for 3D Object Detection. In ICCV.\nGaku Narita, Takashi Seno, Tomoya Ishikawa, and Yohsuke Kaji. 2019. PanopticFusion:\nOnline volumetric semantic mapping at the level of stuff and things. In IROS.\nAlexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. 2021.\nMix3D: Out-of-Context Data Augmentation for 3D Scenes. In 3DV.\nYatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan.\n2022. Masked autoencoders for point cloud self-supervised learning. In ECCV.\nChunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. 2022. Fast Point\nTransformer. In CVPR.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\ntala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library.\nIn NeurIPS.\nCharles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas. 2019. Deep Hough\nV oting for 3D Object Detection in Point Clouds. InCVPR.\nCharles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017a. PointNet: Deep\nlearning on point sets for 3D classification and segmentation. In CVPR.\nCharles R. Qi, Hao Su, Matthias NieÃŸner, Angela Dai, Mengyuan Yan, and Leonidas J.\nGuibas. 2016. V olumetric and multi-view CNNs for object classification on 3D data.\nIn CVPR.\nCharles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017b. PointNet++: Deep\nhierarchical feature learning on point sets in a metric space. In NeurIPS.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al . 2021.\nLearning transferable visual models from natural language supervision. In ICML.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer\nlearning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with CLIP latents. arXiv preprint\narXiv:2204.06125 (2022).\nGernot Riegler, Ali Osman Ulusoy, Horst Bischof, and Andreas Geiger. 2017b. OctNet-\nFusion: Learning depth fusion from data. In 3DV.\nGernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. 2017a. OctNet: Learning deep\n3D representations at high resolutions. In CVPR.\nDavid Rozenberszki, Or Litany, and Angela Dai. 2022. Language-Grounded Indoor 3D\nSemantic Segmentation in the Wild. In ECCV.\nDanila Rukhovich, Anna V orontsova, and Anton Konushin. 2022. FCAF3D: fully\nconvolutional anchor-free 3D object detection. In ECCV.\nTianjia Shao, Yin Yang, Yanlin Weng, Qiming Hou, and Kun Zhou. 2018. H-CNN:\nspatial hashing based CNN for 3D shape analysis. IEEE. T. Vis. Comput. Gr.(2018).\nMartin Simonovsky and Nikos Komodakis. 2017. Dynamic edge-conditioned filters in\nconvolutional neural networks on graphs. In CVPR.\nShuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. 2015. SUN RGB-D: A RGB-D\nscene understanding benchmark suite. In CVPR.\nHang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. 2015. Multi-\nview convolutional neural networks for 3D shape recognition. In ICCV.\nPei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and Dragomir\nAnguelov. 2022. SWFormer: Sparse Window Transformer for 3D Object Detection\nin Point Clouds. In ECCV.\nHugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, FranÃ§ois\nGoulette, and Leonidas J. Guibas. 2019. KPConv: Flexible and deformable convolu-\ntion for point clouds. In ICCV.\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik,\nand Yinxiao Li. 2022. MaxViT: Multi-Axis Vision Transformer. InECCV.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In\nNeurIPS.\nHaiyang Wang, Lihe Ding, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhen-\nguo Li, and Liwei Wang. 2022a. CAGroup3D: Class-aware grouping for 3D object\ndetection on point clouds. In NeurIPS.\nPeng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. 2017. O-CNN:\nOctree-based convolutional neural networks for 3D shape analysis. ACM Trans.\nGraph. (SIGGRAPH) 36, 4 (2017).\nPeng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong. 2018. Adaptive O-CNN:\nA patch-based deep representation of 3D shapes. ACM Trans. Graph. (SIGGRAPH\nASIA) 37, 6 (2018).\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,\nPing Luo, and Ling Shao. 2021. Pyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions. In ICCV.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,\nPing Luo, and Ling Shao. 2022b. PVT v2: Improved baselines with pyramid vision\ntransformer. Comput. Vis. Media 8, 3 (2022).\nRoss Wightman. 2019. PyTorch Image Models. https://github.com/rwightman/pytorch-\nimage-models.\nJane Wilhelms and Allen Van Gelder. 1992. Octrees for faster isosurface generation.\nACM Trans. Graph. 11, 3 (1992).\nWenxuan Wu, Zhongang Qi, and Li Fuxin. 2019. PointConv: Deep Convolutional\nNetworks on 3D Point Clouds. In CVPR.\nXiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. 2022. Point\nTransformer V2: Grouped Vector Attention and Partition-based Pooling. InNeurIPS.\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang,\nand Jianxiong Xiao. 2015. 3D ShapeNets: A deep representation for volumetric\nshape modeling. In CVPR.\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr DollÃ¡r, and Ross Girshick.\n2021. Early convolutions help transformers see better. In NeurIPS.\nQian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Dening Lu, Mingqiang Wei, and Jun\nWang. 2021. VENet: V oting Enhancement Network for 3D Object Detection. In\nICCV.\nQian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun Wang.\n2020. MLCVNet: Multi-level context votenet for 3D object detection. In CVPR.\nYifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. 2018. SpiderCNN: Deep\nLearning on Point Sets with Parameterized Convolutional Filters. In ECCV.\nXu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. 2020. PointASNL:\nRobust point clouds processing using nonlocal neural networks with adaptive sam-\npling. In CVPR.\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and\nJianfeng Gao. 2021. Focal Self-attention for Local-Global Interactions in Vision\nTransformers. In NeurIPS.\nXumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. 2022.\nPoint-BERT: Pre-training 3D point cloud transformers with masked point modeling.\nIn CVPR.\nFeihu Zhang, Jin Fang, Benjamin Wah, and Philip Torr. 2020a. Deep FusionNet for\npoint cloud semantic segmentation. In ECCV.\nZaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. 2020b. H3DNet: 3D object\ndetection using hybrid geometric primitives. In ECCV.\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. 2021. Point\ntransformer. In ICCV.\nKun Zhou, Minmin Gong, Xin Huang, and Baining Guo. 2011. Data-parallel octrees for\nsurface reconstruction. IEEE. T. Vis. Comput. Gr.17, 5 (2011).\nReceived January 2023; accepted March 2023; final version May 2023\nACM Trans. Graph., V ol. 42, No. 4, Article 1. Publication date: August 2023."
}