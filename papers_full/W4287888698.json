{
    "title": "WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models",
    "url": "https://openalex.org/W4287888698",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4202083010",
            "name": "Minixhofer, Benjamin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202083012",
            "name": "Paischer, Fabian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202083011",
            "name": "Rekabsaz, Navid",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3180187578",
        "https://openalex.org/W2057069782",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W3114950584",
        "https://openalex.org/W3112784227",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2888161220",
        "https://openalex.org/W4285143031",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2752630748",
        "https://openalex.org/W2995549860",
        "https://openalex.org/W2948552313",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4299579390",
        "https://openalex.org/W2294774419",
        "https://openalex.org/W3102961637",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2891896107",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W3173172270",
        "https://openalex.org/W2561995736",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W3176119108",
        "https://openalex.org/W3103187652",
        "https://openalex.org/W2741602058",
        "https://openalex.org/W2742113707",
        "https://openalex.org/W3009095382",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W2964114970",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W3157975787",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W2251765408",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W2963002901",
        "https://openalex.org/W2964266061",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2970597249"
    ],
    "abstract": "Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method -- called WECHSEL -- to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.",
    "full_text": "WECHSEL: Effective initialization of subword embeddings for\ncross-lingual transfer of monolingual language models\nBenjamin Minixhofer1 and Fabian Paischer2,3 and Navid Rekabsaz1,3\n1Institute of Computational Perception, Johannes Kepler University Linz\n2Institute for Machine Learning, Johannes Kepler University Linz\n3ELLIS Unit Linz and LIT AI Lab\n{benjamin.minixhofer, navid.rekabsaz}@jku.at\npaischer@ml.jku.at\nAbstract\nLarge pretrained language models (LMs) have\nbecome the central building block of many\nNLP applications. Training these models re-\nquires ever more computational resources and\nmost of the existing models are trained on En-\nglish text only. It is exceedingly expensive\nto train these models in other languages. To\nalleviate this problem, we introduce a novel\nmethod – called WECHSEL – to efﬁciently\nand effectively transfer pretrained LMs to new\nlanguages. WECHSEL can be applied to any\nmodel which uses subword-based tokenization\nand learns an embedding for each subword.\nThe tokenizer of the source model (in English)\nis replaced with a tokenizer in the target lan-\nguage and token embeddings are initialized\nsuch that they are semantically similar to the\nEnglish tokens by utilizing multilingual static\nword embeddings covering English and the tar-\nget language. We use WECHSEL to trans-\nfer the English RoBERTa and GPT-2 models\nto four languages (French, German, Chinese\nand Swahili). We also study the beneﬁts of\nour method on very low-resource languages.\nWECHSEL improves over proposed methods\nfor cross-lingual parameter transfer and outper-\nforms models of comparable size trained from\nscratch with up to 64x less training effort. Our\nmethod makes training large language models\nfor new languages more accessible and less\ndamaging to the environment. We make our\ncode and models publicly available.\n1 Introduction\nLarge LMs based on the Transformer architec-\nture (Vaswani et al., 2017) have become increas-\ningly popular since GPT (Radford et al., 2018)\nand BERT (Devlin et al., 2019) were introduced,\nprompting the creation of many large LMs pre-\ntrained on English text (Yang et al., 2019; Clark\net al., 2020; Lewis et al., 2020; Ram et al., 2021).\nThere is a tendency towards training larger and\nlarger models (Brown et al., 2020; Fedus et al.,\n2021) while the main focus is on the English lan-\nguage. Recent work has called attention to the costs\nassociated with training increasingly large LMs, in-\ncluding environmental and ﬁnancial cost (Strubell\net al., 2019; Bender et al., 2021). If training large\nLMs for English is already costly, it is prohibitively\nexpensive to train new, similarly powerful models\nto cover other languages.\nOne approach to address this issue is creating\nmassively multilingual models (Devlin et al., 2019;\nConneau et al., 2020; Xue et al., 2021) trained on a\nconcatenation of texts in many different languages.\nThese models show strong natural language under-\nstanding capabilities in a wide variety of languages,\nbut suffer from what Conneau et al. (2020) call\nthe curse of multilinguality: beyond a certain num-\nber of languages, overall performance decreases on\nmonolingual as well as cross-lingual tasks. Consis-\ntent with this ﬁnding, Nozza et al. (2020) observe\nthat monolingual LMs often outperform massively\nmultilingual models. This might be attributed to\nsuperior quality of monolingual tokenizers over\ntheir multilingual counterparts (Rust et al., 2021).\nIt is thus desirable to train monolingual models\nin more languages. Training monolingual models\nin non-English languages is commonly done by\ntraining a new model with randomly initialized pa-\nrameters (Antoun et al., 2020; Louis, 2020; Martin\net al., 2020; Rekabsaz et al., 2019). However, to\ntrain a model with capabilities comparable to that\nof an English model in this way, presumably a sim-\nilar amount of compute to what was used to train\nthe English model would be required.\nTo address this issue, we introduce WECHSEL,1\na novel method to transfer monolingual language\nmodels to a new language. WECHSEL uses multi-\nlingual static word embeddings between the source\nlanguage and the target language to initialize model\nparameters. WECHSEL ﬁrst copies all inner (non-\n1Word Embeddings Can Help initialize Subword Embed-\ndings in a new Language.\narXiv:2112.06598v2  [cs.CL]  4 May 2022\nembedding) parameters of the English model, and\nexchanges the tokenizer with a tokenizer for the tar-\nget language. Next, in contrast to prior work doing\nrandom initialization (de Vries and Nissim, 2021),\nthe token embeddings in the target language are\ninitialized such that they are close to semantically\nsimilar English tokens by mapping multilingual\nstatic word embeddings to subword embeddings.\nThe latter step is particularly important consider-\ning that token embeddings take up roughly 31%\nof the parameters of RoBERTa (Liu et al., 2019)\nand roughly 33% of the parameters of GPT2 (Rad-\nford et al., 2019). Intuitively, semantically transfer-\nring embeddings instead of randomly initializing\none third of the model should result in improved\nperformance. Our parameter transfer provides an\neffective initialization in the target language, requir-\ning signiﬁcantly fewer training steps to reach high\nperformance than training from scratch. As mul-\ntilingual static word embeddings are available for\nmany languages (Bojanowski et al., 2017), WECH-\nSEL is widely applicable.\nWe conduct our experiments on RoBERTa and\nGPT-2 as representative models of encoder and\ndecoder language models, respectively. We trans-\nfer the English RoBERTa model to four languages\n(French, German, Chinese and Swahili), and the\nEnglish GPT-2 model to the same four plus another\nfour very low-resource languages (Sundanese, Scot-\ntish Gaelic, Uyghur and Malagasy). We evaluate\nthe transferred RoBERTa models on Named En-\ntity Recognition (NER), and Natural Language In-\nference (NLI) tasks in the respective languages.\nThe transferred GPT-2 models are evaluated in\nterms of Language Modelling Perplexity (PPL) on\na held-out set. We compare WECHSEL with ran-\ndomly initialized models (denoted as FullRand), as\nwell as the recently proposed TransInner method\nwhich only transfers the inner (non-embedding)\nparameters (de Vries and Nissim, 2021). All men-\ntioned models are trained under the same condi-\ntions (around 4 days on a TPUv3-8). We also\ncompare our model with models of comparable\nsize trained from scratch under signiﬁcantly larger\ntraining regimes, in particular CamemBERT (Mar-\ntin et al., 2020) (French), GBERTBase (Chan et al.,\n2020) (German), and BERTBase-Chinese (Devlin\net al., 2019).\nResults show that models initialized with\nWECHSEL outperform randomly initialized mod-\nels and models initialized with TransInner across\nall languages and all tasks, for both RoBERTa and\nGPT-2. In addition, strong performance is reached\nat a fraction of the training steps of other methods.\nOur contribution is summarized as follows.\n• We propose WECHSEL, a novel method for\ntransferring monolingual language models to\na new language by utilizing multilingual static\nword embeddings between the source and the\ntarget language.\n• We show effective transfer of RoBERTa and\nGPT-2 using WECHSEL to four and eight lan-\nguages, respectively, achieved after minimal\ntraining effort.\n• We release more effective GPT-2 and\nRoBERTa models than previously published\nnon-English models, achieved under our more\nefﬁcient training setting. Our code and mod-\nels are publicly available at github.com/\ncpjku/wechsel.\nIn the following, we review related work in Sec-\ntion 2. We introduce the WECHSEL method in\nSection 3, followed by explaining the experiment\nsetup in Section 4. We show and discuss results in\nSection 5.\n2 Related Work\nLarge Language Models. Training Language\nModels is usually done in a self-supervised manner\ni. e. deriving labels from the training text instead\nof needing explicit annotations. One optimization\nobjective is Masked Language Modelling (Devlin\net al., 2019, MLM), where randomly selected to-\nkens in the input are replaced by a special[MASK]\ntoken, and the task is to predict the original tokens.\nAnother common objective is Causal Language\nModelling (CLM), where the task is to predict the\nnext token. These two objectives highlight a funda-\nmental distinction between language models: mod-\nels can be trained as encoders (e.g. with MLM) or\nas decoders (e.g. with CLM).\nInstead of words, the vocabulary of recently pro-\nposed language models commonly consists of sub-\nwords (Clark et al., 2020; Liu et al., 2019; Devlin\net al., 2019).\nMultilingual representations. There has been a\nsigniﬁcant amount of work in creating multilin-\ngual static word embeddings. A common method\nis learning embeddings from scratch using data\nin multiple languages (Luong et al., 2015; Duong\net al., 2016). Alternatively, multilinguality can be\nachieved by aligning existing monolingual word\nembeddings using a bilingual dictionary, so that\nthe resulting embeddings share the same semantic\nspace (Xing et al., 2015; Joulin et al., 2018). Recent\nstudies improve on this by reducing (or completely\nremoving) the need for bilingual data (Artetxe et al.,\n2017, 2018; Lample et al., 2018).\nBeside static word embeddings, multilinguality\nis also well studied in the area of contextualized\nrepresentations. One approach to learn multilingual\ncontextualized representations is through training\na model on a concatenation of corpora in differ-\nent languages. Some models created based on\nthis approach are mBERT (Devlin et al., 2019),\nXLM-R (Conneau et al., 2020) and mT5 (Xue\net al., 2021), trained on text in 104, 100, and 101\nlanguages, respectively. As shown by Pires et al.\n(2019), a multilingual model such as mBERT can\nenable cross-lingual transfer by using task-speciﬁc\nannotations in one language to ﬁne-tune the model\nfor evaluation in another language. Despite the ben-\neﬁts, recent studies outline a number of limitations\nof massively multilingual LMs. Wu and Dredze\n(2020) empirically show that in mBERT “the 30%\nlanguages with least pretraining resources perform\nworse than using no pretrained language model at\nall”. Conneau et al. (2020) report that beyond a\ncertain number of languages in the training data,\nthe overall performance decreases on monolingual\nas well as cross-lingual tasks. These studies moti-\nvate our work on introducing an efﬁcient approach\nfor creating effective monolingual LMs for more\nlanguages.\nCross-lingual transfer of monolingual LMs.\nStudies in this area can be divided into two cat-\negories:\n• Bilingualization of a monolingual LM is\nconcerned with extending a model to a new\nlanguage while preserving its capabilities in\nthe original language. Artetxe et al. (2020)\napproach this problem by replacing the to-\nkenizer and relearning the subword embed-\ndings, while freezing other (non-embedding)\nparameters. Such a model becomes bilingual,\nsince the initial tokenizer and embeddings can\nbe used for tasks in the source language, while\nthe new tokenizer and embeddings can be used\nfor tasks in the target language. Thus, a model\ncan be ﬁnetuned on annotated task data in\nthe source language, and then zero-shot trans-\nferred to the target language. Tran (2020)\nfollow a similar approach, while instead of\nrandomly initializing embeddings, they utilize\nstatic word embeddings to initialize embed-\ndings in the target language close to semanti-\ncally similar English tokens. They then con-\ntinue training the model on an English text\ncorpus as well as on the target language in or-\nder to preserve model capabilities in English.\n• Creating a new monolingual LM in the tar-\nget language is, in contrast, concerned with\ntransferring a model from a source to a tar-\nget language without the necessity to preserve\nits capabilities in the source language. Zoph\net al. (2016) and Nguyen and Chiang (2017)\nshow that cross-lingually transferring a ma-\nchine translation model can improve perfor-\nmance, especially for low-resource languages.\nZoph et al. (2016) use embeddings of random\ntokens in the original vocabulary to initial-\nize token embeddings in the new vocabulary,\nwhile Nguyen and Chiang (2017) utilize vo-\ncabulary overlap between the source and tar-\nget language. More recently, de Vries and Nis-\nsim (2021) follow a similar approach to the\none of Artetxe et al. (2020) for transferring a\nGPT-2 model to a new language. de Vries and\nNissim (2021) add an additional step, where\nthey train the entire model for some amount\nof steps to allow adapting to the target lan-\nguage beyond the lexical level. We refer to\nthe method of de Vries and Nissim (2021) as\nTransInner and consider it as a baseline in our\nexperiments.\nOur WECHSEL method belongs to the second\ncategory. WECHSEL can be seen as an extension\nto the method proposed by Tran (2020) with the\ngoal of creating a new monolingual LM instead\nof bilingualizing the LM. This allows removing\nthe constraints imposed by the need to preserve\nthe model’s capabilities in the source language. In\naddition, we generalize the semantic subword map-\nping done by Tran (2020) to consider an arbitrary\nnumber of semantically similar subword with an\narbitrary temperature. We are the ﬁrst to show\nthat a cross-lingually transferred model can outper-\nform monolingual models which have been trained\nextensively from scratch in the target language,\nwhile requiring substantially less computational\nresources.\n3 Methodology\nTo initialize the model in the target language, we\ncopy the inner (non-embedding) parameters from\nthe source model. Our goal, then, is given the tok-\nenizer Ts in the source language with vocabulary\nUs, the corresponding token embeddings Es, and a\ntokenizer Tt in the target language with vocabulary\nUt, to ﬁnd a good initialization of the embeddings\nEt by using Es. To this end, we use existing bilin-\ngual word embeddings enriched with subword in-\nformation, containing a set of words and subword\nn-grams in the source and target language and their\naligned vectors. We denote the set of words and\nn-grams in the source and target language as Vs\nand Vt respectively, and the aligned static embed-\ndings as Ws and Wt. In Appendix D we consider\nan alternative method if no subword information is\navailable in the bilingual word embeddings.\nFirst, independently for both languages, we com-\npute static subword embeddings for tokens in the\ntokenizer vocabulary in the same semantic space\nas the static word embeddings (Section 3.1). This\nresults in subword embeddings Us and Ut for the\nsource and target language, respectively. Next, we\nuse Us and Ut to compute the semantic similar-\nity of every subword in Us to every subword in\nUt. Using these semantic similarities, we initial-\nize the embeddings in Et through a convex com-\nbination of embeddings in Es (Section 3.2). By\napplying WECHSEL, the vectors of Et are in the\nsame semantic space as Es, where a subword in\nthe target language is semantically similar to its\ncounterpart(s) in the source language. These steps\nare summarized in Figure 1 and explained in more\ndetail in the following.\n3.1 Subword Embedding Computation\nThe process of mapping word embeddings to sub-\nword embeddings is done individually for the\nsource and the target language. Given a tokenizer\nT with vocabulary U and embeddings W , the goal\nis to ﬁnd subword embeddings U for subwords in\nU in the same semantic space as W . To this end,\nwe decompose subwords in U into n-grams and\ncompute the embedding by taking the sum of the\nembeddings of all occuring n-grams, equivalent to\nhow embeddings for out-of-vocabulary words are\ncomputed in fastText (Bojanowski et al., 2017).\nux =\n∑\ng∈G(x)\nwg\nSource  Language\nSubword Embedding\nComputation\nSubword similarity-\nbased Transfer\nSubword Embedding\nComputation\nTarget Language\naligned\ncopy\nSource Model Target Model\n Embeddings   Embeddings  \nTokenizer  Tokenizer  \n Embeddings  \n(Model Input Space) \nNon-Embedding\nweights \n Embeddings  \n(Model Input Space)\nNon-Embedding\nweights\n Embeddings  \n(Word Embedding Space)\n Embeddings  \n(Word Embedding Space)\nFigure 1: Summary of our WECHSEL method. We\nshow inputs, intermediate results and outputs.\nwhere G(x) is the set of n-grams occuring in the sub-\nword xand wg is the embedding of the n-gram g.\nSubwords in which no known n-gram occurs are\ninitialized to zero.\n3.2 Subword similarity-based Transfer\nApplying the previous step to both source and tar-\nget language results in the subword embeddings\nUs and Ut over the subword vocabularies Us and\nUt, respectively. Our aim is to leverage these em-\nbeddings to ﬁnd an effective transformation from\nEs to Et. We ﬁrst compute the cosine similarity\nof every subword x∈Ut to every subword y∈Us,\ndenoted as sx,y.\nsx,y = ut\nxus\ny\nT\n∥utx∥∥usy∥\nWe now exploit these similarities to initialize\nembeddings in Et by a convex combination of\nembeddings in Es. In particular, each subword\nembedding in Et is deﬁned as the weighted mean\nof the k nearest embeddings in Es according to\nthe similarity values. The weighting is done by a\nsoftmax of the similarities with temperature τ.\net\nx =\n∑\ny∈Jx exp (sx,y/τ) ·es\ny∑\ny′∈Jx exp (sx,y′ /τ)\nwhere Jx is the set of k neighbouring subwords\nin the source language. Subword embeddings for\nwhich Ut is zero are initialized from a random\nnormal distribution N(E[Es],Var[Es]).\n4 Experiment Design\nWe evaluate our method by transferring the En-\nglish RoBERTa (Liu et al., 2019) and the English\nGPT-2 model (Radford et al., 2019) to French, Ger-\nman, Chinese and Swahili. We refer to these lan-\nguages as medium-resource languages. In addition,\nwe study the beneﬁts of our method on four low-\nresource languages, namely Sundanese, Scottish\nGaelic, Uyghur and Malagasy.\nWe evaluate WECHSEL-RoBERTa by ﬁne-\ntuning on XNLI (Conneau et al., 2018), and on the\nbalanced train-dev-test split of WikiANN (Rahimi\net al., 2019; Pan et al., 2017) to evaluate NLI and\nNER performance, respectively. The hyperparame-\nters used for ﬁne-tuning are reported in Appendix B.\nGPT-2 is evaluated by Perplexity (PPL) on a held-\nout set from the same corpus on which the model\nwas trained on. Due to the difﬁculty of extrin-\nsic evaluation on low-resource languages, we only\ntrain GPT-2 models in these languages, and eval-\nuate their performance intrinsically via Language\nModelling Perplexity on a held-out set. We use the\npretrained models RoBERTaBase with 125M pa-\nrameters, and the small GPT-2 variant with 117M\nparameters provided by HuggingFace’s Transform-\ners (Wolf et al., 2020) in all experiments.\nSince under limited training regimes such as\nours, using a smaller corpus does not in general\ndegrade performance (Martin et al., 2020), we\nuse a subset of 4GiB from the OSCAR corpus\nfor German, French and Chinese. For the other\nlanguages, we use data from the CC-100 corpus\n(Conneau et al., 2020) which contains 1.6GiB,\n0.1GiB, 0.1GiB, 0.4GiB and 0.2GiB for Swahili,\nSundanese, Scottish Gaelic, Uyghur and Malagasy,\nrespectively. To obtain aligned word embeddings\nbetween the source and the target language we\nuse monolingual fastText word embeddings2 (Bo-\njanowski et al., 2017). We align these embeddings\nusing the Orthogonal Procrustes method (Schöne-\nmann, 1966; Artetxe et al., 2016) with bilingual\ndictionaries from MUSE3 (Conneau et al., 2017)\nfor French, German and Chinese and a bilingual\ndictionary from FreeDict4 (Ba´nski and Wójtowicz,\n2009) for Swahili. For the low-resource languages,\nwe use bilingual dictionaries scraped from Wik-\ntionary.5\n2https://fasttext.cc\n3https://github.com/facebookresearch/MUSE\n4https://freedict.org\n5available at github.com/cpjku/wechsel\nModel Tokens trained on Factor\nWECHSEL-RoBERTa 65.5B 1.0x\nTransInner-RoBERTa 65.5B 1.0x\nFullRand-RoBERTa 65.5B 1.0x\nCamemBERT 419.4B 6.4x\nGBERTBase 255.6B 3.9x\nBERTBase-Chinese 131.1B 2.0x\nTable 1: Tokens trained on in the target language be-\ntween our models and previous monolingual models.\nWe choose temperature τ = 0.1 and neighbors\nk= 10for WECHSEL by conducting a parameter\nsearch over a grid with varying values for k and\nτ using linear probes (Appendix A). We train tok-\nenizers in the target languages using a vocabulary\nsize of 50k tokens and byte-level BPE (Radford\net al., 2019). After applying WECHSEL, we con-\ntinue training RoBERTa on the MLM objective and\nGPT-2 on the CLM objective. We compare against\ntwo baseline methods.\n• TransInner: Randomly initializing Et while\ntransferring all other parameters from the En-\nglish model as in de Vries and Nissim (2021).\nAfter training only embeddings for a ﬁxed\namount of steps while freezing other parame-\nters, the entire model is trained for the remain-\ning steps. In preliminary experiments reported\nin Appendix E, we compare the method by\nZoph et al. (2016) with TransInner, observing\nsuperior performance of TransInner, so we\nchoose TransInner as the baseline for cross-\nlingual transfer in all our experiments.\n• FullRand: Training from scratch in the target\nlanguage, as is commonly done when train-\ning BERT-like or GPT-like models in a new\nlanguage (Antoun et al., 2020; Louis, 2020;\nChan et al., 2020; Martin et al., 2020).\nAll models are trained for 250k steps with the\nsame hyperparameters across all languages (re-\nported in Appendix B). Training one model takes\naround 4 days on a TPUv3-8. For WECHSEL and\nFullRand we use a learning rate (LR) schedule with\nlinear warmup from zero to the peak LR for the ﬁrst\n10% of steps, followed by a linear decay to zero.\nFor TransInner, we perform two warmup phases\nfrom zero to peak LR, once for the ﬁrst 10% of\nsteps for training embeddings only, then again for\nthe remaining steps while training the entire model.\nIn addition to the mentioned baselines trained\nunder this setting, we compare the results of\nLang Model Score@0 Score@25k Score@250k Score (more training)\nNLI NER Avg NLI NER Avg NLI NER Avg NLI NER Avg\nFrench\nWECHSEL-RoBERTa 78.25 86.93 82.59 81.63 90.26 85.95 82.43 90.88 86.65 - - -\nTransInner-RoBERTa 60.86 69.57 65.21 65.49 83.82 74.66 81.75 90.34 86.04 - - -\nFullRand-RoBERTa 55.71 70.79 63.25 69.02 84.24 76.63 75.28 89.30 82.29 - - -\nCamemBERT - - - - - - - - - 80.88 90.26 85.57\nXLM-RBase - - - - - - - - - 79.25 89.48 84.37\nGerman\nWECHSEL-RoBERTa 75.64 84.53 80.08 81.11 89.05 85.08 81.79 89.72 85.76 - - -\nTransInner-RoBERTa 58.51 65.23 61.87 64.78 82.05 73.42 80.75 89.30 85.02 - - -\nFullRand-RoBERTa 54.82 66.84 60.83 68.02 81.53 74.77 75.48 88.36 81.92 - - -\nGBERTBase - - - - - - - - - 78.64 89.46 84.05\nXLM-RBase - - - - - - - - - 78.58 88.76 83.67\nChinese\nWECHSEL-RoBERTa 63.23 72.79 68.01 77.19 79.07 78.13 78.32 80.55 79.44 - - -\nTransInner-RoBERTa 46.95 69.06 58.01 52.96 73.35 63.16 76.99 80.00 78.49 - - -\nFullRand-RoBERTa 44.24 57.95 51.09 58.34 64.84 61.59 71.38 78.35 74.86 - - -\nBERTBase-Chinese - - - - - - - - - 76.55 82.05 79.30\nXLM-RBase - - - - - - - - - 76.41 78.36 77.38\nSwahili\nWECHSEL-RoBERTa 60.28 74.38 67.33 73.87 87.63 80.75 75.05 87.39 81.22 - - -\nTransInner-RoBERTa 54.67 64.46 59.56 58.85 80.27 69.56 74.10 87.05 80.57 - - -\nFullRand-RoBERTa 50.59 62.35 56.47 63.79 83.49 73.64 70.34 87.34 78.84 - - -\nXLM-RBase - - - - - - - - - 69.18 87.37 78.28\nTable 2: Results from ﬁne-tuning RoBERTa models. We report accuracy for NLI on XNLI and micro F1 score for\nNER on WikiANN. Results are averaged over 3 runs. We report scores before training ( Score@0), after 10% of\nsteps (Score@25k) and after training ( Score@250k). We also report results from ﬁne-tuning prior monolingual\nmodels and XLM–R (Score (more training)), all trained on more tokens than our models. For each language, the\nbest results in every column are indicated with underlines. The overall best results including the comparison with\nexisting monolingual/multilingual models of comparable size are shown in bold.\nRoBERTa models with existing comparable mod-\nels trained from scratch with more training ef-\nfort. We consider the total number of tokens the\nmodel has encountered in the target language, com-\nputed as the product of batch size ×sequence\nlength ×train steps (shown in Table 1) as a proxy\nfor training effort. We evaluate the performance\nof CamemBERT (Martin et al., 2020) (French),\nGBERTBase (Chan et al., 2020) (German), and\nBERTBase-Chinese (Devlin et al., 2019) as existing\nmonolingual LMs,6 as well as XLM-RBase (Artetxe\net al., 2020) as a high-performing multilingual LM.\n5 Results\nWe present our results on transferring RoBERTa\nand GPT-2 from English to other languages, fol-\nlowed by analyzing training behavior. In Ap-\npendix C, we provide a qualitative assessment of\nhow well subword tokens are mapped between the\nsource and the target languages.\n5.1 Transferring RoBERTa\nTable 2 reports the evaluation results of RoBERTa.\nAs shown, models initialized with WECHSEL out-\nperform models trained from scratch and models\ninitialized with TransInner across all languages.\n6To the best of our knowledge there is no monolingual\nmodel available for Swahili.\nSurprisingly, close relatedness of the source and\ntarget language is not necessary to achieve effective\ntransfer, as e. g. on NLI WECHSEL improves abso-\nlute accuracy by 7.15%, 6.31%, 6.94% and 4.71%\nover models trained from scratch for French, Ger-\nman, Chinese and Swahili, respectively.\nWe observe that our parameter transfer-based\nmodel consistently outperforms the previously re-\nleased LMs on both monolingual and multilingual\nsettings, while these models beneﬁt from much\nlarger training resources in terms of computation\ntime and corpus size. In particular, the results\nshow an improvement over XLM-RBase by an av-\nerage 3.54% accuracy for NLI and 1.14% micro\nF1 score for NER. For NLI, we improve over the\nprior monolingual models by 1.55%, 3.15% and\n1.77% absolute accuracy for French, German and\nChinese, respectively. For NER, we observe im-\nprovements over monolingual models with 0.62%\nand 0.26% absolute micro F1 score improvement\nfor French and German, respectively. For Chinese,\nthe monolingual model BERTBase-Chinese still out-\nperforms our method by 1.5% absolute micro F1\nscore. We suspect that the discrepancy between\nNLI and NER is due to the limited training cor-\npus size (max. 4GiB), while a larger corpus can\npotentially improve NER as more named entities\nappear (Martin et al., 2020).\n55\n60\n65\n70\n75\n80French\nCamemBERT\n6.4x more training\nNLI Accuracy (%)\n70\n75\n80\n85\n90\n CamemBERT\n6.4x more training\nNER Accuracy (%)\n20\n25\n30\n Language Modelling Perplexity\n55\n60\n65\n70\n75\n80German\nGBERTBase\n3.9x more training\n65\n70\n75\n80\n85\n90\nGBERTBase\n3.9x more training\n25\n30\n35\n45\n50\n55\n60\n65\n70\n75\n80Chinese\nBERTBase–Chinese\n2x more training\n55\n60\n65\n70\n75\n80\n BERTBase–Chinese\n2x more training\n50\n55\n60\n65\n70\n75\n0 50k 100k 150k 200k 250k\nSteps\n50\n55\n60\n65\n70\n75Swahili\nWECHSEL-RoBERTa\nTransInner-RoBERTa\nFullRand-RoBERTa\n0 50k 100k 150k 200k 250k\nSteps\n65\n70\n75\n80\n85\nWECHSEL-RoBERTa\nTransInner-RoBERTa\nFullRand-RoBERTa\nSteps\n10\n15\n20\nWECHSEL-GPT2\nTransInner-GPT2\nFullRand-GPT2\nFigure 2: Test scores over training steps from ﬁne-tuning RoBERTa models on NLI (using XNLI) and NER (using\nWikiANN). Perplexity on the held-out set over training steps of GPT-2 models. We evaluate every 12.5k steps.\nLang Model PPL@0 PPL@25k PPL@250k\nFrench\nWECHSEL-GPT2 1.7e+3 23.47 19.71\nTransInner-GPT2 1.4e+5 67.97 20.13\nFullRand-GPT2 5.9e+4 25.99 20.47\nGerman\nWECHSEL-GPT2 3.7e+3 34.35 26.80\nTransInner-GPT2 1.5e+5 121.67 27.76\nFullRand-GPT2 5.8e+4 37.29 27.63\nChinese\nWECHSEL-GPT2 2.4e+4 71.02 51.97\nTransInner-GPT2 1.5e+5 231.05 56.17\nFullRand-GPT2 5.8e+4 69.29 52.98\nSwahili\nWECHSEL-GPT2 1.4e+5 13.02 10.14\nTransInner-GPT2 1.4e+5 42.95 10.28\nFullRand-GPT2 5.8e+4 13.22 10.58\nTable 3: Results of training GPT2 models. We report\nPerplexity before training (PPL@0), after 10% of steps\n(PPL@25k) and after training (PPL@250k).\nThe ﬁrst two columns of Figure 2 show the\nperformance of RoBERTa models on downstream\ntasks after each 12.5k training steps. Models ini-\ntialized with WECHSEL reach high performance\nin signiﬁcantly fewer steps than models initialized\nwith FullRand or TransInner.\nLang Model Best PPL\nSundanese\nWECHSEL-GPT2 111.72\nTransInner-GPT2 151.86\nFullRand-GPT2 149.46\nScottish Gaelic\nWECHSEL-GPT2 16.43\nTransInner-GPT2 18.62\nFullRand-GPT2 19.53\nUyghur\nWECHSEL-GPT2 34.33\nTransInner-GPT2 39.06\nFullRand-GPT2 42.82\nMalagasy\nWECHSEL-GPT2 14.01\nTransInner-GPT2 14.85\nFullRand-GPT2 15.93\nTable 4: Results of training GPT2 models on low-\nresource languages. We report the best Perplexity on\nthe held-out set, evaluated every 2.5k steps. See Fig-\nure 3 for Perplexity throughout training.\nWe expect FullRand-RoBERTa to approach per-\nformance of the respective prior monolingual mod-\nels when trained on the same amount of tokens. 7\n7It would presumably be slightly worse because we restrict\ntraining corpus size to 4GiB.\nFor French, WECHSEL-RoBERTa outperforms\nCamemBERT after 10% of training steps, reducing\ntraining effort by 64x. For German, WECHSEL-\nRoBERTa outperforms GBERTBase after 10% of\ntraining steps, reducing training effort by 39x.\nFor Chinese, WECHSEL-RoBERTa outperforms\nBERTBase-Chinese on NLI, but does not outper-\nform BERTBase-Chinese on NER.\n5.2 Transferring GPT-2\n5.2.1 To Medium-Resource Languages\nResults on medium-resource languages are shown\nin Table 3. Similar to the results for WECHSEL-\nRoBERTa, the GPT-2 models trained with WECH-\nSEL consistently outperform the models trained\nfrom scratch and with TransInner across all lan-\nguages.\nThe rightmost column of Figure 2 depicts the\nperformance of GPT-2 models after each 12.5k\ntraining steps. Comparing the results across all lan-\nguages throughout training, we observe a stronger\ndependence on similarity of the source to the tar-\nget language than for downstream tasks such as\nNLI or NER. In particular, for French and German,\nWECHSEL is consistently better than TransInner\nand FullRand throughout the entire training, while\nfor Chinese, a decrease in perplexity towards the\nend of training causes WECHSEL to surpass train-\ning from scratch.\n5.2.2 To Low-Resource Languages\nTable 4 reports the perplexity of Language Mod-\nelling on the low-resource languages. Again, we\nobserve consistent improvements using WECHSEL\non all languages. Furthermore, we ﬁnd that the\nimprovement from WECHSEL tends to increase\nas the amount of training data decreases by con-\nducting a sensitivity analysis w. r. t. the amount of\navailable training data (Appendix F).\nIn Figure 3 we report the performance of the\nlow-resource LMs on the held-out set throughout\ntraining. One difference of the low-resource mod-\nels with the ones trained on medium-resource lan-\nguages is that the low-resource LMs are prone to\noverﬁtting, and require appropriate model selec-\ntion even in the early steps of training. Notably,\nTransInner-GPT2 takes more steps to overﬁt since\nall non-embedding parameters are frozen for the\nﬁrst 25k steps (c. f. Section 4).\n200\n400\n600\n800Sundanese\nLanguage Modelling Perplexity\n20\n40\n60\n80\n100Scottish Gaelic\n40\n60\n80\n100Uyghur\n0 10k 20k 30k 40k 50k 60k\nSteps\n20\n40\n60\n80\n100Malagasy\nWECHSEL-GPT2\nTransInner-GPT2\nFullRand-GPT2\nFigure 3: Perplexity throughout training on low-\nresource languages. We evaluate every 2.5k steps and\nstop training if Perplexity on the held-out set does not\nimprove for 10k steps.\n5.3 Is freezing necessary?\nPrevious work using the TransInner method freezes\nnon-embedding parameters for a ﬁxed amount of\nsteps before training the entire model (de Vries\nand Nissim, 2021). This is done to prevent catas-\ntrophic forgetting at the beginning of training. To\nevaluate if freezing non-embedding parameters is\nstill necessary with our method, we conduct an\nadditional experiment. We train a German GPT-2\nmodel with WECHSEL and a model with TransIn-\nner without freezing any parameters, and the same\nmodels with freezing of non-embedding parameters\nfor the ﬁrst 10% of steps. We match hyperparame-\nters of the main experiments except training for 75k\nsteps only. Based on the results shown in Figure 4,\nwe conclude that freezing is necessary when using\nTransInner, but there is no need for freezing when\nusing WECHSEL.\n12.5k 25k 37.5k 50k 62.5k 75k\nSteps\n40\n50\n60\n70\n80\n90\n100\n110\n120Perplexity\nTransInner-GPT2 (freeze inner 10%)\nTransInner-GPT2 (no freeze)\nWECHSEL-GPT2 (no freeze)\nWECHSEL-GPT2 (freeze inner 10%)\nFigure 4: Comparison of German GPT-2 models\ntrained with WECHSEL and TransInner between freez-\ning non-embedding parameters at the start and not\nfreezing any parameters.\n6 Limitations and Potential Risks\n6.1 Limitations\nWe conduct our experiments on up to eight lan-\nguages, showing the beneﬁts of our parameter trans-\nfer method to both medium- and low-resource lan-\nguages. However, there are many more languages\nwith diverse linguistic characteristics on which our\nWECHSEL method is not tested. This is a limi-\ntation forced by computational constraints, as we\ncan not ascertain whether transfer to all other lan-\nguages would result in similar improvements. In\naddition, our extrinsic evaluation is limited to two\ntasks (NLI and NER). While this choice is due\nto the limitations on the available collections in\nvarious languages, this evaluation does not neces-\nsarily provide a comprehensive view of language\nunderstanding tasks.\n6.2 Risks\nIt is well-known that existing LMs trained on En-\nglish text encode societal biases (Bolukbasi et al.,\n2016; Caliskan et al., 2017; Rekabsaz et al., 2021b)\nand stereotypes and using them in downstream\ntasks might lead to unfair treatment of various so-\ncial groups (Zerveas et al., 2022; Krieg et al., 2022;\nGanhör et al., 2022; Rekabsaz et al., 2021a; Mel-\nchiorre et al., 2021; Rekabsaz and Schedl, 2020;\nElazar and Goldberg, 2018). Since we propose\na method to transfer the English LMs to new lan-\nguages, it is highly probable that the existing biases\nare also transferred to the target LMs. We therefore\nadvocate a conscious and responsible use of the\ntransferred LMs in practice.\n7 Conclusion\nWe introduce WECHSEL, an effective method to\ntransfer monolingual language models to new lan-\nguages. WECHSEL exploits multilingual static\nword embeddings to compute an effective initializa-\ntion of subword embeddings in the target language.\nWe conduct experiments by transferring RoBERTa\nand GPT-2 models from English to French, Ger-\nman, Chinese and Swahili, as well as English GPT-\n2 to four low-resource languages. The evaluation\nresults show that the transferred RoBERTa and\nGPT-2 models are more efﬁcient and effective than\nstrong baselines, and consistently outperform prior\nmonolingual models that have been trained for a\nsigniﬁcantly longer time. WECHSEL facilitates\nthe creation of effective monolingual LMs for new\nlanguages with medium to low resources, particu-\nlarly in computationally-limited settings. In addi-\ntion, our work provides strong evidence towards\nthe hypothesis by Artetxe et al. (2020) that deep\nmonolingual language models learn abstractions\nthat generalize across languages.\n8 Acknowledgments\nResearch supported with Cloud TPUs from\nGoogle’s TPU Research Cloud (TRC). We thank\nAndy Koh and Artus Krohn-Grimberghe for provid-\ning additional computational resources. The ELLIS\nUnit Linz, the LIT AI Lab, the Institute for Ma-\nchine Learning, are supported by the Federal State\nUpper Austria. We thank the project INCONTROL-\nRL (FFG-881064). Research also supported in part\nby the NSF (IIS-1956221), the State of Upper Aus-\ntria and the Austria’s Federal Ministry of Educa-\ntion, Science, and Research through the project\nFAIRFLOW (LIT-2021-YOU-215).\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2289–2294, Austin, Texas. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. InPro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nPiotr Ba´nski and Beata Wójtowicz. 2009. Freedict: an\nopen source repository of tei-encoded bilingual dic-\ntionaries.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Proc.\nof NeurIPS.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience.\nBranden Chan, Stefan Schweter, and Timo Möller.\n2020. German’s next language model. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6788–6796, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2017.\nWord translation without parallel data. arXiv\npreprint arXiv:1710.04087.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nWietse de Vries and Malvina Nissim. 2021. As good\nas new. how to successfully recycle English GPT-2\nto make models for other languages. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 836–846, Online. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2016. Learning crosslingual\nword embeddings without bilingual corpora. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1285–\n1295, Austin, Texas. Association for Computational\nLinguistics.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nChristian Ganhör, David Penz, Navid Rekabsaz, Oleg\nLesota, and Markus Schedl. 2022. Mitigating con-\nsumer biases in recommendations with adversarial\ntraining. In Proceedings of the 45th International\nACM SIGIR conference on research and develop-\nment in Information Retrieval, SIGIR 2022. ACM.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHervé Jégou, and Edouard Grave. 2018. Loss in\ntranslation: Learning bilingual word mapping with\na retrieval criterion. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2984, Brussels, Bel-\ngium. Association for Computational Linguistics.\nKlara Krieg, Emilia Parada-Cabaleiro, Markus Schedl,\nand Navid Rekabsaz. 2022. Do perceived gender bi-\nases in retrieval results affect relevance judgements?\nIn Proceedings of the Workshop on Algorithmic Bias\nin Search and Recommendation at the European\nConference on Information Retrieval (ECIR-BIAS\n2022).\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. In Advances in\nNeural Information Processing Systems, volume 33,\npages 18470–18481. Curran Associates, Inc.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAntoine Louis. 2020. BelGPT-2: a GPT-2 model pre-\ntrained on French corpora. https://github.\ncom/antoiloui/belgpt2.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Bilingual word representations with\nmonolingual quality in mind. In Proceedings of the\n1st Workshop on Vector Space Modeling for Natural\nLanguage Processing, pages 151–159, Denver, Col-\norado. Association for Computational Linguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n7203–7219, Online. Association for Computational\nLinguistics.\nAlessandro B. Melchiorre, Navid Rekabsaz, Emilia\nParada-Cabaleiro, Stefan Brandl, Oleg Lesota, and\nMarkus Schedl. 2021. Investigating gender fair-\nness of recommendation algorithms in the music\ndomain. Information Processing and Management,\n58(5):102666.\nToan Q. Nguyen and David Chiang. 2017. Trans-\nfer learning across low-resource, related languages\nfor neural machine translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 296–301, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [mask]? making sense of language-speciﬁc\nbert models. arXiv preprint arXiv:2003.02912.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir\nGloberson, and Omer Levy. 2021. Few-shot ques-\ntion answering by pretraining span selection. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 3066–\n3079, Online. Association for Computational Lin-\nguistics.\nNavid Rekabsaz, Simone Kopeinik, and Markus\nSchedl. 2021a. Societal biases in retrieved contents:\nMeasurement framework and adversarial mitigation\nof bert rankers. In Proceedings of the 44th Interna-\ntional ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval, page 306–316.\nNavid Rekabsaz, Nikolaos Pappas, James Henderson,\nBanriskhem K Khonglah, and Srikanth Madikeri.\n2019. Regularization advantages of multilingual\nneural language models for low resource domains.\narXiv preprint arXiv:1906.01496.\nNavid Rekabsaz and Markus Schedl. 2020. Do neural\nranking models intensify gender bias? In Proceed-\nings of the 43rd International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 2065–2068.\nNavid Rekabsaz, Robert West, James Henderson, and\nAllan Hanbury. 2021b. Measuring societal biases\nfrom text corpora with smoothed ﬁrst-order co-\noccurrence. In Proceedings of the Fifteenth Interna-\ntional AAAI Conference on Web and Social Media,\nICWSM 2021, held virtually, June 7-10, 2021, pages\n549–560. AAAI Press.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2021. How good is\nyour tokenizer? on the monolingual performance of\nmultilingual language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3118–3135, Online. As-\nsociation for Computational Linguistics.\nPeter H Schönemann. 1966. A generalized solution of\nthe orthogonal procrustes problem. Psychometrika,\n31(1):1–10.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nKe Tran. 2020. From english to foreign languages:\nTransferring pre-trained language models. arXiv\npreprint arXiv:2002.07306.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings\nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1006–1011,\nDenver, Colorado. Association for Computational\nLinguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32. Curran\nAssociates, Inc.\nGeorge Zerveas, Navid Rekabsaz, Daniel Cohen, and\nCarsten Eickhoff. 2022. Mitigating bias in search re-\nsults through set-based document reranking and neu-\ntrality regularization. In Proceedings of the 45th In-\nternational ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2022.\nACM.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.\nA Grid search over kand τ\nTo choose number of neighbors kand temperature\nτ for WECHSEL we conduct a grid search over\nlinear probes of models with different initializa-\ntion shown in Table 7. For RoBERTa, we compute\nscores on NLI (using XNLI) and POS tagging (us-\ning the French, German and Chinese GSD corpora\nin Universal Dependencies) using linear probes of\nthe last hidden state. We probe on NLI by taking\na concatenation of the mean of all token represen-\ntations in the premise with the mean of all token\nrepresentations in the hypothesis. We probe on\nPOS tagging by taking the mean of all token rep-\nresentations belonging to each word. For GPT2,\nwe compute Language Modelling Perplexity on the\nheld-out set also used to evaluate performance of\nthe trained models.\nB Hyperparameters\nHyperparameters used to ﬁne-tune RoBERTa on\ndownstream tasks are shown in Table 5. Hyperpa-\nrameters used to train models in our main experi-\nments are shown in Table 6.\nParameter NLI NER\npeak learning rate 2e-5 2e-5\nbatch size 128 32\nsequence length 128 128\nAdam ϵ 1e-8 1e-8\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\ntrain epochs 2 10\nwarmup 10% of steps 10% of steps\nwarmup schedule linear linear\nLR decay linear to zero linear to zero\nTable 5: Hyperparameters used to ﬁne-tune RoBERTa\nmodels on NLI (XNLI) and NER (WikiANN).\nParameter RoBERTa GPT2\npeak learning rate 1e-4 5e-4\nbatch size 512 512\nsequence length 512 512\nweight decay 0.01 0.01\nAdam ϵ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.98 0.98\ntrain steps 250k 250k\nTable 6: Hyperparameters of the models transferred\nfrom RoBERTa and GPT2.\nC Qualitative subword correspondence\nWe show a small random sample of tokens in the\ntarget language and their closest English token (ac-\ncording to WECHSEL) in Table 8.\nD Using Word Embeddings without\nsubword information\nAs an alternative to n-gram decomposition, we in-\ntroduce a method for mapping word embeddings to\nsubword embeddings without using any subword\ninformation (shown in Figure 5). For this method,\nwe require word frequency information in addition\nto the word embeddings. We apply the tokenizer T\nto every word vin V resulting in a set of subwords\nfor each word. We deﬁne V(x) as the set of words\ncontaining the subword x when tokenized. The\nembedding ux of the subword xis then deﬁned as\nthe average of the embeddings of words in V(x),\nweighted by the word frequencies.\nux =\n∑\nv∈V(x) wv ·fv∑\nv∈V(x) fv\nwhere wv is the embedding and fv is the frequency\nof word v.\nLang Model k τ Scores\nNLI POS LM\nFrench\nWECHSEL@0\n1 1 58.4 85.2 2.5e+5\n10 0.1 59.8 86.8 2.0e+5\n10 1 58.3 84.4 4.8e+5\n50 0.1 57.2 83.6 3.1e+6\n50 1 54.0 81.6 1.8e+7\nFullRand@0 - - 46.3 60.6 5.7e+6\nCamemBERT - - 63.5 93.6 -\nGerman\nWECHSEL@0\n1 1 55.8 72.7 6e+5\n10 0.1 58.9 76.0 4.2e+5\n10 1 57.5 75.4 8.3e+6\n50 0.1 55.4 75.4 1.0e+7\n50 1 53.6 69.5 5.9e+7\nFullRand@0 - - 44.5 49.1 6.2e+6\nGBERTBase - - 63.2 81.4 -\nChinese\nWECHSEL@0\n1 1 47.4 75.4 2.7e+6\n10 0.1 48.0 80.7 2.6e+6\n10 1 48.3 80.3 3.1e+6\n50 0.1 48.3 77.8 3.7e+7\n50 1 47.9 76.5 8.6e+7\nFullRand@0 - - 37.5 53.7 5.8e+6\nBERTBase-Chinese - - 61.9 91.9 -\nTable 7: Grid search over the temperatureτand number\nof most similar tokens kparameters of WECHSEL.\nWe call this variant of our method WECHSELTFR.\nWe evaluate WECHSELTFR by training the same\nmodels as for WECHSEL. Results are shown in\nTable 9 for GPT2 and in Table 10 for RoBERTa.\nWe ﬁnd that, on average, performance is on par\nwith WECHSEL.\nE Choosing a transfer baseline\nWe consider two baseline methods to transfer mod-\nels to a new language without using any language-\nspeciﬁc information. One method copies non-\nembedding parameters to the target language and\ninitalizes embeddings from a random normal distri-\nbution as done by de Vries and Nissim (2021). We\nrefer to this method as TransInner. Another option\ncopies non-embedding parameters and assigns the\nembedding of a random token in the source lan-\nguage to each embedding in the target language\n(effectively \"shufﬂing\" the embeddings) as done by\nZoph et al. (2016) and Nguyen and Chiang (2017).\nWe refer to this method as TransInnerShufﬂeEmb.\nWe evaluate these two methods using a setup equiv-\nalent to the experiments in Section 5.3 and ﬁnd that\nTransInner performs slightly better than TransIn-\nnerShufﬂeEmb (Figure 6), so we use TransInner\nfor subsequent experiments.\nT o k e n s V e c t o r F r e q u e n c y \n[ex, citing]\n[teach, ing]\n[ex, position]\n[teach, able]\nT o k e n V e c t o r F r e q u e n c y \nex\nciting\nteach\ning\nex\nposition\nteach\nable\nﬂatten (frequency-weighted)tokenize reduce\nEmbeddings \nW o r d V e c t o r F r e q u e n c y \nexciting\nteaching\nexposition\nteachable\n+ word frequencies \nT o k e n V e c t o r \nex\nciting\nteach\ning\nposition\nable\n Embeddings \n(in the same semantic space as )Tokenizer \nFigure 5: WECHSELTFR, an alternative subword embedding computation method. First, tokenize all words in the\nword embeddings. Then ﬂatten the result by assigning the embeddings of the words in which it occured and their\nword frequencies to each subword. Finally,reduce the embeddings assigned to each subword by taking their mean,\nweighted by word frequency.\n12.5k 25k 37.5k 50k 62.5k 75k\nSteps\n30\n40\n50\n60\n70\n80\n90Perplexity\nTransInner-GPT2 (freeze inner 10%)\nTransInner-GPT2 (no freeze)\nTransInnerShuﬄeEmb-GPT2 (freeze inner 10%)\nTransInnerShuﬄeEmb-GPT2 (no freeze)\nWECHSEL-GPT2 (freeze inner 10%)\nWECHSEL-GPT2 (no freeze)\nFigure 6: Comparison of German GPT-2 models\ntrained with WECHSEL, TransInner and TransInner-\nShufﬂeEmb between freezing non-embedding parame-\nters at the start and not freezing any parameters.\nF Sensitivity Analysis w. r. t. training\ndata size\nEvaluating on languages with different amounts of\navailable data only indirectly measures the effect\nof training data size on WECHSEL since other fac-\ntors (e.g. language similarity to English) are also\ninvolved. We conduct a sensitivity analysis to make\nthe relation to the amount of training data explicit\n(Table 11). Due to computational constraints we\nonly do this for French. We ﬁnd that the improve-\nment from WECHSEL increases as the amount of\ntraining data decreases. In addition, we ﬁnd that\nusing fastText embeddings trained on less data dete-\nriorates performance, but still leaves a clear margin\nto TransInner and FullRand.\nLang Target Token Closest English Token\nFrench\nhéritage legacy\ntremp soaked\népiscop bishop\nscandaleux udicrous\nvertig astonishing\nenregistrer rec\nsucrés sweets\nEmmanuel Emmanuel\nentourage conﬁd\nsecrétariat ariat\nGerman\nmachen ize\nmit with\nSprichwort proverb\nerischen Austrian\nminuten utes\nHaustechnik umbing\ndringen urgent\nverfeinern reﬁne\numgebung vironments\nternehmen irms\nChinese\n到处 everywhere\n巧合 coinc\n第三 third\n杂交 recomb\n利来 chnology\n政务 Govern\n石 stone\n喊麦 sing\n中海 iterranean\n张某 defendant\nSwahili\nshirikishe ive\nHarusi Marriage\npesile ery\ntihani graduate\nchangi ool\nkuugua ingestion\nkuzidi acclaim\nvipigo Trouble\ndhamiri conscience\naliposimama Slowly\nTable 8: Samples of tokens in each language and the\ncorresponding closest tokens from the English vocabu-\nlary according to WECHSEL.\nLang Model PPL@0 PPL@25k PPL@250k\nFrench WECHSEL-GPT2 1.7e+3 23.47 19.71\nWECHSELTFR-GPT2 2.3e+3 23.45 19.70\nGermanWECHSEL-GPT2 3.7e+3 34.35 26.80\nWECHSELTFR-GPT2 5.0e+3 34.46 26.82\nChineseWECHSEL-GPT2 2.4e+4 71.02 51.97\nWECHSELTFR-GPT2 2.5e+4 72.11 52.07\nSwahili WECHSEL-GPT2 1.4e+5 13.02 10.14\nWECHSELTFR-GPT2 1.5e+5 13.03 10.06\nTable 9: Results of training WECHSEL TFR GPT2 models. We report Perplexity before training ( PPL@0), after\n10% of steps (PPL@25k) and after training (PPL@250k).\nLang Model Score@0 Score@25k Score@250k\nNLI NER Avg NLI NER Avg NLI NER Avg\nFrench WECHSEL-RoBERTa 78.25 86.93 82.59 81.63 90.26 85.95 82.43 90.88 86.65\nWECHSELTFR-RoBERTa 78.25 87.43 82.84 81.86 90.07 85.96 82.55 90.80 86.68\nGerman WECHSEL-RoBERTa 75.64 84.53 80.08 81.11 89.05 85.08 81.79 89.72 85.76\nWECHSELTFR-RoBERTa 77.00 84.70 80.85 80.71 89.09 84.90 82.04 89.72 85.88\nChinese WECHSEL-RoBERTa 63.23 72.79 68.01 77.19 79.07 78.13 78.32 80.55 79.44\nWECHSELTFR-RoBERTa 62.75 72.87 67.81 77.07 78.03 77.55 77.99 80.65 79.32\nSwahili WECHSEL-RoBERTa 60.28 74.38 67.33 73.87 87.63 80.75 75.05 87.39 81.22\nWECHSELTFR-RoBERTa 60.14 75.42 67.78 74.04 87.79 80.92 74.58 87.66 81.12\nTable 10: Results from ﬁne-tuning WECHSELTFR-RoBERTa models. Results shown equivalently as in Table 2.\nBest PPL\nModel Subsample Size 16MiB 64MiB 256MiB 1024MiB\nWECHSEL-GPT2 (original fastText embeddings) 78.33 44.75 31.63 24.66\nWECHSEL-GPT2 (fastText embeddings trained on subsample) 97.42 49.50 32.88 24.75\nFullRand-GPT2 281.46 83.43 43.08 27.09\nTransInner-GPT2 216.37 77.71 35.27 25.15\nTable 11: Sensitivity Analysis w. r. t. the amount of training data on transfer to French. We train models on ran-\ndom subsamples of 16MiB, 64MiB, 256MiB and 1024MiB of the original training data, and evaluate on the same\nheld-out set. For WECHSEL-GPT2, we train two models. One using the original, publicly available fastText em-\nbeddings trained on Common Crawl data. The other using fastText embeddings trained only on the corresponding\nsubsample of text."
}