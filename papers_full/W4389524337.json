{
  "title": "Ask Language Model to Clean Your Noisy Translation Data",
  "url": "https://openalex.org/W4389524337",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093126631",
      "name": "Quinten Bolding",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5076808820",
      "name": "Baohao Liao",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5043274029",
      "name": "Brandon Denis",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5016769551",
      "name": "Jun Luo",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5109059955",
      "name": "Christof Monz",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3082674894",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4385570982",
    "https://openalex.org/W4296878660",
    "https://openalex.org/W2963697731",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2065427498",
    "https://openalex.org/W2026498605",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W4361192999",
    "https://openalex.org/W2984051011",
    "https://openalex.org/W2902009742",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W4310145190",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W3119872155",
    "https://openalex.org/W2773493195",
    "https://openalex.org/W3176923149",
    "https://openalex.org/W3202612196",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W3174036215",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W2123241698",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4322716276",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2034190452",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2964247056",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W3101706601",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W3199143471",
    "https://openalex.org/W3115333065",
    "https://openalex.org/W4320167623"
  ],
  "abstract": "TTransformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise in the target sentences while preserving the semantic integrity of the original sentences. Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT showcased its effectiveness in evaluating the robustness of NMT models, highlighting the potential of advanced language models for data cleaning and emphasizing C-MTNT as a valuable resource.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3215‚Äì3236\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nAsk Language Model to Clean Your Noisy Translation Data\nQuinten Bolding ‚àó Baohao Liao\nBrandon James Denis Jun Luo Christof Monz\nLanguage Technology Lab, University of Amsterdam\nHuawei Amsterdam Research Center\nquinten.bolding@gmail.com b.liao@uva.nl\nAbstract\nTransformer models have demonstrated remark-\nable performance in neural machine translation\n(NMT). However, their vulnerability to noisy\ninput poses a significant challenge in practical\nimplementation, where generating clean output\nfrom noisy input is crucial. The MTNT dataset\n(Michel and Neubig, 2018) is widely used as\na benchmark for evaluating the robustness of\nNMT models against noisy input. Neverthe-\nless, its utility is limited due to the presence of\nnoise in both the source and target sentences.\nTo address this limitation, we focus on cleaning\nthe noise from the target sentences in MTNT,\nmaking it more suitable as a benchmark for\nnoise evaluation. Leveraging the capabilities\nof large language models (LLMs), we observe\ntheir impressive abilities in noise removal. For\nexample, they can remove emojis while consid-\nering their semantic meaning. Additionally, we\nshow that LLM can effectively rephrase slang,\njargon, and profanities. The resulting datasets,\ncalled C-MTNT, exhibit significantly less noise\nin the target sentences while preserving the se-\nmantic integrity of the original sentences. Our\nhuman and GPT-4 evaluations also lead to a\nconsistent conclusion that LLM performs well\non this task. Lastly, experiments on C-MTNT\nshowcased its effectiveness in evaluating the\nrobustness of NMT models, highlighting the\npotential of advanced language models for data\ncleaning and emphasizing C-MTNT as a valu-\nable resource.1\n1 Introduction\nNeural machine translation (NMT) has witnessed\nsignificant progress (Bapna et al., 2018; Hieber\net al., 2020; Liao et al., 2021) in recent years,\nparticularly with the introduction of Transformer\n(Vaswani et al., 2017). Despite their impressive\nperformance on clean benchmarks (Barrault et al.,\n2019; Huang et al., 2020; Liao et al., 2020), these\n‚àó Work done while doing this master thesis in Huawei.\n1Up-to-date version at https://arxiv.org/abs/2310.13469.\nmodels exhibit a noticeable decline in translation\nquality when exposed to noisy input. This ham-\npers their performance in real-world scenarios,\nwhere human users unintentionally introduce mis-\nspellings and grammatical errors during text input\n(Karpukhin et al., 2019). Therefore, evaluating the\nrobustness of NMT models against noisy inputs\nbecomes crucial before their deployment.\nDespite the importance of assessing the re-\nsilience of NMT models against noise, the available\nevaluation datasets remain limited. To the best of\nour knowledge, MTNT (Michel and Neubig, 2018)\nstands as one of the few well-established resources\nfor evaluating NMT models‚Äô performance in the\npresence of noise. The noise distribution in MTNT\nclosely resembles real-world use cases, but its ap-\nplicability is constrained by the presence of noise\nin the target sentences. For instance, a French-\nEnglish pair may appear as: ‚ÄúREEEEEEEEEEEE\nles normies sur mon eiffel y‚Äôen a marre‚Äù ‚Üî\n‚ÄúREEEEEEE bored of the normies on my eiffel‚Äù,\nwhich is not desirable. Our expectation is that an\neffective NMT model is capable of translating a\nnoisy source sentence into a clean target sentence.\nIn order to enhance the applicability of MTNT\nfor evaluating NMT models, we propose clean-\ning the target side of this dataset. In contrast to\nconventional cleaning approaches (Xu and Koehn,\n2017; Khayrallah et al., 2018; Koehn et al., 2020)\nthat typically involve filtering out undesirable sen-\ntences and retaining only high-quality ones, we aim\nto remove noise from sentences without reducing\nthe overall sample number. Language-aware rule-\nbased approaches, which rely on predefined rules\nto eliminate noise, have been widely employed as\na common method for such cleaning (Miao et al.,\n2021; Mavrogiorgos et al., 2022). While these\nmethods can effectively remove certain types of\nnoise, it becomes impractical to define rules for\nevery possible noise source. Moreover, some natu-\nral noise introduced by human input is not easily\n3215\nidentified by rule-based approaches alone.\nAnother highly promising approach involves\nleveraging large language models (LLMs) (Tou-\nvron et al., 2023a; Chen et al., 2023). Pre-\nvious works have already demonstrated the ef-\nfectiveness of LLMs in various tasks, including\nQ&A(Robinson et al., 2022), text summarization\n(Pilault et al., 2020), and data generation (Meng\net al., 2022; Chung et al., 2023; Tang et al., 2023).\nHowever, applying an LLM to clean the target sen-\ntences poses several challenges that need to be ad-\ndressed diligently:\n‚Ä¢ Comprehensive noise removal: LLMs should\nbe capable of thoroughly cleaning the target\nsentence by eliminating all forms of noise, in-\ncluding removing semantically meaningless\nemojis, translating emojis with semantic con-\ntent into words, correcting misspellings, etc.\n‚Ä¢ Semantic preservation: A cleaned target sen-\ntence should retain similar semantic informa-\ntion as the original noisy target sentence.\n‚Ä¢ Alignment with the source sentence: The\ncleaned target sentence should convey the\nsame intended meaning as the noisy source\nsentence, ensuring accurate translation and\nfaithful representation of the original content.\nIn this paper, we propose to apply GPT-3.5 (Ope-\nnAI, 2021) to clean the noisy target sentences in the\nMTNT dataset. Our approach addresses the chal-\nlenges of noise removal, semantic preservation, and\nalignment with the source sentence. Inspired by the\nsuccess of prompting methods (Brown et al., 2020;\nWei et al., 2022; Schick et al., 2023), we design\nfew-shot prompts to guide GPT-3.5 to clean the\nnoisy target sentences in three ways: (1) Utilizing\ninformation from both the source and target side for\ncleaning, (2) Cleaning the target sentence indepen-\ndently, and (3) Generating a clean target sentence\nby translating the noisy source sentence. Through\na comprehensive analysis, we demonstrate the ef-\nfectiveness of our cleaning methods, particularly\nwith methods (1) and (3). These methods success-\nfully remove emojis, while considering their seman-\ntic meaning, and also rephrase slang, jargon, and\nprofanities appropriately. The resulting datasets,\nnamed C-MTNT, exhibit significantly reduced lev-\nels of noise compared to the original sentences,\nwhile still preserving their semantic integrity.\nNoise Type ExampleSpelling/ typographical errors ‚Äúacross‚Äù‚Üí‚Äúaccross‚Äù, ‚Äúreceive‚Äù‚Üí‚Äúrecieve‚ÄùWord omission/ insertion/ repetition ‚ÄúI never‚Äù‚Üí‚ÄúI never never‚ÄùGrammatical errors ‚Äúa ton of‚Äù‚Üí‚Äúa tons of‚ÄùSpoken language ‚Äúwant to‚Äù‚Üí‚Äúwanna‚Äù, ‚Äúgoing to‚Äù‚Üí‚Äúgonna‚ÄùInternet slang ‚Äúto be honest‚Äù‚Üí‚Äútbh‚Äù, ‚Äúshaking my head‚Äù‚Üí‚Äúsmh‚ÄùCapitalization ‚ÄúReddit‚Äù ‚Üí‚Äúreddit‚ÄùDialects African American Vernacular English, ScottishCode switching ‚ÄúThis is so cute‚Äù‚Üí‚ÄúThis is so kawaii‚ÄùJargon On Reddit: ‚Äúupvote‚Äù, ‚Äúdownvote‚Äù, ‚Äúsub‚Äù, ‚Äúgild‚ÄùProfanities/slurs (sometimes masked) ‚Äúf*ck‚Äù, ‚Äúsh*‚Äù\nTable 1: Noise types and examples from MTNT.\nFurthermore, our research highlights another re-\nmarkable potential of LLMs in generating high-\nquality parallel data with limited monolingual re-\nsources. This finding has significant implications\nfor low-resource domains and languages, where\nacquiring parallel corpora is often challenging.\nAfter the cleaning, we conduct human and GPT-\n4 (OpenAI, 2023) evaluations. Both evaluations\ndraw the same conclusion that LLM has a great\ncapability for such a cleaning task, especially with\nmethod (1).\nIn the end, we conduct comprehensive experi-\nments to train NMT models on some noisy training\ndatasets, and evaluate them on different evaluation\nsets, including clean benchmarks and the bench-\nmark constructed with a rule-based noise removal\nmethod. C-MTNT consistently demonstrates better\nnoise evaluation manner on the NMT models.\nTo the best of our knowledge, this is the first\nstudy to apply LLMs in the context of cleaning\ndata, specifically addressing the challenges out-\nlined above. Our findings highlight the potential\nof leveraging advanced language models for data-\ncleaning tasks and emphasize C-MTNT as a valu-\nable resource for evaluating the NMT model in\nreal-world scenarios.\n2 Data Generation and Cleaning\nThe primary objective of this study is to harness the\ncapabilities of LLMs in effectively removing noise\nand generating parallel language datasets to eval-\nuate the robustness of NMT models against noisy\ninput. In this section, we will give an overview of\nour data source MTNT and delineate our approach\nto LLM interaction.\n2.1 MTNT Dataset\nMTNT (Michel and Neubig, 2018), which incor-\nporates noisy comments gathered from Reddit\nalongside professionally sourced translations, has\nemerged as a benchmark for assessing the perfor-\nmance of NMT models when exposed to noisy\n3216\ninput. This dataset was developed in response to\nthe scarcity of publicly available parallel corpora\ncontaining naturally occurring noise. It encom-\npasses three distinct languages, namely English,\nFrench, and Japanese, and offers parallel data for\ntwo language pairs: English-French and English-\nJapanese. As shown in Table 1, we present a com-\nprehensive analysis of noisy types within MTNT.\nNotably, these noise types manifest in both source\nand target sentences, which is not expected since\nwe want to evaluate the ability of an NMT model\nto translate a noisy source sentence into a clean\ntarget sentence. Consequently, here we devised an\napproach aimed at cleaning the target sentences in\nMTNT to enhance its assessment capability.\n2.2 Approach\nOur approach to cleaning MTNT entails meticu-\nlous consideration of various available settings and\nresources. It includes the assessment of its effec-\ntiveness in scenarios where bilingual resources are\naccessible, as well as the investigation of its feasi-\nbility in cases where only source or target data is\navailable. To explore the capabilities of LLM, we\nincorporate three methods specifically tailored to\ndifferent data scenarios, accounting for the varying\navailability of language resources.\n‚Ä¢ Bilingual cleaning: This approach involves\nproviding both noisy source and target sam-\nples as input, with the focus on cleaning the\ntarget sample while preserving alignment with\nthe source sample.\n‚Ä¢ Monolingual cleaning: In this approach, a\nnoisy target sample is given as input, and a\nclean target sample is generated as output. It\ndemonstrates the ability of LLM to clean sen-\ntences without relying on the original source\nsample that may contain excessive noise.\n‚Ä¢ Translation: This method generates new par-\nallel data by taking a noisy source sample\nas input and producing a clean target sample\nas output. It showcases LLM‚Äôs capability of\nnoise ignorance.\nChain-of-thought prompting.Inspired by the\nrecent achievements of prompting methods (Brown\net al., 2020; Schick et al., 2023), we craft a set\nof few-shot examples that incorporate a coherent\nchain of thought (Wei et al., 2022). These exam-\nples serve to facilitate the model‚Äôs comprehension\nContext: You'll receive a sentence from Reddit, a \nsocial news and discussion website. The \nsentence may contain various types of noise \nlike spelling errors, slang, emojis, and \nprofanities.\nBilingual Task: You are given two input sentences: \nthe {src} sentence with noise and its \ntranslated {tgt} version also containing noise, \nyour task is to eliminate noise such as weird \ncharacters, incorrect capitalization, emoji's, \nand spelling/grammatical errors from the {tgt} \nsentence. Return the cleaned {tgt} sentence as \nthe output.\nOutput Format: The desired output format for the \nclean {tgt} sentence is enclosed in quotation \nmarks. It can be represented by the following \nregular expression: ^\"(.+?)\"$\nExample: Here are some examples of inputs and \ntheir desired outputs, along with the reasoning \nfor each pair:\nInputs: ‚ÄúI'm sooooo happyyyy!! üòÑ\n\", \"Je suis \ntellement heureuxxxx!! üòÑ\n‚Äù\nDesired output: ‚ÄúJe suis tellement heureux!‚Äù\nReasoning: The noisy French sentence contains \nexcessive letters in the word ‚Äúheureux‚Äù, an \nemoji and an unnecessary exclamation mark. The \nclean French sentence removes the extra \nletters, emoji and the exclamation mark to \nconvey the same message accurately.\nRequest: These are the inputs {src_sent}, \n{tgt_sent}. Please return the desired output in \nthe correct format.\nFigure 1: Prompt design for bilingual cleaning. In prac-\ntice, the full prompt includes four examples. {src} and\n{tgt} indicate the source and target languages, respec-\ntively. Important parts are highlighted. Full examples,\ntask descriptions and requests for all three proposed\nmethods are in Appendix A, B and C. The other API\ninputs stay the same.\nof diverse inputs and their corresponding handling\nstrategies. Based on the optimal performance of\nfour-shot examples (Brown et al., 2020), we man-\nually curate a collection of four-shot examples for\neach method. The full list of examples used in our\napproach can be found in Appendix A.\nPrompt design. As shown in Figure 1, each\nprompt consists of multiple components. The call\nfollows a specific layout, starting with a brief de-\nscription of the context, and providing essential\ninformation about the samples, domain, and lan-\nguage. This is followed by a concise formulation\nof the method: (1) Bilingual cleaning of target sam-\nples; (2) Monolingual cleaning of language sam-\nples; (3) Data generation through translation. Next,\n3217\nwe present a framework for the desired output, fol-\nlowed by the few-shot examples for each method,\nwhich include the input example, chain-of-thought\nreasoning for the output, and the example output\nitself. Finally, we insert the input sample(s) and\nrequest the desired output.\nLanguage model.The prompts are utilized to\ninteract with OpenAI‚Äôs GPT through API calls.\nSpecifically, we use the original GPT-3.5 (text-\ndavinci-003) variant (OpenAI, 2021). In this way,\nwe want to show that some publicly released pre-\ntrained LLMs, like Llama 2 (Touvron et al., 2023b),\nmight also have this ability.\nSemantic similarity.To ensure that the text gen-\nerated by our approach maintains the original mean-\ning without unintentional hallucinations (M√ºndler\net al., 2023), we set a threshold based on LASER\n(Artetxe and Schwenk, 2019). LASER is language\nagnostic, allowing us to compare samples not only\nwithin the same language but also across different\nlanguages. We measure the cosine similarity be-\ntween the representations of original and cleaned\nsentences, as shown in Equation 1. (For bilingual\nand monolingual cleaning, the original sentence\nis the noisy target sentence. For translation, the\noriginal sentence is the noisy source sentence.)\nsim(e1,e2) = e1 ¬∑e2\n‚à•e1‚à•2 ‚à•e2‚à•2\n(1)\nwhere e1 and e2 are representations of original and\nnewly generated sentences, respectively. Based on\nprevious work on sentence embedding similarity\n(Okazaki and Tsujii, 2010; Xia et al., 2015), we\nset a threshold for the LASER score as 0.7. This\nthreshold is selected to strike a balance between\npreserving the meaning of sentences and allowing\nsufficient variations. If sim(e1,e2) < 0.7, we re-\npeat the API call but include a notice in the request:\n‚ÄúPlease ensure that your response accurately reflects\nthe meaning of the original input sentence.‚Äù, to en-\nsure that the meaning of the new sentence aligns\nclosely with the original one. This process contin-\nues until sim(e1,e2) ‚â•0.7 or reaching the maxi-\nmum number of iterations of 10.\n3 Analysis on C-MTNT\nIn this section, we analyze the generated data to\nevaluate its quality and suitability as a noise eval-\nuation benchmark. We compare our method to a\nrule-based baseline and quantitatively assess the\nlevel of noise present in the target sentences. In\naddition, we also compare the new target samples\nfrom C-MTNT to the original ones by evaluating\ntheir semantic similarity.\n3.1 Baseline\nIn addition to our LLM approach, we utilize the\nlanguage_tool_python module2, which is an open-\nsource grammar tool used for spelling, grammar,\nand overall language correction. With this rule-\nbased baseline, we want to determine the perfor-\nmance gap between the rule-based method and our\nLLM-based approaches.\n3.2 Quantitative Noise Measurement\nWe focus on several quantifiable noise types to\nmeasure the amount of noise in a set of samples\nand obtain an objective overview. These types are\npresent in both the source and target sentences of\nMTNT, including spelling and grammatical errors,\nemojis, internet slang, and profanities.\nWe apply the language_tool_python toolkit2 to\nmeasure the misspellings and grammatical errors.\nTo count the occurrences of emojis in the sentences,\nwe use the emoji library3. For detecting profanities\nwithin the text, we employ better_profanity4 for En-\nglish profanities, and profanity_check5 for French\nand Japanese profanities. As there are no available\nlibraries for detecting internet slang, we compile\nlists of popular internet slang for each language\nfrom the past ten years.\nAs shown in Table 2, we contend that the LLM\nmethods possess the capability to simulate natural\nlanguage as it appears in clean benchmarks such as\nNewstest20146, TED (Pryzant et al., 2018), KFTT\n(Neubig, 2011), and JESC (Cettolo et al., 2012),\nthereby generating clean target sentences. While\nconventional language correction tools excel in rec-\ntifying spelling and grammatical errors, they are\ninadequate in effectively eliminating or paraphras-\ning slang, profanities, or emojis. Conversely, the\nLLM methods demonstrate proficiency in address-\ning such language phenomena, as also evidenced\nby some samples in Appendix D. As a result, the\ntarget sentences in C-MTNT exhibit significantly\nless noise compared to MTNT, leveling the cleanli-\nness of the reference benchmarks.\n2https://github.com/jxmorris12/language_tool_python\n3https://github.com/carpedm20/emoji\n4https://github.com/snguyenthanh/better_profanity\n5https://github.com/vzhou842/profanity-check\n6http://www.statmt.org/wmt15/test.tgz\n3218\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nCorrection-tool Monolingual Bilingual Translation Correction-tool Monolingual Bilingual Translation Correction-tool Monolingual Bilingual Translation\nFR-EN EN-FR EN-JA\nLASER similarity Jaro Winkler Rouge-1 Jaccard BLEU\nFigure 2: Semantic similarity between noisy (originally from MTNT) and cleaned English, French, and Japanese\nsentences. The detailed numbers for each bar plot are in Appendix E.\nLang. Eval. Set Spell./Gram. Emojis Slang Profanities\nEN\nNewstest2014 0.415 0.000 0.571 0.173\nMTNT 1.712 0.031 0.816 0.616\nCorrection-tool0.112 0.031 0.818 0.618\nBilingual 0.687 0.000 0.584 0.533\nTranslation 0.798 0.019 0.721 0.509\nMonolingual 0.748 0.019 0.716 0.399\nFR\nNewstest2014 2.878 0.000 0.133 0.431\nMTNT 7.125 0.227 2.225 5.522\nCorrection-tool0.100 0.227 2.139 5.508\nBilingual 0.552 0.016 0.691 0.535\nTranslation 0.950 0.048 0.707 0.545\nMonolingual 0.455 0.000 0.715 0.551\nJA\nTED 0.049 0.000 3.493 3.879\nKFTT 0.011 0.000 0.486 4.269\nJESC 0.036 0.103 7.881 12.084\nMTNT 0.051 0.051 0.794 5.682\nCorrection-tool0.000 0.051 0.794 5.684\nBilingual 0.052 0.012 1.033 5.783\nTranslation 0.053 0.041 1.119 5.873\nMonolingual 0.041 0.006 0.991 5.874\nTable 2: Noise frequency per 100 tokens in the target\nsentences of the evaluation sets. The best scoresare\nhighlighted. Results of Newstest2014, TED, KFTT, and\nJESC only serve as baselines from clean benchmarks.\nNotable is the lower performance in the gener-\nated Japanese target sentences. We attribute this\nto two factors: insufficient capture of slang and\nprofanities, and the known variations in perfor-\nmance of GPT-4 (OpenAI, 2023) across differ-\nent languages (Koubaa, 2023). GPT-4 performs\nmuch worse on Japanese tasks compared to En-\nglish tasks. A similar performance discrepancy is\nexpected with GPT-3.5 (OpenAI, 2021).\n3.3 Meaning Preservation\nOur second objective is to preserve the original\nmeaning during cleaning. We apply multiple met-\nrics to measure the sentence similarity, including\nLASER (Artetxe and Schwenk, 2019), BLEU score\n(Papineni et al., 2002), Rouge-1 score (Lin, 2004),\nJaro Winkler distance (Jaro, 1989), and Jaccard\nscore (Hamers et al., 1989).\nFigure 2 illustrates similarity scores across dif-\nferent language pairs, revealing distinct deviations\namong different methods. These deviations stem\nfrom different input data: bilingual, monolingual\ntarget, and monolingual source. The translation\nmethod exhibits the lowest similarity score between\noriginal and clean sentences. In contrast, the mono-\nlingual method shows the minimal deviation be-\ntween original and clean sentences, while the bilin-\ngual method falls in between. We argue that the\nlarger deviation from the bilingual and translation\nis mainly from rephrasing and word reordering (see\nAppendix D for detailed samples). Despite these\nvariations, all methods retain a substantial portion\nof the original semantic structure.\nNotably, similarity scores from the correction\ntool are the highest for all metrics among all meth-\nods, since this rule-based method can only clean\nor remove noise but lacks the ability to rephrase\nchallenging noise types like slang, emojis, and pro-\nfanities (see Table 2 for its results on slang, profani-\nties, and emojis). Most cleaned sentences stay very\nsimilar to the original noisy ones. Complemented\nby the findings in Section 3.2, our cleaning meth-\nods show their impressive ability in reducing noise\nwhile preserving semantic similarity.\n3.4 Human and GPT-4 Evaluations\nApart from evaluating C-MTNT by measuring its\nnoise amount and its semantic preservation, here\nwe conduct human and GPT-4 (OpenAI, 2023) eval-\nuations.\nDue to the limited research budget, we only con-\nducted the human evaluation with the help of the\nfirst four authors of this paper on some sampled\nsentences instead of all sentences. 100 sentences\nfrom C-MTNT Fr ‚ÜíEn are sampled to generate\nthree files. These three files are about binary com-\nparisons of bilingual vs. monolingual, bilingual\nvs. translation, and monolingual vs. translation.\nThe order of sentences, including their indexes\n3219\nMonolingual\nBilingual\nBilingual\n27.0% 39.5% 33.5%\n32.2% 38.0% 29.8%\n30.0% 46.8% 23.2%\nTranslation\nTranslation\nMonolingual\n35.0% 21.0% 44.0%\n40.0% 26.0% 34.0%\n34.0% 37.0% 29.0%\nLeft is better Tie Right is better\n(a) Human and GPT-4 evaluations on 100 sampled Fr‚ÜíEn pairs. Left: human valuation. Right: GPT-4 evaluation.\nMonolingual\nBilingual\nBilingual\n31.3% 16.8% 51.9%\n45.8% 21.2% 33.0%\n42.3% 32.0% 25.7%\nTranslation\nTranslation\nMonolingual\n35.4% 15.3% 49.3%\n40.2% 22.8% 37.0%\n35.5% 35.7% 28.8%\n(b) GPT-4 evaluation on all samples. Left: En‚ÜíFr. Right: Fr‚ÜíEn.\nFigure 3: Human and GPT-4 evaluations on C-MTNT. Overall, the Bilingual cleaning method results in the most\ncleaned target sentences.\nand which cleaning method comes first, is ran-\ndomly shuffled. There is no chance for the an-\nnotator to guess which sentence corresponds to\nwhich method, and which file corresponds to the\ncomparison between which two methods. Notably,\nwe prefer binary comparison to ranking over three\nmethods, since it‚Äôs easier for human annotators. In\naddition, the cleaned sentences from the correction\ntool are excluded, since they are too easy to be\nbeaten. Four annotators are asked to give their pref-\nerences for each comparison, based on our three\ncriteria of comprehensive noise removal, semantic\npreservation, and alignment with the source sen-\ntence. If both sentences show a similar level of\nnoise, they are asked to give a ‚ÄúTie‚Äù.\nWe also prompt GPT-4 (See Appendix G for the\nprompt) on the same sampled sentences to check\nwhether GPT-4 draws a similar conclusion. As\nshown in Figure 3a, human and GPT-4 evaluations\nshare a similar preference: bilingual > translation\n> monolingual. Compared to GPT-4, human anno-\ntators prefer to vote for \"Tie\". We argue the main\nreason is that most cleaned sentences are very simi-\nlar with a low level of noise, which further justifies\nthe effectiveness of our proposed methods. Since\nhuman annotators and GPT-4 share a similar pref-\nerence, we further evaluate all C-MTNT sentences\nwith GPT-4 in Figure 3b. The conclusion is similar\nto the above discussion, i.e. bilingual > translation\n> monolingual.\n4 Machine Translation Experiments\nIn this section, we further investigate the suitabil-\nity of C-MTNT as a benchmark to evaluate NMT\nmodel‚Äôs robustness against noisy input. Let‚Äôs re-\nemphasize our expected NMT model: Irrespective\nof whether the source sentence is clean or noisy,\nthe model has the ability to generate a coherent\ntarget sentence that is free of errors or distortions.\nWe first mimic the real-world scenario to train a\nset of NMT models on datasets that contain both\nnoisy and clean source sentences but with only\nclean target sentences, then evaluate these models\non C-MTNT and other benchmarks.\n4.1 Model and Training Details\nAll models are trained with the fairseq toolkit (Ott\net al., 2019). The architecture is based on the\nvanilla transformer, with 6 encoder and 6 decoder\nlayers. The hidden dimension is set to 512, with\n1024 for the feed-froward intermediate output. We\nuse Adam optimizer (Kingma and Ba, 2015) with\nits default hyperparameters. Dropout with a prob-\nability of 0.3 and a label smoothing factor of 0.1\n(Gao et al., 2020) is applied. We train all models\nfor 20 epochs with a learning rate of 5e‚àí4 that is\nscheduled by inverse square root with 4K warmup\nsteps, and set a batch size as 128. Subword tok-\nenization is performed using SentencePiece (Kudo\nand Richardson, 2018) with BPE subwords. For all\nlanguages, we use a vocabulary size of 16K without\nsharing embeddings.\n4.2 Training Data\nFor the English ‚ÜîFrench translation directions,\nwe utilize the same training data as Michel and\nNeubig (2018), which comprises the europarl-v77\n7http://www.statmt.org/europarl/\n3220\n0.0%\n2.0%\n4.0%\n6.0%\n8.0%\n10.0%\n12.0%\n14.0%\n0.05 0.1 0.2 0.3 0.4 0.5 0.6\nPerformace Gain (%)\nŒ±parameter\nNon-Contrastive Character Contrastive Character\nNon contrative Error Aug. Contrastive Error Aug.\nFigure 4: Performance vs. augmentation strength, Œ±, for\nmodels trained with different augmentation strategies\non the French-to-English translation direction. Scores\nare computed on the bilingual C-MTNT evaluation set.\nand news-commentaryv108 corpora. The training\nset consisted of 2.2M samples, with 55M French\ntokens and 52M English tokens (non-tokenized).\nThe WMT15 newsdiscussdev20159 serves as the\nvalidation set, used to select the best checkpoint.\nThe trained models are evaluated on the C-MTNT,\nMTNT, and newstest20146 test (eval.) sets.\nRegarding the English-to-Japanese translation\ndirection, we follow the data construction approach\nof Michel and Neubig (2018), combining the train-\ning and validation data from the KFTT (Neubig,\n2011), JESC (Pryzant et al., 2018), and TED talks\n(Cettolo et al., 2012) corpora. The Japanese seg-\nments in each dataset are tokenized using the\nMeCab library (Kudo, 2005). This results in a\ntraining set of 3.9M samples, consisting of 35M\nEnglish tokens without tokenization. We use the\ntraining and dev sets associated with each corpus\nas the training and validation sets, respectively, and\nevaluate the models on MTNT, C-MTNT, and the\nrespective test set from each corpus.\n4.3 Data Augmentation\nThe above-mentioned training data are clean, con-\ntain negligible noise, and can‚Äôt resemble the real-\nworld use case. Therefore, we introduce noise with\ndifferent augmentation methods to the source sen-\ntences, including character augmentation (spelling/-\ntypographical errors, capitalization), contextual\nword embedding augmentation (word omission/in-\nsertion/repetition), MTNT-based error replacement\n(spoken language, jargon, internet slang, grammati-\n8http://www.statmt.org/wmt15/training-parallel-nc-\nv10.tgz\n9http://www.statmt.org/wmt15/dev-v2.tgz\ncal errors), and synonym substitution (grammatical\nerrors).\nCharacter augmentation (Char.) involves\ncharacter-level augmentation methods, including\nrandom or controlled techniques with a probability\nof 0.5 for each choice (Karpukhin et al., 2019).\nContextual word embedding augmentation\n(Con.) utilizes language models, specifically BERT\n(bert-base-uncased) (Devlin et al., 2019), to substi-\ntute some word embeddings with their contextual\nword embeddings (Kobayashi, 2018). We employ\nthe French BERT 10 for French source sentences.\nMTNT-based error replacement (Err.) is\ninspired by the symbolic strategies (Shorten\net al., 2021). Errors are identified with lan-\nguage_tool_python, and only the most valuable\nones occurring more than once are retained in a dic-\ntionary for augmentation. By replacing the correct\nforms with the mapped common errors, we inten-\ntionally introduce these errors into the sentence.\nFor synonym substitution (Syn.), we employ\nWordNet (Miller, 1995) and NLTK (Bird et al.,\n2009), to randomly select and replace words with\ntheir synonyms.\nThese techniques are only tailored for English\nand French, which is also the main reason for our\nexclusion of Japanese-to-English direction. The\nsource sentences x are augmented with a proba-\nbility of Œ±= 0.1, augmenting approximately 10%\nof the tokens in each sample. This process gen-\nerates four augmented versions: zch, zc, ze, and\nzs, representing sentence augmentation with char-\nacter, contextual word embedding, error, and syn-\nonym augmentation, respectively. The selection of\nŒ± = 0.1 is based on previous works (Karpukhin\net al., 2019; Wei and Zou, 2019) and our similar\nfinding (see Figure 4). These augmented sentences\nare combined with the original clean sentences, re-\nsulting in four new training sets, {x,zch}, {x,zc},\n{x,ze}, and {x,zs}. Each is used to train a model,\ncapable of handling some specific types of noise.\nNotably, the distribution of introduced noise is\nnot possible to totally resemble the noise distribu-\ntion in the MTNT (or C-MTNT) source sentences,\nsince we only introduce some types of noise to each\nnew set with a pre-defined rule, i.e. the augmenta-\ntion method. This setting is desired and makes the\nevaluation more general and practical.\n10https://github.com/stefan-it/europeana-bert\n3221\nNon-Contrastive Contrastive\nEval. Set Base. Char. Syn. Con. Err. Char. Syn. Con. Err.\nFR-EN\nNewstest2014 29.2 29.92.4 29.1-0.2 28.9-1.1 29.20.0 29.40.7 29.20.0 28.5-2.5 29.61.2\nMTNT 23.1 25.08.5 23.62.2 23.20.7 23.62.4 24.87.5 22.9-0.7 22.7-1.4 23.62.3\nCorrection tool 23.2 25.28.4 23.72.1 23.40.7 24.34.8 24.97.4 23.0-0.7 23.0-0.9 23.82.6\nBilingual 25.3 28.312.0 26.75.6 27.06.9 26.75.9 27.910.4 27.06.9 26.13.5 27.69.5\nTranslation 26.2 29.010.7 27.65.5 27.86.1 27.44.6 28.69.0 27.44.7 26.71.8 28.17.4\nMonolingual 21.4 23.710.8 21.71.8 21.50.8 21.82.2 23.07.6 20.9-1.9 20.8-2.7 21.71.5\nEN-FR\nNewstest2014 30.3 33.19.0 31.74.5 31.53.9 31.95.3 32.67.6 32.46.8 32.36.7 32.67.4\nMTNT 20.1 22.411.5 20.73.1 21.25.7 21.88.5 22.210.7 21.46.5 21.15.3 22.311.4\nCorrection tool 19.7 22.011.6 20.53.9 20.96.0 21.48.5 21.911.1 21.06.3 20.95.7 22.011.6\nBilingual 19.7 22.112.2 20.75.1 21.06.9 21.610.1 22.012.2 21.38.2 21.17.4 22.615.0\nTranslation 19.3 21.913.5 20.56.0 20.87.7 21.19.2 21.712.6 20.98.0 20.77.0 22.516.4\nMonolingual 16.6 18.511.3 17.23.9 17.66.3 17.98.0 18.511.5 17.55.8 17.55.6 18.813.4\nEN-JA\nTED 14.4 14.93.3 15.04.1 14.2-1.6 14.40.0 14.93.8 14.40.0 14.2-1.6 14.61.5\nKFTT 24.9 25.73.2 24.5-1.7 25.00.2 24.2-2.8 25.52.3 24.7-0.8 25.00.2 24.6-1.3\nJESC 15.2 15.0-1.4 14.8-2.4 14.9-1.9 15.0-1.1 15.1-0.6 14.8-2.9 14.9-1.9 14.9-1.7\nMTNT 8.8 9.02.4 9.13.5 9.02.5 9.01.7 9.02.6 9.12.8 9.02.6 8.90.8\nCorrection tool 8.8 9.02.5 9.13.6 9.02.6 8.90.6 9.12.7 9.12.8 9.02.6 8.90.9\nBilingual 12.6 14.011.3 13.910.1 13.35.5 13.13.7 14.111.7 13.78.3 13.35.5 13.46.7\nTranslation 14.6 16.110.2 15.98.8 15.56.1 15.77.3 16.412.1 16.110.5 15.56.1 15.77.8\nMonolingual 9.3 10.18.7 10.07.7 9.74.2 9.2-1.4 10.18.2 10.18.4 9.74.2 9.2-1.1\nTable 3: BLEU scores on various evaluation sets. The subscript number is the relative performance gain G(%),\ncalculated as Equation 2. The best and second-best Gfor each augmented dataset are highlighted and underlined,\nrespectively. Results of Newstest2014, TED, KFTT, and JESC only serve as baselines from clean benchmarks.\n4.4 Contrastive Learning\nIn addition to the straightforward training on newly\nconstructed sets, we also train models with con-\ntrastive learning, which is inspired by previous\nworks (Chen et al., 2020; Hwang et al., 2021; Hu\nand Li, 2022) that recognize the effectiveness of\ncontrastive learning in improving the robustness of\nNLP and NMT models. By employing this method,\nwe can analyze the performance of C-MTNT on\na wider range of models trained with different ap-\nproaches and settings.\nFor contrastive learning, the Transformer en-\ncoder takes both originalxand augmented zsource\nsentences as inputs and calculates the contrastive\nloss based on their output representations. Simi-\nlar to straightforward training, we train a separate\nmodel on each set with contrastive learning. More\ndetails and experiments on contrastive training are\nin Appendix F.\n4.5 Results and Analysis\nBefore introducing our results in Table 3 for clean\nbenchmarks, MTNT and C-MTNT, we first want\nto emphasize that the BLEU scores across differ-\nent benchmarks are incomparable because these\nbenchmarks contain different evaluation sentences.\nThough MTNT and C-MTNT contain the same\nsource sentences, their target sentences are differ-\nent since we apply LLM to clean the target side of\nMTNT. Therefore, we focus on the relative perfor-\nmance gain:\nG= (sr ‚àísb)/sb (2)\nwhere sb is the BLEU score from the model trained\nonly on the data without any augmentation, i.e. the\ntraining dataset in Section 4.2, and sr is the BLEU\nscore from the model trained on the augmented\ndataset. If a model trained with the augmented\ndataset obtains higher Gon an evaluation set, we\ncan say the evaluation set is an ideal noise evalu-\nation set. The reason is: The augmented dataset\nmimics our expected data distribution. I.e. noise\nonly exists in the source side. A model trained\non this dataset is supposed to have the ability to\ntranslate noisy source sentences into clean target\nsentences. If this model obtains a high G on an\nevaluation set, it shows that this evaluation set also\nfulfills the expected data distribution.\nAs shown in Table 3, all trained models tend\nto have significantly higher G for bilingual and\ntranslation C-MTNT. We also show the averageG\nover four models trained with different augmented\ndatasets in Figure 5. It is even more evident that\nbilingual and translation C-MTNT offers higher G\nacross all cleaning methods.\nSome may argue that C-MTNT achieves a higher\nG score because the augmented training dataset\nhas a similar distribution to C-MTNT, making it\n3222\n0.0%\n2.0%\n4.0%\n6.0%\n8.0%\n10.0%\n12.0%\nNon-Contrastive FR-EN Contrastive FR-EN Non-Contrastive EN-FR Contrastive EN-FR Non-Contrastive EN-JA Contrastive EN-JA\nMTNT Correction tool Monolingual Bilingual Translation\nFigure 5: Average relative performance gain Gover four models that are trained on different augmented datasets.\neasier to evaluate the trained models on C-MTNT.\nHowever, this argument is not valid for two rea-\nsons: (1) Bilingual and translation C-MTNT con-\nsistently offer higher Gacross all models trained\nwith different augmented datasets (see Table 3).\nIt‚Äôs almost impossible to intentionally make every\naugmented dataset has a similar distribution as C-\nMTNT where the source sentence contains natural\nnoise; (2) Monolingual C-MTNT offers lower G,\nsometimes even lower than MTNT and the bench-\nmark constructed from the correction tool. This\nshows that cleaning with a LLM doesn‚Äôt always\nwork. It‚Äôs better to have guidance, like guidance\nfrom a source sentence for the bilingual and trans-\nlation methods. According to our observation, if\nwe clean the noisy target sentence in a monolingual\nway without any guidance, LLM tends to introduce\nextra information or delete important information,\nwhich hurts translation because the cleaned target\nsentence doesn‚Äôt align well with the source sen-\ntence. In sum, C-MTNT generated by bilingual\nand translation methods shows its superiority as a\nnoise evaluation benchmark, encouraging a NMT\nmodel to translate a noisy source sentence to a\nclean target sentence.\n5 Related Work\nThe application of LLMs, such as GPT-3.5 (text-\ndavinci-003) (OpenAI, 2021), in downstream tasks\nhas garnered significant attention and led to exten-\nsive research in this field (Wang et al., 2022; Schick\net al., 2023). Researchers have conducted compre-\nhensive investigations to explore the capabilities\nof LLMs, and built upon their various applications\n(Coyne and Sakaguchi, 2023; M√ºndler et al., 2023;\nJiao et al., 2023).\nResearchers have also observed the potential of\nLLMs to generate high-quality data, leading to\na focus on expanding existing datasets, augment-\ning data, or generating entirely new data (Meng\net al., 2022; Yoo et al., 2021; Chung et al., 2023).\nThese efforts have helped address the issue of data\nscarcity in various domains.\nHowever, it is crucial to note that the aforemen-\ntioned research works lack at least one of the two\nnovel aspects addressed in this paper. Firstly, our\nresearch focuses on evaluating the robustness of\nNMT models to real-world noise introduced by hu-\nman users. Secondly, we explore the generation\nor cleaning of parallel data specifically for NMT\npurposes. These unique aspects of robustness eval-\nuation and parallel data generation/cleaning con-\ntribute to the existing literature in a novel way.\n6 Conclusion\nIn this work, we propose three methods to apply\nLLM to clean a noisy MT benchmark, MTNT,\nwhere natural noise exists in both source and tar-\nget sentences. We aim to clean the target side of\nMTNT to make it more suitable as a benchmark in\nevaluating NMT models‚Äô robustness against noisy\ninput. With a meticulous design of some few-shot\nprompts, we guide GPT to clean the noisy target\nsentence with only the noisy source sentence, only\nthe noisy target sentence, or both noisy source and\ntarget sentences. By measuring the noise frequency\nin the cleaned target sentences, measuring the se-\nmantic similarity between noisy and cleaned target\nsentences, and evaluating with human annotators\nand GPT-4, we show that our proposed methods\ncan effectively remove natural noise with LLM,\nwhile preserving the semantic structure. Our fur-\nther investigation of the newly created benchmark,\nC-MTNT, on some trained models also shows its\neffectiveness as a noise evaluation benchmark for\nNMT models.\n7 Limitations\nDespite the contributions and potential benefits\nof our research, there are several limitations that\n3223\nshould be acknowledged. Firstly, our approach\nrelies on the use of pre-trained LLMs for data\ncleaning and generation. While LLMs have shown\npromising capabilities, they are not immune to bi-\nases and limitations present in the training data.\nAs a result, our proposed dataset may still contain\nbiases similar to those found in the original MTNT\ndataset, even after our efforts to mitigate them.\nFurthermore, our assessment of the robustness of\nNMT models against noisy input relies on the uti-\nlization of C-MTNT, which is created using our pro-\nposed methodology, and MTNT. While C-MTNT\noffers valuable insights into the performance of\nNMT models, it is crucial to acknowledge that\nit may not comprehensively represent all poten-\ntial sources of noise encountered in real-world set-\ntings. Human-generated noise exhibits variability\nand contextual dependencies, and our dataset may\nnot encompass the entire spectrum of noise that\nNMT models may face during actual deployment.\nThe same can be said for MTNT.\nAdditionally, our research focuses on evaluating\nthe robustness of NMT models in specific language\ndirections, namely English ‚ÜîFrench and English\n‚ÜíJapanese. While these directions provide valu-\nable insights, generalizing the findings to other lan-\nguage pairs should be done with caution. Different\nlanguages may exhibit unique linguistic character-\nistics, which can influence the performance and\nrobustness of NMT models. Therefore, further re-\nsearch is needed to investigate the generalizability\nof our findings across a broader range of languages\nand translation directions.\nIn summary, while our research contributes to\nthe assessment of NMT model robustness and the\ngeneration of high-quality evaluation datasets, it is\nimportant to recognize the limitations associated\nwith biases in LLMs, the potential incompleteness\nof our dataset, and the need for further investigation\ninto different language pairs.\n8 Ethical Considerations\nThe utilization of pre-trained LLMs in natural lan-\nguage processing tasks, including data generation\nand machine translation, presents several ethical\nconsiderations that must be carefully examined. In\nthis section, we discuss the ethical implications as-\nsociated with the use of LLMs and highlight the\npotential biases that may arise in C-MTNT.\n8.1 Biases in Pre-trained Large Language\nModels\nPre-trained LLMs, such as GPT-3.5, are trained on\nvast amounts of internet text, which inevitably in-\ntroduces biases present in the training data. These\nbiases can manifest in different forms, including\nbut not limited to cultural, gender, racial, and politi-\ncal biases. The models can inadvertently reproduce\nand amplify these biases when generating new con-\ntent or translating text.\nIt is crucial to acknowledge that biases present in\nLLMs can influence the quality and fairness of the\ngenerated data, potentially perpetuating societal\ninequalities and reinforcing existing stereotypes.\nThe responsible use of LLMs requires diligent ex-\namination and mitigation of these biases to ensure\nequitable outcomes and avoid further marginaliza-\ntion or harm to underrepresented groups.\n8.2 Mitigating Biases in Data Generation\nWhile we employ LLMs for data cleaning and gen-\neration in our proposed dataset, it is essential to\nnote that biases similar to those in MTNT may be\npresent in the generated data. Despite efforts to\nmitigate biases, the LLMs may not fully capture\nthe complexities and nuances of language, leading\nto potential biases in the generated sentences.\nWe carefully evaluated the generated data for any\nbiased content and took steps to minimize biased\noutputs. Additionally, we encourage the involve-\nment of diverse annotators and domain experts dur-\ning the evaluation and curation of the dataset to\nensure a broader perspective and mitigate the in-\nfluence of individual biases. We also encourage\ntranslators and reviewers who are well-versed in\nthe target languages and cultural nuances to en-\nsure the translations accurately reflect the intended\nmeaning while avoiding biased or offensive con-\ntent. Moreover, we actively seek feedback from the\naffected communities and stakeholders to address\nany concerns and rectify biases that might arise.\nAcknowledgements\nWe thank all EMNLP reviewers for their great feed-\nback. The first author, Quinten Bolding, finished\nthis work while doing his thesis at Huwai Amster-\ndam Research Center. This work was supported\nby Huawei‚Äôs infrastructure. The thesis supervisor,\nBaohao Liao, is funded in part by the Netherlands\nOrganization for Scientific Research (NWO) under\nproject number VI.C.192.080.\n3224\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Trans. Assoc.\nComput. Linguistics, 7:597‚Äì610.\nAnkur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, Brus-\nsels, Belgium, October 31 - November 4, 2018, pages\n3028‚Äì3033. Association for Computational Linguis-\ntics.\nLo√Øc Barrault, Ondrej Bojar, Marta R. Costa-juss√†,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M√ºller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Confer-\nence on Machine Translation, WMT 2019, Florence,\nItaly, August 1-2, 2019 - Volume 2: Shared Task\nPapers, Day 1, pages 1‚Äì61. Association for Compu-\ntational Linguistics.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural Language Processing with Python. O‚ÄôReilly.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. WIT3: web inventory of transcribed and\ntranslated talks. In Proceedings of the 16th Annual\nconference of the European Association for Machine\nTranslation, EAMT 2012, Trento, Italy, May 28-30,\n2012, pages 261‚Äì268. European Association for Ma-\nchine Translation.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. CoRR,\nabs/2002.05709.\nZekai Chen, Mariann Micsinai Balan, and Kevin Brown.\n2023. Language models are few-shot learners for\nprognostic prediction. CoRR, abs/2302.12692.\nJohn Joon Young Chung, Ece Kamar, and Saleema\nAmershi. 2023. Increasing diversity while main-\ntaining accuracy: Text data generation with large\nlanguage models and human interventions.\nSteven Coyne and Keisuke Sakaguchi. 2023. An analy-\nsis of gpt-3‚Äôs performance in grammatical error cor-\nrection. CoRR, abs/2303.14342.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nYingbo Gao, Baohao Liao, and Hermann Ney. 2020.\nUnifying input and output smoothing in neural ma-\nchine translation. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\nCOLING 2020, Barcelona, Spain (Online), Decem-\nber 8-13, 2020, pages 4361‚Äì4372. International Com-\nmittee on Computational Linguistics.\nLieve Hamers, Yves Hemeryck, Guido Herweyers,\nMarc Janssen, Hans Keters, Ronald Rousseau, and\nAndr√© Vanhoutte. 1989. Similarity measures in scien-\ntometric research: The jaccard index versus salton‚Äôs\ncosine formula. Inf. Process. Manag., 25(3):315‚Äì\n318.\nFelix Hieber, Tobias Domhan, Michael J. Denkowski,\nand David Vilar. 2020. Sockeye 2: A toolkit for\nneural machine translation. pages 457‚Äì458.\nDongyang Hu and Junhui Li. 2022. Contrastive learn-\ning for robust neural machine translation with ASR\nerrors. In Natural Language Processing and Chi-\nnese Computing - 11th CCF International Confer-\nence, NLPCC 2022, Guilin, China, September 24-25,\n2022, Proceedings, Part I, volume 13551 of Lecture\nNotes in Computer Science, pages 81‚Äì91. Springer.\nXiao Shi Huang, Felipe P√©rez, Jimmy Ba, and Maksims\nV olkovs. 2020. Improving transformer optimization\nthrough better initialization. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 4475‚Äì4483. PMLR.\nYongkeun Hwang, Hyungu Yun, and Kyomin Jung.\n2021. Contrastive learning for context-aware neu-\nral machine translationusing coreference information.\narXiv preprint arXiv:2109.05712.\nMatthew A Jaro. 1989. Advances in record-linkage\nmethodology as applied to matching the 1985 census\nof tampa, florida. Journal of the American Statistical\nAssociation, 84(406):414‚Äì420.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt A\ngood translator? A preliminary study. CoRR,\nabs/2301.08745.\n3225\nVladimir Karpukhin, Omer Levy, Jacob Eisenstein, and\nMarjan Ghazvininejad. 2019. Training on synthetic\nnoise improves robustness to natural noise in machine\ntranslation. CoRR, abs/1902.01509.\nHuda Khayrallah, Hainan Xu, and Philipp Koehn. 2018.\nThe JHU parallel corpus filtering systems for WMT\n2018. In Proceedings of the Third Conference on Ma-\nchine Translation: Shared Task Papers, WMT 2018,\nBelgium, Brussels, October 31 - November 1, 2018,\npages 896‚Äì899. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic rela-\ntions. CoRR, abs/1805.06201.\nPhilipp Koehn, Vishrav Chaudhary, Ahmed El-Kishky,\nNaman Goyal, Peng-Jen Chen, and Francisco\nGuzm√°n. 2020. Findings of the WMT 2020 shared\ntask on parallel corpus filtering and alignment. In\nProceedings of the Fifth Conference on Machine\nTranslation, WMT@EMNLP 2020, Online, Novem-\nber 19-20, 2020, pages 726‚Äì742. Association for\nComputational Linguistics.\nAnis Koubaa. 2023. Gpt-4 vs. gpt-3.5: A concise show-\ndown.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66‚Äì71. Asso-\nciation for Computational Linguistics.\nTakumitsu Kudo. 2005. Mecab : Yet another part-of-\nspeech and morphological analyzer.\nBaohao Liao, Yingbo Gao, and Hermann Ney. 2020.\nMulti-agent mutual learning at sentence-level and\ntoken-level for neural machine translation. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, Online Event, 16-20 November 2020,\nvolume EMNLP 2020 of Findings of ACL, pages\n1715‚Äì1724. Association for Computational Linguis-\ntics.\nBaohao Liao, Shahram Khadivi, and Sanjika Hewavitha-\nrana. 2021. Back-translation for large-scale multilin-\ngual machine translation. In Proceedings of the Sixth\nConference on Machine Translation, WMT@EMNLP\n2021, Online Event, November 10-11, 2021, pages\n418‚Äì424. Association for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\nKonstantinos Mavrogiorgos, Argyro Mavrogiorgou,\nAthanasios Kiourtis, Nikolaos Zafeiropoulos, Spyri-\ndon Kleftakis, and Dimosthenis Kyriazis. 2022. Au-\ntomated rule-based data cleaning using NLP. In 32nd\nConference of Open Innovations Association, FRUCT\n2022, Tampere, Finland, November 9-11, 2022, pages\n162‚Äì168. IEEE.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nNeurIPS.\nZhengjie Miao, Yuliang Li, and Xiaolan Wang. 2021.\nRotom: A meta-learned data augmentation frame-\nwork for entity matching, data cleaning, text classifi-\ncation, and beyond. In SIGMOD ‚Äô21: International\nConference on Management of Data, Virtual Event,\nChina, June 20-25, 2021, pages 1303‚Äì1316. ACM.\nPaul Michel and Graham Neubig. 2018. MTNT: A\ntestbed for machine translation of noisy text. CoRR,\nabs/1809.00388.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39‚Äì41.\nNiels M√ºndler, Jingxuan He, Slobodan Jenko, and Mar-\ntin T. Vechev. 2023. Self-contradictory hallucinations\nof large language models: Evaluation, detection and\nmitigation. CoRR, abs/2305.15852.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nNaoaki Okazaki and Jun‚Äôichi Tsujii. 2010. Simple and\nefficient algorithm for approximate dictionary match-\ning. In COLING 2010, 23rd International Confer-\nence on Computational Linguistics, Proceedings of\nthe Conference, 23-27 August 2010, Beijing, China,\npages 851‚Äì859. Tsinghua University Press.\nOpenAI. 2021. GPT-3 API Documentation.\nhttps://beta.openai.com/docs/\napi-reference/introduction. Accessed\non: February 23, 2023.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Demonstrations, pages\n48‚Äì53. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311‚Äì318. ACL.\n3226\nJonathan Pilault, Raymond Li, Sandeep Subramanian,\nand Chris Pal. 2020. On extractive and abstractive\nneural document summarization with transformer lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 9308‚Äì9319. Association for Computa-\ntional Linguistics.\nReid Pryzant, Youngjoo Chung, Dan Jurafsky, and\nDenny Britz. 2018. JESC: Japanese-English subtitle\ncorpus. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nJoshua Robinson, Christopher Michael Rytting, and\nDavid Wingate. 2022. Leveraging large language\nmodels for multiple choice question answering.\nCoRR, abs/2210.12353.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\nConnor Shorten, Taghi M. Khoshgoftaar, and Borko\nFurht. 2021. Text data augmentation for deep learn-\ning. J. Big Data, 8(1):101.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia\nHu. 2023. Does synthetic data generation of llms\nhelp clinical text mining? CoRR, abs/2303.04360.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur√©lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur√©lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. CoRR,\nabs/2212.10560.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nJason W. Wei and Kai Zou. 2019. EDA: easy data\naugmentation techniques for boosting performance\non text classification tasks. CoRR, abs/1901.11196.\nPeipei Xia, Li Zhang, and Fanzhang Li. 2015. Learning\nsimilarity with cosine similarity ensemble. Inf. Sci.,\n307:39‚Äì52.\nHainan Xu and Philipp Koehn. 2017. Zipporah: a fast\nand scalable data cleaning system for noisy web-\ncrawled parallel corpora. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2017, Copenhagen, Den-\nmark, September 9-11, 2017, pages 2945‚Äì2950. As-\nsociation for Computational Linguistics.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woo-Myoung Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November,\n2021, pages 2225‚Äì2239. Association for Computa-\ntional Linguistics.\n3227\nA Few-Shot Examples\nBilingual cleaning for FR-EN.For the bilingual\ncleaning in the French-to-English translation\ndirection, we use the following examples:\nInput: \"Jss tro contenteee!\",\n\"Im soooo happyyyy! \"\nDesired output: \"I‚Äôm so happy!\n\"\nReasoning: The noisy French\nsentence uses abbreviations\n(\"Jss\" for \"Je suis\") and\nexcessive letters in the\nword \"contenteee.\" The clean\nEnglish sentence replaces the\nabbreviations and removes the\nextra letters to convey the same\nmeaning clearly.\nInput: \"Tkt, c trooo bi1!\n\", \"Don‚Äôt worry, it‚Äôs awesooome!\n\"\nDesired output: \"Don‚Äôt worry,\nit‚Äôs great!\"\nReasoning: The noisy French\nsentence uses internet slang\n(\"Tkt\" for \"T‚Äôinqui√®te\") and\nexcessive letters in the word\n\"trooo.\" The clean English\nsentence replaces the slang with\na more standard expression and\nremoves the extra letters for\nbetter comprehension.\nInput: \"Jvoudr√© vnir avc vs!\n\", \"I wud luv 2 cum wth u guys!\n\" Desired output: \"I would\nlove to come with you!\"\nReasoning: The noisy French\nsentence uses abbreviations\n(\"Jvoudr√©\" for \"Je voudrais\")\nand non-standard spelling\n(\"vnir\" for \"venir\"). The clean\nEnglish sentence replaces the\nabbreviations and corrects\nthe spelling for clearer\ncommunication.\nInput: \"Ct troo bi1, jkiff\ntrp! \", \"It‚Äôs sooo gre8, I luv\nit! \"\nDesired output: \"It‚Äôs amazing, I\nlove it!\"\nReasoning: The noisy French\nsentence uses non-standard\nspelling (\"troo\" for \"tr√®s\") and\nexcessive letters in the words\n\"bi1\" and \"jkiff.\" The clean\nEnglish sentence corrects the\nspelling and removes the extra\nletters for a more natural and\nconcise expression.\nBilingual cleaning for EN-FR.For the bilingual\ncleaning in the English-to-French translation\ndirection, we use the following examples:\nInput: \"I‚Äôm sooooo happyyyy! \",\n\"Je suis tellement heureuxxxx !\n\"\nDesired output: \"Je suis\ntellement heureux!\"\nReasoning: The noisy French\nsentence contains excessive\nletters in the word \"heureux\"\nand an unnecessary exclamation\nmark. The clean French sentence\nremoves the extra letters and the\nexclamation mark to convey the\nsame message accurately.\nInput: \"Can‚Äôt wait to see\nyouuuu! \", \"J‚Äôai trooooop h√¢te\nde te voiiiiir ! \"\nDesired output: \"J‚Äôai tellement\nh√¢te de te voir!\"\nReasoning: The noisy French\nsentence includes excessive\nletters in the words \"trooop\"\nand \"voiiiiir.\" The clean French\nsentence removes the extra\nletters to maintain the same\nmeaning more concisely.\nInput: \"Let‚Äôs grab a bite\nlaterrrr! \", \"Allons\nmanger un morceau plu tarrrrd!\n\"\nDesired output: \"Allons manger\nun morceau plus tard!\"\nReasoning: The noisy French\nsentence has excessive letters\nin the words \"plu\" and \"tarrrrd\"\nand unnecessary fast food emojis.\n3228\nThe clean French sentence removes\nthe extra letters and the emojis\nwhile maintaining the same\nmeaning.\nInput: \"This movie is amaziiing!\n\", \"Ce film est troooop\ng√©niaaaaal! \"\nDesired output: \"Ce film est\ntellement g√©nial!\"\nReasoning: The noisy French\nsentence contains excessive\nletters in the words \"troooop\"\nand \"g√©niaaaaal\" and unnecessary\nfire and heart emojis. The clean\nFrench sentence removes the extra\nletters and the emojis to convey\nthe same message accurately.\nBilingual cleaning for EN-JA.For the bilingual\ncleaning in the English-to-Japanese translation\ndirection, we use the following examples:\nInput: \"I‚Äôm soooo happyyyy!\n\", \"„Åô„Å£„Åî„Éº„ÅèÂ¨â„Åó„ÅÑÔºÅ \"\nDesired output: \" „Åô„Åî„Åè Â¨â„Åó„ÅÑ„Åß\n„ÅôÔºÅ\"\nReasoning: The noisy Japanese\nsentence uses excessive\nelongation in the word \"„Åô „Å£ „Åî„Éº\n„Åè\" and includes an unnecessary\nexclamation mark. The clean\nJapanese sentence removes the\nexcessive elongation and uses a\nmore polite form to convey the\nsame meaning accurately.\nInput: \"Can‚Äôt wait to see\nyouuuu! \" Desired output: \" ‰ºö\n„Åà„Çã„ÅÆ„ÅåÊ•Ω„Åó„Åø„Åß„ÅôÔºÅ \"\nReasoning: The noisy Japanese\nsentence includes excessive\nelongation in the word \"„Çà„Åâ„Åâ„Åâ \"\nand an unnecessary exclamation\nmark. The clean Japanese\nsentence removes the excessive\nelongation and uses a more polite\nform for a clearer and more\nappropriate expression.\nInput: \"Let‚Äôs grab a bite\nlaterrrr! \", \"Âæå „Åß ËªΩ „Åè È£ü\n„Åπ„Çà„Å£„Åã„ÅÅ„ÅÅ„ÅÅ! \"\nDesired output: \" Âæå„Åß„Å°„Çá„Å£„Å®È£ü„Åπ„Åæ\n„Åó„Çá„ÅÜÔºÅ\"\nReasoning: The noisy Japanese\nsentence includes excessive\nelongation in the word \"„Çà „Å£\n„Åã „ÅÅ „ÅÅ „ÅÅ\" and unnecessary fast\nfood emojis. The clean Japanese\nsentence removes the excessive\nelongation and provides a more\npolite and appropriate phrase to\nconvey the same meaning.\nInput: \"This movie is amaziiing!\n\", \"„Åì„ÅÆÊò†Áîª„ÅØ„Åô„Å£„Åî„ÅÑ„ÅÉ„ÅÉ„ÅÉ\n\"\nDesired output: \" „Åì„ÅÆÊò†Áîª„ÅØÁ¥†Êô¥„Çâ„Åó\n„ÅÑ„Åß„ÅôÔºÅ\"\nReasoning: The noisy Japanese\nsentence uses excessive\nelongation in the word \"„Åô „Å£ „Åî\n„ÅÑ„ÅÉ„ÅÉ„ÅÉ\" and includes unnecessary\nfire and heart emojis. The clean\nJapanese sentence removes the\nexcessive elongation and provides\na more appropriate and accurate\nexpression for the same meaning.\nTranslation for FR-EN. For the generative\ntranslation method in the French-to-English trans-\nlation direction, we use the following examples:\nInput: \"Heyyy, √ßa va trop\nbiennn! Jsuis trop hype√©√©√©\npour ce soir! \"\nDesired output: \"Hey, I‚Äôm\ndoing great! I‚Äôm so excited for\ntonight!\"\nReasoning: The noisy sentence\ncontains excessive letters in\nwords and emojis. The clean\nsentence removes the extra\nletters and emojis to convey the\nsame message more clearly.\nInput: \"OMG jpeux pas croire,\nc‚Äôest trooop ouf! \"\nDesired output: \"Oh my God,\nI can‚Äôt believe it, it‚Äôs so\namazing!\"\n3229\nReasoning: The noisy sentence\nuses internet slang (\"OMG,\"\n\"trooop,\" \"ouf\") and excessive\npunctuation (\"!!\"). The clean\nsentence replaces the slang with\nmore standard expressions and\nremoves the excessive punctuation\nfor better comprehension.\nInput: \"Mdr t‚Äôes trop marrant,\ntu me fais tp rire \"\nDesired output: \"Haha, you‚Äôre so\nfunny, you make me laugh a lot.\"\nReasoning: The noisy sentence\ncontains internet slang (\"Mdr,\"\n\"trop,\" \"tp\") and a laughing\nemoji. The clean sentence\nreplaces the slang with more\ncommon expressions and removes\nthe emoji for a more formal and\nclear communication.\nInput: \"H√©√©, on se voit au\nrestau tout de suite?\"\nDesired output: \"Hey, can we\nmeet at the restaurant right\naway? \" Reasoning: The\nnoisy sentence has intentional\nmisspellings (\"H√©√©,\" \"restau\")\nand fast food emojis. The clean\nsentence corrects the spellings\nand removes the emojis to convey\nthe same message accurately.\nTranslation for EN-FR. For the generative\ntranslation method in the English-to-French trans-\nlation direction, we use the following examples:\nInput: \"Heyy, what‚Äôs up? I‚Äôm\nsooo exicteddd to go out tonight!\n\"\nDesired output: \"Salut, quoi\nde neuf ? Je suis tellement\nexcit√©(e) de sortir ce soir !\"\nReasoning: The noisy sentence\ncontains excessive letters in\nwords and emojis. The clean\nsentence removes the extra\nletters and emojis to convey\nthe same message more clearly\nin French.\nInput: \"OMG I can‚Äôt even rn,\nthis party is gonna be LIT AF!\n\"\nDesired output: \"Mon Dieu, je ne\npeux pas le croire, cette f√™te va\n√™tre incroyable !\"\nReasoning: The noisy sentence\nuses internet slang (\"OMG,\" \"rn,\"\n\"LIT AF\") and abbreviations. The\nclean sentence replaces the slang\nwith more standard expressions\nand removes the abbreviations for\nbetter comprehension in French.\nInput: \"lol ur so funny, u\nalways make me laugh \"\nDesired output: \"lol tu es\ntellement dr√¥le, tu me fais\ntoujours rire.\"\nReasoning: The noisy sentence\ncontains internet slang (\"lol,\"\n\"ur\") and a laughing emoji. The\nclean sentence replaces the slang\nwith more common expressions and\nremoves the emoji for a more\nformal and clear communication\nin French.\nInput: \"Heyy, wanna grab some\nfud later? \"\nDesired output: \"Hey, tu veux\nqu‚Äôon aille manger plus tard ?\"\nReasoning: The noisy sentence\nhas intentional misspellings\n(\"Heyy,\" \"fud\") and fast food\nemojis. The clean sentence\ncorrects the spellings and\nremoves the emojis to convey\nthe same message accurately in\nFrench.\nTranslation for EN-JA. For the generative\ntranslation method in the English-to-Japanese\ntranslation direction, we use the following exam-\nples:\nInput: \"Heyyy, what‚Äôs up? I‚Äôm\nsooo excited to go out tonight!\n\"\nDesired output: \" „ÇÑ„ÅÅ„ÇÑ„ÅÅ„ÄÅÂÖÉÊ∞óÔºü‰ªä\nÂ§úÂá∫„Åã„Åë„Çã„ÅÆ„Åå„Åô„Å£„Åî„ÅèÊ•Ω„Åó„ÅøÔºÅ\"\nReasoning: The noisy sentence\n3230\ncontains excessive letters in\nwords and emojis. The clean\nsentence removes the extra\nletters and emojis to convey\nthe same message more clearly\nin Japanese.\nInput: \"OMG I can‚Äôt even rn,\nthis party is gonna be LIT AF!\n\" Desired output: \" „Åà„Åà„Å£ „ÄÅ„Éû„Ç∏\n„ÅßÔºüÔºÅ„Åì„ÅÆ„Éë„Éº„ÉÜ„Ç£„ÅØÊúÄÈ´ò„Å´Áõõ„Çä‰∏ä„Åå„Çã„Åû\"\nReasoning: The noisy sentence\nuses internet slang (\"OMG,\" \"rn,\"\n\"LIT AF\") and abbreviations. The\nclean sentence replaces the slang\nwith more standard expressions\nand removes the abbreviations and\nemojis for better comprehension\nin Japanese.\nInput: \"lol you‚Äôre so funny,\nyou always make me laugh \"\nDesired output: \" Á¨ë „ÅäÂâç„ÄÅ„Éõ„É≥„ÉàÈù¢\nÁôΩ„ÅÑ„Å™„ÄÇ„ÅÑ„Å§„ÇÇÁ¨ë„Çè„Åõ„Å¶„Åè„Çå„Çã„Çì„Å†„Çà\"\nReasoning: The noisy sentence\ncontains internet slang (\"lol\")\nand a laughing emoji. The clean\nsentence replaces the slang with\nmore common expressions and\nremoves the emoji for a more\nnatural and clear communication\nin Japanese.\nInput: \"Heyyy, wanna grab some\nfood later? \"\nDesired output: \" „ÇÑ„ÅÅ„ÇÑ„ÅÅ„ÄÅÂæå„ÅßÈ£ü„Åπ\nÁâ©„Åß„ÇÇË≤∑„Å£„Å¶„ÅÑ„Åã„Å™„ÅÑÔºü\"\nReasoning: The noisy sentence\nhas intentional misspellings\n(\"Heyyy\") and fast food emojis.\nThe clean sentence corrects the\nspellings and removes the emojis\nfor a more natural and clear\ncommunication in Japanese.\nMonolingual cleaning for FR-EN. For the\nmonolingual cleaning in the French-to-English\ntranslation direction, we use the following exam-\nples:\nInput: \"Heyy, what‚Äôs up? I‚Äôm\nsooo exicteddd to go out tonight!\n\"\nDesired output: \"Hey, what‚Äôs\nup? I‚Äôm so excited to go out\ntonight!\"\nReasoning: \"The noisy sentence\ncontains excessive letters in\nwords and emojis. The clean\nsentence removes the extra\nletters and emojis to convey the\nsame message more clearly.\"\nInput: \"OMG I can‚Äôt even rn,\nthis party is gonna be LIT AF!\n\"\nDesired output: \"Oh my God, I\ncan‚Äôt even right now, this party\nis going to be awesome!\"\nReasoning: \"The noisy sentence\nuses internet slang (‚ÄôOMG‚Äô, ‚Äôrn‚Äô,\n‚ÄôLIT AF‚Äô) and abbreviations. The\nclean sentence expands the slang\nand abbreviations for better\ncomprehension.\"\nInput: \"lol ur so funny, u\nalways make me laugh \"\nDesired output: \"Laugh out loud,\nyou‚Äôre so funny. You always make\nme laugh.\"\nReasoning \"The noisy sentence\ncontains internet slang (‚Äôlol‚Äô,\n‚Äôur‚Äô) and a laughing emoji. The\nclean sentence replaces the slang\nwith its full form and removes\nthe emoji for a more formal\nexpression.\"\nInput: \"Heyy, wanna grab some\nfud later? \"\nDesired Output \"Hey, do you want\nto grab some food later?\"\nReasoning: \"The noisy sentence\nhas intentional misspellings\n(‚ÄôHeyy‚Äô, ‚Äôfud‚Äô) and food emojis.\nThe clean sentence corrects\nthe spellings and removes the\nemojis to convey the same message\naccurately.\"\nMonolingual cleaning for EN-FR. For the\nmonolingual cleaning in the English-to-French\n3231\ntranslation direction, we use the following exam-\nples:\nInput: \"Saluttt, √ßa va? Je\nsuiis trp excit√©√©√©√© pr sortir ce\nsoiiiir! \"\nDesired output: \"Salut, √ßa va?\nJe suis trop excit√© pour sortir\nce soir!\"\nReasoning: The noisy sentence\ncontains excessive letters in\nwords and emojis. The clean\nsentence removes the extra\nletters and emojis to convey the\nsame message more clearly.\nInput: \"Tkt, j‚Äôte dm dans 2min,\nok? \"\nDesired output: \"T‚Äôinqui√®te, je\nte donne des nouvelles dans 2\nminutes, d‚Äôaccord?\"\nReasoning: The noisy sentence\nuses internet slang (‚ÄôTkt‚Äô,\n‚Äôj‚Äôte‚Äô, ‚Äôdm‚Äô) and abbreviations.\nThe clean sentence expands the\nslang and abbreviations for\nbetter comprehension.\nInput: \"Mdrr t‚Äôes tro drol, tu\nm‚Äôfais tp rire \"\nDesired output: \"Mort de rire,\ntu es vraiment dr√¥le, tu me fais\ntrop rire.\"\nReasoning: The noisy sentence\ncontains internet slang (‚ÄôMdrr‚Äô,\n‚Äôtro‚Äô, ‚Äôtp‚Äô) and a laughing emoji.\nThe clean sentence replaces the\nslang with its full form and\nremoves the emoji for a more\nformal expression.\nInput: \"H√©√©, on se retrouve au\nmcdo plutar? \"\nDesired output: \"H√©, est-ce\nqu‚Äôon peut se retrouver au\nMcDonald‚Äôs plus tard?\"\nReasoning: The noisy sentence\nhas intentional misspellings\n(‚ÄôH√©√©‚Äô, ‚Äôplutar‚Äô) and fast food\nemojis. The clean sentence\ncorrects the spellings and\nremoves the emojis to convey the\nsame message accurately.\nMonolingual cleaning for EN-JA. For the\nmonolingual cleaning in the English-to-Japanese\ntranslation direction, we use the following ex-\namples: Input: \" ÂÖÉÊ∞ó„Å£„Åô„ÅãÔºü „ÇÅ„Å£\n„Å°„ÇÉÊ•Ω„Åó„Åø„Å†„Åú„Äú \"\nDesired output: \" ÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü„Å®„Å£„Å¶\n„ÇÇÊ•Ω„Åó„Åø„Åß„Åô„Å≠ÔºÅ\"\nReasoning: The noisy sentence\ncontains informal language (\"„Å£\n„Åô „Åã\" instead of \"„Åß „Åô „Åã\") and\nexcessive use of \" \" at the end.\nThe clean sentence removes the\ninformal elements and expresses\nthe same meaning more formally.\nInput: \" „ÇÅ„Å£„Å°„ÇÉ„Åä„ÅÑ„Åó „Éº„ÅÑÔºÅLOL\n\"\nDesired output: \" „Å®„Å£„Å¶„ÇÇ„Åä„ÅÑ„Åó„ÅÑÔºÅ\nÁ¨ë\"\nReasoning: The noisy sentence\nincludes the use of \"„ÇÅ „Å£ „Å° „ÇÉ\"\n(a casual intensifier) and the\nEnglish acronym \"LOL.\" The clean\nsentence removes the casual\nintensifier and replaces \"LOL\"\nwith the Japanese equivalent \"Á¨ë\"\n(meaning \"laugh\").\nInput: \" „Ç¢„Éè„Éè„ÄÅË∂Ö„Åä„ÇÇ„Çç„ÅÑÔºÅ \"\nDesired output: \" Á¨ë„ÄÅ„Å®„Å¶„ÇÇ Èù¢ ÁôΩ\n„ÅÑÔºÅ\"\nReasoning: The noisy sentence\nuses \"„Ç¢ „Éè „Éè\" (a casual laughter\nexpression) and an emoji. The\nclean sentence replaces \"„Ç¢ „Éè „Éè\"\nwith the more standard \"Á¨ë\" and\nremoves the emoji.\nInput: \" „Çà„Å£„Åó„ÇÉ „ÄÅÂæÖ„Å°Âêà„Çè„Åõ„Åæ„Å§\n„ÅãÔºü \"\nDesired output: \" „Çà„Åó„ÄÅÂæÖ„Å°Âêà„Çè„Åõ„Åó\n„Åæ„Åó„Çá„ÅÜ„ÅãÔºü\"\nReasoning: The noisy sentence\ncontains a misspelling (\"„Åæ „Å§ „Åã\"\ninstead of \"„Åæ„Åó„Çá„ÅÜ„Åã \") and fast\nfood emojis. The clean sentence\ncorrects the spelling and removes\nthe emojis while maintaining the\nsame meaning.\n3232\nB Task Descriptions\nThe task description for the bilingual cleaning\nmethod is as follows:\nYour task is to clean the given\n{tgt} sentence. You will receive\ntwo sentences as input: the\n{src} sentence containing noise,\nand the translated tgt version\nof that sentence, also containing\nnoise. Your task is to clean\nonly the {tgt} sentence and\nreturn it as output.\nThe task description for the generative translation\nmethod is as follows:\nYour task is to translate the\ngiven noisy {src} sentence to the\ncorrect {tgt} version, thereby\nremoving all noise. You will\nonly return the clean {tgt}\nsentence as output.\nThe task description for the monolingual cleaning\nmethod is as follows:\nYour task is to clean the {tgt}\nsentence that you will receive\nas input: You will then return\nthe clean version of the {tgt}\nsentence as output.\nFor each task {src} refers to the source language\n(English or French) and {tgt} refers to the target\nlanguage (English, French, or Japanese).\nC Requests\nThis section shows the requests we used in the\nprompts. The request for the monolingual cleaning\nmethod and generative translation methods\nare the same and are as follows: This is\nthe input {input_sent}, . Please\nreturn the desired output in the\ncorrect format. The only difference is that\nthe {input_sent} refers to the target sentence in\nthe case of the monolingual cleaning method,\nwhereas the {input_sent} refers to the source\nsentence for the generative translation method.\nThe request for the bilingual cleaning task is\ndifferent because of the multiple inputs. The\nrequest for this method is as follows: These are\nthe inputs {src_sent}, {tgt_sent}.\nPlease return the desired output\nin the correct format.\nD Samples\nIn Table 4, we show several samples from MTNT,\nand show how each distinct method tends to clean\nthese samples in different ways. In some samples\nabbreviations are corrected, in others emojis are\nremoved and some are changed based on language\nor word choice.\nE Detailed Similarity Scores\nFor the convenience of the latter works that plan to\nuse our method as a baseline, we list the detailed\nsimilarity scores from Figure 2 in Table 5, 6 and 7.\nEval. Set LASER BLEU Jaccard Rouge-1 Jaro Winkler\nBilingual 0.94 0.90 0.79 0.66 0.54\nTranslation 0.89 0.81 0.61 0.44 0.26\nMonolingual 0.95 0.91 0.82 0.71 0.60\nCorrection-tool 0.99 0.98 0.99 0.97 0.94\nTable 5: Similarity scores between noisy and cleaned\nFrench-to-English target samples.\nEval. Set LASER BLEU Jaccard Rouge-1 Jaro Winkler\nBilingual 0.93 0.88 0.69 0.54 0.39\nTranslation 0.89 0.79 0.54 0.37 0.23\nMonolingual 0.97 0.93 0.88 0.80 0.73\nCorrection-tool 0.98 0.97 0.93 0.88 0.84\nTable 6: Similarity scores between noisy and cleaned\nEnglish-to-French target samples.\nEval. Set LASER BLEU Jaccard Rouge-1 Jaro Winkler\nBilingual 0.91 0.86 0.72 0.60 0.39\nTranslation 0.83 0.70 0.48 0.35 0.10\nMonolingual 0.96 0.93 0.90 0.85 0.76\nCorrection-tool 1.00 1.00 1.00 1.00 0.95\nTable 7: Similarity scores between noisy and cleaned\nEnglish-to-Japanese target samples.\nF Contrastive Learning\nF.1 Training Loss\nThe contrastive loss (Chen et al., 2020) is computed\nbased on two source sentences: the original source\nsentences xand the augmented source sentences z.\nLctr = ‚àí\nN‚àë\ni\nlog exp(sim(exi ,ezi )/œÑ)‚àëN\nj exp(sim(exi ,ezj )/œÑ)\nwhere xi is the original sentence, zi is the aug-\nmented version of xi, and œÑ is the temperature\nfactor. Therefore, the positive sample is the corre-\nsponding augmented sentence, while the negative\nsamples are the augmented versions of other orig-\ninal source sentences from the same mini-batch.\n3233\nexi and ezi are the average representations along\nthe sequence dimension from the encoder outputs.\nApart from the contrastive loss, the standard\ncross-entropy loss is calculated as:\nLce = ‚àí\nN‚àë\ni=1\n(log PŒ∏(yi|xi) + logPŒ∏(yi|zi))\nWe combine both losses as the final loss:\nL= Lce + ŒªLctr\nwhere Œªis an interpolation factor. We incorporate\nthe augmented source inputs z to ensure that the\nmodel can still generate correct translations with\nnoisy input.\nF.2 Optimal Hyperparameters\nWe conduct thorough experiments to choose the\noptimal hyperparameters for contrastive learning.\nTemperature œÑ. This hyperparameter plays a\ncrucial role in adjusting the softmax function used\nin the contrastive learning framework, thereby af-\nfecting the distribution of the similarity scores be-\ntween augmented and original sentences. By vary-\ning the value of œÑ, we can control the concentra-\ntion or diffusion of the score distribution. Figure\n6 shows the results from the models trained with\ndifferent augmentation strategies. It is evident that\nœÑ = 0.1 uniformly performs the best. So we set\nœÑ = 0.1 by default.\n-12.0%\n-7.0%\n-2.0%\n3.0%\n8.0%\n13.0%\n 0.05  0.10  0.15  0.20\nPerformance gain (%)\nœÑparameter\nCharcter Error Aug.\nSynonym Context\nFigure 6: Performance vs. temperature factor œÑ for\nmodels trained with different augmentation strategies\non the French-to-English translation direction. Scores\nare computed on the bilingual C-MTNT evaluation set.\nLoss balanceŒª. Our final loss consists of the\nstandard cross-entropy loss and the contrastive loss.\nHere we conduct experiments to choose the op-\ntimal loss balance factor Œª. As shown in Figure\n7, the optimal Œªvaries for different augmentation\nmethods. We set Œª = 0.01 by default since this\nvalue works better for most methods.\n-4%\n-2%\n0%\n2%\n4%\n6%\n8%\n10%\n0.005 0.01 0.02 0.03\nPerformance gain (%)\nŒªparameter\nCharacter Error Aug.\nSynonym Context\nFigure 7: Performance vs. loss balance factor Œª for\nmodels trained with different augmentation strategies\non the French-to-English translation direction. Scores\nare computed on the bilingual C-MTNT evaluation set.\nG Prompt for GPT-4 Evaluation\n\"\"\"In the following, I‚Äôm going\nto show you one noisy source\nsentence in French and one noisy\ntarget sentence in English. In\naddition, I also offer you two\nclean versions of the noisy\ntarget sentence.\nCan you rank these two clean\ntarget sentences based on these\nthree criteria:\n1. Comprehensive noise removal:\nAll forms of noise should be\neliminated from the noisy target\nsentence, including removing\nsemantically meaningless emojis,\ntranslating emojis with semantic\ncontent into words, correcting\nmisspellings, etc.\n2. Semantic preservation:\nThe clean target sentence\nshould retain similar semantic\ninformation as the original noisy\ntarget sentence.\n3. Alignment with the source\nsentence: The clean target\nsentence should convey the\nsame intended meaning as the\nnoisy source sentence, ensuring\naccurate translation and faithful\n3234\nrepresentation of the original\ncontent.\nNoisy source sentence in French:\n{0}\nNoisy target sentence in\nEnglish: {1}\nThe first clean target sentence:\n{2}\nThe second clean target\nsentence: {3}\nFor your output, you don‚Äôt need\nto give any explanation. If\nthe first version is better, you\noutput 1. If the second version\nis better, you output 2. If they\nare equally clean, you output\n3.\"\"\".format(source, target,\ntarget_from_clean_method_v1,\ntarget_from_clean_method_v2)\n3235\nMethod Lang. Noisy Target Sample Clean Target Sample\nBilingual\nEN\n‚Äú:p I don‚Äôt have many juicy stories to tell right\nnow.‚Äù\n‚ÄúI don‚Äôt have many juicy stories to tell at the\nmoment.‚Äù\n‚ÄúIf in doubt, tinker with the doc.‚Äù ‚ÄúIf in doubt, tinker with the document.‚Äù\n‚ÄúSocial network =/= reality but when you add\nnews paper to that...‚Äù\n‚ÄúSocial network does not equal reality, but when\nyou add newspapers to that...‚Äù\nFR\n‚ÄúJe pense que plus l‚Äôon reste c√©to et moins on a\nenvie de ces choses.‚Äù\n‚ÄúJe pense que plus on reste Keto, moins on a\nenvie de ces choses.‚Äù\n‚Äúel oh el Lol merci, j‚Äôai le m√™me espoir pour toi\naussi comp√®re‚Äù\n‚ÄúMort de rire merci, j‚Äôai le m√™me espoir pour\ntoi aussi, compadre‚Äù\n‚ÄúJe ne sais pas quoi faire Passe √† autre chose.‚Äù ‚ÄúJe ne sais pas quoi faire. Allons de l‚Äôavant.‚Äù\nJA\n‚ÄúWreslteMania„ÅÆË©¶Âêà„ÅÆ„Éû„ÉÉ„ÉÅ„ÅÆ‰∫àÊ∏¨„ÅØ‰ªä\nÂπ¥„ÅØÈõ£„Åó„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú‰ªäÂπ¥„ÅÆWrestleMania„ÅÆË©¶ÂêàÈ†Ü„ÅØ‰∫àÊ∏¨„Åó„Å´\n„Åè„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éª„Ç∑„ÉÉ„ÇØ\n„Çπ„Éª„Çπ„É™„Éº„ÄÇ ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éª„Ç∑„ÉÉ„ÇØ\n„Çπ„Éª„Çπ„É™„Éº„ÄÇ‚Äù\n‚Äú„Çì„Çì„Çì„Çì„ÄÅ„Åü„Å∂„Çì„ÄÅËÅ∑Â†¥„ÅßË¶ã„Çã„Å´„ÅØÂ∞ë„Åó\nÂç±Èô∫„Åã„Å™„ÄÇ‚Äù\n‚Äú„Çì„Çì„Çì„Çì„ÄÅ„Å°„Çá„Å£„Å®Âç±„Å™„ÅÑ„Åã„ÇÇ„Åó„Çå„Å™„ÅÑ\n„Åß„Åô„Å≠„ÄÇ‚Äù\nMonolingual\nEN\n‚Äú:p I don‚Äôt have many juicy stories to tell right\nnow.‚Äù\n‚ÄúI don‚Äôt have many juicy stories to tell right\nnow.‚Äù\n‚ÄúIf in doubt, tinker with the doc.‚Äù ‚ÄúIf you are unsure, tinker with the document.‚Äù\n‚ÄúSocial network =/= reality but when you add\nnews paper to that...‚Äù\n‚ÄúSocial network does not equal reality but when\nyou add a newspaper to that...‚Äù\nFR\n‚ÄúJe pense que plus l‚Äôon reste c√©to et moins on a\nenvie de ces choses.‚Äù\n‚ÄúJe pense que plus on reste Keto et moins on a\nenvie de ces choses.‚Äù\n‚Äúel oh el Lol j‚Äôai le m√™me espoir pour toi aussi\ncomp√®re‚Äù\n‚ÄúMort de rire merci, j‚Äôai le m√™me espoir pour\ntoi aussi comp√®re‚Äù\n‚ÄúJe ne sais pas quoi faire Passe √† autre chose.‚Äù ‚ÄúJe ne sais pas quoi faire. Passe √† autre chose.‚Äù\nJA\n‚ÄúWreslteMania„ÅÆË©¶Âêà„ÅÆ„Éû„ÉÉ„ÉÅ„ÅÆ‰∫àÊ∏¨„ÅØ‰ªä\nÂπ¥„ÅØÈõ£„Åó„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú‰ªäÂπ¥„ÅÆWrestleMania„ÅÆË©¶ÂêàÈ†Ü„ÅØ‰∫àÊÉ≥„ÅåÈõ£\n„Åó„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éª„Ç∑„ÉÉ„ÇØ\n„Çπ„Éª„Çπ„É™„Éº„ÄÇ ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éª„Ç∑„ÉÉ„ÇØ\n„Çπ„Éª„Çπ„É™„Éº„ÄÇ‚Äù\n‚Äú„Çì„Çì„Çì„Çì„ÄÅ„Åü„Å∂„Çì„ÄÅËÅ∑Â†¥„ÅßË¶ã„Çã„Å´„ÅØÂ∞ë„Åó\nÂç±Èô∫„Åã„Å™„ÄÇ‚Äù\n‚Äú„Çì„Çì„Çì„Çì„ÄÅ„Åü„Å∂„Çì„ÄÅËÅ∑Â†¥„ÅßË¶ã„Çã„Å´„ÅØÂ∞ë„Åó\nÂç±Èô∫„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ‚Äù\nTranslation\nEN\n‚Äú:p I don‚Äôt have many juicy stories to tell right\nnow.‚Äù\n‚ÄúI have few juicy stories to tell at the moment.‚Äù\n‚ÄúIf in doubt, tinker with the doc.‚Äù ‚ÄúWhen in doubt, tweak the document.‚Äù\n‚ÄúSocial network =/= reality but when you add\nnews paper to that...‚Äù\n‚ÄúSocial media does not equal reality, but when\nyou add journals to that, on the other hand...‚Äù\nFR\n‚ÄúJe pense que plus l‚Äôon reste c√©to et moins on a\nenvie de ces choses.‚Äù\n‚ÄúJe pense que plus nous restons en mode Keto,\nmoins nous avons envie de ces choses.‚Äù\n‚Äúel oh el Lol j‚Äôai le m√™me espoir pour toi aussi\ncomp√®re‚Äù\n‚ÄúMort de rire merci, j‚Äôai la m√™me esp√©rance pour\ntoi aussi compadre‚Äù\n‚ÄúJe ne sais pas quoi faire Passe √† autre chose.‚Äù ‚ÄúJe ne sais pas quoi faire. Avancer.‚Äù\nJA\n‚ÄúWreslteMania„ÅÆË©¶Âêà„ÅÆ„Éû„ÉÉ„ÉÅ„ÅÆ‰∫àÊ∏¨„ÅØ‰ªä\nÂπ¥„ÅØÈõ£„Åó„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú‰ªäÂπ¥„ÅÆWrestleMania„ÅÆË©¶ÂêàÈ†Ü„ÅØ‰∫àÊÉ≥„ÅåÈõ£\n„Åó„ÅÑ„Åß„Åô„ÄÇ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Éª„Ç∑„ÉÉ„ÇØ\n„Çπ„Éª„Çπ„É™„Éº„ÄÇ ‚Äù\n‚Äú„Éà„É†„Éª„ÇØ„É©„É≥„Ç∑„Éº„ÅÆ„É¨„Ç§„É≥„Éú„Éº„Ç∑„ÉÉ„ÇØ\n„Çπ3„ÄÇ‚Äù\n‚Äú„Çì„Çì„Çì„Çì„ÄÅ„Åü„Å∂„Çì„ÄÅËÅ∑Â†¥„ÅßË¶ã„Çã„Å´„ÅØÂ∞ë„Åó\nÂç±Èô∫„Åã„Å™„ÄÇ‚Äù\n‚Äú„ÅÜ„Éº„Çì„ÄÅÂ§öÂàÜ„Å°„Çá„Å£„Å®„ÄÅÂ∞ë„Åó„ÅØNSFW„Å£\n„ÅΩ„ÅÑ„Åã„ÇÇ„Åó„Çå„Å™„ÅÑ‚Äù\nTable 4: Several noisy target samples from MTNT and C-MTNT with different cleaning methods. The red text is\nnoise, the blue text indicates rephrased parts, and the green text indicates the removal or correction of noise.\n3236",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8513764142990112
    },
    {
      "name": "Machine translation",
      "score": 0.7178652286529541
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.713634729385376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5526828169822693
    },
    {
      "name": "Natural language processing",
      "score": 0.5185330510139465
    },
    {
      "name": "Language model",
      "score": 0.511200487613678
    },
    {
      "name": "Noise (video)",
      "score": 0.5025694370269775
    },
    {
      "name": "Noisy data",
      "score": 0.44598543643951416
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4383726119995117
    },
    {
      "name": "Slang",
      "score": 0.4176986813545227
    },
    {
      "name": "Speech recognition",
      "score": 0.41724249720573425
    },
    {
      "name": "Ask price",
      "score": 0.4111124873161316
    },
    {
      "name": "Task (project management)",
      "score": 0.4103166460990906
    },
    {
      "name": "Machine learning",
      "score": 0.3959847092628479
    },
    {
      "name": "Linguistics",
      "score": 0.08460471034049988
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ],
  "cited_by": 4
}