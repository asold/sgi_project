{
    "title": "RetroNLU: Retrieval Augmented Task-Oriented Semantic Parsing",
    "url": "https://openalex.org/W3199922048",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2107792188",
            "name": "Vivek Gupta",
            "affiliations": [
                "University of Utah",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A3009596836",
            "name": "Akshat Shrivastava",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2153818367",
            "name": "Adithya Sagar",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2618171555",
            "name": "Armen Aghajanyan",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2074277926",
            "name": "Denis Savenkov",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288087322",
        "https://openalex.org/W3166298099",
        "https://openalex.org/W3104713013",
        "https://openalex.org/W3036839309",
        "https://openalex.org/W2963895422",
        "https://openalex.org/W2885421725",
        "https://openalex.org/W4287649493",
        "https://openalex.org/W3116040647",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W3156849929",
        "https://openalex.org/W2798416089",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W3100544532",
        "https://openalex.org/W3080427942",
        "https://openalex.org/W3206547074",
        "https://openalex.org/W3118093735",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2978300271",
        "https://openalex.org/W3014408449",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3156414406",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2892248135",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W3103469330",
        "https://openalex.org/W3094437659",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3175194597",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W1793121960",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W3091355780",
        "https://openalex.org/W2963149635",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W3105218021",
        "https://openalex.org/W3134891661",
        "https://openalex.org/W3090145300",
        "https://openalex.org/W3123816584",
        "https://openalex.org/W3105721709",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2793165286"
    ],
    "abstract": "While large pre-trained language models accumulate a lot of knowledge in their parameters, it has been demonstrated that augmenting it with non-parametric retrieval-based memory has a number of benefits ranging from improved accuracy to data efficiency for knowledge-focused tasks such as question answering. In this work, we apply retrieval-based modeling ideas to the challenging complex task of multi-domain task-oriented semantic parsing for conversational assistants. Our technique, RetroNLU, extends a sequence-to-sequence model architecture with a retrieval component, which is used to retrieve existing similar samples and present them as an additional context to the model. In particular, we analyze two settings, where we augment an input with (a) retrieved nearest neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of nearest neighbor utterances (semparse-nn). Our technique outperforms the baseline method by 1.5% absolute macro-F1, especially at the low resource setting, matching the baseline model accuracy with only 40% of the complete data.Furthermore, we analyse the quality, model sensitivity, and performance of the nearest neighbor retrieval component’s for semantic parses of varied utterance complexity.",
    "full_text": "Proceedings of the 4th Workshop on NLP for Conversational AI, pages 184 - 196\nMay 27, 2022 ©2022 Association for Computational Linguistics\nRETRO NLU : Retrieval Augmented Task-Oriented Semantic Parsing\nVivek Gupta1,2∗\n, Akshat Shrivastava2, Adithya Sagar2, Armen Aghajanyan2, Denis Savenkov2\n1School of Computing, University of Utah\n2 Facebook Conversational AI, Menlo Park\nvgupta@cs.utah.edu ; {akshats, adithyasagar, armenag, denxx}@fb.com\nAbstract\nWhile large pre-trained language models accu-\nmulate a lot of knowledge in their parameters, it\nhas been demonstrated that augmenting it with\nnon-parametric retrieval-based memory has a\nnumber of benefits ranging from improved\naccuracy to data efficiency for knowledge-\nfocused tasks such as question answering. In\nthis work, we apply retrieval-based model-\ning ideas to the challenging complex task of\nmulti-domain task-oriented semantic parsing\nfor conversational assistants. Our technique,\nRETRO NLU , extends a sequence-to-sequence\nmodel architecture with a retrieval component,\nwhich is used to retrieve existing similar sam-\nples and present them as an additional context\nto the model. In particular, we analyze two set-\ntings, where we augment an input with (a) re-\ntrieved nearest neighbor utterances (utterance-\nnn), and (b) ground-truth semantic parses of\nnearest neighbor utterances (semparse-nn). Our\ntechnique outperforms the baseline method by\n1.5% absolute macro-F1, especially at the low\nresource setting, matching the baseline model\naccuracy with only 40% of the complete data.\nFurthermore, we analyse the quality, model sen-\nsitivity, and performance of the nearest neigh-\nbor retrieval component’s for semantic parses\nof varied utterance complexity.\n1 Introduction\nRoberts et al. (2020) demonstrated that neural lan-\nguage models quite effectively store factual knowl-\nedge in their parameters without any external infor-\nmation source. However, such implicit knowledge\nis hard to update, i.e. remove certain information\n(Bourtoule et al., 2021), change or add new data\nand labels. Additionally, parametric knowledge\nmay perform worse for less frequent facts, which\ndon’t appear often in the training set, and “hallu-\ncinate” responses. On the other hand, memory-\naugmented models (Sukhbaatar et al., 2015) de-\n∗*Work done by author while interning at Facebook Con-\nversational AI.\ncouple knowledge source and task-specific “busi-\nness logic”, which allows updating memory index\ndirectly without model retraining. Recent stud-\nies showed their potential for knowledge-intensive\nNLP tasks, such as question answering (Khandel-\nwal et al., 2020; Lewis et al., 2020c).\nIn this work, we explore RETRO NLU: retrieval-\nbased modeling approach for task-oriented seman-\ntic parsing problem, where explicit memory pro-\nvides examples of semantic parses, which model\nneeds to learn to transfer to a given input utterance.\nAn example semantic parse for task-oriented di-\nalog utterance and its corresponding hierarchical\nrepresentation are presented in Figure 1.\nFigure 1: An intent-slot based compositional semantic pars-\ning example(coupled) from TOPv2 (Chen et al., 2020).\nIn this paper we are focusing on the following\nquestions: (a) Data Efficiency: Can retrieval based\non non-parametric external knowledge alleviate re-\nliance on parametric knowledge typically acquired\nvia supervised training on large labeled datasets?1\nWe examine how different training settings, depend-\ning on the amount of supervision data available,\n1Parametric knowledge is information stored in model\nparameters. Non-parametric knowledge refers to external data\nsources that the model uses to infer.\n184\nimpact model prediction, i.e. fully supervised vs.\nlimited supervised training. (b) Semi-supervised\nSetting: Can we enhance models by using abundant\nand inexpensive unlabeled external non-parametric\nknowledge rather than structurally labeled knowl-\nedge? We examine the effect of utilizing unla-\nbeled similar utterances instead of labelled seman-\ntic parses as external non-parametric knowledge\non model performance. (c) Robustness to Noise:\nCan a model opt to employ parametric knowledge\nrather than non-parametric knowledge in a resilient\nmanner, for example, when the non-parametric in-\nformation is unreliable? We examine the model’s\nresilience and its reliance on non-parametric ex-\nternal information. External knowledge is not al-\nways precisely labeled and reliable for all exam-\nples/utterances. (d) Utterance Complexity: Is non-\nparametric external knowledge addition effective\nfor both uncommon and complex structured (hi-\nerarchical) examples? We examine whether ex-\nternal knowledge addition is more beneficial in\ncertain cases than others, or if it supports accurate\npredictions for all situations equally. It would be\nfascinating to investigate if external information\ncould also help enhance difficult and complex ex-\namples/utterances. Finally, we examine the upper\nlimit on the utility of external information. We ex-\namine structural redundancy concerns in nearest\nneighbor retrieval. (e) Knowledge Efficiency: Is it\nbeneficial to continue adding external information,\nor are there certain boundaries and challenges? Our\ncontribution are as follows:\n1. We demonstrate that combining paramet-\nric and non-parametric knowledge enhance\nmodel performance on the complex structured\ntask of task-oriented semantic parsing.\n2. We illustrate the effectiveness of our approach\nin a critical situation of learning with sparse\nlabeled data (i.e. limited parametric knowl-\nedge).\n3. We establish the efficacy of retrieval-based\nmethod in semi-supervised settings, where\nmodel’s input is supplemented with unanno-\ntated instances (i.e. unlabeled examples).\n4. By comparing predictions on clean vs. noisy\nneighbours, we establish the model’s re-\nsilience to external non-parametric knowledge\nquality.\n5. Finally, we examine performance gains with\ninputs of varying complexity: semantic struc-\nture composition and it’s frequency (i.e. fre-\nquent/rare).\nOverall, we demonstrate that retrieval enhanced\nmethod can improve performance on complicated\nstructured prediction tasks like task oriented seman-\ntic parsing without extra supervision. Furthermore,\nthe augmentation approach is data efficient and per-\nforms well in low resource settings with limited\nlabel data. The dataset, and associated scripts, will\nbe available at https://retronlu.github.io.\n2 Proposed Approach\nOur proposed approach consists of four main steps:\n(a) index construction by embedding training ex-\namples and computing cosine similarity; (b) re-\ntrieval, where we extract the nearest neighbor ut-\nterances from the index given an example utterance;\n(c) augmentation, in which we append the nearest-\nneighbor utterance ground truth semantic parse\n(semparse-nn) or the utterance itself (utterance-\nnn) to the original input via a special separator\ntoken (such as ‘ |’); and (d) semantic parsing ,\nin which we train the parsing model using the\nretrieval-augmented input with output ground truth.\nFigure 2 illustrates the Retrieval Augmented Se-\nmantic Parsing (RETRO NLU) approach.\nIndexing: To build an index we use a pre-trained\nBART model to get training utterance embeddings.\nMore specifically, we get sentence embedding for\nall the training utterances. These sentence embed-\ndings are obtained as average of token embeddings\nfrom last model layers of the BART models.2 We\nthen used the cosine similarity between embed-\ndings to build a fast and efficient retrieval index\nwith efficient FAISS library (Johnson et al., 2019).\nRetrieval: Next, given a new input (training or\ntest row), we obtain embeddings by running it\nthrough same pre-trained BART, and then query\nthe index with it to retrieve nearest neighbors text\nand their ground truth semantic parses based on\ncosine similarity. For training data, we exclude an\nexample itself from the retrieved list. For exam-\nple, for input utterance “please add 20 minutes on\nthe lasagna timer”, we retrieve the nearest neigh-\nbour “add ten minutes to the oven timer” along\n2extract_features function https://github.com/\npytorch/fairseq/tree/main/examples/bart\n185\nFigure 2: High level flowchart for retrieval augmented semantic parsing (RETRO NLU) approach.\nwith the semantic parse as “[in:add_ time_ timer\nadd [sl:date_ time ten minutes] to the [sl:timer _\nname oven] [sl:method_ timer timer]]”.\nAugmentation: Once we got a list of nearest\nneighbors, we can append either utterance text or\nsemantic parse to the input, following the left to\nright order.3 The closest neighbor appears to the\nimmediate left of the input example utterance. One\ncan also directly append the nearest neighbor utter-\nance rather than the semparse, refer as utterance-nn.\nFor the last example the final input would after aug-\nmentation would be “[in:add_ time_ timer add\n[sl:date_ time ten minutes ] to the [sl:timer_ name\noven] [sl:method_ timer timer ]] |please add 20\nminutes on the lasagna timer” for semparse-nn,\nand “add ten minutes to the oven timer|please add\n20 minutes on the lasagna timer” for utterance-nn.\nHere, the token ‘|’ act as a separator between the\ninput utterance and the neighbour’s.\nSemantic Parsing: The final step is to train a\nsequence-to-sequence model such as LSTM or\nTransformer. We fine-tune a BART model with\ncopy mechanism (Aghajanyan et al., 2020), which\nincorporates benefits of pre-trained language model\n(BART) and sequence copy mechanism (copy-ptr),\nand most importantly obtain state-of-the-art results\non the TOPv2 (Chen et al., 2020), a challenging\n3We followed GPT-3 and other generation model, where\ntask examples are pre-pended to the input. Hence, utterance\nis always nearest to the decoder followed by the first nearest\nneighbour inorder.\ntask oriented semantic parsing dataset with hierar-\nchical compositional instances. The retrieval aug-\nmented example is an input to the encoder and\nthe corresponding ground-truth semantic parse as\nthe labeled decoded sequence. At test time, we\nsimply pass the augmented input to the trained\nRETRO NLU model, and take it’s output as the pre-\ndicted semantic parse for the input utterance.\n3 Experiment and Analysis\nOur experiments examines how our knowledge\nretrieval-based augmentation technique impacts\nmodel performance indicators such as accuracy and\ndata efficiency. We study the following questions:\nRQ1. Can today’s pre-trained models leverage\nnon-parametric information in manner as described\nin §2 to enhance task-oriented semantic parsing?\nRQ2. If only part of the dataset has semantic\nparses, i.e. limited supervision setting, can augmen-\ntation with unannotated instances (utterance _nn)\nenhance semantic parsing accuracy?\nRQ3. How efficient is a retrieval-augmented\nmodel in terms of data? Is it more accurate even\nwith less training data than the baseline seq2seq\nmodel?\nRQ4. Does non-parametric memory benefit in-\nstances equally, e.g., do we notice greater benefits\nfor (a) more complex (i.e. compositional) or (b)\nless frequent semantic frames (i.e. tail over head)?\n186\nRQ5. (a) Does augmentation with more nearest\nneighbors benefits? (b) How sensitive is the model\nto retrieval noise? Can the model predict right\nintent/slots for low-quality retrieve instances?\nOur experiments are designed to demonstrate\nhow non-parametric external information can be\nbeneficial to a parametric model and to undertake\nan in-depth assessment of the impact.4\n3.1 Experimental setup\nIn this section, we discuss the datasets, pre-\nprocessing, and the model used in the experiments.\nDatasets. For our experiments, we used the multi-\ndomain complex compositional queries based pop-\nular TOPv2 (Chen et al., 2020) dataset for task-\noriented semantic parsing. We concentrated our\nefforts on task-oriented parsing because of the com-\nmercial importance of data efficiency requirements\nin conversational AI assistants dialogues. 5 The\nTOPv2 dataset contains utterances and their seman-\ntic representations for 8 domains: source domains -\n‘alarm’, ‘messaging’, ‘music’, ‘navigation’, ‘timer’,\nand ‘event’, and target domains: ‘reminder’ and\n‘weather’, designed especially to test the zero-shot\nsetting. For our experiments we chose source do-\nmains, which has a good mixture of simple (flat)\nand complex (compositional) semantic frames. For\ndataset statistics refer Table 1 in Chen et al. (2020).\nData Processing. To build a retrieval index we\nused the training split of the dataset. Each utterance\nwas represented by its BART-based embedding and\nindexed using FAISS library (Johnson et al., 2019).\n6 With FAISS computation cost of updating index-\ning was kept to bare minimum. The only additional\ncost will be increase in inference time due to aug-\nmented neighbor. To produce augmented examples,\nwe retrieved nearest neighbors for each training\nand test examples from the training set, except ex-\ncluding all training instances with exact utterance\nmatches. In the augmented examples, we use the\nspecial token ‘|’ to separate the nearest neighbors,\nas well as utterance with the first neighbor. 7 We\nused only one neighbor for most experiments ex-\ncept when we analyse multiple neighbors effects\n4We did not seek to modify the architecture which ensure\nthe augmentation methodology is flexible.\n5Regardless of augmented neighbors structure the ap-\nproach remain consistent.\n6We use L2 over unit norm BART embedding for indexing.\n7Using different separator tokens for neighbor-neighbor\npair and utterance-neighbor pair didn’t improve performance.\non performance.\nIn nearest neighbor augmented input, we fol-\nlowed right to left order, where the actual model\ninput comes last, and its highest ranked neighbor is\nappended to the left of the utterance, followed by\nother neighbors in the left based on their ranking.\n8 For input data pre-processing, we follow (Chen\net al., 2020) procedure, we obtain BPE tokens of\nall tokens, except ontology tokens (intents and slot\nlabels), which are treated as atomic tokens and ap-\npended to the BPE vocabulary. Furthermore, we\nuse the decoupled canonical form of semparse for\nall our experiments. For decoupling, phrases irrele-\nvant to slot values are removed from semparse, and\nfor canonicality, slots are arranged in alphabetic\norder (Aghajanyan et al., 2020).\nModels. For fair comparison with the earlier\nbaseline, we use the state-of-the-art BART based\nSeq2Seq-CopyPtr model for task-oriented seman-\ntic parsing. 9 The BART based Seq2Seq-CopyPtr\nmodel initialize both the encoder and decoder with\npre-trained BART (Lewis et al., 2020b) model and\nalso use the copy mechanism similar to See et al.\n(2017), refer Chen et al. (2020) for details. We\nchoose the BART based Seq2Seq-CopyPtr model\nfor the task because it’s a strong baseline, the\nperformance of the other language model such\nas RoBERTa without augmentation was inferior\n(Chen et al., 2020; Aghajanyan et al., 2020). On\nout-of-domain instances, RoBERTa-CopyPtr per-\nforms 0.6 % worse than BART-CopyPtr.10 The\nmodel is using the copy mechanism (See et al.,\n2017), which enables it to directly copy tokens\nfrom the input utterance (or from example seman-\ntic parses from nearest neighbors).\nHyperparameters. We use the same default\nhyper-parameters for all models training , i.e.\nbaseline (without-nn) and RETRO NLU models\n(utternce-nn, semparse-nn). For training we use\n100 epochs, Adam optimizer (Kingma and Ba,\n2014) with learning rate α of 1e−4 and decay\nrate β1 and β2 of 0.9 and 0.98 respectively in all\nour experiments. Also, we didn’t added any left or\nright padding and rely on variable length encoding\nin our experiments. We use warm-up steps of4000,\n8Similar performance is obtained by ordering utterances\nleft to right, followed by their neighbors in index order.\n9We prefer transformer-based language model over non-\ntransformer models, such as LSTM, because the later does not\ncapture extended context as well as the former.\n10Our findings, however, we believe, are universal and can\nbe applied to different models, including RoBERTa.\n187\ndropout ratio of 0.4, and weight decay 0.0001, but\nno clip normalization as regularization during the\ntraining. We use batch size of 128 and maximum\ntoken size of 2048. Furthermore, to ensure both\nencoder and decoder BART, can utilise the extra\nnearest neighbour information, we increase the em-\nbedding dimension to 1024.\n3.2 Results and Analysis\nThis section summarizes our findings in relation to\nthe aforementioned research questions.\nFull Training Setting. To answer RQ1, we com-\npare performance of original baseline (without-nn)\nwith our retrieval augmented models, i.e. aug-\nmenting first neighbour utterance (utterance-nn)\nand augmenting first neighbour semantic parse\n(semparse-nn). Table 1 compares the frame ac-\ncuracy of retrieval augmented (a) top nearest neigh-\nbour utterance (utterance-nn), (b) top nearest neigh-\nbour ground-truth semantic parse (semparse-nn)\nwith original baseline (without-nn) with model\ntrain on complete training data.\nDomains without-nn utterance-nn semparse-nn\nAlarm 86.67 87.17 88.57\nEvent 83.83 85.03 84.77\nMusic 79.80 80.73 80.71\nTimer 81.21 81.75 81.01\nMessaging 93.50 94.52 94.65\nNavigation 82.96 84.16 85.20\nmicro-avg 84.43 85.28 85.74\nmacro-avg 84.66 85.56 85.82\nTable 1: Performance of RETRO NLU w.r.t original\nbaseline (without-nn) with full training.\nAnalysis: We observe performance improve-\nments with retrieval-augmented models for most\ndomains compared to the original baseline\n(without-nn) in both cases. The increase in per-\nformance (micro-avg) is more substantial 1.4 %\nwith semparse-nn compare to 0.85% with utterance-\nnn. The improvement in utterance-nn augmenta-\ntion performance is likely due to memorization-\nbased generalization, as explained earlier by (Khan-\ndelwal et al., 2019). 11 The results shows the re-\ntrieval augmented semantic parsing is overall effec-\ntive. Furthermore, the performance enhancement\ncan be obtained also with unstructured utterance\n(utternace-nn) as nearest neighbour. The utterance-\nnn based augmentation is particularly beneficial in\nsemi-supervised scenarios, where we have a large\nunlabelled dataset.\n11The scores are averaged over three runs with std. of 0.3%\nLimited Training Setting. To answer RQ2, we\ncompare model performance which are trained with\nlimited training data. Figure 3 shows frame accu-\nracy (micro-avg) when we use only 10% to 50% of\nthe training data. The training datasets are created\nin an incremental setting so that next set include\nexamples from the former set. Additionally, we use\nthe complete index to retrieve the nearest neigh-\nbors.\nTraining Data (%)\nFrame Accuracy (%)\n74\n76\n78\n80\n82\n84\n86\n88\n10% 20% 30% 40% 50% 100%\nwithout-nn utterance-nn semparse-nn without-nn (100%)\nFigure 3: Performance of RETRO NLU w.r.t original\nbaseline (without-nn) with limited supervised training.\nThe x-axis is linearly scaled upto 50% data.\nAnalysis: As expected, the performance of all\nmodels increases with training set size. Both re-\ntrieval augmented models i.e. utterance-nn and\nsemparse-nn outperform the without-nn baseline\nfor all the training sizes. The improvement via\naugmentation is more substantial with less training\ndata, i.e. 4.24% at 10% data vs 1.30% at 100% data.\nFurthermore, the semparse-nn augmented model\noutperforms the original completely train (without-\nnn) model with only 40 % of the data (i..e RQ3).\nThe results show that the retrieval augmented se-\nmantic parsing is more data efficient, i.e. when\nthere is (a) limited labelled training dataset with\nmore unlabelled data for indexing (utterance-nn),\nand (b) sufficient training data but limited training\ntime (semparse-nn).\nThe first case is useful when the ground truth\nlabel is missing for utterances due to lack of an-\nnotation resources. In such a scenario, one can\nbuild the index using large amount of unlabeled\nutterances and use the index for augmentation. The\nsecond case helps us train the model faster, while\nmaintaining all annotated examples in the index.\nIn such a case, one can update the retrieval index\nonly, without re-training the model again and again.\nThis is useful when training on full data is not pos-\nsible due to limited access to model (black-box),\n188\na cap on the computation resources, or for saving\ntraining time i.e. industries fast deployment need.\nE.g. There is a constant stream of bugs relating\nto misclassified examples in production systems.\nOur RETRO NLU approach enables rapid adjust-\nment of the system’s behavior without retraining or\nestablishing a new model.\nEffect of Utterance Complexity. To answer\nRQ4(a), we analyse the retrieval augmented model\nperformance improvements (with full training) on\nsimple utterance with only one level in semantic\nrepresentation (depth-1) vs complex utterance with\nhierarchical semantic frames (compositional depth-\n2 and above). Figure 4 shows frame accuracy of\nwithout-nn, utterance-nn and semparse-nn model\nwith utterance complexity.\nFigure 4: Performance comparison (micro-avg) of\nRETRO NLU w.r.t original baseline (without-nn) with\nutterance complexity, i.e. simple and complex.\nPercentage (%)\n65\n70\n75\n80\n85\n90\nIntent \nPrecision\nIntent  Recall Slot \nPrecision\nSlot Recall\nSimple Complex\nFigure 5: Precision and Recall of intents and slots for\nsemparse-nn nearest neighbour w.r.t to gold semparse.\nAnalysis: As expected, all models perform rel-\natively poorly on complex utterances (79.5 %) in\ncomparison to simple utterances (85.5%). Interest-\ningly, both augmentation models equally improve\nperformance on simple queries. And with semantic-\nframe based augmentation we observe a substantial\nperformance improvement on complex challeng-\ning utterances, of 2%, with respect to the original\nbaseline (without-nn). This suggests, that by re-\ntrieving nearest neighbors and providing a model\nwith examples of complex parses, the model learns\nto apply it to a new request. Figure 5 shows pre-\ncision and recall for intents and slots in retrieved\nsemantic parses. The recall for intent and slot re-\ntrieval is 15 % lower for complex utterances. 12\nThus, highlighting one reason for a performance\ngap between simple and complex frames.\nEffect of Frame Rareness. To answer RQ4(b),\nwe analyze the retrieval augmented model perfor-\nmance improvement (with full training data) with\nframe rareness, as shown in Figure 6.\nFrame Frequency\nFrame Accuracy (%)70\n75\n80\n85\n90\n95\nVery Low Low Medium High Very High\nwithout-nn utterance-nn semparse-nn\nFigure 6: Performance of RETRO NLU w.r.t original\nbaseline (without-nn) with varying frame frequency.\nPercentage (%)\n65\n70\n75\n80\n85\n90\n95\n100\nIntent \nPrecision\nIntent  \nRecall\nSlot \nPrecision\nSlot Recall\nVery Low Low Medium High Very High\nFigure 7: Precision and Recall of intents and slots w.r.t\nto frame frequency for semparse-nn of theRETRO NLU.\nRare or uncommon frames are those example ut-\nterances whose ground truth semantic parses with-\nout slot value tokens appear infrequently in the\ntraining set. To analyze this, we divided the test set\ninto five equal sizes i.e., Very Low, Low, Medium,\nHigh, and Very Highsets, based on the frequency of\n12The precision gap was small 1% (intents) and 4% (slots).\n189\nsemantic frame structure. The experiment checks\nif performance improvement is mainly attributed\nto frequently repeating frames (frequent frames) or\nfor rarely occurring frames (uncommon frames).\nAnalysis: Figure 6 shows that all models perform\nworse on rare frames. This is expected as the para-\nmetric model gets less data for training on these\nframes. Furthermore, many of the low-frequency\nframes are also complex utterances with more than\none intent and have more slots too. Moreover, the\nnearest neighbour will be noisier for less frequent\nframes. This is evident from the lower values of\nprecision (20% gap) and recall (25% gap) on the\nintent and slots for nearest neighbors in Figure 7.\nFrequency\nRelative Accuracy Improvement (%)\n0\n1\n2\n3\n4\n5\nVery Low Low Medium High Very High\nutterance-nn semparse-nn\nFigure 8: Relative performance improvement of\nRETRO NLU w.r.t original baseline (without-nn) with\nvarying frame frequency.\nHowever, compared to original baseline\n(without-nn) the relative performance improve-\nment on rare frames with retrieval augmented\nmodel is more substantial, as shown in Figure 8.\nFor example, the relative improvement for Very\nLow frequency frames is 2.37 % (utterance-nn)\nand 4.11% (semparse-nn) compared to just 1.01%\n(utterance-nn) and 1.11 % (semparse-nn)for the\nVery High Frequency frames. We suspect this\nis because of the model’s ability to copy the\nrequired intent and slots from nearest neighbors\nif the parametric knowledge fails to generate it.\nThis shows the retrieval augmented model is even\nmore beneficial for the rare frames. As earlier,\nsemparse-nn outperform utterance-nn.\nEffect of the number of neighbors. To answer\nRQ5(a), we compare k = 1, 2, and 3 nearest neigh-\nbours for both utterance-nn and semparse-nn se-\ntups13 The results are reported in Table 2.\n13Extending beyond 3 neighbors was not useful for many\nreasons: (a) the BART 512 tokenization limit, (b) exponential\nrise in training time, and (c) only minimal performance gain.\n#neighbors k = 1 k = 2 k = 3\nwithout-nn 84.43 84.43 84.43\nutterance-nn 85.28 85.35 85.40\nsemparse-nn 85.74 85.81 85.80\nTable 2: Performance with increasing nearest neighbors.\nMetric Average Precision Average Recall\nIntent Farthest Closest Farthest Closest\nTrain 81.39 84.84 81.81 85.04\nValid 80.46 87.59 81.10 87.93\nTest 79.09 86.23 79.35 86.22\nSlot Farthest Closest Farthest Closest\nTrain 75.02 80.05 79.56 83.19\nValid 73.40 82.38 79.77 85.81\nTest 74.59 83.21 79.51 85.11\nTable 3: Intent-slots precision/recall for RETRO NLU\nsemparse-nn with closest/farthest neighbors.\nAnalysis: As shown in Table 2 the model perfor-\nmance only improves marginally with more nearest\nneighbors. We attribute this to the following two\nreasons (a) redundancy - many utterance exam-\nples can share the same frame, as evident from the\nhigh accuracy for frequent frame Figure 6., and\n(b) complexity - as kincreases, the problem is get-\nting harder for the model with longer inputs, more\nirrelevant and noisier inputs. To further verify the\nabove reasons, we examine the semparse-nn re-\ntrieve nearest neighbors quality by comparing the\nintent and slot both Precision and Recall score for\nclosest (k=1) and farthest (k=3) neighbor w.r.t to\nthe gold semparse. From Table 3 it is evident that\nprecision and recall for intents and slots decrease\nas we go down the ranked neighbors list. Adding\nmore nearest neighbour would only be beneficial\nwhen added neighbour capture diverse and differ-\nent semantic structure which is missing from earlier\nneighbor and essential for the correct semparse.\nEffect of Retrieval Quality. To check if our\nRETRO NLU model is robust to the noise in the\nretrieved examples (i.e. RQ5(b)), we analyse the\neffect of quality of retrieval by comparing semantic\nparsing accuracy of top neighbor augmented mod-\nels on the test data with (a) the top neighbour with\nrandom neighbor from domain other than the exam-\nple domain, and (b) random neighbor selected from\nthe top 100 ranked nearest neighbors in the index.\nIt should be noted that these 100 top rank nearest\nneighbour can have some redundant semparse-nn\nstructure, only slot values might differ. Figure 9\nshows the results of the experiments.\nAnalysis: From Figure 9 it is clear that quality\nof nearest neighbor affect the semantic parsing ac-\n190\nFrame Accuracy (%)\n84.0\n84.5\n85.0\n85.5\n86.0\nsemparse-nn utterance-nn\nwithout-nn cross-domain random top-100\nfirst neighbor\nFigure 9: Performance of RETRO NLU with varying\nnearest neighbor quality on test data.\ncuracy. We observe a 0.4 % drop when random\nneighbor from top 100 nearest neighbors is chosen,\ninstead of first neighbor, the small drop is because\nof redundancy in intent/slots structure between ex-\namples, only slots value could be major difference.\nHowever, the performance is still 0.9 % to 1.0%\nbetter than the one without the nearest neighbor.\nWe suspect this is because of the fact that the data\nhas many utterances with similar semparse output.\nUpon deeper inspection we found that top-100 still\nincludes many relevant frames, and therefore ran-\ndom examples from top-100 are often still relevant.\nFurthermore, there is also frame redundancy, many\ndifferent utterance queries have similar semantic\nparse frames structure and only differ at the slot\nvalues. This is also evident from table 2 which\nshows adding more neighbors is not beneficial, be-\ncause of frame redundancy. Surprisingly, we also\nobserve that the model performance with random\ncross-domain neighbor is better than without-nn\nfor semparse-nn by 0.5 %. This shows that the\nmodel knows when to ignore the nearest neigh-\nbors and when to rely on the parametric model.\nFurthermore, it also indicates that underlying para-\nmetric model parameters is improved by retrieval\naugmented training for the semparse-nn.\nFor the utterance-nn the performance drops\nwhen testing on cross-domain nearest neighbor aug-\nmented example. Thus, underlying the utterance-\nnn model is more sensitive than semparse-nn to the\nnearest neighbor quality. In addition, we also con-\nducted an experiment in which we added the best\npossible neighbor based on the gold parse frame\nstructure. The trained model, though this approach\nwas not robust and relies too heavily on coping\nframes from neighbors, resulting in poor general-\nization. Our technique, on the other hand, with\nembedding-based retrieval, is good at generaliza-\ntion and has enhances the underlying parametric\nmodel. Overall, we can conclude that the semparse-\nnn and utterance-nn model are both quite robust to\nnearest neighbors quality. We can also conclude\nthat the semparse-nn model was able to capture\nricher information through additional similar in-\nputs than without-nn. However, to obtain the best\nperformance good quality neighbour is an essential.\n4 Comparison with Related Work\nTask-oriented Semantic Parsing. Sequence-to-\nsequence (seq2seq) models have recently achieved\nstate of the art results in semantic parsing (Ron-\ngali et al., 2020; Gupta et al., 2018), and they\nalso provide a flexible framework for incorporat-\ning session-based, complex hierarchical semantic\nparsing (Sun et al., 2019; Aghajanyan et al., 2020;\nCheng et al., 2020; Mehri et al., 2020) and multi-\nlingual semantic parsing (Li et al., 2021; Louvan\nand Magnini, 2020). Architectures, such as T5 and\nBART (Raffel et al., 2020; Lewis et al., 2020b),\nwith large pre-trained language models pushed the\nperformance even further. Such models are quite\ncapable of storing a lot of knowledge in their pa-\nrameters (Roberts et al., 2020), and in this work we\nexplore the benefits of additional non-parametric\nknowledge in a form of nearest neighbor retrieval\nfor the task of semantic parsing. To improve low\nresource seq2seq parsers Chen et al. (2020) have\nproposed looking at meta learning methods such\nas reptile, and Ghoshal et al. (2021) have intro-\nduced new fine-tuning objectives. Our approach is\nfocused on non-architecture changes to augment\ngeneration with retrieval and thus can be combined\nwith either of these approaches.\nIncorporating External Knowledge. An idea\nto help a model by providing an additional infor-\nmation, relevant to the task at hand is not new.\nThis includes both implicit memory tables (We-\nston et al., 2014; Sukhbaatar et al., 2015), as well\nas incorporating this knowledge explicitly as an\naugmentation to the input. Explicit knowledge\nare incorporated in one of the following two ways\n(a) suitable model architecture change to incorpo-\nrate dedicated extended memory space internally\ni.e. memory network (Bapna and Firat, 2019; Guu\net al., 2020; Lewis et al., 2020a; Tran et al., 2020)\nor span pointer networks (Desai and Aly, 2021;\nShrivastava et al., 2021), and (b) appending ex-\nample specific extra knowledge externally with the\n191\ninput example directly without modifying model ar-\nchitecture (Papernot and McDaniel, 2018; Weston\net al., 2018; Lewis et al., 2020c; Tran et al., 2020;\nKhandelwal et al., 2021; Fan et al., 2021; Chen\net al., 2018; Wang et al., 2019; Neeraja et al., 2021)\n. Retrieval-augmented approaches have been im-\nproving language model pre-training as well (Guu\net al., 2020; Lewis et al., 2020a; Tran et al., 2020).\nThe idea here is to decouple memorizing factual\nknowledge and actual language modeling tasks,\nwhich can help mitigate hallucinations, and other\ncommon problems.\nMultiple works like DkNN (Papernot and Mc-\nDaniel, 2018), RAG (Lewis et al., 2020c), kNN-\nLM (Tran et al., 2020), kNN-MT (Khandelwal\net al., 2021), and KIF-Transformer (Fan et al.,\n2021) show that external knowledge is useful for\nlarge pre-trained language models, and can help\nfine-tuning. DkNN shows that nearest neighbour\naugmented transformer-based neutral network is\nmore robust and interpretable. RAG shows that one\ncan append external knowledge to improve open do-\nmian, cloze-style question answering, and even fact\nverification task such as FEVER. kNN-LM shows\nthat for cloze task for fact completion, one can com-\nbine nearest neighbour predictions with original\nprediction using appropriate weighting to improve\nmodel performance. However, these works mostly\nstudy knowledge dependent question answering\ntask, while we are exploring a complex task of\nstructural prediction of semantic frame structures\nfor task-oriented dialog.\nVery recently, Pasupat et al. (2021) share similar\nfinding of exemplar augmentation and propose Con-\ntrollAble Semantic Parser via Exemplar Retrieval\n(CASPER). In their work, the semantic parser gets\nrelevant exemplars from a retrieval index, augments\nthem with the query, and then generates an output\nparse using a generative seq2seq model. The exem-\nplars serve as a control mechanism for the generic\ngenerative model: by modifying the retrieval index\nor the construction of the augmented query, one\nmay alter the parser’s behavior. Compare to them,\nour study focuses more on the influence of augmen-\ntation on the performance of the state-of-the-art\nCopy Transformer BART model for task-oriented\nsemantic parsing. By design, the copy transformer\neffectively utilizes it’s copy mechanism to get non-\nparametric information from augmented nearest\nneighbor semparse/utterances. Additionally, we\nconduct a detailed investigation of the influence of\nretrieval quality, utterance and semantic complex-\nity, and the rarity of semantic frames. We anticipate\nthat our findings will shed light on the potential ad-\nvantages of retrieval enhancing parametric neural\nnetworks for the complex structural task of task-\noriented semantic parsing.\n5 Conclusion and Future Work\nWe show that task-oriented semantic parsing per-\nformance can be enhanced by augmenting neu-\nral model-stored parametric information with non-\nparametric external memory. On the TOPv2\ndataset, we demonstrated that adding instances\nderived from a nearest neighbor index greatly\nimproves the semantic parsing performance of\na BART model with copy mechanism. Our\nRETRO NLU model is able to achieve higher accu-\nracy earlier with less training data (limited supervi-\nsion setting), which allows maintaining a large in-\ndex with annotated data, while using only a subset\nto train a model more efficiently. Lastly, we per-\nformed an analysis of performance improvements\non different slices, and found RETRO NLU to be\nmore effective on rarer complex frames, compared\nto a traditional seq2seq model.\nRETRO NLU extensions, we focus on joint train-\ning of retrieval and parsing components. Having\ntask specific utterances representation can benefit\ni.e. finding utterances with similar semantic parse.\nExploring few/zero-shot performance could be in-\nteresting direction. Having an easily-updateable\nindex enables you to amend annotations, add new\nones, or remove existing ones, without affecting\nthe model. It will be useful to study other ap-\nproaches of sentence embedding, such as Reimers\nand Gurevych (2019). Finally, using cross-lingual\nrepresentations such as mBART (Liu et al., 2020),\ncould help multilingual semantic parsing.\nAcknowledgements\nWe thank members of the Utah NLP group for\ntheir valuable insights and suggestions at various\nstages of the project; and reviewers their helpful\ncomments. Additionally, we would like to express\nour gratitude to Xilun Chen, Asish Ghoshal, Arash\nEinolghozati, Shrey Desai, Anchit Gupta, Abhinav\nArora, Sonal Gupta, Alexander Zotov, Ahmed Aly,\nand Luke Zettlemoyer of Meta (Formerly Facebook\nAI) for their insightful feedback and suggestions.\n192\nReferences\nArmen Aghajanyan, Jean Maillard, Akshat Shrivastava,\nKeith Diedrick, Michael Haeger, Haoran Li, Yashar\nMehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis,\nand Sonal Gupta. 2020. Conversational semantic\nparsing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5026–5035, Online. Association for\nComputational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 1921–1931, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. 2021. Ma-\nchine unlearning. In 2021 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 141–159. IEEE.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana\nInkpen, and Si Wei. 2018. Neural natural language\ninference models enhanced with external knowledge.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2406–2417, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nXilun Chen, Asish Ghoshal, Yashar Mehdad, Luke\nZettlemoyer, and Sonal Gupta. 2020. Low-resource\ndomain adaptation for compositional task-oriented\nsemantic parsing. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5090–5100, Online. As-\nsociation for Computational Linguistics.\nJianpeng Cheng, Devang Agrawal, Héctor\nMartínez Alonso, Shruti Bhargava, Joris Driesen,\nFederico Flego, Dain Kaplan, Dimitri Kartsaklis,\nLin Li, Dhivya Piraviperumal, Jason D. Williams,\nHong Yu, Diarmuid Ó Séaghdha, and Anders\nJohannsen. 2020. Conversational semantic parsing\nfor dialog state tracking. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8107–8117,\nOnline. Association for Computational Linguistics.\nShrey Desai and Ahmed Aly. 2021. Diagnosing trans-\nformers in task-oriented semantic parsing. In Find-\nings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 57–62, Online. Associa-\ntion for Computational Linguistics.\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine\nBordes. 2021. Augmenting transformers with KNN-\nbased composite memory for dialog. Transactions of\nthe Association for Computational Linguistics, 9:82–\n99.\nAsish Ghoshal, Xilun Chen, Sonal Gupta, Luke Zettle-\nmoyer, and Yashar Mehdad. 2021. Learning better\nstructured representations using low-rank adaptive\nlabel smoothing. In International Conference on\nLearning Representations.\nSonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-\nmar, and Mike Lewis. 2018. Semantic parsing for\ntask oriented dialog using hierarchical representa-\ntions. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2787–2792, Brussels, Belgium. Association\nfor Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020a. Pre-training via paraphrasing. In Advances in\nNeural Information Processing Systems, volume 33,\npages 18470–18481. Curran Associates, Inc.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020b.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020c.\n193\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950–2962, Online. Association for Computa-\ntional Linguistics.\nChunxi Liu, Qiaochu Zhang, Xiaohui Zhang, Kritika\nSingh, Yatharth Saraf, and Geoffrey Zweig. 2020.\nMultilingual graphemic hybrid ASR with massive\ndata augmentation. In Proceedings of the 1st Joint\nWorkshop on Spoken Language Technologies for\nUnder-resourced languages (SLTU) and Collabora-\ntion and Computing for Under-Resourced Languages\n(CCURL), pages 46–52, Marseille, France. European\nLanguage Resources association.\nSamuel Louvan and Bernardo Magnini. 2020. Recent\nneural methods on slot filling and intent classifica-\ntion for task-oriented dialogue systems: A survey.\nIn Proceedings of the 28th International Confer-\nence on Computational Linguistics, pages 480–496,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tur.\n2020. Dialoglue: A natural language understanding\nbenchmark for task-oriented dialogue. arXiv preprint\narXiv:2009.13570.\nJ. Neeraja, Vivek Gupta, and Vivek Srikumar. 2021.\nIncorporating external knowledge to enhance tabular\nreasoning. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2799–2809, Online. Association\nfor Computational Linguistics.\nNicolas Papernot and Patrick McDaniel. 2018. Deep\nk-nearest neighbors: Towards confident, inter-\npretable and robust deep learning. arXiv preprint\narXiv:1803.04765.\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.\nControllable semantic parsing via retrieval augmen-\ntation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7683–7698, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nSubendhu Rongali, Luca Soldaini, Emilio Monti, and\nWael Hamza. 2020. Don’t Parse, Generate! A Se-\nquence to Sequence Architecture for Task-Oriented\nSemantic Parsing, page 2962–2968. Association for\nComputing Machinery, New York, NY , USA.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nAkshat Shrivastava, Pierce Chuang, Arun Babu, Shrey\nDesai, Abhinav Arora, Alexander Zotov, and\nAhmed Aly. 2021. Span pointer networks for non-\nautoregressive task-oriented semantic parsing. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 1873–1886, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nSainbayar Sukhbaatar, arthur szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nAdvances in Neural Information Processing Systems,\nvolume 28. Curran Associates, Inc.\nYibo Sun, Duyu Tang, Jingjing Xu, Nan Duan, Xi-\naocheng Feng, Bing Qin, Ting Liu, and Ming Zhou.\n2019. Knowledge-aware conversational semantic\nparsing over web tables. In Natural Language Pro-\ncessing and Chinese Computing , pages 827–839,\nCham. Springer International Publishing.\nChau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020.\nCross-lingual retrieval for iterative self-supervised\ntraining. In Advances in Neural Information Process-\ning Systems, volume 33, pages 2207–2219. Curran\nAssociates, Inc.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu,\nKartik Talamadupula, Ibrahim Abdelaziz, Maria\nChang, Achille Fokoue, Bassem Makni, Nicholas\nMattei, and Michael Witbrock. 2019. Improving nat-\nural language inference using external knowledge\nin the science questions domain. volume 33, pages\n7208–7215.\nJason Weston, Sumit Copra, and Antoine Bordes. 2014.\nMemory networks. arXiv preprint arXiv:1410.3916.\n194\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and refine: Improved sequence gener-\nation models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92, Brussels, Belgium. Association for\nComputational Linguistics.\nA Qualitative Examples\nBelow, we provide some qualiative example from\nthe TOPv2 datasets, in the first box, we shows\nthe input utterance, nearest neighbour utterance\ni.e. nearest-nn utterance and it corresponding\nsemantic parse i.e. semparse-nn (from the training\nset). In the second box we showed expected gold\nsemantic parse (expected-sparse) and the semantic\nparse prediction with baseline i.e. without\nnearest neighbor (without-nn), and our models\nutterance only nearest neighbor, where the input is\nappending with nearest-nn utterance (utterance-nn)\nand semantic augmented model i.e. (semparse-nn),\nwhere the input is augmented with semantic parse\nof first nearest neighbor. We can clearly see from\nthe examples that augmented nearest neighbor en-\nhance models in identifying correct intent and slots.\nexample 1 input\nutterance : message just lizzie and trent from my group if\nthey have any updates yet ?\nnearest-nn utterance : message kira and lena saying want\nto get drinks this week ?\nnearest-nn semparse : [in:send_message message\n[sl:recipient kira ] and [sl:recipient lena ] saying\n[sl:content_exact want to get drinks this week ]?]\nexample 1 output\nexpected-parse : [in:send_message [sl:recipient lizzie ]\n[sl:recipient trent ] [sl:content_exact they have any updates\nyet ]]\nwithout-nn : [in:get_message [sl:content_exact they have\nany updates yet ] [sl:group lizzie ] [sl:group trent ] ]\nsemparse-nn : [in:send_message [sl:recipient lizzie ]\n[sl:recipient trent ] [sl:content_exact they have any updates\nyet ] ]\nutterance-nn : [in:send_message [sl:recipient lizzie ]\n[sl:recipient trent ] [sl:content_exact they have any updates\nyet ] ]\nIn example 1, the model misses the correct\nintent and corresponding slots completely, the\ncorrect intent is sending a message rather than\nreceiving a message is correctly identified by both\nsemparse-nn and utterance-nn.\nexample 2 input\nutterance : no more country\nnearest-nn utterance : no more music\nnearest-nn semparse : [in:stop_music [sl:music _type\nmusic ] ]\nexample 2 output\nexpected-parse : [in:remove_from_playlist_music\n[sl:music_genre country ] ]\nwithout-nn : [in:play_music [sl:music_genre country ] ]\nsemparse-nn : [in:remove_from_playlist_music\n[sl:music_genre country ] ]\nutterance-nn : [in:remove_from_playlist_music\n[sl:music_genre country ] ]\nIn example 2, the baseline model without\nnearest neighbour did the exact opposite of\nintended task of removing music of genre country\nfrom the playlist. However, after augmenting\nnearest neighbor context the model quickly\ncorrect the expected intent and slot. It should\nalso be noted the both the correct intent and\nslot (i.e. in:remove _from_playlist_music and\nsl:music_genre) are not present in the nearest-nn\nsemparse but it do contain similar intent and\nslot (i.e. in:stop _music. and sl:music _type),\nwhich help retrieval augmented model in correct\nprediction. As earlier the model is able to predict\ncorrect even with utterance only augmentation too.\nexample 3 input\nutterance : block all songs of mariah carey\nnearest-nn utterance : delete mariah carey songs\nnearest-nn semparse : [in:remove_from_playlist_music\ndelete [sl:music_artist_name mariah carey] [sl:music_type\nsongs ] ]\nexample 3 output\nexpected-parse : [in:remove_from_playlist_music\n[sl:music_artist_name mariah carey ] ]\nwithout-nn : [in:unsupported_music [sl:music_type songs\n]]\nsemparse-nn : [in:remove_from_playlist_music\n[sl:music_type songs ] [sl:music_artist_name mariah carey\n] ]\nutterance-nn : [in:remove_from_playlist_music\n[sl:music_type songs ] [sl:music_artist_name mariah carey\n] ]\nIn example 3 the model without nearest neighbor\naugmentation struggle to identify the intent from\nutterance text token “block\" therefore prediction\nunsupported music as the intent and the music type\nas songs, however the model with augmented near-\nest neighbour example with “delete\" intended slot\n195\nPercentage 10 % 20 % 30 %\nDomain w/o nn uttr-nn sem-nn w/o nn uttr-nn sem-nn w/o nn uttr-nn sem-nn\nAlarm 80.50 84.05 83.60 83.71 84.89 85.76 84.22 85.93 82.92\nEvent 68.56 78.33 79.38 75.01 80.85 82.32 77.64 81.91 82.92\nMusic 69.12 75.74 73.23 74.09 77.53 77.34 75.6 78.01 78.13\nTimer 71.63 76.76 76.27 75.51 76.18 79.28 77.21 79.68 79.84\nNavigation 74.30 73.86 76.44 77.89 79.40 79.96 80.11 81.79 81.61\nMessaging 84.38 87.30 89.44 88.39 91.31 91.50 89.53 92.78 92.25\nTable 4: Limited training setting results on various domain with original baseline (without-nn), RETRO NLU model utterance-nn\nand semparse-nn, shown here as w/o nn, utter-nn and sem-nn respectively.\n#neighbour’s one two three\nDomain w/o nn uttr-nn sem-nn w/o nn uttr-nn sem-nn w/o nn uttr-nn sem-nn\nAlarm 86.67 87.17 88.57 86.67 87.77 87.87 86.67 87.68 87.90\nEvent 83.83 85.03 84.77 83.83 84.92 85.26 83.83 85.26 85.34\nMusic 79.80 80.73 80.71 79.80 80.71 81.50 79.80 80.52 81.11\nTimer 81.21 81.75 81.01 81.21 81.04 82.29 81.21 81.44 82.10\nMessaging 93.50 94.52 94.65 93.50 94.92 95.05 93.50 94.88 94.92\nNavigation 82.96 84.16 85.20 82.96 84.12 84.46 82.96 84.59 84.79\nTable 5: Effect of number of nearest neighbours of RETRO NLU performance across domains\ncorrect identified both the intent and slots. Further-\nmore, using nearest neighbor augmentation, the\nmodel resolves the active passive voice confusion.\nB Domain based Limited Training Setting\nIn Table 4 shows the performance of model for\neach domain on original baseline (without-nn), and\nRetroNLU model utterance-nn and semparse-nn\nwith varying amount of supervised training data.\nOverall, semparse-nn outperform utterance-nn over\nmost of the domains. Surprising, we also found\nthat for few domain (with large number of sam-\nples) utterance-nn perform marginally better than\nsemparse-nn, need to investigate exact reason for\nthat. As expected both model utternace-nn and\nsemparse-nn perform much better than original\nbaseline which is without any nearest neighbour\naugmentation.\nC Domain Specific Effect of Nearest\nNeighbours\nIn Table 5 we shows the performance of model\nfor each domain on original baseline (without-nn),\nand RetroNLU model utterance-nn and semparse-\nnn with varying number of nearest neighbour aug-\nmented. We found the utternace-nn performance\nincreases with increasing number of neighbours\nwhere semparse performance remain mostly con-\nstant after the first neighbour augmentation for\nmany domains. We suspect this is due to the fact\nthat the data contains a large number of utterances\nwith identical semparse output.. There is also frame\nredundancy, since many unique utterance inquiries\nhave comparable semantic parse frames structure\nwith differences only on slot values.\n196"
}