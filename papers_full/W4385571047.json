{
  "title": "ContraCLM: Contrastive Learning For Causal Language Model",
  "url": "https://openalex.org/W4385571047",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5041973252",
      "name": "Nihal Jain",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032721392",
      "name": "Dejiao Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103968040",
      "name": "Wasi Uddin Ahmad",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100371707",
      "name": "Zijian Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101890760",
      "name": "Nan Feng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100355078",
      "name": "Xiaopeng Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113541520",
      "name": "Ming Tan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5109250040",
      "name": "Ramesh Nallapati",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5064541855",
      "name": "Baishakhi Ray",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077875900",
      "name": "Parminder Bhatia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5104143781",
      "name": "Xiaofei Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5107249743",
      "name": "Bing Xiang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W3176015924",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W4281944434",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W4281759695",
    "https://openalex.org/W3128699722",
    "https://openalex.org/W4224313754",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3206240757",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W4287552504",
    "https://openalex.org/W4221166942",
    "https://openalex.org/W3196790170",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W4249573750",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4307418160",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W3171153522",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W2996657533",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W4394664141",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2250790822"
  ],
  "abstract": "Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, Bing Xiang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6436–6459\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCONTRA CLM: Contrastive Learning For Causal Language Model\nNihal Jain∗, Dejiao Zhang∗, Wasi Uddin Ahmad∗,\nZijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati,\nBaishakhi Ray, Parminder Bhatia, Xiaofei Ma, Bing Xiang\nAWS AI Labs, USA\nAbstract\nDespite exciting progress in causal language\nmodels, the expressiveness of their representa-\ntions is largely limited due to poor discrimina-\ntion ability. To remedy this issue, we present\nCONTRA CLM , a novel contrastive learning\nframework at both the token-level and the\nsequence-level. We assess CONTRA CLM on a\nvariety of downstream tasks. We show that\nCONTRA CLM enhances the discrimination\nof representations and bridges the gap with\nencoder-only models, which makes causal lan-\nguage models better suited for tasks beyond\nlanguage generation. Specifically, we attain\n44% relative improvement on the Semantic Tex-\ntual Similarity tasks and 34% on Code-to-Code\nSearch tasks. Furthermore, by improving the\nexpressiveness of representations, CONTRA -\nCLM also boosts the source code generation\ncapability with 9% relative improvement on\nexecution accuracy on the HumanEval bench-\nmark. 1\n1 Introduction\nCausal Language Models (CLM) have seen remark-\nable success in language generation, both in natural\nlanguage (Radford et al., 2018, 2019; Brown et al.,\n2020) and programming language (Chen et al.,\n2021; Nijkamp et al., 2022). However, one limita-\ntion at their core is the poor discrimination ability\nof the representations, which often causes a large\nperformance gap with encoder-only or encoder-\ndecoder models on discriminative tasks (see Ap-\npendix D.1), and hence limits the wide usage of\nCLM beyond language generation.\nPrior studies posit that the anisotropy issue,\ni.e., representations being squeezed into a tiny cone\nin the vector space (Ethayarajh, 2019), can be the\nmain cause of the poor discrimination ability of\n∗Equal Contribution. Correspondence to Dejiao Zhang\n<dejiaoz@amazon.com>.\n1We release our code at https://github.com/\namazon-science/ContraCLM.\nlanguage models across different architectures and\nobjectives. Many efforts have focused on resolving\nthe anisotropy issue on encoder-only or encoder-\ndecoder models, either through post-processing (Su\net al., 2021; Li et al., 2020) or integrating differ-\nent regularization terms into the training objective\n(Gao et al., 2019; Wang et al., 2020). A recent work\n(Su and Collier, 2022) shows that the decoder-only\nCLM does not suffer from the anisotropic prob-\nlem as long as the model is beyond a certain size.\nHowever, we find that such conclusions can vary\nacross domains. As shown in Figure 1a, CLMs pre-\ntrained on text, i.e., GPT-2 (Radford et al., 2019),\ndo yield representations with good isotropy and dis-\ncrimination as long as the model is not smaller than\n774M parameters (GPT2-Large), whilst CodeGen\n(Nijkamp et al., 2022), pretrained on programming\nlanguage data, consistently suffers from anisotropy\nand poor discrimination across different model\nsizes. Therefore, an effective training strategy is\nstill essential for CLMs to improve representation\nquality with better isotropy and discrimination (Fig-\nure 1b). We conjecture that this is essential not only\nfor models suffering from inferior representations,\ne.g., CodeGen, and GPT2 (124M) but also for those\nwith a good starting point (suffer less e.g., GPT2-\nLarge (774M)).\nWe argue that an ideal CLM should yield\nisotropic representations to better leverage the rep-\nresentation space, as well as discriminative rep-\nresentations such that tokens or sequences from\nthe same context are mapped to comparatively\ncloser locations in the vector space compared to\nthose from randomly sampled contexts. To this\nend, we developed CONTRA CLM , a novel con-\ntrastive learning framework at both the token-level\nand sequence-level.\nCONTRA CLM is able to promote more uni-\nformly distributed and hence isotropic representa-\ntions by separating the instances at different seman-\ntic levels, e.g., tokens or sequences, apart from each\n6436\n108 109 1010\nModel Size\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Isotropy\n108 109 1010\nModel Size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Discrimination\nGPT2\nCodeGen\n(a) The isotropy and discrimination abilities of the representa-\ntions yielded by a model do not always improve with increase\nin model sizes.\nGPT2 GPT2-Large CodeGen\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Isotropy\nGPT2 GPT2-Large CodeGen\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Discrimination\nOriginal\nContraCLM\n(b) CONTRA CLM can effectively enhance both isotropy and\ndiscrimination, regardless of whether the original models suf-\nfer from degenerate representations or not.\nFigure 1: Evaluating the representation quality with respect to both isotropy := 1 −intra-similarity and\ndiscrimination := 1−intra-similarity / inter-similarity, where intra-similarity refers to the average cosine similarity\nbetween tokens from the same sequence and inter-similarity is defined with respect to tokens from two randomly\nsampled sequences. We use WIT (Srinivasan et al., 2021) and code search dataset (Guo et al., 2022) to evaluate\nGPT2 and CodeGen models, respectively.\nother. CONTRA CLM improves the discrimination\nof representations due to the implicit grouping ef-\nfect on semantically similar instances, yielded by\npulling together the variations that preserve seman-\ntics or positive pairs, of the same instance (Wang\nand Isola, 2020; Wang and Liu, 2021; Zhang et al.,\n2021).\nA natural question arises as to how would the\nimproved representations affect the generation abil-\nity of CLMs. Towards addressing this, we assess\nCONTRA CLM on language generation tasks in dif-\nferent domains, where we achieve better MAUVE\n(Pillutla et al., 2021) on text generation and 9%\nrelative improvement on pass@1 accuracy on Hu-\nmanEval (Chen et al., 2021). The improvement\nin code completion is indeed significant as it re-\nflects that more model-generated programs pass\na suite of test cases. On the discriminative tasks,\nCONTRA CLM attains 44% relative improvement\non Semantic Textual Similarity tasks and 34% on\nCode-to-Code Search tasks, which largely bridges\nthe gap with the encoder-only or encoder-decoder\nmodels (see Section 4.4 and Appendix D.1). Such\nimprovements allow us to boost the performance\nof decoder-only models on a wide range of dis-\ncriminative tasks where encoder-only models are\ncurrently the workhorse.\n2 Related Work\nAnisotropic Representation of Language Mod-\nels Despite the remarkable success achieved by\nlanguage models (Devlin et al., 2019; Radford et al.,\n2019; Yang et al., 2019; Raffel et al., 2020; Lewis\net al., 2020), they suffer from the anisotropy issue\nwhere the representations are distributed into a tiny\ncone in the vector space (Gao et al., 2019; Etha-\nyarajh, 2019; Li et al., 2020; Wang et al., 2020).\nIn particular, Ethayarajh (2019) shows that the de-\ngeneration is severer on CLM, where the average\ncosine similarity between two words sampled from\nrandomly selected sequences is almost at one when\nevaluating the outputs from the last hidden layer\nof GPT-2 (Radford et al., 2019). However, Su and\nCollier (2022) show that CLMs (Radford et al.,\n2019) are indeed coherent as long as the model is\nlarger than a certain size. We find such conclusions\ncan vary across domains, e.g., when pretraining on\ncode, CodeGen (Nijkamp et al., 2022) consistently\nsuffers from the anisotropy issue over a wide range\nof model sizes. On the bright side, Figure 1b shows\nthat CONTRA CLM can effectively improve the rep-\nresentation quality when we continue to train the\nexisting CLMs with our proposed objectives, re-\ngardless of whether the CLMs suffer from inferior\nrepresentations initially.\nContrastive Learning Contrastive learning\n(Chen et al., 2020; He et al., 2020) has seen remark-\nable successes in Natural Language Processing\n(NLP). A large amount of research has focused on\nsentence representation learning for encoder-only\nmodels, with the main differences lying in how\nthe augmentations are generated (Fang and Xie,\n2020; Giorgi et al., 2021; Wu et al., 2020; Meng\net al., 2021; Yan et al., 2021; Kim et al., 2021; Gao\net al., 2021; Zhang et al., 2022). Recently there\nis an emerging interest in developing effective\ncontrastive learning approaches for text generation\nmodels. However, most existing work mainly\n6437\nfocuses on the encoder-decoder structure (Dong\net al., 2019; Raffel et al., 2020; Lewis et al., 2020)\nby contrasting suboptimal model generations\nobtained via diverse sampling (An et al., 2022)\nor adding perturbations on the embedding space\n(Lee et al., 2021), against the ground truth. On\nthe other hand, it is not intuitive to develop\nan effective contrastive learning strategy for\ndecoder-only models. A recent work (Su et al.,\n2022) proposes SimCTG, a token-level contrastive\nlearning approach that aims to separate each token\napart from others within the same sequence by\na predefined distance. As shown in Section 4,\nour temperature-based token-level contrastive\nlearning approach, CONTRA CLM-T OK, con-\nsistently outperforms SimCTG across different\ntasks. We conjecture that the fixed margin-based\nobjective allows less flexibility for the token-level\nrepresentation separation, especially considering\nhow the semantic relevance among tokens can vary\nacross contexts (sequences).\nCode Generation and BeyondLanguage mod-\neling for source code is a fast growing area of re-\nsearch. Various model architectures have been ex-\nplored recently, including encoder-only (Feng et al.,\n2020; Guo et al., 2021), encoder-decoder (Ahmad\net al., 2021; Wang et al., 2021; Li et al., 2022), and\ndecoder-only models (Chen et al., 2021; Nijkamp\net al., 2022; Chowdhery et al., 2022). Among them,\nthe decoder-only models have been found to be ef-\nfective for code generation. However, as shown in\nSection 4.3.2 and Appendix D.1, they suffer from\nunsatisfactory performance on various discrimina-\ntive tasks (Lu et al., 2021; Huang et al., 2021; Guo\net al., 2022). This motivates us to improve the\ndecoder-only models on the discriminative tasks to\nextend their main usage beyond language genera-\ntion. Furthermore, code is fundamentally different\nfrom natural language in that it is more structured,\nwhich helps validate the generalization of our ap-\nproach beyond plain text.\n3 Model\n3.1 Causal Language Modeling\nLet x = [x1,x2,··· ,x|x|] denote a sequence with\nvariable length |x|, e.g., a piece of text or a code\nsnippet. Causal Language Modeling (CLM) is usu-\nally formulated as sequence distribution estima-\ntion over a set of sequences, x1,x2,..., xN . For\ntractable estimation, common practice is to fac-\ntorize the joint distribution of each sequence into\nthe product of conditional token prediction proba-\nbilities. The model is then trained via maximum\nlikelihood estimation as follows,\nLCLM = −1\nN\nN∑\nj=1\n|xj|∑\ni=1\nlog p(xj\ni |xj\n<i) .\nHere xj\n<i = [xj\n1,..., xj\ni−1] denotes the subse-\nquence before xj\ni and |xj|is the sequence length.\n3.2 Contrastive Learning for CLM\nLet h(i),h(i+) denote two representation variations\nof the same instance that preserve semantics, or\na positive pair for contrastive learning. Then de-\nnote I = {1,2,...,N }∪{ 1+,2+,...,N +}as\nthe set of representation indices associated with\nN instances. Further, let τ denote the temperature\nhyper-parameter and ⋄denote cosine similarity. We\nthen minimize the following,\nL=\nN∑\nj=1\n−\n(\nlog exp(h(j) ⋄h(j+)/τ)∑\nk∈I\\j exp(h(j) ⋄h(k)/τ)\n+ log exp(h(j+) ⋄h(j)/τ)∑\nk∈I\\j+ exp(h(j+) ⋄h(k)/τ)\n)\n.\nNote that in our setting, an instance can refer to\neither a token or a sequence. When h(j),h(j+) de-\nnote a pair of representation variations of the j-th\ntoken within a sequence, N is the sequence length\nthat can vary across sequences; in this case the ob-\njective is LTok. For the sequence-level contrastive\nloss, h(j),h(j+) refer to the pair of representations\nof the j-th sequence within a batch, and N denotes\nthe batch size; in this case the objective is LSeq.2\nTherefore, when applied at both token-level and\nsequence-level, the contrastive learning objective\ndefined above tries to separate tokens at each dis-\ntinct location apart from every other token within\nthe same sequence, and sequences within the same\nrandomly sampled batch apart from each other. In-\ntuitively, such separation can improve the unifor-\nmity (isotropy) of the representations. Further, bet-\nter discriminative representations are achieved due\nto the implicit grouping effect of contrastive learn-\ning on semantically similar instances. Such group-\ning effect of contrastive learning has been studied\nin recent work (Wang and Liu, 2021; Zhang et al.,\n2021; Wang and Isola, 2020) as well.\n2Please refer to Appendix A for the complete formulations.\n6438\n3.3 C ONTRA CLM\nIn addition to the causal language modeling loss,\nCONTRA CLM optimizes the contrastive learning\nobjective defined in Equation (1) at both the token-\nlevel (LTok) and sequence-level (LSeq) as follows\nLCONTRA CLM = LCLM + LTok + LSeq .\nFurthermore, to understand how the token-level\nand sequence-level contrastive learning contribute\nto the overall performance, we assess the perfor-\nmance of LCONTRA CLM-T OK = LCLM + LTok and\nLCONTRA CLM-S EQ = LCLM + LSeq in Section 4.\nUnless otherwise specified, we weigh each loss\nequally and set the temperature τ = 0.05. Al-\nthough better performance can be achieved by hy-\nperparameter optimization, we mainly investigate\nhow CONTRA CLM improves the representation\nquality and the zero-shot transfer learning perfor-\nmance. We hence leave hyperparameter optimiza-\ntion in a supervised setting as future work.\nPositive pair of representations For GPT-2\n(Radford et al., 2019), we consider the simple yet\neffective dropout-based augmentation (Gao et al.,\n2021), where the positive pair of representations is\nobtained by performing a forward pass of the same\nsequence twice. On the other hand, for CodeGen\n(Nijkamp et al., 2022), we simply duplicate the rep-\nresentation of each instance as positive pair for an\napples-to-apples comparison since dropout is dis-\nabled during its initial pretraining stage. Unlike the\nexisting findings that the dropout-based augmenta-\ntion can boost the contrastive learning performance\nwhen (continually) training a language model, we\nfind that the trends can vary when evaluating on\ndiscrimination tasks and generation tasks. A de-\ntailed ablation study can be found in Section 4.4\nand Appendix D.2.\n4 Experiments\nTo demonstrate the effectiveness of our proposed\nframework in different application domains, we\nevaluate our models and baselines on natural lan-\nguage and programming language tasks. We de-\nsign our experiments to address – (1) Does con-\ntrastive learning improve the discrimination abil-\nity of representations? (2) Do the representations\nlearned by contrastive learning lead to better per-\nformance on language generation tasks? (3) Is\nthe joint contrastive learning at both token- and\nsequence-level necessary, and how do they bene-\nfit from each other? (4) How does the impact of\ncontrastive learning vary across language domains?\n4.1 Data and Models\nData & Models For text, we continue training\nGPT-2 (124M) (Radford et al., 2019) on WikiText-\n103, a collection of over 100 million tokens ex-\ntracted from the set of verified Good and Featured\narticles on Wikipedia (Merity et al., 2017). For\ncode, we continue training CodeGen 350M mono-\nlingual (Nijkamp et al., 2022) on collected permis-\nsively licensed Python code from GitHub. Please\nrefer to Appendix B for the training details. We\nconsider the following objectives for the continual\ntraining of both GPT-2 and CodeGen:\n• CLM. The standard left-to-right autoregression\nobjective for training causal language models,\nwhich is also the objective used for pretraining\nboth GPT-2 and CodeGen.\n• SimCTG (Su et al., 2022). A predefined margin3\nbased token-level contrastive learning framework\nthat aims to separate tokens at each distinct loca-\ntion within a sequence apart from each other.\n• CONTRA CLM-T OK & CONTRA CLM-S EQ.\nAs defined in Section 3.3, these two are obtained\nby combining the CLM objective with our pro-\nposed token-level or sequence-level contrastive\nloss, respectively. This investigation allows us\nto better understand how our token-level and\nseqeunce-level contrastive losses contribute to\nthe overall performance of CONTRA CLM.\n4.2 Evaluation on Natural Language\nWe first evaluate our model on discrimination and\ngeneration tasks in natural language.\n4.2.1 Semantic Textual Similarity\nWe assess CONTRA CLM on semantic textual simi-\nlarity (STS), the most commonly used benchmark\nfor evaluating the semantic discrimination capa-\nbility of representations. STS consists of seven\ntasks, namely STS 2012-2016 (Agirre et al., 2012,\n2013, 2014, 2015, 2016), the STS Benchmark (Cer\net al., 2017), and the SICK-Relatedness (Marelli\net al., 2014). In this benchmark, human annotators\nprovide a fine-grained similarity score from 0 to\n5 for each sequence pair. Following Reimers and\nGurevych (2019), for the sequence pairs in each\ndataset, we report the overall Spearman’s correla-\ntion between the cosine similarities of representa-\n3For all experiments in this section, we set the margin\nρ= 0.5 as recommended in Su et al. (2022).\n6439\nModel STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg.\nGPT2 25.84 28.90 26.20 34.74 35.70 42.72 26.27 31.48\nCLM 27.14 20.34 18.73 37.56 27.40 35.70 27.97 27.83\nSimCTG 30.32 37.10 31.99 39.68 42.73 46.26 25.27 36.19\nCONTRACLM-TOK 37.28 37.63 31.33 54.78 50.16 48.10 34.95 42.03\nCONTRACLM-SEQ 29.66 39.89 34.50 43.20 41.99 44.52 25.51 37.04\nCONTRACLM 37.54 45.23 36.41 56.74 50.30 51.52 39.49 45.32\nTable 1: Spearman rank correlation between the cosine similarity of sentence representation pairs and the ground\ntruth similarity scores.\n0.0 0.1 0.2 0.3\nGround Truth Similarity\n600\n700\n800\n900\n1000\n1100\n1200\n1300Similarity Rank (Descending)\nDissimilar Sequence Pairs\nGround Truth\nContraCLM-Seq\nContraCLM-T ok\nContraCLM\n0.7 0.8 0.9 1.0\nGround Truth Similarity\n100\n200\n300\n400\n500\n600\n700\n800\nSimilar Sequence Pairs\nSimilar Sequence PairModel Rank ↓\nS1:a woman is stabbing\na potato with a fork\nGround Truth 40\nCONTRACLM-SEQ 501\nS2:a woman is punctu-\nring a potato with a fork\nCONTRACLM-TOK 272\nCONTRACLM 251\nDissimilar Sequence PairModel Rank ↑\nS1:a man is opening a\nbox and taking out paper\nGround Truth 1310\nCONTRACLM-SEQ 400\nS2:a woman is peeling\na potato\nCONTRACLM-TOK 1054\nCONTRACLM 1181\nFigure 2: CONTRA CLM-T OK is essential for making the sequence-level representations robust to spurious patterns\nof words or phrases (results reported on STS-B). We scale the ground truth similarity scores from [0, 5] to [0,1].\ntions and the human-provided similarity scores in\nTable 1.\nEffectively Enhancing Discrimination Table 1\nshows that both GPT-2 and the one continually\ntrained with CLM perform poorly on STS, which\nis a consequence of poor discrimination: the co-\nsine similarities between semantically similar or\ndissimilar pairs are both almost at one (Figure 4\nin Appendix C.1). Also note that continuing to\ntrain GPT-2 with CLM on WikiText-103 worsens\nperformance, which can occur since the domains\nof WikiText-103 and the STS datasets are differ-\nent.4 In contrast, both CONTRA CLM and SimCTG\nlargely outperform GPT-2, yet still, CONTRA CLM\nattains 25% relative improvement over SimCTG.\nMoreover, CONTRA CLM-T OK outperforms Sim-\nCTG on almost all STS benchmarks and the trend\nremains the same even without the dropout-based\naugmentation (Appendix D.3). Therefore, we posit\nthat our temperature-based contrastive learning ob-\njective allows more flexibility towards separating\nrepresentations based on token semantics, whereas\nrequiring a predefined separation margin between\ntokens (as SimCTG does) is not ideal.\n4STS datasets include text from image captions, news\nheadlines and user forums. As a result, adapting GPT-2 to\nWikiText-103 reduces its transfer ability.\nCONTRA CLM-T OK vs. CONTRA CLM-S EQ\nTable 1 also indicates thatCONTRA CLM-T OK and\nCONTRA CLM-S EQ complement each other, as\nCONTRA CLM consistently performs better than\nboth of them on STS. Note that CONTRA CLM-\nSEQ performs worse than CONTRA CLM-T OK. It\nis surprising, especially since STS mainly assesses\nthe sequence-level representation quality. We inves-\ntigate this by dividing the sequence pairs into two\ngroups – semantically similar pairs with human-\nannotated similarity scores no less than 0.7 and dis-\nsimilar pairs with human scores no larger than 0.3.\nWe plot the rank of the model inferred similarity\nscores against the human similarity scores in Figure\n2 (left). As we can see, CONTRA CLM-S EQ strug-\ngles in ranking semantically dissimilar sequence\npairs higher and similar pairs lower. This suggests\nthat the token-level contrastive loss is essential for\nmaking the sequence-level representations robust\nto spurious patterns of tokens or phrases, e.g., rank-\ning semantically similar sequences with different\nsynonyms low and dissimilar sequences high even\nin presence of the same phrase (Figure 2 (right)).\n4.2.2 Text Generation\nNext, we assess the open-ended language genera-\ntion capability, where each model is required to gen-\nerate text continuations given the prefixes from the\n6440\nWikiText-103 test set. Following Su et al. (2022),\nwe set the lengths of prefix and continuation to\n32 and 128, respectively. We use nucleus sam-\npling (Holtzman et al., 2020) with top-p = 0.95.\nIn addition to Perplexity (PPL; evaluated on the\nground truth only) and MAUVE, we also evaluate\nthe discrimination of representations of generated\ntext under different settings in Table 2.\nCONTRA CLM Leads to More Semantically Co-\nherent Generations It is desired that contextual\ntoken representations within the same or semanti-\ncally similar sequences have relatively higher simi-\nlarities among each other when compared to simi-\nlarities between tokens sampled from random con-\ntexts. Therefore, given a prompt, lower discrimi-\nnation scores are desired between the ground truth\nand generation, while higher discrimination val-\nues are desired between generations for randomly\nsampled prompts.\nAs reported in Table 2, compared to CLM,\nContraCLM attains much better discrimination on\nthe generations under dissimilar context (prompts)\npairs, as indicated by the high value of Disc(D).\nFurther, ContraCLM and ContraCLM-Tok achieve\nbetter or at least comparable semantic coherence\nbetween the generation and the ground truth, as\nindicated by the MAUVE scores. We argue that,\nthe zero valued discrimination score between gen-\neration and ground truth, i.e., Disc(S), attained by\nGPT-2 and CLM does not imply better semantic co-\nherence – this is a consequence of their inferior rep-\nresentations evidenced by the zero discrimination\nscore between semantically irrelevant sequences.\nFinally, a slight increase in PPL is probably ex-\npected, considering that PPL is better aligned with\nthe standard CLM objective. Thereby, contrastive\nlearning can be interpreted as a regularization that\ntrades off between PPL and the desired representa-\ntion properties.\n4.3 Evaluation on Programming Language\nIn this section, we study the effectiveness of our\nproposed contrastive learning framework on pro-\ngramming language applications – code search,\ncode completion, and code re-ranking. Since Code-\nGen models are pretrained without dropout activa-\ntions, we follow the same for our models in this sub-\nsection helping us study the effectiveness of CON-\nTRA CLM without dropout augmentations. We also\ninvestigate how dropout would affect the decoder-\nonly models when evaluated on the downstream\nModel Generated Text\nPPL↓MAUVE↑Disc(S)↓Disc(D)↑\nGPT-2 47.50 0.893 0.00 0.00\nCLM 22.48 0.945 0.00 0.01\nSimCTG 22.51 0.952 0.11 0.54\nCONTRACLM-TOK 22.99 0.953 0.12 0.49\nCONTRACLM-SEQ 22.60 0.933 0.23 0.83\nCONTRACLM 23.01 0.947 0.18 0.62\nTable 2: Evaluation on the Wikitext-103 test set.\nDisc(D) is the discrimination score computed between\nthe generated continuations of two randomly sampled\nprompts. Disc(S) is computed between the ground truth\ntext and the generated one associated with the same\nprompt. A lower Disc(S) indicates better coherence\nwith the ground truth, while a higher Disc(D) indicates\nbetter representation discrimination among the genera-\ntions under different contexts. All metrics are evaluated\nover the entire test set.\ntasks in Section 4.4 and Appendix D.2.\n4.3.1 Code Search\nCode search is the task of retrieving relevant code\nfragments given a code fragment as a query. We\nperform in-language (query and relevant code are\nin the same language) and cross-language (query\nand relevant code are in different languages) code\nsearches. We provide an example in Figure 5 (Ap-\npendix C.2.2). In this study, we experiment in the\nzero-shot setting - we use the models described in\nSection 4.1 to generate dense representations of\ncode and perform a nearest neighbor search to re-\ntrieve relevant code fragments. We use publicly\navailable implementations of Guo et al. (2022).5\nContrastive Learning Yields More Discrimina-\ntive Code Representations For the code-to-code\nsearch task, Guo et al. (2022) used problem solu-\ntions in Ruby, Python, and Java languages from\nCodeNet (Puri et al., 2021). They propose to use\neach program as a query and retrieve all programs\nthat solve the same problem. We present detailed\nstatistics of the dataset in Table 6 (Appendix C.2.2).\nWe set the maximum sequence length as 5126 and\nuse cosine similarity between two mean vectors of\nthe last hidden states as relevance scores. We then\nsort the candidates by their scores to calculate the\nMean Average Precision (MAP) score. We present\n5https://github.com/microsoft/CodeBERT/tree/master/\nUniXcoder/downstream-tasks\n6We also performed experiments with maximum length\n1024 but didn’t observe any significant difference.\n6441\nModel Ruby Python Java Avg.\nRuby Python Java Ruby Python Java Ruby Python Java\nCodeGen 16.18 5.90 0.52 2.66 18.11 0.36 1.61 1.65 10.16 6.35\nCLM 16.36 6.67 0.80 3.07 15.72 0.46 1.41 2.11 10.25 6.32\nSimCTG 17.66 7.19 1.94 7.63 18.31 1.78 1.63 2.32 10.83 7.70\nCONTRACLM-TOK 18.02 7.84 2.51 8.76 20.46 2.48 1.91 2.58 11.43 8.44\nCONTRACLM-SEQ 16.76 5.45 1.06 7.40 16.74 1.41 1.55 2.25 10.23 6.98\nCONTRACLM 17.90 7.78 2.56 9.05 19.74 2.64 1.90 2.50 11.32 8.38\nTable 3: MAP score (%) of the zero-shot code search task. The language names mentioned in the top two rows\nindicate the languages queries and candidates are written in.\n[0, 0.5)\n0.08\n0.10\n[0.5, 0.75)\n0.20\n0.25\n[0.75, 1.0)\n0.45\n0.50\n0.55\nMAP score\nCLM SimCTG ContraCLM-Tok ContraCLM-Seq ContraCLM\n(a) Performance breakdown based on edit similarities (x-axis).\n[0, 64)\n0.175\n0.200\n0.225\n[64, 256)\n0.10\n0.12\n[256, 512)\n0.025\n0.030\nMAP score\n(b) Performance breakdown based on length differences (x-axis).\nFigure 3: Code search performances based on (a) and (b) between the query code fragments (in Python) and\ntheir relevant code fragments (in Python). We observe that in both cases, CONTRA CLM-T OK outperforms CLM,\nSimCTG, and CONTRA CLM-S EQ.\nthe results for the code search tasks in Table 3.7\nWe observe CONTRA CLM-T OK and CONTRA -\nCLM frameworks improve upon CodeGen trained\nwith CLM by 33.5% (absolute 2.12) and 32.6%\n(absolute 2.06) on average, respectively. We\nalso point out that the performance gap between\nCONTRA CLM-T OK and SimCTG are apples-to-\napples comparisons since the dropout-based aug-\nmentation is not used in either models. As afore-\nmentioned, the consistently better performance of\nCONTRA CLM-T OK suggests the superiority of our\ntemperature-based contrastive learning objective.\nOn the other hand, CONTRA CLM-S EQ improves\nover the CLM baseline by 10.4% only. Code search\nresults indicate that CONTRA CLM-S EQ performs\npoorly compared to CONTRA CLM-T OK. This per-\nformance gap is larger than what we observed in\nthe natural language evaluation. We conjecture that\nCONTRA CLM-T OK generates better discrimina-\n7We present a comparison with encoder-only and encoder-\ndecoder models in Table 7b in the Appendix.\ntive representations for code sequences since the\nfiner-grained understanding of the code tokens is\ncrucial to understanding the code sequences’ func-\ntionality (semantics). To verify this, we check if\nnon-semantic factors impact model performances\nin the following section.\nToken-level Contrastive Learning is Effective\nfor Code Understanding We break down the\ncode search performance based on edit similari-\nties and length differences between query code and\ntheir relevant code fragments. While edit similarity\nindicates how much queries and their relevant code\noverlap, the length difference indicates whether\nmodels effectively capture relevance between two\ncode fragments if they are similar in length or differ\nsignificantly. We present the results for Python lan-\nguage in Figure 3 (for all the languages, see Figures\n7 & 8 in Appendix C.2.3). The results show that\nCONTRA CLM-T OK outperforms CLM, SimCTG,\nand CONTRA CLM-S EQ irrespective of edit simi-\nlarities and length differences. Therefore, we can\n6442\nModel Pass@k Ranked Pass@k\nk=1 k=5 k=1 k=5\nCodeGen 12.65 16.89 13.42(+0.77)17.07(+0.18)\nCLM 13.42 18.08 15.38(+1.96)18.29(+0.21)\nSimCTG 13.26 17.29 15.24(+1.98)18.29(+1.00)\nCONTRACLM-TOK 12.96 17.01 15.24(+2.96)17.68(+0.67)\nCONTRACLM-SEQ 13.64 15.85 16.99(+3.35)16.46(+0.61)\nCONTRACLM 14.63 18.83 17.07(+2.44)18.90(+0.07)\nTable 4: Evaluation results on the HumanEval bench-\nmark. The numbers in the subscript indicate the dif-\nference between ranked pass@k and pass@k. While\nCONTRA CLM-T OK and CONTRA CLM-S EQ perform\ncompetitively to the baselines, CONTRA CLM signifi-\ncantly outperforms them.\nconclude that sequence overlap or length are not\nthe reasons for improvements in CONTRA CLM-\nTOK. Presumably, a finer-grained understanding\nof code tokens makes CONTRA CLM-T OK more\neffective for code representations.\n4.3.2 Code Completion and Re-Ranking\nGiven a sequence of tokens composed of natural\nlanguage, function signature, and input-output ex-\namples (as a whole, we call them prompt), the goal\nof the code completion task is to complete the func-\ntion. To evaluate the functional correctness of a\ncomplete code, we use existing benchmarks that in-\nclude unit tests. If the generated code successfully\npasses the unit tests, we refer to this as successful\nexecution. We compute pass@kfor k≤nfollow-\ning (Chen et al., 2021). In addition, we compare\nthe models on the code re-ranking task – given n\nsampled code using a code completion model, the\ngoal is to order the generated samples, for which\nwe use the mean log probability of each sampled\ncode (Chen et al., 2021). For code re-ranking evalu-\nation, we report ranked pass@k(Inala et al., 2022).\nFigure 6 (Appendix C.2.2) illustrates both the code\ncompletion and re-ranking tasks. We detail the\nevaluation metrics in Appendix C.2.1.\nContrastive Learning Improves Source Code\nGeneration Chen et al. (2021) introduced Hu-\nmanEval, a collection of 164 handwritten program-\nming problems and their respective unit tests. Each\nproblem in this dataset is presented using a prompt\nfor a function, and the task is to complete the func-\ntion, such that it can pass all unit tests. In all our\nexperiments, we use nucleus sampling (Holtzman\net al., 2020) with top p= 0.95. We sample n= 10\ncompletions per problem with sampling tempera-\nture 0.2. Table 4 presents the evaluation results on\nthe HumanEval benchmark.\nWhile CONTRA CLM-T OK and CONTRA CLM-\nSEQ perform comparably to CLM and SimCTG,\nCONTRA CLM outperforms them significantly,\ni.e., by 9% and 10.3% in terms of pass@1 accu-\nracy respectively, and by 11% and 12% in terms\nof ranked pass@1 accuracy, respectively. While\nCONTRA CLM-S EQ underperforms in code com-\npletion, it boosts code re-ranking significantly. We\nhypothesize the improvement is due to the con-\ntrastive learning’s alignment with the mean log\nprobability-based re-ranking choice.\n4.4 Discussion\nImpact of Dropout Dropout-based augmenta-\ntion (Gao et al., 2021) for contrastive learning on\nlanguage models has shown to have a significant\nimprovement on discriminative tasks. We observe\nthe same trend on both GPT-2 and CodeGen (see\nTable 8 in Appendix D.2). However, we observed\nthe opposite for language generation, no matter\nwhen training with CLM only or with contrastive\nlearning (see Table 9 in Appendix D.2). Dropout\nhas been one of the key ingredients for training\nlarge models. Further investigation on proper ways\nto use and evaluate it are indeed required. Never-\ntheless, even without dropout, Section 4.3 shows\nCONTRA CLM still yields significant improvement.\nBridge the Gap In comparison with the causal\n(left-to-right) attention mechanism of the decoder-\nonly models, the bidirectional attention mechanism\nbetter leverages the context of sequences, yielding\nbetter representations for discriminative tasks. Take\nthe encoder-only models as an example: as Table 7a\nin Appendix shows, both BERT-Base (Devlin et al.,\n2019) and RoBERTa-Base (Liu et al., 2019) outper-\nform GPT-2 by at least 60% relative performance\non STS. Although the performance gap between\nCodeGen and the encoder-only or encoder-decoder\nmodels decreases in Table 7b, it is still significant\nconsidering that both the model and pretraining\ndata sizes used by CodeGen are much larger. Such\na large performance gap severely limits the usage of\ndecoder-only models in many discriminative tasks.\nOn the bright side, contrastive learning shows the\npromise to bridge the gap,e.g., reducing the relative\nperformance gap between GPT-2 and the encoder-\nonly models by at least 50% when evaluating on\nSTS (see Table 7a). Please refer to Appendix D.1\nfor more detailed discussions.\n6443\n5 Conclusion\nIn this paper, we present CONTRA CLM , an effec-\ntive contrastive learning framework to resolve the\nrepresentation degeneration issue of CLMs trained\nwith the autoregression objective. We assess the\neffectiveness of CONTRA CLM on various down-\nstream tasks in both the natural language and code\ndomains, where we attain significant improvements\non both discrimination and generation tasks. While\nwe explored only the decoder-only CLMs, our pro-\nposed contrastive learning framework can serve as\na drop-in term for encoder-decoder, encoder-only,\nor prefixLM models also. We leave these explo-\nrations as future work.\nLimitations\nWhile our work displays many strengths, we high-\nlight some limitations. First, we focus on Python\nfor programming language evaluation, which is one\nof the most widely used programming languages.\nHowever, we believe that our proposed approach,\nCONTRA CLM , would benefit Code LMs trained\non any programming language. Second, the em-\npirical findings presented in this work are mainly\nbased on the smaller versions of GPT-2 and Code-\nGen with 124M and 350M parameters, respectively.\nHowever, as shown in Figure 1b, by continuing to\ntrain the pretrained models with our proposed ob-\njective, CONTRA CLM is able to address not only\nthe isotropy and poor discrimination issue that both\nGPT2-small and CodeGen suffer from, but also\nimprove the representation quality of GPT2-large\nwhich has a good starting point for both isotropy\nand discrimination. Therefore, we believe the effec-\ntiveness of CONTRA CLM should be applicable to\nlarger versions of these LMs, regardless of whether\nthey suffer from the anisotropy issue ( e.g., large\nCodeGen models) or not (large scale GPT-2 mod-\nels). We leave the explorations of larger models as\nfuture work.\nEthics Statement\nTraining data We use WikiText-103 and source\ncode in Python from permissively licensed GitHub\nrepositories to train GPT2 and CodeGen, respec-\ntively. We do not perform any preprocessing that\nwould get rid of any personally identifiable infor-\nmation or offensive content. However, the use of\ncode LMs comes with certain risks, e.g., generating\nbiased, toxic, and insecure code. We refer readers\nto Chen et al. (2021) (Section 7) for a detailed dis-\ncussion on the broader impact of code LMs.\nCompute We use an in-house cluster of 128\nA100s for all jobs in this paper. Each run takes\na couple of hours to one day to finish, depending\non the configuration and the model size. We per-\nformed one round of training for each setting as\nit is very expensive to repeat them multiple times.\nHowever, we perform the code completion and re-\nranking evaluation with three seeds. STS and code\nsearch evaluation do not need multiple runs of in-\nference (as the predictions are deterministic).\nAuthor Contributions\nDejiao and Wasi proposed the initial framework for\nCONTRA CLM and completed the paper writing.\nNihal and Dejiao setup the pretraining code. Ni-\nhal processed the pretraining data for the program-\nming language experiments. Dejiao designed and\ncompleted all natural language related training and\nevaluations. Nihal and Wasi completed the associ-\nated counterparts for programming language data.\nZijian was in-charge of the pretraining data collec-\ntion and multinode distributed training of CONTR -\nACLM models on the programming language data.\nFeng and Xiaopeng helped with our preliminary ex-\nplorations on natural language data evaluation. All\nthe other co-authors provided thought-provoking\ndiscussions and suggestions for this project, and\nhelped shape and proofread the paper draft.\nAcknowledgments\nWe thank all the helpful discussions and comments\nfrom colleagues at AWS AI Labs.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015),\npages 252–263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,\nMona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,\nRada Mihalcea, German Rigau, and Janyce Wiebe.\n2014. SemEval-2014 task 10: Multilingual semantic\ntextual similarity. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation (SemEval\n2014), pages 81–91.\n6444\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evaluation\n(SemEval-2016), pages 497–511.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393, Montréal, Canada. Association for Computa-\ntional Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Seman-\ntics (*SEM), Volume 1: Proceedings of the Main\nConference and the Shared Task: Semantic Textual\nSimilarity, pages 32–43.\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668,\nOnline. Association for Computational Linguistics.\nChenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong,\nXipeng Qiu, and Xuanjing Huang. 2022. CoNT:\nContrastive neural text generation. In Advances in\nNeural Information Processing Systems.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evalu-\nation (SemEval-2017), pages 1–14.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large lan-\nguage models trained on code. ArXiv preprint ,\nabs/2107.03374.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 1597–1607.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo,\nYang Zhang, Shiyu Chang, Marin Soljacic, Shang-\nWen Li, Scott Yih, Yoon Kim, and James Glass. 2022.\nDiffCSE: Difference-based contrastive learning for\nsentence embeddings. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4207–4218, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042–13054.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 55–65.\nHongchao Fang and Pengtao Xie. 2020. Cert: Con-\ntrastive self-supervised learning for language under-\nstanding. arXiv preprint arXiv:2005.12766.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\n6445\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019. Representation degeneration problem\nin training natural language generation models. In\n7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for un-\nsupervised textual representations. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 879–895.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,\nDuyu Tang, Shujie LIU, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tufano,\nShao Kun Deng, Colin Clement, Dawn Drain, Neel\nSundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.\n2021. Graphcode{bert}: Pre-training code represen-\ntations with data flow. In International Conference\non Learning Representations.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 9726–9735.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020.\nJunjie Huang, Duyu Tang, Linjun Shou, Ming Gong,\nKe Xu, Daxin Jiang, Ming Zhou, and Nan Duan.\n2021. CoSQA: 20,000+ web queries for code search\nand question answering. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5690–5700, Online. Association\nfor Computational Linguistics.\nJeevana Priya Inala, Chenglong Wang, Mei Yang, An-\ndres Codas, Mark Encarnación, Shuvendu K Lahiri,\nMadanlal Musuvathi, and Jianfeng Gao. 2022. Fault-\naware neural code rankers. In Advances in Neural\nInformation Processing Systems.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2528–2540.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In ACL.\nSeanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2021.\nContrastive learning with adversarial perturbations\nfor conditional text generation. In International Con-\nference on Learning Representations.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, MING GONG, Ming Zhou, Nan Duan, Neel\nSundaresan, Shao Kun Deng, Shengyu Fu, and Shu-\njie LIU. 2021. CodeXGLUE: A machine learning\n6446\nbenchmark dataset for code understanding and gener-\nation. In Thirty-fifth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 1).\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A SICK cure for the evaluation of\ncompositional distributional semantic models. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 216–223.\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song. 2021.\nCoco-lm: Correcting and contrasting text sequences\nfor language model pretraining. arXiv preprint\narXiv:2102.08473.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. A conversational paradigm for program\nsynthesis. arXiv preprint arXiv:2203.13474.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. MAUVE: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In Advances in Neural Information Pro-\ncessing Systems.\nRuchir Puri, David Kung, Geert Janssen, Wei Zhang,\nGiacomo Domeniconi, Vladimir Zolotov, Julian T\nDolby, Jie Chen, Mihir Choudhury, Lindsey Decker,\nVeronika Thost, Veronika Thost, Luca Buratti,\nSaurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan\nMalaika, and Frederick Reiss. 2021. Codenet: A\nlarge-scale ai for code dataset for learning a diversity\nof coding tasks. In Proceedings of the Neural Infor-\nmation Processing Systems Track on Datasets and\nBenchmarks, volume 1.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD ’20: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 3505–3506.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2443–2449.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for bet-\nter semantics and faster retrieval. arXiv preprint\narXiv:2103.15316.\nYixuan Su and Nigel Collier. 2022. Contrastive search\nis what you need for neural text generation. arXiv\npreprint arXiv:2210.14140.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. In Advances\nin Neural Information Processing Systems.\nFeng Wang and Huaping Liu. 2021. Understanding\nthe behaviour of contrastive loss. In Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition, pages 2495–2504.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2020. Improv-\ning neural language generation with spectrum control.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020.\nTongzhou Wang and Phillip Isola. 2020. Understanding\ncontrastive representation learning through alignment\nand uniformity on the hypersphere. In Proceedings of\nthe 37th International Conference on Machine Learn-\ning, ICML 2020, 13-18 July 2020, Virtual Event ,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 9929–9939.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8696–8708.\nXing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han,\nZhongyuan Wang, and Songlin Hu. 2022. ESim-\nCSE: Enhanced sample building method for con-\ntrastive learning of unsupervised sentence embed-\n6447\nding. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 3898–\n3907, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa,\nFei Sun, and Hao Ma. 2020. Clear: Contrastive\nlearning for sentence representation. arXiv preprint\narXiv:2012.15466.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. ConSERT: A con-\ntrastive framework for self-supervised sentence repre-\nsentation transfer. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5065–5075.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nDejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen\nLi, Henghui Zhu, Kathleen McKeown, Ramesh Nal-\nlapati, Andrew O. Arnold, and Bing Xiang. 2021.\nSupporting clustering with contrastive learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5419–5430.\nDejiao Zhang, Wei Xiao, Henghui Zhu, Xiaofei Ma,\nand Andrew Arnold. 2022. Virtual augmentation\nsupported contrastive learning of sentence represen-\ntations. In Findings of the Association for Com-\nputational Linguistics: ACL 2022 , pages 864–876,\nDublin, Ireland. Association for Computational Lin-\nguistics.\n6448\nSupplementary Material: Appendices\nA Contrastive Learning for CLM\nWe detail our proposed token-level and sequence-\nlevel contrastive losses. Before that, we first\ncall out the following notations that will be used\nthroughout this section. Let x = [x1,x2,··· ,x|x|]\ndenote a sequence with variable length |x|, e.g., a\ntext document or a code snippet, and h =\n[h1,h2,··· ,h|x|] be its representation output by\nthe last hidden layer of the decoder. For a randomly\nsampled batch B=\n{\nxj}N\nj=1 with N sequences,\nwe use xj\ni and hj\ni to denote the ith token and its rep-\nresentations in the jth sequence, respectively. Let\nhj,hj+\ndenote the representation pair of sequence\nxj and hj\ni ,hj+\ni correspond to the representations\nof the i-th token. Such representation pairs are re-\nferred to as positive pairs in contrastive learning,\nwhich are often obtained via data augmentation.\nA.1 Token-Level Contrastive Learning\nAs aforementioned, hj\ni ,hj+\ni are a pair of represen-\ntations for xj\ni , the i-th token in the j-th sequence.\nLet Ij = {1,2,..., |xj|}denote the indices of to-\nkens in xj. Further let τ denote the temperature\nhyper-parameter and ⋄denotes the cosine similar-\nity, i.e., a ⋄b = aT b/∥a∥2∥b∥2. Then we mini-\nmize LTok defined in Table 5.\nA.2 Sequence-Level Contrastive Learning\nLet IB = {1,2,...,N }∪{1+,2+,...,N +}de-\nnote indices of all 2N sequence-level representa-\ntions for batch B. The sequence-level contrastive\nloss is defined as LSeq in Table 5.\nB Training Details\nTraining Data For text, we use WikiText-103,\na collection of over 100 million tokens extracted\nfrom the set of verified and featured articles on\nWikipedia (Merity et al., 2017). For code, we\ncollect permissively licensed Python code from\nGitHub. Following (Chen et al., 2021; Nijkamp\net al., 2022), we perform filtering and deduplication\nand further remove data that contains a significant\nuse of non-English languages or is not parsable,\nresulting in a dataset of 101GB code.\nModel We use GPT-2 (Radford et al., 2019)\nand CodeGen 350M monolingual (Nijkamp et al.,\n2022) for all experiments on natural language\n(text) and programming language (code), respec-\ntively. We set the batch size to 512 and continue to\ntrain GPT-2 on WikiText-103 and CodeGen on the\nGitHub data for 12 and 2 epochs, respectively. We\ntrain both models using a max sequence length of\n512 tokens and 1024 for WikiText-103 and Code\ndata, respectively. We set the learning rate to 2e-5,\nwarm-up steps as 500 with linear annealing after\npeak learning rate, weight decay of 0.1, tempera-\nture of 0.05 (when using contrastive losses), and\ngradient clipping of 1.0. We use AdamW optimizer\n(Loshchilov and Hutter, 2019) with β1 = 0.9,\nβ2 = 0.999, and ϵ = 10−8 following (Nijkamp\net al., 2022). Our training pipeline is based on Py-\nTorch Lightning8, and we use DeepSpeed (Rasley\net al., 2020) for training optimization.\nProcessing Code Training Data Our pre-\nprocessing strategy for code datasets used for\ntraining is designed to ensure that we optimize\nfor data utilization while retaining the syntactic\nstructure of programming language sequences.\nWe also eliminate duplicate sequences since this\nbenefits training large language models (Lee et al.,\n2022). Specifically, we break long sequences\ninto chunked sequences of smaller lengths\nto retain most parts of the original program.\nFurther, we maintain syntactic structure in\nthe chunks by ensuring that each chunk ends\nwith a ‘ \\n’ character. Each chunk obtained\nthis way contains at most max_chars_per_seq\ncharacters where max_chars_per_seq =\nmax_tokens_per_seq * chars_per_tok. In our\nexperiments, we fix chars_per_tok = 3 .2 and\nmax_tokens_per_seq = 1024. We also perform\ndeduplication using character-based exact matches\nbetween chunked sequences over the entire dataset.\nThis step helps eliminate exact duplicates that\nmight be present after the chunking stage.\nC More on Evaluation\nC.1 Representation Quality Evaluated on STS\nFor each sequence pair in STS, a fine-grained simi-\nlarity score ranging from 0 to 5 is provided, with a\nhigh similarity score indicating semantically sim-\nilar pairs and low similarity scores suggesting se-\nmantically dissimilar or irrelevant pairs. For better\nillustration, we scale the human-annotated similar-\n8https://www.pytorchlightning.ai/\n6449\nContrastive Loss Expression\nLTok\nN∑\nj=1\n|xj |∑\ni=1\n−\n(\nlog exp(hj\ni ⋄hj+\ni /τ)\nexp(hj\ni ⋄hj+\ni /τ) +∑\nt∈Ij \\i\n[\nexp(hj\ni ⋄hj\nt/τ) + exp(hj\ni ⋄hj+\nt /τ)\n]\n+ log exp(hj+\ni ⋄hj\ni /τ)\nexp(hj+\ni ⋄hj\ni /τ) +∑\nt∈Ij \\i\n[\nexp(hj+\ni ⋄hj\nt/τ) + exp(hj+\ni ⋄hj+\nt /τ)\n]\n)\nLSeq\nN∑\nj=1\n−\n(\nlog exp(hj ⋄hj+\n/τ)\nexp(hj ⋄hj+\n/τ) +∑\nk∈IB\\j,j+ exp(hj ⋄hk/τ)\n+ log exp(hj+\n⋄hj/τ)\nexp(hj+\n⋄hj/τ) +∑\nk∈IB\\j,j+ exp(hj+\n⋄hk/τ)\n)\nTable 5: Formulation of our token-level and sequence-level contrastive losses denoted asLTok and LSeq respectively.\n0.0 0.2 0.4 0.6 0.8 1.0\nGround Truth Similarity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Inferred Similarity Score\n CLM\nContraCLM\n0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0\nGround Truth Rank\n1000\n1250\n1500\n1750\n2000\n2250\n2500\n2750Inferred Rank\n(a) STS14: (Left) Predicted cosine similarity vs. human annotated ground truth. Right Similarity ranking according\nto the model predicted similarity scores vs Human similarity based ranking.\n0.0 0.2 0.4 0.6 0.8 1.0\nGround Truth Similarity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Inferred Similarity Score\n CLM\nContraCLM\n0.0 1.0 2.0 3.0 4.0 5.0 6.0\nGround Truth Rank\n500\n750\n1000\n1250\n1500\n1750\n2000\n2250Inferred Rank\n(b) STS15: (Left) Predicted cosine similarity vs. human annotated ground truth. Right Similarity ranking according\nto the model predicted similarity scores vs Human similarity-based ranking.\nFigure 4: CLM versus Contrastive Learning in Similarity Prediction and Ranking. We report the results on two STS\nbenchmarks: (1) STS14 where CLM performs the worst when compared to its own performance on the other STS\ntasks; and STS15 where CLM attains the best performance when compared with its own performance on other STS\ntasks. For the purposes of illustration, we scale the human-annotated similarity scores from [0,5] to [0,1]. A good\nCLM is expected to predict discriminative similarity scores such that the resulting ranking results are as close to the\nranks provided by humans as possible.\nity scores to [0,1] to align with the model-predicted\ncosine similarity scores. This does not affect the\nevaluation as the spearman correlation reported in\nSection 4.2 is a rank-based correlation metric.\nCLM yields poorly discriminative representa-\ntions We report the model predicted similarity\nscores of sequence pairs in the left column in Fig-\nure 4. A good model is expected to yield represen-\ntations that attain higher similarity scores between\nsimilar sequence pairs and lower similarity values\nfor dissimilar sequences. Thereby, a large gap be-\ntween the predicted similarity scores of similar and\ndissimilar pairs is desired. However, as seen in\nFigure 4 (left), the similarity scores attained by the\nmodel trained with the standard CLM only objec-\ntive are almost at one for both similar and dissimilar\nsequence pairs. This suggests that the representa-\n6450\ntions yielded by CLM can9 be squeezed into a tiny\ncone in the representation space rather than being\nscattered apart to leverage the vector space’s ca-\npacity better. Despite the resulting similarity ranks\nnot being entirely flattened, as shown in the right\ncolumn in Figure 4b, CLM struggles in ranking\nsimilar sequences lower and dissimilar sequences\nhigher as a consequence of the poor discriminative\nrepresentations.\nIn contrast, Figure 4 (left) further validates that\ncontrastive learning effectively yields more discrim-\ninative representations with a comparatively larger\nsimilarity gap between similar pairs and dissimi-\nlar pairs. Thereby, the similarity ranking results\nof the sequence pairs are more aligned with those\nobtained according to similarity scores provided by\nhumans, as shown in Figure 4 (right).\nC.2 Programming Language Evaluation\nC.2.1 Evaluation Metrics\nMean Average Precision (MAP) For a set of\nqueries, it indicates the mean of the average preci-\nsion scores for each query.\nMAP =\n∑Q\nq=1 AveP(q)\nQ\nwhere Qis the number of queries.\nPass@k Given a problem (code prompt as shown\nin Figure 6), pass@k indicates the functional cor-\nrectness of model-generated code samples. A prob-\nlem is considered solved if any sample passes the\nunit tests. Following (Chen et al., 2021), we gen-\nerate n ≥k samples per problem (in this paper,\nwe use n= 10and k∈{1,5}), count the number\nof correct samples c≤nthat pass unit tests, and\ncalculate the unbiased estimator of pass@kas:\npass@k:= E\nProblems\n[\n1 −\n(n−c\nk\n)\n(n\nk\n)\n]\n.\nRanked Pass@k Unlike Pass@k, where we ran-\ndomly chose kout of nsamples, in ranked pass@k,\nwe chose the top- k samples based on model-\nprovided scores and then computed pass@k.\nC.2.2 Examples and Statistics\nIn Figure 5, we present an example of a query\ncode fragment in Python and relevant code frag-\nments in Python and Java, respectively. While\n9As investigated in Figure 1, the decoder-only models\npretrained with the CLM-only can suffer from the anisotropy\nissue, which depends on the model size and domain.\nin-language code-to-code search refers to retriev-\ning relevant code fragments in the same language,\ncross-language code-to-code search refers to re-\ntrieving code fragments in a different language.\nWe present the statistics of the code search dataset\nin Table 6. To demonstrate the code completion\ntask, we illustrate an example in Figure 6.\nC.2.3 Detailed Code Search Results\nWe provide a comparison between encoder-only\n(Feng et al., 2020; Guo et al., 2021), encoder-\ndecoder (Ahmad et al., 2021; Wang et al., 2021),\nand decoder-only models (main focus of this work)\non the zero-shot code-to-code search task in Table\n7b. We see that CONTRA CLM-T OK and CONTR -\nACLM outperform the encoder-only model Code-\nBERT and both the encoder-decoder models. It is\nimportant to note that the comparison across these\nmodels is not apple-to-apple as these models dif-\nfer in size, the scale of pretraining, and language\nsettings. This comparison’s purpose is to show\nthe promise of decoder-only models being used in\ndiscriminative tasks like code search.\nWe further break down the code search perfor-\nmances based on edit similarities and length differ-\nences between query code and their relevant code\nfragments. We present the results in Figure 7 and\n8. We observe a similar performance trend in all\nthree languages, although cross-lingual search per-\nformance still needs to improve. Nonetheless, the\nobjective of this performance analysis is to show\nthat sequence overlap or length are not the rea-\nsons for improvements in CONTRA CLM-T OK. In-\nstead, a finer-grained understanding of code tokens\ndue to the token-level contrastive learning makes\nCONTRA CLM-T OK more effective.\nD More Analysis and Discussions\nD.1 Bridge the Gap on Discriminative Tasks\nCompared to the causal (left-to-right) attention\nmechanism of the decoder-only models, the bidi-\nrectional attention mechanism in both encoder-only\nand encoder-decoder models allows for better lever-\nage of the context of the sequence and hence leads\nto better representations.\nTaking the encoder-only models in Table 7a for\nillustration, on average, BERT-Base (Devlin et al.,\n2019) and Roberta-Base (Liu et al., 2019) outper-\nform GPT-2 with 67.25% (absolute 21.17%) and\n84.62% (absolute 26.64%) relative improvement on\nSTS, respectively. Although the performance gap\n6451\nRuby Python Java\nTotal problems 1,708 2072 3142\nTotal #solution 11,744 15,594 23,530\nAvg. #solution / problem 6.9 7.5 7.5\nAvg. length / solution 160.4 214.4 894.9\nStdev. of length / solution (problem-wise) 112.80 113.79 813.0\nSolutions with length >512 409 1,200 10,023\nSolutions with length >1024 78 278 4,766\nAvg. edit similarity 0.48 (+0.13) 0.52(+0.13) 0.49(+0.13)\nTable 6: Statistics of code-to-code search task dataset created from CodeNet (Puri et al., 2021). We truncate the\ncode if its length exceeds the maximum sequence length, which is set to 512.\nbetween CodeGen and the BERT models trained\non programming languages, i.e., CodeBERT (Feng\net al., 2020) and GraphCodeBERT (Guo et al.,\n2021), decreases or even diminishes when eval-\nuated on the code search tasks, the performance\ngap is still significant as both the model size and\npretraining data in CodeGen are much larger than\nthose used by the encoder-only models in Table\n7b. Similar trends were observed in the perfor-\nmance gap between the decoder-only and encoder-\ndecoder models on both natural language (Lewis\net al., 2020; Raffel et al., 2020) and programming\nlanguage (Ahmad et al., 2021; Wang et al., 2021).\nThe large performance gap severely limits the\ndecoder-only models used in many discriminative\ntasks. To this end, contrastive learning shows the\npromise to largely bridge the gap. As seen in Ta-\nble 7a, on STS, CONTRA CLM reduces the relative\nperformance gap from 67.24% (absolute 21.12%)\nto 16.17% (absolute 7.33%) regarding BERT-Base,\nand from 84.62% (absolute 26.64%) to 28.24%\n(absolute 12.8%). Similarly, Table 7b shows that\nCONTRA CLM outperforms encoder-decoder mod-\nels and performs comparably to the encoder-only\nmodel, GraphCodeBERT.\nD.2 Dropout for Contrastive Learning\nGao et al. (2021) showed that the dropout-based\naugmentation is an effective strategy for unsuper-\nvised contrastive learning, and the follow-up works\n(Chuang et al., 2022; Wu et al., 2022) endorse the\neffectiveness. This motivates us to study dropout-\nbased augmentation in our proposed contrastive\nlearning framework. We present the results on dis-\ncriminative and generation tasks in Tables 8 and 9,\nrespectively. From the results, it is evident that the\nadoption of dropout-based augmentation improves\nthe discrimination task performances, which cor-\nroborates the findings of (Gao et al., 2021). In\ncontrast, dropout-based augmentation hurts the gen-\neration task performances. On the other hand, for\ncode completion, we had anticipated that dropout-\nbased augmentation would hurt performance since\nwe used the CodeGen model (Nijkamp et al., 2022)\nwhich does not use dropout activations during its\ninitial pretraining stage. However, we observe a\ndrop in perplexity due to disabling dropout for both\nCLM and CONTRA CLM in Table 9, which does\nnot go with our anticipation, especially consider-\ning that, unlike CodeGen, GPT-2 is pretrained with\ndropout enabled. We leave diving deeper into the\nreasoning behind this finding as future work.\nD.3 C ONTRA CLM outperforms SimCTG\nTo better understand the performance gap between\nCONTRA CLM and SimCTG (Su et al., 2022), we\nrun the following ablations on GPT-2 and report\nthe evaluations on STS. In Table 10, we report the\nresults of (1) running CONTRA CLM w/o dropout-\nbased data augmentation and compare it with the\noriginal SimCTG model and (2) augmenting Sim-\nCTG with both the sequence-level contrastive loss\nand dropout-based augmentation and compare it\nwith our proposed CONTRA CLM model. As we\ncan see, CONTRA CLM consistently outperforms\nSimCTG in both settings. Figure 10 together with\nour results reported in Section 4.3, where we dis-\nabled the dropout-based augmentation for CONTR -\nACLM and its variations but still observed consis-\ntently better performance than SimCTG on both\ndiscrimination and generation tasks, conclude that\nCONTRA CLM is better than SimCTG across do-\nmains and settings.\n6452\nModel STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg.\nEncoder-only Models\nBERT-Base 30.92 59.96 47.72 60.35 63.72 58.25 47.36 52.65\nRoBERTa-Base 53.95 47.42 55.87 64.73 63.55 62.94 58.40 58.12\nEncoder-Decoder Models\nBART-Base 34.46 52.49 44.50 62.51 61.99 57.72 52.30 52.28\nT5-Base 37.78 56.81 49.37 65.50 64.65 60.11 57.52 55.96\nDecoder-only Models\nGPT2 25.84 28.90 26.20 34.74 35.70 42.72 26.27 31.48\nCLM 27.14 20.34 18.73 37.56 27.40 35.70 27.97 27.83\nSimCTG 30.32 37.10 31.99 39.68 42.73 46.26 25.27 36.19\nCONTRA CLM-T OK 37.28 37.63 31.33 54.78 50.16 48.10 34.95 42.03\nCONTRA CLM-S EQ 29.66 39.89 34.50 43.20 41.99 44.52 25.51 37.04\nCONTRA CLM 37.54 45.23 36.41 56.74 50.30 51.52 39.49 45.32\n(a) Spearman rank correlation between the cosine similarity of sentence pairs and the human-annotated similarity scores.\nModel Ruby Python Java Avg.\nRuby Python Java Ruby Python Java Ruby Python Java\nEncoder-only Models\nCodeBERT 13.55 3.18 0.71 3.12 14.39 0.96 0.55 0.42 7.62 4.94\nGraphCodeBERT 17.01 9.29 6.38 5.01 19.34 6.92 1.77 3.50 13.31 9.17\nEncoder-Decoder Models\nPLBART 18.60 10.76 1.90 8.27 19.55 1.98 1.47 1.27 10.41 8.25\nCodeT5-base 18.22 10.02 1.81 8.74 17.83 1.58 1.13 0.81 10.18 7.81\nDecoder-only Models\nCodeGen 16.18 5.90 0.52 2.66 18.11 0.36 1.61 1.65 10.16 6.35\nCLM 16.36 6.67 0.80 3.07 15.72 0.46 1.41 2.11 10.25 6.32\nSimCTG 17.66 7.19 1.94 7.63 18.31 1.78 1.63 2.32 10.83 7.70\nCONTRACLM-TOK 18.02 7.84 2.51 8.76 20.46 2.48 1.91 2.58 11.43 8.44\nCONTRACLM-SEQ 16.76 5.45 1.06 7.40 16.74 1.41 1.55 2.25 10.23 6.98\nCONTRACLM 17.90 7.78 2.56 9.05 19.74 2.64 1.90 2.50 11.32 8.38\n(b) MAP score (%) of the zero-shot code-to-code search task. The language names mentioned in the top two rows indicate the\nlanguages queries and candidates are written in.\nTable 7: CONTRA CLM bridges the gap between CLM and Encoder-Only / Encoder-Decoder models.\n6453\nModel Ruby Python Java Avg.\nRuby Python Java Ruby Python Java Ruby Python Java\nCLM+Dropout 18.04 6.47 1.21 5.52 18.70 1.18 1.62 2.35 11.26 7.37\nCLM−Dropout 16.36 6.67 0.8 3.07 15.72 0.46 1.41 2.11 10.25 6.32\nCONTRACLM +Dropout 20.09 8.84 3.66 9.25 22.39 3.13 1.93 3.06 12.02 9.37\n17.90 7.78 2.56 9.05 19.74 2.64 1.90 2.50 11.32 8.38\n(a) MAP score (%) of zero-shot code-to-code search.\nModel STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg.\nCLM +Dropout 27.14 20.34 18.73 37.56 27.40 35.70 27.97 27.83\nCLM −Dropout 25.60 15.23 13.95 31.64 28.13 34.96 26.15 25.09\nCONTRA CLM +Dropout 37.54 45.23 36.41 56.74 50.30 51.52 39.49 45.32\nCONTRA CLM −Dropout 38.22 40.15 33.57 53.16 45.35 47.47 36.10 42.00\n(b) Spearman rank correlations between the cosine similarity of sentence representation pairs and the ground truth similarity\nscores for STS benchmarks.\nTable 8: Discriminative task performances with (+Dropout) and without (−Dropout) Dropout augmentation applied to\nCLM and CONTRA CLM. We apply Dropout (0.1) to all the layers of the models.\nModel Pass@k Ranked Pass@k\nk=1 k=5 k=1 k=5\nCLM +Dropout 12.65 15.54 13.42 (+0.77) 16.46 (+0.92)\nCLM −Dropout 13.42 18.08 15.38 (+1.96) 18.29 (+0.21)\nCONTRA CLM +Dropout 13.19 15.92 13.41 (+0.22) 16.46 (+3.05)\nCONTRA CLM −Dropout 14.63 18.83 17.07 (+2.44) 18.90 (+0.07)\n(a) Evaluation results on the HumanEval benchmark. The numbers in the subscript indicate the difference between ranked\npass@k and pass@k accuracy.\nCLM −Dropout CLM +Dropout CONTRA CLM −Dropout CONTRA CLM +Dropout\nPerplexity 21.86 22.48 22.07 23.01\n(b) Perplexity of continually trained GPT-2 on the test set of WikiText-103.\nTable 9: Generation task performances with (+Dropout) and without (−Dropout) Dropout augmentation applied to CLM\nand CONTRA CLM. We apply Dropout (0.1) to all the layers of the models.\nModel STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg.\nSimCTG 30.32 37.10 31.99 39.68 42.73 46.26 25.27 36.19\nCONTRA CLM −Dropout 38.22 40.15 33.57 53.16 45.35 47.47 36.10 42.00\nSimCTG+LSeq+Dropout 38.70 43.60 36.29 50.01 45.19 48.25 33.36 42.20\nCONTRA CLM +Dropout 37.54 45.23 36.41 56.74 50.30 51.52 39.49 45.32\nTable 10: CONTRA CLM outperform SimCTG (Su et al., 2022) even without dropout-based data augmentation (first\ntwo rows); or augmenting SimCTG with dropout and sequence-level contrastive loss defined in Table 5.\n6454\nQuery Program in Python\n1 import math\n2 h,w= map (int , input (). split ())\n3 if h %3==0 or w %3==0:\n4 print (0)\n5 else :\n6 x,y= max (h,w),min (h,w)\n7 ans =y\n8 for hi in range (1 ,h):\n9 M= max (hi*w ,(h-hi) *(( w +1) //2) ,(h-hi)*(w //2) )\n10 m= min (hi*w ,(h-hi) *(( w +1) //2) ,(h-hi)*(w //2) )\n11 ans = min (ans ,M-m)\n12 for wi in range (1 ,w):\n13 M= max (wi*h ,(w-wi) *(( h +1) //2) ,(w-wi)*(h //2) )\n14 m= min (wi*h ,(w-wi) *(( h +1) //2) ,(w-wi)*(h //2) )\n15 ans = min (ans ,M-m)\n16 print ( ans )\nRelevant Program in Python\n1 def solve (H,W):\n2 p1 = [H //3*W, (H-H //3) *(W //2) , (H-H //3) *(W-W //2) ]\n3 p2 = [ ceil (H /3) *W, (H- ceil (H /3) )*(W //2) , (H- ceil (H /3) )*(W-W //2) ]\n4 S1 = max (p1)-min (p1)\n5 S2 = max (p2)-min (p2)\n6 S3 = 0 if H %3==0 else W\n7 return min (S1 ,S2 ,S3)\n8\n9 from math import ceil\n10 H, W = map (int , input (). split ())\n11 print ( min ( solve (H,W), solve (W,H)))\nRelevant Program in Java\n1 import java . util .*;\n2 public class Main {\n3 public static void main ( String [] args ){\n4 Scanner sc = new Scanner ( System .in);\n5 long w = sc. nextInt ();\n6 long h = sc. nextInt ();\n7 if(w %3==0 || h %3==0)\n8 System . out . println (0) ;\n9 else\n10 System . out . println ( Math . min ( solve (w, h), solve (h, w)));\n11 }\n12 static long solve ( long w, long h){\n13 long min = Long . MAX_VALUE ;\n14 for ( int i =1;i<h;i ++) {\n15 long a = w*i;\n16 long b, c = 0, 0;\n17 if(w %2==0) {\n18 b = w /2*(h-i);\n19 c = b;\n20 min = Math . min (min , Math . max (a, Math . max (b, c))-Math . min (a, Math . min (b, c)));\n21 }\n22 else if ((h-i) %2==0) {\n23 b = w *((h-i) /2) ;\n24 c = b;\n25 min = Math . min (min , Math . max (a, Math . max (b, c))-Math . min (a, Math . min (b, c)));\n26 }\n27 else {\n28 b = w *((h-i) /2) ;\n29 c = w *((h-i) /2+1) ;\n30 min = Math . min (min , Math . max (a, Math . max (b, c))-Math . min (a, Math . min (b, c)));\n31 b = w /2*(h-i);\n32 c = (w /2+1) *(h-i);\n33 min = Math . min (min , Math . max (a, Math . max (b, c))-Math . min (a, Math . min (b, c)));\n34 }\n35 }\n36 return min ;\n37 }\n38 }\nFigure 5: An example of a query and relevant documents in code-to-code search task, where both query and\ncandidates are complete programs that solve a programming problem. In this search task, each program is used as a\nquery and retrieve all programs (from a collection of 11,744/15,594/23,530 programs in Ruby, Python, and Java,\nrespectively) that solve the same problem.\n6455\nPrompt (function signature and docstring)\n1 from typing import List\n2\n3\n4 def has_close_elements ( numbers : List [ float ], threshold : float ) -> bool :\n5 \"\"\" Check if in given list of numbers , are any two numbers closer to each other than\n6 given threshold .\n7 >>> has_close_elements ([1.0 , 2.0 , 3.0] , 0.5)\n8 False\n9 >>> has_close_elements ([1.0 , 2.8 , 3.0 , 4.0 , 5.0 , 2.0] , 0.3)\n10 True\n11 \"\"\"\nUnit tests\n1 def check ( candidate ):\n2 assert candidate ([1.0 , 2.0 , 3.9 , 4.0 , 5.0 , 2.2] , 0.3) == True\n3 assert candidate ([1.0 , 2.0 , 3.9 , 4.0 , 5.0 , 2.2] , 0.05) == False\n4 assert candidate ([1.0 , 2.0 , 5.9 , 4.0 , 5.0] , 0.95) == True\n5 assert candidate ([1.0 , 2.0 , 5.9 , 4.0 , 5.0] , 0.8) == False\n6 assert candidate ([1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 2.0] , 0.1) == True\n7 assert candidate ([1.1 , 2.2 , 3.1 , 4.1 , 5.1] , 1.0) == True\n8 assert candidate ([1.1 , 2.2 , 3.1 , 4.1 , 5.1] , 0.5) == False\n9\n10 check ( has_close_elements )\nCompletion 1 (passed; mean_logp: -0.1146)\n1 for i in range ( len ( numbers ) - 1):\n2 for j in range (i + 1, len ( numbers )):\n3 if abs ( numbers [i] - numbers [j]) < threshold :\n4 return True\n5 return False\nCompletion 2 (failed; mean_logp: -0.1211)\n1 for i in range ( len ( numbers ) - 1):\n2 for j in range (i + 1, len ( numbers )):\n3 if abs ( numbers [i] - numbers [j]) > threshold :\n4 return True\n5 return False\nCompletion 3 (failed; mean_logp: -0.1598)\n1 for i in range ( len ( numbers ) - 1):\n2 if abs ( numbers [i] - numbers [i + 1]) > threshold :\n3 return True\n4 return False\nCompletion 4 (failed; mean_logp: -0.1607)\n1 for i in range ( len ( numbers ) - 1):\n2 if abs ( numbers [i] - numbers [i + 1]) > threshold :\n3 return False\n4 return True\nCompletion 5 (failed; mean_logp: -0.3904)\n1 return abs ( sum ( numbers ) - sum ( numbers ) / len ( numbers )) < threshold\nFigure 6: An example of prompt from HumanEval and five unique code completions generated by theCONTRA CLM\nmodel. We rank them based on the mean_logp scores. Considering these 5 completions and one of the passes unit\ntests, pass@1 is 0.2 while ranked pass@1 is 1.0.\n6456\n[0, 0.5)\n0.090\n0.095\n0.100\n[0.5, 0.75)\n0.24\n0.25\n0.26\n0.27\nRuby\n[0.75, 1.0)\n0.68\n0.69\n0.70\n0.71\n[0, 0.5)\n0.08\n0.09\n0.10\n[0.5, 0.75)\n0.20\n0.22\n0.24\n0.26\n0.28\nPython\n[0.75, 1.0)\n0.45\n0.50\n0.55\n[0, 0.5)\n0.046\n0.048\n0.050\n0.052\n[0.5, 0.75)\n0.16\n0.17\n0.18\nJava\n[0.75, 1.0)\n0.51\n0.52\n0.53\n0.54\nEdit Similarity\nMAP score\nCLM SimCTG ContraCLM-Tok ContraCLM-Seq ContraCLM\nFigure 7: Code-to-code search performance (MAP score) breakdown for different models based on the edit\nsimilarities ([0, 1]) between the query code fragments and the relevant code fragments. Higher and lower edit\nsimilarity indicates the search task is trivial or difficult, respectively.\n[0, 64)\n0.18\n0.19\n0.20\n[64, 256)\n0.115\n0.120\nRuby\n[256, 512)\n0.022\n0.024\n0.026\n0.028\n[0, 64)\n0.18\n0.20\n0.22\n0.24\n[64, 256)\n0.10\n0.11\n0.12\nPython\n[256, 512)\n0.025\n0.030\n[0, 64)\n0.130\n0.135\n0.140\n0.145\n[64, 256)\n0.070\n0.075\n0.080\nJava\n[256, 512)\n0.0052\n0.0054\n0.0056\n0.0058\n0.0060\nLength Diﬀerence\nMAP score\nCLM SimCTG ContraCLM-Tok ContraCLM-Seq ContraCLM\nFigure 8: Code-to-code search performance (MAP score) breakdown based on (absolute) length differences between\nthe query code fragments and their relevant code fragments.\n6457\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section after the conclusion section.\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics statement after the limitation section.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4: Experiments\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4: Experiments\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nEthics statement\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nEthics statement\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nJustiﬁcation provided in the ethics statement\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4: Experiments\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix\nC □\u0013 Did you run computational experiments?\nSection 4: Experiments\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4: Experiments, Ethics statement\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6458\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nEthics statement\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4: Experiments\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n6459",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.5844025015830994
    },
    {
      "name": "Computer science",
      "score": 0.5458410978317261
    },
    {
      "name": "Linguistics",
      "score": 0.49281957745552063
    },
    {
      "name": "Natural language processing",
      "score": 0.44705548882484436
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42203885316848755
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4142322540283203
    },
    {
      "name": "Cognitive science",
      "score": 0.3987429440021515
    },
    {
      "name": "Programming language",
      "score": 0.36938151717185974
    },
    {
      "name": "Psychology",
      "score": 0.27546364068984985
    },
    {
      "name": "Philosophy",
      "score": 0.26138579845428467
    },
    {
      "name": "History",
      "score": 0.12471860647201538
    },
    {
      "name": "China",
      "score": 0.10519292950630188
    },
    {
      "name": "Archaeology",
      "score": 0.06794306635856628
    },
    {
      "name": "Physics",
      "score": 0.06769677996635437
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}