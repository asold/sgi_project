{
  "title": "Large language models and the perils of their hallucinations",
  "url": "https://openalex.org/W4353015365",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2903806439",
      "name": "Razvan Azamfirei",
      "affiliations": [
        "Johns Hopkins Medicine",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2238289090",
      "name": "Sapna R. Kudchadkar",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2149972988",
      "name": "James Fackler",
      "affiliations": [
        "Johns Hopkins Medicine",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2903806439",
      "name": "Razvan Azamfirei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2238289090",
      "name": "Sapna R. Kudchadkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149972988",
      "name": "James Fackler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4322008312",
    "https://openalex.org/W4213294877",
    "https://openalex.org/W4366548330"
  ],
  "abstract": null,
  "full_text": "Azamfirei et al. Critical Care          (2023) 27:120  \nhttps://doi.org/10.1186/s13054-023-04393-x\nCORRESPONDENCE\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\nCritical Care\nLarge language models and the perils \nof their hallucinations\nRazvan Azamfirei1*, Sapna R. Kudchadkar1,2 and James Fackler1,2 \nTo the Editor,\nWe read with great interest the paper by Salvagno et al. \n[1] As they masterfully stated, “ChatGPT work should \nnot be used as a replacement for human judgment, and \nthe output should always be reviewed by experts before \nbeing used in any critical decision-making or applica -\ntion. ” As is often the case in critical care, new technolo -\ngies and apparent breakthroughs are often touted as \ngame-changers. However, the truth usually emerges \nthe next day when the confetti has settled, and we have \nto clean up the sticky mess left by gallons of printed ink \nmixed with our hopeful wishes.\nSalvagno et al. present a ChatGPT-generated summary \nof three studies. As they noted, the summary was believ -\nable, albeit generic and sparse in the details. The glaring \nproblem is that it’s completely fabricated. ChatGPT can -\nnot access the internet, and its training dataset stops in \nSeptember 2021; it has no reference to any studies pub -\nlished in 2023 [2]. In fact, one of the trials included in the \nsummary, Belohlavek et al. [3], showed no improvement \nin functional neurological outcomes, contradicting Chat -\nGPT’s summary.\nWe must understand one particular aspect of \nlarge language models, which is gracefully termed as \n“hallucinations” , though “fabricating information” may be \nmore accurate [4]. In the case of the ChatGPT summary, \nwe are only reading a generic summary of an average \nstudy with a given title.\nThe following is an excerpt from a “self-reflective” \nChatGPT conversation.\n“Q: Briefly, what is a GPT?\nA: It is a type of deep learning algorithm that is designed \nto generate human-like text. […] Once trained, GPT \nmodels can be used to generate new text that is coherent \nand follows the patterns and styles of the original training \ndata.\nQ: Would it be fair, then, to say that GPT models, such \nas yourself, use a probabilistic approach to generate text \nthat looks like previous text that was written?\nA: Yes, that is a fair statement. […] When given a prompt, \nthe model generates text by predicting the most likely \nword or phrase to follow based on the patterns it has \nlearned from the training data.\nQ: When I ask you to generate new text, are you coming \nup with the ideas yourself or just repackaging previously \nwritten works?\nA: […] I do not come up with the ideas myself. Instead, I \ngenerate text based on patterns and relationships within \nmy training data. This means that I am essentially repack-\naging previously written works, based on what I believe \nto be the most probable continuation of the given input \ntext.\nThis comment refers to the article available online at https:// doi. org/ 10. 1186/ \ns13054- 023- 04380-2.\n*Correspondence:\nRazvan Azamfirei\nrazvan@jhmi.edu\n1 Department of Anesthesiology and Critical Care Medicine, Johns \nHopkins University School of Medicine, Baltimore, MD, USA\n2 Department of Pediatrics, Johns Hopkins University School of Medicine, \nBaltimore, MD, USA\nPage 2 of 2Azamfirei et al. Critical Care          (2023) 27:120 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nQ: Is there any filter of correctness applied to the output?\nA: As an AI language model, I do not have a built-in filter \nto determine the correctness of my output. ”\nWhen the model had no answer, we received whatever \nlooked like the most probable summary of the study—a \nhallucination.\nOf course, we will improve our ability to integrate real-\ntime information and reduce the rate of “hallucinations” , \njust as the nascent field of prompt engineering evolves. \nRunning the exact prompt used by the authors now \nresults in a response recognizing the knowledge gap. The \nbroader point is less about whether we should use Chat -\nGPT, and more about whether we are using it with a full \nunderstanding of its strengths and limitations [5].\nWe commend the authors for their exploration of Chat-\nGPT and some associated important ethical issues. Fore -\nmost, however, it is important to reiterate that because \nChat GPT now has no access to the article it was asked \nto interpret, it was given an impossible task. Our goal \nis simply to emphasize that, whether it’s a new language \nmodel, an innovative monitoring technology, or a novel \nbiomarker, we must be aware of our tools’ limitations. We \nhope that as these technologies evolve, they respond, as \ndid Robot Model B-9 in the 1960’s television show Lost in \nSpace, with “that does not compute” before spewing what \nit must know is a hallucination.\nWe offer this analogy as a conclusion. Imagine a self-\ndriving system trained to safely navigate a car on public \nroadways; would we place the same system in a rocket, \nasking it to navigate us to low earth orbit? Likely not. The \ntasks seem similar, navigating, but are completely differ -\nent. We can build systems to take us to Earth’s orbit, just \nhow we’ll build systems to accurately summarize scien -\ntific articles. Our only hope is that we know whether our \nrocket is taking us to Kansas or the International Space \nStation before strapping ourselves to it. To again quote \nRobot Model B-9, “Danger” .\nAcknowledgements\nChatGPT was used in the writing of this manuscript. All content generated by \nChatGPT is marked as such.\nAuthor contributions\nThe manuscript was drafted by RA with contributions from JF and SRK. All \nauthors contributed to the review and editing of the manuscript. All authors \nread and approved the final manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nJF is a Board Member of Machine Learning for Healthcare and a founder of \nRubicon Health LLC. The remaining authors do not have any potential com-\npeting interests.\nReceived: 27 February 2023   Accepted: 4 March 2023\nReferences\n 1. Salvagno M, Taccone FS, Gerli AG. Can artificial intelligence help for \nscientific writing? Crit Care. 2023;27(1):75. https:// doi. org/ 10. 1186/ \ns13054- 023- 04380-2.\n 2. OpenAI. ChatGPT General FAQ. 2023. https:// web. archi ve. org/ web/ 20230 \n22617 1118/ https:// help. openai. com/ en/ artic les/ 67834 57- chatg pt- gener \nal- faq. Accessed on 26 Feb 2023.\n 3. Belohlavek J, Smalcova J, Rob D, Franek O, Smid O, Pokorna M, et al. Effect \nof intra-arrest transport, extracorporeal cardiopulmonary resuscitation, \nand immediate invasive assessment and treatment on functional neuro-\nlogic outcome in refractory out-of-hospital cardiac arrest: a randomized \nclinical trial. JAMA. 2022;327(8):737–47. https:// doi. org/ 10. 1001/ jama. \n2022. 1025.\n 4. OpenAI. ChatGPT: Optimizing Language Models for Dialogue. 2022. \nhttps:// web. archi ve. org/ web/ 20230 22521 1409/ https:// openai. com/ blog/ \nchatg pt/. Accessed on 26 Feb 2023.\n 5. Zamfirescu-Pereira J, Wong R, Hartmann B, Yang Q, editors. Why Johnny \ncan’t prompt: how non-AI experts try (and fail) to design LLM prompts. In: \nProceedings of the 2023 CHI conference on human factors in comput-\ning systems (CHI ’23); 2023; Hamburg, Germany. https:// doi. org/ 10. 1145/ \n35445 48. 35813 88. https:// www. resea rchga te. net/ publi cation/ 36857 \n7310_ Why_ Johnny_ Can’t_ Prompt_ How_ Non- AI_ Exper ts_ Try_ and_ Fail_ \nto_ Design_ LLM_ Promp ts. Accessed on 02/26/2023.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8421738147735596
    },
    {
      "name": "Visual Hallucination",
      "score": 0.4822860062122345
    },
    {
      "name": "Psychiatry",
      "score": 0.4149421453475952
    },
    {
      "name": "Intensive care medicine",
      "score": 0.3268473744392395
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799853436",
      "name": "Johns Hopkins Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 257
}