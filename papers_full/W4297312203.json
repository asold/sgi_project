{
    "title": "Plant recognition by AI: Deep neural nets, transformers, and kNN in deep embeddings",
    "url": "https://openalex.org/W4297312203",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4209648529",
            "name": "Lukáš Picek",
            "affiliations": [
                "University of West Bohemia"
            ]
        },
        {
            "id": "https://openalex.org/A4224167992",
            "name": "Milan Šulc",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A2096489723",
            "name": "Yash Patel",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A2130163891",
            "name": "Jiří Matas",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A4209648529",
            "name": "Lukáš Picek",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224167992",
            "name": "Milan Šulc",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096489723",
            "name": "Yash Patel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130163891",
            "name": "Jiří Matas",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2126357415",
        "https://openalex.org/W2809392135",
        "https://openalex.org/W2889985731",
        "https://openalex.org/W2256962402",
        "https://openalex.org/W2798381792",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W3203940322",
        "https://openalex.org/W2116562978",
        "https://openalex.org/W2579348194",
        "https://openalex.org/W2752681852",
        "https://openalex.org/W2888892960",
        "https://openalex.org/W2966668365",
        "https://openalex.org/W3097927735",
        "https://openalex.org/W3197583069",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2886022720",
        "https://openalex.org/W2970163832",
        "https://openalex.org/W3084911182",
        "https://openalex.org/W3201163362",
        "https://openalex.org/W3169004985",
        "https://openalex.org/W2287418003",
        "https://openalex.org/W6744211344",
        "https://openalex.org/W2804733524",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W4214950233",
        "https://openalex.org/W1705245392",
        "https://openalex.org/W6800272910",
        "https://openalex.org/W6766141225",
        "https://openalex.org/W3138781819",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W2062532511",
        "https://openalex.org/W2131768008",
        "https://openalex.org/W2141253686",
        "https://openalex.org/W4212985039",
        "https://openalex.org/W2780868291",
        "https://openalex.org/W2998048148",
        "https://openalex.org/W2889188535",
        "https://openalex.org/W2964350391",
        "https://openalex.org/W3145444543",
        "https://openalex.org/W3109241881",
        "https://openalex.org/W2797977484",
        "https://openalex.org/W2885150722",
        "https://openalex.org/W2989853921",
        "https://openalex.org/W7047264204",
        "https://openalex.org/W1968896562",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W2963407932",
        "https://openalex.org/W2209016647",
        "https://openalex.org/W2966172790",
        "https://openalex.org/W1797268635",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3194928777",
        "https://openalex.org/W2750981090",
        "https://openalex.org/W3099319035",
        "https://openalex.org/W2908510526"
    ],
    "abstract": "The article reviews and benchmarks machine learning methods for automatic image-based plant species recognition and proposes a novel retrieval-based method for recognition by nearest neighbor classification in a deep embedding space. The image retrieval method relies on a model trained via the Recall@k surrogate loss. State-of-the-art approaches to image classification, based on Convolutional Neural Networks (CNN) and Vision Transformers (ViT), are benchmarked and compared with the proposed image retrieval-based method. The impact of performance-enhancing techniques, e.g., class prior adaptation, image augmentations, learning rate scheduling, and loss functions, is studied. The evaluation is carried out on the PlantCLEF 2017, the ExpertLifeCLEF 2018, and the iNaturalist 2018 Datasets—the largest publicly available datasets for plant recognition. The evaluation of CNN and ViT classifiers shows a gradual improvement in classification accuracy. The current state-of-the-art Vision Transformer model, ViT-Large/16, achieves 91.15% and 83.54% accuracy on the PlantCLEF 2017 and ExpertLifeCLEF 2018 test sets, respectively; the best CNN model (ResNeSt-269e) error rate dropped by 22.91% and 28.34%. Apart from that, additional tricks increased the performance for the ViT-Base/32 by 3.72% on ExpertLifeCLEF 2018 and by 4.67% on PlantCLEF 2017. The retrieval approach achieved superior performance in all measured scenarios with accuracy margins of 0.28%, 4.13%, and 10.25% on ExpertLifeCLEF 2018, PlantCLEF 2017, and iNat2018–Plantae, respectively.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/seven.tnum September /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nOPEN ACCESS\nEDITED BY\nPierre Bonnet,\nCIRAD, UMR AMAP, France\nREVIEWED BY\nHervé Goëau,\nUMR/five.tnum/one.tnum/two.tnum/zero.tnum Botanique et modélisation\nde l’architecture des plantes et des\nvégétations (AMAP), France\nChuan Lu,\nAberystwyth University,\nUnited Kingdom\n*CORRESPONDENCE\nLukáš Picek\npicekl@kky.zcu.cz;\nlukaspicek@gmail.com\nSPECIALTY SECTION\nThis article was submitted to\nTechnical Advances in Plant Science,\na section of the journal\nFrontiers in Plant Science\nRECEIVED /three.tnum/zero.tnum September /two.tnum/zero.tnum/two.tnum/one.tnum\nACCEPTED /one.tnum/five.tnum July /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /two.tnum/seven.tnum September /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nPicek L, Šulc M, Patel Y and Matas J\n(/two.tnum/zero.tnum/two.tnum/two.tnum) Plant recognition by AI: Deep\nneural nets, transformers, and kNN in\ndeep embeddings.\nFront. Plant Sci./one.tnum/three.tnum:/seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Picek, Šulc, Patel and Matas.\nThis is an open-access article\ndistributed under the terms of the\nCreative Commons Attribution License\n(CC BY)\n. The use, distribution or\nreproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s)\nare credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nPlant recognition by AI: Deep\nneural nets, transformers, and\nkNN in deep embeddings\nLukáš Picek /one.tnum*, Milan Šulc /two.tnum, Yash Patel /two.tnumand Ji ˇrí Matas/two.tnum\n/one.tnumDepartment of Cybernetics, Faculty of Applied Sciences, Uni versity of West Bohemia, Pilsen,\nCzechia, /two.tnumVisual Recognition Group, Department of Cybernetics, Faculty o f Electrical Engineering,\nCzech Technical University in Prague, Prague, Czechia\nThe article reviews and benchmarks machine learning methods for au tomatic\nimage-based plant species recognition and proposes a novel re trieval-\nbased method for recognition by nearest neighbor classiﬁcation in a deep\nembedding space. The image retrieval method relies on a model trained via the\nRecall@k surrogate loss. State-of-the-art approaches to image cl assiﬁcation,\nbased on Convolutional Neural Networks (CNN) and Vision Trans formers\n(ViT), are benchmarked and compared with the proposed image retr ieval-\nbased method. The impact of performance-enhancing techniques, e.g., class\nprior adaptation, image augmentations, learning rate sched uling, and loss\nfunctions, is studied. The evaluation is carried out on the Plan tCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum,\nthe ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum, and the iNaturalist /two.tnum/zero.tnum/one.tnum/eight.tnum Datasets—the largest\npublicly available datasets for plant recognition. The evalua tion of CNN and\nViT classiﬁers shows a gradual improvement in classiﬁcation accuracy. The\ncurrent state-of-the-art Vision Transformer model, ViT-Lar ge//one.tnum/six.tnum, achieves\n/nine.tnum/one.tnum./one.tnum/five.tnum% and /eight.tnum/three.tnum./five.tnum/four.tnum% accuracy on the PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum and ExpertLifeCLEF\n/two.tnum/zero.tnum/one.tnum/eight.tnum test sets, respectively; the best CNN model (ResNeSt-/two.tnum/six.tnum/nine.tnume) error rate\ndropped by /two.tnum/two.tnum./nine.tnum/one.tnum% and /two.tnum/eight.tnum./three.tnum/four.tnum%. Apart from that, additional tricks increased\nthe performance for the ViT-Base//three.tnum/two.tnum by /three.tnum./seven.tnum/two.tnum% on ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum\nand by /four.tnum./six.tnum/seven.tnum% on PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum. The retrieval approach achieved superior\nperformance in all measured scenarios with accuracy margins of /zero.tnum./two.tnum/eight.tnum%,\n/four.tnum./one.tnum/three.tnum%, and /one.tnum/zero.tnum./two.tnum/five.tnum% on ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum, PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum, and iNat/two.tnum/zero.tnum/one.tnum/eight.tnum–\nPlantae, respectively.\nKEYWORDS\nplant, species, classiﬁcation, recognition, machine learni ng, computer vision, species\nrecognition, ﬁne-grained\n/one.tnum. Introduction\nAccurate species identiﬁcation is essential for most ecologically motivated studies,\nin the pharmaceutical industry, agriculture, and conservation. In the case of\nFlora—with more than 400,000 species and high inter-species similarities—cor rect\nspecies determination requires a high level of expertise. An identiﬁcation proces s using\ndichotomous keys may take days, even for specialists, especially in locatio ns with high\nbiodiversity, and it is exceedingly diﬃcult for non-scientists (\nBelhumeur et al., 2008 ). To\novercome that issue, Gaston and O’Neill(2004) proposed to use a computer vision based\nsearch engine to partially assist with plant identiﬁcation and consequentially speed up\nFrontiers in Plant Science /zero.tnum/one.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nthe identiﬁcation process. Since then, we have witnessed an\nincreased research interest in plant species identiﬁcation using\ncomputer vision and machine learning (\nWu et al., 2006 , 2007;\nPrasad et al., 2011 ; Priya et al., 2012 ; Caglayan et al., 2013 ;\nMunisami et al., 2015 ), especially following the advances in deep\nlearning ( Ghazi et al., 2017 ; Bonnet et al., 2018 ; Lee et al., 2018 ;\nŠulc et al., 2018 ; Wäldchen and Mäder, 2018 ; Picek et al., 2019 ).\nThe overall performance of automatic ﬁne-grained image\nclassiﬁers has improved considerably over the last decade\nwith the development of deep neural networks, mostly\nConvolutional Neural Networks (CNNs). We refer readers\nunfamiliar with the principles of deep learning and CNNs\nto the book by\nGoodfellow et al. (2016). The success of\ndeep learning models trained with full supervision is typically\nconditioned by the existence of large databases of annotated\nimages. For plant recognition, such large-scale data are\navailable, thanks to citizen-science and open-data initiatives\nsuch as Encyclopedia of Life (\nEoL), Pl@ntNet, and the Global\nBiodiversity Information Facility ( GBIF). This allowed building\nchallenging datasets for ﬁne-grained classiﬁcation training and\nevaluation, e.g., in PlantCLEF (\nGoëau et al., 2016 , 2017,\n2018, 2020, 2021), LifeCLEF ( Joly et al., 2018 , 2019, 2020,\n2021), iNaturalist ( Van Horn et al., 2018 ), and Pl@ntNet\n(Garcin et al., 2021 ).\nThis article deals with automatic image-based plant species\nidentiﬁcation “in the wild” , thus dealing with: (i) Diﬀerent\nscales: Plant species can be observed from various angles\nand distances. (ii) Intra-class diﬀerences: Plant organs—\nleaf, fruit, bark, etc.—look very distinct. (iii) Inter-class\nsimilarities: The same organ of diﬀerent species might look\nvery similar. (iv) Background and Clutter: Other species\nare present behind or around the observed sample, and\nmany more. Identiﬁcation of plants from images is a ﬁne-\ngrained classiﬁcation problem, due to the high number\nof classes\n/one.tnum, high intra-class variance, and small inter-class\ndiﬀerences. Šulc and Matas (2017) showed that constrained\n/one.tnum We use the term class following the machine learning wording, where\nclasses denote the categories to be recognized, not the taxonomic ran k\n(classis), i.e., we use the term class for species.\nFIGURE /one.tnum\n“In the wild” photograph samples—PlantCLEF datasets. Images by soyoban, Liliane Roubaudi, Hugo Santacreu, Sarah Dechamps, Ric hard\nGautier, Heinz Gass, Alain Bigou, Jean-Michel Launay, and Jose Lu is Romero.\nplant identiﬁcation tasks, such as recognition of scanned leaves,\ncan be solved with a high level of classiﬁcation accuracy\n(± 99%). Yet the “in the wild” scenario, with an unspeciﬁed\nview or organ type, natural background, possible clutter in\nthe scene, etc., remains challenging even for state-of-the-art\ndeep learning methods. For “In the wild” photograph samples,\nrefer to\nFigure 1.\nFirst, is the standard approach, where ﬁne-grained\nrecognition is posed as closed-set classiﬁcation; the learning\ninvolves minimization of cross-entropy loss. Second, a retrieval-\nbased approach, which is very competitive, achieves superior\nin comparable conditions. Here, the training involves learning\nan embedding where the metric space leads to high recall\nin the retrieval task. Formulating ﬁne-grained recognition\nas retrieval has clear advantages—besides providing ranked\nclass predictions, it recovers relevant nearest-neighbor labeled\nsamples. The retrieved nearest neighbors provide explainability\nto the deep network and can be visually checked by an expert.\nMoreover, the user may inspect speciﬁc information, e.g., about\nlocation and date of collection, to further reduce decision\nuncertainty. Besides, the retrieval approach naturally supports\nopen-set recognition problems, i.e., the ability to extend or\nmodify the set of recognized classes after the training stage.\nThe set of classes may change, e.g., as a result of modiﬁcations\nto biological taxonomy. New classes are introduced simply\nby adding training images with the new label, whereas in the\nstandard approach, the classiﬁcation head needs re-training.\nOn the negative side, the retrieval approach requires, on top\nof running the deep net to extract the embedding, to execute\nthe nearest neighbor search eﬃciently, increasing the overall\ncomplexity of the ﬁne-grained recognition system.\nSection 4 discusses techniques that can noticeably improve\nthe performance of any vision-based species recognition system.\nThe techniques are diverse and attend to diﬀerent problems.\nThe prior shift in the datasets, i.e., the diﬀerence between\nthe training and test data class distribution, is a signiﬁcant\nand omnipresent phenomenon. We test existing prior shift\nadaptation methods and their impact on classiﬁcation accuracy.\nClass prior adaptation equips the system with the ability to\nreﬂect the change of prior probability of observing a specimen\nFrontiers in Plant Science /zero.tnum/two.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nof a given species over time and location. Image augmentations\nmake the system robust to acquisition conditions that, in some\napplications, e.g., plant recognition, are far from the lab setting.\nFinally, technical aspects related to training of the deep nets,\nsuch as learning rate schedule, loss functions and the impact of\nthe noisy data, on classiﬁcation performance, are discussed.\nThe performance evaluation part of the article builds on\nour winning submissions to PlantCLEF (\nPicek et al., 2019 ; Sulc\nand Matas, 2019 ) and extends a workshop article ( Šulc et al.,\n2018) and a PhD thesis ( Šulc, 2020 ). It substantially extends\nthe experiments by including recent state-of-the-art methods for\nimage classiﬁcation: Convolutional Neural Networks (CNNs)\n(\nXie et al., 2017 ; Hu et al., 2018 ; Zhang et al., 2020 ; Tan and Le,\n2021), Vision Transformers (ViTs) (Dosovitskiy et al., 2021 ), and\nan interpretable image retrieval approach ( Patel et al., 2021 ).\n/two.tnum. Related work\nThis chapter reviews existing methods, systems, and\napplications for plant species recognition: leaf or bark\nrecognition and “in the wild ” plant species recognition.\n/two.tnum./one.tnum. Leaf and bark recognition\nLeaf and bark recognition was the only application before\ndeep learning where automatic plant species identiﬁcation\nallowed to reliably tackle complex species recognition tasks.\nMost techniques were based on two steps: (i) descriptor\nextraction, often based on combining diﬀerent hand-\ncrafted features such as shape, color, or local descriptors\n(SIFT, SURF, ORB, etc.), and (ii) classical. classiﬁers such\nas k-Nearest Neighbor (\nMunisami et al., 2015 ), Random\nForest ( Caglayan et al., 2013 ), SVM ( Prasad et al., 2011 ;\nPriya et al., 2012 ), and early adoptions of neural networks\n(Wu et al., 2006 , 2007). The generalization capability of\nthese methods was limited, and so was the applicability—\ne.g., most leaf recognition methods relied on the shape\nof scanned leaves; thus, the usability in the “in the\nwild” scenario was limited since the uniform background\nwas required.\n/two.tnum./two.tnum. Flora recognition in the wild\nThe continuous progress in automatic plant species\nrecognition “in the wild ” has been strongly driven by the eﬀorts\nof the LifeCLEF research platform. Established in 2014, the\nLifeCLEF helps track progress and allows reliable evaluation\nof novel methods. In particular, the annual PlantCLEF\nchallenges are an immense source of plant species datasets\ntailored to develop and evaluate automatic plant species\nrecognition methods.\nFollowing the ﬁndings of the LifeCLEF challenges (\nJoly et al.,\n2018, 2019, 2020, 2021), AI-based identiﬁcation of the world\nﬂora has improved signiﬁcantly over the last 5 years, and it\nreached similar performance as human experts for common\n(\nŠulc et al., 2018 ) as well as for rare species ( Picek et al., 2019 ).\nEnsembles of CNN models were able to recognize 10,000 plant\nspecies from Europe and North America and 10,000 from the\nGuiana shield and the Amazonia with approximately 90 and 40%\naccuracy, respectively.\nOverall, there are few methods for plant recognition “in\nthe wild”; thus, we overview relevant methods for general ﬁne-\ngrained recognition.\nWu et al. (2019) developed a Taxonomic\nLoss that sums up loss functions calculated from diﬀerent\ntaxonomy ranks, e.g., species, genus, and family.\nCui et al.\n(2018) studied domain-speciﬁc transfer learning from large-\nscale datasets to domain-speciﬁc ﬁne-grained datasets. Zheng\net al. (2019) propose the Trilinear Attention Sampling Network\nthat generates attention maps by modeling the inter-channel\nrelationships, highlights attended parts with high resolution and\ndistills part features into an object-level feature.\nKeaton et al.\n(2021) utilized object detection as a form of attention with a\nbottom-up approach to detect plant organs and combine the\npredictions from organ-speciﬁc classiﬁers.\nMalik et al. (2021)\nused a standard ensemble-based approach utilizing Inception,\nMobileNet and ResNet CNN architectures.\nSeveral interesting approaches emerged in connection with\nthe annual PlantCLEF workshops. In PlantCLEF 2017, the best\nperforming submission competition with an accuracy of 88.5%\nwas developed by\nLasseck (2017). The underlying method is\nbased on 12 models derived from 3 architectures—GoogLeNet,\nResNet-152, and ResNeXt-101-64x4d. All models were ﬁne-\ntuned from the ImageNet-1k checkpoints utilizing various\naugmentation techniques, e.g., random cropping, horizontal\nﬂipping, variations of saturation and lightness, and rotation.\nWhile testing, 5 crops for all observation images are predicted\nwith all models and averaged. In the PlantCLEF 2018, the best\nperforming submission (\nSulc and Matas, 2019 ) was based on two\nTABLE /one.tnum Datasets for plant recognition; “in the wild” scenario.\nNumber of images in\nDataset Species Training Validation Test\nPl@ntNet-300K 1,081 243,916 31,118 31,112\niNaturalist 2017† 2,101 158,407 38,206 ×\niNaturalist 2018† 2,917 118,800 8,751 ×\niNaturalist 2021† 4,271 1,148,702 42,710 ×\nPlantCLEF 2016 1,000 113,205 × 2,583\nPlantCLEF 2017‡ 10,000 320,544 × 25,170\nExpertLifeCLEF 2018‡ 10,000 320,544 × 6,892\nPlantCLEF 2019 10,000 434,251 × 2,974\nSpecies from the Plantae kingdom marked †, data with “ trusted”, i.e., human veriﬁed,\nlabels marked‡.\nFrontiers in Plant Science /zero.tnum/three.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\narchitectures—Inception-ResNet-v2 and Inception-v4 ( Szegedy\net al., 2017 )—and their ensembles and achieved an accuracy\nof 88.4%. The TensorFlow-Slim API was used to adjust and\nﬁne-tune the networks from the publicly available ImageNet-\n1k pre-trained checkpoints. All networks shared the following\noptimizer settings: RMSprop with momentum and decay set to\n0.9, initial learning rate 0.01, and exponential learning rate decay\nfactor 0.4. Batch size, input resolution, and random crop area\nrange were set diﬀerently for each network. For the used values\nplease refer to the original article (\nSulc and Matas, 2019 ). The\nfollowing image pre-processing was used for training: Random\ncrop, with aspect ratio range (0.75, 1.33) and with various area\nranges, Random left-right ﬂip, and Brightness and Saturation\ndistortion. At test-time, 14 predictions per image are generated\nby using 7 crops and their mirrored versions: full image, central\ncrop covering 80% of the original image dimensions, central\ncrop covering 60% of the original image dimensions, and 4\ncorner crops covering 60% of the original image dimensions.\nThe signiﬁcant improvement in accuracy was achieved by using\nrunning averages of the trained variables instead of the values\nfrom the last training step. This is important especially if the\nnoisy labels are present in the training set where mini-batches\nwith noisy samples may produce large gradients pointing outside\nof the local optima. The use of the Polyak averaging (\nPolyak\nand Juditsky, 1992 ) resulted in a more stable version of the\ntraining variables.\n/three.tnum. Datasets\nThis section overviews datasets suitable for plant recognition\n“in the wild ” which, unlike other plant species datasets, contain\nimages of various plant body parts observed in an open world.\nSuch datasets are unique with high inter-class similarities—\nbark of one species is similar to the bark of another species—\nand high intra-class diﬀerences—the bark, ﬂower, and fruit\nof one species are visually distinct. Currently, datasets with\nlarge species diversity and a suﬃcient number of samples\nto train a reliable machine learning model are available.\nThe most signiﬁcant providers of those datasets—\niNaturalist,\nPl@ntNet, EoL, LifeCLEF—are closely connected to citizen-\nscience platforms, thus their data originate from thousands of\nusers, and are captured on various devices, observed under\ndiﬀerent conditions, and submitted from many countries. The\nmost inﬂuential datasets are described below and their main\ncharacteristics are summarized in\nTable 1.\nFor the experimental evaluation in this article, we used\niNaturalist 2018†, PlantCLEF 2017‡, and ExpertLifeCLEF 2018 ‡,\nas they oﬀer a suﬃcient number of species and test samples while\nkeeping the training set size and, thus, computational demands\nreasonably low.\n/three.tnum./one.tnum. LifeCLEF—PlantCLEF\nThe annual LifeCLEF—PlantCLEF identiﬁcation challenge\nis an important source of data for plant recognition. Since 2017\nthe PlantCLEF challenges present the following classiﬁcation\nproblem: For each plant observations consisting of one or more\nimages of the same specimen, predict the species. Example\nimages from one observation are visualized in\nFigure 2. The\nPlantCLEF datasets are mainly intended for benchmarking\nmachine-learning-based algorithms for plant recognition, thus\nare brieﬂy described below.\nThe PlantCLEF 2016 dataset (\nGoëau et al., 2016 ) comprises\n1,13,205 training images belonging to 41,794 observations of\n1,000 plant species from France and neighboring countries.\nEvery image is annotated with a plant organ label, i.e., ﬂower,\nleaf, fruit, stem, branch, and whole plant. A small fraction\nhas GPS coordinates. The test set contains 2,583 images.\nAs in all PlantCLEF challenges, no predeﬁned validation set\nwas provided.\nThe PlantCLEF 2017 challenge dataset (\nGoëau et al., 2017 )\nincludes 3,20,544 images from the Encyclopedia of Life with\ntrusted labels, and noisy web data crawled with Bing and Google\nsearch engines ( ∼1.15M images). The dataset covers 10,000\nplant species—mainly from North America and Europe—\nrepresenting the biggest plant species identiﬁcation dataset in\nFIGURE /two.tnum\nA PlantCLEF observation—images of diﬀerent plant parts. Images by Hugo Santacreu.\nFrontiers in Plant Science /zero.tnum/four.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nthe number of classes. The test set contains 25,170 images\n(17,868 observations).\nThe ExperLifeCLEF 2018 training dataset (\nGoëau et al.,\n2018) diﬀers from the PlantCLEF 2017 dataset only in\nthe test set. The test set contains 6,892 images (2,072\nobservations) covering species mainly from Western Europe and\nNorth America. In addition, selected endangered species, and\ncultivated and ornamental plant species were added.\nThe PlantCLEF2019 dataset (\nGoëau et al., 2019 ) contains\n434,251 images that belong to 10,000 rare species from the\nGuiana shield and the Amazon rain forest.The images originate\nfrom EoL and Google/Bing search engines; the majority have the\n“noisy” labels. The test set is composed of 742 plant observations\n(2,974 images) collected and identiﬁed by ﬁve experts on\ntropical ﬂora.\n/three.tnum./two.tnum. iNaturalist\niNaturalist is a crowd-based citizen-science platform\nallowing citizens and experts to upload, annotate and categorize\nspecies of the world. iNaturalist has a wide geographic and\ntaxonomic coverage—more than 343 thousand species with\napproximately 97 million observations. The annual iNaturalist\ncompetition datasets that include a signiﬁcant number of plant\nspecies are described below.\niNaturalist 2017 : The iNaturalist 2017 dataset (\nVan Horn\net al., 2018 ) contains 2,101 plant species, with 1,58,407 training\nand 38,206 validation images that have been collected and\nveriﬁed by multiple independent users. The dataset features\nmany visually similar species that have been captured worldwide\nand under various conditions. As labels for the test set were\nnot provided, it is impossible to specify how many plant species\nare contained.\niNaturalist 2018 :\nThe iNaturalist Challenge 2018 dataset\nincludes 2,917 plant species, with 118,800 training and 8,751\nvalidation images acquired the same way as in the previous year.\nAdditionally, complete taxonomy information was given for all\nimages. Test labels were not provided.\niNaturalist 2021 :\nThe iNaturalist Challenge 2021 dataset\nwith 1,148,702 training and 42,710 validation images is the\nmost extensive dataset considering the number of images—the\nnumber of plant species was increased to 4,271. Test labels were\nnot provided as in all iNaturalist Challenge datasets.\n/three.tnum./three.tnum. Pl@ntNet-/three.tnum/zero.tnum/zero.tnumK\nThe Pl@ntNet-300K dataset\nGarcin et al. (2021) is built from\nthe database of the Pl@ntNet citizen observatory and includes\n1,081 species and 306,146 images. The dataset exhibits a long-\ntailed class imbalance, where 20% of the most common species\nprovide 89% of the images. Provided validation and test sets\ninclude 31,118 and 31,112 images, respectively.\n/four.tnum. Methods\nThis section is divided into three parts. First, the pipeline\nfor automatic Plant Recognition by the standard Image\nClassiﬁcation pipeline is described. Second, an alternative and\nnovel approach to Plant Recognition via kNN classiﬁcation in\ndeep embedding space is proposed and described. Finally, a\nrange of methods and techniques that increase classiﬁcation\nperformance are introduced.\n/four.tnum./one.tnum. Deep neural network classiﬁers\nPlant species recognition can be easily automated through\nthe standard image classiﬁcation approach, where a Deep\nNeural Network (DNN) serves as a deep feature extractor\nand a fully convolutional neural network as a classiﬁer.\nImage representations learned by deep neural networks\nprovide signiﬁcantly better results than handcrafted features.\nFurthermore, DNNs are data-driven and require no eﬀort\nor expertise for feature selection as they automatically\nlearn discriminative features for every task. In addition, the\nautomatically learned features are represented hierarchically on\nmultiple levels. Having such deep features is a strong advantage\nover traditional approaches.\nCurrently, many DNN architectures are widely used;\nthus, a broad range of Convolutional Neural Networks\nand Transformer-based architectures are evaluated to test\nthe classiﬁcation capabilities for diﬀerent feature extractor\narchitectures. The ResNet-50 (\nHe et al., 2016 ), Inception-v4,\nand Inception-ResNet-v2 ( Szegedy et al., 2017 ) are chosen as\nbaselines as they are commonly used in related study. We add\nthe following novel and state-of-the-art architectures:\nSE-ResNeXt-101: Extends the ResNet deep residual blocks\nby adding the NeXt dimension, called Cardinality (\nXie et al.,\n2017), and Squeeze and Excite blocks that adaptively re-\ncalibrates channel-wise feature responses by explicitly modeling\ninter-dependencies between channels (\nHu et al., 2018 ).\nResNeSt-269e: Applies channel-wise attention to diﬀerent\nparts of the architecture to leverage and allow the cross-\nfeature interactions and learning of the more diverse\nrepresentations. (\nZhang et al., 2020 ).\nEﬃcientNetV2-S: Similarly to the ﬁrst EﬃcientNet\ngeneration, the EﬃcientNet-v2 architectures are developed by a\ncombination of training-aware architecture search and scaling,\nto jointly optimize training speed and parameter eﬃciency (\nTan\nand Le, 2021 ). Newly, the models: (i) were searched from the\nspace enriched with Fused-MBConv, and (ii) the last stride-1\nstage in the original EﬃcientNet was removed.\nFrontiers in Plant Science /zero.tnum/five.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nFIGURE /three.tnum\nImage augmentations—Horizontal and vertical ﬂip, small brightne ss/contrast adjustments, and /eight.tnum/zero.tnum–/one.tnum/zero.tnum/zero.tnum% crops—used while training the deep\nneural network classiﬁer. Image by Zoya Akulova.\nVision Transformers: Unlike CNN, the Vision Transformer\n(ViT) ( Dosovitskiy et al., 2021 ) does not use convolutions but\ninterprets an image as a sequence of patches and processes it\nby a standard Transformer encoder used primarily for natural\nlanguage processing (\nVaswani et al., 2017 ). Compared to state-\nof-the-art convolutional networks, selected ViT architectures\ndemonstrated excellent performance in ﬁne-grained image\nclassiﬁcation (\nPicek et al., 2022 ).\n/four.tnum./one.tnum./one.tnum. Training strategy\nAll NN architectures were initialized from publicly available\nImageNet-1k or ImageNet-21k pre-trained checkpoints\n(\nWightman, 2019 ) and further ﬁne-tuned for 100 epochs.\nMini-batch gradients were accumulated to reach an eﬀective\nsize of 128 for all the architectures—most of the time, 4 batches\nof size 32 are accumulated. SGD with momentum (0.9) was used\nas an optimizer with a custom learning rate (LR) schedule—\nReduce LR to a fraction of 0.9 if validation loss does not\ndecrease for 2 epochs. The loss was calculated as Softmax Cross\nEntropy. While training, we employ a few data augmentation\ntechniques from the Albumentations library (\nBuslaev et al.,\n2020). A sample image and its augmented variations are shown\nin Figure 3. Augmentation methods, their description, and\nspeciﬁed non-default parameters are:\n• RandomResizedCrop: creates a random resized crop with a\nscale of 0.8 − 1.0.\n• HorizontalFlip: randomly (50% probability) ﬂips the\nimage horizontally.\n• VerticalFlip: randomly (50% probability) ﬂips the\nimage vertically.\n• RandomBrightnessContrast: changes contrast and\nbrightness by a random factor in a range −0.2 − 0.2\nwith 20% probability.\nAll images were: resized to match the pre-trained model\ninput size of 224 × 224 or 384 × 384, re-scaled from 0 − 255\nto 0 − 1, and normalized by mean (0.5) and std (0.5) values in\neach channel.\n/four.tnum./one.tnum./two.tnum. Test-time\nAt the test time, all images are resized to the appropriate\nsize, i.e., 224 × 224 or 384 × 384, and normalized as in\ntraining. Next, all observation images are feed-forward and class\npredictions are combined. The study about diﬀerent methods\nfor prediction combinations is included in Section 5.3. The\nclassiﬁcation performance for all selected models is evaluated on\nboth resolutions—224 × 224 and 384 × 384—and two diﬀerent\ntest sets—PlantCLEF 2017 and ExpertLifeCLEF 2018.\n/four.tnum./two.tnum. Plant recognitionvia kNN\nclassiﬁcation in deep embedding space\nFine-grained recognition of plant species can be alternatively\nsolved via the k-Nearest Neighbors algorithm (kNN) in an\nembedding space where the samples from the same semantic\nclass are grouped together, and the samples from diﬀerent\nclasses are far apart. Recent study by\nTouvron et al. (2021);\nKhosla et al. (2020) have shown such a recognition technique\nto outperform standard cross entropy based training. For\ntraining of such an embedding, we use the current state-of-\nthe-art image retrieval method\nPatel et al. (2021), where a\ndeep neural network is trained on a surrogate loss—Recall@k.\nThe notations and methodology for the retrieval approach are\ndescribed below.\n/four.tnum./two.tnum./one.tnum. Notations\nFor a query example q ∈ X, the objective of a retrieval\nmodel is to obtain semantically similar samples from a collection\n/Omega1 ⊂ X, also known as database, where X is the space of\nall images. The database is divided into two subsets based\non the positive or negative samples to the query q. These\nsubsets are denoted by Pq and Nq, respectively, such that /Omega1 =\nPq ∪ Nq. For the query q, all database samples are ranked\nbased on a similarity score, with the goal to rank positives\nbefore negatives.\nFrontiers in Plant Science /zero.tnum/six.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\n/four.tnum./two.tnum./two.tnum. Deep embedding\nImage embedding, a learned vector representation of an\nimage, is generated by function fθ : X → Rd. Function fθ\nis a deep neural network, either a ResNet-50 or a Vision\nTransformer in this article, mapping input images to an L2-\nnormalized d-dimensional embedding. Embedding for image x\nis denoted by xxx = fθ (x). Parameters θ of the network are learned\nduring the training using Recall@k surrogate loss. The similarity\nscore between a query q and a database image x is computed by\nthe dot product of the corresponding embeddings and is denoted\nby s(q, x) = qqqTxxx, also denoted as sqx.\n/four.tnum./two.tnum./three.tnum. Recall@k surrogate loss\nThe Recall@k Surrogate loss is a diﬀerentiable\napproximation of the Recall@k evaluation metric. For a\nquery q, the Recall@k metric is the ratio of positive (relevant)\nsamples in top-k retrieved samples to the total number of\npositive samples in the database, given by |Pq|. The metric\nfocuses only on top-k ranked samples and is one of the\nstandard metrics to evaluate retrieval benchmarks. Recall@k\ncannot be directly used as a loss function. It requires two\nnon-diﬀerentiable operations: ranking the database samples\nand counting the number of positives that appear in top-k. The\nsubsequent text presents Recall@k expressed mathematically,\nnon-diﬀerentiability, and the diﬀerentiable approximation as\nproposed by\nPatel et al. (2021).\nPatel et al. (2021) denotes Recall@k by Rk\n/Omega1 (q) when\ncomputed for query q and database /Omega1 and expresses it\nmathematically in terms of ranks of samples in the database:\nRk\n/Omega1 (q) =\n∑\nx∈Pq\nH(k − r/Omega1 (q, x))\n|Pq| , (1)\nwhere the rank of sample x is denoted by r/Omega1 (q, x), which depends\non the query sample q and the database /Omega1 . H(.) is the Heaviside\nstep function, which is 0 for negative values and otherwise 1. The\nrank r/Omega1 (q, x) of sample x is computed according to the similarity\nscore, and it can be expressed mathematically as:\nr/Omega1 (q, x) = 1 +\n∑\nz∈/Omega1 ,z̸=x\nH(sqz − sqx), (2)\nwhere H(.) is also the Heaviside step function applied on\nthe diﬀerence of similarity scores. Therefore, Recall@k from\nEquation (\n1) can also be directly expressed as a function of\nsimilarity scores as:\nRk\n/Omega1 (q) =\n∑\nx∈Pq\nH(k − 1 − ∑\nz∈/Omega1 ,z̸=x\nH(sqz − sqx))\n|Pq| . (3)\nThe computation of Recall@k in Equation (3) involves the use\nof two Heaviside step functions, one to obtain the rank and\nthe other to count the positives in top-k retrieved samples. The\ngradient of the Heaviside step function is a Dirac delta function.\nHence, direct optimization of recall with back-propagation is not\nfeasible.\nPatel et al. (2021) provide a smooth approximation of\nthe Heaviside step function by the logistic function, a sigmoid\nfunction στ : R → R controlled by temperature τ :\nστ (u) = 1\n1 + e− u\nτ\n, (4)\nReplacing the two Heaviside step functions with the sigmoid\nfunctions of appropriate temperatures, a smooth approximation\nof Recall@k can be expressed as:\n˜Rk\n/Omega1 (q) =\n∑\nx∈Pq\nστ1 (k − 1 − ∑\nz∈/Omega1\nz̸=x\nστ2 (sqz − sqx))\n|Pq| , (5)\nThe Recall@k Surrogate loss from Equation (5) is diﬀerentiable\nand is used for training the parameters θ of the deep embedding\nmodel. In practice, the Recall@k Surrogate loss is re-scaled to\nhave values between 0 and 1, by dividing it by min( k, |Pq|)\ninstead of |Pq|, and by clipping the values larger than k in\nthe numerator. The single-query loss to be minimized in a\nmini-batch B, with size |B|, and query q ∈ B is given by:\nLk(q) = 1 − ˜Rk\nB\\q(q). (6)\nThe ﬁnal loss is computed by averaging the loss across multiple\nvalues of k as:\nLK (q) = 1\n|K|\n∑\nk∈K\nLk(q). (7)\nIn practice, we use following values K = { 1, 2, 4, 8, 16 }. All\nexamples in the mini-batch are used as queries, and the average\nloss over all queries is minimized during the training.\n/four.tnum./two.tnum./four.tnum. Training\nThe training is set up for 100 epochs using an AdamW\noptimizer (\nLoshchilov and Hutter, 2019 ) with an initial learning\nrate of 0.0001, which decreases by a factor of 0.3 using a step\ndecay. For data augmentation, images are resized to 256 × 256,\nand a random crop of 224 × 224 is taken, followed by a random\nhorizontal ﬂip with a probability of 0.5 and normalization with\nmean and SD. The mini-batch is constructed via class-balanced\nsampling with 4 samples per class and a large batch size of\n4, 000 is used. Two feed-forward passes (\nPatel et al., 2021 ) are\naccumulated to create a larger batch size to address the GPU\nhardware demands. The ﬁrst feed-forward pass is performed on\nthe batch with 4, 000 samples in chunks of 200 samples at a time.\nAll embedding vectors are stored while the intermediate features\nare discarded from the GPU memory. Using the embedding\nvectors and the ground truth labels, the loss (Equation 7) and the\ngradients for each sample with respect to the embedding vectors\nFrontiers in Plant Science /zero.tnum/seven.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nare calculated. Finally, a second feed-forward is performed, also\nin the chunks of 200 samples at a time, allowing the propagation\nof the gradients through the deep embedding model for the\ncurrent chunk of 200 samples. At the end of the second feed-\nforward stage, the model’s weights are updated.\n/four.tnum./two.tnum./five.tnum. Test-time\nAt inference, the test image is resized to 256 × 256, and a\ncentral crop of 224 × 224 with normalization is the input to\nthe deep embedding model. A feed-forward pass is performed\nthrough all the training and testing samples, and the embedding\nvectors are stored. Each test sample is treated as a query for\nretrieval, and the ten closest samples from the training set are\nobtained. A majority vote determines the semantic class of the\ntest sample.\n/four.tnum./three.tnum. Class prior estimation\nCommonly in Machine Learning, the class prior\nprobabilities are the same for the training data and test\ndata. However, plant species distributions change dramatically\nbased on various aspects, i.e., seasonality, geographic location,\nweather, the hour in a day, etc. The problem of adjusting CNN\noutputs to the change in class prior probabilities was discussed\nin\nSulc and Matas (2019), where it was proposed to recompute\nthe posterior probabilities (predictions) p(ck|xi) by Equation (8).\npe(ck|xi) = p(ck|xi) pe(ck)p(xi)\np(ck)pe(xi) =\np(ck|xi) pe(ck)\np(ck)\nK∑\nj=1\np(cj|xi)\npe(cj)\np(cj)\n∝ p(ck|xi) pe(ck)\np(ck) , (8)\nThe subscript e denotes probabilities on the evaluation/test\nset. The posterior probabilities p(ck|xi) are estimated by\nthe Convolutional Neural Network outputs since it was\ntrained with the cross-entropy loss. For class priors p(ck),\nwe have an empirical observation—the class frequency in the\ntraining set. The evaluation and test set priors pe(ck) are,\nhowever, unknown. To evaluate the impact of changing class\npriors, we compare three existing prior estimation algorithms—\nthe Expectation–maximization algorithm (EM) of\nSaerens et al.\n(2002) and the recently proposed CM-L and SCM-L methods of\nSipka et al. (2022).\n/four.tnum./three.tnum./one.tnum. EM—expectation maximization\nIn our ExpertLifeCLEF 2018 challenge submissions, we\nfollowed the proposition from Sulc and Matas (2019) to use\nan EM algorithm of Saerens et al. (2002) for the estimation\nof test set priors by maximization of the likelihood of the test\nobservations. The E and M step are described by Equation (9),\nwhere the super-scripts ( s) or ( s + 1) denote the step of the\nEM algorithm.\np(s)\ne (ck|xi) =\np(ck|xi) p(s)\ne (ck)\np(ck)\nK∑\nj=1\np(cj|xi)\np(s)\ne (cj)\np(cj)\n,\np(s+1)\ne (ck) = 1\nN\nN∑\ni=1\np(s)\ne (ck|xi),\n(9)\nIn our submissions, we estimated the class prior probabilities for\nthe whole test set. However, one may also consider estimating\ndiﬀerent class priors for diﬀerent locations, based on the GPS-\ncoordinates of the observations. Moreover, as discussed by\nSulc\nand Matas (2019), one may use this procedure even in the cases\nwhere the new test samples come sequentially.\n/four.tnum./three.tnum./two.tnum. CM-L—confusion matrix based likelihood\nmaximization\nThe prior estimate is based on maximizing the likelihood\nof the observed classiﬁer decisions. The CM-L method uses the\nclassiﬁer’sconfusion matrix (CM) in the format Cd|y, where the\nvalue in the k-th column and i-th row is the probability p(D =\ni|Y = k) of the classiﬁer deciding for class i when the true class is\nk. The new class priors P are then estimated by maximizing the\nlog-likelihood with the following objective:\nˆP = arg max\nP\nℓ(P) = arg max\nP\nK∑\nk=1\nnk log(Ck,: ·P) (10a)\ns.t.:\nK∑\nk=1\nPk = 1; ∀ k : Pk ≥ 0, (10b)\nwhere nk is the numbers of classiﬁer’s decisions for class k on test\nset and Ck,: is the k-th row of the confusion matrix.\nThe SCM-L method works analogically, but uses the so-\ncalled soft confusion matrix (SCM) Csoft\nd|y estimated from the\nclassiﬁer’s soft predictionsf as\nˆCsoft\n:,k = 1\nNk\n∑\nxi : yi=k\nf(xi), (11)\nwhere ˆCsoft\n:,k denotes the k-th column of SCM. The probability\npsoft\nE (D) can be estimated by averaging predictions f(x) over the\ntest set.\n/five.tnum. Results\nFirst, we compare the state-of-the-art Convolutional\nNeural Networks and Vision Transformers in Section 5.1.\nFrontiers in Plant Science /zero.tnum/eight.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nTABLE /two.tnum Classiﬁcation accuracy on the PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum and the ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum datasets for diﬀerent image prediction combination strategies.\nArchitecture Test set Image-wise Max Softmax Mean Softmax Max L ogits Mean Logits\nEﬃcientNetV2-S 2017 79.21 84.35 85.26 85.54 85.75\nEﬃcientNetV2-S 2018 53.08 67.28 70.32 72.25 74.13\nViT-Base/32 2017 73.50 80.43 80.55 80.79 81.29\nViT-Base/32 2018 49.36 66.94 66.84 68.87 71.53\nTABLE /three.tnum Image classiﬁcation accuracy for Deep Neural NetworkClassiﬁers on the PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum (right) and ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum (left) test sets.\nPlantCLEF 2018—Accuracy [%] PlantCLEF 2017—Accuracy [%]\nArchitecture Input Images Observations Images Observations\nResNet-50 224 × 224 40.03 56.32 68.00 74.57\nInception-v4 224 × 224 43.41 59.41 71.32 77.92\nInception-Resnet-V2 224 × 224 44.14 68.15 70.57 78.96\nViT-Base/32 224 × 224 49.36 71.53 73.50 81.29\nViT-Base/16 224 × 224 51.58 73.70 75.54 82.57\nEﬃcientNetV2-S 224 × 224 53.08 74.13 79.21 85.75\nViT-Tiny/16 384 × 384 47.43 69.06 73.64 80.59\nSE-ResNeXt-101 384 × 384 54.61 73.75 80.31 85.98\nResNeSt-269e 384 × 384 56.27 74.52 81.68 86.74\nViT-Base/16 384 × 384 58.49 77.03 82.28 87.75\nEﬃcientNetV2-L 384 × 384 59.90 77.03 84.15 88.52\nViT-Large/16 384 × 384 67.03 83.54 86.87 91.15\nObservation values calculated as Mean Logits.\nSecond, we evaluate the image retrieval approach\nto classiﬁcation and compare it with the standard\nclassiﬁers in Section 5.2. Finally, additional techniques\nfor performance improvements are evaluated in\nSection 5.3.\n/five.tnum./one.tnum. Image classiﬁcation\n/five.tnum./one.tnum./one.tnum. Combining several predictions per\nobservation\nLifeCLEF datasets include sets of images belonging\nto the same specimen observation. Typically, the images\nrepresent diﬀerent organs of the specimen, e.g., ﬂower, leaf,\nSuch sets of images are connected by the ObservationID\nvalues provided in the metadata. The PlantCLEF 2017\ntest set contains 17,868 observations and 25,170 images.\nThe ExpertLifeCLEF 2018 test set is smaller with 2,072\nobservations and 6,892 images. Plant species prediction based\non multiple images is intuitive; it is inspired by the process\nused for years by botanists. Four simple approaches of per-\nimage prediction combination are evaluated. Decide for the\nclass with\n• Max softmax : maximum posterior probability estimate—\nsoftmax—over all images, i.e., follow the most conﬁdent\nprediction,\n• Mean softmax: maximum average (over images) estimated\nposterior probability,\n• Max logit : maximum activation value (Logit) over\nall images.\n• Mean logits: maximum average (over images) logit value.\nThe best results of species prediction combination was achieved\nby selecting the species with the maximum value of logit\nmean. For the single ViT-Base/32 model and image size\nof 224 × 224, the Mean logits approach outperformed the\nmax softmax by 0.86% on PlantCLEF 2017 and 4.59% on\nExpertLifeCLEF 2018. Overall, the accuracy is signiﬁcantly\nhigher for observations then for single images, in some cases\nincreasing the accuracy by more then 20%. Full results are\nshown in\nTable 2.\nConvolutional neural networks: The comparison of the\nformer and recent state-of-the-art CNN architectures on the\nPlantCLEF2017 and the ExpertLifeCLEF 2018 test sets shows\nsimilar behavior as on other ﬁne-grained datasets (\nWah\net al., 2011 ; Van Horn et al., 2018 ; Picek et al., 2022 ). The\nbest performing model on both datasets is EﬃcientNetV2-L\nFrontiers in Plant Science /zero.tnum/nine.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nFIGURE /four.tnum\nClassiﬁcation performance (F/one.tnum and Accuracy) as box-plot for three backbone architectures and Classiﬁcation and Retrieval app roaches. Tested\non PlantCLEF/two.tnum/zero.tnum/one.tnum/seven.tnum test set with input resolution of /two.tnum/two.tnum/four.tnum× /two.tnum/two.tnum/four.tnum.\nTABLE /four.tnum Performance evaluation for Classiﬁcation (C) and Retrieval (R) based methods.\nExpertLifeCLEF 2018 PlantCLEF 2017 iNat2018–Plantae\nArchitecture Method Acc. Macro F1 Acc Macro F1 Acc Macro F1\nResNet-50 C 59.87 55.11 77.89 54.48 57.73 52.69\nViT-Base/32 C 65.21 60.29 80.68 59.18 57.24 53.17\nViT-Base/16 C 71.71 67.35 84.48 65.40 67.42 64.51\nResNet-50 R 60.15 56.30 80.27 55.57 57.95 56.32\nViT-Base/32 R 66.48 61.49 84.89 60.79 63.12 61.24\nViT-Base/16 R 71.99 69.20 88.61 66.39 77.67 76.74\nAll models were trained for 100 epochs with ﬁxed image size (224 × 224). No test-time augmentations were used. The most conﬁdent ima ge prediction is used for all images belonging to\nthe same observation.\nFrontiers in Plant Science /one.tnum/zero.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nwith 77.03% accuracy on ExpertLifeCLEF 2018 and 88.52%\naccuracy on PlantCLEF 2017. Other deep networks including\nResNeSt-269e and SE-ResNeXt-101 underperformend by a\nsigniﬁcant margin. The achieved scores are summarized in\nTable 3.\nVision transformers: The performance of diﬀerent ViT\narchitectures in the FGVC domain, multiple architectures,\nwas evaluated for two diﬀerent input resolutions—224 ×\n224 and 384 × 384—on two test sets—PlantCLEF2017 and\nExpertLifeCLEF 2018. More precisely, ViT-Base/16 and ViT-\nBase/32 are compared on the input size of 224 × 224 and ViT-\nLarge/16, ViT-Base/16 and ViT-Tiny/16 are tested on the input\nsize of 384 × 384.\nIn the 384 × 384 scenario, ViT-Large/16 outperformed the\nbest CNN model (ResNeSt-269e) 2.63% points on PlantCLEF\n2017 and by 6.51% points on ExpertLifeCLEF 2018 while\nreducing the error by 22.91% and 28.34%, respectively. In\nthe 224 × 224 scenario, the relative performance diﬀered;\nEﬃcientNetV2-S outperformed all the models including\nboth Vision Transformers on the ExpertLifeCLEF 2017\ndataset. Comparison on the PlantCLEF2017 dataset, show\nthe insigniﬁcant performance diﬀerence between ViT-Base/16\nand EﬃcientNetV2-S.\n/five.tnum./two.tnum. Classiﬁcation vs. metric learning\nThis section compares training a softmax image classiﬁer\nexplicitly as in the previous experiments and training an\nimage retrieval system, which is subsequently used for\nnearest neighbor classiﬁcation. The resolution of images,\npre-trained weights and number of training epochs are\nkept the same across the two setups for a fair comparison.\nEven though we compare both methods under the same\nconditions, those conditions handicap the standard\nimage classiﬁcation approach as any additional techniques\nare permitted.\nOverall, the retrieval approach achieved superior\nperformance in all measured scenarios. Notably, the ViT-\nBase/16 feature extractor architecture achieved a higher\nclassiﬁcation accuracy with a margins of 0.28, 4.13, and\n10.25% on ExpertLifeCLEF 2018, PlantCLEF 2017, and\niNat2018–Plantae, respectively. Besides, the macro-F1\nperformance diﬀerences margin is noticeably higher—1.85%\nfor ExpertLifeCLEF 2018 and 12.23% for iNat2018–Plantae\ndatasets. Even though the standard classiﬁcation approach\nperforms better on classes with fewer samples (refer to\nFigure 4), common species with high a-prior probability\nare frequently wrongly predicted. This is primarily due to\nthe high-class imbalance preserved in the dataset mimicked\nby the deep neural network optimized via SoftMax Cross-\nEntropy Loss. Thus, the results of the standard image\nclassiﬁcation approach performs way worst in case of the\nTABLE /five.tnum Ablation study considering diﬀerent techniques for\nViT-Base//three.tnum/two.tnum performance improvements.\nTest 2018 - Acc [%] Test 2017 - Acc [%]\nTTA CCA RC Images Observations Images Observations\n× × × 49.59 71.62 73.59 81.29\n✓ × × +2.51 +1.98 +5.38 +4.65\n× ✓ × +0.32 +1.06 +0.70 +0.80\n× × ✓ –0.48 +1.30 +3.82 +3.86\n× ✓ ✓ –0.10 +1.93 +3.83 +3.89\n✓ × ✓ +2.44 +2.51 +5.22 +4.22\n✓ ✓ × +3.01 +3.72 +5.16 +4.38\n✓ ✓ ✓ +2.83 +2.85 +5.68 +4.67\nTABLE /six.tnum Accuracy before and after prior shift adaptation with the EM\nalgorithm (Saerens et al., /two.tnum/zero.tnum/zero.tnum/two.tnum) and the (S)CM-L methods (Sipka et al.,\n/two.tnum/zero.tnum/two.tnum/two.tnum) on the ExpertLifeCLEF /two.tnum/zero.tnum/one.tnum/eight.tnum and the PlantCLEF /two.tnum/zero.tnum/one.tnum/seven.tnum test sets.\nArchitecture Test set EM CM-L SCM-L\nViT-Large/16 PlantCLEF 2017 +1.17 +1.25 +0.66\nViT-Large/16 ExpertLifeCLEF 2018 +2.21 +1.83 +1.64\nSE-ResNeXt-101 PlantCLEF 2017 +1.65 +1.50 +1.07\nSE-ResNeXt-101 ExpertLifeCLEF 2018 +3.81 +3.28 +3.23\nAll results are using the ﬁne-tuned models and Mean Softmax Accuracy for c ombining\npredictions belonging to the same observation. Input size 384 × 384.\nTABLE /seven.tnum Impact of additional noisy data on classiﬁcation\nperformance.\nTest 2018 - Acc [%] Test 2017 - Acc [%]\nMin. samples Images Observations Images Observations\n10 +0.17 –0.58 –0.20 –0.49\n20 +0.32 –0.53 –0.33 –0.38\n30 –0.13 –0.24 –0.44 –0.66\n40 –0.10 –1.25 –0.60 –0.82\nBaseline 49.77 68.24 74.19 81.16\nmacro-F1 score. A full comparison of the classiﬁcation and\nretrieval-based methods and their appropriate recognition\nscores are listed in\nTable 4. Three architectures—ResNet-\n50, ViT-Base/32, and ViT-Base/16 are evaluated. It can be\nseen from the results that for all selected architectures,\nretrieval leads to better performance. Furthermore, in\nFigure 5, we provide qualitative examples from the\nretrieval approach on the iNaturalist dataset. The Top5\npredictions for randomly selected target images show that\nthe retrieval-like approach allows better interpretability of the\nresults.\nFrontiers in Plant Science /one.tnum/one.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nFIGURE /five.tnum\nQualitative examples from the retrieval approach on the iNatural ist dataset. The leftmost column shows samples from the test set foll owed by\nﬁve nearest neighbors in the learned embedding space from the tra ining set. The red box denotes the wrong species.\nFrontiers in Plant Science /one.tnum/two.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\n/five.tnum./three.tnum. A ﬁne-tuning cookbook\nIn this section, we evaluate several methods that have\nthe potential to increase performance for almost any deep\nneural network architecture considerably. The evaluation\nconsiders diﬀerent loss functions, learning rate schedulers,\nprior estimation methods, and augmentations. Furthermore, the\nimpact of the noisy data and the contribution of the test-time\naugmentations are studied. We list helpful methods and those\nthat will make the performance worst if utilized. The evaluation\nis carried out on the PlantCLEF2017 and ExpertLifeCLEF 2018\ndatasets and ViT/Base-32 architecture with an input size of 224 ×\n224, if not stated diﬀerently. All used methods are described\nbellow. The ablation study for relevant methods is summarized\nin\nTable 5.\nCyclic cosine annealing: We compare standard cosine, a\ncustom adaptive strategy where Learning Rate is decayed by\n10% if validation loss is not reduced for two epochs, and Cyclic\nCosine Annealing (CCA). The CCA is an alternative to standard\nLearning rate scheduling approaches, e.g., Exponential, Linear,\nStep, and Cosine. The CCA is divided into multiple cycles where\nthe start learning rate decreases by 20%, and the learning rate\nin each cycle decreases via the standard cosine function. Such\na learning rate schedule allows for diverging from local minima\nand searching for better optima. We compare standard cosine,\na custom adaptive strategy where Learning Rate is decayed\nby 10% if validation loss is not reduced for two epochs, and\nCyclic Cosine Annealing (CCA). Using the CCA instead of\nthe standard approaches, we measured relative performance\nincreases equal to +1.06 and +0.80% on the ExpertLifeCLEF\n2018 and LifeCLEF2017, respectively.\nTest-time augmentations: Test-time augmentations is a\nprocedure where various mutations of the original image are\nfeed-forwarded through the deep neural network in order to\nprovide images in diﬀerent rotations or scales. In our case, we\nuse a simple test-time augmentation procedure—each test image\nis processed as a batch of 13 images:\n• One original image (resized to 224 × 224 or 384 × 384),\n• Four central crops covering 90, 80, and 70% of the original\nimage size,\n• Two top left corner crops covering 80 and 70% of the\noriginal image size,\n• Two top right corner crops covering 80 and 70% of the\noriginal image size,\n• Two bottom left corner crops covering 80 and 70% of the\noriginal image size,\n• Two bottom right corner crops covering 80 and 70% of the\noriginal image size,\nThe predictions from all 13 cropped/augmented images are then\ncombined. The results in\nTable 5 show than using so called\ntest time augmentation improves the classiﬁcation accuracy\nup to 1.98 and 4.65% on the ExpertLifeCLEF 2018 and\nLifeCLEF2017, respectively.\nRandom crop: Random crop allows for learning more\ndetailed object representation as an image is not resized to a\nsmaller resolution. Furthermore, training with random crops\nhas high synergy with the test-time augmentation process if\ncrops of similar size are used for TTA. For just a random crop,\nwe measured performance increases equal to +1.30 and +3.86%\nachieved on the ExpertLifeCLEF 2018 and LifeCLEF2017,\nrespectively. Combining with TTA, the margin increased to\n+1.93%, +3.89%.\nPrior shift adaptation: The prior shift adaptation methods\ndescribed in Sections 4.3.1 and 4.3.2 are compared in\nTable 6.\nPrior shift adaptation is applied to the prediction of each test\naugmentation, before the combination of augmentation and\nimages per observation by averaging. The results show that in all\ncases, prior shift adaptation improves the recognition accuracy.\nThe EM algorithm of\nSaerens et al. (2002) achieves the best\nresult in three cases, the CM-L method of Sipka et al. (2022)\nin one case, but the diﬀerences are very small among the three\ncompared prior shift adaptation methods.\nFocal loss: Even though commonly used in object detection,\nFocal Loss (\nLin et al., 2017 ) has the potential to focus the training\nprocess on more challenging and rare samples and could prevent\nthe vast majority of images from dominating the optimizer.\nAs any considerable performance increase for ViT and CNN\narchitectures was not measured on both datasets, we do not\nrecommend using Focal Loss for plant recognition.\nImpact of the noisy data: Noisy data, i.e., data without\nhuman-veriﬁed labels, are commonly used to increase the\nnumber of rare species samples and balance long-tailed class\ndistribution. Even though the\nKrause et al. (2016) showed\nunreasonable eﬀectiveness of the noisy labels on small-scale\nFGVC datasets, the contribution in the “in the wild” scenario is\nnot established. In the case of the ﬂora recognition, upsampling\nthe minimum samples for each class (up to 10, 20, 30,\nand 40) did not improve the accuracy on both testing sets,\ni.e., the performance diﬀerence was statistically insigniﬁcant\n(see\nTable 7).\n/six.tnum. Conclusion\nThe article assessed automatic plant identiﬁcation as a\nﬁne-grained classiﬁcation task on the largest available plant\nrecognition datasets coming from the LifeCLEF and CVPR-\nFGVC workshops, counting up to 10,000 plant species.\nState-of-the-art classiﬁers: The comparison of deep neural\nnetwork classiﬁers in Section 5.1 shows the improvement in\nclassiﬁcation accuracy achieved by recent CNN architectures.\nThe state-of-the-art Vision Transformers achieve even higher\nrecognition scores: the best model, ViT-Large/16, achieves\nrecognition scores of 91.15% and 83.54% on the PlantCLEF\nFrontiers in Plant Science /one.tnum/three.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\n2017 and ExpertLifeCLEF 2018 test sets, respectively, before\nadditional post-processing like test-time augmentations and\nprior shift adaptation.\nPrior shift adaptation: The prior shift in the datasets,\ni.e., the diﬀerence between the training and test data class\ndistribution, is a signiﬁcant and omnipresent phenomenon. We\ntest existing prior shift adaptation methods and their impact\non classiﬁcation accuracy. The experiments with state-of-the-\nart methods for prior shift estimation (\nSaerens et al., 2002 ;\nSipka et al., 2022 ), evaluated in Table 6, show that all three\ncompared methods improve the classiﬁcation accuracy in all\ncases. The diﬀerences among all three methods are rather small,\nEM achieving slightly better results in 3 of 4 cases. Given the\noptimization speed, EM algorithm is a preferred choice.\nRetrieval approach to ﬁne-grained classiﬁcation: Training\nan image retrieval system and subsequently performing a nearest\nneighbor classiﬁcation is a competitive alternative, with better\nresults than direct classiﬁcation. The prediction obtained via a\nnearest neighbor search is more interpretable as the samples\ncontributing to the prediction can be visualized. Therefore,\na retrieval-based approach is more suitable if utilized within\nthe humans in the loop. On the other hand, the softmax\npredictions of a standard neural network classiﬁer allow for\nsimple post-processing procedures such as averaging and prior\nshift adaptation, which are yet to be explored for the retrieval\napproach, and which noticeably improve the ﬁnal recognition\naccuracy of the standard classiﬁers.\nOverall, using image-retrieval has clear advantages, e.g.,\nrecovering relevant nearest-neighbor labeled samples, providing\nranked class predictions, and allows user or experts to visually\nverify the species based on the k-nearest neighbors Besides,\nthe retrieval approach naturally supports open-set recognition\nproblems, i.e., the ability to extend or modify the set of\nrecognized classes after the training stage. The set of classes\nmay change e.g., as a results of modiﬁcations to biological\ntaxonomy. New classes are introduced simply by adding training\nimages with the new label, whereas in the standard approach,\nthe classiﬁcation head needs re-training. On the negative side,\nthe retrieval approach requires, on top of running the deep net\nto extract the embedding, to execute the nearest neighbor search\neﬃciently, increasing the overall complexity of the ﬁne-grained\nrecognition system.\nContrary to our expectations, the error analysis in\nFigure 4 shows that the retrieval approach does not bring\nan improvement in classifying images from classes with few\ntraining samples.\nFigure 5 shows that retrieval has a very high\naccuracy for a higher number of species, but it also fails for a\nhigher number of species.\nData availability statement\nThe PlantCLEF datasets used in this study are publicly\navailable in the repository of the LifeCLEF challenge organizers.\nThe test set labels were kindly provided by the challenge Goëau\net al. (2018) organizers. The iNaturalist dataset is publicly\navailable at the competition GitHub page. All images used in the\narticle are with CC-BY licence.\nAuthor contributions\nLP , MŠ, YP , and JM conceived the study and\ndrafted the manuscript. LP , MŠ, and YP implemented\nand conducted the machine learning experiments.\nAll authors critically revised, reviewed, and approved\nthe manuscript.\nFunding\nLP was supported by the UWB project No. SGS-\n2022-017. LP and JM were supported by the Ministry of\nEnvironment of the Czech Republic project No. SS05010008.\nMŠ and JM were supported by Toyota Motor Europe.\nJM and YP were supported by Research Center for\nInformatics (project CZ.02.1.01/0.0/0.0/16 \\_019/0000765\nfunded by OP VVV). YP was supported by the\nGrant Agency of the Czech Technical University in\nPrague, grant No. SGS20/171/OHK3/3T/13, by Project\nStratDL in the realm of COMET K1 center Software\nCompetence Center Hagenberg and an Amazon\nResearch Award.\nAcknowledgments\nComputational resources were supplied by the project\ne-Infrastruktura CZ (e-INFRA CZ LM2018140) supported\nby the Ministry of Education, Youth and Sports of the\nCzech Republic.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those\nof the authors and do not necessarily represent those\nof their aﬃliated organizations, or those of the publisher,\nthe editors and the reviewers. Any product that may be\nevaluated in this article, or claim that may be made\nby its manufacturer, is not guaranteed or endorsed by\nthe publisher.\nFrontiers in Plant Science /one.tnum/four.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nReferences\nBelhumeur, P. N., Chen, D., Feiner, S., Jacobs, D. W., Kress, W . J., Ling, H., et al.\n(2008). “Searching the world’s Herbaria: a system for visual identiﬁcation of plant\nspecies, ” inComputer Vision-ECCV 2008 (Berlin; Heidelberg: Springer), 116–129.\ndoi: 10.1007/978-3-540-88693-8_9\nBonnet, P., Goëau, H., Hang, S. T., Lasseck, M., Šulc, M., Maléco t,\nV., et al. (2018). “ Plant identiﬁcation: experts vs. machines in the era of\ndeep learning , ” in Multimedia Tools and Applications for Environmental &\nBiodiversity Informatics (Cham: Springer International Publishing), 131–149.\ndoi: 10.1007/978-3-319-76445-0_8\nBuslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., D ruzhinin, M., and\nKalinin, A. A. (2020). Albumentations: fast and ﬂexible image au gmentations.\nInformation 11, 125. doi: 10.3390/info11020125\nCaglayan, A., Guclu, O., and Can, A. B. (2013). “A plant recognitio n approach\nusing shape and color features in leaf images, ” in International Conference\non Image Analysis and Processing (Berlin; Heidelberg: Springer), 161–170.\ndoi: 10.1007/978-3-642-41184-7_17\nCui, Y., Song, Y., Sun, C., Howard, A., and Belongie, S. (2018) . “Large scale\nﬁne-grained categorization and domain-speciﬁc transfer lea rning, ”in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(Salt Lake City, UT). doi: 10.1109/CVPR.2018.00432\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X.,\nUnterthiner, T., et al. (2021). “ An image is worth 16x16 words: transformers\nfor image recognition at scale, ” in International Conference on Learning\nRepresentations (Vienna).\nGarcin, C., Joly, A., Bonnet, P., Lombardo, J.-C., Aﬀouard, A ., Chouet,\nM., et al. (2021). “Pl@ ntnet-300k: a plant image dataset with hig h label\nambiguity and a long-tailed distribution, ” in NeurIPS 2021-35th Conference\non Neural Information Processing Systems , ed J. Vanschoren and S. Yeung.\nAvailable online at:\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper /\n2021/ﬁle/7e7757b1e12abcb736ab9a754ﬀb617a-Paper-round2 .pdf\nGaston, K. J., and O’Neill, M. A. (2004). Automated species iden tiﬁcation:\nwhy not? Philos. Trans. R. Soc. Lond. Ser. B Biol. Sci . 359, 655–667.\ndoi: 10.1098/rstb.2003.1442\nGhazi, M. M., Yanikoglu, B., and Aptoula, E. (2017). Plant identi ﬁcation\nusing deep neural networks via optimization of transfer learn ing parameters.\nNeurocomputing 235, 228–235. doi: 10.1016/j.neucom.2017.01.018\nGoëau, H., Bonnet, P., and Joly, A. (2016). “Plant identiﬁcatio n in an open-world\n(lifeclef 2016), ”in CLEF Working Notes 2016 (Évora).\nGoëau, H., Bonnet, P., and Joly, A. (2017). “Plant identiﬁcatio n based on noisy\nweb data: the amazing performance of deep learning (lifeclef 201 7), ” in CEUR\nWorkshop Proceedings (Dublin).\nGoëau, H., Bonnet, P., and Joly, A. (2018). “Overview of expertli feclef 2018: how\nfar automated identiﬁcation systems are from the best expert s?” in CLEF Working\nNotes 2018 (Avignon).\nGoëau, H., Bonnet, P., and Joly, A. (2019). “Overview of lifeclef plant\nidentiﬁcation task 2019: diving into data deﬁcient tropical countries, ” in CLEF\n2019-Conference and Labs of the Evaluation Forum (Lugano: CEUR), 1–13.\nGoëau, H., Bonnet, P., and Joly, A. (2020). “Overview of lifeclef plant\nidentiﬁcation task 2020, ” in CLEF Task Overview 2020, CLEF: Conference and Labs\nof the Evaluation Forum (Thessaloniki).\nGoëau, H., Bonnet, P., and Joly, A. (2021). “Overview of PlantCL EF 2021: cross-\ndomain plant identiﬁcation, ” inWorking Notes of CLEF 2021 - Conference and Labs\nof the Evaluation Forum (Bucharest).\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning Book .\nMIT Press. Available online at: http://www.deeplearningbook.org\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ”in Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (Las Vegas, NV), 770–778. doi: 10.1109/CVPR.2016.90\nHu, J., Shen, L., and Sun, G. (2018). “Squeeze-and-excitation networks, ”in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognitio n\n(Salt Lake City, UT), 7132–7141. doi: 10.1109/CVPR.2018.00 745\nJoly, A., Goëau, H., Botella, C., Glotin, H., Bonnet, P., Planqué, R., et al. (2018).\n“Overview of lifeclef 2018: a large-scale evaluation of species id entiﬁcation and\nrecommendation algorithms in the era of AI, ” in Proceedings of CLEF 2018 (Cham:\nSpringer International Publishing), 247–266. doi: 10.1007/ 978-3-319-98932-7_24\nJoly, A., Goëau, H., Botella, C., Kahl, S., Servajean, M., Glotin, H. , et al.\n(2019). “Overview of lifeclef 2019: identiﬁcation of amazoni an plants, south &\nnorth American birds, and niche prediction, ” in International Conference of the\nCross-Language Evaluation Forum for European Languages (Berlin; Heidelberg:\nSpringer), 387–401. doi: 10.1007/978-3-030-28577-7_29\nJoly, A., Goëau, H., Kahl, S., Deneu, B., Servajean, M., Cole, E., et al. (2020).\n“Overview of lifeclef 2020: a system-oriented evaluation of au tomated species\nidentiﬁcation and species distribution prediction, ” in International Conference of\nthe Cross-Language Evaluation Forum for European Languages (Cham: Springer),\n342–363. doi: 10.1007/978-3-030-58219-7_23\nJoly, A., Goëau, H., Kahl, S., Picek, L., Lorieul, T., Cole, E., et a l. (2021).\n“Overview of lifeclef 2021: an evaluation of machine-learning b ased species\nidentiﬁcation and species distribution prediction, ” in International Conference of\nthe Cross-Language Evaluation Forum for European Languages (Cham: Springer),\n371–393. doi: 10.1007/978-3-030-85251-1_24\nKeaton, M. R., Zaveri, R. J., Kovur, M., Henderson, C., Adjer oh, D. A.,\nand Doretto, G. (2021). Fine-grained visual classiﬁcation of plant species in\nthe wild: object detection as a reinforced means of attention . arXiv preprint\narXiv:2106.02141. doi: 10.48550/ARXIV.2106.02141\nKhosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., et al. (2020).\n“Supervised contrastive learning, ” in Advances in Neural Information Processing\nSystems, Vol. 33 , ed H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin\n(Curran Associates, Inc.), 18661–18673. Available online at: https://proceedings.\nneurips.cc/paper/2020/ﬁle/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf\nKrause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T.,\net al. (2016). “The unreasonable eﬀectiveness of noisy data fo r ﬁne-grained\nrecognition, ” in European Conference on Computer Vision (Cham: Springer),\n301–320. doi: 10.1007/978-3-319-46487-9_19\nLasseck, M. (2017). “ Image-based plant species identiﬁcation with deep\nconvolutional neural networks , ” inCLEF (Dublin).\nLee, S. H., Chan, C. S., and Remagnino, P. (2018). Multi-organ plant\nclassiﬁcation based on convolutional and recurrent neural ne tworks. IEEE Trans.\nImage Process. 27, 4287–4301. doi: 10.1109/TIP.2018.2836321\nLin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P. (2017). “Focal loss\nfor dense object detection, ” in Proceedings of the IEEE International Conference on\nComputer Vision (Venice), 2980–2988. doi: 10.1109/ICCV.2017.324\nLoshchilov, I., and Hutter, F. (2019). “ Decoupled weight decay regularization , ” in\nInternational Conference on Learning Representations (New Orleans, LA).\nMalik, O. A., Faisal, M., and Hussein, B. R. (2021). “Ensemble de ep learning\nmodels for ﬁne-grained plant species identiﬁcation, ” in 2021 IEEE Asia-Paciﬁc\nConference on Computer Science and Data Engineering (CSDE) (IEEE), 1–6.\ndoi: 10.1109/CSDE53843.2021.9718387\nMunisami, T., Ramsurn, M., Kishnah, S., and Pudaruth, S. (20 15). Plant leaf\nrecognition using shape features and colour histogram with k- nearest neighbour\nclassiﬁers. Proc. Comput. Sci . 58, 740–747. doi: 10.1016/j.procs.2015.08.095\nPatel, Y., Tolias, G., and Matas, J. (2021). “Recall@k surrogate lo ss with large\nbatches and similarity mixup, ” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) (New Orleans, LA), 7502–7511.\nPicek, L., Sulc, M., and Matas, J. (2019). “Recognition of the amazonian ﬂora by\ninceptionnetworks with test-time class prior estimation, ” in CLEF (Working Notes)\n(Lugano).\nPicek, L., Šulc, M., Matas, J., Jeppesen, T. S., Heilmann-Clausen , J., Læssøe, T.,\net al. (2022). “Danish fungi 2020 - not just another image reco gnition dataset, ”in\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vi sion\n(WACV) (Waikoloa), 1525–1535. doi: 10.1109/WACV51458.2022.0033 4\nPolyak, B. T., and Juditsky, A. B. (1992). Acceleration of stoc hastic\napproximation by averaging. SIAM J. Control Opt . 30, 838–855.\ndoi: 10.1137/0330046\nPrasad, S., Kudiri, K. M., and Tripathi, R. (2011). “Relative s ub-image based\nfeatures for leaf recognition using support vector machine, ” i n Proceedings of\nthe 2011 International Conference on Communication, Computing & Security\n(Rourkela Odisha), 343–346. doi: 10.1145/1947940.1948012\nPriya, C. A., Balasaravanan, T., and Thanamani, A. S. (2012). “An eﬃcient\nleaf recognition algorithm for plant classiﬁcation using support v ector\nmachine, ” in International Conference on Pattern Recognition, Informatics\nand Medical Engineering (PRIME-2012) (Tamilnadu: IEEE), 428–432.\ndoi: 10.1109/ICPRIME.2012.6208384\nSaerens, M., Latinne, P., and Decaestecker, C. (2002). Adju sting the outputs of\na classiﬁer to new a priori probabilities: a simple procedure. Neural Comput . 14,\n21–41. doi: 10.1162/089976602753284446\nFrontiers in Plant Science /one.tnum/five.tnum frontiersin.org\nPicek et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpls./two.tnum/zero.tnum/two.tnum/two.tnum./seven.tnum/eight.tnum/seven.tnum/five.tnum/two.tnum/seven.tnum\nSipka, T., Sulc, M., and Matas, J. (2022). “The hitchhiker’s gu ide to prior-shift\nadaptation, ” inProceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision (IEEE), 1516–1524. doi: 10.1109/WACV51458.2022.00209\nŠulc, M. (2020). Fine-grained recognition of plants and fungi from images (Ph.D.\nthesis). Czech Technical University in Prague, Prague, Cze chia.\nŠulc, M., and Matas, J. (2017). Fine-grained recognition of pla nts from images.\nPlant Methods 13, 115. doi: 10.1186/s13007-017-0265-4\nŠulc, M., and Matas, J. (2019). “Improving cnn classiﬁers by est imating test-\ntime priors, ” inProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV) Workshops (Seoul). doi: 10.1109/ICCVW.2019.00402\nŠulc, M., Picek, L., and Matas, J. (2018). “ Plant recognition by inception networks\nwith test-time class prior estimation , ” inCLEF (Working Notes) (Avignon).\nSzegedy, C., Ioﬀe, S., Vanhoucke, V., and Alemi, A. A. (2017). “Inception-v4,\ninception-resnet and the impact of residual connections on lea rning, ” inThirty-ﬁrst\nAAAI Conference on Artiﬁcial Intelligence (AAAI).\nTan, M., and Le, Q. V. (2021). “Eﬃcientnetv2: smaller models and faster\ntraining, ” inProceedings of the 38th International Conference on Machine Learning ,\ned M, Marina and Z, Tong (PMLR), 10096–10106. Available online a t: http://\nproceedings.mlr.press/v139/tan21a/tan21a.pdf\nTouvron, H., Sablayrolles, A., Douze, M., Cord, M., and Jégou, H . (2021). “Graﬁt:\nlearning ﬁne-grained image representations with coarse labels , ”in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (Montreal), 874–884.\ndoi: 10.1109/ICCV48922.2021.00091\nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A.,\net al. (2018). “The inaturalist species classiﬁcation and detec tion dataset, ” in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognitio n\n(Salt Lake City, UT), 8769–8778. doi: 10.1109/CVPR.2018.00 914\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\net al. (2017). “Attention is all you need, ” in Advances in Neural Information\nProcessing Systems , Vol. 30, eds I. Guyon, U. Von Luxburg, S. Bengio, H.\nWallach, R. Fergus, S. Vishwanathan, and R. Garnett (Curran As sociates, Inc.),\n5998–6008. Available online at: https://proceedings.neurips.cc/paper/2017/ﬁle/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S . (2011).\nThe Caltech-UCSD Birds-200-2011 Dataset . Technical Report CNS-TR-2011-001,\nCalifornia Institute of Technology.\nWäldchen, J., and Mäder, P. (2018). Machine learning for image based species\nidentiﬁcation. Methods Ecol. Evol . 9, 2216–2225. doi: 10.1111/2041-210X.13075\nWightman, R. (2019). PyTorch Image Models . Available online at: https://github.\ncom/rwightman/pytorch-image-models\nWu, D., Han, X., Wang, G., Sun, Y., Zhang, H., and Fu, H. (2019). Deep\nlearning with taxonomic loss for plant identiﬁcation. Comput. Intell. Neurosci .\n2019, 2015017. doi: 10.1155/2019/2015017\nWu, Q., Zhou, C., and Wang, C. (2006). Feature extraction and automatic\nrecognition of plant leaf using artiﬁcial neural network. Adv. Artif. Intell . 3, 5–12.\nWu, S. G., Bao, F. S., Xu, E. Y., Wang, Y.-X., Chang, Y.-F., and X iang, Q.-L.\n(2007). “A leaf recognition algorithm for plant classiﬁcation us ing probabilistic\nneural network, ” in 2007 IEEE International Symposium on Signal Processing and\nInformation Technology (IEEE), 11–16. doi: 10.1109/ISSPIT.2007.4458016\nXie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. (2017). “Agg regated\nresidual transformations for deep neural networks, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (Honolulu), 1492–1500.\ndoi: 10.1109/CVPR.2017.634\nZhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang, Z., et al. (2020). “ResNest:\nsplit-attention networks, ” Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops (New Orleans, LA), 2736–2746.\nZheng, H., Fu, J., Zha, Z.-J., and Luo, J. (2019). “Looking fo r the devil in\nthe details: learning trilinear attention sampling network for ﬁ ne-grained image\nrecognition, ”in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (Long Beach, CA), 5012–5021. doi: 10.1109/CVPR.2019.0051 5\nFrontiers in Plant Science /one.tnum/six.tnum frontiersin.org"
}