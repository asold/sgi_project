{
    "title": "Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study",
    "url": "https://openalex.org/W4396817330",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2099387957",
            "name": "Yang Wu",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2171492316",
            "name": "Yao Wan",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2103519435",
            "name": "Hong-Yu Zhang",
            "affiliations": [
                "Chongqing University"
            ]
        },
        {
            "id": "https://openalex.org/A2170939854",
            "name": "Yulei Sui",
            "affiliations": [
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5060433773",
            "name": "Wucai Wei",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2008438337",
            "name": "Wei Zhao",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2482500164",
            "name": "Guandong Xu",
            "affiliations": [
                "University of Technology Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2095833031",
            "name": "Hai Jin",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3116342879",
        "https://openalex.org/W4386076140",
        "https://openalex.org/W2752843814",
        "https://openalex.org/W3046744391",
        "https://openalex.org/W4367860052",
        "https://openalex.org/W2798990443",
        "https://openalex.org/W3174906424",
        "https://openalex.org/W3213578841",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4366835679",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2918035772",
        "https://openalex.org/W4290874921",
        "https://openalex.org/W2970515629",
        "https://openalex.org/W4312999884",
        "https://openalex.org/W4389520065",
        "https://openalex.org/W4385571184",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4304479239",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4386566590",
        "https://openalex.org/W4238846128",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2750779823",
        "https://openalex.org/W2911733715"
    ],
    "abstract": "The Natural Language to Visualization (NL2Vis) task aims to transform natural-language descriptions into visual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently, many deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts made by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning multiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models (LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and explore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore the ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and analyze which table content contributes most to the NL2Vis. Our findings suggest that transforming structured tabular data into programs is effective, and it is essential to consider the table schema when formulating prompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The experimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting performance improvements, at times even surpassing fine-tuned models when provided with certain few-shot demonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose to iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter. The experimental results confirm the efficacy of iterative updates and hold great potential for future study.",
    "full_text": "Automated Data Visualization from Natural Language via\nLarge Language Models: An Exploratory Study\nYANG WUâˆ—â€  , Huazhong University of Science and Technology, China\nYAO WANâˆ—â€ â€¡ , Huazhong University of Science and Technology, China\nHONGYU ZHANG, Chongqing University, China\nYULEI SUI, University of New South Wales, Australia\nWUCAI WEIâˆ—, Huazhong University of Science and Technology, China\nWEI ZHAOâˆ—, Huazhong University of Science and Technology, China\nGUANDONG XU, University of Technology Sydney, Australia\nHAI JINâˆ—, Huazhong University of Science and Technology, China\nThe Natural Language to Visualization ( NL2Vis) task aims to transform natural-language descriptions into\nvisual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently,\nmany deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts\nmade by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning\nmultiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models\n(LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and\nexplore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore\nthe ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and\nanalyze which table content contributes most to theNL2Vis. Our findings suggest that transforming structured\ntabular data into programs is effective, and it is essential to consider the table schema when formulating\nprompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only\nmodels (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The\nexperimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting\nperformance improvements, at times even surpassing fine-tuned models when provided with certain few-shot\ndemonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose\nto iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter.\nThe experimental results confirm the efficacy of iterative updates and hold great potential for future study.\nCCS Concepts: â€¢ Human-centered computing â†’Empirical studies in visualization .\nâˆ—Also with National Engineering Research Center for Big Data Technology and System, Services Computing Technology\nand System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of\nScience and Technology, Wuhan, 430074, China.\nâ€ Both authors contributed equally to this research.\nâ€¡Yao Wan is the corresponding author.\nAuthorsâ€™ addresses: Yang Wu, Huazhong University of Science and Technology, China, wuyang_emily@hust.edu.cn;\nYao Wan, Huazhong University of Science and Technology, China, wanyao@hust.edu.cn; Hongyu Zhang, Chongqing\nUniversity, China, hyzhang@cqu.edu.cn; Yulei Sui, University of New South Wales, Australia, y.sui@unsw.edu.au; Wucai\nWei, Huazhong University of Science and Technology, China, m202273789@hust.edu.cn; Wei Zhao, Huazhong University of\nScience and Technology, China, m202073277@hust.edu.cn; Guandong Xu, University of Technology Sydney, Australia,\nguandong.xu@uts.edu.au; Hai Jin, Huazhong University of Science and Technology, China, hjin@hust.edu.cn.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 2836-6573/2024/6-ART115\nhttps://doi.org/10.1145/3654992\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\narXiv:2404.17136v1  [cs.DB]  26 Apr 2024\n115:2 Yang Wu et al.\nAdditional Key Words and Phrases: Data Visualization, Data Analysis, Natural Language Processing, Code\nGeneration, Large Language Models, Exploratory Study\nACM Reference Format:\nYang Wu, Yao Wan, Hongyu Zhang, Yulei Sui, Wucai Wei, Wei Zhao, Guandong Xu, and Hai Jin. 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study.\nProc. ACM Manag. Data 2, 3 (SIGMOD), Article 115 (June 2024), 28 pages. https://doi.org/10.1145/3654992\n1 INTRODUCTION\nData visualizations, typically presented as charts, plots, and histograms, offer an effective means\nto represent, analyze, and explore data, as well as enable the identification and communication\nof valuable insights. Despite the availability of numerous tools (e.g., Tableauâ€™s Ask Data [41] and\nAmazonâ€™s QuickSight [1]) and domain-specific programming languages (e.g., Vega-Lite [58] and\nggplot2 [47]) for data visualization, crafting effective data visualizations remains a complicated\neffort for a range of users, particularly those with limited or no prior visualization experience.\nMoreover, there is a pressing demand for visualizing data on smart devices such as tablets and\nmobile phones without requiring users to acquire data visualization expertise.\nTo facilitate users in conducting data analytics, there has been an increasing interest in the auto-\nmated generation of data visualizations from natural-language descriptions, denoted as NL2Vis [26,\n27]. Existing approaches to NL2Vis mainly fall into two categories: the rule-based [8, 13, 40] and\ndeep-learning-based [25â€“28]. DataTone [8], Eviza [40], and Evizeon [13] employed a parser (i.e., the\nStanford Core NLP Parser [32]), along with a set of predefined rules, to translate natural-language\ndescriptions into visualization queries. DeepEye [25] introduced a novel approach that enables\nthe generation of visualizations based on keyword queries, akin to search engine functionality.\nCurrent methods for deep-learning-based techniques rely mostly on the encoder-decoder paradigm,\nwhich, in an end-to-end fashion, encodes the natural-language specification into hidden states and\nsubsequently generates visualization queries.\nNL2Vis is similar to the the task of NL2SQL (also referred to as Text2SQL) [15], wherein the\nobjective is to translate natural-language descriptions intoStructured Query Language (SQL) queries.\nGenerally, the visualization is articulated using theVisualization Query Language (VQL) , a language\nthat shares similarities with SQL. Both NL2Vis and NL2SQL are based on input tables with diverse\nstructures and aim to generate queries of various complexities, including selection, comparison,\naggregation, and join operations. In comparison toNL2SQL, one distinction is thatNL2Vis faces the\nadditional challenge of considering intricate visualization attributes during generation, including\nthe selection of chart types (e.g., bar, pie, line, and scatter). Drawing inspiration from an established\nNL2SQL dataset (e.g., Spider [53]), Luo et al . [27] proposed the creation of a paired dataset for\nNL2Vis. Based on this dataset, a benchmark called nvBench [26] is built and a Transformer-based\nmodel (named ncNet [28]) is introduced.\nLLMs for NL2Vis. Recently, Large Language Models (LLMs) , such as GPT-3.5 [10] and LLaMA [45],\nhave demonstrated impressive capabilities for few-shot learning in manyNatural Language Process-\ning (NLP) tasks, including question answering [12, 44], machine translation [30, 36, 49], and code\ngeneration [23, 57]. As LLMs sequentially process the entire large-scale training dataset during the\npre-training phase, they face a limitation in directly handling structured data, including tabular\ndata. Alternatively, we can serialize the tabular data, input it into the LLMs, and prompt the LLMs\nto generate data visualizations, which are in the form of domain-specific query language. To fill\nthis gap, this paper aims to address the following research question: â€œ Can LLMs be utilized for\nautomating data visualization from natural-language descriptions grounded on a table and how? â€.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:3\nVQL Query\nChatGPT\nAPI\nPrompt\nRQ2: How do the LLMs perform when compared with \nexisting models in NL2Vis?\nRQ1: How to feed the data  into \nLLMs via prompts?\nRQ3: Can the results be optimized iteratively via some optimization strategies?\nShow me â€¦\nFig. 1. An illustration of NL2Vis. The framework presents our investigation into prompt engineering (RQ1),\noverall performance (RQ2), and iterative updating (RQ3).\nThe challenges of leveraging the LLMs to automate data visualization from natural language are\ntwofold. C1: Feeding the structural table into LLMs. Given that LLMs exclusively accommodate\nsequential prompts, converting a structured grounded table into sequential prompts while preserv-\ning semantics poses a notable challenge. Moreover, LLMs are recognized to face limitations due\nto their restricted token length. Consequently, they are unable to process entire extensive tables,\nmaking it challenging to comprehend comprehensive tabular information on a global scale. C2:\nIteratively updating via conversation. In contrast to traditional neural models for NL2Vis that\ngenerate the data visualizations in a single attempt, one notable advantage of LLMs lies in their\ncapacity to iteratively refine predicted outputs during conversations. Adapting traditional neural\nmodels to the new conversational paradigm also poses a significant challenge.\nOur Work. To answer the aforementioned question, we conduct a pioneering empirical study to\nevaluate the capabilities of LLMs (i.e., T5 [42] and GPT-3.5 [10]) in automating data visualization\nfrom natural-language descriptions, comparing them with traditional approaches. Specifically, we\nstructure the empirical study around the following three Research Questions (RQs) , as depicted in\nFigure 1.\nRQ1: How to feed the natural-language query as well as the structural table into LLMs via\nprompting? In this RQ, we first (1) explore the ways (i.e., table serialization, table summarization,\ntable markup formatting, and table programming) to convert structured tabular data into sequential\nprompts, and subsequently (2) investigate which table content contributes most to the NL2Vis in\nprompting.\nRQ2: How do the LLMs perform when compared with several existing models in NL2Vis?\nIn this RQ, we first (1) evaluate the performance of LLMs (i.e., finetuned models such as T5-Small\nand T5-Base, and inference-only models such as text-davinci-002 and text-davinci-003) for\nNL2Vis, against several traditional neural networks (i.e.,Seq2Vis [27], Transformer [46], ncNet [28],\nRGVisNet [43]), both under the in-domain and cross-domain settings, and (2) analyze how the\nnumber of in-context demonstrations affects the performance of LLMs.\nRQ3: Can the results be iteratively updated via some optimization strategies? In some\ncases, we observe that LLMs may fail to generate the correct visualization in a single attempt. In\nthis RQ, we first (1) analyze when the LLMs fail in generating data visualization, and (2) propose\nto iteratively update the results via optimization strategies such as Chain-of-Thought (CoT), role-\nplaying, self-repair, and code-interpreter.\nKey Findings and Implications. In this paper, we observe the following important findings: (1)\nTo feed the structural table into LLMs, converting it into programs is an effective way. This finding\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:4 Yang Wu et al.\ninspires us to design programming patterns to encode tables. Additionally, the schema information is\nsufficient for NL2Vis task, which facilitates exploration of how to encode extra large databases with\nlimited input length to LLMs. (2) LLMs demonstrate significantly superior performance compared\nto traditional neural models for NL2Vis, highlighting their great potential under both in-domain\nand cross-domain settings. With in-context learning of LLMs, the demonstrations drawn from\ndiverse tables can further increase performance. (3) The failure results can be further optimized via\nseveral iterative optimization strategies, such as CoT, role-playing, self-repair, and code-interpreter.\nTo advance the visualization capability of LLMs, crafting multi-turn dialog prompts for automated\noptimization offers a promising prospect.\nContributions. The key contributions of this paper are as follows.\nâ€¢To the best of our knowledge, this paper reports the first empirical study to investigate the capa-\nbility of LLMs in automating data visualization from natural-language descriptions. Additionally,\na benchmark of LLMs for NL2Vis is built for further study.\nâ€¢This paper systematically studies how to feed the data to visualize into the LLMs via prompts,\nand explores several optimization strategies for iteratively improving the failure results.\nâ€¢To facilitate further study for other researchers, we release all the experimental data and source\ncode used in this paper at https://github.com/CGCL-codes/naturalcc/tree/main/examples/explore-\nLLMs-for-NL2Vis [48].\nOrganization. The rest of this paper is structured as follows. We first introduce some background\nknowledge that will be used in this paper in Sec. 2. We then introduce our pipeline for NL2Vis task\nin Sec. 3 and the evaluation setup in Sec. 4. Sec. 5 reports the experiment results with comprehensive\nanalysis. Sec. 6 is dedicated to discussing the potential threats to validity and the broader impacts\nof this paper. We review the related work to this paper in Sec. 7, and conclude this paper in Sec. 8.\n2 BACKGROUND\nIn this section, we present foundational concepts of visualization query languages and LLMs,\nessential for understanding our work.\n2.1 Visualization Query Language (VQL)\nIn the realm of data visualization, one widely used grammar is Vega-Lite [58], which offers a concise\nand declarative JSON syntax for creating a diverse range of expressive visualizations suitable for\ndata analysis and presentation. While Vega-Lite is intuitive and straightforward to use, training a\nsequence-to-sequence model to automatically generate hierarchical outputs, such as JSON format\nfor Vega-Lite specifications, is challenging. Conversely, training a sequence-to-sequence model to\ngenerate sequential outputs is relatively more manageable. In response to this challenge, several\nworks [25, 27] introduce the VQL, which empowers users to articulate their data visualization\nrequirements in a structured and efficient manner. In contrast to Vega-lite, VQL queries remove\nstructure-aware symbols such as parentheses, commas, and quotes, effectively transforming a JSON\nobject into a sequence of keywords. This streamlining greatly simplifies the process of generating\nVQL queries. Moreover, having eliminated all language-specific configurations, VQL is deemed\nlanguage-agnostic that encompasses necessary data components (e.g., data operations) and vital\nvisualization formats (e.g., visualization types). The VQL query could be easily transformed to\ndiverse specifications (e.g., Vega-Lite, and ggplot2).\nTable 1 shows the details of VQL [25] for specifying visualization queries. Specifically, â€œVISUALIZEâ€\nspecifies the visualization type, including bar, line, scatter, and pie. â€œSELECTâ€ specifies the chosen\ncolumns, where ğ‘‹â€²represents either the original ğ‘‹ or its corresponding binned values, and ğ‘Œâ€²\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:5\nTable 1. The visualization query language [25].\nVISUALIZE ğ‘‡ğ‘Œğ‘ƒğ¸ (ğœ–{ğ‘ğ‘ğ‘Ÿ,ğ‘ğ‘–ğ‘’,ğ‘™ğ‘–ğ‘›ğ‘’,ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ })\nSELECT ğ‘‹\nâ€²\n,ğ‘Œ\nâ€²\n(ğ‘‹\nâ€²\nğœ–{ğ‘‹,BIN(ğ‘‹)},ğ‘Œ\nâ€²\nğœ–{ğ‘Œ,AGG(ğ‘Œ)})\nFROM ğ·1\nJOIN ğ·1 âŠ² âŠ³ğ·2\nWHERE ğ‘‹â€²ğ‘‚ğ‘ƒ ğ‘£\nTRANSFORM ğ‘‹ â†’ğ‘“(ğ‘‹)where ğ‘“ âˆˆ{BIN,GROUP}\nORDER BY ğ‘‹â€²,ğ‘Œâ€²\nAND/OR ğœ™1 âˆ§ğœ™2 , ğœ™1 âˆ¨ğœ™2\nNested ğ‘„(subquery)\ndenotes either the original variable ğ‘Œ or its aggregated value, derived by operations of SUM, AVG,\nand COUNT. â€œFROMâ€ specifies the originating table. â€œJOINâ€ operation links tabular data from more\nthan one table. â€œWHEREâ€ filters the values that meet a certain condition, for instance, those greater\nthan 10. â€œTRANSFORMâ€ modifies the chosen columns, typically by binning ğ‘‹ into designated buckets\nor applying a GROUP operation. â€œBINâ€ operation divides the temporal data for visualization into\nseveral intervals, e.g., binning by year. â€œGROUPâ€ operation transforms the data into specific groups\nbased on detailed stacked type or classification color. â€œORDER BYâ€ arranges the selected column\nin a particular sequence. â€œAND/ORâ€ operation filters data based on multiple conditions. â€œNestedâ€\noperations nesting subquery, implements more complex data queries for visual representation.\nExample 1.Considering a natural-language query: List the name of technicians whose team is not\nâ€œNYYâ€, and count them by a bar chart, rank x-axis in ascending order , the corresponding VQL query\nis as follows.\nVISUALIZE bar SELECT name , COUNT(name) FROM technician WHERE team != â€œNYYâ€ GROUP BY\nname ORDER BY name asc\nWe can see that this example is to select a name with its count from the technician table, where\nteam ! = â€œNYYâ€. Then, it groups the results by name with an ascend order, and finally visualizes\nthe results using a bar chart.\n2.2 Large Language Models\nOver the past year, we have observed a growing proliferation of LLMs, including GPT-3 [ 4]\nand LLaMA [45]. These LLMs have spearheaded a revolution in the field of NLP, especially in\nthe related tasks of text generation [ 30, 49] and code generation [ 23, 36, 57]. As pre-training\nLLMs on a large-scale dataset is a time-consuming and computationally expensive process (e.g.,\nChatGPT requires approximately 4,000 GPU hours for pre-training, at an estimated cost of 2 million\ndollars [16]), numerous prompting techniques (e.g., in-context learning and chain-of-thought) have\nbeen developed with the goal of maximizing the effective utilization of LLMs.\n2.2.1 Prompting. With the emergence of LLMs, the learning paradigm is undergoing a transfor-\nmation from the conventional â€œpre-train and fine-tune â€ paradigm to a more innovative â€œpre-train,\nprompt, and predict â€ framework [24]. In this new paradigm, instead of extensively fine-tuning\nLLMs to accommodate various downstream tasks, there is a shift towards reformulating these tasks\nto align more closely with the tasks for which LLMs are initially trained, with textual prompts\nguiding the process. For instance, when assessing the emotion in a customer review like â€œ I like\nthe book I have read today. â€, we may proceed with a prompt like â€œIt was .â€ and request that LLM\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:6 Yang Wu et al.\nPrompt for NL2Vis task\nK Demonstration\nExamples \nTest Item\nCoT\nTask Instruction\nPlease generate VQL based on description of tabular data and question.\nLet's think step by step. Generate the sketch as intermediate \nrepresentation and then the final VQL.\nTable Description\ncustomers, customer_id, payment_method_code, customer_code, \ncustomer_name, customer_address, customer_phoneâ€¦\nQuestion \nFor each payment method, return how many customers use it. Plot them \nas pie chart.\nSketch\nvisualize _ select _, _ from _ group by _\nVQL\nvisualize pie select payment_method_code , count(*) from customers \ngroup by payment_method_code\nTable Description\ntv_channel, id, series_name, country, language, content, pixel_aspect, \nhight_definition_tv, pay_per_view_ppv, package_option,â€¦\nQuestion\nFor each language, list the number of tv channels that use it. Show a pie \nchart.\nVQL Sketch [To be generated]\nDesired VQL [To be generated]\nFig. 2. An example to illustrate the usage of prompt in in-context learning of LLMs, the ğ‘˜ demonstration\nexamples, the test item, and the CoT prompt.\ncomplete the sentence with an emotion-laden word. In our specific scenario, involving a table and\na natural-language query, we can construct a prompt as follows: â€œ [TABLE], [QUESTION], please\ngenerate VQL based on the description of tabular data and question. â€, where [TABLE] signifies the\nstructured tabular data, and [QUESTION] represents the natural-language query provided by the\nend-user for exploring the data.\n2.2.2 In-Context Learning (ICL). ICL is a special form of prompt-based learning that leverages\ndemonstration examples in prompts to promote the modelâ€™s performance. In ICL, in addition to\ndescribing the current question in the prompt, a few demonstration examples that have the same\nform as the question are also included in the prompt. For instance, to generate a VQL query to\ncount the number of television channels based on the language of each individual channel, a\ndemonstrative example of counting customers by their preferred payment method is provided. The\nmodel is expected to learn the pattern hidden in the demonstration, infer downstream tasks from\nexamples, and accordingly make the right prediction.\nSpecifically, given a task instruction ğ¼ and a test question ğ‘¥ğ‘¡, ICL retrieves ğ‘˜ examples related\nto ğ‘¥ğ‘¡ from the task dataset as demonstration examples, and transforms these examples using the\nprompt function ğ‘“ to form a demonstration example set ğ·ğ‘˜ = {ğ‘“(ğ‘¥1,ğ‘¦1),...,ğ‘“ (ğ‘¥ğ‘˜,ğ‘¦ğ‘˜)}. The task\ndescription ğ¼, the example setğ·ğ‘˜, and the problem is then fed into the language model for predicting\nË†ğ‘¦ğ‘¡. The ICL process can be expressed as follows.\nË†ğ‘¦ğ‘¡ = LLM(ğ¼,ğ‘“ (ğ‘¥1,ğ‘¦1),...,ğ‘“ (ğ‘¥ğ‘˜,ğ‘¦ğ‘˜)\n|                       {z                       }\nğ‘˜demonstration examples\n,ğ‘“ (ğ‘¥ğ‘¡,â€¢)\n|  {z  }\ntest query\n),\n(1)\nwhere ğ‘˜ stands for the number of demonstration examples in the ICL prompt, which typically\nranges from 0 to 20. In particular, for the special case of ğ‘˜ = 0, the ICL constructs the prompt\nwithout demonstration example, referred to as zero-shot learning.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:7\n{\n\"mark\": \"line\",\n\"encoding\": {\n\"x\":{\"field\":\"da\nte\",\"type\":\"temporal\n\",\"timeUnit\":â€œyearâ€},\n\"y\":{\"fieldâ€œ:\"sh\nare_count\",\"type\":\"q\nuantitative\",\"aggreg\nate\": \"mean\"}\n},\"data\":â€¦\n}\nBin all date of transactions into the \nYEAR interval, and calculate the \naverage the share count for each bin. \nVisualize the result using a trend line.\ndate share_count Â·Â·Â·\n1988-09-16 8718 Â·Â·Â·\n1982-06-06 9 Â·Â·Â·\n1979-04-27 8580 Â·Â·Â·\nÂ·Â·Â· Â·Â·Â· Â·Â·Â·\n(a) Feeding Data into LLMs via Prompting (c) Language-Aware Rending\nVISUALIZE line \nSELECT date, AVG(share_count) \nFROM transactions\nBIN date BY year\nÂ·Â·Â· Â·Â·Â·\nModels\n(b) Visualization Query Generation\nFig. 3. The pipeline of NL2Vis. The task process flows from (a) feeding data into the LLMs through prompts,\nto (b) generating visualization queries, and finally to (c) rendering language-aware visual charts.\n2.2.3 Chain-of-Thought (CoT) Prompting. CoT [52] is an improved prompting strategy that, when\nemployed in conjunction with ICL, significantly enhances the capabilities of LLMs in tackling\ncomplex reasoning tasks. In addition to simply constructing the prompts with a few demonstration\nexamples as in ICL, CoT enriches these prompts by integrating intermediate reasoning steps, which\nguide the reasoning process to the final output. Specifically, the CoT prompting strategy augments\neach demonstration example âŸ¨ğ‘¥,ğ‘¦âŸ©in ICL with a chain-of-thought prompt ğ¶ğ‘œğ‘‡, constructing a\ntriplet prompt âŸ¨ğ‘¥,ğ¶ğ‘œğ‘‡,ğ‘¦ âŸ©. The design of the chain-of-thought prompt ğ¶ğ‘œğ‘‡ involves both hand-\ncrafted prompts that are independent from the problem, and the VQL sketch of each problem,\nwhich is inspired by the logical execution process of SQL queries to establish step-by-step infilling\nthe VQL statements with the LLM.\nExample 2.Figure 2 presents an example of in-context learning with chain-of-thought strategy in\nNL2Vis. LLMs like ChatGPT, take the input text and infer the answer based on the task description,\ndemonstration, and the problem. In the few-shot scenario, we add the most relative samples from\nthe training dataset as examples in the demonstration. The demonstration part of the prompt for\nthe NL2Vis task consists of a table description, NL question, and golden VQL. In particular, we\nselect the most relevant three rows of the table by calculating the Jaccard similarity correlation.\n3 LLMS FOR NL2VIS\nIn this section, we first formulate the problem ofNL2Vis with a pipeline provided, and subsequently\ndetail each individual module.\n3.1 Problem Statement\nLet ğ‘denote a natural-language query, ğ‘  denote the schema of the table from a database to analyze.\nThe task of NL2Vis is to generate the visualization query Ë†ğ‘¦, as follows:\nË†ğ‘¦ = LLM(ğ‘,ğ‘ ). (2)\nIt should be noted that the grounded tables are not restricted to one domain. We refer to scenarios\nwhere the databases in the test dataset have appeared in the training set as in-domain, while\ndatabases in the test dataset are unseen as cross-domain.\nFigure 3 illustrates the pipeline of NL2Vis. The input to the models comprises a natural-language\ndescription and the grounded table. The model will then generate the visualization query in Vega-\nZero. Subsequently, visualization specifications in various visual languages (e.g., Vega-Lite) can be\nparsed from the visualization query. Subsequently, the visualization specification will be rendered\ninto an actual visualization chart, enabling users to observe and analyze the data effectively.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:8 Yang Wu et al.\ntable_name, \nColumns = [column_name,â€¦] \nForeign_keys = [â€˜table_name. column_name= \ntable_name. column_nameâ€™,â€¦]\nValues=[column_name(value),â€¦]\nColumn=[]\n[    \n{        \nTableName: \ntable_name, \nColumnName:              \n[column_name, ...],        \nData:             \n[ [value,...], ...]    \n}, \n...\n]\nTable2JSON\n<database>    \n<table_name>        \n<row>                 \n<column_name>                     \nvalue            \n</column_name>      \n</row>    \n</table_name> \n</database>\nTable2XML\n# table_name\n| column_name |...\n| :---: |...\n| value |\ntable_name.column_name, ...\nvalue,...\nTable2CSV\nTable2MD\n###Postgres SQL tables, with their properties:\n#\n#table_name(column_name,...)\nTable (Column)\ntable_name,column_name,value\nSchema serialization\nUse table table_name with columns column_name,â€¦. \nThe column column_name is type int64 and contains \nnumeric values. The column column_name has \ncategorical values:â€¦ \nChat2Vis*\nclass Constraint:\npass\nclass Column:\ndef __init__(self, name, attribute_pk: Constraint = None, \nattribute_fk: Constraint  = None):\nself.name = name\nself.PK_attribute = attribute_pk\nself.FK_attribute = attribute_fk\nclass Record:\ndef __init__(self, content: List):\nself.content = content\nclass Table:\ndef __init__(self, name, column_list: List[Column], data_list: \nList[Record]):\nself.name = name\nself.column_list = column_list\nself.data_list = data_list\nclass Primary_Key(Constraint):\npass\nclass Foreign_Key(Constraint):\ndef __init__(self, referenced_table: Table, \nreferenced_column: \"the name of the column with primary \nkey constraint of the referenced_table\"):        \nsuper(Foreign_Key, self).__init__()\nself.referenced_table = referenced_table\nself.referenced_column = referenced_column\npeople_column_list = [Column(\"people_id\", Primary_Key()), \nColumn(\"name\"), Column(\"country\")]\npeople_data_list = [Record([1, \"mike weir\", \"canada\",])]\npeople = Table(\"people\", people_column_list, \npeople_data_list)\n# this Foreign key mean the field refer to the primary key \n\"people_id\" of table people\nFK_people_id = Foreign_Key(people, \"people_id\")\n# both Column \"male_id\" and \"female_id\" refer to \n\"people_id\".\nwedding_column_list = [Column(\"church_id\"), \nColumn(\"male_id\", FK_people_id), Column(\"year\")]\nwedding_data_list = [Record([1, 1, 2014 ]),]\nwedding = Table(\"wedding\", wedding_column_list, \nwedding_data_list)\nTable2Code\nNL generated by ChatGPT\nTable2NL\nCREATE TABLE table_name (    \ncolumn_name data_type, column_name data_type,   \nPRIMARY KEY (column_name),      \nFOREIGN KEY (column_name) \nREFERENCES table_name(column_name))\nTable2SQL\nD\nC\nB\nA\nFig. 4. A summary of approaches explored to transforming the table into textual prompts. (A) Table Serializa-\ntion flattens database tables into linear schemas, (B) Table Summarization distills table content into concise\ndescriptions, (C) Table Markup Formatting converts data into XML, Markdown, and CSV formats, and (D)\nTable Programming translates table structures into code representations.\n3.2 Feeding Data into LLMs via Prompting\nGiven that most contemporary LLMs are primarily designed to process textual prompts due to their\npre-training on sequential textual datasets, it becomes imperative to efficiently integrate structured\ntabular data into LLMs through effective prompting. In this study, we investigate various strategies,\ni.e., table serialization, table summarization, table markup format, and table programming, to\ntransform structured tabular data into sequential texts while preserving semantics to the greatest\nextent, as summarized in Figure 4.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:9\nA. Table Serialization ( A in Figure 4). Generally, previous research has proposed to feed the\nserialized schema into models [9]. For example, Table (Column) [22] lists each table along with\nits columns inside parentheses to represent the table schemas. Column=[] represents each table\nalong with a list of its columns. On top of Column=[], +FK further adds foreign keys to indicate\nthe relationships between tables [37], and +Value adds rows of tables.\nB. Table Summarization ( B in Figure 4). It is intuitive to simply describe the tables by generating\na natural-language summary. Based on this intuition, Chat2Vis [31] uses a description built from\na template to transform the table into a text prompt. The description is comprised of individual\nentries, each signifying the data type of a corresponding column. In this paper, we generate the\ntable summaries by invoking the API of ChatGPT. Specifically, we accomplish this by flatting the\ntable column names and values into a prompt, delineated as â€œ[TABLE], Describe the tabular data in\ntext, including all metadata such as its name and type. â€.\nC. Table Markup Formating ( C in Figure 4). In this strategy, we explore describing the tabular\ndata using markup languages, including CSV, JSON, Markdown, and XML.Table2CSV converts\ntabular data into CSV format, where each line represents a data record containing one or more\ncomma-separated fields. Table2JSON aims to convert tabular data into a JSON object, where data\nis represented in <name, value> pairs, enclosed by curly braces â€œ{}â€ for objects and square brackets\nâ€œ[]â€ for arrays. Table2MD aims to convert the table into a Markdown file, which uses simple and\nintuitive syntax to denote structure and style. Table2XML aims to convert the table into an XML\nfile, which is an eXtensible Markup Language used to represent structured data in a human-readable\nand machine-readable way.\nD. Table Programming ( D in Figure 4). In order to feed the structured tabular data into LLMs,\nwe propose to represent the structured tabular data in programming languages. Table2SQL uses\nCreate statements to describe database schema [38], which emphasizes the relationship among\ntables. For showcasing the content of a database,+Select [38] employs the â€œSELECT * FROM Table\nLIMIT Râ€ query to display the first ğ‘…rows of each table. Table2Code proposes to transform the\ntabular data into general-purpose programming languages. In particular, we resort to the Python\nprogramming language, which effectively employs an inherent object-oriented paradigm and is\nwell-suited for encoding structured data [50]. Moreover, we utilize Pythonâ€™s type hinting feature\nand represent tabular data using a class-based representation in Python. We define classes ofTable,\nColumn, Constraint, Primary_Key, Foreign_Key, and Record, followed by the instantiation of\nthese classes to create a table object.\n3.3 Visualization Query Generation\nWe utilize APIs provided by OpenAI in the GPT-3.5 and GPT-4 series to engage with LLMs. Specif-\nically, we initially represent the tabular data with the format as outlined in Figure 4. Then, we\nconstruct the tables and queries into prompts as delineated in Figure 2. Finally, these prepared\nprompts are subsequently fed into LLMs via API calls. Our research methodology can be applied to\nany LLMs that support prompting.\n3.4 Language-Aware Rending\nAccording to nvBench [26], the visualization query can be presented as the tree format as introduced\nin [27] for fair evaluation of grammar. These Abstract Syntax Trees (ASTs) can then be translated to\ntarget visualization specification syntax, so as to render the visualization charts. The translation from\na visualization query to a target visualization specification is hard-coded based on the grammar of\nASTs. Currently, the nvBench provides a module of Python 3 code [27] for converting visualization\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:10 Yang Wu et al.\nquery to Vega-Lite. Thus, the pipeline is fully automated and enables an effortless evaluation of the\nvisualization query.\n4 EVALUATION SETUP\nWe investigate the capability of LLMs in automating data visualization from natural-language\ndescriptions, by answering the following three Research Questions (RQs) .\nâ€¢RQ1 [Prompt Engineering]: How to feed the natural-language query as well as the structural\ntable into LLMs via prompting?\nâ€¢RQ2 [Overall Performance]: How do the LLMs perform when compared with several existing\nmodels in NL2Vis?\nâ€¢RQ3 [Iterative Updating]: Can the results be iteratively updated via some optimization strate-\ngies?\nTo address RQ1, we first explore the conversion of structured tables into sequential prompts,\nfollowed by an examination of how the content of these tables influences the results. To answer\nRQ2, we conduct a comparative analysis of the performance exhibited by both conventional neural\nnetworks and LLMs concerning NL2Vis. Furthermore, we analyze the impact of the number of\nin-context demonstrations on the performance of LLMs. To answer RQ3, we delve into instances of\nfailure within the LLMs and subsequently propose strategies for enhancing model performance\nthrough optimization techniques.\nAll experiments are conducted on a machine with 252 GB memory and 4 Tesla V100 32GB\nGPUs. We use the default hyperparameter settings provided by each method. Besides, we split the\ndataset into a training dataset, a valid dataset, and a test dataset with 7:2:1 based on in-domain\nand cross-domain settings, as explained in Sec. 4.1. We use the test dataset for evaluation and the\ntraining dataset for demonstrations.\n4.1 Dataset\nWe employ the widely-recognized benchmark nvBench [26] to assess performance in the NL2Vis\ntask. The nvBench dataset encompasses 780 relational tables sourced from 153 databases across\n105 diverse domains such as sports, colleges, hospitals, and more. It features 7,247 visualizations,\nresulting in 25,750 pairs of natural-language descriptions and corresponding visualizations.\nDomain Setting. In prior studies [27, 28], datasets are predominantly partitioned randomly based\non visualization queries, without considering database divisions. Upon re-implementing the ncNet1,\nwe notice that databases from the test set are exposed during the training process, constituting an\nin-domain setting. We propose a cross-domain setting by partitioning the dataset such that there is\nno overlap between databases in the training and test datasets. In this setup, models are required\nto predict queries on the unseen databases. To ensure a fair and comprehensive evaluation, we\nconduct experiments in both in-domain and cross-domain settings.\nMulti-Table Setting. In our experiments, we categorize scenarios based on the number of tables\ninvolved in the input. Instances with multiple tables are designated as â€œjoinâ€ scenarios, while those\ncentered on a single table are categorized as â€œnon-joinâ€ scenarios. In â€œjoinâ€ scenarios, our objective\nentails generating visualizations by integrating information from multiple tables. This process\ninvolves merging data based on shared columns, presenting challenges to both data integration and\nthe subsequent visualization efforts. Conversely, â€œnon-joinâ€ cases involve generating visualizations\nfrom individual tables, acting as a benchmark for evaluating the modelâ€™s ability to manage less\ncomplex data structures.\n1https://github.com/Thanksyy/ncNet\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:11\nVISUALIZE\nline Cj Ck Bi\nROOT\nSELECT BIN\n(a) ğ´ğ‘†ğ‘‡1\nVISUALIZE\nline Cj Ci Bi\nROOT\nSELECT BIN (b) ğ´ğ‘†ğ‘‡2\nFig. 5. An illustration of ASTs of VQL queries. Even though they may not be exactly matched, their execution\nresults are identical.\n4.2 Evaluation Metrics\nTo ensure a thorough and equitable assessment of the generated VQL queries, we employ three\nwidely recognized metrics: exact accuracy, execution accuracy, and component accuracy, as estab-\nlished in the NL2Vis task [27]. Before introducing these metrics, we first introduce two visualization\nqueries represented as ASTs, as shown in Figure 5. Each of them has three subtrees based on the\ngrammar of VQL described in Sec. 2.1. Specifically, the node of VISUALIZE denotes a line chart\ntype. The SELECT node combines the attributes of the columns ğ¶ğ‘— and ğ¶ğ‘˜. The BIN node with ğµğ‘–\nsets a bucket of values in the temporal column.\nExact Accuracy [ 27]. This metric is designed to assess the exact match between the predicted\nAST and the ground-truth AST of VQL queries. It can be formulated as ğ´ğ‘ğ‘ğ´ğ‘†ğ‘‡ = ğ‘ğ´ğ‘†ğ‘‡/ğ‘, where\nğ‘ğ´ğ‘†ğ‘‡ represents the count of generated ASTs that are exactly equivalent to the ground truth ASTs\nin each node, and ğ‘ signifies the total number of ASTs under consideration.\nExecution Accuracy [ 27]. This metric measures the accuracy of the visualization results by\ndetermining whether the predicted visualization aligns with the ground truth. It can be calculated\nusing the formula ğ´ğ‘ğ‘ğ‘’ğ‘¥ğ‘’ = ğ‘ğ‘’ğ‘¥ğ‘’/ğ‘, where ğ‘ğ‘’ğ‘¥ğ‘’ denotes the number of VQL whose results match\nthe ground truth in the execution, andğ‘ denotes the total number of visualizations. From Figure 5, it\nis apparent that the subtrees rooted at â€œVISUALIZEâ€ and â€œBINâ€ retain consistent structures between\nğ´ğ‘†ğ‘‡1 and ğ´ğ‘†ğ‘‡2. While variations within the â€œSELECTâ€ subtrees suggest they are not exact matches.\nEven though, the VQL execution results could be matched if ğ‘¥/ğ‘¦/ğ‘§-axis data (executed by VQL)\nare correct, as exemplified by queries such as â€œ VISUALIZE line SELECT date, COUNT(date)\nfrom payments BIN date by monthâ€ and â€œVISUALIZE line SELECT date, date_count from\npayments BIN date by monthâ€.\n4.3 Comparison Models for NL2Vis\nTraditional Neural Models. Seq2Vis [27] is a sequence-to-sequence model that initially encodes\nthe natural-language query into a hidden embedding using an LSTM and subsequently decodes it\ninto a visualization through another LSTM. Transformer [46] is another encoder-decoder network\nthat has been considered a foundational component in LLMs. We also apply the Transformer into\nNL2Vis. Based on Transformer, ncNet [28] designs several novel visualization-aware optimizations,\nsuch as using attention-forcing to optimize the learning process and visualization-aware rendering\nto produce better visualization results. RGVisNet [43] is a hybrid framework for NL2Vis to retrieve,\nrefine, and generate visualizations from a large codebase using a graph neural network.\nLLMs (Finetuned Models). T5 [42] is an encoder-decoder model pre-trained on a multi-task\nmixture of unsupervised and supervised tasks for which each task is converted into a text-to-text\nformat. T5-Small is a released T5 model with 60 million parameters. T5-Base is a released T5 model\nwith 220 million parameters.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:12 Yang Wu et al.\nLLMs (Inference-only Models). Chat2Vis [31] leverages the power of code-davinci-002 to\nenable users to generate data visualizations using natural-language queries in Python plots and\nmeasures results against nvBench. Our evaluation focuses on the models accessible via the OpenAI\nAPI: GPT-3.5 (text-davinci-002, text-davinci-003, gpt-3.5-turbo-16k), and GPT-4 (gpt-4).\nGPT-3.5 is a set of models built on the InstructGPT [35], trained on a large corpus of programming\nlanguages and natural languages. text-davinci-0032 is fine-tuned by reinforcement learning\nfrom human feedback [6], which improves its ability to generate better quality and longer output.\ntext-davinci-0022 is trained with supervised fine-tuning instead of reinforcement learning.\ngpt-3.5-turbo-16k is optimized for chat applications and increased input length. GPT-4 is a large\nmultimodal model that is capable of solving complex problems with greater accuracy than any of\nthe previous models, due to its excellent general knowledge and reasoning capabilities [11]. gpt-4\nrevolutionizes loss function computation by incorporating a scaling law and an irreducible loss\nterm [34], enabling precise prediction of final loss within the internal codebase.\n5 RESULTS AND ANALYSIS\nIn this section, we present the experimental results for each RQ with comprehensive analysis.\n5.1 RQ1: Prompt Engineering\nIn this RQ, we investigate various strategies for inputting structured tabular data into LLMs through\nprompting and analyze which aspect of the table content has the greatest influence on NL2Vis.\n5.1.1 RQ1-1: How to transform the structured tabular data into sequential prompts? We evaluate\nthe performance of the model text-davinci-003 while varying the methods for transforming\ntabular data into textual prompts. We carry out this assessment in both cross-domain and in-domain\nsettings. Additionally, we investigate configurations for both non-join and join scenarios, where\nsingle-table and multi-table contexts are considered, respectively. Note that when transforming\ntabular data into a markup format, we only consider a single row of content, specifically the one\nmost relevant to the input question, as determined by Jaccard similarity.\nTable 2 presents the model performance for text-davinci-003 across various methods of\ntransforming tabular data into textual prompts, considering both cross-domain and in-domain\nscenarios. All results are derived from a single-shot example provided in the ICL. From this table,\nit is clear that feeding LLMs with tables by encoding them in programming languages is the\nmost effective method, both in the in-domain and cross-domain settings. Specifically, under the\nin-domain setting, prompting table via Table2SQL, Table2XML, and Table2JSON achieves the best\nperformance, reaching up to 61% in terms of the Exact Accuracy. While under the cross-domain\nsetting, prompting table via Table2Code, Table2SQL, and Chat2Vis* achieves the best Execution\nAccuracy of 56%, 55%, and 55%, respectively. It suggests that converting structured tabular data\ninto machine-readable markup formats and using general-purpose programming languages can\neffectively design prompts to interact with LLMs for NL2Vis. When comparing the prompts of\nTable2Code to Chat2Vis*, we can see that Table2Code obtains 6% and 1% improvement in terms of\nthe Execution Accuracy in the in-domain and cross-domain settings, respectively. Interestingly,\nwe observe that the LLM-based model exhibits slight fluctuations in the performance of several\nsettings. In cross-domain scenarios, the optimal performance is achieved through the application\nof table programming to transform input tables; however, this trend does not persist in in-domain\nsettings. One plausible explanation for this observation is that, in the in-domain scenario, where the\n2Note that this study was conducted between April and October 2023. Subsequently, as of January 2024, the\ntext-davinci-003 and text-davinci-002 have been upgraded to gpt-3.5-turbo-instruct. More details are referred\nto OpenAI documentation: https://platform.openai.com/docs/deprecations.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:13\nTable 2. Performance of the model text-davinci-003 while varying the methods for transforming tabular\ndata into textual prompts, under both the cross-domain and in-domain settings.\nCross-domain In-domain\nNon-join Join Overall Non-join Join Overall\nExa. Exe. Exa. Exe. Exa. Exe. Exa. Exe. Exa. Exe. Exa. Exe.\nTable Serialization\nSchema 0.33 0.50 0.06 0.09 0.32 0.47 0.30 0.31 0.27 0.23 0.29 0.28\nTable (Column) 0.37 0.56 0.09 0.11 0.36 0.54 0.60 0.59 0.52 0.44 0.57 0.53\nColumn=[] 0.37 0.54 0.11 0.12 0.36 0.52 0.61 0.61 0.52 0.45 0.58 0.55\nTable Summarization\nTable2NL 0.35 0.54 0.13 0.17 0.34 0.52 0.63 0.63 0.53 0.46 0.60 0.57\nChat2Vis* 0.37 0.57 0.14 0.20 0.36 0.55 0.58 0.61 0.32 0.28 0.49 0.49\nTable Markup Format\nTable2JSON 0.36 0.54 0.08 0.15 0.35 0.52 0.63 0.63 0.56 0.48 0.61 0.57\nTable2CSV 0.35 0.52 0.07 0.12 0.34 0.50 0.59 0.59 0.54 0.46 0.57 0.54\nTable2MD 0.36 0.53 0.07 0.09 0.34 0.50 0.59 0.61 0.53 0.46 0.57 0.56\nTable2XML 0.36 0.54 0.08 0.16 0.35 0.52 0.63 0.64 0.56 0.47 0.61 0.58\nTable Programming\nTable2SQL 0.39 0.58 0.17 0.25 0.38 0.55 0.64 0.64 0.53 0.47 0.61 0.58\nTable2Code 0.36 0.59 0.18 0.08 0.34 0.56 0.53 0.58 0.47 0.43 0.55 0.55\ndata adheres to a similar distribution, the model has the opportunity to discern the schema of the\ntest database from demonstration examples during in-context learning. Consequently, the impact\nof exploring table transformations becomes diminished within this specific in-domain context.\nFurthermore, when comes to the non-join and join scenarios under the cross-domain setting, we\ncan see that our proposed methods of converting tabular data into code can still maintain the best\nperformance. For example, Table2Code surpasses the state-of-the-art Chat2Vis* by 2% and 4% in\nterms of the Execution Accuracy and Exact Accuracy when handling the non-join cases and join\ncases, respectively. We attribute the improvements to the effective preservation of inner structural\ninformation within tables when concerting structured tabular data into source code. This benefit\narises from the inherent interplay between the structural aspects of source code and tabular data.\nFinding 1-1. Converting structured tabular data into machine-readable markup formats\n(e.g., JSON, XML) or utilizing general-purpose programming languages (e.g., SQL, Python)\nyields superior performance compared to relying solely on natural-language summaries or\nstraightforward table serialization.\n5.1.2 RQ1-2: What table content should be considered in NL2Vis? We further dive into analyzing\nthe importance of table components in prompt construction and pinpoint the most reliant table\ncomponent in different domain and few-shot settings. We categorize table contents into three\nlevels: (i) Table Schema, which encompasses table names and column names, (ii) +RS, denoting\nrelationships within the table, such as foreign keys, and (iii)+Cont, encompassing the actual content\nof the table, including the data in table row values. To simplify our experiment, we have chosen table\nserialization as the primary prompting approach. More specifically, our exploration encompasses\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:14 Yang Wu et al.\n/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000015/uni0000001b\n/uni00000016/uni00000015\n/uni00000016/uni00000019\n/uni00000017/uni00000013\n/uni00000017/uni00000017\n/uni00000017/uni0000001b\n/uni00000018/uni00000015\n/uni00000018/uni00000019/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni0000000e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni0000000e/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n(a) Exa. Acc. in cross-domain\n/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000017/uni00000018\n/uni00000017/uni0000001b\n/uni00000018/uni00000014\n/uni00000018/uni00000017\n/uni00000018/uni0000001a\n/uni00000019/uni00000013\n/uni00000019/uni00000016\n/uni00000019/uni00000019\n/uni00000019/uni0000001c/uni00000028/uni0000005b/uni00000048/uni00000046/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni0000000e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni0000000e/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057 (b) Exe. Acc. in cross-domain\n/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000018/uni00000013\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni0000000e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni0000000e/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n(c) Exa. Acc. in in-domain\n/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000017/uni0000001b\n/uni00000018/uni00000015\n/uni00000018/uni00000019\n/uni00000019/uni00000013\n/uni00000019/uni00000017\n/uni00000019/uni0000001b\n/uni0000001a/uni00000015\n/uni0000001a/uni00000019/uni00000028/uni0000005b/uni00000048/uni00000046/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036\n/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000015/uni00000036/uni00000034/uni0000002f/uni0000000e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057\n/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000020/uni0000003e/uni00000040/uni0000000e/uni00000029/uni0000002e/uni0000000e/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000016/uni00000003/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000000e/uni00000035/uni00000036/uni0000000e/uni00000026/uni00000052/uni00000051/uni00000057 (d) Exe. Acc. in in-domain\nFig. 6. Exact Accuracy and Execution Accuracy of text-davinci-003 under the cross-domain and in-domain\nsettings, with 1, 3, 5, 7, and 15 demonstrations provided in in-context learning. RS and Cont correspond to the\ntable relationship and table content, respectively.\nthree distinct variants: (i) Column=[], (ii) Column=[]+FK, Table2SQL, and (iii) Column=[]+FK+Value3,\nTable2SQL+Select3.\nFigure 6 shows the Exact Accuracy and Execution Accuracy of text-davinci-003 under the\ncross-domain and in-domain settings, with few demonstrations provided in in-context learning.\nFrom this figure, it is evident that the performance of the Table Column=[] configuration surpasses\nthat of the Table+RS Column=[]+FK and Table+RS+Cont Column=[]+FK+Value configurations in\nboth in-domain and cross-domain settings. This observation underscores the significance of the\ntable schema as a comprehensive element within the prompt for Column=[].\nMoreover, when comparing the disparity between Table Column=[] and Table+RS Column=[]+FK,\nwe observe that in the in-domain setting, the gap slightly narrows as the number of examples\nincreases. This suggests that table knowledge in prompts has minimal influence on input, likely due\nto the fact that, in the in-domain setting, LLMs have seen test tables during demonstrations from\nthe same database domain. However, a significant gap emerges in the cross-domain setting (i.e.,\nincreasing from 2.5% to 7.2% in terms of Execution Accuracy as the number of examples increases\nfrom 1 to 3), indicating that the additional table knowledge, specifically table relationships and\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:15\nTable 3. Results of comparing LLMs with several baselines.\nCross-domain In-domain\nExa. Acc. Exe. Acc. Exa. Acc. Exe. Acc.\nBaseline\nSeq2Vis 0.02 - 0.66 -\nTransformer 0.03 - 0.73 -\nncNet 0.26 - 0.77 -\nRGVisNet 0.45 - - -\nChat2Vis - 0.43 - -\nFinetuned\nT5-Small 0.60 0.61 0.92 0.81\nT5-Base 0.71 0.72 0.93 0.82\nInference-only\ntext-davinci-002 0.57 0.68 0.84 0.75\ntext-davinci-003 0.58 0.70 0.87 0.77\ngpt-3.5-turbo-16k 0.56 0.63 0.59 0.59\ngpt-4 0.61 0.72 0.83 0.74\ncontent, in the Column=[] prompt is superfluous. Moreover, this redundancy could potentially\nintroduce complexity and adversely impact the performance of LLMs in the NL2Vis task.\nAdditionally, when comparing the performance of Table+RS Table2SQL and Table+RS+Cont\nTable2SQL+Select3, we observe that there is no reduction in the gap between these two prompts.\nThis observation suggests that Table2SQL remains effective even without the inclusion of row\nvalues. This can be attributed to the nature of the NL2Vis task, where the importance of table\ncontent often takes a secondary role. Users typically only need to reference table and column names\nto convey their visualization requirements. Essentially, the modelâ€™s primary task is to establish a\nseamless connection between the natural-language query and the database schema, subsequently\ngenerating the visualization query successfully. This underscores that table content may not play a\nsignificant role in this context.\nFinally, when comparing the performance of Table+RS Column=[] +FK and Table+RS Table2SQL,\nwe can observe that while both prompts incorporate table schema and table relationship knowledge,\nthey yield markedly distinct results. This observation underscores the substantial impact of well-\ncrafted formats for representing structured tabular data.\nFinding 1-2. Table schema plays an important role in the task of NL2Vis, both under the\ncross-domain and in-domain settings.\n5.2 RQ2: Overall Performance\nIn this RQ, we investigate the overall performance of LLMs, and compare them with several\ntraditional neural models for NL2Vis, in both in-domain and cross-domain settings.\n5.2.1 RQ2-1: How do the LLMs perform when compared with existing works? To answer this RQ,\nwe conduct a comparative analysis to evaluate LLMs, including the fine-tuned T5 models together\nwith the inference-only GPT-3.5 and GPT-4 series models, against several state-of-the-art baseline\nmodels. We fine-tune T5-Small and T5-Base for the NL2Vis task with a maximum of 11.6ğ‘˜ steps\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:16 Yang Wu et al.\n/uni00000013/uni00000017/uni0000001b/uni00000014/uni00000015/uni00000014/uni00000019/uni00000015/uni00000013/uni00000015/uni00000017/uni00000015/uni0000001b\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000017/uni00000013\n/uni00000017/uni00000017\n/uni00000017/uni0000001b\n/uni00000018/uni00000015\n/uni00000018/uni00000019\n/uni00000019/uni00000013\n/uni00000019/uni00000017\n/uni00000019/uni0000001b\n/uni0000001a/uni00000015/uni00000028/uni0000005b/uni00000044/uni00000046/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni0000004a/uni00000053/uni00000057/uni00000010/uni00000016/uni00000011/uni00000018/uni00000010/uni00000057/uni00000058/uni00000055/uni00000045/uni00000052\n/uni0000004a/uni00000053/uni00000057/uni00000010/uni00000017\n/uni00000037/uni00000018/uni00000010/uni00000036/uni00000050/uni00000044/uni0000004f/uni0000004f\n/uni00000037/uni00000018/uni00000010/uni00000025/uni00000044/uni00000056/uni00000048\n(a) Exa. Acc.\n/uni00000013/uni00000017/uni0000001b/uni00000014/uni00000015/uni00000014/uni00000019/uni00000015/uni00000013/uni00000015/uni00000017/uni00000015/uni0000001b\n/uni00000031/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni00000018/uni00000014\n/uni00000018/uni00000017\n/uni00000018/uni0000001a\n/uni00000019/uni00000013\n/uni00000019/uni00000016\n/uni00000019/uni00000019\n/uni00000019/uni0000001c\n/uni0000001a/uni00000015/uni00000028/uni0000005b/uni00000048/uni00000046/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000047/uni00000044/uni00000059/uni0000004c/uni00000051/uni00000046/uni0000004c/uni00000010/uni00000013/uni00000013/uni00000016\n/uni0000004a/uni00000053/uni00000057/uni00000010/uni00000016/uni00000011/uni00000018/uni00000010/uni00000057/uni00000058/uni00000055/uni00000045/uni00000052\n/uni0000004a/uni00000053/uni00000057/uni00000010/uni00000017\n/uni00000037/uni00000018/uni00000010/uni00000036/uni00000050/uni00000044/uni0000004f/uni0000004f\n/uni00000037/uni00000018/uni00000010/uni00000025/uni00000044/uni00000056/uni00000048 (b) Exe. Acc.\nFig. 7. Exact Accuracy and Execution Accuracy with varying number of support examples. The x-axis indicates\nthe number of few-shot examples used.\nand 100ğ‘˜ steps, respectively. Our configurations specify a maximum learning rate of 1 Ã—10âˆ’3, a\nbatch size of 4, and a dropout rate of 0.1. To set up a fair comparison, we explore inference-only\nmodels using the same Table2SQL prompt with 20 examples provided in the ICL (20-shot).\nTable 3 shows the overall performance of LLMs as well as several baselines, under both the\ncross-domain and in-domain settings for the NL2Vis task. From this table, it is clear that LLMs\n(both the finetuned models and inference-only models) significantly outperform the traditional\nbaselines, both in the in-domain and cross-domain settings. A closer examination of in-domain\nperformance reveals that the predominant baseline models, namely, Seq2Vis, Transformer, and\nncNet, achieve their peak scores at 66%, 73%, and 77%, respectively. This underscores the role of\nncNet in enhancing the seq2seq models through attention forcing. Furthermore, the fine-tuned\nlanguage models, T5-Small and T5-Base, significantly outperform state-of-the-art methods with\nscores of 92% and 93%, respectively. For in-context learning, inference-only models also achieve\ncommendable scores. Notably,text-davinci-003 stands out with an 87% score, surpassing existing\nbaseline methods by 10%. This underscores the idea that LLMs, trained on natural-language text or\ncode corpora, represent the preeminent models for the task. The enhancement is attributed to the\nrobust capabilities of LLMs in comprehending the inherent knowledge, including table schema and\nstructure, within the same domain, leading to superior responses to new queries.\nFurthermore, when comparing performance in cross-domain scenarios, we are genuinely sur-\nprised by the substantial decline observed across all baseline models (e.g., ncNet, plummeting from\n77% to 26%). It is noteworthy that RGVisNet is the first to address this phenomenon and succeeds in\nboosting cross-domain performance by 45%. This underscores the vast potential for enhancement\nin this context. The underperformance of traditional neural models, designed for random-splitting\ndata settings, becomes evident as they struggle to generalize to previously unseen databases and\ngrapple with the task of linking natural language to table schema. However, the fine-tuned models,\nspecifically T5-Small (60%) and T5-Base (71%), significantly outshine the baseline models. Among\nthe inference-only models, gpt-4 shows the most impressive improvement, surging by 61%, while\ngpt-3.5-turbo-16k lags behind with a 56% increase. This serves as a testament to the exceptional\ngeneralization capabilities of LLMs, empowering them to effectively assimilate new knowledge and\nestablish connections with queries when applied to previously uncharted databases. The result is\nan unequivocal improvement in visualization results.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:17\nTable 4. Statistics of model parameters and cost time.\nParameters Cost Time Model Size\nT5-Small 60M 3 days 200MB\nT5-Base 220M 5 days 500MB\ntext-davinci-003 1.5B 15 hours 1GB\ngpt-3.5-turbo-16k 4B 4 hours 2GB\ngpt-4 - 4 hours -\nLastly, in comparing the performance of fine-tuned models with that of inference-only models,\nwe observe that while fine-tuned models exhibit the most desirable performance, their advantages\nin fine-tuning can be equaled by inference-only models with in-context learning. As shown in\nFigure 7, both fine-tuned T5-Small and T5-Base models achieve execution accuracies of 61% and\n72%, respectively. The models text-davinci-003 and gpt-3.5-turbo-16k demonstrate superior\nperformance compared to the T5-Small model, particularly with 3-shot and 13-shot scenarios. This\nsuggests that, after exposure to an ample number of examples in contextual learning, LLMs acquire\nthe capability to utilize demonstrations for precise inference and the generation of visualizations in\ntables not encountered before. Additionally, we investigate the comparative costs of fine-tuned\nmodels and inference-only models with 20-shot in-context learning. From Table 4, we can observe\nthat even though the inference-only models typically incur significant computational costs due to\ntheir huge size, they offer considerable time savings through in-context learning.\nFinding 2-1. The LLMs demonstrate a remarkable ability to surpass state-of-the-art models\nin both cross-domain and in-domain scenarios, achieving an improvement of 26% and\n16% in terms of Exact Accuracy, respectively. This underscores the LLMsâ€™ exceptional\ngeneralization capabilities. Furthermore, with the provision of more few-shot samples, the\nperformance of inference-only models consistently improves, eventually surpassing that of\nfine-tuned models.\n5.2.2 RQ2-2: How does the number of in-context demonstrations affect the performance of LLMs?\nIn this RQ, we explore the impact of demonstration selection methods on in-context learning for\nNL2Vis task, by selecting different databases and varying quantities of examples in demonstration.\nTo investigate the effect of the number of databases and examples per database in the demonstration,\nwe conduct experiments encompassing various combinations. Specifically, our demonstration\nexamples for in-context learning are drawn from ğ´distinct databases to simulate the cross-domain\nscenario. From each of these databases, we extract ğµ paired instances consisting of a natural-\nlanguage query and its corresponding VQL query. Collectively, this amounts toğ¶ = ğ´Ã—ğµexamples.\nIn our configuration, we permit both variables ğ´and ğµto attain a maximum value of 4, ensuring\nthat their cumulative total does not surpass the length limit specified by the prompt. We conduct\nthis experiment using the Table2SQL prompt in a cross-domain scenario across all samples within\nthe test dataset.\nIn Figure 8, it is evident that there is a notable improvement (45%-47%) in performance when\nall examples are sourced from the same database. This improvement is observed as the number of\nexamples increases from 1 to 4. When the total number of examples is fixed (e.g., at 4), sourcing\nthem from entirely different databases reveals superior performance (49%) compared to sourcing\nthem from the same database (47%). It indicates that, in in-context learning, selecting a greater\nnumber of examples from diverse databases is beneficial.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:18 Yang Wu et al.\n/uni00000014/uni00000015/uni00000017/uni00000014/uni00000015/uni00000017/uni00000014/uni00000015/uni00000017\n/uni00000028/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000027/uni00000025\n/uni00000017/uni00000017\n/uni00000017/uni00000019\n/uni00000017/uni0000001b\n/uni00000018/uni00000013\n/uni00000018/uni00000015\n/uni00000018/uni00000017/uni00000028/uni0000005b/uni00000048/uni00000046/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000014/uni00000003/uni00000027/uni00000025\n/uni00000015/uni00000003/uni00000027/uni00000025/uni00000056\n/uni00000017/uni00000003/uni00000027/uni00000025/uni00000056\nFig. 8. The average Execution Accuracy across test dataset using Table2SQL by text-davinci-003 with\nrespect to different numbers of databases (DBs) and examples per database (Exp/DB) in the demonstration\nfor in-context learning.\nFinding 2-2. Multiple cross-domain demonstrations are generally more beneficial than\nexamples derived from the same database domain.\n5.2.3 User Study. We conduct a user study on LLMs to evaluate whether LLMs work well in the real\nworld with users from different backgrounds. First, we design two tasks as follows: (1) Given tables\nand a target visualization with description, users express a natural-language query for creating such\na visualization. (2) If the query can not generate the target visualization correctly, the users could\nchoose to revise it three times. Next, we invite 3 graduate students as experts and 3 undergraduate\nstudents as non-experts, all majoring in computer science, to participate in the user study. Each\nexpert possesses over six years of proficiency in software development, demonstrating advanced\nskills in data analysis and visualization. On the other hand, non-experts, with approximately two\nyears of programming experience, are capable of executing basic visualization operations in Excel.\nThen, we select 5 databases at random to ensure a diverse range of data for our study. From each\ndatabase, we pick 3 visualizations for each of the 4 difficulty levels (i.e., easy, medium, hard, and\nextra hard), resulting in a total of 60 visualizations. We design a command-line interface that enables\nusers to engage in data visualization over the selected databases using LLMs by crafting natural-\nlanguage queries. Subsequently, these crafted natural-language queries, along with the serialized\ntable and a set of 20 demonstration examples, are input into the LLM (i.e., text-davinci-003)\nthrough in-context prompting. The generated VQL queries will be transformed into visualizations\nfor users, allowing them to iteratively revise the natural-language queries.\nFigure 10 shows the success rates of users querying for visualizations across four levels of\ndifficulty. We observe that the experts are excellent in formulating queries for complex visualizations,\nsuccessfully obtaining 95.6% hard charts. While the non-experts are proficient in queries for 84.4%\neasy charts. Figure 9 reports that non-experts take approximately 16 seconds more for the initial\ncomposition of queries and 15 seconds more for revision than experts. The system consistently\nmaintains an average response time of 3 seconds for generating prompt examples and 2 seconds for\nVQL generation, applicable to both user groups. Finally, we collect feedback on the ability of LLMs\nfor the NL2Vis task. All users acknowledge the LLMâ€™s proficient understanding of natural-language\nquery for NL2Vis task, and appreciate its substantial facilitation of the visualization process.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:19\n/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000014/uni00000018/uni00000013/uni00000014/uni0000001a/uni00000018\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000052/uni00000051/uni00000047/uni00000056/uni0000000c\n/uni00000031/uni00000052/uni00000051/uni00000010/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057\n/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni0000002c/uni00000051/uni0000004c/uni00000057/uni0000004c/uni00000044/uni0000004f/uni00000003/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c\n/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000035/uni00000048/uni00000059/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048\n/uni00000039/uni00000034/uni0000002f/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\nFig. 9. Quantitative analysis of the composition of user time on average.\n/uni00000028/uni00000044/uni00000056/uni0000005c/uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000003/uni0000002b/uni00000044/uni00000055/uni00000047/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f\n/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057\n/uni00000031/uni00000052/uni00000051/uni00000010/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057\n/uni0000001c/uni00000016/uni00000011/uni00000016/uni0000001b/uni0000001b/uni00000011/uni0000001c/uni0000001c/uni00000018/uni00000011/uni00000019/uni0000001b/uni0000001b/uni00000011/uni0000001c/uni0000001c/uni00000014/uni00000011/uni0000001a\n/uni0000001b/uni00000017/uni00000011/uni00000017/uni0000001a/uni0000001a/uni00000011/uni0000001b/uni00000019/uni00000015/uni00000011/uni00000015/uni00000019/uni00000015/uni00000011/uni00000015/uni0000001a/uni00000014/uni00000011/uni0000001a\n/uni00000018/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001a/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni0000001c/uni00000013/uni00000008/uni00000014/uni00000013/uni00000013/uni00000008\nFig. 10. The average success rates of experts and non-experts queried for 4 difficult levels of visualization.\n5.3 RQ3: Iterative Updating\nIn this RQ, we commence by analyzing the conditions under which LLMs encounter failures in\nNL2Vis. Subsequently, we propose a strategy to mitigate these failures by iteratively updating the\nresults through in-context learning with chain-of-thought.\n5.3.1 RQ3-1: When do the LLMs fail in NL2Vis? To gain a deeper insight into the instances where\nLLMs encounter challenges, we conduct a comprehensive analysis of erroneous outputs. Specifically,\nwe scrutinize all outcomes generated by thetext-davinci-003 model using the Table2SQL prompt\nwith a 20-shot approach in a cross-domain scenario. We classify the cases of failure into distinct\ncategories, depending on the component (e.g., wrong tables and wrong columns) of the VQL it\nstruggles to predict accurately. Based on the AST of visualization query [26], a visualization consists\nof two components: the visual part (visualization types, axis) and the data part (transformation\nfrom a database) [27]. Specifically, the evaluation of visualization types involves measuring type\ntokens such as bar, scatter, line, and pie. With regard to the axis component, the assessment focuses\non the â€œSELECTâ€ component of the visualization query. Data part includes â€œBINâ€, â€œGROUPâ€, â€œJOINâ€,\nâ€œCOND (ORDER, WHERE, and AND/OR)â€, and nested components.\nFigure 11 presents a breakdown of these failure statistics. It is evident from this figure that\ndata-related errors constitute the majority at 73.8%, which is approximately three times more\nprevalent than visual-related errors at 26.2%. This observation underscores the challenge of precisely\nidentifying and visualizing the relevant data. In terms of the data-related errors, the highest\nproportion of errors is associated with the â€œcondâ€ (condition) attribute (35.6%). This indicates a need\nto enhance the ability of LLMs to effectively handle queries related to filtering operations. This\nerror analysis motivates us to iteratively update the results in a conversational manner, mirroring\nthe functioning of LLMs.\nFinding 3-1. In summary, the analysis reveals that errors in the data part of the visualization\nquery are more frequent compared to those in the visual part. Respectively, errors are mainly\nrelated to data filtering and the ğ‘¦-axis of visualization.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:20 Yang Wu et al.\n/uni00000039/uni0000004c/uni00000056\n/uni00000015/uni00000019/uni00000011/uni00000015/uni00000008\n/uni00000027/uni00000044/uni00000057/uni00000044\n/uni0000001a/uni00000016/uni00000011/uni0000001b/uni00000008\n/uni00000037/uni0000005c/uni00000053/uni00000048\n/uni00000018/uni00000011/uni00000014/uni00000008\n/uni00000024/uni00000010/uni0000005b/uni0000004c/uni00000056\n/uni00000015/uni00000014/uni00000011/uni00000014/uni00000008\n/uni00000025/uni0000004c/uni00000051\n/uni0000001b/uni00000011/uni00000019/uni00000008\n/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000053\n/uni00000014/uni00000017/uni00000011/uni00000014/uni00000008/uni0000002d/uni00000052/uni0000004c/uni00000051\n/uni0000001a/uni00000011/uni00000013/uni00000008\n/uni00000026/uni00000052/uni00000051/uni00000047\n/uni00000016/uni00000018/uni00000011/uni00000019/uni00000008\n/uni00000031/uni00000048/uni00000056/uni00000057/uni00000048/uni00000047\n/uni0000001b/uni00000011/uni00000016/uni00000008\n/uni00000025/uni00000044/uni00000055/uni00000015/uni00000011/uni00000016/uni00000008\n/uni00000036/uni00000046/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000013/uni00000011/uni00000015/uni00000008\n/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000015/uni00000011/uni00000015/uni00000008\n/uni00000033/uni0000004c/uni00000048/uni00000013/uni00000011/uni00000017/uni00000008/uni0000003b/uni00000010/uni00000044/uni0000005b/uni0000004c/uni00000056/uni00000019/uni00000011/uni00000019/uni00000008\n/uni0000003c/uni00000010/uni00000044/uni0000005b/uni0000004c/uni00000056/uni00000014/uni00000017/uni00000011/uni00000018/uni00000008\n/uni0000003a/uni00000055/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000056/uni0000001a/uni00000011/uni00000015/uni00000008\n/uni00000031/uni00000052/uni00000057/uni00000010/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000014/uni00000011/uni00000017/uni00000008\n/uni0000003a/uni00000055/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000056/uni00000016/uni00000011/uni00000014/uni00000008\n/uni0000002b/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000014/uni00000014/uni00000011/uni00000014/uni00000008\n/uni0000003a/uni00000055/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000057/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000056/uni00000016/uni00000011/uni00000016/uni00000008\n/uni0000003a/uni00000055/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000046/uni00000052/uni0000004f/uni00000056/uni00000016/uni00000011/uni00000016/uni00000008\n/uni0000003a/uni0000004b/uni00000048/uni00000055/uni00000048/uni00000015/uni00000013/uni00000011/uni00000013/uni00000008\n/uni00000032/uni00000055/uni00000047/uni00000048/uni00000055/uni00000014/uni00000017/uni00000011/uni0000001c/uni00000008\n/uni00000024/uni00000051/uni00000047/uni00000012/uni00000052/uni00000055/uni00000013/uni00000011/uni0000001b/uni00000008\n/uni00000036/uni00000048/uni00000057/uni00000003/uni00000032/uni00000053/uni00000015/uni00000011/uni00000018/uni00000008\n/uni0000003a/uni00000055/uni00000052/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000058/uni00000045/uni00000010/uni00000054/uni00000058/uni00000048/uni00000055/uni0000005c/uni00000018/uni00000011/uni0000001c/uni00000008\nFig. 11. Statistics of failures by text-davinci-003 in 20-shot using Table2SQL.\n5.3.2 RQ3-2: Can we iteratively update the results via optimization strategies? Here, we employ the\nCoT strategy with manual construction [52] to enhance the prompts for NL2Vis, infusing LLMs\n(i.e., gpt-3.5-turbo and gpt-4) with an intermediary cognitive process. Our approach utilizes\na sketch as an intermediate expression, containing essential keywords from the visualization\nquery. Additionally, we harness gpt-3.5-turboâ€™s innate self-instruct capability by introducing\nthe phrase â€œLetâ€™s think step by step. â€ This combined strategy is denoted as CoT. Furthermore, we\ndelve into role-playing, where gpt-3.5-turbo takes on the persona of a visualization expert. In\nconsideration of gpt-4â€™s self-repair capabilities, we explore instructions for rectifying visualization\nqueries. The optimization strategy is elucidated in Figure 12. Additionally, we investigate GPT-4â€™s\nnewly introduced code-interpreter feature in ChatGPT Plus3, which enables GPT-4 to demonstrate\nprogramming proficiency within a conversational context, thus facilitating user learning and\nproblem-solving.\nWe evaluate the total failure results oftext-davinci-003 with Table2SQL in 20 shots to explore\nthe optimization strategy. For gpt-3.5-turbo, we employ the following strategies. (1) CoT guided\nby the prompt â€œLetâ€™s think step by step. Generate the sketch as an intermediate representation and\nthen the final VQL â€. (2) Role-playing, where the prompt â€œYou are a data visualization assistant â€ sets\nthe modelâ€™s persona. For GPT-4 (gpt-4), we explore the (3) self-repair by adopting the following\nprompt â€œYou are a helpful programming assistant and expert data visualization assistant. Please fix\nthe given VQL and generate a correct VQL â€. We also explore the (4) code-interpreter by uploading\ndatabase files and entering natural-language queries on the ChatGPT Plus website. By pairing every\nsame VQL with only the first natural-language query as a test example, we filter 176 different charts\nfrom the LLMâ€™s failed dataset. The generated visualizations are manually checked for accuracy.\nFigure 13 depicts the comparative performance of diverse optimization strategies employed\nby both gpt-3.5-turbo and gpt-4. Our findings reveal significant improvements in Execution\n3https://chat.openai.com/?model=gpt-4-code-interpreter\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:21\nTask Instruction\nYou are a helpful programming assistant and expert data \nvisualization assistant. Please fix the given VQL and \ngenerate a correct VQL with â€˜Fixed VQLâ€™ in one line.\nDemonstration\n[Table Description] [Question] [Correct VQL]\nTest Item\n[Table Description] [Question] [Given VQL]\nFixed VQL [To be generated]\nPrompt for CoT\nPrompt for Role-play\nPrompt for Self-repair\nPrompt for Code-interpreter\n[Table Files]\n[Question]\nVis Charts [To be generated]\nTask Instruction\nYou are a data visualization assistant. Please generate \nVQL based on description of tabular data and question. \nDemonstration\n[Table Description] [Question] [VQL] \nTest Item\n[Table Description] [Question]\nVQL [To be generated]\nTask Instruction\nPlease generate VQL based on description of tabular data \nand question. \nLet's think step by step. \nGenerate the sketch as intermediate representation and \nthen the final VQL.\nDemonstration\n[Table Description] [Question] [Sketch] [VQL]\nTest Item\n[Table Description] [Question]\nVQL Sketch [To be generated]\nDesired VQL [To be generated]\nFig. 12. Optimization strategies employed for LLMs to iteratively update the results.\nAccuracy: the CoT strategy enhances accuracy by 9.3%, while the role-playing strategy shows an\nimpressive 12.8% boost. Notably, gpt-4â€™s self-repair strategy leads to a remarkable 13.3% accuracy\nimprovement. When subjected to the code-interpreter on ChatGPT Plus within the extract dataset,\nthe Execution Accuracy of visualization improves to 50.3%.\nFinding 3-2. The Self-repair strategy outperforms the CoT and role-playing strategies in\nupdating the results within NL2Vis. Furthermore, the code-interpreter in gpt-4 excels,\nindicating a promising avenue for future research.\n6 DISCUSSION\nIn this section, we discuss our findings during the experiments and hope researchers can address\nsome of the issues in future work.\n6.1 Findings and Implications\nIn this study, we have uncovered several significant findings that offer valuable insights.\nFinding 1: Investigating the strategies of inputting tables into LLMs, we find that representing\ntables in a programming language format is most effective, showcasing remarkable performance in\naddressing NL2Vis tasks when working with structured tabular data. Future work should delve\ndeeper into investigating the intricacies of table representation within programming languages.\nFinding 2 : In analyzing the impact of three table components (i.e., schema, relationship, and\ncontent) in prompts, we identify that table schema is the most crucial component in both cross-\ndomain and in-domain settings. Table content is found to be inconsequential in NL2Vis tasks,\nemphasizing the importance of the format and relationships in future research, especially when\nfeeding extra large databases into LLMs.\nFinding 3: LLMs exhibit superior capabilities for automating data visualization based on natural-\nlanguage descriptions, outperforming state-of-the-art models in both cross-domain and in-domain\nscenarios. Furthermore, as more few-shot samples are provided, inference-only models excel, even\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:22 Yang Wu et al.\nScatterPie Bar Line GS SB T otal\nEasy\nMedium\nHard\nExtra Hard\nT otal\n0.0 37.2 20.0 12.5 50.0 22.9\n0.0 10.5 3.5 20.0 0.0 1.6 3.1\n0.0 4.5 0.0 4.1\n0.0 12.5 7.1\n0.0 28.1 9.0 13.8 44.4 3.1 9.3\n0%\n20%\n40%\n60%\n80%\n100%\n(a) CoT\nScatterPie Bar Line GS SB T otal\nEasy\nMedium\nHard\nExtra Hard\nT otal\n0.0 23.3 30.7 25.0 12.5 26.9\n0.0 10.5 7.5 0.0 0.0 2.6 5.2\n0.0 18.2 0.0 16.3\n0.0 18.8 10.7\n0.0 18.8 16.1 20.7 11.1 4.9 12.8\n0%\n20%\n40%\n60%\n80%\n100% (b) Role-playing\nScatterPie Bar Line GS SB T otal\nEasy\nMedium\nHard\nExtra Hard\nT otal\n0.0 7.0 25.0 50.0 37.5 23.8\n0.0 15.8 10.0 40.0 0.0 6.3 8.7\n0.0 13.6 0.0 12.2\n0.0 12.5 7.1\n0.0 9.4 14.9 48.3 33.3 7.1 13.3\n0%\n20%\n40%\n60%\n80%\n100%\n(c) Self-repair\nScatterPie Bar Line GS SB T otal\nEasy\nMedium\nHard\nExtra Hard\nT otal\n100.010.0 76.7 12.5 63.6\n50.0 75.0 66.7 50.0 30.2\n100.087.5 90.0\n14.3 6.7\n80.0 24.0 68.4 20.0 50.3\n0%\n20%\n40%\n60%\n80%\n100% (d) Code-interpreter\nFig. 13. The Execution Accuracy of optimization strategy in the failed results of text-davinci-003 in one-\nshot with Table2SQL. GS and SB refer to grouping scatter and stacked bar chart types, respectively.\nsurpassing fine-tuned models. It is suggested to apply our framework to open-source LLMs for\nmore general NL2Vis task, which involves implicit and multi-type table structures.\nFinding 4: Investigating the selection of demonstrations in in-context learning, we determine that\nmultiple out-of-domain demonstrations generally offer greater benefits than examples from the\nsame database domain, which could guide the design of demonstration selection in the future.\nFinding 5 : The analysis of failed generation reveals that errors are more frequent in the data\npart of the visualization query, with particular issues related to data filtering and the ğ‘¦-axis of\nvisualization. There is room for improvement in generating condition and group attributes, which\ninspires further exploration of iterative updating based on multi-turn dialogs of LLMs.\nFinding 6 : Optimization strategies such as the self-repair of gpt-4 and ChatGPT Plusâ€™s code-\ninterpreter demonstrate superior performance, although challenges persist in handling complex\nnatural-language queries and join cases. Future work should focus on intuitive interfaces to enhance\nthe usability and reliability of NL2Vis tasks in real-world applications.\n6.2 Limitations\nThere are some potential limitations on the validity of our experimental results and conclusions.\nLimited tasks and dataset. In this work, we explore theNL2Vis task with regular table structures\nand descriptive table context. However, in practice, tables could be in irregular structures, such\nas those featuring merged rows and columns in Excel, or containing mixed data content. On the\nother hand, table schema and column headers may not be descriptive. Future studies can extend\nthe dataset to irregular tables for visualization, to enrich NL2Vis benchmarks in more applicable\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:23\nscenarios. Moreover, all experiments in this paper are assessed using a synthesized dataset derived\nfrom the NL2SQL dataset. We anticipate extending our study to real-world datasets in future work.\nLimited visualization specification. In this paper, we choose VQL, an intermediate language\nthat has been widely embraced in existing literature. While some may contend that visualizations\ncan be directly generated through the generation of high-level specifications, such as Vega-Lite\n(in JSON format), we argue that this approach may encounter difficulties in accurately capturing\nspecific details, such as chart type and rendering color. As part of our future work, we plan to\nexplore the direct generation of diverse Vega-Lite specifications.\nSupport of conversational NL2Vis. Data analysts typically perform data visualization in a\nconversational manner. Take conversational visual analysis for example, a conversational natural-\nlanguage inquiry can be made up of a number of separate but related natural-language inquiries. It\nis an intriguing and promising avenue to extend the NL2Vis benchmark to conversational visual\ndata analytics.\nManual prompt design. Like other prompting strategies, the prompt design and optimizations\nare based on human comprehension and observations. The designerâ€™s knowledge may affect the\neffectiveness of the used prompts. Nonetheless, the purpose of our study is to investigate the\nviability of prompt design and related influential factors. Our experimental results demonstrate that\nthe designed prompts are effective. In the future, we will investigate automated prompt construction\ntechniques [24].\n7 RELATED WORK\nIn this section, we review the related literature about NL2Vis task, LLMs for code generation, and\nLLMs for data engineering.\n7.1 NL2Vis\nNL2Vis is crucial in data analysis, aiming to generate visualization representations from brief\nsuccinct natural language explanations. While many approaches harnessing rule-based NLP tech-\nniques have been proposed [8, 13, 40] to address this challenge, their effectiveness is limited by\nthe inflexibility of predefined rules in handling open-form natural-language inputs. Owing to the\navailability of large-scale corpora of natural-language descriptions paired with corresponding\nvisualizations, such as nvBench [26], deep-learning-based techniques [25â€“28] have been employed\nto train a sequence-to-sequence model in an end-to-end manner for the NL2Vis task. For example,\nSeq2Vis [27] utilizes an LSTM to encode natural-language queries into hidden states, which are\nsubsequently decoded into visualizations. Moreover, ncNet [28] considers another encoder-decoder\narchitecture, the Transformer [46], to translate natural-language descriptions into visualizations.\nLeveraging the powerful capabilities of ChatGPT [5], Chat2Vis [31] invokes the API interfaces of\ncode-davinci-002 to enable users to create data visualizations using natural-language queries in\nPython plots. More recently, RGVisNet [43] has introduced a retrieval-based model, designed to\nretrieve the most pertinent visualization representation from an extensive visualization codebase\nfor a given natural-language query.\n7.2 LLMs for Code Generation\nCode generation targets the automatic generation of program code from natural-language de-\nscriptions, which can assist developers in improving programming productivity and efficiency.\nIn recent years, LLMs have exhibited remarkable capabilities in generating code [ 3], including\nPython programs, execution commands for Excel, and SQL queries for databases. These models are\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:24 Yang Wu et al.\ngenerally built upon the Transformer architecture and pre-trained on large-scale corpora using\nself-supervised objectives such as masked language modeling and next-sentence prediction [ 7].\nExamples of these models include CodeGen [ 33], CodeT5+ [51], ChatGPT [ 5], StarCoder [ 21],\nand Code Llama [ 39]. To fully harness the zero-shot potential of LLMs, a range of techniques\nhave emerged, including prompt tuning, in-context learning, chain-of-thought, and instruction\ntuning. In particular, in-context learning stands out as a method to fortify LLMs by providing\ncontextual information or illustrative examples, as explored in [20] for code generation. Similarly,\nchain-of-thought is devised to ensure the logical coherence of LLM outputs, thereby enhancing the\nperformance of code generation [19]. Moreover, instruction tuning has been conceived to enhance\nthe generalization prowess of LLMs across various tasks, exemplified by the creation of Wizard-\nCoder [29], which augments the capabilities of StarCoder through the innovative Evol-Instruct\napproach to generate sophisticated code instructions.\n7.3 LLMs for Data Engineering\nIn the field of data engineering and analysis, the integration of LLMs with data-centric tasks has\nopened avenues for transformative approaches to data interaction, processing, and visualization.\nRecently, numerous studies have been developed to weave natural language into tabular data anal-\nysis [14, 17, 18, 54, 55]. For instance, NL2SQL [14, 18] adeptly translates natural language into SQL\ncommands to manipulate relational databases. Additionally, ChatExcel [2] and NL2Formula [56]\nhave leveraged LLMs to generate Excel execution commands, thereby streamlining user interac-\ntions. SheetCopilot [17] has explored translating languages to VBA (Visual Basic for Applications -\nan embedded scripting language in Microsoft Excel), benefiting from a rich array of spreadsheet\nsoftware functionalities. Data-Copilot [55], an LLM-based system, facilitates the automated man-\nagement, invocation, and processing of a substantial volume of data from various sources, crafting\nsophisticated interface tools autonomously. Designed for table analysis, TableGPT [ 54], blends\ntables, natural language, and commands to manipulate data, visualize information, generate analysis\nreports, answer questions, and make predictions.\n8 CONCLUSION AND FUTURE WORK\nIn this paper, we have investigated whether it is feasible to utilize LLMs for the NL2Vis task.\nSpecifically, we compare LLMs including fine-tuned models (e.g., T5-Small, T5-Base) and inference-\nonly models (e.g., text-davinci-002, text-davinci-003, gpt-3.5 -turbo, and gpt-4) against\nthe state-of-the-art models. To start with, we investigate different approaches to transforming the\nstructured tabular data into sequential prompts, so as to feed them into LLMs. Furthermore, we\nevaluate the LLMs on the NL2Vis benchmark under in-domain and cross-domain settings, against\nseveral traditional neural models for NL2Vis. At last, we analyze when the LLMs fail in NL2Vis,\nand propose to iteratively update the results using strategies such as chain-of-thought, role-playing,\nand code-interpreter.\nIn our future work, we plan to explore table representations by encoding tables with more\nwell-designed programming languages. Moreover, we will extend the benchmarks to implicit and\nmulti-type table structures for visualization in more applicable scenarios, to push forward the field\nof NL2Vis. While our current focus is on few-shot NL2Vis in GPT-3.5, our framework can also\nbe applied to other LLMs, such as LLaMA [45]. Consequently, the application of our prompting\nmethods to multiple semantic parsing tasks represents an intriguing avenue for future exploration.\nACKNOWLEDGMENTS\nThis work is supported by the Major Program (JD) of Hubei Province (Grant No. 2023BAA024). We\nwould like to thank all the anonymous reviewers for their insightful comments.\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:25\nREFERENCES\n[1] [n. d.]. Amazonâ€™s QuickSight. https://aws.amazon.com/cn/blogs/aws/amazon-quicksight-q-to-answer-ad-hoc-\nbusiness-questions.\n[2] [n. d.]. ChatExcel. https://chatexcel.com.\n[3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,\nNiladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John\nHewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,\nXuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa,\nSuraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher\nPotts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher\nRÃ©, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori,\nArmin W. Thomas, Florian TramÃ¨r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael\nXie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 (2021).\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language Models are Few-Shot Learners. InProceedings of the Advances in Neural Information\nProcessing Systems , Vol. 33. 1877â€“1901.\n[5] ChatGPT. 2022. ChatGPT. https://openai.com/blog/chatgpt.\n[6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement\nLearning from Human Preferences. In Advances in Neural Information Processing Systems , Vol. 30.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171â€“4186.\n[8] Tong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, and Karrie G. Karahalios. 2015. DataTone: Managing Ambiguity\nin Natural Language Interfaces for Data Visualization. In Proceedings of the 28th Annual ACM Symposium on User\nInterface Software & Technology (Charlotte, NC, USA) (UIST â€™15) . Association for Computing Machinery, New York, NY,\nUSA, 489â€“500.\n[9] Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. TableGPT: Few-shot Table-\nto-Text Generation with Table Structure Reconstruction and Content Matching. InProceedings of the 28th International\nConference on Computational Linguistics . International Committee on Computational Linguistics, Barcelona, Spain\n(Online), 1978â€“1988.\n[10] GPT3.5. 2023. GPT3.5. https://platform.openai.com/docs/models/gpt-3-5.\n[11] GPT4. 2023. GPT4. https://openai.com/research/gpt-4.\n[12] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. 2023. From\nImages to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10867â€“10877.\n[13] Enamul Hoque, Vidya Setlur, Melanie Tory, and Isaac Dykeman. 2017. Applying pragmatics principles for interaction\nwith visual analytics. IEEE Transactions on Visualization and Computer Graphics 24, 1 (2017), 309â€“318.\n[14] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. ChatDB: Augmenting LLMs with\nDatabases as Their Symbolic Memory. arXiv preprint arXiv:2306.03901 (2023).\n[15] Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural language to SQL: Where are we\ntoday? Proceedings of the VLDB Endowment 13, 10 (2020), 1737â€“1750.\n[16] Chuan Li. 2020. Demystifying gpt-3 language model: A technical overview. https://lambdalabs.com/blog/demystifying-\ngpt-3. [Online; accessed 1-Aug-2022].\n[17] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. 2023. SheetCopilot: Bringing Software Produc-\ntivity to the Next Level through Large Language Models. arXiv preprint arXiv:2305.19308 (2023).\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:26 Yang Wu et al.\n[18] Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and\nYongbin Li. 2023. Graphix-T5: mixing pre-trained transformers with graph-aware layers for text-to-SQL parsing. In\nProceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative\nApplications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence\n(AAAIâ€™23/IAAIâ€™23/EAAIâ€™23). AAAI Press, Article 1467, 9 pages.\n[19] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Enabling Programming Thinking in Large Language Models Toward Code\nGeneration. CoRR abs/2305.06599 (2023).\n[20] Jia Li, Ge Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, and Zhi Jin. 2023. Large Language Model-Aware\nIn-Context Learning for Code Generation. CoRR abs/2310.09748 (2023).\n[21] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\nChristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier\nDehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoÃ£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel\nZebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,\nNour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan\nEbert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Sean Hughes, Thomas\nWolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv\npreprint arXiv:2305.06161 (2023).\n[22] Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. 2023. A comprehensive evaluation of ChatGPTâ€™s zero-shot\nText-to-SQL capability. arXiv preprint arXiv:2303.13547 (2023).\n[23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is Your Code Generated by ChatGPT Really\nCorrect? Rigorous Evaluation of Large Language Models for Code Generation. In Advances in Neural Information\nProcessing Systems , Vol. 36. 21558â€“21572.\n[24] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, Prompt,\nand Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Comput. Surv. 55, 9,\nArticle 195 (jan 2023), 35 pages.\n[25] Yuyu Luo, Xuedi Qin, Nan Tang, Guoliang Li, and Xinran Wang. 2018. DeepEye: Creating Good Data Visualizations\nby Keyword Search. In Proceedings of the 2018 International Conference on Management of Data (Houston, TX, USA)\n(SIGMOD â€™18) . Association for Computing Machinery, New York, NY, USA, 1733â€“1736.\n[26] Yuyu Luo, Jiawei Tang, and Guoliang Li. 2021. nvBench: A large-scale synthesized dataset for cross-domain natural\nlanguage to visualization task. arXiv preprint arXiv:2112.12926 (2021).\n[27] Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. 2021. Synthesizing Natural Language\nto Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks. In Proceedings of the 2021 International Conference\non Management of Data (Virtual Event, China) (SIGMOD â€™21) . Association for Computing Machinery, New York, NY,\nUSA, 1235â€“1247.\n[28] Yuyu Luo, Nan Tang, Guoliang Li, Jiawei Tang, Chengliang Chai, and Xuedi Qin. 2022. Natural Language to Visualization\nby Neural Machine Translation. IEEE Transactions on Visualization and Computer Graphics 28, 1 (2022), 217â€“226.\n[29] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and\nDaxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. CoRR abs/2306.08568\n(2023).\n[30] Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023. New trends in machine translation using large language models:\nCase examples with chatgpt. arXiv preprint arXiv:2305.01181 (2023).\n[31] Paula Maddigan and Teo Susnjak. 2023. Chat2vis: Fine-tuning data visualisations using multilingual natural language\ntext and pre-trained large language models. arXiv preprint arXiv:2303.14292 (2023).\n[32] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014.\nThe Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations . 55â€“60.\n[33] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\n(2022).\n[34] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,\nPaul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim\nBrooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\nAutomated Data Visualization from Natural Language via Large Language Models: An Exploratory Study 115:27\nRory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark\nChen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila\nDunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha\nGontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei\nGuo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey,\nWade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain,\nShawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun,\nTomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,\nJong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Åukasz\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai\nLan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv\nMarkovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey,\nPaul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly, Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen\nOâ€™Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy\nParparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power,\nBoris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real,\nKendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica\nShieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian\nSohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie\nTang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick\nTurley, Jerry Tworek, Juan Felipe CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,\nJiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,\nRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and\nBarret Zoph. 2023. Gpt-4 technical report.\n[35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow\ninstructions with human feedback. In Advances in Neural Information Processing Systems , Vol. 35. 27730â€“27744.\n[36] Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang. 2023. LLM is Like a Box of Chocolates: the Non-\ndeterminism of ChatGPT in Code Generation. arXiv preprint arXiv:2308.02828 (2023).\n[37] Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with\nSelf-Correction. In Proceedings of the Advances in Neural Information Processing Systems , Vol. 36. 36339â€“36348.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of\nMachine Learning Research 21, 140 (2020), 1â€“67.\n[39] Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain\nSauvestre, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton\nFerrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin,\nNicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950 (2023).\n[40] Vidya Setlur, Sarah E. Battersby, Melanie Tory, Rich Gossweiler, and Angel X. Chang. 2016. Eviza: A natural language\ninterface for visual analysis. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology .\n365â€“377.\n[41] Vidya Setlur, Melanie Tory, and Alex Djalali. 2019. Inferencing underspecified natural language utterances in visual\nanalysis. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California)\n(IUI â€™19) . Association for Computing Machinery, New York, NY, USA, 40â€“51.\n[42] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional Generalization and\nNatural Language Variation: Can a Semantic Parsing Approach Handle Both?. In Proceedings of the 59th Annual\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024.\n115:28 Yang Wu et al.\nMeeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) . Association for Computational Linguistics, Online, 922â€“938.\n[43] Yuanfeng Song, Xuefang Zhao, Raymond Chi-Wing Wong, and Di Jiang. 2022. RGVisNet: A Hybrid Retrieval-\nGeneration Neural Framework Towards Automatic Data Visualization Generation. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD â€™22) . Association for\nComputing Machinery, New York, NY, USA, 1646â€“1655.\n[44] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023. Evaluation of ChatGPT as a\nquestion answering system for answering complex questions. arXiv preprint arXiv:2303.07992 (2023).\n[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste\nRoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Å. ukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is All you Need. In Proceedings of the Advances in Neural Information Processing Systems ,\nVol. 30.\n[47] Randle Aaron M. Villanueva and Zhuo Job Chen. 2019. ggplot2: Elegant Graphics for Data Analysis (2nd ed.).\nMeasurement: Interdisciplinary Research and Perspectives 17, 3 (2019), 160â€“167.\n[48] Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui, Hongyu Zhang, Kazuma Hashimoto, Hai Jin, Guandong\nXu, Caiming Xiong, et al. 2022. NaturalCC: an open-source toolkit for code intelligence. In Proceedings of the ACM/IEEE\n44th International Conference on Software Engineering: Companion Proceedings . 149â€“153.\n[49] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-\nLevel Machine Translation with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing . Association for Computational Linguistics, Singapore, 16646â€“16661.\n[50] Xingyao Wang, Sha Li, and Heng Ji. 2023. Code4Struct: Code Generation for Few-Shot Event Structure Prediction.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, Toronto, Canada, 3640â€“3663.\n[51] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\nDecoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,\n8696â€“8708.\n[52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny\nZhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Proceedings of the Advances\nin Neural Information Processing Systems , Vol. 35. 24824â€“24837.\n[53] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\nRoman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887 (2018).\n[54] Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng\nSu, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming\nZhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao. 2023. TableGPT: Towards Unifying Tables,\nNature Language and Commands into One GPT. arXiv preprint arXiv:2307.08674 (2023).\n[55] Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. 2023. Data-Copilot: Bridging Billions of Data and\nHumans with Autonomous Workflow. arXiv preprint arXiv:2306.07209 (2023).\n[56] Wei Zhao, Zhitao Hou, Siyuan Wu, Yang Gao, Haoyu Dong, Yao Wan, Hongyu Zhang, Yulei Sui, and Haidong Zhang.\n2024. NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries. In Findings of the Association\nfor Computational Linguistics: EACL 2024, St. Julianâ€™s, Malta, March 17-22, 2024 . 2377â€“2388.\n[57] Li Zhong and Zilong Wang. 2023. A Study on Robustness and Reliability of Large Language Model Code Generation.\narXiv preprint arXiv:2308.10335 (2023).\n[58] Jonathan Zong, Josh Pollock, Dylan Wootton, and Arvind Satyanarayan. 2023. Animated Vega-Lite: Unifying Animation\nwith a Grammar of Interactive Graphics.IEEE Transactions on Visualization and Computer Graphics 29, 1 (2023), 149â€“159.\nReceived October 2023; accepted January 2024\nProc. ACM Manag. Data, Vol. 2, No. 3 (SIGMOD), Article 115. Publication date: June 2024."
}