{
  "title": "Evaluation and comparison of large language models’ responses to questions related optic neuritis",
  "url": "https://openalex.org/W4411661487",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5111237964",
      "name": "Han-Jie He",
      "affiliations": [
        "Shantou University Medical College",
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2099265988",
      "name": "Fang Fang Zhao",
      "affiliations": [
        "Shantou University",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2600839916",
      "name": "Jia-Jian Liang",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2099835616",
      "name": "Yun Wang",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2243174462",
      "name": "Qian-Qian He",
      "affiliations": [
        "Shantou University Medical College",
        "Shantou University",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2289660170",
      "name": "Hongjie Lin",
      "affiliations": [
        "Shantou University",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5114216000",
      "name": "Jingyun Cen",
      "affiliations": [
        "Shaoguan University"
      ]
    },
    {
      "id": "https://openalex.org/A2102374411",
      "name": "Feifei Chen",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2243877722",
      "name": "Tai-Ping Li",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A3111183397",
      "name": "Zhanchi Hu",
      "affiliations": [
        "Dongguan People’s Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2123637286",
      "name": "Jian Feng Yang",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2102862176",
      "name": "Lan Chen",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Shantou University"
      ]
    },
    {
      "id": "https://openalex.org/A2532350796",
      "name": "Carol Y Cheung",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": [
        "National University of Singapore",
        "National University Health System",
        "Singapore Eye Research Institute",
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A2705108897",
      "name": "Ling-Ping Cen",
      "affiliations": [
        "Guangdong Medical College",
        "Shantou University Medical College",
        "Chinese University of Hong Kong",
        "Shantou University",
        "Dongguan People’s Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5111237964",
      "name": "Han-Jie He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099265988",
      "name": "Fang Fang Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2600839916",
      "name": "Jia-Jian Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099835616",
      "name": "Yun Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2243174462",
      "name": "Qian-Qian He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2289660170",
      "name": "Hongjie Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114216000",
      "name": "Jingyun Cen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102374411",
      "name": "Feifei Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2243877722",
      "name": "Tai-Ping Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3111183397",
      "name": "Zhanchi Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123637286",
      "name": "Jian Feng Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102862176",
      "name": "Lan Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2532350796",
      "name": "Carol Y Cheung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2705108897",
      "name": "Ling-Ping Cen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4382393818",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W4392938070",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4388287351",
    "https://openalex.org/W4386596026",
    "https://openalex.org/W4392462807",
    "https://openalex.org/W4391683454",
    "https://openalex.org/W4391262681",
    "https://openalex.org/W4386726071",
    "https://openalex.org/W4394760737",
    "https://openalex.org/W4389274564",
    "https://openalex.org/W4391045702",
    "https://openalex.org/W4386046428",
    "https://openalex.org/W4389560173",
    "https://openalex.org/W4376956223",
    "https://openalex.org/W4394763201",
    "https://openalex.org/W3143550304",
    "https://openalex.org/W4387440167",
    "https://openalex.org/W4390578994",
    "https://openalex.org/W4393408508",
    "https://openalex.org/W4403467309",
    "https://openalex.org/W1968894666",
    "https://openalex.org/W4376114558",
    "https://openalex.org/W6855841075",
    "https://openalex.org/W4393961018",
    "https://openalex.org/W4386110374",
    "https://openalex.org/W4392862323",
    "https://openalex.org/W4385766819"
  ],
  "abstract": "Objectives Large language models (LLMs) show promise as clinical consultation tools and may assist optic neuritis patients, though research on their performance in this area is limited. Our study aims to assess and compare the performance of four commonly used LLM-Chatbots—Claude-2, ChatGPT-3.5, ChatGPT-4.0, and Google Bard—in addressing questions related to optic neuritis. Methods We curated 24 optic neuritis-related questions and had three ophthalmologists rate the responses on two three-point scales for accuracy and comprehensiveness. We also assessed readability using four scales. The final results showed performance differences among the four LLM-Chatbots. Results The average total accuracy scores (out of 9): ChatGPT-4.0 (7.62 ± 0.86), Google Bard (7.42 ± 1.20), ChatGPT-3.5 (7.21 ± 0.70), Claude-2 (6.44 ± 1.07). ChatGPT-4.0 ( p = 0.0006) and Google Bard ( p = 0.0015) were significantly more accurate than Claude-2. Also, 62.5% of ChatGPT-4.0’s responses were rated “Excellent,” followed by 58.3% for Google Bard, both higher than Claude-2’s 29.2% (all p ≤ 0.042) and ChatGPT-3.5’s 41.7%. Both Claude-2 and Google Bard had 8.3% “Deficient” responses. The comprehensiveness scores were similar among the four LLMs ( p = 0.1531). Note that all responses require at least a university-level reading proficiency. Conclusion Large language models-Chatbots hold immense potential as clinical consultation tools for optic neuritis, but they require further refinement and proper evaluation strategies before deployment to ensure reliable and accurate performance.",
  "full_text": "fmed-12-1516442 June 23, 2025 Time: 14:24 # 1\nTYPE Original Research\nPUBLISHED 25 June 2025\nDOI 10.3389/fmed.2025.1516442\nOPEN ACCESS\nEDITED BY\nWeihua Yang,\nSouthern Medical University, China\nREVIEWED BY\nXiaolai Zhou,\nSun Yat-sen University, China\nShulin Liu,\nFirst Afﬁliated Hospital of Chongqing Medical\nUniversity, China\nYuqing Chen,\nSecond Military Medical University, China\n*CORRESPONDENCE\nLing-Ping Cen\ncenlp@hotmail.com\n†These authors have contributed equally to\nthis work\nRECEIVED 24 October 2024\nACCEPTED 29 May 2025\nPUBLISHED 25 June 2025\nCITATION\nHe H-J, Zhao F-F, Liang J-J, Wang Y,\nHe Q-Q, Lin H, Cen J, Chen F, Li T-P, Hu Z,\nYang J-F, Chen L, Cheung CY, Tham Y-C and\nCen L-P (2025) Evaluation and comparison\nof large language models’ responses\nto questions related optic neuritis.\nFront. Med.12:1516442.\ndoi: 10.3389/fmed.2025.1516442\nCOPYRIGHT\n© 2025 He, Zhao, Liang, Wang, He, Lin, Cen,\nChen, Li, Hu, Yang, Chen, Cheung, Tham and\nCen. This is an open-access article\ndistributed under the terms of the Creative\nCommons Attribution License (CC BY). The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nEvaluation and comparison of\nlarge language models’\nresponses to questions related\noptic neuritis\nHan-Jie He1,2†, Fang-Fang Zhao1†, Jia-Jian Liang1, Yun Wang1,\nQian-Qian He1,2, Hongjie Lin1, Jingyun Cen3, Feifei Chen1,\nTai-Ping Li1, Zhanchi Hu4, Jian-Feng Yang1, Lan Chen1,\nCarol Y. Cheung5, Yih-Chung Tham6,7,8 and Ling-Ping Cen1,2,4,9*\n1Joint Shantou International Eye Center of Shantou University and The Chinese University\nof Hong Kong, Shantou, Guangdong, China, 2Shantou University Medical College, Shantou,\nGuangdong, China, 3Shaoguan University Medical College, Shaoguan, Guangdong, China, 4Dongguan\nGuangming Eye Hospital, Dongguan, Guangdong, China, 5Department of Ophthalmology and Visual\nSciences, The Chinese University of Hong Kong, Hong Kong SAR, China, 6Yong Loo Lin School\nof Medicine, National University of Singapore, Singapore, Singapore, 7Centre of Innovation\nand Precision Eye Health, Department of Ophthalmology, Yong Loo Lin School of Medicine, National\nUniversity of Singapore and National University Health System, Singapore, Singapore, 8Singapore Eye\nResearch Institute, Singapore National Eye Centre, Singapore, Singapore, 9Guangdong Provincial Key\nLaboratory of Medical Immunology and Molecular Diagnostics, The First Dongguan Afﬁliated Hospital,\nSchool of Medical Technology, Guangdong Medical University, Dongguan, China\nObjectives: Large language models (LLMs) show promise as clinical consultation\ntools and may assist optic neuritis patients, though research on their\nperformance in this area is limited. Our study aims to assess and\ncompare the performance of four commonly used LLM-Chatbots—Claude-2,\nChatGPT-3.5, ChatGPT-4.0, and Google Bard—in addressing questions related\nto optic neuritis.\nMethods: We curated 24 optic neuritis-related questions and had three\nophthalmologists rate the responses on two three-point scales for accuracy\nand comprehensiveness. We also assessed readability using four scales.\nThe ﬁnal results showed performance differences among the four LLM-\nChatbots.\nResults: The average total accuracy scores (out of 9): ChatGPT-4.0\n(7.62 ± 0.86), Google Bard (7.42 ± 1.20), ChatGPT-3.5 (7.21 ± 0.70),\nClaude-2 (6.44 ± 1.07). ChatGPT-4.0 ( p = 0.0006) and Google Bard\n(p = 0.0015) were signiﬁcantly more accurate than Claude-2. Also, 62.5%\nof ChatGPT-4.0’s responses were rated “Excellent,” followed by 58.3% for\nGoogle Bard, both higher than Claude-2’s 29.2% (all p ≤ 0.042) and\nChatGPT-3.5’s 41.7%. Both Claude-2 and Google Bard had 8.3% “Deﬁcient”\nresponses. The comprehensiveness scores were similar among the four LLMs\n(p = 0.1531). Note that all responses require at least a university-level\nreading proﬁciency.\nFrontiers in Medicine 01 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 2\nHe et al. 10.3389/fmed.2025.1516442\nConclusion: Large language models-Chatbots hold immense potential as\nclinical consultation tools for optic neuritis, but they require further reﬁnement\nand proper evaluation strategies before deployment to ensure reliable and\naccurate performance.\nKEYWORDS\neye diseases, optic nerve diseases, optic neuritis, artiﬁcial intelligence, natural language\nprocessing\n1 Introduction\nRecent advancements in artiﬁcial intelligence (AI) have\nunlocked limitless possibilities for transforming medicine. Thanks\nto machine learning and deep learning technologies, AI has\nshown tremendous potential in healthcare (1). Currently, key\napplications of AI in medicine include enhancing interaction\nand communication, improving image recognition, supporting\ndiagnostics and nursing, optimizing healthcare management and\nadministrative processes, and assisting in surgeries and drug\ndevelopment.\nLarge language models (LLMs) are AI systems based on\nneural network architectures and use deep learning models, trained\non extensive databases for natural language processing tasks.\nThese models possess human-like language capabilities, oﬀering\nsubstantial beneﬁts to healthcare professionals and patients (2).\nAmong them, ChatGPT, a generative AI developed by OpenAI\n(San Francisco, CA, United States), stands out for its widespread\nuse as an AI chatbot by the general public. It leverages vast\namounts of internet text data to produce coherent responses\ntailored to speciﬁc inputs (3). Unlike traditional search engines,\nChatGPT and similar chatbots excel in simplicity, speciﬁcity, and\ninteractivity, sparking increasing interest in their potential for\nmedical consultations (4).\nThe application of large language models in ophthalmology is\nincreasingly prevalent. Chatbots are utilized to assess proﬁciency\nin ophthalmology (4–8), educate clinical medical students, (9)\nassist in diagnostic processes for clinicians (10, 11), perform image\ndiagnostics (12), manage clinical electronic records (13), educate\npatients (14) and aid in personalized patient management (14, 15).\nNotably, one study showed that ChatGPT achieved an accuracy rate\nabove 90% on the Taiwanese medical licensing examination, yet it\nexhibited the highest error rate (28.95%) in ophthalmology-related\nquestions (8). Moreover, while ChatGPT performed adequately in\ngeneral ophthalmology queries, it showed weaknesses in neuro-\nophthalmology and ocular pathology (5). The eﬃcacy of LLM-\nChatbots in addressing optic neuritis-speciﬁc questions remains\nunexplored. Given their potential role as assistants to doctors and\npatients, particularly in neuro-ophthalmology, there is a pressing\nneed for further exploration of chatbots in this ﬁeld.\nAlthough the incidence of optic neuritis varies across regions\nand ethnic groups, it is reported that there are still about 4–\n8 per 100,000 person years globally (16). Due to the specialized\nexpertise required to diagnose and treat optic neuritis, many\npatients struggle to access timely medical consultations with well-\ntrained doctors. In such cases, more accessible alternatives like\nonline consultations or chatbots become increasingly appealing\n(17). However, the eﬀectiveness of chatbots in managing optic\nneuritis-related inquiries needs thorough evaluation.\nIn this study, we will compare the accuracy,\ncomprehensiveness, and readability of four widely used and\nopenly accessible LLM-Chatbots—ChatGPT-3.5, ChatGPT-4.0,\nGoogle Bard (now updated to Google Gemini), and Claude-2 (now\nupdated to Claude-3) in clinical consultations for optic neuritis.\nOur ﬁndings will provide valuable insights into the eﬀectiveness of\nLLM-Chatbots in clinical consultations for optic neuritis.\n2 Materials and methods\nIn this study, we compared the accuracy, comprehensiveness,\nand readability of responses to optic neuritis-related questions\ngenerated by four LLM-Chatbots. The study was conducted from\n23 January 2024 to 4 March 2024. Given that the study did not\ninvolve any patients or animals, approval from an ethics committee\nwas not required.\n2.1 Study design\nQuestions on optic neuritis were collaboratively developed by\nclinical ophthalmologists (LPC, HJH, FFZ), based on common\nissues faced by patients in clinical settings, frequently asked\nquestions on online platforms, and authoritative information from\nesteemed health websites, such as the National Eye Institute\nand the American Academy of Ophthalmology (18, 19). Upon\naggregating all relevant information, the three doctors, leveraging\ntheir clinical experience, consolidated 24 questions related to\noptic neuritis, categorized into four groups: general, diagnosis,\ntreatment, and follow-up and prevention. This categorization\naimed to assess the varied performance of diﬀerent LLM-Chatbots\nacross question types. For the study, conducted from 23 January\nto 30 January 2024, we utilized four LLMs: Claude-2 (Anthropic,\nSan Francisco, California), ChatGPT (versions GPT-3.5 and GPT-\n4.0, OpenAI, San Francisco, California), and Google Bard (Google,\nMountain View, California). While Claude-2, ChatGPT-3.5, and\nGoogle Bard are freely available, ChatGPT-4.0 requires a paid\nsubscription. Nevertheless, given its enhanced performance in\nneuro-ophthalmology and its relative aﬀordability and ease of use\nfor patients, ChatGPT-4.0 was included in our study (5, 7).\nEach of the 24 optic neuritis-related questions was directly\nentered into four LLM-Chatbots using separate, newly opened\nwindows to prevent interference and preserve response integrity,\nFrontiers in Medicine 02 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 3\nHe et al. 10.3389/fmed.2025.1516442\nwith no specialized prompts employed in the process. A research\nmember (HJH) uniformly collected all responses (Supplementary\nTables 1–4), formatted them into plain text, and removed any\nidentiﬁable features of each LLM-Chatbot without altering the\nmain content. This ensured that evaluators could not determine\nwhich LLM-Chatbot produced the replies. To minimize potential\nbiases across evaluations, three rounds of accuracy assessment were\nconducted, spaced 48 hours apart, with the sequence of responses\nrearranged before each round.\n2.2 Evaluation of readability\nFour validated readability scales (Supplementary Table 5)\nwere used to assess responses from all LLM-Chatbots to optic\nneuritis-related questions (20–22), as well as authoritative online\nconsultations accessible to patients, including content from\nthe Mayo Clinic, Cleveland Clinic, and American Academy\nof Ophthalmology. These assessments utilized the Gunning\nFog Index, Flesch-Kincaid Grade Level, Simple Measure of\nGobbledygook (SMOG) score, and Coleman-Liau Index. Each\nof these scales measures word length, syntactic complexity, and\nsentence length, assigning a United States academic grade level\nnecessary for comprehension. We calculated the readability scores\nfor each response using a freely available online tool (23).\n2.3 Evaluation of accuracy\nThe assessment team was made up of three neuro-\nophthalmologists (FFZ, TPL, YW), each boasting at least 6 years\nof clinical experience. The assessment team was made up of\nthree neuro-ophthalmologists (FFZ, TPL, YW), each boasting at\nleast 6 years of clinical experience. Although the evaluators are\nnon-native English speakers and may have certain shortcomings\nin understanding the linguistic nuances and cultural background\nof English content, the LLM-generated output on optic neuritis\nis predominantly medical (involving fewer complex cultural or\nidiomatic language issues), and importantly, all three hold medical\nmaster’s degrees and possess strong proﬁciency in ophthalmic\nmedical English. They routinely use professional English in their\nclinical practice, which qualiﬁes them to serve as evaluators. To\nensure impartiality, these evaluators were not informed beforehand\nwhich LLM-Chatbot provided the responses. They independently\nassessed the accuracy of the replies using a three-point scale:\n1. “Deﬁcient”: Indicates responses that could signiﬁcantly\nmislead and potentially harm patients due to inaccuracies.\n2. “Marginal”: Signiﬁes responses that contain possible factual\nerrors, but with a lower risk of misleading or harming patients.\n3. “Excellent”: Represents responses that are free from errors.\nRatings were determined based on a majority rule approach,\nwhere the responses from the LLM-Chatbots were assigned a\nrating after the majority rule was applied in each assessment\nround. If a discrepancy occurred among the three grading doctors’\nopinions, the response was classiﬁed as “Pending.” After three\nevaluation rounds, each reply received three ratings. A ﬁnal rating\nwas established through the majority rule process (Supplementary\nTable 7). If this ﬁnal rating remained “Pending, ” a senior doctor\n(LPC) would then provide a conclusive rating.\n2.4 Evaluation of comprehensiveness\nResponses from chatbots deemed “Excellent” in accuracy\nwill undergo further evaluation for comprehensiveness by the\nassessment panel. The evaluation utilizes a three-tiered scale:\n1. Incomplete: Responses lack crucial key information necessary\nfor completeness.\n2. Comprehensive: Responses include all essential key\ninformation required.\n3. Highly Comprehensive: Responses not only provide all key\ninformation but also include additional useful details that\nwere not anticipated.\n2.5 Statistical analysis\nStatistical analyses for this study were conducted using\nGraphPad Prism (version 8.3.0). Descriptive statistics are presented\nas mean values and standard deviations (SD). For the parametric\ndata, readability scores were analyzed using one-way ANOV A\nfollowed by Tukey’s multiple comparison post-hoc test across\nthe four LLM-Chatbots and the overall web content. For non-\nparametric data, the Kruskal-Wallis Rank Sum test and Dunn’s\nmultiple comparison post-hoc test were employed to evaluate\nthe total accuracy and comprehensiveness scores across the four\nmodels. Additionally, a two-tailed Pearson’s χ2 was used to assess\nthe distribution of accuracy ratings among the chatbots. The\nBonferroni correction method was applied to adjust p-values\nfor multiple comparisons, with a p-value below 0.05 considered\nstatistically signiﬁcant.\n3 Results\n3.1 Summary of response lengths\nTable 1 presents the responses of all LLMs to optic neuritis\nquestions. The average word count ± SD was: Claude-2\n(220.29 ± 20.88), ChatGPT-3.5 (238.75 ± 71.36), Google Bard\n(299.75 ± 99.41), and ChatGPT-4.0 (269.25 ± 62.96). The average\ncharacter count ± SD was: Claude-2 (1181.46± 140.96), ChatGPT-\n3.5 (1316.46 ± 410.83), Google Bard (1711.96 ± 576.22), and\nChatGPT-4.0 (1461.63±348.74). The average sentence count±SD\nwas: Claude-2 (15.42 ± 3.12), ChatGPT-3.5 (13.25 ± 4.08), Google\nBard (15.46 ± 5.41), and ChatGPT-4.0 (13.83 ± 4.10).\n3.2 Readability\nFigure 1 shows the average readability scores (Gunning\nFog, Flesch-Kincaid, Coleman-Liau and SMOG; see\nFrontiers in Medicine 03 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 4\nHe et al. 10.3389/fmed.2025.1516442\nTABLE 1 Overview of response length from large language models (LLM)-Chatbots to optic neuritis-related questions.\nLLM Response (words) Response (characters) Response (sentences)\nMean (SD) Minimum Maximum Mean (SD) Minimum Maximum Mean (SD) Minimum Maximum\nClaude-2 220.29 (20.88) 171 258 1181.46 (140.94) 904 1453 15.42 (3.12) 11 22\nChatGPT-3.5 238.75 (71.36) 127 413 1316.46 (410.83) 729 2227 13.25 (4.08) 7 20\nGoogle Bard 299.75 (99.41) 41 470 1711.96 (576.22) 214 2569 15.46 (5.41) 3 23\nChatGPT-4.0 269.25 (62.96) 144 390 1461.63 (348.74) 722 2096 13.83 (4.10) 7 26\nSupplementary Table 6) for LLMs’ responses and professional\nweb content on optic neuritis. The LLMs’ averages were: Claude-\n2, 12.47 ± 1.93; Google Bard, 13.64 ± 2.33; ChatGPT-4.0,\n14.75 ± 2.11; and ChatGPT-3.5, 15.37 ± 1.60—all at the college\nlevel. The web content averaged 11.41 ± 1.73 (college level)\n(one-way ANOV A, p = 0.0188). ChatGPT-3.5 (Tukey’s post hoc,\np = 0.0172) and ChatGPT-4.0 (Tukey’s post hoc, p = 0.0407) had\nsigniﬁcantly higher readability scores than the web content.\n3.3 Accuracy\nFigure 2 depicts the average overall accuracy scores for optic\nneuritis responses from each LLM, as rated by three neuro-\nophthalmologists over three rounds. ChatGPT-4.0 scored highest\n(7.62 ± 0.86), signiﬁcantly outperforming Claude-2 (6.44 ± 1.07,\nDunn’s post-hoc test, p = 0.0006). Google Bard ranked second\n(7.42 ± 1.20; p = 0.0015 compared to Claude-2), followed by\nChatGPT-3.5 (7.21 ± 0.70). Detailed scores for each question are\nin Supplementary Table 7.\nFigure 3 presents the ﬁnal ratings for optic neuritis responses\nfrom each LLM after three rounds. ChatGPT-4.0 had 62.5%\n“Excellent” responses and Google Bard had 58.3%, both\nsigniﬁcantly higher than Claude-2’s 29.2% (Pearson’s chi-squared\ntest, all p ≤ 0.042). ChatGPT-3.5 had 41.7% “Excellent.” “Deﬁcient”\nratings: 8.3% for Claude-2 and Google Bard, compared to 0%\nfor both ChatGPT models. Detailed ratings for each LLM are in\nSupplementary Table 7.\nTable 2 illustrates the rating distributions for LLMs’ responses\nto optic neuritis. ChatGPT-4.0 excelled in all categories (0%\n“Deﬁcient”). ChatGPT-3.5 had no “Deﬁcient” ratings but more\n“Marginal” ratings. Google Bard performed well in diagnosis\nand follow-up and prevention but had “Deﬁcient” ratings in\ngeneral and treatment. Claude-2 showed multiple Marginal ratings,\ntwo Deﬁcient in treatment and no “Excellent” in follow-up and\nprevention.\n3.4 Comprehensiveness\nSupplementary Table 8 shows “Excellent” response\ncomprehensiveness scores. All chatbots performed similarly:\nClaude-2 (2.67 ± 0.34), ChatGPT-3.5 (2.43 ± 0.39), Google Bard\n(2.71 ± 0.34), and ChatGPT-4.0 (2.74 ± 0.19). No signiﬁcant\ndiﬀerences were found (Kruskal-Wallis test, p = 0.1531).\n4 Discussion\nOur study conducted a rigorous evaluation of four widely-\nused LLM-Chatbots—Claude-2, ChatGPT-3.5, Google Bard,\nand ChatGPT-4.0—on their handling of optic neuritis-related\nquestions. We sourced common questions from multiple\nvenues and had them input systematically into the chatbots\nby professional neuro-ophthalmologists. Responses were\nanonymized and randomized before being assessed across\nthree rounds by experienced doctors, with senior doctors\nresolving any inconsistencies. Responses rated as “Excellent” were\nFrontiers in Medicine 04 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 5\nHe et al. 10.3389/fmed.2025.1516442\nFIGURE 1\nAverage total readability scores of responses generated by large language models (LLM)-Chatbots and ofﬁcial website content. *P ≤ 0.05.\nFIGURE 2\nAverage total accuracy scores of responses generated by large language models (LLM)-Chatbots. **P ≤ 0.01; ***P ≤ 0.001.\nfurther examined for comprehensiveness. We also evaluated the\nreadability of outputs from the LLM-Chatbots and established\nmedical websites using an online tool. While previous research\nhas explored LLM-Chatbots’ role in neuro-ophthalmology,\nsuch as producing patient handouts (24), comparing their\nresponses with human experts on neuro-ophthalmology issues\nFrontiers in Medicine 05 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 6\nHe et al. 10.3389/fmed.2025.1516442\nFIGURE 3\nFinal rating of responses generated by large language models (LLM)-Chatbots determined by the majority rule.\n(25), and neuro-ophthalmic disease diagnosis (26), no prior\nstudies have evaluated these chatbots on the three key aspects\nof accuracy, comprehensiveness, and readability for optic\nneuritis-speciﬁc questions—essentially what online patients\nneed the most. Our ﬁndings could signiﬁcantly enhance the\nuse of LLM-Chatbots in neuro-ophthalmology, potentially\nestablishing them as a new avenue for online consultations on optic\nneuritis, thereby underscoring our study’s substantial practical\nimportance.\nRegarding readability, both LLM-Chatbot responses and the\ncontent from accessible authoritative websites require a college-\nlevel reading proﬁciency, as indicated in Supplementary Table 6.\nThis is considerably higher than the sixth-grade or lower level\nrecommended by The American Medical Association (AMA)\n(27). This discrepancy echoes previous ﬁndings where online\npatient education materials (PEMs) on major ophthalmology\nwebsites signiﬁcantly exceeded recommended reading levels (28).\nPoor readability of LLM-generated responses will diminish their\nutility in optic nerve clinical consultations, as patients who\ncannot comprehend the information—even if highly accurate—\ncannot beneﬁt from it. For patients with lower health literacy,\nlow-readability responses may lead to misunderstandings of\nmedical information and even delay treatment. The relatively\npoor readability of LLM-Chatbots compared to standard PEMs\nmay be attributed to the LLMs being trained on vast databases,\nincluding texts from specialized ophthalmology websites (1).\nMoreover, the highly specialized and somewhat niche nature of\noptic neuritis-related content means that LLMs trained with such\ninformation undoubtedly necessitate a higher reading level. Our\nresearch highlights the challenges LLMs face in balancing accuracy\nand readability. For instance, Claude-2 has lower accuracy but\nbetter readability, while ChatGPT-4.0 is the opposite. Contrary\nto other studies suggesting ChatGPT-4.0’s superior readability\namong LLMs, our ﬁndings suggest otherwise (20). Given that\nLLMs have the potential to simplify complex information,\npatients with lower educational levels could beneﬁt by requesting\nsimpliﬁed responses, thus maintaining content quality while\nmaking it more accessible (20, 22). This approach could be\nparticularly useful for patients using LLM tools to address\noptic neuritis-related inquiries, guiding them in leveraging these\ntechnologies eﬀectively.\nIn addressing questions related to optic neuritis, ChatGPT-\n4.0 demonstrates a signiﬁcant advantage, achieving the highest\naverage accuracy score and the most “Excellent” rated responses\n(Figures 2, 3). Google Bard closely follows, with performance\nnearly matching that of ChatGPT-4.0. ChatGPT-3.5 ranks in the\nmiddle, while Claude-2 shows the least favorable performance.\nRegarding comprehensiveness, the four LLM-Chatbots have\nsuccessfully balanced accuracy and comprehensiveness, with\ntheir average scores all exceeding 2, thus achieving at least\nFrontiers in Medicine 06 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 7\nHe et al. 10.3389/fmed.2025.1516442\nTABLE 2 The percentage of ratings generated by each large language models (LLM)-Chatbots in different categories.\nCategory Num-ber\nof\nquest-ions\nClaude-2, n (%) ChatGPT-3.5, n (%) Google Bard, n (%) ChatGPT-4.0, n (%)\nDeﬁcient Marginal Excellent Deﬁcient Marginal Excellent Deﬁcient Marginal Excellent Deﬁcie-t Marginal Excellent\nGeneral 6 0 (0) 4 (66.7) 2 (33.3) 0 (0) 3 (50) 3 (50) 1 (16.7) 2 (33.3) 3 (50) 0 (0) 2 (33.3) 4 (66.7)\nDiagnose 5 0 (0) 3 (60) 2 (40) 0 (0) 3 (60) 2 (40) 0 (0) 1 (20) 4 (80) 0 (0) 2 (40) 3 (60)\nTreatment 6 2 (33.3) 1 (16.7) 3 (50) 0 (0) 3 (50) 3 (50) 1 (16.7) 1 (16.7) 4 (66.7) 0 (0) 2 (33.3) 4 (66.7)\nFollow-up\nand\nprevention\n7 0 (0) 7 (100) 0 (0) 0 (0) 5 (71.4) 2 (28.6) 0 (0) 4 (57.1) 3 (42.9) 0 (0) 3 (42.9) 4 (57.1)\na “Comprehensive” rating (Supplementary Table 8). Our\nresults corroborate earlier studies, indicating that ChatGPT-\n4.0 consistently outperforms other LLMs in the medical\nﬁeld, particularly in neuro-ophthalmology-related inquiries\n(5, 7, 29). The superior performance of ChatGPT-4.0 can\nbe attributed to its enhanced model size and parameters, its\nexpanding user base, and the incorporation of reinforcement\nlearning from human feedback (RLHF), which helps in\ngenerating more relevant and contextually accurate responses\n(30, 31).\nOur study reveals that while the majority of responses\nfrom the four models to optic neuritis-related questions were\nrated “Marginal” or better, Claude-2 and Google Bard each\nhad responses categorized as “Deﬁcient” in the general and\ntreatment categories (Supplementary Table 7). This indicates\nthat using LLMs to provide medical advice increases the risk\nof misleading information, especially for responses that are not\nrated as “Excellent.” Moreover, because most patients currently\nhave access only to general-purpose LLMs, which have not\nreceived formal medical certiﬁcation, their use may also raise\nlegal and ethical concerns. For example, the clinical use of non-\ncertiﬁed LLMs raises ethical concerns, because although LLMs\nare ethically prohibited from providing harmful information,\nerroneous medical advice may still indirectly harm patients, leading\nto delayed treatment or inappropriate self-medication. Given\nthe swift advancement and intricate deployment of LLMs, the\nethical challenges highlighted above are unlikely to be adequately\nsafeguarded by current laws and regulations. Notably, Google\nBard sometimes includes source links in its responses, but these\nlinks are often fabricated and lack authenticity (Supplementary\nTable 3). Previous research indicates that Google Bard has a\ntendency to generate ﬁctitious or incorrect information (32), an\nissue that remains unresolved. Similarly, when ChatGPT-4.0 is\nprompted to provide sources, it might face the same problem\n(33). Therefore, caution is advised when considering the source\ninformation provided by LLMs.\nUnlike traditional search engines, LLMs beneﬁt from deep\nlearning capabilities, continuously enhancing their knowledge\nfrom diverse online databases and user feedback. This highlights\nthe signiﬁcant potential of LLM-Chatbots in clinical settings.\nNumerous studies have explored LLM applications in various\nmedical ﬁelds. For instance, Lim et al. (34) identiﬁed potential\nin handling consultations related to myopia, particularly with\nChatGPT-4.0. Meng et al. (35) found that ChatGPT can\nprovide appropriate responses to fracture prevention and medical\nqueries. However, as Cappellani et al. (14) noted, ChatGPT\ncan still generate incomplete, incorrect, or potentially harmful\ninformation about common ophthalmic diseases, reﬂecting the\nvariable performance of LLMs across diﬀerent medical ﬁelds.\nThis variability is largely inﬂuenced by the speciﬁcity and\ndevelopment of those ﬁelds—the richer and more frequent the\nuser interactions, the more eﬀectively LLMs can learn and improve\ntheir performance.\nOur study has several limitations. First, the small number\nof optic neuritis-related questions and the limited variety within\ncategories might not fully represent the issues patients usually\nface, indicating a need for more diverse questions in future\nresearch. Additionally, to mitigate evaluator subjectivity, we\nused multiple evaluation rounds and majority rule decisions.\nFrontiers in Medicine 07 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 8\nHe et al. 10.3389/fmed.2025.1516442\nWhile readability metrics help assess the educational level needed\nfor understanding, they don’t encompass all comprehension\nfactors. Lastly, the rapid evolution of LLMs, driven by new training\ndata and user feedback, means our ﬁndings are time-sensitive,\nwhen new models emerge, longitudinal re-evaluation studies will\nbe extremely valuable.\nOur study conﬁrms the potential of LLM-Chatbots to provide\nonline clinical consultations for optic neuritis, oﬀering accurate\nand comprehensive information across distances. However,\ntheir readability issues might aﬀect user experience. More\ncritically, any misinformation from LLM-Chatbots could lead\nto unforeseen harmful consequences. Patients using LLM-\nChatbots need to proceed with caution and maintain open\ncommunication with their doctors, who in turn should guide\ntheir use of these tools eﬀectively. Enhancing the readability,\naccuracy, and comprehensiveness of LLM-Chatbots is essential.\nA table summarizing the main ﬁndings can be found in\nSupplementary Table 9.\nFuture research should concentrate on reﬁning assessment\nstrategies for LLMs by developing more comprehensive scoring\ncriteria. Additionally, ongoing training and targeted improvements\nare crucial to enhance the accuracy and readability of LLMs. Such\neﬀorts will ensure that their performance in addressing questions\nrelated to optic neuritis becomes increasingly robust and reliable.\nData availability statement\nThe original contributions presented in this study are included\nin this article/Supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nH-JH: Conceptualization, Formal Analysis, Investigation,\nMethodology, Resources, Validation, Visualization, Writing –\noriginal draft. F-FZ: Investigation, Methodology, Resources,\nWriting – review and editing. J-JL: Investigation, Resources,\nWriting – review and editing. YW: Investigation, Writing – review\nand editing. Q-QH: Resources, Writing – review and editing. HL:\nWriting – review and editing. JC: Writing – review and editing.\nFC: Writing – review and editing. T-PL: Writing – review and\nediting. ZH: Writing – review and editing. J-FY: Writing – review\nand editing. LC: Writing – review and editing. CC: Writing –\nreview and editing. Y-CT: Writing – review and editing. L-PC:\nConceptualization, Funding acquisition, Project administration,\nSupervision, Writing – review and editing.\nFunding\nThe author(s) declare that ﬁnancial support was received for the\nresearch and/or publication of this article. This study was supported\nby the National Natural Science Foundation of China (81570849),\nand the Natural Science Foundation of Guangdong Province, China\n(2020A1515011413).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) verify and take full responsibility for the use of\ngenerative AI in the preparation of this manuscript. Generative\nAI was used during the preparation of this work the authors\nused ChatGPT-4.0 in order to edit the entire article, correct\ngrammatical errors, and enhance sentence coherence and academic\nstyle. After using this tool, the authors reviewed and edited the\ncontent as needed and take full responsibility for the content of\nthe publication.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fmed.2025.\n1516442/full#supplementary-material\nReferences\n1. Li Z, Wang L, Wu X, Jiang J, Qiang W, Xie H, et al. Artiﬁcial intelligence in\nophthalmology: The path to the real-world clinic. Cell Rep Med. (2023) 4:101095.\ndoi: 10.1016/j.xcrm.2023.101095\n2. Esteva A, Robicquet A, Ramsundar B, Kuleshov V , DePristo M, Chou K, et al. A\nguide to deep learning in healthcare. Nat Med.(2019) 25:24–9. doi: 10.1038/s41591-\n018-0316-z\n3. OpenAI. Introducing ChatGPT. (2024). Available online at: https://openai.com/\nblog/chatgpt (accessed April 16, 2024).\n4. Tan S, Xin X, Wu D. ChatGPT in medicine: Prospects and challenges:\nA review article. Int J Surg. (2024) 110:3701–6. doi: 10.1097/JS9.000000000000\n1312\n5. Antaki F , Touma S, Milad D, El-Khoury J, Duval R. Evaluating the performance\nof ChatGPT in ophthalmology: An analysis of its successes and shortcomings.\nOphthalmol Sci.(2023) 3:100324. doi: 10.1016/j.xops.2023.100324\n6. Antaki F , Milad D, Chia M, Giguère C, Touma S, El-Khoury J, et al. Capabilities\nof GPT-4 in ophthalmology: An analysis of model entropy and progress towards\nFrontiers in Medicine 08 frontiersin.org\nfmed-12-1516442 June 23, 2025 Time: 14:24 # 9\nHe et al. 10.3389/fmed.2025.1516442\nhuman-level medical question answering. Br J Ophthalmol.(2023) 108:1371–8. doi:\n10.1136/bjo-2023-324438\n7. Teebagy S, Colwell L, Wood E, Y aghy A, Faustina M. Improved performance of\nChatGPT-4 on the OKAP examination: A comparative study with ChatGPT-3.5.J Acad\nOphthalmol. (2023) 15:e184–7. doi: 10.1055/s-0043-1774399\n8. Lin S, Chan P , Hsu W, Kao C. Exploring the proﬁciency of ChatGPT-4: An\nevaluation of its performance in the Taiwan advanced medical licensing examination.\nDigit Health.(2024) 10:20552076241237678. doi: 10.1177/20552076241237678\n9. Momenaei B, Mansour H, Kuriyan A, Xu D, Sridhar J, Ting D, et al. ChatGPT\nenters the room: What it means for patient counseling, physician education, academics,\nand disease management. Curr Opin Ophthalmol.(2024) 35:205–9. doi: 10.1097/ICU.\n0000000000001036\n10. Zandi R, Fahey J, Drakopoulos M, Bryan J, Dong S, Bryar P , et al. Exploring\ndiagnostic precision and triage proﬁciency: A comparative study of GPT-4 and bard\nin addressing common ophthalmic complaints. Bioengineering. (2024) 11:120. doi:\n10.3390/bioengineering11020120\n11. Delsoz M, Raja H, Madadi Y , Tang A, Wirostko B, Kahook M, et al. The use of\nChatGPT to assist in diagnosing glaucoma based on clinical case reports. Ophthalmol\nTher. (2023) 12:3121–32. doi: 10.1007/s40123-023-00805-x\n12. Mihalache A, Huang R, Popovic M, Patil N, Pandya B, Shor R, et al. Accuracy of\nan artiﬁcial intelligence Chatbot’s interpretation of clinical ophthalmic images. JAMA\nOphthalmol. (2024) 142:321–6. doi: 10.1001/jamaophthalmol.2024.0017\n13. Ittarat M, Cheungpasitporn W, Chansangpetch S. Personalized care in eye\nhealth: Exploring opportunities, challenges, and the road ahead for Chatbots. J Pers\nMed. (2023) 13:1679. doi: 10.3390/jpm13121679\n14. Cappellani F , Card K, Shields C, Pulido J, Haller J. Reliability and accuracy\nof artiﬁcial intelligence ChatGPT in providing information on ophthalmic diseases\nand management to patients. Eye. (2024) 38:1368–73. doi: 10.1038/s41433-023-\n02906-0\n15. Bernstein I, Zhang Y , Govil D, Majid I, Chang R, Sun Y , et al. Comparison of\nophthalmologist and large language model chatbot responses to online patient eye care\nquestions. JAMA Netw Open.(2023) 6:e2330320. doi: 10.1001/jamanetworkopen.2023.\n30320\n16. Del Negro I, Pauletto G, Verriello L, Spadea L, Salati C, Ius T, et al. Uncovering\nthe genetics and physiology behind optic neuritis. Genes. (2023) 14:2192. doi: 10.3390/\ngenes14122192\n17. Pan X, Zhou X, Yu L, Hou L. Switching from oﬄine to online health consultation\nin the post-pandemic era: The role of perceived pandemic risk. Front Public Health.\n(2023) 11:1121290. doi: 10.3389/fpubh.2023.1121290\n18. American Academy of Ophthalmology American Academy of Ophthalmology\n[Internet]. (2024). Available online at: https://www.aao.org/search/results?q=optic%\n20neuritis&realmName=_UREALM_&wt=json&rows=10&start=0 (accessed April 13,\n2024).\n19. National Eye Institute Search Results for \"Optic Neuritis\". (2024). Available online\nat: https://www.nei.nih.gov/search?terms=optic%20neuritis (accessed April 13, 2024).\n20. Srinivasan N, Samaan J, Rajeev N, Kanu M, Yeo Y , Samakar K. Large\nlanguage models and bariatric surgery patient education: A comparative\nreadability analysis of GPT-3.5, GPT-4, Bard, and online institutional\nresources. Surg Endosc. (2024) 38:2522–32. doi: 10.1007/s00464-024-\n10720-2\n21. Herbert A, Nemirovsky A, Hess D, Walter D, Abraham N, Loeb S, et al. An\nevaluation of the readability and content-quality of pelvic organ prolapse youtube\ntranscripts. Urology. (2021) 154:120–6. doi: 10.1016/j.urology.2021.03.009\n22. Decker H, Trang K, Ramirez J, Colley A, Pierce L, Coleman M, et al. Large\nlanguage model-based chatbot vs surgeon-generated informed consent documentation\nfor common procedures. JAMA Netw Open. (2023) 6:e2336997. doi: 10.1001/\njamanetworkopen.2023.36997\n23. Readability Formulas Free Readability Assessment Tools [Internet] . (2024)\nAvailable online at: https://readabilityformulas.com/ (accessed April 16, 2024).\n24. Tao B, Handzic A, Hua N, Vosoughi A, Margolin E, Micieli J. Utility of\nChatGPT for automated creation of patient education handouts: An application\nin neuro-ophthalmology. J Neuroophthalmol. (2024) 44:119–24. doi: 10.1097/WNO.\n0000000000002074\n25. Tailor P , Dalvin L, Starr M, Tajﬁrouz D, Chodnicki K, Brodsky M, et al.\nA comparative study of large language models, human experts, and expert-edited\nlarge language models to neuro-ophthalmology questions. J Neuroophthalmol.(2024)\n45:71–7. doi: 10.1097/WNO.0000000000002145\n26. Madadi Y , Delsoz M, Lao P , Fong J, Hollingsworth T, Kahook M, et al.\nChatGPT assisting diagnosis of neuro-ophthalmology diseases based on case reports.\nJ Neuroophthalmol. (2023):doi: 10.1097/WNO.0000000000002274 [Epub ahead of\nprint].\n27. Weiss B. Health Literacy and Patient Safety: Help Patients Understand. Chicago,\nIL: American Medical Association Foundation and American Medical Association\n(2007).\n28. Huang G, Fang C, Agarwal N, Bhagat N, Eloy J, Langer P. Assessment\nof online patient education materials from major ophthalmologic associations.\nJAMA Ophthalmol. (2015) 133:449–54. doi: 10.1001/jamaophthalmol.2014.\n6104\n29. Raimondi R, Tzoumas N, Salisbury T, Di Simplicio S, Romano M. Comparative\nanalysis of large language models in the Royal College of Ophthalmologists fellowship\nexams. Eye. (2023) 37:3530–3. doi: 10.1038/s41433-023-02563-3\n30. OpenAI, Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, et al.GPT-4 Technical\nReport [Internet]. (2024). Available online at: http://arxiv.org/abs/2303.08774 (accessed\nApril 13, 2024).\n31. ChatGPT Statistics and User Numbers OpenAI Chatbot [Internet]. (2024).\nAvailable online at: https://www.tooltester.com/en/blog/chatgpt-statistics/#top\n(accessed April 13, 2024).\n32. Kumar M, Mani U, Tripathi P , Saalim M, Roy S. Artiﬁcial hallucinations by\nGoogle bard: Think before you leap. Cureus. (2023) 15:e43313.\n33. Huang K, Mehta N, Gupta S, See A, Arnaout O. Evaluation of the safety,\naccuracy, and helpfulness of the GPT-4.0 large language model in neurosurgery. J Clin\nNeurosci. (2024) 123:151–6. doi: 10.1016/j.jocn.2024.03.021\n34. Lim Z, Pushpanathan K, Yew S, Lai Y , Sun C, Lam J, et al. Benchmarking large\nlanguage models’ performances for myopia care: A comparative analysis of ChatGPT-\n3.5, ChatGPT-4.0, and Google Bard. EBioMedicine. (2023) 95:104770. doi: 10.1016/j.\nebiom.2023.104770\n35. Meng J, Zhang Z, Tang H, Xiao Y , Liu P , Gao S, et al. Evaluation of ChatGPT\nin providing appropriate fracture prevention recommendations and medical science\nquestion responses: A quantitative research.Medicine. (2024) 103:e37458. doi: 10.1097/\nMD.0000000000037458\nFrontiers in Medicine 09 frontiersin.org",
  "topic": "Optic neuritis",
  "concepts": [
    {
      "name": "Optic neuritis",
      "score": 0.8027995824813843
    },
    {
      "name": "Psychology",
      "score": 0.47465449571609497
    },
    {
      "name": "Audiology",
      "score": 0.40938058495521545
    },
    {
      "name": "Medicine",
      "score": 0.39144888520240784
    },
    {
      "name": "Linguistics",
      "score": 0.35314756631851196
    },
    {
      "name": "Multiple sclerosis",
      "score": 0.23455080389976501
    },
    {
      "name": "Philosophy",
      "score": 0.19486570358276367
    },
    {
      "name": "Psychiatry",
      "score": 0.12430235743522644
    }
  ]
}