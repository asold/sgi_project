{
  "title": "Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)",
  "url": "https://openalex.org/W4387294423",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222812976",
      "name": "Rao, Jinmeng",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2113633793",
      "name": "Gao Song",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A4225927324",
      "name": "Mai, Gengchen",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A3205987712",
      "name": "Janowicz, Krzysztof",
      "affiliations": [
        "University of California, Santa Barbara",
        "University of Vienna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4376864533",
    "https://openalex.org/W4283070861",
    "https://openalex.org/W4365601293",
    "https://openalex.org/W2982441053",
    "https://openalex.org/W4366850961",
    "https://openalex.org/W2775721950",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4376311928",
    "https://openalex.org/W4281689570",
    "https://openalex.org/W4367859967",
    "https://openalex.org/W4366196934",
    "https://openalex.org/W4309651788",
    "https://openalex.org/W4378942358",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W3164801714",
    "https://openalex.org/W4386977305",
    "https://openalex.org/W3036770823",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4367000527",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4364382852",
    "https://openalex.org/W4378473795",
    "https://openalex.org/W4377371500",
    "https://openalex.org/W4320342485",
    "https://openalex.org/W4376163524",
    "https://openalex.org/W4366850753",
    "https://openalex.org/W3013816534",
    "https://openalex.org/W3090910518",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385565597",
    "https://openalex.org/W4387460511",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4381587628",
    "https://openalex.org/W4388616070",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4386614422",
    "https://openalex.org/W4389475164",
    "https://openalex.org/W4309651804",
    "https://openalex.org/W3034593529",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W4212774754"
  ],
  "abstract": "In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.",
  "full_text": "Building Privacy-Preserving and Secure Geospatial Artificial\nIntelligence Foundation Models\nJinmeng Rao\nUniversity of Wisconsin-Madison, USA\njinmeng.rao@wisc.edu\nSong Gao\nUniversity of Wisconsin-Madison, USA\nsong.gao@wisc.edu\nGengchen Mai\nUniversity of Georgia, USA\ngengchen.mai25@uga.edu\nKrzysztof Janowicz\nUniversity of Vienna, Austria\nkrzysztof.janowicz@univie.ac.at\nABSTRACT\nIn recent years we have seen substantial advances in foundation\nmodels for artificial intelligence, including language, vision, and\nmultimodal models. Recent studies have highlighted the poten-\ntial of using foundation models in geospatial artificial intelligence,\nknown as GeoAI Foundation Models, for geographic question an-\nswering, remote sensing image understanding, map generation,\nand location-based services, among others. However, the develop-\nment and application of GeoAI foundation models can pose serious\nprivacy and security risks, which have not been fully discussed or\naddressed to date. This paper introduces the potential privacy and\nsecurity risks throughout the lifecycle of GeoAI foundation models\nand proposes a comprehensive blueprint for research directions\nand preventative and control strategies. Through this vision paper,\nwe hope to draw the attention of researchers and policymakers\nin geospatial domains to these privacy and security risks inherent\nin GeoAI foundation models and advocate for the development of\nprivacy-preserving and secure GeoAI foundation models.\nCCS CONCEPTS\n• Artificial intelligence; • Security and privacy ;\nKEYWORDS\nGeoAI, Foundation Model, Privacy, Security, Multimodality\n1 INTRODUCTION\nFoundation Models (FMs) are large Artificial Intelligence (AI) mod-\nels pre-trained on vast web-scale data and can be adapted to address\na variety of downstream tasks such as machine translation and im-\nage recognition. Depending on the modalities involved, foundation\nmodels can be further categorized into language foundation mod-\nels (e.g., GPT-3 [4], LLaMA [32]), vision foundation models (e.g.,\nSegment Anything [15]), or multimodal foundation models such as\nvision-language foundation models (e.g., GPT-4 [24], BLIP-2 [16])\nand those connecting more modalities including video and audio\n(e.g., ImageBind [9]). In recent years, chatbots such as ChatGPT and\nBard as well as generic vision tools including Segment Anything\nand Stable Diffusion have showcased the proficiency of foundation\nmodels in addressing a wide range of natural language processing\nand computer vision tasks.\nThe success of foundation models has motivated researchers to\nincorporate them into geospatial domains to tackle challenges in\nGeospatial Artificial Intelligence (GeoAI) [12], known as GeoAI\nFoundation Models or Geo-Foundation Models (GeoFM) [21]. Re-\ncent explorations have delved into areas like geoparsing [20], urban\nplanning [33], geographic question answering [21], remote sensing\nsemantic segmentation [38, 8], and map generation [13], among\nothers, yielding promising results. However, recent studies also\nreveal that the development and use of foundation models could\npotentially unveil substantial privacy and security risks, including\nthe disclosure of sensitive information, representational bias, hallu-\ncinations, and misuse [3, 11]. These risks have sparked widespread\npublic concerns and triggered the imposition of prohibitions and\nstrict regulations in many countries and regions. Early regulations\ncan be inadequate for several reasons, e.g., 1) technology evolves\nfaster than regulations; 2) ensuring compliance and monitoring is\nchallenging; and3) progress in domains that would benefit most can\nslow down. In this vision paper, we summarize the potential privacy\nand security risks in GeoAI foundation models and propose a blue-\nprint for building privacy-preserving and secure GeoAI foundation\nmodels with corresponding research directions and promising pre-\nventative and control strategies. As foundation models increasingly\nexhibit dominance, we hope they also raise awareness of the equal\nimportance of privacy and security, spurring more researchers to\nstudy these aspects of GeoAI foundation models.\n2 PRIVACY AND SECURITY RISKS\nPrivacy and security are two intertwined concepts. Generally speak-\ning, privacy refers to people’s personal or sensitive information\nand their rights to prevent the disclosure of such information. Se-\ncurity refers to how such information is protected. In geospatial\ndomains, privacy and security often concern sensitive geospatial\ninformation such as home location, workspace, Points-of-Interest\n(POI) preferences, daily trajectories, and inferences based on such\ninformation [14, 28]. In the lifecycle of building and utilizing GeoAI\nfoundation models, we identify a series of potential privacy and se-\ncurity risks that exist around the pre-training and fine-tuning stages\nwith geospatial data, centralized serving and tooling, prompting-\nbased interaction, and feedback mechanisms.\n2.1 Risks in Geospatial Pre-training\nAs large pre-trained models, the capability of foundation models re-\nlies heavily on the scale, quality, and diversity of their pre-training\ndata. For example, GPT-3 was pre-trained on a huge language cor-\npus consisting of around 500 billion tokens from Web resources\n(e.g., CommonCrawl, Wikipedia) and books. BLIP-2 and DINOv2\nwere pre-trained on 129 million images (with captions) and 142\nRao et al.\nmillion images, respectively. Similarly, to pre-train a GeoAI foun-\ndation model, large-scale geospatial data are the key. Given the\nmultimodal nature of geospatial domains, geospatial data involve\nvarious modalities such as language (e.g., street address, geo-tagged\nsocial media posts), vision (e.g., remote sensing and street view im-\nagery, atlases), and structured data such as vectors (e.g., trajectories),\ngraphs (e.g., geospatial knowledge graphs), and tabular data (e.g.,\ncensus). All of them may contain personal or sensitive geospatial\ninformation that can be learned and disclosed by GeoAI foundation\nmodels. For example, a language model could potentially memorize\nhome addresses from a pre-training corpus and disclose them to\nanyone who asks. A vision-language model, likewise, may learn\nthe alignment between a building and its residents who have men-\ntioned it on social media. If someone were to upload a picture of\nthat building and asked, \"Who lives in this building?\", the model\nmight list all the residents it recognizes from social media. Since\nhow foundation models determine to learn and utilize the data\nremains opaque, it is challenging to prevent a GeoAI foundation\nmodel from acquiring, retaining, and divulging sensitive geospatial\ninformation without adequate privacy and security measures.\n2.2 Risks in Geospatial Fine-tuning\nIn practice, due to limitations in computational resources and the\navailability of domain-specific data, we usually adopt model weights\nfrom foundation models pre-trained on a general domain and fur-\nther fine-tune it on domain-specific data (i.e., domain adaptation)\nsuch as geospatial data. Common fine-tuning methods for founda-\ntion models include model fine-tuning, where models are fine-tuned\non domain-specific data (for instance, instruction tuning focuses\non fine-tuning models to follow given instructions), and prompt\ntuning, which involves fine-tuning input prompts rather than the\nmodels themselves. There are several risks associated with the fine-\ntuning data and the fine-tuning process: 1) Memorizing sensitive\ndata. Analogous to geospatial pre-training, if the fine-tuning data\ncontains personal or sensitive geospatial information, a GeoAI foun-\ndation model may potentially learn, memorize, and disseminate\nsuch information after the model has been fine-tuned; 2) Poison-\ning by malicious instructions. During instruction tuning, if the\ninstruction dataset has been poisoned or injected with malicious\ninstructions such as backdoors [34], the fine-tuned models can be\neasily manipulated and perform malicious activities (e.g., analyz-\ning and revealing home locations of individuals); and 3) Attacks\ndue to leaked soft prompts. For prompt tuning, since the soft\nprompts (e.g., embeddings) are tuned on user input data, once such\nsoft prompts are leaked, attackers might be able to infer user input\ninformation (e.g., frequently mentioned places) from these prompts.\n2.3 Risks in Centralized Serving and Tooling\nAfter training, GeoAI foundation models are usually hosted on cen-\ntralized servers. There are two paradigms of how GeoAI foundation\nmodels can be used to provide services. One paradigm is that we\nfully rely on the internal knowledge of models learned during the\npre-training or fine-tuning stage to provide services such as ques-\ntion answering. Another paradigm is that we enable the models\nwith geospatial tooling ability by connecting the models to exter-\nnal geospatial resources such as geospatial databases, tools, and\nAPIs via autonomous LLM frameworks [17, 7] such as LangChain 1\nand AutoGPT 2 so that the models can acquire and utilize external\ngeospatial knowledge on which they were not trained to further\nenhance their functionality. Both paradigms may expose privacy\nand security risks. First, centralized serving brings endogenous\nprivacy risks as all users’ requests need to be sent to and stored in a\ncentralized server. Sometimes, these requests may contain sensitive\ngeospatial information that users are unwilling to disclose. Recent\npayment leakage 3 and chat history leakage 4 in ChatGPT show\nthat as soon as the data leaves a user’s device, avoiding privacy and\nsecurity risks becomes very challenging. Second, since the model\nweights are stored in a centralized server, an attacker may be able\nto hack into the server to steal the weights, reconstruct training\ndata from the weights [10], or perform membership inference at-\ntacks [23]. Third, when GeoAI foundation models are connected\nto external geospatial resources, attackers might make models dis-\nclose sensitive information from external resources (e.g., geospatial\ndatabase credentials or third-party private geospatial data).\n2.4 Risks in Prompt-Based Interaction\nPrompt-based interactions are widely supported in most founda-\ntion models. Properly designed prompts can leverage the in-context\nlearning (e.g., zero-shot or few-shot learning) ability of foundation\nmodels to tackle a wide variety of tasks. Many geospatial applica-\ntions built upon foundation models are directly based on prompt\nengineering. For example, ChatGeoPT 5 and MapsGPT 6 both in-\ncorporate language foundation models with pre-defined prompt\ntemplates covering the use of Location-Based Service (LBS) APIs to\nprovide users with flexible location search experience via natural\nlanguage. In contrast, improper prompts can be used to “jailbreak”\nmodels to get sensitive information or “hijack” models to perform\nsomething dangerous, illegal, or unethical. Common prompt attack\nmethods include Do Anything Now 7 (i.e., bypass pre-set content\npolicy), Goal Hijacking [25] (i.e., ignore pre-defined prompts), etc.,\nwhich can cause serious privacy and security issues: 1) Training\ndata leakage. Recent studies [5, 36] reveal that by constructing\ncertain types of prompts or prefixes, attackers can make founda-\ntion models output the content they memorized during the train-\ning stage, such as home locations of individuals; 2) Pre-defined\nprompt leakage. Attackers may use certain prompts to ask the\nmodels to divulge their pre-defined prompts, which may contain\nsensitive information such as instructions to access internal geospa-\ntial systems. Recent prompt leakage accidents of Bing Chat and\nSnap’s MyAI suggest that it is hard to make foundation models fully\nimmune to such prompt attacks even with privacy and security\nmeasures established; 3) External resource leakage. As men-\ntioned previously, attackers may use malicious prompts to induce\nmodels to send queries to the connected geospatial database and\nget sensitive geospatial information; and 4) Malicious behaviors.\nAttackers may use malicious prompts to induce models to send\nphishing emails, scams, or commit cybercrimes [6].\n1https://python.langchain.com/\n2https://github.com/Significant-Gravitas/Auto-GPT\n3https://cybernews.com/news/payment-info-leaked-openai-chatgpt-outage/\n4https://www.bbc.com/news/technology-65047304\n5https://github.com/earth-genome/ChatGeoPT\n6https://www.mapsgpt.com/\n7https://github.com/0xk1h0/ChatGPT_DAN\nBuilding Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models\n2.5 Risks in Feedback Mechanisms\nFeedback mechanisms play an important role in continuously im-\nproving the quality of foundation models and making foundation\nmodels helpful and safe. Two common paradigms are Reinforce-\nment Learning from Human Feedback (RLHF) [1] and Reinforce-\nment Learning from AI Feedback (RLAIF) [2]. The former utilizes\nhuman evaluation as rewards or penalties to improve models, while\nthe latter asks AI to improve itself. In some cases, these feedback\nmechanisms can also be utilized by attackers to impact model qual-\nity and even make models toxic. For example, attackers can create\nplenty of misleading feedback or conduct backdoor reward poison-\ning attack [39, 31] against the RL process, causing the models to\nproduce false geographical knowledge, geographical discrimination\nremarks, or controversial geopolitical statements.\n3 TOWARDS PRIVACY-PRESERVING AND\nSECURE GEOAI FOUNDATION MODELS\nMotivated by the above-mentioned privacy and security risks, we\nadvocate for building privacy-preserving and secure GeoAI foun-\ndation models. We propose a blueprint (shown in Figure 1) that\noutlines multiple research directions, along with their associated\nchallenges and promising approaches.\nTraining Data\nPre - training\nFine - tuning\nRLHF / RLAIF\nModel Training GeoAI Foundation Models Users\n?\n!\nExternal Geospatial Resources\nDatabases\n LBS APIs\n···\nCommunication\nPrivacy - Preserving\nGeospatial Data\nHuman / AI Feedback\nPrivate and Secure Training\nSecure Feedback Mechanisms\nPrivate and Secure Model Serving and Geospatial Tooling\nMalicious Prompt - Based Interaction Detection\nFigure 1: A blueprint of building privacy-preserving and se-\ncure GeoAI foundation models.\n3.1 Privacy-Preserving Geospatial Data\nLarge-scale privacy-preserving geospatial data is key to building\nprivacy-preserving and secure GeoAI foundation models. The mul-\ntimodal nature of geospatial data implies that different types of\ngeospatial data require different privacy protection methods (e.g.,\ngeomasking, K-anonymity, and differential privacy for locations\nand trajectories, and blurring and mosaic for street view images),\nand some privacy risks exist in a cross-modal fashion (e.g., revealing\nhome locations by associating social media posts and street view\nimages). Thus, three challenges arise in this direction: 1) how to\neffectively process and preserve the privacy of large-scale multi-\nmodal geospatial data. This is a novel challenge to GeoAI foundation\nmodels as we must address cross-modal privacy risks in geospatial\ndata while maintaining geospatial alignment; 2) how to strike a\nbalance between geospatial data privacy and utility. This becomes\nmore complex in GeoAI foundation models compared to traditional\napproaches as the dimensions of modalities, levels of scales, and\ndata size increase significantly; and3) how to measure and mitigate\npotential geographic biases [18] existing in large-scale multimodal\ngeospatial data. Promising approaches include using spatial clus-\nter computing frameworks such as Apache Sedona for large-scale\ngeospatial data processing, using deep learning methods to protect\nlocation privacy and balance the privacy-utility trade-off [28, 26],\nusing multimodal instruction tuning to enhance alignment among\nmodalities and reduce biases and hallucinations [19], etc.\n3.2 Private and Secure Training and Serving\nTraditional centralized training and serving strategies require train-\ning data and model weights to be stored on a centralized server.\nHowever, in most cases, users are reluctant to share their data due\nto privacy concerns. This inherent risk in the centralized structure\nposes three challenges for GeoAI foundation models: 1) how to\nconduct geospatial pre-training and fine-tuning in ways that en-\nsure both privacy and security; 2) how to ensure the privacy and\nsecurity of hosted GeoAI foundation models (e.g., preventing model\nweight leakage and inference attacks); and 3) when switching to\ndecentralized training and serving strategies, how to deal with\nnon-Independently Identically Distributed (non-IID) training data.\nThis scenario frequently occurs when data is collected across dif-\nferent geographic regions due to spatial heterogeneity. Promising\napproaches include efficient encoding and encryption for geospa-\ntial data, model weights, and data transfer procedures, federated\nlearning for pre-training, fine-tuning, and prompt engineering [27,\n37, 35], geospatial-aware contrastive pre-training [22], etc.\n3.3 Private and Secure Geospatial Tooling\nGeospatial tooling greatly enhances the usability and extensibility\nof GeoAI foundation models. For example, geographic information\nretrieval can improve the truthfulness and timeliness of the models,\nand various geospatial resources and services can equip the models\nwith location tracking, spatial analysis, and geocomputing capa-\nbilities. However, allowing GeoAI foundation models to connect\nto various geospatial tools without restrictions might result in se-\nvere privacy and security issues such as sensitive geospatial data\nleakage and misuse. This raises two challenges: 1) how to design a\ngeneric and secure protocol to regulate geospatial tooling for GeoAI\nfoundation models; and 2) how to teach models to use geospatial\ntools and interpret results in ways that uphold privacy and security.\nPromising approaches include adopting in-context learning [30],\nfine-tuning [29, 1, 2], and autonomous agents [17] to improve and\nregulate geospatial tooling for GeoAI foundation models.\n3.4 Private and Secure Interaction with GeoFMs\nPrompt-based interaction facilitates natural communication be-\ntween users and GeoAI foundation models, but it also raises privacy\nand security concerns. The key to ensuring privacy-preserving and\nsecure prompt-based interaction is to identify malicious prompts\nfrom the myriad of user inputs. This presents three challenges: 1)\nhow to understand the intentions behind prompts and filter out\nmalicious prompts; 2) how to evaluate the resilience and robustness\nof GeoAI foundation models against malicious prompts; and3) how\nto ensure users do not accidentally send sensitive geospatial infor-\nmation to GeoAI foundation models via prompts and vice versa.\nPromising approaches include developing a generic detector that\ncan detect or filter out either malicious prompts or sensitive geospa-\ntial information from the users’ side, establishing an evaluation\nframework covering different types of malicious prompts to test\nGeoAI foundation models’ resilience and robustness, etc.\nRao et al.\n3.5 Secure Feedback Mechanisms\nFeedback mechanisms such as RLHF and RLAIF can be exploited by\nattackers using poisoned feedback, leading to harmful GeoAI foun-\ndation models that produce inaccurate, controversial, or unethical\ngeographic statements. In terms of ensuring secure feedback mech-\nanisms, we highlight two challenges: 1) how to identify poisoned\nfeedback and determine if, when, where, and how the feedback\nmechanisms or reward functions are compromised (e.g., poisoned\nfeedback could be comments or ratings that induce the model to\ndeviate from expected behavior); and 2) how to determine the best\nrecovery or fallback strategy when we realize the feedback mech-\nanisms are poisoned. In these cases, we can evaluate the model’s\nbehavior using a comprehensive benchmark set of geographic com-\nmonsense knowledge and conversations and measure the shift of\ntruthfulness and harmfulness between versions. In addition, analyz-\ning the data distribution of feedback, monitoring the reward curve,\nand regularly saving model weights are also beneficial.\n4 CONCLUSION\nThis vision paper discusses privacy and security risks in GeoAI\nfoundation models and proposes a blueprint for privacy-preserving\nand secure GeoAI foundation models. Addressing privacy and se-\ncurity risks in new technologies requires not only regulations but\nalso evolving technical solutions and societal ethical discussions.\nWe hope this paper can raise awareness among researchers and\npolicymakers about the privacy and security risks associated with\nGeoAI foundation models and promote positive future development\nof GeoAI foundation models that respect privacy and security.\nACKNOWLEDGMENTS\nSong Gao acknowledges the funding support from the National\nScience Foundation funded AI institute [Grant No. 2112606] for\nIntelligent Cyberinfrastructure with Computational Learning in\nthe Environment (ICICLE).\nREFERENCES\n[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova\nDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\n“Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback”. In: arXiv preprint arXiv:2204.05862(2022).\n[2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\net al. “Constitutional AI: Harmlessness from AI Feedback”. In: arXiv preprint\narXiv:2212.08073 (2022).\n[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. “On the opportunities and risks of foundation models”. In:arXiv\npreprint arXiv:2108.07258(2021).\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. “Language models are few-shot learners”. In: Advances in neural\ninformation processing systems33 (2020), pp. 1877–1901.\n[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson,\net al. “Extracting Training Data from Large Language Models.” In: USENIX\nSecurity Symposium. Vol. 6. 2021.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. “Evaluating large language models trained on code”. In: arXiv\npreprint arXiv:2107.03374(2021).\n[7] Haixing Dai, Yiwei Li, Zhengliang Liu, Lin Zhao, Zihao Wu, Suhang Song, Ye\nShen, Dajiang Zhu, Xiang Li, Sheng Li, et al. “AD-AutoGPT: An Autonomous\nGPT for Alzheimer’s Disease Infodemiology”. In:arXiv preprint arXiv:2306.10095\n(2023).\n[8] Paolo Fraccaro, Carlos Gomes, Johannes Jakubik, Linsong Chu, Nyirjesy Gabby,\nRanjini Bangalore, Devyani Lambhate, Kamal Das, Dario Oliveira Borges, Daiki\nKimura, Naomi Simumba, Daniela Szwarcman, Michal Muszynski, Kommy\nWeldemariam, Blair Edwards, Johannes Schmude, Hendrik Hamann, Bianca\nZadrozny, Raghu Ganti, Carlos Costa, Campbell Watson, Karthik Mukkav-\nilli, Rob Parkin, Sujit Roy, Christopher Phillips, Kumar Ankur, Muthuku-\nmaran Ramasubramanian, Iksha Gurung, Wei Ji Leong, Ryan Avery, Rahul\nRamachandran, Manil Maskey, Pontus Olofossen, Elizabeth Fancher, Tsengdar\nLee, Kevin Murphy, Dan Duffy, Mike Little, Hamed Alemohammad, Michael\nCecil, Steve Li, Sam Khallaghi, Denys Godwin, Maryam Ahmadi, Fatemeh\nKordi, Bertrand Saux, Neal Pastick, Peter Doucette, Rylie Fleckenstein, Dal-\nton Luanga, Alex Corvin, and Erwan Granger. HLS Foundation. Aug. 2023.\ndoi: https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M. url: https:\n//github.com/nasa-impact/hls-foundation-os.\n[9] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev\nAlwala, Armand Joulin, and Ishan Misra. “Imagebind: One embedding space to\nbind them all”. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 2023, pp. 15180–15190.\n[10] Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. “Re-\nconstructing training data from trained neural networks”. In: arXiv preprint\narXiv:2206.07758 (2022).\n[11] Krzysztof Janowicz. “Philosophical foundations of geoai: Exploring sustain-\nability, diversity, and bias in geoai and spatial data science”. In:arXiv preprint\narXiv:2304.06508 (2023).\n[12] Krzysztof Janowicz, Song Gao, Grant McKenzie, Yingjie Hu, and Budhen-\ndra Bhaduri. “GeoAI: spatially explicit artificial intelligence techniques for\ngeographic knowledge discovery and beyond”. In: International Journal of\nGeographical Information Science34.4 (2020), pp. 625–636.\n[13] Yuhao Kang, Qianheng Zhang, and Robert Roth. “The ethics of AI-Generated\nmaps: A study of DALLE 2 and implications for cartography”. In:arXiv preprint\narXiv:2304.10743 (2023).\n[14] Carsten Keßler and Grant McKenzie. “A geoprivacy manifesto”. In:Transactions\nin GIS22.1 (2018), pp. 3–19.\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n“Segment anything”. In: arXiv preprint arXiv:2304.02643(2023).\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. “Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language\nmodels”. In: arXiv preprint arXiv:2301.12597(2023).\n[17] Zhenlong Li and Huan Ning. “Autonomous GIS: the next-generation AI-powered\nGIS”. In: arXiv preprint arXiv:2305.06453(2023).\n[18] Zilong Liu, Krzysztof Janowicz, Ling Cai, Rui Zhu, Gengchen Mai, and Meilin\nShi. “Geoparsing: Solved or Biased? An Evaluation of Geographic Biases in\nGeoparsing”. In: AGILE: GIScience Series3 (2022), p. 9.\n[19] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen\nSun, Carl Yang, and Jie Yang. “Evaluation and Mitigation of Agnosia in Multi-\nmodal Large Language Models”. In: arXiv preprint arXiv:2309.04041(2023).\n[20] Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, and Stefano Er-\nmon. “Towards a foundation model for geospatial artificial intelligence (vision\npaper)”. In: Proceedings of the 30th International Conference on Advances in\nGeographic Information Systems. 2022, pp. 1–4.\n[21] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ning-\nhao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. “On the opportu-\nnities and challenges of foundation models for geospatial artificial intelligence”.\nIn: arXiv preprint arXiv:2304.06798(2023).\n[22] Gengchen Mai, Ni Lao, Yutong He, Jiaming Song, and Stefano Ermon. “CSP:\nSelf-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Repre-\nsentations”. In: arXiv preprint arXiv:2305.01118(2023).\n[23] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf,\nMrinmaya Sachan, and Taylor Berg-Kirkpatrick. “Membership Inference At-\ntacks against Language Models via Neighbourhood Comparison”. In: arXiv\npreprint arXiv:2305.18462(2023).\n[24] OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].\n[25] Fábio Perez and Ian Ribeiro. “Ignore Previous Prompt: Attack Techniques For\nLanguage Models”. In: arXiv preprint arXiv:2211.09527(2022).\n[26] Jinmeng Rao, Song Gao, Yuhao Kang, and Qunying Huang. “LSTM-TrajGAN:\nA Deep Learning Approach to Trajectory Privacy Protection”. In: 11th Inter-\nnational Conference on Geographic Information Science (GIScience 2021)-Part I.\nSchloss Dagstuhl-Leibniz-Zentrum für Informatik. 2020.\n[27] Jinmeng Rao, Song Gao, Mingxiao Li, and Qunying Huang. “A privacy-preserving\nframework for location recommendation using decentralized collaborative ma-\nchine learning”. In: Transactions in GIS25.3 (2021), pp. 1153–1175.\n[28] Jinmeng Rao, Song Gao, and Sijia Zhu. “CATS: Conditional Adversarial Trajec-\ntory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep\nLearning Approaches”. In: International Journal of Geographical Information\nScience 37 (2023), pp. 1–30.\nBuilding Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models\n[29] Timo Schick, Jane Dwivedi-Yu, Roberto Dessı, Roberta Raileanu, Maria Lomeli,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. “Toolformer: Lan-\nguage models can teach themselves to use tools”. In:arXiv preprint arXiv:2302.04761\n(2023).\n[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yuet-\ning Zhuang. “Hugginggpt: Solving ai tasks with chatgpt and its friends in\nhuggingface”. In: arXiv preprint arXiv:2303.17580(2023).\n[31] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. “BadGPT: Exploring Security\nVulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT”. In: arXiv\npreprint arXiv:2304.12298(2023).\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. “Llama: Open and efficient foundation language models”.\nIn: arXiv preprint arXiv:2302.13971(2023).\n[33] Dongjie Wang, Chang-Tien Lu, and Yanjie Fu. “Towards automated urban\nplanning: When generative and chatgpt-like ai meets urban planning”. In:\narXiv preprint arXiv:2304.03892(2023).\n[34] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen.\n“Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for\nLarge Language Models”. In: arXiv preprint arXiv:2305.14710(2023).\n[35] Sixing Yu, J Pablo Muñoz, and Ali Jannesari. “Federated Foundation Models:\nPrivacy-Preserving and Collaborative Learning for Large Models”. In: arXiv\npreprint arXiv:2305.11414(2023).\n[36] Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin,\nand Shuicheng Yan. “Bag of tricks for training data extraction from language\nmodels”. In: arXiv preprint arXiv:2302.04460(2023).\n[37] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin\nWang, and Yiran Chen. “Towards Building the Federated GPT: Federated In-\nstruction Tuning”. In: arXiv preprint arXiv:2305.05644(2023).\n[38] Jielu Zhang, Zhongliang Zhou, Gengchen Mai, Lan Mu, Mengxuan Hu, and\nSheng Li. “Text2Seg: Remote Sensing Image Semantic Segmentation via Text-\nGuided Visual Foundation Models”. In: arXiv preprint arXiv:2304.10597(2023).\n[39] Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. “Adaptive reward-\npoisoning attacks against reinforcement learning”. In: International Conference\non Machine Learning. PMLR. 2020, pp. 11225–11234.",
  "topic": "Geospatial analysis",
  "concepts": [
    {
      "name": "Geospatial analysis",
      "score": 0.819430410861969
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.7539939880371094
    },
    {
      "name": "Computer science",
      "score": 0.7048065662384033
    },
    {
      "name": "Blueprint",
      "score": 0.518268346786499
    },
    {
      "name": "Data science",
      "score": 0.43199318647384644
    },
    {
      "name": "Computer security",
      "score": 0.4118831753730774
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3555803894996643
    },
    {
      "name": "Engineering",
      "score": 0.21620231866836548
    },
    {
      "name": "Remote sensing",
      "score": 0.13304787874221802
    },
    {
      "name": "Geography",
      "score": 0.11678358912467957
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}