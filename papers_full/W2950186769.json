{
  "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
  "url": "https://openalex.org/W2950186769",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Chen, Stanley F.",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": null,
      "name": "Goodman, Joshua T.",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2159782014",
    "https://openalex.org/W2063918473",
    "https://openalex.org/W2059800182",
    "https://openalex.org/W1924403233",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W1512277306",
    "https://openalex.org/W1601728146",
    "https://openalex.org/W2170120409",
    "https://openalex.org/W2061271742",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W1536631629",
    "https://openalex.org/W2126163471"
  ],
  "abstract": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.",
  "full_text": "arXiv:cmp-lg/9606011v1  11 Jun 1996\nTo appear in Proceedings of the 34th Annual Meeting of the ACL, June 1996\nAn Empirical Study of Smoothing Techniques for Language\nModeling\nStanley F. Chen\nHarvard University\nAiken Computation Laboratory\n33 Oxford St.\nCambridge, MA 02138\nsfc@eecs.harvard.edu\nJoshua Goodman\nHarvard University\nAiken Computation Laboratory\n33 Oxford St.\nCambridge, MA 02138\ngoodman@eecs.harvard.edu\nAbstract\nW e present an extensive empirical com-\nparison of several smoothing techniques in\nthe domain of language modeling, includ-\ning those described by Jelinek and Mer-\ncer (1980), Katz (1987), and Church and\nGale (1991). W e investigate for the ﬁrst\ntime how factors such as training data\nsize, corpus ( e.g., Brown versus W all Street\nJournal), and n-gram order (bigram versus\ntrigram) aﬀect the relative performance of\nthese methods, which we measure through\nthe cross-entropy of test data. In addition,\nwe introduce two novel smoothing tech-\nniques, one a variation of Jelinek-Mercer\nsmoothing and one a very simple linear in-\nterpolation technique, both of which out-\nperform existing methods.\n1 Introduction\nSmoothing is a technique essential in the construc-\ntion of n-gram language models, a staple in speech\nrecognition (Bahl, Jelinek, and Mercer, 1983) as well\nas many other domains (Church, 1988; Brown et al.,\n1990; Kernighan, Church, and Gale, 1990). A lan-\nguage model is a probability distribution over strings\nP (s) that attempts to reﬂect the frequency with\nwhich each string s occurs as a sentence in natu-\nral text. Language models are used in speech recog-\nnition to resolve acoustically ambiguous utterances.\nF or example, if we have that P (it takes two) ≫\nP (it takes too), then we know ceteris paribus to pre-\nfer the former transcription over the latter.\nWhile smoothing is a central issue in language\nmodeling, the literature lacks a deﬁnitive compar-\nison between the many existing techniques. Previ-\nous studies (Nadas, 1984; Katz, 1987; Church and\nGale, 1991; MacKay and Peto, 1995) only compare\na small number of methods (typically two) on a sin-\ngle corpus and using a single training data size. As\na result, it is currently diﬃcult for a researcher to\nintelligently choose between smoothing schemes.\nIn this work, we carry out an extensive\nempirical comparison of the most widely used\nsmoothing techniques, including those described\nby Jelinek and Mercer (1980), Katz (1987), and\nChurch and Gale (1991). W e carry out experiments\nover many training data sizes on varied corpora us-\ning both bigram and trigram models. W e demon-\nstrate that the relative performance of techniques\ndepends greatly on training data size and n-gram\norder. F or example, for bigram models produced\nfrom large training sets Church-Gale smoothing has\nsuperior performance, while Katz smoothing per-\nforms best on bigram models produced from smaller\ndata. F or the methods with parameters that can\nbe tuned to improve performance, we perform an\nautomated search for optimal values and show that\nsub-optimal parameter selection can signiﬁcantly de-\ncrease performance. T o our knowledge, this is the\nﬁrst smoothing work that systematically investigates\nany of these issues.\nIn addition, we introduce two novel smooth-\ning techniques: the ﬁrst belonging to the class of\nsmoothing models described by Jelinek and Mer-\ncer, the second a very simple linear interpolation\nmethod. While being relatively simple to imple-\nment, we show that these methods yield good perfor-\nmance in bigram models and superior performance\nin trigram models.\nW e take the performance of a method m to be its\ncross-entropy on test data\n1\nNT\nlT∑\ni=1\n− log2 Pm(ti)\nwhere Pm(ti) denotes the language model produced\nwith method m and where the test data T is com-\nposed of sentences ( t1, . . . , t lT ) and contains a total\nof NT words. The entropy is inversely related to\nthe average probability a model assigns to sentences\nin the test data, and it is generally assumed that\nlower entropy correlates with better performance in\napplications.\n1.1 Smoothing n-gram Models\nIn n-gram language modeling, the probability of a\nstring P (s) is expressed as the product of the prob-\nabilities of the words that compose the string, with\neach word probability conditional on the identity of\nthe last n − 1 words, i.e., if s = w1 · · · wl we have\nP (s) =\nl∏\ni=1\nP (wi|wi− 1\n1 ) ≈\nl∏\ni=1\nP (wi|wi− 1\ni− n+1) (1)\nwhere wj\ni denotes the words wi · · · wj . Typically , n is\ntaken to be two or three, corresponding to a bigram\nor trigram model, respectively . 1\nConsider the case n = 2. T o estimate the proba-\nbilities P (wi |wi− 1) in equation (1), one can acquire\na large corpus of text, which we refer to as training\ndata, and take\nPML(wi |wi− 1) = P (wi− 1wi)\nP (wi− 1)\n= c(wi− 1wi)/N S\nc(wi− 1)/N S\n= c(wi− 1wi)\nc(wi− 1)\nwhere c(α ) denotes the number of times the string\nα occurs in the text and NS denotes the total num-\nber of words. This is called the maximum likelihood\n(ML) estimate for P (wi|wi− 1).\nWhile intuitive, the maximum likelihood estimate\nis a poor one when the amount of training data is\nsmall compared to the size of the model being built,\nas is generally the case in language modeling. F or ex-\nample, consider the situation where a pair of words,\nor bigram, say burnish the , doesn’t occur in the\ntraining data. Then, we have PML(the|burnish) = 0,\nwhich is clearly inaccurate as this probability should\nbe larger than zero. A zero bigram probability can\nlead to errors in speech recognition, as it disallows\nthe bigram regardless of how informative the acous-\ntic signal is. The term smoothing describes tech-\nniques for adjusting the maximum likelihood esti-\nmate to hopefully produce more accurate probabili-\nties.\n1T o make the term P (wi|wi−1\ni−n+1) meaningful for\ni < n , one can pad the beginning of the string with\na distinguished token. In this work, we assume there are\nn − 1 such distinguished tokens preceding each sentence.\nAs an example, one simple smoothing technique is\nto pretend each bigram occurs once more than it ac-\ntually did (Lidstone, 1920; Johnson, 1932; Jeﬀreys,\n1948), yielding\nP+1(wi |wi− 1) = c(wi− 1wi) + 1\nc(wi− 1) + |V |\nwhere V is the vocabulary , the set of all words be-\ning considered. This has the desirable quality of\npreventing zero bigram probabilities. However, this\nscheme has the ﬂaw of assigning the same probabil-\nity to say , burnish the and burnish thou (assuming\nneither occurred in the training data), even though\nintuitively the former seems more likely because the\nword the is much more common than thou.\nT o address this, another smoothing technique is to\ninterpolate the bigram model with a unigram model\nPML(wi ) = c(wi)/N S , a model that reﬂects how of-\nten each word occurs in the training data. F or ex-\nample, we can take\nPinterp(wi |wi− 1) = λP ML(wi |wi− 1) + (1 −λ )PML(wi )\ngetting the behavior that bigrams involving common\nwords are assigned higher probabilities (Jelinek and\nMercer, 1980).\n2 Previous Work\nThe simplest type of smoothing used in practice is\nadditive smoothing (Lidstone, 1920; Johnson, 1932;\nJeﬀreys, 1948), where we take\nPadd(wi |wi− 1\ni− n+1) = c(wi\ni− n+1) + δ\nc(wi− 1\ni− n+1) + δ |V | (2)\nand where Lidstone and Jeﬀreys advocate δ = 1.\nGale and Church (1990; 1994) have argued that this\nmethod generally performs poorly .\nThe Good-T uring estimate (Good, 1953) is cen-\ntral to many smoothing techniques. It is not used\ndirectly for n-gram smoothing because, like additive\nsmoothing, it does not perform the interpolation of\nlower- and higher-order models essential for good\nperformance. Good-T uring states that an n-gram\nthat occurs r times should be treated as if it had\noccurred r∗ times, where\nr∗ = ( r + 1) nr+1\nnr\nand where nr is the number of n-grams that occur\nexactly r times in the training data.\nKatz smoothing (1987) extends the intuitions of\nGood-T uring by adding the interpolation of higher-\norder models with lower-order models. It is perhaps\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 10 100 1000 10000 100000\nlambda\nnumber of counts in distribution\nold bucketing\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.001 0.01 0.1 1 10 100\nlambda\naverage non-zero count in distribution minus one\nnew bucketing\nFigure 1: λ values for old and new bucketing schemes for Jelinek-Mercer smoothing; each point represents a\nsingle bucket\nthe most widely used smoothing technique in speech\nrecognition.\nChurch and Gale (1991) describe a smoothing\nmethod that combines the Good-T uring estimate\nwith bucketing, the technique of partitioning a set\nof n-grams into disjoint groups, where each group\nis characterized independently through a set of pa-\nrameters. Like Katz, models are deﬁned recursively\nin terms of lower-order models. Each n-gram is as-\nsigned to one of several buckets based on its fre-\nquency predicted from lower-order models. Each\nbucket is treated as a separate distribution and\nGood-T uring estimation is performed within each,\ngiving corrected counts that are normalized to yield\nprobabilities.\nThe other smoothing technique besides Katz\nsmoothing widely used in speech recognition is due\nto Jelinek and Mercer (1980). They present a class\nof smoothing models that involve linear interpola-\ntion, e.g., Brown et al. (1992) take\nPinterp(wi|wi− 1\ni− n+1) =\nλ wi−1\ni−n+1\nPML(wi |wi− 1\ni− n+1) +\n(1 − λ wi−1\ni−n+1\n) Pinterp(wi|wi− 1\ni− n+2) (3)\nThat is, the maximum likelihood estimate is inter-\npolated with the smoothed lower-order distribution,\nwhich is deﬁned analogously . T raining a distinct\nλ wi−1\ni−n+1\nfor each wi− 1\ni− n+1 is not generally felicitous;\nBahl, Jelinek, and Mercer (1983) suggest partition-\ning the λ wi−1\ni−n+1\ninto buckets according to c(wi− 1\ni− n+1 ),\nwhere all λ wi−1\ni−n+1\nin the same bucket are constrained\nto have the same value.\nT o yield meaningful results, the data used to esti-\nmate the λ wi−1\ni−n+1\nneed to be disjoint from the data\nused to calculate PML.2 In held-out interpolation,\none reserves a section of the training data for this\npurpose. Alternatively , Jelinek and Mercer describe\na technique called deleted interpolation where diﬀer-\nent parts of the training data rotate in training either\nPML or the λ wi−1\ni−n+1\n; the results are then averaged.\nSeveral smoothing techniques are motivated\nwithin a Bayesian framework, including work by\nNadas (1984) and MacKay and Peto (1995).\n3 Novel Smoothing T echniques\nOf the great many novel methods that we have tried,\ntwo techniques have performed especially well.\n3.1 Method average-count\nThis scheme is an instance of Jelinek-Mercer\nsmoothing. Referring to equation (3), recall that\nBahl et al. suggest bucketing the λ wi−1\ni−n+1\naccording\nto c(wi− 1\ni− n+1). W e have found that partitioning the\nλ wi−1\ni−n+1\naccording to the average number of counts\nper non-zero element\nc(wi−1\ni−n+1)\n|wi :c(wi\ni−n+1)>0| yields better\nresults.\nIntuitively , the less sparse the data for estimat-\ning PML(wi|wi− 1\ni− n+1), the larger λ wi−1\ni−n+1\nshould be.\nWhile larger c(wi− 1\ni− n+1 ) generally correspond to less\nsparse distributions, this quantity ignores the allo-\ncation of counts between words. F or example, we\nwould consider a distribution with ten counts dis-\ntributed evenly among ten words to be much more\n2When the same data is used to estimate both, setting\nall λ wi−1\ni−n+1\nto one yields the optimal result.\nsparse than a distribution with ten counts all on a\nsingle word. The average number of counts per word\nseems to more directly express the concept of sparse-\nness.\nIn Figure 1, we graph the value of λ assigned to\neach bucket under the original and new bucketing\nschemes on identical data. Notice that the new buck-\neting scheme results in a much tighter plot, indicat-\ning that it is better at grouping together distribu-\ntions with similar behavior.\n3.2 Method one-count\nThis technique combines two intuitions. First,\nMacKay and Peto (1995) argue that a reasonable\nform for a smoothed distribution is\nPone(wi|wi− 1\ni− n+1) = c(wi\ni− n+1) + αP one(wi |wi− 1\ni− n+2)\nc(wi− 1\ni− n+1) + α\nThe parameter α can be thought of as the num-\nber of counts being added to the given distribution,\nwhere the new counts are distributed as in the lower-\norder distribution. Secondly , the Good-T uring esti-\nmate can be interpreted as stating that the number\nof these extra counts should be proportional to the\nnumber of words with exactly one count in the given\ndistribution. W e have found that taking\nα = γ [n1(wi− 1\ni− n+1) + β ] (4)\nworks well, where\nn1(wi− 1\ni− n+1) = |wi : c(wi\ni− n+1 ) = 1 |\nis the number of words with one count, and where β\nand γ are constants.\n4 Experimental Methodology\n4.1 Data\nW e used the Penn treebank and TIPSTER cor-\npora distributed by the Linguistic Data Consor-\ntium. F rom the treebank, we extracted text from\nthe tagged Brown corpus, yielding about one mil-\nlion words. F rom TIPSTER, we used the Associ-\nated Press (AP), W all Street Journal (WSJ), and\nSan Jose Mercury News (SJM) data, yielding 123,\n84, and 43 million words respectively . W e created\ntwo distinct vocabularies, one for the Brown corpus\nand one for the TIPSTER data. The former vocab-\nulary contains all 53,850 words occurring in Brown;\nthe latter vocabulary consists of the 65,173 words\noccurring at least 70 times in TIPSTER.\nF or each experiment, we selected three segments\nof held-out data along with the segment of train-\ning data. One held-out segment was used as the\ntest data for performance evaluation, and the other\ntwo were used as development test data for opti-\nmizing the parameters of each smoothing method.\nEach piece of held-out data was chosen to be roughly\n50,000 words. This decision does not reﬂect practice\nvery well, as when the training data size is less than\n50,000 words it is not realistic to have so much devel-\nopment test data available. However, we made this\ndecision to prevent us having to optimize the train-\ning versus held-out data tradeoﬀ for each data size.\nIn addition, the development test data is used to op-\ntimize typically very few parameters, so in practice\nsmall held-out sets are generally adequate, and per-\nhaps can be avoided altogether with techniques such\nas deleted estimation.\n4.2 Smoothing Implementations\nIn this section, we discuss the details of our imple-\nmentations of various smoothing techniques. Due\nto space limitations, these descriptions are not com-\nprehensive; a more complete discussion is presented\nin Chen (1996). The titles of the following sections\ninclude the mnemonic we use to refer to the imple-\nmentations in later sections. Unless otherwise speci-\nﬁed, for those smoothing models deﬁned recursively\nin terms of lower-order models, we end the recursion\nby taking the n = 0 distribution to be the uniform\ndistribution Punif(wi) = 1 / |V |. F or each method, we\nhighlight the parameters ( e.g., λ n and δ below) that\ncan be tuned to optimize performance. Parameter\nvalues are determined through training on held-out\ndata.\n4.2.1 Baseline Smoothing ( interp-baseline)\nF or our baseline smoothing method, we use an\ninstance of Jelinek-Mercer smoothing where we con-\nstrain all λ wi−1\ni−n+1\nto be equal to a single value λ n for\neach n, i.e.,\nPbase(wi |wi− 1\ni− n+1) = λ n PML(wi|wi− 1\ni− n+1) +\n(1 − λ n ) Pbase(wi|wi− 1\ni− n+2)\n4.2.2 Additive Smoothing ( plus-one and\nplus-delta)\nW e consider two versions of additive smoothing.\nReferring to equation (2), we ﬁx δ = 1 in plus-one\nsmoothing. In plus-delta, we consider any δ .\n4.2.3 Katz Smoothing ( katz)\nWhile the original paper (Katz, 1987) uses a single\nparameter k, we instead use a diﬀerent k for each\nn > 1, kn. W e smooth the unigram distribution\nusing additive smoothing with parameter δ .\n4.2.4 Church-Gale Smoothing\n(church-gale)\nT o smooth the counts nr needed for the Good-\nT uring estimate, we use the technique described by\nGale and Sampson (1995). W e smooth the unigram\ndistribution using Good-T uring without any bucket-\ning.\nInstead of the bucketing scheme described in the\noriginal paper, we use a scheme analogous to the\none described by Bahl, Jelinek, and Mercer (1983).\nW e make the assumption that whether a bucket is\nlarge enough for accurate Good-T uring estimation\ndepends on how many n-grams with non-zero counts\noccur in it. Thus, instead of partitioning the space\nof P (wi− 1)P (wi) values in some uniform way as was\ndone by Church and Gale, we partition the space\nso that at least cmin non-zero n-grams fall in each\nbucket.\nFinally , the original paper describes only bigram\nsmoothing in detail; extending this method to tri-\ngram smoothing is ambiguous. In particular, it is\nunclear whether to bucket trigrams according to\nP (wi− 1\ni− 2 )P (wi ) or P (wi− 1\ni− 2 )P (wi |wi− 1). W e chose the\nformer; while the latter may yield better perfor-\nmance, our belief is that it is much more diﬃcult\nto implement and that it requires a great deal more\ncomputation.\n4.2.5 Jelinek-Mercer Smoothing\n(interp-held-out and interp-del-int)\nW e implemented two versions of Jelinek-Mercer\nsmoothing diﬀering only in what data is used to\ntrain the λ ’s. W e bucket the λ wi−1\ni−n+1\naccording to\nc(wi− 1\ni− n+1) as suggested by Bahl et al. Similar to our\nChurch-Gale implementation, we choose buckets to\nensure that at least cmin words in the data used to\ntrain the λ ’s fall in each bucket.\nIn interp-held-out, the λ ’s are trained using\nheld-out interpolation on one of the development\ntest sets. In interp-del-int, the λ ’s are trained\nusing the relaxed deleted interpolation technique de-\nscribed by Jelinek and Mercer, where one word is\ndeleted at a time. In interp-del-int, we bucket\nan n-gram according to its count before deletion, as\nthis turned out to signiﬁcantly improve performance.\n4.2.6 Novel Smoothing Methods\n(new-avg-count and new-one-count)\nThe implementation new-avg-count, correspond-\ning to smoothing method average-count, is identical\nto interp-held-out except that we use the novel\nbucketing scheme described in section 3.1. In the\nimplementation new-one-count, we have diﬀerent\nparameters β n and γ n in equation (4) for each n.\n5 Results\nIn Figure 2, we display the performance of the\ninterp-baseline method for bigram and trigram\nmodels on TIPSTER, Brown, and the WSJ subset\nof TIPSTER. In Figures 3–6, we display the relative\nperformance of various smoothing techniques with\nrespect to the baseline method on these corpora, as\nmeasured by diﬀerence in entropy . In the graphs\non the left of Figures 2–4, each point represents an\naverage over ten runs; the error bars represent the\nempirical standard deviation over these runs. Due\nto resource limitations, we only performed multiple\nruns for data sets of 50,000 sentences or less. Each\npoint on the graphs on the right represents a sin-\ngle run, but we consider sizes up to the amount of\ndata available. The graphs on the bottom of Fig-\nures 3–4 are close-ups of the graphs above, focusing\non those algorithms that perform better than the\nbaseline. T o give an idea of how these cross-entropy\ndiﬀerences translate to perplexity , each 0.014 bits\ncorrespond roughly to a 1% change in perplexity .\nIn each run except as noted below, optimal val-\nues for the parameters of the given technique were\nsearched for using Powell’s search algorithm as real-\nized in Numerical Recipes in C (Press et al., 1988,\npp. 309–317). Parameters were chosen to optimize\nthe cross-entropy of one of the development test sets\nassociated with the given training set. T o constrain\nthe search, we searched only those parameters that\nwere found to aﬀect performance signiﬁcantly , as\nveriﬁed through preliminary experiments over sev-\neral data sizes. F or katz and church-gale, we did\nnot perform the parameter search for training sets\nover 50,000 sentences due to resource constraints,\nand instead manually extrapolated parameter val-\nues from optimal values found on smaller data sizes.\nW e ran interp-del-int only on sizes up to 50,000\nsentences due to time constraints.\nF rom these graphs, we see that additive smooth-\ning performs poorly and that methods katz and\ninterp-held-out consistently perform well. Our\nimplementation church-gale performs poorly ex-\ncept on large bigram training sets, where it performs\nthe best. The novel methods new-avg-count and\nnew-one-count perform well uniformly across train-\ning data sizes, and are superior for trigram models.\nNotice that while performance is relatively consis-\ntent across corpora, it varies widely with respect to\ntraining set size and n-gram order.\nThe method interp-del-int performs signiﬁ-\ncantly worse than interp-held-out, though they\ndiﬀer only in the data used to train the λ ’s. However,\nwe delete one word at a time in interp-del-int; we\n7.5\n8\n8.5\n9\n9.5\n10\n10.5\n11\n11.5\n100 1000 10000\ncross-entropy of test data (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nTIPSTER bigram\nTIPSTER trigram\nWSJ bigram\nWSJ trigram\n6.5\n7\n7.5\n8\n8.5\n9\n9.5\n10\n10.5\n11\n11.5\n100 1000 10000 100000 1e+06 1e+07\ncross-entropy of test data (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size\nTIPSTER bigram\nTIPSTER trigram\nBrown bigram\nBrown trigram\nWSJ bigram\nWSJ trigram\nFigure 2: Baseline cross-entropy on test data; graph on left displays averages over ten runs for training sets\nup to 50,000 sentences, graph on right displays single runs f or training sets up to 10,000,000 sentences\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n0.04\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nFigure 3: T rigram model on TIPSTER data; relative performan ce of various methods with respect to baseline;\ngraphs on left display averages over ten runs for training se ts up to 50,000 sentences, graphs on right display\nsingle runs for training sets up to 10,000,000 sentences; to p graphs show all algorithms, bottom graphs zoom\nin on those methods that perform better than the baseline met hod\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\nchurch-gale\nFigure 4: Bigram model on TIPSTER data; relative performanc e of various methods with respect to baseline;\ngraphs on left display averages over ten runs for training se ts up to 50,000 sentences, graphs on right display\nsingle runs for training sets up to 10,000,000 sentences; to p graphs show all algorithms, bottom graphs zoom\nin on those methods that perform better than the baseline met hod\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~21 words/sentence)\nbigram model\nchurch-gale\ninterp-del-int\nkatz\ninterp-held-out\nnew-avg-count new-one-count\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~21 words/sentence)\ntrigram model\ninterp-del-int\nkatz\ninterp-held-out\nnew-avg-count\nnew-one-count\nFigure 5: Bigram and trigram models on Brown corpus; relativ e performance of various methods with respect\nto baseline\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000 100000 1e+06\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nbigram model\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\nchurch-gale\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n0.04\n100 1000 10000 100000 1e+06\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\ntrigram model\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nkatz\nFigure 6: Bigram and trigram models on W all Street Journal co rpus; relative performance of various methods\nwith respect to baseline\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n0.001 0.01 0.1 1 10 100 1000\ndifference in test cross-entropy from baseline (bits/token)\ndelta\nperformance of katz with respect to delta\n100 sent\n1,000 sent\n10,000 sent\n50,000 sent\n-0.14\n-0.13\n-0.12\n-0.11\n-0.1\n-0.09\n-0.08\n-0.07\n-0.06\n-0.05\n1 10 100 1000 10000 100000\ndifference in test cross-entropy from baseline (bits/token)\nminimum number of counts per bucket\nperformance of new-avg-count with respect to c-min\n100 sent\n10,000 sent\n1,000,000 sent\n10,000,000 sent\nFigure 7: Performance of katz and new-avg-count with respect to parameters δ and cmin, respectively\nhypothesize that deleting larger chunks would lead\nto more similar performance.\nIn Figure 7, we show how the values of the pa-\nrameters δ and cmin aﬀect the performance of meth-\nods katz and new-avg-count, respectively , over sev-\neral training data sizes. Notice that poor parameter\nsetting can lead to very signiﬁcant losses in perfor-\nmance, and that optimal parameter settings depend\non training set size.\nT o give an informal estimate of the diﬃculty of\nimplementation of each method, in T able 1 we dis-\nplay the number of lines of C++ code in each imple-\nmentation excluding the core code common across\ntechniques.\n3 T o implement the baseline method, we just used the\ninterp-held-out code as it is a special case. Written\nanew, it probably would have been about 50 lines.\nMethod Lines\ninterp-baseline3 400\nplus-one 40\nplus-delta 40\nkatz 300\nchurch-gale 1000\ninterp-held-out 400\ninterp-del-int 400\nnew-avg-count 400\nnew-one-count 50\nT able 1: Implementation diﬃculty of various meth-\nods in terms of lines of C++ code\n6 Discussion\nT o our knowledge, this is the ﬁrst empirical compari-\nson of smoothing techniques in language modeling of\nsuch scope: no other study has used multiple train-\ning data sizes, corpora, or has performed parameter\noptimization. W e show that in order to completely\ncharacterize the relative performance of two tech-\nniques, it is necessary to consider multiple training\nset sizes and to try both bigram and trigram mod-\nels. Multiple runs should be performed whenever\npossible to discover whether any calculated diﬀer-\nences are statistically signiﬁcant. F urthermore, we\nshow that sub-optimal parameter selection can also\nsigniﬁcantly aﬀect relative performance.\nW e ﬁnd that the two most widely used techniques,\nKatz smoothing and Jelinek-Mercer smoothing, per-\nform consistently well across training set sizes for\nboth bigram and trigram models, with Katz smooth-\ning performing better on trigram models produced\nfrom large training sets and on bigram models in\ngeneral. These results question the generality of the\nprevious reference result concerning Katz smooth-\ning: Katz (1987) reported that his method slightly\noutperforms an unspeciﬁed version of Jelinek-Mercer\nsmoothing on a single training set of 750,000 words.\nF urthermore, we show that Church-Gale smooth-\ning, which previously had not been compared with\ncommon smoothing techniques, outperforms all ex-\nisting methods on bigram models produced from\nlarge training sets. Finally , we ﬁnd that our novel\nmethods average-count and one-count are superior\nto existing methods for trigram models and perform\nwell on bigram models; method one-count yields\nmarginally worse performance but is extremely easy\nto implement.\nIn this study , we measure performance solely\nthrough the cross-entropy of test data; it would\nbe interesting to see how these cross-entropy diﬀer-\nences correlate with performance in end applications\nsuch as speech recognition. In addition, it would be\ninteresting to see whether these results extend to\nﬁelds other than language modeling where smooth-\ning is used, such as prepositional phrase attachment\n(Collins and Brooks, 1995), part-of-speech tagging\n(Church, 1988), and stochastic parsing (Magerman,\n1994).\nAcknowledgements\nThe authors would like to thank Stuart Shieber and\nthe anonymous reviewers for their comments on pre-\nvious versions of this paper. W e would also like to\nthank William Gale and Geoﬀrey Sampson for sup-\nplying us with code for “Good-T uring frequency esti-\nmation without tears.” This research was supported\nby the National Science F oundation under Grant No.\nIRI-93-50192 and Grant No. CDA-94-01024. The\nsecond author was also supported by a National Sci-\nence F oundation Graduate Student F ellowship.\nReferences\n[Bahl, Jelinek, and Mercer1983] Bahl, Lalit R., F red-\nerick Jelinek, and Robert L. Mercer. 1983.\nA maximum likelihood approach to continuous\nspeech recognition. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , PAMI-\n5(2):179–190, March.\n[Brown et al.1990] Brown, Peter F., John Cocke,\nStephen A. DellaPietra, Vincent J. DellaPietra,\nF rederick Jelinek, John D. Laﬀerty , Robert L.\nMercer, and Paul S. Roossin. 1990. A statistical\napproach to machine translation. Computational\nLinguistics, 16(2):79–85, June.\n[Brown et al.1992] Brown, Peter F., Stephen A. Del-\nlaPietra, Vincent J. DellaPietra, Jennifer C. Lai,\nand Robert L. Mercer. 1992. An estimate of an\nupper bound for the entropy of English. Compu-\ntational Linguistics, 18(1):31–40, March.\n[Chen1996] Chen, Stanley F. 1996. Building Proba-\nbilistic Models for Natural Language. Ph.D. the-\nsis, Harvard University . In preparation.\n[Church1988] Church, Kenneth. 1988. A stochastic\nparts program and noun phrase parser for unre-\nstricted text. In Proceedings of the Second Con-\nference on Applied Natural Language Processing,\npages 136–143.\n[Church and Gale1991] Church, Kenneth W. and\nWilliam A. Gale. 1991. A comparison of the\nenhanced Good-Turing and deleted estimation\nmethods for estimating probabilities of English bi-\ngrams. Computer Speech and Language, 5:19–54.\n[Collins and Brooks1995] Collins, Michael and James\nBrooks. 1995. Prepositional phrase attachment\nthrough a backed-oﬀ model. In David Y arowsky\nand Kenneth Church, editors, Proceedings of the\nThird Workshop on Very Large Corpora , pages\n27–38, Cambridge, MA, June.\n[Gale and Church1990] Gale, William A. and Ken-\nneth W. Church. 1990. Estimation procedures\nfor language context: poor estimates are worse\nthan none. In COMPSTAT, Proceedings in Com-\nputational Statistics, 9th Symposium , pages 69–\n74, Dubrovnik, Y ugoslavia, September.\n[Gale and Church1994] Gale, William A. and Ken-\nneth W. Church. 1994. What’s wrong with\nadding one? In N. Oostdijk and P. de Haan,\neditors, Corpus-Based Research into Language .\nRodolpi, Amsterdam.\n[Gale and Sampson1995] Gale, William A. and Ge-\noﬀrey Sampson. 1995. Good-Turing frequency\nestimation without tears. Journal of Quantitative\nLinguistics, 2(3). T o appear.\n[Good1953] Good, I.J. 1953. The population fre-\nquencies of species and the estimation of popu-\nlation parameters. Biometrika, 40(3 and 4):237–\n264.\n[Jeﬀreys1948] Jeﬀreys, H. 1948. Theory of Probabil-\nity. Clarendon Press, Oxford, second edition.\n[Jelinek and Mercer1980] Jelinek, F rederick\nand Robert L. Mercer. 1980. Interpolated esti-\nmation of Markov source parameters from sparse\ndata. In Proceedings of the Workshop on Pattern\nRecognition in Practice, Amsterdam, The Nether-\nlands: North-Holland, May .\n[Johnson1932] Johnson, W.E. 1932. Probability: de-\nductive and inductive problems. Mind, 41:421–\n423.\n[Katz1987] Katz, Slava M. 1987. Estimation of prob-\nabilities from sparse data for the language model\ncomponent of a speech recognizer. IEEE Transac-\ntions on Acoustics, Speech and Signal Processing,\nASSP-35(3):400–401, March.\n[Kernighan, Church, and Gale1990] Kernighan,\nM.D., K.W. Church, and W.A. Gale. 1990. A\nspelling correction program based on a noisy chan-\nnel model. In Proceedings of the Thirteenth Inter-\nnational Conference on Computational Linguis-\ntics, pages 205–210.\n[Lidstone1920] Lidstone, G.J. 1920. Note on the gen-\neral case of the Bayes-Laplace formula for induc-\ntive or a posteriori probabilities. Transactions of\nthe Faculty of Actuaries, 8:182–192.\n[MacKay and Peto1995] MacKay , David J. C. and\nLinda C. Peto. 1995. A hierarchical Dirichlet\nlanguage model. Natural Language Engineering,\n1(3):1–19.\n[Magerman1994] Magerman, David M. 1994. Natu-\nral Language Parsing as Statistical Pattern Recog-\nnition. Ph.D. thesis, Stanford University , F ebru-\nary .\n[Nadas1984] Nadas, Arthur. 1984. Estimation of\nprobabilities in the language model of the IBM\nspeech recognition system. IEEE Transactions on\nAcoustics, Speech and Signal Processing , ASSP-\n32(4):859–861, August.\n[Press et al.1988] Press, W.H., B.P. Flannery , S.A.\nT eukolsky , and W.T. V etterling. 1988. Numer-\nical Recipes in C . Cambridge University Press,\nCambridge.",
  "topic": "Smoothing",
  "concepts": [
    {
      "name": "Smoothing",
      "score": 0.5914872884750366
    },
    {
      "name": "Econometrics",
      "score": 0.4403931200504303
    },
    {
      "name": "Computer science",
      "score": 0.4096328616142273
    },
    {
      "name": "Economics",
      "score": 0.2816295623779297
    },
    {
      "name": "Statistics",
      "score": 0.231387197971344
    },
    {
      "name": "Mathematics",
      "score": 0.22778725624084473
    }
  ],
  "cited_by": 631
}