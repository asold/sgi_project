{
  "title": "Generative language models exhibit social identity biases",
  "url": "https://openalex.org/W4405298817",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2547360108",
      "name": "Tian-Cheng Hu",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4382511843",
      "name": "Yara Kyrychenko",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A3121799347",
      "name": "Steve Rathje",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2098750953",
      "name": "Nigel Collier",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2250208338",
      "name": "Sander van der Linden",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2041726629",
      "name": "Jon Roozenbeek",
      "affiliations": [
        "University of Cambridge",
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A2547360108",
      "name": "Tian-Cheng Hu",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4382511843",
      "name": "Yara Kyrychenko",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A3121799347",
      "name": "Steve Rathje",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2098750953",
      "name": "Nigel Collier",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2250208338",
      "name": "Sander van der Linden",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2041726629",
      "name": "Jon Roozenbeek",
      "affiliations": [
        "King's College London",
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963612262",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W4206285331",
    "https://openalex.org/W4401947350",
    "https://openalex.org/W2124751389",
    "https://openalex.org/W2904152473",
    "https://openalex.org/W2801945329",
    "https://openalex.org/W2147602649",
    "https://openalex.org/W2098069028",
    "https://openalex.org/W4293117726",
    "https://openalex.org/W2022514786",
    "https://openalex.org/W3047117292",
    "https://openalex.org/W4365388135",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4389519968",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4366769817",
    "https://openalex.org/W4392153984",
    "https://openalex.org/W4319167005",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W4394868309",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W6867136294",
    "https://openalex.org/W4386977707",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4386942223",
    "https://openalex.org/W2166076367",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285190530",
    "https://openalex.org/W3128553165",
    "https://openalex.org/W3211639606",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4401506443",
    "https://openalex.org/W52410547",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W3176626547",
    "https://openalex.org/W2173132378",
    "https://openalex.org/W4403039308",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2063534953",
    "https://openalex.org/W4247340923",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4387892136",
    "https://openalex.org/W4385564993",
    "https://openalex.org/W4385298751",
    "https://openalex.org/W4362707004",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W6858023062",
    "https://openalex.org/W6861980801",
    "https://openalex.org/W4308244210",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W6845921028",
    "https://openalex.org/W4388890574",
    "https://openalex.org/W6857464309",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W3177071108",
    "https://openalex.org/W6925398127",
    "https://openalex.org/W2328503557",
    "https://openalex.org/W2915177913"
  ],
  "abstract": "Abstract Social identity biases, particularly the tendency to favor one’s own group (ingroup solidarity) and derogate other groups (outgroup hostility), are deeply rooted in human psychology and social behavior. However, it is unknown if such biases are also present in artificial intelligence systems. Here we show that large language models (LLMs) exhibit patterns of social identity bias, similarly to humans. By administering sentence completion prompts to 77 different LLMs (for instance, ‘We are…’), we demonstrate that nearly all base models and some instruction-tuned and preference-tuned models display clear ingroup favoritism and outgroup derogation. These biases manifest both in controlled experimental settings and in naturalistic human–LLM conversations. However, we find that careful curation of training data and specialized fine-tuning can substantially reduce bias levels. These findings have important implications for developing more equitable artificial intelligence systems and highlight the urgent need to understand how human–LLM interactions might reinforce existing social biases.",
  "full_text": "Nature Computational Science | Volume 5 | January 2025 | 65–75\n 65\nnature computational science\nhttps://doi.org/10.1038/s43588-024-00741-1\nAnalysis\nGenerative language models exhibit social \nidentity biases\n \nTiancheng Hu    1,5 , Yara Kyrychenko    2,5 , Steve Rathje3, Nigel Collier1, \nSander van der Linden2 & Jon Roozenbeek    2,4\nSocial identity biases, particularly the tendency to favor one’s own group \n(ingroup solidarity) and derogate other groups (outgroup hostility), are \ndeeply rooted in human psychology and social behavior. However, it is \nunknown if such biases are also present in artificial intelligence systems. \nHere we show that large language models (LLMs) exhibit patterns of social \nidentity bias, similarly to humans. By administering sentence completion \nprompts to 77 different LLMs (for instance, ‘We are… ’), we demonstrate that \nnearly all base models and some instruction-tuned and preference-tuned \nmodels display clear ingroup favoritism and outgroup derogation. These \nbiases manifest both in controlled experimental settings and in naturalistic \nhuman–LLM conversations. However, we find that careful curation of \ntraining data and specialized fine-tuning can substantially reduce bias levels. \nThese findings have important implications for developing more equitable \nartificial intelligence systems and highlight the urgent need to understand \nhow human–LLM interactions might reinforce existing social biases.\nLarge language models (LLMs) such as ChatGPT have exploded in \npopularity1. Investigating the political and social biases of LLMs has \nalso rapidly become an important research topic 2. Previous work \nhas shown that language models tend to exhibit human-like biases \nwith respect to specific protected groups such as gender, ethnicity \nor religious orientation3–6. However, researchers have yet to explore \nwhether LLMs exhibit the more general group biases that are theorized \nto underlie much of societal discrimination—‘us versus them’ . Essen-\ntial to the study of affective polarization in the United States, as well \nas other intergroup conflicts7,8, the social-psychological theories of \nsocial identity and self-categorization9,10 posit that when an individual’s \nsocial or group identity is activated, they tend to display preferential \nattitudes and behaviors toward their own group (ingroup solidarity) \nand distrust and dislike toward other groups (outgroup hostility)9,11,12. \nSocial psychologists have shown that even arbitrary distinctions (for \nexample, a preference for the abstract painters Klee or Kandinsky) \ncan lead to immediate intergroup discrimination13,14. Such discrimina-\ntion is also visible in language, which tends to be more abstract when \npeople describe their outgroups’ negative behavior and resort more \nto dehumanizing terms 15,16. LLMs could inadvertently reinforce or \namplify such identity-based biases in humans, carrying implications \nfor important societal issues such as intergroup conflict and political \npolarization17–19.\nAn older technique known as word embeddings has been shown \nto capture human-like social biases when trained on a large-scale web \ncorpus20. T oday’s state-of-the-art language models exhibit far greater \ncomplexity, which also comes with new opportunities and challenges. \nOn the one hand, these models are shaped by human training data and \nexhibit many human abilities, such as reasoning by analogy21, theory \nof mind22, and personality23, which makes them compelling proxies for \nstudying human behavior and attitude change24,25. On the other hand, \nLLMs can influence and persuade humans26, with research demonstrat-\ning that LLM-based writing assistants are capable of swaying people’s \nviews27. Evaluating the expanding capabilities of LLMs is a complex \nresearch area28,29, with group-specific bias benchmarks shown to be \ntime-consuming to develop and utilize30–33, and the overall field lack-\ning measurement validity and theoretical grounding 30,34. However, \ngiven the speed and scale of LLM adoption, even relatively minor social \nReceived: 22 May 2024\nAccepted: 7 November 2024\nPublished online: 12 December 2024\n Check for updates\n1Department of Theoretical and Applied Linguistics, University of Cambridge, Cambridge, UK. 2Department of Psychology, University of Cambridge, \nCambridge, UK. 3Department of Psychology, New York University, New York, NY, USA. 4Department of War Studies, King’s College London, London, UK. \n5These authors contributed equally: Tiancheng Hu, Yara Kyrychenko.  e-mail: th656@cam.ac.uk; yk408@cam.ac.uk\nNature Computational Science | Volume 5 | January 2025 | 65–75 66\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\ndictionary-based approaches in sentiment analysis51–53 and (2) this par-\nticular fine-tuned classifier provides strong sentiment analysis perfor-\nmance, with a neutral sentiment class50. We also conducted robustness \nchecks with ten alternate sentiment classification strategies, including \nother deep-learning classifiers and dictionaries such as VADER, and inter-\nnal meta-analyses, which show broad agreement with the main results \nacross different methodologies (Supplementary Sections 5 and 6).\nIf ingroup sentences are more likely to be classified as positive \n(versus neutral or negative) than outgroup sentences, we interpret it as \nevidence of a model displaying ingroup solidarity. If outgroup sentences \nare more likely to be classified as negative (versus neutral or positive) \nthan ingroup sentences, it suggests that the model exhibits outgroup \nhostility. Example model-generated sentences are shown in Table 1.\nT o estimate ingroup solidarity, that is, the odds of an ingroup sen-\ntence to be classified as positive as compared to an outgroup sentence, \nwe use the 2,000 group sentences to fit a logistic regression predicting \npositive sentiment based on a binary indicator of sentence group with \noutgroup as the reference category, controlling for type-to-token \nratio54 and sentence length as proxies for data generation quality. \nSimilarly, to estimate outgroup hostility, that is, the odds of an out -\ngroup sentence (versus ingroup) to be classified as negative, we fit a \nlogistic regression predicting negative sentiment using an indicator \nof sentence group with ingroup as reference, controlling for the same \nfactors as above. In Study 1, in all individual LLM regressions reported, \nwe deem results significant if P < 0.0004, obtained by dividing 0.05 by \nthe total number of tests with the default prompt (112).\nOf the 56 models tested with the default prompt, only four did \nnot exhibit ingroup solidarity (the smallest BLOOMZ, Cerebras-GPT, \ntext-bison and Gemme-7B-IT), and six did not show outgroup hostil-\nity (BLOOM-560M, all of the BLOOMZ family, and text-bison; Fig. 1a,b \npresents outliers, Supplementary Tables 3–5 provide all coefficients \nand Supplementary Figs. 5–10 variation across sentiment classifiers). \nConducting a mixed-effects logistic regression on pooled data with \nmodel name as a random effect showed that an ingroup (versus out -\ngroup) sentence was 93% more likely to be positive, indicating a general \npattern of ingroup solidarity. Similarly, an outgroup sentence was \n115% more likely to be negative, suggesting strong outgroup hostility \n(Supplementary Table 10).\nand political biases left undetected could potentially lead to adverse \noutcomes, for instance through human–algorithmic feedback loops19.\nIn this Analysis we present a large-scale and comprehensive test of \nsocial identity biases in LLMs. We develop a simple probe of the overall \ningroup solidarity and outgroup hostility of an LLM that requires only \nprompt-completion capabilities available through application pro -\ngramming interfaces (APIs). Across three studies, we tested whether \n(1) LLMs possess human-like social identity biases, (2) social identity \nbiases are influenced by the models’ training data and (3) these biases \nmanifest in real-world human–artificial intelligence (AI) conversations. \nStudy 1 examines affective polarization in 77 different LLMs, including \nbase models and instruction-tuned and preference-tuned models. We \nprompted each model to generate 2,000 sentences starting with ‘We \nare’ or ‘They are’ and assess their sentiment using a separate pretrained \nclassification model. We also compared the ingroup solidarity and out-\ngroup hostility of LLMs to those of humans, estimated from large-scale \nweb corpora commonly used to pretrain models. Study 2 assesses how \ntraining data affect models’ social identity biases by fine-tuning LLMs \non a corpus of US partisan Twitter (now X) data. Study 3 tests whether \nthe biases found in Studies 1 and 2 are evident in real-world conversa-\ntions between humans and LLMs using two open-source datasets: \nWildChat35, which contains over half a million user conversations with \nChatGPT, and LMSYS-Chat-1M36, containing one million conversations \nwith 25 different state-of-the-art language models. Overall, we find that \nmany LLMs exhibit ingroup solidarity and outgroup hostility, that these \nbiases can be mitigated by training-data curation, and that these biases \nare present in real-world human–LLM conversations.\nResults\nStudy 1—measuring social identity biases in LLMs\nWe first investigate the extent of social identity biases across 77 LLMs of \ntwo types: base LLMs, such as GPT-337, Llama 238, Pythia39, Gemma40 and \nMixtral41, and LLMs fine-tuned for instruction-following, such as GPT-442, \nGPT-3.5 (text-davinci-003)43, Dolly2.044, Alpaca45 and OpenChat3.546  \n(a full model list is provided in the Methods). In these model sizes, M \nstands for million parameters and B stands for billion parameters. For \nexample, GPT-2 124M has 124 million parameters, while GPT-3 175B has \n175 billion parameters. These numbers reflect the total count of learn-\nable weights in the neural network. T o assess the social identity biases for \neach language model, we generated a total of 2,000 sentences prompt-\ning with ‘We are’ and ‘They are’ , which are associated with the ‘us versus \nthem’ dynamics47, excluding sentences that did not pass minimal quality \nand diversity checks (Methods). We call sentences starting with ‘We \nare’ ingroup sentences and those starting with ‘They are’ outgroup sen-\ntences. For many models, it suffices to use the prompt ‘We are’ or ‘They \nare’ and let the model complete the sentence by repeatedly generating \nthe next tokens. We refer to this prompt setting as the ‘default prompt’ .\nCurrently, the vast majority of consumer-facing models are subject \nto instruction and preference fine-tuning to improve interactability in \nuser experience and to better align with human preferences. Therefore, \nour analysis also encompasses a diverse set of such instruction and \npreference-fine-tuned models. Often, these models are optimized for \nchat-based applications, which renders it impossible to test them with \nthe default prompt. A rudimentary prompt, such as ‘Can you help me \nfinish a sentence? The sentence is: we are’ , typically also yields repetitive \nsentences (Supplementary Section 1 presents examples). T o circum-\nvent this issue, we introduced additional context to this rudimentary \nprompt, utilizing sentences from the C4 corpus 48, a large-scale web \ncorpus frequently used in language model pretraining. We refer to this \nrefined prompt set-up as the ‘instruction prompt’ (Methods).\nWe then classified the sentences into positive, neutral or nega-\ntive using a sentiment classifier based on RoBERTa 49, which was \nfine-tuned for sentiment classification 50. We use this sentiment \nclassifier for our analyses throughout all studies in the main text \nbecause (1) machine-learning-based classifiers generally outperform \nTable 1 | Example ingroup and outgroup sentences\nText Model RoBERTa VADER TTR\nThey are in the business of \ncollecting a fee for doing \nresearch for you.\nDolly2.0-7B Neutral 0 0.9286\nThey are just a bunch of \ndumb f**ks.\nOPT-IML-30B Negative −0.7506 1\nThey are the true brothers, \nthe true cousins, the true \nsisters, the true daughters \nof all men, the true friends \nof all people.\nCerebras- \nGPT-6.7B\nPositive 0.9442 0.565\nWe are living through a \ntime in which society at all \nlevels is searching for new \nways to think about and \nlive out relationships.\ndavinci Neutral 0 1\nWe are also sorry for all \nthe inconvenience this \nhas caused to you, but we \nare unable to change the \nterms that have existed.\nBLOOM-1.1B Negative −0.2263 0.8333\nWe are a group of talented \nyoung people who are \nmaking it to the next level.\n GPT-2-large-\n774M\nPositive 0.5106 0.9375\nSentences are presented along with the model that generated them, RoBERTa and VADER \nsentiment, and a measure of lexical diversity called type-to-token ratio (TTR).\nNature Computational Science | Volume 5 | January 2025 | 65–75\n 67\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\na LLMs with default prompt\nGPT−2GPT−3 & 3.5BLOOMBLOOMZOPTOPT−IMLPythiaDolly2.0Cerebras−GPTOther\nText−bison@001\nOLMo−7B\nJ2−Jumbo−Instruct\nMixtral−8x7B\nMistral−7B\nCerebras−GPT−13B\nCerebras−GPT−6.7B\nCerebras−GPT−2.7B\nCerebras−GPT−1.3B\nCerebras−GPT−590M\nCerebras−GPT−256M\nCerebras−GPT−111M\nDolly2.0−12B\nDolly2.0−7B\nDolly2.0−3B\nPythia−12B\nPythia−6.9B\nPythia−2.8B\nPythia−1.4B\nPythia−1B\nPythia−410M\nPythia−160M\nPythia−70M\nOPT−IML−30B\nOPT−IML−1.3B\nOPT−66B\nOPT−30B\nOPT−13B\nOPT−6.7B\nOPT−2.7B\nOPT−1.3B\nOPT−350M\nOPT−125M\nBLOOMZ−3B\nBLOOMZ−1B7\nBLOOMZ−1B1\nBLOOMZ−560M\nBLOOM−3B\nBLOOM−1B7\nBLOOM−1B1\nBLOOM−560M\nText−davinci−003\nDavinci\nGPT−2−XL−1.5B\nGPT−2−large−774M\nGPT−2−medium−355M\nGPT−2−124M\n0 1 2 3 4 5\nOdds ratio\nOdds ratio\nOutgroup hostility\nIngroup solidarity\nb Outlier LLMs with default prompt\nLLMs with instruction prompt\nLLaMALLaMA 2Gemma Gemma−7B−IT\nGemma−7B\nLlama 2−70B\nLlama 2−13B\nLlama 2−7B\nLLaMA−65B\nLLaMA−33B\nLLaMA−13B\nLLaMA−7B\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nOdds ratio\nc\nGPTLLaMA 2Dolly2.0Flan\nGoogle\nOLMoTulu 2Other\nMixtral−8x7B−instruct\nGemma−7B−IT\nOpenChat3.5−7B\nStarling−7B\nZephyr−7B−beta\nAlpaca−7B\nJ2−Jumbo−Instruct\nTulu−2−DPO−70B\nTulu−2−DPO−13B\nTulu−2−DPO−7B\nTulu−2−70B\nTulu−2−13B\nTulu−2−7B\nOLMo−7B−SFT\nOLMo−7B−instruct\nChat−bison@001\nText−bison@001\nFlan−UL2−20B\nFlan−T5−XXL−11B\nFlan−T5−XL−3B\nDolly2.0−12B\nDolly2.0−7B\nDolly2.0−3B\nLlama 2−70B−chat\nLlama 2−13B−chat\nLlama 2−7B−chat\nText−davinci−003\nGPT−4\n0 1 2 3 4 5\nOdds ratio\nHuman training datad\nTraining data The Pile\nOLM\nGPT2\nC4\n0 1 2 3 4 5\nFig. 1 | Study 1—ingroup solidarity and outgroup hostility of LLMs and human \ndatasets. The plots show the results of individual logistic regressions predicting \npositive (or negative) sentiment based on whether a sentence is ingroup (or \noutgroup), controlling for the number of words and type-to-token ratio, across \nmodel and human-written texts. In the model names, M stands for million \nparameters and B stands for billion parameters. For example, GPT-2 124M has 124 \nmillion parameters, while GPT-3 175B has 175 billion parameters. These numbers \nreflect the total count of learnable weights in the neural network. Data are presented \nas odds ratios with error bars for 95% confidence intervals. a, Social identity biases \nin LLMs tested with the default prompt (N = 94,000 sentences). b, Social identity \nbiases in LLMs tested with the default prompt with outlier levels of outgroup \nhostility (N = 18,000 sentences). c, Social identity biases in LLMs tested with the \ninstruction prompt (N = 76,000 sentences). d, Social identity biases in human data \nobtained from four different pretraining corpora (N = 16,000 sentences).\nNature Computational Science | Volume 5 | January 2025 | 65–75 68\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\nOur findings for instruction fine-tuned models prompted with \nthe instruction prompt indicate that they exhibited lower ingroup \nsolidarity and outgroup hostility compared to the base LLMs (Fig. 1c). \nThis was evidenced by lower odds ratios, which mostly remain below 2, \nand several models demonstrating statistically non-significant ingroup \nsolidarity or outgroup hostility (Supplementary Table 12). A small selec-\ntion of models (Dolly2.0 series, text-bison@001, J2-Jumbo-Instruct \nand Gemma-7B-IT) were capable of responding to both the default \nand instruction prompts, permitting a comparison. The comparison \nyielded mixed outcomes: J2-Jumbo-Instruct presented significantly \nreduced ingroup solidarity and outgroup hostility in the instruction \nprompt setting. Conversely, Dolly2.0 displayed a considerable decrease \nonly in ingroup solidarity, while text-bison@001 showed an increase \nin both ingroup solidarity and outgroup hostility. Gemma-7B-IT had \na decrease in outgroup hostility in the instruction prompt setting.\nT o juxtapose social identity biases measured in LLMs against \nhuman-level biases, we obtained human-written ingroup and outgroup \nsentences from large-scale web corpora commonly used to pretrain \nLLMs, including C448, The Pile55, OpenWebT ext56 and the November–\nDecember 2022 edition of OLM57. We processed these sentences in the \nsame way as LLM-generated sentences, thereby establishing a human \nbaseline level of ingroup solidarity and outgroup hostility, and ran -\ndomly subsampled the datasets to match the scale of LLM-generated \nsentences. We found statistically significant social identity biases in all \nof the four pretraining corpora (Fig. 1d). C4 and OLM display a slightly \nhigher outgroup derogation than ingroup solidarity, whereas GPT-2 \nand The Pile show slightly higher ingroup solidarity. Pooling the four \ndifferent pretraining corpora together, a mixed-effects regression \nshows that ingroup sentences are 68% more likely to be positive and \noutgroup sentences are 70% more likely to be negative (Supplementary \nTable 20). We then compared human bias levels to the model-estimated \nvalues for models with the default prompt and found that the ingroup \nsolidarity bias of 44 LLMs was statistically the same as the human aver-\nage, while 42 models had a statistically similar outgroup hostility bias \n(Supplementary Section 3).\nAs LLMs have been shown to follow scaling laws on many tasks58, \nwith larger models generally performing better, we investigated \nwhether the size of the LLM influences the extent of the social identity \nbiases. An additional regression analysis among the 13 model families \nfor which we tested multiple sizes with size as a predictor and model \nfamily as the random effect shows that, although there is no increase \nin ingroup solidarity with model size, there is a very small increase in \noutgroup hostility (Supplementary Table 11).\nMoreover, because instruction and preference fine-tuning has \nbeen shown to reduce certain types of bias in LLMs59, we wanted to test \nwhether instruction and preference fine-tuned models of the same fam-\nily and size exhibit different social identity biases as compared to the \ncorresponding base models (Supplementary Table 15). We compared \nopen-source LLMs with and without instruction fine-tuning (OPT ver-\nsus OPT-IML, BLOOM series versus BLOOMZ, Dolly2.0 versus Pythia). \nA mixed-effects logistic regression with model family as random effect \nshowed that instruction fine-tuned models had statistically signifi -\ncantly lower outgroup hostility but not ingroup solidarity (Supplemen-\ntary Table 13). We also tested whether preference-tuning has an effect \non the social identity biases by comparing base and preference-tuned \nmodels (LLaMa 2 series versus LLaMa 2 Chat and Tulu 2 DPO, OLMo-7B \nversus OLMo-7B-Instruct, Mistral-7B versus Starling-7B; Mixtral-8 × 7B \nversus OpenChat3.5-7B; preference-tuned models were prompted with \nthe instruction prompt). We found that preference fine-tuned models \ntend to exhibit lower ingroup solidarity and outgroup hostility (Sup-\nplementary Table 14).\nStudy 2—training data effects on social identity biases\nIn Study 2, we aimed to evaluate the impact of the training corpora of \nLLMs on social identity biases. Given the prohibitive computational \nresources required to train a set of LLMs from scratch with data devoid \nof social identity biases, we decided to fine-tune already pretrained \nLLMs. Doing so updates the LLMs’ parameters on text not necessarily \nseen in the pretraining stage. Typically, LLMs are fine-tuned to adapt \nfrom a general-purpose model to a specific use case or domain. This \napproach allows us to approximate the impact of pretraining data \nwithout the need for resource-intensive training from scratch.\nWe utilized a dataset of previously collected Twitter (now X) posts \nfrom US Republicans and Democrats60 to fine-tune all the models from \nthe GPT-2, BLOOM and BLOOMZ families. We show a comparison of \nmodel-generated sentences before and after fine-tuning in Table 2. \nAfter fine-tuning, all models exhibited more ingroup solidarity and sub-\nstantially more outgroup hostility (Fig. 2 and Supplementary Table 27). \nRunning a mixed-effects logistic regression again (including model \nand partisanship as random effects, with RoBERTa sentiment as the \ndependent variable), an ingroup sentence was 361% more likely to be \npositive, and an outgroup sentence was 550% more likely to be negative, \ncompared to 86% and 83% for the same models without fine-tuning \n(Supplementary Tables 17 and 18).\nWe then pooled the data from the partisan models and their \nnon-partisan versions and ran a mixed-effects logistic regression with \nbinary indicators of sentence type, whether the model was fine-tuned \nor not, and their interaction (with the same random effects as above). \nAlthough all sentences are less likely to be positive after fine-tuning, \ningroup sentences are impacted less. Notably, the same analysis for \noutgroup hostility showed that outgroup sentences are especially likely \nto be negative after fine-tuning (Supplementary Table 19). This signals \nan asymmetric effect, where fine-tuning with partisan social media data \nhas an especially pronounced effect on outgroup hostility, in line with \nprevious research on the viral potential of outgroup language61,62. Then \nagain, other research (for instance, ref. 63 ) has instead emphasized \nthe importance of ingroup solidarity as a driver of online interactions.\nGiven the large increase in both ingroup solidarity and outgroup \nhostility in the models after fine-tuning, we hypothesized that the \ndegree of social identity bias in LLMs is influenced by the training data. \nWe therefore fine-tuned GPT-2 seven separate times with full data, with \n50% ingroup positive sentences (or outgroup negative, or both), and \nwith 0% ingroup positive sentences (or outgroup negative, or both). \nBecause the impact of partisan fine-tuning seems very similar across \nmodels (Fig. 3 and Supplementary Table 28), we used the GPT-2 model \nwith 124 million parameters as the test LLM for this study. The ingroup \nsolidarity and outgroup hostility produced by the resulting models are \ndepicted in Fig. 2. Fine-tuning with full partisan data greatly increases \nTable 2 | Example ingroup and outgroup sentences \ngenerated by GPT-2-124M before and after fine-tuning with \nthe US Republican and Democratic Twitter (now X) corpora\nBase GPT-2-124M Republican \nGPT-2-124M\nDemocratic \nGPT-2-124M\nThey are more concerned \nwith securing the fate \nof their parents than \nprotecting their own \npersonal financial interests.\nThey are really doing \neverything possible to \nblock any attempts at \nreconciliation.\nThey are the same \npeople who have \nbeen on a list of \nanti-LGBTQ hate \ngroups for decades.\nThey are, however, capable \nof acting as an agent of \nchange.\nThey are the evil \nDemocrats who have \nfailed America.\nThey are just as \ndespicable as Trump \nsupporters.\nWe are taking the lead to \nfight against the spread of \nmisinformation.\nWe are so fortunate \nthat the US military \ndoesn’t look like this \nanymore.\nWe are at the \nepicenter of change \nin the lives of black & \nbrown people across \nthe country.\nWe are seeing many, many \nthings go wrong on an \neconomic level.\nWe are a leader in the \nfight against sexual \nabuse of children…\nWe are all working \nso hard to save the \nworld from climate \nchange.\nNature Computational Science | Volume 5 | January 2025 | 65–75\n 69\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\nboth social identity biases, especially for the Republican data. Keeping \n50% of either ingroup positive or outgroup negative sentences leads to \nslightly lower but similar levels of social identity biases. Keeping 0% of \neither ingroup positive or outgroup negative sentences further reduces \nthe bias. Notably, when we fine-tune with 0% of both ingroup positive \nand outgroup negative sentences, we can mitigate the biases to levels \nsimilar or even lower than the original pretrained GPT-2 model, with \ningroup solidarity dropping to almost parity level (no bias).\nStudy 3—social identity biases in real-world human–AI \nconversations\nT o understand how biases demonstrated in controlled experimental \nset-ups translate into real-world human–LLM interactions, we turned \nto WildChat35 and LMSYS-Chat-1M36, two open-source datasets captur-\ning natural dialogs between users and language models. Following the \nmethodology from Studies 1 and 2, we retrieved all sentences by users \nand models starting with ‘We are’ or ‘They are’ and classified them as \npositive, negative or neutral (using RoBERTa). Using mixed-effects \nlogistic regressions with the dataset variable as a random effect, we \nfound that WildChat and LMSYS datasets have statistically significant \nlevels of both model and user ingroup solidarity and outgroup hostility \nbiases. Ingroup sentences written by LLMs were 80% more likely to be \npositive, while outgroup sentences were 57% more likely to be negative \n(Supplementary Table 24). Moreover, the users of WildChat and LMSYS \nexhibited comparable social identity biases with the models, with \ningroup sentences being 86% more likely to be positive and outgroup \nsentences 158% more likely to be negative (Supplementary Table 25).\nDiscussion\nIn this study we investigated social identity biases in 77 LLMs. Our \nstudy provides a theory-grounded addition to the existing literature \non social biases in language technologies. This body of work originated \nwith studies into the social biases present in word embedding models \ntrained on large language corpora20,64,65. With the advent of modern \ndeep-learning models, such investigations have extended to more \ncomplex architectures66,67. Although insightful, these studies have faced \ncriticism for their lack of measurement validity, as well as insufficient \nconceptual grounding30,34. Furthermore, such studies typically treat bias \nagainst specific groups (for example, sexism or racism) in isolation3–5, and \nforego the study of intergroup biases as posited by social psychology.\nAs predicted by social identity and intergroup emotions theory9,11, \nwe found that most out-of-the-box language models exhibit both \ningroup solidarity and outgroup hostility to a similar degree, mirroring \nhuman-level averages found in the pretraining corpora. Our results also \nshow that consumer-facing LLMs (such as ChatGPT), which have been \nfine-tuned through human feedback, tend to exhibit lower degrees of \ningroup solidarity and outgroup hostility than non-fine-tuned base \nLLMs. This suggests that fine-tuning with human feedback could help \nreduce social identity biases in LLMs that emerge from already biased \ntraining data. Moreover, we found social identity biases in real-world \nconversations between humans and language models, with users \nexhibiting higher outgroup hostility than the models. In contrast to \nprevious studies conducted in controlled laboratory settings15,68,69, our \nresults offer insights from a less experimentally controlled but more \necologically valid environment. Our findings also align with previous \nresearch on biases in word embeddings trained on internet text20,64,65. \nHowever, we also observe that alignment techniques such as instruc-\ntion fine-tuning and preference-tuning are effective at reducing social \nidentity bias, corroborating previous research59,70. Despite this, we find \nthat even human-preference-tuned models still exhibit persistent and \nsignificant levels of ingroup bias, which may be linked to the sycophan-\ntic behavior of LLMs observed in earlier research71,72.\nAdditionally, we find that both ingroup solidarity and outgroup \nhostility are amplified after the models are fine-tuned with partisan \nsocial media data, and that this effect is larger for outgroup hostility than \nfor ingroup solidarity. Language models, on average, become roughly \nfive times more hostile toward a general (non-specific) outgroup after \nfine-tuning with US partisan social media data, in line with previous \nwork on outgroup hostility on US social media61. Our results also sup-\nport previous findings that language models can acquire political bias \nthrough fine-tuning73. Moreover, we find that we can lower LLMs’ ingroup \nsolidarity and outgroup hostility levels by removing ingroup-positive or \noutgroup-negative sentences from the training data. The effectiveness \nof targeted data curation in reducing the levels of both ingroup solidarity \nand outgroup hostility suggests promising directions for model develop-\nment and training. However, this finding also raises important ethical \nquestions about the balance between bias reduction and maintaining \nauthentic representation of diverse viewpoints in training data. If we \nwere to interpret the language models as proxies for social media users \nand news consumers, as some studies indicate is reasonable24,60,74, this \nsuggests that reducing the exposure to either ingroup solidarity- or \noutgroup hostility-related posts on social media platforms could reduce \naffective polarization on social media. This finding opens a new avenue \nfor depolarization research, which ordinarily focuses on removing \npotentially harmful or hostile content75, while neglecting the role that \nboosting the visibility of positive ingroup content may have to play.\nIn real-world conversation datasets, we observe that LLMs exhibit \nsimilar levels of ingroup and outgroup bias compared to the overall \namount of bias found across all models, including those before and \nafter instruction-tuning and preference-tuning. This finding buttresses \nthe construct validity of our study, and suggests that the biases present \nin LLMs are representative of the biases found in the broader model \nlandscape. Interestingly, user queries in WildChat and LMSYS display \nhigher levels of ingroup and outgroup bias compared to the pretraining \ncorpora available online. This discrepancy could be attributed to the \npotentially non-representative nature of these datasets or the inherent \ndifferences between conversational data and aggregate online text. \nThese findings highlight a critical challenge in AI alignment—ensur -\ning that bias reduction remains robust across different interaction \ncontexts, particularly in the presence of biased user input.\nGPT−2BLOOMBLOOMZ\nBLOOMZ−3B\nBLOOMZ−1B7\nBLOOMZ−1B1\nBLOOMZ−560M\nBLOOM−3B\nBLOOM−1B7\nBLOOM−1B1\nBLOOM−560M\nGPT−2−XL−1.5B\nGPT−2−Large−774M\nGPT−2−medium−355M\nGPT−2−124M\n0 1 2 3 4 5 6 7 8 9\nOd ds ratio\nBase outgroup hostility\nBase ingroup solidar ity\nP ar tisan outgroup hostility\nP ar tisan ingroup solidar ity\nBase versus partisan models\nFig. 2 | Study 2—ingroup solidarity and outgroup hostility in fine-tuned \nlanguage models on partisan social media data. The plot depicts the results of \nindividual logistic regressions predicting positive (or negative) sentiment based \non whether a sentence is ingroup (or outgroup), controlling for the number \nof words and type-to-token ratio and the party (Republican or Democrat) for \npartisan models (N = 24,000 sentences). Data are presented as odds ratios with \nerror bars for 95% confidence intervals.\nNature Computational Science | Volume 5 | January 2025 | 65–75 70\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\nOur study is not without limitations. Although our operation-\nalization of social identity biases is a theoretically grounded and \nsimple-to-implement probe of the overall ingroup solidarity and \noutgroup hostility of an LLM, it provides only a simplified view of \ncomplex social-psychological phenomena and is not meant as a \nsentence-level classifier of ingroup solidarity or outgroup hostility. \nThe English-centric nature of our study also limits its generalizability to \nother languages and cultural contexts, where social identity dynamics \nmay manifest differently. Future research could address the limitations \ninherent in our approach by, for example, including more specific \nprompts eliciting identity language (although we did include several \nprompt variations; see Methods), measuring user reactions to various \ntypes of ingroup-positive and outgroup-negative outputs generated \nby LLMs, and extending the analysis to multiple languages and cul -\ntural contexts. Moreover, our measure of bias is single-turn, whereas \nreal-world user conversations are often dynamic and multi-turn. Our \nfindings that LLMs exhibit social identity biases in real-world conversa-\ntions—which might be influenced by the high levels of bias present in \nuser queries—also raise the possibility that the model alignment may \nbe weaker in multi-turn settings compared to single-turn interactions, \nas previously demonstrated in ref. 76. These findings underscore the \nimportance of further research into the dynamics of bias in conversa-\ntional AI and the development of effective strategies to measure and \nmitigate these biases in a user-centric, multi-turn setting.\nMethods\nModel and data selection\nIn our study we use the term ‘base LLMs’ to describe language mod -\nels that are trained solely using self-supervised objectives such as \nnext-token prediction, meaning predicting the next token conditioned \non a number of context tokens. Through this mechanism, base models \ngain a certain level of competence in natural language understanding \nand generation. However, interacting with these models is challeng-\ning and often requires a substantial amount of prompt engineering to \nelicit desired behaviors.\nIn contrast, virtually all commercial chatbot models are subse -\nquently fine-tuned, typically through both instruction-tuning and \npreference-tuning. Instruction-tuning involves fine-tuning an LLM \nwith labeled datasets containing pairs of instruction prompts and \noutputs. This step enhances the model’s performance on specific \ntasks and its general ability to follow instructions, thereby improv -\ning its overall practical usability. Preference-tuning (also known as \nreinforcement learning from feedback) optimizes the model’s outputs \nbased on human evaluations, which further aligns the model with \nuser expectations and preferences. This dual fine-tuning approach \ntransforms base LLMs into more practically useful systems capable \nof handling diverse tasks effectively.\nOur analysis spans 77 LLMs across both base and fine-tuned models. \nThe base models include GPT-2 (124M, 355M, 774M, 1.5B)77, GPT-3 (davinci, \n175B)37, Cerebras-GPT (111M, 256M, 590M, 1.3B, 2.7B, 6.7B, 13B)78, BLOOM \n(560M, 1.1B, 1.7B, 3B)79, LLaMA (7B, 13B, 33B, 65B)80, Llama 2 (7B, 13B, \n70B)38, OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B)81, Pythia (70M, \n160M, 410M, 1.4B, 2.8B, 6.9B, 12B)39, Gemma (7B)40, Mistral (7B)82, Mixtral \n(8 × 7B)41 and OLMo (7B)83. The instruction-tuned or preference-tuned \nmodels comprise GPT-442, GPT-3.5 (text-davinci-003)43, BLOOMZ (560M, \n1.1B, 1.7B, 3B)84, OPT-IML (1.3B, 30B)85, Flan-T5 (3B, 11B),59, Flan-UL2 \n(20B)86, Dolly2.0 (3B, 7B, 12B) 44, Jurassic-2 Jumbo Instruct87, Alpaca \n(7B)45, Gemma-IT (7B)40, Mixtral-Instruct (8 × 7B)41, OLMo-Instruct (7B), \nOLMo-SFT (7B)83, Tulu 2 (7B, 13B, 70B)88, Tulu 2 DPO (7B, 13B, 70B)88, \nZephyr-beta (7B)89, Starling (7B) 90, OpenChat3.5 (7B) 46 and PaLM 2 \n(text-bison@001 and chat-bison@001)91. In these model sizes, M stands \nfor million parameters and B stands for billion parameters. For example, \nGPT-2 124M has 124 million parameters, while GPT-3 175B has 175 billion \nparameters. These numbers reflect the total count of learnable weights \nin the neural network.\nText generation and processing\nWe implemented text generation using the Huggingface Transform-\ners library92 with nucleus sampling 93 with a set P  value of 0.95 and a \ntemperature value of 1.0. If the model developers had clearly indicated \nhyperparameter recommendations, those were applied instead. In \nall of our text-generation experiments, we loaded the LLMs in 8-bit \nprecision94. Our experiments were conducted utilizing an NVIDIA \nA100-SXM-80GB graphics processing unit. For several models we \nassessed, including Jurassic-2 Jumbo Instruct, GPT-3, the GPT-3.5 series, \nGPT-4 and PaLM 2, we do not have direct access to the models, but \nrather only to their outputs via API calls.\nWe employed two distinct prompting strategies to elicit sen -\ntence completions from language models: the default prompt and the \ninstruction prompt. The default prompt, used in Fig. 1a, consisted of \nthe simple phrases ‘We are’ or ‘They are’ , followed by next-token predic-\ntion with a maximum generation length of 50 tokens. The instruction \nprompt, used for instruction-tuned and preference-tuned models in \nFig. 1c, followed the template: ‘Context: [context]. Now generate a \n0% both\n0% outgroup negativ e\n0% ingroup p ositiv e\n50% both\n50% outgroup negativ e\n50% ingroup p ositiv e\nRepub lican GPT−2−124M\nGPT−2−124M\n0 1 2 3 4 5 6 7 8 9 10 11\nOd ds ratio\nRepub lican GPT−2\n0% both\n0% outgroup negativ e\n0% ingroup p ositiv e\n50% both\n50% outgroup negativ e\n50% ingroup p ositiv e\nDemocr at GPT−2−124M\nGPT−2−124M\n0 1 2 3 4 5 6 7 8 9 10 11\nOd ds ratio\nOutgroup hostility\nIngroup solidar ity\nDemocrat GPT−2\nFig. 3 | Study 2—ingroup solidarity and outgroup hostility of Republican \nand Democrat fine-tuned models after removing different proportions of \npositive and negative ingroup and outgroup sentences from training data. \nThe plots show the results of individual logistic regressions predicting positive \n(or negative) sentiment based on whether a sentence is ingroup (or outgroup), \ncontrolling for the number of words and type-to-token ratio (N = 32,000 \nsentences). Data are presented as odds ratios with error bars for 95% confidence \nintervals.\nNature Computational Science | Volume 5 | January 2025 | 65–75\n 71\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\nsentence starting with \"We are (They are)\", ’ where [context] was ran-\ndomly sampled from the C4 corpus. This contextual augmentation \ngreatly enhanced response diversity and is crucial for instruction-tuned \nmodels that otherwise exhibited limited variation in their outputs. \nOn aggregate, the context sentence does not introduce bias, as the \nrandomness ensures an even distribution of contexts.\nT o ensure data quality, we implemented a rigorous filtering pro-\ntocol—sentences were excluded if they contained fewer than ten char-\nacters or five words, and we eliminated responses with 5-gram overlap \nto maintain uniqueness. This process continued until we accumulated \na minimum of 1,000 distinct sentences per model per sentence group. \nIn general, between 40 and 70% of raw sentences were filtered out \n(Supplementary Section 2).\nFor sentiment analysis, we utilized a RoBERTa-based classifi -\ncation model 50, specifically the ‘cardiffnlp/twitter-roberta-base-  \nsentiment-latest’ checkpoint from HuggingFace, one of the most widely \nadopted deep-learning-based models for sentiment classification. This \nmodel is a fine-tuned RoBERTa49 model, initially on Twitter data and \nsubsequently specifically for sentiment classification. The classifier cat-\negorized each sentence into one of three sentiment categories: positive, \nneutral or negative. Given that our generated texts are single sentences, \nsimilar in length to social media posts, this model is well-suited for our \nanalysis. We also conducted robustness checks with other sentiment \nclassification tools95–98, which show broad agreement with the RoBERTa \nresults (Supplementary Sections 5 and 6).\nStudy 1—measuring social identity biases in LLMs\nWe first generated ingroup and outgroup sentences using \nmodel-appropriate prompting strategies. For base models, we \nemployed the default prompt as it represents the most direct approach \nto eliciting model outputs. For instruction-tuned and preference-tuned \nmodels, we utilized the instruction prompt. Additionally, we  \ncollected responses from the instruction-tuned models that were \ncapable of responding to the default prompt, analyzing these outputs \nseparately. All generated sentences underwent the filtering process \ndescribed earlier.\nFollowing sentence generation and quality filtering, we conducted \nsentiment analysis. We then fit two logistic regressions for each LLM \nusing the 2,000 generated sentences (1,000 per group) to estimate \ningroup solidarity and outgroup hostility. For ingroup solidarity, \nwe fit a logistic regression predicting positive (versus negative or \nneutral) sentiment based on a binary indicator variable of whether a \nsentence was ingroup- or outgroup-related and control variables of \ntype-to-token ratio and total tokens per sentence, with the outgroup \nas the reference category. The regression equation for ingroup soli -\ndarity is\nPositivesentiment = α+β1Ingroup+β2TTR\n+β3Totaltokensscaled +ϵ\n(1)\nSimilarly, to measure outgroup hostility, we ran another logistic \nregression predicting negative (versus positive or neutral) sentiment \nbased on the binary group indicator and the same control variables, \nwith the ingroup as the reference category. The regression equation for \noutgroup hostility is\nNegativesentiment = α+β1Outgroup+β2TTR\n+β3Totaltokensscaled +ϵ\n(2)\nThis procedure allowed us to obtain one measurement (the odds \nratio of the binary group indicator) that would reflect ingroup solidar-\nity and another one for outgroup hostility following a simple logic that \nif the ingroup (or outgroup) sentences are more likely to be positive \n(or negative), we can interpret it as evidence of the model displaying \ningroup solidarity (or outgroup hostility).\nT o establish human social identity bias values, we analyzed sev-\neral major LLM pretraining corpora, including C4 48, OpenWebT ext, \nan open-source replica of GPT-2 training corpus56, OLM (November/\nDecember 2022 Common Crawl data) 57 and The Pile55. These diverse \ncorpora, which have been widely used in training state-of-the-art \nLLMs, predominantly feature text from a broad spectrum of internet \nwebpages, including sources such as Wikipedia, news sites and Red-\ndit pages. Some of these corpora also include data from specialized \ndomains, such as arXiv, PubMed and StackExchange. We selected \nthese corpora as they are well-known, are widely used in the LLMs \nspace and span slightly different time periods to account for any \npotential temporal variations in the prevalence of social identity \nbiases across the internet. For our analysis in Study 1, we identified \nsentences starting with ‘We are’ and ‘They are’ and then applied the \nsame filtering and analysis process that we used for sentences gener-\nated by LLMs.\nWe present our measurements of ingroup solidarity and outgroup \nhostility across four conditions in Fig. 1 . These include (1) responses \nfrom models using the default prompt (Fig. 1a), (2) responses from \noutlier models using the default prompt (Fig. 1b), (3) responses from \ninstruction-tuned and preference-tuned models using the instruction \nprompt (Fig. 1c) and (4) measurements from human-written text in \npretraining corpora that serve as our baseline (Fig. 1d).\nWe used the same regression procedure for the pretraining data \nfrom each corpus and overall by randomly downsampling to 2,000 \nsentences per corpus per sentence group. We also estimated overall \ningroup-solidarity and outgroup-hostility values using mixed-effects \nlogistic regressions with the same fixed effects and model names as the \nrandom intercept. We considered controlling for sentence topic in the \nregression; however, given that the results are quite similar without this \ncontrol, we decided to omit it to maintain the simplicity and clarity of \nthe analysis (Supplementary Section 4).\nAdditionally, we explored several design choices to ensure the \nrobustness of our results. First, to establish the generalizability and \nrobustness of the sentiment classification methodology used, we \ncompared the results produced by the RoBERTa classifier used in the \nmain analyses with ten other available sentiment classifiers, both \ndictionary-based and machine-learning-based, as presented in Sup -\nplementary Section 5. We then investigated the impact of prompting \nwith specific identity mentions on the model’s responses (Supple -\nmentary Section 7). Additionally, we examined the effect of using a \nconversation-like prompt for base LLMs to assess its influence on the \ngenerated outputs (Supplementary Section 8).\nStudy 2—training data effects on social identity biases\nWe fine-tuned selected models (GPT-2, BLOOM, BLOOMZ) on US par-\ntisan Twitter (now X) data with the same hyperparameter as used in \nref. 60 for one epoch. In this context, fine-tuning refers to the practise \nof taking a pretrained model, typically trained on large-scale, general \ncorpora, and conducting additional self-supervised pretraining on a \nmore specialized corpus, without involving human-annotated data. \nThe goal of this fine-tuning was not necessarily to improve the LLMs \nbut to adapt them to the specific domain of US partisan Twitter data. \nThis process can be interpreted as exposing the model to a ‘news diet’ \nof partisan tweets, aligning with the interpretation by ref. 74.\nAs all models investigated in Study 2 are base LLMs, we generated \n‘We’ and ‘They’ sentences using the default prompt, classified sentence \nsentiment using RoBERTa, and performed a similar analysis as in Study \n1. In addition, we applied VADER95 in Study 2 to examine fine-grained \nsentiment scores (compound score) of model-generated sentences \nbefore and after fine-tuning for illustration purposes (Supplementary \nFigs. 1 and 2).\nT o remove different proportions of affectively valenced ingroup \nand outgroup sentences, we first split the text into sentences from \nthe same US partisan Twitter (now X) data, and identified the ‘We’ or \nNature Computational Science | Volume 5 | January 2025 | 65–75 72\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\n‘They’ sentences as sentences that contain one of the ‘We’ or ‘They’ \nwords as defined in LIWC 2022 99. We then ran VADER on these sen -\ntences and used established cutoff points of 0.05 and −0.05 on the \ncompound score for positive and negative classification, respectively. \nFinally, we removed a varying proportion of the data and performed \nfine-tuning experiments. T o establish the generalizability and robust-\nness of the effects observed, we experimented with different group \nidentity prompting strategies other than ‘We are’ and ‘They are’ , such \nas ‘We/They are’ , ‘Ours/Theirs is’ , ‘We/They typically’ , ‘Our/Their way \nis’ , ‘We/They often’ and ‘We/They believe’ (Supplementary Section 6), \nshowing similar results.\nStudy 3—social identity biases in real-world human–AI \nconversations\nWe retrieved all ingroup and outgroup sentences from user and model \nutterances from two large-scale repositories of human–LLM conver-\nsations: WildChat35, specific to ChatGPT (GPT-3.5-Turbo and GPT-4),  \nand LMSYS36, which has 25 different models. We then used the same \nRoBERTa classifier and regression methodology as in Study 1 to \nestimate ingroup solidarity and outgroup hostility of the user- and \nmodel-generated sentences. We analyzed a total of 25,395 sentences: \n10,507 from WildChat models, 2,453 from WildChat users, 10,247 \nfrom LMSYS models and 2,188 from LMSYS users. When fitting the \nmixed-effects regression for users predicting the negative RoBERTa \nsentiment classification, we found that the module is singular (mean-\ning that the estimated variance of the random effect is very close to 0);  \nhowever, we do not consider this problematic because it is a com -\nmon occurrence in mixed models that signifies that the variation \nacross the two corpora is adequately captured by the fixed effects \nalone. Please see Supplementary Section 10 for a robustness check \nwith non-mixed-effects models for each corpus, which align with the \nmixed-effects regression results.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this Article.\nData availability\nAll data needed to reproduce the analyses in this paper is available on \nOSF ref. 100. The statistical values depicted in Fig. 1  are available in \nSupplementary Tables 3–6. The values depicted in Fig. 2 are available \nin Supplementary Table 27. The values depicted in Fig. 3 are available \nin Supplementary Table 28.\nCode availability\nAll code needed to reproduce the analyses in this paper is available \non OSF ref. 100.\nReferences\n1. Milmo, D. ChatGPT reaches 100 million users two months \nafter launch The Guardian (2 February 2023); https://www.\ntheguardian.com/technology/2023/feb/02/chatgpt-100-million-u\nsers-open-ai-fastest-growing-app\n2. Microsoft. Global online safety survey results (Microsoft, 2024); \nhttps://www.microsoft.com/en-us/DigitalSafety/research/\nglobal-online-safety-survey\n3. Bordia, S. & Bowman, S. R. Identifying and reducing gender bias \nin word-level language models. In Proc. 2019 Conference of the \nNorth American Chapter of the Association for Computational \nLinguistics: Student Research Workshop 7–15 (ACL, 2019);  \nhttps://doi.org/10.18653/v1/N19-3002\n4. Abid, A., Farooqi, M. & Zou, J. Persistent anti-muslim bias in large \nlanguage models. In Proc. 2021 AAAI/ACM Conference on AI, \nEthics and Society 298–306 (ACM, 2021); https://doi.org/ \n10.1145/3461702.3462624\n5. Ahn, J. & Oh, A. Mitigating language-dependent ethnic bias in \nBERT. In Proc. 2021 Conference on Empirical Methods in Natural \nLanguage Processing 533–549 (ACL, 2021); https://aclanthology.\norg/2021.emnlp-main.42\n6. Hofmann, V., Kalluri, P. R., Jurafsky, D. & King, S. AI generates \ncovertly racist decisions about people based on their dialect. \nNature 633, 147–154 (2024).\n7. Iyengar, S., Sood, G. & Lelkes, Y. Affect, not ideology: a social \nidentity perspective on polarization. Public Opinion Q. 76, \n405–431 (2012).\n8. Iyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N. &  \nWestwood, S. J. The origins and consequences of affective \npolarization in the United States. Annu. Rev. Political Sci. 22, \n129–146 (2019).\n9. Tajfel, H., & Turner, J. C. An integrative theory of intergroup \nconflict. In The Social Psychology of Intergroup Relations  \n(eds Austin, W. G. & Worchel, S.) 33–37 (Brooks/Cole, 1979).\n10. Turner, J. C., Hogg, M. A., Oakes, P. J., Reicher, S. D. &  \nWetherell, M. S. Rediscovering the Social Group: A \nSelf-Categorization Theory. (Basil Blackwell, 1987).\n11. Mackie, D. M. & Smith, E. R. Intergroup emotions theory: \nproduction, regulation and modification of group-based \nemotions. Adv. Exp. Soc. Psychol 58, 1–69 (2018).\n12. Hogg, M. A. & Abrams, D. Social Identifications: A Social \nPsychology of Intergroup Relations and Group Processes  \n(Taylor & Francis, 1988).\n13. Tajfel, H., Billig, M. G., Bundy, R. P. & Flament, C. Social \ncategorization and intergroup behaviour. Eur. J. Soc. Psychol. 1, \n149–178 (1971).\n14. Pinter, B. & Greenwald, A. G. A comparison of minimal group \ninduction procedures. Group Process. Intergr. Relat. 14, 81–98 (2011).\n15. Maass, A., Salvi, D., Arcuri, L. & Semin, G. Language use in \nintergroup contexts: the linguistic intergroup bias. J. Pers. Soc. \nPsychol. 57, 981–993 (1989).\n16. Viki, G. T. et al. Beyond secondary emotions: the \ninfrahumanization of outgroups using human-related and \nanimal-related words. Soc. Cogn. 24, 753–775 (2006).\n17. Cave, S. & Dihal, K. The whiteness of AI. Philos. Technol. 33, \n685–703 (2020).\n18. Noble, S. U. Algorithms of Oppression: How Search Engines \nReinforce Racism (New York Univ. Press, 2018).\n19. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On \nthe dangers of stochastic parrots: can language models be too \nbig? In Proc. 2021 ACM Conference on Fairness, Accountability \nand Transparency 610–623 (ACM, 2021); https://dl.acm.org/\ndoi/10.1145/3442188.3445922\n20. Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived \nautomatically from language corpora contain human-like biases. \nScience 356, 183–186 (2017).\n21. Webb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in \nlarge language models. Nat. Hum. Behav. 7, 1526–1541 (2023).\n22. Kosinski, M. Evaluating large language models in theory of mind \ntasks. Proc. Natl Acad. Sci. USA 121, e2405460121 (2024).\n23. Caron, G. & Srivastava, S. Manipulating the perceived personality \ntraits of language models. In Findings of the Association for \nComputational Linguistics: EMNLP 2023 (eds Bouamor, H. \net al.) 2370–2386 (ACL, 2023); https://aclanthology.org/2023.\nfindings-emnlp.156\n24. Argyle, L. P. et al. Out of one, many: using language models to \nsimulate human samples. Political Anal 31, 337–351 (2023).\n25. Park, J. S. et al. Generative agents: interactive simulacra of human \nbehavior. In Proc. 36th Annual ACM Symposium on User Interface \nSoftware and Technology (UIST ’23) Vol. 2, 1–22 (ACM, 2023).\n26. Matz, S. et al. The potential of generative AI for personalized \npersuasion at scale. Sci. Rep. 14, 4692 (2024).\nNature Computational Science | Volume 5 | January 2025 | 65–75\n 73\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\n27. Jakesch, M., Bhat, A., Buschek, D., Zalmanson, L. & Naaman, M.  \nCo-writing with opinionated language models affects users’ \nviews. In Proc. 2023 CHI Conference on Human Factors in \nComputing Systems 1–15 (ACM, 2023); https://dl.acm.org/\ndoi/10.1145/3544548.3581196\n28. Bowman, S. R. & Dahl, G. E. What will it take to fix benchmarking \nin natural language understanding? In Proc. 2021 Conference of \nthe North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies (eds Toutanova, K. \net al.) 4843–4855 (ACL, 2021); https://aclanthology.org/2021.\nnaacl-main.385\n29. Anwar, U. et al. Foundational challenges in assuring alignment \nand safety of large language models. Trans. Mach. Learn. Res. \n(2024); https://openreview.net/forum?id=oVTkOs8Pka\n30. Blodgett, S. L., Barocas, S., Daumé III, H. & Wallach, H. Language \n(technology) is power: a critical survey of ‘bias’ in NLP. In Proc. \n58th Annual Meeting of the Association for Computational \nLinguistics (eds Jurafsky, D. et al.) 5454–5476 (ACL, 2020);  \nhttps://aclanthology.org/2020.acl-main.485\n31. Parrish, A. et al. BBQ: a hand-built bias benchmark for question \nanswering. In Findings of the Association for Computational \nLinguistics: ACL 2022 (eds Muresan, S. et al.) 2086–2105 (ACL, \n2022); https://aclanthology.org/2022.findings-acl.165\n32. Ganguli, D., Schiefer, N., Favaro, M. & Clark, J. Challenges in \nevaluating AI systems https://www.anthropic.com/index/\nevaluating-ai-systems (Anthropic, 2023).\n33. Santurkar, S. et al. Whose opinions do language models reflect? \nIn Proc. 40th International Conference on Machine Learning 1244, \n29971–30004 (ACM, 2023).\n34. Blodgett, S. L., Lopez, G., Olteanu, A., Sim, R. & Wallach, H. \nStereotyping Norwegian salmon: an inventory of pitfalls in \nfairness benchmark datasets. In Proc. 59th Annual Meeting of the \nAssociation for Computational Linguistics and the 11th International \nJoint Conference on Natural Language Processing (Volume 1:  \nLong Papers) (eds Zong, C. et al.) 1004–1015 (ACL, 2021);  \nhttps://aclanthology.org/2021.acl-long.81\n35. Zhao, W. et al. WildChat: 1M ChatGPT interaction logs in the \nwild. In Proc. Twelfth International Conference on Learning \nRepresentations (ICLR, 2024); https://openreview.net/\nforum?id=Bl8u7ZRlbM\n36. Zheng, L. et al. LMSYS-Chat-1M: a large-scale real-world LLM \nconversation dataset. In Proc. Twelfth International Conference on \nLearning Representations (ICLR, 2024); https://openreview.net/\nforum?id=BOfDKxfwt0\n37. Brown, T. B. et al. Language models are few-shot learners. In \nAdvances in Neural Information Processing Systems 33: Annual \nConference on Neural Information Processing Systems 2020 \n(eds Larochelle, H. et al.) (Curran Associates, 2020); https://\nproceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb496741\n8bfb8ac142f64a-Abstract.html\n38. Touvron, H. et al. Llama 2: open foundation and fine-tuned  \nchat models. Preprint at https://arxiv.org/abs/2307.09288  \n(2023).\n39. Biderman, S. et al. Pythia: a suite for analyzing large language \nmodels across training and scaling. In Proc. 40th International \nConference on Machine Learning Vol. 102 (eds Krause, A. et al.) \n2397–2430 (PMLR, 2023); https://proceedings.mlr.press/v202/\nbiderman23a.html\n40. The Gemma Team et al. Gemma: open models based on \ngemini research and technology. Preprint at https://arxiv.org/\nabs/2403.08295 (2024).\n41. Jiang, A. Q. et al. Mixtral of experts. Preprint at https://arxiv.org/\nabs/2401.04088 (2024).\n42. OpenAI et al. GPT-4 technical report. Preprint at https://arxiv.org/\nabs/2303.08774 (2024).\n43. OpenAI. Introducing ChatGPT https://openai.com/index/chatgpt/ \n(2022).\n44. Conover, M. et al. Free Dolly: introducing the world’s first truly open \ninstruction-tuned LLM https://www.databricks.com/blog/ \n2023/04/12/dolly-first-open-commercially-viable-instruction- \ntuned-llm (2023).\n45. Taori, R. et al. Stanford alpaca: an instruction-following LLaMA \nmodel. GitHub https://github.com/tatsu-lab/stanford_alpaca \n(2023).\n46. Wang, G. et al. OpenChat: advancing open-source language \nmodels with mixed-quality data. In Proc. Twelfth International \nConference on Learning Representations (ICLR, 2024);  \nhttps://openreview.net/forum?id=AOJyfhWYHf\n47. Perdue, C. W. et al. Us and them: social categorization and the \nprocess of intergroup bias. J. Pers. Soc. Psychol. 59, 475–486 \n(1990).\n48. Raffel, C. et al. Exploring the limits of transfer learning with a \nunified text-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 \n(2020).\n49. Liu, Y. et al. RoBERTa: a robustly optimized BERT pretraining \napproach. Preprint at https://arxiv.org/abs/1907.11692 (2019).\n50. Loureiro, D., Barbieri, F., Neves, L., Espinosa Anke, L. & \nCamacho-Collados, J. TimeLMs: diachronic language models \nfrom Twitter. In Proc. 60th Annual Meeting of the Association for \nComputational Linguistics: System Demonstrations 251–260  \n(ACL, 2022); https://aclanthology.org/2022.acl-demo.25\n51. Van Atteveldt, W., Van der Velden, M. A. & Boukes, M. The \nvalidity of sentiment analysis: comparing manual annotation, \ncrowd-coding, dictionary approaches and machine learning \nalgorithms. Commun. Methods Measures 15, 121–140  \n(2021).\n52. Frankel, R., Jennings, J. & Lee, J. Disclosure sentiment: machine \nlearning vs. dictionary methods. Manage. Sci. 68, 5514–5532 \n(2022).\n53. Rathje, S. et al. GPT is an effective tool for multilingual \npsychological text analysis. Proc. Natl Acad. Sci. USA 121, \ne2308950121 (2024).\n54. Templin, M. C.Certain Language Skills in Children; their \nDevelopment and Interrelationships (Univ. Minnesota Press, 1957).\n55. Gao, L. et al. The Pile: an 800 GB dataset of diverse text for \nlanguage modeling. Preprint at https://arxiv.org/abs/2101.00027 \n(2020).\n56. Gokaslan, A. & Cohen, V. OpenWebText corpus. GitHub  \nhttp://Skylion007.github.io/OpenWebTextCorpus (2019).\n57. Thrush, T., Ngo, H., Lambert, N. & Kiela, D. Online language \nmodelling data pipeline. GitHub https://github.com/huggingface/\nolm-datasets (2022).\n58. Kaplan, J. et al. Scaling laws for neural language models. Preprint \nat https://arxiv.org/abs/2001.08361 (2020).\n59. Chung, H. W. et al. Scaling instruction-finetuned language \nmodels. J. Mach. Learn. Res. 25, 1–53 (2024).\n60. Jiang, H., Beeferman, D., Roy, B. & Roy, D. CommunityLM: probing \npartisan worldviews from language models. In Proc. 29th \nInternational Conference on Computational Linguistics 6818–6826 \n(ACL, 2022); https://aclanthology.org/2022.coling-1.593\n61. Rathje, S., Van Bavel, J. J. & van der Linden, S. Out-group \nanimosity drives engagement on social media. Proc. Natl Acad. \nSci. USA 118, e2024292118 (2021).\n62. Abramowitz, A. I. & Webster, S. The rise of negative partisanship \nand the nationalization of US elections in the 21st century.  \nElect. Stud. 41, 12–22 (2016).\n63. Kyrychenko, Y., Brik, T., van der Linden, S. & Roozenbeek, J. Social \nidentity correlates of social media engagement before and after \nthe 2022 Russian invasion of Ukraine. Nat. Commun. 15, 8127 \n(2024).\nNature Computational Science | Volume 5 | January 2025 | 65–75 74\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\n64. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V. & Kalai, A. T. \nMan is to computer programmer as woman is to homemaker? \nDebiasing word embeddings. In Advances in Neural Information \nProcessing Systems Vol. 29 (Curran Associates, 2016); https://\nproceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e\n4ac3d270571622f4f316ec5-Abstract.html\n65. Garg, N., Schiebinger, L., Jurafsky, D. & Zou, J. Word embeddings \nquantify 100 years of gender and ethnic stereotypes. Proc. Natl \nAcad. Sci. USA 115, E3635–E3644 (2018).\n66. Nadeem, M., Bethke, A. & Reddy, S. StereoSet: measuring \nstereotypical bias in pretrained language models. In Proc. 59th \nAnnual Meeting of the Association for Computational Linguistics \nand the 11th International Joint Conference on Natural Language \nProcessing (Volume 1: Long Papers) (eds Zong, C. et al.) 5356–5371 \n(ACL, 2021); https://aclanthology.org/2021.acl-long.416\n67. Liang, P. P., Wu, C., Morency, L.-P. & Salakhutdinov, R. Towards \nunderstanding and mitigating social biases in language models. \nIn Proc. 38th International Conference on Machine Learning 139 \n(eds Meila, M. & Zhang, T.) 6565–6576 (PMLR, 2021); https://\nproceedings.mlr.press/v139/liang21a.html\n68. Fiedler, K., Semin, G. R. & Finkenauer, C. The battle of words \nbetween gender groups: a language-based approach to \nintergroup processes. Hum. Commun. Res. 19, 409–441 (1993).\n69. Maass, A., Milesi, A., Zabbini, S. & Stahlberg, D. Linguistic \nintergroup bias: differential expectancies or in-group protection? \nJ. Pers. Soc. Psychol. 68, 116–126 (1995).\n70. Bai, Y. et al. Training a helpful and harmless assistant with \nreinforcement learning from human feedback. Preprint at  \nhttps://arxiv.org/abs/2204.05862 (2022).\n71. Sharma, M. et al. Towards understanding sycophancy in \nlanguage models. In Proc. Twelfth International Conference on \nLearning Representations (ICLR, 2024); https://openreview.net/\nforum?id=tvhaxkMKAn\n72. Laban, P., Murakhovs’ka, L., Xiong, C. & Wu, C.-S. Are you sure? \nChallenging LLMs leads to performance drops in the flipflop \nexperiment. Preprint at https://arxiv.org/abs/2311.08596 (2024).\n73. Feng, S., Park, C. Y., Liu, Y. & Tsvetkov, Y. From pretraining data \nto language models to downstream tasks: tracking the trails of \npolitical biases leading to unfair NLP models. In Proc. 61st Annual \nMeeting of the Association for Computational Linguistics (Volume \n1: Long Papers) 11737–11762 (ACL, 2023); https://aclanthology.\norg/2023.acl-long.656\n74. Chu, E., Andreas, J., Ansolabehere, S. & Roy, D. Language models \ntrained on media diets can predict public opinion. Preprint at \nhttps://arxiv.org/abs/2303.16779 (2023).\n75. Guess, A. M. et al. How do social media feed algorithms affect \nattitudes and behavior in an election campaign? Science 381, \n398–404 (2023).\n76. Anil, C. et al. Many-shot jailbreaking https://www.anthropic.com/\nresearch/many-shot-jailbreaking (Anthropic, 2024).\n77. Radford, A. et al. Language models are unsupervised multitask \nlearners https://openai.com/research/better-language-models \n(OpenAI, 2019).\n78. Dey, N. et al. Cerebras-GPT: open compute-optimal language \nmodels trained on the Cerebras Wafer-Scale Cluster. Preprint at \nhttps://arxiv.org/abs/2304.03208 (2023).\n79. BigScience Workshop et al. BLOOM: A 176B-parameter \nopen-access multilingual language model. Preprint at https://\narxiv.org/abs/2211.05100 (2023).\n80. Touvron, H. et al. LLaMA: open and efficient foundation language \nmodels. Preprint at https://arxiv.org/abs/2302.13971 (2023).\n81. Zhang, S. et al. OPT: open pre-trained transformer language \nmodels. Preprint at https://arxiv.org/abs/2205.01068 (2022).\n82. Jiang, A. Q. et al. Mistral 7b. Preprint at https://arxiv.org/\nabs/2310.06825 (2023).\n83. Groeneveld, D. et al. OLMo: Accelerating the science of language \nmodels. In Proc. 62nd Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers) (eds Ku, L.-W. \net al.) 15789–15809 (ACL, 2024); https://aclanthology.org/2024.\nacl-long.841\n84. Muennighoff, N. et al. Crosslingual generalization through \nmultitask finetuning. In Proc. 61st Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers) \n15991–16111 (Association for Computational Linguistics, 2023).\n85. Iyer, S. et al. OPT-IML: scaling language model instruction meta \nlearning through the lens of generalization. Preprint at  \nhttps://arxiv.org/abs/2212.12017 (2023).\n86. Tay, Y. et al. UL2: unifying language learning paradigms. In The \nEleventh International Conference on Learning Representations \n(ICLR, 2023); https://openreview.net/forum?id=6ruVLB727MC\n87. AI21studio. Announcing Jurassic-2 and task-specific APIs  \nhttps://www.ai21.com/blog/introducing-j2 (AI21, 2023).\n88. Ivison, H. et al. Camels in a changing climate: enhancing LM \nadaptation with TULU 2. Preprint at https://arxiv.org/abs/ \n2311.10702 (2023).\n89. Tunstall, L. et al. Zephyr: direct distillation of LM alignment. \nPreprint at https://arxiv.org/abs/2310.16944 (2023).\n90. Zhu, B. et al. Starling-7B: improving helpfulness and harmlessness \nwith RLAIF. In Proc. First Conference on Language Modeling \nhttps://openreview.net/forum?id=GqDntYTTbk (2024).\n91. Anil, R. et al. Palm 2 technical report. Preprint at https://arxiv.org/\nabs/2305.10403 (2023).\n92. Wolf, T. et al. Transformers: state-of-the-art natural language \nprocessing. In Proc. 2020 Conference on Empirical Methods in \nNatural Language Processing: System Demonstrations 38–45 \n(ACL, 2020); https://aclanthology.org/2020.emnlp-demos.6\n93. Holtzman, A., Buys, J., Du, L., Forbes, M. & Choi, Y. The curious \ncase of neural text degeneration. In Proc. International Conference \non Learning Representations (ICLR, 2020); https://openreview.net/\nforum?id=rygGQyrFvH\n94. Dettmers, T., Lewis, M., Belkada, Y. & Zettlemoyer, L. GPT3.int8(): \n8-bit matrix multiplication for transformers at scale. In Proc. \nAdvances in Neural Information Processing Systems 35 (eds \nKoyejo, S. et al.) 30318–30332 (Curran Associates, 2022); https://\nproceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c0\n5c49636d4c6206a97e9c8a-Paper-Conference.pdf\n95. Hutto, C. & Gilbert, E. VADER: a parsimonious rule-based model \nfor sentiment analysis of social media text. Proc. Int. AAAI Conf. \nWeb Social Media 8, 216–225 (2014).\n96. Årup Nielsen, F. A new ANEW: evaluation of a word list for \nsentiment analysis in microblogs. In Proc. ESWC2011 Workshop on \n‘Making Sense of Microposts’: Big Things Come in Small Packages, \nCEUR Workshop Proceedings Vol. 718 (eds Rowe, M. et al.) 93–98 \n(2011).\n97. Loria, S. textblob Documentation, release 0.18.0.post0 edn \nhttps://readthedocs.org/projects/textblob/downloads/pdf/dev/ \n(Readthedocs, 2024).\n98. Potts, C., Wu, Z., Geiger, A. & Kiela, D. DynaSent: A dynamic \nbenchmark for sentiment analysis. Proc. 59th Annu. Meet. Assoc. \nComput. Linguist. 11th Int. Jt. Conf. Nat. Lang. Process. Vol. 1: Long \nPap. 2388–2404 (2021).\n99. Boyd, R. L., Ashokkumar, A., Seraj, S. & Pennebaker, J. W. The \nDevelopment and Psychometric Properties of LIWC-22. (Univ. Texas \nat Austin, 2022).\n100. Hu, T. et al. Generative language models exhibit social identity \nbiases https://doi.org/10.17605/OSF.IO/9HT32 (OSF, 2024).\nAcknowledgements\nThis work was supported by the Gates Cambridge Trust (grant no. \nOPP1144 from the Bill & Melinda Gates Foundation) awarded to \nNature Computational Science | Volume 5 | January 2025 | 65–75\n 75\nAnalysis https://doi.org/10.1038/s43588-024-00741-1\nT.H. and Y.K. and the Alan Turing Institute’s Enrichment Scheme \nawarded to Y.K. We thank L. Dixon for very helpful discussions. This \nwork was performed using resources provided by the Cambridge \nService for Data Driven Discovery (CSD3) operated by the University \nof Cambridge Research Computing Service (www.csd3.cam.\nac.uk), provided by Dell EMC and Intel using Tier-2 funding from the \nEngineering and Physical Sciences Research Council (capital grant no. \nEP/T022159/1), and DiRAC funding from the Science and Technology \nFacilities Council (www.dirac.ac.uk).\nAuthor contributions\nT.H. and Y.K. conceptualized the study, collected and analyzed the \ndata, and led the write-up. S.R., N.C., S.v.d.L. and J.R. helped with the \nstudy’s conceptualization, provided feedback throughout the process, \nand assisted with the write-up.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains \nsupplementary material available at  \nhttps://doi.org/10.1038/s43588-024-00741-1.\nCorrespondence and requests for materials should be addressed to \nTiancheng Hu or Yara Kyrychenko.\nPeer review information Nature Computational Science thanks Michal \nKosinski, María Pérez-Ortiz and Germans Savcisens for their contribution \nto the peer review of this work. Primary Handling Editor: Fernando \nChirigati, in collaboration with the Nature Computational Science team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\n\n\n",
  "topic": "Ingroups and outgroups",
  "concepts": [
    {
      "name": "Ingroups and outgroups",
      "score": 0.7221347093582153
    },
    {
      "name": "Outgroup",
      "score": 0.6365818977355957
    },
    {
      "name": "Psychology",
      "score": 0.621979832649231
    },
    {
      "name": "Social psychology",
      "score": 0.617682695388794
    },
    {
      "name": "In-group favoritism",
      "score": 0.5658016204833984
    },
    {
      "name": "Preference",
      "score": 0.5464159846305847
    },
    {
      "name": "Social identity theory",
      "score": 0.541219174861908
    },
    {
      "name": "Derogation",
      "score": 0.5410385131835938
    },
    {
      "name": "Identity (music)",
      "score": 0.5175284743309021
    },
    {
      "name": "Hostility",
      "score": 0.5091521143913269
    },
    {
      "name": "Prejudice (legal term)",
      "score": 0.4883810877799988
    },
    {
      "name": "Solidarity",
      "score": 0.4325235188007355
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4312704801559448
    },
    {
      "name": "Social group",
      "score": 0.3538816571235657
    },
    {
      "name": "Political science",
      "score": 0.11501821875572205
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    }
  ]
}