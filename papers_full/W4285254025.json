{
    "title": "Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning",
    "url": "https://openalex.org/W4285254025",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5070446496",
            "name": "Demian Gholipour Ghalandari",
            "affiliations": [
                null,
                "University College Dublin"
            ]
        },
        {
            "id": "https://openalex.org/A5023447446",
            "name": "Chris Hokamp",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054845773",
            "name": "Georgiana Ifrim",
            "affiliations": [
                "University College Dublin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2145882814",
        "https://openalex.org/W2962704246",
        "https://openalex.org/W2251656952",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2962972512",
        "https://openalex.org/W3043430239",
        "https://openalex.org/W2038705803",
        "https://openalex.org/W2526471240",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3175541789",
        "https://openalex.org/W2890116189",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3034539042",
        "https://openalex.org/W2949854508",
        "https://openalex.org/W2890419630",
        "https://openalex.org/W2884900624",
        "https://openalex.org/W2526303618",
        "https://openalex.org/W1664028424",
        "https://openalex.org/W3037702185",
        "https://openalex.org/W2119850747",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W2970892365",
        "https://openalex.org/W2155027007",
        "https://openalex.org/W3163129978",
        "https://openalex.org/W2063490579",
        "https://openalex.org/W2971032890",
        "https://openalex.org/W2962849707",
        "https://openalex.org/W2741938760",
        "https://openalex.org/W2115322217",
        "https://openalex.org/W2239203031",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W2971409044",
        "https://openalex.org/W2983226400",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2886154179"
    ],
    "abstract": "Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality. Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for learning and inference. Recent unsupervised sentence compression approaches use custom objectives to guide discrete search; however, guided search is expensive at inference time. In this work, we explore the use of reinforcement learning to train effective sentence compression models that are also fast when generating predictions. In particular, we cast the task as binary sequence labelling and fine-tune a pre-trained transformer using a simple policy gradient approach. Our approach outperforms other unsupervised models while also being more efficient at inference time.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1267 - 1280\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nEfficient Unsupervised Sentence Compression by Fine-tuning Transformers\nwith Reinforcement Learning\nDemian Gholipour Ghalandari1,2, Chris Hokamp1,\nGeorgiana Ifrim2\n1Aylien Ltd., Dublin, Ireland\n2Insight Centre for Data Analytics, University College Dublin, Ireland\n1{first-name}@aylien.com\ngeorgiana.ifrim@ucd.ie\nAbstract\nSentence compression reduces the length of text by\nremoving non-essential content while preserving\nimportant facts and grammaticality. Unsupervised\nobjective driven methods for sentence compression\ncan be used to create customized models without\nthe need for ground-truth training data, while al-\nlowing flexibility in the objective function(s) that\nare used for learning and inference. Recent unsu-\npervised sentence compression approaches use cus-\ntom objectives to guide discrete search; however,\nguided search is expensive at inference time. In this\nwork, we explore the use of reinforcement learning\nto train effective sentence compression models that\nare also fast when generating predictions. In partic-\nular, we cast the task as binary sequence labelling\nand fine-tune a pre-trained transformer using a sim-\nple policy gradient approach. Our approach outper-\nforms other unsupervised models while also being\nmore efficient at inference time.\n1 Introduction\nIn general, the information content of text is cor-\nrelated with its length. However, for a given text,\na shorter version may still convey the essential in-\nformation while preserving grammaticality (Sid-\ndharthan, 2014). The definition of essential can\nchange depending on the downstream application,\nthus models for text compression must be able to\nadapt based on information about the downstream\ntask.\nSentence compression models have been used\nas sub-modules of text and speech summarization\n(Banerjee et al., 2015; Shang et al., 2018), for head-\nline generation (Dorr et al., 2003), subtitle gener-\nation (Vandeghinste and Pan, 2004), and summa-\nrizing emails (Zajic et al., 2008). Potential applica-\ntions also include snippet generation and highlight-\ning for social media, blog posts or search results.\nFigure 1: Reinforcement learning framework for sen-\ntence compression.\nGiven a particular text compression task, rele-\nvant evaluation metrics and auxiliary models of\ncompression quality may not be straightforward to\nformulate as well-behaved differentiable objectives\nthat can be used with standard backpropagation. In\naddition, ground-truth examples may be difficult\nto obtain because the annotation task is difficult\nto fully specify, and metrics which capture differ-\nent facets of compression quality, such as fluency\nand optimal sentence length, may be negatively\ncorrelated. Even in the case where ground-truth\nexamples are available, they are likely to represent\nonly a subset of the possible outputs, so there is a\nrisk of over-fitting or biasing models when relying\nsolely upon a small amount of gold training data\nfor optimization.\nRecent unsupervised sentence compression ap-\nproaches leverage powerful neural language mod-\nels to directly optimize objectives such as flu-\nency and faithfulness of compressed sentences, us-\ning discrete search strategies, without relying on\nground-truth examples (Niu et al., 2019; Zhou and\nRush, 2019; Schumann et al., 2020). However,\nthese search-based methods are very inefficient at\ninference-time because the search must navigate\nthrough a large candidate space while recomputing\nexpensive reward functions.\n1267\nFigure 2: Model architecture for compressing sentences.\nTo allow for flexible reward specification, while\nalso enabling efficient inference, we design a sim-\nple and effective reinforcement learning (RL) setup:\nour model is initialized as an unsupervised pre-\ntrained language model with an untrained binary\nclassification head (see Figure 2), and the sentence\ncompression task is framed as sequence labeling,\nwith optimization via policy gradient using a suite\nof reward functions. Sentences are compressed in\nan instantaneous, one-step fashion, similar to mod-\nern part-of-speech tagging or named entity recogni-\ntion models. This approach simplifies the learning\nsetup while also allowing for high throughput.\nAccording to quantitative evaluation on sev-\neral summarization benchmarks, our approach\nshows similar or superior performance compared\nto search-based methods, while also being much\nfaster at inference time.\nOur approach to unsupervised extractive sen-\ntence compression has the following benefits:\n• Unsupervised: No labelled examples are re-\nquired.\n• Fast inference: At test time, the model only\nperforms one-step sequence labeling.\n• Configurable: Rewards can be tailored to\nspecific use cases.\nWe review related work in Section 2. Section 3\nformalizes the task. Section 4 gives a detailed de-\nscription of the model and reward functions. Sec-\ntion 5 presents experimental results, and Sections\n6 and 7 provide analysis and discussion of our find-\nings.\n2 Related Work\nUnsupervised Sentence Compression\nEarly work on sentence compression casts the task\nas an optimization problem under linguistically mo-\ntivated constraints (Hori and Furui, 2004; Clarke\nand Lapata, 2006, 2008). The objectives to be opti-\nmized include n-gram language model scores and\nfrequency-based word relevance measures. Con-\nstraints are designed to ensure the grammaticality\nof compressions.\nSome recent work follows the discrete optimiza-\ntion paradigm while leveraging powerful models\nas objective functions in place of hand-crafted con-\nstraints, while exploring different strategies for\nheuristic search: Zhou and Rush (2019) use beam\nsearch to optimize a fluency and a similarity ob-\njective. Schumann et al. (2020) use a greedy hill-\nclimbing search to optimize fluency and similarity\nobjectives. Niu et al. (2019) use a greedy search\nwith a look-ahead mechanism, only optimizing flu-\nency. All of these recent approaches use large neu-\nral language models to estimate fluency. While the\napproach presented in whis work does not involve\ndiscrete search, we consider it complementary and\northogonal to our RL-based approach (see Section\n7 for more discussion).\nAnother commonly proposed unsupervised\nframework is to use autoencoders and reconstruc-\ntion objectives (Miao and Blunsom, 2016; Févry\nand Phang, 2018; Malireddy et al., 2020). These\napproaches are based on the assumption that a good\nsentence compression is one from which the origi-\nnal sentence can be inferred.\nWang et al. (2018) is an example of prior work\nusing reinforcement learning for unsupervised sen-\ntence compression. They use a Deep Q-Network to\noptimize a reward incorporating n-gram language\nmodel probabilities and grammatical constraints.\nThis model repeatedly deletes a token until it termi-\nnates, as opposed to our one-step approach. Zhao\net al. (2018) also use RL to optimize a syntax-\nfocused language model score. However, their\npolicy is initialized with a supervised sentence com-\npression model, whereas ours is fully unsupervised.\nReinforcement Learning for Summarization\nReinforcement learning has become popular in the\nwider field of text summarization, finding applica-\ntions in both extractive and abstractive sub-tasks.\nOne use case of RL is in supervised scenarios,\nwhere rewards are computed based on ground-truth\nexamples, e.g., ROUGE scores, to overcome is-\nsues with cross-entropy losses (Paulus et al., 2017;\nNarayan et al., 2018; Dong et al., 2018). BANDIT -\nSUM (Dong et al., 2018) in particular has a very\nsimilar RL setup to ours: they train in one-step\nepisodes where a policy predicts extractive labels\n1268\nand immediately receives a reward. Scialom et al.\n(2019) augment a ROUGE-based reward with a\nreward based on question answering. Böhm et al.\n(2019) and Stiennon et al. (2020) learn reward func-\ntions from human quality ratings of summaries.\nSimilar to our unsupervised approach, Laban et al.\n(2020) use RL for unsupervised abstractive summa-\nrization, optimizing reward functions representing\nfluency, coverage under a length constraint, and\nalso use a policy gradient approach.\n3 Task\nWe focus on the specific task of summarizing\na sentence by extracting a subset of its tokens\nin their original order. Given an input sentence\nx consisting of n tokens x = (x0, x1, ..., xn),\nwe aim to produce a sequence of binary labels\ny = (y0, y1, ..., yn) ∈ {0, 1}n, where each label\nindicates whether the corresponding input token\nshould be included in the compressed version of a\nsentence.\nWe further assume an objective function, orre-\nward function R(x, y) that measures how well ap-\nplying the labels y summarizes the original sen-\ntence x. For a particular x, the goal is to find\nargmaxy R(x, y), without access to any ground-\ntruth examples.\nIn general, there are 2n possibilities to shorten\na sentence in this task. A fixed summary length\nL would reduce this to\n\u0000n\nL\n\u0001\npossibilities, peaking\nat L = n\n2 (for even n). We do not constrain our\napproach to a fixed length, but we compare it to\nsearch-based techniques that are constrained to the\u0000n\nL\n\u0001\nsearch space.\n4 Method\n4.1 Training Procedure\nWe train a policy πθ with parameters θ to produce\nbinary labels. Given an input x, the policy πθ pre-\ndicts a binary keep/discard probability distribution\nfor each token index in x. We use the notation\nπθ(∗|x) to refer to the collection of these distribu-\ntions for all tokens in x. We obtain the probability\nπθ(y|x) of a label sequence y given input sequence\nx as follows:\nπθ(y|x) =\nY\ni\nπθ(yi|x), (1)\nwhere πθ(yi|x) is the probability of a token xi be-\ning included if yi = 1or excluded if yi = 0. To\ncompress a sentence using πθ, we select the higher\nscoring label for each token:\nya = {argmax\nyi\nπθ(yi|x) for yi ∈ y}. (2)\nWe train our model using a policy gradient tech-\nnique (Sutton et al., 1999). Unlike typical sequen-\ntial reinforcement learning scenarios, our πθ only\nperforms one action for a given input, receiving the\ncorresponding reward immediately, without transi-\ntioning through other intermediate states. There-\nfore, our setup is similar to a contextual multi-\narmed bandit problem (Langford and Zhang, 2008),\nwhere each \"arm\" corresponds to a particular label\nsequence y = (y0, y1, ..., yn) ∈ {0, 1}n. However,\nin our scenario, the policy is generally allowed to\naccess rewards for multiple possible actions via\nsampling, which is different from typical bandit\nsettings where only one (action, reward) pair is\navailable for each episode.\nThe training objective is to maximize the ex-\npected reward assigned to a predicted label se-\nquence y for a given input x, computed by the\nreward function R:\nJ(θ) =E[R(x, y)] (3)\nThe policy gradient theorem states that the gradi-\nent of this expectation can be expressed as follows\n(Sutton et al., 1999):\n∇θJ(θ) =∇θE[R(x, y) log πθ(y|x)] (4)\nSince the above expectation is intractable for a\nlarge dataset and the corresponding action space,\nthis gradient is estimated by sampling:\n∇θJ(θ) =∇θrs log πθ(ys|x), (5)\nwhere ys ∼ πθ(∗|x) is a sample from the current\npolicy at a given step, consisting of binary token\nlabels ys = (ys0, ys1, ..., ysn), and rs = R(x, ys).\nAs is commonly done when using policy gradi-\nents, we subtract a baseline from the reward for\nvariance reduction. We instantiate the baseline as\nra = R(x, ya), the reward given to the the most\nlikely label sequence ya according to the current\npolicy. The gradient becomes:\n∇θJ(θ) =∇θ(rs − ra) log πθ(ys|x) (6)\nAccordingly, we train our model by minimizing the\nfollowing loss function:\n1269\nℓθ = (ra − rs) log πθ(ys|x). (7)\nUsing the baseline ra allows the intuitive inter-\npretation that a sample ys is encouraged if its re-\nward is higher than the current policy’s prediction,\ni.e., when factor (ra − rs) is negative, and discour-\naged otherwise.\nBest-of-k Sampling\nPrior work with a similar application of policy gra-\ndient (Dong et al., 2018; Laban et al., 2021) ob-\nserved an advantage in samplingk times and taking\nthe average loss over all samples rather than using\na single sample. However, in our experiments, we\nobserve that only using the sample with the maxi-\nmum reward from a large number of samples works\nsignificantly better than taking the average or only\nsampling once. A large k improves the discovery\nof high-quality compressions – if we only use a sin-\ngle sample or a very small k, we observe a higher\ntendency of models to converge on simple behav-\niors with low reward improvements, such as only\nextracting the first- L tokens of a sentence. The\nchoice of k controls a trade-off: with a higher k,\nwe spend more time computing the rewards of sam-\nples and less on model updates, given a limited\nwall-time constraint for training. We determine k\nin an unsupervised manner using a validation set\n(details in Section 5.2).\n4.2 Model Architecture\nπθ is initialized as a transformer encoder model\nwith a linear classification head. In particular, we\nuse the 6-layer DistilRoBERTa model (Sanh et al.,\n2019) due to its efficiency and smaller size com-\npared to other BERT-like models, while retaining\ngood results on the GLUE benchmark 1. During\ntraining, the whole model is fine-tuned. For each to-\nken in the input, our model will determine whether\nit should be kept or filtered. Figure 2 visualizes\nthe design. This architecture produces summaries\nin an instantaneous, non-autoregressive fashion, al-\nlowing for fast prediction (see Section 5.6).\n4.3 Reward Functions\nWe do not have direct access to ground-truth train-\ning data in our setup, so we consider a suite of\nreward functions that may correlate with different\naspects of sentence compression quality.\n1https://huggingface.co/\ndistilroberta-base\nFluency\nThis reward function is intended to ensure gram-\nmatically correct and well-written sentences. We\nuse a masked language model (LM) to estimate the\nfluency of a compressed sentence. In particular,\nwe compute fluency as the average logit of a token\nyi in the compressed sentence y. We do this with-\nout masking yi to reduce the running time during\ntraining, as masking would require to re-encode the\nsentence for each token. Based on our experiments,\nthis simplification still produces good estimates of\nfluency.\nRfluency (y) = 1\n|y|\nX\ni=1\nLM(yi|y) (8)\nWe normalize Rfluency by dividing it by an em-\npirically set constant, to keep its values in a similar\nrange compared to the other rewards. The con-\nstant is an observed minimum value from a sample\ndataset. We argue that a masked language model is\nmore appropriate in our setup compared to a left-\nto-right (causal) language model – when predicting\nor sampling a compressed sentence during train-\ning, the sentence is treated as a finished rather than\nan intermediate output, which is not captured by\nthe auto-regressive inference of causal LMs. We\nconfirm the advantage of a masked LM over a left-\nto-right LM in a comparison on a development set\n(Appendix A).\nWe note the precedent for using language mod-\nels to measure fluency: Zhou and Rush (2019)\nand Schumann et al. (2020) use language mod-\nels trained on a summarization target domain, e.g.,\nheadlines. Laban et al. (2020) uses a generic causal\nlanguage model to estimate fluency. Niu et al.\n(2019) use a masked language model to score can-\ndidate compressions.\nSimilarity-to-Source\nThe similarity reward is intended to preserve the\nmeaning of the source sentence in the compressed\nsentence. We experiment with several options\nto compute similarity, all using models from the\nsentence-transformers library2 (Reimers\nand Gurevych, 2019):\n• Bi-Encoder Similarity: A sentence encoder\nf separately computes embeddings for the\nsource and the predicted summary. We calcu-\nlate the cosine similarity between both embed-\ndings: Rsim(x, y) =cos(f(x), f(y))\n2https://www.sbert.net/\n1270\n• Cross-Encoder Similarity: Output of a cross-\nencoder model fsim measuring the seman-\ntic textual similarity between both sentences:\nRsim(x, y) =fsim(x, y)\n• Cross-Encoder NLI: We also test a natural\nlanguage inference (NLI) model fnli to esti-\nmate how well a compressed sentence retains\nsource information. The intuition is that the\nsource should imply information in the output:\nRnli(x, y) =fnli(y|x)\nBased on experiments on a development dataset,\nthe bi-encoder similarity performs best in our setup.\nLength and Compression Ratio\nBecause our model is non-sequential, we cannot\neasily employ a hard constraint to control the length\nof compressed sentences. Instead, we impose a soft\nlength control using Gaussian reward functions. In\nparticular, we either use a reward function for the\nlength (token count) in a compressed sentenceRlen,\nor one for the compression ratio between the source\nand prediction, in terms of token counts, Rcr. We\nchoose one of these two depending on whether a\nconsistent length or a consistent ratio is desired,\nwhich differs for different evaluation datasets. We\nset the distribution means of both rewards as the\ndesired values for word count and compression\nratio. We set the standard deviations as the mean\ntimes a factor s which we set to 0.4 for both reward\nfunctions (Equations 9, 10):\nRlen = N(µL, (s × µL)2), (9)\nRcr = N(µcr, (s × µcr)2). (10)\nReward Aggregation\nThe final reward function is an average of the re-\nward functions Rfluency , Rsim, combined with ei-\nther Rlen or Rcr:\nrtotal(x, y) =1\n3\nX\ni\nRi(x, y). (11)\nIn practice, when the downstream task is known,\nreward functions may be designed and calibrated\nbased upon insights and domain expertise, e.g.,\nan optimal summary length for a specific applica-\ntion or different language models corresponding\nto different summary styles. In this work, we only\nuse publicly available and commonly-used off-the-\nshelf models to construct reward functions.\n5 Experiments\nThis section presents a detailed analysis and evalu-\nation results for our proposed model. We name our\nmodel SCRL (Sentence Compression with Rein-\nforcement Learning). We make all code, model\noutputs and data available3.\n5.1 Datasets\n5.1.1 Training Datasets\nWe use two datasets for training: Newsroom\n(Grusky et al., 2018) and Gigaword (Rush et al.,\n2015). For Newsroom, we extract the first three\nsentences from each article, only keeping sentences\nwith a number of tokens between 15 and 60. News-\nroom was chosen due to the large size and a va-\nriety of un-preprocessed news articles from dif-\nferent sources. Ground-truth summaries are not\nincluded in the training data, thus the two datasets\nare treated as large unlabeled text collections. We\ntrain a model for short headline-like summaries on\nGigaword to evaluate it on the Gigaword test set,\nwhich comes in a specific preprocessed format 4.\nTraining on Gigaword allows to expose the model\nto the same preprocessing, for a fair evaluation.\n5.1.2 Development Dataset\nWe constructed a small labelled validation dataset\nfor model development: we automatically identi-\nfied sentence-summary pairs in Newsroom, also\nincluding title-summary pairs, by extracting cases\nwhere the tokenized summary is contained in a\ntokenized sentence, with preserved order. We man-\nually filter a subset of these examples based on\ngrammaticality and informativeness and obtain 280\nexamples. This dataset was only used during ini-\ntial development to compare the different reward\nfunction variants discussed in Section 4.3.\n5.1.3 Evaluation Datasets\nThe evaluation includes five test sets – key statistics\nare listed in Table 1.Lsrc, Ltgt are the token counts\nin source and target sentences and cr = Ltgt/Lsrc\nis the compression ratio. Following Schumann\net al. (2020), we compare our models on Gigaword\nagainst baselines of comparable length brackets\nusing ROUGE F1-scores5. For DUC2004 (Task\n3https://github.com/complementizer/\nrl-sentence-compression\n4Lowercased, pre-tokenized, rare words and digits re-\nplaced with special tokens.\n5We only consider lengths similar to the ground-truth, i.e.\n8-10 tokens.\n1271\nTestset Type Size Lsrc Ltgt cr\nGigaword abs 1951 29.7 8.8 0.4\nDUC2004 abs 500 32.9 11.9 0.41\nGoogle ext 1000 27 11 0.45\nBroadcast ext 1370 19.8 14.4 0.76\nBNC ext 1629 27.9 19.3 0.72\nTable 1: Overview of the evaluation datasets. The Type\ncolumn indicates whether the ground-truth is extractive\nor abstractive. Size gives the number of sentences.\n1), following prior work, we truncate model out-\nputs to 75 characters and compute ROUGE recall\nscores. While Gigaword and DUC2004 contain\nabstractive ground-truth summaries, the remaining\nthree datasets have token-level extractive ground-\ntruth summaries. The ground-truth compressions\nin the Google sentence compression dataset (Fil-\nippova and Altun, 2013) were automatically gen-\nerated using grammatical constraints and distant\nsupervision via headlines. The Broadcast and\nBNC datasets (Clarke and Lapata, 2008) contain\nmanually created extractive sentence compressions\nwhich tend to be longer compared to the other\nevaluation datasets. Following previous work, we\nreport a simple F1-score based on tokenized pre-\ndicted and ground-truth summaries on the three\nextractive datasets, but also measure ROUGE F1\nscores.\n5.2 Model Development\nWe tune our approach in several phases. At first,\nwe identify an optimal learning rate and batch size\nusing a grid search with a fixed training duration.\nWe compare different settings based on the average\nreward achieved on a unlabelled, held-out set of the\ntraining data. Next, we test different values of k\n(1, 5, 10, 50, 100), the number of samples per step,\nand pick the best k based on the average reward on\nthe validation set. This method of hyperparameter\ntuning is fully unsupervised.\nUsing learning rate 1e − 05, batch size 4 and\nk = 100identified in the previous runs, we next\ncompare the different options for the similarity re-\nward listed in Section 4.3 and pick the best (bi-\nencoder similarity) based on the F1-score on our\nlabelled Newsroom-based validation set (see Ap-\npendix B).\n5.3 Training\nWe initialize the encoder component of our model\nwith the pretrained 6-layer DistilRoBERTa model\n(Sanh et al., 2019). The binary classifier module\nName Train data Test Data Time\nSCRL-L8 Gigaword Gigaword 9\nSCRL-L11 Newsroom DUC04, Google 9.5\nSCRL-CR75 Newsroom Broadcast, BNC 10\nTable 2: Overview of trained models and training time\nin hours.\nis initialized randomly. We train each model for\n8,000 steps with a batch size of 4 on a Google\nCloud virtual machine with one NVIDIA Tesla T4\nGPU, using the AdamW optimizer (Loshchilov and\nHutter, 2019). Our default reward combination con-\ntains masked-LM fluency and bi-encoder similarity\ncombined with either Rlen or Rcr. Table 2 gives\nan overview of the three models that are used in\nthe evaluation. Note that the sample size of 100 is\nresponsible for the long training durations. SCRL-\nL8 and SCRL-L11 are trained with Rlen whereas\nSCRL-CR75 is trained with Rcr, with a compres-\nsion ratio of 0.75. This is because the ground-truth\nsummary lengths are approximated better by a fixed\nlength rather than a fixed ratio in the Google and\nDUC2004 datasets, whereas a fixed ratio describes\nthe Broadcast and BNC datasets better.\n5.4 Baselines\nWe compare our model to the greedy stochastic hill\nclimbing approach in Schumann et al. (2020) which\nobtained state-of-the-art ROUGE results for unsu-\npervised baselines on the Gigaword and DUC2004\ndatasets. Because this method and SCRL do not\nhave identical objective functions, we implement\nthe hill climbing algorithm applied to our reward\nfunctions, which we will name HC throughout this\nwork. This allows for a clearer comparison between\nRL and discrete search. HC optimizes Rfluency ,\nRsim under fixed length constraints instead of us-\ning Rlen and Rcr. Different from Schumann et al.\n(2020), it runs for a fixed number of 2000 steps and\nrestarts only when the search is stuck rather than\nin equal intervals (details in Appendix E). We ana-\nlyze the performance of HC for different budgets\nto understand at what point search can surpass the\nlearned policies. We also compare against Zhou\nand Rush (2019), Niu et al. (2019) and the RL-\nbased method by Wang et al. (2018) on datasets\nwhere results are available.\n5.5 Evaluation Results\nTable 3 shows the evaluation results on all used\ntest datasets. Results of methods apart from SCRL\nand HC are taken from previous works. We com-\n1272\nDataset Model ROUGE F1 Ld Lo cro Inf. Time (s)\n1 2 L\nLead-L8 21.39 7.42 20.03 8 7.9\nZhou and Rush (2019) 26.48 10.05 24.41 9.3\nGigaword Schumann et al. (2020) L8 26.32 9.36 24.19 8 7.9\nSchumann et al. (2020) L10 28.80 10.66 25.82 10 9.8\nHC-L8 28.00 8.53 25.90 8 7.96 0.31 11.733\nSCRL-L8 29.64 9.98 26.57 8 7.68 0.28 0.004\nDUC2004 Zajic et al. (2004)⋆ 25.12 6.46 20.12\nBaziotis et al. (2019) 22.13 6.18 19.3\nWest et al. (2019) 22.85 5.71 19.87\nSchumann et al. (2020) 27.41 8.76 23.89 13\nHC-L11 27.40 8.65 24.16 11 10.69 0.36 12.305\nSCRL-L11 25.27 7.82 22.14 11 10.58 0.35 0.004\nFilipova⋆ 0.82 0.38\nWang et al. (2017)⋆ 0.8 0.43\nWang et al. (2018) 0.565\nGoogle Zhou and Rush (2019) 0.61\nNiu et al. (2019) 0.5\nHC-L11 68.04 49.21 67.40 0.637 11 11.0 0.46 11.261\nSCRL-L11 70.22 53.03 69.84 0.711 11 10.8 0.44 0.004\nWang et al. (2017)⋆ 0.66\nBroadcast Wang et al. (2018) 0.665\nHC-CR75 82.20 63.78 81.76 0.792 75% 14.9 0.77 13.516\nSCRL-CR75 82.22 66.01 81.78 0.787 75% 15.1 0.78 0.004\nWang et al. (2017)⋆ 0.66 0.53\nBNC Wang et al. (2018) 0.675\nHC-CR75 78.91 60.10 78.13 0.768 75% 21.0 0.76 15.268\nSCRL-CR75 79.49 62.32 78.63 0.765 75% 21.0 0.76 0.004\nTable 3: Results on evaluation datasets. ⋆ indicates supervised models. ROUGE F1-scores are shown for all\ndataset but DUC2004, where ROUGE recall is used. Ld: desired output length, Lo / cro: actual average length /\ncompression ratio of the outputs.\npute ROUGE scores using the implementation\nfrom Google Research 6. On Gigaword, SCRL\noutperforms all baselines, except Schumann et al.\n(2020) with a 10 token constraint in ROUGE-2. On\nDUC2004, SCRL remains behind the hill climb-\ning methods, but outperforms other unsupervised\nbaselines. On the Google dataset, SCRL obtains\nstate-of-the-art results among unsupervised meth-\nods. On Broadcast and BNC,SCRL and HC obtain\nvery similar scores, which are both higher than pre-\nviously reported results. Figure 3 shows ROUGE-1\nscores obtained by HC at different search budgets,\ncompared to SCRL . The hill climbing strategy ap-\nproaches or outperforms the trained model at dif-\nferent paces, depending on the dataset.\nInterestingly, HC still achieves higher rewards\nthan SCRL relatively early during its search (see\nAppendix F), which is inconsistent with the evalua-\ntion results. Potential reasons for this disparity are\ndisadvantages through the hard length constraints,\na mismatch between the heuristic reward functions\nand evaluation metrics, and beneficial biases in-\nduced through our training framework.\n6https://github.com/google-research/\ngoogle-research/tree/master/rouge\nFigure 3: Evaluation scores of RL model compared to\nhill climbing algorithm (HC) at different search budgets.\n5.6 Prediction Running Times\nWe compare the inference-time speed of SCRL\nwith HC using different budgets of search steps7.\nThe fastest batch size for both approaches is used.\nThe Inference Time in Table 3 shows the average\nnumber of seconds per processed sentence, with\nthe number of search steps set toT = 2000for HC.\nSCRL is roughly 4000× faster than HC with T =\n7On a Google Colab Notebook with a Tesla P-100 GPU\n1273\nFigure 4: Distribution of summary lengths and com-\npression ratios. The maximum frequencies of HC are\ncropped.\nFigure 5: Distribution of relative positions from which\ntokens are extracted in source sentences.\n2000, and ∼ 200× faster when T is reduced to 100,\nfor example. We believe that such a speed-up with\na preserved evaluation performance is a critical\nfactor when considering real-world applications of\nsentence compression.\n6 Analysis\n6.1 Summary Length and Extraction Regions\nThe length and compression ratio of summaries pro-\nduced by SCRL is distributed around the desired\nvalues, with peakier distributions than in ground-\ntruth summaries (examples in Figure 4). HC pro-\nduces exactly the desired value whenever possible,\ndue to the enforced constraint for length or ratio.\nFigure 5 shows how SCRL and HC extract tokens\nfrom different relative positions within source sen-\ntences. SCRL has a higher tendency to extract\nearly tokens. We hypothesize that this is a reliable\nhigh-reward strategy discovered during training,\nconsidering that a milder form of the lead-bias also\nshows in HC. Note that neither method is inher-\nently biased in its design to prefer tokens from\ncertain regions.\nFigure 6: Development of reward functions (of SCRL-\nL11) and summary length during training (all models),\nfrom a moving average over 50 steps, using a log scale.\n6.2 Training Dynamics\nFigure 6 shows how rewards and summary length\ndevelop throughout training. The rewards generally\nincrease quickly in the first few hundred training\nsteps and then continue to grow very slowly. Flu-\nency starts to increase later than the other reward\nfunctions, which is likely related to our observa-\ntion that it is more sensitive to small changes in a\nsummary. Interestingly, the summary lengths de-\nvelop differently depending on the length or com-\npression setting – SCRL-L8 and SCRL-L11 start\nwith short summaries and increase the size over\ntime whereas SCRL-CR75 starts with long sum-\nmaries before settling on a shorter certain range.\n6.3 Learned Summarization Techniques\nOur models learn a variety of behaviors to com-\npress sentences, such as removing articles, auxil-\niary verbs, relative clauses and temporal expres-\nsions. Figure 7 shows some examples.\n6.4 Error Analysis\nEven though our models learn to produce gram-\nmatical sentences fairly well, grammatical errors\ndo still appear, and are more common for the mod-\nels with a short output length ( SCRL-8 , SCRL-\n11). In some cases, semantic errors occur where\nthe original meaning is changed or made unintel-\nligeble. Both SCRL and HC are susceptible of\nsemantic and grammatical errors, as can be seen\nin some examples in Appendix G. A type of error\nthat is specific to SCRL is the splitting or merging\nof tokens resulting from its operation on Byte Pair\nEncoding-based subword tokens (more details in\nAppendix C).\n6.5 Customization via Reward Functions\nTo demonstrate that our approach is flexible for\ncustomization, we pick a simple example of re-\n1274\nFigure 7: Examples of learned summarization techniques. Selected tokens are marked in green and common removal\nbehaviors are pointed out with underlining.\nprogramming model behavior using a hand-crafted\nreward function. We note that in some cases, the\nmodel unnecessarily keeps day references in com-\npressed sentences, such as \"Thursday\" or \"yester-\nday\". We construct a simple reward function that\nreturns zero if any day-like word from a small\ngazetteer appears in an output and a score of 1\notherwise. We fine-tune an existing model with\nthis additional reward and observe that it success-\nfully avoids including day-words that the previous\nmodel would include. Importantly, it additionally\nlearned to remove other tokens attached to day-\nwords, e.g. \"on\" in \"on Monday\", keeping the\nsentences grammatical. Table 4 shows some exam-\nples. Empirically, the new model’s outputs contain\nwords from the gazeteer in 1% of cases where they\nappear in the source, compared to 12% in the initial\nmodel.\nInitial Model Customized Reward\nThe burrito chain said\non Tuesday that comparable\nsales fell 26.1% last month.\nThe burrito chain said\nthat comparable sales\nfell 26.1%.\nHis car was found\nlast Thursday\nalongside Rubyvale Road.\nHis car was found\nalongside\nRubyvale Road.\nTable 4: Example outputs of model with customized\nreward function to exclude mentions of days.\n7 Discussion\nWe argue that RL offers the following advantages\nover discrete search strategies for sentence com-\npression and similar text editing or generation tasks.\nThe necessary search and exploration is moved into\nthe training stage, allowing fast inference indepen-\ndently of how efficient objectives are to compute.\nFurthermore, discrete search unnecessarily spends\ntime navigating through low-quality outputs that\na trained model can quickly learn to avoid. Lim-\nitations of our approach compared to the search-\nbased approach are its lesser flexibility in terms of\non-the-fly customization and a sensitivity to dis-\nparities between training data and the application\ndomain. Furthermore, the trained models show a\nlower capability to optimize the selected objectives\ncompared to search, though this does not have a\nnegative impact on the evaluation in most cases.\nThe fact that most of our training time is spent\non estimating the quality of sampled compressions\ndue to large sample sizek, shows that our approach\nis somewhat similar to large-scale search strate-\ngies applied to a whole dataset, with the difference\nthat the sampling behavior at each step changes\nover time and is informed by previous steps. This\nsuggests that discrete search could support the RL\ntraining, similarly to the learning-from-search ap-\nproach described by (Li et al., 2020).\n8 Conclusion\nThis work presents a simple and effective approach\nfor learning sentence compression models based on\nobjective functions rather than ground-truth exam-\nples. Because it is unsupervised, it is well-suited\nfor creating customized applications even when no\ngold training data is available, allowing for task-\nspecific tuning based on arbitrary sets of reward\nfunctions, which do not need to be differentiable.\nImportantly, our approach is very fast at inference\ntime compared to alternative discrete search-based\nmethods. We are interested in several future direc-\ntions related to this work: 1) systematic approaches\nto design reward functions for summarization, 2)\nRL-based summarization models with length con-\ntrol on the fly, 3) testing our approach on other\nlanguages, and 4) the design of curricula for dif-\nferent reward functions as they might pose varying\ndifficulties at different stages of the training.\n1275\nAcknowledgments\nThis work was funded by the Irish Research Coun-\ncil (IRC) under grant number EBPPG/2018/23,\nthe Science Foundation Ireland (SFI) under grant\nnumber 12/RC/2289_P2 and the enterprise partner\nAylien Ltd.\nReferences\nSiddhartha Banerjee, Prasenjit Mitra, and Kazunari\nSugiyama. 2015. Multi-document abstractive sum-\nmarization using ilp based multi-sentence compres-\nsion. In Twenty-Fourth International Joint Confer-\nence on Artificial Intelligence.\nChristos Baziotis, Ion Androutsopoulos, Ioannis Kon-\nstas, and Alexandros Potamianos. 2019. Seq3: Differ-\nentiable sequence-to-sequence-to-sequence autoen-\ncoder for unsupervised abstractive sentence compres-\nsion.\nFlorian Böhm, Yang Gao, Christian M. Meyer, Ori\nShapira, Ido Dagan, and Iryna Gurevych. 2019. Bet-\nter rewards yield better summaries: Learning to sum-\nmarise without references. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3110–3120, Hong Kong,\nChina. Association for Computational Linguistics.\nJames Clarke and Mirella Lapata. 2006. Models for sen-\ntence compression: A comparison across domains,\ntraining requirements and evaluation measures. In\nProceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meet-\ning of the Association for Computational Linguistics,\npages 377–384, Sydney, Australia. Association for\nComputational Linguistics.\nJames Clarke and Mirella Lapata. 2008. Global in-\nference for sentence compression an integer linear\nprogramming approach. Journal of Artificial Intelli-\ngence Research, 31:399–429.\nYue Dong, Yikang Shen, Eric Crawford, Herke van\nHoof, and Jackie Chi Kit Cheung. 2018. Bandit-\nSum: Extractive summarization as a contextual ban-\ndit. In Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing, pages\n3739–3748, Brussels, Belgium. Association for Com-\nputational Linguistics.\nBonnie Dorr, David Zajic, and Richard Schwartz. 2003.\nHedge trimmer: A parse-and-trim approach to head-\nline generation. In Proceedings of the HLT-NAACL\n03 Text Summarization Workshop, pages 1–8.\nThibault Févry and Jason Phang. 2018. Unsupervised\nsentence compression using denoising auto-encoders.\nIn Proceedings of the 22nd Conference on Computa-\ntional Natural Language Learning, pages 413–422,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKatja Filippova and Yasemin Altun. 2013. Overcom-\ning the lack of parallel data in sentence compression.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1481–1491, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nChiori Hori and Sadaoki Furui. 2004. Speech summa-\nrization: An approach through word extraction and\na method for evaluation. IEICE Trans. Inf. Syst. ,\n87-D(1):15–25.\nPhilippe Laban, Andrew Hsi, John Canny, and Marti A\nHearst. 2020. The summary loop: Learning to write\nabstractive summaries without examples. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5135–5150.\nPhilippe Laban, Tobias Schnabel, Paul Bennett, and\nMarti A. Hearst. 2021. Keep it simple: Unsupervised\nsimplification of multi-paragraph text. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6365–6378, Online.\nAssociation for Computational Linguistics.\nJohn Langford and Tong Zhang. 2008. The epoch-\ngreedy algorithm for multi-armed bandits with side\ninformation. In Advances in Neural Information Pro-\ncessing Systems, volume 20. Curran Associates, Inc.\nJingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael\nLyu, and Irwin King. 2020. Unsupervised text gener-\nation by learning from search. Advances in Neural\nInformation Processing Systems, 33:10820–10831.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nChanakya Malireddy, Tirth Maniar, and Manish Shri-\nvastava. 2020. SCAR: Sentence compression using\nautoencoders for reconstruction. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 88–94, Online. Association for Computational\nLinguistics.\nYishu Miao and Phil Blunsom. 2016. Language as a la-\ntent variable: Discrete generative models for sentence\ncompression. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 319–328, Austin, Texas. Association for\nComputational Linguistics.\n1276\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1747–1759, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nTong Niu, Caiming Xiong, and Richard Socher.\n2019. Deleter: Leveraging bert to perform unsu-\npervised successive text compression. arXiv preprint\narXiv:1909.03223.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nRaphael Schumann, Lili Mou, Yao Lu, Olga Vechto-\nmova, and Katja Markert. 2020. Discrete optimiza-\ntion for unsupervised sentence summarization with\nword-level extraction. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5032–5042.\nThomas Scialom, Sylvain Lamprier, Benjamin Pi-\nwowarski, and Jacopo Staiano. 2019. Answers unite!\nunsupervised metrics for reinforced summarization\nmodels. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3246–3256, Hong Kong, China. Association for Com-\nputational Linguistics.\nGuokan Shang, Wensi Ding, Zekun Zhang, Antoine Tix-\nier, Polykarpos Meladianos, Michalis Vazirgiannis,\nand Jean-Pierre Lorré. 2018. Unsupervised abstrac-\ntive meeting summarization with multi-sentence com-\npression and budgeted submodular maximization. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 664–674, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAdvaith Siddharthan. 2014. A survey of research on\ntext simplification. ITL – International Journal of\nApplied Linguistics, 165:259–298.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul Christiano. 2020. Learning\nto summarize from human feedback. arXiv preprint\narXiv:2009.01325.\nRichard S. Sutton, David A. McAllester, Satinder P.\nSingh, and Yishay Mansour. 1999. Policy gradient\nmethods for reinforcement learning with function\napproximation. In Advances in Neural Information\nProcessing Systems 12, [NIPS Conference, Denver,\nColorado, USA, November 29 - December 4, 1999],\npages 1057–1063. The MIT Press.\nVincent Vandeghinste and Yi Pan. 2004. Sentence com-\npression for automated subtitling: A hybrid approach.\nIn Text Summarization Branches Out, pages 89–95,\nBarcelona, Spain. Association for Computational Lin-\nguistics.\nLiangguo Wang, Jing Jiang, Hai Leong Chieu, Chen Hui\nOng, Dandan Song, and Lejian Liao. 2017. Can\nsyntax help? improving an LSTM-based sentence\ncompression model for new domains. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1385–1393, Vancouver, Canada. Association\nfor Computational Linguistics.\nLiangguo Wang, Jing Jiang, and Lejian Liao. 2018. Sen-\ntence compression with reinforcement learning. In\nInternational Conference on Knowledge Science, En-\ngineering and Management, pages 3–15. Springer.\nPeter West, Ari Holtzman, Jan Buys, and Yejin Choi.\n2019. BottleSum: Unsupervised and self-supervised\nsentence summarization using the information bottle-\nneck principle. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3752–3761, Hong Kong, China. Association\nfor Computational Linguistics.\nDavid Zajic, Bonnie Dorr, and Richard Schwartz. 2004.\nBbn/umd at duc-2004: Topiary. In Proceedings\nof the HLT-NAACL 2004 Document Understanding\nWorkshop, Boston, pages 112–119.\nDavid M Zajic, Bonnie J Dorr, and Jimmy Lin. 2008.\nSingle-document and multi-document summarization\ntechniques for email threads using sentence com-\npression. Information Processing & Management ,\n44(4):1600–1610.\nYang Zhao, Zhiyuan Luo, and Akiko Aizawa. 2018. A\nlanguage model based evaluator for sentence com-\npression. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 2: Short Papers), pages 170–175.\n1277\nJiawei Zhou and Alexander M Rush. 2019. Simple\nunsupervised summarization by contextual match-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5101–5106.\nA Masked vs. Causal Language Model\nfor Fluency\nWe compare the masked DistilRoBERTa8 language\nmodel to the causal DistilGPT29 model on our de-\nvelopment dataset. Both models have roughly the\nsame number of parameters (82M). Table 5 shows\nthe results.\nLM DistilRoBERTa DistilGPT2\nF1-Score 0.565 0.546\nTable 5: Comparison of a masked and a causal language\nmodel to estimate fluency.\nB Similarity Functions\nTable 6 compares different variants of the similarity\nreward functions on our development dataset.\nSimilarity Bi-Sim Cross-Sim NLI\nF1-Score 0.624 0.598 0.564\nTable 6: Comparison of similarity reward functions on\nour development dataset.\nC Error Analysis: Split and Merged\nTokens\nOne type of error is the splitting or merging of to-\nkens from the source which results from the fact\nour model predicts labels at the level of BPE sub-\nword tokenization used in the pretrained language\nmodel that we finetune. While some of these oc-\ncurrences are minor, e.g. ’St.’ → ’St’, or even\nuseful compressions, e.g. ’29th’ → ’29’, many of\nthese cases produce noisy outputs, e.g ’Perigord’\n→ ’Perig’. Based upon analysis of prediction be-\nhavior, we estimate that 6% of output sentences\ncontain some form of this phenomenon.\nD Implementation Details\nD.1 Pretrained Model IDs\nTable 7 lists the model IDs of all open-source pre-\ntrained models used in this work, which can be\nfound at https://huggingface.co.\n8https://huggingface.co/\ndistilroberta-base\n9https://huggingface.co/distilgpt2\nAlgorithm 1 Stochastic First-Choice Hill Climbing\ninput objective function R(x, y), source sentence x, sum-\nmary length L, number of steps T, initialization function\nInit(x, L), sampling function S(y)\ny0 ← Init(x, L)\nfor t = 1to T do\ny′ ← S(yt−1)\nif R(x, y′) ≥ R(x, yt−1) then\nyt = y′\nelse\nyt = yt−1\nreturn yt\nD.2 Tokenization\nWe use the NLTK10 Punkt Tokenizer for several\npurposes in this work:\n• Obtaining the sentence length and compres-\nsion ratio in Rlen and Rcr.\n• Compressing sentences by selecting tokens in\nour hill climbing implementation HC.\n• Obtaining source and summary tokens to\ncompute the F1-score in Table 3, except for\nsource and reference tokens on the Gigaword,\nBroadcast and BNC datasets which are preto-\nkenized.\nThe involved transformer models ( SCRL ,\nRfluency , Rsim) internally tokenize sentences\nbased on Byte Pair Encoding.\nE Hill Climbing Baseline\nAlgorithm 1 shows the hill climbing search ap-\nproach HC used in our experiments, which is based\non Schumann et al. (2020). At the beginning, a bi-\nnary label sequence y0 is initialized by setting L\nrandomly selected labels to 1 and the rest to 0. At\neach step t, S(y) samples a new label sequence y′\nby randomly selecting a positive and a negative-\nvalued label yi and yj and swapping their value.\nNote that this always keeps the number of tokens\nat L. The sampled y′ is accepted if it obtains a\nhigher or equal objective score than the previously\nbest candidate yt−1. We keep track of previously\ncreated sequences and skip these. If no new label\nsequence can be discovered at step t, we termi-\nnate the algorithm and restart it with T − t remain-\ning steps. In the end, the highest-scoring y found\nacross different runs is returned. We generally set\nT to 2000 and keep track of intermediate results to\nevaluate HC also at fewer search steps. Due to this,\n10https://www.nltk.org/\n1278\nModel Usage Model ID\nEncoder initialization for SCRL models distilroberta-base\nMasked LM Fluency Reward distilroberta-base\nCausal LM Fluency Reward distilgpt2\nBi-Encoder Similarity Reward all-distilroberta-v1\nCross-Encoder Similarity Reward cross-encoder/stsb-distilroberta-base\nCross-Encoder NLI Reward cross-encoder/nli-distilroberta-base\nTable 7: Overview of pretrained models used throughout this work.\nwe decided restart the search dynamically rather in\nequal-paced intervals, which we believe should be\ntuned with respect to a known maximum budget T.\nF Rewards Obtained by HC vs. SCRL\nFigure 8 compares the Rfluency and Rsim rewards\nof SCRL to HC with different search budgets. The\nlength and compression ratio rewards are not in-\ncluded as these are enforced through a constraint by\nHC. Note that these figures need to be interpreted\ncarefully as they assume that both approaches pro-\nduce summaries of comparable lengths. For exam-\nple, the similarity reward tends to increase with the\nsummary length.\nG Output Examples\nTable 8 lists a few examples outputs produced by\nSCRL and HC.\nFigure 8: Rewards of RL model compared to hill climbing algorithm (HC) at different search budgets.\n1279\nSource the us space shuttle atlantis separated from the orbiting russian mir space station early\nsaturday , after three days of test runs for life in a future space facility , nasa announced\n.\nSCRL-L8 the space shuttle atlantis separated from russian station\nHC-L8 atlantis space station after test runs for nasa\nSource a katyusha rocket fired from lebanon on saturday morning hit the western galilee in\nnorth israel , causing two lightly hurt , israel radio reported .\nSCRL-L8 katyusha rocket fired from lebanon hit galilee israel\nHC-L8 katyusha rocket fired hit western galilee israel israel\nSource Manchester United have agreed a £35m deal to sign Sporting Lisbon midfielder\nWilliam Carvalho, according to talkSPORT.\nSCRL-L11 Manchester United agreed £35m deal to sign Lisbon midfielder William Carvalho.\nHC-L11 Manchester United have agreed a £35m deal to sign William Carvalho\nSource Egyptian President Hosni Mubarak met here Sunday with Syrian President Hafez\nAssad to try to defuse growing tension between Syria and Turkey.\nSRCL-L11 Egyptian President Hosni Mubarak met with Syrian President Hafez Assad def.\nHC-L11 Egyptian President Hosni Mubarak met Sunday with Syrian President Hafez Assad\nSource Russian President Boris Yeltsin, who is still recuperating from his latest illness, has\ncanceled a trip to an Asian summit next month, his office said Friday.\nSCRL-L11 Russian President Boris Yeltsin recuperating has canceled a trip to Asian summit.\nHC-L11 Russian President Boris Yeltsin has canceled trip to an Asian summit\nSource Laurie had a passion and a warmth for people rather than the state .\nSCRL-CR75 Laurie had a passion and warmth for people.\nHC-CR75 Laurie had a passion and warmth for the state .\nSource And speaking of the royals , the Duchess of York , Sarah Ferguson , was in Los\nAngeles last week holed up at the Four Seasons Hotel and when she ventured out , I\nhear she visited some of the studios like Sony to have meetings involving TV projects .\nSCRL-CR75 And speaking of the royals, the Duchess of York, Sarah Ferguson, was in Los Angeles\nlast week holed up at the Four Seasons Hotel and I hear she visited studios like Sony\nto have meetings.\nHC-CR75 And speaking of royals , Duchess of York Sarah Ferguson was in Los Angeles last\nweek at the Four Seasons Hotel and when she ventured out she visited some of the\nstudios like Sony to have meetings .\nSource Of the 24,058 people interviewed , 37.7 per cent of women attended arts events and\n33.1 per cent of men .\nSCRL-CR75 Of 24,058 people interviewed, 37.7 per cent of women attended arts events and 33.1.\nHC-CR75 Of 24,058 people interviewed , 37.7 per cent women attended arts events and 33.1\nmen .\nTable 8: Output examples, with semantic and grammatical errors highlighted.\n1280"
}