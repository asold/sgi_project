{
    "title": "Hitachi at SemEval-2020 Task 11: An Empirical Study of Pre-Trained Transformer Family for Propaganda Detection",
    "url": "https://openalex.org/W3116231956",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2911179418",
            "name": "Gaku Morio",
            "affiliations": [
                "Hitachi (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2977565552",
            "name": "Terufumi Morishita",
            "affiliations": [
                "Hitachi (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2105416107",
            "name": "Hiroaki Ozaki",
            "affiliations": [
                "Hitachi (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2120881923",
            "name": "Toshinori Miyoshi",
            "affiliations": [
                "Hitachi (Japan)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963106052",
        "https://openalex.org/W2986191653",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W28412257",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2970529259",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2984030454",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2047028564",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2949676527",
        "https://openalex.org/W3113763975",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2984208213",
        "https://openalex.org/W2970487286",
        "https://openalex.org/W2978871345",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2982953871",
        "https://openalex.org/W2982957557",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2946595845",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W4234698323",
        "https://openalex.org/W3105729093",
        "https://openalex.org/W1940872118",
        "https://openalex.org/W2143612262",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2962897394",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2980282514"
    ],
    "abstract": "In this paper, we show our system for SemEval-2020 task 11, where we tackle propaganda span identification (SI) and technique classification (TC). We investigate heterogeneous pre-trained language models (PLMs) such as BERT, GPT-2, XLNet, XLM, RoBERTa, and XLM-RoBERTa for SI and TC fine-tuning, respectively. In large-scale experiments, we found that each of the language models has a characteristic property, and using an ensemble model with them is promising. Finally, the ensemble model was ranked 1st amongst 35 teams for SI and 3rd amongst 31 teams for TC.",
    "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1739–1748\nBarcelona, Spain (Online), December 12, 2020.\n1739\nHitachi at SemEval-2020 Task 11: An Empirical Study of Pre-Trained\nTransformer Family for Propaganda Detection\nGaku Morio∗, Terufumi Morishita*, Hiroaki Ozaki and Toshinori Miyoshi\nHitachi, Ltd.\nResarch and Development Group\nKokubunji, Tokyo, Japan\n{gaku.morio.vn, terufumi.morishita.wp,\nhiroaki.ozaki.yu, toshinori.miyoshi.pd}@hitachi.com\nAbstract\nIn this paper, we show our system for SemEval-2020 task 11, where we tackle propaganda span\nidentiﬁcation (SI) and technique classiﬁcation (TC). We investigate heterogeneous pre-trained\nlanguage models (PLMs) such as BERT, GPT-2, XLNet, XLM, RoBERTa, and XLM-RoBERTa\nfor SI and TC ﬁne-tuning, respectively. In large-scale experiments, we found that each of the lan-\nguage models has a characteristic property, and using an ensemble model with them is promising.\nFinally, the ensemble model was ranked 1st amongst 35 teams for SI and 3rd amongst 31 teams\nfor TC.\n1 Introduction\nThis paper shows our proposed system for the SemEval-2020 task 11: Detection of Propaganda Tech-\nniques in News Articles (Da San Martino et al., 2020). The goal of the task was to design a model to\ndetect and classify propaganda. To this end, there are two subtasks: span identiﬁcation (SI) for predicting\npropaganda spans and technique classiﬁcation (TC) for predicting propaganda technique types used for\na given span. SI can be assumed as a sequence labeling problem and TC as a multi-label classiﬁcation\nproblem.\nRecent studies such as (Yoosuf and Yang, 2019; Vlad et al., 2019) proposed employing BERT (Devlin\net al., 2019), a pre-trained language model, for propaganda detection. Since propaganda detection tasks\nrequire highly semantic understanding, leveraging such strong pre-trained language models is promis-\ning. However, new state-of-the-art pre-trained language models, such as XLNet (Yang et al., 2019) and\nRoBERTa (Liu et al., 2019), are being proposed rapidly, and there are no sufﬁcient studies on them in\nthe research on propaganda detection. Revealing the ability of capturing propaganda semantics with\nstate-of-the-art models could contribute to discussing future applications.\nTherefore, we investigate state-of-the-art pre-trained language models ( PLMs) for propaganda detec-\ntion. We employ not only BERT (Devlin et al., 2019) but other PLMs such as GPT-2 (Radford et al.,\n2019), RoBERTa(Liu et al., 2019),XLM-RoBERTa(Conneau et al., 2019),XLNet (Yang et al., 2019),\nand XLM (Lample and Conneau, 2019). The PLMs are ﬁne-tuned by our proposed SI and TC models\nwith various hyperparameters. We also propose an effective ensemble method with stacked generaliza-\ntion, which is generally better than a naive average ensemble.\nOur ensemble model was ranked 1st in SI and 3rd in TC, showing that leveraging state-of-the-art\nPLMs is promising. We also empirically gained the following insights as described later.\n1.RoBERTa and XLNet generally perform better for propaganda detection.\n2.An ensemble model with all types of PLMs showed stable and better performance than employing\na single PLM type.\n3.Each PLM has a different optimal learning rate, and ﬁnding the optimal one is essential to elicit high\nperformance.\n∗Contributed equally.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1740\n2 Related Work\nPropaganda generally intends to promote an agenda or point of view with speciﬁc information such\nas biased or misleading descriptions. Since detecting propaganda in text could be useful for further\napplications such as detecting fake news, research on propaganda detection has been attracting much\nattention. Barrón-Cedeño et al. (2019) proposed a model for assessing the “level” of propaganda in\nan article. Da San Martino et al. (2019) focused on more ﬁne-grained analysis, proposing a model for\ndetecting propaganda spans and classifying their techniques. Some recent studies (Yoosuf and Yang,\n2019; Vlad et al., 2019; Hua, 2019; Fadel et al., 2019; Tayyar Madabushi et al., 2019) utilized PLMs\nsuch as BERT (Devlin et al., 2019) to detect ﬁne-grained propaganda. Our work is related to the studies\nof (Fadel et al., 2019; Al-Omari et al., 2019), which employed ensemble models with PLMs. Different\nfrom these studies, we further investigate the number of PLMs.\n3 Pre-Trained Language Models (PLMs)\nIn this paper, six types of Transformer (Vaswani et al., 2017) basedPLMs were used. The reason behind\nthis is to unify our implementation and evaluation. We provide a brief description of each PLM:\nBERT (Devlin et al., 2019) is the epoch-making Transformer-based masked language model. We employ\na pre-trained model called bert-large-cased-whole-word-masking.\nGPT-2 (Radford et al., 2019) is a model that followed Open AI GPT (Radford and Sutskever, 2018).\nGPT-2 has achieved state-of-the-art results in a zero-shot setting.\nRoBERTa(Liu et al., 2019) is a carefully ﬁne-tuned BERT-based model, where the authors investigated\nhyperparameters and training data size. RoBERTa has achieved state-of-the-art results on major text\nbenchmarks. We employ a pre-trained model called roberta-large.\nXLM-RoBERTa (Conneau et al., 2019) is a cross-lingual version of RoBERTa. XLM-RoBERTa has\noutperformed the cross-lingual BERT. We employ a pre-trained model calledxlm-roberta-large.\nXLNet (Yang et al., 2019) employs a generalized autoregressive pre-training in Transformers. XLNet\nhas outperformed BERT in major tasks. We employ a pre-trained model called xlnet-large-cased.\nXLM (Lample and Conneau, 2019) is a cross-lingual language model, introducing an unsupervised\ncross-lingual learning. We employ a pre-trained model called xlm-mlm-en-2048.\n4 Propaganda Detection Models\nGiven a split sentence1, our SI model predicts propaganda spans. The TC model in turn predicts propa-\nganda techniques for given spans in a sentence.2 The technique labels include Loaded Language, which\nuses words with strong emotional implications, Name Calling or Labelling , which describe an object\nor something that the audience fears or sees as undesirable, Doubt, which questions the credibility of\nsomething, and so on (Da San Martino et al., 2019). Refer to (Da San Martino et al., 2020) for more\ndetails. The following subsections give details on the proposed SI and TC models.\n4.1 SI Model\nFigure 1a shows an overview of the proposed model for SI. Given a tokenized sentence, PLM ∈\n{BERT,GPT-2,RoBERTa,XLM-RoBERTa,XLNet,XLM}, and bi-directional long short term memory\n(BiLSTM) (Graves et al., 2013) encode the sentence, predicting propaganda spans with token-levelBIO\ntags, namely propaganda begins ( B), is inside ( I), or is outside ( O) the spans. In addition, we provide\ntwo joint auxiliary tasks to effectively train the model. These auxiliary tasks are described later.\nInput Representation: To obtain input representations, we provide a layer-wise attention to fuse the\n1For both SI and TC, given a news article, we split it into sentences. All the training and predictions were conducted on a\nsentence level. Each sentence was tokenized by spaCy ( https://spacy.io/), jointly obtaining the part-of-speech (PoS)\ntag and named entity (NE) tag to make common input features.\n2Note that we consider only sentences that contain propaganda in TC because the propaganda spans are given in the sub-task.\n1741\nNE embeddings\n[BoS] token1 ・・・ token Ntoken2\nSentence-level classification\nCRF\nBIO tag  /  technique class\n[EoS]\nStructural \nfeature\nPLM (BERT, GPT-2, RoBERTa,\nXLNet, XLM, or XLM-RoBERTa)\nPLM layer attentionPLM layer attention\ntoken1[BoS]\nPoS embeddings\nFFN\ntoken2 token N [EoS] token1[BoS] token2 token N [EoS]\nFFN\nconcat\nBiLSTMs\n(a) SI model\nNE embeddings\n[BoS] token1 token3token2\nsentence\nStructural \nfeature\nPLM (BERT, GPT-2, RoBERTa,\nXLNet, XLM, or XLM-RoBERTa)\nPLM layer attention\nPoS embeddings\nFFN &\nconcat\ngiven span\ntoken4 ・・・\nstart attention maxpool end\nFFN\nLoaded_Language\n・・・\nFlag-Waving Doubt\nFFN FFN\nconcat (b) TC model\nFigure 1: Overview of our proposed models\noutputs of PLM layers (Kondratyuk and Straka, 2019; Peters et al., 2018):\nh(si)\nPLM,i = c(si) ∑\nj\nPLMij ·softmax (s(si))j,\nwhere s(si) and c(si) are trainable parameters, and PLMij is an embedding of the i-th word token in the\nj-th layer of a PLM.3 We also concatenated part-of-speech (PoS) embeddings (h(si)\nPoS,i) and named entity\n(NE) embeddings (h(si)\nNE,i) for each token i.4 Therefore, the i-th word token is represented as:\nh(si)\ni = h(si)\nPLM,i ⊕h(si)\nPoS,i ⊕h(si)\nNE,i,\nwhere ⊕is a concatenate operation.\nBiLSTM-CRF: We employed BiLSTM-CRF because we preliminarily found that stacking BiLSTM-\nCRF (Huang et al., 2015) on a PLM leads to better performance in SI. The input token representationshi\nare fed to the multi-layered BiLSTM to obtain a further contextualized token representation:\ne(si)\ni = BiLSTM(h(si)\ni ).\nWe apply a feed forward network (FFN) with one hidden layer and a fully-connected layer to the recurrent\nstates before classiﬁcation:\nˆy(si_bio)\ni = W(si_bio)FFN(si_bio)\n(\ne(si)\ni\n)\n+ b(si_bio),\nwhere W(si_bio) and b(si_bio) are parameters. ˆy(si_bio)\ni is the output of B, I, and O labels.5 Finally, we\nemploy a conditional random ﬁeld (CRF) for training and prediction.\n[Auxiliary1] Token-Level Technique Classiﬁcation: This auxiliary task predicts token-level propa-\nganda technique classes to add more information for span identiﬁcation. 6 In fact, a previous study\n3For subword-level PLM, subwords are averaged, and the averaged output per word token is used as the PLMij.\n4Special embeddings are used for the PoS and NE embeddings of BoS and EoS tokens.\n5Note that we ignored special tokens such as the beginning of sentence (BoS) token (e.g., [CLS]) and the end of sentence\n(EoS) token (e.g., [SEP]) in the classiﬁer.\n6We extracted TC labels aligning with spans in SI in the training set. We did not use any TC labels in the development and\ntest set, and there was no problem when we contacted the organizers.\n1742\n(Schulz et al., 2018) suggests that joint training with multiple token-level tasks helps improve perfor-\nmance in a low-resource setting. Our expectation is that spans for low-frequency propaganda techniques\ncan be detected with this auxiliary task. We achieve this by simply providing another output layer:\nˆy(si_tech)\ni = W(si_tech)FFN(si_tech)\n(\ne(si)\ni\n)\n+ b(si_tech),\nwhere W(si_tech) and b(si_tech) are parameters, and ˆy(si_tech)\ni is the output of 14 propaganda technique\nclasses and a non-propaganda class.7 We also employ a CRF for training and prediction.\n[Auxiliary2] Sentence-Level Classiﬁcation: Given that sentences that contain propaganda are com-\nparatively low in number when compared with non-propaganda sentences, we introduce a sentence-level\nauxiliary task. This auxiliary task predicts sentence-level classes with lower granularity on the basis of\nwhether the sentence contains propaganda or not, and higher granularity token-level tasks are backprop-\nagated only when a sentence contains propaganda. Therefore, we do not learn much information from\nnon-propaganda sentences for detecting spans. A similar idea was derived from the work of Da San Mar-\ntino et al. (2019), in which they proposed incorporating a higher granularity task on the basis of lower\ngranularity information (sentence-level task) with a gating mechanism.\nWe provide another PLM layer attention and multi-layered BiLSTM to distinguish information be-\ntween sentence-level and token-level tasks:\nh(si_sent)\ni = h(si_sent)\nPLM,i ⊕h(si)\nPoS,i ⊕h(si)\nNE,i,\ne(si_sent)\ni = BiLSTM(si_sent)(h(si_sent)\ni ).\nFinally, we use the output from the BoS token 8 9 , predicting the probability that a sentence contains\npropaganda:\nˆy(si_sent) = σ\n(\nv(si_sent)⊤FFN(si_sent)\n(\ne(si_sent)\nBoS ⊕φ\n)\n+ b(si_sent)\n)\n,\nwhere v(si_sent) and b(si_sent) are parameters, e(si_sent)\nBoS is the hidden state of the BoS token (e.g., normally\ne(si_sent)\nBoS = e(si_sent)\n1 ), and σ is a sigmoid function. We also concatenated a structural feature vector\nφ including the sentence length and positional information in its article. The positional information\nincludes binary signals if the sentence is located in the upper, middle, or lower.\nObjective: Given that we have distinct sentence- and token-level tasks, our objective is described as:\nL(si) = L(si_sent) +\n{ 0 if the sentence contains no propaganda spans\nL(si_bio) + λL(si_tech) else\nwhere L(si_sent) is a cross-entropy loss for the sentence-level task, and L(si_bio) and L(si_tech) are CRF loss\nwith a negative log likelihood for span identiﬁcation and technique classiﬁcation, respectively. λ is a\nhyperparameter for controlling the auxiliary tasks. The objective means that if a sentence contains no\npropaganda spans, token-level tasks are ignored. Also, we assign a weight to the sentence-level loss\naccording to the inverse proportion of positive samples to deal with class imbalance.\nAfter training, we modify thresholds of the sentence-level task on the basis of validation scores. At\ninference, if the output probability in the task is below the threshold, it is regarded as a non-propaganda\nsentence. Propaganda spans are predicted only when the probability is above the threshold.\n7Given this task is an auxiliary and multi-label samples (i.e., multi-labels in a same span) are much less common, we\nconverted them into a single class and discarded the rest.\n8To process any type of PLM in the same manner, we inserted original special tokens in the input tokens when no BoS or\nEoS tokens were available in the PLM.\n9We accidentally dealt with XLNet encoding as “<cls> tokens <sep>,” while the correct form is “tokens <sep> <cls>.” The\nexperimental results might had been affected, however, it still showed high performance.\n1743\nPLM = BERT\nhyperparameter\nset = 1\nPLM = BERT\nhyperparameter\nset = 2\nPLM = XLM\nhyperparameter\nset = 1\nPLM = XLM\nhyperparameter\nset = 2\nmodel train \nprocedure\nmeta-estimator\ntrain procedure\nValidation\nfold\ntrain folds\ntrained\nmodels\npredict the validation fold\nconcatenate\nmeta-estimator\nFigure 2: Example of our ensemble method. We employ stacking and cross-validation. Note that this\nﬁgure shows only two PLMs and two hyperparameter sets.\n4.2 TC Model\nFigure 1b shows an overview of the proposed TC model. In TC, given a propaganda span, we predict\npropaganda technique(s) for each span.10\nPropaganda Span Representation: To produce a propaganda span representation, we provide two\ndistinct FFNs, feeding input representation h(tc)\ni , that were obtained in the same manner as the SI model.\nOne of the two FFNs is for the BoS token and produces sentence representations, and the other is for\ntokens in a propaganda span:\ne(tc)\ni =\n\n\n\nFFN(tc_bos)\n(\nh(tc)\ni ⊕φ\n)\nif iis a BoS token\nFFN(tc)\n(\nh(tc)\ni\n)\nelse\nwhere h(tc)\nBoS is a sentence representation obtained from the BoS token. The propaganda span representa-\ntion is obtained by concatenating the representation of the BoS token (e(tc)\nBoS), tokens located at span start\n(e(tc)\nstart) and end (e(tc)\nend), and representations aggregated by attention ( e(tc)\natt ) and maxpooling (e(tc)\nmaxp) in the\nspan as follows.\ne(tc_span) = e(tc)\nBoS ⊕e(tc)\nstart ⊕e(tc)\nend ⊕e(tc)\natt ⊕e(tc)\nmaxp,\nClassiﬁer and Objective: We provide an additional label-wise FFN and linear layer to extract label-\nspeciﬁc information for each propaganda technique before prediction:\nˆy(tc)\nℓ = σ\n(\nv(tc)\nℓ\n⊤FFN(tc)\nℓ\n(\ne(tc_span)\n)\n+ b(tc)\nℓ\n)\n,\nwhere v(tc)\nℓ and b(tc)\nℓ are trainable parameters, and ℓ denotes a technique label such as ﬂag-waving.\nSince TC is a multi-label problem, we provide a binary cross-entropy loss for each class. Similar to\nDa San Martino et al. (2019), we assign weight to a loss according to the proportion of positive samples\nto deal with class imbalance. After training, we multiply the output probability for each label on the\nbasis of the validation scores automatically. At inference, we sort predicted labels for each sentence in\ndescending order and assign labels according to the order in a multi-label span.11\n10There are 14 possible labels: appeal to authority, appeal to fear-prejudice, bandwagon, reductio ad hitlerum, black-and-\nwhite fallacy, causal oversimpliﬁcation, doubt, exaggeration, minimization, ﬂag-waving, loaded language, name calling, la-\nbeling, repetition, slogans, thought-terminating clichés, or whataboutism, straw men, red herring. Refer to the task description\npaper (Da San Martino et al., 2020) for more details.\n11This is because the number of labels in a span was given in TC.\n1744\n4.3 Ensemble with Stacking\nWe propose an ensemble strategy based on the concept of stacked generalization (Wolpert, 1992).\nStacked generalization feeds prediction results (i.e., output probabilities from classiﬁers) into a meta-\nestimator and trains the estimator with gold labels. In this study, the keys are hyperparameter search and\ncross-validation. The simpliﬁed training procedure of the ensemble model can be found in Figure 2.\nIn the procedure for model training, for either the SI and TC, assume we havek-fold cross-validation,\nNH hyperparameter sets, and NP PLMs. A hyperparameter set includes the dropout ratio and learning\nrate. For each hyperparameter set, we ﬁne-tune the SI or TC models with training folds without using the\nvalidation fold. Therefore, NP ×kmodels for each hyperparameter set are generated. To select better\nmodels, we use only the top NHT hyperparameter sets on the basis of the validation score, resulting in\nNHT ×NP ×kmodels. For example, as in Figure 2, NHT = 2, NP = 2(i.e., BERT and XLM), and\nk= 3.\nIn the meta-estimator training procedure, we train a linear model (i.e., classiﬁer or regressor) on the\nbasis of the outputs of the ﬁne-tuned SI or TC models. First, we predict the validation fold in the\ntraining data through the ﬁne-tuned model. By concatenating the predicted validation folds for each\nhyperparameter set, the representativeout-of-folds of each hyperparameter set are organized. This means\nthat we have meta-features D ∈Rd×NHT to train the meta-estimator, where dis the size of the training\ndata.\nIn the test procedure, we predict test labels with the ﬁne-tuned models in the top hyperparameter sets\nthat were selected in the training step. The predicted labels are then fed into the trained meta-estimator,\nobtaining ﬁnal predictions.\nMeta-Estimators: We employ the ridge classiﬁer (Hoerl and Kennard, 1970) implemented in scikit-\nlearn (Pedregosa et al., 2011) for the meta-estimator. We estimate that even with a naive linear model, the\nﬁnal outputs are generally more robust and accurate. We provided the meta-estimators for the sentence-\nlevel task in SI, BIO classiﬁcation in SI, and all TC labels. The meta-estimators of the sentence-level\ntask and TC labels receive the output probabilities of the corresponding labels as input representations.\n5 Experiments\nImplementation Detail: All SI and TC models were implemented with PyTorch (Paszke et al., 2019)\nand Hugging Face’s transformer library (Wolf et al., 2019). Layer attentions were applied for the last\neight layers in all PLMs, employing dropout. CRF classiﬁers were implemented using pytorch-crf 12. At\ntraining, we split the network into two parameter groups: one for the parameters of PLM and one for all\nother non PLM parameters, applying discriminative ﬁne-tuning (Kondratyuk and Straka, 2019). We froze\nPLM parameters for the ﬁrst few epochs to improve training stability. Adam optimizer (Kingma and Ba,\n2015) was used as an optimizer. We applied a 10% and 5% linear learning rate warm-up for all epochs\nfor SI and TC, respectively.\nSubmitted System: We employed only the ofﬁcial training dataset to train our model. For the SI\nsubmission, we generated 24 hyperparameter sets for eachPLM, and the top 3 sets chosen on the basis of\nthe validation score for each PLM were used for the stacked generalization. For the TC submission, the\ntop 11 sets amongst 60 hyperparameter sets were used.\nFixed hyperparameter values are shown in Table 1. The tunable hyperparameter set included learning\nrates and the dropout ratio for the FFNs and BiLSTMs. Optuna (Akiba et al., 2019) was used to generate\nhyperparameter sets.\nThe hyperparameter search results for the optimal learning rates are shown in Table 2. Generally,\nlearning rates of non- PLM parameters are larger than those of PLM parameters. Interestingly, most of\nthe learning rates for TC were lower than for SI. This insight suggests that complicated models such as\nPLMs with BiLSTMs require a larger learning rate to produce a better model.\nMetrics: Overlap-based F1 scores for SI and micro-averaged F1 scores for TC were employed in the\nshared task. Refer to (Da San Martino et al., 2020) for more details.\n12https://pypi.org/project/pytorch-crf/\n1745\nhyperparameter SI TC\ncross-validation folds (k) 5 6\nPoS embedding dim 50 50\nNE embedding dim 50 50\nBiLSTM dim, layers 600, 2 -\nPLM layer dropout 0.1 0.1\nFFN(si_bio) dim 200 -\nFFN(si_tech) dim 200 -\nFFN(si_sent) dim 500 -\nFFN(tc_bos) dim - 200\nFFN(tc) dim - 500\nFFN(tc)\nℓ dim - 500\nFFN activation ReLU ReLU\nλ 0.5 -\nAdam β1,β2 0.9, 0.999 0.9, 0.999\ngradient clipping 5.0 5.0\nepochs 18 30\nPLM frozen ﬁrst epochs 2 1\nbatch size 6 5\nTable 1: Hyperparameter values\nmodel with PLM PLM parameters\n(SI / TC)\nnon PLM parameters\n(SI / TC)\nBERT 3.7e-6 / 1.0e-6 3.5e-4 / 1.6e-4\nGPT-2 1.8e-5 / 1.2e-5 5.4e-4 / 5.0e-5\nRoBERTa 2.9e-6 / 1.9e-6 2.8e-4 / 1.0e-4\nXLM-RoBERTa 4.4e-6 / 1.7e-6 7.3e-4 / 7.0e-5\nXLNet 4.1e-6 / 2.7e-6 7.3e-4 / 1.5e-4\nXLM 3.0e-6 / 6.9e-6 8.9e-5 / 1.5e-4\nTable 2: Suggested learning rates\nteam SI (ranking) TC (ranking)\nHitachi (ours) 51.551 (1) 61.732 (3)\nApplicaAI 49.153 (2) 62.067 (1)\naschern 49.100 (3) 62.011 (2)\nLTIatCMU 47.663 (4) -\nUPB 46.060 (5) 54.302 (19)\nSolomon 40.683 (15) 58.939 (4)\nnewsSweeper 42.209 (13) 58.436 (5)\nTable 3: Ofﬁcial F scores for test set. We show\nonly top 5 teams for each subtask. Bold and\nunderline scores show ﬁrst and second ranked re-\nsults, respectively.\nmodel SI TC\nensemble all 49.386 64.628\nensemble only w/ BERT 46.734 59.454\nensemble only w/ GPT-2 44.161 57.761\nensemble only w/ RoBERTa 49.326 62.841\nensemble only w/ XLM-RoBERTa 47.371 61.524\nensemble only w/ XLNet 47.876 63.406\nensemble only w/ XLM 44.688 60.960\nensemble w/o BERT 48.583 64.440\nensemble w/o GPT-2 48.920 63.688\nensemble w/o RoBERTa 48.065 64.346\nensemble w/o XLM-RoBERTa 49.028 63.688\nensemble w/o XLNet 48.838 63.782\nensemble w/o XLM 49.095 64.911\nTable 4: Ablation study on development set\n5.1 Results\nTable 3 shows the ofﬁcial test results of the top-performing teams for SI and TC. Our proposed SI model\noutperformed all the other teams with an improvement of more than 2 points. Our team was ranked third\nfor TC; however, the performance of the top three teams seemed to be almost the same.\nOn the Role of PLMs: To show the role ofPLMs, we show the ablation results for eachPLM in Table 4.\nIn the table, we show the performance for the development data. The table shows that RoBERTa and\nXLNet were generally the best performing models. Given that RoBERTa is a carefully tuned model\nbased on BERT, this result is reasonable. Interestingly, the table also shows that using all PLMs in an\nensemble was better in most cases. For example, while the GPT-2-based model itself is not a better\nmodel for both SI and TC, excluding the GPT-2-based model results in worse performance. This result\nsuggests that stacked generalization was effectively applied.\nPLM Layer Weight: Comprehensive overviews of ﬁne-tuned PLM states are shown in Figure 3, visu-\nalizing weights of PLM layers (Clark et al., 2019). The ﬁgure illustrates that the last several layers were\ngenerally weighted. GPT-2 and XLNet interestingly show different distributions, ranging widely.\nOn the Importance of Learning Rate Tuning: Through hyperparameter tuning, we found that tuning\na PLM learning rate is essential to elicit better results. We show Figure 4 and Figure 5, visualizing the\nlearning rate space for the two parameter groups, that is, PLM parameters and non-PLM parameters. We\nfound that the SI models required tuning for either group, while the TC models required the tuning of\nPLM parameters rather than non- PLM parameters. We attribute this to the complexity of SI models. SI\nmodels employ BiLSTM-CRF in PLMs, and BiLSTMs are complicated when compared with FFNs in\nTC. Therefore, SI training requires a higher learning rate for either group. TC models use only FFNs in\nPLMs and therefore require a lower learning rate for non-PLM parameters.\n1746\n0 1 2 3 4 5 6 7\nlayer index (7 is the last layer)\n0.123\n0.124\n0.125\n0.126weight\nBERT\nXLM-RoBERTa\nRoBERTa\nXLNet\nXLM\nGPT-2\n(a) Token-level tasks in SI\n0 1 2 3 4 5 6 7\nlayer index (7 is the last layer)\n0.124\n0.126\n0.128weight\nBERT\nXLM-RoBERTa\nRoBERTa\nXLNet\nXLM\nGPT-2 (b) TC\nFigure 3: Attention weight visualization for last eight layers in PLMs\n10μ 2 5 100μ2 5 \n7 8 9 1μ\n2 \n3 \n4 \n5 \n6 7 8 9 10μ\n−0.475\n−0.45\n−0.425\nObjective Value\n(a) BERT\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n−0.45\n−0.4\n−0.35\nObjective Value (b) GPT-2\n10μ 2 5 100μ 2 5 6 7 8 9 1μ\n2 \n3 \n4 \n5 \n6 7 8 9 10μ\n−0.5\n−0.45\n−0.4\nObjective Value (c) RoBERTa\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n−0.45\n−0.4\nObjective Value\n(d) XLM-RoBERTa\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n−0.5\n−0.45\n−0.4\nObjective Value (e) XLNet\n2 5 100μ 2 5 \n7 8 9 1μ\n2 \n3 \n4 \n5 \n6 7 8 9 10μ\n−0.48\n−0.47\n−0.46\n−0.45\nObjective Value (f) XLM\nFigure 4: Negative SI validation scores in learning rate space, where X-axis shows learning rate for\nnon-PLM parameters, and Y-axis showsPLM parameters. Each point indicates searched hyperparameter.\nNote that brighter colors indicate better performance.\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.6\n−0.5\nObjective Value\n(a) BERT\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.64\n−0.6\n−0.56\n−0.52\n−0.48\nObjective Value (b) GPT-2\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.6\n−0.5\nObjective Value (c) RoBERTa\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.6\n−0.5\nObjective Value\n(d) XLM-RoBERTa\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.6\n−0.5\nObjective Value (e) XLNet\n10μ 2 5 100μ2 5 \n1μ\n2 \n5 \n10μ\n2 \n5 \n100μ\n2 \n−0.64\n−0.6\n−0.56\nObjective Value (f) XLM\nFigure 5: Negative TC validation scores in learning rate space\n6 Conclusion\nWe detected propaganda by leveraging heterogeneous pre-trained language models. The results sug-\ngested that employing heterogeneous pre-trained language models could result in better performance.\nFuture work includes examining more effective methods for utilizing heterogeneous pre-trained language\nmodels.\n1747\nAcknowledgments\nComputational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of\nAdvanced Industrial Science and Technology (AIST) was used. We thank the anonymous reviewers who\ngave us insightful comments. We also thank Dr. Masaaki Shimizu for the convenience afforded by these\ncomputational resources.\nReferences\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A next-\ngeneration hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, KDD ’19, pages 2623–2631, New York, NY , USA. ACM.\nHani Al-Omari, Malak Abdullah, Ola AlTiti, and Samira Shaikh. 2019. JUSTDeep at NLP4IF 2019 task 1:\nPropaganda detection using ensemble deep learning models. InProceedings of the Second Workshop on Natural\nLanguage Processing for Internet Freedom: Censorship, Disinformation, and Propaganda , pages 113–118,\nHong Kong, China, November. Association for Computational Linguistics.\nAlberto Barrón-Cedeño, Israa Jaradat, Giovanni [Da San Martino], and Preslav Nakov. 2019. Proppy: Organizing\nthe news based on their propagandistic content. Information Processing Management , 56(5):1849 – 1864.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at?\nan analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 276–286, Florence, Italy, August. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale. arXiv preprint arXiv:1911.02116.\nGiovanni Da San Martino, Seunghak Yu, Alberto Barrón-Cedeño, Rostislav Petrov, and Preslav Nakov. 2019.\nFine-grained analysis of propaganda in news articles. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, EMNLP-IJCNLP 2019, Hong Kong, China, November.\nGiovanni Da San Martino, Alberto Barrón-Cedeño, Henning Wachsmuth, Rostislav Petrov, and Preslav Nakov.\n2020. SemEval-2020 task 11: Detection of propaganda techniques in news articles. In Proceedings of the 14th\nInternational Workshop on Semantic Evaluation, SemEval 2020, Barcelona, Spain, September.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nAli Fadel, Ibraheem Tuffaha, and Mahmoud Al-Ayyoub. 2019. Pretrained ensemble learning for ﬁne-grained\npropaganda detection. In Proceedings of the Second Workshop on Natural Language Processing for Internet\nFreedom: Censorship, Disinformation, and Propaganda, pages 139–142, Hong Kong, China, November. Asso-\nciation for Computational Linguistics.\nAlex. Graves, Abdel rahman. Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent\nneural networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing , pages\n6645–6649.\nArthur E. Hoerl and Robert W. Kennard. 1970. Ridge regression: Biased estimation for nonorthogonal problems.\nTechnometrics, 12(1):55–67.\nYiqing Hua. 2019. Understanding BERT performance in propaganda analysis. In Proceedings of the Second\nWorkshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propa-\nganda, pages 135–138, Hong Kong, China, November. Association for Computational Linguistics.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. CoRR,\nabs/1508.01991.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the\n3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015.\n1748\nDan Kondratyuk and Milan Straka. 2019. 75 languages, 1 model: Parsing universal dependencies universally.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2779–2795, Hong\nKong, China, November. Association for Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. Advances in Neural\nInformation Processing Systems (NeurIPS).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,\nMartin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\ntala. 2019. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 32, pages 8024–8035. Curran Associates, Inc.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-\nlearn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June. Association for Computational Linguistics.\nAlec Radford and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. In arXiv.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nClaudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse, and Iryna Gurevych. 2018. Multi-task learn-\ning for argumentation mining in low-resource settings. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2\n(Short Papers), pages 35–41, New Orleans, Louisiana, June. Association for Computational Linguistics.\nHarish Tayyar Madabushi, Elena Kochkina, and Michael Castelle. 2019. Cost-sensitive BERT for generalisable\nsentence classiﬁcation on imbalanced data. In Proceedings of the Second Workshop on Natural Language\nProcessing for Internet Freedom: Censorship, Disinformation, and Propaganda , pages 125–134, Hong Kong,\nChina, November. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 ,\npages 5998–6008. Curran Associates, Inc.\nGeorge-Alexandru Vlad, Mircea-Adrian Tanase, Cristian Onose, and Dumitru-Clementin Cercel. 2019. Sentence-\nlevel propaganda detection in news articles with transfer learning and BERT-BiLSTM-capsule model. In Pro-\nceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disin-\nformation, and Propaganda , pages 148–154, Hong Kong, China, November. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771.\nDavid H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet:\nGeneralized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\npages 5753–5763. Curran Associates, Inc.\nShehel Yoosuf and Yin Yang. 2019. Fine-grained propaganda detection with ﬁne-tuned BERT. In Proceedings of\nthe Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and\nPropaganda, pages 87–91, Hong Kong, China, November. Association for Computational Linguistics."
}