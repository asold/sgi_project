{
    "title": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction",
    "url": "https://openalex.org/W3188511781",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2101529693",
            "name": "Tang, Hao",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2744565219",
            "name": "Sebe, Nicu",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2743231409",
            "name": "Ricci Elisa",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2370281093",
            "name": "Yang Guanglei",
            "affiliations": [
                "University of Trento"
            ]
        },
        {
            "id": "https://openalex.org/A2007056435",
            "name": "Ding Ming-li",
            "affiliations": [
                "University of Trento"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W2963727650",
        "https://openalex.org/W2125416623",
        "https://openalex.org/W2962807621",
        "https://openalex.org/W6781070145",
        "https://openalex.org/W3034428934",
        "https://openalex.org/W6776176673",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W6678569853",
        "https://openalex.org/W6786046587",
        "https://openalex.org/W2962960377",
        "https://openalex.org/W2963591054",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3009662750",
        "https://openalex.org/W3034723120",
        "https://openalex.org/W6640376812",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3112288616",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2798441115",
        "https://openalex.org/W2605938684",
        "https://openalex.org/W2981932175",
        "https://openalex.org/W2964014680",
        "https://openalex.org/W6796604954",
        "https://openalex.org/W2990946490",
        "https://openalex.org/W6767088534",
        "https://openalex.org/W2962809185",
        "https://openalex.org/W2963488291",
        "https://openalex.org/W2115579991",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W3047787625",
        "https://openalex.org/W2985775862",
        "https://openalex.org/W6771531989",
        "https://openalex.org/W3034604951",
        "https://openalex.org/W6780698268",
        "https://openalex.org/W2953096069",
        "https://openalex.org/W3034346071",
        "https://openalex.org/W6729541841",
        "https://openalex.org/W1915250530",
        "https://openalex.org/W3095980151",
        "https://openalex.org/W6775170262",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3108735274",
        "https://openalex.org/W6760951061",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W3205944169",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6797427164",
        "https://openalex.org/W3096087403",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W1905829557",
        "https://openalex.org/W6685261749",
        "https://openalex.org/W6681786536",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W2962891704",
        "https://openalex.org/W6715287400",
        "https://openalex.org/W6781325470",
        "https://openalex.org/W2612236014",
        "https://openalex.org/W3093407252",
        "https://openalex.org/W6623517193",
        "https://openalex.org/W6781130681",
        "https://openalex.org/W6794370974",
        "https://openalex.org/W3035434014",
        "https://openalex.org/W6605121731",
        "https://openalex.org/W2598666589",
        "https://openalex.org/W2964968086",
        "https://openalex.org/W2798927139",
        "https://openalex.org/W3040304705",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2995884594",
        "https://openalex.org/W3169909366",
        "https://openalex.org/W125693051",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2978074392",
        "https://openalex.org/W2124907686",
        "https://openalex.org/W2550402137",
        "https://openalex.org/W3107389224",
        "https://openalex.org/W3104663494",
        "https://openalex.org/W3106811106",
        "https://openalex.org/W854541894",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3105431515",
        "https://openalex.org/W2951234442",
        "https://openalex.org/W2171740948",
        "https://openalex.org/W2950493473",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3154062460",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2971000934",
        "https://openalex.org/W3107156787",
        "https://openalex.org/W4287863801",
        "https://openalex.org/W3176680381",
        "https://openalex.org/W2969450841",
        "https://openalex.org/W2146814781",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W3089453837",
        "https://openalex.org/W3108179104",
        "https://openalex.org/W1928906481",
        "https://openalex.org/W3097065222"
    ],
    "abstract": "&amp;lt;p&amp;gt;While convolutional neural networks have shown a&amp;lt;br&amp;gt;\\ntremendous impact on various computer vision tasks, they&amp;lt;br&amp;gt;\\ngenerally demonstrate limitations in explicitly modeling&amp;lt;br&amp;gt;\\nlong-range dependencies due to the intrinsic locality of&amp;lt;br&amp;gt;\\nthe convolution operation. Initially designed for natural&amp;lt;br&amp;gt;\\nlanguage processing tasks, Transformers have emerged as&amp;lt;br&amp;gt;\\nalternative architectures with innate global self-attention&amp;lt;br&amp;gt;\\nmechanisms to capture long-range dependencies. In this&amp;lt;br&amp;gt;\\npaper, we propose TransDepth, an architecture that benefits&amp;lt;br&amp;gt;\\nfrom both convolutional neural networks and transformers.&amp;lt;br&amp;gt;\\nTo avoid the network losing its ability to capture locallevel&amp;lt;br&amp;gt;\\ndetails due to the adoption of transformers, we propose&amp;lt;br&amp;gt;\\na novel decoder that employs attention mechanisms&amp;lt;br&amp;gt;\\nbased on gates. Notably, this is the first paper that applies&amp;lt;br&amp;gt;\\ntransformers to pixel-wise prediction problems involving&amp;lt;br&amp;gt;\\ncontinuous labels (i.e., monocular depth prediction and&amp;lt;br&amp;gt;\\nsurface normal estimation). Extensive experiments demonstrate&amp;lt;br&amp;gt;\\nthat the proposed TransDepth achieves state-of-theart&amp;lt;br&amp;gt;\\nperformance on three challenging datasets. Our code is&amp;lt;br&amp;gt;\\navailable at: https://github.com/ygjwd12345/&amp;lt;br&amp;gt;\\nTransDepth.&amp;lt;/p&amp;gt;",
    "full_text": null
}