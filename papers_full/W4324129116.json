{
    "title": "Medical image captioning via generative pretrained transformers",
    "url": "https://openalex.org/W4324129116",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3120770908",
            "name": "Alexander Selivanov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2471553949",
            "name": "Oleg Y. Rogov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4289913241",
            "name": "Daniil Chesakov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology",
                "Zhukovsky Air Force Engineering Academy"
            ]
        },
        {
            "id": "https://openalex.org/A294125378",
            "name": "Artem Shelmanov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology",
                "Zhukovsky Air Force Engineering Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2081722432",
            "name": "Irina Fedulova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1202935724",
            "name": "Dmitry V. Dylov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3120770908",
            "name": "Alexander Selivanov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2471553949",
            "name": "Oleg Y. Rogov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4289913241",
            "name": "Daniil Chesakov",
            "affiliations": [
                "Zhukovsky Air Force Engineering Academy"
            ]
        },
        {
            "id": "https://openalex.org/A294125378",
            "name": "Artem Shelmanov",
            "affiliations": [
                "Zhukovsky Air Force Engineering Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2081722432",
            "name": "Irina Fedulova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1202935724",
            "name": "Dmitry V. Dylov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2904183610",
        "https://openalex.org/W2152772232",
        "https://openalex.org/W2795964626",
        "https://openalex.org/W3156342878",
        "https://openalex.org/W3024545783",
        "https://openalex.org/W4312605942",
        "https://openalex.org/W3134395396",
        "https://openalex.org/W3156038334",
        "https://openalex.org/W3036341692",
        "https://openalex.org/W2963834202",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W2334763311",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2508429489",
        "https://openalex.org/W2963409068",
        "https://openalex.org/W2302086703",
        "https://openalex.org/W2963967185",
        "https://openalex.org/W2770165365",
        "https://openalex.org/W2611650229",
        "https://openalex.org/W3004831296",
        "https://openalex.org/W2957184985",
        "https://openalex.org/W2979861699",
        "https://openalex.org/W2997704374",
        "https://openalex.org/W3195516695",
        "https://openalex.org/W6760721734",
        "https://openalex.org/W2778310824",
        "https://openalex.org/W3099632249",
        "https://openalex.org/W3093896805",
        "https://openalex.org/W2993044507",
        "https://openalex.org/W3151410070",
        "https://openalex.org/W3104609094",
        "https://openalex.org/W2979956313",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3208082248",
        "https://openalex.org/W2963620441",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W1931639407",
        "https://openalex.org/W2995225687",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4246579775",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W3158665049",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W3156031277",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W3189436189",
        "https://openalex.org/W2963373823",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3098325931",
        "https://openalex.org/W2481240925",
        "https://openalex.org/W2990138404"
    ],
    "abstract": "Abstract The proposed model for automatic clinical image caption generation combines the analysis of radiological scans with structured patient information from the textual records. It uses two language models, the Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive radiology records. The generated textual summary contains essential information about pathologies found, their location, along with the 2D heatmaps that localize each pathology on the scans. The model has been tested on two medical datasets, the Open-I, MIMIC-CXR, and the general-purpose MS-COCO, and the results measured with natural language assessment metrics demonstrated its efficient applicability to chest X-ray image captioning.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports\nMedical image captioning \nvia generative pretrained \ntransformers\nAlexander Selivanov 1,2,4, Oleg Y. Rogov 1,4, Daniil Chesakov 1,3, Artem Shelmanov 1,3, \nIrina Fedulova 2 & Dmitry V. Dylov 1*\nThe proposed model for automatic clinical image caption generation combines the analysis of \nradiological scans with structured patient information from the textual records. It uses two language \nmodels, the Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive radiology \nrecords. The generated textual summary contains essential information about pathologies found, \ntheir location, along with the 2D heatmaps that localize each pathology on the scans. The model has \nbeen tested on two medical datasets, the Open-I, MIMIC-CXR, and the general-purpose MS-COCO, \nand the results measured with natural language assessment metrics demonstrated its efficient \napplicability to chest X-ray image captioning.\nMedical imaging is indispensable in the current diagnostic workflows. Out of the plethora of existing imaging \nmodalities, X-ray remains one of the most widely-used visualization methods in many hospitals around the \nworld, because it is inexpensive and easily  accessible1. Analyzing and interpreting X-ray images is especially \ncrucial for diagnosing and monitoring a wide range of lung diseases, including  pneumonia2,  pneumothorax3, \nand COVID-19  complications4.\nToday, the generation of a free-text description based on clinical radiography results has become a conveni-\nent tool in clinical  practice5. Having to study approximately 100 X-rays  daily5, radiologists are overloaded by \nthe necessity to report their observations in writing, a tedious and time-consuming task that requires a deep \ndomain-specific knowledge. The typical manual annotation overload can lead to several problems, such as missed \nfindings, inconsistent quantification, and delay of a patient’s stay in the hospital, which brings increased costs \nfor the treatment. Among all, the qualification of radiologists as far as the correct diagnosis establishing should \nbe stated as major problems.\nIn the COVID-19 era, there is a higher need for robust image  captioning5–7 framework. Thus, many health-\ncare systems outsource the medical image analysis task. Automatic generation of chest X-ray medical reports \nusing deep learning can assist and accelerate the diagnosis establishing process followed by clinicians. Providing \nautomated support for this task has the potential to ease clinical workflows and improve both care quality and \nstandardization. For that, we propose to adapt powerful models from non-medical domain.\nMedical background. Radiology is the medical discipline that uses medical imaging to diagnose and treat \ndiseases. Today, radiology actively implements new artificial intelligence  approaches8–10. There are three types of \nradiologists—diagnostic radiologists, interventional radiologists and radiation oncologists. They all use medical \nimaging procedures such as X-rays, computed tomography (CT), magnetic resonance imaging (MRI), nuclear \nmedicine, positron emission tomography (PET) and ultrasound. Diagnostic radiologists interpret and report \non images resulted from imaging procedures, diagnose the cause of patient’s symptoms, recommend treatment \nand offer additional clinical tests. They specialize on different parts of human body—breast imaging (mam-\nmograms), cardiovascular radiology (heart and circulatory system), chest radiology (heart and lungs), gastro-\nintestinal radiology (stomach, intestines and abdomen), etc. Interventional radiologists use radiology images to \nperform clinical procedures with minimally invasive techniques. They are often involved in treating cancer, heart \ndiseases, stroke, blockages in the arteries and veins, fibroids in the uterus, back pains, liver and kidney problems.\nOPEN\n1Skolkovo Institute of Science and Technology, Bolshoy blvd., 30/1, Moscow 121205, Russia. 2Philips \n(Russia), Skolkovo Technopark 42, Building 1, Bolshoi Boulevard, Moscow 121205, Russia. 3AIRI, Kutuzovsky \nAve, 32 bld. 1, Moscow 121170, Russia.  4 These authors contributed equally: Alexander Selivanov and Oleg \nY . Rogov. *email: d.dylov@skoltech.ru\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nTechnical background. Because image captioning is a multimodal problem, it draws a significant attention \nof both computer vision and natural language processing communities. The latest surveys in the medical image \ncaptioning  task5,11 offer a detailed description of domain knowledge from radiology and deep learning. The first \narchitectures to address this problem were CNN-RNN models  from12,13. However, the latter shows satisfactory \nresults only on the single-pathology tasks.\nWith the new concept of attention  approach14, more papers have begun to use visual  attention15–17, being the \nfirst to use attention on medical images. The authors  of15 presented a model that can fix its attention on salient \nobjects while generating the corresponding words in the output sequence. Shortly after the visual-attention \nconcept was exposed, text-attention was introduced by authors of TieNet—a framework that generates natural \n reports18–20 for the Chest-Xray  dataset21. They used both semantic and visual attention, that allowed them to \nget high natural language generation (NLG) metrics on medical datasets. It was trained for solving several \ntasks such as classification, localization, and text generation. It used a non-hierarchical CNN-LSTM22 approach \ntogether with the attention to semantic and visual features, as it allowed to overperform the current state-of-\nthe-art results. In  the23, bone fracture X-ray reports were generated by identifying image features and filling text \ntemplates. The authors  of20 suggested a multi-task framework, that can both predict tags and generate texts using \nco-attention. This model is still not sufficient for producing accurate diagnosis from X-rays as the produced texts \nstill contained repeated sentences due to a lack of contextual coherence in the hierarchical models. The authors \n of24 took advantage of a sentence-level attention mechanism in a late fusion fashion. They took advantage of the \nmulti-view images from both frontal and lateral view angles from the Open-I  dataset25.\nThe authors  of26 proposed to utilize a pre-constructed knowledge graph embedding module (extracted from \nthe Open-I images using Chexnet  models27) on multiple disease findings to assist the report generation process. \nThe authors  of28 exposed an anomaly detection method for detecting abnormalities on chest X-rays with deep \nperceptual autoencoders. The authors  of29 first generated topics for sentences using reinforcement learning (RL) \nfollowed by the word decoder sequence generation from the topic with attention to the original images. RL was \nused for tuning to optimize readability. We solve this problem in a simpler method without losing in quality. To \nextract topics, we use the NegBio  labeller21,30, which provides topics from clinical reports. We add these topics \nto the beginning of the medical report, for our model to understand where exactly the text should be generated.\nThe  work31  focuses on reporting abnormal findings on radiology images. The proposed method learns condi-\ntional visual-semantic embeddings in radiology images; and the reports are further used to measure the similarity \nbetween the image regions and the medical reports. This by optimizing a triplet ranking loss. The authors  of32 \ndeveloped an algorithm that learns a description of findings from images and uses their pattern of occurrences to \nretrieve and customize similar reports from a large report database. The work  in33 proposed a Contrast Induced \nAttention Network (CIA-Net), using contrastive learning on the aligned positive and negative samples for the \ndisease localization on the chest X-ray images. The work  in34 studies the cross-domain performance, agreement \nbetween models, and model representations for X-rays diagnostic prediction tasks. The authors test for concept \nsimilarity by regularizing a network to group tasks across multiple datasets together and observe variation across \nthe tasks. The model  in22 generates a short textual summary with essential information on the found pathologies \nalong with their location and severity. The model is trained on only 2% of the MIMIC-CXR dataset, and gener-\nates short reports. Although, in this work, we train on the whole MIMIC-CXR and generate a full-text report.\nThe authors  of35–39 attempted to use transformer-based models as decoders in the image captioning  domain22. \nThe  work38 affirmed to have generated radiology reports through the custom transformer with additional mem-\nory-driven unit. Another model was introduced  in39 where encoder detects regions of interest via a bottom-up \nattention module and extracts top-down visual features. In this study, the decoder is presented as a custom \ntransformer. For example, the paper  in36 proposes an approach called “pseudo self-attention” . Its main idea is \nto incorporate the conditioning input as a pseudo history to a pretrained transformer. They add a new key and \nvalue weights in the self-attention module to be projected onto the decoder’s self-attention space,  while37 focuses \non visual and weighted semantic features.\nContributions. The contributions of this paper are the following:\n• We introduce a new architecture for image captioning, based on a combination of two language models with \nimage-attention (SAT) and text-attention (GPT-3), outperforming current state-of-the-art models\n• We introduce a new preprocessing pipeline for radiology reports that allows to get higher NLG metrics\n• We perform extensive experiments to show the capability of the proposed method\n• Finally, we contribute to deep learning community by training two language models on a large data-\nset MIMIC-CXR\nThe rest of the paper is organized as follows: section “ Methods” describes the architecture of two language \nmodels separately, section “ Proposed architecture ” provides the description of the proposed approach, sec-\ntion “Experiments” describes the data and the computing, the last sections compare the results and conclude \nthe paper.\nMethods\nShow attend and tell. Show Attend and Tell (SAT)15 is an attention-based image caption generation neural \nnet. An attention-based technique allows to get well interpretable results, which can be utilized by radiologist to \nensure their findings on X-ray. By including attention, the module gives the advantage to visualize where exactly \nthe model ‘sees’ the specific pathology. SAT consists of three blocks: Encoder, Attention module, and Decoder. \n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nIt takes an image, encodes it, attends each part of the image, and generates an L-length caption z , an encoded \nsequence of words from the W-length vocabulary:\nEncoder. Encoder is a convolutional neural network (CNN). It encodes an image and outputs a set of C vectors, \neach of which is a D-dimensional representation of the image corresponding part:\nHere, C represents the number of channels in the output of the encoder. It depends on the used type of the \nencoder: 1024 for DenseNet-121 40, 512 for VGG-1641, 2048 for  InceptionV342 and ResNet-10143. D is a config-\nurable parameter representing the encoded vectors size. Features are extracted from the lower convolutional \nlayer prior to the fully connected layers, and are being passed through the Adaptive Average Pooling layer. This \nallows the decoder to selectively focus on certain parts of an image by selecting a subset of all the feature vectors.\nDecoder with attention module. The decoder is implemented as an LSTM neural  network44. It produces a cap-\ntion by generating one word at every time step conditioned by the attention (context) vector, the previous hidden \nstate and the previously generated words. The LSTM can be represented as the following set of equations:\nVectors it , ft , ct , ot , ht represent the input/update gate activation vector, forgetting gate activation vector, memory \nor cell state vector, while outputting gate activation vector and hidden state of the LSTM respectively. T s,t is an \naffine transformation, such that Rs → Rt with non-zero bias. m  denotes the embedding dimension, while n \nrepresents LSTM dimension. σ and ⊙ stand for the sigmoid activation function and element-wise multiplication, \nrespectively. E ∈ Rm×L is an embedding matrix. The vector ˆa ∈ RD holds the visual information from a particu-\nlar input location of the image at time t. Thus, ˆa called context vector. Attention is a function φ , that computes \ncontext vector ˆat from the encoded vectors ai (2), produced by the encoder. The attention module generates a \npositive number αi for each location i on the image. This number can be interpreted as the relative importance \nto give to the location i , among others. Attention module is implemented as a multi-layer perceptron (MLP) \nwith a softmax activation function, conditioned at the previous hidden state ht−1 (5) of the LSTM. The attention \nmodule is depicted in Fig.  1. The set of linear layers in MLP is denoted as a function f att . The weights αti are \ncomputed using the following equations:\nThe sum of weights αti (7) should be equal to 1 ∑C\ni=1 αti = 1 . The context vector ˆat is computed by the \nattention function  φ with the set of encoded vectors a (2) and their corresponding weights αti (7) as inputs: \nˆat = φ ({ai }, {αti }) . According to the original paper function, φ can be either ‘soft’ or ‘hard’ attention. Due to \nspecific task of medical image caption, function φ was chosen to be the ‘soft’ attention, as it allows model to focus \nmore on some specific parts of X-rays from others and to detect pathologies and major organs such as heart, lung \n(1)z = {z1 ,..., zL}, zi ∈ RW SAT\n(2)a = {a1 ,..., aC }, ai ∈ RD ×D\n(3)\n\n\nit\nft\not\ngt\n\n =\n\n\nσ\nσ\nσ\ntanh\n\nTD+m +n,n\n�Ez t−1\nht−1\nˆat\n�\n(4)ct = ft ⊙ ct−1 + it ⊙ g t\n(5)ht = ot ⊙ tanh (ct ).\n(6)e ti = f att (ai,ht−1 )\n(7)α ti = exp(eti)∑C\np=1 exp(etp)\nFigure 1.  Attention module used in SAT.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\netc. It is named as a ‘deterministic soft attention’ and recognized as a weighted sum : φ ({ai}, {αti}) = ∑C\ni αiai . \nHence, context vector can be computed as:\nThe initial memory state and hidden state of the LSTM are initialized with two separate multi-layer perceptrons \n( init-c and init-h ) with the encoded vectors ai (2) for a faster convergence:\nTo compute the output of LSTM representing a probabilities vector the next word, a ‘deep output layer’ 44 was \nused. It looks both on the LSTM state ht (5), on context vector ˆat (8) and the one previous word zt−1 (2):\nwhere Lo∈ RW ×m , Lh ∈ Rm ×n , La ∈ Rm×D , and E ∈ Rm×L represent the embedding matrix.\nThe authors  in15 suggest to use the ‘doubly stochastic attention’ , where ∑\nt αti ≈ 1 . This can be interpreted \nas encouraging the model to pay equal attention to every part of the image. Y et, this method is not relevant for \nX-rays, as each part of the chest is almost at the same position from image to image. If the model learned, e.g., \nthat heart is in its specific position, a model does not have to search for the heart somewhere else. The model is \ntrained in an end-to-end manner by minimizing the cross-entropy loss LCE between vector with a softmaxed \ndistribution probability of next word and true caption as LCE =− log(P(z|a)).\nGenerative pretrained transformer. Generative Pretrained Transformer (GPT-3) 45 is a large trans-\nformer-based language model with 1.75 × 1011 parameters, trained on 570 GB of text. GPT-3 can be used to \ngenerate realistic continuations texts from the arbitrary domain. Basically, GPT-3 is a transformer that can look \nat a part of the sentence and predict the next word, thus being a language model. The original  transformer46 is \nmade up of encoder stack and decoder stack, in which encoders and decoders stacked upon each other. Whereas \nGPT-3 is built using just decoder blocks. One decoder block consists of Masked Self-Attention layer and Feed-\nForward neural network. It is called Masked as it pays attention only to previous inputs. The input should be \nencoded prior to going into the decoder block. In transformers and in the GPT-3 particularly, there are two \nsubsequent encodings: Byte Pair Token Encoding and Positional Encoding. Byte Pair Encoding (BPE) is a sim-\nple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a \nsingle, unused byte. The algorithm compresses data by finding the most frequently occurring pairs of adjacent \nsubtokens in the data and replacing all instances of the pair with a single subword. The algorithm repeats this \nprocess until no further compression is possible. Such tokenization avoids adding a special <unk> token to the \nvocabulary, as now all words can be encoded and obtained by combination of subwords from the vocabulary.\nProposed architecture\nWe introduce two architectures for X-ray image captioning. The overall goal of our approach is to improve the \nquality of Encoder-Decoder generated clinical records by using the GPT-3 language model. The suggested model \nconsists of two parts: the Encoder, Decoder (LSTM) with an attention module and GPT-3. While Encoder with \nLSTM detects pathologies and indicates zones of higher attention demand, the GPT-3 takes it as input and writes \na comprehensive medical report. There are two possible approaches for this task.\nApproach 1 The first method consists in forcing the models to learn a joint word distribution. Within this \nmethod (Fig. 2), both models A and B output scores for the next word in a sentence. Afterwards, due to concat-\nenating these scores and pushing them through the feed-forward neural net C, we get the final scores for subse-\nquent word. Whilst the disadvantage of this approach is the following: the GPT-3 model has its own vocabulary \nbuilt by the byte pair tokenizer. This vocabulary is different from the one used by the SAT model. We need to \ntake from continuous GPT-3 distribution separate scores corresponding to the words present in the Show Attend \nand Tell vocabulary. This turns continuous distribution from the GPT-3 into discrete and hence, while we do not \nuse all the potential generation power from the GPT-3.\nApproach 2 The Approach 2 is shown in Fig. 3 and is based on stacked A and B models. Show Attend and Tell \nA gets an image as an input and generates a report based on the data found on X-ray with an Attention module. \nIt learns where to focus and gives a seed for the GPT-3 B to continue generating text. The GPT-3 was fine-tuned \non MIMIC-CXR in self-supervised manner using the Huggingface  framework47. It learns to predict the next \nword in the text. The GPT-3 continues the report outputed by SAT and generates a detailed and complete clinical \nreport based on pathologies found by SAT. Such an approach is better for the GPT-3 as it gets more context as \ninput (from SAT) than in the first approach. Thus, the second approach performs better, and was hence chosen \nas the main architecture.\n(8)ˆat =\nC∑\ni\nαiati\n(9)c0 = finit-c\n(\n1\nC\nC∑\ni\nai\n)\n(10)h0 = finit-h\n(\n1\nC\nC∑\ni\nai\n)\n(11)P (zt|ˆat,zt−1 ) = softmax(Lo(Lh ht + La ˆat + Ezt−1 ))\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nFirst language model. The first part of the suggested model is realized as the Show Attend and Tell model \n(SAT), the encoder to encode the image, and the LSTM for decoding into sequence. The encoder encodes the \ninput image with 3 or 1 color channels into a smaller image with ‘learned’ channels. The resulted encoded images \ncan be interpreted as a summary representation of findings in the X-ray (Eq.  2). Those encoders, pretrained \non the  ImageNet48, are not suitable for the medical image caption task, as medical images do not have typi-\ncal objects from the natural domain. Thus, the DenseNet-121  from49 pretrained on the MIMIC-CXR dataset \nwas taken. It was trained for the classification task on 18 labels : Atelectasis, Consolidation, Infiltration, Pneu-\nFigure 2.  The first approach. Learn the joint distribution of two models.\nFigure 3.  The second approach. Pretrained GPT-3 (B) continues text generated by SAT (A).\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nmothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural Thickening, Cardiomegaly, Nodule, \nMass, Hernia, Lung Lesion, Fracture, Lung Opacity, and Enlarged Cardiomediastinum. Hence, the last classifi-\ncation layer was removed and features from the last convolutional layer were taken. These features were passed \nthrough the Adaptive Average Pooling layer. They can be represented by the tensor with the following dimen-\nsions: ( batchsize× C , D , D  ) (Eq. 2). C stands for the number of channels or how many different image regions to \nconsider. D implies the dimension of the image encoded region. Furthermore, the fine-tune method for encoder \nwas added. It enables or disables the calculation of gradients for the encoder’s parameters through the last lay-\ners. Then, at every time step, the decoder with the attention module observes the encoded small images with \nfindings and generates a caption word by word. The Encoder output is received and flattened to dimensions \n( batchsize, C , D × D  ). Since captions are padded with a special token <pad> , captions are sorted by decreasing \nlengths and at every time-step of generating a word, an effective batch size is computed in order not to process \nthe <pad> token.\nThe Show Attend and Tell model was trained using the Teacher-Forcing method while at each step the input \nto the model was the ground truth word on this step and not the previous generated word. As a result, we can \nconsider the SAT as a language model A . It gets a tokenized text of length m , an image as input and outputs a \nvector of probabilities for the next word at each time step t:\nwhere W is the SAT vocabulary size and L is the length of generated report (Eq.  1). Where P1 is computed as it \nis shown in the Eq. (11).\nOver the training process, the LSTM outputs a word with a maximum probability after the softmax layer. \nSimilarly  to50, we applied the K-Beam search, but only in the inference stage.\nSecond language model. The second part of the architecture proposed is the GPT-3. The GPT-3 is built \nfrom decoder blocks using the transformer architecture. At the same time, the decoder block consists of masked \nself-attention and a feed-forward neural network (FFNN). The output yields the token probabilities, i.e., logits. \nThe GPT-3 was pretrained separately on the MIMIC-CXR dataset and was then fine-tuned together with the \nSAT to enhance clinical reports.\nWe put a special token <start> at the end of the text generated by the SAT allowing the GPT-3 to under -\nstand where to start the generation process. We also used the K-Beam search after the GPT-3 generation and \ntook the second best sentence from the output as a continuation. The pretrained GPT-3 performs as a separate \nlanguage model B and generates good records based on the input text or tags. The GPT-3 generates report till \nthe moment when it generates the special token <|endoftext|> . We denote the length of the GPT-3 gener-\nated text as l\nCombination of two language models. We use a combination of two models placing them sequentially: \nthe SAT model extracts visual features from the image and allows us to focus on its specific parts. The GPT-3 \nprovides good and comprehensive text, based on what is found by the first model. Thus, the predictions from the \nfirst model improve those of the second language model.\nEvaluation metrics. The common evaluation metrics used for image captioning are : bilingual evalua-\ntion understudy (BLEU)51, recall-oriented understudy for gisting evaluation (ROUGE) 52, metric for evaluation \nof translation with explicit ordering (METEOR )53, consensus-based image description evaluation (CIDEr) 54, \nand semantic propositional image caption evaluation (SPICE)55. The Microsoft Common Objects in  Context56 \nprovides the kit with implementation of these metrics for the image caption task.\nExperiments\nDatasets. For training and evaluation of medical image captioning, we use three publicly available datasets. \nTwo of them are medical images datasets and the third one is a general-purpose one.\nMIMIC-CXR The MIMIC Chest X-ray (MIMIC-CXR) 57 dataset is a large publicly available dataset of chest \nradiographs in DICOM format with free-text radiology reports. This dataset consists of 377,110 images corre -\nsponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston, MA.\nOpen-I The Indiana University Chest X-ray Collection (IU X-ray)25 contains radiology reports associated with \nX-ray images. This dataset contains 7470 image-report pairs. All the reports enclose the following sections: \nimpression, findings, tags, comparison, and indication. We use the concatenation of impression and findings as \nthe target captions.\n(12)\nA : text,image→ P1(zt|true words = z�1�z�2� ...z �t−1�, image ),\nt∈{ 2,...m, ...L },\nP1 ∈ Rm ×W SAT\n(13)B : text→ P 2(zt|true words = z<1> ...z<L > < s >), t ∈{ L + 1,...L + l},\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nMSCOCO Microsoft Common Objects in Context dataset (MS COCO dataset) 58 is large-scale non-medical \ndataset for scene understanding. The dataset is commonly used for training and benchmark object detection, \nsegmentation, and captioning algorithms.\nImage preprocessing. Hierarchical Data Format (HDF5) 59 dataset was used to store all images. X-rays \nare in gray-scale and have one channel. To process them with the pre-trained CNN DenseNet-121, we used 1 \nchannel image. Each image was resized to the size of 224 × 224 pixels, normalized to the range from 0 to 1, and \nconverted to the float32 type and stored in the HDF5 dataset.\nImage captions pre-processing. Following the logic  in60, a medical report is considered as a concatena-\ntion of Impression and Findings sections, if both of these sections are empty, this report was excluded. This \nresulted in 360,666 DICOMs with reports for the MIMIC-CXR dataset. The text records are pre-processed by \nconverting all tokens to lowercase, removing all non-alphanumerical tokens. For our experiments we used 75% \nof data for training, 24.75 % for validation and 0.25% for testing.\nThe MIMIC-CXR database was used to access metadata and labels derived from free-text radiology reports. \nThese labels were extracted using the NegBio  tool21,30 that outputs one of 14 pathologies along with their sever-\nity and (or) absence. To generate more accurate reports, we added the extracted labels to the beginning of the \nreport. This allows language models to know the summary of the report for a more precise description generation.\nWe additionally formed the abbreviations dictionary of 150 + words from the Unified Medical Language \nSystem (UMLS)61. We also extended our dictionary size with several commonly used medical terms from the \nMedical Concept Annotation  Tool62.\nTraining of the neural network. The pipeline is implemented using PyTorch. Experiments were con-\nducted on a server running the Ubuntu 16.04 (32 GB RAM). All models were trained with NVIDIA Tesla V100 \nGPU (32 GB RAM). In all experiments, we use a 5-fold cross-validation and reported the mean performance. \nThe SAT was trained for 70 epochs with the batch size of 16, embedding dimension of 100, attention and decoder \ndimension of 512, dropout value 0.1. The encoder and decoder learning rates were 4 × 10−7 and 3 × 10−7 , \nrespectively. The Cross Entropy loss was used for training. The best model is chosen according to the highest \ngeometric mean of BLEU-n, as it is done in other  works63. SAT was trained in Teacher-Forcing technique, while \nthe Greedy approach is used for counting metrics. The GPT-3 small was fine-tuned with the MIMIC-CXR data-\nset for 30 epochs with batch size of 4, learning rate of 5 × 10−5 , the Adam epsilon of 1 × 10−8 , where the block \nsize equals 1024, with clipping gradients, which are bigger than 1.0. It was fine-tuned in a self-supervised manner \nas a language model. No data augmentation was applied.\nResults and discussion\nQuantitative results. The quantitative results for the baseline models, preceding works, and our models \nare presented in Table 1. The models were evaluated on the most common Open-I dataset, as well as on the big \nand rarely reported  data from the  MIMIC-CXR with free-text radiology reports. We implemented the most \ncommonly used metrics for evaluation—BLEU-n, CIDEr and ROUGE_L. The proposed approach outperforms \nthe existing models in terms of the NLG metrics—BLEU-n, CIDEr and ROUGE. BLEU-n measures the accu-\nracy, ROUGE_L measures the recall of the generated report while CIDEr helps estimate the ability of the model \nto capture context information in the ground truth report. The higher the metrics values, the better the perfor -\nmance of the model.\nWe additionally illustrated the performance of our model in Fig. 4 containing 4 original X-ray images from \nthe MIMIC-CXR dataset, the ground truth expert label, and the model predictions (Approaches 1 & 2). We \nmanually underlined the similarities and identical diagnoses in texts to guide the eye. Table  2 presents the \nmeasured clinical efficacy (CE) metrics on the MIMIC-CXR dataset for the baseline models and our proposed \nApproaches 1 and 2. The metrics are calculated by comparing the critical radiology terminology extracted from \nthe generated and the reference reports.\nDiscussion. The first language model (SAT) learned to generate a short summary at the beginning of the \nreport, based on the findings from a given medical image to provide the content details. This offers text genera-\ntion direction seed for the second model. The preprocessing of the medical reports enabled these high metrics. \nWe also address the biased data problem by applying domain-specific text preprocessing while using the NegBio \nlabeller. In a radiology database, the data is unbalanced because abnormal cases are rarer than the normal ones. \nThe NegBio labeller allowed us to get a not negative-biased diagnosis clinical records as it added short sentences \nat the beginning of the ground truth reports, making this task closer (in some ways) to a classification task, when \nthe state-of-the-art models had already managed to achieve a strong performance. The SAT also provides 2D \nlocalization heatmaps of pathologies, assisting and accelerating the diagnosis process.\nThe second language model, the Generative Pretrained Transformer GPT-3, showed promising results in \nthe medical domain. It successfully continued the extracted texts from the first language model, taking into \nconsideration all the findings provided. As GPT-3 is a rather powerful transformer, it summarizes and provides \nmore details on the findings. Natural language generation metrics suggest that using two language models \nsubsequently provides a notable advantage. Such an approach can be considered as accurate and efficient for \nthe medical captions  generation.\nOne may notice a gap in the context-related performance (CIDEr)as each ground truth image is accompanied \nby multiple reference captions. The drawback in the CIDEr performance points to a suboptimal suitability of \nthe generated output, whereas the Approach 2 does its best. This is due to the image-relevant n-grams occurring \n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nfrequently in the respective set of reference sentences. The drawback is in the sampling from the GPT-3 dis-\ntribution. The Approach 2, featuring SAT followed by the GPT-3, outperformed the reported state-of-the-art \n(SOTA) models in all the 3 datasets considered. Notably, the proposed approach outperforms SOTA models \non MIMIC-CXR, demonstrating the highest performance in all the metrics. The performance for the main \nevaluation dataset, the MIMIC-CXR, is measured by the CE metrics using micro-averaging and demonstrates \n0.861 for the proposed SAT + GPT-3 Approach 2 model vs. 0.840 with the Approach 1, and 0.743 for the SAT, \nrespectively, as reported in Table 2.\nExamples of the reports generated jointly via the SAT + GPT-3 with Approaches 1 and 2 are shown in Fig. 4. \nOne may notice that some generated sentences coinside with the ground truth. For example, in both generated \nand the true reports, for the first X-ray it reads “no acute cardiopulmonary abnormality” . Some sentences are \nclose in their meaning, even if they are different in terms of chosen words and n-grams (“no pneumonia. no \npleural effusion. no edema. ... ” compared to “ without pulmonary edema or pneumothorax”).\nConclusions\nWe introduced a new technique of combining two language models for the medical image captioning task. \nPrincipally, the new preprocessing and squeezing approaches for clinical records were implemented along with a \ncombined language model, where the first component is based on attention mechanism and the second one rep-\nresents a generative pretrained transformer. The proposed combination of the models generates a descriptive \ntextual summary with essential information on found pathologies along with their location and severity. Besides, \nthe 2D Grad-CAM 67 heatmaps localize each pathology on the original scans. The results, measured with the \nnatural language generation metrics on both the MIMIC-CXR and the Open-I datasets, speak for an efficient \napplicability to the chest X-ray image captioning task. This approach also provides well-interpretable results and \nallows to support clinical decision making.\nWe investigated various approaches to automatic generation of X-ray image captioning. We proved that the \nSAT is a strong baseline, outperforming models with Transformer-based decoders. With the help of GPT-3 \npre-trained language model, we managed to improve this baseline. The simple method, where the GPT-3 model \nA\nBC D\nApproach 2A pproach 1G round truth           DC Image sample cases\nwithout \nevidence of focal airspace consolidation,\nedema or pneumothorax.Irregularity \nin the right humeral neck is related to a\nknown healing fracture secondary to \nrecent fall. PA and lateral views of the\nchest at 09:55 are submitted\nand no evidence of acute \ncardiopulmonary disease. no \npneumonia, vascular congestion, \nof incidental note is an \n. this raises possibility of a\nnormal variant.\npulmonary vascularity is\nnormal in caliber and distribution .\nimpression : no evidence of acute\npulmonary pathology with\npossible development of right pleural\nA\nand adjacent atelectasis are overall \nunchanged. The heart is top-normal in \nsize, unchanged. \nThere is decrease in now small right \npneumothorax. There is a new right\npacer pigtail catheter. \nCardiomediastinal contours are \nunchanged. Lines and tubes are in \nstandard position. \npresent. lung opacity present. \natelectasis present. bilateral pleural \nbibasilar opacities potentially atelectasis\nin setting of low lung volumes. infection \nbe excluded.frontal and lateral views of \nchest demonstrate low lung volumes. \nthere is no focal consolidation \npneumothorax.\nCompared to prior chest radiographs \n. Previous mild pulmonary edema has\nimproved, moderate cardiomegaly and \nmediastinal vascular engorgement have \nnot. ET tube, right transjugular \ntemporary pacer lead are in standard\nplacements and an esophageal drainage\n tube passes into the stomach\npresent. no edema. cardiomegaly \npresent. atelectasis present. as compared\nto previous radiograph, there is an\nincrease in extent of a pre existing small\natelectasis at left lung bases. no new \nfocal parenchymal opacities suggesting \npneumonia. \nsupport devices present. no \nlung opacity present. uncertain enlarged\ncardiomediastinum. no edema. \natelectasis present. right internal jugular\n central line has its tip in distal superior \nvena cava. overall cardiac and \nmediastinal contours are likely\nstable given patient rotation on current \nstudy. \nbilateral patchy areas of consolidation \nappearing possibly due to edema \nand/or pneumonia . anteriorchest \nsurgery . clothing artifact appearing . \nanterior right upper abdomen surgery .\nAtelectasis Lung Opacity Cardiomegaly, Edema, Lung Opacity\nandatelectasis. 2. Enlarged pulmonary \nartery, suggesting pulmonary \nhypertension.  Bilateral small pleural \noverall unchanged. The heart is \ntop-normal in size, unchanged. \nminimal linear densities in the \ncostophrenic angles characteristic of \nscarring . healed rib fractures. \nminimal tortuosity thoracic aorta . \nconsistent with pulmonary edema .\nFigure 4.  Image sample cases with the disease classes (DC) along with original (ground truth) and generated \nreports by the proposed SAT + GPT-3 model implemented as in Approach 1 and 2, respectively. Insets in the \nupper corners of the original images feature localization heatmaps. Heatmaps are generated using Matplotlib \nv.3.7.064.\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nfinishes the report extracted by the Show-Attend-Tell model, yields significant improvements to the standard \ntext generation scores. Recent advancements in interactive training, such as active  learning68 and dialog-based \n ChatGPT69, have the potential to improve the performance of medical image captioning models even further. \nThis is an area of research that will be explored in the future.\nData availability\nAll data generated or analysed during this study are included in this published article. The datasets used and/or \nanalysed during the current study available from the corresponding author on reasonable request.\nReceived: 24 October 2022; Accepted: 8 March 2023\nReferences\n 1. Irvin, J. et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Proc. AAAI Conf. Artif. \nIntell. 33, 590–597 (2019).\n 2. Demner-Fushman, D. et al. Preparing a collection of radiology examinations for distribution and retrieval. J. Am. Med. Inform. \nAssoc. 23, 304–310 (2016).\nTable 1.  Reported mean performance using word-overlap metrics for two medical radiology datasets and one \nnon-medical for general purpose. Models labelled with † stand for the models we implemented and trained \nwith the preprocessed MIMIC-CXR data. Other results are cited from the original papers. BLUE-n denotes the \nBLEU score that uses up to n-grams. The best performance in each configuration is in bold.\nModel CIDEr ROUGE_L BLEU-1 BLEU-2 BLEU-3 BLEU-4\nMIMIC-CXR\nS &T12 0.886 0.300 0.307 0.201 0.137 0.093\nOriginal  SAT15 0.967 0.288 0.318 0.205 0.137 0.093\nTieNet19 1.004 0.296 0.332 0.212 0.142 0.095\nNLG29 1.153 0.307 0.352 0.223 0.153 0.104\nSAT† 1.986 0.478 0.634 0.549 0.451 0.383\nApproach 1 1.974  0.477 0.622 0.573 0.497 0.401\nApproach 2 1.989 0.480 0.725 0.626 0.505 0.418\nOpen-I\nCo-Attention60 0.327 0.447 0.517 0.386 0.306 0.247\nTieNet19 – 0.311 0.330 0.194 0.124 0.081\nCNN-RNN12 0.111 0.267 0.316 0.211 0.140 0.095\nLRCN65 0.190 0.278 0.369 0.229 0.149 0.138\nATT-RK18 0.155 0.323 0.369 0.226 0.151 0.108\nCDGPT237 0.257 0.289 0.387 0.245 0.166 0.111\nOriginal  SAT15 0.320 0.361 0.433 0.281 0.194 0.138\n SAT† 0.699 0.413 0.407 0.258 0.210 0.125\n Approach 1 0.687 0.402 0.450 0.299 0.224 0.141\nApproach 2 0.701 0.450 0.520 0.390 0.296 0.235\nMS-COCO\nBRNN66 – – 0.642 0.451 0.304 0.203\nOriginal  SAT15 – – 0.718 0.504 0.357 0.250\n SAT† 1.300 0.592 0.815 0.663 0.516 0.395\n Approach 1 1.298 0.505 0.818 0.664 0.509 0.385\nApproach 2 1.360 0.606 0.821 0.672 0.529 0.409\nTable 2.  The clinical efficacy (CE) metrics on the MIMIC-CXR dataset. The best results are highlighted in \nbold. Models labelled with † stand for the models we implemented and trained with the preprocessed MIMIC-\nCXR data. Other results are cited from the original papers. .\nModel Accuracy Precision Recall F1-Score\nS &T12 0.423 0.084 0.066 0.072\nOriginal  SAT15 0.703 0.181 0.134 0.144\nTieNet19† 0.741 0.265 0.178 0.197\nNLG29† 0.792 0.413 0.286 0.317\nSAT† 0.743 0.166 0.121 0.129\nApproach 1 0.840 0.420 0.303 0.134\nApproach 2 0.861 0.445 0.351 0.369\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\n 3. Chan, Y .-H. et al. Effective pneumothorax detection for chest X-ray images using local binary pattern and support vector machine. \nJ. Healthc. Eng. 2018, 1–11 (2018).\n 4. Maghdid, H. S. et al. Diagnosing covid-19 pneumonia from X-ray and CT images using deep learning and transfer learning algo-\nrithms. In Multimodal Image Exploitation and Learning 2021 Vol. 11734, 117340E (International Society for Optics and Photonics, \n2021).\n 5. Monshi, M. M. A., Poon, J. & Chung, V . Deep learning in generating radiology reports: A survey. Artif. Intell. Med. 106, 101878 \n(2020).\n 6. García Gilabert, J. Image Captioning using pre-trained GPT-2 models. Ph.D. thesis, Universitat Politècnica de València (2022).\n 7. Chen, J., Guo, H., Yi, K., Li, B. & Elhoseiny, M. Visualgpt: Data-efficient adaptation of pretrained language models for image \ncaptioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18030–18040 (2022).\n 8. Sermesant, M., Delingette, H., Cochet, H., Jaïs, P . & Ayache, N. Applications of artificial intelligence in cardiovascular imaging. \nNat. Rev. Cardiol. 18, 600–609 (2021).\n 9. Gurgitano, M. et al. Interventional radiology ex-machina: Impact of artificial intelligence on practice. La radiologia medica  126, \n998–1006 (2021).\n 10. Belikova, K., Rogov, O. Y ., Rybakov, A., Maslov, M. V . & Dylov, D. V . Deep negative volume segmentation. Sci. Rep. 11 (2021).\n 11. Pavlopoulos, J., Kougia, V . & Androutsopoulos, I. A survey on biomedical image captioning. In Proceedings of the Second Workshop \non Shortcomings in Vision and Language, 26–36 (Association for Computational Linguistics, 2019).\n 12. Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: A neural image caption generator. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 3156–3164 (2015).\n 13. Shin, H.-C. et al. Learning to read chest X-rays: Recurrent neural cascade model for automated image annotation. In Proceedings \nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016).\n 14. Bahdanau, D., Cho, K. & Bengio, Y . Neural machine translation by jointly learning to align and translate. In 3rd International \nConference on Learning Representations (ICLR) (2016).\n 15. Xu, K. et al. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International \nConference on International Conference on Machine Learning Vol. 37, ICML ’15, 2048-2057 (JMLR.org, 2015).\n 16. Donahue, J. et al. Long-term recurrent convolutional networks for visual recognition and description. IEEE Trans. Pattern Anal. \nMach. Intell. 39, 677–691 (2017).\n 17. Zhang, Z., Xie, Y ., Xing, F ., McGough, M. & Y ang, L. Mdnet: A semantically and visually interpretable medical image diagnosis \nnetwork. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3549–3557 (2017).\n 18. Y ou, Q., Jin, H., Wang, Z., Fang, C. & Luo, J. Image captioning with semantic attention. In 2016 IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 4651–4659 (2016).\n 19. Wang, X., Peng, Y ., Lu, L., Lu, Z. & Summers, R. M. Tienet: Text-image embedding network for common thorax disease classifica-\ntion and reporting in chest X-rays. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  \n(2018).\n 20. Jing, B., Xie, P . & Xing, E. On the automatic generation of medical imaging reports. In Proceedings of the 56th Annual Meeting of \nthe Association for Computational Linguistics Vol. 1: Long Papers, 2577–2586 (Association for Computational Linguistics, 2018).\n 21. Wang, X. et al. Chestx-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localiza-\ntion of common thorax diseases. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3462–3471 (2017).\n 22. Rodin, I., Fedulova, I., Shelmanov, A. & Dylov, D. V . Multitask and multimodal neural network model for interpretable analysis \nof x-ray images. In 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) (IEEE, 2019).\n 23. Gale, W ., Oakden-Rayner, L., Carneiro, G., Palmer, L. J. & Bradley, A. P . Producing radiologist-quality reports for interpretable \ndeep learning. In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019) (IEEE, 2019).\n 24. Yuan, J., Liao, H., Luo, R. & Luo, J. Automatic radiology report generation based on multi-view image fusion and medical con-\ncept enrichment. In Medical Image Computing and Computer Assisted Intervention—MICCAI 2019 (eds Shen, D. et al.) 721–729 \n(Springer International Publishing, 2019).\n 25. Demner-Fushman, D. et al. Preparing a collection of radiology examinations for distribution and retrieval. J. Am. Med. Inform. \nAssoc. 23, 304–310 (2016).\n 26. Zhang, Y . et al. When radiology report generation meets knowledge graph. Proc. AAAI Conf. Artif. Intell. 34, 12910–12917 (2020).\n 27. Rajpurkar, P . et al. Chexnet: Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv: 1711. 05225 (2017).\n 28. Shvetsova, N., Bakker, B., Fedulova, I., Schulz, H. & Dylov, D. V . Anomaly detection in medical imaging with deep perceptual \nautoencoders. IEEE Access 9, 118571–118583 (2021).\n 29. Liu, G. et al. Clinically accurate chest X-ray report generation. In Proceedings of the 4th Machine Learning for Healthcare, Proceed-\nings of Machine Learning Research Vol. 106 (eds Doshi-Velez, F . et al.) 249–269 (PMLR, 2019).\n 30. Peng, Y . et al. Negbio: A high-performance tool for negation and uncertainty detection in radiology reports. In AMIA Summits \non Translational Science Proceedings Vol. 2017 (2017).\n 31. Ni, J., Hsu, C.-N., Gentili, A. & McAuley, J. Learning visual-semantic embeddings for reporting abnormal findings on chest X-rays. \nIn Findings of the Association for Computational Linguistics: EMNLP 2020, 1954–1960 (Association for Computational Linguistics, \nOnline, 2020).\n 32. Syeda-Mahmood, T. et al. Chest X-ray report generation through fine-grained label learning. In Medical Image Computing and \nComputer Assisted Intervention—MICCAI 2020, 561–571 (Springer International Publishing, 2020).\n 33. Liu, J. et al. Align, attend and locate: Chest X-ray diagnosis via contrast induced attention network with limited supervision. In \nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019).\n 34. Cohen, J. P ., Hashir, M., Brooks, R. & Bertrand, H. On the limits of cross-domain generalization in automated X-ray prediction. In \nProceedings of the Third Conference on Medical Imaging with Deep Learning, Proceedings of Machine Learning Research (eds Arbel, \nT. et al.) 136–155 (PMLR, 2020).\n 35. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human \nLanguage Technologies Vol. 1 (Long and Short Papers), 4171–4186 (Association for Computational Linguistics, 2019).\n 36. Ziegler, Z. M., Melas-Kyriazi, L., Gehrmann, S. & Rush, A. M. Encoder-agnostic adaptation for conditional language generation. \narXiv: 1908. 06938 (2019).\n 37. Alfarghaly, O., Khaled, R., Elkorany, A., Helal, M. & Fahmy, A. Automated radiology report generation using conditioned trans-\nformers. Inform. Med. Unlocked 24, 100557 (2021).\n 38. Chen, Z., Song, Y ., Chang, T.-H. & Wan, X. Generating radiology reports via memory-driven transformer. In Proceedings of the \n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1439–1449 (Association for Computational \nLinguistics, Online, 2020).\n 39. Xiong, Y ., Du, B. & Y an, P . Reinforced transformer for medical image captioning. In Machine Learning in Medical Imaging, 673–680 \n(Springer International Publishing, 2019).\n 40. Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks. In 2017 IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), 2261–2269 (2017).\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\n 41. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015 (eds Bengio, Y . & LeCun, Y .) (Conference Track \nProceedings, 2015).\n 42. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In 2016 \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, 2016).\n 43. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR) (IEEE, 2016).\n 44. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997).\n 45. Brown, T. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems Vol. 33 (eds Laro-\nchelle, H. et al.) 1877–1901 (Curran Associates Inc, 2020).\n 46. Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems Vol. 30 (eds Guyon, I. et al.) \n(Curran Associates Inc, 2017).\n 47. Wolf, T. et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing: System Demonstrations, 38–45 (Association for Computational Linguistics, Online, 2020).\n 48. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern \nRecognition, 248–255 (IEEE, 2009).\n 49. Cohen, J. P . et al. Torchxrayvision: A library of chest X-ray datasets and models. In Proceedings of the 5th International Conference \non Medical Imaging with Deep Learning, Proceedings of Machine Learning Research  Vol. 172 (eds Konukoglu, E. et al.) 231–249 \n(PMLR, 2022).\n 50. Wiseman, S. & Rush, A. M. Sequence-to-sequence learning as beam-search optimization. In Proceedings of the 2016 Conference \non Empirical Methods in Natural Language Processing, 1296–1306 (Association for Computational Linguistics, 2016).\n 51. Papineni, K., Roukos, S., Ward, T. & Zhu, W .-J. BLEU. In Proceedings of the 40th Annual Meeting on Association for Computational \nLinguistics—ACL ’02 (Association for Computational Linguistics, 2001).\n 52. Lin, C.-Y . ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, 74–81 (Association \nfor Computational Linguistics, 2004).\n 53. Banerjee, S. & Lavie, A. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In \nProceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, \n65–72 (Association for Computational Linguistics, 2005).\n 54. Vedantam, R., Zitnick, C. L. & Parikh, D. Cider: Consensus-based image description evaluation. In 2015 IEEE Conference on \nComputer Vision and Pattern Recognition (CVPR), 4566–4575 (2015).\n 55. Anderson, P ., Fernando, B., Johnson, M. & Gould, S. SPICE: Semantic propositional image caption evaluation. In Computer \nVision—ECCV 2016, 382–398 (Springer International Publishing, 2016).\n 56. Fang, H. et al. From captions to visual concepts and back. In 2015 IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR) (IEEE, 2015).\n 57. Johnson, A. E. W . et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci. \nData 6 (2019).\n 58. Lin, T.-Y . et al. Microsoft COCO: Common objects in context. In Computer Vision—ECCV 2014, 740–755 (Springer International \nPublishing, 2014).\n 59. Koziol, Q. et al. HDF5. In Encyclopedia of Parallel Computing, 827–833 (Springer US, 2011).\n 60. Jing, B., Xie, P . & Xing, E. On the automatic generation of medical imaging reports. In Proceedings of the 56th Annual Meeting of \nthe Association for Computational Linguistics Vol. 1: Long Papers (Association for Computational Linguistics, 2018).\n 61. Bodenreider, O. The unified medical language system (UMLS): Integrating biomedical terminology. Nucleic Acids Res. 32, 267D \n– 270 (2004).\n 62. Kraljevic, Z. et al. Multi-domain clinical natural language processing with MedCAT: The medical concept annotation toolkit. Artif. \nIntell. Med. 117, 102083 (2021).\n 63. Papineni, K., Roukos, S., Ward, T. & Zhu, W .-J. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the \n40th Annual Meeting on Association for Computational Linguistics, ACL ’02, 311–318 (Association for Computational Linguistics, \n2002). https:// doi. org/ 10. 3115/ 10730 83. 10731 35.\n 64. Hunter, J. D. Matplotlib: A 2d graphics environment. Comput. Sci. Eng. 9, 90–95. https:// matpl otlib. org/ stable/ index. html (2007).\n 65. Donahue, J. et al. Long-term recurrent convolutional networks for visual recognition and description. IEEE Trans. Pattern Anal. \nMach. Intell. 39, 677–691 (2017).\n 66. Karpathy, A. & Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In 2015 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) (IEEE, 2015).\n 67. Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE International \nConference on Computer Vision (ICCV), 618–626 (2017).\n 68. Shelmanov, A. et al. Active learning for sequence tagging with deep pre-trained models and bayesian uncertainty estimates. In \nProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (Associa-\ntion for Computational Linguistics, 2021).\n 69. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. \nPLoS Digit. Health 2, e0000198 (2023).\nAcknowledgements\nThe authors of this paper thank Alexander Panchenko and Alexander Shvets for the helpful discussion. The work \nwas supported by Philips in 2021.\nAuthor contributions\nAll authors participated in data analysis, manuscript writing, and final text review. A.S. and O.R. contributed \nequally to the network architecture design.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to D.V .D.\nReprints and permissions information is available at www.nature.com/reprints.\n12\nVol:.(1234567890)Scientific Reports |         (2023) 13:4171  | https://doi.org/10.1038/s41598-023-31223-5\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}