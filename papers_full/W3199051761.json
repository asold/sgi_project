{
  "title": "BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets",
  "url": "https://openalex.org/W3199051761",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2162599171",
      "name": "Yanzhu Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3119078183",
      "name": "Virgile Rennard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2792823661",
      "name": "Christos Xypolopoulos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1914497179",
      "name": "Michalis Vazirgiannis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3214298066",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2888421737",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2737887843",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3098388437",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2397944984",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4285711473"
  ],
  "abstract": "We introduce BERTweetFR, the first large-scale pre-trained language model for\\nFrench tweets. Our model is initialized using the general-domain French\\nlanguage model CamemBERT which follows the base architecture of RoBERTa.\\nExperiments show that BERTweetFR outperforms all previous general-domain French\\nlanguage models on two downstream Twitter NLP tasks of offensiveness\\nidentification and named entity recognition. The dataset used in the\\noffensiveness detection task is first created and annotated by our team,\\nfilling in the gap of such analytic datasets in French. We make our model\\npublicly available in the transformers library with the aim of promoting future\\nresearch in analytic tasks for French tweets.\\n",
  "full_text": "Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 445–450\nNovember 11, 2021. ©2021 Association for Computational Linguistics\n445\nBERTweetFR : Domain Adaptation of Pre-T rained Language Models for\nFrench T weets\nY anzhu Guo\nÉcole Polytechnique, France\nShanghai Jiao T ong University , China\nyanzhu.guo@polytechnique.edu\nVirgile Rennard\nÉcole Polytechnique, France\nvirgile@rennard.org\nChristos Xypolopoulos\nÉcole Polytechnique, France\nchristos.xypolopoulos@polytechnique.edu\nMichalis V azirgiannis\nÉcole Polytechnique, France\nmvazirg@lix.polytechnique.fr\nAbstract\nW e introduce BERT weetFR, the ﬁrst large-\nscale pre-trained language model for French\ntweets. Our model is initialized using\nthe general-domain French language model\nCamemBER T (\nMartin et al. , 2020) which fol-\nlows the base architecture of BER T . Exper-\niments show that BER T weetFR outperforms\nall previous general-domain French language\nmodels on two downstream T witter NLP tasks\nof offensiveness identiﬁcation and named en-\ntity recognition. The dataset used in the of-\nfensiveness detection task is ﬁrst created and\nannotated by our team, ﬁlling in the gap of\nsuch analytic datasets in French. W e make\nour model publicly available in the transform-\ners library with the aim of promoting future\nresearch in analytic tasks for French tweets.\n1 Introduction\nV ector representations of words have given rise\nto the application of deep learning methods in\nNLP . Traditional pre-training approaches, such as\nword2vec ( Mikolov et al. , 2013) and GloV e ( Pen-\nnington et al. , 2014) are static, learning a single\nrepresentation for every word regardless of con-\ntext. However, words are often polysemous with\ndifferent meanings depending on the context. More\nrecently , models are trained to integrate contextual\nmeaning : the output word embeddings depend\non the whole input sequence rather than only the\nword itself. While the idea was initially imple-\nmented with recurrent neural networks (\nDai and\nLe, 2015) ( Ramachandran et al. , 2017), recent mod-\nels have predominantly been based on the trans-\nformers architecture (\nV aswani et al. , 2017), with\nBERT ( Devlin et al. , 2019) and RoBERT a ( Liu\net al. , 2019) among the most popular. These contex-\ntualized word representation models have opened\nnew doors for researchers as they can be applied\nto numerous downstream tasks by ﬁne-tuning or\nprompting. In fact, large-scale pre-trained language\nmodels have become the go-to tool for building new\nNLP applications, saving researchers from the enor-\nmous amount of computational resource and data\nrequired for training model weights from scratch.\nIn the past few years, human society has become\nmore digitally connected than ever before. People\nuse social media to report the latest news, but also\nto express their opinions and feelings about real-\nworld events. As one of the most popular micro-\nblogging platforms, T witter has become a primary\nsource for social media user-generated data ( Ghani\net al. , 2019). However, tweets are more often writ-\nten in informal language compared to the carefully\nedited texts that are published in traditional data\nsources such as Wikipedia and printed media. They\nhave their own set of features such as the recurrent\nuse of irregular or abbreviated words, the large\nquantity of spelling or grammatical mistakes, the\nemployment of improper sentence structures and\nthe occurrence of mixed languages ( Farzindar and\nInkpen, 2020). This presents challenges to standard\nNLP methods when applied to T witter data.\nDomain-adaptive pre-training has been revealed\nto provide signiﬁcant gains in helping models en-\ncode the complexity of speciﬁc textual domains\n(Gururangan et al. , 2020). While efforts on domain\nadaptation of large-scale language models to T wit-\nter language have been made in English ( Nguyen\net al. , 2020), there has been no similar work in any\nother language.\nW e start addressing this problem by releasing\n446\nBERT weetFR, a pre-trained language model for\nFrench tweets together with a manually labeled\ndataset for offensiveness identiﬁcation in French\ntweets. W e evaluate our model on two downstream\ntasks : offensiveness identiﬁcation and named en-\ntity recognition.\nW e compare the performance of our model to\nthe best-performing transformer-based models pre-\ntrained on general domain French texts, namely\nCamemBER T ( Martin et al. , 2020), FlauBER T ( Le\net al. , 2020) and BARThez ( Eddine et al. , 2020).\nExperiments show that our model outperforms all\nthree of them on both of the downstream tasks.\nIn order to facilitate future research on French\ntweets, we make our model publicly available on\nHuggingface’s model hub 1 as well as on our team’s\nwebsite dedicated to French linguistic resources 2.\nW e will also release our offensiveness identiﬁcation\ndataset while respecting the limitations of T witter’s\ndevelopper policy .\n2 Building the Model\nIn this section, we describe the collection steps and\npre-procesing pipeline for our pre-training dataset,\npresent the model architecture and introduce the\ntraining objective and optimization setup.\n2.1 Pre-training Data\nW e use a 16GB dataset of 226M deduplicated\nFrench tweets. The tweets are deduplicated using\nopen-source tool runiq 3. In addition, we ﬁlter out\ntweets with fewer than 5 tokens assuming they do\nnot contain substantial information. The average\nlength of a tweet is 30 tokens.\n2.1.1 Data Collection\nOur ﬁnal dataset for pre-training is an aggregation\nof three corpora from different sources. The ag-\ngregation of these corpora makes this dataset the\nlargest one for French tweets up to this date. It\nis also beneﬁcial to aggregate different corpora in\norder to cover tweets from different time periods\nwith diverse topics and styles.\nThe three sources we use are as follows :\n• W e start by downloading tweets from the gen-\n1 https://huggingface.co/Yanzhu/\nbertweetfr-base\n2 http://master2-bigdata.polytechnique.\nfr/FrenchLinguisticResources/bertweetFr\n3 https://github.com/whitfin/runiq\neral T witter Stream 4 grabbed by the Archive\nT eam, containing of tweets streamed from\nJanuary 2016 to December 2019. Selecting\nonly the French tweets with T witter’s built-in\nfeature, we obtain a corpus of 34M unique\ntweets.\n• W e also build a COVID-19 related corpus of\nFrench tweets relevant to the COVID-19 pan-\ndemic posted between September 2020 and\nApril 2021. In this case, our ﬁlters are focused\non tweets that include the hashtags “covid19”\nand “coronavirus”. Through T witter’s pub-\nlic streaming API, we extracted tweets in\nFrench marked with either or both of the two\nabove hashtags. This corpora consists of 19M\nunique tweets.\n• Finally , we make use of a previous T witter\ndataset constructed for socioeconomic anal-\nysis (\nAbitbol et al. , 2018). This corpus in-\ncludes a collection of tweets in French be-\ntween the years 2014 and 2018. W e extract\n173M unique tweets form this corpus.\n2.1.2 Data Pre-Processing\nW e only implement minimal data cleaning before\ninputting the sequences into the tokenizer. Follow-\ning ( Nguyen et al. , 2020), we normalize the T weets\nby converting user mentions and web/url links into\nspecial tokens @USER and HTTPURL.\nFor tokenization, we apply the CamemBER T to-\nkenizer ( Martin et al. , 2020). The CamemBERT\ntokenizer segments input sequences into subword\nunits using the SentencePiece ( Kudo and Richard-\nson, 2018) algorithm. This algorithm is an exten-\nsion of Byte-Pair encoding (BPE) ( Shibata et al. ,\n1999) and W ordPiece ( Kudo, 2018) that eliminates\nthe pre-tokenization step, thus more generally appli-\ncable. The vocabulary size is 32k subword tokens.\n2.2 Model Architecture\nBERT weetFR is initialized from the base version\nof CamemBERT . W e choose to further ﬁne-tune\nthis model instead of starting from scratch because\ndomain-adaptive pre-training have been proven to\ngive very satisfying results in numerous down-\nstream tasks spanning across a wide range of do-\nmains (\nGururangan et al. , 2020). The choice to\n4 https://archive.org/details/\ntwitterstream\n447\nemploy CamemBER T instead of other French lan-\nguage models is based on its overall best perfor-\nmance on downstream tasks experimented in previ-\nous works ( Eddine et al. , 2020).\nCamemBERT applies the multi-layer bidirec-\ntional Transformer architecture. The base version\nuses the same architectures as the base version of\nBER T with 12 layers, 768 hidden dimensions and\n12 attention heads, adding up to a total of 110M\nparameters. CamemBERT follows the same opti-\nmized pre-training approach as RoBER T a, the only\ndifference is that it uses whole-word masking and\nthe SentencePiece tokenization instead of W ord-\nPiece.\n2.3 T raining Objective\nOur model is trained on the Masked Language\nModeling (MLM) task. With any given input se-\nquence,\n15% of the tokens are chosen for pos-\nsible replacement. Among the selected tokens,\n80% are further selected to be replaced by the\nspecial <MASK> token, 10% remain unchanged\nand 10% are replaced by random tokens. Finally ,\nthe model is trained to predict the tokens replaced\nby <MASK> using cross-entropy loss. Following\nRoBERT a, we do not ﬁx the whole set of masked\ntokens during pre-processing but select them dy-\nnamically during the training process. The data is\nthus augmented when training for multiple epochs.\n2.4 Optimization Setup\nW e employ the CamemBERT implementation\nin the transformers library ( W olf et al. , 2020).\nThe maximum sequence length is set to be 128,\ngenerating approximately 226M × 30/128 ≈\n53M sequence blocks. Following ( Gururangan\net al. , 2020), we optimize the model using Adam\n(Kingma and Ba , 2015) with a batch size of 1280\nacross 8 V100 GPUs (32GB each) and a peak\nlearning rate of 0.0001. W e pre-train the model\nfor 20 epochs in about 8 days with a total of\n53M × 20/1280 ≈ 83K training steps.\n3 Downstream T ask Datasets\nW e evaluate the performance of BERT weetFR on\ntwo downstream T witter NLP tasks. The datasets\nused in these downstream tasks are either con-\nstructed by ourselves or obtained from shared tasks\nin past conferences.\n3.1 Offensive Language Identiﬁcation\nAlong with the outbreak of COVID-19 came severe\ndisruption in the French society . Effects include\nthe public holding the government accountable for\ncertain ways in which the pandemic was handled,\nas well as a rise of hateful sentiment towards the\nAsian community . In fact, ever since the explosion\nof COVID-19 on a global scale, unrest has led to\nan increase of violent incidents towards people of\nAsian descent.\nIn response to this phenomenon, we have cre-\nated a human annotated dataset for general offen-\nsiveness detection in T weets collected during the\nCOVID-19 pandemic. Our dataset contains 5786\nFrench tweets among which 1301 have been la-\nbeled as offensive. Offensiveness is not straight-\nforward and can be subjective. In our labeling pro-\ncedure, we consider a tweet as offensive in cases\nwhere personal attacks are detected. For example,\n\"The chinese virus is tiring\" would not be consid-\nered offensive, while \"I hate the chinese for bring-\ning us the chinese virus\" would be. This is a binary\nsequence classiﬁcation task. W e randomly sample\na 70/15/15 training/validation/test split with each\nclass proportionally represented in each part of the\nsplit.\n3.2 Named Entity Recognition\nFor the NER task, we take data from the CAp 2017\nchallenge ( Lopez et al. , 2017). This challenge pro-\nposes a new benchmark for the problem of NER\nfor tweets written in French. The tweets were col-\nlected using the publicly available T witter API and\nannotated with 13 types of entities : person, musi-\ncArtist, organisation, geoLoc, product, transport-\nLine, media, sportsT eam, event, tvShow , movie,\nfacility and other. Overall, the dataset comprises\n6685 annotated tweets split into two parts: a train-\ning set consisting of 3000 tweets and a test set with\n3685 tweets. For compatibility with previous re-\nsearch, the data were released tokenized using the\nCoNLL format and the BIO encoding.\n4 Baselines\nAs our BER T weetFR model is the ﬁrst pre-trained\nlanguage model for French tweets, we compare it\nwith the following general-domain language mod-\nels for French.\nCamemBERT\nAs mentioned in 2.2, CaemBER T\n(Martin et al. , 2020) is the model from which we\n448\nT able 1: Offensive Identiﬁcation scores on the best model selected.\nCamemBER T FlauBER T BAR Thez BERT weetFR\nAccuracy 86.47 86.87 84.35 88.07\nF1 Score 68.89 65.20 67.51 71.27\nT able 2: NER score on the best model selected.\nCamemBER T FlauBER T BERT weetFR\nAccuracy 94.78 94.73 94.99\nF1 Score 61.01 60.57 62.77\nstarted our ﬁne-tuning for domain adaptation. It\ntherefore serves as a natural baseline. Its architec-\nture is already introduced in 2.2.\nFlauBERT FlauBERT ( Le et al. , 2020) is an-\nother transfomer-based model trained on a very\nlarge and heterogeneous French corpus. It basically\nfollows the same architecture as CamemBER T and\nis shown to outperform CamemBER T on some of\nthe downstream tasks.\nBARThez\nBAR Thez (Eddine et al. , 2020) is the\nﬁrst French sequence-to-sequence model based on\nthe base version of the BART architecture ( Lewis\net al. , 2020). It has 6 encoder and 6 decoder layers\nwith 768 hidden dimensions and 12 attention heads\nin both the encoder and the decoder. It is shown to\nbe competitive in comparison with CamemBERT\nand FlauBER T .\n5 Experiments and Results\nIn this section, we ﬁrst describe our ﬁne-tuning\napproaches. The baseline models follow the same\nﬁne-tuning procedures as BERT weetFR. W e then\nreport our results and compare with the baselines.\nAs a result, our model substantially outperforms all\nbaselines.\n5.1 Offensive Language Identiﬁcation\nThe offensive language identiﬁcation task is a su-\npervised sequence classiﬁcation task. Following\n(Devlin et al. , 2019), we append a linear prediction\nlayer on top of the pooled output.\nFor ﬁne-tuning, we employ transformers library\nto train BERT weetFR on the training set for 15\nepochs. W e use AdamW (\nLoshchilov and Hutter ,\n2019) with a ﬁxed learning rate of 2.e-5 and a batch\nsize of 32 following ( Liu et al. , 2019). W e compute\nthe classiﬁcation accuracy and F1 score after each\ntraining epoch on the validation set, applying early\nstopping if their is no improvement after 3 con-\nsecutive epochs. W e eventually select the model\ncheckpoint with the highest F1 score to predict the\nﬁnal labels on the test set. Our results are listed in\nT able 1.\n5.2 Named Entity Recognition\nThe NER task is a supervised token classiﬁcation\ntask. Following ( Devlin et al. , 2019), we append\na linear prediction layer on top of the last Trans-\nformer layer with regards to the ﬁrst subword of\neach word token.\nFor ﬁne-tuning, we again employ transformers\nlibrary to train for 30 epochs. W e use AdamW\n(Loshchilov and Hutter , 2019) with a ﬁxed learn-\ning rate of 2.e-5 and a batch size of 32, adding in\nweight decay . W e compute performance scores for\neach entity class as well the overall F1 Micro score.\nW e eventually select the model checkpoint with the\nhighest F1 Micro score to predict the ﬁnal labels\non the test set. Our results are listed in T able 2. W e\ndo not compare with BAR Thez in this task because\nthe original model is not implemented for sequence\nclassiﬁcation tasks.\n6 Conclusion\nIn this work, we investigated the effectiveness of\napplying domain adaptation to the T witter domain\nfor large-scale pre-trained French language mod-\nels. W e demonstrate the value of our model show-\ning that it outperforms all previous general-domain\nFrench language models on two downstream T wit-\nter NLP tasks of offensiveness identiﬁcation and\nnamed entity recognition.\nOur contributions are as follows :\n• W e train and release the ﬁrst large-scale pre-\ntrained language model for French tweets :\nBER T weetFR. W e make it publicly available\nin the transformers library and hope that it\n449\ncan facilitate and promote future research in\nanalytic tasks for French tweets.\n• W e create and annotate the ﬁrst dataset for\noffensiveness identiﬁcation in French tweets.\nSuch datasets already exist in several other\nlanguages and our effort ﬁlls in the gap for the\nFrench language.\n• W e create a framework and baseline for eval-\nuating language models for French tweets.\nDatasets for T witter tasks in French is very\nscarce and no previous work has ever com-\nbined different analytic tasks together in a\nuniﬁed framework.\nWith around 70% of all T witter posts being in\nnon-English languages, the lack of corresponding\nlanguage models strongly hinders the community\nfrom exploiting the information contained in these\nvaluable resources. For future work, we plan to\ntrain and release a series of such pre-trained lan-\nguage models for tweets in other low resource lan-\nguages. W e also call upon researchers from all over\nthe world involved in natural language processing\nfor social media to adapt language models in their\nrespective languages and make them publicly avail-\nable.\nAcknowledgements\nThis research is supported by the French National\nresearch agency (ANR) via the ANR XCOVIF\n(AAP RA-COVID-19 V6) project. W e would also\nlike to thank the National Center for Scientiﬁc Re-\nsearch (CNRS) for giving us access to their Jean\nZay supercomputer.\nReferences\nJacob Levy Abitbol, Márton Karsai, Jean-Philippe\nMagué, Jean-Pierre Chevrot, and Eric Fleury . 2018.\nSocioeconomic dependencies of linguistic patterns\nin twitter: A multivariate analysis\n. In Proceedings\nof the 2018 W orld Wide W eb Conference, WWW ’18,\npage 1125–1134, Republic and Canton of Geneva,\nCHE. International W orld Wide W eb Conferences\nSteering Committee.\nAndrew M Dai and Quoc V Le. 2015.\nSemi-supervised\nsequence learning . In Advances in Neural Informa-\ntion Processing Systems , volume 28. Curran Asso-\nciates, Inc.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding\n. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short P apers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMoussa Kamal Eddine, Antoine J-P Tixier, and\nMichalis V azirgiannis. 2020. Barthez: a skilled pre-\ntrained french sequence-to-sequence model. arXiv\npreprint arXiv:2010.12321.\nAnna Atefeh Farzindar and Diana Inkpen. 2020. Nat-\nural language processing for social media. Syn-\nthesis Lectures on Human Language T echnologies ,\n13(2):1–219.\nNorjihan Abdul Ghani, Suraya Hamid, Ibrahim\nAbaker T argio Hashem, and Ejaz Ahmed. 2019. So-\ncial media big data analytics: A survey . Computers\nin Human Behavior, 101:417–428.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy , Doug Downey ,\nand Noah A. Smith. 2020.\nDon’t stop pretraining:\nAdapt language models to domains and tasks . In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nDiederik P . Kingma and Jimmy Ba. 2015.\nAdam: A\nmethod for stochastic optimization . In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nT aku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates . In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (V olume 1: Long P apers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nT aku Kudo and John Richardson. 2018.\nSentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing . In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nHang Le, Loïc V ial, Jibril Frej, V incent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020.\nFlauBER T: Unsupervised language\nmodel pre-training for French . In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\n450\nLevy , V eselin Stoyanov , and Luke Zettlemoyer.\n2020.\nBAR T: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension\n. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy , Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov . 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nCédric Lopez, Ioannis Partalas, Georgios Balikas, Na-\ndia Derbas, Amélie Martin, Coralie Reutenauer,\nFrédérique Segond, and Massih-Reza Amini. 2017.\nCap 2017 challenge: T witter named entity recogni-\ntion. arXiv preprint arXiv:1707.07568.\nIlya Loshchilov and Frank Hutter. 2019.\nDecou-\npled weight decay regularization . In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview .net.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Y oann Dupont, Laurent Romary , Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020.\nCamemBER T: a tasty French language model .\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nT omas Mikolov , Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity\n. In Advances in Neural Information Processing\nSystems, volume 26. Curran Associates, Inc.\nDat Quoc Nguyen, Thanh V u, and Anh Tuan Nguyen.\n2020. BER Tweet: A pre-trained language model\nfor English tweets . In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages 9–\n14, Online. Association for Computational Linguis-\ntics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\nUnsupervised pretraining for sequence to sequence\nlearning\n. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 383–391, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nY usuke Shibata, T akuya Kida, Shuichi Fukamachi,\nMasayuki T akeda, A yumi Shinohara, T akeshi Shino-\nhara, and Setsuo Arikawa. 1999. Byte pair encoding:\nA text compression scheme that accelerates pattern\nmatching.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017.\nAttention is all\nyou need . In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas W olf, Lysandre Debut, V ictor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Y acine Jernite, Julien Plu, Canwen Xu,\nT even Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020.\nTrans-\nformers: State-of-the-art natural language process-\ning\n. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8341989517211914
    },
    {
      "name": "Domain adaptation",
      "score": 0.7065085172653198
    },
    {
      "name": "Natural language processing",
      "score": 0.6775383949279785
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6414234638214111
    },
    {
      "name": "Language model",
      "score": 0.6393563747406006
    },
    {
      "name": "Transformer",
      "score": 0.6237043738365173
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5936911702156067
    },
    {
      "name": "Task (project management)",
      "score": 0.5665251612663269
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5492210984230042
    },
    {
      "name": "Architecture",
      "score": 0.4849579334259033
    },
    {
      "name": "Identification (biology)",
      "score": 0.43476077914237976
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210139461",
      "name": "Laboratoire d'Informatique de l'École Polytechnique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I142476485",
      "name": "École Polytechnique",
      "country": "FR"
    }
  ],
  "cited_by": 16
}