{
  "title": "LAMOL: LAnguage MOdeling for Lifelong Language Learning",
  "url": "https://openalex.org/W2995409942",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282214639",
      "name": "Sun, Fan-Keng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ho, Cheng-Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221356874",
      "name": "Lee, Hung-Yi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2771964490",
    "https://openalex.org/W2901159965",
    "https://openalex.org/W2767245334",
    "https://openalex.org/W2983826605",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2178031510",
    "https://openalex.org/W2583761661",
    "https://openalex.org/W2737492962",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W2777054756",
    "https://openalex.org/W2804580284",
    "https://openalex.org/W2426267443",
    "https://openalex.org/W2963072899",
    "https://openalex.org/W2963588172",
    "https://openalex.org/W2618767506",
    "https://openalex.org/W2963850662",
    "https://openalex.org/W2565989828",
    "https://openalex.org/W2605043629",
    "https://openalex.org/W2963209029",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W2740765036",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2340944142",
    "https://openalex.org/W2962724315",
    "https://openalex.org/W2962783425",
    "https://openalex.org/W2952677972",
    "https://openalex.org/W2907085143",
    "https://openalex.org/W2948743095",
    "https://openalex.org/W2427527485"
  ],
  "abstract": "Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound. The source code is available at https://github.com/jojotenya/LAMOL.",
  "full_text": "Published as a conference paper at ICLR 2020\nLAMOL: LA NGUAGE MODELING FOR\nLIFELONG LANGUAGE LEARNING\nFan-Keng Sun∗†\nMIT\nCambridge, MA, USA\nfankeng@mit.edu\nCheng-Hao Ho∗\nNational Taiwan University\nTaipei, Taiwan\njojotenya@gmail.com\nHung-Yi Lee\nNational Taiwan University\nTaipei, Taiwan\nhungyilee@ntu.edu.tw\nABSTRACT\nMost research on lifelong learning applies to images or games, but not language.\nWe present LAMOL, a simple yet effective method for lifelong language learning\n(LLL) based on language modeling. LAMOL replays pseudo-samples of pre-\nvious tasks while requiring no extra memory or model capacity. Speciﬁcally,\nLAMOL is a language model that simultaneously learns to solve the tasks and\ngenerate training samples. When the model is trained for a new task, it gen-\nerates pseudo-samples of previous tasks for training alongside data for the new\ntask. The results show that LAMOL prevents catastrophic forgetting without\nany sign of intransigence and can perform ﬁve very different language tasks se-\nquentially with only one model. Overall, LAMOL outperforms previous meth-\nods by a considerable margin and is only 2–3% worse than multitasking, which\nis usually considered the LLL upper bound. The source code is available at\nhttps://github.com/jojotenya/LAMOL.\n1 I NTRODUCTION\nThe current dominant paradigm for machine learning is to run an algorithm on a given dataset to\nproduce a trained model speciﬁcally for a particular purpose; this is isolated learning (Chen & Liu,\n2016, p. 150). In isolated learning, the model is unable to retain and accumulate the knowledge it\nhas learned before. When a stream of tasks are joined to be trained sequentially, isolated learning\nfaces catastrophic forgetting (McCloskey & Cohen, 1989) due to a non-stationary data distribution\nthat biases the model (left ﬁgure of Figure 1). In contrast, lifelong learning is designed to address a\nstream of tasks by accumulating interconnected knowledge between learned tasks and retaining the\nperformance of those tasks. A human easily achieves lifelong learning, but this is nontrivial for a\nmachine; thus lifelong learning is a vital step toward artiﬁcial general intelligence.\nIn this paper, we focus on lifelong language learning, where a machine achieves lifelong learning\non a stream of natural language processing (NLP) tasks. To the best of our knowledge, lifelong\nlanguage learning has been studied in only a few instances; for sentiment analysis (Chen et al.,\n2015b; Xia et al., 2017), conversational agents (Lee, 2017), word representation learning (Xu et al.,\n2018), sentence representation learning (Liu et al., 2019), text classiﬁcation, and question answer-\ning (d’Autume et al., 2019). However, in all previous work, the tasks in the stream are essentially\nthe same task but in different domains. To achieve lifelong language learning on fundamentally\ndifferent tasks, we propose LAMOL — LAnguage MOdeling for Lifelong language learning.\nIt has been shown that many NLP tasks can be considered question answering (QA) (Bryan McCann\n& Socher, 2018). Therefore, we address multiple NLP tasks with a single model by training a\nlanguage model (LM) that generates an answer based on the context and the question. Treating QA\nas language modeling is beneﬁcial because the LM can be pre-trained on a large number of sentences\nwithout any labeling (Radford et al., 2019); however, this does not directly solve the problem of LLL.\nIf we train an LM on a stream of tasks, catastrophic forgetting still occurs. However, as an LM is\nintrinsically a text generator, we can use it to answer questions while generating pseudo-samples of\n∗Equal contribution.\n†Work done while at National Taiwan University.\n1\narXiv:1909.03329v2  [cs.CL]  23 Dec 2019\nPublished as a conference paper at ICLR 2020\nFigure 1: Left: After learning Task 2, the learner has already forgetten how to solve Task 1. This\nis “catastrophic forgetting”. Middle: The basic idea of the data-based LLL approach. A generator\nis learned to generate examples it has seen before. Using the generator, the learner also learns\nfrom examples from the previous task to prevent it from forgetting. Right: A language model that\nsimultaneously takes on the roles of learner and generator.\nthe previous task to be replayed later. LAMOL is inspired by the data-based approach for LLL in\nwhich a generator learns to generate samples in previous tasks (middle of Figure 1) (Hanul Shin &\nKim, 2017; Kemker & Kanan, 2017). In contrast to previous approaches, LAMOL needs no extra\ngenerator (right of Figure 1). LAMOL is also similar to multitask training, but the model itself\ngenerates data from previous tasks instead of using real data.\nOur main contributions in this paper are:\n• We present LAMOL, a simple yet effective method for LLL. Our method has the advantages of\nno requirements in terms of extra memory or model capacity. We also do not need to know how\nmany tasks to train in advance and can always train on additional tasks when needed.\n• Experimental results show that our methods outperform baselines and other state-of-the-art meth-\nods by a considerable margin and approaches the multitasking upper bound within 2–3%.\n• Furthermore, we propose adding task-speciﬁc tokens during pseudo-sample generation to evenly\nsplit the generated samples among all previous tasks. This extension stabilizes LLL and is partic-\nularly useful when training on a large number of tasks.\n• We analyze how different amounts of pseudo-samples affect the ﬁnal performance of LAMOL,\nconsidering results both with and without the task-speciﬁc tokens.\n• We open-source our code to facilitate further LLL research.\n2 R ELATED WORK\nLifelong learning research is based on regularization, architecture, or data. Here is a brief survey of\nworks in these three categories.\n2.1 R EGULARIZATION -BASED METHODS\nIn this approach, a constraint, i.e., a regularization term, is added to minimize deviation from trained\nweights while updating the weights in a new task. Most regularization based methods estimate the\nimportance of each parameter and add the importance as a constraint to the loss function. Elas-\ntic weight consolidation (EWC) (Kirkpatrick et al., 2017) calculates a Fisher information matrix\nto estimate the sensitivity of parameters as importance. Online EWC (Schwarz et al., 2018) is a\ntransformed version of EWC. Instead of tracking the importance of parameters for each task, online\nEWC simply accumulates the importance of the stream of tasks. Synaptic intelligence (SI) (Zenke\net al., 2017) assigns importance to each parameter according to its contribution to the change in the\ntotal loss. Memory aware synapses (MAS) (Aljundi et al., 2018) estimate importance via the gradi-\nents of the model outputs. In contrast to estimating the importance of weights, incremental moment\nmatching (IMM) (Lee et al., 2017) matches the moment of weights between different tasks.\n2\nPublished as a conference paper at ICLR 2020\n2.2 A RCHITECTURE -BASED METHODS\nFor this category, the main idea is to assign a dedicated capacity inside a model for each task. After\ncompleting a task, the weights are frozen and may not be changed thereafter. Some methods allow\nmodels to expand, whereas some ﬁx the size but must allocate capacity for tasks at the beginning.\nProgressive neural networks (Rusu et al., 2016) utilize one column of the neural network per task.\nOnce a new task is trained, progressive neural networks augment a new column of the neural network\nfor the task while freezing the past trained columns. Columns that have been frozen are not allowed\nto change but are connected to the new column to transfer knowledge from old tasks. Towards\nTraining Recurrent Neural Networks for Lifelong Learning (Sodhani et al., 2018) uniﬁes Gradient\nepisodic memory (Lopez-Paz et al., 2017) and Net2Net (Chen et al., 2015a). Using the curriculum-\nbased setting, the model learns the tasks in easy-to-hard order. The model alleviates the forgetting\nproblem by GEM method, and if it fails to learn the current task and has not been expanded yet, the\nmodel will expand to a larger model by the Net2Net approach.\nPathNet (Fernando et al., 2017) reuses subsets of a neural network to transfer knowledge between\ntasks. Unlike progressive neural networks, PathNet does not allow the model to expand. Instead,\nit builds a huge ﬁxed-size model composed of a neural network and paths between different layers\nof the neural networks. While training a task, it selects the best combination of neural networks\nand paths for that particular task. Similar to progressive neural networks, selected parts are ﬁxed to\nallow only inference and not training. Inspired by network pruning, PackNet (Mallya & Lazebnik,\n2018) prunes and re-trains the network iteratively to pack numerous tasks into a single huge model.\nThis category has some drawbacks. When resources are limited, model expansion is prohibited.\nAlso, some architecture-based methods require the number of tasks in advance to allocate the ca-\npacity for the tasks, which greatly reduces their practicality.\n2.3 D ATA-BASED METHODS\nThis method restricts weights through the data distribution of old tasks. One data-based approach\nkeeps a small amount of real samples from old tasks, and the other distills the knowledge from\nold data and imagines pseudo-data of old tasks later on. While training a new task, the data or\npseudo-data is used to prevent weights from greatly deviating from the previous status.\nGradient episodic memory (GEM) (Lopez-Paz et al., 2017) preserves a subset of real samples from\nprevious tasks. Utilizing these real samples during optimization helps somewhat to constrain pa-\nrameter gradients. Averaged-GEM (A-GEM) (Chaudhry et al., 2018) is a more efﬁcient version of\nGEM which achieves the same or even better performance than the original GEM. Learning without\nforgetting (Li & Hoiem, 2017) minimizes the alteration of shared parameters by recording the out-\nputs from old task modules on data from the new task before updating. Hanul Shin & Kim (2017)\nand Kemker & Kanan (2017) encode data from old tasks into a generative model system. The lat-\nter imitates the dual-memory system of the human brain, in that the model automatically decides\nwhich memory should be consolidated. Both methods replay pseudo-data of previous tasks using\nthe generative model during training.\nd’Autume et al. (2019) investigates the performance of the episodic memory system on NLP\nproblems. It distills the knowledge of previous tasks into episodic memory and replays it afterward.\nThis work evaluates the method on two streams of tasks: question answering and text classiﬁcation.\n3 LAMOL\nA pre-trained LM can generate a coherent sequence of text given a context. Thus, we propose\nLAMOL, a method of training a single LM that learns not only to answer the question given the\ncontext but also to generate the context, the question, and the answer given a generation token. That\nis, in LAMOL, a model plays the role of both LM and QA model. Hence, answering questions and\ngenerating pseudo-old samples can both be done by a single model. During LLL, these pseudo-old\nsamples are trained with new samples from new tasks to help mitigate catastrophic forgetting.\n3\nPublished as a conference paper at ICLR 2020\nFigure 2: Upper: LM learns to answer question given context. Lower: LM learns to generate\ntraining samples given generation token GEN.\n3.1 D ATA FORMATTING\nInspired by the protocol used by decaNLP (Bryan McCann & Socher, 2018), samples from the\ndatasets we used are framed into a SQuAD-like scheme, which consists of context, question, and\nanswer. Although the LM is simultaneously a QA model, the data format depends on the training\nobjective. When training as a QA model, the LM learns to decode the answer after reading the\ncontext and question. On the other hand, when training as an LM, the LM learns to decode all three\nparts given a generation token.\nIn addition to context, question, and answer, we add three special tokens:\nANS Inserted between question and answer. As the context and question are known during infer-\nence, decoding starts after inputting ANS.\nEOS The last token of every example. Decoding stops when EOS is encountered.\nGEN The ﬁrst token during pseudo-sample generation. Decoding starts after inputting GEN.\nThe data formats for QA and LM training are shown in Figure 2.\n3.2 T RAINING\nAssume a stream of tasks {T1,T2,... }, where the number of tasks may be unknown. Directly\ntraining the LM on these tasks sequentially results in catastrophic forgetting. Thus, before beginning\ntraining on a new taskTi,i> 1, the model ﬁrst generates pseudo samples T\n′\ni by top-ksampling that\nrepresent the data distribution of previous tasks T1,...,T i−1. Then, the LM trains on the mixture\nof Ti and T\n′\ni. To balance the ratio between |Ti|and |T\n′\ni|, the LM generates γ|Ti|pseudo samples,\nwhere |Ti|denotes the number of samples in task Ti and γ is the sampling ratio. If the generated\nsample does not have exactly one ANS in it, then the sample is discarded. This happens in only\n0.5%-1% of generated samples.\nDuring training, each sample is formatted into both the QA format and the LM format. Then, in the\nsame optimization step, both formats are fed into the LM to minimize the QA lossLQA and LM loss\nLLM together. Overall, the loss is L= LQA + λLLM, where λis the weight of the LM loss.\n3.3 T ASK -SPECIFIC TOKENS\nUsing the same GEN token for all tasks is problematic when training for many tasks because the\nportion of old tasks decreases exponentially in theory. For instance, if γ = 0.01, then the portion of\nthe ﬁrst task when training the second task is about 1%, but is only about 0.01% when training the\nthird task. This issue is deﬁnitely harmful to LLL. To mitigate this, we can choose to replace the\nGEN token with a task-speciﬁc token for each task to inform the model to generate pseudo-samples\nbelonging to the speciﬁc task. Under this setup, all previous tasks have the same share of the γ|Ti|\ngenerated pseudo samples. That is, when beginning training for thei-th task Ti, we generate γ\ni−1 |Ti|\n4\nPublished as a conference paper at ICLR 2020\nTask Dataset # Train # Test Metric\nQuestion answering SQuAD 87599 10570 nF1\nSemantic parsing WikiSQL 56355 15878 lfEM\nSentiment analysis SST 6920 1821 EM\nSemantic role labeling QA-SRL 6414 2201 nF1\nGoal-oriented dialogue WOZ 2536 1646 dsEM\nText classiﬁcation\nAGNews\n115000 7600 EM\nAmazon\nDBPedia\nYahoo\nYelp\nTable 1: Summary of tasks, datasets, dataset sizes, and their corresponding metrics. As this work\nuses no development set, only the training and test datasets are shown. nF1 is the normalized version\nof the F1 score; EM represents an exact match between texts: for text classiﬁcation, this amounts to\naccuracy; for WOZ, it is equivalent to dfEM (turn-based dialogue state exact match); for WikiSQL,\nit is equivalent to lfEM (exact match of logical forms).\nSQuAD WikiSQL SST SRL WOZ AGNews Amazon DBPedia Yahoo Yelp\nGPT-2 score 72.3 70.7 90.9 70.4 84.9 94.6 62.3 99.1 73.9 67.7\nOther scores 75.5 72.6 88.1 75.2 84.4 93.8 60.1 30.5 68.6 50.7\nTable 2: Comparison of GPT-2 and other methods on single task scores. Other scores are retrieved\nfrom Bryan McCann & Socher (2018) or d’Autume et al. (2019). Better performance in boldface.\nfor the previous i−1 tasks. Note that as each task uses a speciﬁc token, the vocabulary size and the\nembedding weight of the LM increase slightly as more tasks are trained.\n4 E XPERIMENT SETUP\n4.1 T ASKS , DATASETS , AND METRICS\nWe collect ﬁve disparate tasks mentioned in decaNLP (Bryan McCann & Socher, 2018): question\nanswering, semantic parsing, sentiment analysis, semantic role labeling, and goal-oriented dialogue,\nwith a dataset for each task.\nFurthermore, to compare our method with d’Autume et al. (2019), we conducted experiments on four\ntext classiﬁcation tasks: news classiﬁcation, sentiment analysis, Wikipedia article classiﬁcation, and\nquestion-and-answer categorization with ﬁve datasets. We use the procedure from d’Autume et al.\n(2019) to produce equal-sized datasets.\nWe do not train on all datasets from both papers due to a lack of computational resources. For each\ntask, there is a corresponding evaluation metric. Table 1 contains a summary of tasks, datasets,\nand metrics. Additional details are provided in Appendix A. Note that the score of any metric lies\nbetween 0 and 100%.\n4.2 M ETHODS TO BE COMPARED\nAll methods use the smallest pre-trained GPT-2 model (Radford et al., 2019)1 as the LM. Each task\nis trained for nine epochs; greedy decoding is applied during inference.\n• LAMOL In all experiments, k = 20 in top-ksampling and λ = 0.25 for weight of the LM loss\nare set. LAMOL γ\nGEN denotes LAMOL with a sampling ratio of γ, and the same GEN token is\nused for all tasks. If the task-speciﬁc tokens are used, GEN is replaced by TASK.\n• Keep real data Pseudo-samples are replaced by real samples from previous tasks. The quantity\nof real samples is equally split between previous tasks. This approach can be considered the upper\nbound of LAMOL. We denote it as LAMOLγ\nREAL.\n1https://github.com/huggingface/pytorch-transformers\n5\nPublished as a conference paper at ICLR 2020\nMethods SST SRL WOZ SST WOZ SRL SRL SST WOZ SRL WOZ SST WOZ SST SRL WOZ SRL SST Average Std\nFine-tuned 50.2 24.7 62.9 31.3 32.8 33.9 39.3 12\nEWC 50.6 48.4 64.7 35.5 43.9 39.0 47.0 8.7\nMAS 36.5 45.3 56.6 31.0 49.7 30.8 41.6 8.9\nGEM 50.4 29.8 63.3 32.6 44.1 36.3 42.8 11\nLAMOL0\nGEN 46.5 36.6 56.6 38.6 44.9 45.2 44.8 6.0\nLAMOL0.05\nGEN 79.6 78.9 73.1 73.7 68.6 75.7 74.9 3.4\nLAMOL0.2\nGEN 80.0 80.7 79.6 78.7 78.4 80.5 79.7 0.8\nLAMOL0\nTASK 41.0 33.5 50.1 41.9 49.3 41.5 42.9 5.2\nLAMOL0.05\nTASK 77.3 76.9 78.1 74.7 73.4 75.8 76.0 1.5\nLAMOL0.2\nTASK 79.4 79.9 80.1 78.7 79.8 79.0 79.5 0.5\nLAMOL0.05\nREAL 81.0 78.9 80.1 80.9 77.7 78.0 79.4 1.2\nLAMOL0.02\nREAL 81.8 80.6 81.6 81.2 80.4 80.5 81.0 0.5\nMultitasked 81.5\nTable 3: Summary of averaged metric scores for different methods under permuted task orders\nusing models at last epoch of last task. The Average and Std columns respectively are the average\nand standard deviation of the averaged scores for each row of the methods. Multitasked learning as\nan upper bound is shown at the bottom.\nFine-tuned MAS LAMOL0.05GEN LAMOL0.2GEN LAMOL0.05TASK LAMOL0.2TASK LAMOL0.05REAL LAMOL0.2REAL Multitasked\n51.5 49.5 69.6 73.1 71.5 74.3 74.5 76.0 76.6\nTable 4: Summary of averaged score on ﬁve tasks. The scores are reported as the averaged score\nover all tasks of the models after training on every task. The rightmost three columns – LAMOL\nwith γ = 0.05 and γ = 0.2 of real samples from previous tasks and Multitasked – are upper bounds\nfor comparison. Best performance in boldface.\n• Fine-tune The model is directly ﬁne-tuned on the stream of tasks, one after another.\n• Multitask learning All tasks are trained simultaneously. Multitask learning is often seen as an\nupper bound of lifelong learning. In addition, it is also used to determine whether forgetting is\ncaused by a lack of model capacity.\n• Regularization-based methods Online EWC (Schwarz et al., 2018) and MAS (Aljundi et al.,\n2018) are compared. They are chosen because they are more computationally efﬁcient than\nSI (Zenke et al., 2017) and more memory efﬁcient than IMM (Lee et al., 2017). Additionally,\nexperiments such as Elhoseiny et al. (2018) show that MAS has better performance overall.\n• Gradient Episodic Memory (GEM) When training each task, we randomly sample data from\nprevious task with the amount equivalent to 5% of the current task size into the memory. In each\noptimization step, the GEM (Lopez-Paz et al., 2017) approach retrieves all the data in the memory\nto calculate the gradients for the previous tasks.\n• Improved memory-based parameter adaptation (MBPA++) Sparse experience replay and lo-\ncal adaptation for LLL as proposed in d’Autume et al. (2019). We also re-implement the paper\nand report better scores using different hyperparameters.\n5 E XPERIMENTAL RESULTS\n5.1 S INGLE TASK\nTo establish a reference on the capability of the GPT-2 model on every dataset, we trained the model\non each dataset independently. The results are shown in Table 2. We observe that the performance\nof the GPT-2 model is actually quite good, even beating the BERT-based model (d’Autume et al.,\n2019) on text classiﬁcation datasets by a large margin. Thus, the GPT-2 model has the potential for\nsuperior LLL performance, as long as we can prevent catastrophic forgetting.\n6\nPublished as a conference paper at ICLR 2020\n5.2 SST, QA-SRL, AND WOZ TASKS\nFor an initial understanding of the performance on all of the methods and the effect of task order,\nwe ﬁrst conducted a small-scale experiment on three small datasets: SST, QA-SRL, and WOZ. We\ntrained all but the the multitasked method on all six permutations of the task order. The ﬁnal score\nfor each order was obtained by evaluating the model at the conclusion of the training process. The\nresults are shown in Table 3; we make several observations. Note that LAMOL with γ = 0 is not\nthe same as Fine-tuned, as the LM loss is still optimized.\n• Fine-tuned, EWC, MAS, and LAMOL withγ = 0show similar performance and are much worse\nthan LAMOL with γ >0.\n• LAMOL0.2\nGEN, our best performing method, is only 1.8 percent away from Multitasked, which\nimplies almost no forgetting during LLL.\n• The order of the tasks is crucial to the performance. For instance, the WOZ score drops signif-\nicantly after training other tasks. Thus, if WOZ is not the last task, the performance is usually\nnoticeably worse.\n• When using LAMOL, the performance of old tasks maintains almost the same level throughout\nthe training process. When the sampling ratio γ is increased, the performance also increases,\nespecially when increased from 0 to 0.05.\n• When γ = 0, adding task-speciﬁc tokens harms performance, because the model must ﬁt addi-\ntional special tokens that are useless. Adding task-speciﬁc tokens is also not helpful if γ = 0.2.\nWe believe that 0.2 is enough for three tasks; thus task-speciﬁc tokens are redundant. However,\nwhen γ = 0.05, task-speciﬁc tokens are beneﬁcial because the tokens are needed to help retain a\nsubstantial presence of the ﬁrst task when training the third task.\n• We see that a better LLL method usually has a smaller standard deviation, which implies that it is\neffected less by task order. Adding task-speciﬁc tokens also has a stabilizing effect.\nThe complete forgetting progress is illustrated in Appendix B. Clearly, Fine-tuned, EWC, MAS,\nLAMOL0\nGEN, and LAMOL0\nTASK reveal similar patterns. However, the proposed LAMOL withγ >0\ndisplays the ability to retain its learned knowledge. In the case of WOZ →SRL →SST, the WOZ\nscore even increases after training the third task using LAMOL with γ = 0.2.\n5.3 F IVE DECA NLP TASKS\nHere, we train the following ﬁve tasks sequentially: SQuAD, WikiSQL, SST, QA-SRL, and WOZ.\nGiven the limited computing resources, we explore only one task order: from large to small tasks,\naccording to the number of training samples.\nAs shown in Table 4, LAMOL outperforms all baselines by a large margin and on average ap-\nproaches within 2–3% of the multitasked upper bound. Also, as expected, the performance of\nLAMOL improves as the sampling ratio γincreases and task-speciﬁc tokens are used.\nThere is also a gap between our method and the method of keeping real samples. As shown in\nthe table, using real samples is much more sample-efﬁcient, as 5% of real samples beats 20% of\npseudo-samples. This may be due to the less-than-ideal quality of the pseudo-data. The longer\nthe paragraphs are, the harder it is for the model to create high-quality samples. After observing\nthe samples generated when using task-speciﬁc tokens, we discover some “chaos”. That is, some\nexamples generated by the model do not exactly correspond to the task-speciﬁc token. This implies\nthat the task-speciﬁc tokens are sometimes too weak to constrain the model; thus their inﬂuence\nis overshadowed by other tokens. We believe that solving this problem will bring the performance\nwhen using task-speciﬁc tokens closer to using real samples; however, we leave this as future work.\nFigure 3 illustrates the test scores of each method on each task throughout the training. We clearly\nsee that when using LAMOL, the model remembers nearly perfectly.\nWe make several observations:\n• When training SQuAD, QA-SRL has not been trained yet, but the score of QA-SRL is\nalready around 40. Also, when training QA-SRL, the SQuAD score revives if the model\n7\nPublished as a conference paper at ICLR 2020\nFigure 3: Training progress of ﬁve tasks. The\ngraph records the performance of the model\nat each epoch of each task.\nFigure 4: Performance after each epoch under\nﬁve different sampling ratios, with or without task\nspeciﬁc-speciﬁc tokens.\nhas forgotten SQuAD. These two facts imply that SQuAD and SRL are similar tasks, such\nthat the model is capable of transferring knowledge from one to the other.\n•If forward transfer exists, replaying pseudo-data also retains the forward transfer. That is,\nthe QA-SRL score does not drop after training on WikiSQL and SST when LAMOL is\nused but drops signiﬁcantly for other methods.\n•The transferability between SQuAD and QA-SRL is expected. On the other hand, the\ntransferability between WikiSQL and QA-SRL is quite surprising; the WikiSQL score im-\nproves considerably when training on QA-SRL for Fine-tuned and MAS after WikiSQL is\nforgotten during SST training.\n5.4 T EXT CLASSIFICATION TASKS\nWe compared the proposed method against the state-of-the-art MBPA++ proposed in d’Autume\net al. (2019), both by citing their original numbers and also by reproducing their methods. We chose\ntext classiﬁcation as opposed to QA because we believe that LM has more of a disadvantage in text\nclassiﬁcation than in QA. We compared with LAMOL0.2\nTASK due to its good performance and stability.\nFollowing their paper and testing our model on the same four kinds of task orders, the results are\nshown in Table 5.\nOur implementation results in much higher scores than the original ones. However, the proposed\nLAMOL0.2\nTASK still outperforms our implementation of MBPA++.\n5.5 I NFLUENCE OF SAMPLING RATIO γ\nAs the value of γ determines the performance of LLL, we conducted a medium-scale experiment\nto understand the inﬂuence of γ with and without task-speciﬁc tokens. In this experiment we used\n8\nPublished as a conference paper at ICLR 2020\nOrder MBPA++ MBPA++ (our impl.) LAMOL 0.2\nTASK\ni 70.8 74.1 76.7\nii 70.9 74.9 77.2\niii 70.2 73.1 76.1\niv 70.7 74.9 76.1\nAverage 70.7 74.2 76.5\nTable 5: Summary of results on text classiﬁcation tasks using averaged EM score (equivalent to\naveraged accuracy in d’Autume et al. (2019)) of models at last epoch of last task. The four orders\nmirror those in d’Autume et al. (2019). For MBPA++ (our impl.) and LAMOL0.2\nTASK, the results are\naveraged over two runs. Thep-value of pairtedt-test between eight numbers of MBPA++ (our impl.)\nand LAMOL0.2\nTASK is smaller than 1%, which shows that there is signiﬁcant difference. Our imple-\nmentation of MBPA++ is available athttps://github.com/Daikon-Sun/EM-in-LLL .\nWikiSQL (blue color), SST (orange), QA-SRL (green), and WOZ (red), in that training order. The\nresults are shown in Figure 4.\nUnsurprisingly, the less generation done by the model, the more likely the vanishing distribution in\nSection 3 occurs: the model forgets how to generate previous tasks, as the ratio of previous tasks in\nthe total dataset decreases exponentially over time. Models using task-speciﬁc tokens mitigate this\nsomewhat, as demonstrated in the ﬁrst subgraph where the performance of LAMOL 0.03\nTASK is much\nbetter than that of LAMOL0.03\nGEN.\nIn addition, the more samples the model generates, the better the overall performance of the model.\nHowever, this performance gain disappears when the sampling ratio γis around 0.1 to 0.3.\n6 C ONCLUSION\nWe propose LAMOL, a simple yet effective method for LLL based on language modeling. A single\nLM achieves LLL without additional model components and without keeping old examples. More-\nover, any pre-trained LM can be used to leverage a large amount of unlabeled text to improve LLL.\nFinally, more tasks can be added whenever needed.\nACKNOWLEDGEMENT\nThis work was supported by the Ministry of Science and Technology of Taiwan.\nREFERENCES\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.\nMemory aware synapses: Learning what (not) to forget. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pp. 139–154, 2018.\nCaiming Xiong Bryan McCann, Nitish Shirish Keskar and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient\nlifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge\ntransfer. arXiv preprint arXiv:1511.05641, 2015a.\nZhiyuan Chen and Bing Liu. Lifelong Machine Learning. Morgan & Claypool Publishers, 2016.\nZhiyuan Chen, Nianzu Ma, and Bing Liu. Lifelong learning for sentiment classiﬁcation. In Pro-\nceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) ,\n2015b.\n9\nPublished as a conference paper at ICLR 2020\nCyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic\nmemory in lifelong language learning. arXiv preprint arXiv:1906.01076, 2019.\nMohamed Elhoseiny, Francesca Babiloni, Rahaf Aljundi, Marcus Rohrbach, Manohar Paluri, and\nTinne Tuytelaars. Exploring the challenges towards lifelong fact learning. In Asian Conference\non Computer Vision, pp. 66–84. Springer, 2018.\nChrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,\nAlexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super\nneural networks. arXiv preprint arXiv:1701.08734, 2017.\nJaehong Kim Hanul Shin, Jung Kwon Lee and Jiwon Kim. Continual learning with deep generative\nreplay. arXiv preprint arXiv:1705.08690, 2017.\nLuheng He, Kenton Lee, Mike Lewis, and Luke Zettlemoyer. Deep semantic role labeling: What\nworks and whats next. In Proceedings of the 55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 473–483, 2017.\nRonald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning.\narXiv preprint arXiv:1711.10563, 2017.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,\n114(13):3521–3526, 2017.\nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming\ncatastrophic forgetting by incremental moment matching. In Advances in neural information\nprocessing systems, pp. 4652–4662, 2017.\nSungjin Lee. Toward continual learning for conversational agents. In arXiv, 2017.\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\nand machine intelligence, 40(12):2935–2947, 2017.\nTianlin Liu, Lyle Ungar, and Jo ˜ao Sedoc. Continual learning for sentence representations using\nconceptors. In NAACL-HLT, 2019.\nDavid Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural\nInformation Processing Systems, pp. 6467–6476, 2017.\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative\npruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,\npp. 7765–7773, 2018.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.\nElsevier, 1989.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444, 2017.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint\narXiv:1606.04671, 2016.\nJonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-\nwork for continual learning. arXiv preprint arXiv:1805.06370, 2018.\n10\nPublished as a conference paper at ICLR 2020\nShagun Sodhani, Sarath Chandar, and Yoshua Bengio. On training recurrent neural networks for\nlifelong learning. arXiv preprint arXiv:1811.07017, 2018.\nTsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M Rojas-Barahona, Pei-Hao\nSu, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue\nsystem. arXiv preprint arXiv:1604.04562, 2016.\nR. Xia, J. Jiang, and H. He. Distantly supervised lifelong learning for large-scale social media\nsentiment analysis. IEEE Transactions on Affective Computing, 8(4):480–491, 2017.\nYann LeCun Xiang Zhang, Junbo Zhao. Character-level convolutional networks for text classiﬁca-\ntion. arXiv preprint arXiv:1509.01626, 2015.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. Lifelong domain word embedding via meta-learning.\nIn Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, 2018.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3987–\n3995. JMLR. org, 2017.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.\n11\nPublished as a conference paper at ICLR 2020\nA T ASKS , DATASET, AND METRICS\nFive tasks and their corresponding datasets from decaNLP (Bryan McCann & Socher, 2018):\n• Question Answering – Stanford Question Answering Dataset (SQuAD) (Rajpurkar\net al., 2016): This dataset consists of context, questions, and answers. The context is para-\ngraphs from English Wikipedia, and the answers are spans from its corresponding question\nparagraphs. For evaluation, we use the normalized F1 score (nF1), which strips out articles\nand punctuation as in Bryan McCann & Socher (2018). Test datasets in this task are hidden\nfrom the host so that users must upload models to their platform to generate the test results;\ndue to this inconvenience and our many models, we elected to use the development set to\ntest the metric. Note that we do not use the development set in the training process. The\nsize of the training set is 87,599 while that of the development set is 10,570.\n• Semantic Parsing – WikiSQL (Zhong et al., 2017): In this task, normal sentences are\ntranslated into SQL-structured SQL queries. WikiSQL provides logical forms along with\nnatural language utterances. The exact match of the logical forms (lfEM) is used to evaluate\nthe performance. The model outputs are required to be matched the SQL format. Other-\nwise, its won’t get any score. The size of the training set is 56,355; that of the test set is\n15,878.\n• Sentiment Analysis – Stanford Sentiment Treebank (SST, binary version) (Radford\net al., 2017): This dataset consists of movie reviews with its answers, including positive\nand negative binary options. The exact match score is used as the metric. The size of the\ntraining set is 6,920; that of the test set is 1,821.\n• Semantic Role Labeling – QA-SRL (He et al., 2017): QA-SRL is a question answering\nform of the SRL task. The normalized F1 (nF1) score is used. The size of the training set\nis 6,414; that of the test set is 2,201.\n• Goal-Oriented Dialogue – English Wizard of Oz (WOZ) (Wen et al., 2016): WOZ is a\nrestaurant reservation task that provides a predeﬁned ontology of a series of information for\nhelping an agent to make reservations for customers. To keep track of the dialogue state,\nturn-based dialogue state EM (dsEM), which requires the model outputs exactly follow the\ncharacters’ conversation order, is used for judgment. The size of the training set is 2,536;\nthat of the test set is 1,646.\nFour text classiﬁcation tasks and ﬁve datasets from MBPA++ (dAutume et al. 2019):\n• News Classiﬁcation – AGNews: News articles to be classiﬁed into 4 classes.\n• Sentiment Analysis – Yelp and Amazon: Customer reviews and ratings on Yelp and\nAmazon. Both datasets include 5 classes.\n• Wikipedia Article Classiﬁcation – DBPedia: Articles and their corresponding categories\non Wikipedia, including 14 classes.\n• Questions and Answers Categorization – Yahoo: Questions and answers on the Yahoo!\nplatform, including 10 classes.\nThe dataset collected by Xiang Zhang (2015) is available at http://goo.gl/JyCnZq. Given the unbal-\nanced dataset sizes, we randomly sample 115,000 training examples and 7,600 test examples from\nall the datasets per d’Autume et al. (2019). All the tasks use exact match accuracy as the evaluation\nmetric.\n12\nPublished as a conference paper at ICLR 2020\nB O VERVIEW OF THE FORGETTING PROGRESS FOR THREE TASKS\nFigure 5: Overview of the forgetting progress for different methods and permuted orders. The blue\nline indicate the scores of the ﬁrst task after training each task. The orange line corresponds to that\nof the second task.\n13\nPublished as a conference paper at ICLR 2020\nC R EVERSE ORDER OF FIVE DECANLP TASKS\nFine-tuned MAS LAMOL0.05GEN LAMOL0.2GEN LAMOL0.05TASK LAMOL0.2TASK LAMOL0.05REAL LAMOL0.2REAL Multitasked\n45.4 44.7 63.2 73.0 75.3 76.9 75.9 78.2 76.6\nTable 6: Summary of averaged score on reversed ﬁve tasks. The scores are reported as the averaged\nscore over all tasks of the models after training on every task. The rightmost three columns –\nLAMOL with γ = 0.05 and γ = 0.2 of real samples from previous tasks. Best performance in\nboldface.\nFigure 6: Training progress of the ﬁve tasks with reverse order, i.e. small to large. The graph\nrecords the performance of the model at each epoch of each tasks. The order of tasks in the progress\nfollows: WOZ, QA-SRL, SST, WikiSQL, and then SQuAD. From the ﬁgure, we can clearly see that\nFine-tune and MAS completely forget WOZ and WikiSQL.\n14\nPublished as a conference paper at ICLR 2020\nD G ENERATED EXAMPLES\nNormal Examples\nsquad1The United States has been accused of a wide ranging war in Afghanistan since 9 / 11. During the campaign,\nUS forces in Afghanistan were involved in an extensive air campaign. At least 1,600 American servicemen and women were\nkilled, while more than 1,600 civilians were injured. After the US-led invasion of Afghanistan on 12 / 11 / 2001, an estimated\n10,000 American soldiers were killed in combat.What were the targets included in the conﬂict?ans Afghanistan\nsquad1In 1849, the French army was forced to withdraw, and the French were ﬁnally expelled, although it was not until\nlate November that the French recaptured most of their territories. French troops then reached Egypt. On 21 January 1852\n(the year after he left), in Cairo, they captured Tripoli, Benghazi, Benghazi, and the eastern part of Libya. After Gaddaﬁ’s\nreturn to ofﬁce, he established the Gaddaﬁ regime. On 13 February 1856, the Gaddaﬁ family relocated to Egypt. On 13 May\n1857, the army was forced to withdraw from Libya, and the army returned to Benghazi.On whom did Gaddaﬁ’s army return\nto Benghazi?ans Gaddaﬁ’s family\nwikisqlthe table has columns rank, athlete, sport, nation, gold, silver and key words max, min, count, sum, avg, =,>, <,\nop, select, where, and, col, table, caption, page, section, op, cond, question, agg, aggops, condops - - what is the nation when\nthey have the rank 10 and the gold is 6?what is the translation from english to sql?ans select nation from table where rank\n= 10 and gold = 6\nwikisqlthe table has columns home team, home team score, away team, away team score, venue, crowd, date and key\nwords max, min, count, sum, avg, =,>, <, op, select, where, and, col, table, caption, page, section, op, cond, question,\nagg, aggops, condops - - what is the home team’s score when they played at richﬁeld?what is the translation from english to\nsql? ans select home team score from table where venue = richﬁeld\nsst it’s a movie that’s not worth a look.is this review negative or positive?ans negative\nsst it’s an ugly movie about an old man who’s been given power by a new god to bring about a new era.is this review\nnegative or positive?ans negative\nsrl in a statement on his ofﬁcial page, juan valderrama said that the president’s nominees would be appointed by the\npresident and conﬁrmed by the national senate.who might be appointed?ans president\nsrl the ﬁrst two ﬁlms, released on august 21, 1999, are the ﬁrst in a line of ﬁlms that were made by two different people,\neach of whom worked in tandem on the ﬁlm.what was released?ans the ﬁrst two ﬁlms\nExamples of Chaos\nsquad1the table has columns rank ( nation ), gold, silver, bronze, total and key words max, min, count, sum, avg, =,>, <,\nop, select, where, and, col, table, caption, page, section, op, cond, question, agg, aggops, condops - - which total has a gold\nsmaller than 2, and has the lowest rank of bronze of 3?what is the translation from english to sql?ans select min total from\ntable where gold<2 and bronze = 3\nsquad1In addition to the role played by the main characters, the main cast members are played by the same cast members\nwho were in season one of the original series.is this review negative or positive?ans positive\nwikisqlit’s a shame that the story doesn’t end with a good ending, but it certainly means the world to me.is this review\nnegative or positive?ans positive\nwikisqlon the same day, june 19, 2012, former heavyweight champion arthur lang defended his title against alexander\ngreen in the ﬁnal of the world heavyweight championship.what did someone defend?ans his title\nsst at this point, he was only twenty - ﬁve years old at the time of his ﬁrst appearance on the mcf tv series.who was something\ntwenty - ﬁve years old?ans he\nsst on paper, the movie’s two most fascinating characters are two peas in the same class, both of whom have made an\nappearance in this ﬁlm.who did something feature?ans two most fascinating characters\nsrl a popular seasonal ﬁlm in austrian ﬁlms and the most inﬂuential in the history of cinema.is this review negative or\npositive?ans positive\nsrl it’s not a bad ﬁlm, it’s just not as good as you’ve seen it before.is this review negative or positive?ans negative\nTable 7: Examples generated by LAMOL with task-speciﬁc tokens. Annotations squad1 ,\nwikisql , sst , srl correspond to each task-speciﬁc token of SQuAD, WikiSQL, SST, and\nQA-SRL, respectively. ans is the ANS token that separates the question from the answer. The\nupper frame shows the normal situation whereas the lower frame shows generated contents that are\ninconsistent with their task-speciﬁc token.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7959291934967041
    },
    {
      "name": "Forgetting",
      "score": 0.7820478677749634
    },
    {
      "name": "Human multitasking",
      "score": 0.7104101777076721
    },
    {
      "name": "Lifelong learning",
      "score": 0.7000882625579834
    },
    {
      "name": "Task (project management)",
      "score": 0.6941490173339844
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6386640667915344
    },
    {
      "name": "Language model",
      "score": 0.6333196759223938
    },
    {
      "name": "Code (set theory)",
      "score": 0.5808432102203369
    },
    {
      "name": "Natural language processing",
      "score": 0.4848131239414215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.482331246137619
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.45226162672042847
    },
    {
      "name": "Machine learning",
      "score": 0.2785481810569763
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.20173224806785583
    },
    {
      "name": "Programming language",
      "score": 0.18311354517936707
    },
    {
      "name": "Cognitive psychology",
      "score": 0.10155755281448364
    },
    {
      "name": "Psychology",
      "score": 0.09269410371780396
    },
    {
      "name": "Engineering",
      "score": 0.07133686542510986
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}