{
  "title": "ProteinCLIP: enhancing protein language models with natural language",
  "url": "https://openalex.org/W4397043191",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4379666731",
      "name": "Kevin E Wu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2157064815",
      "name": "Howard Chang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2207784945",
      "name": "James Zou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4379666731",
      "name": "Kevin E Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157064815",
      "name": "Howard Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2207784945",
      "name": "James Zou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W4392351837",
    "https://openalex.org/W4213095938",
    "https://openalex.org/W4394763992",
    "https://openalex.org/W4385737488",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4303645584",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4309609816",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4225917625",
    "https://openalex.org/W3199468887",
    "https://openalex.org/W4387819650",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4368361703",
    "https://openalex.org/W3144696998",
    "https://openalex.org/W3129866267",
    "https://openalex.org/W2154139219",
    "https://openalex.org/W2890223884",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W4296293027",
    "https://openalex.org/W4383957026",
    "https://openalex.org/W4392500220",
    "https://openalex.org/W4226343290",
    "https://openalex.org/W4388583690",
    "https://openalex.org/W3022285953",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2899664253",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W4379932151",
    "https://openalex.org/W4391554849",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W1493541064",
    "https://openalex.org/W2144893575",
    "https://openalex.org/W2046200242",
    "https://openalex.org/W2011343696",
    "https://openalex.org/W2079178900",
    "https://openalex.org/W2053937911",
    "https://openalex.org/W2072010276",
    "https://openalex.org/W2412207659",
    "https://openalex.org/W2330198382",
    "https://openalex.org/W2158230102",
    "https://openalex.org/W1760278939",
    "https://openalex.org/W788080004",
    "https://openalex.org/W2134757533",
    "https://openalex.org/W2129736881",
    "https://openalex.org/W2094223039",
    "https://openalex.org/W2048945712",
    "https://openalex.org/W2010053735",
    "https://openalex.org/W2085144875",
    "https://openalex.org/W2623556860",
    "https://openalex.org/W2593070627",
    "https://openalex.org/W2564736581",
    "https://openalex.org/W2344038501",
    "https://openalex.org/W2950629294",
    "https://openalex.org/W2215005393",
    "https://openalex.org/W2782613909",
    "https://openalex.org/W2937251344",
    "https://openalex.org/W2889874867",
    "https://openalex.org/W2092572492",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3035965352",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W2181523240",
    "https://openalex.org/W6969259846",
    "https://openalex.org/W2011301426",
    "https://openalex.org/W3150635270",
    "https://openalex.org/W3099878876",
    "https://openalex.org/W3211728297"
  ],
  "abstract": "Abstract Language models have enabled a new era of biological sequence modeling. However, extracting meaningful sequence-level embeddings from these models remains challenging. In this work, we introduce ProteinCLIP, which applies contrastive learning between a protein’s amino acid sequence and curated text describing its function. ProteinCLIP thus learns to take a pre-trained protein language model’s sequence embedding and refines it produce a function-centric embedding. We show that this embedding space yields sequence representations that enable state-of-the-art performance across a variety of important yet challenging tasks in the study of proteins – from predicting protein protein interactions to accurately detecting homologous proteins despite low sequence similarity. More broadly, ProteinCLIP demonstrates the effectiveness of multi-modal learning in biological contexts, and how such strategies can help isolate key signals from large models and further improve their utility.",
  "full_text": null,
  "topic": "Natural (archaeology)",
  "concepts": [
    {
      "name": "Natural (archaeology)",
      "score": 0.5334729552268982
    },
    {
      "name": "Natural language",
      "score": 0.46878206729888916
    },
    {
      "name": "Computer science",
      "score": 0.44526663422584534
    },
    {
      "name": "Linguistics",
      "score": 0.423183798789978
    },
    {
      "name": "Natural language processing",
      "score": 0.3749064803123474
    },
    {
      "name": "History",
      "score": 0.12896201014518738
    },
    {
      "name": "Philosophy",
      "score": 0.0944681465625763
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}