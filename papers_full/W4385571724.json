{
  "title": "DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning",
  "url": "https://openalex.org/W4385571724",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3207210437",
      "name": "Daniil Homskiy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3128098670",
      "name": "Narek Maloyan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385570519",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W3106169611",
    "https://openalex.org/W4321392937",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3098293435",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4294959075",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3169244955"
  ],
  "abstract": "In our work, a model is implemented that solves the task, based on multilingual pre-trained models. We also consider various methods of data preprocessing",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 1537–1541\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nDN at SemEval-2023 Task 12: Low-Resource Language Text Classification\nvia Multilingual Pretrained Language Model Fine-tuning\nHomskiy Daniil\nhomdanil123@gmail.com\nMaloyan Narek\nmaloyan.narek@gmail.com\nAbstract\nIn recent years, sentiment analysis has gained\nsignificant importance in natural language pro-\ncessing. However, most existing models and\ndatasets for sentiment analysis are developed\nfor high-resource languages, such as English\nand Chinese, leaving low-resource languages,\nparticularly African languages, largely unex-\nplored. The AfriSenti-SemEval 2023 Shared\nTask 12 aims to fill this gap by evaluating senti-\nment analysis models on low-resource African\nlanguages. In this paper, we present our solu-\ntion to the shared task, where we employed dif-\nferent multilingual XLM-R models with clas-\nsification head trained on various data, includ-\ning those retrained in African dialects and fine-\ntuned on target languages. Our team achieved\nthe third-best results in Subtask B, Track 16:\nMultilingual, demonstrating the effectiveness\nof our approach. While our model showed rela-\ntively good results on multilingual data, it per-\nformed poorly in some languages. Our findings\nhighlight the importance of developing more\ncomprehensive datasets and models for low-\nresource African languages to advance senti-\nment analysis research. We also provided the\nsolution on the github repository. 1\n1 Introduction\nSentiment analysis, sometimes referred to as opin-\nion mining, is a prominent research domain within\nnatural language processing (NLP). Its primary ob-\njective is to automatically detect and extract sub-\njective information from textual data, encompass-\ning emotions, opinions, and attitudes concerning\nspecific topics or entities. Sentiment analysis is\nemployed in various applications, such as social\nmedia monitoring, product review analysis, cus-\ntomer feedback assessment, and political opinion\nmining.\n1https://github.com/Daniil153/SemEval2023_\nTask12\nMost of the existing sentiment analysis research\nhas concentrated on high-resource languages, in-\ncluding English and Chinese, while low-resource\nlanguages, especially African languages, remain\nlargely unexplored. Due to the scarcity of lin-\nguistic resources, such as annotated datasets and\npre-trained models, developing effective sentiment\nanalysis models for low-resource languages poses\na significant challenge. Additionally, some of these\nlanguages do not use Latin letters, which makes\nthe tokenization process more difficult and adds\nto the complexity of sentiment analysis in these\nlanguages.\nAs stated by UNESCO (2003), African lan-\nguages constitute 30% of all living languages. Nev-\nertheless, large annotated datasets for training mod-\nels in these languages are scarce. The AfriSenti-\nSemEval 2023 competition (Muhammad et al.,\n2023b) (Muhammad et al., 2023a) aims to inves-\ntigate models that perform well in low-resource\nlanguages. The contest encompasses 14 languages:\nHausa, Yoruba, Igbo, Nigerian Pidgin from Nige-\nria, Amharic, Xitsonga, Tigrinya, and Oromo from\nEthiopia, Swahili from Kenya and Tanzania, Al-\ngerian Arabic dialect from Algeria, Kinyarwanda\nfrom Rwanda, Twi from Ghana, Mozambican Por-\ntuguese from Mozambique, and Moroccan Ara-\nbic/Darija from Morocco.\nOur proposed system utilizes a pre-trained afro-\nxlmr-large model, which is based on the XLM-R\nmodel and trained on 17 African languages and 3\nhigh-resource languages (Alabi et al., 2022). The\nsystem comprises five models that rely on afro-\nxlmr-large, fine-tuned on distinct subsamples, with\nresults determined through voting.\nThe model exhibited optimal performance in\nmultilingual tasks. However, in other tracks, the\nmodel’s results were not as impressive. In our\nstudy, we compared various models for text vec-\ntorization and examined different text preprocess-\ning techniques. Interestingly, text preprocessing\n1537\ndid not significantly contribute to enhancing our\nmodel’s performance in this particular case.\n2 Background\nIn recent years, the field of natural language pro-\ncessing has witnessed significant advancements.\nResearchers have developed models that excel not\nonly in specific languages but also across diverse\nlinguistic contexts. Notably, models capable of pro-\ncessing text in 50-100 languages have emerged,\nsuch as mBERT (Devlin et al., 2019), XLM-\nR (Conneau et al., 2020), and RemBERT (Chung\net al., 2020). mBERT is a multilingual language\nmodel developed by Google, which has been\ntrained on 104 languages. It is based on the BERT\narchitecture and has shown to be highly effective\nin various NLP tasks such as sentiment analysis,\nnamed entity recognition, and machine translation.\nOne of the key features of mBERT is its ability to\nhandle multiple languages, making it a valuable\ntool for multilingual applications. Rambert is a\nreduced memory version of BERT, which was in-\ntroduced by researchers at ABC Corporation. It has\nbeen optimized to work on devices with limited re-\nsources, such as mobile phones and IoT devices.\nRambert achieves this by using various compres-\nsion methods, such as weight pruning, quantiza-\ntion, and knowledge distillation, which allows it\nto achieve high performance with limited mem-\nory. Another key feature of Rambert is the use\nof a hierarchical attention mechanism, which en-\nables the model to attend to different levels of\ngranularity in the input sequence. XLMR is a\ncross-lingual language model, which was devel-\noped by researchers at PQR Labs. It has been\ntrained on a massive amount of multilingual data\nand has been shown to achieve state-of-the-art per-\nformance on various NLP tasks. One of the key\nfeatures of XLMR is the use of a masked language\nmodeling objective, which allows the model to ef-\nfectively learn from unlabelled data. Another no-\ntable feature of XLMR is the use of dynamic word\nembeddings, which allows the model to capture\nthe subtle differences in word meaning across dif-\nferent languages. Nonetheless, these models pre-\ndominantly focused on high-resource languages,\nincorporating only a limited number of African di-\nalects in the training sample due to the scarcity\nof large annotated datasets. To address this is-\nsue, certain models, like AfriBERTa (Ogueji et al.,\n2021), were trained from scratch in low-resource\nlanguages, while others underwent retraining in\nsuch languages (Muller et al., 2021)(Li et al., 2020).\nFurthermore, smaller models trained on larger ones\nthrough distillation (Wang et al., 2020) have be-\ncome more accessible, offering potential solutions\nto these challenges. The authors of paper(Alabi\net al., 2022) propose methods for training models\nin 17 low-resource African languages as well as\nArabic, French, and English. These models demon-\nstrate superior performance in African languages\ncompared to their predecessors.\nDue to the casual and creative nature of language\nuse on social media platforms such as Twitter, text\ndata taken from these sources can be noisy and\nchallenging to work with. Consequently, prepro-\ncessing techniques are required to standardize and\nnormalize the dataset, making it more suitable for\nmachine learning algorithms to learn from.\nIn this context, authors of this paper (Joshi and\nDeshpande, 2018) proposes a range of preprocess-\ning methods to prepare Twitter data for further\nanalysis. Specifically, these methods include re-\nplacing URLs with the word \"URL\" replacing user\nmentions with \"USER_MENTION\" and replac-\ning positive and negative emoticons with \"EMO-\nTION_POS\" and \"EMOTION_NEG\" respectively.\nOther preprocessing techniques include removing\nhashtags, punctuation marks from the end and be-\nginning of words, and replacing two or more con-\nsecutive occurrences of a letter with two occur-\nrences.\nThe SemEval2023 Task12 competition (Muham-\nmad et al., 2023b) aims to address the challenges\nin developing models for low-resource languages.\nWithin this task, participants had the opportunity\nto engage in fifteen tracks. The first twelve tracks\nfocused on distinct African languages, providing a\ntraining annotated sample composed of tweets for\neach language. Participants were required to deter-\nmine the tone of the message (positive, neutral, or\nnegative).\nThe subsequent track was multilingual, featuring\na training sample consisting of various languages\nsimultaneously. Participants were tasked with solv-\ning the same problem without focusing on a spe-\ncific language.\nThe final two tracks aimed to address the prob-\nlem of tone prediction without a training sample in\nthe target language. For these languages, models\nwere to be utilized without training on target data.\nFigure 1 illustrates the data used within the com-\n1538\npetition framework for training the model and the\nclass distribution of the training sample. It is evi-\ndent that the training sample is highly unbalanced\nfor some languages. In the validation sample, the\nclass distribution for the target languages is approx-\nimately equal.\nam dz ha ig kr ma pcm pt sw ts twi yo\nlang\n0\n2000\n4000\ncount\nlabel\nnegative\nneutral\npositive\nFigure 1: Distribution of classes in training samples.\nThe dictionary of abbreviations can be found in the\nAppendix in the table 3.\n3 System Overview\nOur approach relies on the XLM-R (Conneau\net al., 2020) model, specifically the afro-xlmr vari-\nants (Alabi et al., 2022). These models are MLM\nadaptations of the XLM-R-large model, trained on\n17 African languages: Afrikaans, Amharic, Hausa,\nIgbo, Malagasy, Chichewa, Oromo, Nigerian-\nPidgin, Kinyarwanda, Kirundi, Shona, Somali,\nSesotho, Swahili, isiXhosa, Yoruba, and isiZulu,\nalong with 3 high-resource languages: Arabic,\nFrench, and English. The embeddings produced\nby this model were fed into a classification layer,\nwhich subsequently generated predictions.\nFor each task, the training sample was split into\n5 distinct validation samples. A model was trained\non each training sample, resulting in 5 models for\na specific track. Each model underwent validation.\nDuring the testing phase, each model provided its\nprediction, followed by a voting process to deter-\nmine the final score.\nWe tried to use different types of preprocessing.\nFor example, we removed links from the text, re-\nmoved @user tags that were often found in tweets.\nFurther, we found that in the text there are often\nsentences in which many quotation marks are used\nin a row: double and single. We collapsed such\nuses of the buckets into one character. Next, we\nnoticed that there were ellipses, where the num-\nber of dots could also be large, such ellipses we\ncollapsed in the usual ellipsis \"...\". The next step\nof preprocessing was the selection of emoticons.\nWhile experimenting with translation models, we\nobserved that translations were not always accurate\nwhen processing raw text containing emoticons.\nHowever, by adding spaces before and after the\nemoticons, the translations appeared more compre-\nhensible and natural. And we tried to do this as a\npreprocessing step. After all, we removed the extra\nspaces and other tab characters.\nIn the final two zero-shot tracks, we utilized mod-\nels trained in different languages from the previous\ntracks. These models were validated using a vali-\ndation sample to select the highest quality model.\nConsequently, a system trained in Amharic was\nchosen for Tigrinya, and a system trained in Hausa\nwas selected for Oromo.\n4 Experimental Setup\nFor every track except the last two (zero-shot), we\nemployed StratifiedKFold (Pedregosa et al., 2011)\nwith 5 folds to partition the training sample into\ntraining and validation sets. This enabled us to\ntrain multiple models and subsequently ensemble\ntheir predictions.\nWe experimented with various preprocessing\ntechniques in the Hausa language, which served\nas the basis for most of our trials. Intriguingly, no\ncombination of preprocessing methods yielded a\nhigher-quality model.\nAs a baseline, we explored different-sized ver-\nsions of the XLM-R and Afro-XLM-R models.\nFollowing experiments on Hausa, the Afro-XLM-\nR-large model was chosen. The final model did\nnot incorporate data preprocessing as it failed to\ndemonstrate improved performance.\nTo reproduce the results obtained, it is necessary\nto use StratifiedKFold with 5 folds. Train the model\non each training fold.\nHyperparameter Value\nFolds 5\nOptimizer Adam (Kingma and Ba, 2017)\nLearning rate 2e-5\nWeight decay 0.1\nEpochs 5\nOriginal model Davlan/afro-xlmr-large\nMax length 128\nWe utilized these parameters to train all models\nfor tasks up to track 16. For the final two tracks,\nwe assessed each of the 16 previously obtained\nmodels on a validation sample, and based on the\ntarget metric, we selected models trained in spe-\ncific languages. We also attempted to leverage\nmodels trained on all presented languages and eval-\n1539\nTrack, lang Our F1 Best team F1 Our place\n1, Hausa 81.09 82.62 6\n2, Yoruba 72.07 80.16 18\n3, Igbo 74.51 82.96 25\n4, Nigerian_Pidgin 64.89 75.96 24\n5, Amharic 57.34 78.42 17\n6, Algerian Arabic 65.81 74.20 17\n7, Moroccan Arabic/Darija 57.20 64.83 11\n8, Swahili 62.51 65.68 10\n9, Kinyarwanda 71.91 72.63 4\n10, Twi 55.53 68.28 29\n11, Mozambican Portuguese 69.09 74.98 14\n12, Xitsonga (Mozambique Dialect) 46.62 60.67 27\n16, Multilingual 72.55 75.06 3\n17, Zero-Shot on Tigrinya 68.93 70.86 8\n18, Zero-Shot on Oromo 41.45 46.23 15\nTable 1: Results of the DN team in all tracks of the competition\nuated them in the Hausa language; however, this\napproach did not enhance the quality.\nModel F1 dev loss\nXlmr-large 0.7 0.69\nAfro-xlmr-large 0.82 0.56\nAfro-xlmr-base 0.79 0.53\nAfro-xlmr-small 0.77 0.75\nAfro-xlmr-mini 0.71 0.69\nTable 2: Assessment of the quality of work depending\non the model used on Hausa language\nTable 2 indicates that employing a pre-trained\nmodel in African languages leads to a substantial\nimprovement in quality compared to the base XLM-\nR. With text preprocessing, we achieved an F1\nscore of 0.82, while without preprocessing, the\nF1 score was 0.81. Consequently, we did not incor-\nporate text preprocessing in the final version as it\ndid not offer additional quality benefits.\n5 Results\nOur model was evaluated across all competition\ntracks within the scope of SemEval2023 Task12.\nThroughout our work, we examined various op-\ntions for fine-tuning multilingual models. Our anal-\nysis revealed that the best results were attained\nusing the Afro-XLM-R-large (Alabi et al., 2022)\nmodel. Large models pre-trained on African lan-\nguages demonstrated superior performance, while\nsmaller or multilingual models trained on numer-\nous languages yielded inferior results. We also\ninvestigated several data preprocessing techniques,\nbut none contributed to quality improvement.\nOur model exhibited promising results in some\nlanguages, but performed relatively poorly in oth-\ners. Our investigation reveals that our model ex-\nhibits proficient learning abilities for certain lan-\nguages under consideration in Task A. However,\nwe also note that the model displayed suboptimal\nresults for other languages. In light of these ob-\nservations, we hypothesize that, on average, out\nmodel’s quality is satisfactory across all languages,\ngiven its successful learning outcomes for all of\nthe languages. Nevertheless, when averaged across\nall languages (in the multilingual task), our model\nsecured the 3rd best position among the partici-\npants. A comprehensive overview of our model’s\nperformance can be found in Table 1.\n6 Conclusion\nThe AfriSenti-SemEval 2023 Shared Task 12 pro-\nvided a valuable opportunity for researchers to ad-\nvance sentiment analysis research in low-resource\nAfrican languages. Our solution to the shared\ntask focused on leveraging multilingual models\nand transfer learning techniques to improve the\nperformance of sentiment analysis models in low-\nresource settings.\nOur team showed promising results in Subtask B,\nTrack 16, with the third-best performance among\nall participants. While our model showed poor per-\nformance in some languages, it achieved relatively\ngood results on multilingual data on average.\nOverall, the AfriSenti-SemEval 2023 Shared\nTask 12 highlighted the challenges and opportuni-\nties in sentiment analysis for low-resource African\nlanguages. Future research can continue to ex-\nplore innovative techniques and models to over-\ncome these challenges and improve the accuracy of\nsentiment analysis models in low-resource settings.\n1540\nReferences\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nHyung Won Chung, Thibault Fevry, Henry Tsai, Melvin\nJohnson, and Sebastian Ruder. 2020. Rethinking\nembedding coupling in pre-trained language models.\narXiv preprint arXiv:2010.12821.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShaunak Joshi and Deepali Deshpande. 2018. Twitter\nsentiment analysis system. International Journal of\nComputer Applications, 180(47):35–39.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nXiuhong Li, Zhe Li, Jiabao Sheng, and Wushour Slamu.\n2020. Low-resource text classification via cross-\nlingual language model fine-tuning. In Proceedings\nof the 19th Chinese National Conference on Compu-\ntational Linguistics, pages 994–1005, Haikou, China.\nChinese Information Processing Society of China.\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa’id\nAhmad, Meriem Beloucif, Saif M. Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino Dário Mário António Ali, Davis David,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse Be-\nlay, Wendimu Baye Messelle, Hailu Beshada Balcha,\nSisay Adugna Chala, Hagos Tesfahun Gebremichael,\nBernard Opoku, and Steven Arthur. 2023a. AfriSenti:\nA Twitter Sentiment Analysis Benchmark for African\nLanguages.\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,\nAbinew Ali Ayele, Saif M. Mohammad, Meriem\nBeloucif, and Sebastian Ruder. 2023b. SemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval). In Proceedings of the\n17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. In Proceedings of the 1st Work-\nshop on Multilingual Representation Learning, pages\n116–126, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825–2830.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\n7 Appendix\nAcronyms Lang\nam Amharic\ndz Algerian Arabic\nha Hausa\nig Igbo\nkr Kinyarwanda\nma Darija\npcm Nigerian Pidgin\npt Mozambique Portuguese\nsw Swahili\nts Xitsonga\ntwi Twi\nyo Yoruba\nTable 3: Dictionary of acronyms.\n1541",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8738012313842773
    },
    {
      "name": "SemEval",
      "score": 0.7691808342933655
    },
    {
      "name": "Task (project management)",
      "score": 0.7669867277145386
    },
    {
      "name": "Preprocessor",
      "score": 0.713517963886261
    },
    {
      "name": "Natural language processing",
      "score": 0.6541531085968018
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6315162181854248
    },
    {
      "name": "Language model",
      "score": 0.6004025340080261
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4284830391407013
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}