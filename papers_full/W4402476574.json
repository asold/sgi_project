{
    "title": "Assessing the proficiency of large language models in automatic feedback generation: An evaluation study",
    "url": "https://openalex.org/W4402476574",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A1994056853",
            "name": "Wei Dai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2231463661",
            "name": "Yi-Shan Tsai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2980838338",
            "name": "Jionghao Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5107157987",
            "name": "Ahmad Aldino",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102618407",
            "name": "Hua Jin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2297517972",
            "name": "Tongguang Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4214018992",
            "name": "Dragan Gaševic",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2132579758",
            "name": "Guanliang Chen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6990734492",
        "https://openalex.org/W4313262066",
        "https://openalex.org/W2940725132",
        "https://openalex.org/W2969278757",
        "https://openalex.org/W2801630847",
        "https://openalex.org/W2162700863",
        "https://openalex.org/W6852522137",
        "https://openalex.org/W6840713517",
        "https://openalex.org/W2138228303",
        "https://openalex.org/W2560140854",
        "https://openalex.org/W2933483998",
        "https://openalex.org/W6839494081",
        "https://openalex.org/W6856811335",
        "https://openalex.org/W3107855336",
        "https://openalex.org/W2761487768",
        "https://openalex.org/W3121259022",
        "https://openalex.org/W3038648614",
        "https://openalex.org/W6847675181",
        "https://openalex.org/W6841963672",
        "https://openalex.org/W6849198627",
        "https://openalex.org/W3036685446",
        "https://openalex.org/W2164668300",
        "https://openalex.org/W2142995122",
        "https://openalex.org/W6809368653",
        "https://openalex.org/W2905050135",
        "https://openalex.org/W4317790171",
        "https://openalex.org/W6859944451",
        "https://openalex.org/W3156128204",
        "https://openalex.org/W2135995390",
        "https://openalex.org/W2126091572",
        "https://openalex.org/W6858198198",
        "https://openalex.org/W2111497108",
        "https://openalex.org/W4322801648",
        "https://openalex.org/W4386718981",
        "https://openalex.org/W4247987692",
        "https://openalex.org/W2904035102",
        "https://openalex.org/W4245703941",
        "https://openalex.org/W4312563200",
        "https://openalex.org/W4220717796",
        "https://openalex.org/W4214819927",
        "https://openalex.org/W2611176525",
        "https://openalex.org/W4392445612"
    ],
    "abstract": "Assessment feedback is important to student learning. Learning analytics (LA) powered by artificial intelligence exhibits profound potential in helping instructors with the laborious provision of feedback. Inspired by the recent advancements made by Generative Pre-trained Transformer (GPT) models, we conducted a study to examine the extent to which GPT models hold the potential to advance the existing knowledge of LA-supported feedback systems towards improving the efficiency of feedback provision. Therefore, our study explored the ability of two versions of GPT models – i.e., GPT-3.5 (ChatGPT) and GPT-4 – to generate assessment feedback on students' writing assessment tasks, common in higher education, with open-ended topics for a data science-related course. We compared the feedback generated by GPT models (namely GPT-3.5 and GPT-4) with the feedback provided by human instructors in terms of readability, effectiveness (content containing effective feedback components), and reliability (correct assessment on student performance). Results showed that (1) both GPT-3.5 and GPT-4 were able to generate more readable feedback with greater consistency than human instructors, (2) GPT-4 outperformed GPT-3.5 and human instructors in providing feedback containing information about effective feedback dimensions, including feeding-up, feeding-forward, process level, and self-regulation level, and (3) GPT-4 demonstrated higher reliability of feedback compared to GPT-3.5. Based on our findings, we discussed the potential opportunities and challenges of utilising GPT models in assessment feedback generation.",
    "full_text": null
}