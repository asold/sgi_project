{
  "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
  "url": "https://openalex.org/W4285252216",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3034975406",
      "name": "Lianghao Xia",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101072704",
      "name": "Chao Huang",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2096026659",
      "name": "Yong Xu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2126330539",
      "name": "Jian Pei",
      "affiliations": [
        "Simon Fraser University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6692935382",
    "https://openalex.org/W3177890934",
    "https://openalex.org/W2605350416",
    "https://openalex.org/W4224983022",
    "https://openalex.org/W3188604155",
    "https://openalex.org/W2900229157",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W2962992837",
    "https://openalex.org/W2899457523",
    "https://openalex.org/W2963146368",
    "https://openalex.org/W3045200674",
    "https://openalex.org/W3207257408",
    "https://openalex.org/W2964296635",
    "https://openalex.org/W3035669589",
    "https://openalex.org/W2951570486",
    "https://openalex.org/W3206932362",
    "https://openalex.org/W2798385737",
    "https://openalex.org/W3173955760",
    "https://openalex.org/W2798283910",
    "https://openalex.org/W3094605801",
    "https://openalex.org/W3093563174",
    "https://openalex.org/W2945827670",
    "https://openalex.org/W3034329572",
    "https://openalex.org/W3034364571",
    "https://openalex.org/W2965744319",
    "https://openalex.org/W3035666843",
    "https://openalex.org/W3012705926",
    "https://openalex.org/W3035642447",
    "https://openalex.org/W4212931205",
    "https://openalex.org/W2604662567",
    "https://openalex.org/W2951369132",
    "https://openalex.org/W3088203142",
    "https://openalex.org/W2951626319",
    "https://openalex.org/W2949340817",
    "https://openalex.org/W6770756359",
    "https://openalex.org/W2914721378",
    "https://openalex.org/W3044893918",
    "https://openalex.org/W2963707260",
    "https://openalex.org/W6680830989",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W3035588407",
    "https://openalex.org/W2912664727",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W2625746539",
    "https://openalex.org/W3035566692",
    "https://openalex.org/W2911778742",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3012772192",
    "https://openalex.org/W2964051675",
    "https://openalex.org/W2964044287",
    "https://openalex.org/W2809307135",
    "https://openalex.org/W3042770487",
    "https://openalex.org/W3035287707",
    "https://openalex.org/W3065542300",
    "https://openalex.org/W3156622960",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W3035135368",
    "https://openalex.org/W3080152140",
    "https://openalex.org/W2783118243",
    "https://openalex.org/W2997261254",
    "https://openalex.org/W3025937945",
    "https://openalex.org/W6767551140",
    "https://openalex.org/W2807021761",
    "https://openalex.org/W3028156525",
    "https://openalex.org/W3190794503",
    "https://openalex.org/W2941489188",
    "https://openalex.org/W3206531148",
    "https://openalex.org/W2990221466",
    "https://openalex.org/W3100278010",
    "https://openalex.org/W3207682456",
    "https://openalex.org/W3098231197",
    "https://openalex.org/W2982298563",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W3101707147",
    "https://openalex.org/W3104492324",
    "https://openalex.org/W2140310134",
    "https://openalex.org/W3099375322",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W3004578093",
    "https://openalex.org/W3153325943",
    "https://openalex.org/W3166827814",
    "https://openalex.org/W3100480425",
    "https://openalex.org/W3099026360",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3100260481",
    "https://openalex.org/W3100848837"
  ],
  "abstract": "Modeling time-evolving preferences of users with their sequential item\\ninteractions, has attracted increasing attention in many online applications.\\nHence, sequential recommender systems have been developed to learn the dynamic\\nuser interests from the historical interactions for suggesting items. However,\\nthe interaction pattern encoding functions in most existing sequential\\nrecommender systems have focused on single type of user-item interactions. In\\nmany real-life online platforms, user-item interactive behaviors are often\\nmulti-typed (e.g., click, add-to-favorite, purchase) with complex cross-type\\nbehavior inter-dependencies. Learning from informative representations of users\\nand items based on their multi-typed interaction data, is of great importance\\nto accurately characterize the time-evolving user preference. In this work, we\\ntackle the dynamic user-item relation learning with the awareness of\\nmulti-behavior interactive patterns. Towards this end, we propose a new\\nTemporal Graph Transformer (TGT) recommendation framework to jointly capture\\ndynamic short-term and long-range user-item interactive patterns, by exploring\\nthe evolving correlations across different types of behaviors. The new TGT\\nmethod endows the sequential recommendation architecture to distill dedicated\\nknowledge for type-specific behavior relational context and the implicit\\nbehavior dependencies. Experiments on the real-world datasets indicate that our\\nmethod TGT consistently outperforms various state-of-the-art recommendation\\nmethods. Our model implementation codes are available at\\nhttps://github.com/akaxlh/TGT.\\n",
  "full_text": "1\nMulti-Behavior Sequential Recommendation\nwith Temporal Graph Transformer\nLianghao Xia, Chao Huangâˆ—, Y ong Xu, and Jian Pei,Fellow, IEEE\nAbstractâ€”Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in\nmany online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the\nhistorical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential\nrecommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive\nbehaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning\nfrom informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately\ncharacterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of\nmulti-behavior interactive patterns. Towards this end, we propose a new Temporal Graph Transformer (TGT) recommendation\nframework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations\nacross different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated\nknowledge for type-speciï¬c behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets\nindicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation\ncodes are available at https://github.com/akaxlh/TGT.\nIndex Termsâ€”Multi-Behavior Recommendation, Sequential Recommendation, Graph Neural Network\n!\n1 I NTRODUCTION\nPersonalized recommender systems have become increas-\ningly important to alleviate information overload and sat-\nisfy user diverse interests in many online platforms ( e.g.,\nE-commerce sites, online movie and music sites) [9], [17],\n[62]. While many collaborative ï¬ltering methods have been\nproposed to model user-item interactions [12], [59], they\nfocus on the static settings and ignore the dynamic nature\nof userâ€™s time-evolving preferences [26]. Hence, among var-\nious recommendation scenarios, sequential recommender\nsystems are introduced to model dynamic preference of\nusers based on their sequential interaction behaviors [28],\n[36], [40]. The key idea of sequential recommendation mod-\nels is to understand the evolution of userâ€™s preference by\ncapturing temporal dependency of user-item interactions,\nbased on the past observed behaviors.\nWith the advent of deep learning techniques in recent\nyears, a substantial number of approaches have been pro-\nposed to solve the sequential recommendation problem,\nvia utilizing different neural network techniques. For ex-\nample, recurrent neural network-based models propose to\nencode the sequential information from the interacted item\nsequence of each user [13], [32]. In addition, convolutional\nneural networks and attention mechanism also serve as\nâ€¢ L. Xia, C. Huang are with the Department of Computer Science &\nMusketeers Foundation Institute of Data Science, at the University of\nHong Kong, Hong Kong, China. Email: aka xia@foxmail.com, chao-\nhuang75@gmail.com.\nâ€¢ Y. Xu is with the School of Computer Science and Technology, South China\nUniversity of Technology, Guangzhou, China. Email: yxu@scut.edu.cn.\nâ€¢ J. Pei is with School of Computing Science, Simon Fraser University.\nE-mail: jpei@cs.sfu.ca.\nâ€¢ Chao Huang is the corresponding author.\neffective solutions for modeling sequential patterns of item-\nitem transitions. e.g., Caser [37] designs convolution-based\nkernel functions to aggregate information from neighboring\ntime slots. NARM [23] and SASRec [22] develop attentive re-\nlation encoder to capture usersâ€™ general interests with long-\nterm temporal dependency. Inspired by the effectiveness of\ngraph neural networks, several GNN-based methods ( e.g.,\nHyRec [41], MA-GNN [28] and MTD [16]) exploit the user-\nitem graph structure to guide the embedding learning with\nthe incorporation of temporal context of user interactions.\nWhile the aforementioned methods have achieved com-\npelling results, we argue that there is a common deï¬ciency\nin them: most of current frameworks are designed for sin-\ngle behavior type. In practice, the intention of user-item\ninteractions can change over time depending on different\ncontexts [20], [38]. Let us consider an example from online\nretailing platforms: users could view products or tag them\nas favourites if they like, or make ï¬nal purchase if the\nproducts meet their needs [7] (as illustrated in Figure 1).\nHence, user-item interactions are often exhibited with time-\ndependent and behavior diversity in nature. To recommend\nthe future purchase for a user, it is important and beneï¬cial\nto explore not only what he/she has purchased before,\nbut also what products this user has viewed previously or\ntagged them as his/her favorite items [55]. In this work, we\npropose to capture the time-evolving user preference with\nthe modeling of behavior heterogeneity and the underlying\ndependency in a dynamic environment, so as to improve\nthe performance of sequential recommender systems.\nHowever, exploring relation dependencies behind multi-\nbehavior user-item interactions is intrinsically difï¬cult, es-\npecially when the evolution of user interest is incorpo-\nrated into the recommendation framework. Speciï¬cally, we\narXiv:2206.02687v1  [cs.IR]  6 Jun 2022\n2\nFigure 1: Illustration example of sequential\nrecommendation with multi-behavior dynamics. Best\nviewed in color.\nface two key challenges: i) Cross-type Behavior Inter-\ndependency. Different types of user behaviors ( e.g., click,\ntag as favorite, add-to-cart) may offer complementary sig-\nnals for predicting the target behavior ( e.g., purchase) [7],\n[20]. If feature representations are learned from different\nbehavior types separately and then loosely concatenate em-\nbeddings together, it can hardly capture the complex inter-\ndependency across various types of interaction behaviors.\nii) Temporal Multi-behavior Pattern Fusion. Distinct to\nstationary inter-dependent relations, it is challenging to be\nresponsive for the evolution of multi-behavior semantics\nand the underlying cross-type behavior dependency. In\npractical applications, users often interact with items in a\nvariety of behaviors which are inherently correlated, due to\nhis/her specialty at different timestamps. Therefore, to built\neffective multi-behavior sequential recommendation model,\nit requires the careful design to jointly distill the behavior\nheterogeneity and underlying type-dependent patterns.\nIn light of these challenges, we propose a multi-behavior\nsequential recommendation model with T emporal G raph\nTransformer (short for TGT). In particular, to capture the\nshort-term multi-typed interaction patterns of users, we\ndevelop a behavior-aware transformer network to inject the\nbehavior heterogeneous signals into the sequential mod-\neling of item transitions. In this regard, we preserve the\nexpressive multi-behavior characteristics and the changes\nin interaction semantics. To exploit the long-term multi-\nbehavior dependencies, we propose a temporal graph neu-\nral network to infer the latent user representations from\ntheir diversiï¬ed activities on items with arbitrary durations.\nInstead of parameterizing each type of user-item interac-\ntions into separate embedding spaces, our designed multi-\nchannel augmented message passing paradigm allow be-\nhavior of different types to maintain not only their speciï¬c\ntime-aware semantics, but also the behavior type-speciï¬c\ndependent representations with long-range dynamics. Fur-\nthermore, in TGT, we recursively reï¬ne the global-level\nrepresentations over the time-aware user-item interaction\ngraph to capture the dynamic cross-sequence correlations\namong different users. This has been largely overlooked by\nmost of existing sequential recommender systems [28], [36],\n[41] for simplifying the model design.\nWe summarize the contributions of this work as follows:\nâ€¢ This work tackles the multi-behavior sequential recom-\nmendation with the exploration of multi-behavior charac-\nteristics of users from both the short-term and long-term\nperspectives. We show that modeling the dynamic cross-\ntype behavior inter-dependency is essential for improving\nthe recommendation quality.\nâ€¢ We propose a general sequential recommendation model\nTGT to maintain the dedicated representations for dif-\nferent types of user-item interactions over time. In TGT,\nwe adapt the graph neural network along the temporal\ndimension to capture the dynamism of multi-behavior\ninteraction patterns.\nâ€¢ We conduct empirical studies on two real-world datasets\nto demonstrate the superiority of our TGT framework\nwhen competing with various 18 recommendation base-\nlines. Our evaluation also justiï¬es the effectiveness of\ncomponents in our TGT model, as well as the inter-\npretability of the learned multi-behavior dependent rep-\nresentations.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 discusses the related work. In Section 3, we introduce\nthe details of our proposed framework TGT. We report our\nexperimental results that demonstrate the effectiveness of\nthe proposed model in Section 4. Finally, Section 5 concludes\nour study and discusses the future work.\n2 R ELATED WORK\nIn this section, we summarize the relevant research work\nfrom the following research lines: i.e., i) sequential rec-\nommendation, ii) graph-based recommender systems, iii)\nrecommendation with heterogeneous relational context, iv)\nmulti-behavior/intent recommendation.\n2.1 Sequential Recommender Systems\nThe sequential recommendation is usually formulated as\nsequence prediction problem with the modeling of item\ntransitional regularities [19], [42], [64]. Some earlier studies\nare on the basis of Markov Chain to model correlations\nbetween different items, such as factorizing personalized\nMarkov Chains (FPMC) [33] and fusing item similarity\nwith Markov Chains (Fossil) [10]. To tackle the challenge of\nsequential behavior learning, many neural network-based\nmethods have been proposed to capture complex sequential\ninformation [6]. In particular, recurrent neural network is\nutilized as an intuitive way to model temporal dependen-\ncies [13]. Attention mechanisms allows the model to identify\nspeciï¬c parts of input item sequences and preserve long-\nterm dependencies, e.g., NARM [23] and SASRec [22]. For\nexample, SASRec relies on the self-attention mechanism to\nidentify item relevance from user past behaviors [22]. In\naddition, SR-GNN [53], MTD [16] and GCGNN [48] convert\nthe sequential item transitions into graphs and design graph\nneural networks for item representations. However, these\nmodels only consider singular type of sequential behaviors.\nGiven usersâ€™ interacted item sequence, how to effectively\nexploit dynamic multi-behavior knowledge to improve rec-\nommendation accuracy, remains a signiï¬cant challenge.\n2.2 Graph Neural Network for Recommendation\nMotivated by the strength of graph neural network, there\nexist many recently proposed recommendation methods\n3\nmodeling user-item interaction graph via information prop-\nagation between connected user/item nodes [4], [45], [56],\n[61], [66]. For example, Wanget al. [45] and Yinget al. [60] in-\ntroduces the graph convolutional network in modeling col-\nlaborative effects between users and items. The embedding\npropagation is conducted over the constructed user-item\ninteraction graph. Later on, He et al. [11] propose to improve\ntraining efï¬ciency by removing the feature transformation\nand nonlinear activation operations with the light graph\nconvolutional framework. With the self-supervised learning\ntechniques, SGL [50] performs data augmentation on user-\nitem interaction graphs to generate additional supervision\nsignals. HCCF [56] enhances the graph-based collaborative\nï¬ltering with hypergraph-enhanced contrastive learning. In\nthis work, our TGT framework is built upon the dynamic\ngraph neural architecture to capture the high-order con-\nnectivity, with the exploration of evolving multi-behavior\npreference.\n2.3 Recommendation with Heterogeneous Relations\nAnother relevant research line is to consider different types\nof relationships in recommender systems [15], [21]. First,\nwith the emergence of online social networks, social-aware\nrecommendation models have been proposed to jointly\nconsider user-user and user-item relations for better user\nembedding generation [5], [18], [52]. Second, recent studies\nattempt to incorporate knowledge graph relational informa-\ntion as side features to model item correlations [1], [2], [30].\nThird, to further alleviate data sparsity issue, heterogeneous\ngraph learning approaches aims to transfer knowledge from\nboth user and item domains (e.g., social links, item semantic\nrelatedness), with the goal of augmenting the data of user-\nitem pairs [27], [35]. Different from those methods which\nrely on the the side information to construct meta-relations\nbetween users and items, this work focuses on exploiting\nthe dynamic characteristics of multi-behavioral interaction\ndata and validates its positive effects in sequential recom-\nmendation.\n2.4 Multi-Behavior/Intent Recommender Systems\nThere exist some recommendation methods which aim\nto learn the collective knowledge from different types of\nuser-item interactions ( e.g., clicks, purchases) [7], [20], [49],\n[57]. For example, Gao et al . [7] deï¬ne the cascading rela-\ntionship to account for multiple types of user behaviors.\nAIR [3] is an attention-based recurrent neural model to\ncapture the intention-aware sequential patterns. In Inten-\ntion2Basket [43], heterogeneous user intentions are con-\nsidered to encode the dynamic user preference for next-\nbasket planning. Additionally, RIB [65] proposes to learn the\nsequential patterns from the perspective of micro behaviors,\nso as to improve the recommendation performance. In a\nsession-based recommendation scenario, Meng et al . [31]\nintroduces a new recommender system MKM-SR to in-\ntegrate the item knowledge and different types of user\nbehaviors for capturing item transitional patterns under a\nmulti-task learning framework. MATN [54] is an attention-\nbased recommendation method which fuses behavior-aware\npatterns to generate weighed summarized representations.\nAdditionally, based on graph neural networks, Jin et al. [20]\nTable 1: Descriptions of Key Notations\nNotations Description\nui âˆˆU User set\nvj âˆˆV Item set\nbâˆˆB Behavior types (indexed by b)\nSi Multi-behavior interaction sequence\n(vr\nk,tr\nk,br\nk) Interaction instance in the r-th sub-sequence\nevr\nk , ebr\nk , etr\nk Embedding of item, behavior type and time\nÂ¯Er\nk âˆˆRd Embedding of the k-th interacted item\nÎ±k,kâ€² item relevance weight between vr\nk and vr\nkâ€²\nGr User sub-sequence interaction graph\nG Graph over users and their time-aware interactions\nÂ¯Hr âˆˆRd Time-aware multi-behavior context representation\nÂ¯Î“i âˆˆRd Cross-time global user embedding\nuses graph convolutional network to model behavior-aware\ncollaborative signals. CML [49] designs a self-supervised\nlearning method for multi-behavior recommendation. How-\never, those models are designed for static recommendation\nscenarios, and can hardly be adaptive for modeling user-\nitem interactions which are inherently dynamic. To ï¬ll this\ngap, we design a dedicated temporal graph-contextualized\ntransformer network for modeling multi-behavior sequen-\ntial interactions. Additionally, disentangled representation\nlearning has been leveraged to separate embedding into\ndistinct factors [29], [46], [63]. Nevertheless, most of exist-\ning disentangled representation works force the embedding\nseparation with the assumption of implicit factor indepen-\ndence. Different from them, this work exploits the dynamic\ndependency across different types of behavioral interaction\npatterns in an explicit manner.\n3 M ETHODOLOGY\nIn this section, we ï¬rst introduce the problem formulation\nand then elaborate our proposed TGT framework. The\nmodel ï¬‚ow of our method is illustrated in Figure 2. We also\nsummarize key notations of our methodology in Table 1.\n3.1 Task Formulation\nIn our recommendation scenario, we let U =\n{u1,...,u i,...,u I} denote the set of users and\nV = {v1,...,v j,...,v J}denote the set of items. We further\ndeï¬ne B(indexed by b) to represent the types of different\ninteraction behaviors, such as browsing, adding to cart,\ntagging as favorite and purchasing. We differentiate usersâ€™\ninteractions by associating them with the corresponding\nbehavior types (bâˆˆB).\nDeï¬nition 1. Multi-behavior Interaction Sequence\nSi. Given a user ui âˆˆ U, his/her behavior-aware in-\nteraction sequence Si is consisted of individual triple\n(vk,i,b,t ), representing the k-th interacted item (ordered\nby time) under the b-th behavior type at time t, i.e.,\nSi = [(v1,i,t1,i,b1,i),..., (vk,i,tk,i,bk,i),..., (vK,i,tK,i,bK,i)],\nwhere K (indexed by k) is the sequence length K = |Si|.\nIn our multi-behavior sequential recommendation prob-\nlem, we deï¬ne the behavior type we aim to forecast as target\nbehavior (e.g., purchase in online retailing platforms). Other\ntypes of user behaviors ( e.g., page view, tag-as-favorite) are\nconsidered as context behavior for characterizing the pref-\nerence of users over the target behavior. This work aims\n4\n=\nğ‘†1\n1/ğ‘†2\n1 ğ‘†1\n2/ğ‘†2\n2 ğ‘†1\n3/ğ‘†2\n3\nğ‘¢2\n1 ğ‘¢2\n2 ğ‘¢2\n3\nğ‘¢1\n1 ğ‘¢1\n2 ğ‘¢1\n3\nğ‘¢1\nğ‘¢2\nUsers\nâ†“\nSub-users\nSub-users\nâ†‘\nUsers\nBehavior-Aware Context Embedding\nItem-Wise Sequential Dependency\nÃ—\nğ‘‘ğ‘˜\nğœ\nğ‘ ğ‘˜ ğ‘£ğ›¼ =\nğ‘£\nğ›¼\nğ›¼\n=\nğ‘£\nğ‘£\nâ„0\nâ„1\nTemporal Embedding Behavior Embedding\nId Embedding\nMulti-Behavior Pattern Aggregation\nMulti-Behavior Graph\nMulti-Channel Projection\nğ›½ ğ›½ ğ›½ ğ›½\n=\nCross-Type Aggregation\nReLU Activation\nğ›¾ ğ‘™ =2\nğ‘™ =1\nGlobal Relational\nContext Learning\nÃ—=\nGlobal to Local\nLocal to Global\nğœ‚ ğœ‚\n ğ‡(ğ‘™) â†  ğ„(ğ‘™) ğšª(ğ‘™+1) â†  ğ‡(ğ‘™)\n ğ‡(ğ‘™+1) â†ğšª(ğ‘™) ğ„(ğ‘™+1) â†  ğ‡(ğ‘™+1)\nHigh-order Iteration\nTransformation\nItem-Sub-User\nRelations\nFigure 2: Model ï¬‚ow of the proposed TGT. (a) Hierarchical graph structure between user and item with respect to both\nshort-term and long-term multi-behavior relations. (b) Dynamic individual interest modeling with behavior-aware\nsequential context. (c) Multi-behavior pattern aggregation which jointly preserves behavior semantics and cross-type\nbehavior dependency. (d) Global relational context injection with the high-order embedding propagation paradigm.\nto predict the future item that user ui will be interested\nafter the observed interaction sequence Si, with the explo-\nration of multi-behavior sequential patterns and interaction\nrelation heterogeneity. Formally, our studied problem can\nbe formalized as: Input: the past multi-behavior interacted\nitem sequence of each user Si (ui âˆˆU); Output: a learning\nfunction which accurately predicts the interacted item of\neach user after time tK,i.\n3.2 Framework Overview\nOur proposed TGT is an integrative multi-behavior sequen-\ntial recommendation framework which consists of several\nkey learning phases (as shown in Figure 2):\nâ€¢ To capture the behavior-aware sequential dependency\nacross user-item interactions, we design an integrative\nrelational learning framework with a behavior-aware con-\ntext embedding module and an item-wise sequential de-\npendency encoder. This component is able to model the\ntemporal interaction patterns of users with the preserva-\ntion of multi-behavior contextual signals.\nâ€¢ The component of multi-behavior pattern aggregation\nis proposed to capture short-term user preference over\nbehavior-aware sub-interactions. In this module, the time-\naware user-item interactive patterns are encoded by the\ndual-stage behavior-aware message passing paradigm for\nembedding propagation and reï¬nement.\nâ€¢ Finally, a global context learning component is introduced\nto endow our TGT with the ability of incorporating\nthe long-term multi-behavior patterns into the preference\nlearning framework. Through the recursive embedding\npropagation, both the short-term and long-term behavior-\nspeciï¬c interaction patterns are preserved in the learned\nrepresentations of users/items.\nIn the following subsections, we will elaborate the design\nmotivations and technical details of each learning compo-\nnent in our proposed TGT framework.\n3.3 Dynamic Individual Interest Modeling\nTo capture the time-evolving user interests with the mod-\neling of underlying relation heterogeneity across different\ntypes of behaviors, we develop a multi-behavior trans-\nformer network to distill short-term behavior dynamics. In\nparticular, we explicitly model user short-term preference\nby splitting multi-behavior interaction sequence Si into R\nï¬ne-grained sub-sequences Sr\ni (indexed by r). Formally, we\ndeï¬ne Sr\ni = [(vr\n1,i,tr\n1,i,br\n1,i),..., (vr\n|Sr\ni |,i,tr\n|Sr\ni |,i,br\n|Sr\ni |,i)]. With-\nout loss of generality, the individual interaction instance in\nthe r-th sub-sequence is denoted as (vr\nk,tr\nk,br\nk).\n3.3.1 Behavior-aware Context Embedding Layer\nWe design an embedding layer to jointly inject the multi-\nbehavior contextual and temporal signals into the item rep-\nresentations. Following the mainstream recommendation\nparadigms [1], [34], we ï¬rst describe individual item vwith\nid-corresponding embedding ev âˆˆ Rd, where d denotes\nthe latent state dimensionality. Then, user behaviors are\ndifferentiated with behavior type embedding eb âˆˆRd corre-\nsponding to b-th type of user-item interaction. Furthermore,\nto capture the behavior dynamics, we enhance the con-\ntext embedding layer by introducing a temporal encoding\nstrategy. Motivated by the positional embedding method\nin Transformer [39], we adopt the sinusoid functions over\ntimestamp information to generate base time embeddings\ntr\nk âˆˆR2d with the following operations:\ntr,(2l)\nk = sin( Ï„(tr\nk)\n10000\n2l\nd\n)/\nâˆš\nd (1)\ntr,(2l+1)\nk = cos( Ï„(tr\nk)\n10000\n2l+1\nd\n)/\nâˆš\nd\nwhere Ï„(Â·) represents the time slot mapping function\nwith temporal periods. The odd and even index of 2d-\ndimensional vector tr\nk are represented by 2l and 2l + 1.\nWe further apply\nâˆš\nd as the scale factor to alleviate the\neffect of large embedding values. In addition, we introduce\n5\na tunable linear projection over the generated base time\nembedding tr\nk to endow our model with learning ï¬‚exibility\nof temporal context as etr\nk = tr\nk Â·Wt, where Wt âˆˆR2dÃ—d.\nAfter that, we inject the compound contextual signals of\nmulti-behavioral patterns and interaction dynamics with\nthe integration operation as: Er\nk = evr\nk âŠ•ebr\nk âŠ•etr\nk , where\nEr\nk âˆˆRd is the learned context-aware item embedding, and\nâŠ•denotes the element-wise addition operation.\n3.3.2 Item-wise Sequential Dependency\nTo capture the dynamism of short-term transitions across\ndifferent items, our item-wise sequential dependency en-\ncoder is built upon the transformer architecture, based on\nself-attention mechanism under multi-head representation\nspaces. In speciï¬c, the encoded embedding of k-th inter-\nacted item vr\nk is calculated by the following multi-head dot-\nproduct attention for item-wise mutual relation learning:\nÂ¯Er\nk =\nHâââ\nâââ\nh=1\nKâˆ‘\nkâ€²=1\nÎ±k,kâ€² VhEr\nkâ€²\nÎ±k,kâ€² = Ïƒ((QhEr\nk)âŠ¤(KhEr\nkâ€² )âˆš\nd/H ) (2)\nwhere Î±k,kâ€² is the learned item-wise relevance score be-\ntween vr\nk and vr\nkâ€² . Ïƒ(Â·) denotes the softmax function. The\nitem representation reï¬nement ( Â¯Er\nk) is conducted through\nthe above aggregation to capture relations across different\nitems. We enable our encoder with multiple representation\nsubspaces to perform sequential information aggregation\nfrom different semantic dimensions ( i.e., h âˆˆ H atten-\ntion heads).\nâââ\nâââ denotes the vector concatenation operator.\nQh,Kh,Vh âˆˆR\nd\nH Ã—d denote the h-th head query, key and\nvalue transformations, respectively. With the integration of\ncontext embedding layer and item-wise dependency en-\ncoder, our TGT not only preserves the cross-item transitional\nrelations, but also captures the multi-behavioral dynamism.\n3.4 Multi-Behavior Pattern Aggregation\nIn this section, we perform the multi-behavior pattern ag-\ngregation with the modeling of time-aware user-item inter-\nactions. Towards this end, a bipartite graph Gr = {ur\ni âˆª\nSi,r,Er}, where ur\ni denotes the intermediate user vertex\ncorresponding to the r-th interaction sub-sequence Si,r. Er\ni\nrepresents the interactions between user ui and all items\nincluded in the sequence Si,r. To capture the short-term be-\nhavior heterogeneity, we design a behavior-aware message\npassing scheme to differentiate and aggregate interactive\npatterns from different behavior types. This is a two-phase\nmessage passing paradigm which consists of propagating\nand reï¬ning embeddings from user to item side and vice\nversa. Given the learned item representations Â¯Er\nk âˆˆRd by\ninjecting multi-behavioral temporal context, the message\npassing from item to user side can be formally represented\nas follows:\nHr\nb = Aggre(Â¯Er,b) = Ïƒ(\nKâˆ‘\nk=1\nÏ†(br\nk = b) Â·Â¯Er\nk Â·Wb) (3)\nwhere Hr\nb âˆˆRd represents the generated embedding of node\nur\ni under the b-th behavior type. We utilize Wb as behavior-\naware transformation to enhance the modeling of behavior-\nspeciï¬c semantics. Ïƒ(Â·) is the ReLU activation function. We\ndeï¬ne Ï†(Â·) as an indicator function to ï¬t the behavior type-\naware embedding propagation, i.e., Ï†(Â·) = 1 given br\nk = b.\n3.4.1 Multi-Channel Projection\nTo effectively discriminate multi-typed of interactions dur-\ning the message passing process, we adopt a multi-channel\nparameter learning strategy for the calculation of transfor-\nmation Wb âˆˆRdÃ—d, which can be formalized as follows:\nWb =\nHâˆ‘\nh=1\nÎ²h Â·Â¯Wh; Î²h = Ïƒ(P Â·b + Âµ)(h) (4)\nwhere Â¯W âˆˆRdÃ—d denotes the H base transformations (in-\ndexed by h) shared by all behavior types. Î²h is the learned\nweight for the h-th latent channel representation, which is\ncalculated by a fully-connected layer with transformation\nP âˆˆRHÃ—d and bias ÂµâˆˆRH.\nIn our TGT framework, the multi-channel projection\nlayer serve as an important component in the global item-\nwise dependency modeling with the preservation of behav-\nior heterogeneity. Speciï¬cally, our designed multi-channel\nprojection aims to differentiate the multi-relational inter-\nactions during the message passing across time. After the\nbehavior embedding projection over multiple base transfor-\nmations, the behavior-aware semantics are encoded with the\ndeveloped channel-wise aggregation layer, which endows\nour TGT method to preserve the inherent behavior seman-\ntics of different types of user-item interactions.\n3.4.2 Aggregation over Cross-Type Relations\nAfter encoding the type-speciï¬c behavior semantics, the\nnext step is to feed the behavior-aware embeddings into an\naggregation layer to uncover the implicit relations across\ndifferent types of behavioral patterns. Different from static\nattentive relation aggregation schemes [47], [58], we ï¬t\nour learning scenario with the dynamic user preference,\nby proposing an adaptive attention network to distinguish\ninï¬‚uence of different behavior types in predicting the target\none, with the awareness of time-evolving multi-behavioral\ninteraction patterns.\nÂ¯Hr =\nBâˆ‘\nb=1\nÎ³b Â·Hr\nb\nÎ³b = Ïƒ1(Hr\nb\nâŠ¤Â·Ïƒ2(\nBâˆ‘\nb=1\nHr\nbWA + ÂµA) (5)\nwhere WA âˆˆ RdÃ—d,ÂµA âˆˆ Rd are the transformation and\nbias for our aggregation layer, respectively. Ïƒ1(Â·) denotes\nthe softmax function, and Ïƒ2(Â·) denotes the ReLU function.\nÎ³b is the learned attentive weight for the b-th behavior type,\ncalculated using the summation of all behavioral embed-\ndings as query. As a result, we explicitly preserve the time-\naware multi-behavior contextual information in the user\nrepresentation Â¯Hr.\nTo model time-aware userâ€™s inï¬‚uence over different\nitems, we perform the message passing from user to item\n6\nside through the aforementioned embedding propagation\nand aggregation schemes:\nÂ¯Eb\nj = Ïƒ(\nâˆ‘\nvr\nk=j\nÏ†(br\nk = b) Â·Â¯Hr Â·Wb); Â¯Ej =\nBâˆ‘\nb=1\nÎ³b Â·Â¯Ej,b (6)\nwhere Wb is calculated by the multi-channel projection\nscheme with different transformation mappings (Eq 4), and\nÎ³b is returned from our attentive aggregation mechanism. It\nis worth noting that Â¯Eb\nj,Â¯Ej âˆˆRd are cross-time embeddings\nof item vj (without super-script r), which could capture\nchanges in item semantics over time.\n3.5 Global Relational Context Learning\nSince userâ€™s preference is inherently dynamic and evolve\nover time, our TGT further proposes to inject the long-\nrange dynamism into the global-level graph relation en-\ncoder, which captures the dynamic multi-behavior patterns\nfrom both the long-term and short-term perspectives. We\ndeï¬ne a global user graph to contain all users ( ui âˆˆ U)\nand their intermediate sub-sequence nodes ( ur\ni âˆˆUr), i.e.,\nG = {U,Ur,E}, where E represents the connections be-\ntween each user ui and his/her intermediate sub-sequence\nnodes ur\ni , i.e., an edge connects ui and ur\ni (1 â‰¤ r â‰¤ R).\nIn this component, the global user embedding Â¯Î“i âˆˆ Rd\nis generated by performing graph-structured information\naggregation, over short-term user preference embeddings\n( Â¯Hr) across different interaction sub-sequences.\n3.5.1 Global-level User Representation\nTo learn global-level user representations, we incorporate\nthe temporal context into the message passing procedure\nof attentional graph neural architecture, to allow the short-\nterm preference encoded with different timestamps to inter-\nact with each other in a differentiable manner:\nÂ¯Î“i = Î¦( Â¯Hr) = Ïƒ(\nRiâˆ‘\nr=1\nÎ·r Â·Â¯Hr)\nÎ·r = Î“âŠ¤\ni Â·Â¯Hr; Â¯Hr = Î“i + tr (7)\nwhere Î·r is the learned attentive weights for pairwise\nrelationship between ui and ur\ni . Î¦(Â·) denotes the global\ninformation aggregation function with the Ïƒ(Â·) activation\nfunction. tr âˆˆRd is our temporal embedding encoded from\nour context embedding layer. The sub-sequence represen-\ntation Â¯Hr will be updated with the injection of temporal\nlocalized information as: Â¯Hr = Î“i + tr, where Î“i âˆˆRd is\nthe user embedding which is either initialized or smoothed\nby the neighboring nodes. By doing so, TGT maintains the\nuser preference embeddings from different time periods as\na uniï¬ed feature embedding for global user representation,\nwith the modeling of temporal dependency and evolution\nof user interaction patterns.\n3.6 High-order Relation Aggregation\nAs discussed above, TGT performs two-phase information\naggregation for encoding information of i) short-term multi-\nbehavior interactions between user and item ; ii) long-\nrange dynamic structural dependency of user interest across\ntime durations. Generally, our TGT architecture takes the\nrelational structure of Gr and Gas computation graphs for\nembedding propagation, during which the local relational\nfeatures from neighborhood will be aggregated to obtain a\ncontextual representation. Such message passing paradigm\ncan be generalized with the incorporation of high-order con-\nnectivity as follows (suppose ldenotes the (l)-th GNN layer\nin the graph-based information propagation architecture):\nÂ¯Hr,(l) =\nBâˆ‘\nb=1\nÎ³b Â·(Ïƒ(\nKâˆ‘\nk=1\nÏ†(br\nk = b) Â·Â¯Er,(l)\nk Â·Wb))\nÎ“(l+1)\ni = Ïƒ(\nRiâˆ‘\nr=1\nÎ·r Â·Â¯Hr,(l)), Â¯Hr,(l+1) = Î“(l)\ni + tr\nÂ¯E(l+1)\nj =\nBâˆ‘\nb=1\nÎ³b Â·Ïƒ(\nâˆ‘\nvr\nk=j\nÏ†(br\nk = b) Â·Â¯Hr,(l+1) Â·Wb) (8)\nwhere Â¯Er,(0)\nk ,Â¯Î“(0)\ni are initialized id-corresponding item and\nuser embeddings, respectively. We endow TGT with the\nability of capturing high-order collaborative effects across\ndifferent users/items, by extending our developed graph\nrelation encoder from one layer to multiple layers ( i.e., L).\nWe ï¬nally generate user ( ËœÎ“i âˆˆ Rd), item ( ËœEj âˆˆ Rd) and\nintermediate sub-sequence node ( ËœH\nr\nâˆˆ Rd) embeddings\nacross Lgraph layers with the element-wise summation:\nËœÎ“i =\nLâˆ‘\nl=0\nÎ“(l)\ni , ËœH\nr\n=\nLâˆ‘\nl=0\nHr,(l), ËœEj =\nLâˆ‘\nl=0\nE(l)\nj (9)\n3.7 Model Prediction and Optimization\nFollowing the same settings in sequential recommender\nsystems [41], to reï¬‚ect the dynamic characteristics of user\npreference, we leverage the time-aware user representation\nËœH\nr\nto make forecasting on future user-item interactions. The\nprobability of future interaction between user ui and item\nvj is estimated by Pr i,j = zâŠ¤( ËœH\n(âˆ’1)\ni â—¦ËœEj), where z âˆˆRd\nis a parametric vector. ËœH\nâˆ’1\ni represents the embedding of\nthe last interacted item of user ui in the r-th interaction\nsub-sequence. â—¦ denotes the element-wise multiplication\noperation. Next, we will describe our deï¬ned optimized\nobjective and perform the time complexity analysis for our\nTGT framework.\n3.7.1 Optimization Objective\nDuring the training phase, the parameter inference process\nis optimized by leveraging historical user-item interactions\nfor label data augmentation [22], [36]. In this case, the\nprediction score for ui and vj during the time interval of\nsub-sequence Si is calculated as: Pr r\ni,j = zâŠ¤( ËœH\nr\ni â—¦ËœEj). We\nformally deï¬ne our objective function with the marginal\npairwise loss as follows:\nL=\nNâˆ‘\ni=1\nRiâˆ‘\nr=1\nCâˆ‘\nc=1\nmax(0,1 âˆ’Prr\ni,pc + Prr\ni,nc) + Î»Â·âˆ¥Î˜âˆ¥2\nF\n(10)\nwhere N represents the number of users and Ri denotes\nthe number of interaction sub-sequences for user ui. pc and\nnc refer to C positive and the negative samples (indexed\nby c), respectively. We further apply the weight-decay is the\n7\nAlgorithm 1:Model Inference of TGT Framework\nInput: user-item interaction sequences Si, sub-user size |Sr\ni |,\ngraph iterations L, target behavior B, sample number\nC, maximum epoch number E, regularization weight\nÎ», learning rate Ï\nOutput: trained parameters in Î˜\n1 Initialize all parameters in Î˜\n2 Divide Si into sub-sequences Sr\ni of size |Sr\ni |\n3 for e= 1 to Edo\n4 Generate behavior-aware context embeddings Er\nk\n5 Capture item-wise sequential dependency to yield Â¯Er\nk\n6 for l= 1 to Ldo\n7 Compute sub-usersâ€™ behavior embeddingsHr\nb\n8 Conduct cross-type aggregation to get Â¯Hr\n9 Aggregate for the global user embedding Â¯Î“i\n10 Reï¬ne the sub-user embeddings Â¯Hr using Â¯Î“i\n11 Update the item embeddings Â¯Ej using Â¯Hr\n12 end\n13 Integrate the cross-order graph embeddings ËœÎ“i,ËœH\nr\n,ËœEj\n14 Draw a mini-batch U from all users, each with C\npositive-negative samples\n15 L= Î»Â·âˆ¥Î˜âˆ¥2\nF\n16 for each ui âˆˆU do\n17 Compute predictions Pri,pc,Pri,nc\n18 L+ = âˆ‘C\nc=1 max(0,1 âˆ’Pri,pc + Pri,nc)\n19 end\n20 for each parameter Î¸âˆˆÎ˜ do\n21 Î¸= Î¸âˆ’ÏÂ·âˆ‚L/âˆ‚Î¸\n22 end\n23 end\n24 return all parameters Î˜\nweight-decay regularization term Î»Â·âˆ¥Î˜âˆ¥2\nF for alleviating\noverï¬tting phenomenon.\nWe summarize the learning process of our TGT in Alg 1.\nIn particular, the model optimization involves several key\nsteps: (i) We ï¬rst generate behavior-ware context embed-\ndings Er\nk. Then, we feed it into the item-wise sequential\ndependency encoder to obtain Â¯Er\nk corresponding to the\nk-th behavior type and r-th interaction sub-sequence. (ii)\nThrough the recursive message passing procedure, we learn\nthe sub-sequence behavior representation Hr\nb of users and\ninject the cross-type behavior relations into the reï¬ned em-\nbedding Â¯Hr. (iii) The aggregated global user embedding Â¯Î“i\npreserves his/her long-term multi-behavior patterns. The\nsub-user and item embeddings are updated accordingly. (iv)\nLayer-speciï¬c embeddings are combined to generate cross-\nlayer representations ËœÎ“i,ËœH\nr\n,ËœEj.\n3.7.2 Model Complexity Analysis\nIn this subsection, we analyze the time complexity of\nour TGT model. In particular, in the encoding process of\nitem-wise sequential dependency, the computational cost\nfor pairwise item relation learning and embedding pro-\njection are O(|Er|Ã—| Sr\ni |Ã— d) and O(|Er|Ã— d2), respec-\ntively. Different from directly performing sequential learn-\ning over the entire item sequence Si, our method signif-\nicantly reduces the computational cost for self-attention\noperations. Additionally, during the message passing and\naggregation in the graph neural architecture, TGT costs\nO(LÃ—|Er|Ã—d+ (M+ |E|/|Sr\ni |) Ã—LÃ—d2) for encoding the\nshort-term multi-behavioral patterns, and O(|E|/Si Ã—LÃ—d)\nfor learning global relation context, where L is the depth\nof our graph neural network. Considering the small value\nof L, this complexity term has very small inï¬‚uence on the\nTable 2: Statistics of experimented datasets\nDataset # of Users # of Items # of Interactions\nTaobao 147894 99037 7658926\nIJCAI 423423 874328 36203512\noverall model cost. Under the same experimental settings\n(e.g., sequence length and latent state dimensionality), TGT\ncould achieve comparable complexity to the most recently\ndeveloped baselines: HyRec [41] and GCGNN [48].\n4 E VALUATION\nIn this section, we perform experiments to validate the effec-\ntiveness of our proposed TGT recommendation framework\nby investigating the research questions as follows:\nâ€¢ RQ1: How does our proposed TGT method perform com-\npared with state-of-the-art recommendation methods?\nâ€¢ RQ2: How do the different components ( e.g., behavior-\naware transformer network and global relational context\nlearning) of TGT contribute to the model performance?\nâ€¢ RQ3: How do the integration of different types of behav-\nioral patterns affect the prediction of target behavior?\nâ€¢ RQ4: How do the conï¬gurations of model hyperparame-\nters affect the recommendation performance?\nâ€¢ RQ5: Can the proposed TGT framework show model in-\nterpretability, through the effective modeling of sequential\nmulti-behaviour patterns?\n4.1 Experiment Settings\nIn this section, we ï¬rst describe the details of our experi-\nmented datasets and introduce the evaluation metrics. Then,\nwe present the baseline methods from a variety of research\nlines for performance comparison, as well as the parameter\nsettings of our developed TGT framework.\n4.1.1 Data Description\nIn our evaluation, experiments are performed on two pub-\nlic datasets (see Table 2) collected from real-world on-\nline platforms. Both datasets contain different types of\nuser behaviors in e-commerce services. Taobao-Data: This\ndata records four types of user-item interactions collected\nfrom the Taobao online retail system-one of the worldâ€™s e-\ncommerce marketplace. Particularly, multi-typed behaviors\ninclude page view , add-to-cart, tag-as-favorite and purchase.\nIJCAI-Contest Data: This data is released by IJCAI compe-\ntition to record usersâ€™ online activities from online business-\nto-consumer e-commerce site, also involving four types\nof behaviors: clicking, adding-to-cart, tagging with favor and\npurchasing. The temporally-ordered multi-typed interaction\nlogs are used to generated multi-behavior sequence Si for\neach user ( ui âˆˆU). Because of the importance of customer\npurchase activities in practical e-commerce platforms [17],\n[51], we consider the purchases of users as the target behav-\niors and other types of user behaviors are regarded as the\ncontext behaviors for representation enhancement.\n8\n4.1.2 Evaluation Metrics\nIn our experiments, we evaluate the recommendation per-\nformance of all compared approaches using two widely\nadopted metrics: NDCG@N and Recall@N, so as to mea-\nsure the accuracy of top- N recommended item for different\nusers. Note that the better recommendation performance is\nreï¬‚ected by higher NDCG@N and Recall@N scores. Here,\nNDCG score reï¬‚ects the position-aware recommendation\naccuracy which assigns high weights for higher ranked item\nhits. We utilize the leave-one-out evaluation [22], [36] for\ngenerating the training and testing data instances based on\nthe temporal information of user behavior data. Speciï¬cally,\nthe last interacted item of each evaluated user is considered\nas the test instance for making recommendation.\n4.1.3 Methods for Comparison\nTo comprehensively evaluate the performance, we compare\nour TGT with state-of-the-art recommendation techniques\nwhich can be grouped into different model types:\nConventional Matrix Factorization Model: We ï¬rst con-\nsider the representative matrix factorization methodâ€“\nBayesian personalized ranking as the baseline.\nâ€¢ BPR [34]. It enhances the matrix factorization model with\npairwise loss for learning personalized rankings.\nNeural Collaborative Filtering Methods: The conventional\ncollaborative ï¬ltering methods have been enhanced by deep\nlearning techniques for nonlinear deep feature extraction.\nâ€¢ NCF [12]. This approach replaces the dot-product opera-\ntion in matrix factorization with multi-layer perceptrons\nto model the non-linearity in user-item interactions.\nâ€¢ DeepFM [8]. This wide-and-deep model augments clas-\nsic factorization machine with deep neural networks, to\nconsider both low- and high-order feature interactions.\nRNN-based Sequential Recommendation Method: Recur-\nrent neural network has been utilized in sequential recom-\nmendation model for dynamic behavior modeling.\nâ€¢ GRURec [13]. This method adopts the gated recurrent\nunit to model sequential behaviors of users, based on the\ngating mechanism to control the information ï¬‚ow.\nConvolution-based for Sequential Recommendation: We\nconsider the sequential recommendation method using con-\nvolutional ï¬lters to encode local temporal patterns.\nâ€¢ Caser [37]. It utilizes the convolution neural network to\nperform information aggregation across temporally-order\nitems in both horizontal and vertical way.\nAttention/Transformer-based Neural Network for Sequen-\ntial Recommender Systems: Inspired by the effectiveness of\nattention mechanism in relational learning, there exist many\nattentional recommendation frameworks for modeling user\ndynamic preference in sequential recommendation.\nâ€¢ NARM [23]. It jointly learns the sequential patterns of\nuser interactions as well as the session-speciï¬c general\ninterest with the integration of recurrent network and\nattention mechanism.\nâ€¢ SASRec [22]. In SASRec, self-attention is employed to\nencode the sequential patterns of user interaction, with-\nout the recurrent operations over input sequences. The\nembedding transformation is performed with query and\nkey dimensions.\nâ€¢ Bert4Rec [36]. It is a transformer-based sequential recom-\nmender system with bidirectional self-attention architec-\nture and cloze objective to incorporate sequential signals.\nKnowledge-aware Sequential Recommendation: We also\ninclude the sequential recommendation baseline with the\nconsideration of knowledge-aware item relationships.\nâ€¢ Chorus [40]. This method accounts for both temporal\ncontext and item correlations in the sequential recommen-\ndation model. For this method, the TransE is adopted as\nthe translation function to obtain entity representations.\nSequential/Session-based Recommendation Models with\nGraph-based Neural Networks : Another important re-\nsearch line of time-aware recommender systems lies in the\nutilization of graph neural networks for behavior depen-\ndence learning.\nâ€¢ HyRec [41]. It predicts the next interacted item of users\nby utilizing the hypergraph neural network to model the\nshort-term item relations. Then, a fusion layer is intro-\nduced to aggregate dynamic item semantics.\nâ€¢ MAGNN [28]. It captures both the short- and long-term\nuser preference with memory-augmented graph neural\nnetwork. The generated short- and long-term pattern em-\nbeddings are aggregated through an interesst fusion layer.\nâ€¢ MGNN [44]. It is a relation-aware graph neural network-\nbased architecture which utilizes the gating mechanism\nfor representation fusion, with the consideration of multi-\nrelational user sequence.\nâ€¢ SR-GNN [53]. It designs a gated graph neural network to\ngenerate session-level item representations with transition\nregularities. The constructed graph data contains different\nsession sequences in session-based recommendation.\nâ€¢ GCGNN [48]. This approach performs the information\naggregation over the constructed session graphs by con-\nsidering pairwise item-transition information.\nHeterogeneous Graph Neural Model : Additionally, we\nconsider another type of baseline by utilizing the hetero-\ngeneous graph neural network to capture the behavior het-\nerogeneity during the information propagation procedure.\nâ€¢ HGT [14]. It is a state-of-the-art heterogeneous graph\nlearning model which integrates the transformer with\nthe graph-based message passing scheme. We utilize its\nheterogeneous graph relation encoder to model different\ntypes of user-item interactions.\nMulti-Behavior Recommendation Frameworks: Several re-\ncent studies attempt to capture multi-behavioral context for\nrecommendation by encoding behavior dependency based\non various techniques.\nâ€¢ NMTR [7]: It integrates the neural collaborative ï¬ltering\nand multi-task learning paradigm to model the behavior-\nwise dependency of users with pre-deï¬ned correlations.\nâ€¢ MATN [54]: This recommendation method designs\nmemory-based neural units to enhance the attention\nmechanism, to capture the behaviour patterns of users. It\nonly encodes the local connectivity patterns between users\n9\nTable 3: Overall performance comparison of all methods in terms of HR@N and NDCG@N (N = 1,5,10).\nMethods\nTaobao Data IJCAI Contest\nTop @ 1 Top @ 5 Top @ 10 Top @ 1 Top @ 5 Top @ 10\nHR Imp HR Imp NDCG Imp HR Imp NDCG Imp HR Imp HR Imp NDCG Imp HR Imp NDCG Imp\nBPR 0.048 139% 0.209 39% 0.143 26% 0.295 53% 0.179 47% 0.073 102% 0.184 117% 0.130 125% 0.257 102% 0.153 116%\nNCF 0.061 89% 0.231 26% 0.157 15% 0.325 39% 0.201 31% 0.139 6% 0.351 14% 0.255 15% 0.459 13% 0.294 12%\nDeepFM 0.059 95% 0.223 30% 0.142 27% 0.328 38% 0.175 50% 0.138 7% 0.332 20% 0.244 20% 0.469 11% 0.290 14%\nGRURec 0.081 42% 0.237 22% 0.161 12% 0.328 38% 0.187 41% 0.107 38% 0.284 41% 0.194 51% 0.418 24% 0.247 34%\nCaser 0.094 22% 0.239 21% 0.168 7% 0.364 24% 0.197 34% 0.129 14% 0.295 35% 0.208 41% 0.422 23% 0.252 31%\nNARM 0.085 35% 0.220 32% 0.151 19% 0.320 41% 0.181 45% 0.107 38% 0.289 38% 0.200 47% 0.396 31% 0.229 44%\nSASRec 0.089 29% 0.250 16% 0.169 7% 0.353 28% 0.203 30% 0.114 29% 0.306 30% 0.216 36% 0.420 24% 0.247 34%\nBert4Rec 0.113 2% 0.265 9% 0.174 3% 0.369 22.5% 0.229 15% 0.141 4% 0.356 12% 0.261 12% 0.467 11% 0.297 11%\nChorus 0.100 15% 0.235 23% 0.171 5% 0.388 17% 0.232 13% 0.140 5% 0.345 15.7% 0.247 19% 0.457 14% 0.283 17%\nHyRec 0.088 31% 0.228 27% 0.161 12% 0.324 40% 0.191 38% 0.137 8% 0.323 24% 0.229 28% 0.442 17% 0.266 24%\nMAGNN 0.108 6% 0.235 23% 0.174 3% 0.320 41% 0.199 32% 0.127 16% 0.291 37% 0.212 38% 0.392 32% 0.245 35%\nSR-GNN 0.086 34% 0.240 21% 0.162 11% 0.332 36% 0.191 38% 0.133 11% 0.316 26% 0.214 37% 0.431 20% 0.263 26%\nGCGNN 0.087 32% 0.250 16% 0.168 7% 0.349 30% 0.201 31% 0.136 8% 0.289 38% 0.210 40% 0.428 21% 0.257 28%\nMGNN 0.096 20% 0.259 12% 0.172 5% 0.374 21% 0.219 20% 0.138 7% 0.244 64% 0.248 18% 0.457 14% 0.286 15%\nHGT 0.101 14% 0.261 11% 0.174 3% 0.364 24% 0.216 22% 0.128 15% 0.309 29% 0.207 41.5% 0.450 15% 0.265 25%\nNMTR 0.079 46% 0.218 33% 0.147 22% 0.332 36% 0.179 47% 0.141 4% 0.360 10.8% 0.254 15% 0.481 8% 0.304 9%\nMATN 0.081 42% 0.226 29% 0.153 17% 0.354 28% 0.209 26% 0.142 4% 0.375 6% 0.273 7% 0.489 6% 0.309 7%\nMBGCN 0.110 5% 0.259 12% 0.172 5% 0.369 23% 0.222 19% 0.137 8% 0.332 20% 0.228 31% 0.463 12% 0.277 19%\nTGT 0.115 â€“ 0.290 â€“ 0.180 â€“ 0.452 â€“ 0.263 â€“ 0.148 0.399 â€“ 0.293 â€“ 0.519 â€“ 0.330 â€“\nand items and cannot capture the high-order dependence.\nâ€¢ MBGCN [20]: it is a state-of-the-art graph-based rec-\nommendation model which alleviates the data sparsity\nissue with the modeling of multi-typed user behavior\ndata. In this model, the graph convolution operations are\nconducted to perform message passing.\n4.1.4 Parameter Settings\nWe implement our TGT using TensorFlow and adopt Adam\nfor model optimization. During the training phase, the\nmodel inference process is conducted with the learning rate\nof 1eâˆ’3 (conï¬gured with 0.96 decay rate). The batch size\nis selected from the range {32,128,256,512}with the best\nsetting of 256 for Taobao dataset and 512 for IJCAI dataset.\nThe weight-decay regularization term is conï¬gured with Î»\nselected from {0.05, 0.01, 0.005, 0.001, 0.0005 }. The default\nsettings for hidden state dimensionality is 16. The length of\nuserâ€™s sub-sequence is chosen from the range [2,4,6,8,10]\nwith the best setting of 6 for Taobao dataset and 10 for IJCAI\ndataset. We conï¬gure our self-attention mechanism under 2\nheads of representation subspaces. The number of channels\nfor base transformation in behavior semantic encoding is set\nas 2. The number of information propagation layersLin our\nTGT is chosen from [1,2,3]. For a fair comparison settings,\nall compared neural network models are evaluated using\ntheir released source code or implementing them according\nto their original papers. Detailed model hyperparameter\nsettings can also be found in our released model implemen-\ntation in the abstract section.\n4.2 Performance Comparison (RQ1)\nWe present the performance comparison results of our\nTGT and baselines in Table 3 in terms of (HR@1), (HR@5,\nNDCG@5) and (HR@10, NDCG@10). As we can see, TGT\nconsistently outperforms all compared methods by a large\nmargin under all metrics. We attribute such improvements\nto the multi-behavior dependency modeling of TGT: i)\nthrough uncovering multi-behavior user intentions, TGT\ncan better characterize the user-item relations; ii) beneï¬ting\nfrom our designed temporal graph neural transformer archi-\ntecture, both short- and long-term behavior-type dependent\ninterests of users are effectively preserved in our learned\nuser and item representations.\nWe further summarize several observations as follow:\nâ€¢ (1) Our TGT method consistently outperforms baesline\nmethods BPR, NCF, DeepFM in all cases on different\ndatasets, which validates the superiority of our pro-\nposed multi-behavior sequential recommender. Most of\nsequential recommendation methods achieves better per-\nformance than BPR, NCF and DeepFM. This observation\nsuggests the beneï¬ts of modeling behavior dynamics with\nsequential interaction patterns for predicting user prefer-\nence. Furthermore, the performance superiority of multi-\nbehavior recommender systems as compared to NCF and\nDeepFM justiï¬es the importance of incorporating multi-\nbehavior dependencies into the modeling of complex and\ndiverse user preferences.\nâ€¢ (2) Bert4Rec method achieves better performance than\nother attention-based approaches ( i.e., NARM, SASRec)\nand convolution-based sequential recommender system\n(i.e., Caser), which indicates the effectiveness of trans-\nformer in encoding sequential patterns. However, the\nperformance improvement of our method over Bert4Rec\nshows that TGT signiï¬cantly boosts the recommenda-\ntion performance by embracing sequential multi-behavior\nmodeling for recommendation.\nâ€¢ (3) The performance gap between TGT and GNN-based\nrecommender systems with the modeling of temporal\ninformation ( i.e., HyRec, MAGNN, SR-GNN, GCGNN),\nclarifying the importance of encoding behavior hetero-\ngeneity during the message passing paradigms. In partic-\nular, our TGT is equipped with dynamic multi-behavior\nmodeling under a graph-based global relation aggrega-\ntion, which shows the superior performance in encoding\nusersâ€™ diverse preference.\nâ€¢ (4) For a comprehensive performance comparison set-\ntings, we also compare TGT with two recently devel-\noped time-aware recommendation (MAGNN) and net-\nwork embedding (HGT) methods with heterogeneous\ngraph neural networks. As relation-aware graph neural\nmodels, these models are generally superior to other\n10\nTable 4: Ablation study on IJCAI Contest dataset.\nSetting Top-5 Top-10\nMetrics HR NDCG HR NDCG\nw/o CE 0.392 0.290 0.515 0.321\nw/o SD 0.367 0.265 0.499 0.306\nw/o MCP 0.346 0.240 0.490 0.306\nw/o LBD 0.387 0.283 0.511 0.319\nw/o CTA 0.392 0.287 0.510 0.323\nFBA 0.379 0.274 0.511 0.320\nTGT 0.399 0.293 0.519 0.330\nSetting Top-15 Top-20\nMetrics HR NDCG HR NDCG\nw/o CE 0.603 0.354 0.670 0.367\nw/o SD 0.591 0.332 0.661 0.349\nw/o MCP 0.575 0.308 0.647 0.325\nw/o LBD 0.597 0.348 0.658 0.364\nw/o CTA 0.603 0.352 0.666 0.368\nFBA 0.604 0.343 0.677 0.359\nTGT 0.608 0.356 0.681 0.374\nGNN-based methods by considering the interaction het-\nerogeneity. However, they fail to capture dynamic cross-\ntype behavior dependencies from both short- and long-\nterm perspectives, which limits their capability in effec-\ntively transferring knowledge among different types of\nuser behaviors.\nâ€¢ (5) As Table 3 shows, TGT effectively improves the rec-\nommendation performance when competing with state-\nof-the-art multi-behavior recommendation models ( i.e.,\nNMTR, MBGCN and MATN). We can observe the relative\nimprovement ratio (measured by NDCG@10) between our\nTGT and MBGCN is 22.5% and 12.1% on Taobao and\nIJCAI-Contest dataset, respectively. Such performance im-\nprovement further veriï¬es the rationality of our designed\ntemporal graph neural architecture, which enables TGT to\naccurately capture the dynamic multi-behavior patterns.\nIn most existing multi-behavior recommender systems,\nthe time-evolving cross-type behavior dependencies are\nignored, which degrades the performance.\n4.3 Model Ablation Study (RQ2)\nTo verify the effects of the designed different components\nin our model, we perform a detailed model ablation study\nof TGT on the IJCAI-Contest dataset. Speciï¬cally, we design\ndifferent variants to make comparison with our TGT recom-\nmendation framework:\nâ€¢ w/o CE (Context Emebedding): The time- and behavior-\naware context are ignored during the sequential depen-\ndency encoding. Only item and positional embeddings are\nfed into the sequential dependency encoding component.\nâ€¢ w/o SD (Sequential Dependency) : We do not involve\nthe multi-behavior transformer for capturing short-term\npreference, and directly apply the temporal graph neural\nnetwork to model the behavior heterogeneity.\nâ€¢ w/o MCP (Multi-Channel Projection): This variant sim-\npliï¬es the designed multi-behavior message passing\nparadigm without the multi-channel projection layer to\npreserve distinct behavior semantics.\nâ€¢ w/o LBD (Long-Term Behavior Dynamics): This model\ndoes not take the long-term multi-behavior dynamics into\nconsideration. It ignores the global relation learning with\nbehavior type-aware message passing component.\nâ€¢ w/o CTA (Cross-Type Behavior Aggregation): This model\nvariant does not explore the cross-type behavioral rela-\ntions with our attentive aggregation layer. Instead, we di-\nrectly concatenate the propagated type-speciï¬c behavior\nembeddings to generate the user representation.\nWe show the ablation results in Table 4 and discuss the ï¬nd-\nings and summarization from different aspects as below:\n(1) Effect of behavior-aware context embedding. Com-\npared to w/o CE variant, TGT achieves better results. We\nowe it to the injection of multi-behavioral context into the\nmodeling of item-wise sequential dependency, which allows\nthe model to preserve short-term multi-behavior semantics.\n(2) Effect of item-wise short-term dependency modeling.\nOur multi-behavior transformer framework enhances the\nvalina transformer with the incorporation of behavior-aware\ncontext into the modeling process of item-wise sequential\ndependencies. By comparing the performance of TGT with\nw/o SD, it is clear that with the multi-behavior trans-\nformer network, TGT achieves better performance by ex-\nploring short-term item-transition information under multi-\nbehavior context. The designed multi-behavior transformer\nenables the item sequence encoder with the awareness\nof multi-behavior contextual signals between users and\nitems, so as to capture the heterogeneous item-wise multi-\nrelational dependencies.\n(3) Effect of multi-channel projection. We can observe that\nTGT is superior to w/o LBD. This demonstrates that the\nmulti-channel projection layer is beneï¬cial to enhance the\nencoding of behavior semantics during the multi-behavior\npattern aggregation.\n(4) Effect of long-term multi-behavior pattern aggregation.\nThe performance gap between TGT and w/o LBD indi-\ncates the importance of capturing userâ€™s long-term prefer-\nence through discriminating different types of interactions.\nTo characterize multi-behavior sequential patterns, we de-\nsign the graph-structured embedding propagation to learn\nglobal-level user/item representations.\n(5) Effect of cross-type behavior dependency learning.\nWith the designed attention-based aggregation network, the\nrecommendation performance of our TGT is improved over\nthe variant w/o CTA (performing simpliï¬ed embedding\nconcatenation). In the variant w/o CTA, our graph learning\nframework will give same weights to the propagated type-\nspeciï¬c behavior representations, which may weaken the\nmulti-behavior collaborative effect modeling. This suggests\nthat the implicit relations across different types of interac-\ntion behaviors cannot be equally weighted. In addition, we\ndesign a new method variant FBA which fuses the multi-\nbehavior patterns with the behavior-speciï¬c interaction fre-\nquency information. From the results, we can observe that\nour TGT consistently outperforms the FBA variant with\ndifferent top-N item positions ( e.g., top-5, top-10, top-20).\nThis further justiï¬es the effectiveness of the learnable weight\nmechanism for cross-type behavior dependency modeling.\n11\n4.4 Effects of Context Behaviors (RQ3)\nThis section evaluates the inï¬‚uence of type-speciï¬c behav-\nior data in the recommendation performance for usersâ€™\ntarget behaviors. For IJCAI-Contest data, we deï¬ne â€œ-pvâ€,\nâ€œ-favâ€, â€œ-cartâ€ to represent our multi-behavior sequential\nrecommendation framework by removing the individual\nbehavior type of â€œpage viewâ€, â€œtag-as-favoriteâ€, â€œadd-to-\ncartâ€ behavior, respectively. In our ablation study, another\nvariant â€œ+buyâ€ is designed to only contain the target type\nof user behaviors for making recommendation.TGT denotes\nthe full version of our method to incorporate all types of\nuser behaviors.\nThe results (measured by HR@10 and NDCG@10) are\nshown in Figure 3. We can see that TGT consistently\nachieves the best performance in all cases, which veriï¬es\nthe signiï¬cance of context behavior diversity for modeling\nsequential interaction patterns. We can observe that the page\nview and browse behaviors contribute more for purchase\nprediction, which shows consistent trend across two differ-\nent datasets. Furthermore, we can notice that leaving be-\nhavior relation heterogeneity untapped (i.e., variant â€œ+buyâ€)\nwill degrade the recommendation performance. Overall, the\neffect studies of context behavior integration reveal that the\ncharacteristics of multi-behavior data offer useful insights\nfor modeling user sequences.\nWe conduct experiments to evaluate the model perfor-\nmance by varying the percentages of removing user aux-\niliary behaviors. In particular, the percentage of 0% indi-\ncates that we removing all behavior data under a speciï¬c\ntype (e.g., page view or tag-as- favorite or add-to-cart); the\npercentage of 50% indicates that we only keep 50% of a\nspeciï¬c type of user behaviors in the effect studies of context\nbehaviors. From the new results shown in Figure 3, we can\nobserve that incorporating more context behavior data into\nthe multi-behavior pattern modeling is helpful in improving\nthe performance.\n0% 50%0.47\n0.48\n0.49\n0.50\n0.51\n0.52HR\nTGT\n+buy\n-pv\n-fav\n-cart\n(a) IJCAI-HR\n0% 50%\n0.28\n0.30\n0.32NDCG\nTGT\n+buy\n-pv\n-fav\n-cart (b) IJCAI-NDCG\n0% 50%0.1\n0.2\n0.3\n0.4HR\nTGT\n+buy\n-pv\n-fav\n-cart\n(c) Tmall-HR\n0% 50%0.00\n0.05\n0.10\n0.15\n0.20\n0.25NDCG\nTGT\n+buy\n-pv\n-fav\n-cart (d) Tmall-NDCG\nFigure 3: Impact study of behavioral relation integration.\n4.5 Study on Model Hyperparameters (RQ4)\nTo explore the effects of modeling dynamic multi-behavior\npatterns, we study how three key hyperparameters ( i.e.,\nnumber of projection channels H, the length of sub-\nsequence |Sr\ni |, the depth of graph modelL) affect the perfor-\nmance of our TGT framework. We show the empirical study\nresults in Figure 4 and draw the following conclusions:\n4.5.1 Impact of Projection Channels\nIn the component for encoding multi-behavior patterns, we\ndesign a multi-channel parameter learning scheme for em-\nbedding transformation. From the evaluation results shown\nin Figure 4, we can observe that the best recommendation\nperformance can be achieved with the conï¬guration of two\nprojection channels. When we further increase the number\nof channels â‰¥4, the performance becomes worse due to the\noverï¬tting problem.\n4.5.2 Impact of Interaction Sub-sequence Length\nIn our TGT, the interaction sub-sequence Sr\ni serve as the\nsequence unit for short-term multi-behavior pattern model-\ning. From the results shown in Figure 4, we observe that the\ninï¬‚uence of encoded sequence length may vary by datasets.\nIn particular, the performance on IJCAI-Contest data is not\nvery sensitive to the sequence length. The reason is that the\nshort-term item-transitions could also be captured through\nthe high-order information propagation paradigm over the\nconstructed large-scale graph. When the sub-sequence is\nshort, the model cannot achieve better performance, because\nthe involved interaction behaviors are sparse.\n4.5.3 Impact of Graph Model Layers\nIn our proposed TGT framework, the number of graph neu-\nral network layers is searched in the range of{1,2,3}. We can\nobserve that increasing the number of embedding propaga-\ntion layers to 2 (TGT-2) could substantially improve the per-\nformance over TGT-1 (with the relation learning over one-\nhop connections). Such performance gain can be attributed\nto the exploration of high-order multi-behavior patterns as\nwell as the underlying cross-sequence relations. When we\nstack more propagation layers for information propagation,\nthe performance of TGT-3 begins to deteriorate. The reason\nof such observation lies in the over-smoothing issue with\nmore message passing layers, which is consistent with the\nï¬ndings in recent graph neural networks [24], [25].\n4.6 Case Study of TGT (RQ5)\nIn this section, we conduct case studies on Taobao data to of-\nfer an intuitive impression of our model explainability from\nthe perspectives of both short-term and long-term multi-\nbehavior pattern modeling in our sequential recommender.\nThe results are shown in Figure 5. The details are as follows:\nâ€¢ We sample several users and their sub-sequences as con-\ncrete examples. For example, we can observe that the pur-\nchase behavior of user u3 on item i578 shows dependence\non his/her recent activities (in the 23-th sub-sequence\nS23\n3 ) with the learned different relevance scores, such as\nthe view behavior on item i585, add-to-cart behavior on\nitem i566, and purchase behavior on item i561. Hence,\n12\n2 4 6 8 101214160.48\n0.49\n0.5\n0.51\n0.52\n0.53\nNumber of ChannelsH\nHit Rate@10\nIJCAI 2 4 6 8 101214160.28\n0.3\n0.32\n0.34\nNumber of ChannelsH\nNDCG@10\nIJCAI 4 5 6 7 8 9 100.51\n0.52\n0.52\n0.53\n0.53\nLength of Sub-Users|Sri|\nHit Rate@10\nIJCAI 4 5 6 7 8 9 100.32\n0.33\n0.33\n0.34\n0.34\nLength of Sub-Users|Sri|\nNDCG@10\nIJCAI 0 0.5 1 1.5 2 2.5 30.49\n0.5\n0.51\n0.52\n0.53\nNumber of GNN LayersL\nHit Rate@10\nIJCAI 0 0.5 1 1.5 2 2.5 30.29\n0.3\n0.31\n0.32\n0.33\n0.34\nNumber of GNN LayersL\nNDCG@10\nIJCAI\n2 4 6 8 101214160.2\n0.3\n0.4\n0.5\nNumber of ChannelsH\nHit Rate@10\nTaobao 2 4 6 8 101214160\n0.1\n0.2\n0.3\nNumber of ChannelsH\nNDCG@10\nTaobao4 6 8 1012141618200.4\n0.42\n0.44\n0.46\n0.48\n0.5\nLength of Sub-Users|Sri|\nHit Rate@10\nTaobao4 6 8 1012141618200.23\n0.24\n0.25\n0.26\n0.27\n0.28\nLength of Sub-Users|Sri|\nNDCG@10\nTaobao 0 0.5 1 1.5 2 2.5 3\n0.2\n0.3\n0.4\n0.5\nNumber of GNN LayersL\nHit Rate@10\nTaobao 0 0.5 1 1.5 2 2.5 35Â·10âˆ’2\n0.1\n0.15\n0.2\n0.25\n0.3\nNumber of GNN LayersL\nNDCG@10\nTaobao\nFigure 4: Hyperparameter study of the TGT framework on both Taobao and IJCAI datasets.\nthe behavior-aware item correlations learned by our TGT\nmodel can show the explicit relevance scores among dif-\nferent types of short-term behaviors pertinent to the userâ€™s\ntarget behavior.\nâ€¢ We present the learned item-wise dependencies encoded\nby our method across different user sub-sequences. From\nthe sampled user cases, we can observe that the purchase\nbehavior of user u0 on item i39 is correlated with his/her\nview activity on item i41 and item i41. The dependencies\nbetween view behaviors on item i41 and item i41 across\nsub-sequences S1\n0 and S2\n0 can also be captured with the\nexplicit weights by our global relational context learning\nin TGT. Therefore, the dynamic long-term behavior-aware\nrelationships between items can be preserved through our\nglobal message passing scheme.\nğ‘†0\n1\nğ‘–38 ğ‘–39 ğ‘–40 ğ‘–41 ğ‘–42\n ğ‘–41 ğ‘–42 ğ‘–39 ğ‘–38 ğ‘–39\nğ‘†0\n2\nğ‘†0\n1\nğ‘–38 ğ‘–39 ğ‘–40 ğ‘–41 ğ‘–42\n ğ‘–41 ğ‘–42 ğ‘–39 ğ‘–38 ğ‘–39\nğ‘†0\n2\n0.42\n0.21\n0.42\n0.47\n0.470.21\nğ‘–44\nğ‘–937\nğ‘–938ğ‘–939\nğ‘–940\nğ‘–69\nğ‘–495\nğ‘–489ğ‘–491\nğ‘–500\nğ‘–585\nğ‘–566\nğ‘–561\nğ‘–568\nğ‘–578\nğ‘–1652\nğ‘–1645\nğ‘–1646\nğ‘–1649\nğ‘–1650\nğ‘†9\n72 ğ‘†2\n16\nğ‘†3\n23 ğ‘†18\n139\nLearned Strong Connections Same Item\nFigure 5: Model interpretation with the case studies on the\nlearned multi-behavior relations from both short- and\nlong-term perspectives.\n4.7 Model Efï¬ciency Test\nWe conduct experiments to measure the training time of our\nproposed TGT method and several representative baselines\non both Taobao and IJCAI datasets. We report the results\nin Table 5. Evaluations are conducted on the machine with\nthe conï¬guration of NVIDIA TITAN RTX GPU, Xeon W-\n2133 CPU. We can observe that our method TGT can achieve\ncomparable model efï¬ciency when competing with state-of-\nthe-art recommendation techniques based on graph neural\nnetworks in terms of model training time. This indicates the\nscalability of our TGT in handling large-scale datasets.\nTable 5: Model training time per epoch (measured by\nseconds) on Taobao and IJCAI data.\nModel Taobao IJCAI Model Taobao IJCAI\nMATN 13.6 39.7 GCGNN 35.5 80.9\nHGT 22.7 41.4 MAGNN 32.4 86.7\nMBGCN 23.8 56.3 DeepFM 40.5 105.8\nMGNN 34.8 72.0 TGT 32.0 83.1\n5 C ONCLUSION AND DISCUSSION\nIn this paper, we propose a novel learning framework (TGT)\nfor sequential recommendation by decomposing interac-\ntions with behavior heterogeneity. The goal of TGT is to\naggregate dynamic relation contextual signals from differ-\nent types of user behaviors and generate contextualized\nrepresentations for making predictions on target behaviors.\nExtensive experiment results demonstrate the superiority of\nTGT as compared to state-of-the-art baselines, and verify\nthe rationality of incorporating multi-behavioral patterns in\nperforming sequential modeling of user-item interactions.\nMoreover, our evaluation also provides ablation study and\nefï¬ciency study to further show the model effectiveness.\nIn this work, we enhance the sequential recommender\nsystem with the consideration of multi-typed user-item in-\nteractions for user preference learning. There exist several\npromising research directions for future investigation. For\nexample, future work could involve the investigation of ex-\nternal user and item features ( e.g., user proï¬le and location,\nitem multi-modal features) to improve the recommendation\nperformance for online user behavior modeling scenarios. In\naddition, how to effectively alleviate the bias factors which\nconfounds the user preference learning for recommender\nsystems, is also desirable to explore in future work.\nACKNOWLEDGMENTS\nWe thank the reviewers for their valuable feedback and\ncomments. This research work is supported by the research\ngrants from the Department of Computer Science & Muske-\nteers Foundation Institute of Data Science at the University\nof Hong Kong (HKU). The research is also supported by\nNational Nature Science Foundation of China (62072188),\nMajor Project of National Social Science Foundation of\nChina (18ZDA062), Science and Technology Program of\nGuangdong Province (2019A050510010).\n13\nREFERENCES\n[1] Y. Cao, X. Wang, X. He, Z. Hu, and T.-S. Chua. Unifying\nknowledge graph learning and recommendation: Towards a better\nunderstanding of user preferences. In The Web conference (WWW),\npages 151â€“161, 2019.\n[2] C. Chen, M. Zhang, W. Ma, Y. Liu, and S. Ma. Jointly non-sampling\nlearning for knowledge graph enhanced recommendation. In\nInternational Conference on Research and Development in Information\nRetrieval (SIGIR). ACM, 2020.\n[3] T. Chen, H. Yin, H. Chen, R. Yan, Q. V . H. Nguyen, and X. Li.\nAir: Attentional intention-aware recommender systems. In In-\nternational Conference on Data Engineering (ICDE) , pages 304â€“315.\nIEEE, 2019.\n[4] S. Fan, J. Zhu, X. Han, C. Shi, L. Hu, B. Ma, and Y. Li. Metapath-\nguided heterogeneous graph neural network for intent recommen-\ndation. In International Conference on Knowledge Discovery & Data\nMining (KDD), pages 2478â€“2486, 2019.\n[5] W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin. Graph\nneural networks for social recommendation. In The Web Conference\n(WWW), pages 417â€“426, 2019.\n[6] H. Fang, D. Zhang, Y. Shu, and G. Guo. Deep learning for\nsequential recommendation: Algorithms, inï¬‚uential factors, and\nevaluations. Transactions on Information Systems (TOIS), 39(1):1â€“42,\n2020.\n[7] C. Gao, X. He, D. Gan, X. Chen, F. Feng, Y. Li, T.-S. Chua, and\nD. Jin. Neural multi-task recommendation from multi-behavior\ndata. In International Conference on Data Engineering (ICDE) , pages\n1554â€“1557. IEEE, 2019.\n[8] H. Guo, R. Tang, Y. Ye, et al. Deepfm: a factorization-machine\nbased neural network for ctr prediction. In International Joint\nConference on Artiï¬cial Intelligence (IJCAI) , pages 1725â€“1731, 2017.\n[9] C. Hansen, C. Hansen, L. Maystre, R. Mehrotra, B. Brost, F. Tomasi,\nand M. Lalmas. Contextual and sequential user embeddings for\nlarge-scale music recommendation. In International Conference on\nRecommender Systems (Recsys), pages 53â€“62, 2020.\n[10] R. He and J. McAuley. Fusing similarity models with markov\nchains for sparse sequential recommendation. In International\nConference on Data Mining (ICDM). IEEE, 2016.\n[11] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang. Lightgcn:\nSimplifying and powering graph convolution network for recom-\nmendation. In International Conference on Research and Development\nin Information Retrieval (SIGIR). ACM, 2020.\n[12] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, et al. Neural collaborative\nï¬ltering. In The Web conference (WWW), pages 173â€“182, 2017.\n[13] B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk. Session-based\nrecommendations with recurrent neural networks. arXiv preprint\narXiv:1511.06939, 2015.\n[14] Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph\ntransformer. In The Web conference (WWW), pages 2704â€“2710, 2020.\n[15] C. Huang. Recent advances in heterogeneous relation learning for\nrecommendation. arXiv preprint arXiv:2110.03455, 2021.\n[16] C. Huang, J. Chen, L. Xia, Y. Xu, P . Dai, Y. Chen, L. Bo, J. Zhao,\net al. Graph-enhanced multi-task learning of multi-level transition\ndynamics for session-based recommendation. In AAAI Conference\non Artiï¬cial Intelligence (AAAI), pages 4123â€“4130, 2021.\n[17] C. Huang, X. Wu, X. Zhang, C. Zhang, J. Zhao, D. Yin, and N. V .\nChawla. Online purchase prediction via multi-scale modeling\nof behavior dynamics. In International Conference on Knowledge\nDiscovery & Data Mining (KDD), pages 2613â€“2622, 2019.\n[18] C. Huang, H. Xu, Y. Xu, P . Dai, L. Xiao, M. Lu, L. Bo, H. Xing,\nX. Lai, and Y. Ye. Knowledge-aware coupled graph neural net-\nwork for social recommendation. In AAAI Conference on Artiï¬cial\nIntelligence (AAAI), 2021.\n[19] J. Huang, W. X. Zhao, H. Dou, J.-R. Wen, and E. Y. Chang.\nImproving sequential recommendation with knowledge-enhanced\nmemory networks. In International Conference on Research and\nDevelopment in Information Retrieval (SIGIR), pages 505â€“514, 2018.\n[20] B. Jin, C. Gao, X. He, D. Jin, and Y. Li. Multi-behavior recom-\nmendation with graph convolutional networks. In International\nConference on Research and Development in Information Retrieval\n(SIGIR), pages 659â€“668, 2020.\n[21] J. Jin, J. Qin, Y. Fang, K. Du, W. Zhang, Y. Yu, et al. An efï¬cient\nneighborhood-based interaction model for recommendation on\nheterogeneous graph. In International Conference on Knowledge\nDiscovery & Data Mining (KDD), pages 75â€“84, 2020.\n[22] W.-C. Kang and J. McAuley. Self-attentive sequential recommen-\ndation. In International Conference on Data Mining (ICDM) , pages\n197â€“206. IEEE, 2018.\n[23] J. Li, P . Ren, Z. Chen, Z. Ren, T. Lian, and J. Ma. Neural atten-\ntive session-based recommendation. In International Conference on\nInformation and Knowledge Management (CIKM) , pages 1419â€“1428,\n2017.\n[24] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph con-\nvolutional networks for semi-supervised learning. In International\nConference on Artiï¬cial Intelligence (AAAI) , volume 32, 2018.\n[25] M. Liu, H. Gao, and S. Ji. Towards deeper graph neural networks.\nIn International Conference on Knowledge Discovery & Data Mining\n(KDD), pages 338â€“348, 2020.\n[26] Q. Liu, Y. Zeng, R. Mokhosi, and H. Zhang. Stamp: short-term\nattention/memory priority model for session-based recommen-\ndation. In International Conference on Knowledge Discovery & Data\nMining (KDD), pages 1831â€“1839, 2018.\n[27] S. Liu, I. Ounis, C. Macdonald, and Z. Meng. A heterogeneous\ngraph neural model for cold-start recommendation. In Interna-\ntional Conference on Research and Development in Information Retrieval\n(SIGIR), pages 2029â€“2032, 2020.\n[28] C. Ma, L. Ma, Y. Zhang, J. Sun, X. Liu, et al. Memory augmented\ngraph neural networks for sequential recommendation. In Inter-\nnational Conference on Artiï¬cial Intelligence (AAAI) , 2020.\n[29] J. Ma, C. Zhou, P . Cui, H. Yang, and W. Zhu. Learning disen-\ntangled representations for recommendation. Neural Information\nProcessing Systems (NIPS), 2019.\n[30] W. Ma, M. Zhang, Y. Cao, W. Jin, C. Wang, Y. Liu, S. Ma, and\nX. Ren. Jointly learning explainable rules for recommendation\nwith knowledge graph. In The World Wide Web Conference (WWW),\npages 1210â€“1221, 2019.\n[31] W. Meng, D. Yang, et al. Incorporating user micro-behaviors and\nitem knowledge into multi-task learning for session-based recom-\nmendation. In International Conference on Research and Development\nin Information Retrieval (SIGIR), pages 1091â€“1100, 2020.\n[32] M. Quadrana, A. Karatzoglou, B. Hidasi, and P . Cremonesi.\nPersonalizing session-based recommendations with hierarchical\nrecurrent neural networks. In International Conference on Recom-\nmender Systems (Recsys), pages 130â€“137, 2017.\n[33] S. Rendle, C. Freudenthaler, et al. Factorizing personalized markov\nchains for next-basket recommendation. In International Conference\non World Wide Web (WWW), pages 811â€“820, 2010.\n[34] S. Rendle, C. Freudenthaler, Z. Gantner, et al. Bpr: Bayesian per-\nsonalized ranking from implicit feedback. International Conference\non Uncertainty in Artiï¬cial Intelligence (UAI) , pages 452â€“461, 2009.\n[35] C. Shi, B. Hu, W. X. Zhao, and S. Y. Philip. Heterogeneous infor-\nmation network embedding for recommendation. Transactions on\nKnowledge and Data Engineering (TKDE), 31(2):357â€“370, 2018.\n[36] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P . Jiang. Bert4rec:\nSequential recommendation with bidirectional encoder represen-\ntations from transformer. In International Conference on Information\nand Knowledge Management (CIKM), pages 1441â€“1450, 2019.\n[37] J. Tang and K. Wang. Personalized top-n sequential recommen-\ndation via convolutional sequence embedding. In International\nConference on Web Search and Data Mining (WSDM), pages 565â€“573,\n2018.\n[38] M. M. Tanjim, C. Su, E. Benjamin, D. Hu, L. Hong, and J. McAuley.\nAttentive sequential models of latent intent for next item recom-\nmendation. In The Web Conference (WWW), pages 2528â€“2534, 2020.\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Å. Kaiser, et al. Attention is all you need. In Neural\nInformation Processing Systems (NIPS), pages 6000â€“6010, 2017.\n[40] C. Wang, M. Zhang, W. Ma, Y. Liu, and S. Ma. Make it a chorus:\nknowledge-and time-aware item modeling for sequential recom-\nmendation. In International Conference on Research and Development\nin Information Retrieval (SIGIR), pages 109â€“118, 2020.\n[41] J. Wang, K. Ding, L. Hong, H. Liu, and J. Caverlee. Next-item\nrecommendation with sequential hypergraphs. In International\nConference on Research and Development in Information Retrieval\n(SIGIR), pages 1101â€“1110, 2020.\n[42] S. Wang, L. Hu, Y. Wang, L. Cao, Q. Z. Sheng, et al. Sequential\nrecommender systems: challenges, progress and prospects. In\nInternational Joint Conference on Artiï¬cial Intelligence (IJCAI) , 2019.\n[43] S. Wang, L. Hu, Y. Wang, Q. Z. Sheng, M. A. Orgun, and L. Cao.\nIntention2basket: A neural intention-driven approach for dynamic\nnext-basket planning. In International Joint Conference on Artiï¬cial\nIntelligence (IJCAI), pages 2333â€“2339, 2020.\n14\n[44] W. Wang, W. Zhang, S. Liu, B. Zhang, et al. Beyond clicks: Model-\ning multi-relational item graph for session-based target behavior\nprediction. In The Web Conference (WWW), pages 3056â€“3062, 2020.\n[45] X. Wang, X. He, M. Wang, F. Feng, et al. Neural graph collaborative\nï¬ltering. In International Conference on Research and Development in\nInformation Retrieval (SIGIR), pages 165â€“174. ACM, 2019.\n[46] Y. Wang, S. Tang, Y. Lei, W. Song, S. Wang, and M. Zhang.\nDisenhan: Disentangled heterogeneous graph attention network\nfor recommendation. In International Conference on Information and\nKnowledge Management (CIKM), pages 1605â€“1614, 2020.\n[47] Z. Wang, G. Lin, H. Tan, Q. Chen, and X. Liu. Ckan: Collaborative\nknowledge-aware attentive network for recommender systems. In\nInternational Conference on Research and Development in Information\nRetrieval (SIGIR), pages 219â€“228, 2020.\n[48] Z. Wang, W. Wei, G. Cong, X.-L. Li, X.-L. Mao, and M. Qiu. Global\ncontext enhanced graph neural networks for session-based recom-\nmendation. In International Conference on Research and Development\nin Information Retrieval (SIGIR), pages 169â€“178, 2020.\n[49] W. Wei, C. Huang, L. Xia, Y. Xu, J. Zhao, and D. Yin. Contrastive\nmeta learning with behavior multiplicity for recommendation. In\nInternational Conference on Web Search and Data Mining (WSDM) ,\npages 1120â€“1128, 2022.\n[50] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie. Self-\nsupervised graph learning for recommendation. In International\nConference on Research and Development in Information Retrieval\n(SIGIR), pages 726â€“735, 2021.\n[51] L. Wu, D. Hu, L. Hong, and H. Liu. Turning clicks into purchases:\nRevenue optimization for product search in e-commerce. In\nInternational Conference on Research and Development in Information\nRetrieval (SIGIR), pages 365â€“374, 2018.\n[52] L. Wu, P . Sun, Y. Fu, R. Hong, X. Wang, and M. Wang. A\nneural inï¬‚uence diffusion model for social recommendation. In\nInternational Conference on Research and Development in Information\nRetrieval (SIGIR), pages 235â€“244, 2019.\n[53] S. Wu, Y. Tang, Y. Zhu, L. Wang, et al. Session-based recommen-\ndation with graph neural networks. In International Conference on\nArtiï¬cial Intelligence (AAAI), volume 33, pages 346â€“353, 2019.\n[54] L. Xia, C. Huang, Y. Xu, P . Dai, B. Zhang, and L. Bo. Multiplex\nbehavioral relation learning for recommendation via memory\naugmented transformer network. In International Conference on\nResearch and Development in Information Retrieval (SIGIR) , pages\n2397â€“2406, 2020.\n[55] L. Xia, C. Huang, Y. Xu, P . Dai, X. Zhang, H. Yang, J. Pei, and L. Bo.\nKnowledge-enhanced hierarchical graph transformer network for\nmulti-behavior recommendation. In AAAI Conference on Artiï¬cial\nIntelligence (AAAI, volume 35, pages 4486â€“4493, 2021.\n[56] L. Xia, C. Huang, Y. Xu, J. Zhao, D. Yin, and J. X. Huang.\nHypergraph contrastive collaborative ï¬ltering. arXiv preprint\narXiv:2204.12200, 2022.\n[57] L. Xia, Y. Xu, C. Huang, P . Dai, and L. Bo. Graph meta network\nfor multi-behavior recommendation. In International Conference\non Research and Development in Information Retrieval (SIGIR) , pages\n757â€“766, 2021.\n[58] X. Xin, X. He, Y. Zhang, Y. Zhang, and J. Jose. Relational collab-\norative ï¬ltering: Modeling multiple item relations for recommen-\ndation. In International Conference on Research and Development in\nInformation Retrieval (SIGIR), pages 125â€“134, 2019.\n[59] F. Xue, X. He, X. Wang, J. Xu, K. Liu, and R. Hong. Deep item-\nbased collaborative ï¬ltering for top-n recommendation. Transac-\ntions on Information Systems (TOIS), 37(3):1â€“25, 2019.\n[60] R. Ying, R. He, K. Chen, P . Eksombatchai, W. L. Hamilton, and\nJ. Leskovec. Graph convolutional neural networks for web-scale\nrecommender systems. In International Conference on Knowledge\nDiscovery & Data Mining (KDD), pages 974â€“983. ACM, 2018.\n[61] M. Zhang, S. Wu, X. Yu, Q. Liu, and L. Wang. Dynamic graph\nneural networks for sequential recommendation. Transactions on\nKnowledge and Data Engineering (TKDE), 2022.\n[62] C. Zhao, C. Li, R. Xiao, H. Deng, and A. Sun. Catn: Cross-\ndomain recommendation for cold-start users via aspect transfer\nnetwork. In International Conference on Research and Development in\nInformation Retrieval (SIGIR), pages 229â€“238, 2020.\n[63] Y. Zheng, C. Gao, X. Li, X. He, Y. Li, and D. Jin. Disentangling\nuser interest and conformity for recommendation with causal\nembedding. In WWW, pages 2980â€“2991, 2021.\n[64] K. Zhou, H. Wang, W. X. Zhao, Y. Zhu, S. Wang, F. Zhang, Z. Wang,\nand J.-R. Wen. S3-rec: Self-supervised learning for sequential\nrecommendation with mutual information maximization. In In-\nternational Conference on Information and Knowledge Management\n(CIKM), pages 1893â€“1902, 2020.\n[65] M. Zhou, Z. Ding, et al. Micro behaviors: A new perspective in\ne-commerce recommender systems. In International Conference on\nWeb Search and Data Mining (WSDM), pages 727â€“735, 2018.\n[66] H. Zhu, F. Feng, X. He, X. Wang, et al. Bilinear graph neural net-\nwork with neighbor interactions. In International Joint Conference\non Artiï¬cial Intelligence (IJCAI), volume 5, pages 1452â€“1458, 2020.\nLianghao Xia is currently a postdoctoral fellow\nin the Department of Computer Science & Mus-\nketeers Foundation Institute of Data Science, at\nthe University of Hong Kong. He received his\nB.E. and PhD degrees from South China Uni-\nversity of Technology in 2017 and 2021, respec-\ntively. His research interests include data mining,\ngraph neural networks and recommender sys-\ntems. His research work has appeared in sev-\neral major international conferences and jour-\nnals such as SIGIR, AAAI, IJCAI, ICDE, CIKM,\nICDM as well as ACM TOIS.\nChao Huang is a tenure-track assistant pro-\nfessor in the Department of Computer Science\n& Musketeers Foundation Institute of Data Sci-\nence, at the University of Hong Kong. He ob-\ntained the PhD degree from the University of\nNotre Dame. His research focuses on applied\nmachine learning, graph neural networks, rec-\nommendation and spatial-temporal data mining.\nHis work has appeared in several major inter-\nnational conferences and journals such as KDD,\nWWW, SIGIR, IJCAI, AAAI, WSDM and etc. He\nhas served as the PC member for several top conferences including\nKDD, WWW, SIGIR, WSDM, AAAI, IJCAI, NIPS, ICLR and etc. Addi-\ntionally, he has been recognized as the outstanding reviewer in both\nACM WSDMâ€™2020 and WSDMâ€™2022 conference.\nYong Xu is a Professor at the School of Com-\nputer Science and Engineering in South China\nUniversity of Technology. His research interests\ninclude machine learning, pattern recognition\nand big data analysis. He has published over\n80 research papers in refereed journals and\nconferences ( e.g., SIGIR, AAAI, IJCAI, CIKM,\nCVPR, NIPS, ICCV, TIP , TMM and TOIS) and\nbeen serving as PC for conferences & journals\nincluding AAAI, CVPR, ICCV, TIP and etc. Dr.\nXu is a member of the IEEE Computer Society\nand the ACM.\nJian Pei is currently a Canada research chair\n(Tier 1) in big data science and a professor with\nthe School of Computing Science, Simon Fraser\nUniversity, Canada. He is one of the most cited\nauthors in data mining, database systems, and\ninformation retrieval. He has published proliï¬-\ncally and served regularly for the leading aca-\ndemic journals and conferences in his ï¬elds.\nSince 2000, he has published one textbook, two\nmonographs and over 200 research papers in\nrefereed journals and conferences, which have\nbeen cited by more than 107,186 in literature. He was the editor-in-chief\nof the IEEE Transactions of Knowledge and Data Engineering (TKDE)\nin 2013-2016. He is a fellow of the Association for Computing Machinery\n(ACM) and the Institute of Electrical and Electronics Engineers (IEEE).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8785134553909302
    },
    {
      "name": "Recommender system",
      "score": 0.7191469669342041
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.3943505883216858
    },
    {
      "name": "Information retrieval",
      "score": 0.3741520047187805
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3599046468734741
    },
    {
      "name": "Machine learning",
      "score": 0.355745404958725
    }
  ]
}