{
  "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis.",
  "url": "https://openalex.org/W3081304984",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1993312157",
      "name": "Daniel Loureiro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3022277775",
      "name": "Kiamehr Rezaee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009341098",
      "name": "Mohammad Taher Pilehvar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565581196",
      "name": "Jose Camacho Collados",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2120699290",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2507950385",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2550186622",
    "https://openalex.org/W2950538796",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W2573916414",
    "https://openalex.org/W2963850840",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W3101094968",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W3017190214",
    "https://openalex.org/W2945284729",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034782826",
    "https://openalex.org/W2251322122",
    "https://openalex.org/W2065157922",
    "https://openalex.org/W2303034160",
    "https://openalex.org/W2989478014",
    "https://openalex.org/W3038047120",
    "https://openalex.org/W2131540451",
    "https://openalex.org/W2757205734",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W1971220772",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2997248598",
    "https://openalex.org/W2978810035",
    "https://openalex.org/W1713614699",
    "https://openalex.org/W2782863194",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3098090436",
    "https://openalex.org/W190128371",
    "https://openalex.org/W2852336278",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W3029233046",
    "https://openalex.org/W2102292252",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2950502294",
    "https://openalex.org/W2953307569",
    "https://openalex.org/W191693080",
    "https://openalex.org/W2741863321",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2971531230",
    "https://openalex.org/W2951539960",
    "https://openalex.org/W2518202280",
    "https://openalex.org/W2951652751",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963212717",
    "https://openalex.org/W2091117392",
    "https://openalex.org/W3041518139",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2295097532",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3102617222",
    "https://openalex.org/W2921848006",
    "https://openalex.org/W2251529656",
    "https://openalex.org/W2968289784",
    "https://openalex.org/W2164973920",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2131357087",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2035349619",
    "https://openalex.org/W3119854206",
    "https://openalex.org/W2788445023",
    "https://openalex.org/W2950050806",
    "https://openalex.org/W3002152281",
    "https://openalex.org/W2997292130",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2251581485",
    "https://openalex.org/W1977182536",
    "https://openalex.org/W2101293500",
    "https://openalex.org/W3100614935",
    "https://openalex.org/W2990890447",
    "https://openalex.org/W1574012512",
    "https://openalex.org/W2969505328",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2250387780",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2137489006",
    "https://openalex.org/W2578530254",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2953949198",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2112796553",
    "https://openalex.org/W2293453615",
    "https://openalex.org/W2971569798",
    "https://openalex.org/W2031026221",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W3034675880",
    "https://openalex.org/W2250369422",
    "https://openalex.org/W2963956638",
    "https://openalex.org/W2788456201",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W2888282873",
    "https://openalex.org/W2511478665",
    "https://openalex.org/W2250621296",
    "https://openalex.org/W1525367170",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W3023517664",
    "https://openalex.org/W2966176804",
    "https://openalex.org/W2740540529",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2998500561",
    "https://openalex.org/W2079656678"
  ],
  "abstract": "Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations for encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT performs a decent job in capturing high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model based WSD strategies, i.e., fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data.",
  "full_text": "Analysis and Evaluation of Language Models\nfor Word Sense Disambiguation\nDaniel Loureiro∗\nLIAAD - INESC TEC\nDepartment of Computer Science - FCUP\nUniversity of Porto, Portugal\ndloureiro@fc.up.pt\nKiamehr Rezaee∗\nDepartment of Computer Engineering,\nIran University of Science and\nTechnology, Tehran, Iran\nk_rezaee@comp.iust.ac.ir\nMohammad Taher Pilehvar\nTehran Institute for Advanced Studies,\nTehran, Iran\nmp792@cam.ac.uk\nJose Camacho-Collados\nSchool of Computer Science and\nInformatics, Cardiff University,\nUnited Kingdom\ncamachocolladosj@cardiff.ac.uk\nTransformer-based language models have taken many ﬁelds in NLP by storm. BERT and its\nderivatives dominate most of the existing evaluation benchmarks, including those for Word\nSense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic\nnuances. However, there is still little knowledge about their capabilities and potential limitations\nin encoding and recovering word senses. In this article, we provide an in-depth quantitative and\nqualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the\nmain conclusions of our analysis is that BERT can accurately capture high-level sense distinctions,\neven when a limited number of examples is available for each word sense. Our analysis also reveals\nthat in some cases language models come close to solving coarse-grained noun disambiguation\nunder ideal conditions in terms of availability of training data and computing resources. However,\nthis scenario rarely occurs in real-world settings and, hence, many practical challenges remain\neven in the coarse-grained setting. We also perform an in-depth comparison of the two main\nlanguage model based WSD strategies, i.e., ﬁne-tuning and feature extraction, ﬁnding that\nthe latter approach is more robust with respect to sense bias and it can better exploit limited\navailable training data. In fact, the simple feature extraction strategy of averaging contextualized\nembeddings proves robust even using only three training sentences per word sense, with minimal\nimprovements obtained by increasing the size of this training data.\n1. Introduction\nIn the past decade, word embeddings have undoubtedly been one of the major points\nof attention in research on lexical semantics. The introduction of Word2vec (Mikolov\net al. 2013b), as one of the pioneering word embedding models, generated a massive wave\n∗Authors marked with * contributed equally\nSubmission received: 19 August 2020, Revised version received: 15 February 2021, Accepted for publication: 4\nMarch 2021.\n© 2020 Association for Computational Linguistics\narXiv:2008.11608v3  [cs.CL]  17 Mar 2021\nComputational Linguistics Volume 0, Number 1\nin the ﬁeld of lexical semantics the impact of which is still being felt today. However,\nstatic word embeddings (such as Word2vec) suffer from the limitation of being ﬁxed\nor context insensitive, i.e., the word is associated with the same representation in all\ncontexts, disregarding the fact that different contexts can trigger various meanings of\nthe word, which might be even semantically unrelated. Sense representations were an\nattempt at addressing the meaning conﬂation deﬁciency of word embeddings (Reisinger\nand Mooney 2010; Camacho-Collados and Pilehvar 2018). Despite computing distinct\nrepresentations for different senses of a word, hence addressing this deﬁciency of word\nembeddings, sense representations are not directly integrable into downstream NLP\nmodels. The integration usually requires additional steps, including a (non-optimal)\ndisambiguation of the input text, which make sense embeddings fall short of fully\naddressing the problem.\nThe more recent contextualized embeddings (Peters et al. 2018a; Devlin et al. 2019) are\nable to simultaneously address both these limitations. Trained with language modelling\nobjectives, contextualized models can compute dynamic meaning representations for\nwords in context that highly correlate with humans’ word sense knowledge (Nair,\nSrinivasan, and Meylan 2020). Moreover, contextualized embeddings provide a seamless\nintegration into various NLP models, with minimal changes involved. Even better, given\nthe extent of semantic and syntactic knowledge they capture, contextualized models get\nclose to the one system for all tasks setting. Surprisingly, ﬁne-tuning the same model\non various target tasks often results in comparable or even higher performance when\ncompared to sophisticated state-of-the-art task-speciﬁc models (Peters, Ruder, and Smith\n2019). This has been shown for a wide range of NLP applications and tasks, including\nWSD, for which they have provided a signiﬁcant performance boost, especially after\nthe introduction of Transformer-based language models like BERT (Loureiro and Jorge\n2019a; Vial, Lecouteux, and Schwab 2019; Wiedemann et al. 2019).\nDespite their massive success, there has been limited work on the analysis of recent\nlanguage models and on explaining the reasons behind their effectiveness in lexical\nsemantics. Most analytical studies focus on syntax (Hewitt and Manning 2019; Saphra\nand Lopez 2019) or explore the behaviour of self-attention heads (Clark et al. 2019) or\nlayers (Tenney, Das, and Pavlick 2019), but there has been little work on investigating the\npotential of language models and their limitations in capturing other linguistic aspects,\nsuch as lexical ambiguity. Moreover, the currently-popular language understanding\nevaluation benchmarks, e.g., GLUE (Wang et al. 2018) and SuperGLUE (Wang et al. 2019),\nmostly involve sentence-level representation which does not shed much light on the\nsemantic properties of these models for individual words.1 To our knowledge, there has\nso far been no in-depth analysis of the abilities of contextualized models in capturing the\nambiguity property of words.\nIn this article, we carry out a comprehensive analysis to investigate how pre-\ntrained language models capture lexical ambiguity in the English language. Speciﬁ-\ncally, we scrutinize the two major language model-based WSD strategies (i.e., feature\nextraction and ﬁne-tuning) under various disambiguation scenarios and experimental\nconﬁgurations. The main contributions of this work can be summarized as follows:\n(1) we provide an extensive quantitative evaluation of pre-trained language models in\nstandard WSD benchmarks; (2) we develop a new dataset, CoarseWSD-20, which is\n1 WiC (Pilehvar and Camacho-Collados 2019) is the only SuperGLUE task where systems need to model the\nsemantics of words in context (extended to several more languages in XL-WiC (Raganato et al. 2020)). In\nthe appendix we provide results for this task.\n2\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nparticularly suited for the qualitative analysis of WSD systems; and (3) with the help of\nthis dataset, we perform an in-depth qualitative analysis and test the limitations of BERT\non coarse-grained WSD. Data and code to reproduce all our experiments is available at\nhttps://github.com/danlou/bert-disambiguation.\nThe remainder of the article is organized as follows. In Section 2, we delineate the\nliterature on probing pre-trained language models and on analyzing the potential of\nrepresentation models in capturing lexical ambiguity. We also describe in the same\nsection the existing benchmarks for evaluating Word Sense Disambiguation. Section 3\npresents an overview of Word Sense Disambiguation and its conventional paradigms.\nWe then describe in the same section the two major approaches to utilizing language\nmodels for WSD, i.e., nearest neighbours feature extraction and ﬁne-tuning. We also\nprovide a quantitative comparison of some of the most prominent WSD approaches in\neach paradigm in various disambiguation scenarios, including ﬁne- and coarse-grained\nsettings. This quantitative analysis is followed by an analysis of models’ performance\nper word categories (parts of speech) and for various layer-wise representations (in the\ncase of language model based techniques). Section 4 introduces CoarseWSD-20, the WSD\ndataset we have constructed to facilitate our in-depth qualitative analysis. In Section 5\nwe evaluate the two major BERT-based WSD strategies on the benchmark. To highlight\nthe improvement attributable to contextualized embeddings, we also provide results of a\nlinear classiﬁer based on pre-trained FastText static word embeddings. Based on these\nexperiments, we carry out an analysis on the impact of ﬁne-tuning and also compare\nthe two strategies with respect to robustness across domains and bias towards the most\nfrequent sense. Section 6 reports our observations upon further scrutinizing the two\nstrategies on a wide variety of settings such as few-shot learning and different training\ndistributions. Section 7 summarizes the main results from the previous sections and\ndiscusses the main takeaways. Finally, Section 8 presents the concluding remarks and\npotential areas for future work.\n2. Related Work\nRecently, there have been several attempts at analyzing pre-trained language models. In\nSection 2.1 we provide a general overview of the relevant works, while Section 2.2 covers\nthose related to lexical ambiguity. Finally, in Section 2.3 we outline existing evaluation\nbenchmarks for WSD, including CoarseWSD-20, which is the disambiguation dataset we\nhave constructed for our qualitative analysis.\n2.1 Analysis of pre-trained language models\nDespite their young age, pre-trained language models, in particular those based on Trans-\nformers, have now dominated the evaluation benchmarks for most NLP tasks (Devlin\net al. 2019; Liu et al. 2019b). However, there has been limited work on understanding\nbehind the scenes of these models.\nVarious studies have shown that fulﬁlling the language modeling objective inherently\nforces the model to capture various linguistic phenomena. A relatively highly-studied\nphenomenon is syntax, which is investigated both for earlier LSTM-based models\n(Linzen, Dupoux, and Goldberg 2016; Kuncoro et al. 2018) as well as for the more\nrecent Transformer-based ones (Goldberg 2019; Hewitt and Manning 2019; Saphra and\nLopez 2019; Jawahar, Sagot, and Seddah 2019; van Schijndel, Mueller, and Linzen 2019;\nTenney et al. 2019). A recent work in this context is the probe proposed by Hewitt and\nManning (2019) which enabled them to show that Transformer-based models encode\n3\nComputational Linguistics Volume 0, Number 1\nhuman-like parse trees to a very good extent. In terms of semantics, fewer studies exist,\nincluding the probing study of Ettinger (2020) on semantic roles, and that of Tenney, Das,\nand Pavlick (2019) which also investigates entity types and relations. The closest analysis\nto ours is that of Peters et al. (2018b), which provides a deep analysis of contextualized\nword embeddings, both from the representation point of view and per architectural\nchoices. In the same spirit, Conneau et al. (2018) proposed a number of linguistic probing\ntasks to analyze sentence embedding models. Perhaps more related to the topic of this\npaper, Shwartz and Dagan (2019) showed how contextualized embeddings are able to\ncapture non literal usages of words in the context of lexical composition. For a complete\noverview of existing probe and analysis methods, the survey of Belinkov and Glass (2019)\nprovides a synthesis of analysis studies on neural network methods. The more recent\nsurvey of Rogers, Kovaleva, and Rumshisky (2020) is a similar synthesis but targeted at\nBERT and its derivatives.\nDespite all this analytical work, the investigation of neural language models from\nthe perspective of ambiguity (and, in particular, lexical ambiguity) has been surprisingly\nneglected. In the following we discuss studies that aimed at shedding some light on this\nimportant linguistic phenomenon.\n2.2 Lexical ambiguity and language models\nGiven its importance, lexical ambiguity has for long been an area of investigation in\nvector space model representations (Schütze 1993; Reisinger and Mooney 2010; Camacho-\nCollados and Pilehvar 2018). In a recent study on word embeddings, Yaghoobzadeh\net al. (2019) showed that Word2vec (Mikolov et al. 2013a) can effectively capture different\ncoarse-grained senses if they are all frequent enough and evenly distributed. In this\nwork we try to extend this conclusion to language model based representation and to the\nmore realistic scenario of disambiguating words in context, rather than probing them in\nisolation for if they capture speciﬁc senses (as was the case in that work).\nMost of the works analyzing language models and lexical ambiguity have opted\nfor lexical substitution as their experimental benchmark. Amrami and Goldberg (2018)\nshowed that an LSTM language model can be effectively applied to the task of word\nsense induction. In particular, they analyzed how the predictions of an LSTM for a word\nin context provided a useful way to retrieve substitutes, proving that this information\nis indeed captured in the language model. From a more analytical point of view, Aina,\nGulordava, and Boleda (2019) proposed a probe task based on lexical substitution to\nunderstand the internal representations of an LSTM language model for predicting\nwords in context. Similarly, Soler et al. (2019) provided an analysis of LSTM-based\ncontextualized embeddings in distinguishing between usages of words in context. As\nfor Transformer-based models, Zhou et al. (2019) proposed a model based on BERT to\nachieve state-of-the-art results in lexical substitution, showing that BERT is particularly\nsuited to ﬁnd senses of a word in context. While lexical substitution has been shown to\nbe an interesting proxy for WSD, we provide a direct and in-depth analysis of the explicit\ncapabilities of recent language models in encoding lexical ambiguity, both quantitatively\nand qualitatively.\nAnother related work to ours is the analysis of Reif et al. (2019) on quantifying the\ngeometry of BERT. The authors observed that, generally, when contextualized BERT\nembeddings for ambiguous words are visualized, clear clusters for different senses\nare identiﬁable. They also devised an experiment to highlight a potential failure with\nBERT (or presumably other attention-based models): it does not necessarily respect\nsemantic boundaries when attending to neighboring tokens. In our qualitative analysis\n4\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nin Section 6.4 we further explore this. Additionally, Reif et al. (2019) presents evidence\nsupporting the specialization of representations from intermediate layers of BERT for\nsense representation, which we further conﬁrm with layer-wise WSD evaluation in\nSection 3.4.5. Despite these interesting observations, the paper mostly focuses on the\nsyntactic properties of BERT, similarly to most other studies in the domain (see Section\n2.1).\nFinally, a few works have attempted to induce semantic priors coming from knowl-\nedge resources like WordNet to improve the generalization of pre-trained language\nmodels like BERT (Levine et al. 2020; Peters et al. 2019). Other works have investigated\nBERT’s emergent semantic space using clustering analyses (Yenicelik, Schmidt, and\nKilcher 2020; Chronis and Erk 2020), seeking to characterize how distinct sense-speciﬁc\nrepresentations occupy this space.\nOur work differs in that we are trying to understand to what extent pre-trained\nlanguage models already encode this semantic knowledge and, in particular, what are\ntheir implicit practical disambiguation capabilities.\n2.3 Evaluation benchmarks\nThe most common evaluation benchmarks for WSD are based on ﬁne-grained resources,\nwith WordNet (Fellbaum 1998) being the de-facto sense inventory. For example, the\nuniﬁed all-words WSD benchmark of Raganato, Camacho-Collados, and Navigli (2017)\nis composed of ﬁve datasets from Senseval/SemEval tasks, i.e., Senseval-2 (Edmonds\nand Cotton 2001, SE02), Senseval-3 (Mihalcea, Chklovski, and Kilgarriff 2004, SE03),\nSemEval-2007 (Agirre, Màrquez, and Wicentowski 2007, SE07), SemEval-2013 (Navigli,\nJurgens, and Vannella 2013, SE13), and SemEval-2015 (Moro and Navigli 2015, SE15).\nVial, Lecouteux, and Schwab (2018) extended this framework with other manually and\nautomatically constructed datasets.2 All these datasets are WordNet-speciﬁc and mostly\nuse SemCor (Miller et al. 1993) as their training set. Despite being the largest WordNet-\nbased sense-annotated dataset, SemCor does not cover many senses occurring in the test\nsets, besides providing a limited number of examples per sense. While scarcity in the\ntraining data is certainly a realistic setting, in this paper we are interested in analyzing the\nlimits of language models with and without training data, also for senses not included in\nWordNet, and run a qualitative analysis.\nTo this end, in addition to running evaluation in standard benchmarks, for this paper\nwe constructed a coarse-grained word sense disambiguation dataset, called CoarseWSD-\n20. CoarseWSD-20 includes a selection of twenty ambiguous words of different nature\n(see Section 4 for more details on CoarseWSD-20) where we run a qualitative analysis\non various aspects of sense-speciﬁc information encoded in language models. Perhaps\nthe closest datasets to CoarseWSD-20 are those of Lexical Sample WSD (Edmonds and\nCotton 2001; Mihalcea, Chklovski, and Kilgarriff 2004; Pradhan et al. 2007). These datasets\nusually target dozens of ambiguous words and list speciﬁc examples for their different\nsenses. However, these examples are usually ﬁne-grained, limited in number3 and are\nlimited to concepts (i.e., no entities such asJava are included). The CoarseWSD-20 dataset\n2 Pasini and Camacho-Collados (2020) provide an overview of existing sense-annotated corpora for WordNet\nand other resources.\n3 For instance, the dataset of Pradhan et al. (2007), which is the most recent and the largest among the three\nmentioned lexical sample datasets, provides an average of 320/50 training/test instances for each of the 35\nnouns in the dataset. In contrast, CoarseWSD-20 includes considerably larger datasets for all words (1,160\nand 510 sentences on average for each word in the training and test sets, respectively).\n5\nComputational Linguistics Volume 0, Number 1\nis similar in spirit, but has larger training sets extracted from Wikipedia. Constructing\nthe dataset based on the sense inventory of Wikipedia brings the additional advantage\nof having both entities and concepts as targets, and a direct mapping to Wikipedia pages,\nwhich is the most common resource for entity linking (Ling, Singh, and Weld 2015;\nUsbeck et al. 2015), along with similar inter-connected resources such as DBpedia.\nAnother related dataset to CoarseWSD-20 is WIKI-PSE (Yaghoobzadeh et al. 2019).\nSimilarly to ours, WIKI-PSE is constructed based on Wikipedia, but with a different\npurpose. WIKI-PSE clusters all Wikipedia concepts and entities into eight general\n“semantic classes”. This is an extreme coarsening of the sense inventory that may not\nfully reﬂect the variety of human-interpretable senses that a word has. Instead, for\nCoarseWSD-20, sense coarsening is performed at the word level which preserves sense-\nspeciﬁc information. For example, the word bank in WIKI-PSE is mainly identiﬁed as a\nlocation only, conﬂating the ﬁnancial institution and river meanings of the word. Whereas\nCoarseWSD-20 distinguishes between the two senses of bank. Moreover, our dataset is\nadditionally post-processed in a semi-automatic manner (an automatic pre-processing,\nfollowed by a manual check for problematic cases), which helps remove errors from the\nWikipedia dump.\n3. Word Sense Disambiguation: An Overview\nOur analysis is focused on the task of Word Sense Disambiguation (WSD). WSD is a core\nmodule of human cognition and a long-standing task in Natural Language Processing.\nFormally, given a word in context, the task of WSD consists of selecting the intended\nmeaning (sense) from a pre-deﬁned set of senses for that word deﬁned by a sense\ninventory (Navigli 2009). For example consider the word star in the following context:\n•Sirius is the brightest star in Earth’s night.\nThe task of a WSD system is to identify that the usage of star in this context refers to its\nastronomical meaning (as opposed to celebrity or star shape, among others). The context\ncould be a document, a sentence, or any other information-carrying piece of text that can\nprovide a hint on the intended semantic usage4, probably as small as a word, e.g., “dwarf\nstar”.5\nWSD is described as an AI-hard6 problem (Mallery 1988). In a comprehensive survey\nof WSD, Navigli (2009) discusses some of the reasons behind its difﬁculty, including\nheavy reliance on knowledge, difﬁculty in distinguishing ﬁne-grained sense distinctions,\nand lack of application to real-world tasks. On WordNet-style sense inventories, the\nhuman level performance (which is usually quoted as glass ceiling) is estimated to\nbe 80% in the ﬁne-grained setting (Gale, Church, and Yarowsky 1992a) and 90% for\nthe coarse-grained one (Palmer, Dang, and Fellbaum 2007). This performance gap can\nbe mainly attributed to the ﬁne-grained semantic distinctions in WordNet which are\nsometimes even difﬁcult for humans to distinguish. For instance, the noun star has 8\nsenses in WordNet 3.1, two of which refer to the astronomical sense (celestial body) with\nthe minor semantic difference of if the star is visible from Earth at night. In fact, it is\nargued that sense distinctions in WordNet are too ﬁne-grained for many NLP applications\n(Hovy, Navigli, and Ponzetto 2013). CoarseWSD-20 addresses this issue by devising\n4 For this analysis we focus on sentence-level WSD, since it is the most standard practice in the literature.\n5 A dwarf star is a relatively small star with low luminosity, such as the Sun.\n6 By analogy to NP-completeness, the most difﬁcult problems are referred to as AI-complete, implying that\nsolving them is equivalent to solving the central artiﬁcial intelligence problem.\n6\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nsense distinction that are easily interpretable by humans, essentially pushing the human\nperformance on the task.\nSimilarly to many other tasks in Natural Language Processing, WSD has gone\nunder signiﬁcant change after the introduction of Transformer-based language models,\nwhich are now dominating most WSD benchmarks. In the following we ﬁrst present\na background on existing sense inventories, with a focus on WordNet (Section 3.1),\nand then describe the state of the art in both the conventional paradigm (Section 3.2)\nand the more recent paradigm based on (Transformer-based) language models (Section\n3.3). We then carry out a quantitative evaluation of some of the most prominent WSD\napproaches in each paradigm in various disambiguation scenarios, including ﬁne- and\ncoarse-grained settings (Section 3.4). This quantitative analysis is followed by an analysis\nof layer-wise representations (Section 3.4.5) and performance per word categories (parts\nof speech, Section 3.4.6).\n3.1 Sense inventories\nGiven that WSD is usually tied with sense inventories, we brieﬂy describe existing sense\ninventories that are also used in our experiments. The main sense inventory for WSD\nresearch in English is the Princeton WordNet (Fellbaum 1998). The basic constituents\nof this expert-made lexical resource are synsets, which are sets of synonymous words\nthat represent unique concepts. A word can belong to multiple synsets denoting to its\ndifferent meanings. Version 3.0 of the resource, which is used in our experiments, covers\n147,306 words and 117,659 synsets.7 WordNet is also available for languages other than\nEnglish through the Open Multilingual WordNet project (Bond and Foster 2013) and\nrelated efforts.\nOther common sense inventories are Wikipedia and BabelNet. The former is gen-\nerally used for Entity Linking or Wikiﬁcation (Mihalcea and Csomai 2007), in which the\nWikipedia pages are considered as concept or entities to be linked in context. On the\nother hand, BabelNet (Navigli and Ponzetto 2012) is a merger of WordNet, Wikipedia\nand several other lexical resources, such as Wiktionary and OmegaWiki. One of the key\nfeatures of this resource is its multilinguality, highlighted by the 500 languages covered\nin its most recent release (version 5.0).\n3.2 WSD paradigms\nWSD approaches are traditionally categorized as knowledge-based and supervised. The\nlatter makes use of sense-annotated data for its training whereas the former exploits\nsense inventories, such as WordNet, for the encoded knowledge, such as sense glosses\n(Lesk 1986; Banerjee and Pedersen 2003; Basile, Caputo, and Semeraro 2014), semantic\nrelations (Agirre, de Lacalle, and Soroa 2014; Moro, Raganato, and Navigli 2014) or sense\ndistributions (Chaplot and Salakhutdinov 2018). Supervised WSD has been shown to\nclearly outperform the knowledge-based counterparts, even before the introduction of\npre-trained language models (Raganato, Camacho-Collados, and Navigli 2017). Large\npre-trained language models have further provided improvements, with BERT-based\nmodels currently approaching human-level performance (Loureiro and Jorge 2019a;\n7 There are several other variants of WordNet available, either the newer v3.1, which is slightly different\nfrom the former version, or other non-Princeton versions that improve coverage, such as WordNet 2020\n(McCrae et al. 2020) or CROWN (Jurgens and Pilehvar 2015). We opted for v3.0 given that it is the\nwidely-used inventory according to which most existing benchmarks are annotated.\n7\nComputational Linguistics Volume 0, Number 1\nVial, Lecouteux, and Schwab 2019; Huang et al. 2019; Bevilacqua and Navigli 2020;\nBlevins and Zettlemoyer 2020). A third category of WSD techniques, called hybrid, has\nrecently attracted more attention. In this approach, the model beneﬁts from both sense-\nannotated instances and knowledge encoded in sense inventories.8 Most of the recent\nstate-of-the-art approaches can be put in this category.\n3.3 Language models for WSD\nIn the context of Machine Translation (MT), a language model is a statistical model\nthat estimates the probability of a sequence of words in a given language. Recently, the\nscope of LMs has gone far beyond MT and generation tasks. This is partly due to the\nintroduction of Transformers (Vaswani et al. 2017), attention-based neural architectures\nthat have proven immense potential in capturing complex and nuanced linguistic\nknowledge. In fact, despite the young age, Transformer-based LMs dominate most\nlanguage understanding benchmarks, such as GLUE (Wang et al. 2018) and SuperGLUE\n(Wang et al. 2019).\nThere are currently two popular varieties of Transformer-based Language Models,\ndifferentiated most signiﬁcantly by their choice of language modelling objective. There\nare causal (or left-to-right) models, epitomized by GPT-3 (Brown et al. 2020), where the\nobjective is to predict the next word, given the past sequence of words. Alternatively,\nthere are masked models, where the objective is to predict a masked (i.e. hidden) word\ngiven its surrounding words, traditionally known as the Cloze task (Taylor 1953), of\nwhich the most prominent example is BERT. Benchmark results reported in Devlin et al.\n(2019) and Brown et al. (2020) show that masked LMs are preferred for semantic tasks,\nwhereas causal LMs are more suitable for language generation tasks. As a potential\nexplanation for the success of BERT-based models, Voita, Sennrich, and Titov (2019)\npresent empirical evidence suggesting that the masked LM objective induces models to\nproduce more generalized representations in intermediate layers.\nIn our experiments, we opted for the BERT (Devlin et al. 2019) and ALBERT (Lan\net al. 2020) models given their prominence and popularity. Nonetheless, our empirical\nanalysis could be applied to other pre-trained language models as well (e.g., Liu et al.\n2019b; Raffel et al. 2020). Our experiments focus on two dominant WSD approaches\nbased on language models: (1) nearest neighbors classiﬁers based on features extracted\nfrom the model (Section 3.3.1), and (2) ﬁne-tuning of the model for WSD classiﬁcation\n(Section 3.3.2). In the following we describe the two strategies.\n3.3.1 Feature extraction. Neural LMs have been utilized for WSD, even before the\nintroduction of Transformers, when LSTMs were the ﬁrst choice for encoding sequences\n(Melamud, Goldberger, and Dagan 2016; Yuan et al. 2016; Peters et al. 2018a). In this\ncontext, LMs were often used to encode the context of a target word, or in other words,\ngenerate a contextual embedding for that word. Allowing for various sense-inducing\ncontexts to produce different word representations, these contextual embeddings proved\nmore suitable for lexical ambiguity than conventional word embeddings (e.g. Word2vec).\nConsequently, Melamud, Goldberger, and Dagan (2016); Yuan et al. (2016); Peters\net al. (2018a) independently demonstrated that, given sense-annotated corpora (e.g.,\n8 Note that knowledge-based WSD systems might beneﬁt from sense frequency information obtained from\nsense-annotated data, such as SemCor. Given that such models do not incorporate sense-annotated\ninstances, we do not categorize them as hybrid.\n8\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nSemCor), it is possible to compute an embedding for a speciﬁc word sense as the average\nof its contextual embeddings. Sense embeddings computed in this manner serve as the\nbasis for a series of WSD systems. The underlying approach is straightforward: match\nthe contextual embedding of the word to be disambiguated against its corresponding\npre-computed sense embeddings. The matching is usually done using a simple k\nNearest Neighbors (often with k = 1) classiﬁer; hence, we refer to this feature extraction\napproach as 1NN in our experiments. A simple 1NN approach based on LSTM contextual\nembeddings proved effective enough to rival the performance of other systems using\ntask-speciﬁc training, such as Raganato, Delli Bovi, and Navigli (2017), despite using no\nWSD speciﬁc modelling objectives. Loureiro and Jorge (2019a, LMMS) and Wiedemann\net al. (2019) independently showed that the same approach using contextual embeddings\nfrom BERT could in fact surpass the performance of those task-speciﬁc alternatives.\nLoureiro and Jorge (2019a) also explored a propagation method using WordNet to\nproduce sense embeddings for senses not present in training data (LMMS 1024) and a\nvariant which introduced information from glosses into the same embedding space\n(LMMS2048). Similar methods have been also introduced for larger lexical resources such\nas BabelNet, with similar conclusions (Scarlini, Pasini, and Navigli 2020a, SensEmBERT).\nThere are other methods based on feature extraction, while not using 1NN for\nmaking predictions. Vial, Lecouteux, and Schwab (2019, Sense Compression) used\ncontextual embeddings from BERT as input for additional Transformer encoder layers\nwith a softmax classiﬁer on top. Blevins and Zettlemoyer (2020) also experimented\nwith a baseline using the ﬁnal states of a BERT model with a linear classiﬁer on top.\nFinally, the solution by Bevilacqua and Navigli (2020) relied on an ensemble of sense\nembeddings from LMMS and SensEmBERT, along with additional resources, to train a\nhigh performance WSD classiﬁer.\n3.3.2 Fine-tuning. Another common approach to beneﬁting from contextualized lan-\nguage models in downstream tasks is ﬁne-tuning. For each target task, it is possible to\nsimply plug in the task-speciﬁc inputs and outputs into pre-trained models, such as\nBERT, and ﬁne-tune all or part of the parameters end-to-end. This procedure adjusts\nmodel’s parameters according to the objectives of the target task, e.g., the classiﬁcation\ntask in WSD. One of the main drawbacks of this type of supervised model is their\nneed for building a model for each word, which is unrealistic in practice for all-words\nWSD. However, there are several successful WSD approaches in this category that\novercome this limitation in different ways. GlossBERT (Huang et al. 2019) uses sense\ndeﬁnitions to ﬁne-tune the language model for the disambiguation task, similarly to a\ntext classiﬁcation tasks. KnowBERT (Peters et al. 2019) ﬁne-tunes BERT for entity linking\nexploiting knowledge bases (WordNet and Wikipedia) as well as sense deﬁnitions. BEM\n(Blevins and Zettlemoyer 2020) proposes a bi-encoder method which learns to represent\nsense embeddings leveraging sense deﬁnitions while performing the optimization jointly\nwith the underlying BERT model.\n3.4 Evaluation in standard benchmarks\nIn our ﬁrst experiment, we perform a quantitative evaluation on the uniﬁed WSD\nevaluation framework (Section 3.4.3), which veriﬁes the extent to which a model can\ndistinguish between different senses of a word as deﬁned by WordNet’s inventory.\n3.4.1 BERT models. For this task we employ a Nearest Neighbors strategy (1NN\nhenceforth) that has been shown to be effective with pre-trained language models,\n9\nComputational Linguistics Volume 0, Number 1\nboth for LSTMs and more recently for BERT (see Section 3.3.1). In particular, we used\nthe cased base and large variants of BERT released by (Devlin et al. 2019), as well as\nthe xxlarge (v2) variant of ALBERT (Lan et al. 2020), via the Transformers framework\n(v2.5.1) (Wolf et al. 2020). Following LMMS, we also average sub-word embeddings and\nrepresent contextual embeddings as the sum of the corresponding representations from\nthe ﬁnal four layers. However, here we do not apply LMMS propagation method aimed\nat fully representing the sense inventory, resorting to the conventional MFS fallback for\nlemmas unseen during training.\n3.4.2 Comparison systems. In addition to BERT and ALBERT, we include results for 1NN\nsystems that exploit precomputed sense embeddings, namely Context2vec (Melamud,\nGoldberger, and Dagan 2016) and ELMo (Peters et al. 2018a). Moreover, we include\nresults for hybrid systems, i.e., supervised models that also make use of additional\nknowledge sources (cf. Section 3.2), particularly semantic relations and textual deﬁnitions\nin WordNet. Besides the models already discussed in Sections 3.3.1 and 3.3.2, we also\nreport results from additional hybrid models. Raganato, Delli Bovi, and Navigli (2017,\nSeq2Seq) trained a neural BiLSTM sequence model with losses speciﬁc not only to\nspeciﬁc senses from SemCor but also part-of-speech tags and WordNet supersenses.\nEWISE (Kumar et al. 2019), which inspired EWISER (Bevilacqua and Navigli 2020), also\nemploys a BiLSTM to learn contextual representations that can be matched against sense\nembeddings learned from both sense deﬁnitions and semantic relations.\nFor completeness we also add some of the best linear supervised baselines, namely\nIMS (Zhong and Ng 2010) and IMS with embeddings (Zhong and Ng 2010; Iacobacci, Pile-\nhvar, and Navigli 2016, IMS+emb), which are Support Vector Machine (SVM) classiﬁers\nbased on several manually-curated features. Finally, we report results for knowledge-\nbased systems (KB) that mainly rely on WordNet: Lesk ext+emb (Basile, Caputo, and\nSemeraro 2014), Babelfy (Moro, Raganato, and Navigli 2014), UKB (Agirre, López de\nLacalle, and Soroa 2018), and TM (Chaplot and Salakhutdinov 2018). More recently,\nSyntagRank (Scozzafava et al. 2020) showed best KB results by combining WordNet\nwith the SyntagNet (Maru et al. 2019) database of syntagmatic relations. However, as it\nwas discussed in Section 3.2, we categorize these as knowledge-based since they do not\ndirectly incorporate sense-annotated instances as their source of knowledge.\n3.4.3 Datasets: Uniﬁed WSD Benchmark. Introduced by Raganato, Camacho-Collados,\nand Navigli (2017) as an attempt to construct a standard evaluation framework for WSD,\nthe uniﬁed benchmark comprises ﬁve datasets from Senseval/SemEval workshops (see\nSection 2.3).9 The framework provides 7,253 test instances for 4,363 sense types. In total,\naround 3,663 word types are covered with an average polysemy of 6.2 and across four\nparts of speech: nouns, verbs, adjectives, and adverbs.\nNote that the datasets are originally designed for the ﬁne-grained WSD setting.\nNonetheless, in addition to the ﬁne-grained setting, we provide results on the coarse-\ngrained versions of the same test sets. To this end, we merged those senses that\nbelonged to the same domain according to CSI (Coarse Sense Inventory) domain labels\nfrom Lacerra et al. (2020). 10 With this coarsening, we can provide more meaningful\ncomparisons and draw interpretable conclusions. Finally, we followed the standard\nprocedure and trained all models on SemCor (Miller et al. 1993).\n9 Dataset downloaded from http://lcl.uniroma1.it/wsdeval/\n10 CSI domains downloaded from http://lcl.uniroma1.it/csi\n10\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nType System SE2 SE3 SE07 SE13 SE15 ALL\nFN CS FN CS FN CS FN CS FN CS FN CS\nKB\nLeskext+emb 63.0 74.9 63.7 75.5 56.7 71.6 66.2 77.4 64.6 73.9 63.7 75.3\nBabelfy† 67.0 78.4 63.5 77.5 51.6 68.8 66.4 77.0 70.3 79.1 65.5 77.3\nTM 69.0 - 66.9 - 55.6 - 65.3 - 69.6 - 66.9 -\nUKB 68.8 81.2 66.1 78.1 53.0 70.8 68.8 79.1 70.3 77.4 67.3 78.7\nSyntagRank 71.6 - 72.0 - 59.3 - 72.2 - 75.8 - 71.7 -\nSupervised\nSVM IMS 70.9 81.5 69.3 80.8 61.3 74.3 65.3 77.4 69.5 75.7 68.4 79.1\nIMS+emb 72.2 82.8 70.4 81.5 62.6 75.8 65.9 76.9 71.5 76.7 69.6 79.8\n1NN\nContext2vec 71.8 82.6 69.1 80.5 61.3 74.5 65.6 78.0 71.9 76.6 69.0 79.7\nELMo 71.6 82.8 69.6 80.9 62.2 74.7 66.2 77.7 71.3 77.0 69.0 79.6\nBERT-Base 75.5 84.9 71.5 81.4 65.1 78.9 69.8 82.1 73.4 78.1 72.2 82.0\nBERT-Large 76.3 84.8 73.2 82.9 66.2 80.0 71.7 83.1 74.1 79.1 73.5 82.8\nALBERT-XXL 76.6 85.6 73.1 82.6 67.3 80.1 71.8 83.5 74.3 78.3 73.7 83.0\nHybrid\nSeq2SeqAtt+Lex+PoS70.1 - 68.5 - 63.1* - 66.5 - 69.2 - 68.6* -\nSense Compr.Ens. 79.7 - 77.8 - 73.4 - 78.7 - 82.6 - 79.0 -\nLMMS1024 75.4 - 74.0 - 66.4 - 72.7 - 75.3 - 73.8 -\nLMMS2048 76.3 84.5 75.6 85.1 68.1 81.3 75.1 86.4 77.0 80.8 75.4 84.4\nEWISE 73.8 - 71.1 - 67.3* - 69.4 - 74.5 - 71.8* -\nKnowBert†WN+WK76.4 85.6 76.0 85.1 71.4 82.6 73.1 83.8 75.4 80.2 75.1 84.1\nGlossBERT 77.7 - 75.2 - 72.5* - 76.1 - 80.4 - 77.0* -\nBEM 79.4 - 77.4 - 74.5* - 79.7 - 81.7 - 79.0* -\nEWISER† 80.8 - 79.0 - 75.2 - 80.7 - 81.8* - 80.1* -\n- MFS Baseline 65.6 77.4 66.0 77.8 54.5 70.6 63.8 74.8 67.1 75.3 64.8 76.2\nTable 1: F-Measure performance on the uniﬁed WSD evaluation framework (Raganato,\nCamacho-Collados, and Navigli 2017) for three classes of WSD models, i.e., knowledge-\nbased (KB), supervised, and hybrid, and for two sense speciﬁcation settings, i.e., ﬁne-\ngrained (FN) and coarse-grained (CS). Results marked with * make use of SE07/SE15 as\ndevelopment set. Systems marked with †rely on external resources other than WordNet.\nThe results from complete rows were computed by ourselves given the system outputs,\nwhile those from incomplete rows were taken from the original papers.\n3.4.4 Results. Table 1 shows the results11 of all comparison systems on the uniﬁed word\nsense disambiguation framework, both for ﬁne-grained (FN) and coarse-grained (CS)\nversions. The LMMS 2048 hybrid model, which is based on the 1NN BERT classiﬁer\nis the best-performer based solely on feature extraction. The latest ﬁne-tuning hybrid\nsolutions, particularly BEM and EWISER, show overall best performance, making the\ncase for leveraging glosses and semantic relations to optimize pre-trained weights for the\nWSD task. Generally, all BERT-based models achieve ﬁne-grained results which are in\nthe same ballpark as human average inter-annotator agreements for ﬁne-grained WSD,\nwhich ranges from 64% and 80% in the three earlier datasets of this benchmark (Navigli\n2009). In the more interpretable coarse-grained setting, LMMS achieves a score of 84.4%,\nsimilar to the other BERT-based models which surpass 80%. The remaining supervised\nmodels perform roughly equal, marginally below 80% and clearly underperformed by\nBERT-based models.\n3.4.5 Layer performance. Current BERT-based 1NN WSD methods (see Section 3.3.1),\nsuch as LMMS and SensEmBERT, apply a pooling procedure to combine representations\n11 SensEmBERT not included because it is only applicable to the noun portions of these test sets.\n11\nComputational Linguistics Volume 0, Number 1\nLayer\n50\n60\n70\n80\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nbert-base-uncased bert-large-uncased\nFigure 1: F-measure performance on a restricted version of the MASC corpus (Ide et al.\n2008) for representations derived from individual layers of the two BERT models used in\nour experiments.\nextracted from various layers of the model. The convention is to sum the embeddings\nfrom the last four layers, following the Named Entity Recognition experiments reported\nby Devlin et al. (2019). It is generally understood that lower layers are closer to their\nstatic representations (i.e., initialization) and, conversely, upper layers better match\nthe modelling objectives (Tenney, Das, and Pavlick 2019). Still, Reif et al. (2019) have\nshown that this relation is not monotonic when it comes to sense representations from\nBERT. Additional probing studies have also pointed to irregular progression of context-\nspeciﬁcity and token identity across the layers (Ethayarajh 2019; Voita, Sennrich, and\nTitov 2019), two important pre-requisites for sense representation.\nGiven our focus on measuring BERT’s adeptness for WSD, and the known variability\nin layer specialization, we performed an analysis to reveal which layers produce\nrepresentations that are most effective for WSD. This analysis involved obtaining sense\nrepresentations learned from SemCor for each layer individually using the process\ndescribed in Section 3.3.1.\nFigure 1 shows the performance of each layer using a restricted version of the\nMASC corpus (Ide et al. 2008) as a validation set where only annotations for senses that\noccur in SemCor are considered. Any sentence that contained annotations for senses not\noccurring in SemCor was removed, restricting this validation set to 14,645 annotations out\nof 113,518. We restrict the MASC corpus so that our analysis isn’t affected by strategies\nfor inferring senses (e.g. Network Propagation) or fallbacks (e.g. Most Frequent Sense).\nThis restricted version of MASC is based on the release introduced in Vial, Lecouteux,\nand Schwab (2018), which mapped annotations to Princeton WordNet (3.0).\nSimilarly to Reif et al. (2019), we ﬁnd that lower layers are not as effective for\ndisambiguation as upper layers. However, our experiment speciﬁcally targets WSD\nand its results suggest a different distribution of the best performing layers than\nthose reported by Reif et al. (2019). Nevertheless, this analysis shows that the current\nconvention of using the sum of last four layers for sense representations is sensible, even\nif not optimal.\n12\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nType System Nouns Verbs Adjectives Adverbs\nFN CS FN CS FN CS FN CS\nKB\nUKB 71.2 80.5 50.7 69.2 75.0 82.7 77.7 91.3\nLeskext+emb 69.8 79.0 51.2 69.2 51.7 62.4 80.6 92.8\nBabelfy† 68.6 78.9 49.9 67.6 73.2 82.1 79.8 91.6\nSupervised\n1NN\nContext2vec 71.0 80.5 57.6 72.9 75.2 83.1 82.7 92.5\nELMo 70.9 80.0 57.3 73.5 77.4 85.4 82.4 92.8\nBERT-Base 74.0 83.0 61.7 75.3 77.7 84.9 85.8 93.9\nBERT-Large 75.1 83.7 63.2 76.6 79.5 85.4 85.3 94.2\nSVM IMS 70.4 79.4 56.1 72.5 75.6 84.1 82.9 93.1\nIMS+emb 71.9 80.5 56.9 73.1 75.9 83.8 84.7 93.4\nHybrid LMMS2048 78.0 86.2 64.0 76.5 80.7 86.7 83.5 92.8\nKnowBert†WN+WK 77.0 85.0 66.4 78.8 78.3 86.1 84.7 93.9\n- MFS Baseline 67.6 77.0 49.6 67.2 73.1 82.0 80.5 92.9\nTable 2: F-Measure performance in the concatenation of all datasets of the uniﬁed WSD\nevaluation framework (Raganato, Camacho-Collados, and Navigli 2017), split by Part-of-\nSpeech. As in Table 1 systems marked with †make use of external resources other than\nWordNet.\nSeveral model probing works have revealed that the scalar mixing method in-\ntroduced by Peters et al. (2018a) allows for combining information from all layers\nwith improved performance on lexico-semantic tasks (Liu et al. 2019a; Tenney et al.\n2019; de Vries, van Cranenburgh, and Nissim 2020). However, scalar mixing essentially\ninvolves training a learned probe, which can limit attempts at analysing the inherent\nsemantic space represented by NLMs (Mickus et al. 2020).\n3.4.6 Analysis by Part-of-Speech. Table 2 shows the results of BERT and the comparison\nsystems by part of speech.12 The results clearly show that verbs are substantially more\ndifﬁcult to model, which corroborates the ﬁndings of Raganato, Camacho-Collados, and\nNavigli (2017), while adverbs are the least problematic in terms of disambiguation. For\nexample, in the ﬁne-grained setting, BERT-Large achieves an overall F1 of 75.1% on nouns\nvs. 63.2% on verbs (85.3% on adverbs). The same trend is observed for other models,\nincluding hybrid ones. This may also be related to the electrophysiological evidence\nsuggesting humans process nouns and verbs differently (Federmeier et al. 2000). Another\nmore concrete reason to this gap is due to the ﬁne granularity of verb senses in WordNet.\nFor instance, the verb run has 41 sense entries in WordNet, twelve of which denote some\nkind of motion.\nThe coarsening of sense inventory does help in bridging this gap, with the best\nmodels performing in the 75% ballpark. Nonetheless, the lower performance is again\nfound in verb instances, with noun, adjective and adverb performance being above\n80% on the BERT-based models (above 90% in the case of adverbs). One problem with\nthe existing coarsening methods is that they usually exploit domain-level information,\nwhereas in some cases verbs do not belong to clear domains. For our example verb run,\nsome of the twelve senses denoting motion are clustered into different domains, which\neases the task for automatic models due to having fewer number of classes. However,\n12 For this table we only included systems for which we got access to their system outputs.\n13\nComputational Linguistics Volume 0, Number 1\none could argue that this clustering is artiﬁcial as all senses of the verb belong to the\nsame domain.\nIndeed, while the sense clustering provided by CSI (Lacerra et al. 2020) covers\nall PoS categories, it extends BabelDomains (Camacho-Collados and Navigli 2017), a\ndomain clustering resource that covers mainly nouns. While out of scope for this paper,\nin the future it would be interesting to investigate verb-speciﬁc clustering methods, e.g.,\n(Peterson and Palmer 2018).\nIn the remainder of this article we focus on noun ambiguity, and check the extent to\nwhich language models can solve coarse-grained WSD in ideal settings. In Section 7, we\nextend the discussion about sense granularity in WSD.\n4. CoarseWSD-20 Dataset\nStandard WSD benchmarks mostly rely on WordNet. This makes the evaluations carried\nout on these datasets and the conclusions drawn from them speciﬁc to this resource only.\nMoreover, sense distinctions in WordNet are generally known to be too ﬁne-grained (see\nmore details about the ﬁne granularity of WordNet in the discussion of Section 7) and\nannotations are scarce given the knowledge-acquisition bottleneck (Gale, Church, and\nYarowsky 1992a; Pasini 2020). This prevents from testing the limits of language models\nin WSD, which is one of the main motivations of this paper.\nTo this end, we devise a new dataset, CoarseWSD-20 henceforth, in an attempt to\nsolve the aforementioned limitations. CoarseWSD-20 is aimed at providing a benchmark\nfor the qualitative analysis of certain types of easily-interpretable sense distinctions.\nOur dataset also serves as a tool for testing the limits of WSD models in ideal training\nscenarios, i.e., with plenty of training data available per word.\nIn the following we describe the procedure we followed to construct CoarseWSD-20\n(Section 4.1). Then, we present an estimation of the human performance (Section 4.2) and\noutline some relevant statistics (Section 4.3). Finally, we discuss the out-of-domain test\nset we built as a benchmark for experiments in Section 5.3.\n4.1 Dataset construction\nCoarseWSD-20 targets noun ambiguity13 for which, thanks to Wikipedia, data is more\neasily available. The dataset focuses on the coarse-grained disambiguation setting, which\nis more interpretable by humans (Lacerra et al. 2020). To this end, twenty words 14\nand their corresponding senses were selected by a group of two expert computational\nlinguists in order to provide a diverse dataset. Wikipedia 15 was used as reference\ninventory and corpus. In this case, each Wikipedia page corresponds to an unambiguous\nsense. Sentences where a given Wikipedia page is referred to via a hyperlink are\nconsidered to be its corresponding sense-annotated sentences. The process to select\n13 There are arguably more types of ambiguity, including word categories (e.g.,play as a noun or as a verb).\nNevertheless, this type of ambiguity can be solved to a good extent by using state-of-the-art PoS taggers,\nwhich are able to achieve performances above 97% for English in general settings (Akbik, Blythe, and\nVollgraf 2018).\n14 The main justiﬁcation to select twenty words (and no more) was the extent of experiments and the\ncomputation required to run a deep qualitative analysis (see Section 5.1). A larger number of words would\nhave prevented us from running the analyses at the depth we envisaged: twenty provided a good trade-off\nbetween having a heterogeneous set of words and a deep qualitative analysis.\n15 We used the Wikipedia dump of May 2016.\n14\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\ntwenty ambiguous words and their corresponding sense-annotated sentences was as\nfollows:\n1. A larger set of a few hundred ambiguous words that had a minimum of thirty\noccurrences16 (i.e., sentences where one of their senses is referred to via a hyperlink)\nwas selected.\n2. Two experts selected twenty words based on a variety of criteria: type of ambiguity\n(e.g., spanning across domains or not), polysemy, overall frequency, distribution\nof instances across senses of the word, and interpretability. This process was\nperformed semi-automatically, as initially the experts ﬁltered words and senses\nmanually providing a reduced set of words and associated senses. The main goal\nof this ﬁltering was to discard those senses that were not easily interpretable or\ndistinguishable by humans.\nOnce these twenty words were selected, we tokenized and lowercased the English\nWikipedia and extracted all sentences that contained them and their selected senses\nas hyperlinks. All sentences were then semi-automatically veriﬁed so as to remove\nduplicate and noisy sentences. Finally, for each word we created a single dataset based\non a standard 60/40 train/test split.\n4.2 Human performance estimation\nAs explained above, this WSD dataset was designed to be simple for humans to annotate.\nIn other words, the senses considered for CoarseWSD-20 are easily interpretable. As a\nsanity check, we performed a disambiguation exercise with 1,000 instances randomly\nsampled from the test set (50 for each word). Four annotators 17 were asked to disam-\nbiguate a given target word in context using the CoarseWSD-20 sense inventory. Each\nannotator completed the task for ﬁve words. In the following section we provide details\nabout the results of this annotation exercise, as well as general statistics of CoarseWSD-20.\n4.3 Statistics\nTable 3 shows the list of words, their associated senses and the frequency of each word\nsense in CoarseWSD-20, along with the ratio of the ﬁrst sense with respect to the rest\n(F2E), normalized entropy18 (Ent.) and an estimation of the human accuracy (see Section\n4.2). The number of senses per word varies from 2 to 5 (eleven words with two associated\nsenses, six with three, two with four and one with ﬁve) while the overall frequency\nranges from 110 instances (68 for training) for digit to 9,240 (6,421 for training) for pitcher.\nAs for the human performance, we can see how annotators did not have special difﬁculty\nin assigning the right sense for each word in context. Annotators achieve an accuracy of\nover 96% in all cases except for a couple of senses with slightly ﬁner grained distinctions\nsuch as club and bass.\nNormalized entropy ranges from 0.04 to 0.99 (higher entropy shows more balanced\nsense distribution). While some words contain a roughly balanced distribution of senses\n16 This threshold was selected for the goal of testing the language models under close-to-ideal conditions. A\nreal setting should also include senses with even lower frequency, the so calledlong tail (Ilievski, Vossen,\nand Schlobach 2018; Blevins and Zettlemoyer 2020), which would clearly harm automatic models.\n17 All annotators were ﬂuent English speakers and understood the pre-deﬁned senses for their assigned\nwords.\n18 Computed as ∑fi log(fi) normalized by log(n) where n is the number of senses.\n15\nComputational Linguistics Volume 0, Number 1\nWord F2R Ent. Hum Senses Frequency\napple1.6 0.96 100 apple_inc 1466/634\napple 892/398\narm 2.8 0.83 100 arm_architecture 311/121\narm 112/43\nbank23.1 0.28 98 bank 1061/433\nbank_(geography) 46/22\nbass 2.9 0.67 90\nbass_guitar 2356/1005\nbass_(voice_type) 609/298\ndouble_bass 208/88\nbow 1.0 0.87 98\nbow_ship 266/117\nbow_and_arrow 185/72\nbow_(music) 72/26\nchair1.4 0.91 98 chairman 156/88\nchair 115/42\nclub 0.9 0.85 86\nclub 186/108\nnightclub 148/73\nclub_(weapon) 54/21\ncrane1.3 0.99 98 crane_(machine) 211/81\ncrane_(bird) 161/76\ndeck8.4 0.37 96 deck_(ship) 152/92\ndeck_(building) 18/7\ndigit2.2 0.74 100 numerical_digit 47/33\ndigit_(anatomy) 21/9\nhood1.6 0.88 98\nhood_(comics) 105/47\nhood_(vehicle) 42/13\nhood_(headgear) 24/22\nWord F2R Ent. Hum Senses Frequency\njava 1.4 0.96 100 java 2641/1180\njava_(progr._lang.) 1863/749\nmole 0.4 0.93 98\nmole_(animal) 148/77\nmole_(espionage) 120/44\nmole_(unit) 108/42\nmole_sauce 53/23\nmole_(architecture) 51/20\npitcher355.7 0.04 100pitcher 6403/2806\npitcher_(container) 18/13\npound6.2 0.48 100 pound_mass 160/87\npound_(currency) 26/10\nseal 0.5 0.87 100\npinniped 305/131\nseal_(musician) 267/106\nseal_(emblem) 265/114\nseal_(mechanical) 38/12\nspring0.9 0.91 100\nspring_(hidrology) 516/236\nspring_(season) 389/148\nspring_(device) 159/73\nsquare1.1 0.83 96\nsquare 264/103\nsquare_(company) 167/62\ntown_square 56/29\nsquare_number 21/13\ntrunk 1.3 0.85 100\ntrunk_(botany) 93/47\ntrunk_(automobile) 36/16\ntrunk_(anatomy) 35/14\nyard 5.3 0.62 100 yard 121/61\nyard_(sailing) 23/11\nTable 3: Target words and their associated senses, represented by their Wikipedia page\ntitle, with their overall associated frequency in CoarseWSD-20 (train/test). F2R denotes\nthe ratio of instances for ﬁrst sense to the rest, while Ent. is the normalized entropy of\nsense distribution. Moreover, the Human performance is reported in terms of accuracy.\n(e.g. crane or java), other words’ distribution are highly skewed (see normalized entropy\nvalues, e.g., for pitcher or bank).\nFinally, in the appendix we include more information for each of the senses available\nin CoarseWSD-20, including deﬁnitions and an example sentence from the dataset.\n4.4 Out of domain test set\nThe CoarseWSD-20 dataset was constructed exclusively based on Wikipedia. Therefore,\nthe variety of language present in the dataset might be limited. To verify the robustness\nof WSD models in a different setting, we constructed an out-of-domain test set from\nexisting WordNet-based datasets.\nTo construct this test set, we leveraged BabelNet mappings from Wikipedia to\nWordNet (Navigli and Ponzetto 2012) to link the Wikipedia-based CoarseWSD-20 to\nWordNet senses. After a manual veriﬁcation of all senses, we retrieved all sentences\ncontaining one of the target words in either SemCor (Miller et al. 1993) or any of the\nSenseval/SemEval evaluation datasets from Raganato, Camacho-Collados, and Navigli\n(2017). Finally, we only kept those target words for which all the associated senses were\npresent in the WordNet-based sense annotated corpora and occurred at least 10 times.\n16\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nPolysemy Normalized entropy No. of instances Sense distribution\nbank 2 0.87 48 34/14\nchair 2 0.47 40 4/36\npitcher 2 0.52 17 15/2\npound 2 0.43 46 42/4\nspring 3 0.63 31 3/24/4\nsquare 3 0.49 26 22/2/2\nclub 2 0.39 13 12/1\nTable 4: Statistics of the out of domain dataset. The two rightmost columns show the\nnumber of instances for each of the seven words and their distribution across senses.\nThis resulted in a test set with seven target words (i.e., bank, chair, pitcher, pound, spring,\nsquare and club). Table 4 shows the relevant statistics of this out-of-domain test set.\n5. Evaluation\nIn this section we report on our quantitative evaluation in the coarse-grained WSD\nsetting on CoarseWSD-20. We describe the experimental setting in Section 5.1 and then\npresent the main results on CoarseWSD-20 (Section 5.2) and the out-of-domain test set\n(Section 5.3).\n5.1 Experimental setting\nCoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses\nof the corresponding target word. Therefore, the evaluation can be framed as a standard\nclassiﬁcation task for each word.\nGiven the classiﬁcation nature of the CoarseWSD-20 datasets, we can perform\nexperiments with our 1NN BERT system and compare it with a standard ﬁne-tuned BERT\nmodel (see Section 3.3 for more details on the LM-based WSD approaches). Note that\nﬁne-tuning for individual target words results in many models (one per word). Therefore,\nthis setup would not be computationally feasible in a general WSD setting, as the number\nof models would approach the vocabulary size. However, in our experiments we are\ninterested in verifying the limits of BERT, without any other confounds or model-speciﬁc\nrestrictions.\nTo ensure our conclusions are generalizable, we also report 1NN and ﬁne-tuning\nresults using ALBERT. In spite of substantial operational differences, BERT and ALBERT\nhave the most similar training objectives and tokenization methods out of several other\nprominent Transformer-based models (Yang et al. 2019; Liu et al. 2019b), thus being\nthe most directly comparable. Given the similar performance between BERT-Large\nand ALBERT-XXLarge on the main CoarseWSD-20 dataset, we proceed with further\nexperiments using only BERT.\nWe also include two FastText linear classiﬁers (Joulin et al. 2017) as baselines: FTX-B\n(base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings\nfrom Common Crawl). We chose FastText as baseline given its efﬁciency and competitive\nresults for sentence classiﬁcation.\nConﬁguration. Our experiments with BERT and ALBERT used the Transformers frame-\nwork (v2.5.1) developed by Wolf et al. (2020), and we used the uncased pre-trained\n17\nComputational Linguistics Volume 0, Number 1\nbase and large models released by Devlin et al. (2019) for BERT, and the xxlarge\n(v2) models released by Lan et al. (2020) for ALBERT. We use the uncased variants\nof Transformers models to match the casing in CoarseWSD-20 (except for ALBERT,\nwhich is only available in cased variants). Following previous feature extraction works\n(including our experiment in Section 3.4.1), with CoarseWSD-20 we also average sub-\nword representations and use the sum of the last four layers when extracting contextual\nembeddings. For ﬁne-tuning experiments, we used a concatenation of the average\nembedding of target word’s sub-words with the embedding of the [CLS] token, and\nfed them to a classiﬁer. We used the same default hyper-parameter conﬁguration for all\nthe experiments. Given the ﬂuctuation of results with ﬁne-tuning, all the experiments\nare based on the average of three independent runs. Our experiments with FastText\nused the ofﬁcial package 19 (v0.9.1), with FastText-Base corresponding to the default\nsupervised classiﬁcation pipeline using randomly-initialized vectors, and FastText-Crawl\ncorresponding to the same pipeline but starting with pre-trained 300-dimensional vectors\nbased on Common Crawl. Following Joulin et al. (2017), classiﬁcation with FastText is\nperformed using multinomial logistic regression and averaged sub-word representations.\nEvaluation measures. In a classiﬁcation setting, the performance of a model is measured by\nvarious metrics, among which precision, recall and F-score are the most popular. Let TPi\n(true-positive) and FPi (false-positive) be the number of instances correctly / incorrectly\nclassiﬁed as class ci respectively. Also, letTNi (true-negative) and FNi (false-negative)\nbe the number of instances correctly / incorrectly classiﬁed as class cj for any j ̸= i.\nTherefore, for class ci, precision Pi and recall Ri are deﬁned as follows:\nPi = TPi\nTPi + FPi\n(1) Ri = TPi\nTPi + FNi\n(2)\nIn other words, precision is the fraction of relevant instances among the retrieved\ninstances, while recall is the fraction of the total number of relevant instances that\nwere actually retrieved. The F-score Fi for class ci is then deﬁned as the harmonic mean\nof its precision and recall values:\nFi = 2\nP−1\ni + R−1\ni\n= 2 Pi.Ri\nPi + Ri\n(3)\nIn order to have a single value to measure the overall performance of the model, we\ncan take the weighted average of these computed values over all the classes, which is\nreferred to as average micro, if the weights are set to be the number of instances for each\nclass, and macro if the weights are set to be equal. For our experiments we mainly report\nMacro-F1 and Micro-F1.\nNumber of experiments. To provide an idea of the experiments run on (including the\nanalysis in Section 6, in the following we detail the number of computations required. We\nevaluated six models, each of them trained and tested separately for each word (there are\ntwenty of them). The same models are also trained with balanced datasets (Section 6.2.1).\nIn total, 240 models trained and tested for the main results (excluding multiple runs).\n19 https://fasttext.cc/\n18\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nThen, the computationally more demanding models (BERT-Large) are also evaluated on\nthe out of domain test set, and trained with different training data sizes (Section 6.2.2)\nand with ﬁxed number of examples (Section 6.3). In the latter case being BERT-base and\nFastText models also considered (sometimes with multiple runs). As a rough estimate,\nall the experiments took over 1500 hours on a Tesla K80 GPU. These experiments do\nnot include the experiments run in the standard benchmarks (Section 3.4) and all the\nextra-analyses and prior experimental tests that did not make into the paper.\n5.2 Results\nWord-speciﬁc results for different conﬁgurations of BERT and ALBERT as well as the\nFastText baseline are shown in Table 5. In general, results are high for all Transformer-\nbased models, over 90% in most cases. This reinforces the potential of language models\nfor WSD, both in its light-weight 1NN and in the ﬁne-tuning settings. While BERT-Large\nslightly improves over BERT-Base, the performance of the former is very similar to that\nof ALBERT-XXL across different conﬁgurations, despite having different architectures,\nnumber of parameters, and training objectives. Overall, performance variations in\ndifferent models are similar to those for the human baseline. For instance, words such\nas java and digit seem easy for both humans and models to disambiguate, whereas\nwords such as bass and club are challenging perhaps due to their more ﬁne-grained\ndistinctions.20 As a perhaps surprising result, having more training instances does not\nnecessarily lead to better performance, indicated by the very low Pearson correlation\n(0.2 or lower) of the number of training instances with results in all BERT conﬁgurations.\nAlso, higher polysemy is not a strong indicator of lower performance (see Table 4.3\nfor statistics of the twenty words, including polysemy), as one would expect from a\nclassiﬁcation task with higher number of classes (near zero average correlation across\nsettings). In the following we also discuss other relevant points with respect to Most\nFrequent Sense (MFS) bias and ﬁne-tuning.\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classiﬁcation\nmodels (FastText and ﬁne-tuned BERT), indicating the inherent sense biases captured by\nthe model which lead to lowered performance for the obscure senses (see the work by\nPostma et al. (2016) for a more thorough analysis on this issue). However, BERT proves\nto be much more robust with this respect whereas FastText suffers heavily (highlighted\nin the macro setting).\nImpact of ﬁne-tuning. By average, ﬁne-tuning improves the performance for BERT-Large\nby 1.6 points in terms of micro-F1 (from 95.8% to 97.5%) but decreases on macro-F1 (from\n96.4% to 95.1%). While BERT-Base signiﬁcantly correlates with BERT-Large in the 1NN\nsetting (Pearson correlation above 0.9 for both micro and macro), it has a relatively low\ncorrelation with the ﬁne-tuned BERT-Base (0.60 on Micro-F1 and 0.75 on macro-F1). The\nsame trend is observed for BERT-Large, where the correlation between ﬁne-tuning and\n1NN is 0.71 and 0.63 on micro-F1 and macro-F1, respectively. The operating principles\nbehind both approaches are signiﬁcantly different, which may explain this relatively\nlow correlation. While ﬁne-tuning is optimizing a loss function during training, the\n20 Given that the human performance is estimated based on a small subset of the test set, and given the\nskewed distribution of sense frequencies, macro-F1 values can be highly sensitive to less-frequent senses\n(which might even have no instance in the subset); hence, we do not report macro-F1 for human\nperformance.\n19\nComputational Linguistics Volume 0, Number 1\nWord Human MFS Static emb. 1NN Fine-tune\nFTX-B FTX-C BRT-B BRT-L ALBRT BRT-B BRT-L ALBRT\nMicro-F1 (Accuracy)\ncrane 98.0 51.6 91.7 94.9 93.6 96.8 98.1 97.5 98.1 96.8\njava 100 61.2 98.8 99.4 99.6 99.6 99.6 99.7 99.7 99.5\napple 100 61.4 96.5 98.4 99.0 99.2 99.4 99.6 99.6 99.3\nmole 98.0 37.4 87.4 93.2 97.1 98.5 98.1 98.9 98.9 98.5\nspring 100 51.6 91.9 94.5 97.4 97.8 99.3 98.0 98.3 98.2\nchair 98.0 67.7 81.5 88.5 96.2 96.2 95.4 96.7 96.2 94.1\nhood 98.0 57.3 80.5 89.0 98.8 100 98.8 98.0 99.6 98.8\nseal 100 36.1 88.7 95.0 96.4 98.1 97.5 99.0 99.0 98.3\nbow 98.0 54.4 89.8 95.8 96.3 95.3 96.7 97.5 98.5 97.7\nclub 86.0 53.5 79.2 80.7 81.2 85.1 82.7 85.2 84.7 84.3\ntrunk 100 61.0 84.4 90.9 96.1 98.7 98.7 97.8 98.3 99.1\nsquare 96.0 49.8 87.0 90.3 95.2 96.1 94.2 95.8 95.7 96.5\narm 100 73.8 94.5 98.2 99.4 99.4 99.4 99.4 99.4 99.6\ndigit 100 78.6 92.9 100 100 100 100 99.2 100 100\nbass 90.0 72.3 93.9 94.2 80.7 84.5 85.5 95.5 95.8 95.7\nyard 100 84.7 86.1 94.4 76.4 88.9 93.1 98.6 99.5 99.5\npound 100 89.7 87.6 87.6 86.6 89.7 95.9 94.9 94.9 96.6\ndeck 96.0 92.9 91.9 93.9 89.9 91.9 94.9 96.6 95.3 97.0\nbank 98.0 95.2 96.9 98.0 99.6 99.8 99.8 99.6 99.3 99.3\npitcher 100 99.5 99.6 99.7 99.9 99.9 100 100 100 99.8\nA VG 66.5 90.0 93.8 94.0 95.8 96.4 97.4 97.5 97.4\nMacro-F1\ncrane – 34.0 91.7 94.8 93.5 96.7 98.1 97.5 98.1 96.8\njava – 38.0 98.7 99.4 99.7 99.6 99.6 99.7 99.7 99.5\napple – 38.1 96.2 98.1 99.0 99.1 99.3 99.6 99.6 99.3\nmole – 10.9 84.4 91.0 97.6 99.0 98.4 98.9 99.2 98.8\nspring – 22.7 91.1 94.9 97.4 97.8 99.2 97.8 98.1 98.2\nchair – 40.4 79.5 86.5 94.7 94.7 94.7 96.1 95.5 93.3\nhood – 24.3 70.5 83.2 98.5 100 98.5 97.8 99.6 98.3\nseal – 13.3 72.7 92.6 97.3 98.5 98.1 98.9 98.6 97.9\nbow – 23.5 83.3 93.7 97.0 95.7 97.3 97.5 98.6 96.8\nclub – 23.2 73.2 80.5 84.6 88.7 87.1 84.3 84.1 84.0\ntrunk – 25.3 76.0 85.9 97.9 99.3 99.3 97.6 98.0 99.0\nsquare – 16.6 67.7 76.3 92.5 94.7 89.7 92.2 91.4 93.5\narm – 42.5 92.5 98.0 99.6 99.6 99.6 99.2 99.2 99.5\ndigit – 44.0 83.3 100 100 100 100 98.8 100 100\nbass – 28.0 80.2 81.3 79.1 84.0 87.1 87.5 87.6 86.9\nyard – 45.9 54.5 81.8 86.1 93.4 95.9 97.2 99.1 99.1\npound – 47.3 48.9 53.3 92.5 94.3 97.7 84.4 83.9 90.4\ndeck – 48.2 56.1 57.1 88.0 95.7 84.1 83.4 78.0 85.2\nbank – 48.8 68.2 79.5 95.5 97.7 97.7 97.9 95.6 96.3\npitcher – 49.9 61.5 69.2 99.9 100 100 97.3 97.3 89.2\nA VG – 33.2 76.5 84.9 94.5 96.4 96.1 95.2 95.1 95.1\nTable 5: Micro-F1 (top) and macro-F1 (bottom) performance on the full CoarseWSD-20\ndataset for eight different models: FastText-Base (FTX-B) and -Crawl (FTX-C), 1NN\nand ﬁne-tuned BERT-Base (BRT-B), -Large (BRT-L), and ALBERT-XXL (ALBRT). An\nestimation of the human performance (see Section 4.2 for more details) and the most\nfrequent sense (MFS) baseline are also reported for each word. Rows in each table are\nsorted by the entropy of sense distribution (see Table 3), in descending order. Table cells\nare highlighted (from red to green) for better interpretability.\n20\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nMicro F1 Macro F1\n1NN F-T une 1NN F-T une\nBRT-B BRT-L BRT-B BRT-L BRT-B BRT-L BRT-B BRT-L\nbank 97.9 100 92.4 93.1 96.4 100 89.8 90.5\nchair 100 100 98.3 99.2 100 100 94.8 97.4\npitcher 82.4 100 100 100 90.0 100 100 100\npound 89.1 87.0 96.4 94.9 94.0 81.5 85.5 77.5\nspring 100 96.8 94.6 96.8 100 91.7 91.2 90.5\nsquare 73.1 73.1 93.6 96.2 89.4 89.4 83.2 92.6\nclub 100 100 100 100 100 100 100 100\nAVG 91.8 93.8 96.5 97.2 95.7 94.7 92.1 92.6\nTable 6: Out of domain WSD results: Models trained on the CoarseWSD-20 training set\nand tested on the out-of-domain test set.\n1NN approach is simply memorizing states. By optimizing losses, ﬁne-tuning is more\nsusceptible to overﬁt on the MFS. In contrast, by memorizing states, 1NN models senses\nindependently and disregards sense distributions entirely. These differences can explain\nthe main discrepancies between the two strategies, reﬂected for both micro and macro\nscores (macro F1 penalizes models which are not as good for less frequent senses). The\ndifferences between 1NN and ﬁne-tuned models will be analyzed in more detail in our\nanalysis section (Section 6).\nIn our error analysis we will show, among others, that there are some cases which\nare difﬁcult even for humans to disambiguate, e.g., the intended meaning of apple (fruit\nvs. company) or club (nightclub vs. association) in the following contexts taken from the\ntest set: “it also likes apple” and “she was discovered in a club by the record producer\npeter harris”.\n5.3 Out of domain\nTo verify the robustness of BERT and to see if the conclusions can be extended to other\nsettings, we carried out a set of cross-domain evaluations in which the same BERT models\n(trained on CoarseWSD-20) were evaluated on the out-of-domain dataset described in\nSection 4.4.\nTable 6 shows the results. The performance trend is largely in line with that\npresented in Table 5, with some cases even having higher performance in this out-\nof-domain test set. Despite the relatively limited size of this test set, these results seem to\ncorroborate previous ﬁndings and highlight the generalization capability of language\nmodels to perform WSD in different contexts. The ﬁne-tuned version of BERT clearly\nachieves the highest micro-F1 scores, in line with previous experiments. Perhaps more\nsurprisingly, BERT-Base 1NN achieves the best macro F1 performance, also highlighting\nits competitiveness with respect to BERT-Large in this setting. As explained before, the\n1NN strategy seems less prone to biases than the ﬁne-tuned model, and this experiment\nshows the same conclusion extends to domain speciﬁcity as well, therefore the higher\nﬁgures according to the macro metric. Interestingly, BERT-Base produces better results\naccording to macro-F1 in the 1NN setting, despite lagging behind according to micro-F1.\nThis suggests that data-intensive methods (e.g., ﬁne-tuning) do not generally lead to\n21\nComputational Linguistics Volume 0, Number 1\nFigure 2: 2-D visualizations of contextualized representations for different occurrences\nof square in the test set. While the company and public-square senses are grouped into\ndistinct clusters, the numerical and geometrical meanings mostly overlap. Using UMAP\nfor dimensionality reduction.\nsigniﬁcantly better results. Indeed, the results in Table 5 also conﬁrm that the gains using\na larger BERT model are not massive.\n6. Analysis\nIn this section we perform an analysis on different aspects relevant to WSD on the\nCoarseWSD-20 dataset. In particular, we ﬁrst present a qualitative analysis on the type of\ncontextualized embeddings learned by BERT (Section 6.1) and then analyze the impact\nof sense distribution of the training data (Section 6.2.1) as well as its size (Section 6.3) on\nWSD performance. Finally, we carry out an analysis on the inherent sense biases present\nin the pre-trained BERT models (Section 6.4).\n6.1 Contextualized embeddings\nThe strong performance of BERT-based 1NN WSD method reported for both ﬁne and\ncoarse-grained WSD proves that the representations produced by BERT are sufﬁciently\nprecise to allow for effective disambiguation. Figures 2 and 3 illustrate the 2-D semantic\nspace for contextualized representations of two target words (square and spring) in the\ntest set. For each case, we applied the dimensionality technique that produced the\nmost interpretable visualization, considering UMAP (McInnes et al. 2018) and Principal\nComponent Analysis (PCA), although similar observations could be made using either\nof these two techniques. BERT is able to correctly distinguish and place most occurrences\nin distinct clusters. Few challenging exceptions exist, e.g., two geometric senses of square\n22\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nFigure 3: 2-D visualizations of contextualized representations for different occurrences\nof spring. A ﬁne-grained distinction can be observed for the season meaning of spring,\nwith a distinct cluster (on the right) denoting the spring of a speciﬁc year. Using PCA for\ndimensionality reduction.\nare misclassiﬁed as public-square, highlighted in the ﬁgure (“... small square park located\nin ...” and “ ... the narrator is a square ...”). Another interesting observation is for the\nseason meaning of spring. BERT not only places all the contextualized representations for\nthis sense in the same proximity in the space, it also makes a ﬁne-grained distinction for\nthe spring season of a speciﬁc year (e.g., “... in spring 2005 ...”).\nBeyond simply checking whether the nearest neighbor corresponds to the correct\nsense, there is still the question of the extent to which these representations are differ-\nentiated. In order to quantitatively analyse this, we plotted the distribution of cosine\nsimilarities between the contextual embeddings of the target word (to be disambiguated)\nfrom the test set and the closest predicted sense embedding learned from the training set.\nIn Figure 4 we grouped these similarities by correct and incorrect predictions, revealing\nsubstantially different distributions. While incorrect prediction spans across the 0.5-0.9\ninterval, correct predictions are in the main higher than 0.75 for most words (over 97%\nof all predictions using BERT-Large with similarity higher than 0.75 are correct, for\nexample). Consequently, this analysis also shows that a simple threshold could be used\nfor effectively discarding false matches, increasing the precision of 1NN methods.\n23\nComputational Linguistics Volume 0, Number 1\nFigure 4: Distribution of cosine similarities between contextual embeddings (BERT-\nLarge) of words to be disambiguated (in test set) and their corresponding closest sense\nembeddings learned from training data, for each word in the CoarseWSD-20 dataset,\ngrouped by correct and incorrect prediction.\n6.2 Role of training data\nIn order to gain insights on the role of training data, we perform two types of analysis:\n(1) distribution of training data and, in particular, a comparison between skewed and\nbalanced training sets (Section 6.2.1), and (2) the size of the training set (Section 6.2.2).\n6.2.1 Distribution. To verify the impact of the distribution of the training data, we\ncreated a balanced training dataset for each word by randomly removing instances for\nthe more frequent senses in order to have a balanced distribution over all senses. Note\nthat the original CoarseWSD-20 dataset has a skewed sense distribution, given that it is\nconstructed based on naturally-occurring texts.\nTable 7 shows the performance drop or increase when using a fully balanced training\nset instead of the original CoarseWSD-20 skewed training set (tested on the original\nskewed test set). Performance is generally similar across the two settings for the less\nentropic words (on top) that tend to have more uniform distributions. For the more\nentropic words (e.g., deck, bank or pitcher), even though balancing the data inevitably\nreduces the overall number of training instances to a large extent, it can result in improved\nmacro results for FastText, and even improved macro-recall results for ﬁne-tuning, as we\nwill see in Table 8.\nThis can be attributed to the better encoding of the least frequent senses, which\ncorroborates the ﬁndings of Postma, Izquierdo Bevia, and Vossen (2016) for conventional\nsupervised WSD models, such as IMS or, in this case, FastText. In contrast, the micro-\naveraged results clearly depend on accurately knowing the original distribution in both\nthe supervised and ﬁne-tuning settings, as was also discussed in previous works (Bennett\net al. 2016; Pasini and Navigli 2018). Moreover, the feature extraction procedure (1NN\nin this case) is much more robust to training distribution changes. Indeed, being solely\nbased on vector similarities, the 1NN strategy is not directly inﬂuenced by the number\nof occurrences of each sense in the CoarseWSD-20 training set.\n24\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nMicro F1 Macro F1\nStatic emb. 1NN F-T une Static emb. 1NN F-T une\nFTX-B FTX-C BRT-B BRT-L BRT-B BRT-L FTX-B FTX-C BRT-B BRT-L BRT-B BRT-L\ncrane -3.8 0.0 0.6 0.0 0.0 0.0 -3.7 0.0 0.6 0.0 0.0 0.0\njava -0.1 0.1 0.0 0.0 0.0 -0.1 -30.3 -15.1 0.1 0.0 0.0 -0.1\napple -0.2 -0.6 0.0 0.0 0.0 -0.1 0.4 -0.4 0.0 0.0 0.0 -0.1\nmole -11.2 -1.5 0.0 0.0 -0.7 -0.7 -0.9 2.0 0.0 0.0 -0.5 -0.7\nspring -5.0 -2.0 0.0 0.2 -1.1 -0.9 -12.3 1.5 -0.2 0.1 -1.0 -0.7\nchair -6.2 -3.1 0.0 0.0 -1.0 0.3 -4.5 -2.3 0.0 0.0 -1.2 0.3\nhood -7.3 -1.2 0.0 -1.2 -0.4 0.0 12.2 4.4 -0.8 -1.5 -0.9 -0.3\nseal -23.1 -7.2 0.3 0.0 -2.9 -0.7 -9.0 -11.5 0.2 0.0 -7.3 -2.4\nbow -9.3 -3.7 0.0 0.0 -1.4 -0.8 -2.3 -2.0 0.0 0.0 -1.8 -1.5\nclub -16.8 -5.9 0.0 -1.5 -0.8 -3.0 -8.6 -0.6 -0.3 -1.5 -0.4 -2.4\ntrunk -13.0 -9.1 -3.9 0.0 -0.9 -1.7 -6.4 -4.3 -2.1 0.0 -0.9 -1.7\nsquare -23.7 -8.2 -6.8 -7.7 -4.7 -1.3 1.4 9.6 -3.4 -3.9 -4.8 1.1\narm -2.4 -1.2 0.0 0.0 0.0 0.0 0.6 -0.8 0.0 0.0 0.0 0.0\ndigit -16.7 -7.1 0.0 0.0 0.8 0.0 1.5 -4.5 0.0 0.0 1.2 0.0\nbass -9.1 -8.2 0.4 0.8 -5.1 -4.4 6.8 6.5 0.5 0.9 -5.6 -4.0\nyard -12.5 -5.6 -2.8 -4.2 -6.0 -2.3 18.2 11.6 -1.6 -2.5 -8.9 -3.9\npound -34.0 -24.7 0.0 -1.0 -8.9 -1.4 18.5 36.7 7.5 -0.6 -8.8 2.0\ndeck -26.3 -9.1 -2.0 -1.0 -5.7 -3.7 12.3 28.1 -1.1 -0.5 -5.0 2.1\nbank -17.4 -10.3 0.2 0.0 -2.6 -1.9 10.3 9.7 2.3 0.0 -10.6 -6.5\npitcher -13.0 -6.4 -0.1 0.0 -1.3 -0.4 16.8 22.4 0.0 0.0 -26.7 -12.7\nA VG -12.6 -5.8 -0.7 -0.8 -2.1 -1.1 1.0 4.6 0.1 -0.5 -4.2 -1.6\nTable 7: Performance drop or increase when using a fully balanced training set instead of\nthe original CoarseWSD-20 skewed training set.\nTo complement these results, Table 8 shows the performance difference on the MFS\n(Most Frequent Class) and LFS (Least Frequent Class) classes when using the balanced\ntraining set. The most interesting takeaway from this experiment is the marked difference\nbetween precision and recall for the LFS in entropic words (bottom). While the recall of\nthe BERT-Large ﬁne-tuned model increases signiﬁcantly (up to 52.4 points in the case\nof deck), the precision decreases (e.g. -27.1 points for deck). This means that the model\nis clearly less biased towards the MFS with a balanced training set, as we could expect.\nHowever, the precision for LFS is also lower, due to the model’s lower sensitivity for\nhigher-frequency senses. In general, these results suggest that the ﬁne-tuned BERT model\nis overly sensitive to the distribution of the training data, while its feature extraction\ncounterpart suffers considerably less from this issue. In Section 6.4 we will extend the\nanalysis on the bias present in each of the models.\n6.2.2 Size. We performed an additional experiment to investigate the impact of training\ndata size on the performance for the most and least frequent senses. To this end, we\nshrank the training dataset for all words, while preserving their original distribution.\nTable 9 shows a summary of the aggregated micro-F1 and macro-F1 results, including the\nperformance on the most and least frequent senses.21 Clearly, the 1NN model performs\nconsiderably better than ﬁne-tuning in settings with low training data (e.g., 74.2% to\n21 In the appendix we include detailed results for each word and their MFS and LFS performance.\n25\nComputational Linguistics Volume 0, Number 1\nF-T une (BRT-L) 1NN (BRT-L)\nPrecision Recall Precision Recall\nMFS LFS MFS LFS MFS LFS MFS LFS\ncrane 0.4 -0.4 -0.4 0.4 0.0 0.0 0.0 0.0\njava 0.0 -0.3 -0.2 0.0 0.0 0.0 0.0 0.0\napple -0.1 -0.1 -0.1 -0.2 0.0 0.0 0.0 0.0\nmole -0.9 -0.8 -0.9 -1.5 0.0 0.0 0.0 0.0\nspring -0.6 -1.0 -1.3 -1.4 0.0 0.6 0.0 0.0\nchair 0.0 0.9 0.4 0.0 0.0 0.0 0.0 0.0\nhood 0.7 0.0 0.0 -2.6 -2.1 0.0 0.0 0.0\nseal -0.3 0.0 -0.5 0.0 0.0 0.0 0.0 0.0\nbow 0.8 -1.0 -0.6 -1.4 0.0 0.0 0.0 0.0\nclub -3.4 -1.6 -2.2 -6.9 -3.9 0.0 0.9 -5.5\ntrunk 0.7 -7.4 -3.6 0.0 0.0 0.0 0.0 0.0\nsquare 6.5 -0.5 -9.4 0.0 -0.4 0.0 -15.5 0.0\narm 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\ndigit 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\nbass 2.5 -0.4 -8.6 -0.8 -0.7 1.8 0.7 1.1\nyard 0.5 -14.0 -3.3 3.0 0.0 -7.9 -4.9 0.0\npound 4.0 -23.4 -5.8 36.7 -2.4 0.0 0.0 -1.1\ndeck 3.9 -27.1 -8.0 52.4 -2.9 0.0 0.0 -1.1\nbank 0.8 -32.5 -2.8 15.2 0.0 0.0 0.0 0.0\npitcher 0.1 -46.0 -0.4 10.3 0.0 0.0 0.0 0.0\nAVG 0.8 -7.8 -2.4 5.2 -0.6 -0.3 -0.9 -0.3\nTable 8: Precision and recall drop or increase on the Most Frequent Sense (MFS) and\nLeast Frequent Sense (LFS) classes when using a fully balanced training set.\nFine-tuning (BRT-L) 1NN (BRT-L)\n1% 5% 10% 25% 50% ALL% 1% 5% 10% 25% 50% ALL%\nMacro 74.2 81.6 85.8 91.5 94.2 95.1 94.4 95.3 95.6 95.8 96.0 96.4\nMicro 89.0 93.5 95.3 96.3 97.0 97.5 95.5 95.8 95.7 95.7 95.6 95.8\nMFS 91.9 95.3 96.4 97.2 97.5 98.0 95.8 95.8 95.6 95.6 95.4 95.4\nLFS 52.1 64.3 71.9 83.4 88.5 91.0 91.6 93.3 94.1 94.6 95.5 96.6\nTable 9: Macro- and micro-F1 % performance for the two BERT-Large models. The last\ntwo rows indicate the F1 performance on the Most Frequent Sense (MFS) and Least\nFrequent Sense (LFS) classes.\n94.4% macro-F1 with 1% of the training data). Interestingly, the 1NN’s performance\ndoes not deteriorate with few training data, as the results with 1% and 100% of the\ntraining data do not vary much (less than two absolute points decrease in performance\nfor micro-F1 and 0.3 in terms of micro-F1). Even for the LFS, the overall performance\nwith 1% of the training data is above 90 (i.e., 91.6). This is an encouraging behaviour, as\nin real settings sense-annotated data is generally scarce.\n26\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nFine-tuning (BRT-L) 1NN (BRT-L)\n1% 5% 10% 25% 50% ALL 1% 5% 10% 25% 50% ALL\ncrane 83.3 95.7 95.7 96.8 95.5 98.1 96.4 96.6 96.7 96.7 96.7 96.7\njava 99.0 99.1 99.6 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6\napple 99.3 99.4 99.4 99.4 99.5 99.6 99.1 99.1 99.1 99.1 99.1 99.1\nmole 79.8 94.8 97.6 99.3 99.3 99.2 98.6 99.1 99.0 99.0 99.0 99.0\nspring 94.8 97.6 96.8 96.9 97.8 98.1 97.9 97.9 97.9 97.9 97.9 97.8\nchair 76.2 92.2 95.2 96.1 96.4 95.5 94.3 94.6 94.7 94.7 94.7 94.7\nhood 57.2 89.3 92.3 96.6 97.7 99.6 94.7 98.6 99.2 99.5 100 100\nseal 80.3 95.8 96.5 98.2 98.0 98.6 98.6 98.6 98.7 98.6 98.6 98.5\nbow 49.3 86.8 95.7 96.0 97.5 98.6 93.5 96.0 96.2 95.9 95.7 95.7\nclub 70.1 77.4 77.0 80.0 83.0 84.1 85.6 86.5 87.4 87.6 88.0 88.7\ntrunk 77.9 84.6 97.5 98.6 98.6 98.0 97.7 98.3 98.7 99.3 99.3 99.3\nsquare 68.4 69.6 73.5 76.6 79.4 91.4 86.7 88.0 87.8 88.1 91.1 94.7\narm 90.1 98.1 99.2 99.2 99.2 99.2 99.6 99.6 99.6 99.6 99.6 99.6\ndigit 92.4 79.7 92.1 98.8 100 100 99.1 100 100 100 100 100\nbass 72.2 79.4 84.3 86.7 87.8 87.6 83.1 83.8 84.4 84.8 84.8 84.0\nyard 82.7 85.7 88.3 94.3 99.1 99.1 93.4 93.4 92.8 92.6 92.2 93.4\npound 53.5 50.4 47.3 52.6 83.2 83.9 87.0 92.4 93.3 93.2 94.3 94.3\ndeck 56.7 48.2 48.2 70.2 77.2 78.0 85.5 85.1 88.9 91.1 92.1 95.7\nbank 50.2 55.9 74.9 97.1 95.7 95.6 97.0 98.6 98.9 98.5 97.7 97.7\npitcher 49.9 52.3 63.9 96.5 99.3 97.3 100 100 100 100 100 100\nAverage 74.2 81.6 85.8 91.5 94.2 95.1 94.4 95.3 95.6 95.8 96.0 96.4\nTable 10: Macro-F1 results on the CoarseWSD-20 test set using training sets of different\nsizes sampled from the original training set.\nTo get a more detailed picture for each word, Table 10 shows the macro-F1 results for\neach word and training size.22 Again, we can observe a large drop for the most entropic\nwords in the ﬁne-tuning setting. Examples of words with a considerable degrading\nperformance are pitcher or bank, which decrease from macro-F1 scores higher than 95% in\nboth cases (97.3 and 95.6, respectively) to as low as 49.9 and 50.2 (almost random chance)\nwith 1% of the training data, and still lower than 75% with 10% of the training data (63.9\nand 74.9, respectively). This trend clearly highlights the need for gathering reasonable\namounts of training data for the obscure senses. Moreover, this establishes a trade-off\nbetween balancing or preserving the original skewed distribution depending on the end\ngoal, as discussed in Section 6.2.1.\n6.3 n-shot learning\nGiven the results of the previous section, one may wonder how many instances would be\nenough for BERT to perform well in coarse-grained WSD. To verify this, we ﬁne-tuned\nBERT on limited amounts of training data, with uniform distribution over word senses,\neach having between 1 (i.e., one-shot) and 30 instances. Figure 5 shows the performance of\nboth 1NN and Fine-Tuning strategies on this set of experiments. Perhaps surprisingly, we\ncan see how having only three instances per sense is enough for achieving a competitive\nresult. Then, only small improvements can be obtained by adding more instances. This\n22 In the appendix we include the same table for the micro-F1 results.\n27\nComputational Linguistics Volume 0, Number 1\nFigure 5: Micro and macro F-scores for different values of n in the n-shot setting, for all\nthe words and for the two WSD strategies. Results are averaged from three runs over\nthree different samples.\nis relevant in the context of WSD, as generally current sense-annotated corpora follow\nthe Zipf’s law (Zipf 1949), and therefore contain many repeated senses that are very\nfrequent. Signiﬁcant improvements may therefore be obtained by simply getting a few\nsense annotations for less frequent instances. Figure 6 summarizes Figure 5 by showing\nthe distribution of words according to their performance in the two strategies. In the case\nof Fine-Tuning, the performance is generally better in terms of micro compared to macro\nF-score. This further corroborates the previous observation, that there is a bias towards\nthe most frequent sense (cf. Section 6.2.1). Additionally, in contrast to 1NN, Fine-tuning\ngreatly beneﬁts from the increase in the training-data size, which also indicates the more\nrobust behaviour of 1NN strategy compared to its counterpart (cf. Section 6.2.1).\n6.4 Bias analysis\nSupervised classiﬁers are known to have label bias towards more frequent classes, i.e.,\nthose that are seen more frequently in the training data (Hardt et al. 2016), and this is\nparticularly noticeable in WSD (Postma, Izquierdo Bevia, and Vossen 2016; Blevins and\nZettlemoyer 2020). Label bias is a reasonable choice for maximizing performance when\nthe distribution of classes is skewed, particularly for classiﬁcation tasks with a small\nnumber of categories (which is often the case in WSD). For the same reason, many of the\nknowledge-based systems are coupled with the MFS back-off strategy: when the system\nis not conﬁdent in its disambiguation, it backs off to the most frequent sense (MFS) of\nthe word (instead of resorting to the low-conﬁdence decision).\nWe were interested in investigating the inherent sense biases in the two BERT-based\nWSD strategies. We opted for the n-shot setting given that it provides a suitable setting\nfor evaluating the relationship between sense bias and training data size. Moreover,\ngiven that the training data in the n-shot setting is uniformly distributed (balanced), the\nimpact of sense-annotated training data in introducing sense bias is minimized. This\nanalysis is mainly focused on two questions: (1) how do the two strategies (ﬁne-tuning\n28\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nFigure 6: Distribution of performance scores for all 20 words according to micro and\nmacro F1 in the two WSD strategies (left: ﬁne-tuning, right: 1NN) and for different values\nof n, i.e., 1, 3, 10, 30 (if available).\nand 1NN) compare in terms of sense bias?, and (2) what are the inherent sense biases (if\nany) in the pre-trained BERT language model?\n6.4.1 Sense bias deﬁnition. We propose the following procedure for computing the\ndisambiguation bias towards a speciﬁc sense. 23 For a word with polysemy n, we are\ninterested in computing the disambiguation bias Bj towards its jth sense ( sj). Let\nnij be the total number of test instances with the gold label si that were mistakenly\ndisambiguated as sj (i ̸= j). We ﬁrst normalize nij by the total number of (gold-labeled)\ninstances for si, i.e., Σjnij, to obtain bias bij, which is the bias from sense i to sense j. In\nother words, bij denotes the ratio of si-labeled instances which were misclassiﬁed as sj.\nThe total bias towards a speciﬁc sense, Bj, is then computed as:\nBj =\nn∑\ni=1\ni̸=j\n( nij\nΣjnij\n) (4)\nThe value of Bj denotes the tendency of the disambiguation system to disambiguate\na word with the intended sense of sk, k ̸= j incorrectly as sj. The higher the value of\nBj, the more the disambiguation model is biased towards sj. We ﬁnally compute the\nsense bias B as the maximum Bj value towards different senses of a speciﬁc word, i.e.,\nmax(Bj), j∈[1, n]. Given ﬂuctuations in the results, particularly for the case of small\ntraining data, we take the median of three runs to compute Bj.\nIn our coarse-grained disambiguation setting, the bias B can be mostly attributed to\nthe case where the system did not have enough evidence to distinguish sj from other\nsenses and had pre-training bias towards sj. One intuitive explanation for this would be\nthat the language model is biased towards sj because it has seen the target word more\noften with this intended sense than other sk,j̸=k senses.\n23 The procedure can presumably be used for quantifying bias in other similar classiﬁcation settings.\n29\nComputational Linguistics Volume 0, Number 1\nOne-shot 3-shot 10-shot 30-shot\nF-Tune 1NN F-Tune 1NN F-Tune 1NN F-Tune 1NN\n0.232 0.137 0.111 0.078 0.050 0.052 0.021 0.025\nTable 11: Average sense bias values ( B) for the two WSD strategies and for different\nvalues of n.\nFigure 7: Sense bias for a few representative cases from each polysemy class for the two\nWSD strategies (left: ﬁne-tuning, right: 1NN) and for different values of n, i.e., 1, 3, 10, 30\n(if available).\n6.4.2 Results. Table 11 reports the average sense bias values ( B) for the two WSD\nstrategies and for different values of n (training data size) in the n-shot setting. We\nalso illustrate using radar charts in Figure 7 the sense bias for a few representative cases.\nThe numbers reported in the ﬁgure (in parentheses) represent the bias value B for the\ncorresponding setting (word, WSD strategy, andn’s value).\nBased on our observations, we draw the following general conclusions.\nBias and training size. There is a consistent pattern across all words and for both the\nstrategies: sense bias rapidly reduces with increase in the training data. Speciﬁcally, the\naverage bias B approximately reduces by half with each step of increase in the training\nsize. This is supported by the radar charts in Figure 7 (see, for instance, apple, yard, and\nbow). The WSD system tends to be heavily biased in the one-shot setting (particularly in\n30\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nthe ﬁne-tuning setting), but the bias often improves signiﬁcantly with just 3 instances in\nthe training data (3-shot).\nDisambiguation strategy: 1NN vs. Fine-tuning. Among the two WSD strategies, the 1NN\napproach proves to be more robust with respect to sense biases. This is particularly\nhighlighted in the one-shot setting where the average sense bias value is 0.137 for 1NN in\ncomparison to 0.232 for ﬁne-tuning. The trend is also clearly visible for almost all words\nin the radar charts in Figure 7. This corroborates our ﬁndings in Section 6.3 that the 1NN\nstrategy is the preferable choice particularly with limited data. For higher values of n\n(larger training sizes) the difference between the two strategies diminishes, with both\nsettings proving robust with respect to sense bias.\nIt is also notable that the two strategies, despite being usually similar in behaviour,\nmight not necessarily have matching biases towards the same senses. For instance, the\nﬁne-tuning setting shows bias only towards the arrow sense of bow, whereas 1NN is\ninstead (slightly) biased towards its music sense. Another example is for the word digit\nfor which with the same set of training instances in the one-shot setting (one sentence for\neach of the two senses), all the mistakes (5 in total) of the ﬁne-tuning model are numerical\ndigit incorrectly tagged as anatomical, whereas all the mistakes in the 1NN setting (5 in\ntotal) are the reverse.\nFinally, we also observed that for cases with subtle disambiguation, both the\nstrategies failed consistently in the one-shot setting. For instance, a common mistake\nshared by the two strategies was for cases where the context contained semantic cues for\nmultiple senses, e.g., “the English word digit as well as its translation in many languages\nis also the anatomical term for ﬁngers and toes.” in which the intended meaning of digit\nis the numerical one (both strategies failed on disambiguation this). This observation\nis in line with the analysis of Reif et al. (2019) which highlighted the failure of BERT in\nidentifying semantic boundaries of words.\nPre-training label bias. In most of the conventional supervised WSD classiﬁers (such as\nIMS), which rely on sense-annotated training data as their main source of information,\nthe source of sense bias is usually the skewed distribution of instances for different senses\nof a word (Pilehvar and Navigli 2014). For instance, the word digit would appear much\nmore frequently with its numerical meaning than the ﬁnger meaning in an open-domain\ntext. Therefore, a sense-annotated corpus that is sampled from open-domain texts shows\na similar sense distribution, resulting in a bias towards more frequent senses in the\nclassiﬁcation.\nGiven that in the n-shot setting we restrict the training datasets to have a uniform\ndistribution of instances, sense bias in this scenario can be indicative of inherent sense\nbiases in BERT’s pre-training. We observed that the pre-trained BERT indeed exhibits\nsense biases, often consistently across the two WSD strategies. For instance, we observed\nthe following biases towards (often) more frequent senses of words: java towards its\nprogramming sense (rather than island), deck towards ship deck (rather than building\ndeck), yard towards its sailing meaning (rather than measure unit), and digit and square\ntowards their numerical meanings. We also observed some contextual cues that misled\nthe WSD system, especially in the one-shot setting. For instance, we observed that our\nBERT-based WSD system had tendency to classify square as its digit meaning whenever\nthere was a number in its context, e.g., “marafor is a roman square with two temples\nattached” or “it has 4 trapezoid and 2 square faces”. Not surprisingly, the source of most\nbias towards the digit sense of square is from its geometrical sense (which has domain\nrelatedness). Also, classiﬁcation for digit was often biased towards its numerical meaning.\n31\nComputational Linguistics Volume 0, Number 1\nSimilarly to the case of square, the existence of a number in context seems to bias the\nmodel towards numerical meanings, e.g., “There were ﬁve digit on each hand and four\non each foot”.\nSensitivity to initialization. We observed a high variation in the results, especially for\nthe one-shot setting, suggesting the high sensitivity of the model with little evidence\nfrom training to the initialization point. For instance, in the one-shot experiment for\nthe ﬁne-tuning model and the word bank, in three runs, 1%, 60%, and 70% of the test\ninstances for the ﬁnancial bank are incorrectly classiﬁed as river bank. Similarly, forcrane,\n12%, 25%, and 72% of the machine instances are misclassiﬁed as bird in three runs. The\n1NN strategy, in addition to being less prone to sense biases, is generally more robust\nacross multiple runs. For the above two examples, the ﬁgures are 2%, 0%, and 0% for\nbank and 15%, 0%, and 27% for crane. Other than the extent of bias, we observed that\nthe direction can also change dramatically from run to run. For example, in the one-shot\n1NN setting and for the word apple, almost all the mistakes in the ﬁrst two runs (37 of\n38 and 12 of 14) were incorporation for fruit, whereas in the third run, almost all (6 of 7)\nwere fruit for incorporation.\n7. Discussion\nIn the previous sections we have run an extensive set of experiments to investigate\nvarious properties of language models when adapted to the task of WSD. In the following\nwe discuss some of the general conclusions and open questions arising from our analysis.\nFine-grained vs. coarse-grained. A well-known issue of WordNet is the ﬁne granularity of\nits sense distinctions (Navigli 2009). For example, the noun star has 8 senses in WordNet,\ntwo of which refer to a “celestial body”, only differing in if they are visible from the Earth\nor not. Both meanings translate to estrella in Spanish and therefore this sense distinction\nserves no advantage in MT, for example. In fact, it has been shown that coarse-grained\ndistinctions are generally more suited to downstream applications (Rüd et al. 2011;\nSeveryn, Nicosia, and Moschitti 2013; Flekova and Gurevych 2016; Pilehvar et al. 2017).\nHowever, the coarsening of sense inventories is certainly not a solved task. While in\nthis paper we relied either on experts for selecting senses from Wikipedia (given the\nreduced number of selected words) or domain labels from lexical resources for WordNet\n(Lacerra et al. 2020), there are other strategies for coarsening sense inventories (McCarthy,\nApidianaki, and Erk 2016; Hauer and Kondrak 2020), for instance, based on translations\nor parallel corpora (Resnik and Yarowsky 1999; Apidianaki 2008; Bansal, DeNero, and\nLin 2012). This is generally an open problem, especially for verbs (Peterson and Palmer\n2018), which have not been analyzed in-depth in this article due to lack of effective\ntechniques for an interpretable coarsening. Indeed, while in this work we have shown\nhow contextualized embeddings encode meaning to a similar extent as humans do, for\nﬁne-grained distinctions these have been shown to correlate to a much lesser extent, an\narea that requires further exploration (Haber and Poesio 2020).\nFine-tuning vs. feature extraction (1NN). The distinction between ﬁne-tuning and feature\nextraction has been already studied in the literature for different tasks (Peters, Ruder,\nand Smith 2019). The general assumption is that ﬁne-tuned models perform better when\nreasonable amounts of training data are available. In the case of WSD, however, feature\nextraction (speciﬁcally the 1NN strategy explained in this paper) is the more solid choice\n32\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\non general grounds, even when training data is available. The advantages of feature\nextraction (1NN) with respect to ﬁne-tuning are threefold:\n1. It is signiﬁcantly less expensive to train as it simply relies on extracting contextual-\nized embeddings from the training data. This is especially relevant when the WSD\nmodel is to be used in an all-words setting.\n2. It is more robust to changes in the training distribution (see Section 6.2.1).\n3. It works reasonably well for limited amounts of training data (see Section 6.2.2),\neven in few-shot settings (see Section 6.3).\nFew-shot learning. An important limitation of supervised WSD models is their dependence\non sense-annotated corpora, which is expensive to construct, i.e., the so-called knowledge-\nacquisition bottleneck (Gale, Church, and Yarowsky 1992b; Pasini 2020). Therefore, being\nable to learn from a limited set of examples is a desirable property of WSD models.\nEncouragingly, as mentioned above, the simple 1NN method studied in this article\nshows robust results even with as few as three training examples per word sense. In the\nfuture it would be interesting to investigate models relying on knowledge from lexical\nresources that can perform WSD with no training instances available (i.e., zero-shot), in\nthe line of Kumar et al. (2019) and Blevins and Zettlemoyer (2020).\n8. Conclusions\nIn this paper we have provided an extensive analysis on how pre-trained language\nmodels (particularly BERT) capture lexical ambiguity. Our aim was to inspect the\ncapability of BERT in predicting different usages of the same word depending on its\ncontext, similarly as humans do (Rodd 2020). The general conclusion we draw is that in\nthe ideal setting of having access to enough amounts of training data and computing\npower, BERT can approach human-level performance for coarse-grained noun WSD,\neven in cross-domain scenarios. However, this ideal setting rarely occurs in practice,\nand challenges remain to make these models more efﬁcient and less reliant on sense-\nannotated data. As an encouraging ﬁnding, feature extraction-based models (referred\nto as 1NN throughout the article) show strong performance even with a handful of\nexamples per word sense. As future work it would be interesting to focus on the internal\nrepresentation of the Transformer architecture by, e.g., carrying out an in-depth study of\nlayer distribution (Tenney, Das, and Pavlick 2019), investigating the importance of each\nattention head (Clark et al. 2019), or analyzing the differences for modeling concepts,\nentities and other categories of words, e.g., verbs. Moreover, our analysis could be\nextended to additional Transformer-based models, such as RoBERTa (Liu et al. 2019b)\nand T5 (Raffel et al. 2020).\nTo enable further analysis of this type, another contribution of the paper is the release\nof the CoarseWSD-20 dataset (Section 4), which also includes the out-of-domain test set\n(Section 4.4). This dataset can be reliably used for quantitative and qualitative analyses\nin coarse-grained WSD, as we performed. We hope that future research in WSD will\ntake inspiration on the types of analyses performed in this paper, as they help shed light\non the advantages and limitations of each approach. In particular, few-shot and bias\nanalysis along with training distribution variations are key aspects to understand the\nversatility and robustness of any given approach.\nFinally, WSD is clearly not a solved problem, even in the coarse-grained setting,\ndue to a few challenges: (1) it is an arduous process to manually create high-quality\nfull-coverage training data; therefore, future research should also focus on reliable ways\n33\nComputational Linguistics Volume 0, Number 1\nof automising this process (Taghipour and Ng 2015; Delli Bovi et al. 2017; Scarlini,\nPasini, and Navigli 2019; Pasini and Navigli 2020; Loureiro and Camacho-Collados 2020;\nScarlini, Pasini, and Navigli 2020b) and/or leveraging speciﬁc knowledge from lexical\nresources (Luo et al. 2018; Kumar et al. 2019; Huang et al. 2019); and (2) the existing\nsense-coarsening approaches are mainly targeted at nouns, and verb sense modelling\nremains an important open research challenge.\n9. Acknowledgments\nWe would like to thank Claudio Delli Bovi and Miguel Ballesteros for early pre-BERT\ndiscussions on the topic of ambiguity and language models. We would also like to thank\nthe anonymous reviewers for their comments and suggestions that helped improve the\narticle.\nReferences\nAgirre, Eneko, Oier López de Lacalle, and\nAitor Soroa. 2018. The risk of sub-optimal\nuse of open source NLP software: UKB is\ninadvertently state-of-the-art in\nknowledge-based WSD. In Proceedings of\nWorkshop for NLP Open Source Software\n(NLP-OSS), pages 29–33, Association for\nComputational Linguistics, Melbourne,\nAustralia.\nAgirre, Eneko, Oier López de Lacalle, and\nAitor Soroa. 2014. Random walks for\nknowledge-based word sense\ndisambiguation. Computational Linguistics,\n40(1):57–84.\nAgirre, Eneko, Lluís Màrquez, and Richard\nWicentowski, editors. 2007. Proceedings of\nthe Fourth International Workshop on\nSemantic Evaluations (SemEval-2007).\nAssociation for Computational Linguistics,\nPrague, Czech Republic.\nAina, Laura, Kristina Gulordava, and Gemma\nBoleda. 2019. Putting words in context:\nLSTM language models and lexical\nambiguity. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 3342–3348, Association\nfor Computational Linguistics, Florence,\nItaly.\nAkbik, Alan, Duncan Blythe, and Roland\nVollgraf. 2018. Contextual string\nembeddings for sequence labeling. In\nProceedings of the 27th International\nConference on Computational Linguistics,\npages 1638–1649, Association for\nComputational Linguistics, Santa Fe, New\nMexico, USA.\nAmrami, Asaf and Yoav Goldberg. 2018.\nWord sense induction with neural biLM\nand symmetric patterns. In Proceedings of\nthe 2018 Conference on Empirical Methods in\nNatural Language Processing, pages\n4860–4867, Association for Computational\nLinguistics, Brussels, Belgium.\nApidianaki, Marianna. 2008.\nTranslation-oriented word sense induction\nbased on parallel corpora. In Proceedings of\nthe Sixth International Conference on\nLanguage Resources and Evaluation\n(LREC’08), European Language Resources\nAssociation (ELRA), Marrakech, Morocco.\nBanerjee, Satanjeev and Ted Pedersen. 2003.\nExtended gloss overlap as a measure of\nsemantic relatedness. In Proceedings of the\n18th International Joint Conference on\nArtiﬁcial Intelligence, pages 805–810,\nAcapulco, Mexico.\nBansal, Mohit, John DeNero, and Dekang Lin.\n2012. Unsupervised translation sense\nclustering. In Proceedings of the 2012\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, pages\n773–782.\nBasile, Pierpaolo, Annalina Caputo, and\nGiovanni Semeraro. 2014. An Enhanced\nLesk Word Sense Disambiguation\nAlgorithm through a Distributional\nSemantic Model. In Proceedings of COLING\n2014, the 25th International Conference on\nComputational Linguistics: Technical Papers,\npages 1591–1600, Dublin, Ireland.\nBelinkov, Yonatan and James Glass. 2019.\nAnalysis methods in neural language\nprocessing: A survey. Transactions of the\nAssociation for Computational Linguistics,\n7:49–72.\nBennett, Andrew, Timothy Baldwin, Jey Han\nLau, Diana McCarthy, and Francis Bond.\n2016. LexSemTm: A semantic dataset based\non all-words unsupervised sense\ndistribution learning. In Proceedings of the\n34\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\n54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1513–1524, Association for\nComputational Linguistics, Berlin,\nGermany.\nBevilacqua, Michele and Roberto Navigli.\n2020. Breaking through the 80% glass\nceiling: Raising the state of the art in word\nsense disambiguation by incorporating\nknowledge graph information. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics,\npages 2854–2864, Association for\nComputational Linguistics, Online.\nBlevins, Terra and Luke Zettlemoyer. 2020.\nMoving down the long tail of word sense\ndisambiguation with gloss informed\nbi-encoders. In Proceedings of the 58th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 1006–1017,\nAssociation for Computational Linguistics,\nOnline.\nBond, Francis and Ryan Foster. 2013. Linking\nand extending an open multilingual\nWordNet. In Proceedings of the 51st Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n1352–1362.\nBrown, Tom B., Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language models\nare few-shot learners.\nCamacho-Collados, Jose and Roberto Navigli.\n2017. BabelDomains: Large-scale domain\nlabeling of lexical resources. In Proceedings\nof the 15th Conference of the European Chapter\nof the Association for Computational\nLinguistics: Volume 2, Short Papers, pages\n223–228, Association for Computational\nLinguistics, Valencia, Spain.\nCamacho-Collados, Jose and\nMohammad Taher Pilehvar. 2018. From\nword to sense embeddings: A survey on\nvector representations of meaning. Journal\nof Artiﬁcial Intelligence Research, 63:743–788.\nChaplot, Devendra Singh and Ruslan\nSalakhutdinov. 2018. Knowledge-based\nword sense disambiguation using topic\nmodels. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence,\nvolume 32, pages 5062–5069.\nChronis, Gabriella and Katrin Erk. 2020.\nWhen is a bishop not like a rook? when it’s\nlike a rabbi! multi-prototype BERT\nembeddings for estimating semantic\nrelationships. In Proceedings of the 24th\nConference on Computational Natural\nLanguage Learning, pages 227–244,\nAssociation for Computational Linguistics,\nOnline.\nClark, Kevin, Urvashi Khandelwal, Omer\nLevy, and Christopher D. Manning. 2019.\nWhat does BERT look at? an analysis of\nBERT’s attention. InProceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages\n276–286, Association for Computational\nLinguistics, Florence, Italy.\nConneau, Alexis, German Kruszewski,\nGuillaume Lample, Loïc Barrault, and\nMarco Baroni. 2018. What you can cram\ninto a single $&!#* vector: Probing sentence\nembeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 2126–2136,\nAssociation for Computational Linguistics,\nMelbourne, Australia.\nDelli Bovi, Claudio, Jose Camacho-Collados,\nAlessandro Raganato, and Roberto Navigli.\n2017. EuroSense: Automatic harvesting of\nmultilingual sense annotations from\nparallel text. In Proceedings of ACL,\nvolume 2, pages 594–600.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional\ntransformers for language understanding.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Association for\nComputational Linguistics, Minneapolis,\nMinnesota.\nEdmonds, Philip and Scott Cotton. 2001.\nSENSEVAL-2: Overview. In Proceedings of\nSENSEV AL-2 Second International Workshop\non Evaluating Word Sense Disambiguation\nSystems, pages 1–5, Association for\nComputational Linguistics, Toulouse,\nFrance.\nEthayarajh, Kawin. 2019. How contextual are\ncontextualized word representations?\ncomparing the geometry of BERT, ELMo,\nand GPT-2 embeddings. In Proceedings of\nthe 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th\n35\nComputational Linguistics Volume 0, Number 1\nInternational Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP),\npages 55–65, Association for\nComputational Linguistics, Hong Kong,\nChina.\nEttinger, Allyson. 2020. What BERT is not:\nLessons from a new suite of\npsycholinguistic diagnostics for language\nmodels. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nFedermeier, Kara D., Jessica B. Segal, Tania\nLombrozo, and Marta Kutas. 2000. Brain\nresponses to nouns, verbs and\nclass-ambiguous words in context. Brain,\n123(12):2552–2566.\nFellbaum, Christiane, editor. 1998. WordNet:\nAn Electronic Database. MIT Press,\nCambridge, MA.\nFlekova, Lucie and Iryna Gurevych. 2016.\nSupersense embeddings: A uniﬁed model\nfor supersense interpretation, prediction,\nand utilization. In Proceedings of the 54th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 2029–2041, Association for\nComputational Linguistics, Berlin,\nGermany.\nGale, William A., Kenneth Church, and David\nYarowsky. 1992a. Estimating upper and\nlower bounds on the performance of\nword-sense disambiguation programs. In\nProceedings of the 30th Annual Meeting of the\nAssociation for Computational Linguistics\n(ACL), pages 249–256, Newark, DE, USA.\nGale, William A, Kenneth W Church, and\nDavid Yarowsky. 1992b. A method for\ndisambiguating word senses in a large\ncorpus. Computers and the Humanities,\n26(5):415–439.\nGoldberg, Yoav. 2019. Assessing BERT’s\nsyntactic abilities. arXiv preprint\narXiv:1901.05287.\nHaber, Janosch and Massimo Poesio. 2020.\nWord sense distance in human similarity\njudgements and contextualised word\nembeddings. In Proceedings of the Probability\nand Meaning Conference (PaM 2020), pages\n128–145.\nHardt, Moritz, Eric Price, Eric Price, and Nati\nSrebro. 2016. Equality of opportunity in\nsupervised learning. In D. D. Lee,\nM. Sugiyama, U. V . Luxburg, I. Guyon, and\nR. Garnett, editors, Advances in Neural\nInformation Processing Systems 29. Curran\nAssociates, Inc., pages 3315–3323.\nHauer, Bradley and Grzegorz Kondrak. 2020.\nOne homonym per translation. In\nProceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages\n7895–7902.\nHewitt, John and Christopher D. Manning.\n2019. A structural probe for ﬁnding syntax\nin word representations. In Proceedings of\nthe 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages\n4129–4138, Association for Computational\nLinguistics, Minneapolis, Minnesota.\nHovy, Eduard H., Roberto Navigli, and\nSimone Paolo Ponzetto. 2013.\nCollaboratively built semi-structured\ncontent and Artiﬁcial Intelligence: The\nstory so far. Artiﬁcial Intelligence, 194:2–27.\nHuang, Luyao, Chi Sun, Xipeng Qiu, and\nXuanjing Huang. 2019. GlossBERT: BERT\nfor word sense disambiguation with gloss\nknowledge. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n3507–3512, Association for Computational\nLinguistics, Hong Kong, China.\nIacobacci, Ignacio, Mohammad Taher\nPilehvar, and Roberto Navigli. 2016.\nEmbeddings for word sense\ndisambiguation: An evaluation study. In\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 897–907,\nAssociation for Computational Linguistics,\nBerlin, Germany.\nIde, Nancy, Collin Baker, Christiane Fellbaum,\nCharles Fillmore, and Rebecca Passonneau.\n2008. Masc: The manually annotated\nsub-corpus of american english. In 6th\nInternational Conference on Language\nResources and Evaluation, LREC 2008, pages\n2455–2460, European Language Resources\nAssociation (ELRA).\nIlievski, Filip, Piek Vossen, and Stefan\nSchlobach. 2018. Systematic study of long\ntail phenomena in entity linking. In\nProceedings of the 27th International\nConference on Computational Linguistics,\npages 664–674.\nJawahar, Ganesh, Benoît Sagot, and Djamé\nSeddah. 2019. What does BERT learn about\nthe structure of language? In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3651–3657,\nAssociation for Computational Linguistics,\nFlorence, Italy.\nJoulin, Armand, Edouard Grave, Piotr\nBojanowski, and Tomas Mikolov. 2017. Bag\nof tricks for efﬁcient text classiﬁcation. In\nProceedings of the 15th Conference of the\n36\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nEuropean Chapter of the Association for\nComputational Linguistics: Volume 2, Short\nPapers, pages 427–431, Association for\nComputational Linguistics, Valencia, Spain.\nJurgens, David and Mohammad Taher\nPilehvar. 2015. Reserating the\nawesometastic: An automatic extension of\nthe WordNet taxonomy for novel terms. In\nProceedings of the 2015 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 1459–1465, Association\nfor Computational Linguistics, Denver,\nColorado.\nKumar, Sawan, Sharmistha Jat, Karan Saxena,\nand Partha Talukdar. 2019. Zero-shot word\nsense disambiguation using sense\ndeﬁnition embeddings. In Proceedings of the\n57th Annual Meeting of the Association for\nComputational Linguistics, pages 5670–5681,\nAssociation for Computational Linguistics,\nFlorence, Italy.\nKuncoro, Adhiguna, Chris Dyer, John Hale,\nDani Yogatama, Stephen Clark, and Phil\nBlunsom. 2018. LSTMs can learn\nsyntax-sensitive dependencies well, but\nmodeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1426–1436,\nAssociation for Computational Linguistics,\nMelbourne, Australia.\nLacerra, Caterina, Michele Bevilacqua,\nTommaso Pasini, and Roberto Navigli.\n2020. CSI: A coarse sense inventory for 85%\nword sense disambiguation. In Proceedings\nof the 34th Conference on Artiﬁcial Intelligence,\npages 8123–8130, AAAI Press.\nLan, Zhenzhong, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma,\nand Radu Soricut. 2020. ALBERT: A lite\nBERT for self-supervised learning of\nlanguage representations. In 8th\nInternational Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020, OpenReview.net.\nLesk, Michael. 1986. Automatic sense\ndisambiguation using machine readable\ndictionaries: How to tell a pine cone from\nan ice cream cone. In Proceedings of the 5th\nAnnual Conference on Systems Documentation,\nToronto, Ontario, Canada, pages 24–26.\nLevine, Yoav, Barak Lenz, Or Dagan, Ori Ram,\nDan Padnos, Or Sharir, Shai\nShalev-Shwartz, Amnon Shashua, and\nYoav Shoham. 2020. SenseBERT: Driving\nsome sense into BERT. In Proceedings of the\n58th Annual Meeting of the Association for\nComputational Linguistics, pages 4656–4667,\nAssociation for Computational Linguistics,\nOnline.\nLing, Xiao, Sameer Singh, and Daniel S Weld.\n2015. Design challenges for entity linking.\nTransactions of the Association for\nComputational Linguistics, 3:315–328.\nLinzen, Tal, Emmanuel Dupoux, and Yoav\nGoldberg. 2016. Assessing the ability of\nLSTMs to learn syntax-sensitive\ndependencies. Transactions of the Association\nfor Computational Linguistics, 4:521–535.\nLiu, Nelson F., Matt Gardner, Yonatan\nBelinkov, Matthew E. Peters, and Noah A.\nSmith. 2019a. Linguistic knowledge and\ntransferability of contextual\nrepresentations. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), pages 1073–1094,\nAssociation for Computational Linguistics,\nMinneapolis, Minnesota.\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019b. RoBERTa: A robustly\noptimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692.\nLoureiro, Daniel and Jose Camacho-Collados.\n2020. Don’t neglect the obvious: On the\nrole of unambiguous words in word sense\ndisambiguation. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n3514–3520, Association for Computational\nLinguistics, Online.\nLoureiro, Daniel and Alípio Jorge. 2019a.\nLanguage modelling makes sense:\nPropagating representations through\nWordNet for full-coverage word sense\ndisambiguation. In Proceedings of the 57th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 5682–5691,\nAssociation for Computational Linguistics,\nFlorence, Italy.\nLoureiro, Daniel and Alípio Jorge. 2019b.\nLIAAD at SemDeep-5 challenge:\nWord-in-context (WiC). In Proceedings of the\n5th Workshop on Semantic Deep Learning\n(SemDeep-5), pages 1–5, Association for\nComputational Linguistics, Macau, China.\nLuo, Fuli, Tianyu Liu, Qiaolin Xia, Baobao\nChang, and Zhifang Sui. 2018.\nIncorporating glosses into neural word\nsense disambiguation. In Proceedings of the\n56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 2473–2482, Association for\nComputational Linguistics, Melbourne,\n37\nComputational Linguistics Volume 0, Number 1\nAustralia.\nMallery, J. C. 1988. Thinking about foreign\npolicy: Finding an appropriate role for artiﬁcial\nintelligence computers, Ph.D. Thesis. M.I.T.\nPolitical Science Department, Cambridge,\nMA.\nMaru, Marco, Federico Scozzafava, Federico\nMartelli, and Roberto Navigli. 2019.\nSyntagNet: Challenging supervised word\nsense disambiguation with lexical-semantic\ncombinations. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n3534–3540, Association for Computational\nLinguistics, Hong Kong, China.\nMcCarthy, Diana, Marianna Apidianaki, and\nKatrin Erk. 2016. Word sense clustering\nand clusterability. Computational Linguistics,\n42(2):245–275.\nMcCrae, John Philip, Alexandre Rademaker,\nEwa Rudnicka, and Francis Bond. 2020.\nEnglish WordNet 2020: Improving and\nextending a WordNet for English using an\nopen-source methodology. In Proceedings of\nthe LREC 2020 Workshop on Multimodal\nWordnets (MMW2020), pages 14–19, The\nEuropean Language Resources Association\n(ELRA), Marseille, France.\nMcInnes, Leland, John Healy, Nathaniel Saul,\nand Lukas Großberger. 2018. Umap:\nUniform manifold approximation and\nprojection. Journal of Open Source Software,\n3(29):861.\nMelamud, Oren, Jacob Goldberger, and Ido\nDagan. 2016. context2vec: Learning generic\ncontext embedding with bidirectional\nLSTM. In Proceedings of The 20th SIGNLL\nConference on Computational Natural\nLanguage Learning, pages 51–61, Association\nfor Computational Linguistics, Berlin,\nGermany.\nMickus, Timothee, Denis Paperno, Mathieu\nConstant, and Kees van Deemter. 2020.\nWhat do you mean, BERT? In Proceedings of\nthe Society for Computation in Linguistics\n2020, pages 279–290, Association for\nComputational Linguistics, New York,\nNew York.\nMihalcea, Rada, Timothy Chklovski, and\nAdam Kilgarriff. 2004. The Senseval-3\nEnglish lexical sample task. In Proceedings\nof Senseval-3: The Third International\nWorkshop on the Evaluation of Systems for the\nSemantic Analysis of Text, pages 25–28.\nMihalcea, Rada and Andras Csomai. 2007.\nWikify! linking documents to encyclopedic\nknowledge. In Proceedings of the sixteenth\nACM conference on Conference on information\nand knowledge management, pages 233–242.\nMikolov, Tomas, Kai Chen, Greg Corrado,\nand Jeffrey Dean. 2013a. Efﬁcient\nestimation of word representations in\nvector space. CoRR, abs/1301.3781.\nMikolov, Tomas, Ilya Sutskever, Kai Chen,\nGreg S Corrado, and Jeff Dean. 2013b.\nDistributed representations of words and\nphrases and their compositionality. In\nAdvances in neural information processing\nsystems, pages 3111–3119.\nMiller, George A., Claudia Leacock, Randee\nTengi, and Ross Bunker. 1993. A semantic\nconcordance. In Proceedings of the 3rd\nDARP A Workshop on Human Language\nTechnology, pages 303–308, Plainsboro, N.J.\nMoro, Andrea and Roberto Navigli. 2015.\nSemeval-2015 task 13: Multilingual\nall-words sense disambiguation and entity\nlinking. Proceedings of SemEval-2015.\nMoro, Andrea, Alessandro Raganato, and\nRoberto Navigli. 2014. Entity Linking\nmeets Word Sense Disambiguation: a\nUniﬁed Approach. Transactions of the\nAssociation for Computational Linguistics\n(TACL), 2:231–244.\nNair, Sathvik, Mahesh Srinivasan, and\nStephan Meylan. 2020. Contextualized\nword embeddings encode aspects of\nhuman-like word sense knowledge. In\nProceedings of the Workshop on the Cognitive\nAspects of the Lexicon, pages 129–141.\nNavigli, Roberto. 2009. Word Sense\nDisambiguation: A survey. ACM\nComputing Surveys, 41(2):1–69.\nNavigli, Roberto, David Jurgens, and Daniele\nVannella. 2013. SemEval-2013 Task 12:\nMultilingual Word Sense Disambiguation.\nIn Proceedings of SemEval 2013, pages\n222–231.\nNavigli, Roberto and Simone Paolo Ponzetto.\n2012. BabelNet: The automatic\nconstruction, evaluation and application of\na wide-coverage multilingual semantic\nnetwork. Artiﬁcial Intelligence, 193:217–250.\nPalmer, Martha, Hoa Dang, and Christiane\nFellbaum. 2007. Making ﬁne-grained and\ncoarse-grained sense distinctions, both\nmanually and automatically. Natural\nLanguage Engineering, 13(2):137–163.\nPasini, Tommaso. 2020. The knowledge\nacquisition bottleneck problem in\nmultilingual word sense disambiguation.\nIn Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artiﬁcial\nIntelligence, IJCAI-20, pages 4936–4942,\nInternational Joint Conferences on Artiﬁcial\nIntelligence Organization. Survey track.\n38\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nPasini, Tommaso and Jose Camacho-Collados.\n2020. A short survey on sense-annotated\ncorpora. In Proceedings of The 12th Language\nResources and Evaluation Conference, pages\n5759–5765, European Language Resources\nAssociation, Marseille, France.\nPasini, Tommaso and Roberto Navigli. 2018.\nTwo knowledge-based methods for\nhigh-performance sense distribution\nlearning. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence,\npages 5374–5381, New Orleans, United\nStates.\nPasini, Tommaso and Roberto Navigli. 2020.\nTrain-o-matic: Supervised word sense\ndisambiguation with no (manual) effort.\nArtiﬁcial Intelligence, 279:103215.\nPeters, Matthew, Mark Neumann, Mohit\nIyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018a.\nDeep contextualized word representations.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages\n2227–2237, Association for Computational\nLinguistics, New Orleans, Louisiana.\nPeters, Matthew, Mark Neumann, Luke\nZettlemoyer, and Wen-tau Yih. 2018b.\nDissecting contextual word embeddings:\nArchitecture and representation. In\nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 1499–1509, Association for\nComputational Linguistics, Brussels,\nBelgium.\nPeters, Matthew E., Mark Neumann, Robert\nLogan, Roy Schwartz, Vidur Joshi, Sameer\nSingh, and Noah A. Smith. 2019.\nKnowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 43–54,\nAssociation for Computational Linguistics,\nHong Kong, China.\nPeters, Matthew E, Sebastian Ruder, and\nNoah A Smith. 2019. To tune or not to\ntune? adapting pretrained representations\nto diverse tasks. In Proceedings of the 4th\nWorkshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 7–14.\nPeterson, Daniel and Martha Palmer. 2018.\nBayesian verb sense clustering. In\nProceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 32, pages\n5398–5405.\nPilehvar, Mohammad Taher and Jose\nCamacho-Collados. 2019. WiC: the\nword-in-context dataset for evaluating\ncontext-sensitive meaning representations.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers), pages 1267–1273, Minneapolis,\nMinnesota.\nPilehvar, Mohammad Taher, Jose\nCamacho-Collados, Roberto Navigli, and\nNigel Collier. 2017. Towards a seamless\nintegration of word senses into\ndownstream NLP applications. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1857–1869,\nAssociation for Computational Linguistics,\nVancouver, Canada.\nPilehvar, Mohammad Taher and Roberto\nNavigli. 2014. A large-scale\npseudoword-based evaluation framework\nfor state-of-the-art word sense\ndisambiguation. Computational Linguistics,\n40(4):837–881.\nPostma, Marten, Ruben Izquierdo, Eneko\nAgirre, German Rigau, and Piek Vossen.\n2016. Addressing the MFS bias in WSD\nsystems. In Proceedings of the Tenth\nInternational Conference on Language\nResources and Evaluation (LREC’16), pages\n1695–1700, European Language Resources\nAssociation (ELRA), Portorož, Slovenia.\nPostma, Marten, Ruben Izquierdo Bevia, and\nPiek Vossen. 2016. More is not always\nbetter: balancing sense distributions for\nall-words word sense disambiguation. In\nProceedings of COLING 2016, the 26th\nInternational Conference on Computational\nLinguistics: Technical Papers, pages\n3496–3506, The COLING 2016 Organizing\nCommittee, Osaka, Japan.\nPradhan, Sameer, Edward Loper, Dmitriy\nDligach, and Martha Palmer. 2007.\nSemEval-2007 task-17: English lexical\nsample, SRL and all words. In Proceedings\nof SemEval, pages 87–92.\nRaffel, Colin, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J.\nLiu. 2020. Exploring the limits of transfer\nlearning with a uniﬁed text-to-text\nTransformer. Journal of Machine Learning\nResearch, 21(140):1–67.\nRaganato, Alessandro, Jose\nCamacho-Collados, and Roberto Navigli.\n2017. Word sense disambiguation: A\nuniﬁed evaluation framework and\n39\nComputational Linguistics Volume 0, Number 1\nempirical comparison. In Proceedings of the\n15th Conference of the European Chapter of the\nAssociation for Computational Linguistics:\nVolume 1, Long Papers, pages 99–110,\nAssociation for Computational Linguistics,\nValencia, Spain.\nRaganato, Alessandro, Claudio Delli Bovi,\nand Roberto Navigli. 2017. Neural\nsequence learning models for word sense\ndisambiguation. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing, pages 1156–1167,\nAssociation for Computational Linguistics,\nCopenhagen, Denmark.\nRaganato, Alessandro, Tommaso Pasini, Jose\nCamacho-Collados, and Taher Mohammad\nPilehvar. 2020. XL-WiC: A multilingual\nbenchmark for evaluating semantic\ncontextualization. EMNLP 2020, pages\n7193–7206.\nReif, Emily, Ann Yuan, Martin Wattenberg,\nFernanda B Viegas, Andy Coenen, Adam\nPearce, and Been Kim. 2019. Visualizing\nand measuring the geometry of BERT. In\nAdvances in Neural Information Processing\nSystems, pages 8592–8600.\nReisinger, Joseph and Raymond J. Mooney.\n2010. Multi-prototype vector-space models\nof word meaning. In Proceedings of ACL,\npages 109–117.\nResnik, Philip and David Yarowsky. 1999.\nDistinguishing systems and distinguishing\nsenses: New evaluation methods for word\nsense disambiguation. Natural language\nengineering, 5(2):113–133.\nRodd, Jennifer M. 2020. Settling into semantic\nspace: An ambiguity-focused account of\nword-meaning access. Perspectives on\nPsychological Science, 15(2):411–427. PMID:\n31961780.\nRogers, Anna, Olga Kovaleva, and Anna\nRumshisky. 2020. A primer in BERTology:\nWhat we know about how BERT works.\nTransactions of the Association for\nComputational Linguistics, 8:842–866.\nRüd, Stefan, Massimiliano Ciaramita, Jens\nMüller, and Hinrich Schütze. 2011.\nPiggyback: Using search engines for robust\ncross-domain named entity recognition. In\nProceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics:\nHuman Language Technologies, pages\n965–975, Association for Computational\nLinguistics, Portland, Oregon, USA.\nSaphra, Naomi and Adam Lopez. 2019.\nUnderstanding learning dynamics of\nlanguage models with SVCCA. In\nProceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers), pages 3257–3267, Association for\nComputational Linguistics, Minneapolis,\nMinnesota.\nScarlini, Bianca, Tommaso Pasini, and\nRoberto Navigli. 2019. Just “OneSeC” for\nproducing multilingual sense-annotated\ndata. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 699–709, Association for\nComputational Linguistics, Florence, Italy.\nScarlini, Bianca, Tommaso Pasini, and\nRoberto Navigli. 2020a. SensEmBERT:\nContext-Enhanced Sense Embeddings for\nMultilingual Word Sense Disambiguation.\nIn Proceedings of the Thirty-Fourth Conference\non Artiﬁcial Intelligence, pages 8758–8765,\nAssociation for the Advancement of\nArtiﬁcial Intelligence.\nScarlini, Bianca, Tommaso Pasini, and Roberto\nNavigli. 2020b. With more contexts comes\nbetter performance: Contextualized sense\nembeddings for all-round word sense\ndisambiguation. In Proceedings of the 2020\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n3528–3539, Association for Computational\nLinguistics, Online.\nvan Schijndel, Marten, Aaron Mueller, and Tal\nLinzen. 2019. Quantity doesn’t buy quality\nsyntax with neural language models. In\nProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5830–5836,\nAssociation for Computational Linguistics,\nHong Kong, China.\nSchütze, Hinrich. 1993. Word space. In S. J.\nHanson, J. D. Cowan, and C. L. Giles,\neditors, Advances in Neural Information\nProcessing Systems 5. Morgan-Kaufmann,\npages 895–902.\nScozzafava, Federico, Marco Maru, Fabrizio\nBrignone, Giovanni Torrisi, and Roberto\nNavigli. 2020. Personalized PageRank with\nsyntagmatic information for multilingual\nword sense disambiguation. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics: System\nDemonstrations, pages 37–46, Association\nfor Computational Linguistics, Online.\nSeveryn, Aliaksei, Massimo Nicosia, and\nAlessandro Moschitti. 2013. Learning\nsemantic textual similarity with structural\nrepresentations. In Proceedings of the 51st\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 2: Short\n40\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nPapers), pages 714–718, Association for\nComputational Linguistics, Soﬁa, Bulgaria.\nShwartz, Vered and Ido Dagan. 2019. Still a\npain in the neck: Evaluating text\nrepresentations on lexical composition.\nTransactions of the Association for\nComputational Linguistics, 7:403–419.\nSoler, Aina Garí, Anne Cocos, Marianna\nApidianaki, and Chris Callison-Burch. 2019.\nA comparison of context-sensitive models\nfor lexical substitution. In Proceedings of the\n13th International Conference on\nComputational Semantics-Long Papers, pages\n271–282.\nTaghipour, Kaveh and Hwee Tou Ng. 2015.\nOne million sense-tagged instances for\nword sense disambiguation and induction.\nIn Proceedings of the Nineteenth Conference on\nComputational Natural Language Learning,\npages 338–344, Association for\nComputational Linguistics, Beijing, China.\nTaylor, Wilson L. 1953. “Cloze procedure”: A\nnew tool for measuring readability.\nJournalism quarterly, 30(4):415–433.\nTenney, Ian, Dipanjan Das, and Ellie Pavlick.\n2019. BERT rediscovers the classical NLP\npipeline. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, pages 4593–4601, Association\nfor Computational Linguistics, Florence,\nItaly.\nTenney, Ian, Patrick Xia, Berlin Chen, Alex\nWang, Adam Poliak, R. Thomas McCoy,\nNajoung Kim, Benjamin Van Durme,\nSamuel R. Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in\ncontextualized word representations. In\nProceeding of the 7th International Conference\non Learning Representations (ICLR).\nUsbeck, Ricardo, Michael Röder, Axel-Cyrille\nNgonga Ngomo, Ciro Baron, Andreas Both,\nMartin Brümmer, Diego Ceccarelli, Marco\nCornolti, Didier Cherix, Bernd Eickmann,\net al. 2015. GERBIL: general entity\nannotator benchmarking framework. In\nProceedings of the 24th international conference\non World Wide Web, pages 1133–1143.\nVaswani, Ashish, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need.\nIn Advances in neural information processing\nsystems, pages 5998–6008.\nVial, Loïc, Benjamin Lecouteux, and Didier\nSchwab. 2018. UFSAC: Uniﬁcation of sense\nannotated corpora and tools. In Proceedings\nof the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC\n2018), European Language Resources\nAssociation (ELRA), Miyazaki, Japan.\nVial, Loïc, Benjamin Lecouteux, and Didier\nSchwab. 2019. Sense vocabulary\ncompression through the semantic\nknowledge of WordNet for neural word\nsense disambiguation. In Proceedings of the\n10th Global Wordnet Conference, pages\n108–117, Global Wordnet Association,\nWroclaw, Poland.\nVoita, Elena, Rico Sennrich, and Ivan Titov.\n2019. The bottom-up evolution of\nrepresentations in the Transformer: A study\nwith machine translation and language\nmodeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th\nInternational Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP),\npages 4396–4406, Association for\nComputational Linguistics, Hong Kong,\nChina.\nde Vries, Wietse, Andreas van Cranenburgh,\nand Malvina Nissim. 2020. What’s so\nspecial about BERT’s layers? a closer look\nat the NLP pipeline in monolingual and\nmultilingual models. In Findings of the\nAssociation for Computational Linguistics:\nEMNLP 2020, pages 4339–4350, Association\nfor Computational Linguistics, Online.\nWang, Alex, Yada Pruksachatkun, Nikita\nNangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel\nBowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language\nunderstanding systems. In Advances in\nNeural Information Processing Systems,\nvolume 32, Curran Associates, Inc.\nWang, Alex, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel\nBowman. 2018. GLUE: A multi-task\nbenchmark and analysis platform for\nnatural language understanding. In\nProceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 353–355,\nAssociation for Computational Linguistics,\nBrussels, Belgium.\nWiedemann, Gregor, Steffen Remus, Avi\nChawla, and Chris Biemann. 2019. Does\nBERT make any sense? interpretable word\nsense disambiguation with contextualized\nembeddings. In Proceedings of the 15th\nConference on Natural Language Processing\n(KONVENS 2019).\nWolf, Thomas, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault,\nRemi Louf, Morgan Funtowicz, Joe\n41\nComputational Linguistics Volume 0, Number 1\nF There’s a lot of trash on thebed of the river | I keep a glass of water next to my bed\nwhen I sleep\nF Justify the margins | The end justiﬁes the means\nT Air pollution | Open a window and let in some air\nT The expanded window will give us time to catch the thieves | You have a two-hour\nwindow of clear weather to ﬁnish working on the lawn\nTable 12: Sample positive (T) and negative (F) pairs from the WiC dataset (target word in\nitalics).\nCambridge, MA.\nAPPENDIX\nWord in Context Evaluation\nWord-in-Context (Pilehvar and Camacho-Collados 2019, WiC) is a binary classiﬁcation\ntask from the SuperGLUE language understanding benchmark (Wang et al. 2019) aimed\nat testing the ability of models to distinguish between different senses of the same word\nwithout relying on a pre-deﬁned sense inventory. In particular, given a target word (either\na verb or a noun) and two contexts where such target word occurs, the task consists of\ndeciding whether the two target words in context refer to the same sense or not. Even\nthough no sense inventory is explicitly given, this dataset was also constructed based on\nWordNet. Table 12 shows a few examples from the dataset.\nBERT-based model. Given that the task in WiC is a binary classiﬁcation, the 1NN model\nis not applicable since a training to learn sense margins is necessary. Therefore, we\nexperimented with the BERT model ﬁne-tuned on WiC’s training data. We followed\nWang et al. (2019) and fused the two sentences and fed them as input to BERT. A classiﬁer\nwas then trained on the concatenation of the resulting BERT contextual embeddings.\nBaselines. In addition to our BERT-based model, we include results for two FastText su-\npervised classiﬁers (Joulin et al. 2017) as baselines: a basic one with random initialization\n(FastText-B) and another initialized with FastText embeddings trained on the Common\nCrawl (FastText-C). As other indicative reference points, we added two language models\nwhich are enriched with WordNet (Levine et al. 2020; Loureiro and Jorge 2019b) and\nanother with WordNet and Wikipedia (Peters et al. 2019).\nResults. Table 13 shows the result of BERT models and the other baselines on the WiC\nbenchmark.24 We can see that BERT signiﬁcantly outperforms the FastText static word\nembedding. The two versions of BERT (Base and Large) perform equally well on this\ntask, achieving results close to the state of the art. As with ﬁne-grained all-words WSD,\nthe additional knowledge drawn from WordNet proves to be beneﬁcial, as shown by the\nresults for KnowBERT and SenseBERT.\n24 Data and results from comparison systems taken from https://pilehvar.github.io/wic/\n42\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nType Model Accuracy\nHybrid\nKnowBERT (Peters et al. 2019) 70.9\nSenseBERT (Levine et al. 2020) 72.1\nLMMS-LR (Loureiro and Jorge 2019b) 68.1\nFine-tuned/\nSupervised\nBERT-Base 69.6\nBERT-Large 69.6\nFastText-B 52.3\nFastText-C 54.7\nLowerbound Most Frequent Class 50.0\nUpperbound Human performance 80.0\nTable 13: Accuracy (%) performance of different models on the WiC dataset.\nCoarseWSD-20: Sense Information\nTable 17 shows for each sense their ID (as per their Wikipedia page title), deﬁnition and\nexample usage from the dataset.\nComplementary Results in CoarseWSD-20\n1. Table 14 shows micro-F1 results for the experiment with different training data\nsizes sampled from the original CoarseWSD-20 training set (cf. Section 6.2.2 of the\npaper).\n2. Table 15 shows the micro-F1 performance for ﬁne-tuning and 1NN and for varying\nsizes of the training data (with similar skewed distributions) for both Most Frequent\nSense (MFS) and Least Frequent Sense (LFS) classes (cf. Section 6.2.2 of the paper).\n3. Table 16 includes the complete results for the n-shot experiment, including the\nFastText baselines (cf. Section 6.3 of the paper).\n43\nComputational Linguistics Volume 0, Number 1\nFine-tuning (BRT-L) 1NN (BRT-L)\n1% 5% 10% 25% 50% ALL 1% 5% 10% 25% 50% ALL\ncrane 84.1 95.8 95.8 96.8 95.5 98.1 96.5 96.7 96.8 96.8 96.8 96.8\njava 99.1 99.1 99.6 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6\napple 99.4 99.4 99.5 99.5 99.5 99.6 99.2 99.2 99.2 99.2 99.2 99.2\nmole 80.1 96.0 97.7 99.0 99.0 98.9 97.7 98.6 98.5 98.5 98.5 98.5\nspring 95.0 97.5 96.9 96.8 97.8 98.3 98.0 98.0 98.0 98.0 97.9 97.8\nchair 82.8 93.6 95.9 96.7 96.9 96.2 95.1 95.8 96.2 96.2 96.2 96.2\nhood 77.6 90.7 93.5 97.2 97.6 99.6 97.2 99.0 99.4 99.6 100 100\nseal 92.4 97.6 98.1 98.8 98.5 99.0 98.1 98.2 98.3 98.2 98.2 98.1\nbow 74.1 92.4 96.1 96.7 97.5 98.5 94.9 95.9 95.8 95.5 95.3 95.3\nclub 72.8 78.7 78.7 80.4 83.5 84.7 82.0 82.9 83.8 84.0 84.4 85.1\ntrunk 86.2 88.7 97.8 98.7 98.7 98.3 97.8 98.2 98.4 98.7 98.7 98.7\nsquare 88.4 87.3 92.6 92.4 92.9 95.7 93.9 94.2 94.1 95.2 95.7 96.1\narm 93.1 98.6 99.4 99.4 99.4 99.4 99.4 99.4 99.4 99.4 99.4 99.4\ndigit 95.2 89.7 95.2 99.2 100 100 99.6 100 100 100 100 100\nbass 92.0 93.4 94.4 95.1 95.6 95.8 86.6 86.1 85.8 85.5 85.2 84.5\nyard 90.7 94.0 95.4 97.2 99.5 99.5 89.8 88.9 87.8 87.5 86.8 88.9\npound 88.3 90.0 89.7 89.0 94.9 94.9 92.6 92.8 92.0 90.4 89.7 89.7\ndeck 93.6 92.9 92.9 93.9 95.0 95.3 91.4 91.9 91.7 91.6 91.4 91.9\nbank 95.2 95.5 97.1 99.5 99.3 99.3 99.7 99.9 99.9 99.9 99.8 99.8\npitcher 99.5 99.6 99.6 99.9 100 100 100 100 99.9 100 99.9 99.9\nAverage 89.0 93.5 95.3 96.3 97.0 97.5 95.5 95.8 95.7 95.7 95.6 95.8\nTable 14: Micro-F1 results on the CoarseWSD-20 test set using training sets of different\nsizes sampled from the original training set.\n44\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nMost Frequent Sense (MFS)\nFine-tuning (BRT-L) 1NN (BRT-L)\n1% 5% 10% 25% 50% ALL 1% 5% 10% 25% 50% ALL\ncrane 86.9 96.1 96.0 97.0 95.9 98.2 100 100 100 100 100 100\njava 99.2 99.3 99.7 99.6 99.7 99.8 99.4 99.4 99.4 99.4 99.4 99.4\napple 99.5 99.5 99.6 99.6 99.6 99.7 99.5 99.5 99.5 99.5 99.5 99.5\nmole 75.4 96.2 97.1 98.7 98.7 98.5 95.2 97.7 97.4 97.4 97.4 97.4\nspring 96.0 97.7 97.2 97.0 98.0 98.8 97.7 97.8 97.8 97.7 97.7 97.5\nchair 88.8 95.5 97.0 97.6 97.8 97.2 96.6 98.2 98.9 98.9 98.9 98.9\nhood 91.6 93.4 95.6 98.3 97.9 99.7 100 100 100 100 100 100\nseal 90.9 97.0 97.5 98.7 98.4 98.9 98.6 98.5 98.5 98.2 98.5 98.5\nbow 85.5 97.7 97.2 98.2 98.2 98.7 97.6 98.1 98.3 98.3 98.3 98.3\nclub 74.6 80.4 80.6 81.9 84.1 85.2 79.5 78.5 78.5 78.4 78.2 77.8\ntrunk 90.6 91.4 98.2 98.9 98.9 98.6 97.9 97.9 97.9 97.9 97.9 97.9\nsquare 89.6 88.5 93.0 92.8 93.2 95.7 93.7 93.6 93.4 95.8 95.1 94.2\narm 95.5 99.1 99.6 99.6 99.6 99.6 99.2 99.2 99.2 99.2 99.2 99.2\ndigit 97.0 93.9 97.1 99.5 100 100 100 100 100 100 100 100\nbass 95.2 96.0 96.8 96.7 97.1 97.2 86.0 85.2 84.6 84.1 83.7 82.9\nyard 94.4 96.6 97.4 98.4 99.7 99.7 88.3 86.9 85.7 85.2 84.4 86.9\npound 93.7 94.7 94.6 94.1 97.2 97.2 94.1 92.9 91.7 89.7 88.5 88.5\ndeck 96.7 96.3 96.3 96.8 97.3 97.5 92.4 93.0 92.1 91.7 91.3 91.3\nbank 97.6 97.7 98.5 99.7 99.6 99.6 100 100 100 100 100 100\npitcher 99.8 99.8 99.8 100 100 100 100 100 99.9 100 99.9 99.9\nAverage 91.9 95.3 96.4 97.2 97.5 98.0 95.8 95.8 95.6 95.6 95.4 95.4\nLeast Frequent Sense (LFS)\nFine-tuning (BRT-L) 1NN (BRT-L)\n1% 5% 10% 25% 50% ALL 1% 5% 10% 25% 50% ALL\ncrane 79.7 95.4 95.4 96.6 95.2 98.0 92.8 93.2 93.4 93.4 93.4 93.4\njava 98.8 98.8 99.5 99.4 99.5 99.6 99.9 99.9 99.9 99.9 99.9 99.9\napple 99.2 99.2 99.3 99.3 99.4 99.5 98.7 98.7 98.7 98.7 98.7 98.7\nmole 72.5 86.7 97.4 100 100 100 100 100 100 100 100 100\nspring 95.0 98.2 97.0 97.9 97.9 97.5 97.3 97.3 97.3 97.3 97.3 97.3\nchair 63.7 88.9 93.4 94.6 95.0 93.8 92.1 91.0 90.5 90.5 90.5 90.5\nhood 65.7 82.5 89.2 95.2 95.2 99.2 97.0 97.3 97.7 98.5 100 100\nseal 39.1 89.3 91.0 96.0 96.0 97.3 100 100 100 100 100 100\nbow 0.0 73.2 95.3 94.6 98.0 99.4 91.0 98.5 100 100 100 100\nclub 63.5 74.4 73.3 80.0 81.6 82.5 95.2 95.2 95.2 95.2 95.2 95.2\ntrunk 49.7 66.6 95.2 100 100 96.5 95.2 97.1 98.2 100 100 100\nsquare 9.5 21.4 4.8 20.5 30.3 76.0 53.8 58.5 57.7 56.4 69.2 84.6\narm 84.7 97.2 98.9 98.9 98.9 98.9 100 100 100 100 100 100\ndigit 87.7 65.5 87.2 98.0 100 100 98.1 100 100 100 100 100\nbass 29.5 48.2 61.8 65.6 68.5 67.3 69.7 73.2 75.6 77.7 78.4 77.3\nyard 70.9 74.8 79.2 90.3 98.4 98.4 98.5 100 100 100 100 100\npound 13.3 6.1 0.0 11.1 69.3 70.6 80.0 92.0 95.0 96.7 100 100\ndeck 16.7 0.0 0.0 43.6 57.1 58.6 78.6 77.1 85.7 90.5 92.9 100\nbank 2.9 14.0 51.4 94.4 91.8 91.6 93.9 97.3 97.7 97.0 95.5 95.5\npitcher 0.0 4.8 28.0 93.0 98.7 94.6 100 100 100 100 100 100\nAverage 52.1 64.3 71.9 83.4 88.5 91.0 91.6 93.3 94.1 94.6 95.5 96.6\nTable 15: Micro-F1 performance for the two WSD strategies and for varying sizes of the\ntraining data (with similar skewed distributions) for the MFS (top) and LFS (bottom).45\nComputational Linguistics Volume 0, Number 1\nMicro F1 Macro F1\n1 3 10 30 1 3 10 30\ncrane\nStatic emb. Fasttext-B 48.6 (5.3) 57.5 (5.3) 57.7 (3.5) 70.9 (5.2) 48.1 (4.5) 57.9 (4.8) 58.5 (3.3) 71.1 (5.3)\nFasttext-C 52.7 (1.2) 69.4 (6.3) 82.0 (3.0) 83.4 (6.6) 51.4 (1.1) 69.6 (6.3) 81.9 (3.0) 83.5 (6.8)\n1NN BERT-Base 84.5 (9.8) 93.8 (1.8) 93.6 (1.4) 94.5 (0.3) 84.3 (10.1) 93.7 (1.9) 93.4 (1.4) 94.4 (0.3)\nBERT-Large 86.4 (3.8) 94.7 (3.5) 95.5 (1.4) 96.8 (0.9) 86.4 (3.9) 94.5 (3.7) 95.4 (1.4) 96.7 (0.9)\nFine-Tuning BERT-Base 65.6 (1.9) 88.7 (7.8) 94.1 (3.2) 95.8 (0.6) 63.4 (1.3) 88.7 (7.8) 94.0 (3.3) 95.7 (0.6)\nBERT-Large 74.7 (10.5) 92.6 (2.2) 95.3 (1.7) 96.4 (1.3) 73.2 (12.3) 92.5 (2.2) 95.3 (1.7) 96.4 (1.3)\njava\nStatic emb. Fasttext-B 63.1 (1.3) 66.8 (2.4) 68.5 (2.8) 80.6 (6.8) 55.8 (3.5) 60.5 (3.9) 66.9 (1.6) 80.9 (6.4)\nFasttext-C 78.4 (5.8) 90.1 (3.3) 90.9 (1.9) 94.9 (2.3) 78.1 (7.8) 90.3 (2.5) 90.5 (2.3) 95.1 (2.1)\n1NN BERT-Base 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.7 (0.0) 99.7 (0.0)\nBERT-Large 99.6 (0.1) 99.6 (0.1) 99.6 (0.0) 99.6 (0.0) 99.7 (0.0) 99.7 (0.1) 99.7 (0.0) 99.7 (0.0)\nFine-Tuning BERT-Base 99.2 (0.4) 98.8 (0.7) 99.4 (0.2) 99.3 (0.1) 99.1 (0.5) 98.8 (0.7) 99.4 (0.2) 99.3 (0.1)\nBERT-Large 99.2 (0.6) 99.4 (0.1) 99.5 (0.1) 99.6 (0.1) 99.1 (0.6) 99.4 (0.1) 99.5 (0.1) 99.5 (0.1)\napple\nStatic emb. Fasttext-B 43.1 (2.1) 52.0 (4.9) 55.2 (8.4) 74.7 (1.2) 45.4 (3.8) 55.3 (5.1) 61.3 (5.4) 73.9 (2.4)\nFasttext-C 71.2 (9.9) 81.2 (2.9) 87.3 (2.1) 93.2 (0.2) 63.7 (12.8) 80.7 (1.8) 86.7 (3.0) 92.4 (0.3)\n1NN BERT-Base 95.5 (3.9) 99.0 (0.1) 99.0 (0.1) 99.0 (0.0) 96.1 (3.2) 99.0 (0.1) 99.0 (0.1) 99.0 (0.0)\nBERT-Large 98.1 (1.2) 99.2 (0.0) 99.3 (0.1) 99.3 (0.1) 98.3 (0.9) 99.2 (0.1) 99.2 (0.1) 99.3 (0.1)\nFine-Tuning BERT-Base 90.7 (9.0) 98.3 (0.6) 99.0 (0.1) 99.0 (0.1) 89.6 (10.3) 98.2 (0.7) 98.9 (0.1) 98.9 (0.1)\nBERT-Large 91.5 (5.5) 96.4 (2.5) 99.3 (0.1) 99.1 (0.5) 90.7 (6.2) 96.2 (2.6) 99.2 (0.1) 99.0 (0.5)\nmole\nStatic emb. Fasttext-B 21.2 (9.9) 16.3 (7.3) 38.2 (1.1) 65.9 (1.6) 17.9 (2.2) 22.8 (3.9) 41.5 (9.2) 68.8 (2.0)\nFasttext-C 48.7 (2.6) 63.3 (4.3) 75.9 (6.3) 88.0 (0.9) 57.3 (1.3) 68.6 (2.5) 79.4 (4.3) 89.4 (1.7)\n1NN BERT-Base 75.9 (5.5) 91.1 (4.0) 95.1 (2.2) 97.4 (0.6) 84.9 (4.6) 93.9 (1.8) 96.6 (1.2) 97.7 (0.3)\nBERT-Large 89.3 (1.1) 95.6 (0.8) 98.1 (0.8) 98.5 (0.0) 93.4 (0.6) 97.1 (0.7) 98.8 (0.4) 99.0 (0.0)\nFine-Tuning BERT-Base 71.2 (4.1) 86.2 (5.2) 95.8 (1.3) 97.6 (0.4) 70.7 (4.8) 87.8 (3.3) 95.8 (1.4) 97.5 (0.6)\nBERT-Large 77.3 (2.2) 88.7 (4.3) 96.3 (0.9) 98.5 (0.7) 76.4 (2.0) 90.2 (2.9) 96.3 (1.0) 98.8 (0.7)\nspring\nStatic emb. Fasttext-B 33.0 (6.8) 43.8 (8.2) 46.4 (7.4) 67.0 (2.2) 35.4 (0.5) 32.8 (0.7) 35.8 (3.6) 66.7 (3.1)\nFasttext-C 46.0 (14.6) 57.6 (4.0) 73.5 (3.7) 83.7 (2.8) 45.0 (9.0) 64.2 (3.3) 76.5 (3.5) 86.1 (2.8)\n1NN BERT-Base 94.4 (2.0) 97.2 (0.5) 97.1 (0.3) 97.3 (0.1) 94.6 (2.2) 97.3 (0.9) 97.0 (0.4) 97.3 (0.1)\nBERT-Large 97.1 (1.5) 97.9 (0.5) 97.6 (0.4) 97.7 (0.2) 96.8 (1.6) 97.8 (0.2) 97.5 (0.3) 97.8 (0.1)\n46\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nMicro F1 Macro F1\n1 3 10 30 1 3 10 30\nFine-Tuning BERT-Base 75.2 (4.7) 92.9 (0.3) 96.0 (0.5) 95.3 (0.5) 73.9 (4.3) 92.9 (0.2) 96.1 (0.5) 95.2 (0.6)\nBERT-Large 80.3 (10.1) 94.2 (2.7) 97.0 (0.7) 97.2 (0.2) 77.1 (12.6) 94.4 (2.4) 97.1 (0.6) 97.1 (0.4)\nchair\nStatic emb. Fasttext-B 62.8 (9.0) 73.8 (6.6) 74.4 (4.5) 74.4 (5.2) 58.4 (6.9) 68.4 (5.1) 68.4 (4.3) 72.1 (4.2)\nFasttext-C 76.2 (8.0) 75.4 (4.5) 81.3 (2.0) 83.6 (2.4) 64.1 (12.8) 72.9 (0.2) 75.8 (0.9) 81.7 (2.2)\n1NN BERT-Base 88.7 (7.1) 95.9 (0.7) 95.9 (0.4) 95.6 (0.4) 84.2 (11.6) 94.5 (0.5) 94.5 (0.3) 94.3 (0.3)\nBERT-Large 82.3 (19.0) 94.6 (1.3) 96.2 (0.0) 95.9 (0.4) 84.4 (14.1) 93.5 (0.9) 94.7 (0.0) 94.5 (0.3)\nFine-Tuning BERT-Base 84.1 (11.3) 91.0 (5.7) 95.6 (0.7) 96.7 (0.4) 75.6 (21.8) 90.1 (5.8) 94.9 (0.8) 96.1 (0.4)\nBERT-Large 72.1 (11.9) 93.3 (1.5) 94.4 (2.0) 95.1 (1.5) 65.4 (12.9) 92.4 (1.7) 93.6 (2.1) 94.4 (1.6)\nhood\nStatic emb. Fasttext-B 56.1 (11.3) 33.3 (16.2) 47.6 (18.7) - 48.8 (4.1) 42.1 (6.3) 51.4 (5.1) -\nFasttext-C 66.3 (5.0) 77.6 (2.1) 86.6 (5.0) - 61.5 (6.2) 73.7 (4.7) 82.1 (6.8) -\n1NN BERT-Base 96.8 (3.0) 98.4 (0.6) 98.8 (0.0) - 94.8 (5.9) 98.0 (0.7) 98.5 (0.0) -\nBERT-Large 96.3 (4.3) 99.2 (0.6) 99.6 (0.6) - 92.7 (9.3) 99.0 (0.7) 99.5 (0.7) -\nFine-Tuning BERT-Base 86.6 (3.6) 95.5 (2.5) 97.6 (1.7) - 79.0 (6.4) 94.4 (3.0) 96.7 (2.4) -\nBERT-Large 89.8 (5.8) 96.3 (1.0) 98.8 (1.0) - 80.1 (16.2) 95.1 (1.5) 98.0 (1.7) -\nseal\nStatic emb. Fasttext-B 29.9 (1.0) 31.6 (3.2) 39.9 (10.0) 60.2 (4.2) 25.0 (0.0) 25.7 (1.0) 39.8 (5.6) 57.4 (1.1)\nFasttext-C 46.4 (8.1) 64.6 (4.0) 73.7 (2.3) 82.1 (1.6) 43.6 (11.2) 67.7 (5.1) 79.0 (2.4) 85.4 (2.8)\n1NN BERT-Base 91.5 (6.8) 96.1 (0.7) 96.4 (0.4) 96.6 (0.3) 89.4 (11.0) 97.0 (0.6) 97.3 (0.3) 97.5 (0.3)\nBERT-Large 96.1 (1.8) 97.0 (0.7) 98.0 (0.6) 98.2 (0.1) 96.5 (1.5) 97.7 (0.6) 98.4 (0.5) 98.6 (0.1)\nFine-Tuning BERT-Base 79.6 (7.9) 95.0 (1.0) 94.4 (1.5) 96.6 (0.5) 72.3 (12.5) 90.0 (1.5) 88.7 (3.4) 92.3 (0.8)\nBERT-Large 76.2 (12.4) 94.0 (0.9) 94.6 (2.4) 97.1 (0.6) 68.0 (16.4) 89.0 (3.5) 88.3 (4.4) 94.0 (1.4)\nbow\nStatic emb. Fasttext-B 29.8 (16.7) 41.2 (20.6) 40.2 (9.1) 63.3 (4.3) 39.1 (7.6) 35.2 (2.3) 39.5 (7.9) 64.8 (1.5)\nFasttext-C 52.3 (8.1) 62.6 (4.6) 79.4 (0.8) 87.6 (1.2) 49.3 (7.2) 60.5 (6.1) 76.4 (1.8) 87.1 (1.5)\n1NN BERT-Base 86.5 (1.3) 91.8 (2.7) 95.5 (0.2) 95.3 (0.4) 80.7 (4.2) 88.6 (2.0) 93.7 (1.5) 95.0 (0.5)\nBERT-Large 87.4 (1.7) 94.9 (2.3) 97.4 (0.6) 96.4 (1.0) 84.3 (4.2) 93.3 (4.5) 97.8 (0.5) 96.7 (1.0)\nFine-Tuning BERT-Base 83.1 (3.1) 89.5 (4.1) 94.0 (0.8) 96.1 (0.2) 73.2 (6.4) 85.6 (4.9) 93.1 (1.5) 95.3 (0.4)\nBERT-Large 78.8 (13.6) 91.0 (3.9) 96.7 (0.7) 97.5 (0.6) 73.1 (12.0) 91.0 (2.2) 96.9 (0.9) 97.5 (1.1)\nclub\nStatic emb. Fasttext-B 35.0 (11.8) 26.2 (19.3) 23.8 (5.5) 56.1 (4.1) 31.5 (1.0) 32.6 (1.0) 37.1 (1.5) 54.4 (1.2)\nFasttext-C 35.2 (8.1) 47.7 (5.4) 60.2 (3.8) 74.6 (2.8) 37.4 (1.9) 52.3 (3.6) 61.7 (2.5) 79.0 (2.3)\n47\nComputational Linguistics Volume 0, Number 1\nMicro F1 Macro F1\n1 3 10 30 1 3 10 30\n1NN BERT-Base 58.3 (5.5) 80.2 (1.9) 81.2 (1.1) 81.2 (0.8) 70.5 (2.8) 84.9 (2.3) 84.9 (1.7) 84.7 (1.0)\nBERT-Large 54.1 (10.5) 82.8 (3.5) 83.8 (1.8) 84.5 (0.2) 69.0 (7.7) 87.4 (3.2) 88.9 (1.7) 88.5 (0.3)\nFine-Tuning BERT-Base 52.5 (6.1) 68.5 (8.9) 80.5 (1.5) 84.2 (0.8) 50.0 (6.2) 68.3 (10.0) 79.9 (1.9) 83.4 (0.8)\nBERT-Large 51.3 (7.3) 71.8 (1.9) 81.2 (3.9) 83.7 (1.9) 49.8 (6.5) 69.5 (2.7) 80.8 (4.1) 83.4 (1.5)\ntrunk\nStatic emb. Fasttext-B 32.9 (16.4) 21.2 (0.6) 45.9 (17.7) 66.7 (3.4) 34.0 (2.1) 35.4 (2.0) 43.5 (7.4) 67.2 (2.8)\nFasttext-C 65.4 (6.8) 63.2 (7.4) 76.6 (2.1) 82.7 (3.7) 65.3 (8.8) 67.0 (7.9) 78.3 (0.7) 87.4 (2.8)\n1NN BERT-Base 77.9 (18.4) 84.8 (7.1) 94.8 (1.8) 95.2 (2.2) 84.1 (12.1) 91.2 (4.6) 97.2 (1.0) 97.4 (1.2)\nBERT-Large 83.1 (20.2) 96.1 (1.1) 96.5 (1.2) 98.3 (0.6) 89.7 (11.5) 97.9 (0.6) 98.1 (0.7) 99.1 (0.3)\nFine-Tuning BERT-Base 72.7 (16.7) 89.2 (5.4) 93.9 (1.6) 97.0 (1.6) 72.5 (11.2) 89.1 (4.6) 93.5 (1.6) 96.7 (1.8)\nBERT-Large 79.2 (14.7) 91.3 (2.4) 95.2 (1.2) 96.1 (1.8) 79.9 (11.7) 90.9 (2.4) 94.9 (1.4) 95.7 (2.0)\nsquare\nStatic emb. Fasttext-B 20.5 (7.2) 8.9 (3.6) 38.5 (10.3) - 25.9 (0.8) 25.0 (0.0) 38.8 (1.9) -\nFasttext-C 40.1 (19.0) 60.4 (10.8) 71.8 (3.2) - 39.4 (7.1) 61.5 (5.5) 74.6 (2.9) -\n1NN BERT-Base 70.2 (7.7) 83.3 (7.8) 90.7 (3.0) - 74.1 (10.3) 83.7 (5.1) 88.6 (2.2) -\nBERT-Large 74.9 (13.0) 84.7 (8.3) 93.7 (1.2) - 79.4 (4.6) 84.4 (5.6) 89.0 (4.0) -\nFine-Tuning BERT-Base 64.9 (11.3) 75.4 (11.3) 85.2 (3.6) - 56.7 (4.3) 71.0 (8.1) 81.3 (4.0) -\nBERT-Large 63.9 (17.0) 81.0 (10.2) 87.3 (2.0) - 57.7 (7.3) 76.9 (8.2) 82.9 (1.8) -\narm\nStatic emb. Fasttext-B 53.7 (11.1) 60.0 (3.9) 53.3 (8.9) 80.3 (2.0) 58.6 (1.9) 61.4 (3.5) 62.3 (3.3) 79.9 (2.5)\nFasttext-C 79.1 (7.1) 85.4 (7.0) 90.9 (4.7) 95.7 (0.5) 85.1 (5.1) 86.6 (5.5) 91.6 (3.0) 95.1 (0.8)\n1NN BERT-Base 99.4 (0.00) 99.4 (0.0) 99.4 (0.0) 99.4 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0)\nBERT-Large 99.4 (0.00) 99.4 (0.0) 99.4 (0.0) 99.4 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0) 99.6 (0.0)\nFine-Tuning BERT-Base 96.3 (2.8) 99.4 (0.0) 99.4 (0.0) 99.4 (0.0) 95.0 (3.9) 99.2 (0.0) 99.2 (0.0) 99.2 (0.0)\nBERT-Large 97.4 (2.1) 98.8 (0.9) 99.4 (0.0) 99.4 (0.0) 96.4 (2.9) 98.4 (1.1) 99.2 (0.0) 99.2 (0.0)\ndigit\nStatic emb. Fasttext-B 45.2 (5.1) 54.0 (16.8) 43.7 (9.0) - 49.0 (5.1) 69.4 (11.1) 62.8 (5.0) -\nFasttext-C 69.8 (9.2) 84.9 (8.1) 87.3 (5.9) - 63.3 (3.8) 85.0 (5.5) 89.2 (2.7) -\n1NN BERT-Base 96.8 (2.2) 99.2 (1.1) 100 (0.0) - 92.6 (5.2) 98.1 (2.6) 100 (0.0) -\nBERT-Large 94.4 (6.3) 100 (0.0) 100 (0.0) - 87.0 (14.6) 100 (0.0) 100 (0.0) -\nFine-Tuning BERT-Base 96.0 (2.2) 100 (0.0) 100 (0.0) - 93.6 (4.1) 100 (0.0) 100 (0.0) -\nBERT-Large 95.2 (5.1) 98.4 (1.1) 100 (0.0) - 91.2 (10.0) 97.5 (1.7) 100 (0.0) -\n48\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nMicro F1 Macro F1\n1 3 10 30 1 3 10 30\nbass\nStatic emb. Fasttext-B 27.8 (8.9) 22.2 (0.5) 33.5 (15.3) 60.7 (3.6) 39.2 (3.6) 36.8 (2.6) 39.2 (6.7) 71.4 (1.1)\nFasttext-C 37.1 (8.2) 49.8 (5.3) 65.1 (6.2) 78.9 (3.2) 49.4 (3.9) 57.5 (3.3) 67.1 (1.8) 78.3 (2.2)\n1NN BERT-Base 65.6 (17.9) 75.2 (7.1) 70.4 (5.7) 77.2 (2.1) 60.4 (3.2) 71.8 (5.8) 71.2 (1.6) 77.2 (0.5)\nBERT-Large 70.2 (17.8) 76.9 (6.5) 77.1 (4.5) 83.4 (4.2) 70.3 (4.6) 78.6 (4.5) 80.3 (1.9) 83.4 (0.6)\nFine-Tuning BERT-Base 63.0 (12.3) 67.8 (5.5) 82.5 (1.8) 86.5 (1.2) 54.2 (2.6) 62.2 (3.9) 71.1 (1.4) 76.2 (0.8)\nBERT-Large 79.4 (15.5) 70.4 (10.0) 76.4 (7.5) 88.1 (2.8) 67.6 (7.7) 65.0 (5.3) 68.7 (5.2) 78.5 (2.7)\nyard\nStatic emb. Fasttext-B 48.6 (10.8) 35.6 (2.6) 40.3 (12.3) - 56.0 (7.0) 54.6 (7.5) 61.0 (5.6) -\nFasttext-C 63.4 (26.2) 74.1 (9.8) 83.8 (9.2) - 62.3 (13.9) 78.5 (7.2) 86.7 (6.2) -\n1NN BERT-Base 52.8 (15.8) 70.8 (11.9) 77.3 (2.9) - 68.4 (13.0) 82.8 (7.1) 86.6 (1.7) -\nBERT-Large 65.3 (20.5) 81.0 (14.3) 85.6 (7.6) - 79.5 (12.1) 88.8 (8.4) 91.5 (4.5) -\nFine-Tuning BERT-Base 56.0 (13.6) 67.1 (15.9) 92.1 (4.6) - 48.3 (10.5) 62.1 (13.1) 87.8 (6.2) -\nBERT-Large 48.6 (8.6) 82.9 (9.6) 90.7 (5.7) - 46.7 (7.2) 77.4 (9.7) 85.9 (7.8) -\npound\nStatic emb. Fasttext-B 57.7 (20.7) 39.9 (18.3) 40.5 (3.4) - 57.3 (7.0) 51.7 (2.6) 55.1 (2.0) -\nFasttext-C 61.2 (20.5) 58.8 (5.8) 68.7 (5.1) - 57.7 (7.5) 62.3 (2.9) 73.7 (3.8) -\n1NN BERT-Base 61.9 (19.0) 66.3 (14.5) 77.7 (9.1) - 55.1 (6.6) 81.2 (8.1) 86.1 (4.1) -\nBERT-Large 69.4 (26.0) 69.4 (8.5) 82.1 (5.1) - 59.4 (3.0) 81.5 (6.1) 90.0 (2.8) -\nFine-Tuning BERT-Base 61.9 (10.2) 63.6 (16.6) 74.2 (9.7) - 45.1 (4.7) 52.8 (10.7) 64.5 (8.2) -\nBERT-Large 64.6 (17.2) 59.1 (2.6) 81.1 (9.4) - 47.9 (5.9) 51.4 (1.5) 70.0 (8.0) -\ndeck\nStatic emb. Fasttext-B 54.6 (17.2) 73.1 (10.8) 71.0 (10.5) - 51.4 (7.9) 54.7 (4.3) 62.4 (2.6) -\nFasttext-C 68.4 (32.0) 66.0 (13.0) 77.8 (3.3) - 61.0 (2.1) 61.9 (4.1) 72.6 (10.0) -\n1NN BERT-Base 81.8 (12.0) 85.2 (8.1) 86.9 (1.4) - 81.4 (9.0) 81.0 (4.6) 90.7 (2.8) -\nBERT-Large 76.4 (15.9) 87.5 (2.5) 90.9 (0.8) - 78.5 (2.9) 84.5 (5.3) 95.1 (0.4) -\nFine-Tuning BERT-Base 86.9 (7.3) 87.9 (1.4) 86.9 (2.2) - 70.8 (11.9) 69.9 (1.3) 70.3 (3.6) -\nBERT-Large 77.4 (17.8) 88.6 (2.4) 90.6 (2.7) - 55.2 (10.4) 63.8 (12.2) 77.7 (4.0) -\nbank\nStatic emb. Fasttext-B 46.5 (12.1) 46.9 (4.6) 51.0 (8.8) 77.8 (5.5) 56.8 (2.0) 64.9 (1.2) 62.7 (2.7) 72.5 (3.7)\nFasttext-C 38.4 (6.8) 70.1 (11.9) 80.7 (2.8) 88.2 (4.6) 61.9 (0.3) 72.8 (4.6) 79.1 (3.4) 85.9 (2.4)\n1NN BERT-Base 98.8 (0.7) 99.4 (0.5) 99.7 (0.1) 99.8 (0.0) 94.3 (3.3) 95.4 (4.9) 97.7 (0.1) 97.7 (0.0)\nBERT-Large 99.0 (0.7) 99.5 (0.1) 99.9 (0.1) 99.9 (0.1) 95.9 (3.5) 97.6 (1.8) 99.2 (1.1) 99.2 (1.1)\n49\nComputational Linguistics Volume 0, Number 1\nMicro F1 Macro F1\n1 3 10 30 1 3 10 30\nFine-Tuning BERT-Base 91.9 (5.7) 97.8 (1.4) 97.9 (0.8) 99.0 (0.1) 76.1 (9.4) 89.4 (5.2) 90.5 (3.3) 95.3 (0.5)\nBERT-Large 58.0 (28.9) 98.6 (0.7) 99.5 (0.1) 98.6 (0.6) 52.3 (28.3) 92.3 (4.0) 97.3 (0.5) 93.5 (2.4)\npitcher\nStatic emb. Fasttext-B 92.1 (2.2) 82.7 (9.3) 82.8 (1.3) - 69.2 (8.0) 73.4 (10.1) 82.4 (4.3) -\nFasttext-C 96.5 (4.2) 95.9 (1.8) 91.2 (3.0) - 84.2 (13.5) 94.1 (0.9) 94.3 (3.0) -\n1NN BERT-Base 100 (0.0) 99.9 (0.1) 99.8 (0.0) - 100 (0.0) 99.9 (0.0) 99.9 (0.0) -\nBERT-Large 100 (0.0) 100 (0.0) 99.9 (0.0) - 100 (0.0) 100 (0.0) 100 (0.0) -\nFine-Tuning BERT-Base 98.6 (0.5) 98.9 (0.6) 97.2 (1.1) - 70.6 (5.3) 76.2 (10.2) 62.8 (4.0) -\nBERT-Large 97.1 (2.3) 98.7 (1.2) 98.5 (1.1) - 70.5 (17.3) 77.3 (13.8) 74.8 (13.6) -\nTable 16: Micro- and macro-F1 results in the n-shot setting for all the two BERT-based WSD strategies (as well as for the static embedding\nbaseline) in our experiments and for all the words in the dataset. Results are the average of three runs (standard deviation is shown in\nparentheses).\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\nCrane\ncrane1 crane\n(machine)\nA crane is a type of machine, generally equipped with a hoist\nrope, wire ropes or chains, and sheaves, that can be used both to\nlift and lower materials and to move them horizontally.\nlaunching and recovery is accomplished with the\nassistance of a shipboard crane .\ncrane2 crane (bird) Cranes are a family, the Gruidae, of large, long-legged, and long-\nnecked birds in the group Gruiformes.\ntibet hosts species of wolf , wild donkey , crane ,\nvulture , hawk , geese , snake , and buffalo .\nJava\njava1 java Java is an island of Indonesia, bordered by the Indian Ocean on\nthe south and the Java Sea on the north.\nin indonesia , only sumatra , borneo , and papua\nare larger in territory , and only java and sumatra\nhave larger populations .\njava2 java (pro-\ngramming\nlanguage)\nJava is a general-purpose programming language that is class-\nbased, object-oriented, and designed to have as few implemen-\ntation dependencies as possible.\nexamples include the programming languages\nperl , java and lua .\nApple\napple1 apple inc. Apple Inc. is an American multinational technology company\nheadquartered in Cupertino, California, that designs, develops,\nand sells consumer electronics, computer software, and online\nservices.\nshopify released a free mobile app on the apple\napp store on may 13 , 2010 .\n50\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\napple2 apple An apple is an edible fruit produced by an apple tree. cherry , apple , pear , peach and apricot trees are\navailable .\nMole\nmole1 mole (animal) Moles are small mammals adapted to a subterranean lifestyle\n(i.e., fossorial).\nits primary prey consists of mice , rat , squirrel ,\nchipmunk , shrew , mole and rabbits .\nmole2 mole\n(espionage)\nIn espionage jargon, a mole is a long-term spy who is recruited\nbefore having access to secret intelligence, subsequently manag-\ning to get into the target organization.\nphilip meets claudia where she tells him that\nthere is a mole working for the fbi .\nmole3 mole (unit) The mole (symbol: mol) is the unit of measurement for amount\nof substance in the International System of Units (SI).\nso the speciﬁc heat of a classical solid is always\n3k per atom , or in chemistry units , 3r per mole\nof atoms .\nmole4 mole sauce Mole is a traditional marinade and sauce originally used in\nMexican cuisine.\nfood such as cake , chicken with mole , hot choco-\nlate , coffee , and atole are served .\nmole5 mole (architec-\nture)\nA mole is a massive structure, usually of stone, used as a pier,\nbreakwater, or a causeway between places separated by water.\nthe islands of pomègues and ratonneau are con-\nnected by a mole built in 1822 .\nSpring\nspring1 spring (hydrol-\nogy)\nA spring is a point at which water ﬂows from an aquifer to the\nEarth’s surface. It is a component of the hydrosphere.\nthe village was famous for its mineral water\nspring used for healing in sanatorium , including\nthe hawthorne and lithia springs .\nspring2 spring\n(season)\nSpring, also known as springtime, is one of the four temperate\nseasons, succeeding winter and preceding summer.\nthe species is most active during the spring and\nearly summer although it may be seen into late\njune .\nspring3 spring\n(device)\nA spring is an elastic object that stores mechanical energy. often spring are used to reduce backlash of the\nmechanism .\nChair\nchair1 chairman The chairperson (also chair, chairman, or chairwoman) is the pre-\nsiding ofﬁcer of an organized group such as a board, committee,\nor deliberative assembly.\ngan is current chair of the department of envi-\nronmental sciences at university of california ,\nriverside .\nchair2 chair One of the basic pieces of furniture, a chair is a type of seat. a typical western living room may contain fur-\nnishings such as a sofa , chair , occasional table ,\nand bookshelves , electric lamp , rugs , or other\nfurniture .\n51\nComputational Linguistics Volume 0, Number 1\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\nHood\nhood1 hood (comics) Hood (real name Parker Robbins) is a ﬁctional character, a\nsupervillain, and a crime boss appearing in American comic\nbooks published by Marvel Comics.\nthe hood has hired him as part of his criminal\norganization to take advantage of the split in\nthe superhero community caused by the super-\nhuman registration act .\nhood2 hood (vehicle) The hood (North American English) or bonnet (Commonwealth\nEnglish excluding Canada) is the hinged cover over the engine\nof motor vehicles that allows access to the engine compartment,\nor trunk (boot in Commonwealth English) on rear-engine and\nsome mid-engine vehicles) for maintenance and repair.\neuropean versions of the car also had an air intake\non the hood .\nhood3 hood\n(headgear)\nA hood is a kind of headgear that covers most of the head and\nneck, and sometimes the face.\nin some sauna suits , the jacket also includes a\nhood to provide additional retention of body heat\n.\nSeal\nseal1 pinniped Pinnipeds, commonly known as seals, are a widely distributed\nand diverse clade of carnivorous, ﬁn-footed, semiaquatic marine\nmammals.\nanimals such as shark , stingray , weever ﬁsh ,\nseal and jellyﬁsh can sometimes present a danger\n.\nseal2 seal (musi-\ncian)\nHenry Olusegun Adeola Samue (born 19 February 1963), known\nprofessionally as Seal, is a British singer-songwriter.\nshe was married to english singer seal from 2005\nuntil 2012 .\nseal3 seal (emblem) A seal is a device for making an impression in wax, clay, paper,\nor some other medium, including an embossment on paper, and\nis also the impression thus made.\neach level must review , add information as nec-\nessary , and stamp or seal that the submittal was\nexamined and approved by that party .\nseal4 seal (mechani-\ncal)\nA mechanical seal is a device that helps join systems or mecha-\nnisms together by preventing leakage (e.g. in a pumping system),\ncontaining pressure, or excluding contamination.\ngenerally speaking , standard ball joints will out-\nlive sealed ones because eventually the seal will\nbreak , causing the joint to dry out and rust .\nBow\nbow1 bow (ship) The bow is the forward part of the hull of a ship or boat, the stem is the most forward part of a boat or ship\n’s bow and is an extension of the keel itself .\nbow2 bow and ar-\nrow\nThe bow and arrow is a ranged weapon system consisting of\nan elastic launching device (bow) and long-shafted projectiles\n(arrows).\nbow and arrow used in warfare .\nbow3 bow (music) In music, a bow is a tensioned stick which has hair (usually\nhorse-tail hair) coated in rosin (to facilitate friction) afﬁxed to it.\nhorsehair is used for brush , the bow of musical\ninstruments and many other things .\nClub\nclub1 club A club is an association of people united by a common interest\nor goal.\nthis is a partial list of women ’s association foot-\nball club teams from all over the world sorted by\nconfederation .\n52\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\nclub2 nightclub A nightclub, music club, or club, is an entertainment venue and\nbar that usually operates late into the night.\nalthough several of his tracks were club hits , he\nhad limited chart success .\nclub3 club (weapon) A club (also known as a cudgel, baton, bludgeon, truncheon,\ncosh, nightstick or impact weapon) is among the simplest of all\nweapons: a short staff or stick, usually made of wood, wielded\nas a weapon since prehistoric times.\nbefore their adoption of guns , the plains indi-\nans hunted with spear , bows and arrows , and\nvarious forms of club .\nTrunk\ntrunk1 trunk (botany) In botany, the trunk (or bole) is the stem and main wooden axis\nof a tree.\nits leaves are different from the leaves of true\npalms , and unlike true palms it does not develop\na woody trunk .\ntrunk2 trunk\n(automobile)\nThe trunk (North American English), boot (British English),\ndickey (Indian English) (also spelled dicky or diggy) or compart-\nment (South-East Asia) of a car is the vehicle’s main storage or\ncargo compartment.\nunlike the bmw x5 , the x-coupe had an alu-\nminium body , a trunk opening downwards and\ntwo doors that swing outward .\ntrunk3 trnuk\n(anatomy)\nThe torso or trunk is an anatomical term for the central part or\ncore of many animal bodies (including humans) from which\nextend the neck and limbs.\nsurface projections of the major organs of the\ntrunk , using the vertebral column and rib cage\nas main reference points of superﬁcial anatomy .\nSquare\nsquare1 square In geometry, a square is a regular quadrilateral, which means\nthat it has four equal sides and four equal angles (90-degree\nangles, or 100-gradian angles or right angles).\nsimilarly , a square with all sides of length has\nthe perimeter and the same area as the rectangle .\nsquare2 square\n(company)\nSquare Co., Ltd. was a Japanese video game company founded\nin September 1986 by Masafumi Miyamoto. It merged with Enix\nin 2003 to form Square Enix.\nvideo game by square , features the orbital eleva-\ntor ” a.t.l.a.s. ” .\nsquare3 town square A town square is an open public space commonly found in the\nheart of a traditional town used for community gatherings.\nhere is a partial list of notable expressways ,\ntunnel , bridge , road , avenues , street , crescent ,\nsquare and bazaar in hong kong .\nsquare4 square\nnumber\nIn mathematics, a square number or perfect square is an integer\nthat is the square of an integer.\nin mathematics eighty-one is the square of 9 and\nthe fourth power of 3 .\nArm\narm1 arm architec-\nture\nArm (previously ofﬁcially written all caps as ARM and usually\nwritten as such today), previously Advanced RISC Machine,\noriginally Acorn RISC Machine, is a family of reduced instruc-\ntion set computing (RISC) architectures for computer processors,\nconﬁgured for various environments.\nwindows embedded compact is available for arm\n, mips , superh and x86 processor architectures .\n53\nComputational Linguistics Volume 0, Number 1\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\narm2 arm In human anatomy, the arm is the part of the upper limb between\nthe glenohumeral joint (shoulder joint) and the elbow joint.\non the human body , the limb can be divided into\nsegments , such as the arm and the forearm of\nthe upper limb , and the thigh and the leg of the\nlower limb .\nDigit\ndigit1 numerical\ndigit\nA numerical digit is a single symbol (such as \"2\" or \"5\") used\nalone, or in combinations (such as \"25\"), to represent numbers\n(such as the number 25) according to some positional numeral\nsystems.\nit uses the digit 0 , 1 , 2 and 3 to represent any real\nnumber .\ndigit2 digit\n(anatomy)\nA digit is one of several most distal parts of a limb, such as\nﬁngers or toes, present in many vertebrates.\na ﬁnger is a limb of the human body and a type\nof digit , an organ of and found in the hand of\nhuman and other primate .\nBass\nbass1 bass (guitar) The bass guitar, electric bass, or simply bass, is the lowest-\npitched member of the guitar family.\nthe band decided to continue making music after\nthirsk ’s death , and brought in bass guitarist\nrandy bradbury from one hit wonder .\nbass2 bass (voice\ntype)\nA bass is a type of classical male singing voice and has the lowest\nvocal range of all voice types.\nhe is known for his distinctive and untrained bass\nvoice .\nbass3 double bass The double bass, also known simply as the bass (or by other\nnames), is the largest and lowest-pitched bowed (or plucked)\nstring instrument in the modern symphony orchestra.\nhis instruments were the bass and the tuba .\nYard\nyard1 yard The yard (abbreviation: yd) is an English unit of length, in both\nthe British imperial and US customary systems of measurement,\nthat comprises 3 feet or 36 inches.\naccuracy is sufﬁcient for hunting small game at\nranges to 50 yard .\nyard2 yard (sailing) A yard is a spar on a mast from which sails are set. aubrey improves sophie s sailing qualities by\nadding a longer yard which allows him to spread\na larger mainsail .\nPound\npound1 pound (mass) The pound or pound-mass is a unit of mass used in the imperial,\nUnited States customary and other systems of measurement.\nit is approximately 16.38 kilogram ( 36.11 pound\n) .\npound2 pound\n(currency)\nA pound is any of various units of currency in some nations. in english , the maltese currency was referred to\nas the pound originally and for many locals this\nusage continued .\nDeck\ndeck1 deck (ship) A deck is a permanent covering over a compartment or a hull of\na ship.\nthe protective deck was thick and ran the full\nlength of the ship .\n54\nLoureiro and Rezaee et al. Analysis and Evaluation of Language Models for WSD\nWord Sense # Sense ID Deﬁnition (1st sentence from Wikipedia) Example usage (tokenized)\ndeck2 deck\n(building)\nIn architecture, a deck is a ﬂat surface capable of supporting\nweight, similar to a ﬂoor, but typically constructed outdoors,\noften elevated from the ground, and usually connected to a\nbuilding.\ntypically , it is a wooden deck near a hiking trail\nthat provides the hikers a clean and even place to\nsleep .\nBank\nbank1 bank A bank is a ﬁnancial institution that accepts deposits from the\npublic and creates a demand deposit, while simultaneously\nmaking loans.\nthe bank , which loans money to the player after\nthey have a house for collateral .\nbank2 bank (geogra-\nphy)\nIn geography, a bank is the land alongside a body of water. singapore ’s ﬁrst market was located at the south\nbank of the singapore river .\nPitcher\npitcher1 pitcher In baseball, the pitcher is the player who throws the baseball\nfrom the pitcher’s mound toward the catcher to begin each play,\nwith the goal of retiring a batter, who attempts to either make\ncontact with the pitched ball or draw a walk.\nkasey garret olemberger ( born march 18 , 1978 ) is\nan italian american professional baseball pitcher .\npitcher2 pitcher\n(container)\nIn American English, a pitcher is a container with a spout used\nfor storing and pouring liquids.\npottery was found as grave goods , including\ncombinations of pitcher and cup .\nTable 17: Sense deﬁnitions in the CoarseWSD-20 dataset. Each sense is accompanied with an example usage from the dataset. Sense IDs\ncorrespond to the current Wikipedia page of each sense by the date of the submission.\n55",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8464750051498413
    },
    {
      "name": "Ambiguity",
      "score": 0.7488638162612915
    },
    {
      "name": "Word-sense disambiguation",
      "score": 0.7110375761985779
    },
    {
      "name": "Natural language processing",
      "score": 0.6545267105102539
    },
    {
      "name": "Noun",
      "score": 0.6053341031074524
    },
    {
      "name": "Exploit",
      "score": 0.5880489349365234
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5840708017349243
    },
    {
      "name": "Word (group theory)",
      "score": 0.5661330819129944
    },
    {
      "name": "Language model",
      "score": 0.4874579906463623
    },
    {
      "name": "Transformer",
      "score": 0.4678305685520172
    },
    {
      "name": "Context (archaeology)",
      "score": 0.442403107881546
    },
    {
      "name": "WordNet",
      "score": 0.2616530656814575
    },
    {
      "name": "Linguistics",
      "score": 0.25377827882766724
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}