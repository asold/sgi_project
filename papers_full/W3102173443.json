{
  "title": "Connecting the Dots: Event Graph Schema Induction with Path Language Modeling",
  "url": "https://openalex.org/W3102173443",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2232120597",
      "name": "Manling Li",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A1986157660",
      "name": "Qi Zeng",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2106668053",
      "name": "Ying Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": [
        "New York University",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2127420617",
      "name": "Heng Ji",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2096853820",
      "name": "Jonathan May",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2179135478",
      "name": "Nathanael Chambers",
      "affiliations": [
        "United States Naval Academy"
      ]
    },
    {
      "id": "https://openalex.org/A3204199026",
      "name": "Clare Voss",
      "affiliations": [
        "DEVCOM Army Research Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2366141641",
    "https://openalex.org/W3081279176",
    "https://openalex.org/W2133919533",
    "https://openalex.org/W2145004847",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2511178802",
    "https://openalex.org/W2739724928",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2250836735",
    "https://openalex.org/W2578412240",
    "https://openalex.org/W2895544589",
    "https://openalex.org/W2335559472",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W191570832",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2257051837",
    "https://openalex.org/W1978024959",
    "https://openalex.org/W2514651853",
    "https://openalex.org/W1984984816",
    "https://openalex.org/W2251344056",
    "https://openalex.org/W2985074250",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W4288621201",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W2124996875",
    "https://openalex.org/W2963214720",
    "https://openalex.org/W2169943035",
    "https://openalex.org/W4298275775",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2145374219",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2510168323",
    "https://openalex.org/W2897050046",
    "https://openalex.org/W2973208349",
    "https://openalex.org/W2963847835",
    "https://openalex.org/W2972780142",
    "https://openalex.org/W2517585385",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2252139350",
    "https://openalex.org/W2320652849",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W1702669762",
    "https://openalex.org/W2951248129",
    "https://openalex.org/W2955654429",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2294429012",
    "https://openalex.org/W2158794898",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W4293547730",
    "https://openalex.org/W2913743423",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2790755967",
    "https://openalex.org/W3017458938"
  ],
  "abstract": "Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers, Clare Voss. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 684–695,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n684\nConnecting the Dots: Event Graph Schema Induction\nwith Path Language Modeling\nManling Li1, Qi Zeng1, Ying Lin1, Kyunghyun Cho2, Heng Ji1,\nJonathan May3, Nathanael Chambers4, Clare Voss5\n1University of Illinois at Urbana-Champaign\n2New York University3University of Southern California\n4US Naval Academy 5US Army Research Laboratory\n{manling2,qizeng2,yinglin8,hengji}@illinois.edu,\nkyunghyun.cho@nyu.edu, jonmay@isi.edu,\nnchamber@usna.edu, clare.r.voss.civ@mail.mil\nAbstract\nEvent schemas can guide our understanding\nand ability to make predictions with respect\nto what might happen next. We propose a\nnew Event Graph Schema , where two event\ntypes are connected through multiple paths\ninvolving entities that ﬁll important roles in\na coherent story. We then introduce Path\nLanguage Model, an auto-regressive language\nmodel trained on event-event paths, and se-\nlect salient and coherent paths to probabilisti-\ncally construct these graph schemas. We de-\nsign two evaluation metrics, instance cover-\nage and instance coherence , to evaluate the\nquality of graph schema induction, by check-\ning when coherent event instances are covered\nby the schema graph. Intrinsic evaluations\nshow that our approach is highly effective at\ninducing salient and coherent schemas. Ex-\ntrinsic evaluations show the induced schema\nrepository provides signiﬁcant improvement\nto downstream end-to-end Information Extrac-\ntion over a state-of-the-art joint neural extrac-\ntion model, when used as additional global fea-\ntures to unfold instance graphs.1\n1 Introduction\nExisting approaches to automated event extraction\nretain the overly simplistic assumption that events\nare atomic occurrences. Understanding events re-\nquires knowledge in the form of a repository of ab-\nstracted event schemas (complex event templates).\nScripts (Schank and Abelson, 1977) encode fre-\nquently recurring event sequences, where events\nare ordered by temporal relation (Chambers and Ju-\nrafsky, 2009), causal relation (Mostafazadeh et al.,\n2016b), or narrative order (Jans et al., 2012). Event\nschemas have become increasingly important for\nnatural language understanding tasks such as story\n1Our code and data are publicly available for research pur-\npose at http://blender.cs.illinois.edu/software/\npathlm.\nending prediction (Mostafazadeh et al., 2016a) and\nreading comprehension (Koˇcisk´y et al., 2018; Os-\ntermann et al., 2019).\nPrevious schema induction methods mostly ig-\nnore uncertainty, re-occurring events and multiple\nhypotheses, with limited attention to capture com-\nplex relations among events, other than temporal or\ncausal relations. Temporal relations exist between\nalmost all events, even those that are not semanti-\ncally related; while research in identifying causal\nrelations has been hobbled by low inter-annotator\nagreement (Hong et al., 2016).\nIn this paper, we hypothesize that two events\nare connected when their entity arguments are co-\nreferential or semantically related. For example, in\nFigure 1, (a) and (b) refer to very different event\ninstances, but they both illustrate a typical scenario\nwhere a group of people moved from one place to\nanother and then attacked the destination. From\nmany such event instance pairs, we can induce\nmultiple paths connecting a movement event to\na related attack event: the person being moved\nbecame the attacker, and the weapon or vehicle\nbeing moved became the instrument of the attack.\nLow-level primitive components of event schemas\nare abundant, and can be part of multiple, sparsely\noccurring, higher-level graph schemas. We thus\npropose a new schema representation,Event Graph\nSchema, where two event types are connected by\nsuch paths containing entity-entity relations. Each\nnode represents an entity type or event type, and\neach edge represents an entity-entity relation type\nor the argument role of an entity played in an event.\nHowever, between two event types, there may\nalso be noisy paths that should be excluded from\ngraph schemas. We deﬁne the following criteria to\nselect good paths in a graph schema: (1). Salience:\nA good path should appear frequently between two\nevent types; (2). Coherence: Multiple paths be-\ntween the same pair of event types should tell a\n685\nEvent Instance Graphs Path Language Model\nPERTransport Attackartifact attacker Transport GPE PERagent afﬂiation Attackattacker \nTransport GPEorigin WEAartifact instrument AttackTransportWEAartifact instrument Attack\nTransport FAC AttackGPEplace part- \nwhole \ndestination . . .\nTransport FAC GPEdestination part- \nwhole \nAttackPERlocated_in target \nGraph Schema Induction\nAttackTransport\nFAC\nartifact \nattacker \ninstrument WEALOCorigin \ndestination destination part- whole \nlocated_in \nGPE place \nPER\nORG\ntarget general afﬁliation attacker \npart- \nwhole \ntarget \npart-whole \nartifact \nFAC\nPERtroops \nAttackattack deploy \nTransport\ndestination \nSevastopol \nUkraine \nGPE\nGPERussia \nagent \norigin \npart-whole \ninstrument \nartifact \nafﬁliation attacker \nartifact \ntarget \n(a)\nAttack\nhit \nPER\nprotesters \nPERpolice GPEKyiv \nFAC\nSquare Maidan \npart- \nwhole \nlocated_in target \nplace \nWEAstone GPE\nUkraine \nTransport\ncarry \ndestination \nattacker \nlocated_in artifact \ninstrument \npart- \nwhole \nafﬁliation \n(b) \npart- \nwhole \nWEAtank \nFigure 1: The framework of event graph schema induction. Given a news article, we construct an instance graph\nfor every two event instances from information extraction (IE) results. In this example, instance graph (a) tells\nthe story about Russia deploying troops to attack Ukraine using tanks from Russia; instance graph (b) is about\nUkrainian protesters hit police using stones that are being carried to Maidan Square. We learn a path language\nmodel to select salient and coherent paths between two event types and merge them into a graph schema. The\ngraph schema between ATTACK and TRANSPORT is an example output containing the top 20% ranked paths.\ncoherent story, namely they should co-occur fre-\nquently in the same discourse (e.g., the same docu-\nment). Table 1 shows some examples of good paths\nand bad paths.\nAs the ﬁrst attempt to extract such schemas, we\npropose a path language model to select paths\nwhich clearly indicate how two events are con-\nnected through their shared entity arguments or\nthe entity-entity relations between their arguments.\nFor example, in Figure 1 (b), Maidan Square and\nUkraine connect events TRANSPORT and ATTACK\nthrough the path TRANSPORT\nDESTINATION\n− −−−−−− −→FAC\nPART-WHOLE\n−−−−−−−→GPE\nPLACE −1\n−−−−−→ATTACK . We train the\npath language model on two tasks: learning an\nauto-regressive language model (Ponte and Croft,\n1998; Dai and Le, 2015; Peters et al., 2018; Rad-\nford et al.; Yang et al., 2019) to predict an edge or\na node, given previous edges and nodes in a path,\nand a neighboring path classiﬁcation task to predict\nhow likely two paths co-occur. The path language\nmodel is trained from all the paths between two\nevent instances from the same document, based on\nthe assumption that events from the same document\n(especially news document) tell a coherent story.\nWe propose two intrinsic evaluation metrics,in-\nstance coverage and instance coherence, to assess\nwhen event instance graphs are covered by each\ngraph schema, and when different schemas appear\nin the same document. Intrinsic evaluation on held-\nout documents demonstrates that our approach can\nproduce highly salient and coherent schemas.\nSuch event graph schemas can also be exploited\nto enhance the performance of Information Extrac-\ntion (IE) tasks, such as entity extraction, relation\nextraction, event extraction and argument role label-\ning, because most of the existing methods ignore\nsuch inter-event dependencies. For example, from\nthe following sentence “Following the trail of Mo-\nhammed A. Salameh, the ﬁrst suspect arrested in\nthe bombing, investigators discovered a jumble of\nchemicals, chemistry implements and detonating\nmaterials... ”, the state-of-the-art IE system (Lin\net al., 2020) successfully extracts the ARREST JAIL\nevent but fails to extract the INVESTIGATE CRIME\ntriggered by “discovered” and its DEFENDANT ar-\ngument “Mohammed A. Salameh” . Event graph\nscehmas can inform the model that a person who\nis arrested was usually investigated, our IE system\ncan ﬁx this missing error. Therefore we also con-\nduct extrinsic evaluations and show the effective-\nness of the induced schema repository in enhancing\ndownstream end-to-end IE tasks.\nIn summary, we make the following novel con-\ntributions:\n686\n•A novel semantic schema induction frame-\nwork for the new event schema representation,\nEvent Graph Schema, that encodes rich event\nstructures and event-event connections, and\ntwo new evaluation metrics to assess graph\nschemas for coverage and coherence.\n•A Path Language Model to select salient and\ncoherent event-event paths and construct an\nevent graph schema repository that is proba-\nbilistic and semantically coherent.\n•The ﬁrst work to show how to apply event\nschema to enhance end-to-end IE.\n2 Problem Formulation\nGiven an input document, we extract instances of\nentities, relations, and events. The type set of enti-\nties and events isΦ, and the type set of entity-entity\nrelations and event argument roles is Ψ. For every\ntwo event instances, we construct an event instance\ngraph g= (V,E,ϕ ) ∈G with all paths connecting\nthe two, as in Figure 1 (a) and (b). V and Eare the\nnode and edge sets, and ϕ: {V,E}→{ Φ,Ψ}is a\nmapping function to obtain the type of each node or\nedge. Each node vi = ⟨wi,ϕ(vi)⟩∈ V represents\nan entity or an event with text mention wi, and\nϕ(vi) ∈Φ denotes its node type. Each set of coref-\nerential entities or events is mapped to one single\nnode. Each edge eij = ⟨vi,ϕ(eij),vj⟩∈ E rep-\nresents an event-argument role or an entity-entity\nrelation, where iand jdenote the involved nodes.\nϕ(eij) ∈Ψ indicates the edge type. Figure 1 shows\ntwo example instance graphs.\nEvent graph schema induction aims to generate\na set of recurring graph schemas Sfrom instance\ngraphs G. For every event type pair, we induce\nan event graph schema s = ( U,H) ∈S, where\nU and H are the node and edge sets. Each node\nui = ⟨φi⟩∈ U is a node type φi ∈Φ in instance\ngraphs G, and each edge hij = ⟨φi,ψij,φj⟩∈ H\nrepresents an edge type ψij ∈Ψ in instance graphs\nG, where φi and φj denote the involved node types.\nFigure 1 shows an example of an induced graph\nschema between TRANSPORT and ATTACK .\n3 Path Language Model based Graph\nSchema Induction\n3.1 Overview\nAs shown in Table 1, a graph schema for two event\ntypes consists of salient and coherent paths be-\ntween them. A salient path reveals knowledge of\nrecurring event-event connection patterns. For ex-\nample, the frequent path in Table 1 shows that the\nattacker is a member of the government conduct-\ning a deployment, which repeatedly appears in the\nstory about attackers sending weapons and people\nto attack a target place. However, the attacker is\nunlikely to be afﬁliated with a target place, so the\ninfrequent path in Table 1 should be excluded from\nthe schema.\nIn addition, a good path is semantically coherent.\nFor example, the coherent path in Table 1 shows\nthat the origin of transportation is a subarea of the\nattacker’s country, which captures the hierarchical\npart-whole relation between two places. However,\nin the bad path example, a person is afﬁliated with\nboth the origin and destination of the transportation,\nwhich is a weakly coherent situation.\nFurthermore, multiple paths in a good schema\nshould be semantically consistent, namely they\nshould co-occur frequently in the same scenario.\nFor example, in Table 1, the destination of trans-\nportation is the attack’s target, and meanwhile, is\nthe location of the transported people. The co-\noccurrence of these two paths represents a repeti-\ntive pattern to connect TRANSPORT and ATTACK .\nHowever, the incoherent example in Table 1 indi-\ncates that the attack place is both the destination\nand the origin of the transportation, where two\npaths rarely co-occur.\nTo induce such salient and coherent graph\nschemas, we start by applying Information Extrac-\ntion (IE) to construct instance graphs between event\ninstances in each document (Section 3.2). We con-\nsider a path sequence as a text sequence, and learn\nan auto-regressive path language model to score\neach path (Section 3.3). To capture the coherence\nbetween paths, we learn a neighbor path classiﬁer\nto predict whether two paths co-occur (Section 3.4).\nThe path language model is trained jointly on these\ntwo tasks (Section 3.5), which enables us to score\nand rank paths between event type pairs, and merge\nsalient and coherent paths into graph schemas (Sec-\ntion 3.6).\n3.2 Instance Graph Construction\nStarting with entities, entity-entity relations, events\nand their arguments extracted from an input doc-\nument by IE systems or manual annotation, we\nconstruct an event instance graph gfor two event\ninstances vand v′, that includes all instance paths\n687\nCriteria Examples Frequency\nSingle\nPath\nSalience High TRANSPORT\nAGENT\n− −− −→GPE\nAFFILIATION −1\n−−−−−−−−→PER\nATTACKER −1\n−−−−−−−→ATTACK 31\nLow TRANSPORT\nDESTINATION\n−−−−−−−→GPE\nAFFILIATION −1\n−−−−−−−−→PER\nATTACKER −1\n−−−−−−−→ATTACK 2\nSemantic\nCoherence\nHigh TRANSPORT\nORIGIN\n−−−−→FAC\nPART-WHOLE\n− −−−−− −→LOC\nPART-WHOLE\n− −−−−− −→GPE\nAFFILIATION −1\n−−−−−−−−→\nPER\nATTACKER −1\n−−−−−−−→ATTACK\n9\nLow TRANSPORT\nAGENT\n− −− −→GPE\nAFFILIATION −1\n−−−−−−−−→PER\nAFFILIATION\n−−−−−−→GPE\nRESIDENT −1\n−−−−−−−→\nPER\nTARGET −1\n−−−−−−→ATTACK\n24\nMultiple\nPaths\nSemantic\nConsistency\nHigh TRANSPORT\nDESTINATION\n−−−−−−−→GPE\nPLACE −1\n−−−−−→ATTACK 20\nTRANSPORT\nARTIFACT\n−−−−−→PER\nLOCATED IN\n−−−−−−→GPE\nPLACE −1\n−−−−−→ATTACK\nLow TRANSPORT\nDESTINATION\n−−−−−−−→GPE\nPLACE −1\n−−−−−→ATTACK 0\nTRANSPORT\nORIGIN\n−−−−→GPE\nPLACE −1\n−−−−−→ATTACK\nTable 1: The criteria of path ranking to construct event schema graph. Frequency is computed based on ACE 2005\nannotations. We use ‘−1’ to indicate the reversed edge direction.\nbetween them. Each instance path\npI =\n[\nv,e0;1,v1,...,e n−1;n,v′]\nis a sequence of nodes v,v1,...,v′∈V and edges\ne0;1,...,en−1;n∈E, such as the instance path\nattack\nINSTRUMENT\n−−−−−−−→tank\nARTIFACT\n−−−−−→Russia\nAGENT −1\n−−−−−→deploy\nin Figure 1 (a). The node instances in each path are\ndistinct to avoid cycles. An event-event path is a\nsequence of types of nodes and edges,\np=\n[\nϕ(v),ϕ(e0;1),ϕ(v1),...,ϕ (en−1;n),ϕ(v′)\n]\n.\nFor example, the path abstracted from the instance\npath above is ATTACK\nINSTRUMENT\n−−−−−−−→WEA\nARTIFACT\n−−−−−→\nGPE\nAGENT −1\n−−−−−→TRANSPORT . We consider paths in\nboth directions, namely that reversed paths are\nvalid.\n3.3 Autoregressive Path Language Model\nTo score and select salient and semantically co-\nherent path sequences, we take a language mod-\neling approach, inspired by node representation\nlearning (Grover and Leskovec, 2016; Goikoetxea\net al., 2015) using language model over paths.\nAutoregressive language model (Ponte and Croft,\n1998; Dai and Le, 2015; Peters et al., 2018; Rad-\nford et al.; Yang et al., 2019) learns the prob-\nability of text sequences as the probability dis-\ntribution of each word, given its context factor-\nizing the likelihood of prior words into a for-\nward product or, for context in the other direc-\ntion, a backward product. Similarly, for a path\ninstance pI, we estimate the probability distribu-\ntion of a node type ϕ(vi) (or edge type ϕ(ej;j+1)),\ngiven the sequence of previously observed nodes\nand edges [ϕ(v),ϕ(e0;1),ϕ(v1),...,ϕ (ei−1;i)],\n(or [ϕ(v),ϕ(e0;1),ϕ(v1),...,ϕ (vi)]), i.e.,\nLLM=\n∑\npI\n[∑\nvi∈pI\nlog P(ϕ(vi)|ϕ(v),...,ϕ(ei−1;i))\n+\n∑\nej;j+1∈pI\nlog P(ϕ(ej;j+1)|ϕ(v),ϕ(e0;1),...,ϕ(vi))\n]\n.\nFollowing (Yang et al., 2019), we apply the Trans-\nformer (Vaswani et al., 2017) to learn the probabil-\nity distribution, with permutation operation (Yang\net al., 2019) to capture bidirectional contexts. Un-\nlike in text sequences, we have nodes and edges\nthat alternate within path sequences. As shown in\nFigure 2, to distinguish nodes and edges, we add\ntype embedding ET = [1,2,1,..., 2,1] into the\ntoken representation, where 1 stands for nodes, 2\nfor edges, and 0 for special tokens such as [CLS].\nWe hypothesize that event instances from the\nsame discourse (e.g., a news document) describe\na coherent story, and so we use the paths between\nthem as training paths.\n3.4 Neighbor Path Classiﬁcation\nTo capture the consistency between paths, we train\na binary neighbor path classiﬁer to learn the oc-\ncurrence probability of two paths. For each path\npi ∈P⟨v,v′⟩between two event instances vand v′,\nwe obtain its neighbor path set as its co-occuring\npaths between the same event instances vand v′,\nNpi = {pj|pj ∈P⟨v,v′⟩,v,v ′∈V}.\n688\nAttackattacker [SEP]GPE\nPath A:\nAttack \t-\t a ttacker \t -\t GPE \t -\t a gent \t -1 \t -\t T ransport \nattackerGPE [CLS]agent- 1 \nNeighbor \nPath \nClassiﬁer\nAuto-\nregressive\nPath\nLanguage\nModel\n1 = neighbor\n0 = not neighbor\nWord\nPosition\nSegment\nElement\n0 0 0\n1 2 1\n1 2 3\nagent- 1 \nTransport\n0\n2\n4\nTransport\n[SEP]\n0\n1\n5\n[SEP]AttackinstrumentWEAartifactGPEagent- 1 \nPath B:    \nAttack \t-\t instrument \t -\t WEA \t-\t artifact \t -\t GPE \t-\t a gent \t-1 \t -\t T ransport \nTransport\nAttackinstrumentWEAartifactGPEagent- 1 Transport[SEP]\n1 1 1 1 1 1 1 1\n0 1 2 1 2 1 2 1\n6 7 8 9 10 11 12 13 14\n1\n0\nFigure 2: Autoregressive path language model with neighbor path classiﬁcation.\nWe sample negative neighbor paths from paths that\nappear between the same event types ϕ(v) and\nϕ(v′), but never occur with pi in the corpus.\nN′\npi = {pj|pj ∈P⟨ϕ(v),ϕ(v′)⟩,pj /∈Npi}.\nWe also swap each path pair to improve the con-\nsistency of the neighbor path classiﬁcation. The\nneighbor path classiﬁer (top of Figure 2) is a linear\nlayer with the classiﬁcation token x[CLS] as input,\nP(pj ∈Npi) = sigmoid\n(\nWx[CLS] + b\n)\n.\nWe balance the positive and negative path pairs\nduring training, and optimize cross-entropy loss,\nLNP =\n∑\npi\n[ ∑\npj∈Npi\nlog P(pj ∈Npi)\n+\n∑\npj∈N′pi\nlog(1 −P(pj ∈Npi))\n]\n.\n3.5 Joint Training\nWe jointly optimize autoregressive language model\nloss and neighbor path classiﬁer loss,\nL= LLM + λLNP.\n3.6 Graph Schema Construction\nGiven two event types φ and φ′, we construct\na graph schema s by merging the top k percent\nranked paths. Paths in P⟨φ,φ′⟩are ranked in terms\nof a score function f(p),\nf(pi) = fLM(pi) + αfNP(pi),\nwhere fLM(p) captures salience and coherence of\na single path,\nfLM(pi) = log P(\n[\nφ,ψ0;1,φ1,ψ1;2,...,φ ′]\n),\nand where fNP(p) scores a path pi by its average\nprobability of co-occuring with other paths pj ∈\nP⟨φ,φ′⟩between the given event types φand φ′,\nfNP(pi) = 1\n|P⟨φ,φ′⟩|\n∑\npj∈P⟨φ,φ′⟩\nlog P(pj ∈Npi).\nWe merge instance paths into a graph schema sby\nmapping nodes of the same type into a single node.\nWe allow some self-loops in the graph, such as\nGPE\nPART-WHOLE\n−−−−−−−→GPE . Each path in the schema\nhas a probability,\nP(pi) = exp(f(pi))∑\npj∈sexp(f(pj)).\nEach edge and node is assigned a salience score by\naggregating the scores of paths passing through it,\nf(ψi;j) =\n∑\np∈{p|ψi;j∈p,p∈s}\nP(p), f (φi) =\n∑\np∈{p|φi∈p,p∈s}\nP(p).\n4 Evaluation Benchmark\n4.1 Dataset\nWe use Automatic Content Extraction (ACE) 2005\ndataset2, the widely used dataset with annotated\n2https://www.ldc.upenn.edu/collaborations/\npast-projects/ace\n689\ninstances of 7 entity types, 6 relation types, 33\nevent types, and 22 argument roles. We follow\nour recent work on ACE IE (Lin et al., 2020) to\nsplit the data. We consider the training set as his-\ntorical data to train the LM, and the test set as\nour target data to induce schema for target scenar-\nios. The instance graphs of the target data set are\nconstructed from manual annotations. For histori-\ncal data, we construct event instance graphs from\nboth manual annotations (Historicalann) and system\nextraction results (Historicalsys) from the state-of-\nthe-art IE model (Lin et al., 2020). We perform\ncross-document entity coreference resolution by\napplying an entity linker (Pan et al., 2017) for both\nannotated and system generated instance graphs.\nTable 2 shows the data statistics.\nSplit #Docs #Entities #Rels #Events #Args\nHistoricalann 529 47,525 7,152 4,419 7,888\nHistoricalsys 529 48,664 7,018 4,426 6,614\nValidation 40 3,422 728 468 938\nTarget 30 3,673 802 424 897\nTable 2: Data statistics.\n4.2 Instance Coverage\nA salient schema can serve as a skeleton to recover\ninstance graphs. Therefore, we use each graph\nschema s∈S to match back to each ground-truth\ninstance graph g∈G and evaluate their intersection\ng∩sin terms of Precision and Recall.\nIntersection is obtained by searching instance\ngraphs with each graph schema as a query. Since\ninstance graphs can be regarded as partially in-\nstantiated graph schema, we employ the substruc-\ntures of the schema graph, i.e., paths of different\nlengths, as queries. For example, a path of length\nl= 3 is a triple in graph schema ⟨φi,ψij,φj⟩∈ s.\nWe consider an instance triple ⟨vm,emn,vn⟩∈ g\nmatched if instance types match, i.e., ϕ(vm)=φi,\nϕ(emn)=ψij, ϕ(vn)=φj. Let |·|I denote the num-\nber of instance substructures matched, and |·|S is\nthe number of schema substructures matched, i.e.,\n|g∩s|I =\n∑\n⟨φi,ψij,φj⟩∈s\ncount(⟨vm,emn,vn⟩),\n|g∩s|S =\n∑\n⟨vm,emn,vn⟩∈g\ncount(⟨φi,ψij,φj⟩).\nThe cardinality for an instance graph and a schema\nwill be the number of substructures in each, i.e.,\n|g|I =\n∑\n⟨vm,emn,vn⟩∈g\ncount(⟨vm,emn,vn⟩),\n|s|S =\n∑\n⟨φi,ψij,φj⟩∈s\ncount(⟨φi,ψij,φj⟩).\nBy extension, each path of length l=5 in a graph\nschema [φi,ψij,φj,ψjk,φk] contains two consec-\nutive triples ⟨φi,ψij,φj⟩,⟨φj,ψjk,φk⟩∈s, and a\nmatched instance path contains two consecutive in-\nstance triples ⟨vm,emn,vn⟩,⟨vn,eno,vo⟩∈g, where\nϕ(vm)=φi, ϕ(emn)=ψij, ϕ(vn)=φj, ϕ(eno)=ψjk,\nϕ(vo)=φk. Similarly, a path of lengthl=7 contains\nthree consecutive triples. Then we compute:\nPrecision =\n∑\ns∈S\n∑\ng∈G|g∩s|S\n∑\ns∈S|s|S\n,\nRecall =\n∑\ns∈S\n∑\ng∈G|g∩s|I\n∑\ng∈G|g|I\n.\n4.3 Instance Coherence\nFor an instance graph between two events v and\nv′, we hypothesize that the graph is coherent if v\nand v′ are from the same discourse (document).\nWe carefully select 24 documents with each doc-\nument talking about a unique complex event such\nas Iraq War or North Korea Nuclear Test. A co-\nherent schema should have the maximal number\nof matched instance graphs g ∩s from a single\ndocument, but the minimal number of matched\ngraphs connecting two event instances from dif-\nferent documents. We deﬁne Instance Coherence\nas the proportion of event-event path instances in\ngraphs within one document.\nCoherence =\n∑\ns∈S\n∑\ng∈G\n∑\np∈g∩sf(p) ·Ig\n∑\ns∈S\n∑\ng∈G\n∑\np∈g∩sf(p) ,\nwhere Ig is an indicator function taking value 1\nwhen gis between event instances from the same\ndocument, and value 0 otherwise.\n4.4 Schema-Guided Information Extraction\nAs a case study for extrinsic evaluation, we evaluate\nthe impact of our induced schema3 on end-to-end\nInformation Extraction (IE). We choose our state-\nof-the-art IE system ONEIE (Lin et al., 2020) 4\n3The schema is induced from annotated instance graphs of\nhistorical data, which is the training data of IE system.\n4Code is public available at http://blender.cs.\nillinois.edu/software/oneie/\n690\nas our baseline for two reasons: (1) it achieves\nstate-of-the-art performance on all IE components;\n(2) it can easily incorporate global features during\ndecoding converting each input sentence into an\ninstance graph.\nGiven an input sentence, ONEIE generates a\nset of candidate IE graphs at each decoding step,\nas shown in Figure 3. The candidate IE graphs\nare ranked by type prediction scores s′(G) of each\nentity, relation and event in each graph G. We\nconsider schemas as global features and use them\nas an additional scoring mechanism for ONEIE 5.\nThe schemas are induced from the training data of\nour IE system. If a path pi in the schema appears\nni times in a candidate graph, we add ni ∗wi to\nobtain the global score of this graph,\ns(G) = s′(G) +\n∑\npi∈s,s∈S\nni ∗wi,\nwhere wi is a learnable weight. The candidate\ngraphs are then ranked in terms of their global\nscores. In this way, the model can promote can-\ndidate graphs containing positive global features,\neven if the graphs may have lower local type pre-\ndiction scores.\n5 Experiments\n5.1 Settings\nBaselines. As the ﬁrst to induce event graph\nschema, we compare our method to various path\nranking methods: (1) Frequency Ranking Model\nranks paths between every two event types by the\nnumber of associated instance paths in the histor-\nical and target data. (2) Unigram, Bigram, and\nTrigram Language Models assign probabilities\nto path sequences by estimating the probability\nof each node (or edge) from the unigram, bigram,\nand trigram frequency counts, respectively. We\nalso include a variant of PathLM by removing the\nneighbor path classiﬁer ( CLSNP) as an ablation\nstudy.\nSchema@K. To compare the ranking of paths with\nbaselines, we evaluate graph schemas containing\ntop k% ranked paths.\nImplementation Details. We use the same hyper-\nparameters as in XLNet-base-cased (Yang et al.,\n2019), with dropout = 0.5. λ= 0.1, and α= 0.3.\nDetailed parameter settings are in Appendix.\n5To show the effectiveness of schema, we remove the\noriginal human-designed global features in ONEIE.\n5.2 Results and Analysis\nWe induce 124 and 197 graph schemas for\nSchema@10 and Schema@20 respectively. Fig-\nure 1 shows an output graph schema.6 According\nto Table 3 and Table 4, PathLM achieves signiﬁ-\ncant improvement on both instance coverage and\ninstance coherence. T-test shows that the gains\nachieved by PathLM are all statistically signiﬁ-\ncant over baselines (Frequency, UnigramLM, Bi-\ngramLM, TrigramLM), with a P value less than\n0.01. We make the following observations:\n(1) PathLM achieves larger gains compared to base-\nlines on Schema@10 than Schema@20 in Table 3,\ndemonstrating the effectiveness of our ranking ap-\nproach, especially on top ranked ones.\n(2) The improvement relative to baselines on longer\npath queries (e.g. l= 7) is greater than shorter paths\n(e.g., l= 3) in Table 3, showing that our approach\nis able to capture complex graph structures involv-\ning long distance between related events. In the\nl=3 setting, the performance of PathLM is close to\nbaselines. The reason is that l=3 setting evaluates a\nsingle overlapped triple, which is exactly the objec-\ntive of TrigramLM. We conduct t-test, and the gain\nis statistically signiﬁcant (P value less than 0.01).\n(3) The neighbor path classiﬁcation proves to be ef-\nfective in enhancing the salience (see ‘w/oCLSNP’\nin Table 3) and coherence (see ‘w/oCLSNP’ in Ta-\nble 4) of the induced schemas, showing that salient\nsubstructures can be better captured by frequently\nco-occurring paths. The model outputs consistent\nneighbor path classiﬁcation results for the swapped\npath pairs. 96.17% swapped path pairs yield the\nsame results as original pairs.\n(4) The schemas induced from Historical sys and\nHistoricalann have comparable performance. This\nproves our approach is robust to extraction noise\nand effective even with lower quality input.\nAs shown in Table 5, our event graph schemas\nhave provided signiﬁcant improvement on rela-\ntion extraction and event extraction which require\nknowledge of complex connections among events\nand entities. Our approach achieves dramatic im-\nprovement on relation extraction, because existing\nmethods mainly rely on local contexts between\ntwo entities, which are typically short and ambigu-\nous. In contrast, the paths in our graph schemas\ncan capture the global context between two events,\nand thus event-related information captures deeper\n6Visualization of schema repository is in http://\nblender.cs.illinois.edu/software/pathlm.\n691\nHistorical\nModel\nSchema@10 Schema@20\nInstance l = 7 l = 5 l = 3 l = 7 l = 5 l = 3\nGraphs P R F 1 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1\nHistoricalann\nFrequency 76.7 9.5 16.9 90.5 48.3 63.0 100 37.5 54.6 63.6 17.9 28.0 87.6 70.6 78.2 100 42.6 59.7\nUnigram LM 63.9 7.3 13.1 87.1 35.4 50.3 100 33.7 50.4 55.4 14.8 23.4 86.0 60.8 71.2 100 43.8 60.9\nBigram LM 75.4 8.5 15.3 92.6 36.8 52.6 100 33.4 50.1 62.6 16.4 26.0 88.1 63.2 73.6 100 43.2 60.3\nTrigram LM 62.7 8.5 15.0 89.4 41.6 56.7 100 39.9 57.0 53.4 17.8 26.7 85.6 68.2 75.9 100 44.6 61.6\nPathLM 54.3 16.6 25.4 83.7 63.8 72.4 100 41.8 58.9 53.8 27.2 36.1 83.0 80.0 81.5 100 44.7 61.8\nw/o CLSNP 71.2 14.5 24.1 90.3 58.3 70.9 100 39.8 56.9 57.8 25.8 35.6 85.7 80.1 82.8 100 42.9 60.1\nHistoricalsys\nFrequency 68.6 9.8 17.1 87.0 49.4 63.0 100 37.6 54.7 67.8 19.3 29.9 88.5 70.1 78.2 100 41.6 58.8\nUnigram LM 54.3 7.5 13.1 83.7 36.2 50.5 100 41.0 58.2 52.4 17.9 26.7 83.0 66.4 73.8 100 44.6 61.7\nBigram LM 61.4 7.9 13.9 88.5 37.7 52.8 100 39.2 56.3 58.3 15.3 24.2 86.5 63.8 73.4 100 43.5 60.6\nTrigram LM 65.2 9.8 17.1 89.6 46.8 61.5 100 37.3 54.4 54.5 17.6 26.6 86.2 68.7 76.5 100 44.1 61.2\nPathLM 51.8 18.5 27.3 83.2 68.0 74.8 100 41.7 58.8 49.6 29.3 36.9 81.7 85.4 83.5 100 44.8 61.9\nw/o CLSNP 72.7 14.4 24.1 89.5 55.1 68.2 100 40.1 57.3 54.8 24.7 34.0 83.8 75.9 80.0 100 44.7 61.7\nTable 3: Instance coverage (%) by checking the intersection of schemas and instance graphs.\nHistorical Model Schema@10 Schema@20\nHistoricalann\nFrequency 67.8 65.6\nUnigram LM 62.4 69.9\nBigram LM 59.0 67.5\nTrigram LM 56.6 64.9\nPathLM 76.0 79.9\nw/o CLSNP 75.3 79.2\nHistoricalsys\nFrequency 60.1 65.6\nUnigram LM 61.8 70.0\nBigram LM 59.7 69.6\nTrigram LM 55.8 65.8\nPathLM 76.4 78.5\nw/o CLSNP 73.9 77.1\nTable 4: Instance coherence (%) of schema graphs cov-\nering top kpercent paths, k= 10,20.\nModel Entity Rel Event\nTrig-I Trig-C Arg-I Arg-C\nOneIE Baseline 90.3 44.7 75.8 72.7 57.8 55.5\n+PathLM 90.2 60.9 76.0 73.4 59.0 56.6\nw/o CLSNP 90.1 60.3 75.7 72.8 58.3 55.8\nTable 5: F 1 score (%) of schema-guided information\nextraction, including entity extraction (Entity), rela-\ntion extraction (Rel), event trigger identiﬁcation (Trig-\nI) and classiﬁcation (Trig-C), event argument identiﬁca-\ntion (Arg-I) and argument role classiﬁcation (Arg-C).\ncontextual features, yielding a big boost in perfor-\nmance. For example, when decoding candidate IE\ngraph in Figure 3, the LOCATED IN relation is ex-\ntracted by promoting the structures matching paths\nCandidate IE GraphInput Sentence\nPERTransportCNN Pentagon correspondent\nBarbara Starr reports coalition\ntroops entering [Transport] \nBaghdad were met with ﬁerce\nﬁghting [Attack], and there\nwere casualties on both sides\nAttack\nentering\nﬁghting\ntroops\nGPEBaghdad\nartifact\ntarget\nlocated_in \nTransport GPEdestination AttackPERlocated_in target\nTransport PER AttackGPE placelocated_inartifact\ndestination\nplace\nPaths from Schema Repository\nFigure 3: An example showing how schema improves\nthe quality of IE by promoting the candidate IE graph\nmatching paths from schema.\nin the graph schema.\n5.3 Remaining Challenges\nA major challenge in schema induction is to auto-\nmatically decide the type granularity. For example,\nif two events happen on the same street, it is likely\nthat they are related; if it is a country that connects\nto two events through place arguments, they can\nbe independent. In this case, the ﬁne-grained type\ninformation of shared place argument is required\nin schemas. However, to induce schemas about\nwar, geopolitical entities of different granularities\nshould be generalized as GPE.\n6 Related Work\nAtomic Event Schema Induction. Atomic event\nschema induction methods (Chambers, 2013; Che-\nung et al., 2013; Nguyen et al., 2015; Huang et al.,\n692\n2016; Sha et al., 2016; Yuan et al., 2018) focus\non discovering event types and argument roles of\nindividual atomic events.\nNarrative Event Schema Induction. Previous\nwork (Chambers and Jurafsky, 2008, 2009, 2010;\nJans et al., 2012; Balasubramanian et al., 2013; Pi-\nchotta and Mooney, 2014, 2016; Rudinger et al.,\n2015; Granroth-Wilding and Clark, 2016; Modi,\n2016; Mostafazadeh et al., 2016a; Peng et al., 2019)\nfocuses on inducing narrative schemas as partially\nordered sets of events (represented as verbs) shar-\ning a common argument. The event order is fur-\nther extended to include causality (Mostafazadeh\net al., 2016b; Kalm et al., 2019), and temporal\nscript graph is proposed where events and argu-\nments are abstracted as event types and participant\ntypes (Modi et al., 2017; Wanzare et al., 2017; Zhai\net al., 2019). In our work, we propose a new event\ngraph schema representation to capture more com-\nplex connections between events, and use event\ntypes instead of verbs as in previous work for more\nabstraction power.\nPath-based Language Model. Language mod-\nels (LMs) (Ponte and Croft, 1998) achieve great\nadvances on contextualizing LMs in the last few\nyears (Peters et al., 2018; Devlin et al., 2019; Yang\net al., 2019). LM has been used over paths to learn\nnode representations in a network (Goikoetxea\net al., 2015; Grover and Leskovec, 2016; Dong\net al., 2017). To the best of our knowledge, there\nhas not been an effort to incorporate latent linguis-\ntic structures into language models based on typed\nevent-event paths. This is also the ﬁrst work to\ndemonstrate how to leverage event schemas to en-\nhance the performance of an IE system.\nGraph Pattern Mining. Motif ﬁnding on hetero-\ngeneous networks (Prakash et al., 2004; Carranza\net al., 2018; Rossi et al., 2019; Hu et al., 2019)\ndiscovers highly recurrent instance graph patterns,\nbut fails in abstracting schema graphs to the type\nlevel. Previous work applies graph summarization\nto discover frequent subgraph patterns for heteroge-\nneous networks (Cook and Holder, 1993; Buehrer\nand Chellapilla, 2008; Li and Lin, 2009; Zhang\net al., 2010; Koutra et al., 2014; Wu et al., 2014;\nSong et al., 2018; Bariatti et al., 2020), but ignores\nsemantic coherence among multiple patterns.\n7 Conclusions and Future Work\nWe propose Event Graph Schema induction as a\nnew step towards semantic understanding of inter-\nevent connections. We develope a path language\nmodel based method to construct graph schemas\ncontaining salient and semantically coherent event-\nevent paths, which also effectively enhances end-\nto-end Information Extraction. In the future, we\naim to extend graph schemas to encode hierarchical\nand temporal relations, as well as rich ontologies\nin open domain. We will also assemble our graph\nschemas to represent more complex scenarios in-\nvolving multiple events, so they can be applied\nto more downstream applications including event\ngraph completion and event prediction.\nAcknowledgement\nThis research is based upon work supported in part\nby U.S. DARPA KAIROS Program Nos. FA8750-\n19-2-1004, FA8750-19-2-0500 and FA8750-19-2-\n1003, U.S. DARPA AIDA Program No. FA8750-\n18-2-0014 and Air Force No. FA8650-17-C-7715.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies, either\nexpressed or implied, of DARPA, or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein.\nReferences\nNiranjan Balasubramanian, Stephen Soderland, Oren\nEtzioni, et al. 2013. Generating coherent event\nschemas at scale. In Proceedings of the 2013 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1721–1731.\nFrancesco Bariatti, Peggy Cellier, and S ´ebastien Ferr´e.\n2020. Graphmdl: Graph pattern selection based on\nminimum description length. In International Sym-\nposium on Intelligent Data Analysis , pages 54–66.\nSpringer.\nGregory Buehrer and Kumar Chellapilla. 2008. A scal-\nable pattern mining approach to web graph compres-\nsion with communities. In Proceedings of the 2008\nInternational Conference on Web Search and Data\nMining, pages 95–106.\nAldo G. Carranza, Ryan A. Rossi, Anup Rao, and Eun-\nyee Koh. 2018. Higher-order spectral clustering for\nheterogeneous graphs. CoRR, abs/1810.02959.\nNathanael Chambers. 2013. Event schema induction\nwith a probabilistic entity-driven model. In Pro-\nceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP2013),\nvolume 13, pages 1797–1807.\n693\nNathanael Chambers and Dan Jurafsky. 2008. Unsu-\npervised learning of narrative event chains. In Pro-\nceedings of the 2008 Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL2008) ,\npages 789–797.\nNathanael Chambers and Dan Jurafsky. 2009. Unsuper-\nvised learning of narrative schemas and their partici-\npants. In Proceedings of the Joint conference of the\n47th Annual Meeting of the Association for Compu-\ntational Linguistics and the 4th International Joint\nConference on Natural Language Processing (ACL-\nIJCNLP2009).\nNathanael Chambers and Daniel Jurafsky. 2010. A\ndatabase of narrative schemas. In Proceedings of\nthe 9th International Conference on Language Re-\nsources and Evaluation (LREC2010).\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy\nVanderwende. 2013. Probabilistic frame induction.\nNAACL HLT 2013, pages 837–846.\nDiane J Cook and Lawrence B Holder. 1993. Substruc-\nture discovery using minimum description length\nand background knowledge. Journal of Artiﬁcial In-\ntelligence Research, 1:231–255.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nYuxiao Dong, Nitesh V . Chawla, and Ananthram\nSwami. 2017. metapath2vec: Scalable representa-\ntion learning for heterogeneous networks. In Pro-\nceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, Halifax, NS, Canada, August 13 - 17, 2017 ,\npages 135–144. ACM.\nJosu Goikoetxea, Aitor Soroa, and Eneko Agirre. 2015.\nRandom walks and neural network language mod-\nels on knowledge bases. In NAACL HLT 2015,\nThe 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Denver, Colorado,\nUSA, May 31 - June 5, 2015, pages 1434–1439. The\nAssociation for Computational Linguistics.\nMark Granroth-Wilding and Stephen Clark. 2016.\nWhat happens next? event prediction using a com-\npositional neural network model. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence.\nAditya Grover and Jure Leskovec. 2016. node2vec:\nScalable feature learning for networks. In Proceed-\nings of the 22nd ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining,\nSan Francisco, CA, USA, August 13-17, 2016, pages\n855–864. ACM.\nYu Hong, Tongtao Zhang, Tim O’Gorman, Sharone\nHorowit-Hendler, Heng Ji, and Martha Palmer. 2016.\nBuilding a cross-document event-event relation cor-\npus. In Proceedings of the 10th Linguistic Annota-\ntion Workshop held in conjunction with ACL 2016\n(LAW-X 2016), pages 1–6, Berlin, Germany. Associ-\nation for Computational Linguistics.\nJiafeng Hu, Reynold Cheng, Kevin Chen-Chuan\nChang, Aravind Sankar, Yixiang Fang, and Brian\nY . H. Lam. 2019. Discovering maximal motif\ncliques in large heterogeneous information networks.\nIn 35th IEEE International Conference on Data En-\ngineering, ICDE 2019, Macao, China, April 8-11,\n2019, pages 746–757. IEEE.\nLifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng\nJi, Clare R. V oss, Jiawei Han, and Avirup Sil. 2016.\nLiberal event extraction and event schema induction.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL2016).\nBram Jans, Steven Bethard, Ivan Vuli ´c, and\nMarie Francine Moens. 2012. Skip n-grams\nand ranking functions for predicting script events.\nIn Proceedings of the 13th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics , pages 336–344. Association for\nComputational Linguistics.\nPavlina Kalm, Michael Regan, and William Croft.\n2019. Event structure representation: Between\nverbs and argument structure constructions. In Pro-\nceedings of the First International Workshop on De-\nsigning Meaning Representations, pages 100–109.\nDanai Koutra, U Kang, Jilles Vreeken, and Christos\nFaloutsos. 2014. V og: Summarizing and understand-\ning large graphs. In Proceedings of the 2014 SIAM\ninternational conference on data mining , pages 91–\n99. SIAM.\nTom´aˇs Ko ˇcisk´y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, G´abor Melis, and\nEdward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nCheng-Te Li and Shou-De Lin. 2009. Egocentric in-\nformation abstraction for heterogeneous social net-\nworks. In 2009 International Conference on Ad-\nvances in Social Network Analysis and Mining ,\npages 255–260. IEEE.\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA joint end-to-end neural model for information ex-\ntraction with global features. In Proceedings of the\n2020 Annual Meeting of the Association for Compu-\ntational Linguistics (ACL2020).\nAshutosh Modi. 2016. Event embeddings for seman-\ntic script modeling. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 75–83.\n694\nAshutosh Modi, Tatjana Anikina, Simon Ostermann,\nand Manfred Pinkal. 2017. Inscript: Narrative texts\nannotated with script information. arXiv preprint\narXiv:1703.05260.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016a. A cor-\npus and cloze evaluation for deeper understanding\nof commonsense stories. In Proceedings of NAACL-\nHLT, pages 839–849.\nNasrin Mostafazadeh, Alyson Grealish, Nathanael\nChambers, James Allen, and Lucy Vanderwende.\n2016b. Caters: Causal and temporal relation scheme\nfor semantic annotation of event structures. In Pro-\nceedings of the Fourth Workshop on Events , pages\n51–61.\nKiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret,\nand Romaric Besanc ¸on. 2015. Generative event\nschema induction with entity disambiguation. In\nProceedings of the 53rd Annual Meeting of the Asso-\nciation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 188–\n197.\nSimon Ostermann, Michael Roth, and Manfred Pinkal.\n2019. Mcscript2. 0: A machine comprehension cor-\npus focused on script events and participants. InPro-\nceedings of the Eighth Joint Conference on Lexical\nand Computational Semantics (* SEM 2019), pages\n103–117.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1946–1958.\nHaoruo Peng, Qiang Ning, and Dan Roth. 2019.\nKnowsemlm: A knowledge infused semantic lan-\nguage model. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 550–562.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nKarl Pichotta and Raymond Mooney. 2014. Statisti-\ncal script learning with multi-argument events. In\nProceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 220–229.\nKarl Pichotta and Raymond J Mooney. 2016. Learn-\ning statistical scripts with lstm recurrent neural net-\nworks. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence.\nJay M Ponte and W Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In Pro-\nceedings of the 21st annual international ACM SI-\nGIR conference on Research and development in in-\nformation retrieval, pages 275–281.\nAmol Prakash, Mathieu Blanchette, Saurabh Sinha,\nand Martin Tompa. 2004. Motif discovery in hetero-\ngeneous sequence data. In Biocomputing 2004, Pro-\nceedings of the Paciﬁc Symposium, Hawaii, USA, 6-\n10 January 2004, pages 348–359. World Scientiﬁc.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding\nby generative pre-training.\nRyan A. Rossi, Nesreen K. Ahmed, Aldo G. Carranza,\nDavid Arbour, Anup Rao, Sungchul Kim, and Eu-\nnyee Koh. 2019. Heterogeneous network motifs.\nCoRR, abs/1901.10026.\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro,\nand Benjamin Van Durme. 2015. Script induction as\nlanguage modeling. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1681–1686.\nRoger C Schank and Robert P Abelson. 1977. Scripts,\nplans, goals and understanding: An inquiry into\nhuman knowledge structures. Mhwah, NJ (US):\nLawrence Erlbaum Associates.\nLei Sha, Sujian Li, Baobao Chang, and Zhifang Sui.\n2016. Joint learning templates and slots for event\nschema induction. In Proceedings of NAACL-HLT,\npages 428–434.\nQi Song, Yinghui Wu, Peng Lin, Luna Xin Dong, and\nHui Sun. 2018. Mining summaries for knowledge\ngraph search. IEEE Transactions on Knowledge and\nData Engineering, 30(10):1887–1900.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nLilian Wanzare, Alessandra Zarcone, Stefan Thater,\nand Manfred Pinkal. 2017. Inducing script struc-\nture from crowdsourced event descriptions via semi-\nsupervised clustering. In Proceedings of the 2nd\nWorkshop on Linking Models of Lexical, Sentential\nand Discourse-level Semantics, pages 1–11.\nYe Wu, Zhinong Zhong, Wei Xiong, and Ning Jing.\n2014. Graph summarization for attributed graphs.\nIn 2014 International Conference on Information\nScience, Electronics and Electrical Engineering ,\nvolume 1, pages 503–507. IEEE.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\n695\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nQuan Yuan, Xiang Ren, Wenqi He, Chao Zhang, Xinhe\nGeng, Lifu Huang, Heng Ji, Chin-Yew Lin, and Ji-\nawei Han. 2018. Open-schema event proﬁling for\nmassive news corpora. In Proceedings of the 27th\nACM International Conference on Information and\nKnowledge Management, pages 587–596.\nFangzhou Zhai, Vera Demberg, Pavel Shkadzko, Wei\nShi, and Asad Sayeed. 2019. A hybrid model for\nglobally coherent story generation. In Proceedings\nof the Second Workshop on Storytelling , pages 34–\n45.\nNing Zhang, Yuanyuan Tian, and Jignesh M Patel.\n2010. Discovery-driven graph summarization. In\n2010 IEEE 26th International Conference on Data\nEngineering (ICDE 2010), pages 880–891. IEEE.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6404997110366821
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.6335517168045044
    },
    {
      "name": "Natural language processing",
      "score": 0.5336651802062988
    },
    {
      "name": "Path (computing)",
      "score": 0.5082285404205322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4875161349773407
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3493029475212097
    },
    {
      "name": "Programming language",
      "score": 0.3175676465034485
    },
    {
      "name": "Information retrieval",
      "score": 0.22298821806907654
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I189158971",
      "name": "United States Naval Academy",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I166416128",
      "name": "DEVCOM Army Research Laboratory",
      "country": "US"
    }
  ]
}