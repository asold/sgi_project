{
  "title": "Video Background Music Generation with Controllable Music Transformer",
  "url": "https://openalex.org/W3207290297",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4365691874",
      "name": "Di, Shangzhe",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4306434332",
      "name": "Jiang, Zeren",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2185671682",
      "name": "Liu, Si",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2357364085",
      "name": "Wang, Zhaokai",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4303664883",
      "name": "Zhu, Leyan",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A3163766190",
      "name": "He Zexin",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2011903242",
      "name": "Liu Hongming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2479487170",
      "name": "Yan, Shuicheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2811167645",
    "https://openalex.org/W2772474126",
    "https://openalex.org/W3092879656",
    "https://openalex.org/W3092850823",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2898148140",
    "https://openalex.org/W2962981281",
    "https://openalex.org/W2910577860",
    "https://openalex.org/W3077305422",
    "https://openalex.org/W3045082460",
    "https://openalex.org/W2475687244",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3049174246",
    "https://openalex.org/W2403342846",
    "https://openalex.org/W2959020461",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2584032004",
    "https://openalex.org/W2524365899",
    "https://openalex.org/W2746068898",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2792210438",
    "https://openalex.org/W3047325651",
    "https://openalex.org/W2950547518",
    "https://openalex.org/W2902968583",
    "https://openalex.org/W3103837916",
    "https://openalex.org/W3046715528",
    "https://openalex.org/W3173531154",
    "https://openalex.org/W2963681776",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3111853169",
    "https://openalex.org/W2963032576",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3090740343"
  ],
  "abstract": "In this work, we address the task of video background music generation. Some\\nprevious works achieve effective music generation but are unable to generate\\nmelodious music tailored to a particular video, and none of them considers the\\nvideo-music rhythmic consistency. To generate the background music that matches\\nthe given video, we first establish the rhythmic relations between video and\\nbackground music. In particular, we connect timing, motion speed, and motion\\nsaliency from video with beat, simu-note density, and simu-note strength from\\nmusic, respectively. We then propose CMT, a Controllable Music Transformer that\\nenables local control of the aforementioned rhythmic features and global\\ncontrol of the music genre and instruments. Objective and subjective\\nevaluations show that the generated background music has achieved satisfactory\\ncompatibility with the input videos, and at the same time, impressive music\\nquality. Code and models are available at\\nhttps://github.com/wzk1015/video-bgm-generation.\\n",
  "full_text": "Video Background Music Generation\nwith Controllable Music Transformer\nShangzhe Di‚àó\nBeihang University\nBeijing, China\ndishangzhe@buaa.edu.cn\nZeren Jiang‚àó\nBeihang University\nBeijing, China\nzeren.jiang99@gmail.com\nSi Liu‚Ä†\nBeihang University\nBeijing, China\nliusi@buaa.edu.cn\nZhaokai Wang\nBeihang University\nBeijing, China\nwzk1015@buaa.edu.cn\nLeyan Zhu\nBeihang University\nBeijing, China\nleyan.zhu@buaa.edu.cn\nZexin He\nBeihang University\nBeijing, China\njacquesdeh@buaa.edu.cn\nHongming Liu\nCharterhouse School\nGodalming, Surrey, UK\nlclcpg2018@gmail.com\nShuicheng Yan\nSea AI Lab\nSingapore\nYansc@sea.com\nABSTRACT\nIn this work, we address the task of video background music gen-\neration. Some previous works achieve effective music generation\nbut are unable to generate melodious music tailored to a particu-\nlar video, and none of them considers the video-music rhythmic\nconsistency. To generate the background music that matches the\ngiven video, we first establish the rhythmic relations between video\nand background music. In particular, we connect timing, motion\nspeed, and motion saliency from video with beat, simu-note density,\nand simu-note strength from music, respectively. We then propose\nCMT, a Controllable Music Transformer that enables local con-\ntrol of the aforementioned rhythmic features and global control of\nthe music genre and instruments. Objective and subjective eval-\nuations show that the generated background music has achieved\nsatisfactory compatibility with the input videos, and at the same\ntime, impressive music quality. Code and models are available at\nhttps://github.com/wzk1015/video-bgm-generation.\nCCS CONCEPTS\n‚Ä¢ Applied computing ‚ÜíSound and music computing .\nKEYWORDS\nVideo background music generation, Transformer, Music represen-\ntation\n‚àóBoth authors contributed equally to this research.\n‚Ä†Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM ‚Äô21, October 20‚Äì24, 2021, Virtual Event, China\n¬© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475195\nACM Reference Format:\nShangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He,\nHongming Liu, and Shuicheng Yan. 2021. Video Background Music Gen-\neration with Controllable Music Transformer. In Proceedings of the 29th\nACM International Conference on Multimedia (MM ‚Äô21), October 20‚Äì24, 2021,\nVirtual Event, China. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3474085.3475195\nInput Video\nGenerated Background Music\nMotion Speed\nMotion Saliency\nTiming\nRhythmic Features\nInstruments\nGenre\nUser-defined Features\nControllable \nMusic \nTransformer\nUser\nFigure 1: An overview of our proposed framework. We estab-\nlish three rhythmic relations between video and music. To-\ngether with the user-defined genre and instruments, the ex-\ntracted rhythmic features are then passed to a carefully de-\nsigned Controllable Music Transformer to generate proper\nbackground music for a given video.\n1 INTRODUCTION\nNowadays, people can conveniently edit short videos with video\nediting tools and share their productions with others on social\nvideo-sharing platforms. To make a video more attractive, adding\nbackground music is a common practice, which, however, is not so\neasy for those without much knowledge of music or film editing. In\nmany cases, finding proper music material and make adjustments\nto make the music fit for a video is already very difficult and time-\nconsuming. Not to mention the copyright protection issue that is\narXiv:2111.08380v1  [cs.MM]  16 Nov 2021\ncausing increasingly broader public concern. Therefore, automati-\ncally generating appropriate background music for a given video\nbecomes a task of real-world significance, yet it is barely studied\nin the multimedia community to the best of our knowledge. There\nare some previous works [27] [4] [10] tackling music generation\nbased on deep learning models, However, none of them takes into\naccount the music-video rhythmic consistency.\nIn this paper, we address the task of video background music\ngeneration. Instead of adopting a costly data-driven practice as in\ntradition, i.e. collecting paired video and music samples for train-\ning models, we explore the rhythmic relations between video and\nbackground music and propose a transformer-based method free\nof reliance upon annotated training data. In Fig. 1, we provide an\nillustration of this task and an overview of our method.\nA video may contain diverse visual motions of different patterns.\nFor example, a man in a video is walking, fast or slowly, and he\nmay suddenly stop. To generate proper background music for such\na scene, we should consider the speed and change of the motion.\nIn particular, we establish three rhythmic relations between the\nvideo and background music. Firstly, for a video clip, fast motion\n(e.g., in a sport ) should correspond to intense music, and vice\nversa. Accordingly, we build a positive correlation betweenmotion\nspeed and simu-note density , where motion speed refers to the\nmagnitude of motion in a small video clip calculated by the average\noptical flow, and simu-note density is the number of simu-notes per\nbar. A simu-note [25] is a group of notes that start simultaneously,\nas shown in Fig. 3. Secondly, distinctive motion changes, such\nas shot boundaries, should correspond to strong musical beats\nor music boundaries, making the audience feel both visual and\nauditory impact at the same time, leading to a more rhythmic video.\nTherefore, we align the local-maximum motion saliency with\nsimu-note strength, where local-maximum motion saliency labels\nsome rhythmic keyframes and simu-note strength is the number of\nnotes in a simu-note. Thirdly, it is more harmonious to sync the\nepilogue and the prologue between the video and the generated\nbackground music. That is to say; the background music should\nappear and disappear smoothly along with the start and end of the\nvideo. Thus, we extract timing information from the given video,\nand take it as the position encoding to guide the music generation\nprocess, namely beat-timing encoding.\nWe build our music representation based on compound words [9]\n(CP). We group neighboring tokens according to their types to\nconstruct 2-D tokens for note-related and rhythm-related tokens.\nThese rhythmic features are added as additional attributes in the\ntoken. Furthermore, we add music genres and used instruments\nas initial tokens, as shown in the bottom part of Fig. 1, in order to\ncustomize the music generation process to match the given video.\nWe use linear transformer [12] as the backbone of our proposed\npipeline to model the music generation process, considering its\nlightweight and linear-complexity in attention calculation. During\ntraining, we use the Lakh Pianoroll Dataset (LPD) [4] to train our\nmodel on music modeling, where we provide the above musical\nfeatures directly. For inference, the visual features are obtained\nfrom the video and used to guide the generation process.\nIn summary, our contributions are threefold. 1) For our task\nvideo background music generation , we propose the Controllable\nMusic Transformer (CMT) model, which makes use of several key\nrelations between video and music, but does not require paired\nvideo and music annotated data during training. 2) We introduce\nnew representations of music, including note density and strength\nof simu-notes, which result in better-generated music and a more\ncontrollable multi-track generation process. 3) Our approach suc-\ncessfully matches the music to the rhythm and mood of a video,\nand at the same time, achieves high musical quality. We put a demo\nvideo of an input video and our generated music in the supplemen-\ntary material for demonstrative purposes.\n2 RELATED WORK\nRepresentations of music. Most previous works on symbolic\nmusic generation take the music represented in MIDI-like event se-\nquences [10] [16] as input. REMI [11] imposes a metrical structure\nin the input data, i.e., providing explicit notations of bars, beats,\nchords, and tempo. This new representation helps to maintain the\nflexibility of local tempo changes and provides a basis upon which\nwe can control the rhythmic and harmonic structure of the music.\nCompound words [9] (CP) further converts REMI tokens to a se-\nquence of compound words by grouping neighboring tokens, which\ngreatly reduces the length of the token sequence. In this paper, we\nemploy a representation based on CP. We categorize music tokens\ninto rhythm-related tokens and note-related tokens. We add genre\nand instrument type as initial tokens to provide global information\nof the music, and density and strength attributes to enable local\ncontrol of the generation process.\nMusic generation models. Some recent models [25] [23] [19]\nuse autoencoders to learn a latent space for symbolic music and\ngenerate new pieces. Some [ 27] [4] consider piano rolls as 2-D\nimages and build models based on convolution networks. Since\nmusic and language are both represented as sequences, the trans-\nformer and its variants are also frequently used as the backbone of\nmusic generation models [10] [11] [3] [9]. Apart from generating\nsymbolic music, some models generate audio directly in waveform\n[15] [5] [14] or indirectly through transcription and synthesis [7].\nOur model is based on linear transformer [12], which implements\na linear attention mechanism to reduce time complexity.\nComposing music from silent videos. Previous works on mu-\nsic composition from silent videos focus on generating the music\nfrom video clips containing people playing various musical instru-\nments, such as the violin, piano, and guitar [6] [21] [22]. Much of\nthe generation result, e.g., the instrument type and even the rhythm,\ncan be directly inferred from the movement of human hands, so\nthe music is to some extent determined. Comparably, our method\nworks for general videos and aims to produce non-determined gen-\neration results. In addition, there is currently no dataset of paired\narbitrary videos and music specifically for this video background\nmusic generation task. In some existing audiovisual datasets like\n[1] [13], the videos often contain noise like human speech or simply\ndo not involve music. Due to the lack of annotated training data,\ntraditional supervised training methods based on audiovisual data\ndo not function regarding this task. To the best of our knowledge,\nno existing work focuses on generating music from arbitrary video\nclips. This paper proposes to generate background music based on\nmotion saliency, visual beats, and global timing of the video, along\nwith user-defined music genres and instruments.\n25 26 27 28 29 30 31 32 33 34 35\nTime (s)\n60\n72\n84Pitch (MIDI)\nExample Video Frames Motion Speed Motion Saliency\nGenerated Music (Piano Track) Simu-note Density (Piano Track) Simu-note Strength (Piano Track)\nFigure 2: Rhythmic relations between a video clip and the background music generated using our method. Shown here are\nthe original video and its rhythmic features (top), as well as our generated music and its corresponding features (bottom). Our\nmethod constructs rhythmic relations between the video and music, and use them to guide the generation of music.\nSimu-note Note\nBar\nSimu-note Density = 3Simu-note Strength = 2\nMM\n31\n14\n/uniE0A4/uniE0A4/uniE0A3/uniE0A3\n/uniE0A4/uniE0A4/uniE0A4/uniE0A3/uniE0A3/uniE0A3/uniE0A4/uniE0A4/uniE0A4/uniE0A3\n/uniE0A4/uniE050\n/uniE050\n/uniE050/uniE084/uniE084\n/uniE4E3/uniE4E6\n/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3\n/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3\n/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3/uniE4E3\n/uniE4E3/uniE4E3/uniE4E6/uniE4E6\n/uniE240\n/uniE240/uniE240\nFigure 3: Illustration of simu-note density and strength.\nSimu-note density stands for the number of simu-notes in a\nbar, and simu-note strength stands for the number of notes\nin a simu-note.\n3 ESTABLISHING VIDEO-MUSIC RHYTHMIC\nRELATIONS\nOne would expect to hear romantic music when watching a roman-\ntic film or intense music for a shooting game video. Rhythm exists\nnot only in music but also in videos. It can reflect how visual mo-\ntions in a video or note onsets in music are distributed temporally.\nTo make the generated background music match the given video,\nwe analyze and establish the rhythmic relations between video and\nmusic.\nBelow in Sec. 3.1, we first build a connection between time in\nthe video and musical beat. Based on this connection, in Sec. 3.2,\nwe establish the relation between motion speed and note density.\nIn Sec. 3.3, we build the relation between motion saliency and note\nstrength. These established relations between video and music will\nbe used to guide the generation of background music that matches\nrhythmically with the given video.\nIn the left part of Fig. 2, we show a video clip and the generated\nbackground music. The generated music has a large simu-note\ndensity when motion speed is high (as shown in the middle of\nFig. 2), and a large simu-note strength when a salient visual beat\noccurs (as shown in the right part of Fig. 2).\n3.1 Video Timing and Music Beat\nFormally, we consider a video ùëâ ‚ààRùêª√óùëä√óùëá√ó3 that contains ùëá\nframes. We aim to convert the ùë°th (0 < ùë° ‚©Ω ùëá) frame to its beat\nnumber with the following equation:\nùëìùëèùëíùëéùë°(ùë°)= ùëáùëíùëöùëùùëú ¬∑ùë°\nùêπùëÉùëÜ ¬∑60 , (1)\nwhereùëáùëíùëöùëùùëú is the speed at which the background music should be\nplayed, and ùêπùëÉùëÜ is short for frame per second, which is an intrinsic\nattribute of the video. We take 1\n4 beat (one tick) as the shortest unit.\nAlso, we can convert the ùëñth beat to the video frame number\nbased on its inverse function:\nùëìùëìùëüùëéùëöùëí (ùëñ)= ùëì‚àí1\nùëèùëíùëéùë°(ùëñ)= ùëñ¬∑ùêπùëÉùëÜ ¬∑60\nùëáùëíùëöùëùùëú . (2)\nThese two equations serve as the basic block to build the rhythmic\nrelation between the video and music.\n3.2 Motion Speed and Simu-note Density\nWe first divide the entire video into ùëÄ clips, with ùëÄ defined as\nfollows:\nùëÄ =\n\u0018ùëìùëèùëíùëéùë°(ùëá)\nùëÜ\n\u0019\n, (3)\nwhereùëá is the total number of frames in the video, and we setùëÜ = 4,\nwhich means the length of each clip corresponds to 4 beats (one\nbar) in the music. Then we calculate the motion speed based on the\noptical flow in each clip.\nOptical flow is a useful tool for analyzing video motions. For-\nmally, an optical flow field ùëìùë°(ùë•,ùë¶)‚àà Rùêª√óùëä√ó2 measures the dis-\nplacement of individual pixels between two consecutive frames\nùêºùë°,ùêºùë°+1 ‚ààRùêª√óùëä√ó3.\nIn analogy to distance and speed, we define optical flow mag-\nnitude ùêπùë° as the average of the absolute optical flow to measure\nthe motion magnitude in the ùë°th frame:\nùêπùë° =\n√ç\nùë•,ùë¶ |ùëìùë°(ùë•,ùë¶)|\nùêªùëä , (4)\nand motion speedof theùëöth (0 < ùëö ‚©Ω ùëÄ) video clip as the average\noptical flow magnitude:\nùë†ùëùùëíùëíùëëùëö =\n√çùëìùëì ùëüùëéùëöùëí(ùëÜùëö)\nùë°=ùëìùëì ùëüùëéùëöùëí(ùëÜ(ùëö‚àí1))+1 ùêπùë°\nùëìùëìùëüùëéùëöùëí (ùëÜ) . (5)\nAs for music, we manipulate simu-note density to connect with\nthe motion speed. Here, asimu-note [25] is a group of notes having\nthe same onset:\nsimu-noteùëñ,ùëó,ùëò = {ùëõ1,ùëõ2,...,ùëõ ùëÅ}, (6)\nwhere ùëñ denotes the ùëñth bar, ùëó denotes the ùëóth tick (4 ticks is 1 beat)\nin this bar, ùëò denotes the instrument, and ùëõdenotes a single note.\nCompared with notes, the concept of simu-note focuses more on\nthe rhythmic feature since no matter it is a seventh chord or a ninth\nchord, it is one simu-note.\nMoreover, abar can be expressed as a group of non-empty simu-\nnotes:\nùëèùëéùëüùëñ,ùëò = {simu-noteùëñ,ùëó,ùëò |simu-noteùëñ,ùëó,ùëò ‚â† ‚àÖ,ùëó = 1,2,..., 16}, (7)\nwhere ùëó = 1,2,..., 16 as we divide a bar into 16 ticks. Thesimu-note\ndensity of a bar is then defined as:\nùëëùëíùëõùë†ùëñùë°ùë¶ùëñ =\n\f\f\b\nùëó|‚àÉùëò ‚ààK,simu-noteùëñ,ùëó,ùëò ‚ààùëèùëéùëüùëñ,ùëò\n\t\f\f. (8)\nThen, we statistically analyze the distribution of both ùë†ùëùùëíùëíùëëùëö\nand ùëëùëíùëõùë†ùëñùë°ùë¶ùëñ in a batch of videos and the music in the Lakh Pianoroll\nDataset. We separate the value range ofùë†ùëùùëíùëíùëëùëö to 16 classes, the\nsame as the number of classes of ùëëùëíùëõùë†ùëñùë°ùë¶, based on the correspond-\ning percentage of density levels. For example, when there are 5%\nbars with a ùëëùëíùëõùë†ùëñùë°ùë¶ = 16 in the training set, we classify the top\n5% ùë†ùëùùëíùëíùëë as ùëëùëíùëõùë†ùëñùë°ùë¶ = 16. Since the ùëöth video clip has the same\nlength as the ùëñth bar, we replace theùëëùëíùëõùë†ùëñùë°ùë¶ùëñ with classified ùë†ùëùùëíùëíùëëùëö\nin inference stage to build the relation, as discussed in Sec. 4.2.\n3.3 Motion Saliency and Simu-note Strength\nThe motion saliency at the ùë°th frame is calculated as the aver-\nage positive change of optical flow of all directions between two\nconsecutive frames.\nWe then obtain a series of visual beats[2] by selecting frames\nwith both local-maximum motion saliency and a near-constant\ntempo. Each visual beat is a binary tuple (ùë°,ùë†)where ùë° is its frame\nindex and ùë† is its saliency. As shown in Fig. 2, ùë† will have a large\nvalue when a sudden visible change occurs.\nAs shown in Fig. 3, we define the simu-note strengthas the\nnumber of notes in it:\nùë†ùë°ùëüùëíùëõùëîùë°‚Ñéùëñ,ùëó,ùëò = |simu-noteùëñ,ùëó,ùëò |. (9)\nIntuitively, simu-note strength denotes the richness of an ex-\ntended chord or harmony, giving the audience a rhythmic feeling\nalong with its progression. The higher simu-note strength it has,\nthe more auditory impact the audience will feel. We establish a\npositive correlation between visual beat saliency and simu-note\nstrength so that a distinct visual motion will be expressed by a clear\nmusic beat, making the video more rhythmic.\nVideo FramesMusic MIDI\nUser-defined \nInitial Tokens Simu-note\nCompound Words\nR\nBAR\nDEN\nR\nBEAT\nDEN\nSTR\nN\nINST\nPITCH\nDUR\nSimu-note Density\nSimu-note Strength\nMotion Speed\nMotion Saliency\nTraining\nGenre Piano Bar Beat Note Note Bar\nInference\nTransformer \nDecoder\nBeat-Timing \nEncoding ‚®Å\nCompound Word \nEmbedding\nTimingAbsolute Beat\nGenerated Music\nRhythmic Relations\nFigure 4: Illustration of the proposed CMT framework. We\nextract rhythmic features from MIDI music (in training) or\nthe video (in inference), and construct compound words as\nthe representation of music tokens. The compound word\nembedding is then combined with beat-timing encoding,\nand fed into the transformer for sequence modeling.\n4 CONTROLLABLE MUSIC TRANSFORMER\nOn top of the above established video-music rhythmic relations, we\npropose a transformer-based approach to generate background mu-\nsic for given videos, termed Controllable Music Transformer (CMT).\nThe overall framework is shown in Fig. 4. We extract rhythmic\nfeatures from both video and MIDI, which is indicated in the above\nsection. In the training stage, only rhythmic features from MIDI are\nincluded. In the inference stage, we replace the rhythmic feature\nwith that from the video to perform controllable music generation.\n4.1 Music Representation\nWe design a music token representation for controllable multi-\ntrack symbolic music generation. Inspired by PopMAG [ 18] and\nCWT [9], we group related attributes into a single token to shorten\nthe sequence length.\nAs shown in Fig. 4, we consider seven kinds of attributes: type,\nbeat/bar, density, strength, instrument, pitch and duration. We sep-\narate those attributes into two groups: a rhythm-related group\n(denoted as R in Fig. 4) including beat, density and strength, and a\nnote-related group (denoted as N in Fig. 4) includingpitch, duration\nand instrument type that the note belongs to. Then we use the type\nattribute (the R/N row of Fig. 4) to distinguish those two groups.\nTo make computational modeling feasible, we set note-related at-\ntributes to ùëÅùëúùëõùëí in the rhythm-related token and vice versa, as\nshown in Fig. 4 with the blank attribute. Each rhythm token has a\nstrength attribute to indicate the number of following note tokens.\nBesides, the density attribute is monotonically decreasing in each\nbar, indicating the number of remaining simu-note in that bar. The\nembedding vector for each token is calculated as follows:\nùëùùëñ,ùëò = Embeddingùëò\n\u0000ùë§ùëñ,ùëò\n\u0001 ,ùëò = 1,...,ùêæ, (10)\nùë•ùëñ = ùëäùëñùëõ\n\u0002\nùëùùëñ,1 ‚äï... ‚äïùëùùëñ,ùêæ\n\u0003\n,. (11)\nHereùë§ùëñ,ùëò is the input words for attributeùëòat theùëñùë°‚Ñé time step before\nembedding, and ùêæ is the number of attributes in each token. Here\nùêæ = 7. ùëäùëñùëõ is a linear layer to project the concatenated embedded\ntoken to ùëÖùëë. Here ùëë = 512 is a pre-defined hyper-parameter for the\nembedded size. ùë•ùëñ ‚ààùëÖùëë is the embedded token.\nBesides, we take the genre and instrument type of each music\nas initial tokens and apply an independent embedding layer for it.\nThe embedded size is the same as ordinary tokens.\n4.2 Control\nAfter finishing training, it is expected that the CMT model has\nalready understood the meaning behind strength and density at-\ntributes. Thus, we only need to replace those two attributes when\nappropriate in the inference stage to make the music more harmo-\nnious with the given video.\nDensity replacement. To make the density of the generated\nmusic match the density of motion speed of the video, we replace\nthe density attribute on each bar token with the corresponding\nvideo density extracted from the optical flow. Since the CMT model\nhas already learned the meaning of density in the bar token, it will\nautomatically generate the corresponding number of beat tokens\nin this bar, making the density controllable.\nStrength replacement. Likewise, we take advantage of the in-\nformation from the visual beats of the video to control the strength\nof the generated simu-notes. If the CMT model predicts a beat token\nafter or at the given visual beat, we replace that beat token with\nthe given visual beat and its strength. Then, the CMT model will\nautomatically predict the corresponding number of note tokens in\nthis beat, making strength controllable.\nHyper-parameter ùê∂for control degree. We also need to con-\nsider the trade-off between the compatibility of music with the\nvideo and its melodiousness. This is because the more constraint\nwe put in inference, the more unnatural music we will get. To deal\nwith this problem, we design a hyper-parameter ùê∂ to indicate the\ncontrol degree on the generated music. The larger theùê∂is, the more\nconstraint will be added in inference. This means we will get a piece\nof music from scratch when ùê∂ equals 0 and get full compatibility\nwhen ùê∂ equals 1. Users can set ùê∂ according to their own needs.\nBeat-timing encoding. In order to leverage the time or length\ninformation from the video, we add a beat-timing encoding on the\ntoken embedding both in training and inference. This design tells\nthe CMT model when to start a prologue and when to predict an\nEOS token. Beat-timing encoding indicates the ratio of the current\nbeat number to the total beat number in the given video. Specifically,\nwe divide the value range of that ratio into ùëÄ bins (classes) with\nthe same width and use a learnable embedding layer to project it\nto the same dimension as the token embedding. Then we add them\ntogether to form the input for our CMT model.\nAssuming ùëñ is the index of the token, ùëèùëíùëéùë°ùëñ is the beat number\ngenerated at the current step and ùëÅùëèùëíùëéùë° = ùëìùëèùëíùëéùë°(ùëá)is the total\nbeat number of the video, and ùëá is the total frame number of the\nvideo, we compute beat-timing encoding according to the equations\nbelow:\nùë°ùëñ = Embeddingùë°\n\u0012\nùëüùëúùë¢ùëõùëë\n\u0012\nùëÄùëèùëíùëéùë°ùëñ\nùëÅùëèùëíùëéùë°\n\u0013\u0013\n, (12)\n¬Æùë•ùëñ = ùë•ùëñ +ùêµùëÉùê∏ +ùë°ùëñ, (13)\nwhere ùë°ùëñ ‚ààùëÖùëë is beat-timing encoding for the token ùëñ, ùë•ùëñ is the\nembedding vector explained in Equation (11), and ùêµùëÉùê∏ is beat-\nbased position encoding explained in Equation (14) and (15). We set\nùëÄ = 100 to separate each video into 100 bins. ¬Æùë•ùëñ is the final input\nfed into the CMT model.\nMoreover, instead of using the traditional position encoding, we\nintroduce a beat-based position encoding for each token. In partic-\nular, each token within the same beat will get the same position\nencoding. It is in line with the semantic information of the music\nsequence since multiple notes in the same beat will be converted\nto the same audio segment eventually, regardless of their order in\nthe sequence.\nBeat-based position encoding of the ùëñ-th beat ùêµùëÉùê∏ is computed\nas follows:\nùêµùëÉùê∏(ùëèùëíùëéùë°ùëñ,2ùëõ)= ùë†ùëñùëõ( ùëèùëíùëéùë°ùëñ\n100002ùëõ/ùëëùëöùëúùëëùëíùëô\n) (14)\nùêµùëÉùê∏(ùëèùëíùëéùë°ùëñ,2ùëõ+1)= ùëêùëúùë†( ùëèùëíùëéùë°ùëñ\n100002ùëõ/ùëëùëöùëúùëëùëíùëô\n) (15)\nwhereùëëùëöùëúùëëùëíùëô = 512 is the model hidden size, andùëõ ‚àà[0,...,ùëë ùëöùëúùëëùëíùëô/2)\nis the index of ùëëùëöùëúùëëùëíùëô. Beat-based position encoding will be added\non each embedding vector ùë•ùëñ in Equation (13) eventually.\nGenre and instrument type. In our method, there are 6 genres\n(Country, Dance, Electronic, Metal, Pop, and Rock) and 5 instru-\nments (Drums, Piano, Guitar, Bass, and Strings). We take each from\nthem respectively as the initial token for our CMT model. Users can\nchoose different genres and instruments by using different initial\ntokens in the inference stage to generate the background music\nthat matches up with the emotion of the video.\nThe above-mentioned controlling strategies will be combined to-\ngether in the inference stage. The more detailed inference algorithm\nis illustrated in Algorithm 1.\n4.3 Sequence Modeling\nThe sequence of music tokens (as explained in Sec. 4.1) is fed into\nthe transformer [24] model to model the dependency among el-\nements. We employ the linear transformer [12] as our backbone\nModel Data Without Control With Control\nNo. - 1 2 3 4 5 6 7 8\nDensity - - ‚ó¶ - - ‚ó¶ ‚Ä¢ ‚ó¶ ‚Ä¢\nStrength - - - ‚ó¶ - ‚ó¶ ‚ó¶ ‚Ä¢ ‚Ä¢\nBeat-timing encoding - - - - ‚àö ‚àö ‚àö ‚àö ‚àö\nPitch Histogram Entropy 4.452 3.634 2.998 3.667 3.573 3.617 3.496 4.044 4.113\nGrooving Pattern Similarity 0.968 0.677 0.714 0.647 0.778 0.810 0.773 0.678 0.599\nStructureness Indicator 0.488 0.219 0.227 0.215 0.223 0.241 0.268 0.211 0.200\nOverall Rank ‚Üì - 5.000 5.000 5.333 4.000 2.667 3.667 4.667 5.667\nTable 1: Objective evaluation for each controlling attribute on melodiousness. - means that the attribute is not added during\ntraining. ‚ó¶denotes that the attribute is only added in training but not controlled in the inference time. ‚Ä¢means that we not\nonly add it during training but also control that attribute with the corresponding rhythmic feature from a given video in\ninference. ‚àödenotes that we add beat-timing encoding in both the training and inference stage. See Sec. 5.3 for details.\nAlgorithm 1 Inference stage\nSet initial genre and instrument tokens\nrepeat\nPredict next token with given beat-timing from the video based\non sampling strategy\nif next token is bar then\nReplace its density attribute with prob of C\nend if\nif next token is after visual beat then\nReplace next token with visual beat and its strength with\nprob of C\nend if\nAppend next token to music token list\nuntil EOS token predicted\nreturn music token list\narchitecture, considering its lightweight and linear-complexity in\nattention calculation.\nThe multi-head output module, following the design of [9], pre-\ndicts 7 attributes of each music token in a two-stage way. In the first\nstage, the model predicts the type token by applying a linear projec-\ntion on the output of the transformer. In the second stage, it uses\ntype to pass through 6 feed-forward heads to predict the remaining\n6 attributes at the same time. During inference, we adopt stochastic\ntemperature-controlled sampling [8] to increase the diversity of\nthe generated tokens.\n5 EXPERIMENTS\nWe perform an ablation study on three control attributes on music\ngeneration we propose in this work. Both objective and subjec-\ntive evaluations are conducted. Objective evaluations focus on the\nquality of the generated music itself, where we generate ten music\npieces for each genre using all the instruments in the initial tokens\nfor each video. Then we calculate the average on each objective\nmetric. For subjective evaluation, we designed a questionnaire and\ninvited users to evaluate the quality of the generated music as well\nas its compatibility with the corresponding video.\n5.1 Dataset\nWe adopt the Lakh Pianoroll Dataset (LPD) [4] to train our CMT\nmodel. LPD is a collection of 174,154 multi-track pianorolls derived\nfrom the Lakh MIDI Dataset (LMD) [17]. We use the lpd-5-cleansed\nversion of LPD, which goes through a cleaning process and has all\ntracks in a single MIDI file merged into five common categories\n(Drums, Piano, Guitar, Bass, and Strings). We then select 3,038 MIDI\nmusic pieces from lpd-5-cleansed as our training set. The selected\npieces fall into six genres (Country, Dance, Electronic, Metal, Pop,\nRock) from the tagtraum genre annotations [20].\n5.2 Implementation Details\nFollowing the design in [9], we choose the embedding size for each\nattribute based on its vocabulary size,i.e. (32,64,64,64,512,128,32)\nfor (type, beat, density, strength, pitch, duration, and instrument ) at-\ntributes respectively. Those embedded attributes are concatenated\ntogether and projected to model hidden size in Equation (11).\nAs for the model settings, we use 12 self-attention layers, each\nwith 8 attention heads. The model hidden size and inner layer size\nof the feed-forward part are set to 512 and 2,048, respectively. The\ndropout rate in each layer is set to 0.1. The input sequence length\nis padded to 10,000 with ‚ü®ùê∏ùëÇùëÜ‚ü©token.\nWe set the initial learning rate to 1e-4 and use Adam as the\noptimizer. We train our model for 100 epochs on the LPD dataset,\ntaking approximately 28 hours on 4 RTX 1080Ti GPUs. The objec-\ntive metrics are computed with MusDr [26].\n5.3 Objective Evaluation\nHere we analyze the contribution of each of the controllable at-\ntributes. We adopt some of the objective metrics from [26], including\nPitch Class Histogram Entropy that assesses the music‚Äôs qual-\nity in tonality, Grooving Pattern Similarity that measures the\nmusic‚Äôs rhythmicity, and Structureness Indicator that measures\nthe music‚Äôs repetitive structure. Note that the overall quality is not\nindicated by how high or low these metrics are but instead by their\ncloseness to the real data. Finally, we sort each metric among the\neight models in Tab. 1 and take the mean of the ranking results\namong the three metrics mentioned above as the final criterion,\nnamely Overall rank.\nFor each model with control in Tab. 1, we generate 10 MIDI for\neach genre using all five instruments for each video. To make a\nfair comparison, we generate the same number of MIDI with the\nsame length of the controlled counterparts for each model without\ncontrol.\nIn Tab. 1, we first statistically analyze the objective metrics on\nthe LPD dataset. The generated music should be close to music in\nthe dataset to be more natural regarding the adopted metrics. Then\nwe train a model without our proposed three rhythmic features,\ni.e., experiment No.1 in Tab. 1, serving as a baseline model for\ncomparison. Then we add each rhythmic feature one by one, i.e.,\nexperiment No. 2, 3, 4 in Tab. 1. Density and beat-timing encoding\nhelp improve the overall structures of the music. Strength makes\nit easy for the model to learn the combination of different pitch\nclasses to form a simu-note, leading to a better pitch histogram\nentropy. When we combine those three proposed rhythmic features,\nas shown in experiment No.5, we take all the advantages and get a\nhigher score on each metric than the baseline model, indicating an\nimprovement in overall melodiousness. However, when we try to\ncontrol those attributes with the given video, i.e., experiment No. 6,\n7, 8 as shown in Tab. 1, we observe a degeneration on structures of\nthe music. It is reasonable since we force the generated music to\nalign with the rhythm from the video. Thus, in Sec. 5.4, we conduct\na user study on hyper-parameter ùê∂, in order to find the trade-off of\nthe structures degeneration and the music-video compatibility.\nTo sum up, the highest overall rank for experiment No. 5 demon-\nstrates that the rhythmic features we extract from the music not\nonly make the controlling workable in inference but also improve\nthe overall melodiousness of the generated music because the ex-\ntracted density and strength prompt the CMT model to learn the\nunderlying pattern of the rhythm of the background music. In\nother words, the rhythmic feature makes it easy for the network to\nconverge and thus improves the structures of the music.\n5.4 Subjective Evaluation\nThe best way to evaluate a music generation model today remains\nusing user study. Besides, objective metrics do not consider the\nextent to which the video and music are matched. Therefore, we\ndesign a questionnaire for subjective evaluation of our model and\ninvite 36 people to participate. 13 among them have related expe-\nrience in music or basic understanding of music performing and\nare considered professionals. Each participant is asked to listen to\nseveral pieces of music (in random order) corresponding to one\ninput video, rate based on subjective metrics introduced below, and\nrank them based on their preferences. As the music can be long,\nthe questionnaire may take around 10 minutes to complete.\nWe select several subjective metrics [9] to evaluate the melodi-\nousness of our music: 1) Richness: music diversity and interesting-\nness; 2) Correctness: perceived absence of notes or other playing\nmistakes. (i.e., weird chords, sudden silence, or awkward usage\nof instruments); 3) Structuredness: whether there are structural\npatterns such as repeating themes or development of musical ideas.\nMoreover, in terms of the compatibility of the music with the\ngiven video, we choose the following metrics for evaluation: 1)\nRhythmicity: how much the rhythm of the generated music matches\nwith the motion of the video. For example, an intense sports vlog\nModel Baseline Matched Ours\nMelodiousness ‚Üë 3.4 4.0 3.8\nCompatibility ‚Üë 3.4 3.7 3.9\nOverall Rank ‚Üì 2.3 1.9 1.8\nTable 2: Subjective evaluation on melodiousness and com-\npatibility with the video with ùê∂ = 0.7. Our music reaches\ncomparable performance compared with matched data from\ntraining set, especially in compatibility with the video.\nwith large movements should be matched up with fast-paced mu-\nsic. A clam and smooth travel vlog with gentle movements should\nbe matched up with slow-paced music. 2) Correspondence: how\nmuch the major stress or the boundary of the generated music\nmatches with the video boundary or the visual beat. For instance,\nrhythmic motion, such as dancing, and some obvious video bound-\naries should be accompanied by major stress to improve musicality.\n3) Structure Consistency: the start and the end of the generated\nmusic should match up with those of the video. Similarly, music and\nvideo should both have a prologue, epilogue, and episode, so those\nstructures should be matched to make the video more harmonious.\nTo take all those subjective metrics into account and give a\ncomprehensive evaluation of the generated background music, we\nask participants to rank those videos based on the overall quality,\nand then we take the mean of the rank as the final result, namely\nOverall rank.\nWe first experiment on different levels of hyper-parameterùê∂ to\nchoose an appropriate value for the trade-off issue between com-\npatibility with the video and melodiousness of the generated music.\nWe choose a given video, and for eachùê∂value, we run the inference\nstage and generate one music clip. All music clips are included in\nthe questionnaire to be evaluated by the participants. The result\nis shown in Tab. 3. Although when ùê∂ = 1.0, we get better com-\npatibility between the video and the music, it is detrimental to the\nmelodiousness of the music, especially on the correctness metrics,\nleading to a lower overall rank. It is reasonable since the constraint\non the rhythm will force the model to generate some relatively\nunnatural notes. Considering the overall rank, we eventually take\nùê∂ = 0.7 as our pre-defined hyper-parameter.\nThen, we evaluate our music in terms of melodiousness and\ncompatibility with the given video. The baseline model is the one\nwithout using any controllable attributes. Moreover, we design an\nalgorithm to match a video with music from the training set based\non our proposed rhythmic relations. Specifically, given a video and\na music piece, we calculate their matching score (ùëÄùëÜ) as:\nùëÄùëÜ(dm,dv,sm,sv)= 1\nùëÄùëÜùê∏(dm,dv)+ùëÄùëÜùê∏(sm ‚äô1(sv),sv).\n(16)\nHere, dm and dv are simu-note density extracted from the music\nand the video and are truncated to the same size. Likewise, sm and\nsv are extracted simu-note strength with truncation. ùëÄùëÜùê∏ is the\nmean squared error, ‚äôdenotes the Hadamard product, 1(¬∑)maps\neach positive element to 1 and non-positive element to 0. We then\nmanually select one from the top-5 matched music based on the\nvideo style.\n(a) barbeat loss\n (b) duration loss\n (c) pitch loss\n (d) instrument loss\nFigure 5: The loss curve for the baseline model and the CMT model. The blue line is the baseline model, and the red line is our\nproposed CMT model. Our method shows an increased converging speed with the help of density, strength, and beat-timing\nencoding. For a better demonstration, we perform exponential smoothing on each loss with ùõº = 0.9.\nMetrics ùê∂\n0.0 0.3 0.7 1.0\nMelodiousness\nRichness ‚Üë 3.6 3.4 3.8 3.7\nCorrectness ‚Üë 3.2 3.7 3.7 2.8\nStructuredness ‚Üë 3.6 3.6 3.6 3.3\nCompatibility\nRhythmicity ‚Üë 3.2 3.5 3.7 3.7\nCorrespondence ‚Üë 2.6 3.3 3.7 4.1\nStructure Consistency ‚Üë 2.9 3.9 3.8 3.8\nOverall rank ‚Üì 3.1 2.2 2.1 2.6\nTable 3: Subjective ablation study for different ùê∂ values on\nmelodiousness and compatibility with the video. We observe\nthat higher ùê∂ leads to higher compatibility, while lower\nùê∂ leads to better melodiousness. The overall performance\nreaches its peak when ùê∂ is set to 0.7.\nAttribute Density Strength Time\nControl Error 0.107 0.001 0.028\nTable 4: The error rate for density, strength, and time con-\ntrol. Our method demonstrates impressive performance in\ncontrolling the three rhythmic features of the music.\nWe choose three video clips from different categories (edited,\nunedited, and animation video) and provide the generated music of\nours, baseline and matched in the questionnaire. They are randomly\nshuffled, so the participants do not know which one is generated\nby the model and which one is selected from the dataset.\nTab. 2 demonstrates that our generated background music even\nsurpasses the matched music overall. The matched music shows\nbetter compatibility than our baseline, indicating that our proposed\nrhythmic relation is valuable and beneficial for the overall musi-\ncality of the video. Although the melodiousness of our composed\nmusic is still below the real one in the training set, the excellent com-\npatibility compensates for those weaknesses, making the generated\nbackground music more suitable than human-made music.\n5.5 Controlling Accuracy\nWe evaluate the accuracy of the proposed three controllable at-\ntributes. We recalculate those three attributes in the music and take\nthe L2 distance between the rhythmic feature from the video and\nour music as the control error. Then, errors of density, strength and\ntime are normalized by the average number of simu-notes per bar,\naverage number of notes per simu-note and the total video time,\nrespectively, to eventually form the error rates. As shown in Tab.\n4, our results are impressive. The control error for music density\nis around 0.1 while the average number of simu-notes per bar is\n9.9, which means in each bar the beat number will approximately\nfluctuate only one beat with the given video density. The strength\ncontrol error shows that the majority of the simu-notes will have\nthe exact same number of notes as the given video strength.\n5.6 Visualization\nWe visualize the loss curves for the rhythm-related attributes and\nthe note-related attributes on both the baseline model and our CMT\nmodel. The results are shown in Fig. 5. It is obvious that our CMT\nmodel has a faster converging process on each attribute, especially\non the beat attribute. That is to say, our extracted rhythmic feature\nmake it easy for the model to grasp the crucial knowledge in music,\nresulting in a more fetching generated music.\n6 CONCLUSION\nIn this paper, we address the unexplored task ‚Äì video background\nmusic generation . We first establish three rhythmic relations be-\ntween video and background music. We then propose a Controllable\nMusic Transformer (CMT) to achieve local and global control of the\nmusic generation process. Our proposed method does not require\npaired video and music data for training while generates melodious\nand compatible music with the given video. Future studies may\ninclude exploring more abstract connections between visual and\nmusic (e.g., emotion and style), utilizing music in the waveform, and\nadopting unsupervised audiovisual representation learning from\npaired data.\nACKNOWLEDGMENTS\nThis research is supported in part by National Natural Science\nFoundation of China (Grant 61876177), Beijing Natural Science\nFoundation (4202034), Fundamental Research Funds for the Central\nUniversities.\nREFERENCES\n[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George\nToderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. 2016.\nYouTube-8M: A Large-Scale Video Classification Benchmark. arXiv preprint\narXiv:1609.08675 (2016).\n[2] Abe Davis and Maneesh Agrawala. 2018. Visual rhythm and beat. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition Workshops .\n2532‚Äì2535.\n[3] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garrison W Cottrell, and\nJulian McAuley. 2019. LakhNES: Improving multi-instrumental music generation\nwith cross-domain pre-training. arXiv preprint arXiv:1907.04868 (2019).\n[4] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. 2018. Musegan:\nMulti-track sequential generative adversarial networks for symbolic music gen-\neration and accompaniment. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 32.\n[5] Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Don-\nahue, and Adam Roberts. 2018. GANSynth: Adversarial Neural Audio Synthesis.\nIn International Conference on Learning Representations .\n[6] Chuang Gan, Deng Huang, Peihao Chen, and Joshua B Tenenbaum. [n.d.]. Foley\nmusic: Learning to generate music from videos. ([n. d.]).\n[7] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna\nHuang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck. 2018. En-\nabling Factorized Piano Music Modeling and Generation with the MAESTRO\nDataset. In International Conference on Learning Representations .\n[8] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious\ncase of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019).\n[9] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. 2021. Compound\nWord Transformer: Learning to Compose Full-Song Music over Dynamic Directed\nHypergraphs. arXiv preprint arXiv:2101.02402 (2021).\n[10] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis\nHawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Din-\nculescu, and Douglas Eck. 2018. Music Transformer: Generating Music with\nLong-Term Structure. InInternational Conference on Learning Representations .\n[11] Yu-Siang Huang and Yi-Hsuan Yang. 2020. Pop Music Transformer: Beat-based\nmodeling and generation of expressive Pop piano compositions. In Proceedings\nof the 28th ACM International Conference on Multimedia . 1180‚Äì1188.\n[12] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. 2020. Transformers are\nRNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings\nof the International Conference on Machine Learning (ICML) .\n[13] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra\nVijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 2017.\nThe kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017).\n[14] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain,\nJose Sotelo, Aaron Courville, and Yoshua Bengio. 2016. SampleRNN: An uncondi-\ntional end-to-end neural audio generation model. arXiv preprint arXiv:1612.07837\n(2016).\n[15] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol\nVinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.\n2016. Wavenet: A generative model for raw audio.arXiv preprint arXiv:1609.03499\n(2016).\n[16] Christine Payne. 2019. MuseNet. OpenAI Blog 3 (2019).\n[17] Colin Raffel. 2016. Learning-based methods for comparing sequences, with appli-\ncations to audio-to-midi alignment and matching . Ph.D. Dissertation. Columbia\nUniversity.\n[18] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2020. Pop-\nmag: Pop music accompaniment generation. In Proceedings of the 28th ACM\nInternational Conference on Multimedia . 1198‚Äì1206.\n[19] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck.\n2018. A hierarchical latent vector model for learning long-term structure in\nmusic. In International Conference on Machine Learning . PMLR, 4364‚Äì4373.\n[20] Hendrik Schreiber. 2015. Improving Genre Annotations for the Million Song\nDataset.. In ISMIR. 241‚Äì247.\n[21] Kun Su, Xiulong Liu, and Eli Shlizerman. 2020. Audeo: Audio generation for a\nsilent performance video. arXiv preprint arXiv:2006.14348 (2020).\n[22] Kun Su, Xiulong Liu, and Eli Shlizerman. 2020. Multi-Instrumentalist Net:\nUnsupervised Generation of Music from Body Movements. arXiv preprint\narXiv:2012.03478 (2020).\n[23] Andrea Valenti, Antonio Carta, and Davide Bacciu. 2020. Learning Style-Aware\nSymbolic Music Representations by Adversarial Autoencoders. arXiv preprint\narXiv:2001.05494 (2020).\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems . 6000‚Äì6010.\n[25] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Junbo Zhao,\nand Gus Xia. 2020. Pianotree vae: Structured representation learning for poly-\nphonic music. arXiv preprint arXiv:2008.07118 (2020).\n[26] Shih-Lun Wu and Yi-Hsuan Yang. 2020. The Jazz Transformer on the Front\nLine: Exploring the Shortcomings of AI-composed Music through Quantitative\nMeasures. In ISMIR.\n[27] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. 2017. MidiNet: A convolutional\ngenerative adversarial network for symbolic-domain music generation. arXiv\npreprint arXiv:1703.10847 (2017).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6994238495826721
    },
    {
      "name": "Rhythm",
      "score": 0.6649035215377808
    },
    {
      "name": "Transformer",
      "score": 0.4900730848312378
    },
    {
      "name": "Speech recognition",
      "score": 0.4645620882511139
    },
    {
      "name": "Beat (acoustics)",
      "score": 0.4322768449783325
    },
    {
      "name": "Popular music",
      "score": 0.431282639503479
    },
    {
      "name": "Multimedia",
      "score": 0.39187583327293396
    },
    {
      "name": "Acoustics",
      "score": 0.13518878817558289
    },
    {
      "name": "Art",
      "score": 0.09856531023979187
    },
    {
      "name": "Engineering",
      "score": 0.09296262264251709
    },
    {
      "name": "Visual arts",
      "score": 0.0826827883720398
    },
    {
      "name": "Voltage",
      "score": 0.07586345076560974
    },
    {
      "name": "Electrical engineering",
      "score": 0.06843280792236328
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 72
}