{
  "title": "HIV- Bidirectional Encoder Representations From Transformers: A Set of Pretrained Transformers for Accelerating HIV Deep Learning Tasks",
  "url": "https://openalex.org/W4280582410",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5008724934",
      "name": "Will Dampier",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5052805859",
      "name": "Robert W. Link",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5114144671",
      "name": "Joshua P. Earl",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5003078781",
      "name": "Mackenzie Collins",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5061867642",
      "name": "Diehl R. De Souza",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5080783722",
      "name": "Kelvin Koser",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5022311253",
      "name": "Michael R. Nonnemacher",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine",
        "Sidney Kimmel Cancer Center",
        "Thomas Jefferson University"
      ]
    },
    {
      "id": "https://openalex.org/A5034657105",
      "name": "Brian Wigdahl",
      "affiliations": [
        "Drexel University",
        "Institute for Molecular Medicine",
        "Sidney Kimmel Cancer Center",
        "Thomas Jefferson University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2519276118",
    "https://openalex.org/W2416133343",
    "https://openalex.org/W1994845416",
    "https://openalex.org/W2096047537",
    "https://openalex.org/W2058618886",
    "https://openalex.org/W3037920250",
    "https://openalex.org/W2469196507",
    "https://openalex.org/W2341588587",
    "https://openalex.org/W2062669228",
    "https://openalex.org/W2008351676",
    "https://openalex.org/W6716504786",
    "https://openalex.org/W3202242963",
    "https://openalex.org/W1974449256",
    "https://openalex.org/W2154164013",
    "https://openalex.org/W2509030308",
    "https://openalex.org/W2897254825",
    "https://openalex.org/W2769664965",
    "https://openalex.org/W2004335990",
    "https://openalex.org/W2043724713",
    "https://openalex.org/W2101247921",
    "https://openalex.org/W2066480278",
    "https://openalex.org/W2960803335",
    "https://openalex.org/W4200423503",
    "https://openalex.org/W2753590007",
    "https://openalex.org/W2429897253",
    "https://openalex.org/W3165453365",
    "https://openalex.org/W3112013923",
    "https://openalex.org/W2520252470",
    "https://openalex.org/W2807227130",
    "https://openalex.org/W6720828752",
    "https://openalex.org/W2118764795",
    "https://openalex.org/W3044294049",
    "https://openalex.org/W2132926880",
    "https://openalex.org/W2144362290",
    "https://openalex.org/W2789843538",
    "https://openalex.org/W3212684988",
    "https://openalex.org/W1496257230",
    "https://openalex.org/W1541931954",
    "https://openalex.org/W2800857888",
    "https://openalex.org/W2790244616",
    "https://openalex.org/W2790103870",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4255738661",
    "https://openalex.org/W2114850508",
    "https://openalex.org/W4297914773",
    "https://openalex.org/W1900729316",
    "https://openalex.org/W2054688372",
    "https://openalex.org/W2136431383",
    "https://openalex.org/W3027658738",
    "https://openalex.org/W2171270480",
    "https://openalex.org/W2056326102",
    "https://openalex.org/W2562684256",
    "https://openalex.org/W2619671582",
    "https://openalex.org/W2347168355",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2477831026",
    "https://openalex.org/W2416360749"
  ],
  "abstract": "The human immunodeficiency virus type 1 (HIV-1) is a global health threat that is characterized by extensive genetic diversity both within and between patients, rapid mutation to evade immune controls and antiretroviral therapies, and latent cellular and tissue reservoirs that stymie cure efforts. Viral genomic sequencing has proven effective at surveilling these phenotypes. However, rapid, accurate, and explainable prediction techniques lag our sequencing ability. Modern natural language processing libraries, like the Hugging Face transformers library, have both advanced the technical field and brought much-needed standardization of prediction tasks. Herein, the application of this toolset to an array of classification tasks useful to HIV-1 biology was explored: protease inhibitor resistance, coreceptor utilization, and body-site identification. HIV-Bidirectional Encoder Representations from Transformers (BERT), a protein-based transformer model fine-tuned on HIV-1 genomic sequences, was able to achieve accuracies of 88%, 92%, and 89% on the respective tasks, making it competitive with leading models capable of only one of these tasks. This model was also evaluated using a data augmentation strategy when mutations of known function were introduced. The HIV-BERT model produced results that agreed in directionality 10- to 1000-fold better than traditional machine learning models, indicating an improved ability to generalize biological knowledge to unseen sequences. The HIV-BERT model, trained task-specific models, and the datasets used to construct them have been released to the Hugging Face repository to accelerate research in this field.",
  "full_text": "HIV- Bidirectional Encoder\nRepresentations From\nTransformers: A Set of Pretrained\nTransformers for Accelerating HIV\nDeep Learning Tasks\nWill Dampier1,2,3,4*, Robert W. Link1,2,3, Joshua P. Earl1,5, Mackenzie Collins1,2,3,\nDiehl R. De Souza1,2,3, Kelvin Koser1,2,3,6, Michael R. Nonnemacher1,2,3,4,7\nand Brian Wigdahl1,2,3,4,7*\n1 Department of Microbiology and Immunology, Drexel University College of Medicine, Philadelphia, PA, United States,\n2 Center for Neurovirology and Translational Neuroscience, Institute for Molecular Medicine and Infectious Disease, Drexel\nUniversity College of Medicine, Philadelphia, PA, United States,3 Center for Pathogenic Emergence and Bioinformatics, Institute\nfor Molecular Medicine and Infectious Disease, Drexel University College of Medicine, Philadelphia, PA, United States,4 Center\nfor Clinical and Translational Research, Institute for Molecular Medicine and Infectious Disease, Drexel University College of\nMedicine, Philadelphia, PA, United States,\n5 Center for Genomic Sciences, Institute for Molecular Medicine and Infectious\nDisease, Drexel University College of Medicine, Philadelphia, PA, United States,6 Department of Biochemistry and Molecular\nBiology, Drexel University College of Medicine, Philadelphia, PA, United States,7 Sidney Kimmel Cancer Center, Thomas\nJefferson University, Philadelphia, PA, United States\nThe human immunodeﬁciency virus type 1 (HIV-1) is a global health threat that is\ncharacterized by extensive genetic diversity both within and between patients, rapid\nmutation to evade immune controls and antiretroviral therapies, and latent cellular and\ntissue reservoirs that stymie cure efforts. Viral genomic sequencing has proven effective at\nsurveilling these phenotypes. However, rapid, accurate, and explainable prediction\ntechniques lag our sequencing ability. Modern natural language processing libraries, like\nthe Hugging Face transformers library, have both advanced the technical ﬁeld and\nbrought much-needed standardization of prediction tasks. Herein, the application of\nthis toolset to an array of classiﬁcation tasks useful to HIV-1 biology was explored:\nprotease inhibitor resistance, coreceptor utilization, and body-site identiﬁcation. HIV-\nBidirectional Encoder Representations from Transformers (BERT), a protein-based\ntransformer model ﬁne-tuned on HIV-1 genomic sequences, was able to achieve\naccuracies of 88%, 92%, and 89% on the respective tasks, making it competitive with\nleading models capable of only one of these tasks. This model was also evaluated using a\ndata augmentation strategy when mutations of known function were introduced. The HIV-\nBERT model produced results that agreed in directionality 10- to 1000-fold better than\ntraditional machine learning models, indicating an improved ability to generalize biological\nknowledge to unseen sequences. The HIV-BERT model, trained task-speciﬁc models,\nand the datasets used to construct them have been released to the Hugging Face\nrepository to accelerate research in thisﬁeld.\nKeywords: HIV-1, deep learning, transformers, genetic variation, natural language processing, coreceptor tropism,\ncompartmentalization, drug resistance\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806181\nEdited by:\nManuela Sironi,\nEugenio Medea (IRCCS), Italy\nReviewed by:\nSlim Fourati,\nEmory University, United States\nMoses Ekpenyong,\nUniversity of Uyo, Nigeria\n*Correspondence:\nWill Dampier\nwnd22@drexel.edu\nBrian Wigdahl\nbw45@drexel.edu\nSpecialty section:\nThis article was submitted to\nBioinformatic and Predictive Virology,\na section of the journal\nFrontiers in Virology\nReceived: 21 February 2022\nAccepted: 19 April 2022\nPublished: 18 May 2022\nCitation:\nDampier W,Link RW,Earl JP,\nCollins M,De Souza DR,Koser K,\nNonnemacher MR andWigdahl B\n(2022) HIV- Bidirectional Encoder\nRepresentations From Transformers:\nA Set of Pretrained Transformers for\nAccelerating HIV Deep Learning Tasks.\nFront. Virol. 2:880618.\ndoi: 10.3389/fviro.2022.880618\nORIGINAL RESEARCH\npublished: 18 May 2022\ndoi: 10.3389/fviro.2022.880618\n1 INTRODUCTION\nThe human immunodeﬁciency virus type 1 (HIV-1) is a global\nhealth threat that impacts millions of people worldwide. This\nlentivirus primarily infects the immune system by attaching to\nthe CD4 receptor, entering the cell, integrating into the genome,\nand then producing viral copies to infect new cells, a process\nreviewed in greater detail by Mailler et al. (1). Antiretroviral\ntherapy has proven to be an effective treatment for individuals;\nhowever, the error-prone replication process leads to rapid\ndiversiﬁcation of the virus and eventual resistance through the\naccumulation of mutations which impact the efﬁcacy of the\ntreatment (2, 3). This rapid evolution also allows the virus to\nevade host antibodies and invade new niches through the\nchanges to the viral envelope and accessory proteins (4–7).\nThese phenotypes have drastic impacts on patient morbidity\nand mortality (5, 8, 9). Given their selective advantage, strains\ncontaining drug resistant mutations (DRMs) with low ﬁtness\nrequirements tend to proliferate within infected individuals and\nhave been shown to drastically increase the potential for virologic\nfailure (10, 11). At present, there are ﬁve classes of HIV-1\nantiretroviral therapies: nucleoside reverse transcriptase inhibitors\n(NRTIs), non-nucleoside reverse transcriptase inhibitors\n(NNRTIs), protease inhibitors (PIs), integrase inhibitors (INIs),\nand entry inhibitors (12, 13). Indeed, DRMs have been identiﬁed\ncorresponding to each class of inhibitors, albeit at different\nproportions (12).\nSince these DRMs and other genotypes/phenotypes are\ntransmitted within the proviral genome, there has been a long\nhistory of techniques for predicting these traits from genomic\nsequence. This manuscript has chosen three diverse tasks from\nthe HIV-1ﬁeld across the viral lifecycle as described inFigure 1\nand below:\n1. Protease inhibitor resistance – The protease gene is a 99\namino acid protein responsible for cleaving viral proteins that\nare translated as a single polyprotein into their active form;\nthus making it an ideal target for antiretroviral therapy. The\nStanford HIV database maintains a prediction server for\nannotating HIV-1 sequences with known resistance\nmutations (14). Investigators have also developed predictive\ntools such as SHIVA, a decision tree classiﬁer (15), generative\nmachine learning algorithms (16), and the ANRS (Agence\nNationale de Recherches sur le SIDA) algorithm, a well-\nregarded program for predicting protease receptor and\nreverse transcriptase resistance (17).\n2. Coreceptor utilization – HIV-1 enters human cells byﬁrst\nbinding the CD4 receptor and then recruiting one of two\npossible coreceptors: CCR5 or CXCR4. This allows for the\nentry into two different cellular reservoirs with CCR5\nprimarily responsible for T-cell infection and CXCR4 for\nmacrophages (4, 18). This phenotype is primarily due to the\n3\nrd variable loop of the envelope gene, the V3 loop. This 35\namino acid loop mediates the interaction between the\nFIGURE 1 | A diagram for the biological processes explored. The left column depicts the basic viral lifecycle starting with entry through the binding of CD4 andcell-\nspeciﬁc coreceptors, followed by unpackaging of the capsid, reverse transcription of viral RNA, integration into the host genome, subsequent translationof viral\nproteins and repackaging. Steps where antiretroviral medications interfere with the lifecycle are also noted. The middle panel indicates bodysites that are known to\nbe anatomic reservoirs of latent viral replication. The right panel shows aﬂow diagram of the experiments and training procedures described in the text. Figure\ncreated with BioRender.com.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806182\nenvelope protein and coreceptor primarily at positions 13-21\n(19). The earliest methods of coreceptor prediction used\nalignment-based methods to create a position-speci ﬁc\nscoring matrix (PSSM) ( 20). Newer methodologies like\ndecision trees (21) and XGboost (22) have been applied\nwith great success.\n3. Bodysite identiﬁcation – Due to the ability of HIV-1 to infect\nimmune cell populations, it spreads across the body rapidly\nand adapts to new compartments (23–26). When exploring\nHIV-1 cure strategies such as“shock & kill” and anti-HIV\nCRISPR-Cas9 gene editing, it is important to surveil these\ncompartments (27–29). This task is traditionally tackled\nusing phylogenetic methods in which the evolutionary\nrelationship between sequences isolated from different\nbodysites is explored (30–32). In our review of the relevant\nliterature this prediction task has not been previously\nattempted.\nUsing sequence data for machine learning (ML) techniques poses\nmultiple challenges. First, most ML techniques require numeric\ninputs. This requires a conversion of the amino acid sequence\ninto a numeric form that can be consumed by downstream\nalgorithms. Techniques for doing this can be grouped into two\ncategories: alignment-based and alignment-free.\nAlignment-based methods require using alignment tools like\nMUSCLE (33), T-coffee (34), or minimap2 (35) to associate each\nposition in the query with a consistent column. This is critical\nwhen using machine learning methods like SVM, decision trees,\nor other classic techniques as they assume that each column\nconsistently represents the same physical property. Once aligned,\na feature matrix can be constructed by converting each column\nof the alignment into a number, or set of numbers, either through\none-hot encoding or encoding physiochemical properties of each\namino acid at each position. This matrix can then be used like\nany other ML dataset.\nHowever, the rapid mutation of HIV-1 leads to difﬁculties with\nalignment-based methods as the insertion and deletion rate of the\nvirus leads to alignments with copious gaps. Alignment-free\nmethods sidestep this problem by summarizing a protein of any\nlength into aﬁxed length vector. Tools like MathFeature (36)a n d\nProtDCal (37), perform this through calculation of physiochemical\nproperties of the protein. However, this presupposes that these\nproperties are relevant to the prediction task. K-mer based\napproaches count occurrences of ﬁxed-length subsequences of\nthe protein. This generates aﬁxed-length vector for each protein\nthat compares the presence, absence, or quantity of short amino\nacid sequences. However, this requires one to balance the length of\nthe k-mer with the sparsity of thedownstream dataset. As each k-\nmer is a distinct feature in the matrix, it also does not account for\nthe similarities between amino-acids.\nMany of these problems are also present in relatedﬁelds like\nnatural language processing (NLP): not all sentences are the\nsame length, words have different meanings in different contexts,\nand similar words can be used interchangeably. This has spurred\na great deal of crosstalk between the NLPﬁeld and the genomic\nlanguage processing (GLP)ﬁeld (38, 39). Of particular interest to\nthis manuscript are advances in arti ﬁcial intelligence (AI)\nmethods, particularly the Transformer models (40,\n41). While\nthe initial investment is large (42), once trained, models can be\nreused on multiple tasks (43–45).\nIn 2021 a high-performanc e computing group led by\nBurkhard Rost set out to leverage Summit, the world’s second\nfastest computer, to accelerate GLP research. They leveraged\n1096 GPU containing nodes to train a Bidirectional Encoder\nRepresentations from Transformers (BERT) model on a dataset\nof 393 million amino acids from Uniprot50, Uniref100, and the\nBig Fantastic Database (BFD) of human isolated metagenomic\nproteomes (46). This model architecture considers the entire\nprotein at once (Bidirectional) and encodes proteins intoﬁxed\nlength vectors (Encoder Representation) for downstream\npredictions. Like all transformer style language models, it can\nbe trained on unlabeled data for many tasks through a technique\ncalled Masked Language Modeling (MaskedLM). In this process,\na subset of amino acids are‘masked’ from the model during\ntraining and it is tasked with predicting them and its weights are\nupdated through gradient back-propagation. Accomplishing this\ntask pretrains the model for future tasks. The Rost-Lab showed\nthat this model could be further reﬁned in a process called\n“transfer learning” to predict new tasks such as subcellular\nlocalization, secondary structure, and enzyme activity (46).\nThis group released their prediction models into the Hugging\nFace Model Repository for open-source use (https://huggingface.\nco/models). The Hugging Face transformer library is a Python-\nbased library for implementing state of the art AI models in a\nconsistent, reproducible, and extensible fashion (47). Pretrained\nmodels can be downloaded with a single command and applied\nto new data with another Hugging Face transformer pipeline\n(47). The library also provides an interface for re ﬁning the\npretrained models on new data (48). These studies reported\nherein applies these tools to the three HIV-1 prediction tasks\ndescribed above. It explores the effects of pre-training, class\nweighting, and dataset size on the task. It also releases the\nmodels and datasets to the Hugging Face dataset and model\nhub for the community at large to accelerate future work. These\nstudies also provide advice and expectations on adapting this to\nother applicable tasks.\n2 MATERIALS AND METHODS\nThe entire analysis pipeline was implemented as a Snakemake\npipeline and is available on the public Github repository (49).\nThis allows any researcher to download, reproduce, transform,\nor extend this analysis. The deep learning aspect of this pipeline\nrequires access to sufﬁcient computational resources. While the\nscript auto-scales to the user’s available memory the training of\nthe transformer models require at least 6GB of GPU RAM. Once\ntrained, the models can be used on generally available hardware.\n2.1 Explanation of Public Datasets\n2.1.1 HIV-1 Full Genome Dataset\nThe Los Alamos National Laboratory HIV sequence (LANL)\ndatabase maintains a standard dataset of 1,690 high-quality full-\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806183\nlength genomes. The most recent version was downloaded\n(2016-Full-genome) on 12/21/2021 (https://www.hiv.lanl.gov).\nThis was then processed using the LANL GeneCutter tool to\nextract and splice the DNA sequence of each gene (https://www.\nhiv.lanl.gov/content/sequen ce/GENE_CUTTER/cutter.html).\nThe GeneCutter tool aligns the query to a gold-standard codon\naligned multiple-sequence alignment using HMMER v 2.32. This\nensures that any gene within the region is properly extracted in\nthe proper reading-frame. These extracted DNA sequences were\nthen translated into the appropriate protein sequence to theﬁrst\nstop-codon using the BioPython library Seq.translate function\nwith a human codon table (50). The database was processed\nusing the script workﬂow/scripts/process_lanl_ﬂt.py.ipynb and\ndeposited as a Hugging Face Dataset damlab/HIV_FLT.\n2.1.2 Protease Drug Resistance\nThe high-quality interactions from the Stanford HIV Genotype-\nP h e n o t y p ed a t a b a s e(51) were downloaded on 12/21/2021 and\ncontained 1959 lines at the time of download. Theﬁle stores the\namino-acid differences from the provided reference sequence, for\nexample V30I|D46N, and an array of drug-susceptibility scores for\neach isolate. This was converted into a Hugging Face dataset by\ninferring the full protease sequence by exchanging the indicated\namino acids from the pre-translated reference sequence and\nlabeling any drug with a >4-fold increase in resistance as“True”\nconforming with the methodology described by Rhee et al. After\nﬁltering the dataset to only include the drugs FPV, IDV, NFV, and\nSQV, and dropping all missing items, there were 1733 total PR\nsequences for analysis with roughly half being resistant to at least\none drug. The database was processed using the script workﬂow/\nscripts/process_stanford_pr.py.ipynb and deposited as a Hugging\nFace Dataset damlab/HIV_PI.\n2.1.3 Coreceptor Tropism\nV3-loop sequences were downloaded from the LANL database\nthrough the Search Interface (https://www.hiv.lanl.gov/\ncomponents/sequence/HIV/search/search.html) on 12/20/21. The\nq u e r yw a sg e n e r a t e db yl i m i t i n gt h eS u b t y p e st oA ,B ,C ,a n dD ;\nselecting“list inﬁeld output” for theCoreceptorand Sample Tissue\nﬁelds; and selecting V3 in theGenomic Region Selectionbox. This\ngenerated approximately 220,000 results at the time of search. The\nLANL search tools were used to align, trim, and return the selected\nsequences with the associated background info. The background\ninformation was parsed to create an independent binary variable\neach for CCR5 and CXCR4 bindingstatus. This search generated\n2935 sequences with 91% being CCR5 tropic and 23% labeled as\nCXCR4 tropic. The database was processed using the script\nworkﬂow/scripts/process_lanl_v3.py.ipynb and deposited as a\nHugging Face Dataset damlab/HIV_V3_coreceptor.\n2.1.4 Bodysite Identiﬁcation\nUsing the same V3 dataset mentioned above, theSample Tissue\nﬁeld was aggregated so similar bodysites were grouped together.\nThe grouping was performed as follows:\n Periphery-tcell: plasma, PBMC, T cells, CD4+ T cells, resting\nCD4+ T cells, effector memory CD4+ T cells, transitional\nmemory T cells, central memory T cells, serum, blood, lymph\nnode, CD4+ T cell supernatant, lymph node CD4+ T cells,\nCD14+ monocytes, activated CD4+ T cells, naive CD4+ T\ncells, effector memory T cells, T-cell, CD8+ T cells, PMBC,\nPBMC supernatant, stem memory T cells, terminally\ndifferentiated T cells\n Periphery-monocyte : lamina propria mononuclear cells,\nCD14+ monocytes, monocyte, CD16+ monocytes\n CNS: brain, CSF, spinal cord, dendrites\n Lung: lung, BAL, sputum, diaphragm\n Breast-milk: breast milk\n Gastric: colon, rectum, jejunum, ileum, GALT, rectalﬂuid,\nintestine, feces, stomach, choroid plexus, sigmoideum, gastric\naspirate, esophagus\n Male-genitals: semen, seminal plasma, foreskin, seminal cells,\nurethra, prostate, testis, prostatic secretion\n Female-genitals: vaginal ﬂuid, cervix, vagina, vaginal cells,\ncervicovaginal secretions\n Umbilical-cord: umbilical cord plasma, placenta\n Organ: liver, kidney, epidermis, thymus, pancreas, adrenal\ngland, spleen, bone marrow\n Dropped sequence: supernatant, saliva, urine, meninges, skin\ntumor, qVOA, urine cells, breast milk supernatant, aorta,\nglioma\nThen, sequences were grouped such that each unique\nsequence was annotated with all bodysites it was associated\nwith. This allows a sequence to be annotated with multiple\nbodysites. Due to the over-representation of periphery-tcell\ntags, a random 95% were discarded. After processing, there\nwere 5510 unique V3 sequences annotated with about half\nfrom the periphery-tcell tag and the rest ranging from 5-20%\ncomposition. The database was processed using the script\nworkﬂow/scripts/process_lanl_v3.py.ipynb and deposited as a\nHugging Face Dataset damlab/HIV_V3_bodysite.\n2.2 Models\nFour models were constructed to evaluate the real-world\nperformance on these prediction tasks. The datasets above\nwere split using 5-fold cross-validation with the folds preserved\nacross the different model trainings. This allows for an honest\ncomparison of each fold across each model.\n2.2.1 Naive Models\nFirst, a Dummy model was created using thesklearn.dummy.\nDummyClassi ﬁer class with strati ﬁed strategy. This model\nrepresents randomly guessing utilizing the known class\ndistribution of the training data. For example, 91% of V3\nsequences are CCR5 tropic, therefore this model will guess True\n91% of the time. This naïve model represents the lowest reasonable\nb a rt os e tf o rp r e d i c t i o nt a s k s .\nNext, a basic Random Forest Classiﬁer was used as a biology\nnaive model machine learning model. The variable length\nsequences were encoded into ﬁxed length vectors using term\nfrequency-inverse documen t frequency (TF-IDF). This\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806184\ntechnique creates a feature for each N-gram present in the\ndatabase and encodes proteins by the sum of the number of\ntimes the N-gram occurs in the sequence divided by the number of\ntimes it occurs in the database. Sequences, V3 or PR, were\ntransformed with the sklearn.feature_extraction.text.\nTﬁdfVectorizer class using angram_range = (1,3)and analyzer=\n‘char’. This creates a vector of each k-mer sized 1-3 with the count\nnormalized by the inverse of its prevalence in the rest of the\ntraining dataset. This naturally highlights k-mers that are over-\nrepresented in the sequence. After a basic variance threshold to\nremove invariant columns, a classi ﬁer was built with the\nsklearn.ensemble.RandomForestClassiﬁer class using the default\nparameters. This model repre sents a purely mathematical\napproach to the prediction problem. These models were trained\nusing the workﬂow/scripts/sklearn_train.py script using scikit-\nlearn version 1.0.2.\n2.2.2 Transformer Models\nBiologically informed Transformer models were imported using\nthe Hugging FaceAutoModelForSequenceClassiﬁcation tool. The\nHugging Face library provides a Trainer module for reﬁning\npretrained models on new datasets. However, the default Trainer\ncannot accommodate multi-label prediction problems like those\nposed here. As such, aCustomTrainer class was developed per\nrecommendations in a related forum post ( 52 )a n d\ndocumentation ( 53). In brief, there were three changes\nimplemented. First, the CategoricalCrossEntropy loss function\nwas replaced with aBinaryCrossEntropy function; this allows the\nmodel to predict multiple True values in eachﬁeld (i.e. a protease\nsequence can be resistant to zero, one, or potentially all drugs).\nSecond, as the class labels are not equally distributed for each\nﬁeld, 91% of V3 sequences are CCR5 tropic weighting was used\nto balance the imbalanced classes as described in the BCELoss\ndocumentation. Finally, the loss from eachﬁeld of the prediction\nwas added together in equal weights. This CustomTrainer is\nimplemented in the module work ﬂow/scripts/common.py.\nTraining was performed using the same parameters across all\nmodels: learning rate of 1E-5, 50K warm-up steps, and a\ncosine_with_restarts learning rate schedule and continued until\n3 consecutive epochs did not improve the validation loss metric.\nFor these experiments, the Prot-Bert-BFD model from the\nRostLab was used as the basis for pretraining (46), commit-tag\n6c5c8a5. It has been trained across a wide array of proteins and is\neasily available in the Hugging Face library. This pre-trained\nmodel was used as the initial weights for training each of the\nthree models described above. This is implemented in the script\nworkﬂow/scripts/huggingface_train.py.\nPrevious research has shown that reﬁning language models on\ndomain-speciﬁcs e q u e n c ec a ni m p r o v ed o w n s t r e a mp e r f o r m a n c e\n(43–45). Using the whole genome sequence data described above,\nthe Prot-BERT-BFD model was reﬁned using MaskedLM training.\nIn this task, a random set of amino acids are masked from the\nmodel, which is then asked to predict them. A training script was\nadapted from the HuggingFace transformers run_mlm.py script\n(54). Pre-training was performed by concatenating all protein\nsequences from the Full HIV Genome Dataset described above\nand chunked into 256 amino acid segments. 80% of the sequences\nwere used for training and 20% were reserved for validation. This is\nimplemented in the script workﬂow/scripts/huggingface_lm.py\nusing transformers v4.15.0.\n2.3 Conceptual Error\nConceptual error was calculated using a set of reference\nmutations known from other sources of information to induce\na known effect. It relies on our knowledge of the structure-to-\nfunction relationships involved in these processes and quantiﬁes\nthe biological“unexpectedness” of a model’s output.\nProtease resistance conceptual error was measured by generating\na dataset of known protease inhibitorresistance mutations. Utilizing\nthe Stanford HIV database mutation explorer, 10 mutations were\nidentiﬁed as inducing a >4-fold increase in drug resistance when\nacting alone: D30N, V32I, M46I, M46L, G48V, I54V, V82F, I84C,\nN88S and L90M. For each sequence in the testing dataset of each\nfold, they were examined for these mutations. If absent, they were\nintroduced; if present they were removed. Then, the original and\naltered sequences were inferred by each trained model. The logit\noutput of each pair was subtracted and squared. Then, the squared-\nerror for pairs which introduced resistance mutations that increased\nresistance were set to zero; as were pairs which removed resistance\nmutations that decreased resistance. The resulting masked vector\nwas then averaged. This error represents the mean-squared error\nchange in theunexpecteddirection.\nV3 coreceptor prediction can also be interrogated in this way.\nStructural alignments by Fouchier et al. (55) have demonstrated an\n11/24/25 rule: if there is a positive amino acid at positions 11, 24,\nor 25 of the V3 loop then it can bind to CXCR4, otherwise it binds\nto CCR5. Utilizing the same switching and error calculation\nstrategy described above, pairs of sequences were made with\nS11H, S11R, S11K, G24H, G24R, G24K, E25H, E25R, and E25K.\nAs there are no known structure-function relationships that\ncan be used as a gold-standard for bodysite identiﬁcation, it was\nexcluded from the conceptual error calculations.\n2.4 Statistical Comparisons\nAs each prediction task used the same 5-fold cross-validation\nscheme, it is possible to compare different models trained on the\nsame 80% and evaluated on the last 20%. For this reason, a paired\nt-test provides the most power to evaluate differences. For each\ncomparison below, a paired t-test was implemented with\nscipy.stats.ttest_rel (v1.7.3). As these pairwise comparisons\nresult in many tests per experiment, a Bonferroni correction\nwas used to correct for false positives. A corrected p<0.05 was\nused as a threshold for declaring statistical signiﬁcance.\n3 RESULTS\n3.1 Dataset Release\nThis manuscript publicly releases four HIV-focused datasets.\nThese have been prepared for other researchers by conforming to\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806185\nthe Hugging Face Dataset Hub style. This allows the datasets to\nbe downloaded using a simple command like:\nfrom datasets import load_dataset\nhiv_dataset = load_dataset(‘damlab/HIV_FLT’)\nwhich are then available as high-speed data objects for\ndownstream use.\nAs shown inFigure 2A, the whole genome dataset contains a\nmixture of genes of the correct length as well as those with\npremature stop codons, with 33.3% of genomes contained at least\none gene with a premature stop-codon. When concatenated, this\ndataset contains 3.9 million characters, approximately 1% of the\nsize of the original BFD training dataset of 393 million characters\n(46). The classi ﬁcation datasets are independent from the\ngenome dataset as these full-genomes lack drug-resistance,\ncoreceptor binding type, or tissue isolation information. As\nsuch, three datasets have been prepared for the three\nclassiﬁcation tasks relevant to HIV biology.Figure 2B shows\nthe prevalence of drug resistance in Protease sequences across\nfour drugs from the Stanford HIV Database. Out of the 1733\nProtease sequences with kno wn drug resistance, 55.8% of\nsequences have resistance to at least one drug, while 28.0%\nhave resistance to all four.Figure 2C describes the proﬁle of\nbody-sites where 5510 unique V3 sequences have been isolated\nwith 28.3% isolated from mult iple locations. A partially\noverlapping set of 2935 V3 sequences contained coreceptor\ninformation with the majority being CCR5 binding 92.1%,\n23.9% CXCR4 binding, and 16.0% dual tropic as shown in\nFigure 2D . Over 200,000 V3 sequences were discarded as\nhaving neither body-site nor coreceptor information.\n3.2 Classiﬁcation Tasks\nTable 1and Figure 3show the precision, recall, and accuracy of\neach of the trained models when a standard 50% cutoff was used\nto binarize the predictions. When considering accuracy, the\npercentage of correctly called sequences from the validation\nfold, the TF-IDF model best predicted protease resistance\nmutations with a 91.3% accuracy, while the HIV-BERT model\nperformed the best at coreceptor prediction and bodysite\nidentiﬁcation with a 92.5% and 89.1% accuracy respectively.\nHowever, the TF-IDF and HIV-BERT models performed\nAB\nDC\nFIGURE 2 | Description of publicly released datasets.(A) The length of each translated sequence is shown as a heatmap with darker regions indicating a greater\nconcentration of sequences at that length. The proteome represents the total length of all translated sequences.(B) The number of protease sequences with\nobserved resistance (orange) and no resistance (green) to each of four drugs. MultiDrug resistance represents the sum of individual drug resistances and is indicated\nby the key above.(C) The number of V3 sequences observed at each body-site (orange) and not observed (green) to each of the grouped sites. MultiSite represents\nthe total number of sites that a unique V3 sequence was observed in.(D) The number of V3 sequences annotated with known coreceptor usage with those able to\nbind the coreceptor in orange and those not able to bind the coreceptor in green. DualTropic represents sequences that were determined to utilize bothcoreceptors\nin orange and those only able to bind to one are shown in green.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806186\nwithin 3% mean accuracy across all tasks, which is less than 1\nstandard deviation when considering models across folds.\nWhile commonly measured, accuracy may not be the ideal\nmetric for comparing model performance in a clinical context.\nDepending on the situation, a researcher may wish to trade\nprecision for recall by altering the cutoff between a positive and\nnegative prediction. To account for this, the area under the\nreceiver-operator characteristic curve (AUC) was calculated and\nshown in Table 1 and Figure 3D. Here the TF-IDF model\nperformed the best across all tasks but again this was statistically\nsigniﬁcant in the protease resistance and tissue tasks but not\ncoreceptor prediction.\nAs these tasks are multi-class predictions, it is important to\nexamine the ability of the model to perform on each class.\nFigure 4 shows the accuracy of each model across each of the\nclasses of each prediction task. When considering each model\nTABLE 1 |Average model performance metrics across 5-fold cross-validation.\nTask Model Precision Recall Accuracy AUC\nProtease Resistance Null Model 43.4% (7.6) 43.6% (7.9) 51.7% (2.7) 49.9% (2.3)\nTF-IDF 87.2% (5.0) 92.4% (4.9) 91.3% (2.8) 97.0% (1.4)\nProt-BERT 80.6% (10.6) 82.2% (22.9) 82.8% (10.4) 87.8% (13.1)\nHIV-BERT 85.5% (9.0) 88.5% (4.3) 88.4% (3.2) 94.3% (2.4)\nCoreceptor Usage Null Model 57.7% (36.3) 58.0% (35.8) 74.0% (11.9) 49.7% (2.1)\nTF-IDF 92.7% (4.4) 82.0% (18.2) 92.4% (2.9) 92.5% (2.6)\nProt-BERT 91.0% (6.6) 81.5% (15.6) 91.2% (2.3) 91.6% (3.0)\nHIV-BERT 91.7% (6.0) 84.5% (13.5) 92.5% (2.2) 92.4% (2.8)\nTissue of Isolation Null Model 14.1% (15.7) 14.2% (15.6) 79.3% (13.4) 49.7% (1.5)\nTF-IDF 81.8% (19.7) 20.6% (20.8) 88.6% (9.6) 85.0% (6.7)\nProt-BERT 6.1% (17.5) 11.1% (31.8) 86.4% (12.4) 52.1% (7.2)\nHIV-BERT 53.4% (34.0) 33.3% (25.4) 89.1% (9.9) 81.6% (7.4)\nThe number in the cell represents the mean metric across all folds and across each predictiveﬁeld. The numbers in the parentheses represent the standard deviation of the metric. The\nbolded elements indicate the best performing model for each prediction task based on the metric.\nAB\nDC\nFIGURE 3 | Pretraining improves prediction metrics across all tasks. The accuracy(A), F1-score(B), precision(C), and AUC(D) are shown for each model and\neach prediction task. The bar indicates the mean value and the error bars represent the 95% conﬁdence interval of 5-fold cross validation. The Null model is shown\nin red, the TF-IDF model is show in blue, the Prot-BERT model in green, and the HIV-BERT model is in purple. The test-comparison bars represent the results of a\npaired t-test between each group; undrawn comparisons p<0.05, *(0.05<p<=0.01), **(0.01<p<=0.001), ***(0.001<p<=0.0001), ****(p<=0.00001). ABonferroni\ncorrection based on all possible tests in theﬁgure.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806187\nindividually, there is a consistent level of prediction ability\nacross classes with AUC scores within 5%. In the protease\nresistance task, TF-IDF and HIV-BERT have an increased\nability to predict IDV relative to SQV (p<0.05). When\nexamining the tissue prediction task, the TF-IDF and HIV-\nBERT did show differences across tasks with breast-milk\nhaving the highest accuracy and periphery-tcell having\nthe lowest.\n3.3 HIV-BERT Pretraining\nUtilizing the full genome dataset described above, theRostLab/\nprot_bert_bfd model was reﬁned for HIV speciﬁct a s k s .T h i s\npretraining reduced the masked token cross-entropy loss from\n1.85 nats for the unreﬁned model to 0.36 nats. This indicates\nthat the average prediction for the correct amino acid\nimproved from approximately 15% to 70%. This was\nvisualized by subjecting the consensus subtype B V3 loop\nCTRPNNNTRKSIHIGPGRAFYTTGEIIGDIRQAHC ( 19)t o\nsingle amino acid masking across all 35 positions. Figure 5\nshows the difference between the unreﬁned model and the HIV-\nreﬁned model across this masking task. The HIV-reﬁned model\nhas a greater predicted probability for the consensus amino acid\nat most positions (31/35) compared to the unreﬁned model.\n3.4 Conceptual Error\nEach sequence in the protease dataset was subjected to single\namino acid mutations to either add or remove a known DRM,\nas listed in the methods above. Figure 6 shows the effect of\nthose substitutions on the model’s prediction of the likelihood\nof drug resistance with red dots indicating a gain of a DRM and\ngray dots indicating a loss. A well performing model would\nshow that an addition of a DRM (red dots) increases the\nresistance likelihood and would therefore be above the x=y\nline. The converse would be true when removing a known\nDRM (gray dots). When examiningFigure 6, there are many\ninstances in the TF-IDF and Prot-BERT models in which\nmutations mislead the prediction of the models; red dots that\nare below the line or grey dots that are above the line. This is\nnoticeably less prevalent in the HIV-BERT model.\nA similar pattern can be observed inFigure 7 when adding\nmutations known to increase CXCR4 binding (orange dots)\nand removing mutations known to indicate CXCR4 binding\n(green dots). The TF-IDF model also has poor conﬁdence in\nclassifying CXCR4 values after the addition of promoting\nmutations with few mutated sequences increasing past a 50%\nthreshold. Prot-BERT and HIV-BERT do not suffer from\nthis limitation.\nAB\nC\nFIGURE 4 | Area under the curve (AUC) scores for individualﬁelds of drug resistance and coreceptor prediction are consistent, but tissue identiﬁcation is not. The model\nAUC scores were disambiguated for eachﬁeld of each prediction task. Each task is shown in(A) protease drug resistance,(B) coreceptor prediction, and(C) tissue isolation\nwith colors indicating the predictionﬁeld. The bar indicates the mean value and the error bars represent the 95% conﬁdence interval of 5-fold cross validation. The test-\ncomparison bars represent the results of a paired t-test between each group; undrawn comparisons p<0.05, *(0.05<p<=0.01), **(0.01<p<=0.001), ***(0.001<p<=0.0001),\n****(p<=0.00001). A Bonferroni correction based on all possible tests in theﬁgure.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806188\nIn order to quantify this, a mean-squared error was\ncalculated as described above such that only predictions in\nthe unexpected direction were penalizing. Figure 8A shows\nthe results of this analysis grouped by model. In the coreceptor\nprediction task, all models improve upon the naïve\npredictions but are not statistically signi ﬁcantly different\nfrom each other. However, for the protease resistance\nprediction task the Prot-BERT and HIV-BERT models are\nmore statistically signi ﬁcant than the TF-IDF model but not\nfrom each other. Across all tasks, Figures 8B, C,t h eH I V -\nBERT model outperforms al lo t h e rm o d e l sb u tn o tt oa\nstatistically signi ﬁcant level.\n4 DISCUSSION\nOver the past decade there has been an explosion in open-\nsource natural language processing tools, especially in the AI\nﬁeld. However, biological datasets are rarely in a form\namenable to easy implementation. This is particularly true of\nspecialized datasets like those discussed in this analysis.\nCreating publicly accessible datasets in an easily retrieved\nform will help bridge the gap between AI and biological\nresearchers. The study reported herein releases four HIV\nspeciﬁc datasets to the public for new researchers to iterate\nupon. This is coupled with the release of a generically-trained\nH I V - B E R Tm o d e la sw e l la st h et h r e et a s k - s p e c iﬁcr eﬁnements\ndiscussed above. It is our hope that depositing these in the\nopen-source Hugging Face transformers library will allow for a\ndemocratization of research and prevent issues like link-death,\na common problem when attempting to build upon the work\nof others.\nExamining previous machine learning attempts at these tasks\nreveals that our technique is competitive with existing methods.\nEarly work in DRM mutation by Beerenwinkel et al. achieved\nsensitivities ranging from 58-92% and speciﬁcities ranging from\n62-92% using decision trees across different inhibitors (56). Later\nwork by Heider et al. was able to achieve AUCs ranging from\n0.79-0.89 using chains of classiﬁers (57). Recent work published\nin 2020 by Steiner et al. tested older AI techniques like multi-\nlayered perceptrons and recurrent networks achieving AUCs\nfrom 0.8 to 0.97 (58). The HIV-BERT model released on the\nHugging Face repository has an average AUC of 0.94, making it\ncompetitive with current state of the art techniques. When\nexamining our coreceptor prediction model we perform\ncompetitively with recent work by Chen et al. in 2019 using\nthe XGBoost method (22).\nIt is important to note that none of the previous methods used\nthe same dataset for training and validation, making direct\ncomparison difﬁcult. This is likely because HIV-1 sequence data\nis sequestered within databases that require domain speci ﬁc\nknowledge to access and process for modern machine learning\nand AI tools. Our release of a standard-formatted dataset and\nprocessing scripts will help to alleviate these difﬁculties.\nTo our knowledge, the tissue identiﬁcation task has never\nbeen posed as a classiﬁcation problem in this manner. Previous\nFIGURE 5 | Full genome pretraining of the Prot-BERT model increases HIV-1 sequence awareness. The probability of each consensus amino acid of the V3 loop\nwhen performing masked prediction task. Green bars represent the prediction from the Prot-BERT model and red bars represent the HIV-BERT model.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 8806189\nresearch has used phylogenetic methods to identify the level of\ncompartmentalization by ﬁnding mutations unique to a single\ntissue type when examining isolates from diverse tissues (59, 60).\nHowever, when performing routine surveillance sequencing of\npatients, invasive methods like lumbar punctures for CSF or\nbronchiolar lavage for lung sampling are impractical. Framing\nthe problem as a classiﬁcation task allows for sequencing of virus\nfrom peripheral blood to detect recent reactivation events from\nthese latent reservoirs. This will be useful when evaluating the\nsuccess of HIV cure strategies such as anti-HIV-1 CRISPR/Cas9\ngene editing or latency reactivation (27).\nIn our analysis, a simple k-mer and tree-based prediction,\nTF-IDF, was able to outperform advanced AI models in many\nof the tasks when considering classic metrics such as accuracy\nand AUC. However, when it was subjected to biological\ninvestigation by introducing mutations with known\nfunction, it performed the worst. This re ﬂects poor\ngeneralization and indicates that the TF-IDF model may be\n“memorizing ” patterns that are correlated with the prediction\ntask, but not causative. BERT-style models, pre-trained on\nmillions of sequences, can distinguish between functional\nchanges in the sequence and random mutations. This result\nFIGURE 6 | Transformer models accurately predict the outcome of drug resistance mutations (DRMs). Each sequence in the resistance dataset was mutated by\neach of the ten DRMs as described in the methods and each mutated sequence is shown as a single point; there are ten points per sequence. Sequences where a\nDRM was added are shown in red which should increase the probability of resistance. Points in grey indicate sequences where a DRM was removed and should\ndecrease in probability of resistance. The dashed black line shows the x=y line and the assumption of no change due to the mutation. Each column of axes shows\nthe predictions using each of the three models. With theﬁrst column indicating the TF-IDF model; the second indicating the Prot-BERT model; and the third showing\nthe HIV-BERT results. Each row represents the prediction on a different drug in the order FPV, IDV, NFV, and SQV.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 88061810\nindicates a need for detailed evaluation of prediction models\nto ensure that they reﬂect biological reality. The introduction\nof conceptual error described above is an initial attempt at\nperforming this at-scale for HIV-1 tasks. Future work should\nexpand this by incorporating deep mutational scanning\ndatasets (61).\nWhen examining the BERT style models across related\ntasks, there is a notable level of consistency across predicted\nﬁelds. The AUC for each drug and coreceptor fall within 5% of\neach other. This may indicate that information is being shared\nwithin the model across these related ﬁelds, a common\nstrength of transformer models. This pattern was not seen\nin the tissue classi ﬁcation task, this may indicate more\nsequences are needed or that other areas of the HIV-1\ngenome should be examined. Other published research has\nshown that regions across the genome have a role to play in\ntissue speciﬁcity and cellular tropism. It may be that the V3\nloop is important for cellular tropism but the accessory\nproteins Vpr, Tat, and Nef may play a greater role in tissue\nspeciﬁcity (62, 63). Future work should explore other proteins\nusing similar prediction tasks.\nThe trained models have multiple biological applications.\nThe PI- and Tropism-train ed models are immediately\napplicable to predicting the mos t useful antiretroviral drug\nfor a given patient. They can alsobe utilized for exploring the\nstructure-function relationship between inhibitors and viral\nsequences. The bodysite prediction task allows for the\ntracking of leakages of strains from viral reservoirs. This\nmay lead to new avenues of research around the exchange of\nviral quasispecies between compartments. Finally, the HIV-\nBERT model functions as a base that future HIV predictive\nmodels can be built upon.\nTaken together, this work shows that AI models, particularly\ntransformers, are well suited to b iological prediction tasks.\nReﬁning the model on unlabeled sequences improves prediction\naccuracy with minimal upstream cost; the HIV-BERT model\ndiscussed above can be downloaded and ﬁnetuned for new\nprediction tasks with minimal additional effort and has been\nFIGURE 7 | Transformer models accurately predict the outcome of CXCR4 enhancing mutations. Each sequence in the V3 coreceptor dataset was mutated by\neach of the nine CXCR4 promoting mutations as described in the methods and each mutated sequence is shown as a single point; there are nine points per\nsequence. Sequences where a CXCR4 promoting mutation was added is shown in orange which should increase the probability of CXCR4 binding and decrease\nthe probability of CCR5 binding. Points in green indicate sequences where a CXCR4 promoting mutation was removed and should decrease CXCR4 binding and\nincrease CCR5 binding. The dashed black line shows the x=y line and the assumption of no change due to the mutation. Each column of axes shows the\npredictions using each of the three models with theﬁrst column indicating the TF-IDF model; the second indicating the Prot-BERT model; and the third showing the\nHIV-BERT. Each row represents the prediction on a different coreceptor with CCR5 on the top and CXCR4 on the bottom.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 88061811\nsuccessful with fewer than 2000 labeled sequences. This puts many\nbiologically relevant HIV-1 prediction tasks within reach and can\naccelerate any additional protein-to-function prediction task and\nwill be useful to the HIV community as a whole.\nDATA AVAILABILITY STATEMENT\nAll trained models and datasets are available at https://huggingface.\nco/damlab. The training scripts and the code used to generateﬁgures\nis available at https://git hub.com/DamLabResources/\nhiv-transformers.\nAUTHOR CONTRIBUTIONS\nC o n c e p t u a l i z a t i o n ,W D ;m e t h o d o l o g y ,W D ;s o f t w a r e ,W D ;\nvalidation, WD; formal analysis, WD; investigation, WD;\nresources MN, BW, and WD; data curation, WD; writing—\noriginal draft preparation, WD; writing— review and editing,\nWD, RL, JE, MC, DS, KK, MN, and BW; visualization, WD;\nsupervision, WD; project administration, WD; funding\nacquisition, MN, BW, and WD. All authors have read and\nagreed to the published version of the manuscript.\nFUNDING\nThe authors were funded in part by the Public Health Service,\nNational Institutes of Health, through grants from the National\nInstitute of Mental Health (NIMH) R01 MH110360 (Contact PI,\nBW), the NIMH Comprehensive NeuroAIDS Center (CNAC)\nP30 MH092177 (KK, PI; BW, PI of the Drexel subcontract\ninvolving the Clinical and Translational Research Support\nCore), the Ruth L. Kirschstein National Research Service\nAward T32 MH079785 (PI, Tricia Burdo; BW, Principal\nInvestigator of the Drexel University College of Medicine\ncomponent) and from the National Institute of Neurological\nDisorders and Stroke (NINDS) R01 NS089435 (PI, MN).\nREFERENCES\n1. Mailler E, Bernacchi S, Marquet R, Paillart JC, Vivet-Boudou V, Smyth RP.\nThe Life-Cycle of the HIV-1 Gag-RNA Complex. Viruses (2016) 8(9).\ndoi: 10.3390/v8090248\n2. Mourad R, Chevennet F, Dunn DT, Fearnhill E, Delpech V, Asboe D, et al. A\nPhylotype-Based Analysis Highlights the Role of Drug-Naive HIV-Positive\nIndividuals in the Transmission of Antiretroviral Resistance in the UK.AIDS\n(2015) 29(15):1917–255. doi: 10.1097/QAD.0000000000000768\n3. Arias A, Ruiz-Jarabo CM, Escarmis C, Domingo E. Fitness Increase of\nMemory Genomes in a Viral Quasispecies.J Mol Biol (2004) 339(2):405–\n12. doi: 10.1016/j.jmb.2004.03.061\n4. Aiamkitsumrit B, Dampier W, Ante ll G, Rivera N, Martin-Garcia J,\nPirrone V, et al. Bioinformatic Analysis of HIV-1 Entry and Pathogenesis.\nAB\nC\nFIGURE 8 | HIV-1 pretraining decreases conceptual error.(A) The average conceptual error of each model is shown across the coreceptor and protease resistance\ntasks for each model.(B) The conceptual error was disambiguated across eachﬁeld of the coreceptor model.(C) The conceptual error was disambiguated across\neach ﬁeld of the protease resistance model. Across all axes the Null model is shown in red, the TF-IDF model is shown in blue, the Prot-BERT model shown in green\nand the HIV-BERT model shown in purple. The test-comparison bars represent the results of a paired t-test between each group; undrawn comparisons p<0.05,\n*(0.05<p<=0.01), **(0.01<p<=0.001), ***(0.001<p<=0.0001), ****(p<=0.00001). A Bonferroni correction based on all possible tests in theﬁgure.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 88061812\nCurr HIV Res (2014) 12(2):132 –61. doi: 10.2174/1570162X1266614052\n6121746\n5. Peters PJ, Duenas-Decamp MJ, Sullivan WM, Clapham PR. Variation of\nMacrophage Tropism Among HIV-1 R5 Envelopes in Brain and Other\nTissues. J Neuroimmune Pharmacol(2007) 2(1):32–41. doi: 10.1007/s11481-\n006-9042-2\n6. Marino J, Maubert ME, Mele AR, Spector C, Wigdahl B, Nonnemacher MR.\nFunctional Impact of HIV-1 Tat on Cells of the CNS and its Role in HAND.\nCell Mol Life Sci(2020) 77(24):5079–99. doi: 10.1007/s00018-020-03561-4\n7. Dampier W, Antell GC, Aiamkitsumrit B, Nonnemacher MR, Jacobson JM,\nPirrone V, et al. Speci ﬁc Amino Acids in HIV-1 Vpr are Signi ﬁcantly\nAssociated With Differences in Patient Neurocognitive Status.J Neurovirol\n(2017) 23(1):113–24. doi: 10.1007/s13365-016-0462-3\n8. Nonnemacher MR, Pirrone V, Feng R, Moldover B, Passic S, Aiamkitsumrit\nB, et al. HIV-1 Promoter Single Nucleotide Polymorphisms Are Associated\nWith Clinical Disease Severity.PloS One(2016) 11(4):e0150835. doi: 10.1371/\njournal.pone.0150835\n9. Gorry PR, Churchill M, Crowe SM, Cunningham AL, Gabuzda D.\nPathogenesis of Macrophage Tropic HIV-1.Curr HIV Res (2005) 3(1):53–\n60. doi: 10.2174/1570162052772951\n10. Wagner BG, Garcia-Lerma JG, Blower S. Factors Limiting the Transmission of\nHIV Mutations Conferring Drug Resistance: Fitness Costs and Genetic\nBottlenecks. Sci Rep(2012) 2:320. doi: 10.1038/srep00320\n11. Briones C, Domingo E. Minority Report: Hidden Memory Genomes in HIV-1\nQuasispecies and Possible Clinical Implications.AIDS Rev (2008) 10(2):93–\n109.\n12. Blassel L, Zhukova A, Villabona-Arenas CJ, Atkins KE, Hue S, Gascuel O. Drug\nResistance Mutations in HIV: New Bioinformatics Approaches and Challenges.\nCurr Opin Virol(2021) 51:56–64. doi: 10.1016/j.coviro.2021.09.009\n13. Ross L, Lim ML, Liao Q, Wine B, Rodriguez AE, Weinberg W, et al. Prevalence\nof Antiretroviral Drug Resistance and Resistance-Associated Mutations in\nAntiretroviral Therapy-Naive HIV -Infected Individuals From 40 United\nStates Cities.HIV Clin Trials(2007) 8(1):1–8. doi: 10.1310/hct0801-1\n14. Liu TF, Shafer RW. Web Resources for HIV Type 1 Genotypic-Resistance Test\nInterpretation. Clin Infect Dis(2006) 42(11):1608–18. doi: 10.1086/503914\n15. Riemenschneider M, Hummel T, Heider D. SHIVA - A Web Application for\nDrug Resistance and Tropism Testing in HIV.BMC Bioinform (2016) 17\n(1):314. doi: 10.1186/s12859-016-1179-2\n16. Pawar SD, Freas C, Weber IT, Harrison RW. Analysis of Drug Resistance in\nHIV Protease.BMC Bioinform(2018) 19(Suppl 11):362. doi: 10.1186/s12859-\n018-2331-y\n17. Singh Y. Machine Learning to Imp rove the Effectiveness of ANRS in\nPredicting HIV Drug Resistance. Healthc Inform Res (2017) 23(4):271–6.\ndoi: 10.4258/hir.2017.23.4.271\n18. Gorry PR, Sterjovski J, Churchill M, Witlox K, Gray L, Cunningham A, et al.\nThe Role of Viral Coreceptors and Enhanced Macrophage Tropism in Human\nImmunodeﬁciency Virus Type 1 Disease Progression.Sex Health (2004) 1\n(1):23–34. doi: 10.1071/sh03006\n19. Tamamis P, Floudas CA. Molecular Recognition of CCR5 by an HIV-1 Gp120\nV3 Loop.PloS One(2014) 9(4):e95767. doi: 10.1371/journal.pone.0095767\n20. Jensen MA, Coetzer M, van 't Wout AB, Morris L, Mullins JI. A Reliable\nPhenotype Predictor for Human Immunodeﬁciency Virus Type 1 Subtype C\nBased on Envelope V3 Sequences. JV i r o l(2006) 80(10):4698 –704.\ndoi: 10.1128/JVI.80.10.4698-4704.2006\n21. Lengauer T, Sander O, Sierra S, Thielen A, Kaiser R. Bioinformatics Prediction\nof HIV Coreceptor Usage. Nat Biotechnol (2007) 25(12):1407 – 10.\ndoi: 10.1038/nbt1371\n22. Chen X, Wang ZX, Pan XM. HIV-1 Tropism Prediction by the XGboost and\nHMM Methods.Sci Rep(2019) 9(1):9997. doi: 10.1038/s41598-019-46420-4\n23. Borrajo A, Svicher V, Salpini R, Pellegrino M, Aquaro S. Crucial Role of Central\nNervous System as a Viral Anatomical Compartment for HIV-1 Infection.\nMicroorganisms(2021) 9(12):2537. doi: 10.3390/microorganisms9122537\n24. Khan S, Telwatte S, Trapecar M, Yukl S, Sanjabi S. Differentiating Immune\nCell Targets in Gut-Associated Lymphoid Tissue for HIV Cure.AIDS Res\nHum Retroviruses(2017) 33(S1):S40–58. doi: 10.1089/AID.2017.0153\n25. Salemi M, Rife B. Phylogenetics and Phyloanatomy of HIV/SIV Intra-Host\nCompartments and Reservoirs: The Key Role of the Central Nervous System.\nCurr HIV Res(2016) 14(2):110–20. doi: 10.2174/1570162x13666151029102413\n26. Banga R, Munoz O, Perreau M. HIV Persistence in Lymph Nodes.Curr Opin\nHIV AIDS(2021) 16(4):209–14. doi: 10.1097/COH.0000000000000686\n27. Atkins AJ, Allen AG, Dampier W, Haddad EK, Nonnemacher MR, Wigdahl\nB. HIV-1 Cure Strategies: Why CRISPR?Expert Opin Biol Ther(2021) 21\n(6):781–93. doi: 10.1080/14712598.2021.1865302\n28. Stein J, Storcksdieck Genannt Bonsmann M, Streeck H. Barriers to HIV Cure.\nHLA (2016) 88(4):155–63. doi: 10.1111/tan.12867\n29. Gantner P, Ghosn J. Genital Reservoir: A Barrier to Functional Cure?Curr\nOpin HIV AIDS(2018) 13(5):395–401. doi: 10.1097/COH.0000000000000486\n30. Stam AJ, Nijhuis M, van den Bergh WM, Wensing AM. Differential Genotypic\nEvolution of HIV-1 Quasispecies in Cerebrospinal Fluid and Plasma: A\nSystematic Review.AIDS Rev(2013) 15(3):152–61.\n31. Smit TK, Brew BJ, Tourtellotte W, Morgello S, Gelman BB, Saksena NK.\nIndependent Evolution of Human Immunode ﬁciency Virus (HIV) Drug\nResistance Mutations in Diverse Areas of the Brain in HIV-Infected\nPatients, With and Without Dementia, on Antiretroviral Treatment.J Virol\n(2004) 78(18):10133–48. doi: 10.1128/JVI.78.18.10133-10148.2004\n32. Giatsou E, Abdi B, Plu I, Desire N, Palich R, Calvez V, et al. Ultradeep\nSequencing Reveals HIV-1 Diversity and Resistance Compartmentalization\nDuring HIV-Encephalopathy. AIDS (2020) 34(11):1609–14. doi: 10.1097/\nQAD.0000000000002616\n33. Edgar RC. MUSCLE: Multiple Sequence Alignment With High Accuracy and\nHigh Throughput. Nucleic Acids Res(2004) 32(5):1792–7. doi: 10.1093/nar/\ngkh340\n34. Notredame C, Higgins DG, Heringa J. T-Coffee: A Novel Method for Fast and\nAccurate Multiple Sequence Alignment. J Mol Biol (2000) 302(1):205–17.\ndoi: 10.1006/jmbi.2000.4042\n35. Li H. Minimap2: Pairwise Alig nment for Nucleotide Sequences.\nBioinformatics (2018) 34(18):3094–100. doi: 10.1093/bioinformatics/bty191\n36. Bonidia RP, Domingues DS, Sanches DS, de Carvalho A. MathFeature:\nFeature Extraction Package for DNA, RNA and Protein Sequences Based\non Mathematical Descriptors.Brief Bioinform(2022) 23(1). doi: 10.1093/bib/\nbbab434\n37. Ruiz-Blanco YB, Paz W, Green J, Marrero-Ponce Y. ProtDCal: A Program to\nCompute General-Purpose-Numerical Descriptors for Sequences and 3D-\nStructures of Proteins.BMC Bioinform (2015) 16:162. doi: 10.1186/s12859-\n015-0586-0\n38. Yandell MD, Majoros WH. Genomics and Natural Language Processing.Nat\nRev Genet(2002) 3(8):601–10. doi: 10.1038/nrg861\n39. Yue T, Wang H. Deep Learning for Genomics: A Concise Overview(2018)\n(Accessed February 01, 2018).\n40. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention Is All You Need(2017) (Accessed June 01, 2017). arXiv:1706.03762.\n41. Zeng W, Wu M, Jiang R. Prediction of Enhancer-Promoter Interactionsvia\nNatural Language Processing. BMC Genomics (2018) 19(Suppl 2):84.\ndoi: 10.1186/s12864-018-4459-6\n42. Patterson D, Gonzalez J, Le Q, Liang C, Munguia L-M, Rothchild D, et al.\nCarbon Emissions and Large Neural Network Training(2021) (Accessed April\n01, 2021).\n43. Howard J, Ruder S. Universal Lang uage Model Fine-Tuning for Text\nClassiﬁcation, In:Arxiv (2018) (Accessed January 01, 2018).\n44. Cohn D, Zuk O, Kaplan T. Enhancer Identiﬁcation Using Transfer and Adversarial\nDeep Learning of DNA Sequences.bioRxiv(2018), 264200. doi: 10.1101/264200\n45. Plekhanova E, Nuzhdin SV, Utkin LV, Samsonova MG. Prediction of\nDeleterious Mutations in Coding Regions of Mammals With Transfer\nLearning. Evol Appl(2019) 12(1):18–28. doi: 10.1111/eva.12607\n46. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, et al.\nProtTrans: Towards Cracking the Language of Lifes Code Through Self-\nSupervised Deep Learning and High Performance Computing.IEEE Trans\nPattern Anal Mach Intell(2021). doi: 10.1109/TPAMI.2021.3095381\n47. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al.\nTransformers: State-of-the-Art Natural Language Processing. (2020).\n48. Lhoest Q, del Moral AV, Jernite Y, Thakur A, von Platen P, Patil S, et al.\nDatasets: A Community Library for Natural Language Processing (2021)\n(Accessed September 01, 2021).\n49. Koster J, Rahmann S. Snakemake-A Scalable Bioinformatics Work ﬂow\nEngine. Bioinformatics (2018) 34(20):2520-2. doi: 10.1093/bioinformatics/\nbty350\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 88061813\n50. Cock PJ, Antao T, Chang JT, Chapman BA, Cox CJ, Dalke A, et al. Biopython:\nFreely Available Python Tools for Computational Molecular Biology and\nBioinformatics. Bioinformatics (2009) 25(11):1422 –3. doi: 10.1093/\nbioinformatics/btp163\n51. Rhee SY, Gonzales MJ, Kantor R, Betts BJ, Ravela J, Shafer RW. Human\nImmunode ﬁciency Virus Reverse Transcriptase and Protease Sequence\nDatabase. Nucleic Acids Res(2003) 31(1):298–303. doi: 10.1093/nar/gkg100\n52. Tunstall L. Fine-Tune for MultiClass or MultiLabel-MultiClass (2021).\nAvailable at: https://dis cuss.huggingface.co/t/ ﬁne-tune-for-multiclass-or-\nmultilabel-multiclass/4035/9 (Accessed 12/25/2021).\n53. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch\nBCELoss Documentation. Available at : https://pytorch.org/docs/stable/\ngenerated/torch.nn.BCELoss.html (Accessed 12/25/2021).\n54. Thomas Wolf LD, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, et al.\nHuggingFace's Transformers: State-Of-the-Art Natural Language Processing\n(2019). Available at: https://github .com/huggingface/transformers/blob/\nmaster/examples/pytorch/language -modeling/run_mlm.py (Accessed\n12/25/2021).\n55. Fouchier RA, Groenink M, Kootstra NA, Tersmette M, Huisman HG,\nMiedema F, et al. Phenotype-Associated Sequence Variation in the Third\nVariable Domain of the Human Immunodeﬁciency Virus Type 1 Gp120\nMolecule. J Virol(1992) 66(5):3183–7. doi: 10.1128/JVI.66.5.3183-3187.1992\n56. Beerenwinkel N, Schmidt B, Walter H, Kaiser R, Lengauer T, Hoffmann D,\net al. Diversity and Complexity of HIV-1 Drug Resistance: A Bioinformatics\nApproach to Predicting Phenotype From Genotype.Proc Natl Acad Sci USA\n(2002) 99(12):8271–6. doi: 10.1073/pnas.112177799\n57. Heider D, Senge R, Cheng W, Hullermeier E. Multilabel Classiﬁcation for\nExploiting Cross-Resistance Information in HIV-1 Drug Resistance\nPrediction. Bioinformatics (2013) 29(16):1946 – 52. doi: 10.1093/\nbioinformatics/btt331\n58. Steiner MC, Gibson KM, Crandall KA. Drug Resistance Prediction Using\nDeep Learning Techniques on HIV-1 Sequence Data.Viruses (2020) 12(5).\ndoi: 10.3390/v12050560\n59. van Marle G, Gill MJ, Kolodka D, McManus L, Grant T, Church DL.\nCompartmentalization of the Gut Viral Reservoir in HIV-1 Infected\nPatients. Retrovirology (2007) 4:87. doi: 10.1186/1742-4690-4-87\n60. Sturdevant CB, Dow A, Jabara CB, Joseph SB, Schnell G, Takamune N, et al.\nCentral Nervous System Compartmentalization of HIV-1 Subtype C Variants\nEarly and Late in Infection in Young Children.PloS Pathog (2012) 8(12):\ne1003094. doi: 10.1371/journal.ppat.1003094\n61. Fernandes JD, Faust TB, Strauli NB, Smith C, Crosby DC, Nakamura RL, et al.\nFunctional Segregation of Overlapping Genes in HIV. Cell (2016) 167\n(7):1762–73.e12. doi: 10.1016/j.cell.2016.11.031\n62. Antell GC, Dampier W, Aiamkitsumrit B, Nonnemacher MR, Pirrone V,\nZhong W, et al. Evidence of Divergent Amino Acid Usage in Comparative\nAnalyses of R5- and X4-Associated HIV-1 Vpr Sequences.Int J Genomics\n(2017) 2017:4081585. doi: 10.1155/2017/4081585\n63. Antell GC, Dampier W, Aiamkitsumrit B, Nonnemacher MR, Jacobson JM,\nPirrone V, et al. Utilization of HIV-1 Envelope V3 to Identify X4- and R5-\nSpeciﬁc Tat and LTR Sequence Signatures. Retrovirology (2016) 13(1):32.\ndoi: 10.1186/s12977-016-0266-9\nAuthor Disclaimer:The contents of the paper are solely the responsibility of the\nauthors and do not necessarily reﬂ\nect the ofﬁcial views of the NIH.\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Dampier, Link, Earl, Collins, De Souza, Koser, Nonnemacher and\nWigdahl. This is an open-access article distributed under the terms of the Creative\nCommons Attribution License (CC BY). The use, distribution or reproduction in other\nforums is permitted, provided the original author(s) and the copyright owner(s) are\ncredited and that the original publication in this journal is cited, in accordance with\naccepted academic practice. No use, distribution or reproduction is permitted which\ndoes not comply with these terms.\nDampier et al. HIV BERTology\nFrontiers in Virology | www.frontiersin.org May 2022 | Volume 2 | Article 88061814",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6552666425704956
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6491623520851135
    },
    {
      "name": "Human immunodeficiency virus (HIV)",
      "score": 0.5694898366928101
    },
    {
      "name": "Transformer",
      "score": 0.5523828864097595
    },
    {
      "name": "Autoencoder",
      "score": 0.5350538492202759
    },
    {
      "name": "Machine learning",
      "score": 0.5311350226402283
    },
    {
      "name": "Computational biology",
      "score": 0.4763493835926056
    },
    {
      "name": "Deep learning",
      "score": 0.463126003742218
    },
    {
      "name": "Standardization",
      "score": 0.44333210587501526
    },
    {
      "name": "Scalability",
      "score": 0.43446293473243713
    },
    {
      "name": "Biology",
      "score": 0.31665563583374023
    },
    {
      "name": "Virology",
      "score": 0.19611483812332153
    },
    {
      "name": "Engineering",
      "score": 0.14808061718940735
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I72816309",
      "name": "Drexel University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210156620",
      "name": "Institute for Molecular Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210106595",
      "name": "Sidney Kimmel Cancer Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I149251103",
      "name": "Thomas Jefferson University",
      "country": "US"
    }
  ]
}