{
    "title": "RegionViT: Regional-to-Local Attention for Vision Transformers",
    "url": "https://openalex.org/W3166942762",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4227183477",
            "name": "Chen Chun-fu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2753930136",
            "name": "Panda, Rameswar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751300816",
            "name": "Fan, Quanfu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W3035452548",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3139587317",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3159663321",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3165150763",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3153842237",
        "https://openalex.org/W3120885796",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2138011018",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2625366777",
        "https://openalex.org/W1977295328",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W3170227631",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3164540605",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2908510526"
    ],
    "abstract": "Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ a novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extract global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at https://github.com/ibm/regionvit.",
    "full_text": "arXiv:2106.02689v3  [cs.CV]  31 Mar 2022\nPublished as a conference paper at ICLR 2022\nRE G I O N VIT: R E G I O NA L -TO -L O C AL AT T E N T IO N F O R\nVI S I O N TR A N S F O R M E R S\nChun-Fu (Richard) Chen, Rameswar Panda, Quanfu Fan\nMIT -IBM W atson AI Lab\nchenrich@us.ibm.com, rpanda@ibm.com, qfan@us.ibm.com\nABSTRACT\nV ision transformer (V iT) has recently shown its strong capa bility in achieving\ncomparable results to convolutional neural networks (CNNs ) on image classiﬁca-\ntion. However, vanilla V iT simply inherits the same archite cture from the natural\nlanguage processing directly, which is often not optimized for vision applications.\nMotivated by this, in this paper, we propose a new architectu re that adopts the\npyramid structure and employ novel regional-to-local atte ntion rather than global\nself-attention in vision transformers. More speciﬁcally, our model ﬁrst generates\nregional tokens and local tokens from an image with differen t patch sizes, where\neach regional token is associated with a set of local tokens b ased on the spatial\nlocation. The regional-to-local attention includes two st eps: ﬁrst, the regional self-\nattention extracts global information among all regional t okens and then the local\nself-attention exchanges the information among one region al token and the asso-\nciated local tokens via self-attention. Therefore, even th ough local self-attention\nconﬁnes the scope in a local region but it can still receive gl obal information.\nExtensive experiments on four vision tasks, including imag e classiﬁcation, ob-\nject and keypoint detection, semantics segmentation and ac tion recognition, show\nthat our approach outperforms or is on par with state-of-the -art V iT variants in-\ncluding many concurrent works. Our source codes and models a re available at\nhttps://github.com/IBM/RegionViT.\n1 I NTRODUC TI ON\nTransformers ( V aswani et al. , 2017) based on self-attention come naturally with the ability to\nlearn long-range dependencies in sequential data. As impor tant as it is to language model-\ning ( Devlin et al. , 2019), such ability is also highly desired for many vision tasks w here contextual\nmodeling plays a signiﬁcant role. For this reason, self-att ention and transformers have been receiv-\ning an increasing attention in the vision community ( Bello, 2021; Srinivas et al. , 2021; Zhao et al. ,\n2020; Ramachandran et al. , 2019a; Bello et al. , 2019; Hu et al. , 2019; Ramachandran et al. , 2019b;\nW ang et al. , 2018). Especially, the recent V ision Transformers (V iT) ( Dosovitskiy et al. , 2021)\ndemonstrates comparable image classiﬁcation results agai nst the ﬁrmly established and prevalent\nCNNs in computer vision ( He et al. , 2016; T an & Le , 2019; Brock et al. , 2021), albeit relying on a\nhuge amount of training data. It has since then led to an explo sion of interest in further investigating\nits potential for a wide variety of vision applications ( Wu et al. , 2021; W ang et al. , 2021; Heo et al. ,\n2021; Zhang et al. , 2021a; Li et al. , 2021; Graham et al. , 2021; Liu et al. , 2021; Chu et al. , 2021a;\nY an et al. , 2021; Chen et al. , 2021b).\nThe V iT inherits the entire architecture from the vanilla tr ansformer ( V aswani et al. , 2017), which\nis designed for natural language processing tasks, and some of those designs thus may not meet the\nneeds of vision tasks. For example, the transformer has an is otropic network structure with a ﬁxed\nnumber of tokens and unchanged embedding size, which loses t he capability to model the context\nwith different scales and allocates computations at differ ent scales. As opposed to this, a majority\nof CNNs adopt a popular pyramid architecture to compute mult i-scale features efﬁciently. Recent\nvision transformers such as PVT ( W ang et al. , 2021) and PiT ( Heo et al. , 2021) also follow a similar\npyramid structure as CNNs, showing improvement on both comp utation and memory efﬁciency as\nwell as on model accuracy. Another critical bottleneck of th e transformer is that the self-attention\nmodule has a quadratic cost in memory and computation with re gard to the sequence length (i.e., the\n1\nPublished as a conference paper at ICLR 2022\nnumber of tokens). This issue is even worse in V iT as images ar e 2-D, suggesting a quadratic rela-\ntionship between the number of tokens and image resolution. As a result, V iT indicates a quadruple\ncomplexity w .r.t image resolution. The highly compute- and memory-intensive self-attention makes\nit challenging to train vision transformer models at ﬁne-gr ained patch sizes. It also signiﬁcantly\nundermines the applications of these models to tasks such as object detection and semantic seg-\nmentation, which beneﬁt from or require ﬁne feature informa tion computed from high-resolution\nimages.\nOther arrows are \nignored for simplicity. \n(a) ViT (b) PVT (c) RegionViT (Ours) \n16 ⨉16 \n16 ⨉16 \n16 ⨉16 \n32 ⨉32 \n16 ⨉16 \n8⨉8\n32 ⨉32 \n16 ⨉16 \n8⨉8\n64 ⨉64 \n32 ⨉32 \n16 ⨉16 \nFigure 1: Regional-to-Local Attention\nfor Vision T ransformers. (a) ViT uses\na ﬁxed patch size through the whole net-\nwork, (b) PVT adopts a pyramid structure to\ngradually enlarge the patch size in the net-\nwork. Both ViT and PVT uses all tokens\nin self-attention, which are computational-\nand memory-intensive. (c) Our proposed ap-\nproach combines a pyramid structure with\nan efﬁcient regional-to-local (R2L) attention\nmechanism to reduce computation and mem-\nory usage. Our approach divides the input\nimage into two groups of tokens, regional to-\nkens of large patch size (red) and local ones\nof small patch size (black). The two types of\ntokens communicate efﬁciently through R2L\nattention, which jointly attends to the local\ntokens in the same region and the associated\nregional token. Note that the numbers de-\nnote the patch sizes at each stage of a model.\nT o address the aforementioned computational limitations\nof vision transformers, in this work, we develop a\nmemory-friendly and efﬁcient self-attention method for\ntransformer models to reach their promising potential for\nvision applications. W e propose a novel coarse-to-ﬁne\nmechanism to compute self-attention in a hierarchical\nway. Speciﬁcally, our approach ﬁrst divides the input im-\nage into a group of non-overlapping patches of large size\n(e.g., 28 × 28), on which regional tokens are computed via\nlinear projection. Similarly, local tokens are created for\neach region using a smaller patch size (e.g., 4 × 4). W e\nthen use a standard transformer to process regional and lo-\ncal tokens separately. T o enable communication between\nthe two types of tokens, we ﬁrst perform self-attention\non regional tokens ( regional attention ) and then jointly\nattend to the local tokens of each region including their\nassociated regional token ( local attention ). By doing so,\nregional tokens pass global contextual information to lo-\ncal tokens efﬁciently while being able to effectively learn\nfrom local tokens themselves. For clarity, we represent\nthis two-stage attention mechanism as Regional-to-Local\nAttention, or R2L attention for short (see Figure\n1 for an\nillustration). Since both regional and local attention in-\nvolve much fewer tokens, our R2L attention requires sub-\nstantially less memory than regular global self-attention\nused in vision transformers. For example, in our default\nsetting, the memory saving using R2L attention can be up to as much as 73%. W e demonstrate the\neffectiveness of our approach on image classiﬁcation and se veral downstream vision tasks including\nobject detection and action recognition.\nT o summarize, our key contributions in this work are as follo ws:\n1. W e propose a new vision transformer (RegionV iT) based on r egional-to-local attention to learn\nboth local and global features. Our proposed regional-to-l ocal attention alleviates the overhead\nof standard global attention (too many tokens) and the weakn ess of pure local attention (no inter-\naction between regions) used in existing vision transforme rs.\n2. Our regional-to-local attention reduces the memory comp lexity signiﬁcantly as compared to stan-\ndard self-attention, leading to a savings in memory complex ity by about O(N/M 2), where M is\nthe window size of a region and N is the total number of tokens. This effectively allows us to\ntrain a more deep network for better performance while with c omparable complexity.\n3. Our models outperform or on par with several concurrent wo rks on vision transformer that exploit\npyramid structure for image classiﬁcation. Experiments al so demonstrate that our models work\nwell on several downstream classiﬁcation tasks, including object detection and action recognition.\n2 R ELATED WORK\nCNNs with Attention. Attention has been widely used for enhancing the features in CNNs,\ne.g., SENet (\nHu et al. , 2018) and CBAM ( W oo et al. , 2018). V arious methods have been pro-\nposed that combine self-attention with CNNs ( Bello, 2021; Srinivas et al. , 2021; Zhao et al. ,\n2020; Ramachandran et al. , 2019a; Bello et al. , 2019; Hu et al. , 2019; Ramachandran et al. , 2019b;\n2\nPublished as a conference paper at ICLR 2022\nT able 1: Comparison to related works. Most works use non-overlapped windows to group tokens, and t hen\npropose the corresponding methods to assure the informatio n exchange among regions.\nMethods Structure Attention T ypes Windows of\nLocal Attention\nHandling Non-overlapped\nRegions Global T okens\nV iT ( Dosovitskiy et al. , 2021) Isotropic Global N/A N/A N/A\nPVT ( W ang et al. , 2021) Pyramid Global N/A N/A N/A\nSwin ( Liu et al. , 2021) Pyramid Local Strictly Non-overlapped Shifting the windo w None\nV iL ( Zhang et al. , 2021a) Pyramid Local + Global Overlapped N/A 1 One learnable token\nT wins (Chu et al. , 2021a) Pyramid Local + Global Non-overlapped GA Subsampled from l ocal tokens\nRegionV iT (Ours) Pyramid Local + Regional Non-overlapped R egional-to-local attention Regional tokens\nGA: global attention. 1: not applicable as ViL used overlapped windows.\nW ang et al. , 2018). E.g., SAN ( Zhao et al. , 2020) and SASA ( Ramachandran et al. , 2019a) at-\ntempts to replace all convolutional layers with local self- attention network. While these works\nshow promising results, their complexity is relatively hig h due to the self-attention. On the other\nhand, BoTNet ( Srinivas et al. , 2021) replaces few convolutional layers with a slightly-modiﬁe d self-\nattention to balance computation and accuracy. LambdaNet w ork ( Bello, 2021) uses the approxi-\nmated self-attention to reduce the overhead of self-attent ion and make the network efﬁcient. Few\nworks use attention approach to deﬁne the regional of intere st for the ﬁne-grained visual recogni-\ntion ( Zheng et al. , 2020; Ding et al. , 2021; Wharton et al. , 2021). In contrast, our model utilizes\nregional-to-local attention to alleviate the workload of g lobal self-attention by local attention while\nstill keeping the global information via regional attentio n.\nVision T ransformer . V iT ( Dosovitskiy et al. , 2021) has recently achieved comparable results to\nCNNs when using a huge amount of data, e.g., JFT300M ( Sun et al. , 2017). DeiT ( T ouvron et al. ,\n2020; Wightman, 2019) subsequently proposes an efﬁcient training scheme that al lows vision trans-\nformer to work on par with CNNs while training only on ImageNe t1K ( Deng et al. , 2009). Despite\npromising performance of V iT , the architecture is directly borrowed from natural language process-\ning, which might not be suitable for vision applications. Mo tivated by this, two main line of research\nworks have been developed to improve V iT . One is to enhance di fferent components of the vision\ntransformer ( Y uan et al. , 2021; Han et al. , 2021; T ouvron et al. , 2021; Jiang et al. , 2021) while still\nusing isotropic structure (i.e., ﬁxed token numbers and cha nnel dimension) like V iT , e.g., while T2T -\nV iT (Y uan et al. , 2021) introduces a T okens-to-T oken (T2T) transformation to enc ode local structure\nfor each token, CrossV iT propose a dual-path architecture, each with different scales, to learn multi-\nscale features ( Chen et al. , 2021a). CaiT ( T ouvron et al. , 2021) proposes a layer-scalar to train a\ndeeper network for better performance and L V -V iT ( Jiang et al. , 2021) modiﬁed how the model is\ntrained when CutMix ( Y un et al. , 2019) augmentation is applied on V iT . Another parallel thread fo r\nimproving vision transformer is in incorporating CNN-like pyramid structure into V iT ( Wu et al. ,\n2021; W ang et al. , 2021; Heo et al. , 2021; Zhang et al. , 2021a; Li et al. , 2021; Graham et al. , 2021;\nLiu et al. , 2021; Chu et al. , 2021a; Y an et al. , 2021; Chen et al. , 2021b). PVT ( W ang et al. , 2021)\nand PiT ( Heo et al. , 2021) introduce the pyramid structure into V iT , which makes them more\nsuitable for objection detection as it can provide multi-sc ale features. LocalV iT ( Li et al. , 2021)\nand ConT ( Y an et al. , 2021) mix convolutions with self-attention to encode locality i nformation.\nSwin ( Liu et al. , 2021), V iL ( Zhang et al. , 2021a) and T wins ( Chu et al. , 2021a) limited the self-\nattention into a local region and then propose different met hods to allow the interaction among each\nlocal region. Our model also utilizes pyramid structure and limits the self-attention in a local region;\nwhile we propose the regional-to-local attention to exchan ge the information among each region\nefﬁciently.\nDifference from PVT , Swin, ViL and T wins. T able\n1 summarizes the difference between our\nproposed approach, RegionV iT with the closely related work s. PVT ( W ang et al. , 2021) uses pyra-\nmid structure but the scope of self-attention is still globa l. While V iL ( Zhang et al. , 2021a) limits\nself-attention into a local region, they adopt overlapping windows as convolutions to process all the\ntokens. On the other hand, Swin ( Liu et al. , 2021), T wins ( Chu et al. , 2021a) and ours adopt non-\noverlapped windows. While Swin shifts the position of windows alternat ively in the consecutive\ntransformer encoder to allow the interaction between regio ns, T wins subsamples local tokens as the\nglobal tokens, and then use the global tokens as the keys for s elf-attention to achieve the interac-\ntion between regions. By contrast, our proposed method util izes a extra set of regional tokens with\nregional-to-local attention to perform self-attention on regional tokens only for the global informa-\ntion and then each regional token is sent to the associated lo cal tokens to pass the global information.\nOne major difference from others is that the local self-atte ntion in our approach includes one extra\nregional token to exchange the global information with loca l tokens.\n3\nPublished as a conference paper at ICLR 2022\nR2L \nTransformer \nEncoder \nC⨉H/4 ⨉W/4 \nInput \nTokenization \nTokenization \nC⨉H/28 ⨉W/28 \nC⨉H/4 ⨉W/4 \nC⨉H/28 ⨉W/28 \nDownsamp \nling \nR2L \nTransformer \nEncoder \n2C ⨉H/8 ⨉W/8 \n2C ⨉H/56 ⨉W/56 \nDownsamp \nling \nR2L \nTransformer \nEncoder \n4C ⨉H/16 ⨉W/16 \n4C ⨉H/112 ⨉W/112 \nDownsamp \nling \nR2L \nTransformer \nEncoder \n8C ⨉H/32 ⨉W/32 \n8C ⨉H/224 ⨉W/224 \nPrediction \nFC \nStage: 4 Stage: 3 Stage: 2 Stage: 1 Regional \ntokens \nLocal \ntokens \nFigure 2: Architecture of the proposed RegionViT . T wo paths are in our proposed network, including\nregional tokens and local tokens, and their information is e xchanged in the regional-to-local (R2L) transformer\nencoders. In the end, we average all regional tokens and use it for the classiﬁcation. The tensor shape here is\ncomputed based on that regional tokens take a patch of 282 and local tokens take a patch of 42.\n3 M ETHOD\nOur method is built on top of a vision transformer, so we ﬁrst p resent a brief background of V iT and\nthen describe our method (RegionV iT) on regional-to-local attention for vision transformers.\nVision T ransformer . As opposed to CNN-based approaches for image classiﬁcation , V iT is a purely\nattention-based counterpart, borrowed from NLP . It consis ts of stacked transformer encoders, each\nof which contains a multihead self-attention (MSA) and a fee d-forward network (FFN) with layer\nnormalization and residual shortcut. T o classify an image, V iT ﬁrst splits it into patches of ﬁxed size,\ne.g. 16 × 16, and then transforms them into tokens by linear projectio n. A class token is additionally\nprepended to the patch tokens to form the input sequence. Bef ore the token are fed into transformer\nencoders, a learnable absolute positional embedding is add ed to each token to learn the position\ninformation. At the end of the network, the class token is use d as the ﬁnal feature representation for\nclassiﬁcation. Mathematically, V iT can be expressed as\nx0 = [xcls||xpatch] +xpos\nyk = xk− 1 + MSA(LN(xk− 1)), xk = yk + FFN(LN(yk)), (1)\nwhere xcls ∈ R1× C and xpatch ∈ RN× C are the class token and patch tokens respectively and\nxpos ∈ R(1+N)× C is the position embedding, and [ ||] denotes the tensor concatenation. k, N and\nC are the layer index, the number of patch tokens and dimension of the embedding, respectively.\nWhile the vanilla V iT is ideally capable of learning global i nteraction among all the patch tokens, the\nmemory complexity of self-attention becomes high when ther e are many tokens as the complexity\nis quadratically linear to the length of the input sequence. Moreover, the use of isotropic structure\nlimits the capability of extending the vanilla V iT model to m any vision applications that require\nhigh-resolution details, e.g., object detection. T o addre ss these issues, we propose regional-to-local\nattention for vision transformer, RegionV iT for short.\nAn Overview of Our Proposed Approach (RegionViT). Figure 2 illustrates the architecture of the\nproposed vision transformer, which consists of two tokeniz ation processes that convert an image into\nregional (upper path) and local tokens (lower path). Each tokenization is a convolution wit h different\npatch sizes, e.g., in Figure 2, the patch size of regional tokens is 282 while 42 is used for local tokens\nwith dimensions projected to C, which means that one regional token covers 72 local tokens based\non the spatial locality, leading to the window size of a local region to 72. At stage 1, two sets of\ntokens are passed through the proposed regional-to-local t ransformer encoders. However, for the\nlater stages, to balance the computational load and to have f eature maps at different resolutions, we\ndeploy a downsampling process to halve the spatial resoluti on while doubling the channel dimension\nlike CNN on both regional and local tokens before going to the next stage. Finally, at the end of\nthe network, we average the remaining regional tokens as the ﬁnal embedding for the classiﬁcation\nwhile the detection uses all local tokens at each stage since it provides more ﬁne-grained location\ninformation. By having the pyramid structure, the V iT can ge nerate multi-scale features and hence\nit could be easily extended to more vision applications, e.g ., object detection, rather than image\nclassiﬁcation only. W e explain the main components of the re gional-to-local transformer encoder in\nthe next section.\nRegional-to-Local T ransformer Encoder . Figure\n3 shows the proposed regional-to-local (R2L)\ntransformer encoder which includes R2L attention and feed- forward network ( FFN). Speciﬁcally,\nwithin R2L attention, the regional self-attention (RSA) ﬁr st involves all regional tokens to learn the\nglobal information efﬁciently as the number of regional tok ens is few and then local self-attention\n(LSA) takes the local tokens with the associated regional to ken to learn local feature while the\n4\nPublished as a conference paper at ICLR 2022\ninvolved regional token could provide global information a t the same time. Both RSA and LSA\nare multihead self-attention (MSA) but with different inpu t tokens. Finally, the FFN is applied to\nenhance the features. W e also add layer normalization ( LN) and residual shortcuts as in standard\ntransformer encoders. Mathematically, given the regional and local tokens, xd− 1\nr and xd− 1\nl as the\ninputs at layer d, the R2L transformer encoder can be expressed as:\nyd\nr = xd− 1\nr + RSA(LN(xd− 1\nr )), yd\ni,j = [yd\nri,j ||{xd− 1\nli,j,m,n}m,n∈ M ]\nzd\ni,j = yd\ni,j + LSA(LN(yd\ni,j)), xd\ni,j = zd\ni,j + FFN(LN(zd\ni,j))\n(2)\nwhere i, j are the spatial index with respect to regional tokens while m, n are the index of local\ntoken in the window size M2. Note that the input of LSA, yd\ni,j, includes one regional token and cor-\nresponding local tokens, and thus, the information between local and regional tokens are exchanged;\non the other hand, the outputs, xd\nr and xd\nl , can be extracted from xd\ni,j like the top-right equation.\nRSA \nRegional Tokens \nLocal Tokens \nFeed-Forward \nNetwork \nRSA: Regional Self-Attention \nLSA: Local Self-Attention \nLSA \nR2L Attention \nFigure 3: Illustration of Regional-to-Local (R2L) T ransformer\nEncoder. All regional tokens are ﬁrst passed through regional\nself-attention (RSA) to exchange the information among reg ions\nand then local self-attention (LSA) performs parallel self -attention\nwhere each takes one regional token and corresponding local to-\nkens. After that, all the tokens are passed through the feed- forward\nnetwork and split back to the regional and local tokens. RSA a nd\nLSA share the same weights.\nThe RSA exchanges information\namong all tokens, which covers the\ncontext of the whole image; while the\nLSA combines the features among\nthe tokens belonging to the spatial\nregion, including both regional and\nlocal tokens. Since the regions are\ndivided by non-overlapped windows,\nthe RSA is also designed to exchange\nthe information among regions where\nthe LSA takes one regional token and\nthen combines with it the local tokens\nin the same region. In such a case, all\nlocal tokens are still capable of get-\nting global information while being\nmore focused on local neighbors. It\nis worth noting that the weights are\nshared between RSA and LSA except for the layer normalizatio n; therefore, the number of param-\neters won’t increase signiﬁcantly when compared to the stan dard transformer encoder. With these\ntwo attentions, the R2L can effectively and efﬁciently exch ange information among all regional and\nlocal tokens. In particular, the self-attention on regiona l tokens aims to extract high-level informa-\ntion and act as a bridge to pass information of local tokens fr om one region to other regions. On the\nother hand, the R2L attention focus on local contextual info rmation with one regional token.\nMoreover, the R2L transformer encoder can further reduce th e memory complexity signiﬁcantly.\nThe memory complexity of a self-attention is O(N2), where N is the number of local tokens. When\na region contains M local tokens, and there are N/M regions, the memory complexity is O((M +\n1)2 × (N/M )); while the complexity from the attention on regional tokens is O((N/M )2). Hence,\nthe overall complexity becomes O((M + 1)2 × (N/M ) + (N/M )2). E.g., the main component\nof RegionV iT is stage 3, where M is 49 and N is 196; therefore, the memory complexity saving is\n∼ 73%. The saving of each model and each stage is varied based on the model conﬁguration.\nRelative Position Bias. The locality is an important clue for understanding visual c ontent; therefore,\ninstead of adding absolute position embedding used by the va nilla V iT , we introduce the relative\nposition bias into the attention map of R2L attention since r elative position between patches (or\npixels) are more important than the absolution position as o bjects in the images can be placed in an\narbitrary way (\nRamachandran et al. , 2019a; Liu et al. , 2021). W e only add this bias to the attention\nbetween local tokens and not the attention between regional tokens and local tokens. Speciﬁcally,\nfor a given pair of local tokens at location, (xm, y m), (xn, y n), the attention value a can be expressed\nas\na(xm,ym),(xn,yn) = softmax\n(\nq(xm,ym)kT\n(xn,yn) + b(xm− xn,ym− yn)\n)\n, (3)\nwhere b(xm− xn,ym− yn) is taken from a learnable parameter B ∈ R2M− 1× 2M− 1, where M is the\nwindow size of a region. The ﬁrst term q(xm,ym)kT\n(xn,yn) is the attention value based on their content.\n5\nPublished as a conference paper at ICLR 2022\nT able 2: Model architectures of RegionViT. For all networks, the dim ension per head is 32 and the expanding\nratio r in FFN is 4. The patch size of local tokens is always 4 while the patch size of regional tokens is 4 × M.\nModel T okenization Window size ( M) Dimension ( C) # of Encoders\nLocal Regional at each stage\nRegionV iT-Ti 3-conv linear 7 {64, 128, 256, 512 } { 2, 2, 8, 2 }\nRegionV iT-S 3-conv linear 7 {96, 192, 384, 768 } { 2, 2, 8, 2 }\nRegionV iT-M 1-conv linear 7 {96, 192, 384, 768 } { 2, 2, 14, 2 }\nRegionV iT-B 1-conv linear 7 {128, 256, 512, 1024 } { 2, 2, 14, 2 }\n1-conv: Conv(out=C, k=8, s=4, p=3). 3-conv: Conv(out=C/4, k=3, s=2, p=1) → Conv(out=C/2, k=3, s=2, p=1) →\nConv(out=C, k=3, s=1, p=1), where C is the channel dimension of the ﬁrst block. LayerNorm and GeLU are added between Conv .\nWith relative position bias, our model does not require the a bsolute positional embedding as vanilla\nV iT since this relative position bias helps to encode the pos ition information.\nDownsampling. The spatial and channel dimension are changed along differe nt stages of our net-\nwork by simply applying 3 × 3 depth-wise convolution with stride 2 for halving the resolu tion and\ndoubling the channel dimension for both regional and local t okens (weights are shared) ( Heo et al. ,\n2021). W e also test with regular convolutions but it does not impr ove the accuracy with increased\ncomplexity.\nInput T okenization. The tokenization for local tokens can be implemented by usin g a 4 × 4 con-\nvolution with channel size C and stride 4 (i.e. non-overlapped). However, as pointed out in\nT2T (\nY uan et al. , 2021), CrossV iT ( Chen et al. , 2021a), and PiT ( Heo et al. , 2021), using a stronger\nbut still simple subnetwork for the tokenization could furt her improve the performance, especially\nfor the smaller models. Thus, we adopt two input tokenizatio ns in our model, one still contains only\none convolutional layer and another one contains three conv olutional layers (see T able 2 for details).\nRegional T okens. W e adopt a simple approach to generate regional tokens, that is, using linear\nprojection with larger patch sizes in comparison to the patc h size for the local tokens. E.g., as shown\nin Figure 2, the patch size for local tokens is 4 × 4, we simply use patch size 28 × 28 to split the image\nand then project each patch linearly to generate regional to kens.\nT able 2 shows our RegionV iT models with different conﬁgurations. B y default, the window size is\n7 while we also experiment with a larger window size (14), tho se models are annotated with +.\n4 E XPERIME NT S\n4.1 I M AG E CL A S S IFICAT IO N\nDatasets. W e use ImageNet1K ( Deng et al. , 2009) (IN1K) and ImageNet21K ( Deng et al. , 2009)\n(IN21K) to validate our method. ImageNet1K contains 1.28 mi llion training images and 50k vali-\ndation images over 1k classes, and ImageNet21K is a large-sc ale dataset that consists of around 14\nmillion images over 21,841 classes. W e use all images for tra ining and then ﬁnetune the model on\nImageNet1K. Moreover, we also perform the transfer learnin g from ImageNet1K to ﬁve downstream\ndatasets, including CIF AR10 ( Krizhevsky et al. , 2009), CIF AR100 ( Krizhevsky et al. , 2009), II-\nIPets ( Parkhi et al. , 2012), StandfordCars ( Krause et al. , 2013) and ChestXRay8 ( W ang et al. , 2017).\nT able 3: Comparisons with recent pyramid-like structure-based ViT models on ImageNet1K. The bold num-\nbers indicate the best number within each section.\nModel Params (M) FLOPs (G) Acc. (%)\nPiT -XS (Heo et al. , 2021) 10.6 1.4 78.1\nConT -S (Y an et al. , 2021) 10.1 1.5 76.5\nPVT -T (W ang et al. , 2021) 13.2 1.9 75.1\nConV iT -Ti+ (d’Ascoli et al. , 2021) 10.0 2.0 76.7\nT wins-SVT -S (Chu et al. , 2021a) 24.0 2.8 81.7\nPiT -S (Heo et al. , 2021) 23.5 2.9 80.9\nConT -M (Y an et al. , 2021) 19.2 3.1 80.2\nT wins-PCPVT -S (Chu et al. , 2021a) 24.1 3.7 81.2\nPVT -S (W ang et al. , 2021) 24.5 3.8 79.8\nRegionV iT-Ti 13.8 2.4 80.4\nRegionV iT-Ti+ 14.3 2.7 81.5\nDeiT -S (T ouvron et al. , 2020) 22.1 4.6 79.9\nCvT -13 (Wu et al. , 2021) 20.0 4.5 81.6\nSwin-T ( Liu et al. , 2021) 29.0 4.5 81.3\nLocalV iT -S (Li et al. , 2021) 22.4 4.6 80.8\nV iL-S ( Zhang et al. , 2021a) 24.6 4.9 82.0\nV isformer-S (Chen et al. , 2021b) 40.2 4.9 82.3\nConV iT -S (d’Ascoli et al. , 2021) 27.0 5.4 81.3\nNesT -S (Zhang et al. , 2021b) 17.0 5.8 81.5\nRegionV iT-S 30.6 5.3 82.6\nRegionV iT-S+ 31.3 5.7 83.3\nModel Params (M) FLOPs (G) Acc. (%)\nConT -M (Y an et al. , 2021) 39.6 6.4 81.8\nT wins-PCPVT -B (Chu et al. , 2021a) 43.8 6.4 82.7\nPVT -M (W ang et al. , 2021) 44.2 6.7 81.2\nCvT -21 (Wu et al. , 2021) 32.0 7.1 82.5\nT wins-SVT -B (Chu et al. , 2021a) 56 8.3 83.2\nV iL-M ( Zhang et al. , 2021a) 39.7 8.7 83.3\nSwin-S ( Liu et al. , 2021) 50.0 8.7 83.0\nPVT -L (W ang et al. , 2021) 61.4 9.8 81.7\nConV iT -S+ (d’Ascoli et al. , 2021) 48.0 10.0 82.2\nNesT -S (Zhang et al. , 2021b) 38.0 10.4 83.3\nRegionV iT-M 41.2 7.4 83.1\nRegionV iT-M+ 42.0 7.9 83.4\nDeiT -B (T ouvron et al. , 2020) 86.6 17.6 81.8\nPiT -B (Heo et al. , 2021) 73.8 12.5 82.0\nV iL-B ( Zhang et al. , 2021a) 55.7 13.4 83.2\nT wins-SVT -L (Chu et al. , 2021a) 99.2 14.8 83.7\nSwin-B ( Liu et al. , 2021) 88.0 15.4 83.5\nConV iT -B (d’Ascoli et al. , 2021) 86.0 17.0 82.4\nNesT -B (Zhang et al. , 2021b) 68.0 17.9 83.8\nRegionV iT-B 72.7 13.0 83.2\nRegionV iT-B+ 73.8 13.6 83.8\n6\nPublished as a conference paper at ICLR 2022\nT able 4: Results on IN1K with IN21K and transfer learning.\n(a) IN1K results with IN21K.\nModel Params (M) FLOPs (G) Acc. (%)\nV iL-B 55.7 43.7 86.0\nSwin-L 197.0 103.9 87.3\nCvT -W24 277 193.2 87.7\nRegionV iT-B+ 76.5 42.6 86.5\n(b) Transfer learning on downstream tasks.\nCIF AR10 CIF AR100 Pet StanfordCars ChestXRay8\nDeiT -S 99.1 90.9 94.9 91.5 55.4\nDeiT -B 99.1 90.8 94.4 91.7 55.8\nRegionV iT-S 98.9 90.0 95.3 92.8 57.8\nRegionV iT-M 99.0 90.8 95.5 91.9 58.3\nT able 5: Object detection performance on MS COCO val2017 with 1 × and 3 × schedule. The bold number\nindicates the best number within the section, and for MaskRC NN, both AP b and AP m are annotated.\nParams FLOPs RetineNet Params FLOPs MaskRCNN\n1× 3× 1× 3×\nBackbone (M) (G) AP AP (M) (G) AP b AP m AP b AP m\nResNet50 37.7 234 36.3 39.0 44.2 260.0 38.0 34.4 41.0 37.1\nConT -M (Y an et al. , 2021) 27.0 217.2 39.3 − − − 40.5 38.1 − −\nPVT -S (W ang et al. , 2021) 34.2 − 40.4 42.2 44.1 − 40.4 37.8 43.0 39.6\nV iL-S ( Zhang et al. , 2021a) 35.7 252.2 41.6 42.9 45.0 174.3 41.8 38.5 43.4 39.6\nSwin-T ( Liu et al. , 2021) 38.5 245 41.5 43.9 47.8 264 42.2 39.1 46.0 41.6\nT wins-SVT -S (w/ PEG) ( Chu et al. , 2021a) 34.3 209 43.0 45.6 44.0 228 43.5 40.3 46.8 42.6\nRegionV iT-S 40.8 192.6 42.2 45.8 50.1 171.3 42.5 39.5 46.3 42.3\nRegionV iT-S+ 41.5 204.2 43.1 46.9 50.9 182.9 43.5 40.4 47.3 43.4\nRegionV iT-S+ w/ PEG 41.6 204.3 43.9 46.7 50.9 183.0 44.2 40.8 47.6 43.4\nResNet101 56.7 315 38.5 40.9 63.2 336 40.4 36.4 42.8 38.5\nResNeXt101-32x4d 56.4 319.1 39.9 41.4 62.8 340 41.9 37.5 44.0 39.2\nPVT -M (W ang et al. , 2021) 53.9 − 41.9 43.2 63.9 − 42.0 39.0 44.2 40.5\nV iL-M ( Zhang et al. , 2021a) 50.8 338.9 42.9 43.7 60.1 261.1 43.4 39.7 44.6 40.7\nSwin-S ( Liu et al. , 2021) 59.8 335 44.5 46.3 69.1 354 44.8 40.9 47.6 42.8\nT wins-SVT -B (w/ PEG) ( Chu et al. , 2021a) 67.0 322 45.3 46.9 76.3 340 45.2 41.5 48.0 43.0\nRegionV iT-B 83.4 308.9 43.3 46.1 92.2 287.9 43.5 40.1 47.2 43.0\nRegionV iT-B+ 84.4 328.1 44.2 46.9 93.2 307.1 44.5 41.0 48.1 43.5\nRegionV iT-B+ w/ PEG 84.5 328.2 44.6 46.9 93.2 307.2 45.4 41.6 48.3 43.5\nResNeXt101-64x4d 95.5 473 41.0 − 101.9 493 42.8 38.4 − −\nPVT -L (W ang et al. , 2021) 71.1 345 42.6 − 81.0 364 42.9 39.5 − −\nV iL-B ( Zhang et al. , 2021a) 66.7 443.0 44.3 44.7 76.1 365.1 45.1 41.0 45.7 41.3\nSwin-B ( Liu et al. , 2021) 98.4 477 44.7 − 107.2 496 45.5 41.3 − −\nT wins-SVT -L (w/ PEG) ( Chu et al. , 2021a) 110.9 455 45.7 − 119.7 474 45.9 41.6 − −\nRegionV iT-B+ w/ PEG † 84.5 506.4 46.1 48.2 93.2 464.4 46.3 42.4 49.2 44.5\nThe reported results of Swin are from T wins as the original pa per does not include resutls with ImageNet1K weights. †: input resolution is 896 × 1344.\nT raining and Evaluation. W e follow DeiT ( T ouvron et al. , 2020) to train our models on IN1K\nexcept that we use batch size 4,096 with a base learning rate 0 .004 and the warm-up epochs\nis 50. W e adopt the AdamW ( Loshchilov & Hutter , 2019) optimizer with cosine learning rate\nscheduler ( Loshchilov & Hutter , 2017). W e apply Mixup ( Zhang et al. , 2018), CutMix ( Y un et al. ,\n2019), RandomErasing ( Zhong et al. , 2020), label smoothing ( Szegedy et al. , 2016), RandAug-\nment ( Cubuk et al. , 2020) and instance repetition ( Hoffer et al. , 2020). During training, we random\ncropped a 224 × 224 region and take a 224 × 224 center crop after resizing the shorter side to 256 for\nevaluation. W e used a similar setting for IN21K and transfer learning, and more details can be found\nin Section A.1.\nResults on ImageNet1K. T able 3 shows the results on ImageNet1K where the methods listed all\nadopt CNN-like pyramid structure into the V iT and are all ver y recent and concurrent works. For\nthe smaller models (Ti, S), RegionV iT achieves a better trad e-off between accuracy and complexity\n(parameters or FLOPs); on the other hand, for the larger mode ls (M and B), it obtains better accuracy\nwhile having fewer FLOPs and parameters. The efﬁciency of Re gionV iT comes from the proposed\nR2L transformer encoder, and hence with such efﬁciency, it e nables the network to be wide and deep\nfor better accuracy with comparable complexity.\nResults on ImageNet21K. T able\n4a shows that our model achieves a good accuracy-efﬁciency tra de-\noff with ImageNet21K for pretraining. RegionV iT-B+ only ne eds 25% parameters and FLOPs com-\npared to CvT -W24 ( Wu et al. , 2021) and half parameters and FLOPs to Swin-L ( Liu et al. , 2021).\nResults on T ransfer Learning. T able 4b shows the transfer learning results with ImageNet1K\npretrained weights. Our model outperforms DeiT ( T ouvron et al. , 2020) by 2 ∼ 3% on ChestXRay8,\nwhich has a larger domain gap from ImageNet1K than other four datasets. W e think this is because\nthe hierarchical feature models could provide better gener alization for the domain gap compared to\nDeiT which uses isotropic spatial resolution throughout th e whole network.\n7\nPublished as a conference paper at ICLR 2022\n4.2 O BJ E CT DE T E CT IO N A N D SE M A N T IC SE G M E N TAT IO N\nDataset. W e use MS COCO 2017 to validate our models ( Lin et al. , 2014) on object detection.\nCOCO 2017 dataset contains 118K images for training and 5K im ages for validation across 80\ncategories. W e also test our model on keypoint detection and results are shown in Sec. B. On the\nother hand, we validate our model with semantic segmentatio n on ADE20K ( Zhou et al. , 2017). The\nADE20K dataset contains 20k images in the training set, 2k im ages for validation.\nT raining and Evaluation. For object detection, we adopt RetinaNet ( Lin et al. , 2017) and MaskR-\nCNN ( He et al. , 2017) as our detection framework. W e simply replace the backbone with RegionV iT ,\nand then output the local tokens at each stages as multi-scal e features for the detector. The regional\ntokens are not used in the detection. Before sending the feat ures to the detector framework, we add\nnew layer normalization layers to normalize the features. W e use RegionV iT-S and RegionV iT-B\nas the backbone and the weights are initialized from ImageNe t1K pretraining. The shorter side and\nlonger side of the input image is resized to 672 and 1,120, res pectively. W e train our models based\non the settings in 1 × and 3 × schedule in Detectron2 ( Wu et al. , 2019) for object detection. More\ntraining details could be found in A.2. For semantic segmentation, we adopt Semantic FPN as the\nframework ( Kirillov et al. , 2019) and use RegionV iT as the backbone. W e initialize models wit h Im-\nageNet1K weights, and mostly follow the training receipt in T wins’ paper ( Chu et al. , 2021a). More\ntraining details can be found in Sec. A.3.\nResults on Object Detection. T able 5 compares RegionV iT with other methods. For the smaller\nmodels, RegionV iT-S and RegionV iT-S+ achieve similar accu racy while being moderately efﬁcient\nin terms of FLOPs. W e ﬁnd that the position encoding generato r (PEG) proposed by ( Chu et al. ,\n2021b) could help the detection accuracy substantially, so we als o provide the results with PEG,\nwhich are also used in T wins ( Chu et al. , 2021a). For the middle size models with RetinaNet,\nRegionV iT-B are slightly worse than T wins with 1 × schedule; however, our model catches up\nthe performance with 3 × schedule. When comparing the large models under similar FLO Ps\n(RegionV iT-B+ w/ PEG †), we further train our model with the similar resolution use d by other\nworks, and observe that our models outperform all others whi le being more parameter efﬁcient\n(models marked with † in T able 5).\nT able 6: Performance on semantic segmentation.\nModel FLOPs (G) Params (M) mIoU (%)\nSwin-T∗ 46 31.9 41.5\nT wins-SVT -S 37 28.3 43.2\nRegionV iT-S+ (w/ PEG) 37 35.7 45.3\nSwin-S∗ 70 53.2 45.2\nT wins-SVT -B 67 60.4 45.3\nRegionV iT-B+ (w/ PEG) 67 78.3 47.5\n∗ : Numbers are cited from the reproduced results of T wins’ pap er.\nResults on Semantic Segmentation. T able 6 com-\npares ours to Swin ( Liu et al. , 2021) and T wins. As\nPEG is useful for object detection, we show the\nresults with PEG. Our models achieved signiﬁcant\nimprovement with similar FLOPs for both models.\nThis suggests that the proposed R2L attention can\neffectively model global context.\n4.3 A CT IO N RE CO G N IT IO N\nDatasets and Setup. W e validate our approach on Kinetics400 (K400) (\nKay et al. , 2017) and\nSomething-Something V2 (SSV2) ( Goyal et al. , 2017). W e adopt the divided-space-time attention\nin TimeSformer ( Bertasius et al. , 2021) as the temporal modeling to perform action recognition ex-\nperiments. More details could be found in Sec. A.4.\nT able 7: Performance on action recognition.\nModel FLOPs ∗ (G) Params (M) K400 Acc. (%) SSV2 Acc. (%)\nTimeSformer 197 121.4 75.8 59.5\nx TimeSformer † 197 121.4 77.1 59.2\nRegionV iT-S 59.4 42.9 76.6 59.7\nRegionV iT-M 83.1 57.5 77.6 59.8\n∗ : FLOPs of single crop, †: retrained with the same setting.\nResults. T able 7 shows that with RegionV iT\nas the backbone, the model can be much more\nefﬁcient with competitive accuracy to TimeS-\nformer, which uses vanilla V iT as the back-\nbone. RegionV iT-M could reduce more than\n50% FLOPs and parameters than the TimeS-\nformer. This not only shows the importance of spatial modeli ng in action recognition but also\nvalidates that our proposed model can also be extended for ef ﬁcient action recognition.\n4.4 A BL AT IO N ST U DY\nW e perform the following experiments to verify the effectiv eness of different components in Region-\nV iT . All experiments are conducted based on RegionV iT-S.\n8\nPublished as a conference paper at ICLR 2022\nT able 8: Ablation on regional tokens.\nDataset IN1K MS COCO ADE20K\nRegional Acc. MaskRCNN RetinaNet SemanticFPN\nT okens (AP b/APm) (AP b) (mIoU)\nN 82.2 40.5/37.8 40.7 42.3\nY 82.6 42.5/39.5 42.2 43.7\nRegional T okens. T able 8 shows the results\nwith and without regional tokens on three tasks,\nand T able A5 includes FLOPs and parameters\ncomparison. For image classiﬁcation, the re-\ngional tokens provide around 0.4% improve-\nment with negligible overhead in both compu-\ntations (6%) and parameters (0.7%). On the other hand, the re gional tokens clearly improve the\nperformance of object detection and semantic segmentation with negligible overhead ( <2% on both\nFLOPs and parameters). This is because dense prediction tas ks require more multi-scale features\nwith global contextual information (provided by the region al tokens) than image classiﬁcation.\nT able 9: Different downsampling approaches.\nDownsampling Params FLOPs (G) IN1K Acc. (%)\n2× 2 convolution 32.1 5.5 82.3\n3× 3 convolution 34.1 5.7 82.5\n3× 3 depth-wise conv. 30.6 5.3 82.6\nDownsampling. T able 9 shows different ap-\nproaches for downsampling the patches. 3 × 3\nkernel size achieves better results than 2 × 2 con-\nvolution, because the kernel is non-overlapped\nwith 2 × 2 convolution, which limits the interac-\ntion among local tokens. Moreover, using regular convoluti on does not improve the performance\nbut increases computations and parameters. Thus, we use 3 × 3 depthwise convolution for downsam-\npling.\nT able 10: W eight sharing.\nW eight sharing Params FLOPs (G) IN1K Acc. (%)\nN 40.4 5.3 82.7\nY 30.6 5.3 82.6\nW eight sharing between RSA and LSA as\nwell as downsampling. T able 10 shows the re-\nsults with and without weight sharing. As seen\nfrom the table, using separated weights for RSA\nand LSA only slightly improves the accuracy\nbut signiﬁcantly increases the model size. This suggests sh aring parameters between RSA and LSA\nsufﬁces for learning local and global information in our app roach.\nT able 11: Keys in LSA.\nRegional tokens Params FLOPs (G) IN1K Acc. (%)\nAll 30.6 6.0 82.7\nCorresponding 30.6 5.3 82.6\nKeys in LSA. T able11 studies the number of re-\ngional tokens in LSA. When using all regional\ntokens in the LSA, it provides the local tokens\nthe possibility to explore all global information.\nNonetheless, this model only improves 0.1%\nbut with 13% more computations, which suggests that when con sidering the regional tokens, the\none associated with the current region is sufﬁcient to achie ve good performance.\nT able 12: Different position information.\nAbs. pos. Rel. pos. Params (M) FLOPs (G) IN1K Acc. (%)\nN N 30.6 5.3 82.4\nY N 30.9 5.3 82.2\nY Y 30.9 5.3 82.7\nN Y 30.6 5.3 82.6\nPosition information. T able 12 compares per-\nformance of different combinations of absolute\nposition embedding (APE) and relative position\nbias (RPB). The models with RPB achieve bet-\nter accuracy with similar FLOPs and param-\neters. Although the model with APE could\nslightly improve the performance, it limits the model to run at a ﬁxed resolution, which is not\nsuitable for vision tasks where image size could be varied. T hus, we only adopt the relative position\nbias in our models.\nT able 13: Overlapped windows.\nModel Params FLOPs (G) IN1K Acc. (%)\noverlapped 30.6 18.8 82.8\nnon-overlapped 30.6 5.3 82.6\nOverlapped windows. T able 13 compares the\nresults with and without overlapped windows.\nThe overlapping ratio of neighboring windows\nis 50% and hence this model allows more in-\nteractions among local tokens. It improves\nRegionV iT-S by 0.2% but increases the FLOPs by 3.5 × . This suggests that the information ex-\nchange between the border of windows is sufﬁciently covered by the proposed R2L attention.\nT able 14: Comparison of different attentions.\nModel Params FLOPs (G) IN1K Acc. (%)\nShifted-window 29.0 4.5 81.3\nR2L 32.1 5.2 82.0\nComparison between R2L attention and\nshifted-window attention. T able\n14 shows\nthe comparison between shifted-window atten-\ntion (\nLiu et al. , 2021) and proposed R2L at-\ntention. R2L attention outperforms shifted-\nwindow attention by 0.7%, which shows that our proposed R2L a ttention can effectively model\nglobal information to achieve good performance.\n9\nPublished as a conference paper at ICLR 2022\n5 C ONCLUSIO N\nIn this paper, we propose a new V iT architecture that exploit s the pyramid structure used in most\nof CNNs to provide multi-scale features and hence, the model s can be more easily extended to dif-\nferent vision applications, like object detection. Moreov er, the proposed regional-to-local attention\nrelaxes the memory overhead of performing self-attention o n the ﬁne-grained image tokens by lim-\niting the scope of attention but still keeping the capabilit y to explore global information. Extensive\nexperiments on several standard benchmark datasets well de monstrate that our proposed models\noutperform or are on par with many concurrent V iT variants on four vision applications, including\nimage classiﬁcation, object and keypoint detection, seman tic segmentation and action recognition.\n10\nPublished as a conference paper at ICLR 2022\nETHICS STATEMENT\nOur work introduces a memory-friendly and efﬁcient self-at tention method for transformer mod-\nels, which have wide-ranging applications to image classiﬁ cation, object detection, human activity\nrecognition, etc. By improving efﬁciency, our work can have a positive effect on these applications\nin society, allowing faster processing. E.g., this could en able faster responses to detected events\nsuch as medical emergencies or detecting defects in manufac turing parts. Lower computation costs\ncould also save energy and therefore have a positive impact o n the environment. Negative impacts\nof our research are difﬁcult to predict, however, it shares m any of the pitfalls associated with deep\nclassiﬁcation models. E.g., Image and video classiﬁcation systems have negative implications on\nprivacy and could be used by malicious actors or governments to infringe on the privacy of citizens.\nFuture research into private and ethical aspects of visual r ecognition is an important direction.\nREPRODUC I B IL IT Y STATEMENT\nIn order to reproduce our results, we describe the details of the hyperparameters used in our training\nin Appendix A for all the tasks and we also attach our codes for image classi ﬁcation as part of\nthe supplemental material. W e will publicly release all the codes and models after acceptance. In\nthe attached codes, we set all the hyperparameters as their d efault value, so that users can re-run\nour experiments with exactly the same hyperparameters. The README ﬁle also describes the\nrequirement to build necessary Python environment for runn ing the codes.\n11\nPublished as a conference paper at ICLR 2022\nREFERENC ES\nIrwan Bello. Lambdanetworks: Modeling long-range interac tions without atten-\ntion. In International Conference on Learning Representations , 2021. URL\nhttps://openreview.net/forum?id=xTJEN-ggl1b.\nIrwan Bello, Barret Zoph, Ashish V aswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In Proceedings of the IEEE/CVF International Conference on Co mputer\nV ision, pp. 3286–3295, 2019.\nGedas Bertasius, Heng W ang, and Lorenzo T orresani. Is Space -Time Attention All Y ou Need for\nV ideo Understanding? arXiv .org, February 2021.\nAndrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. H igh-Performance Large-Scale\nImage Recognition Without Normalization. arXiv, 2021.\nChun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossV iT: Cro ss-Attention Multi-Scale V ision\nTransformer for Image Classiﬁcation. arXiv .org, March 2021a.\nZhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui W ei, and Qi Tian. V isformer: The\nV ision-friendly Transformer. arxiv .org, April 2021b.\nXiangxiang Chu, Zhi Tian, Y uqing W ang, Bo Zhang, Haibing Ren , Xiaolin W ei, Huaxia Xia, and\nChunhua Shen. T wins: Revisiting Spatial Attention Design i n V ision Transformers. arXiv .org,\nApril 2021a.\nXiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong W ang, Xiaolin W e i, Huaxia Xia, and Chunhua Shen.\nConditional Positional Encodings for V ision Transformers . arXiv, 2021b.\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. RandA ugment: Practical Automated\nData Augmentation with a Reduced Search Space. In H Larochel le, M Ranzato, R Hadsell, M F\nBalcan, and H Lin (eds.), Advances in Neural Information Processing Systems , pp. 18613–18624.\nCurran Associates, Inc., 2020.\nSt´ ephane d’Ascoli, Hugo T ouvron, Matthew Leavitt, Ari Mor cos, Giulio Biroli, and Levent Sagun.\nConV iT: Improving V ision Transformers with Soft Convoluti onal Inductive Biases. arXiv .org,\ncs.CV , 03 2021. URL arXiv.org.\nJia Deng, W ei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li F ei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recogni tion,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T out anova. BER T: Pre-training of\ndeep bidirectional transformers for language understandi ng. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage T echnologies, V olume 1 (Long and Short P apers) , pp. 4171–4186, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguist ics. doi: 10.18653/v1/N19-1423. URL\nhttps://www.aclweb.org/anthology/N19-1423.\nY ifeng Ding, Zhanyu Ma, Shaoguo W en, Jiyang Xie, Dongliang C hang, Zhongwei Si, Ming Wu,\nand Haibin Ling. Ap-cnn: W eakly supervised attention pyram id convolutional neural network for\nﬁne-grained visual classiﬁcation. IEEE T ransactions on Image Processing , 30:2826–2836, 2021.\ndoi: 10.1109/TIP .2021.3055617.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Di rk W eissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg H eigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transf ormers for image recog-\nnition at scale. In International Conference on Learning Representations , 2021. URL\nhttps://openreview.net/forum?id=YicbFdNTTy.\nRaghav Goyal, Samira Ebrahimi Kahou, V incent Michalski, Jo anna Materzynska, Susanne W est-\nphal, Heuna Kim, V alentin Haenel, Ingo Fruend, Peter Y ianil os, Moritz Mueller-Freitag, et al.\nThe” something something” video database for learning and e valuating visual common sense. In\nICCV, 2017.\n12\nPublished as a conference paper at ICLR 2022\nBen Graham, Alaaeldin El-Nouby, Hugo T ouvron, Pierre Stock , Armand Joulin, Herv ´ e J´ egou,\nand Matthijs Douze. LeV iT: a V ision Transformer in ConvNet’ s Clothing for Faster Inference.\narXiv .org, April 2021.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Y un he W ang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112 , 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep R esidual Learning for Image\nRecognition. In The IEEE Conference on Computer V ision and P attern Recognit ion (CVPR) ,\nJune 2016.\nKaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshi ck. Mask r-cnn. In Proceedings of the\nIEEE International Conference on Computer V ision (ICCV) , Oct 2017.\nByeongho Heo, Sangdoo Y un, Dongyoon Han, Sanghyuk Chun, Jun suk Choe, and Seong Joon Oh.\nRethinking Spatial Dimensions of V ision Transformers. arXiv .org, March 2021.\nElad Hoffer, T al Ben-Nun, Itay Hubara, Niv Giladi, T orsten H oeﬂer, and Daniel Soudry. Aug-\nment your batch: Improving generalization through instanc e repetition. In Proceedings of the\nIEEE/CVF Conference on Computer V ision and P attern Recogni tion (CVPR) , June 2020.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relat ion networks for image recognition.\nIn Proceedings of the IEEE/CVF International Conference on Co mputer V ision , pp. 3464–3473,\n2019.\nJ. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks . In 2018 IEEE/CVF Conference\non Computer V ision and P attern Recognition , pp. 7132–7141, 2018. doi: 10.1109/CVPR.2018.\n00745.\nZihang Jiang, Qibin Hou, Li Y uan, Daquan Zhou, Xiaojie Jin, A nran W ang, and Jiashi Feng. T o-\nken Labeling: Training a 85.4% T op-1 Accuracy V ision Transf ormer with 56M Parameters on\nImageNet. arxiv .org, 04 2021. URL arxiv.org.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra V ijaya-\nnarasimhan, Fabio V iola, Tim Green, Trevor Back, Paul Natse v, et al. The kinetics human action\nvideo dataset. arXiv:1705.06950 , 2017.\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Do llar. Panoptic feature pyramid net-\nworks. In Proceedings of the IEEE/CVF Conference on Computer V ision a nd P attern Recognition\n(CVPR), June 2019.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\ncategorization. In 4th International IEEE W orkshop on 3D Representation and Re cognition\n(3dRR-13), Sydney, Australia, 2013.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\nY awei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc V an G ool. LocalV iT: Bringing Locality\nto V ision Transformers. arXiv .org, April 2021.\nTsung-Y i Lin, Michael Maire, Serge Belongie, James Hays, Pi etro Perona, Deva Ramanan, Piotr\nDoll´ ar, and C Lawrence Zitnick. Microsoft COCO: Common Obj ects in Context. In David Fleet,\nT omas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer V ision – ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12 , 2014, Proceedings, P art V , pp. 740–\n755. Springer International Publishing, Cham, 2014.\nTsung-Y i Lin, Priya Goyal, Ross Girshick, Kaiming He, and Pi otr Dollar. Focal loss for dense object\ndetection. In Proceedings of the IEEE International Conference on Comput er V ision (ICCV) , Oct\n2017.\nZe Liu, Y utong Lin, Y ue Cao, Han Hu, Y ixuan W ei, Zheng Zhang, S tephen Lin, and Baining Guo.\nSwin Transformer: Hierarchical V ision Transformer using S hifted Windows. arXiv .org, March\n2021.\n13\nPublished as a conference paper at ICLR 2022\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradien t descent with warm restarts. In Interna-\ntional Conference on Learning Representations , 2017.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay re gulariza-\ntion. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=Bkg6RiCqY7.\nOmkar M. Parkhi, Andrea V edaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In IEEE\nConference on Computer V ision and P attern Recognition , 2012.\nPrajit Ramachandran, Niki Parmar, Ashish V aswani, Irwan Be llo, Anselm Levskaya, and Jon Shlens.\nStand-Alone Self-Attention in V ision Models. In H W allach, H Larochelle, A Beygelzimer, F d\nAlch e Buc, E Fox, and R Garnett (eds.), Advances in Neural Information Processing Systems .\nCurran Associates, Inc., 2019a.\nPrajit Ramachandran, Niki Parmar, Ashish V aswani, Irwan Be llo, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909 , 2019b.\nAravind Srinivas, Tsung-Y i Lin, Niki Parmar, Jonathon Shle ns, Pieter Abbeel, and Ashish V aswani.\nBottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605 , 2021.\nC. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting un reasonable effectiveness of data in deep\nlearning era. In 2017 IEEE International Conference on Computer V ision (ICC V), pp. 843–852,\n2017. doi: 10.1109/ICCV .2017.97.\nChristian Szegedy, V incent V anhoucke, Sergey Ioffe, Jon Sh lens, and Zbigniew W ojna. Rethink-\ning the inception architecture for computer vision. In Proceedings of the IEEE Conference on\nComputer V ision and P attern Recognition (CVPR) , June 2016.\nMingxing T an and Quoc Le. EfﬁcientNet: Rethinking Model Sca ling for Convolutional Neural\nNetworks. In Kamalika Chaudhuri and Ruslan Salakhutdinov ( eds.), Proceedings of the 36th\nInternational Conference on Machine Learning , pp. 6105–6114, Long Beach, California, USA,\nJune 2019. PMLR.\nHugo T ouvron, Matthieu Cord, Matthijs Douze, Francisco Mas sa, Alexandre Sablayrolles, and\nHerv ´ e J´ egou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877 , 2020.\nHugo T ouvron, Matthieu Cord, Alexandre Sablayrolles, Gabr iel Synnaeve, and Herv ´ e J´ egou. Going\ndeeper with Image Transformers. arXiv .org, cs.CV , 03 2021. URL arXiv.org.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N Gomez, ukasz\nKaiser, and Illia Polosukhin. Attention is All you Need. In I Guyon, U V Luxburg, S Bengio,\nH W allach, R Fergus, S V ishwanathan, and R Garnett (eds.), Advances in Neural Information\nProcessing Systems . Curran Associates, Inc., 2017.\nW enhai W ang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, T ong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile back bone for dense prediction without\nconvolutions, 2021.\nXiaolong W ang, Ross Girshick, Abhinav Gupta, and Kaiming He . Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pa ttern recognition , pp. 7794–7803,\n2018.\nXiaosong W ang, Y ifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi B agheri, and Ronald M Sum-\nmers. Chestx-ray8: Hospital-scale chest x-ray database an d benchmarks on weakly-supervised\nclassiﬁcation and localization of common thorax diseases. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pp. 2097–2106, 2017.\nZachary Wharton, Ardhendu Behera, and Asish Bera. An attent ion-driven hierarchical multi-scale\nrepresentation for visual recognition, 2021.\nRoss Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n14\nPublished as a conference paper at ICLR 2022\nSanghyun W oo, Jongchan Park, Joon-Y oung Lee, and In So Kweon . Cbam: Convolutional block at-\ntention module. In Proceedings of the European Conference on Computer V ision ( ECCV), Septem-\nber 2018.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai , Lu Y uan, and Lei Zhang. CvT:\nIntroducing Convolutions to V ision Transformers. arXiv .org, March 2021.\nY uxin Wu, Alexander Kirillov, Francisco Massa, W an-Y en Lo, and Ross Girshick. Detectron2.\nhttps://github.com/facebookresearch/detectron2, 2019.\nHaotian Y an, Zhe Li, W eijian Li, Changhu W ang, Ming Wu, and Ch uang Zhang. ConTNet: Why\nnot use convolution and transformer at the same time? arXiv .org, April 2021.\nLi Y uan, Y unpeng Chen, T ao W ang, W eihao Y u, Y ujun Shi, Franci s EH T ay, Jiashi Feng, and\nShuicheng Y an. T okens-to-token vit: Training vision trans formers from scratch on imagenet,\n2021.\nSangdoo Y un, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Jun suk Choe, and Y oungjoon Y oo.\nCutMix: Regularization Strategy to Train Strong Classiﬁer s With Localizable Features. In Pro-\nceedings of the IEEE/CVF International Conference on Compu ter V ision (ICCV) , October 2019.\nHongyi Zhang, Moustapha Cisse, Y ann N. Dauphin, and David Lo pez-Paz. mixup: Beyond empir-\nical risk minimization. In International Conference on Learning Representations , 2018. URL\nhttps://openreview.net/forum?id=r1Ddp1-Rb.\nPengchuan Zhang, Xiyang Dai, Jianwei Y ang, Bin Xiao, Lu Y uan , Lei Zhang, and Jianfeng Gao.\nMulti-Scale V ision Longformer: A New V ision Transformer fo r High-Resolution Image Encod-\ning. arXiv .org, March 2021a.\nZizhao Zhang, Han Zhang, Long Zhao, Ting Chen, and T omas Pﬁst er. Aggregating Nested Trans-\nformers. arXiv, 2021b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring s elf-attention for image recognition. In\nProceedings of the IEEE/CVF Conference on Computer V ision a nd P attern Recognition (CVPR) ,\nJune 2020.\nHeliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo, and T a o Mei. Learning Rich Part Hierar-\nchies With Progressive Attention Networks for Fine-Graine d Image Recognition. IEEE T ransac-\ntions on Image Processing , 29:476–488, 2020. ISSN 1057-7149. doi: 10.1109/tip.2019 .2921876.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Y i Y an g. Random Erasing Data Aug-\nmentation. Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, 34(07):13001–13008,\nApril 2020.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barr iuso, and Antonio T orralba. Scene\nparsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer V ision and\nP attern Recognition (CVPR) , July 2017.\n15\nPublished as a conference paper at ICLR 2022\nT able A1: Details of training settings for image classiﬁcation on Ima geNet1K and ImageNet21K.\nIN1K IN21K Finetune to IN1K@ 3842 Transfer\nBatch size 4,096 4,096 1,024 768\nEpochs 300 120 30 1,000\nOptimizer AdamW AdamW AdamW SGD\nW eight Decay 0.05 0.01 1e-8 1e-4\nLinear-rate Scheduler Cosine (0.004) Cosine (0.001) Cosine (0.0002) Cosine (0.01 )(Initial LR)\nW armup Epochs 50 5 0 5\nW armup linear-rate Linear (1e-6)Scheduler (Initial LR)\nData Aug. RandAugment (m=9, n=2)\nMixup ( α ) 0.8\nCutMix ( α ) 1.0\nRandom Erasing 0.25 0.0\nInstance 3Repetition∗\nDrop-path 0.1 0.0\nLabel Smoothing 0.1\n∗ : disabled for RegionViT-Ti.\nAPPENDIX\nSummary This appendix contains the following additional details. F irst, in Sec. A, we describe\nthe training details on image classiﬁcation, object and key point detection, semantic segmentation\nand action recognition separately. Then, we provide detail ed results on object detection and ablation\nstudies. The supplementary materials include our codes to r eproduce our results on image classiﬁ-\ncation and the README ﬁle in the attached codes ( RegionViT Code.zip) also provides the detailed\ninstructions to train and evaluate the networks.\nA T RAINING DETAILS\nA.1 I M AG E CL A S S IFICAT IO N\nW e follow DeiT ( T ouvron et al. , 2020) to train our models on ImageNet1K ( Deng et al. , 2009)\nexcept that we use batch size 4,096 with base learning rate 0. 004 and the warm-up epochs\nis 50. W e adopt the AdamW ( Loshchilov & Hutter , 2019) optimizer with cosine learning rate\nscheduler ( Loshchilov & Hutter , 2017). W e apply Mixup ( Zhang et al. , 2018), CutMix ( Y un et al. ,\n2019), RandomErasing ( Zhong et al. , 2020), label smoothing ( Szegedy et al. , 2016), RandAug-\nment ( Cubuk et al. , 2020) and instance repetition ( Hoffer et al. , 2020). During training, we ran-\ndomly crop a 224 × 224 region and take a 224 × 224 center crop after resizing the shorter side to\n256 for evaluation. While pretraining on ImageNet21K ( Deng et al. , 2009), we use similar settings\nwith slight modiﬁcations. W e train the model using 120 epoch s with 5 epochs for warmup, weight-\ndecay of 0.01 and base learning rate of 0.001. For transfer le arning experiments, we ﬁnetune the\nImageNet1K pretrained models with 1,000 epochs, batch size of 768, learning rate of 0.01, SGD op-\ntimizer, weight decay of 0.0001, and using the same data augm entation in training on ImageNet1K.\nDuring evaluation, we resize the shorter side of an image to 2 56 and then take a 224 × 224 region at\nthe center and report top-1 accuracy. For ﬁnetuning experim ents from ImageNet21K, we ﬁnetune\nthe models with higher resolution of 384 × 384, for 30 epochs with cosine learning rate scheduler,\nbase linear rate of 0.002, weight-decay of 1e-8. Moreover, a s it is trained on larger resolution, we\nadjust the window size M to have the same number of regional tokens at the ﬁrst stage, e .g., we\nchange the window size to 12 for the the models trained at 224 × 224 with window size 7. W e adopt\nthe bicubic interpolation to upsample the weights of tokeni zations and relative position bias. During\nevaluation, we directly resize the shorter side to 384 and th en take center 384 × 384 crop. W e trained\nthe models with 32 GPUs.\n16\nPublished as a conference paper at ICLR 2022\nA.2 O BJ E CT DE T E CT IO N A N D KE Y P O IN T DE T E CT IO N\nW e adopt RetinaNet ( Lin et al. , 2017) and MaskRCNN ( He et al. , 2017) as our detection framework\nand use KeypoinyRCNN for keypoint detection, and we simply r eplace the backbone with Region-\nV iT , and then output the local tokens at each stages as multi- scale features for the detector. The\nregional tokens are not used in the detection. Before sendin g the features to the detector frame-\nwork, we add new layer normalization layers to normalize the features. W e use RegionV iT-S and\nRegionV iT-B as the backbone and the weights are initialized from ImageNet1K pretraining. W e\ntrain our models based on the settings in 1 × schedule in Detectron2 ( Wu et al. , 2019), i.e., the batch\nsize is 16 and the learning rate is set to 0.0001 which drops 10 × at the 60,000-th and 80,000-th itera-\ntion. W e use AdamW optimizer with weight-decay of 0.05, and r esize the shorter and longer side of\nan image to 672 and 1,120, respectively. W e also adopt drop-p ath when ﬁnetuning the detector, with\na rate set to 0.2 for both RetinaNet and MaskRCNN. W e trained t he models with 8 GPUs. When\ntraining with longer schedule (3 × ), we adopt stronger data augmentation (multi-scale traini ng) used\nin Swin ( Liu et al. , 2021) or T wins ( Chu et al. , 2021a) rather than ﬁxed-size training used in 1 ×\nschedule.\nA.3 S E M A N T IC SE G M E N TAT IO N\nW e adopt Semantic FPN as our semantic segmentation framewor k ( Kirillov et al. , 2019) by replacing\nthe backbone network with RegionV iT like the object detecti on task. W e train the models with\nImageNet1K pretrained weights. W e mostly follow the traini ng receipt in T wins ( Chu et al. , 2021a)\nThe shorter side of the image is resized to 448 and the longer s ide won’t exceed 1792, and then we\ncrop a 448 × 448 region for training. During the evaluation, we take the w hole image after resizing\nas illustrated above. W e train the model with AdamW optimize r with learning rate 1e-4 for 80k\niterations, and the learning rate is decayed with poly sched ule (power is 0.9). The weight decay and\ndrop-path rate is 0.05 and 0.2, respectively. W e trained all models with 8 GPUs.\nA.4 A CT IO N RE CO G N IT IO N\nW e adopt the divided-space-time attention in TimeSformer ( Bertasius et al. , 2021) as the temporal\nmodeling to perform action recognition experiments. More s peciﬁcally, we plugin the temporal at-\ntention before every R2L transformer encoder and ﬁnetune th e model pretrained with ImageNet1K\n(instead of ImageNet21K). W e use the AdamW optimizer ( Loshchilov & Hutter , 2019) with base\nlinear rate of 2e-4 and weight-decay of 0.0001. W e apply the s ame Mixup ( Zhang et al. , 2018),\nCutMix ( Y un et al. , 2019) and drop path ( T an & Le , 2019) in the image classiﬁcation. For the in-\nput, we uniformly sample 8 frames from the whole video and the 224× 224 region is cropped after\nshorter side of images is resized to the range of 256 to 320, re sulting the size of the input video\nas 8 × 224× 224. During evaluation, we use the same way to sample frames b ut take 3 224 × 224\nspatial crops (top-left, center and bottom-right) after re sizing shorter side of image to 256, and then\nensemble the predictions as ﬁnal prediction for the input vi deo. W e trained the models with 16\nGPUs.\nB M ORE DETAILED RESULTS\nW e provided more details of some T ables shown in the main pape r here. T able A2 and T able A3\ninclude more metrics as compared to T able 5. T able A4 shows the results on keypoint detection.\nRegionV iT outperforms ResNet-50 with moderate increases i n FLOPs and parameters. This sug-\ngests that RegionV iT can model the global context in keypoin t detection as well. T able A5 includes\ncomplexity comparison for the ablation study on regional to kens as shown in T able 8.\n17\nPublished as a conference paper at ICLR 2022\nT able A2: Object detection performance on the COCO val2017 with 1 × schedule. The bold number indicates\nthe best number within the section, and for MaskRCNN, both AP b and AP m are annotated.\nParams FLOPs RetineNet Params FLOPs MaskRCNN\nBackbone (M) (G) AP AP50 AP75 APS APM APL (M) (G) AP b AP b\n50 AP b\n75 AP m AP m\n50 AP m\n75\nResNet50 37.7 234 36.3 55.3 38.6 19.3 40.0 48.8 44.2 260.0 38.0 58.6 41.4 34.4 55.1 36.7\nConT -M (Y an et al. , 2021) 27.0 217.2 39.3 59.3 41.8 23.1 43.1 51.9 − − 40.5 − − 38.1\nPVT -S (W ang et al. , 2021) 34.2 − 40.4 61.3 43.0 25.0 42.9 55.7 44.1 − 40.4 62.9 43.8 37.8 60.1 40.3\nV iL-S ( Zhang et al. , 2021a) 35.7 252.2 41.6 62.5 44.1 24.9 44.6 56.2 45.0 174.3 41.8 64.1 45.1 38.5 61.1 41.4\nSwin-T ( Liu et al. , 2021) 38.5 245 41.5 62.1 44.2 25.1 44.9 55.5 47.8 264 42.2 64.6 46.2 39.1 61.6 42.0\nT wins-SVT -S (w/ PEG) ( Chu et al. , 2021a) 34.3 209 43.0 64.2 46.3 28.0 46.4 57.5 44.0 228 43.5 66.0 47.3 40.3 63.2 43.4\nRegionV iT-S 40.8 192.6 42.2 64.1 45.1 27.5 45.4 55.3 50.1 171.3 42.5 65.8 46.1 39.5 62.8 42.2\nRegionV iT-S+ 41.5 204.2 43.1 64.8 46.2 29.6 46.6 56.1 50.9 182.9 43.5 66.9 47.5 40.4 63.7 43.4\nRegionV iT-S+ w/ PEG 41.6 204.3 43.9 65.5 47.3 28.5 47.3 57.9 50.9 183.0 44.2 67.3 48.2 40.8 64.1 44.0\nResNet101 56.7 315 38.5 57.8 41.2 21.4 42.6 51.1 63.2 336 40.4 61.1 44.2 36.4 57.7 38.8\nResNeXt101-32x4d 56.4 319.1 39.9 59.6 42.7 22.3 44.2 52.5 62.8 340 41.9 62.5 45.9 37.5 59.4 40.2\nPVT -M (W ang et al. , 2021) 53.9 − 41.9 63.1 44.3 25.0 44.9 57.6 63.9 − 42.0 64.4 45.6 39.0 61.6 42.1\nV iL-M ( Zhang et al. , 2021a) 50.8 338.9 42.9 64.0 45.4 27.0 46.1 57.2 60.1 261.1 43.4 65.9 47.0 39.7 62.8 42.1\nSwin-S ( Liu et al. , 2021) 59.8 335 44.5 65.7 47.5 27.4 48.0 59.9 69.1 354 44.8 66.6 48.9 40.9 63.4 44.2\nT wins-SVT -B (w/ PEG) ( Chu et al. , 2021a) 67.0 322 45.3 66.7 48.1 28.5 48.9 60.6 76.3 340 45.2 67.6 49.3 41.5 64.5 44.8\nRegionV iT-B 83.4 308.9 43.3 65.2 46.4 29.2 46.4 57.0 92.2 287.9 43.5 66.7 47.4 40.1 63.4 43.0\nRegionV iT-B+ 84.4 328.1 44.2 66.2 47.1 29.2 47.5 58.6 93.2 307.1 44.5 67.6 48.7 41.0 64.4 43.9\nRegionV iT-B+ w/ PEG 84.5 328.2 44.6 66.4 47.6 29.6 47.6 59.0 93.2 307.2 45.4 68.4 49.6 41.6 65.2 44.8\nResNeXt101-64x4d 95.5 473 41.0 60.9 44.0 23.9 45.2 54.0 101.9 493 42.8 63.8 47.3 38.4 60.6 41.3\nPVT -L (W ang et al. , 2021) 71.1 345 42.6 63.7 45.4 25.8 46.0 58.4 81.0 364 42.9 65.0 46.6 39.5 61.9 42.5\nV iL-B ( Zhang et al. , 2021a) 66.7 443.0 44.3 65.5 47.1 28.9 47.9 58.3 76.1 365.1 45.1 67.2 49.3 41.0 64.3 44.2\nSwin-B ( Liu et al. , 2021) 98.4 477 44.7 65.9 49.2 − − − 107.2 496 45.5 − − 41.3 − −\nT wins-SVT -L (w/ PEG) ( Chu et al. , 2021a) 110.9 455 45.7 67.1 49.2 − − − 119.7 474 45.9 − − 41.6 − −\nRegionV iT-B+ w/ PEG † 84.5 506.4 46.1 68.0 49.5 30.5 49.9 60.1 93.2 464.4 46.3 69.1 51.2 42.4 66.2 45.6\nThe reported results of Swin ( Liu et al. , 2021) are from T wins ( Chu et al. , 2021a) as the original paper does not include resutls with ImageNe t1K weights. †: input resolution is 896 × 1344.\nT able A3: Object detection performance on the COCO val2017 with 3 × schedule. The bold number indicates\nthe best number within the section, and for MaskRCNN, both AP b and AP m are annotated.\nParams FLOPs RetineNet Params FLOPs MaskRCNN\nBackbone (M) (G) AP AP50 AP75 APS APM APL (M) (G) AP b AP b\n50 AP b\n75 AP m AP m\n50 AP m\n75\nResNet50 37.7 234 39.0 58.4 41.8 22.4 42.8 51.6 44.2 260.0 41.0 61.7 44.9 37.1 58.4 40.1\nPVT -S (W ang et al. , 2021) 34.2 − 42.2 62.7 45.0 26.2 45.2 57.2 44.1 245 43.0 65.3 46.9 39.9 62.5 42.8\nV iL-S ( Zhang et al. , 2021a) 35.7 252.2 42.9 63.8 45.6 27.8 46.4 56.3 45.0 174.3 43.4 64.9 47.0 39.6 62.1 42.4\nSwin-T ( Liu et al. , 2021) 38.5 245 43.9 64.8 47.1 28.4 47.2 57.8 47.8 264 46.0 68.2 50.2 41.6 65.1 44.8\nT wins-SVT -S (w/ PEG) ( Chu et al. , 2021a) 34.3 209 45.6 67.1 48.6 29.8 49.3 60.0 44.0 228 46.8 69.2 51.2 42.6 66.3 45.8\nRegionV iT-S 40.8 192.6 45.8 67.2 49.2 30.0 50.0 60.5 50.1 171.3 46.3 68.8 50.6 42.3 65.5 45.7\nRegionV iT-S+ 41.5 204.2 46.9 68.3 50.7 31.1 51.0 62.0 50.9 182.9 47.3 69.5 52.0 43.1 66.4 46.7\nRegionV iT-S+ w/ PEG 41.6 204.3 46.7 68.2 50.2 30.7 50.8 62.4 50.9 183.0 47.6 70.0 52.0 43.4 67.1 47.0\nResNet101 56.7 315 40.9 60.1 44.0 23.7 45.0 53.8 63.2 336 42.8 63.2 47.1 38.5 60.1 41.3\nResNeXt101-32x4d 56.4 319.1 41.4 61.0 44.3 23.9 45.5 53.7 62.8 340 44.0 64.4 48.0 39.2 61.4 41.9\nPVT -M (W ang et al. , 2021) 53.9 − 43.2 63.8 46.1 27.3 46.3 58.9 63.9 302 44.2 66.0 48.2 40.5 63.1 43.5\nV iL-M ( Zhang et al. , 2021a) 50.8 338.9 43.7 64.6 46.4 27.9 47.1 56.9 60.1 261.1 44.6 66.3 48.5 40.7 63.8 43.7\nSwin-S ( Liu et al. , 2021) 59.8 335 46.3 67.4 49.8 31.1 50.3 60.9 69.1 354 47.6 69.4 52.5 42.8 66.5 46.4\nT wins-SVT -B (w/ PEG) ( Chu et al. , 2021a) 67.0 322 46.9 68.0 50.2 31.7 50.3 61.8 76.3 340 48.0 69.5 52.7 43.0 66.8 46.6\nRegionV iT-B 83.4 308.9 46.1 67.8 49.1 31.5 50.2 61.2 92.2 287.9 47.2 69.1 51.7 43.0 66.4 46.5\nRegionV iT-B+ 84.4 328.1 46.9 68.6 50.1 30.8 50.7 62.6 93.2 307.1 48.1 70.2 52.5 43.5 67.1 47.1\nRegionV iT-B+ w/ PEG 84.5 328.2 46.9 68.3 50.3 31.1 50.5 62.4 93.2 307.2 48.3 70.1 52.8 43.5 67.1 47.0\nV iL-B ( Zhang et al. , 2021a) 66.7 443.0 44.7 65.5 47.6 29.9 48.0 58.1 76.1 365.1 45.7 67.2 49.9 41.3 64.4 44.5\nRegionV iT-B+ w/ PEG † 84.5 506.4 48.2 69.9 51.5 34.3 51.5 61.7 93.2 464.4 49.2 71.0 53.7 44.5 68.4 48.3\nThe reported results of Swin ( Liu et al. , 2021) are from T wins ( Chu et al. , 2021a) as the original paper does not include resutls with ImageNe t1K weights. †: input resolution is 896 × 1344.\nT able A4: Performance on person keypoint detection.\nModel FLOPs (G) Params (M) bbox AP (%) keypoint AP (%)\nResNet-50 137.7 59.1 53.6 64.0\nRegionV iT-S+ (w/ PEG) 172.2 65.7 56.0 66.1\nT able A5: Performance w/ and w/o regional tokens on RegionV i T-S.\nRegional T okens FLOPs (G) Params (M) ImageNet1K Acc. (%)\nN 5.0 30.4 82.2\nY 5.3 30.6 82.6\nMaskRCNN MS COCO (AP b/APm)\nN 168.5 49.9 40.5/37.8\nY 171.3 50.1 42.2/39.0\nRetinaNet MS COCO (AP b)\nN 189.8 40.5 40.7\nY 192.6 40.8 41.7\nSemanticFPN ADE20K (mIoU)\nN 36.8 34.7 42.3\nY 37.3 35.0 43.7\n18\nPublished as a conference paper at ICLR 2022\nT able A6: Throughput comparison.\nModel Acc. (%) Params (M) FLOPs (G) Throughput (images/sec)\nSwin-T 81.3 29.0 4.5 1129\nSwin-S 83.0 50.0 8.7 710\nRegionV iT-S 82.6 30.6 5.3 823\nRegionV iT-M 83.1 41.2 7.4 706\n19"
}