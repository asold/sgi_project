{
    "title": "ResT: An Efficient Transformer for Visual Recognition",
    "url": "https://openalex.org/W3164208409",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A951993420",
            "name": "Zhang Qinglong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2077733855",
            "name": "Yang Yubin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034399919",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W3156109214",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W3016719260",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W3162418282",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W3138191931",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3012846999",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W2156387975",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3106546328",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W2922509574",
        "https://openalex.org/W2995426144"
    ],
    "abstract": "This paper presents an efficient multi-scale vision Transformer, called ResT, that capably served as a general-purpose backbone for image recognition. Unlike existing Transformer methods, which employ standard Transformer blocks to tackle raw images with a fixed resolution, our ResT have several advantages: (1) A memory-efficient multi-head self-attention is built, which compresses the memory by a simple depth-wise convolution, and projects the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads; (2) Position encoding is constructed as spatial attention, which is more flexible and can tackle with input images of arbitrary size without interpolation or fine-tune; (3) Instead of the straightforward tokenization at the beginning of each stage, we design the patch embedding as a stack of overlapping convolution operation with stride on the 2D-reshaped token map. We comprehensively validate ResT on image classification and downstream tasks. Experimental results show that the proposed ResT can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResT as strong backbones. The code and models will be made publicly available at https://github.com/wofmanaf/ResT.",
    "full_text": "ResT: An Efﬁcient Transformer for Visual\nRecognition\nQing-Long Zhang, Yu-Bin Yang∗\nState Key Laboratory for Novel Software Technology\nNanjing University, Nanjing 21023, China\nwofmanaf@smail.nju.edu.cn, yangyubin@nju.edu.cn\nAbstract\nThis paper presents an efﬁcient multi-scale vision Transformer, called ResT, that\ncapably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a ﬁxed resolution, our ResT have several advantages:\n(1) A memory-efﬁcient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across the\nattention-heads dimension while keeping the diversity ability of multi-heads; (2)\nPositional encoding is constructed as spatial attention, which is more ﬂexible and\ncan tackle with input images of arbitrary size without interpolation or ﬁne-tune;\n(3) Instead of the straightforward tokenization at the beginning of each stage, we\ndesign the patch embedding as a stack of overlapping convolution operation with\nstride on the token map. We comprehensively validate ResT on image classiﬁcation\nand downstream tasks. Experimental results show that the proposed ResT can\noutperform the recently state-of-the-art backbones by a large margin, demonstrating\nthe potential of ResT as strong backbones. The code and models will be made\npublicly available at https://github.com/wofmanaf/ResT.\n1 Introduction\nDeep learning backbone architectures have been evolved for years and boost the performance of\ncomputer vision tasks such as classiﬁcation [ 5, 25, 32, 10], object detection [ 2, 39, 17, 24], and\ninstance segmentation [9, 23, 30], etc.\nThere are mainly two types of backbone architectures most commonly applied in computer vision:\nconvolutional network (CNN) architectures [10, 37] and Transformer ones [5, 32]. Both of them\ncapture feature information by stacking multiple blocks. The CNN block is generally a bottleneck\nstructure [10], which can be deﬁned as a stack of 1 ×1, 3 ×3, and 1 ×1 convolution layers\nwith residual learning (shown in Figure 1a). The 1 ×1 layers are responsible for reducing and\nthen increasing channel dimensions, leaving the 3 ×3 layer a bottleneck with smaller input/output\nchannel dimensions. The CNN backbones are generally faster and require less inference time thanks\nto parameter sharing, local information aggregation, and dimension reduction. However, due to\nthe limited and ﬁxed receptive ﬁeld, CNN blocks may be less effective in scenarios that require\nmodeling long-range dependencies. For example, in instance segmentation, being able to collect and\nassociate scene information from a large neighborhood can be useful in learning relationships across\nobjects [22].\nTo overcome these limitations, Transformer backbones are recently explored for their ability to\ncapture long-distance information [5, 32, 25, 18]. Unlike CNN backbones, the Transformer ones ﬁrst\n∗This work is funded by the Natural Science Foundation of China (No. 62176119).\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2105.13677v5  [cs.CV]  14 Oct 2021\nFigure 1: Examples of backbone blocks. Left: A standard ResNet Bottleneck Block [10]. Middle: A\nStandard Transformer Block. Right: The proposed Efﬁcient Transformer Block. The only difference\ncompared with standard Transformer block is the replacement of the Multi-Head Self-Attention\n(MSA) with Efﬁcient Multi-head Self-Attention (EMSA).\nsplit an image into a sequence of patches (i.e., tokens), then sum these tokens with positional encoding\nto represent coarse spatial information, and ﬁnally adopt a stack of Transformer blocks to capture\nfeature information. A standard Transformer block [27] comprises a multi-head self-attention (MSA)\nthat employs a query-key-value decomposition to model global relationships between sequence\ntokens, and a feed-forward network (FFN) to learn wider representations (shown in Figure 1b). As a\nresult, Transformer blocks can dynamically adapt the receptive ﬁeld according to the image content.\nDespite showing great potential than CNNs, the Transformer backbones still have four major short-\ncomings: (1) It is difﬁcult to extract the low-level features which form some fundamental structures\nin images (e.g., corners and edges) since existing Transformer backbones direct perform tokenization\nof patches from raw input images. (2) The memory and computation for MSA in Transformer blocks\nscale quadratically with spatial or embedding dimensions (i.e., the number of channels), causing\nvast overheads for training and inference. (3) Each head in MSA is responsible for only a subset\nof embedding dimensions, which may impair the performance of the network, particularly when\nthe tokens embedding dimension (for each head) is short, making the dot product of query and key\nunable to constitute an informative function. (4) The input tokens and positional encoding in existing\nTransformer backbones are all of a ﬁxed scale, which are unsuitable for vision tasks that require\ndense prediction.\nFigure 2: The pipeline of the proposed ResT. Similar to ResNet [10], ResT build stages with stacked\nblocks, making it ﬂexible to serve as the backbone of downstream tasks, such as Object detection,\nPerson ReID, and Instance Segmentation, etc.\nIn this paper, we proposed an efﬁcient general-purpose backbone ResT (named after ResNet [10]) for\ncomputer vision, which can remedy the above issues. As illustrated in Figure 2, ResT shares exactly\nthe same pipeline of ResNet, i.e., a stem module applied for extracting low-level information and\nstrengthening locality, followed by four stages to construct hierarchical feature maps, and ﬁnally\na head module for classiﬁcation. Each stage consists of a patch embedding, a positional encoding\n2\nmodule, and multiple Transformer blocks with speciﬁc spatial resolution and channel dimension.\nThe patch embedding module creates a multi-scale pyramid of features by hierarchically expanding\nthe channel capacity while reducing the spatial resolution with overlapping convolution operations.\nUnlike the conventional methods which can only tackle images with a ﬁxed scale, our positional\nencoding module is constructed as spatial attention which is conditioned on the local neighborhood\nof the input token. By doing this, the proposed method is more ﬂexible and can process input images\nof arbitrary size without interpolation or ﬁne-tune. Besides, to improve the efﬁciency of the MSA, we\nbuild an efﬁcient multi-head self-attention (EMSA), which signiﬁcantly reduce the computation cost\nby a simple overlapping Depth-wise Conv2d. In addition, we compensate short-length limitations\nof the input token for each head by projecting the interaction across the attention-heads dimension\nwhile keeping the diversity ability of multi-heads.\nWe comprehensively validate the effectiveness of the proposed ResT on the commonly used bench-\nmarks, including image classiﬁcation on ImageNet-1k and downstream tasks, such as object detection,\nand instance segmentation on MS COCO2017. Experimental results demonstrate the effectiveness\nand generalization ability of the proposed ResT compared with the recently state-of-the-art Vision\nTransformers and CNNs. For example, with a similar model size as ResNet-18 (69.7%) and PVT-Tiny\n(75.1%), our ResT-Small obtains a Top-1 accuracy of 79.6% on ImageNet-1k.\n2 ResT\nAs illustrated in Figure 2, ResT shares exactly the same pipeline as ResNet [10], i.e., a stem module\napplied to extract low-level information, followed by four stages to capture multi-scale feature\nmaps. Each stage consists of three components, one patch embedding module (or stem module),\none positional encoding module, and a set of Lefﬁcient Transformer blocks. Speciﬁcally, at the\nbeginning of each stage, the patch embedding module is adopted to reduce the resolution of the input\ntoken and expanding the channel dimension. The positional encoding module is fused to restrain\nposition information and strengthen the feature extracting ability of patch embedding. After that,\nthe input token is fed to the efﬁcient Transformer blocks (illustrated in Figure 1c). In the following\nsections, we will introduce the intuition behind ResT.\n2.1 Rethinking of Transformer Block\nThe standard Transformer block consists of two sub-layers of MSA and FFN. A residual connection\nis employed around each sub-layer. Before MSA and FFN, layer normalization (LN [1]) is applied.\nFor a token input x ∈Rn×dm , where n, dm indicates the spatial dimension, channel dimension,\nrespectively. The output for each Transformer block is:\ny= x′+ FFN(LN(x′)), and x′= x + MSA(LN(x)) (1)\nMSA. MSA ﬁrst obtains query Q, key K, and value V by applying three sets of projections to the\ninput, each consisting of klinear layers (i.e., heads) that map the dm dimensional input into a dk\ndimensional space, where dk = dm/kis the head dimension. For the convenience of description, we\nassume k= 1, then MSA can be simpliﬁed to single-head self-attention (SA). The global relationship\nbetween the token sequence can be deﬁned as\nSA(Q,K,V) = Softmax(QKT\n√dk\n)V (2)\nThe output values of each head are then concatenated and linearly projected to form the ﬁnal output.\nThe computation costs of MSA are O(2dmn2 + 4d2\nmn), which scale quadratically with spatial\ndimension or embedding dimensions according to the input token.\nFFN. The FFN is applied for feature transformation and non-linearity. It consists of two linear layers\nwith a non-linearity activation. The ﬁrst layer expands the embedding dimensions of the input from\ndm to df and the second layer reduce the dimensions from df to dm.\nFFN(x) =σ(xW1 + b1)W2 + b2 (3)\nwhere W1 ∈Rdm×df and W2 ∈Rdf ×dm are weights of the two Linear layers respectively,b1 ∈Rdf\nand b2 ∈Rdm are the bias terms, and σ(·) is the activation function GELU [ 11]. In standard\nTransformer block, the channel dimensions are expanded by a factor of 4, i.e., df = 4dm. The\ncomputation costs of FFN are 8nd2\nm.\n3\n2.2 Efﬁcient Transformer Block\nAs analyzed above, MSA has two shortcomings: (1) The computation scales quadratically with dm\nor naccording to the input token, causing vast overheads for training and inference; (2) Each head in\nMSA only responsible for a subset of embedding dimensions, which may impair the performance of\nthe network, particularly when the tokens embedding dimension (for each head) is short.\nFigure 3: Efﬁcient Multi-Head Self-Attention.\nTo remedy these issues, we propose an efﬁcient multi-head self-attention module (illustrated in\nFigure 3). Here, we make some explanations.\n(1) Similar to MSA, EMSA ﬁrst adopt a set of projections to obtain query Q.\n(2) To compress memory, the 2D input token x ∈Rn×dm is reshaped to 3D one along the spatial\ndimension (i.e., ˆ x∈Rdm×h×w) and then feed to a depth-wise convolution operation to reduce the\nheight and width dimension by a factor s. To make simple, sis adaptive set by the feature map size\nor the stage number. The kernel size, stride and padding are s+ 1, s, and s/2 respectively.\n(3) The new token map after spatial reduction ˆ x∈Rdm×h/s×w/s is then reshaped to 2D one, i.e.,\nˆ x∈Rn′×dm , n′= h/s×w/s. Then ˆ xis feed to two sets of projection to get key K and value V.\n(4) After that, we adopt Eq. 4 to compute the attention function on query Q, K and value V.\nEMSA(Q,K,V) = IN(Softmax(Conv(QKT\n√dk\n)))V (4)\nHere, Conv(·) is a standard 1 ×1 convolutional operation, which model the interactions among\ndifferent heads. As a result, attention function of each head can depend on all of the keys and\nqueries. However, this will impair the ability of MSA to jointly attend to information from different\nrepresentation subsets at different positions. To restore this diversity ability, we add an Instance\nNormalization [26] (i.e, IN(·)) for the dot product matrix (after Softmax).\n(5) Finally, the output values of each head are then concatenated and linearly projected to form the\nﬁnal output.\nThe computation costs of EMSA are O(2dmn2\ns2 + 2d2\nmn(1 + 1\ns2 ) +dmn(s+1)2\ns2 + k2n2\ns2 ), much lower\nthan the original MSA (assume s> 1), particularly in lower stages, where sis tend to higher.\n4\nAlso, we add FFN after EMSA for feature transformation and non-linearity. The output for each\nefﬁcient Transformer block is:\ny= x′+ FFN(LN(x′)), and x′= x + EMSA(LN(x)) (5)\n2.3 Patch Embedding\nThe standard Transformer receives a sequence of token embeddings as input. Take ViT [ 5] as an\nexample, the input image x ∈R3×h×w is split with a patch size of p×p. These patches are ﬂattened\ninto 2D ones and then mapped to latent embeddings with a size ofc, i.e, x ∈Rn×c, where n= hw/p2.\nHowever, this straightforward tokenization is failed to capture low-level feature information (such as\nedges and corners) [32]. In addition, the length of tokens in ViT are all of a ﬁxed size in different\nblocks, making it unsuitable for downstream vision tasks such as object detection and instance\nsegmentation that require multi-scale feature map representations.\nHere, we build an efﬁcient multi-scale backbone, calling ResT, for dense prediction. As introduced\nabove, the efﬁcient Transformer block in each stage operates on the same scale with identical\nresolution across the channel and spatial dimensions. Therefore, the patch embedding modules are\nrequired to progressively expand the channel dimension, while simultaneously reducing the spatial\nresolution throughout the network.\nSimilar to ResNet, the stem module (can be seen as the ﬁrst patch embedding module) are adopted\nto shrunk both the height and width dimension with a reduction factor of 4. To effectively capture\nthe low-feature information with few parameters, here we introduce a simple but effective way, i.e,\nstacking three 3 ×3 standard convolution layers (all with padding 1) with stride 2, stride 1, and stride\n2, respectively. Batch Normalization [13] and ReLU activation [6] are applied for the ﬁrst two layers.\nIn stage 2, stage 3, and stage 4, the patch embedding module is adopted to down-sample the spatial\ndimension by 4×and increase the channel dimension by 2×. This can be done by a standard 3 ×3\nconvolution with stride 2 and padding 1. For example, patch embedding module in stage 2 changes\nresolution from h/4 ×w/4 ×cto h/8 ×w/8 ×2c(shown in Figure 2).\n2.4 Positional Encoding\nPositional encodings are crucial to exploiting the order of sequence. In ViT [5], a set of learnable\nparameters are added into the input tokens to encode positions. Let x ∈Rn×c be the input, θ∈Rn×c\nbe position parameters, then the encoded input can be represent as\nˆ x = x +θ (6)\nHowever, the length of positions is exactly the same as the input tokens length, which limits the\napplication scenarios.\nFigure 4: Patch and PE in ResT.\nTo remedy this issue, the new positional encodings are required\nto have variable lengths according to input tokens. Let us look\ncloser to Eq. 6, the summation operation is much like assigning\npixel-wise weights to the input. Assume θis related with x,\ni.e., θ = GL(x), where GL(·) is the group linear operation\nwith the group number of c. Then Eq. 6 can be modiﬁed to\nˆ x = x + GL(x) (7)\nBesides Eq. 7, θcan also be obtained by more ﬂexible spatial\nattention mechanisms. Here, we propose a simple yet effective\nspatial attention module calling PA(pixel-attention) to encode\npositions. Speciﬁcally, PA applies a 3 ×3 depth-wise convolu-\ntion (with padding 1) operation to get the pixel-wise weight and then scaled by a sigmoid function\nσ(·). The positional encoding with PA module can then be represented as\nˆ x = PA(x) = x∗σ(DWConv(x)) (8)\nSince the input token in each stage is also obtained by a convolution operation, we can embed\nthe positional encoding into the patch embedding module. The whole structure of stage ican be\nillustrated in Figure 4. Note that PA can be replaced by any spatial attention modules, making the\npositional encoding ﬂexible in ResT.\n5\n2.5 Classiﬁcation Head\nThe classiﬁcation head is performed by a global average pooling layer on the output feature map of\nthe last stage, followed by a linear classiﬁer. The detailed ResT architecture for ImageNet-1k is shown\nin Table 1, which contains four models, i.e., ResT-Lite, ResT-Small and ResT-Base and ResT-Large,\nwhich are bench-marked to ResNet-18, ResNet-18, ResNet-50, and ResNet-101, respectively.\nTable 1: Architectures for ImageNet-1k. Here, we make some deﬁnitions. “Conv −k_c_s\" means\nconvolution layers with kernel size k, output channel cand stride s. “MLP_c\" is the FFN structure\nwith hidden channel 4cand output channel c. And “EMSA_n_r\" is the EMSA operation with the\nnumber of heads nand reduction r. “C\" is 64 for ResT-Lite and ResT-Small, and 96 for ResT-Base\nand ResT-Large.“PA\" is short for pixel-wise attention, which are introduced in Section 2.4.\nName Output Lite Small Base Large\nstem 56×56 patch_embed: Conv-3_C/2_2, Conv-3_C/2_1, Conv-3_C_2,PA\nstage1 56×56\n[ EMSA_1_8\nMLP_64\n]\n×2\n[ EMSA_1_8\nMLP_64\n]\n×2\n[ EMSA_1_8\nMLP_96\n]\n×2\n[ EMSA_1_8\nMLP_96\n]\n×2\nstage2 28×28\npatch_embed: Conv-3_2C_2, PA\n[ EMSA_2_4\nMLP_128\n]\n×2\n[ EMSA_2_4\nMLP_128\n]\n×2\n[ EMSA_2_4\nMLP_192\n]\n×2\n[ EMSA_2_4\nMLP_192\n]\n×2\nstage3 14×14\npatch_embed: Conv-3_4C_2, PA\n[ EMSA_4_2\nMLP_256\n]\n×2\n[ EMSA_4_2\nMLP_256\n]\n×6\n[ EMSA_4_2\nMLP_384\n]\n×6\n[ EMSA_4_2\nMLP_384\n]\n×18\nstage4 7×7\npatch_embed: Conv-3_8C_2, PA\n[ EMSA_8_1\nMLP_512\n]\n×2\n[ EMSA_8_1\nMLP_512\n]\n×2\n[ EMSA_8_1\nMLP_768\n]\n×2\n[ EMSA_8_1\nMLP_768\n]\n×2\nClassiﬁer 1 ×1 average pool, 1000d fully-connected\nGFLOPs 1.4 1.94 4.26 7.91\n3 Experiments\nIn this section, we conduct experiments on common-used benchmarks, including ImageNet-1k for\nclassiﬁcation, MS COCO2017 for object detection, and instance segmentation. In the following\nsubsections, we ﬁrst compared the proposed ResT with the previous state-of-the-arts on the three\ntasks. Then we adopt ablation studies to validate the important design elements of ResT.\n3.1 Image Classiﬁcation on ImageNet-1k\nSettings. For image classiﬁcation, we benchmark the proposed ResT on ImageNet-1k, which contains\n1.28M training images and 50k validation images from 1,000 classes. The setting mostly follows [25].\nSpeciﬁcally, we employ the AdamW [19] optimizer for 300 epochs using a cosine decay learning\nrate scheduler and 5 epochs of linear warm-up. A batch size of 2048 (using 8 GPUs with 256 images\nper GPU), an initial learning rate of 5e-4, a weight decay of 0.05, and gradient clipping with a max\nnorm of 5 are used. We include most of the augmentation and regularization strategies of [ 25] in\ntraining, including RandAugment [4], Mixup [34], Cutmix [33], Random erasing [38], and stochastic\ndepth [12]. An increasing degree of stochastic depth augmentation is employed for larger models,\ni.e., 0.1, 0.1, 0.2, 0.3 for ResT-Lite, Rest-Small, ResT-Base, and ResT-Large, respectively. For the\ntesting on the validation set, the shorter side of an input image is ﬁrst resized to 256, and a center\ncrop of 224 × 224 is used for evaluation.\nResults. Table 2 presents comparisons to other backbones, including both Transformer-based ones\nand ConvNet-based ones. We can see, compared to the previous state-of-the-art Transformer-based\narchitectures with similar model complexity, the proposed ResT achieves signiﬁcant improvement\n6\nTable 2: Comparison with state-of-the-art backbones on ImageNet-1k benchmark. Throughput\n(images / s) is measured on a single V100 GPU, following [ 25]. All models are trained and evaluated\non 224×224 resolution. The best records and the improvements over bench-marked ResNets are\nmarked in bold and blue, respectively.\nModel #Params (M) FLOPs (G) Throughput Top-1 (%) Top-5 (%)\nConvNet\nResNet-18 [10] 11.7 1.8 1852 69.7 89.1\nResNet-50 [10] 25.6 4.1 871 79.0 94.4\nResNet-101 [10] 44.7 7.9 635 80.3 95.2\nRegNetY-4G [21] 20.6 4.0 1156 79.4 94.7\nRegNetY-8G [21] 39.2 8.0 591 79.9 94.9\nRegNetY-16G [21] 83.6 15.9 334 80.4 95.1\nTransformer\nDeiT-S [25] 22.1 4.6 940 79.8 94.9\nDeiT-B [25] 86.6 17.6 292 81.8 95.6\nPVT-T [28] 13.2 1.9 1038 75.1 92.4\nPVT-S [28] 24.5 3.7 820 79.8 94.9\nPVT-M [28] 44.2 6.4 526 81.2 95.6\nPVT-L [28] 61.4 9.5 367 81.7 95.9\nSwin-T [18] 28.29 4.5 755 81.3 95.5\nSwin-S [18] 49.61 8.7 437 83.3 96.2\nSwin-B [18] 87.77 15.4 278 83.5 96.5\nResT-Lite (Ours) 10.49 1.4 1246 77.2 (↑7.5) 93.7 (↑4.6)\nResT-Small (Ours) 13.66 1.9 1043 79.6 (↑9.9) 94.9 (↑5.8)\nResT-Base (Ours) 30.28 4.3 673 81.6 (↑2.6) 95.7 (↑1.3)\nResT-Large (Ours) 51.63 7.9 429 83.6 (↑3.3) 96.3 (↑1.1)\nby a large margin. For example, for smaller models, ResT noticeably surpass the counterpart PVT\narchitectures with similar complexities: +4.5% for ResT-Small (79.6%) over PVT-T (75.1%). For\nlarger models, ResT also signiﬁcantly outperform the counterpart Swin architectures with similar\ncomplexities: +0.3% for ResT-Base (81.6%) over Swin-T (81.3%), and +0.3% for ResT-Large\n(83.6%) over Swin-S(83.3%) using 224 ×224 input.\nCompared with the state-of-the-art ConvNets, i.e., RegNet, the ResT with similar model complexity\nalso achieves better performance: an average improvement of 1.7% in terms of Top-1 Accuracy.\nNote that RegNet is trained via thorough architecture search, the proposed ResT is adapted from the\nstandard Transformer and has strong potential for further improvement.\n3.2 Object Detection and Instance Segmentation on COCO\nSettings. Object detection and instance segmentation experiments are conducted on COCO 2017,\nwhich contains 118k training, 5k validation, and 20k test-dev images. We evaluate the performance\nof ResT using two representative frameworks: RetinaNet [17] and Mask RCNN [9]. For these two\nframeworks, we utilize the same settings: multi-scale training (resizing the input such that the shorter\nside is between 480 and 800 while the longer side is at most 1333), AdamW [19] optimizer (initial\nlearning rate of 1e-4, weight decay of 0.05, and batch size of 16), and 1×schedule (12 epochs).\nUnlike CNN backbones, which adopt post normalization and can directly apply to downstream tasks.\nResT employs the pre-normalization strategy to accelerate network convergence, which means the\noutput of each stage is not normalized before feeding to FPN [16]. Here, we add a layer normalization\n(LN [1]) for the output of each stage (before FPN [16]), similar to Swin [18]. Results are reported on\nthe validation split.\n7\nObject Detection Results. Table 3 lists the results of RetinaNet with different backbones. From\nthese results, it can be seen that for smaller models, ResT-Small is +3.6 box AP higher (40.3 vs. 36.7)\nthan PVT-T with a similar computation cost. For larger models, our ResT-Base surpassing the PVT-S\nby +1.6 box AP.\nTable 3: Object detection performance on the COCO val2017 split using the RetinaNet framework.\nBackbones AP50:95 AP50 AP75 APs APm APl Param (M)\nR18 [10] 31.8 49.6 33.6 16.3 34.3 43.2 21.3\nPVT-T [28] 36.7 56.9 38.9 22.6 38.8 50.0 23.0\nResT-Small(Ours) 40.3 61.3 42.7 25.7 43.7 51.2 23.4\nR50 [10] 37.4 56.7 40.3 23.1 41.6 48.3 37.9\nPVT-S [28] 40.4 61.3 43.0 25.0 42.9 55.7 34.2\nSwin-T [18] 41.5 62.1 44.1 27.0 44.2 53.2 38.5\nResT-Base (Ours) 42.0 63.2 44.8 29.1 45.3 53.3 40.5\nR101 [10] 38.5 57.8 41.2 21.4 42.6 51.1 56.9\nPVT-M [28] 41.9 63.1 44.3 25.0 44.9 57.6 53.9\nSwin-S [18] 44.5 65.7 47.5 27.4 48.0 59.9 59.8\nResT-Large (Ours) 44.8 66.1 48.0 28.3 48.7 60.3 61.8\nInstance Segmentation Results. Table 4 compares the results of ResT with those of previous state-\nof-the-art models on the Mask RCNN framework. Rest-Small exceeds PVT-T by +2.9 box AP and\n+2.1 mask AP on the COCO val2017 split. As for larger models, ResT-Base brings consistent +1.2\nand +0.9 gains over PVT-S in terms of box AP and mask AP, with slightly larger model size.\nTable 4: Object detection and instance segmentation performance on the COCO val2017 split using\nMask RCNN framework.\nBackbones APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75 Param (M)\nR18 [10] 34.0 54.0 36.7 31.2 51.0 32.7 31.2\nPVT-T [28] 36.7 59.2 39.3 35.1 56.7 37.3 32.9\nResT-Small(Ours) 39.6 62.9 42.3 37.2 59.8 39.7 33.3\nR50 [10] 38.6 59.5 42.1 35.2 56.3 37.5 44.3\nPVT-S [28] 40.4 62.9 43.8 37.8 60.1 40.3 44.1\nResT-Base(Ours) 41.6 64.9 45.1 38.7 61.6 41.4 49.8\n3.3 Ablation Study\nIn this section, we report the ablation studies of the proposed ResT, using ImageNet-1k image\nclassiﬁcation. To thoroughly investigate the important design elements, we only adopt the simplest\ndata augmentation and hyper-parameters settings in [ 10]. Speciﬁcally, the input images are randomly\ncropped to 224 × 224 with random horizontal ﬂipping. All the architectures of ResT-Lite are trained\nwith SGD optimizer (with weight decay 1e-4 and momentum 0.9) for 100 epochs, starting from the\ninitial learning rate of 0.1 ×batch_size/512 (with a linear warm-up of 5 epochs) and decreasing it\nby a factor of 10 every 30 epochs. Also, a batch size of 2048 (using 8 GPUs with 256 images per\nGPU) is used.\nDifferent types of stem module. Here, we test three type of stem modules: (1) the ﬁrst patch\nembedding module in PVT [ 28], i.e., 4 ×4 convolution operation with stride 4 and no padding;\n(2) the stem module in ResNet [10], i.e., one 7 ×7 convolution layer with stride 2 and padding 3,\nfollowed by one 3 ×3 max-pooling layer; (3) the stem module in the proposed ResT, i.e., three 3 ×3\nconvolutional layers (all with padding 1) with stride 2, stride 1, and stride 2, respectively. We report\nthe results in Table 5. The stem module in the proposed ResT is more effective than that in PVT and\nResNet: +0.92% and +0.64% improvements in terms of Top-1 accuracy, respectively.\n8\nTable 5: Comparison of various stem modules on\nResT-Lite. Results show that the proposed stem\nmodule is more effective than existing ones in\nPVT and ResNet.\nStem Top-1 (%) Top-5 (%)\nPVT [28] 71.96 89.87\nResNet [10] 72.24 90.17\nResT (Ours) 72.88 90.62\nTable 6: Comparison of different reduction strate-\ngies of EMSA on ResT-Lite. Results show that\nAverage Pooling can be an alternative to Depth-\nwise Conv2d to make a trade-off.\nReduction Top-1 (%) Top-5 (%)\nDWConv 72.88 90.62\nAvg Pooling 72.64 90.41\nMax Pooling 72.20 89.97\nAblation study on EMSA. As shown in Figure!3, we adopt a Depth-wise Conv2d to reduce the\ncomputation of MSA. Here, we provide the comparison of more strategies with the same reduction\nstride s. Results are shown in Table 6. As can be seen, average pooling achieves slightly worse\nresults (-0.24%) compared with the original Depth-wise Conv2d, while the results of the Max Pooling\nstrategy are the worst. Since the pooling operation introduces no extra parameters, therefore, average\npooling can be an alternative to Depth-wise Conv2d in practice.\nTable 7: Ablation study results on the important\ndesign elements of EMSA on ResT-Lite, includ-\ning the 1 ×1 convolution operation and Instance\nNormalization in Eq. 4.\nMethods Top-1 (%) Top-5 (%)\norigin 72.88 90.62\nw/o IN 71.98 90.32\nw/o Conv-1&IN 71.72 89.93\nTable 8: Comparison of various positional encod-\ning (PE) strategies on ResT-Lite.\nEncoding Top-1 (%) Top-5 (%)\nw/o position 71.54 89.82\n+ LE 71.98 90.32\n+ GL 72.04 90.41\n+ PA 72.88 90.62\nIn addition, EMSA also adding two important elements to the standard MSA, i.e., one 1 ×1 convolu-\ntion operation to model the interaction among different heads, and the Instance Normalization(IN) to\nrestore diversity of different heads. Here, we validate the effectiveness of these two settings. Results\nare shown in Table 7. We can see, without IN, the Top-1 accuracy is degraded by 0.9%, we attribute\nit to the destroying of diversity among different heads because the 1 ×1 convolution operation makes\nall heads focus on all the tokens. In addition, the performance drops 1.16% without the convolution\noperation and IN. This can demonstrate that the combination of long sequence and diversity are both\nimportant for attention function.\nDifferent types of positional encoding. In section 2.4, we introduced 3 types of positional encoding\ntypes, i.e., the original learnable parameters with ﬁxed lengths [5] (LE), the proposed group linear\nmode(GL), and PA mode. These encodings are added/multiplied to the input patch token at the\nbeginning of each stage. Here, we compared the proposed GL and PA with LE, results are shown in\nTable 8. We can see, the Top-1 accuracy degrades from 72.88% to 71.54% when the PA encoding is\nremoved, this means that positional encoding is crucial for ResT. The LE and GL, achieve similar\nperformance, which means it is possible to construct variable length of positional encoding. Moreover,\nthe PA mode signiﬁcantly surpasses the GL, achieving 0.84% Top-1 accuracy improvement, which\nindicates that spatial attention can also be modeled as positional encoding.\n4 Conclusion\nIn this paper, we proposed ResT, a new version of multi-scale Transformer which produces hierarchi-\ncal feature representations for dense prediction. We compressed the memory of standard MSA and\nmodel the interaction between multi-heads while keeping the diversity ability. To tackle input images\nwith arbitrary, we further redesign the positional encoding as spatial attention. Experimental results\ndemonstrate that the potential of ResT as strong backbones for dense prediction. We hope that our\napproach will foster further research in visual recognition.\n9\nReferences\n[1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,\n2016.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst Bischof, Thomas\nBrox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part I , volume 12346 of Lecture Notes in Computer\nScience, pages 213–229. Springer, 2020.\n[3] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. CoRR, abs/2012.00364, 2020.\n[4] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020.\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR,\nabs/2010.11929, 2020.\n[6] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Geoffrey J.\nGordon, David B. Dunson, and Miroslav Dudík, editors, Proceedings of the Fourteenth International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13,\n2011, volume 15 of JMLR Proceedings, pages 315–323. JMLR.org, 2011.\n[7] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\nCoRR, abs/2103.00112, 2021.\n[8] Karttikeya Mangalam Yanghao Li Zhicheng Yan Jitendra Malik Christoph Feichtenhofer Haoqi Fan,\nBo Xiong. Multiscale vision transformers. arXiv:2104.11227, 2021.\n[9] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International\nConference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2980–2988. IEEE\nComputer Society, 2017.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 770–778. IEEE Computer Society, 2016.\n[11] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error\nlinear units. CoRR, abs/1606.08415, 2016.\n[12] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic\ndepth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016\n- 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV,\nvolume 9908 of Lecture Notes in Computer Science, pages 646–661. Springer, 2016.\n[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Francis R. Bach and David M. Blei, editors,Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 ofJMLR Workshop\nand Conference Proceedings, pages 448–456. JMLR.org, 2015.\n[14] Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional neural\nnetworks encode? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net, 2020.\n[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. InIEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages\n510–519. Computer Vision Foundation / IEEE, 2019.\n[16] Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie.\nFeature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 936–944, 2017.\n[17] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object\ndetection. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29,\n2017, pages 2999–3007. IEEE Computer Society, 2017.\n[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. CoRR, abs/2103.14030, 2021.\n[19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[20] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,\nFlorence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pages 68–80, 2019.\n[21] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Dollár. Designing\nnetwork design spaces. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n10\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10425–10433. IEEE, 2020.\n[22] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. CoRR, abs/2101.11605, 2021.\n[23] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In Andrea\nVedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, 16th European Conference on\nComputer Vision, ECCV 2020, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of\nLecture Notes in Computer Science, pages 282–298. Springer, 2020.\n[24] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: A simple and strong anchor-free object detector.\nCoRR, abs/2006.09214, 2020.\n[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. CoRR, abs/2012.12877,\n2020.\n[26] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing ingredient\nfor fast stylization. CoRR, abs/1607.08022, 2016.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors,Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n[28] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[29] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. CoRR,\nabs/1711.07971, 2017.\n[30] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic, faster and stronger.\nCoRR, abs/2003.10152, 2020.\n[31] Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transfor-\nmations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5987–5995. IEEE Computer Society, 2017.\n[32] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. CoRR, abs/2101.11986,\n2021.\n[33] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In 2019 IEEE/CVF\nInternational Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 6022–6031. IEEE, 2019.\n[34] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n[35] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas\nMueller, R. Manmatha, Mu Li, and Alexander J. Smola. Resnest: Split-attention networks. CoRR,\nabs/2004.08955, 2020.\n[36] Qing-Long Zhang, Lu Rao, and Yubin Yang. Group-cam: Group score-weighted visual explanations for\ndeep convolutional networks. CoRR, abs/2103.13859, 2021.\n[37] Qing-Long Zhang and Yu-Bin Yang. Sa-net: Shufﬂe attention for deep convolutional neural networks.\nCoRR, abs/2102.00240, 2021.\n[38] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative\nApplications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13001–\n13008. AAAI Press, 2020.\n[39] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable\ntransformers for end-to-end object detection. CoRR, abs/2010.04159, 2020.\nA Appendix\nWe provide the related work and more experimental results to complete the experimental sections of\nthe main paper.\nA.1 Related Work\nConvolutional Networks. As the cornerstone of deep learning computer vision, CNNs have been\nevolved for years and are becoming more accurate and faster. Among them, the ResNet series [10,\n31, 35] are the most famous backbone networks because of their simple design and high performance.\n11\nThe base structure of ResNet is the residual bottleneck, which can be deﬁned as a stack of one 1 ×1,\none 3 ×3, and one 1 ×1 Convolution layer with residual learning. Recent works explore replacing\nthe 3 ×3 Convolution layer with more complex modules [ 22, 15] or combining with attention\nmodules [37, 29]. Similar to vision Transformers, CNNs can also capture long-range dependencies if\ncorrectly incorporated with self-attention such as Non-Local or MSA. These studies show that the\nadvantage of CNN lies in parameter sharing and focuses on the aggregation of local information,\nwhile the advantage of self-attention lies in the global receptive ﬁeld and focuses on the aggregation of\nglobal information. Intuitively speaking, global and local information aggregation are both useful for\nvision tasks. Effectively combining global information aggregation and local information aggregation\nmay be the right direction for designing the best network architecture.\nVision Transformers. Transformer is a type of neural network that mainly relies on self-attention\nto draw global dependencies between input and output. Recently, Transformer-based models are\nexplored to solve various computer vision tasks such as image processing [3], classiﬁcation [5, 5, 32],\nand object detection [2, 39], etc. Here, we focus on investigating the classiﬁcation vision Transformers.\nThese Transformers usually view an image as a sequence of patches and perform classiﬁcation with a\nTransformer encoder. The encoder consists of several Transformer blocks, each including an MSA\nand an FFN. Layer-norm (LN) is applied before each layer and residual connections are employed in\nboth the self-attention and FFN module.\nAmong them, ViT [5] is the ﬁrst fully Transformer classiﬁcation model. In particular, ViT split each\nimage into 14 ×14 or 16 ×16 with a ﬁxed length, then several Transformer layers are adopted\nto model global relation among these tokens for input classiﬁcation. DeiT [ 25] explores the data-\nefﬁcient training and distillation of ViT. Tokens-to-Tokens (T2T-ViT) [32] point out that the simple\ntokenization of input images in ViT fails to model the important local structure (e.g., edges, lines)\namong neighboring pixels, leading to its low training sample efﬁciency. Transformer-in-Transformer\n(TNT) [7] split each image into a sequence of patches and each patch is reshaped to pixel sequence.\nAfter embedding, two Transformer layers are applied for representation learning where the outer\nTransformer layer models the global relationships among the patch embedding and the inner one\nextracts local structure information of pixel embedding. Pyramid Vision Transformer (PVT) [ 28]\nfollows the ResNet paradigm to construct Transformer backbones, making it more suitable for\ndownstream tasks. MViT [8] further apply pooling function to reduce computation costs.\nPositional Encoding. Different from CNNs, which can implicitly encode spatial position information\nby zero-padding [14], the self-attention in Transformers has no ability to distinguish token order in\ndifferent positions. Therefore, positional encoding is essential for the patch embeddings to retain\npositional information. There are mainly two types of positional encoding most commonly applied in\nvision Transformers, i.e., absolute positional encoding and relative positional encoding. The absolute\npositional encoding is used in ViT [ 5] and its extended methods, where a standard learnable 1D\nposition embedding is added to the sequence of embedded patches. The relative method is used in\nBoTNet [22] and Swin Transformer [18], where the split 2D relative position embeddings are added.\nGenerally speaking, the relative positional encodings are better suited for vision tasks, this can be\nattributed to attention not only taking into account the content information but also relative distances\nbetween features at different locations [20].\nA.2 Visualization and Interpretation\nAnalysis on EMSA. In this part, we measure the diversity of EMSA. To simplify, we apply A ∈\nRk×n×n′\nto denote the attention map, with kbe the number of heads in EMSA. Assume Ai ∈R1×m\nis the i-th attention map (after reshape), where m = nn′. Then we can compute the cross-layer\nsimilarity to measure the diversity of different heads.\nMij = AiAT\nj\n∥Ai∥∥Aj∥ (9)\nTo thoroughly measure the diversity, we calculate extract three types ofA: the attention map before\nConv-1 (i.e., QKT/√dk), the one after Conv-1 (i.e., Conv(QKT/√dk)), and the one after Instance\nNormalization [26] (i.e., IN(Softmax(Conv(QKT/√dk)))). We randomly sample 1,000 images\nfrom the ImageNet-1k validation set and visualize the mean diversity in Figure 5.\n12\nFigure 5: Attention map visualization of the last blocks of stage 4 of the ResT-Lite.\nAs shown in Figure 5b, although the 1 ×1 convolution model the interactions among different heads,\nit impairs the ability of MSA to jointly attend to information from different representation subsets at\ndifferent positions. After instance normalization (in Figure 5c), the diversity ability is restored.\nInterpretabilty. In order to validate the effectiveness of ResT more intuitively, we sample 6 images\nfrom the ImageNet-1k validation split. We use Group-CAM [ 36] to visualize the heatmaps at the\nlast convolutional layer of ResT-Lite. For comparison, we also draw the heatmaps of its counterpart\nResNet-50. As shown in Figure 6, the proposed ResT-Lite can adaptively produce heatmaps according\nto the image content.\nFigure 6: Sample visualization on ImageNet-1k val split generated by Group-CAM [36].\nA.3 More Experiments\nComparisons of EMSA and MSA. In this part, we make a quantitative comparison of the perfor-\nmance and efﬁciency of the two modules on ResT-Lite (keeping other components intact). Experi-\nmental settings are the same as Ablation Study (Section 3.37), except for the batch size of MSA,\nwhich is 512 since each V100 GPU can only tackle 64 images at the same time. We report the results\nin Table 9. Both versions of ResT-Lite share almost the same parameters. However, the EMSA\nversion achieves better Top-1 accuracy (+0.2%) with fewer computations(-0.2G). The actual inference\nthroughput indicates EMSA (1246) is 2.4x faster than the original MSA(512). Therefore, EMSA can\ncapably serve as an effective replacement for MSA.\n13\nTable 9: Comparison of MSA and EMSA.\nModel #Params (M) FLOPs (G) Throughput Top-1 (%) Top-5 (%)\nMSA 10.48 1.6 512 72.68 90.46\nEMSA 10.49 1.4 1246 72.88 90.62\nObject Detection. In Section 3.2, we replace the backbone in RetinaNet [17] with ResT and add a\nlayer normalization (LN [1]) for the output of each stage (before FPN [ 16]), just like Swin. Here,\nwe further validate the effectiveness of LN. We follow the same setting as Section 3.2. Results are\nreported in Table 10.\nTable 10: Object Detection.\nBackbones Setting AP50:95\nResT-Small w/o LN 39.5\nw LN 40.3\nResT-Base w/o LN 41.2\nw LN 42.0\nFrom Table 10, we can see, LN is indeed matters in downstream tasks. An average +0.8 box AP\nimprovement is achieved with LN for RetinaNet [17].\nA.4 Discussions\nMathematical Deﬁnition of GL. Given xwith size Rn×dm , where nis spatial dimension and dm is\nthe channel dimension, GL ﬁrst splits xinto gnon-overlapping groups,\nx= Concat(x1,··· ,xg) (10)\nwhere the size of xi is n×dm\ng .\nAll xi’s are then simultaneously transformed byglinear operations to produce goutputs\nyi = xiWi (11)\nwhere Wi is the linear operation weight.\nyi’s are then concatenated to produce the ﬁnaln×dm output y= Concat(y1,··· ,yg). In ResT, we\nset g= dm, i.e., the channel dimension of the input.\nAblation Study Settings. Note that the settings between ablation study and the main results are\ndifferent. Here, we give the explanations. We adopt the simplest data augmentation and hyper-\nparameters settings in ResNet [ 10] to thoroughly investigate the important components of ResT,\ni.e., eliminating the inﬂuence of strong data augmentation and training tricks. Under this setting,\nwe demonstrate that Vision Transformers can still achieve better results without training tricks.\nSpeciﬁcally, the Top-1 accuracy of ResT-Lite is 72.88, which outperforms the ResNet-18 (69.76) by\n+3.1 improvement. We believe the same setting as ResNet in the ablation study can eliminate the\ndoubt of Vision Transformer to some extent, i.e., the improvements of Vision Transformer over CNN\nmainly come with strong data augmentation and training tricks. This can promote the ongoing and\nfuture research of Vision Transformer.\n14"
}