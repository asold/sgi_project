{
  "title": "Federated Fine-Tuning for Pre-Trained Foundation Models Over Wireless Networks",
  "url": "https://openalex.org/W4406856790",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2155031994",
      "name": "Zixin Wang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2107158513",
      "name": "Yong Zhou",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2675978000",
      "name": "Yuanming Shi",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A3184290674",
      "name": "Khaled B. Letaief",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4408326133",
    "https://openalex.org/W4390659326",
    "https://openalex.org/W4384158407",
    "https://openalex.org/W3212941463",
    "https://openalex.org/W4390828922",
    "https://openalex.org/W6777349122",
    "https://openalex.org/W4399665791",
    "https://openalex.org/W4399665748",
    "https://openalex.org/W4400111235",
    "https://openalex.org/W6863479216",
    "https://openalex.org/W4399665712",
    "https://openalex.org/W4391590983",
    "https://openalex.org/W4390873714",
    "https://openalex.org/W6849437247",
    "https://openalex.org/W4386804501",
    "https://openalex.org/W6837789219",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W4392796555",
    "https://openalex.org/W4393152647",
    "https://openalex.org/W4399665798",
    "https://openalex.org/W6855950187",
    "https://openalex.org/W6862979923",
    "https://openalex.org/W4401508637",
    "https://openalex.org/W4401508395",
    "https://openalex.org/W6870809866",
    "https://openalex.org/W3089655738",
    "https://openalex.org/W4320015823",
    "https://openalex.org/W4389610145",
    "https://openalex.org/W2999074226",
    "https://openalex.org/W6862993741",
    "https://openalex.org/W6869008638",
    "https://openalex.org/W6870173257",
    "https://openalex.org/W3212347076",
    "https://openalex.org/W3046726468",
    "https://openalex.org/W3168768468",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3126228234",
    "https://openalex.org/W4220937845"
  ],
  "abstract": "Pre-trained foundation models (FMs), with extensive number of neurons, are key to advancing next-generation intelligence services, where personalizing these models requires massive amount of task-specific data and computational resources. The prevalent solution involves centralized processing at the edge server, which, however, raises privacy concerns due to the transmission of raw data. Instead, federated fine-tuning (FedFT) is an emerging privacy-preserving fine-tuning (FT) paradigm for personalized pre-trained foundation models. In particular, by integrating low-rank adaptation (LoRA) with federated learning (FL), federated LoRA enables the collaborative FT of a global model with edge devices, achieving comparable learning performance to full FT while training fewer parameters over distributed data and preserving raw data privacy. However, the limited radio resources and computation capabilities of edge devices pose significant challenges for deploying 3 LoRA over wireless networks. To this paper, we propose a split federated LoRA framework, which deploys the computationally-intensive encoder of a pre-trained model at the edge server, while keeping the embedding and task modules at the edge devices. The information exchanges between these modules occur over wireless networks. Building on this split framework, the paper provides a rigorous analysis of the upper bound of the convergence gap for the wireless federated LoRA system. This analysis reveals the weighted impact of the number of edge devices participating in FedFT over all rounds, motivating the formulation of a long-term upper bound minimization problem. To address the long-term constraint, we decompose the formulated long-term mixed-integer programming (MIP) problem into sequential sub-problems using the Lyapunov technique. We then develop an online algorithm for effective device scheduling and bandwidth allocation. Simulation results demonstrate the effectiveness of the proposed online algorithm in enhancing learning performance. Â© 2025 IEEE.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6893325448036194
    },
    {
      "name": "Wireless",
      "score": 0.6265299320220947
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5554900765419006
    },
    {
      "name": "Wireless network",
      "score": 0.5229718685150146
    },
    {
      "name": "Computer network",
      "score": 0.49144017696380615
    },
    {
      "name": "Telecommunications",
      "score": 0.3950320780277252
    },
    {
      "name": "Political science",
      "score": 0.08956512808799744
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    }
  ]
}