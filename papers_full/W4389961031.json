{
    "title": "Remote Sensing Image Road Segmentation Method Integrating CNN-Transformer and UNet",
    "url": "https://openalex.org/W4389961031",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2036086788",
            "name": "Rui Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2163242393",
            "name": "Mingxiang Cai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2511861557",
            "name": "Zixuan Xia",
            "affiliations": [
                "Heilongjiang University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2478409957",
            "name": "Zhi-cui Zhou",
            "affiliations": [
                "Heilongjiang University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2935921904",
        "https://openalex.org/W2980080875",
        "https://openalex.org/W3119762707",
        "https://openalex.org/W2966089644",
        "https://openalex.org/W3190822327",
        "https://openalex.org/W3216070494",
        "https://openalex.org/W2791426782",
        "https://openalex.org/W3147204088",
        "https://openalex.org/W3034124162",
        "https://openalex.org/W3198062544",
        "https://openalex.org/W3189999916",
        "https://openalex.org/W3188900617",
        "https://openalex.org/W3128987237",
        "https://openalex.org/W3094166412",
        "https://openalex.org/W3034897322",
        "https://openalex.org/W2991212029",
        "https://openalex.org/W2972005800",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W3176443629",
        "https://openalex.org/W2566121015",
        "https://openalex.org/W2945408524",
        "https://openalex.org/W4213058019",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W6739696289",
        "https://openalex.org/W3162672913",
        "https://openalex.org/W3188836801",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2955608659",
        "https://openalex.org/W4313134080",
        "https://openalex.org/W6682889407",
        "https://openalex.org/W6746023985",
        "https://openalex.org/W2982083293",
        "https://openalex.org/W6737664043",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2630837129",
        "https://openalex.org/W2623331213",
        "https://openalex.org/W4297775537",
        "https://openalex.org/W4320930577",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W1686810756"
    ],
    "abstract": "Real-time and accurate road information is crucial for updating electronic navigation maps. To address the problem of low precision and poor robustness in current semantic segmentation methods for road extraction from remote sensing imagery, we proposed a UNet road semantic segmentation model based on attention mechanism improvement. First, we introduce a CNN-Transformer hybrid structure to the encoder to enhance the feature extraction capabilities of global and local details. Second, the traditional upsampling module in the decoder is replaced with a dual upsampling module to improve feature extraction capabilities and segmentation accuracy. Furthermore, the hard-swish activation function is used instead of ReLU activation function to smooth the curve, which helps to improve the generalization and non-linear feature extraction abilities and avoid gradient vanishing. Finally, a comprehensive loss function combining cross entropy and dice is used to strengthen the segmentation result constraints and further improve segmentation accuracy. Experimental validation is performed on the Ottawa Road Dataset and the Massachusetts Road Dataset. Experimental results show that compared with U-Net, PSPNet, DeepLab V3 and TransUNet networks, this algorithm is the best in terms of MIoU, MPA and F1 score. Among them, on the Ottawa road data set, the MPA of this algorithm reached 95.48&#x0025;. On the Massachusetts road data set, MPA is 92.56&#x0025;. This method shows good performance in road extraction.",
    "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nRemote sensing image road \nsegmentation method integrating CNN-\nTransformer and UNet \nRui Wang1, Mingxiang Cai1,*, and Zixuan Xia2，Zhicui Zhou3 \n1 China Transport Telecommunications \\& information Center, Beijing 100011, China  \n2 Heilongjiang University of Technology, Heilongjiang 150022, China   \n3 No. 1 Middle School of Jixi City, Heilongjiang 150022, China  \nCorresponding author: Mingxiang Cai (e-mail: jamtsai@whu.edu.cn). \n \nABSTRACT Real-time and accurate road information is crucial for updating electronic navigation maps. To \naddress the problem of low precision and poor robustness in current semantic segmentation methods for road \nextraction from remote sensing imagery, we proposed a UNet road semantic segmentation model based on \nattention mechanism improvement. First, we introduce a CNN -Transformer hybrid structure to the encoder \nto enhance the feature extraction capabilities of global and local details. Second, the traditional upsamp ling \nmodule in the decoder is replaced with a dual upsampling module to improve feature extraction capabilities \nand segmentation accuracy.  Furthermore, the hard -swish activation function is used instead of ReLU \nactivation function to smooth the curve, which helps to improve the generalization and non -linear feature \nextraction abilities and avoid gradient vanishing. Finally, a comprehensive l oss function combining cross \nentropy and dice is used to strengthen the segmentation result constraints and further im prove segmentation \naccuracy. Experimental validation is performed on the Ottawa Road Dataset and the Massachusetts Road \nDataset. Experimental results show that compared with U -Net, PSPNet, DeepLab V3 and TransUNet \nnetworks, this algorithm is the best in terms of MIoU, MPA and F1 score . Among them, on the Ottawa road \ndata set, the MPA of this algorithm reached 95.48%. On the Massachusetts road data set, MPA is 92.56%. \nThis method shows good performance in road extraction. \nINDEX TERMS road segmentation; deep learning; CNN-Transformer; attention; UNet \nI. INTRODUCTION \nReal-time and accurate road information is crucial for \nupdating navigation electronic maps, and road extraction is \nan important issue in the field of computer vision [1]. In \npractical applications, roads are an important basis for \ntransportation, and accur ate road extraction can improve \ntraffic safety and efficiency in fields such as autonomous \ndriving and intelligent transportation [2]. With the \ncontinuous development of computer vision and artificial \nintelligence, road extraction algorithms based on deep \nlearning have become a very popular research direction [3]. \nCompared with traditional algorithms, deep learning -based \nalgorithms have stronger self -learning and feature \nextraction capabilities, better adaptability, and higher \naccuracy [4]. These algorithms  can extract road \ninformation from images, help autonomous vehicles better \nunderstand the environment in which they operate, provide \ncorrect driving decisions, and improve the safety and \nefficiency of autonomous vehicles [5]. In addition, road \nextraction i s also crucial in urban planning. By extracting \nroad information in the city, urban planners can better \nunderstand information such as traffic flow and road layout, \nso as to better carry out urban planning and improve traffic \nconditions [6]. Therefore, road extraction can improve the \nsafety and efficien cy of self -driving vehicles and help \nurban planners better understand urban road layout and \ntraffic flow, thereby better planning the city and improving \ntraffic conditions.  \nTraditional road extraction algorithms mainly rely on \ngeometric and physical principles in image processing, and \nusually require a large amount of manual intervention and \nparameter optimization [7,  8]. In recent years, deep \nlearning algorithms have achieved significant success in \nthe field of computer vision, especially in speech and image \nrecognition [9, 10]. Deep learning algorithms have \npowerful self -learning and feature extraction capabilities, \nand can deeply mine deep features in data. They are also \nused to solve road extraction problems [11,  12]. How to \nachieve high -reliability road extraction based on deep \nlearning algorithms, in -depth research on the better \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nadaptability of deep learning models to complex terrain \nimages, and achieve high accuracy and robustness in \npractice have become hot issues. In recent years, commonly \nused semantic segmentation methods for road extraction \ninclude traditional methods and im age segmentation based \non deep learning [13, 14]. Traditional semantic \nsegmentation methods use the texture, color, geometric \nshape and spatial structure information of the image to \nsegment objects, dividing pixels with the same semantics \ninto a region, an d there is no intersection between each \nregion [2, 15]. Traditional semantic segmentation can be \ndivided into threshold -based, clustering -based, edge -based \nand region -based segmentation types [3, 16]. However, \ntraditional methods have high computational co mplexity \nand long processing time, and cannot effectively handle \nhigh-resolution images with noise, multiple objects, and \ncomplex backgrounds. Therefore, the accuracy of the \nsegmentation results is low, resulting in the limited scope \nof application of trad itional methods [17,18]. With the \nemergence of deep neural networks, image segmentation \ntechnology based on deep learning has entered a stage of \nrapid development beyond traditional methods [19, 20].  \nCurrently, existing research mainly relies on designing \nroad extraction algorithms based on convolutional neural \nnetworks (CNNs) [21, 22]. The principle of CNN is to use \na large amount of road extraction training data to train \nweights. In the process of sc anning the entire image, \nconvolution operations are performed and further feature \ninformation is extracted from the image. Compared with \ntraditional methods, this method has good self -learning and \nfeature extraction capabilities, can automatically mine \ncertain features in the data, and provide effective solutions \nfor scenarios in complex environments. However, the CNN \nnetwork has shortcomings such as inputting fixed -size \nimages. In recent years, Long et al. [ 23] proposed fully \nconvolutional networks (FCN) to introduce encoders and \ndecoders into the field of image segmentation. Ronneberger \net al. [ 24] proposed an improved version of FCN called \nUNet. This version connects the feature maps of the \nencoder and decoder to form a ladder structure, allowing \neach dec oder to learn features lost in the encoder, thereby \nimproving the segmentation capabilities of remote sensing \nimages. Zhao et al. [ 25] proposed PSPNet, which uses the \npyramid pooling module to significantly improve the \nability to extract global information features. Chen et al. \n[26] proposed DeepLabV3, which uses dilated convolution \nto extract features, and uses ASPP module to expand the \nreceptive field and enhance the feature extraction ability. \nThe above networks have achieved good application results \nin remote sensing image segmentation tasks. Therefore, \nmany scholars in the field of remote sensing have improved \nthe above network and designed a network structure \nsuitable for remote sensing image road segmentation tasks. \nAmong them, Kong et al. [ 27] proposed an improved UNet \nnetwork for extracting road information  from remote \nsensing images. This method adds a stripe pool module to \nthe down -sampling part of the coding layer to focus on \nlocal information. A hybrid pool module is added to the \nconvolution of the coding layer to enhance its ability to \nobtain network context. The al gorithm was verified by \nusing the GF -2 remote sensing image data set. The results \nshow that the algorithm can effectively extract the road. Li \net al. [ 28] proposed an improved UNet network, which \ncombines core attention and global attention to  extract \nroads from remote sensing images. Experiments were \nconducted on the Massachusetts road dataset and the \nDeepGlobe CVPR 2018 road dataset. The results show that \nthe method can effectively extract the road area occluded \nby the canopy and improve the connectivity of the road \nnetwork. Han et al. [ 29] added dilated convolution to \nextract road information on the basis of DeepLab V3 \nnetwork. Experiments show that this method has higher \nextraction accuracy than other methods. Liu et al. [ 30] \nproposed a Deep Lab V3 + road extraction method with \nattention module. This method obtains more spatial context \ninformation through spatial attention and enhances the \nextraction of road information. The effectiveness of the \nmethod is verified by using the Cityscapes datas et. The \nresults show that the ability of this method to extract road \ninformation is significantly better than that of the original \nnetwork. \nAlthough these methods have their own advantages in \nroad extraction, they still have some shortcomings. There \nis still the problem of incomplete context information \nextraction during the road extraction process. And with the \nimprovement of remote sensing i mage quality, problems \nsuch as lack of road details and discontinuous extraction are \nfully exposed under complex background information. \nSince its establishment in 2017, Transformer has quickly \ndominated the field of natural language processing (NLP) \nand has become an absolute leader in a short period of time. \nVaswani et al. [3 1] proposed Transformer 's self -attention \n(SA) mechanism, which focuses on extracting the feature \ninformation of the object of interest in the feature map, \nreducing unnecessary information and improving the \nefficiency of feature extraction. Chen et al. [ 32] proposed \nTransUNet (a combination of Transformers and UNet), \nwhich integrates Transformer into UNet network to obtain \nglobal image connection, realize multi -scale prediction and \nsupervision of feature maps, and combines the advantages \nof Transformer and UNet. The above scholars have verified \nthat the integration of Transformer into the UNet network \nhas a good effect, but when it is applied to the road semantic \nsegmentation scene, it is easily affected by seasonal \nchanges, lighting conditions and environmental impacts. \nAccurate segmentation in complex background is \nchallenging. In the coding process, there is a lack of local \ninformation exchange, and the ability to extract features \nsuch as image edges and geometric shapes is relatively \nweak. In order to solve the above problems, we propose an \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nimproved road semantic segmentation network based on \nUNet. The main contributions of this work are as follows :  \n(1) We introduce a combined loss function of cross \nentropy (CE) and Dice to strengthen the constraints of the \nmodel on the segmentation results and further improve the \nsegmentation accuracy.  \n(2) In the encoder, a CNN -Transformer hybrid structure \nis added to enhance the feature extraction ability of global \ninformation and local detail information.  \n(3) In the decoder, a double upsampling module is \nintroduced to improve the feature extraction ability and \nsegmentation accuracy. We use the hard -swish activation \nfunction to enhance the generalization and nonlinear \nfeature extraction capabilities and prev ent the gradient \nfrom disappearing.  \nII. Methods  \nThis section includes the network architecture and module \ndetails of the UNet road extraction semantic segmentation \nmodel that has been improved based on the attention \nmechanism in this paper. It also includes the CNN -\nTransformer hybrid structure, dual up-sample module, hard-\nswish activation function, and cross entropy + dice loss \nfunction. The improved UNet network model architecture is \nshown in Figure 1. \n \nFIGURE 1. Improved UNet architecture. \nA. CNN-Transformer hybrid structure \nTraditional CNN has good local perception capabilities and \nlocal spatial information. Different convolution kernels can \nhave different receptive fields. However, CNN may lose some \nfeature information in the pooling layer and lacks correlation \nbetween local and global features. However, the Transformer \nnetwork has a self-attention structure and has a strong ability \nto extract global information features. Therefore, we adopt a \nCNN Transformer hybrid structure in the encoder part to \nimprove the local and global correlation in the CNN road \ninformation extraction process. Among them, CNN serves as \na feature extractor and learns through convolution operations \nto obtain detailed high -resolution spatial information . \nAfterwards, we introduced the self-attention mechanism of the \nTransformer into the encoder design, which improved the \nlimitation of convolutional operations not being able to \nestablish long-distance models. \nFirst, the original image is input into the CNN for feature \nextraction. Three layers of convolutional downsampling are \nperformed, which reduces the size of the feature map to 1/2, \n1/4, and 1/8 of the original image. Then, the downsampled \nimages are input into the embedding layer. The images are first \ndivided into patches, then mapped to one-dimensional vectors \nvia linear mapping based on the given size. This outputs a \nvector sequence, or a two-dimensional matrix, which is then \ninputted to the Transformer layer for 10 iterations. \nThe Transformer module with global self-attention is based \non the vision Transformer [ 33], which includes layer norm, \nmulti-head self -attention module (MSA), and multi -layer \nperceptron (MLP). The operation formula of its multi -head \nself-attention module is shown in Eq. (1). \n \n( , , ) ( )\nT\nk\nQKAttention Q K V Softmax V\nd\n=  (1)  \nB. Dual up-sample module \nUNet neural network belongs to the model framework of \nencoding and decoding. Features are learned by convolutional \noperations, and the feature map is smaller than the original \nimage, so it needs to be upsampled to restore the image size in \nthe decoder. Ordinary deconvolution causes the checkerboard \neffect, while the reverse max pooling and pixel shuffle \nupsampling methods destroy the continuity of the features. \nMoreover, the basic bilinear upsampling does not have \nlearnable parameters and the effect is poor. The main function \nof the pixel shuffle convolution layer is to obtain a high -\nresolution feature map by convolving and reconstituting a \nlow-resolution feature map with multiple channels, effectively \nimproving the checkerboard effect. Compared to the nearest \nneighbor method, bilinear upsampling is smoother, faster, and \nrequires less computation. \nTang et al. [ 34] used bilinear interpolation and \ndeconvolution to construct upsampling blocks to reduce the \nnegative effects of downsampling in the pooling stage. Fan et \nal. [35] proposed a dual upsampling block structure to prevent \nthe checkerboard effect, which was used in image denoising \ntasks. We replaced the original upsampling scheme of UNet \nwith a dual up -sample module, which combines the pixel \nshuffle convolution layer and bilinear upsampling, and added \nthe hard -swish activation function to improve the feature \nextraction ability and image edge segmentation accuracy \nduring upsampling,  to prevent checkerboard effects and \ncompensate for the loss of feature resolution caused by the \nTransformer. The dual up-sample module, as shown in Figure \n2, is an upsampling structure composed of two channels, each \ncontaining two convolutional layers, a hard-swish layer, a \npixel shuffle convolution layer, and bilinear interpolation. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \n \nFIGURE 2. Dual up-sample module. \nC. Hard-swish activation function \nIn traditional neural networks, the choice of activation \nfunction has a significant impact on both training efficiency \nand performance of the model. The most widely used \nactivation function is ReLU [ 36]. Ramachandran et al. [ 37] \nproposed the swish activation function, which performs better \nthan ReLU in deep networks. Swish has a simple structure and \nis similar to ReLU, but its disadvantage is that it requires more \ncomputation. \n \n( 3)() 6\nReLU xhard swish x x +− =   (2) \nWe used the hard -swish activation function proposed by \nHoward et al. [ 38] in MobileNetV3, as shown in Eq. (2). \nCompared to swish, hard-swish has better numerical stability \nand computational speed, and using the hard-swish non-linear \nactivation function did not show a significant difference in \naccuracy in our experiments. The function graph of hard-swish \nand its derivative (hard-swish'(x)) are shown in Figure 3. In \npractical applications, the hard-swish activation function can \nbe reduced using a segmented method to reduce the number of \nmemory accesses and significantly reduce waiting time, as \nshown in Eq. (3). \n \n0, 3\n( ) , 3\n( 3) / 6,\nx\nhard swish x x x\nx x other\n−\n− =  \n +  (3)  \n \nFIGURE 3. The function image of Hard-swish and its derivative. \nD. Cross entropy and dice mixed loss function \nDuring the neural network training phase, the loss function \nis used to calculate the difference between the iteration result \nand the actual value, in order to guide subsequent training in \nthe correct direction. Improvements to the loss function \nmainly focus on the problem of class imbalance. We used a \nhybrid loss function that combines the fusion of cross-entropy \nand dice loss function [44]. This method combines the stability \nof cross-entropy loss and the ability to address class imbalance \nissues, without affecting the characteristics of the dice loss. \nThe hybrid loss function we used has better stability than the \ndice loss and can better solve the problem of class imbalance \nthan the cross-entropy. The formula for the cross-entropy loss \nfunction and the dice loss function are shown in Eq. (4) and \n(5), respectively. \n \n1\n11 lg( )\nC\nCE i ic ic\ni i c\nLoss L y pNN =\n=−    (4) \nThe cross-entropy loss function is used to evaluate the loss \ncaused by pixel classification when image data is segmented. \nIt can measure the difference between two different \nprobability distributions under the same random variable. The \nhigher the numerical value, the better the model's prediction \nresult. Among them, C is the number of categories, indicating \nwhether the category is i, if so \n1iy = , if not \n0iy = . \nip  refers \nto the probability that sample i belongs to category C. \n \n21Dice\nXYLoss XY\n=− +  (5) \nDice loss function is used to measure the similarity between \nthe predicted segmentation image and the actual segmentation \nimage. The value range is . Among them,   represents the \nintersection of the true and the predicted value,   and   \nrepresents the number of elements. The comprehensive loss \nfunction of the improved UNet model is  , as shown in Eq. (6). \nAmong them,  , . \n \nTotal CE DiceLoss Loss Loss=+  (6)  \nE. Evaluation metrics \nThe commonly used evaluation indicators for semantic \nsegmentation include pixel accuracy (PA), mean pixel \naccuracy (MPA), mean intersection over union (MIoU), \nprecision (P), recall (R), F1 score, etc. \nFor the convenience of explaining the calculation formula \nfor evaluation metrics, if the number of pixel points is k, \njjp  \nrepresents the total number of pixels belonging to class i and \npredicted as class i (TP), \njjp  represents the actual number of \npixels belonging to class j and predicted as class j (TN), \nijp  \nrepresents the total number of pixels belonging to class i but \npredicted as class j (FP), and \njip  represents the total number \nof pixels belonging to class j but predicted as class i. \nPixel accuracy (PA) represents the ratio of the number of \ncorrectly classified pixels in the test target to the total number \nof pixels in the whole test area, as shown in Eq. (7). Mean pixel \naccuracy (MPA) is the mean value of all PA values, as shown \nin Eq. (8). Here, C is the number of classes, and c is one of the \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nclasses. Mean intersection over union (MIoU) is obtained by \ncalculating the average of intersection over union (IoU), which \nis the ratio of intersection to union of the true and predicted \nareas in the test dataset and is defined in Eq. (9). MIoU is \ndefined in Eq. (10). Precision, recall, and F1 score are defined \nin Eqs. (11)~(13), respectively. \n \n00\n0 0 0 0 0 0\nkk\nii jj\nij\nk k k k k k\nii jj ij ji\ni j i j j i\nPP\nPA\nP P P P\n==\n= = = = = =\n+\n=\n+ + +\n\n     (7)  \n \n0\n1 C\nc\nc\nMPA PAC =\n=   (8)  \n \n0\n0 0 0 0 0\nk\nii\ni\nk k k k k\nii ij ji\ni i j j i\nP\nIoU\nP P P\n=\n= = = = =\n=\n++\n\n    (9)  \n \n0\n1 C\nc\nc\nMIoU IoUC =\n=   (10)  \n \n00\n00\nkk\njjii\nkk\nij\nij ii ji jj\nji\nPPP or\nP P P P==\n==\n=\n++\n\n  (11)  \n \n0\n0\nk\nii\nk\ni\nji ii\nj\nPR\nPP=\n=\n=\n+\n\n  (12)  \n \n21 PRF PR\n= +  (13)  \nIII. Results  \nA. Dataset \nThe road extraction images used in the experiment come \nfrom the publicly available Massachusetts road Dataset and \nOttawa road Dataset. The Massachusetts roads dataset \nconsists of 1,171 aerial images of Massachusetts, each image \nis 1500 ×  1500 pixels in size and has a spatial resolution of \n1m. The spatial resolution of the Ottawa road dataset is 0.2m, \nand the dataset covers various areas in cities suburbs and \nrural areas. We select images in dense road areas for \ncropping. Both data are cropped to a size of 512× 512 pixels, \nand rotated 90 degrees, 180 degrees and 270 degrees for data \nexpansion. The images and labels are shown in Figure 1. \n \nFIGURE 4. Dataset images and labels. (a) Image of Massachusetts -\nDataset; (b) Corresponding label of Massachusetts -Dataset; (c) Image \nof Ottawa-Dataset; (d) Ottawa-Dataset corresponding label. \nB. Setup \nThe experiment used the PyTorch deep learning framework, \nthe Windows 10 operating system, an NVIDIA GeForce RTX \n3080Ti GPU, and 32 GB of RAM. The detailed configuration \nis shown in Table 1. \nTABLE I \nEXPERIMENTAL CONFIGURATION. \nName Settings \nOperating system Windows10 \nExperimental environment Python 3.6.13 \nDeep learning framework PyTorch 1.7.0 \nCUDA CUDA 11.0 \nGPU NVIDIA GeForce RTX 3080Ti (12 \nGB) \nCPU Intel® Core™ i7-12700 \nMemory 32 GB \nC. Training \nWe trained the Massachusetts dataset using the improved \nUNet network model. To measure the effectiveness of the \nproposed model in this paper and  taking into account  the \nperformance of the experimental equipment and pre-training \nefficiency, the batch size for each experiment was set to 16, \nthe initial learning rate was set to 0.01, and the epoch was set \nto 100. Pre-training improved the training speed of the model \nand effectively enhanced the network fitting, achieving the \ngoal of improving road segmentation accuracy on a limited \ndataset. The MIoU of the experiment finally reached 84.97%. \nThe pre-trained model was trained on the Massachusetts \ndataset, and the training results were recorded every 5 epochs. \nThe MIoU was calculated to measure the segmentation \naccuracy. Figure 4 shows the variation in MIoU over the \ntraining epochs. The model's segmentation accuracy improved \nas the number of training iterations increased. Above epoch 50, \nthe MIoU stabilized above 80%, with fluctuations of less than \n2 percentage points. The results indicate that the model \nachieved high segmentation accuracy and demonstrated good \nrobustness. \n \n \nFIGURE 5. The change curve of MIoU. \nD. Comparison experiments \nIn order to further evaluate the performance and \neffectiveness of our model, we replicated the mainstream \nsemantic segmentation models of UNet, PSPNet, DeepLabV3 \nand TransUNet to verify in the Massachusetts road dataset.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nUNet uses an encoder-decoder framework with VGG model \n[39] as the feature extractor network. PSPNet and DeepLabV3 \nuse MobileNet [40] as the feature extractor network, which \ncan reduce the computational cost while ensuring performance. \nTransUNet and improved UNet models both use ResNet -50 \n[41] as the feature extractor network. Under the consistent \nexperimental parameter configuration, Massachusetts dataset \nis used for comparison with the improved UNet model and the \nother four models. The experimental compari son metrics \ninclude precision, recall, F1 score, MPA, MIoU and training \ntime, as shown in Figure 5 and Table Ⅱ. \n \nFIGURE 6. Semantic segmentation results of different models in the Massachusetts road dataset . \nFigure 5 shows the semantic segmentation results of \ndifferent models for road extraction. The road segmentation \nresults of the UNet and PSPNet models are comparable, both \nable to segment the rough area of roads, but there are still some \nareas with segmentation inaccuracies. The DeepLabV3 model \nenlarges the receptive field through atrous convolutions, but \nthis method performs poorly in road edge segmentation, and \ntoo much detail is lost during bilinear up-sampling, resulting \nin unsatisfactory segmentation re sults. Compared with the \nprevious models, the TransUNet model has better \nsegmentation results and higher accuracy, but there are still \nproblems with some image edge segmentations. We address \nthese problems through the introduction of CNN-Transformer \nstructures and dual up-sample modules, effectively improving \nedge segmentation problems, resulting in better segmentation \nresults and improved segmentation accuracy. The predicted \nresults can be effectively used for road extraction. \nTABLE Ⅱ \nCOMPARISON RESULTS OF EACH MODEL. NOTES: BOLD IS THE BEST, AND UNDERLINE IS THE SECOND. \nModel MPA (%) MIoU (%) F1 score (%) Time (h) \nUNet 84.97 75.84 85.54 4.27 \nPSPNet 83.16 75.27 84.74 3.84 \nDeepLabV3 82.55 74.82 83.48 4.07 \nTransUNet 90.53 82.36 89.64 4.73 \nOurs 92.56 84.97 91.48 4.31 \nFrom the perspective of evaluation metrics in Table Ⅱ, the \nmean pixel accuracy of the five models is 84.97%, 83.16%, \n82.55%, 90.53%, and 92.56%, respectively. The overall \nperformance of DeepLabV3 is poor, as it has low accuracy and \nsuboptimal segmentation effect on this dataset. Meanwhile, \nPSPNet shows bette r network segmentation effect than \nDeepLabV3, and has the shortest training time. UNet yields a \nbetter overall performance, with its encoder -decoder \narchitecture having been applied successfully in road \nextraction. Compared to UNet, TransUNet with the CNN -\nTransformer module improves the mean pixel accuracy by \n5.56%, and the MIoU by 6.52%. The improved UNet model \nperforms best, with an MPA of 92.56% and an MIoU of \n84.97%, which represent improvements of about 2.03% and \n2.61% over TransUNet, respectively. These results indicate an \nincrease in the proportion of correctly segmented pixels and \nimproved segmentation accuracy. In terms of training time, \nthe PSPNet model is the fastest, followed by the DeepLabV3 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nand UNet models, with training times of 3.84h, 4.07h, and \n4.27h respectively. The slower training time of the TransUNet \nmodel is due to the addition of an attention mechanism, which \nincreases the number of parameters and calculations. In this \nstudy, to bal ance the effect of the attention mechanism, the \nhard-swish activation function was used to reduce memory \naccesses, and a cross-entropy + dice mixed loss function was \nintroduced to reduce training time, accelerate model \nconvergence, and ultimately achieve a training time of 4.31h. \nIn order to verify the superiority and generalization ability \nof this algorithm in road extraction, the public data set Ottawa \nRoads road data set was used for experimental verification. \nUNet, PSPNet, DeepLabV3 and TransUNet were trained with \nthe algorithm and model of this article respectively. The \nexperimental results of UNet, PSPNet, DeepLabV3, \nTransUNet and this algorithm are shown in Figure 7 and Table \nⅢ. \n \nFIGURE 7. Semantic segmentation results of different models in the O ttawa road dataset. \nAs shown in Figure 7, the road is clear and regular, but it is \nalso blocked by trees. UNet road extraction results are \nincomplete and are greatly affected by tree occlusion. The \nextraction results of PSPNet and DeepLabV3 are worse than \nUNet, there are stil l discontinuities in the extraction results, \nand the detail processing is poor. The extraction results of \nTransUNet are better than U -Net and PSPNet in terms of \ncontinuity. However, it is also affected by occlusions, \nresulting in inaccurate extraction. The method proposed in this \npaper can effectively extract roads. Edge details are handled \nwell, and extraction is complete and continuous. We solve \nthese problems by introducing a CNN-Transformer structure \nand a dual upsampling module, which effectively impro ves \nthe problem of insufficient edge details, thereby obtaining \nbetter segmentation results and improving segmentation \naccuracy. \nTABLE Ⅲ \nCOMPARISON RESULTS OF EACH MODEL. NOTES: BOLD IS THE BEST, AND UNDERLINE IS THE SECOND. \nModel MPA (%) MIoU (%) F1 score (%) Time (h) \nUNet 86.28 78.85 84.02 4.68 \nPSPNet 89.20 80.32 84.74 3.96 \nDeepLabV3 84.34 78.32 76.29 4.23 \nTransUNet 92.19 87.37 87.68 4.92 \nOurs 95.48 90.94 93.25 4.82 \nAs can be seen from Table Ⅲ, the average pixel accuracy \nof the five models has improved in the Ottawa road data set, \nreaching 86.28%, 89.20%, 84.34%, 92.19% and 95.48% \nrespectively. DeepLabV3 road extraction results are still the \nworst, with lower overall performance. The Miou index  and \nF1 score of UNet and PSPNet are better than DeepLabV3. \nCompared with UNet, TransUNet with CNN Transformer \nmodule improves the average pixel accuracy by 5.91% and \nMIoU by 3.66%. This method performed the best, with MPA \nof 95.48% and MIoU of 90.94%, which were approximately \n3.29% and 3.57% higher than TransUNet respectively. These \nresults show that our method is also superior in road extraction \nin higher-resolution remote sensing images. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nE. Ablation study \nWe selected the UNet as the baseline. To verify the \nfeasibility and effectiveness of the improved UNet model \ndesigned in this paper, and under the same experimental \nconditions, the following five ablation experiment schemes \nwere adopted:  \n(1) basic UNet network;  \n(2) adding a CNN-Transformer hybrid structure to Scheme \n1;  \n(3) adding a dual up-sample (DU) module to Scheme 1;  \n(4) adding a dual up-sample (DU) module and a hard-swish \n(HS) activation function to Schemes 2; \n(5) changing a cross-entropy + dice mixed loss function to \nScheme 4 for model training, as shown in Figure 8 and Table \nⅣ. \n \nFIGURE 8. Ablation experimental image and road extraction result map . \nAs shown in Figure 8, the leakage extraction result of \nScheme 1 is not accurate. The road extraction results are \ndiscontinuous and are greatly affected by other features. \nCompared with Scheme 1, Scheme 2 adds a channel CNN -\nTransformer hybrid structure, whi ch has significantly \nimproved the problem of discontinuous extraction results, but \nsuch problems still exist. In Scheme 3, based on Scheme 1, \ntraditional upsampling is replaced by double upsampling. \nCompared with the extraction results of Scheme 1, the \nextraction results are better. Option 4 is based on Option 2 and \nsimultaneously references up -sample (DU) module and a \nhard-swish (HS). It can be seen that the extraction results are \nsignificantly improved, and the extraction results are more \nconsistent. Case 5 is the method of this article. From the results, \nthe extraction discontinuity has been effectively solved, and \nthe method proposed in this article has achieved good \nextraction results. Based on the above, the algorithm module \nin this study is effective and the method of this study is more \nsuitable. \nTABLE Ⅳ  \nABLATION STUDY OF IMPROVED UNET. NOTES: BOLD IS THE BEST, AND \nUNDERLINE IS THE SECOND. U = UNET, CNN = C, TRANSFORMER = T, DU \n= DUAL UP-SAMPLE, HS = HARD-SWISH, AND CE = CROSS ENTROPY. \nU C T DU HS Loss MPA  MIoU  \n     CE 84.97 75.84 \n     CE 88.46 79.52 \n     CE 86.87 77.35 \n     CE 91.12 82.59 \n     CE + \nDice 92.56 84.97 \nAmong the results of the five schemes mentioned above, \ncomparing Scheme 1 and Scheme 2, it can be seen that adding \na CNN Transformer hybrid structure significantly improved \nthe model's precision and MIoU by 3.49% and 3.68%, \nrespectively. Comparing Scheme 1 and Scheme 3, it can be \nseen that adding the dual up -sample module has slightly \nimproved the MPA and MIoU of the model, with increases of \n1.9% and 1.51%, respectively. Veri fied the effect of CNN -\nTransformer hybrid structure and dual up-sample module on \nimproving model performance. It can be seen from the \ncomparison between Scheme 1 and Scheme 4 that by adding \nthe CNN -Transformer hybrid structure, dual up -sample \nmodule and hard-swish activation function at the same time, \nMPA and MIoU reach 91.12% and 82.59%, with good results. \nIt can be seen from Scheme 4 and Scheme 5 that changing \ncross-entropy + dice mixed loss function on the basis of \nScheme 4 will further improve the model performance. From \nthis, it can be seen that the network training strategy and model \nimprovement plan in this study are effective and feasible. \nIV. Conclusion \nReal-time and accurate road information is the prerequisite \nfor updating navigation electronic maps, and it is of great \nsignificance to obtain road information quickly and accurately. \nIn order to solve the problems of discontinuous and inaccurate \nroad extraction by the traditional UNet network, we improved \nthe UNet network to extract road information from remote \nsensing images. We use UNet based on the CNN-Transformer \nmodel architecture to enhance the feature extraction \ncapabilities of global information and local detail information. \nAt the same time, we introduce a double upsampling module \nto improve feature extraction capabilities and segmentation \naccuracy, and introduce a hard -swish activation function to \nenhance generalization and nonlinear feature extra ction \ncapabilities and prevent gradient disappearance. And use the \nloss function of cross entropy (CE) and Dice to strengthen the \nmodel's constraints on segmentation results, further improving \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \nsegmentation accuracy. We perform model validation on the \nMassachusetts Road Dataset and the Ottawa Road Dataset. In \ncomparative experiments, we compared this algorithm with \nUNet, PSPNet, DeepLabV3 and TransUNet. The results \nverify that this paper has good  training stability, robustness \nand generalization ability in road extraction semantic \nsegmentation. In addition, the effectiveness of the improved \npart of this article was verified in the ablation experiment. \nNonetheless, the improved UNet -based semantic \nsegmentation algorithm is not easily applicable to mobile or \nembedded devices. It has the characteristics of high \ncomputational complexity, long training time and large \nparameter space. Therefore, future research work aims to \ninvestigate efficient and ligh tweight methods for image \nsemantic segmentation that can be conveniently used on \nmobile devices.\nREFERENCES \n[1] Liu, P.; Xu, Z.; Zhao, X. Road tests of self-driving vehicles: Affective \nand cognitive pathways in acceptance formation. Transportation \nresearch part A: policy and practice 2019, 124, 354–369. \n[2] Chen, Z.; Fan, W.; Zhong, B.; Li, J.; Du, J.; Wang, C. Corse -to-fine \nroad extraction based on local Dirichlet mixture models and \nmultiscale-high-order deep learning. IEEE Transactions on Intelligent \nTransportation Systems 2019, 21, 4283–4293. \n[3] Zeybek, M. Extraction of road lane markings from mobile LiDAR data. \nTransportation research record 2021, 2675, 30–47. \n[4] Tejenaki, S.A.K.; Ebadi, H.; Mohammadzadeh, A. A new hierarchical \nmethod for automatic road centerline extraction in urban areas using \nLIDAR data. Advances in Space Research 2019, 64, 1792–1806. \n[5] Liao, J.; Cao, L.; Luo, X.; Sun, X.; Duan, C.; Li, J.; Yuan, F. Road \ngarbage segmentation with deep supervision and high fusion network \nfor cleaning vehicles. IEEE Transactions on Intelligent Transportation \nSystems 2021, 23, 11190–11204. \n[6] Shen, G.; Han, X.; Chin, K.; Kong, X. An attention -based digraph \nconvolution network enabled framework for congestion recognition in \nthree-dimensional road networks. IEEE Transactions on Intelligent \nTransportation Systems 2021, 23, 14413–14426. \n[7] Zhang, Y.; Wang, J.; Wang, X.; Dolan, J.M. Road-segmentation-based \ncurb detection method for self-driving via a 3D-LiDAR sensor. IEEE \ntransactions on intelligent transportation systems 2018, 19, 3981–3991. \n[8] Wen, C.; Habib, A.F.; Li, J.; Toth, C.K.; Wang, C.; Fan, H. Special \nissue on 3D sensing in intelligent transportation. IEEE Transactions \non Intelligent Transportation Systems 2021, 22, 1947–1949. \n[9] Fouchal, H.; Boudra, S.; Ercan, S.; Yahiaoui, I. Pseudonym limitation \nfor privacy in cooperative transport systems. IEEE Network 2020, 34, \n73–77. \n[10] Zhou, W.; Liu, J.; Lei, J.; Yu, L.; Hwang, J.N. GMNet: graded-feature \nmultilabel-learning network for RGB -thermal urban scene semantic \nsegmentation. IEEE Transactions on Image Processing 2021, 30, \n7790–7802. \n[11] Feng, H.; Li, W.; Luo, Z.; Chen, Y.; Fatholahi, S.N.; Cheng, M.; Wang, \nC.; Junior, J.M.; Li, J. GCN -based pavement crack detection using \nmobile LiDAR point clouds. IEEE Transactions on Intelligent \nTransportation Systems 2021, 23, 11052–11061. \n[12] Deng, L.; Liu, X.Y.; Zheng, H.; Feng, X.; Chen, Y. Graph spectral \nregularized tensor completion for traffic data imputation. IEEE \nTransactions on Intelligent Transportation Systems 2021, 23, 10996–\n11010. \n[13] Yang, H.; Liu, C.; Zhu, M.; Ban, X.; Wang, Y. How fast you will drive? \npredicting speed of customized paths by deep neural network. IEEE \ntransactions on intelligent transportation systems 2021, 23, 2045–2055. \n[14] Guo, S.; Chen, C.; Wang, J.; Ding, Y.; Liu, Y.; Xu, K.; Yu, Z.; Zhang, \nD. A force -directed approach to seeking route recommendation in \nride-on-demand service using multi -source urban data. IEEE \nTransactions on Mobile Computing 2020, 21, 1909–1926. \n[15] Espinosa, J.E.; Velastí n, S.A.; Branch, J.W. Detection of motorcycles \nin urban traffic using video analysis: A review. IEEE Transactions on \nIntelligent Transportation Systems 2020, 22, 6115–6130. \n[16] Yin, J.L.; Chen, B.H.; Lai, K.H.R. Driver danger -level monitoring \nsystem using multi -sourced big driving data. IEEE Transactions on \nIntelligent Transportation Systems 2019, 21, 5271–5282. \n[17] Jung, Y.; Seo, S.W.; Kim, S.W. Curb detection and tracking in low -\nresolution 3d point clouds based on optimization framework. IEEE \nTransactions on Intelligent Transportation Systems 2019, 21, 3893 –\n3908. \n[18] Simonyan, K.; Zisserman, A. Very deep convolutional networks for \nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 2014. \n[19] Bakker, B.; Zabłocki, B.; Baker, A.; Riethmeister, V.; Marx, B.; Iyer, \nG.; Anund, A.; Ahlströ m, C. A multi -stage, multi -feature machine \nlearning approach to detect driver sleepiness in naturalistic road \ndriving conditions. IEEE Transactions on Intelligent Transportation \nSystems 2021, 23, 4791–4800. \n[20] Maboudi, M.; Amini, J.; Hahn, M.; Saati, M. Object -based road \nextraction from satellite images using ant colony optimization. \nInternational Journal of Remote Sensing 2017, 38, 179–198. \n[21] Leng, J.; Liu, Y.; Du, D.; Zhang, T.; Quan, P. Robust obstacle \ndetection and recognition for driver assistance systems. IEEE \ntransactions on intelligent transportation systems 2019, 21, 1560–1571. \n[22] Yang, M.; Yuan, Y.; Liu, G. SDUNet: Road extraction via spatial \nenhanced and densely connected UNet. Pattern Recognition 2022, 126, \n108549. \n[23] Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for \nsemantic segmentation. In Proceedings of the Proceedings of the IEEE \nconference on computer vision and pattern recognition, 2015, pp. \n3431–3440. \n[24] Ronneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional networks \nfor biomedical image segmentation. In Proceedings of the Medical \nImage Computing and Computer -Assisted Intervention –MICCAI \n2015: 18th International Conference, Munich, Germany, October 5-9, \n2015, Proceedings, Part III 18. Springer, 2015, pp. 234–241. \n[25] Zhao, H.; Shi, J.; Qi, X.; Wang, X.; Jia, J. Pyramid scene parsing \nnetwork. In Proceedings of the Proceedings of the IEEE conference on \ncomputer vision and pattern recognition, 2017, pp. 2881–2890. \n[26] Chen, L.C.; Papandreou, G.; Schroff, F.; Adam, H. Rethinking atrous \nconvolution for semantic image segmentation. arXiv preprint \narXiv:1706.05587 2017. \n[27] Kong, X, W. Wang, C, Y. Zhang, S, C. Li, J, H. and Sui, Y. \nApplication of Improved U-Net Network in Road Extraction from RS \nImage Remote Sensing Information. 2022, 37(02): 97-104.  \n[28] Li, J. Liu, Y. and Zhang, Y. Cascaded Attention DenseUNet \n(CADUNet) for Road Extraction from Very-High-Resolution Images. \nInternational Journal of Geo-Information. 2021, 10(5):329. \n[29] Han, L. Yang, Z, H. LI, L, Z. Liu, Z, X. and Huang, B, W. Road \nextraction of high resolution RS imagery based on DeepLab V3. \nremote sensing Information. 2021, 36(1): 22-28.  \n[30] Liu, R. He, D. Semantic Segmentation Based on DeepLabv3+ and \nAttention Mechanism, 2021 IEEE 4th Advanced Information \nManagement, Communicates, Electronic and Automation Control \nConference (IMCEC). IEEE, 2021. \n[31] Chen, J.; Lu, Y.; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y.; Lu, L.; Yuille, \nA.L.; Zhou, Y. Transunet: Transformers make strong encoders for \nmedical image segmentation. arXiv preprint arXiv:2102.04306 2021. \n[32] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. \nProceedings of the 31st International Conference on Neural \nInformation Processing Systems. Long Beach: Curran Associates Inc., \n2017. 6000–6010.  \n[33] Shi, W.; Caballero, J.; Huszá r, F.; Totz, J.; Aitken, A.P.; Bishop, R.; \nRueckert, D.; Wang, Z. Real -time single image and video super -\nresolution using an efficient sub -pixel convolutional neural network. \nIn Proceedings of the Proceedings of the IEEE confer ence on \ncomputer vision and pattern recognition, 2016, pp. 1874–1883. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2023 10 \n[34] Tang, Z.; Jiang, W.; Zhang, Z.; Zhao, M.; Zhang, L.; Wang, M. \nDenseNet with Up -Sampling block for recognizing texts in images. \nNeural Computing and Applications 2020, 32, 7553–7561. \n[35] Fan, C.M.; Liu, T.J.; Liu, K.H. SUNet: swin transformer UNet for \nimage denoising. In Proceedings of the 2022 IEEE International \nSymposium on Circuits and Systems (ISCAS). IEEE, 2022, pp. 2333–\n2337. \n[36] Glorot, X.; Bordes, A.; Bengio, Y. Deep sparse rectifier neural \nnetworks. In Proceedings of the Proceedings of the fourteenth \ninternational conference on artificial intelligence and statistics. JMLR \nWorkshop and Conference Proceedings, 2011, pp. 315–323. \n[37] Ramachandran, P.; Zoph, B.; Le, Q.V. Searching for activation \nfunctions. arXiv preprint arXiv:1710.05941 2017. \n[38] Howard, A.; Sandler, M.; Chu, G.; Chen, L.C.; Chen, B.; Tan, M.; \nWang, W.; Zhu, Y.; Pang, R.; Vasudevan, V.; et al. Searching for \nmobilenetv3. In Proceedings of the Proceedings of the IEEE/CVF \ninternational conference on computer vision, 2019, pp. 1314–1324. \n[39] Mnih, V. Machine learning for aerial image labeling; University of \nToronto (Canada), 2013. \n[40] Howard, A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; \nWeyand, T.; Andreetto, M.; Adam, H. Mobilenets: Efficient \nconvolutional neural networks for mobile vision applications. arXiv \npreprint arXiv:1704.04861 2017. \n[41] He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image \nrecognition. In Proceedings of the Proceedings of the IEEE conference \non computer vision and pattern recognition, 2016, pp. 770–778. \n \nRUI WANG  Dr. Wang Rui graduated from \nWuhan University. His research interests include \nthe application of traffic data in the insurance \nindustry, spatial big data management and the \nstudy of image recognition algorithms based on \nmachine learning. He currently works at the China \nTransport Telecommunications and information \nCenter.  \nMINGXIANG CAI Dr. Cai Mingxiang, graduated \nfrom Wuhan University. His research interests \ninclude spatio -temporal information mining and \napplication of fundamental models in traffic big \ndata. \n \n \n \n \n \n \nZIXUAN XIA entered Heilongjiang University of \nTechnology in 2020 and will receive a degree in \nsurveying and mapping engineering in 2024. His \ncurrent research interests include flight calibration \nand validation of forestry applications of high -\ndefinition aerial syste ms and the study of \nvegetation cover differentiation based on the \nimage dichotomous model. \n \n \n \n \n \n \n \nZhicui Zhou graduated with a master's degree \nfrom Shaanxi Normal University. Research \ninterests include ecology, artificial intelligence, \nand image recognition algorithms based on \nmachine learning. \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3344797\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}