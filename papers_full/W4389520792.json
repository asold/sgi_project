{
    "title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
    "url": "https://openalex.org/W4389520792",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2318558981",
            "name": "Amita Kamath",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2032101315",
            "name": "Jack Hessel",
            "affiliations": [
                "Allen Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2208999240",
            "name": "Kai-Wei Chang",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2904565150",
        "https://openalex.org/W4386076015",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W3214685499",
        "https://openalex.org/W2606220156",
        "https://openalex.org/W4303648003",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W4281633937",
        "https://openalex.org/W4306820534",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W4312206006",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4312910992",
        "https://openalex.org/W3163542683",
        "https://openalex.org/W4382323313",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2067816745",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W4381802186",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4312261477",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W4229042118",
        "https://openalex.org/W4285192809",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W4312090938",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2423576022"
    ],
    "abstract": "Recent vision-language (VL) models are powerful, but can they reliably distinguish “right” from “left”? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What’sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9161–9175\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWhat’s “up” with vision-language models?\nInvestigating their struggle with spatial reasoning\nAmita Kamath1 Jack Hessel2 Kai-Wei Chang1\n1 University of California, Los Angeles\n2 Allen Institute for AI\n{kamatha, kwchang}@cs.ucla.edu, jackh@allenai.org\nAbstract\nRecent vision-language (VL) models are pow-\nerful, but can they reliably distinguish “right”\nfrom “left”? We curate three new corpora to\nquantify model comprehension of such basic\nspatial relations. These tests isolate spatial rea-\nsoning more precisely than existing datasets\nlike VQAv2, e.g., our What’sUp benchmark\ncontains sets of photographs varying only the\nspatial relations of objects, keeping their iden-\ntity fixed (see Figure 1: models must compre-\nhend not only the usual case of a dog under\na table, but also, the same dog on top ofthe\nsame table). We evaluate 18 VL models, find-\ning that all perform poorly, e.g., BLIP fine-\ntuned on VQAv2, which nears human parity\non VQAv2, achieves 56% accuracy on our\nbenchmarks vs. humans at 99%. We conclude\nby studying causes of this surprising behavior,\nfinding: 1) that popular vision-language pre-\ntraining corpora like LAION-2B contain little\nreliable data for learning spatial relationships;\nand 2) that basic modeling interventions like\nup-weighting preposition-containing instances\nor fine-tuning on our corpora are not sufficient\nto address the challenges our benchmarks pose.\nWe are hopeful that these corpora will facilitate\nfurther research, and we release our data and\ncode at https://github.com/amitakamath/\nwhatsup_vlms.\n1 Introduction\nPre-trained vision-language models perform well\non complex tasks such as VQAv2 (Goyal et al.,\n2016) and Nocaps (Agrawal et al., 2019), even\nin the zero-shot setting (Li et al., 2023). How-\never, recent work has re-surfaced a concern that\nhas long plagued vision-language models (Yatskar\net al., 2016; Johnson et al., 2017): new multimodal\nmodels still exhibit poor behavior on simple tasks\nlike attribute attachment, counting, etc. (Yamada\net al., 2022; Thrush et al., 2022; Yuksekgonul et al.,\n2023; Parcalabescu et al., 2021). Despite improve-\nments, models still fail to reliably capture even\nA dog ona table\n!\n\"A dog right ofa table\nA dog left ofa tableA dog right ofa table\nA dog right ofa tableA dog right ofa table\nA dog undera tableA dog right ofa table\nFigure 1: We propose three tightly controlled bench-\nmarks to assess model capacity for fine-grained spatial\nreasoning, showing that popular vision-language mod-\nels fall far behind human performance when asked to\nselect the correct spatial relation between two objects in\nan image (real examples shown).\nbasic spatial factors of images, a prerequisite for\nmore precise and complex reasoning benchmarks.\nBut why?In this work, we study vision-language\nmodels’ performance on basic spatial relations,\nsuch as “left of” and “right of”. Existing bench-\nmarks which aim to operationalize spatial under-\nstanding such as VQAv2 and GQA (Hudson and\nManning, 2019) often conflate the evaluation of\nspatial reasoning with other types of reasoning,\nsuch as in the GQA question “Is there a woman to\nthe left of the person that is wearing a wetsuit?”.\n9161\nHence, we first curate COCO-spatial and\nGQA-spatial based on the COCO (Lin et al.,\n2014) and GQA datasets respectively, to isolate\nand assess more strictly only basic spatial relations.\nIn addition, we collect a third evaluation corpus,\nWhat’sUp, with even tighter controls. The images\nwithin COCO and GQA often contain many ob-\njects/relations, and exhibit biases that reflect our\nusual world (e.g., a mug is usually on a table, not\nunder it). We manually capture controlled pho-\ntographs of household objects in various positions:\ne.g., to overcome the social bias of dogs being pho-\ntographed under tables, we (carefully, gently, and\nwith many treats) placed a dog on a table and took\na picture of her (see Figure 1). What’sUp consists\nof 205 sets of four images each, resulting in 820\nimages in total. Each set of images varies the un-\nderlying preposition that describes the relationship\nbetween two objects, e.g., one set of images con-\ntains a mug on, under, left of, and right of a table.\nFurthermore, background objects are minimized,\nso there is no ambiguity.\nFor all three datasets, our setup is as follows: for\na given image, the model is given a correct caption\nand 1 or 3 distractor captions, which differ only\nby a preposition: it must select the correct one.\nWe evaluate 18 popular vision-language models,\ncovering various architectures (e.g., one-stack vs.\ntwo-stack), training objectives (e.g., generative vs.\ncontrastive models), and training data. All models\nperform poorly across benchmarks, with many per-\nforming just a few points above random chance and\nall models falling far behind human performance.\nNext, we investigate why these models fail to\nlearn much about spatial relationships. All models\nwe consider are pre-trained on large-scale image-\ncaption corpora. We perform a corpus study of\nthe LAION-2B dataset (Schuhmann et al., 2022),\nwhich was used to train OpenCLIP (Ilharco et al.,\n2021). We see that (1) common spatial prepositions\noccur in less than 0.2% of the training data; (2)\nwhen they do occur, they can be ambiguous or\nextraneous to the image, e.g., “left” defined from\nthe viewer’s perspective vs the subject’s; and (3)\nthey can often be guessed without looking at the\nimage, e.g., “a house above water”.\nWe consider several modeling improvements\nbased on these findings, including: (1) re-\nnormalizing model probabilities to account for the\nimplicit text-only prior of captions in LAION-2B;\n(2) replacing the preposition “behind” with one\nmore frequent in the training data, “in the back-\nground”, as a case study to investigate if models\nmay indeed “understand\" spatial relationships (but\nare not surfacing that knowledge due to distribution\nmismatches); and (3) finetuning on several different\nrelevant training sets (e.g., COCO-spatial/GQA-\nspatial training sets, preposition-containing sub-\nsets of LAION-2B, and auto-generated hard neg-\natives with switched prepositions). None of these\napproaches dramatically improves model perfor-\nmance on understanding spatial relations.\nIn summary, our contributions are: (1) three\nnew benchmarks evaluating spatial relations in\nvision-language models, alongside results of 18\nVL models on them; (2) a study of the training\ndata of some of these models, with observations\nthat could explain poor model performance on the\nbenchmarks; and (3) a study of various methods\nto improve model performance, with insights that\ncould guide future research in overcoming this\nissue. We release code and data to encourage\nthe same at https://github.com/amitakamath/\nwhatsup_vlms.\n2 Benchmarks\nExisting benchmarks include spatial reasoning\nquestions, such as VQAv2 (Goyal et al., 2016) and\nGQA (Hudson and Manning, 2019). However, in-\nstances in these corpora often conflate several types\nof reasoning: in GQA, over 92% of the validation\nquestions do so. For example, the GQA question\n“Are there men to the left of the person that is hold-\ning the umbrella?” conflates evaluation of spatial\nreasoning, object relationships, and object detec-\ntion – in contrast, our questions require only spatial\nreasoning about one or two objects.\nOur three new evaluation corpora are presented\nin the same format: an image paired with sev-\neral captions which differ only by a preposi-\ntion. What’sUp consists of tightly controlled pho-\ntographs we captured ourselves, whereas COCO-\nspatial and GQA-spatial are curated from well-\nrecognized image datasets. One key contribution\nis that all instances in all of our corpora require\nonly spatial reasoning about one or two objects,\ne.g., in What’sUp, we circumvent the part-and-\nwhole problem discussed in Yamada et al. (2022)\nby careful construction.\nFigure 2 contains examples of images from each\nof our three benchmarks, along with the caption\noptions each image is paired with.\n9162\nA mug on a tableA mug under a tableA mug to the left of a tableA mug to the right of a table\nA mug on a tableA mug under a tableA mug to the left of a tableA mug to the right of a table\nA mug on a tableA mug under a tableA mug to the left of a tableA mug to the right of a table\nA mug on a tableA mug under a tableA mug to the left of a tableA mug to the right of a table\nA mug in front of a plateA mug behind a plateA mug to the left of a plateA mug to the right of a plate\nA mug in front of a plateA mug behind a plateA mug to the left of a plateA mug to the right of a plate\nA mug in front of a plateA mug behind a plateA mug to the left of a plateA mug to the right of a plate\nA mug in front of a plateA mug behind a plateA mug to the left of a plateA mug to the right of a plate\nA boy to the left of a racketA boy to the right of a racket\nWhat’sUp (Subset A)\nWhat’sUp (Subset B)\nCOCO-spatial\nA dog to the left of a benchA dog to the right of a bench\nGQA-spatial\nA person on the leftA person on the rightAn umpire on the leftAn umpire on the right\nFigure 2: Examples from our three proposed benchmarks. Each image is paired with four text options in What’sUp\nand two text options in COCO-spatial and GQA-spatial. Given a single image and the corresponding text options,\na VL model must select the correct option.\n2.1 Collection and statistics\nWhat’sUp We captured 820 images of pairs of\nhousehold objects in unambiguous spatial relation\nto each other. 408 of these (Subset A) contain an\nobject on, under, left of, or right of a table, chair\nor armchair. The other 412 (Subset B) contain\nan object in front of, behind, left of or right of\nanother object on a black tabletop. For a given\nobject pair, each preposition is represented; thus\neach subset of What’sUp has equal representation\nof each preposition.\nThese images were captured with a tripod, with\nminimal changes between images in terms of posi-\ntion and lighting, except for the placement of the\nobjects. This allows the benefit of real-world im-\nages, while exhibiting the controlled nature of syn-\nthetic images. This control has several advantages:\n(1) we are able to evaluate model performance on\npairs or sets of images, as described in §2.2; (2) we\novercome textual biases that could falsely improve\nmodel performance, e.g. always guessing that the\nmug is on the table based on training priors; and (3)\nwe are able to run specialized experiments studying\nmodel representations such as in §2.4.\nThe primary differences between the two subsets\nare: (1) in Subset B, the two objects are closer in\nsize than in Subset A; and (2) in Subset B, there is\nno obvious prior on the spatial relationship between\nthe two objects, whereas in Subset A, e.g., a mug\nwould usually go on a table.\nCOCO-spatial We created a benchmark from\nthe validation set of COCO (Lin et al., 2014) using\ndetection annotation data. We select images with\nonly one instance of each object mentioned in the\ntext input, where the area of each is at least 3%\nthe area of the image. Unlike in What’sUp, these\nimages contain objects that may embody multiple\nspatial relations, e.g., an object that is both to the\ntop of and to the left of another object. Thus, we\nprovide only caption options that are mutually ex-\n9163\nclusive (to the left of vs to the right of, above vs\nbelow). Similarly for one-object images, we only\ntest for mutually exclusive spatial relations (on the\nleft vs on the right, on the top vs on the bottom).\nThis benchmark contains 2687 images, with two\ncaption options each.\nGQA-spatial We isolated questions targeting\nbasic spatial relations from the GQA validation\ndataset (Hudson and Manning, 2019), which is\nsourced from Visual Genome (Krishna et al., 2016).\nThe questions we isolate are of the form “Is the\nobject on the preposition of the image?” or “Is the\nobject1 to the preposition of object2?”, when the\nobject(s) mentioned are all present in the image, to\navoid conflation with object detection. We retain\nattribute-object pairs (e.g., “white car”) only if the\nattribute does not affect the answer (e.g., there is\nonly one car in the image), to avoid conflation with\nattribute detection. Similar to COCO-spatial, we\nselect images where the area of each object in the\nquestion is at least 3% of the image. We manually\nfiltered out noisy images, e.g., those with multiple\ninstances of objects in the question with different\nspatial relations. Finally, we convert these ques-\ntions to a templated caption format. This bench-\nmark contains 1451 images, with two caption op-\ntions each, due to the same ambiguity as inCOCO-\nspatial of objects having multiple spatial relations.\n2.2 Evaluation\nTask. For all three benchmarks, the input is an\nimage paired with several caption options that dif-\nfer only by the preposition they contain. The model\nmust select the caption with the correct preposi-\ntion. As shown in Figure 2, for What’sUp, there\nare four caption options; for COCO-spatial and\nGQA-spatial, there are two.\nMetric. The primary metric we use is the per-\ncentage of images for which the image-text match-\ning score is highest for the correct caption com-\npared to the incorrect caption(s). The controlled\nand balanced structure of What’sUp enables two\nadditional metrics for that corpus: pair-wise and\nset-wise accuracy. Pair-wise accuracy is the ac-\ncuracy on pairs of images that contain opposing\nprepositions. For example, if the model guesses\ncorrectly for “mug on table” and “mug under ta-\nble”, it gets one point. Set-wise accuracy is similar,\nbut is awarded only when all four prepositions for\na given object pair are guessed correctly.\nHuman estimated performance. We also esti-\nmate human performance on our three benchmarks.\nWe sample 100 data points from each benchmark\nand, to ensure quality of the annotations, invite\nexperts to voluntarily annotate the data. The anno-\ntators have all taken at least one graduate course\nin NLP. They are asked to determine whether the\ncorrect caption is an obvious choice, or if there is\nany scope for ambiguity. This estimate of human\nperformance is 97.3% on COCO-spatial, 99% on\nGQA-spatial, and 100% on What’sUp.\nModels. The models we study in the zero-shot\nsetting are: CLIP (Radford et al., 2021) ViT-B/32\nand ViT-L/14; a version of CLIP ViT-B/32 that\nhas been finetuned on word order shuffling data\ncalled negCLIP (Yuksekgonul et al., 2023); a ver-\nsion of CLIP ViT-B/32 that has been initialized\nwith RoBERTa-pretrained weights (Ilharco et al.,\n2021); CoCa, a model trained with generative and\ncontrastive objectives (Yu et al., 2022); XVLM\n(Zeng et al., 2022) with 4M and 16M parameters;\nBLIP (Li et al., 2022) with 14M and 129M param-\neters; BLIP2 (Li et al., 2023) image-text matching\nhead (ITM) and image-text contrastive learning\nhead (ITC); and FLA V A (Singh et al., 2022). These\nmodels span various modeling choices: one- and\ntwo-stack models, generative and contrastive train-\ning objectives, different training data, etc.\nWe also study several models that have been\nfinetuned on downstream tasks: CoCa which has\nbeen finetuned on COCO captioning; two versions\nof XVLM-16M that have been respectively fine-\ntuned on Flickr30K retrieval and COCO retrieval;\nand three versions of BLIP-14M that have been re-\nspectively finetuned on Flickr30K retrieval, COCO\nretrieval, and VQAv2.\nAlmost all of these models are capable of yield-\ning a score representing how well a given caption\nmatches a given image. We use this score to evalu-\nate whether the model “selects” the correct caption\nfrom the given options for an image. As BLIP-\nVQA and BLIP2-ITC have a text generation head\nrather than a scoring head, we phrase the input as\na set of questions, e.g. “Is the mug on the table?”,\n“Is the mug under the table?”, etc, and evaluate\nthe model by measuring the probability of the re-\nsponses “yes” and “no”: if the probability of “yes”\nis highest for the gold option (or “no” is lowest for\nthe gold option if all option responses are “no”),\nwe award a point.\n9164\nModel Whats-\nUp\nCOCO-\nspatial\nGQA-\nspatialAvg\nCLIP ViT-B/32 31.0 47.4 46.9 41.8\nCLIP ViT-L/14 26.1 49.5 47.3 41.0\nNegCLIP 34.4 46.9 46.0 42.4\nRoBERTaCLIP 25.1 50.0 49.8 41.6\nCoCa 29.4 46.7 47.1 41.0\nXVLM 4M 31.5 61.7 58.7 50.6\nXVLM 16M 41.9 65.0 58.2 55.0\nBLIP 14M 38.5 54.0 49.8 47.5\nBLIP 129M 30.4 49.3 49.0 42.9\nBLIP2-ITM 37.6 53.0 49.8 46.8\nBLIP2-ITC 29.0 53.7 51.0 44.6\nFLA V A 30.5 52.6 51.7 44.9\nCoCa-Caption 24.1 48.6 49.5 40.8\nXVLM-Flickr30K 44.3 65.2 61.4 56.9\nXVLM-COCO 42.1 71.0 68.1 60.4\nBLIP-Flickr30K 33.8 54.2 48.9 45.6\nBLIP-COCO 32.8 51.4 51.4 45.2\nBLIP-VQA 47.8 62.0 58.4 56.0\nRandom / Text-only 25.0 50.0 50.0 41.7\nHuman Estimate 100.0 97.3 99.0 98.8\nTable 1: Results of varied VL models on our bench-\nmarks: models in the first section are evaluated zero-\nshot, and models in the second section have been fine-\ntuned on some downstream task: COCO captioning,\nretrieval on Flickr30K or COCO, or VQA. All models\nperform poorly on basic spatial relations.\n2.3 Results\nThe performance of the models on our benchmarks\nis listed in Table 1. All models fall far behind\nhuman-estimated performance, with many models\nscoring within a few points of random chance. The\nnumber of models we evaluate allows us to draw\ninferences about various aspects of model design\nand training, as discussed below.\nModel architecture. XVLM and BLIP2 perform\nbetter than other models in the zero-shot setting,\nhinting that the increased expressiveness of one-\nstack, cross-attention models vs the two-stack mod-\nels may indeed matter in this case.\nModel size in parameters. Scaling up model\nsize does not necessarily improve spatial reasoning\ncapabilities. In the case of XVLM, the 16M model\noutperforms the 4M model; however, CLIP ViT-\nB/32 outperforms CLIP ViT-L/14 and BLIP 14M\noutperforms BLIP 129M averaged across our three\nbenchmarks.\nTraining objective. Despite helping on other\nzero-shot tasks such as ImageNet-1K (Deng et al.,\n2009; Yu et al., 2022), the generative training objec-\ntive does not seem to encourage spatial reasoning\nabilities more than a contrastive objective: CoCa\nscores less than CLIP ViT-B/32, and BLIP2-ITC\nscores less than BLIP2-ITM.\nSupervision. XVLM is the highest-performing\nmodel of those we evaluate, likely due to its more\nfine-grained supervision at the bounding-box level\nin addition to the image-level.\nFinetuning. Finetuning on downstream tasks ap-\npears to improve model performance sometimes,\ne.g. BLIP-VQA outperforms BLIP significantly,\nbut not always, e.g. CoCa-Captioning underper-\nforms CoCa.\nPair/Set and One-object/Two-object accuracy.\nDetailed results including pair and set accuracy for\nWhat’sUp, and one- and two-object accuracy for\nCOCO-spatial and GQA-spatial are presented\nin Appendix Table 3. All models show very\npoor pair and set accuracy, showing their lack of\nunderstanding of the concept of each preposition.\nThere does not seem to be a uniform trend of\nmodel performance on one-object images vs\ntwo-object images.\nInspection of the failure cases shows some mod-\nels always predicting 1-2 prepositions for all inputs,\nand others predicting seemingly randomly. Overall,\nour data allows a very precise evaluation of spatial\nreasoning, revealing that these models exhibit a\nfailure to understand basic spatial relations, despite\nnearing human performance on VQAv2, as in the\ncase of BLIP-VQA.\n2.4 Visual analogies\nNext, we study the representations of CLIP models\non the What’sUp Benchmark. The models are\nable to get some examples correct (e.g. “dog on\na table”, “dog under a table”), but as they are not\nable to get higher performance, particularly on the\npair and set metrics, it hints that they are not learn-\ning the generalizable concept of “under” or other\nspatial relations. To study whether the representa-\ntions encode these concepts in a generalizable man-\nner, we study whether the image representations of\nthese images exhibit the same linear analogies as\nstudied in NLP (king −man+ woman = queen)\n(Mikolov et al., 2013). We study only CLIP vari-\nants in this setting, as they alone of the models\nwe study are trained in a manner to encourage lin-\near recoverability. Specifically, we evaluate CLIP\nViT-B/32, ViT-L/14, NegCLIP and RoBERTaCLIP.\n9165\nFigure 3: Example of edited images with four colors.\nPrepositions. We select 25 sets of 4 from\nWhat’sUp Subset A: specifically, images\nwhere objects are placed around a table. We\nnow evaluate whether I(mug on table) −\nI(mug under table) + I(bowl under table) is\nthe closest to I(bowl on table), compared to\nI(bowl left/right/under table), where I(·) is the\nimage representation. Given 25 objects and 4\npreposition options, there are 7200 such analogies.\nWe measure the percentage of these where our\ncondition holds. On average, the four CLIP-based\nmodels we study achieve an analogy accuracy of\nonly 9%. The average performance of the models\nwhen directly evaluated on the images according\nto our usual accuracy metric is 31%.\nColors. As a control test for our setup, we\nnext study whether these linear analogies appear\nin the representation of various colors, which\nCLIP has been shown to generalize to very well\n(e.g., correctly identifying a blue cow). We\nisolate 25 objects from the What’sUp Bench-\nmark, and edit the images to attribute one of\nfour different colors to the object: red, yel-\nlow, green or blue, as in Figure 3. We now\nevaluate whether I(red mug) −I(yellow mug) +\nI(yellow bowl) is the closest to I(red bowl), com-\npared to I(yellow/green/blue bowl), where I(·) is\nthe image representation. Here, again, we have\n7200 analogies and measure the percentage of\ntimes the condition holds. On average, the four\nCLIP-based models we study achieve an accuracy\nof 61%1 – much higher than for prepositions. They\nalso achieve 100% accuracy when directly evalu-\nated on the color options in the same format as our\nbasic evaluation (given one image and four cap-\ntion options with different colors, select the correct\ncaption). These experiments suggest that models\nappear to learn the concept of color attachments\nmore effectively than spatial relations.\n1The linear analogy accuracy is not very high, but this\nis perhaps not too surprising given that even performing\nJPEG compression before encoding changes the image repre-\nsentation significantly for CLIP (see https://github.com/\nallenai/mmc4/issues/12).\n3 Why do they struggle? Studying\nLAION\nAll models we consider in Section 2.2 utilize large-\nscale image-caption corpora for pretraining. Here,\nwe investigate one popular such corpus, LAION-\n2B (Schuhmann et al., 2022), to better understand\nwhy spatial relations might not be learned by mod-\nels when trained on this type of data. LAION was\nalso used to train OpenCLIP (Ilharco et al., 2021).\nPrepositions occur rarely. We find that captions\nin the corpus contain common spatially specific\nprepositions like “under\" or “left of\" only 0.2% of\nthe time (we additionally filter spatial prepositions\nthat are used in non-spatial contexts, e.g., “under\n$25”). The individual frequency of each preposi-\ntion is given in Appendix Table 4.\nThere are several reasons why this may be the\ncase: alt-text authors may choose not to specify\nprepositions they feel are obvious (e.g., a house\n“above” the water) or ambiguous (e.g., “left” from\nthe viewer’s perspective, or from the subject of\nthe image’s perspective?); the preposition may not\nbe important in the writer’s eyes when trying to\ncapture holistic information about the entire im-\nage in a short caption (e.g., “a cluttered kitchen”,\nrather than “a fork to the left of a knife on a kitchen\ncounter”); the writer may choose more casual lan-\nguage (e.g., “next to” rather than “to the left of”).\nSee Berg et al. (2012) for a discussion of how de-\nscriptions manifest according to similar factors in\ncrowdsourced image captioning corpora.\nPrepositions can be ambiguous.Of the spatial\nprepositions that do occur in LAION, examination\nof the associated images reveals ambiguity. For\nexample, the frame of reference could be defined\nfrom the perspective of the viewer of the photo, or\nof the subject of the photo — in our benchmarks,\nwe follow the same convention as CLEVR (John-\nson et al., 2017), i.e., the perspective of the viewer;\nhowever, image-text pairs in LAION are scraped\nfrom the internet, and thus follow no single conven-\ntion. As another example, “in front of” could mean\ncloser to the viewer of the photo, or ahead of a sub-\nject that is facing in a certain direction in the photo.\nEven the same preposition with the same meaning\ncould have very different visual appearances, e.g.\n“a ball under the desk” vs “a ball under the water”.\nA few examples are discussed in Figure 4.\n9166\nImage and captionDiscussion\nReally pleased with this startrail. Only managing approx5hrs of darkness because of the long days. Taken between 1030pm and sunrise following day. May 312009in SthLeics, UK. Love the opposite curvature of the trails aboveand belowthe celestial equator. Olympus E3, 7-14mm lens. Just over 1000 exposures stacked in startrails.\nThe celestial equator is not obvious in this image, and thus the description of trails above and below it does not provide much information.\nMaury Determined That Was a Lie you said the next bus/train was coming up right behindyou the half an hour wait determined that was a lie , made with livememememe creator\nThe caption is a transcription of the text overlaid on the image; the image does not contain a bus or train at all.\nLearning objects. Fabric with sewing item and accesorieswhich are required to learn to sew on wooden table background. Directly aboveand copy space.\nUnclear what the preposition refers to.\nFigure 4: Examples of ambiguity in spatial prepositions\nused in LAION captions, alongside discussions thereof.\nPrepositions are rarely needed to satisfy the\ncontrastive learning objective. CLIP and sim-\nilar models trained contrastively rely on a large\nbatch size to obtain negative examples that require\nmore precise visual representations. For exam-\nple, the model learns a visual representation of\n“Bernese Mountain Dog” rather than just “dog”, as\nthere could be several types of dogs in the 32K\nbatch. However, this is not the case for preposi-\ntions. Given the combinatorial space of all possible\nsentences, it is unlikely that the exact same descrip-\ntion would apply to two images in a batch with the\nexception of a specific preposition. Furthermore,\nsome preposition-object combinations are much\nmore common, e.g., “dog under table\" vs. “dog\non table\". Thus, we hypothesize that the model\ncan perform well on the contrastive training objec-\ntive despite ignoring spatial relationships between\nobjects in the image.\n4 Data-informed attempts at\nimprovement\nIn this section, we operationalize our hypotheses\ndetailed above to yield potential solutions to mod-\nels’ struggle with learning spatial relations.\n4.1 Incorporating Caption Priors\nThe first method we consider is a re-normalization\nof probabilities. Intuitively, some captions are\nmore likely on average across all images. We es-\ntimate the prior for a caption by calculating its\naverage dot product with a large set of images\nfrom a different source to avoid test set contam-\nination (e.g. COCO to estimate priors of a VG\ncaption). We then use that prior to re-normalize\nthe caption probability for a given image. Specifi-\ncally, we compute a re-normalized caption proba-\nbility as the difference between the un-normalized\nprobability and the caption’s calculated prior. This\nprocess is similar to the text-only normalization\nof Holtzman et al. (2021). This normalization en-\ncodes that P(caption|image) should not depend\non P(caption).\nTables 5 and 6 in the Appendix contain the re-\nsults of models with and without considering cap-\ntion priors from different datasets. Overall, it seems\nthat normalizing by caption priors does not tend\nto improve performance on What’sUp much (al-\nthough a slight improvement is observed in pair\nand set accuracies). The priors are slightly help-\nful for performance on COCO-spatial and GQA-\nspatial, likely because those two image distribu-\ntions are closer to each other than either is to\nWhat’sUp. However, overall, this approach did\nnot drastically improve model performance on any\nof the benchmarks. Thus, poor performance of\nvision-language models cannot be attributed en-\ntirely to difficult-to-overcome text-only priors on\ncorrect options of the captions we evaluate.\n4.2 Better prompts: don’t fall (for) “behind\"\nFrom our study of the LAION-2B dataset, we see\none word that is not a basic spatial preposition,\nbut gives information about spatial relations, and\nhas relatively high prevalence in the data: “back-\nground”. This word alone appears in 0.84% of\nthe captions, four times more than all of the other\nprepositions we study combined. Many of these\ncaptions describe synthetic images (e.g., “the words\nhappy new year on a red background”), but others\nprovide spatial information (e.g., “two people talk-\ning with some flowers in the background”). The\nmost similar preposition we evaluate is “behind”,\nin What’sUp Subset B.\nTo determine whether models understand the\nconcept of “behind” (but this knowledge may not\nbe accessible by using that particular word), we\n9167\ndo a case study of whether models trained on\nLAION perform better when given a prompt of\n\"background\" or \"behind\". We take the “in front\nof” and “behind” images from What’sUp Subset\nB (disregarding the “left of” and “right of” images),\nchanging the text input options to (1) “object1 be-\nhind object2” and “ object2 behind object1”, or\n(2) “object2 with object1 in the background” and\n“object1 with object2 in the background”. This al-\nlows us to evaluate only performance on “behind”\nvs “background” without conflating other factors\nsuch as performance on other prepositions. For\nCLIP ViT-B/32 and CLIP ViT-L/14 (both Open-\nCLIP versions trained on LAION), performance\non (1) is an average of 52%, just two points above\nrandom chance, whereas performance on (2) is an\naverage of 67%.\nDiscussion. This is a significant jump, and shows\nthat spatial information may indeed be present in\nthese models, but may have to be teased out more\ncarefully. A strong caveat to these results is that the\nword “background” seems to be a special case: we\nare able to run this experiment because it appears\nvery frequently in LAION, but we did not come\nacross any other such words that appear frequently\nand provide spatial understanding. Thus, while this\nis an interesting thought experiment and provides\nhope that with more data, the issue can be mitigated,\nwe do not believe it is the solution for models’ poor\nperformance on all spatial reasoning tasks.\n4.3 Finetuning\nFinally, we run several experiments with finetuning.\nIdeally, models should be able to understand basic\nspatial relations without finetuning, especially as\nfinetuning tends to lose some benefits from pretrain-\ning and is tedious and expensive to do for various\ndownstream tasks. However, we experiment with\nsome finetuning settings with CLIP ViT-B/32 to\ndetermine whether spatial reasoning can be easily\nlearned by our models with extra training. The\nresults are presented in Table 2.\nFinetuning on the train equivalents ofCOCO-\nspatial and GQA-spatial. We repeat the auto-\nmated process to curate spatial relations data from\nGQA and COCO on the training set (rather than\nthe validation set, which was used to create the\nbenchmarks), dropping the filter for the objects to\nbe at least 3% the area of the image, and dropping\nthe human quality filter. We also combine an equal\nweight of COCO captions, so the model does not\nModel Whats-\nUp\nCOCO-\nspatial\nGQA-\nspatialAvg\nCLIP ViT-B/32 31.0 47.4 46.9 41.8\n+trainCOCO-spatial\nandGQA-spatial 26.7 63.9 59.5 50.0\n+LAION-4M-prep 33.1 46.0 47.6 42.2\n+LAION-4M-prep\nwith neg. cap. 29.3 44.4 46.5 40.1\nRandom / Text-only 25.0 50.0 50.0 41.7\nHuman Estimate 100.0 97.3 99.0 98.8\nTable 2: Results of different types of finetuning on\nCLIP ViT-B/32. Even with finetuning, the results do not\nincrease by a large margin across all benchmarks.\nforget standard English. This gives us 900,000\ndata points, which we downsample to 300,000 for\ncompute reasons. When we finetune on this data,\nwe see the model improves on COCO-spatial\nand GQA-spatial by an average of 14.6 accuracy\npoints. But performance drops on What’sUp by\n4.3 accuracy points. Plausible explanations include\nthe image distributions being different, and that the\nWhat’sUp data contains unusual placements of\nobjects. Also, even with significant supervised in-\ndistribution data, performance on COCO-spatial\nand GQA-spatial still lag significantly behind hu-\nman performance (by ∼50 accuracy points).\nFinetuning on a subset of LAION including\nprepositions. We next isolate a subset of LAION\nincluding the prepositions we evaluate across our\nbenchmarks. After filtering noise, this subset con-\ntains 4M image-text pairs. When finetuned on this\ndata, performance improvements are marginal. The\nreasons for this could be as discussed in Section 3 –\nprepositions in LAION are ambiguous and rarely\nrequired to identify the image, even from a large\nbatch (we finetune with a batch size of 2048 across\n4 NVIDIA RTX A6000 GPUs).\nFinetuning on LAION-4M with hard negative\ncaptions. Taking inspiration from Yuksekgonul\net al. (2023), we add hard negative captions to the\nLAION-4M subset we curate, by programmatically\nswitching the preposition with its opposite. This\nensures that the model is forced to distinguish be-\ntween the two in order to meet the training objec-\ntive. For CLIP ViT-B/32, we observe a very high\ntraining loss, suggesting that the model cannot fit\nthis augmented corpus.2 We additionally track how\n2Across several hyperparameter settings, we consistently\nobserved loss of 5.0, compared to the loss of 0.01 for the\nsame configuration without the very hard negatives.\n9168\nthe model allocates its probability across the batch:\nloss on the positive caption is similar to the loss on\nthe negative caption, which suggests that CLIP is\nable to narrow text options down to those two cap-\ntions, but cannot consistently learn which is correct\nof the two. Experiments with ViT-B/32, ViT-B/16\nand ViT-L/14 all show this pattern when finetuned\non both 50% and 100% of the data, implying that,\nat least for the training regime we consider, scaling\nthe data or model size does not help. It is likely that\nan inductive bias or denser supervision is needed\nto enable the model to learn this, as in XVLM. The\ntrain loss curves are provided in the Appendix.\n5 Related work\nSpatial reasoning has long been evaluated by vision-\nlanguage benchmarks: VQAv2 (Goyal et al., 2016),\nGQA (Hudson and Manning, 2019), NLVR2 (Suhr\net al., 2018), CLEVR (Johnson et al., 2017) and\nShapeWorld (Kuhnle and Copestake, 2017) all con-\ntain questions requiring spatial reasoning. How-\never, many of these questions conflate several types\nof reasoning. Performance on these benchmarks\ntherefore masks VL models’ struggle with spatial\nunderstanding specifically.\nMore recently, vision-language benchmarks eval-\nuating more specific phenomena have been pro-\nposed, testing understanding of word order (Thrush\net al., 2022; Yuksekgonul et al., 2023), counting\n(Parcalabescu et al., 2021), object-attribute associ-\nation (Yamada et al., 2022), and compositionality\n(Kamath et al., 2023; Ma et al., 2022). Other work\nincluding V ALSE (Parcalabescu et al., 2022), VSR\n(Liu et al., 2023), VL-Checklist (Zhao et al., 2023)\nand ReCLIP (Subramanian et al., 2022) evaluate\nspatial reasoning in isolation, as we do in our three\ncorpora, testing VL models’ ability to match an\nimage to the more fitting of two captions where\nonly the spatial preposition is flipped. They show\nthat models have room for improvement in both\nzero-shot and finetuned settings.\nHowever, all of the non-synthetic benchmarks\nabove testing spatial reasoning are based on COCO\n(Lin et al., 2014) or Visual Genome (Krishna et al.,\n2016), which are sourced from Flickr. These im-\nages tend to have many objects, usually in cluttered\nenvironments, which can confuse models trained\nwith only image-level supervision (Yamada et al.,\n2022). The images also reflect biases in our usual\nworld, such as mugs usually beingon tables and not\nunder them3. Models may learn these priors and\nattain high scores on these benchmarks without ac-\ntually attending to the images (Hsieh et al., 2023) —\ne.g., text-only GPT-1 (Radford et al., 2018) scores\n27 accuracy points above random chance on spa-\ntial reasoning questions in V ALSE. In contrast, we\ncapture sets of photographs for What’sUp which\nare uncluttered, unambiguous, and contain all four\npreposition options for any pair of objects — thus\nexposing any bias models may have for the “usual”\nrelation between two objects, as well as preventing\nmodels with such a bias from leveraging it to mask\ntheir spatial understanding abilities.\nText-to-image generation has also been shown to\nstruggle with correctly depicting spatial relations\n(Gokhale et al., 2022; Hu et al., 2023). Our work\nsheds light on why this could be the case: e.g.,\nDALL-E 2 (Ramesh et al., 2022) uses a frozen\nCLIP backbone, and as we show in our work, CLIP\nitself struggles with spatial reasoning.\n6 Conclusion\nIn this work, we propose three new benchmarks:\nWhat’sUp, COCO-spatial and GQA-spatial, to\nevaluate VL models on basic spatial relations in a\nrange of environments, with the controlled nature\nof What’sUp allowing us to evaluate pairs and\nsets of prepositions for a given object pair. We\nobserve that all 18 models we evaluate perform\npoorly on these benchmarks in a zero-shot fashion.\nNext, we study the LAION dataset which was used\nto train OpenCLIP, revealing that prepositions are\nrare, ambiguous, and extraneous in the captions.\nFinally, we explore potential remedies, ultimately\nfinding that CLIP models, at least in the regime\nof scale we consider, fail to even fit a large-scale\ntraining set that requires precise spatial reasoning.\nHow might models solve our newly proposed\nevaluations going forward? Three promising future\ndirections include: (1) Auto-generation of hard neg-\natives for spatial prepositions (and beyond) during\npre-training; (2) Consideration of more expressive\nfine-tuned models that support image-text cross-\nattention and mixes of contrastive and generation\nobjectives; and (3) Thorough scaling experiments\nto probe for potentially promising relationships\nbetween increasing compute of vision-language\nmodels vs. performance on our benchmarks.\n3As of the time of this writing, even querying Google\nImage Search with “a mug under/left of/right of a table” did\nnot yield any accurate images.\n9169\nLimitations\nFirst, the benchmarks we propose, especially\nWhat’sUp, are restricted in scale compared to\nbenchmarks like ARO (Yuksekgonul et al., 2023)\nand GQA (Hudson and Manning, 2019). Second,\nour paper focuses on investigating how and why\nvision-language models struggle with basic spatial\nrelations: our methods to improve models, while\ngrounded in observations from our investigation,\ndo not improve model performance significantly on\nall of our benchmarks. Third, our work is restricted\nto spatial reasoning. It would be interesting to per-\nform a wide-scale study tackling several types of\nreasoning.\nAcknowledgements\nWe thank John Hewitt, Akhila Yerukola, Liunian\nHarold Li, and the anonymous reviewers for helpful\ndiscussion and feedback, as well as Travis McGuire\nand Emily Chua for helping us take photographs of\nLucy, the star of Figure 1. This work was funded\nby the Allen Institute for AI. AK was additionally\nsupported by the UCLA Computer Science Depart-\nment First-Year Fellowship. KC was supported\nin part by DARPA MCS under contract number\nN660011924032, ONR N00014-23-1-2780, and a\nSloan Fellowship. The views and conclusions con-\ntained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofficial policies, either expressed or implied, of\nDARPA, or the U.S. Government.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. no-\ncaps: novel object captioning at scale. International\nConference on Computer Vision, pages 8947–8956.\nAlexander C. Berg, Tamara L. Berg, Hal Daumé, Jesse\nDodge, Amit Goyal, Xufeng Han, Alyssa Mensch,\nMargaret Mitchell, Aneesh Sood, Karl Stratos, and\nKota Yamaguchi. 2012. Understanding and predict-\ning importance in images. In 2012 IEEE Conference\non Computer Vision and Pattern Recognition, pages\n3562–3569.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference\non Computer Vision and Pattern Recognition, pages\n248–255.\nTejas Gokhale, Hamid Palangi, Besmira Nushi, Vib-\nhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral,\nand Yezhou Yang. 2022. Benchmarking spatial rela-\ntionships in text-to-image generation. arXiv preprint\narXiv:2212.10015.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2016. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. International Journal of\nComputer Vision, 127:398–414.\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Conference on Empirical Methods in Natu-\nral Language Processing.\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha\nKembhavi, and Ranjay Krishna. 2023. Sugarcrepe:\nFixing hackable benchmarks for vision-language\ncompositionality. In Thirty-Seventh Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang,\nMari Ostendorf, Ranjay Krishna, and Noah A Smith.\n2023. Tifa: Accurate and interpretable text-to-\nimage faithfulness evaluation with question answer-\ning. 2023 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nDrew A Hudson and Christopher D Manning. 2019.\nGQA: A new dataset for real-world visual reasoning\nand compositional question answering. In CVPR,\npages 6700–6709.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman,\nCade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-\nwig Schmidt. 2021. Openclip.\nJustin Johnson, Bharath Hariharan, Laurens Van\nDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and\nRoss Girshick. 2017. CLEVR: A diagnostic dataset\nfor compositional language and elementary visual\nreasoning. In CVPR.\nAmita Kamath, Jack Hessel, and Kai-Wei Chang. 2023.\nText encoders are performance bottlenecks in con-\ntrastive vision-language models. In EMNLP.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2016. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. Interna-\ntional Journal of Computer Vision, 123:32–73.\nAlexander Kuhnle and Ann Copestake. 2017. Shape-\nworld - a new test methodology for multimodal lan-\nguage understanding.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In ICML.\n9170\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. BLIP: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In Proceedings of the 39th Interna-\ntional Conference on Machine Learning, volume 162\nof Proceedings of Machine Learning Research, pages\n12888–12900. PMLR.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In European conference\non computer vision, pages 740–755. Springer.\nFangyu Liu, Guy Edward Toh Emerson, and Nigel Col-\nlier. 2023. Visual spatial reasoning. Transactions of\nthe Association for Computational Linguistics.\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona\nGandhi, Irena Gao, and Ranjay Krishna. 2022. Crepe:\nCan vision-language foundation models reason com-\npositionally? In CVPR.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. NeurIPS.\nLetitia Parcalabescu, Michele Cafagna, Lilitta Murad-\njan, Anette Frank, Iacer Calixto, and Albert Gatt.\n2022. V ALSE: A task-independent benchmark for\nvision and language models centered on linguistic\nphenomena. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8253–8280, Dublin,\nIreland. Association for Computational Linguistics.\nLetitia Parcalabescu, Albert Gatt, Anette Frank, and\nIacer Calixto. 2021. Seeing past words: Testing\nthe cross-modal capabilities of pretrained V&L mod-\nels on counting tasks. In Proceedings of the 1st\nWorkshop on Multimodal Semantic Representations\n(MMSR), pages 32–44, Groningen, Netherlands (On-\nline). Association for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with CLIP latents.\narXiv preprint arXiv:2204.06125.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade W Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton\nMullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa R Kundurthy, Katherine Crowson, Lud-\nwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\n2022. LAION-5b: An open large-scale dataset for\ntraining next generation image-text models. InThirty-\nsixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2022. FLA V A: A foun-\ndational language and vision alignment model. In\nCVPR.\nSanjay Subramanian, William Merrill, Trevor Darrell,\nMatt Gardner, Sameer Singh, and Anna Rohrbach.\n2022. ReCLIP: A strong zero-shot baseline for re-\nferring expression comprehension. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5198–5215, Dublin, Ireland. Association for\nComputational Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2018. A corpus for\nreasoning about natural language grounded in pho-\ntographs. arXiv preprint arXiv:1811.00491.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. 2022. Winoground: Probing vision and lan-\nguage models for visio-linguistic compositionality.\nIn CVPR.\nYutaro Yamada, Yingtian Tang, and Ilker Yildirim. 2022.\nWhen are lemons purple? the concept association\nbias of clip. arXiv preprint arXiv:2212.12043.\nMark Yatskar, Luke Zettlemoyer, and Ali Farhadi. 2016.\nSituation recognition: Visual semantic role labeling\nfor image understanding. 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 5534–5542.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou. 2023. When and why\nvision-language models behave like bags-of-words,\nand what to do about it? In The Eleventh Interna-\ntional Conference on Learning Representations.\nYan Zeng, Xinsong Zhang, and Hang Li. 2022. Multi-\ngrained vision language pre-training: Aligning texts\nwith visual concepts. In Proceedings of the 39th\nInternational Conference on Machine Learning, vol-\nume 162 of Proceedings of Machine Learning Re-\nsearch, pages 25994–26009. PMLR.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\n2023. Vl-checklist: Evaluating pre-trained vision-\nlanguage models with objects, attributes and rela-\ntions.\n9171\nA Appendix\nThis section contains additional results. Table 3\ncontains detailed results of VL models on our three\nproposed benchmarks. Table 4 breaks down the\nprevalence of various prepositions in the LAION-\n2B dataset, before and after removing noisy prepo-\nsitions such as “under $25” — to emphasize that a\ndirect count of word occurrence is not sufficient to\nunderstand the low prevalence of spatial relations\nin LAION captions. Tables 5 and 6 contain results\nof the experiments targeting re-normalization of\ncaption priors. Table 7 contains detailed results of\ndifferent types of finetuning on our three bench-\nmarks. Figures 5 and 6 contain loss curves from\nfinetuning with and without hard negative captions\ntargeting prepositions — the train loss from the lat-\nter is about 500x smaller than the former, and the\nloss on the gold caption and hard negative caption\nis about the same, showing that the model struggles\nto disambiguate between the correct caption and\nthe hard distractor, amongst the entire batch.\n9172\nWhat’sUpSubset A What’sUpSubset B COCO-spatial GQA-spatial Indiv.\nAverageIndiv. Pairs Set of 4 Indiv. Pairs Set of 4 One-obj Two-obj One-obj Two-obj\nCLIP ViT-B/32 30.3 0.5 0.0 31.6 1.0 0.0 43.7 51.1 46.5 47.4 41.8\nCLIP ViT-L/14 26.5 1.0 0.0 25.7 2.0 0.0 49.2 49.8 46.1 48.5 41.0\nNegCLIP 32.5 5.3 0.0 36.3 2.0 0.0 47.4 46.4 45.3 46.7 42.4\nRoBERTaCLIP 25.2 2.4 0.0 25.0 0.0 0.0 46.3 53.6 50.8 48.8 41.6\nCoCa 29.4 2.4 0.0 29.4 3.9 0.0 48.1 45.2 45.0 49.1 41.0\nXVLM 4M 40.0 23.3 0.0 23.0 2.0 0.0 58.4 65.0 62.8 54.6 50.6\nXVLM 16M 50.7 31.1 1.9 33.1 10.8 0.0 65.4 64.5 63.2 53.3 55.0\nBLIP 14M 38.8 23.8 0.0 38.2 5.4 0.0 54.2 53.9 49.1 50.5 47.5\nBLIP 129M 30.3 4.9 1.0 30.4 3.9 0.0 44.8 53.9 50.5 47.4 42.9\nBLIP2-ITM 44.9 24.3 0.0 30.4 2.0 0.0 48.3 57.7 46.0 53.6 46.8\nBLIP2-ITC 35.9 3.4 0.0 22.1 0.0 0.0 55.6 51.8 52.6 49.5 44.6\nFLA V A 33.7 17.5 0.0 27.2 4.4 0.0 50.3 55.0 52.2 51.2 44.9\nCoCa-Caption 25.5 1.9 0.0 22.8 0.0 0.0 45.9 51.4 48.5 50.5 40.8\nXVLM-Flickr30K 45.1 16.5 0.0 43.4 17.2 1.0 63.1 67.3 64.7 58.1 56.9\nXVLM-COCO 41.7 17.0 1.9 42.4 15.7 2.9 68.4 73.6 69.1 67.0 60.4\nBLIP-Flickr30K 29.6 3.9 0.0 38.0 10.3 0.0 50.0 58.4 50.3 47.4 45.6\nBLIP-COCO 35.7 1.9 0.0 29.9 2.0 0.0 46.4 56.4 50.3 52.6 45.2\nBLIP-VQA 57.8 44.2 1.9 37.7 21.1 0.0 63.6 60.5 63.8 52.9 56.0\nRandom chance 25.0 6.3 0.4 25.0 6.3 0.4 50.0 50.0 50.0 50.0 41.7\nTable 3: Detailed results of varied vision-language models on our benchmarks: models in the first section are\nevaluated zero-shot, and models in the second section have been finetuned on a downstream task: COCO captioning,\nretrieval on Flickr30K or COCO, or VQAv2. All models perform poorly on basic spatial relations, especially under\nthe pair and set metrics (not included in the individual averages column).\nPreposition\n% before\nremoving\nnoise\n% after\nremoving\nnoise\nin front of 0.1084 0.0862\nbehind 0.0983 0.0489\nabove 0.0898 0.0422\non top of 0.0183 0.0134\nunder 0.2700 0.0097\nat the top 0.0074 0.0050\nbelow 0.0309 0.0040\non the left 0.0059 0.0038\non the right 0.0065 0.0028\nat the bottom 0.0037 0.0023\nto the right of 0.0011 0.0005\nto the left of 0.0009 0.0005\nTotal 0.6412 0.2191\nTable 4: Frequency of appearance of various prepo-\nsitions in LAION-2B (english). The spatial relations\nwe study represent less than 0.22% of the training data\nwhen combined, after removing noise.\n9173\nWhat’sUp COCO-spatial GQA-spatial Average\nw.o. priors with priors w.o. priors with priors w.o. priors with priors w.o. priors with priors\nCLIP ViT-B/32 31.0 30.0 47.4 54.0 46.9 46.2 41.8 43.4\nCLIP ViT-L/14 26.1 28.2 49.5 51.5 47.3 46.8 41.0 42.2\nNegCLIP 34.4 32.9 46.9 51.0 46.0 47.0 42.4 43.6\nRoBERTaCLIP 25.1 25.7 50.0 50.7 49.8 51.3 41.6 42.6\nCoCa 29.4 32.3 46.7 49.2 47.1 47.7 41.0 43.1\nCoCa-Caption 24.1 26.5 48.6 48.9 49.5 48.6 40.8 41.3\nRandom chance 25.0 25.0 50.0 50.0 50.0 50.0 41.7 41.7\nTable 5: Summarized results of the experiments incorporating caption priors. For each model we have shown\nthe best performance from different methods of calculating caption priors. Incorporating the low caption priors\nimproves performance in some cases, but not by a large margin overall – in many cases, even with improvement the\nmodel still performs below random chance. Detailed results are shown in Table 6.\nFigure 5: Train loss (left) and negative caption loss (right) when finetuning variants of CLIP on LAION-4M-prep\nwith hard negatives targeting prepositions, on either the full dataset or half of the dataset (suffix _2M).\nFigure 6: Train loss when finetuning variants of CLIP on LAION-4M-prep without hard negatives, on either the full\ndataset or half of the dataset. The loss is about 500x lower than in Figure 5.\n9174\nNo Priors What’sUpSubset A What’sUpSubset B COCO-spatial GQA-spatialIndiv. Avg. w.o.\nCOCO-spatial\nIndiv. Avg. w.o.\nGQA-spatialIndiv. Pairs Set of 4 Indiv. Pairs Set of 4 One-obj. Two-obj. One-obj. Two-obj.\nCLIP ViT-B/32 30.3 0.5 0.0 31.6 1.0 0.0 43.7 51.1 46.5 47.4 39.0 39.2\nCLIP ViT-L/14 26.5 1.0 0.0 25.7 2.0 0.0 49.2 49.8 46.1 48.5 36.7 37.8\nNegCLIP 32.5 5.3 0.0 36.3 2.0 0.0 47.4 46.4 45.3 46.7 40.2 40.6\nRoBERTaCLIP 25.2 2.4 0.0 25.0 0.0 0.0 46.3 53.6 50.8 48.8 37.5 37.5\nCoCa 29.4 2.4 0.0 29.4 3.9 0.0 48.1 45.2 45.0 49.1 38.2 38.0\nCoCa-Caption 25.5 1.9 0.0 22.8 0.0 0.0 45.9 51.4 48.5 50.5 36.8 36.4\nRandom chance 25.0 6.3 0.4 25.0 6.3 0.4 50.0 50.0 50.0 50.0 37.5 37.5\nCOCO priorsWhat’sUpSubset A What’sUpSubset B COCO-spatial GQA-spatialIndiv. Avg. w.o.\nCOCO-spatial\nImprovement\nover no priorIndiv. Pairs Set of 4 Indiv. Pairs Set of 4 One-obj. Two-obj. One-obj. Two-obj.\nCLIP ViT-B/32 29.4 8.7 0.0 30.6 2.5 0.0 - - 47.5 45.0 38.1 -0.9\nCLIP ViT-L/14 31.1 5.8 0.0 25.2 5.4 0.0 - - 46.0 47.7 37.5 0.8\nNegCLIP 31.1 7.8 0.0 34.8 2.0 0.0 - - 45.9 48.1 40.0 -0.3\nRoBERTaCLIP 24.3 0.0 0.0 26.0 1.5 0.0 - - 50.3 52.3 38.2 0.8\nCoCa 34.2 6.3 1.0 30.4 0.5 0.0 - - 45.4 50.0 40.0 1.8\nCoCa-Caption 26.7 3.9 0.0 26.2 2.0 0.0 - - 47.9 49.2 37.5 0.7\nRandom chance 25.0 6.3 0.4 25.0 6.3 0.4 50.0 50.0 50.0 50.0 37.5 -\nVG priors What’sUpSubset A What’sUpSubset B COCO-spatial GQA-spatialIndiv. Avg. w.o.\nGQA-spatial\nImprovement\nover no priorIndiv. Pairs Set of 4 Indiv. Pairs Set of 4 One-obj. Two-obj. One-obj. Two-obj.\nCLIP ViT-B/32 29.9 8.7 0.0 29.2 2.0 0.0 51.7 56.3 - - 41.7 2.5\nCLIP ViT-L/14 30.3 5.3 0.0 25.5 5.4 0.0 50.5 52.5 - - 39.7 1.9\nNegCLIP 31.1 7.8 0.0 33.8 2.0 0.0 50.4 51.5 - - 41.7 1.1\nRoBERTaCLIP 24.8 0.0 0.0 26.7 1.5 0.0 48.1 53.2 - - 38.2 0.7\nCoCa 33.5 5.3 0.0 30.4 1.0 0.0 50.5 47.8 - - 40.6 2.5\nCoCa-Caption 26.7 2.9 0.0 26.2 1.5 0.0 50.1 47.8 - - 37.7 1.3\nRandom chance 25.0 6.3 0.4 25.0 6.3 0.4 50.0 50.0 50.0 50.0 37.5 -\nTable 6: Detailed results of the experiments incorporating caption priors, with different methods of calculating\ncaption priors: no priors (top), COCO priors (middle), VG priors (bottom). Incorporating the low caption priors\nimproves performance in some cases, but not by a large margin overall.\nWhat’sUpSubset A What’sUpSubset B COCO-spatial GQA-spatialIndiv. Avg.Indiv. Pairs Set of 4 Indiv. Pairs Set of 4 One-obj. Two-obj. One-obj. Two-obj.\nCLIP ViT-B/32 30.3 0.5 0.0 31.6 1.0 0.0 43.7 51.1 46.5 47.4 41.8\nFT on trainCOCO-spatial,GQA-spatial28.2 7.3 0.0 25.2 2.0 0.0 67.2 60.7 64.4 54.6 50.0\nFT on LAION-4M-prep 31.6 1.0 0.0 34.6 2.9 0.0 43.1 48.9 44.3 50.9 42.2\nFT on LAION-4M-prep + neg. cap. 32.0 0.0 0.0 26.5 0.0 0.0 39.9 48.9 47.3 45.7 40.1\nRandom chance 25.0 6.3 0.4 25.0 6.3 0.4 50.0 50.0 50.0 50.0 41.7\nTable 7: Detailed results of different types of finetuning on CLIP ViT-B/32.\n9175"
}