{
  "title": "Creating Image Datasets in Agricultural Environments using DALL.E: Generative AI-Powered Large Language Model",
  "url": "https://openalex.org/W4393147314",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2176382472",
      "name": "Ranjan Sapkota",
      "affiliations": [
        "Washington State University"
      ]
    },
    {
      "id": "https://openalex.org/A2251628206",
      "name": "Dawood Ahmed",
      "affiliations": [
        "Washington State University"
      ]
    },
    {
      "id": "https://openalex.org/A1776828804",
      "name": "Manoj Karkee",
      "affiliations": [
        "Washington State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166254754",
    "https://openalex.org/W4311776816",
    "https://openalex.org/W4309995522",
    "https://openalex.org/W4281484041",
    "https://openalex.org/W4206324004",
    "https://openalex.org/W3166763252",
    "https://openalex.org/W4309191446",
    "https://openalex.org/W4309703584",
    "https://openalex.org/W3165489647",
    "https://openalex.org/W3110877328",
    "https://openalex.org/W4312480149",
    "https://openalex.org/W2970399728",
    "https://openalex.org/W4220763935",
    "https://openalex.org/W2148462611",
    "https://openalex.org/W2803094037",
    "https://openalex.org/W4220986677",
    "https://openalex.org/W4312950208",
    "https://openalex.org/W3215644105",
    "https://openalex.org/W2772330423",
    "https://openalex.org/W3173170759",
    "https://openalex.org/W4365392748",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W2997266778",
    "https://openalex.org/W3001613285",
    "https://openalex.org/W3033864984",
    "https://openalex.org/W4285181553",
    "https://openalex.org/W3169421469",
    "https://openalex.org/W2974769261",
    "https://openalex.org/W3038379525",
    "https://openalex.org/W3214774339",
    "https://openalex.org/W3210047443",
    "https://openalex.org/W3047147399",
    "https://openalex.org/W3112181535",
    "https://openalex.org/W3204748455",
    "https://openalex.org/W3164040222",
    "https://openalex.org/W3080200407",
    "https://openalex.org/W2999363320",
    "https://openalex.org/W3086635719",
    "https://openalex.org/W3088685598",
    "https://openalex.org/W3127253737",
    "https://openalex.org/W2936718694",
    "https://openalex.org/W3111506096",
    "https://openalex.org/W3216854021",
    "https://openalex.org/W3107573522",
    "https://openalex.org/W3127682982",
    "https://openalex.org/W3194148782",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W3193997983",
    "https://openalex.org/W4319301446",
    "https://openalex.org/W3083926560",
    "https://openalex.org/W3180353325",
    "https://openalex.org/W6631455383",
    "https://openalex.org/W2141983208"
  ],
  "abstract": "This research investigated the role of artificial intelligence (AI), specifically the DALL.E model by OpenAI, in advancing data generation and visualization techniques in agriculture. DALL.E, an advanced AI image generator, works alongside ChatGPT's language processing to transform text descriptions and image clues into realistic visual representations of the content. The study used both approaches of image generation: text-to-image and image-to-image (variation). Six types of datasets depicting fruit crop environment were generated. These AI-generated images were then compared against ground truth images captured by sensors in real agricultural fields. The comparison was based on Peak Signal-to-Noise Ratio (PSNR) and Feature Similarity Index (FSIM) metrics. The image-to-image generation exhibited a 5.78% increase in average PSNR over text-to-image methods, signifying superior image clarity and quality. However, this method also resulted in a 10.23% decrease in average FSIM, indicating a diminished structural and textural similarity to the original images. Similar to these measures, human evaluation also showed that images generated using image-to-image-based method were more realistic compared to those generated with text-to-image approach. The results highlighted DALL.E's potential in generating realistic agricultural image datasets and thus accelerating the development and adoption of imaging-based precision agricultural solutions.",
  "full_text": "Open Peer Review on Qeios\nCreating Image Datasets in Agricultural Environments using\nDALL.E: Generative AI-Powered Large Language Model\nRanjan Sapkota\n1\n, \nDawood Ahmed\n1\n, \nManoj Karkee\n1\n1\n Washington State University\nFunding:\n No specific funding was received for this work.\nPotential competing interests:\n No potential competing interests to declare.\nAbstract\nThis research investigated the role of artificial intelligence (AI), specifically the DALL.E model by OpenAI, in advancing\ndata generation and visualization techniques in agriculture. DALL.E, an advanced AI image generator, works alongside\nChatGPT's language processing to transform text descriptions and image clues into realistic visual representations of\nthe content. The study used both approaches of image generation: text-to-image and image-to-image (variation). Six\ntypes of datasets depicting fruit crop environment were generated. These AI-generated images were then compared\nagainst ground truth images captured by sensors in real agricultural fields. The comparison was based on Peak Signal-\nto-Noise Ratio (PSNR) and Feature Similarity Index (FSIM) metrics. The image-to-image generation exhibited a 5.78%\nincrease in average PSNR over text-to-image methods, signifying superior image clarity and quality. However, this\nmethod also resulted in a 10.23% decrease in average FSIM, indicating a diminished structural and textural similarity to\nthe original images. Similar to these measures, human evaluation also showed that images generated using image-to-\nimage-based method were more realistic compared to those generated with text-to-image approach. The results\nhighlighted DALL.E's potential in generating realistic agricultural image datasets and thus accelerating the development\nand adoption of imaging-based precision agricultural solutions.\nRanjan Sapkota\n*\n, \nDawood Ahmed\n, and \nManoj Karkee\n*\nCenter for Precision Automated Agricultural Systems, Washington State University, 24106 N. Bunn Rd, Prosser, 99350,\nWashington, USA\n \nKeywords: \nDALL.E, Text-to-image, Image-to-image, Large Language Model (LLM), ChatGPT, OpenAI, Generative\nArtificial Intelligence (AI), Generative Adversarial Networks (GANs) and Image Generation.\n \nIntroduction\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n1\n/\n22\nIn recent years, synthetic images, generated by computer algorithms to resemble real-world entities, have been widely\nused in various sectors, such as healthcare\n[1]\n, biomedicine\n[2]\n, fashion\n[3]\n, architecture\n[4]\n, geospatial studies\n[5]\n, automotive\nindustry\n[6]\n, and agriculture\n[7]\n due to their ability to provide realistic visual representations for observation, and analysis,\nand driving innovations\n[8]\n.\nTraditional methods of creating these images include parametric techniques such as \n[9]\nBezier Curves used by Chen et\nal. \n[9]\n to develop more accurate synthetic images of cell nuclei for biomedical applications. Similarly, Alberto et al. \n[10]\n used\nBezier Curves for designing complex mechanical structures using synthetic images. Another classical method is ray\ntracing, a technique to render realistic images by simulating light paths, improved upon by Ben et al. \n[11]\n through Neural\nRadiance Fields for better low-light image reconstruction. Additionally, Physics-based Rendering (PBR)\n \n[12]\n, a method that\nmimics real-world light flow, has been used effectively for creating photorealistic images, as demonstrated by Hodan et\nal. \n[12]\n in their AI-based object detection research. These traditional image generation methods highlighted the evolving\nrole of synthetic images in driving technological advancements \n[13]\n. However, these traditional models of synthetic image\ngeneration come with notable limitations. Parametric models, for instance, rely heavily on the accuracy of parameters and\nequations, making them less adaptable to complex or irregular shapes\n[14]\n and environments that don't align with\npredefined mathematical structures \n[15]\n. The ray tracing technique faces challenges due to high computational intensity\nand time\n[16]\n. This technique also can be limiting in simulating complex lighting effects like indirect light and reflections \n[17]\n.\nPBR, on the other hand, has reduced flexibility and high computational demands\n[18]\n.\nGenerative Adversarial Networks (GANs) present a promising alternative to generate synthetic images. These networks,\nconsisting of a generator and a discriminator, efficiently produce realistic synthetic images\n[19]\n. GANs provide greater\nflexibility than parametric and other traditional models by learning from high-dimensional data distributions, enabling them\nto generate more realistic images, even in complex scenarios \n[20]\n. This approach also addresses the computational\nchallenges of ray tracing, as trained GANs can quickly generate new images \n[17]\n. Furthermore, GANs balance the realism-\nflexibility trade-off better than the PBR method, allowing detailed image generations without sacrificing quality\n[21]\n. Their\nability to create realistic images in complex environments enable them to be adoptable to wider fields and applications\n[22]\n,\nmaking them a crucial tool in synthetic image generation.\nIn recent years, the application of Generative Adversarial Networks (GANs) in agriculture has gained increasing attention,\nparticularly for tasks like disease detection and image augmentation, yielding promising results. For instance, Abbas et\nal.\n[20]\n demonstrated the effectiveness of GANs, specifically using a Conditional GAN (C-GAN), to generate synthetic\nimages of tomato plant leaves. This technique, combined with a DenseNet121 model and transfer learning, achieved a\nhigh accuracy of 99.5% in classifying tomato leaf diseases. It's noteworthy that their approach integrated both synthetic\nand actual images to enhance classification accuracy, suggesting a blend of novel and traditional methodologies.\nFurthermore, Lu et al. \n[23]\n utilized GANs to create synthetic images of insect pests, thereby augmenting limited actual\ndatasets (collected with sensors). This innovation significantly improved the performance of classifiers for insect pests,\nhighlighting the utility of GANs in scenarios where actual data is scarce. Nazki et al. \n[24]\n explored a different aspect of\nGANs by employing them for image-to-image translation in plant disease datasets, which facilitated more accurate\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n2\n/\n22\ndisease classification. These studies indicate a trend towards using GANs not just for dataset augmentation - a role\ntypically filled by conventional techniques like rotation and flipping - but also as a crucial tool in synthesizing and\nenhancing the quality of agricultural datasets. This shift marks a significant advancement in the application of AI in\nagriculture, opening new pathways for research and practical applications in the field.\nStudies\n[20]\n[25]\n have shown that GANs effectively address the challenges of biological variability and the complexity of\nunstructured agricultural environments by successfully identifying and classifying pest and plant leaf diseases. GANs have\nbeen instrumental in several key areas of agricultural image processing. GANs enhance model efficiency by reducing the\nneed for extensive data collection and labeling, particularly in diverse crop scenarios\n[26]\n. For instance, Gomaa et\nal.\n \n[27]\n utilized a combination of Convolutional Neural Networks (CNN) and GANs for disease detection in tomato plants,\nhighlighting the synergistic potential of combining traditional and generative models. Similarly, Madsen et al.\n[28]\n applied\nWasserstein auxiliary classifier generative adversarial networks (Wac-GAN) to model seedlings of nine different plants,\nshowcasing the versatility of GANs in handling varied crop types. Zhu et al.\n \n[29]\n took a specialized approach with\nConditional Deep Convolutional GANs (C-DCGAN) for orchid seedling vigor rating, emphasizing the precision capabilities\nof GANs. Further, studies like Hartley et al. \n[30]\n with wheat for plant head detection using CycleGAN\n \n[30]\n, and Bird et\nal. \n[31]\n focusing on lemon quality assessment using C-GAN illustrate the broad applicability of these networks across\ndifferent crop environment.\nTable 1 summarizes these recent efforts, showcasing how the integration of GANs in synthetic image generation is\nrevolutionizing agricultural applications and contributing to the advancement of machine vision systems in agriculture.\nAuthor Reference\nTarget Crop\nSynthetic Image Generation Technique\nPrimary Objective\nAbbas et. al \n[20]\nTomato plants\nConditional Generative Adversarial Network (C-GAN)\nDisease detection\nGomaa et. al \n[27]\nTomato plants\nConvolutional Neural Network (CNN) and GAN\nDisease detection\nMadsen et. al \n[28]\nNine different plant seedlings\nas:\n1. Charlock\n2. Cleavers\n3. Common Chickweed\n4. Fat Hen\n5. Maize\n6. Scentless Mayweed\n7. Shepherd’s Purse\n8. Small-flowered Cranesbill\n9. Sugar Beets\nWasserstein auxiliary classifier generative adversarial network\n(Wac-GAN)\nModeling plant seedlings\nZhu et. al \n[29]\nOrchid seedlings\nConditional deep convolutional generative adversarial network (C-\nDCGAN)\nPlant Vigor rating\nHartley et. al \n[30]\nWheat\nCycleGAN\nPlant head detection\nBird et. al \n[31]\nLemons\nC-GAN\nFruit quality assessment and defect\nclassification\nTable 1. \nOverview of GAN-based Synthetic Image Generation in Agriculture (2019-2024), Highlighting Image Generation Techniques, Crops, and\nKey Achievements\n.\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n3\n/\n22\nclassification\nShete et. al \n[32]\nMaize plants\nTasselGAN and deep convolutional generative adversarial\nnetworks (DCGAN)\nImage generation of maize tassels\nagainst sky backgrounds\nGuo et. al \n[33]\nJujubes\nDCGAN\nQuality grading\nDrees et. al \n[34]\nArabidopsis thaliana and\ncauliflower\nplants\nC-GAN (Pix2Pix)\nLaboratory-grown and field-grown\nimage generation\nKierdorf et. al \n[35]\nGrapevine Berries\nC-GAN/CDCGAN\nEstimation of occluded fruits\nOlatunji et. al \n[36]\nKiwifruit\nC-GAN\nFilling in missing fruit surface (Re-\nconstruction)\nBellocchio et.\nal \n[37]\nApple orchard\nCycleGAN\nUnseen fruits counting\nFawakherji et.\nal \n[38]\nSugar beet, sunflower\nCGAN/CDCGAN\nCrop/weed segmentation in precision\nfarming\nZeng at. al \n[39]\nCitrus\nDCGAN\nDisease severity detection\nKim at. al \n[40]\nBlueberry leaves\nDCGAN\nFruit tree disease classification\nTian et. al \n[41]\nApple canopy\nCycleGAN\nDisease detection\nCap et. al \n[42]\nCucumber leaves\nCycleGAN\nPlant disease diagnosis\nMaqsood et. al \n[43]\nWheat\nsuper-resolution generative adversarial networks (SR-GAN)\nWheat stripe(yellow) rust classifica-\ntion\nBi et. al \n[44]\nGrape, Orange, Potato,\nSquash,\nTomato\nWasserstein generative adversarial network with gradient penalty\n(WGAN-GP)\nPlant disease classification\nZhao et. al \n[45]\nApple, Corn, Grape, Potato,\nTomato\nDoubleGAN\nPlant disease detection\nNerkar et. al \n[46]\nApple, corn, tomato, potato\nReinforced GAN\nLeaf disease detection\nBuilding upon the foundational advancements introduced by Generative Adversarial Networks (GANs) in agricultural\nimage processing, the DALL·E model by OpenAI (OpenAI, California, USA) represents a significant leap forward in the\ndomain of AI-based image generation. Integrating the principles of GANs with innovative technologies such as Compact\nLanguage-Image Pretrained (CLIP) embeddings \n[47]\n, and Principal Component Analysis (PCA) for dimensionality\nreduction \n[48]\n, DALL·E transcends the capabilities of traditional image generation methods \n[49]\n. One of the major\nbreakthroughs with the DALL·E model is that it can convert textual descriptions into realistic images. Additionally, the\nmodel can generate a variation within an image representing similar environments. This capability of the model is\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n4\n/\n22\nachieved using text-conditional hierarchical image generation strategy \n[48]\n. Building on the foundation of ChatGPT\ndeveloped by the same organization (OpenAI, California, USA), this model has been trained on an extensive variety and\nsize of image-text pairs. Both models, stemming from the same OpenAI lineage, manifest exceptional competence in\nmanaging intricate, multi-dimensional tasks\n \n[50]\n. For instance, while ChatGPT excels at generating contextually relevant\ntextual responses, DALL·E emerges as a powerhouse in producing images that accurately represent the semantics of the\ninput text\n[51]\n. Even though synthetic image generation has become easier and more accessible while providing more\nrealistic images with OpenAI’s DALL.E model, there is a need to thoroughly assess and evaluate its capability in\nrepresenting field environments and its practicality in agricultural applications. To address this need, the following specific\nobjectives were pursued in this study:\nTo assess and evaluate the DALL·E model's proficiency in translating detailed textual prompts into accurate and\nrealistic visual representations using text-to-image generation feature of the model.\nTo evaluate DALL·E model's ability to accurately transform an image prompt into generating realistic images of the\nsimilar environment using image variation feature of the model.\nMethods\nData Collection and Compilation\nIn this study, the focus of image analysis was six distinct fruit crops datasets, as shown in Figure 1. The dataset\nencompassed a variety of fruit crops, including strawberries, mangoes, apples, avocados, rockmelons, and oranges.\nThese fruits were carefully selected for their distinctive morphological, textural, and color characteristics, as well as their\ndiverse backgrounds.\nFigure 1. \nWorkflow showing process of utilizing DALL.E for dataset creation in this research focusing on fruit crops in agriculture\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n5\n/\n22\nThe original ground truth images for both datasets were obtained from “A Survey of Public Datasets for Computer Vision\nTasks in Precision Agriculture” by Lu and Young \n[52]\n. Six representative images from the fruit crops dataset were randomly\nselected.\nFigure 2. \nFlow diagram depicting the Two-Step process utilized to generate agricultural image datasets using by the Generative AI Model DALL.E:\nThe first step involves synthesizing images from textual prompts without any visual input, and the second step generates variations using a ground\ntruth image as a reference. \nFollowing this, in the initial image generation step, input text prompts were crafted by carefully examining the original\nimages as depicted in Figure 2. These prompts were then used to generate the category of images. For the second\napproach, we directly used the ground truth images as input to the DALL.E model to create variations.\nAfter a processing step, images were generated for both categories. The generated images from both approaches were\ncompared against their respective ground truth images. We evaluated the resulting visuals using key metrics: Peak Signal-\nto-Noise Ratio (PSNR) for image clarity and pixel accuracy, and Feature Similarity Index (FSIM) for structural similarity.\nAdditionally, human assessments were conducted to confirm their realism.\nDALL.E Image Generation Model\nIn this study, DALL·E 2 (OpenAI, California, USA) image generation model was used, which utilizes hierarchical text-\nconditional image generation to produce images based on textual descriptions \n[53]\n. The hierarchical text-conditional image\ngeneration involves a (contrastive model) CLIP image embedding from a given text caption, taking advantage of CLIP's\nability to learn robust image representations that encompass both the subject matter and stylistic elements \n[54]\n. The\nsecond stage involves a decoder that creates an image based on this embedding. This method is designed to enhance\nthe variations in the generated images while maintaining their photorealism and relevance to the caption \n[55]\n. Additionally,\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n6\n/\n22\nit allows for the generation of image variations that retain the core semantics and style, altering only the incidental details\nnot captured in the image representation. The DALL.E model leverages diffusion models in the decoding phase to\ndiscover effective techniques for creating high-quality images. These images can be finely tuned based on textual\ndirections, eliminating the necessity for the model to undergo specialized pre-training for distinct image editing operations.\nThe model consists of three stage processes: encoder, prior and decoder. The model takes a textual input which is then\nencoded into a Compact Language-Image Pretrained (CLIP) text embedding based on a neural network trained on\nhundreds of millions of tax-image pairs. Dimensionality of the resulting CLIP text embedding is then reduced using\nPrincipal Component Analysis (PCA) before the results are provided the prior stage. In the prior stage, a Transformer\nmodel with an attention mechanism transforms the CLIP text embedding into an image embedding. Following the prior\nstage, the image embedding goes through the decoder stage, also known as the unCLIP phase, in which a diffusion\nmodel based on Generative Adversarial Network (GAN) is used to convert it into an image. The output is subsequently\ngenerated through two Convolutional Neural Networks (CNNs) for upscaling: first from 64x64 resolution to 256x256, and\nthen to a final resolution of 1024x1024. The model utilizes semantic components, handling inpainting tasks, and altering\nimages based on subtle changes in the contextual understanding of the input text to produce the output.\nText-to-image generation\nIn this study, text prompts displayed in Figure 2 were created to generate images across the six specified categories of\nfruit crops. These text prompts were carefully designed to ensure that the synthetic images conveyed significant\ninformation, closely representing the real images. Initially, a manual analysis of randomly selected ground truth (actual)\nimages was conducted for six fruit crop environments. Text prompts were then crafted based on the visual characteristics\nof the ground truth images, with input text ranging from a minimum of 4 words to a maximum of 10 words, as illustrated in\nFigure 2. For all fruit crops, the input text prompts were uniform, describing the \"\nname of the fruit\n in the field for\nharvesting,\" where \"in the field for harvesting\" was a common phrase reflecting the harvesting condition, making a 5-word\ninput text prompt used for each category.\nImage-to-image (variations) generation\nIn this approach, actual images (ground truth images) representing the specific fruit crop categories were provided to the\nmodel as input image prompts as shown in Figure 3. The model was then activated to generate four variations of the\ngiven input image upon receiving the command \"Generate Variations.\" An illustration of this approach to image generation\nis depicted in Figure 3.\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n7\n/\n22\nFigure 3. \nAn example showing image-to-image variation generation using the DALL.E model\nAnalysis of the generated images\nIn this research, the generated images were analyzed to assess their fidelity and realism. Evaluation metrics as PSNR and\nFSIM were employed to quantify the similarity between AI-generated and ground-truth images. Additionally, human\nevaluations by 15 scholars from Washington State University, Irrigated Agriculture Research and Extension Center\n(IAREC) provided subjective insights into the realistic portrayal of these images. Figure 4 depicts the image analysis\nprocedure used. All generated image datasets, obtained through the text-to-image and image-to-image approaches\ndiscussed above, underwent a standardized preprocessing procedure. Initially, the images were converted to grayscale\nand resized to a resolution of 256 by 256 pixels for subsequent pixel-level analysis. For the statistical comparison of the\ngenerated images with the respective ground truth images, the images were resized and converted to grayscale as shown\nin Figure 4.\nEvaluation Measures\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n8\n/\n22\nFigure 4. \nBlock diagram showing the process of calculating feature similarity index (FSIM)\nIn this study, the images generated in both Step 1 (text-prompt-based image generation) and Step 2 (image-prompt-based\nimage variation generation) were compared against the ground truth images using two standard metrics as follows.\nPeak Signal to Noise Ratio (PSNR): PSNR served as a metric for assessing image quality by evaluating the ratio of the\nmaximum potential power of the signal (represented by the original image) to the power of disruptive noise (capturing the\ndisparities between the original and the AI-generated image) as given by Equation 1. Mean Squared Error (MSE)\nestimated using Equation 2 was used to calculate this ratio. A higher PSNR typically indicates that the generated image is\ncloser in quality to the original image and has minimal distortion.\nP\nS\nN\nR\n=\n10\nlog\n10\nM\nA\nX\n2\nI\nM\nS\nE\nWhere, MAX\nI\n \n​\ndenotes the maximum possible pixel value in the image and Mean Squared Error (MSE) is computed as the\naverage of the squared differences between corresponding pixels in the two images. In simple words, MSE helps in\nunderstanding how much the generated image (G) deviates from the original image (O) on a pixel-by-pixel basis.\nM\nS\nE\n=\n1\nN\nN\n∑\ni\n=\n1\n(\nO\n−\nG\n)\n2\nFeature Similarity Index (FSIM):\n FSIM assesses the similarity between the AI-generated images and the original/actual\nimages based on their features. It evaluates both basic and intricate image features, providing a thorough measure of\nsimilarity. FSIM considers aspects like structure, luminance, and contrast of the images. Although the exact calculation of\nFSIM (Equation 3) involves complex comparisons at multiple scales, the key idea is that it measures how closely the\nfeatures of the generated image match those of the original image \n[56]\n. In this analysis, the images were preprocessed by\n()\n()\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n9\n/\n22\nbeing resized to 256 by 256 pixels and then converted into grayscale images. Canny edge features were then extracted\nfrom the grayscale images to calculate the Gabor filter responses. These responses were used to calculate the similarity\nmeasures and evaluate the feature similarity score, as shown in Figure 4.\nF\nS\nI\nM\n=\nK\n∏\nk\n=\n1\nl\nk\n⋅\nc\nk\n⋅\nS\nk\nL\nk\n⋅\nC\nk\n⋅\nS\nk\nα\nk\nWhere, l\nk\n, c\nk\n, and s\nk\n are the local similarity, contrast, and structure measurements at scale k, respectively, and α\nk\n​\n is the\nweight assigned to each scale.\nHuman Evaluation: \nA group of 15 agricultural scholars including graduate students and professors from Biological\nSystems Engineering and Horticulture departments at Washington State University participated in an unbiased survey to\nevaluate the realism of images generated by the DALL·E model. To ensure independence and minimize bias, the same\nset of images used for PSNR and FSIM analysis (8 images each for ground truth, text-generated set and image-variation-\ngenerated set) was provided to the participants. The participants were unaware of whether the individual images were\ngenerated using AI or were acquired in the field using a camera. Each participant was given only the name of the crop\nenvironment with no additional information. They used a 5-point likelihood scale to rate the realism, ranging from 'Not at all\nrealistic' (1) to 'Extremely realistic' (5).\nResults and Discussion\nThe result images generated in this study by the DALL·E model from textual inputs are presented in Figure 5. The model\naccurately depicted strawberries in the field condition with plastic mulch (Figure 5a), mangoes on tree branches (Figure\n5b), mature apples in a tree (Figure 5c), avocados in tree canopies with foliage (Figure 5d), a rockmelon with its\ncharacteristic netted skin (Figure 5e), and oranges on tree section realistically as shown in Figure 5f. Each image\neffectively illustrated the distinct morphological features of the fruits and their environments, showcasing the model’s\nability to create detailed and contextually precise visual representations from text descriptions. Variations of these fruit\ncrops, based on ground truth images, were also generated by the model. These variations were subtly different yet\nretained the essence of the original images. For strawberries, variations in the ground cover were shown; avocados were\ndepicted hanging in different positions; apples were presented in various cluster formations; and the oranges, rockmelons,\nand mangoes were characterized by their vibrant colors, unique textures, and distinct shapes.\n( )\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n10\n/\n22\nFigure 5. \nFruit crop images generated using text-to-image generation approach using the DALL.E model; (a) strawberries; (b) mangoes; (c) apples;\n(d) avocados; (e) rockmelon; and (f) oranges.\nLikewise, the image generated by the DALL.E model using the ground truth image as an input to generate image-\nvariations are depicted in Figure 6. Each of the six fruit types including strawberries (Figure 6a), avocados (Figure 6b),\napples (Figure 6c), mangoes (Figure 6d), rockmelons (Figure 6e), and oranges (Figure 6f) were characterized by subtle\nyet realistic modifications, maintaining the essence of the original, ground truth images.\nQuantitative Similarity Measures\nFor the text-generated images, as shown in Figure 7a the PSNR values ranged from a low of 8.3 for rockmelons to a high\nof 10.6 for avocados, indicating a variation in the model’s ability to replicate image quality. In contrast, the PSNR for\nimage-generated (variation) images was 14.6 for mangoes, suggesting a potential for superior representation of the reality\nof fruit crop environments. However, the lowest PSNR in this category was 8.8 for strawberries, highlighting a potential\nweakness in representing reality in a wider range of agricultural environments.\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n11\n/\n22\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n12\n/\n22\nFigure 6. \nThree variations (right) of fruit crop images generated by DALL·E 2 model using original images (left) as an input; (a) strawberries; (b)\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n13\n/\n22\nFigure 6. \nThree variations (right) of fruit crop images generated by DALL·E 2 model using original images (left) as an input; (a) strawberries; (b)\nmangoes; (c) apples; (d) avocados; (e) rockmelon; and (f) oranges.\nThe observation that image variation generation underperforms in comparison to text-to-image generation, as measured\nby PSNR and FSIM, may initially seem counterintuitive. However, this outcome can be attributed to the inherent\ndifferences in the model's approach to generating images from textual versus image prompts. Text-to-image generation\nrelies on the model's understanding and interpretation of textual descriptions to create an image from scratch, potentially\nallowing the model to \"idealize\" the output, closely matching key features described in the text while maintaining overall\ncoherence and fidelity. In contrast, image variation generation starts with an existing image and attempts to introduce\nvariations within the constraints of the original image's context. This process may inherently limit the extent to which the\nmodel can optimize for clarity and structural similarity, as it must balance between preserving the original image's integrity\nand introducing meaningful variations. As a result, the variations might introduce or exacerbate minor discrepancies in\ntexture or structural details, which could explain the lower PSNR and FSIM scores. This suggests a trade-off in the\nmodel's performance between generating novel images from textual descriptions and modifying existing images to create\nvariations, highlighting the challenges in achieving both high fidelity and meaningful diversity in generated images.\nFigure 7. \nBox plots illustrating the distribution of PSNR and FSIM for all fruit crops tested; a) comparing text-to-image; b) image-to-image generation\nmethods in agricultural AI applications.\nThese results suggest a generally lower performance in maintaining feature similarity compared to the original images,\nparticularly in the case of avocados (0.2) when compared to text-generated images. While both text and image-prompt\napproaches displayed strengths in certain aspects, there were notable variations in performance across different fruit\ntypes and metrics. This analysis indicates that the model's effectiveness in generating accurate and realistic images in\ndiverse agriculture environment is promising but is dependent on crop types and cropping environments.\nHuman Evaluation Results\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n14\n/\n22\nResults of the human assessment of the AI-generated and original images for all six fruit crops are depicted in Figure 8. In\nthe text-to-image category, apples consistently received high ratings, indicating a strong capability of the AI model to\ninterpret textual prompts and generate realistic visual representations. On the other hand, Avocados recorded lower\nratings, suggesting challenges in capturing their unique textures, colors and/or other features through text descriptions\nalone. In generating the image-to-image variations, Mangoes and Rockmelons received notably high ratings, showcasing\nthe model's proficiency in creating realistic variations from existing images. The lower ratings for Strawberries in this\ncategory might reflect difficulties in maintaining the fruit's distinct characteristics in generating variations. Ground truth\nimages, as expected, generally received the highest ratings across all categories, affirming their authenticity, which also\nindicated that there is a huge room for improvement in AI modeling to replicate complexities in the plant canopies and\nagricultural fields.\nDespite those challenges, it is noted that there were instances where text-based or image-based AI generations\noutperformed the original images in specific fruit crops. For example, image-to-image variations of Mangoes and\nRockmelons occasionally surpassed ground truth ratings. This could be attributed to the AI's ability to enhance certain\nvisual aspects, such as color vibrancy or clarity, making them more appealing than the actual photographs to human\nobservers. The success in these instances shows the potential of AI-based image generation to not only replicate but\npotentially improve upon real-world images of agricultural fields.\nFigure 8. \nBar chart illustrating average human evaluation ratings for Text-to-Image, Image-to-Image variations, and Ground Truth across six\ndifferent fruit crops images in this survey of image generation process using Generative AI.\nFigure 9 presents a heatmap detailing the MSE, PSNR, and FSIM for both Text-to-Image and Image-to-Image (variation)\ngenerated outputs. For fruit crops, text-to-image generation yielded PSNR values up to 10.95 (for avocados), indicating a\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n15\n/\n22\ncommendable image quality. However, image-based variations surpassed this, with rockmelons achieving a higher PSNR\nof 14.592, suggesting a closer resemblance to actual images. The Feature Similarity Index (FSIM) echoed this trend; while\ntext-generated images like mangoes scored 0.308, indicating satisfactory structural similarity, image-based generation\nscored marginally lower at 0.287 for the same fruit.\nThese results indicate that while text-based generation showed notable proficiency, image-based variations generally\noffered enhanced clarity and fidelity. The best results was achieved for rockmelons image-based generation, whereas the\nlowest was in text-based generation for avocados. This pattern suggests that while AI applications like DALL.E in this\nstudy can generate reasonably accurate representations from textual descriptions, providing image prompts leads to more\nprecise and realistic visual outputs, especially in complex agricultural scenarios.\nThe image-to-image generation method consistently outperformed the text-to-image approach. Notably, rockmelons\ngenerated through image-to-image variations exhibited high clarity and detail, as evidenced by superior PSNR score of\n14.6. This was indicative of the AI's proficiency in producing clear and detailed images, essential for tasks like crop growth\nmonitoring and yield estimation. Mango crop images, generated through both methods, showcased high structural\nsimilarity with the actual images achieving an FSIM score of 0.31 for text-to-image generation method and 0.29 in image-\nto-image generation method. These results also highlight the AI's capability to maintain structural integrity, crucial for\nshape-based agricultural tasks such as on fruit crops results.\nFigure 9. \nHeatmap for the DALL.E Model Performance: Showcases a detailed comparison of MSE, PSNR, and FSIM metrics\nacross the six fruit crops\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n16\n/\n22\nThese results demonstrated that the DALL.E model can be used to generate large image datasets for agricultural\napplications with good accuracy and structural integrity. Such AI-generated images can significantly simplify the data\ngeneration process, reducing time and costs associated with traditional methods that rely on advanced sensors and\nintensive field data collection.\nThe findings suggest that DALL.E 2's capabilities in image generation hold potential for advancing machine vision and\nrobotic operations in agriculture, contributing to the development of more efficient and accurate AI-driven agricultural\nsystems and accelerate their adoption. Previous studies for image generation in agricultural environments typically\ndepended on labor-intensive and expensive field data collection, often hindering efficiency. However, in this study, we\nshowed an efficient workflow of creating agricultural images using AI that could potentially avoid the reliance on labor-\nintensive and costly field data collection methods in the near future. Our evaluation of the feature similarity between AI-\ngenerated images and real sensor-captured images of crop environments not only validates the practical utility of this\ntechnology but also opens up new possibilities for its application in precision agriculture. This shift towards AI-generated\nimagery could potentially revolutionize the way agricultural studies are conducted, offering a more cost-effective, rapid,\nand versatile method of data collection.\nConclusion and Future Prospects\nIn modern agriculture, the need for comprehensive image datasets is paramount, especially given the limitations of\ntraditional data collection methods, which are often labor-intensive and time-consuming. Synthetic image generation\nemerges as a compelling solution, addressing these challenges by creating realistic and diverse datasets efficiently. The\nutilization of AI-based methods, particularly the DALL.E model developed by OpenAI, exemplifies this approach.\nFunctioning similarly to its counterpart ChatGPT, DALL.E is trained on a vast array of images and textual data, enabling it\nto generate accurate and diverse images from textual descriptions and existing images. The DALL.E model's potential in\nagricultural applications is, therefore, substantial. It offers innovative solutions for critical tasks such as fruit quality\nassessment, automated harvesting, and crop yield estimation. By generating realistic images of various fruit crops,\nDALL.E aids in the development of smart farming techniques. For instance, the model's ability to create images of fruits in\ndifferent growth stages can help in training AI models for precise fruit detection, thus improving crop monitoring and\nharvesting strategies. Additionally, there could potentially come a day soon when we will need no sensors and tedious\nimage collection procedure to perform RGB data acquisition due to the recent advancement of LLMs as DALL.E. This\nstudy conducted a detailed evaluation of the DALL.E model's efficacy in generating agriculturally relevant images,\nfocusing on its ability to replicate and enhance real-world field conditions through synthetic imagery.\nBased on the results, the following specific conclusions can be made from this study for the AI based image generation\nusing DALL.E in six diverse fruit crops in agriculture:\nImage-to-image generation methods resulted in a 5.78% increase in average PSNR, indicating improved image clarity\nand quality over text-to-image generation.\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n17\n/\n22\nHowever, there was a decrease of 10.23% in average FSIM for image-to-image generation, suggesting a reduction in\nstructural and textural similarity to the original images compared to text-to-image generation.\nThis study underscores the potential transformative impact of integrating advanced AI model DALL.E into advancing\nagricultural technologies and solutions. The successful application of this model in generating realistic images for various\nagricultural scenarios opens new opportunities for enhancing agricultural efficiency and improving crop yield and quality in\nthe coming days. By leveraging the capabilities of the generative AI model DALL.E, which is a Large Language Models\n(LLMs), the agricultural sector could see a significant shift in how data is gathered and analyzed. The traditional reliance\non sensors and manual data collection processes, often cumbersome and time-intensive, could be greatly reduced or\neven be completely replaced in the future. Instead, AI-generated images, as demonstrated in this study, could provide a\nmore efficient and scalable alternative. The ability of models like DALL.E to create accurate depictions of diverse\nagricultural environments from different fruits in diverse realistic backgrounds offers new potential for smart and precision\nagricultural practices. In the future, tasks like yield estimation, disease detection, and crop health monitoring could be\nconducted using datasets generated entirely by AI, streamlining the process and increasing its accuracy and adaptability.\nLooking ahead, the trajectory of generative AI in agriculture is set to expand with models like DALL.E already laying the\ngroundwork for synthetic dataset creation. While this study focused on leveraging DALL.E for text-to-image generation,\nfuture advancements could see the inclusion of text-to-video generator models like SORA by OpenAI, potentially offering\neven more powerful and useful applications. The recent ongoing development of AI Model such as SORA text to video\ngenerator could be instrumental in generating dynamic visual content that captures the complexities of agricultural\nenvironments across time, complementing DALL.E's capabilities.\nStatements and Declarations\nAuthor contributions\nR.S. was the principal investigator, responsible for the conceptualization and design of the study, execution of the\nresearch activities, and the performance of formal analysis and data interpretation. The manuscript was drafted and\ncritically revised by both R.S. and M.K., with M.K. serving as the supervisory figure throughout the research process. D.A.\nplayed a supporting role by assisting in the data analysis and offering essential critiques during the manuscript\npreparation phase. All authors have thoroughly reviewed and given their consent to the final manuscript.\nAcknowledgements and Funding\nThe authors wish to express their profound gratitude to the following scholars for their invaluable contribution in assessing\nthe realism of the images evaluated in this study: Dr. Sindhuja Sankaran, Dr. Joan Wu, Dr. Markus Keller, Dr. Safal\nKshetri, Dr. Salik Khanal, Bernardita Veronica, Achyut Paudel, Datta Bhalekar, Shafik Kiraga, Alexander You, Karisma\nYumnam, Elda Yitbarek Bizuayene,, Atif Bilal Asad, Dr. Chenchen Kang, Priyanka Upadhayaya, Martin Churuvija and\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n18\n/\n22\nSyed Usama Bin Sabir.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\n*\nCorrespondence and requests for materials should be addressed to R.S. and/or M.K.\nReferences\n1\n. \n^\nChen, R. J., Lu, M. Y., Chen, T. Y., Williamson, D. F. K. & Mahmood, F. Synthetic data in machine learning for\nmedicine and healthcare. Nat Biomed Eng 5, 493–497 (2021).\n2\n. \n^\nBarrera, K., Merino, A., Molina, A. & Rodellar, J. Automatic generation of artificial images of leukocytes and leukemic\ncells using generative adversarial networks (syntheticcellgan). Comput Methods Programs Biomed 229, 107314\n(2023).\n3\n. \n^\nChoi, I., Park, S. & Park, J. Generating and modifying high resolution fashion model image using StyleGAN. in 2022\n13th International Conference on Information and Communication Technology Convergence (ICTC) 1536–1538 (IEEE,\n2022).\n4\n. \n^\nBermano, A. H. et al. State\n‐\nof\n‐\nthe\n‐\nArt in the Architecture, Methods and Applications of StyleGAN. in Computer\nGraphics Forum vol. 41 591–611 (Wiley Online Library, 2022).\n5\n. \n^\nLuo, C., Wang, Y., Zhang, X., Zhang, W. & Liu, H. Spatial prediction of soil organic matter content using multiyear\nsynthetic images and partitioning algorithms. Catena (Amst) 211, 106023 (2022).\n6\n. \n^\nRio-Torto, I., Campaniço, A. T., Pereira, A., Teixeira, L. F. & Filipe, V. Automatic quality inspection in the automotive\nindustry: a hierarchical approach using simulated data. in 2021 IEEE 8th International Conference on Industrial\nEngineering and Applications (ICIEA) 342–347 (IEEE, 2021).\n7\n. \n^\nSapkota, B. B. et al. Use of synthetic images for training a deep learning model for weed detection and biomass\nestimation in cotton. Sci Rep 12, 19580 (2022).\n8\n. \n^\nMan, K. & Chahl, J. A Review of Synthetic Image Data and Its Use in Computer Vision. J Imaging 8, 310 (2022).\n9\n. \na\n, \nb\nChen, A. et al. Three-dimensional synthetic non-ellipsoidal nuclei volume generation using bezier curves. in 2021\nIEEE 18th International Symposium on Biomedical Imaging (ISBI) 961–965 (IEEE, 2021).\n10\n. \n^\nÁlvarez-Trejo, A., Cuan-Urquizo, E., Roman-Flores, A., Trapaga-Martinez, L. G. & Alvarado-Orozco, J. M. Bézier-\nbased metamaterials: Synthesis, mechanics and additive manufacturing. Mater Des 199, 109412 (2021).\n11\n. \n^\nMildenhall, B., Hedman, P., Martin-Brualla, R., Srinivasan, P. P. & Barron, J. T. Nerf in the dark: High dynamic range\nview synthesis from noisy raw images. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition 16190–16199 (2022).\n12\n. \na\n, \nb\nHodaň, T. et al. Photorealistic image synthesis for object instance detection. in 2019 IEEE international conference\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n19\n/\n22\non image processing (ICIP) 66–70 (IEEE, 2019).\n13\n. \n^\nZhao, Z. & Bao, G. Artistic Style Analysis of Root Carving Visual Image Based on Texture Synthesis. Mobile\nInformation Systems 2022, (2022).\n14\n. \n^\nVelikina, J. V, Alexander, A. L. & Samsonov, A. Accelerating MR parameter mapping using sparsity\n‐\npromoting\nregularization in parametric dimension. Magn Reson Med 70, 1263–1273 (2013).\n15\n. \n^\nAraújo, T., Mendonça, A. M. & Campilho, A. Parametric model fitting-based approach for retinal blood vessel caliber\nestimation in eye fundus images. PLoS One 13, e0194702 (2018).\n16\n. \n^\nDiolatzis, S., Philip, J. & Drettakis, G. Active exploration for neural global illumination of variable scenes. ACM\nTransactions on Graphics (TOG) 41, 1–18 (2022).\n17\n. \na\n, \nb\nZhang, Y. et al. Modeling indirect illumination for inverse rendering. in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition 18643–18652 (2022).\n18\n. \n^\nEversberg, L. & Lambrecht, J. Generating images with physics-based rendering for an industrial object detection task:\nRealism versus domain randomization. Sensors 21, 7901 (2021).\n19\n. \n^\nWu, X., Xu, K. & Hall, P. A survey of image synthesis and editing with generative adversarial networks. Tsinghua Sci\nTechnol 22, 660–674 (2017).\n20\n. \na\n, \nb\n, \nc\n, \nd\nAbbas, A., Jain, S., Gour, M. & Vankudothu, S. Tomato plant disease detection using transfer learning with C-\nGAN synthetic images. Comput Electron Agric 187, 106279 (2021).\n21\n. \n^\nMatuszczyk, D., Tschorn, N. & Weichert, F. Deep Learning Based Synthetic Image Generation for Defect Detection in\nAdditive Manufacturing Industrial Environments. in 2022 7th International Conference on Mechanical Engineering and\nRobotics Research (ICMERR) 209–218 (IEEE, 2022).\n22\n. \n^\nYu, J. et al. Generative image inpainting with contextual attention. in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition 5505–5514 (2018).\n23\n. \n^\nLu, C.-Y., Rustia, D. J. A. & Lin, T.-T. Generative adversarial network based image augmentation for insect pest\nclassification enhancement. IFAC-PapersOnLine 52, 1–5 (2019).\n24\n. \n^\nNazki, H., Lee, J., Yoon, S. & Park, D. S. Image-to-image translation with GAN for synthetic data augmentation in\nplant disease datasets. Smart Media Journal 8, 46–57 (2019).\n25\n. \n^\nLiu, B., Tan, C., Li, S., He, J. & Wang, H. A data augmentation method based on generative adversarial networks for\ngrape leaf disease identification. IEEE Access 8, 102188–102198 (2020).\n26\n. \n^\nDe, S., Bhakta, I., Phadikar, S. & Majumder, K. Agricultural Image Augmentation with Generative Adversarial\nNetworks GANs. in International Conference on Computational Intelligence in Pattern Recognition 335–344 (Springer,\n2022).\n27\n. \na\n, \nb\nGomaa, A. A. & Abd El-Latif, Y. M. Early prediction of plant diseases using cnn and gans. International Journal of\nAdvanced Computer Science and Applications 12, (2021).\n28\n. \na\n, \nb\nMadsen, S. L., Dyrmann, M., Jørgensen, R. N. & Karstoft, H. Generating artificial images of plant seedlings using\ngenerative adversarial networks. Biosyst Eng 187, 147–159 (2019).\n29\n. \na\n, \nb\nZhu, F., He, M. & Zheng, Z. Data augmentation using improved cDCGAN for plant vigor rating. Comput Electron\nAgric 175, 105603 (2020).\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n20\n/\n22\n30\n. \na\n, \nb\n, \nc\nHartley, Z. K. J. & French, A. P. Domain adaptation of synthetic images for wheat head detection. Plants 10,\n2633 (2021).\n31\n. \na\n, \nb\nBird, J. J., Barnes, C. M., Manso, L. J., Ekárt, A. & Faria, D. R. Fruit quality and defect image classification with\nconditional GAN data augmentation. Sci Hortic 293, 110684 (2022).\n32\n. \n^\nShete, S., Srinivasan, S. & Gonsalves, T. A. TasselGAN: An Application of the generative adversarial model for\ncreating field-based maize tassel data. Plant Phenomics (2020).\n33\n. \n^\nGuo, Z. et al. Quality grading of jujubes using composite convolutional neural networks in combination with RGB color\nspace segmentation and deep convolutional generative adversarial networks. J Food Process Eng 44, e13620 (2021).\n34\n. \n^\nDrees, L., Junker-Frohn, L. V., Kierdorf, J. & Roscher, R. Temporal prediction and evaluation of Brassica growth in the\nfield using conditional generative adversarial networks. Comput Electron Agric 190, 106415 (2021).\n35\n. \n^\nKierdorf, J. et al. Behind the leaves: estimation of occluded grapevine berries with conditional generative adversarial\nnetworks. Front Artif Intell 5, 830026 (2022).\n36\n. \n^\nOlatunji, J. R., Redding, G. P., Rowe, C. L. & East, A. R. Reconstruction of kiwifruit fruit geometry using a CGAN\ntrained on a synthetic dataset. Comput Electron Agric 177, 105699 (2020).\n37\n. \n^\nBellocchio, E., Costante, G., Cascianelli, S., Fravolini, M. L. & Valigi, P. Combining domain adaptation and spatial\nconsistency for unseen fruits counting: a quasi-unsupervised approach. IEEE Robot Autom Lett 5, 1079–1086 (2020).\n38\n. \n^\nFawakherji, M., Potena, C., Pretto, A., Bloisi, D. D. & Nardi, D. Multi-spectral image synthesis for crop/weed\nsegmentation in precision farming. Rob Auton Syst 146, 103861 (2021).\n39\n. \n^\nZeng, Q., Ma, X., Cheng, B., Zhou, E. & Pang, W. Gans-based data augmentation for citrus disease severity\ndetection using deep learning. IEEE Access 8, 172882–172891 (2020).\n40\n. \n^\nKim, C., Lee, H. & Jung, H. Fruit tree disease classification system using generative adversarial networks.\nInternational Journal of Electrical and Computer Engineering (IJECE) 11, 2508–2515 (2021).\n41\n. \n^\nTian, Y., Yang, G., Wang, Z., Li, E. & Liang, Z. Detection of apple lesions in orchards based on deep learning\nmethods of cyclegan and yolov3-dense. J Sens 2019, (2019).\n42\n. \n^\nCap, Q. H., Uga, H., Kagiwada, S. & Iyatomi, H. Leafgan: An effective data augmentation method for practical plant\ndisease diagnosis. IEEE Transactions on Automation Science and Engineering 19, 1258–1267 (2020).\n43\n. \n^\nMaqsood, M. H. et al. Super resolution generative adversarial network (Srgans) for wheat stripe rust classification.\nSensors 21, 7903 (2021).\n44\n. \n^\nBi, L. & Hu, G. Improving image-based plant disease classification with generative adversarial network under limited\ntraining set. Front Plant Sci 11, 583438 (2020).\n45\n. \n^\nZhao, Y. et al. Plant disease detection using generated leaves based on DoubleGAN. IEEE/ACM Trans Comput Biol\nBioinform 19, 1817–1826 (2021).\n46\n. \n^\nNerkar, B. & Talbar, S. Cross-dataset learning for performance improvement of leaf disease detection using\nreinforced generative adversarial networks. International Journal of Information Technology 13, 2305–2312 (2021).\n47\n. \n^\nZhao, L., Zheng, K., Zheng, Y., Zhao, D. & Zhou, J. RLEG: vision-language representation learning with diffusion-\nbased embedding generation. in International Conference on Machine Learning 42247–42258 (PMLR, 2023).\n48\n. \na\n, \nb\nSchuhmann, C. et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Adv\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n21\n/\n22\nNeural Inf Process Syst 35, 25278–25294 (2022).\n49\n. \n^\nVan Dis, E. A. M., Bollen, J., Zuidema, W., van Rooij, R. & Bockting, C. L. ChatGPT: five priorities for research. Nature\n614, 224–226 (2023).\n50\n. \n^\nMcLean, S. et al. The risks associated with Artificial General Intelligence: A systematic review. Journal of\nExperimental & Theoretical Artificial Intelligence 35, 649–663 (2023).\n51\n. \n^\nLiebrenz, M., Schleifer, R., Buadze, A., Bhugra, D. & Smith, A. Generating scholarly content with ChatGPT: ethical\nchallenges for medical publishing. Lancet Digit Health 5, e105–e106 (2023).\n52\n. \n^\nLu, Y. & Young, S. A survey of public datasets for computer vision tasks in precision agriculture. Comput Electron\nAgric 178, 105760 (2020).\n53\n. \n^\nDing, M., Zheng, W., Hong, W. & Tang, J. Cogview2: Faster and better text-to-image generation via hierarchical\ntransformers. Adv Neural Inf Process Syst 35, 16890–16902 (2022).\n54\n. \n^\nConde, M. V & Turgutlu, K. CLIP-Art: Contrastive pre-training for fine-grained art classification. in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition 3956–3960 (2021).\n55\n. \n^\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with clip\nlatents. arXiv preprint arXiv:2204.06125 1, 3 (2022).\n56\n. \n^\nZhang, L., Zhang, L., Mou, X. & Zhang, D. FSIM: A feature similarity index for image quality assessment. IEEE\ntransactions on Image Processing 20, 2378–2386 (2011).\nQeios, CC-BY 4.0   ·   Article, \nMarch 25, 2024\nQeios ID: A8DYJ7   ·   https://doi.org/10.32388/A8DYJ7\n22\n/\n22",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.6450932621955872
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5320642590522766
    },
    {
      "name": "Agriculture",
      "score": 0.5239084959030151
    },
    {
      "name": "Generative model",
      "score": 0.4861437976360321
    },
    {
      "name": "Computer science",
      "score": 0.4787944257259369
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4164067804813385
    },
    {
      "name": "Natural language processing",
      "score": 0.3493935465812683
    },
    {
      "name": "Geography",
      "score": 0.24547147750854492
    },
    {
      "name": "Archaeology",
      "score": 0.06796303391456604
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I72951846",
      "name": "Washington State University",
      "country": "US"
    }
  ]
}