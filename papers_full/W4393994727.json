{
  "title": "Language models accurately infer correlations between psychological items and scales from text alone",
  "url": "https://openalex.org/W4393994727",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4226853982",
      "name": "Björn Erik Hommel",
      "affiliations": [
        "Leipzig University",
        null
      ]
    },
    {
      "id": "https://openalex.org/A2276510814",
      "name": "Ruben C. Arslan",
      "affiliations": [
        "Leipzig University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319232666",
    "https://openalex.org/W4309267672",
    "https://openalex.org/W4387607602",
    "https://openalex.org/W4255268143",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4398594135",
    "https://openalex.org/W4377220900",
    "https://openalex.org/W6977497049",
    "https://openalex.org/W2031802938",
    "https://openalex.org/W2073934198",
    "https://openalex.org/W3134934304",
    "https://openalex.org/W2058090646",
    "https://openalex.org/W2804339972",
    "https://openalex.org/W1987179471",
    "https://openalex.org/W2146618936",
    "https://openalex.org/W2105204282",
    "https://openalex.org/W2803268521",
    "https://openalex.org/W4292994367",
    "https://openalex.org/W2913469361",
    "https://openalex.org/W6959016827",
    "https://openalex.org/W2172273046",
    "https://openalex.org/W3215385304",
    "https://openalex.org/W4297851150",
    "https://openalex.org/W2565998027",
    "https://openalex.org/W4288348577",
    "https://openalex.org/W3108041100",
    "https://openalex.org/W4393097430",
    "https://openalex.org/W4226427195",
    "https://openalex.org/W1985936397",
    "https://openalex.org/W2582743722",
    "https://openalex.org/W4394792717",
    "https://openalex.org/W4386497319",
    "https://openalex.org/W2093106172",
    "https://openalex.org/W1996299251",
    "https://openalex.org/W1986393741",
    "https://openalex.org/W2012077067",
    "https://openalex.org/W4387356588",
    "https://openalex.org/W1983968346",
    "https://openalex.org/W1964218847",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W4389395857",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226163610",
    "https://openalex.org/W4250872345",
    "https://openalex.org/W4323315336",
    "https://openalex.org/W2242464395",
    "https://openalex.org/W2979325830",
    "https://openalex.org/W2099465598",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W4231018735",
    "https://openalex.org/W2133896663",
    "https://openalex.org/W4387704935",
    "https://openalex.org/W4389514989",
    "https://openalex.org/W2436336594",
    "https://openalex.org/W3013355453",
    "https://openalex.org/W4253892058",
    "https://openalex.org/W4384559184"
  ],
  "abstract": "Many behavioural scientists do not agree on core constructs and how they should be measured. Different literatures measure related constructs, but the connections are not always obvious to readers and meta-analysts. Many measures in behavioural science are based on agreement with survey items. Because these items are sentences, computerised language models can make connections between disparate measures and constructs and help researchers regain an overview over the rapidly growing, fragmented literature. Our fine-tuned language model, the SurveyBot3000, accurately predicts the correlations between survey items, the reliability of aggregated measurement scales, and intercorrelations between scales from item positions in semantic vector space. In our pilot study, the out-of-sample accuracy for item correlations was .71, .86 for reliabilities, and .89 for scale correlations. In a preregistered study, we will investigate whether the performance of our model generalises to measures across behavioural science.",
  "full_text": "Stage 2 Registered Report (Stage 1 peer reviewed) \nLanguage  models accurately infer \ncorrelations between psychological \nitems and scales from text alone  \n \nBjörn E. Hommel1, 2* and Ruben C. Arslan1 \n1 Wilhelm Wundt Institute of Psychology, Leipzig University, Germany \n2 magnolia psychometrics GmbH, Germany \n \nBjörn E. Hommel  https://orcid.org/0000-0002-7375-006X \nRuben C. Arslan   https://orcid.org/0000-0002-6670-5658 \n* Corresponding author: Björn E. Hommel (bjoern.hommel@uni-leipzig.de)  \nBjörn E. Hommel and Ruben C. Arslan contributed equally to this paper. \n \nOnline supplement: \n• Stage 1 preprint: https://osf.io/preprints/psyarxiv/kjuce_v1 \n• Statistical reports and interactive plots: https://synth-science.github.io/surveybot3000/  \n• OSF (Code & Data): https://osf.io/z47qs/,  \n• App: https://huggingface.co/spaces/magnolia-psychometrics/synthetic-correlations  \n \nAbstract: \nMany behavioural scientists do not agree on core constructs and how they should be \nmeasured. Different literatures measure related constructs, but the connections are not \nalways obvious to readers and meta-analysts. Many measures in behavioural science are \nbased on agreement with survey items. Because these items are sentences, computerised \nlanguage models can make connections between disparate measures and constructs and \nhelp researchers regain an overview over the rapidly growing, fragmented literature. Our \nfine-tuned language model, the SurveyBot3000, accurately predicts the correlations between \nsurvey items, the reliability of aggregated measurement scales, and intercorrelations \nbetween scales from item positions in semantic vector space. We measured the model's \nperformance as the convergence between its synthetic model estimates and empirical \ncoefficients observed in human data. In our pilot study, this out-of-sample accuracy for item \ncorrelations was .71, .89 for reliabilities, and .89 for scale correlations. In our preregistered \nvalidation study using novel items, the out-of-sample accuracy was slightly reduced to .59 for \n\nitem correlations, .84 for reliabilities, and .84 for scale correlations. The synthetic item \ncorrelations showed an average prediction error of .17, with larger errors for middling \ncorrelations. Predictions exhibited generalizability beyond the training data and across \nvarious domains, with some variability in accuracy. Our work shows language models can \nreliably predict psychometric relationships between survey items, enabling researchers to \nevaluate new measures against existing scales, reduce redundancy in measurement, and \nwork towards a more unified behavioural science taxonomy. \n  \nIntroduction \nBehavioural science struggles to be cumulative in part because scientists in many fields fail \nto agree on core constructs (Bainbridge et al., 2022; Sharp et al., 2023). The literature silos, \nwhich consequently develop, can appear unconnected but pursue the same phenomena \nunder different labels (see e.g., grit and conscientiousness; Credé et al., 2017). \nOne reason why connections are lacking is the asymmetry inherent in measure and \nconstruct validation: adding novel constructs to the pile is easier than sorting through it. \nInvestigators can easily invent a new ad-hoc measure and benefit reputationally if a new \nconstruct becomes associated with their name (Elson et al., 2023; Flake & Fried, 2020). By \ncontrast, finding out whether a purported new construct or measure is redundant with the \nthousands of existing ones is cumbersome and can cause conflict with other researchers \n(Bainbridge et al., 2022; Elson et al., 2023). The same holds for replicating construct \nvalidation studies and reporting evidence of overfitting or other problems (Hussey et al., \n2024; Kopalle & Lehmann, 1997). \nUntangling the \"nomological net\"—a term coined by Cronbach and Meehl (1955) to describe \nthe relationships between measures and constructs—has become increasingly difficult given \nthe growing number of published measures (Anvari et al., 2024; Elson et al., 2023). \nConventional construct validation methods, though effective in mapping these relationships, \ndo not scale to, for instance, the thousands of measures that might be related to neuroticism. \nTo tackle this problem, Condon and Revelle (2015; see also Condon, 2017; Condon et al., \n2017) have championed the Synthetic Aperture Personality Assessment in which survey \nparticipants respond to a small random selection of a large set of items from the personality \nliterature. Over time, as the sample size grows, this procedure allows estimating pairwise \ncorrelations between all items. Although the approach is efficient, each new item requires \nthousands of participants to answer the survey before it can be correlated with all existing \nitems. Hence, the approach cannot be used to quickly evaluate new proposed scales. What \nis missing is an efficient way to prioritise, to prune the growth in constructs and measures \nand to sort through the disorganised pile of existing measures. \nNatural language processing could provide this efficiency. In the social and behavioural \nsciences, subjective self-reports are one of the predominant forms of measurement. The \ntextual nature of survey items lends itself to natural language processing. Recently, \ntransformer models have become the state-of-the-art in language models (Vaswani et al., \n2017), displaying proficiency in understanding and generating text. They have dramatically \nreduced the costs of many tasks and chores, notably in programming and generating images \nfrom verbal prompts. Although capabilities for natural language generation are currently \nmore visible in the public eye through the use of chat-like interfaces, they are backed by \ncapabilities in natural language understanding (e.g., classifying or extracting features from \ntext).  \nOn a technical level, this understanding is implemented by the so-called encoder block, \nwhich processes input text and encodes it as a high-dimensional numeric vector. The vector \nrepresentation of a word like “party” in the resulting semantic vector space is context-\ndependent. The same word will yield a different vector representation if it occurs in the \nstatement “I am the life of the party” compared to “I always vote for the same party”. The \nencoder block's ability to contextualise words is crucial for recognizing the nuances of \nlanguage. At heart, the efficiency of the transformer model can largely be attributed to its \nself-attention mechanism (Vaswani et al., 2017). As the name suggests, it is loosely \nanalogous to the executive function in human cognition. Instead of “memorising” an entire \ncorpus of text, word by word, the attention mechanism weights the relevance of words in a \ncontext window for a given target word. \nTransformer models excel in transfer learning, that is, they adapt to new tasks easily (Tunstall \net al., 2022). Following the pre-training stage, which establishes a base level of linguistic \nexpertise, the models can undergo domain adaptation, which involves training the model on \na corpus of text specifically curated for the task at hand. In a process called fine-tuning, the \nmodel then learns to carry out a specific task (e.g., text classification). Fine-tuning often \ninvolves slight architectural adjustments to the model's output layer, although the term is \nused somewhat inconsistently in the literature to describe various adaptation approaches. \nEssentially, the model builds on the fundamental knowledge acquired during pre-training to \nadapt to specialised tasks, even with limited training data. High-quality annotated training \ndata is key for the domain adaptation that turns generalists into specialists. \nUsing linguistic information to scaffold scientific models has a long history in personality \npsychology, where the lexical hypothesis states that more important personality \ncharacteristics are more likely to be encoded as words. To find important personality \ndimensions, researchers had human subjects rate themselves on prominent adjectives, or \nitems, identified systematic correlations between items, and applied factor analytic \ntechniques to reduce the number of dimensions. The most popular organising framework, \nthe Big Five, was distilled from personality-descriptive items in this manner (Digman, 1990). \nPre-transformer era attempts to use semantic features of items to predict associations \nbetween measurement scales using latent semantic analysis have demonstrated moderate \nutility (Arnulf et al., 2014; Larsen & Bong, 2016; Rosenbusch et al., 2020). As the ability of \ncomputerised language models to capture meaning has grown, researchers have sought to \ndirectly quantify relationships between adjectives from textual data (Cutler & Condon, 2022), \nto assign items to constructs (Fyffe et al., 2024; Guenole et al., 2024), to directly predict item \nresponses (Abdurahman et al., 2024; Argyle et al., 2023) and quantify answers to open-\nended questions (Kjell et al., 2019, 2024).  \nWulff & Mata (2023) used large language models (LLMs) to map survey items to vector \nspace and predict empirical item correlations. They tested various transformer models for \ntheir ability to predict properties of psychological inventories. They observed a correlation of \nr = .22 between the semantic similarities of items as judged by OpenAI’s ada-002 model \n(Greene et al., 2022) and the item correlations estimated in empirical data, with accuracy \nimproving when aggregating vectors to the scale level. Their work shows large language \nmodels can approximately infer item correlations and outperform latent semantic analysis. \nHowever, their approach relied on pre-trained models that were not adapted to the domain \nof survey items and do not appreciate that empirical item correlations are often negative \nbecause of negation. This approach cannot be expected to unlock the latent ability of the \nmodels, but rather to give a lower bound of their usefulness. At the same time, pre-trained \nmodels can overfit to their training data. Because OpenAI’s large language models obtain \nknowledge from scraping large quantities of internet text, they presumably have seen items \nfrom existing measures co-occur in online studies and public item repositories (see \nSupplementary Note 11 for details on training data leakage). The results for survey items that \ninadvertently were part of the training data can lead to more optimistic results than could be \nexpected for novel items.  \nWe have adapted a sentence transformer model to the domain of survey response patterns \nand trained our model, the SurveyBot3000, to place items in vector space. The distances \nbetween item pairs in vector space produce what we will call synthetic item correlations, \nscale correlations, and reliabilities. These synthetic estimates can potentially help to cheaply \nevaluate measures and constructs. We have validated that the SurveyBot3000 can \napproximately infer empirical item correlations beyond its training data, by preregistering the \nmodel’s synthetic estimates before collecting empirical data. Based on our pilot study, we \npredicted that the model will exhibit substantial accuracy in inferring empirical item \ncorrelations (r = .71, 95% CI [.70;.72]), and even higher accuracy in inferring latent \ncorrelations between scales (r = .89 [.88;.90]) and in inferring reliability coefficients (r = .89 \n[.84;.94]). We detail our predictions in our Design Table.  \nOur model can be put to work in multiple areas. Synthetic correlations will always require \ncareful follow-up with empirical data, but they can be used to search and prioritise. Authors \ncan use our model as a semantic search engine to find existing constructs and measures \nand avoid reinventions. Synthetic correlations could be used as inputs for more realistic a \npriori power analyses. Scientific reviewers can use it to flag optimistic reliability coefficients \nand unstable factor structures, especially when researchers have not validated an ad-hoc \nmeasure out-of-sample yet. Generally, discrepancies between reported estimates and LLM-\nbased synthetic estimates can motivate greater attention to replication and construct \nvalidation. Finally, meta-scientists and measurement researchers can use the model to start \nsorting through the pile of tens of thousands existing constructs and measures (Anvari et al., \n2024; Elson et al., 2023).  \nAs a showcase, we have made the model available as an app on Huggingface. Researchers \ncan enter item texts and the app will generate synthetic item correlations, scale correlations \nand reliability coefficients. The app contains a prominent cautionary note to discourage \nresearchers from taking the synthetic estimates at face value before further validation has \noccurred. \n \nMethods \nMaterials, data, and code for the present study are available through the Open Science \nFramework: https://osf.io/z47qs/. Data pre-processing, model training, and statistical analyses \nwere conducted using Python (version 3.10.12; Van Rossum & Drake, 2009), R (version \n4.3.1; R Core Team, 2023), with an Nvidia GeForce RTX 2070 Super GPU, using the CUDA \n11.7.1 toolkit (NVIDIA et al., 2022). \nEthics information \nThe planned research complies with the ethics guidelines by the German Society for \nPsychology (Berufsverband Deutscher Psychologinnen und Psychologen, 2022). Data used \nin model training were collected by third parties, as shown in the online supplemental section \n(https://osf.io/z47qs/). Participants in the validation study were recruited from the \ncrowdsourcing platform Prolific, and compensated at a median wage of $12 per hour. \nInformed consent has been obtained from all human respondents. Ethics approval for the \nvalidation study has been granted from the Institutional Review Board (IRB) at Leipzig \nUniversity. All necessary support is in place for the proposed research.  \nPre-trained language model \nOur preliminary work has focused on improving the predictions of item correlations with \nsentence transformer models using high-quality training corpora for domain adaptation. We \nmodified a LLM to generate synthetic item correlations by fine-tuning a pre-trained sentence \ntransformer model (Reimers & Gurevych, 2019). Unlike conventional transformer models \nused in natural language understanding tasks which produce vector representations of \nindividual tokens (i.e., basic linguistic units, such as words or syllables), sentence \ntransformers produce vector representations for longer sequences of text (e.g., sentences).  \nSentence transformers—specifically the bi-encoder architecture used throughout this \nresearch—work by using two parallel LLMs that process text inputs independently but share \nthe same structure and parameters. The central idea behind these models is to capture the \nsemantic essence of a sentence. One method to accomplish this is by pooling (e.g., \naveraging) the contextualised token vectors for each of the two models and then combining \nthem. The underlying neural network then learns these combined representations by \npredicting sentence similarities, for instance using natural language inference data. In natural \nlanguage inference, a given text (i.e., the premise) is evaluated based on its relation to \nanother text (i.e., the hypothesis), classified as either contradicting, entailing, or being neutral \nto it. The network's output layer consists of three neurons, each representing one of these \nclasses. The model's learning effectiveness is assessed using cross-entropy loss, with \nimprovements in sentence vector representation achieved through backpropagation. \nInterested readers are referred to Reimers & Gurevych (2019), as well as Schroff et al. \n(2015) for further details on bi-encoders. Accessible in-depth introductions to transformer \nmodels and deep neural networks can be found in Hussain et al. (2023) and Hommel et al. \n(2022). \nWe chose the all-mpnet-base-v2 model (hereafter referred to as the “SBERT model” for \nfurther fine-tuning from the Hugging Face model hub (Hugging Face Model Hub, n.d.), based \non its commendable performance across 14 benchmark datasets (Pretrained Models — \nSentence-Transformers Documentation, n.d.). This pre-trained model is a sentence-\ntransformer adaptation of the mpnet-base model (Song et al., 2020), initially trained on 160 \ngigabytes of English language text, including Wikipedia, BooksCorpus, OpenWebText, CC-\nNews, and Stories. The SBERT model places sentences in a 768-dimensional semantic \nvector space. Distances in this Euclidean space can be computed using, for instance, cosine \nsimilarity. In our case, we hypothesised that the cosine similarity between the vector \nrepresentations of any two survey items (e.g., personality statements) should correspond to \nthe correlation coefficients obtained from survey data. \n     Domain adaptation and fine-tuning \nWe fine-tuned the pre-trained model in two steps. In the first step, we trained the model \nto distinguish between semantically opposing concepts. In the second step, we trained the \nmodel to predict pairwise item correlations, using survey data. Figure 1 depicts the multi-\nstep training procedure. \nStep 1: Polarity calibration Although cosine similarity spans from -1 to 1, negative \ncoefficients are rarely produced when comparing vector representations of sentences (cf. \nthe croissant shape of the top left plot in Figure 2). This limitation primarily arises because \nthe high-dimensional vector representation of sentences encodes a range of abstract \nlinguistic features, many of which tend to be positively correlated across text sequences. \nThis poses a challenge in accurately predicting correlations for items of opposing scale \npolarities, such as those on the introversion-extraversion continuum. To illustrate, when \nassessing cosine similarity between items from the pre-trained model, the item “I am the life \nof the party” produces comparable coefficients with “I make friends easily” (Θ = .32) and “I \nkeep in the background” (Θ = .35). This occurs even though the last item represents the \npolar opposite of the first item. \nWe fine-tuned the pre-trained model with the goal of maximising the cosine distance \nbetween vector representations of opposing concepts. We achieved this by augmenting the  \nStanford Natural Language Inference corpus (SNLI version 1.0, see also Supplementary \nNote 3; Williams et al., 2018) for our purposes. SNLI comprises around 570,000 sentence \npairs, each labelled for textual entailment as either contradiction, neutral, or entailment. We \nre-labelled each sentence pair by additionally assigning a magnitude to the semantic \nrelationship. We let the pre-trained SBERT model generate the cosine similarity of the \nsentence pair (e.g., “the moon is shining” and “it is a sunny day”, Θ = .46), but assigned a \nnegative direction if the pair was labelled as contradictory (e.g., Θ = -.46). Hence, our new \ncriterion combined the magnitude and direction of the similarity, capturing various forms of \nnegation in the process. The fine-tuned model was then trained to predict this new criterion, \nso that it would learn that similar sentences have negative cosine similarities when one \nsentence negates or contradicts the other (see Supplementary Note 6 for more detailed \nevaluation metrics).  \nStep 2: Domain adaptation We found that the SBERT model's predictions of item \ncorrelations were skewed by the presence of non-trait-related text in the item stems. \nSpecifically, we identified a tendency for item correlations to be overestimated in statements \ncontaining the same adverbs of frequency. For example, the phrase “I often feel blue” from \nthe depression facet of the NEO-PI-R in the IPIP exhibits similar cosine similarity to the two \nitems “I feel that my life lacks direction” (Θ = .28) and “I often forget to put things back in \ntheir proper place” (Θ = .26), even though the first item is also from the depression facet \nwhile the second is from the orderliness facet. \nTo address this, we aimed to fine-tune the model to focus on text segments that convey \ninformation relevant to psychological traits and their similarity. This adjustment aimed to \nenhance the model's accuracy in identifying and processing trait-relevant language and to \nteach it about personality structure, thus improving the validity of its synthetic correlations. \nWe compiled training data from 29 publicly available online repositories (see Supplementary \nNote 4). Our inclusion criteria for the corpus mandated that raw item-level data be available, \na minimum sample size of N ≥ 300, the use of a rating scale as response format, and clear \nmapping of item stems to variable names in the datasets. In pre-processing, we retained \npairwise Pearson coefficients from the lower triangular matrix across all datasets and \ncleaned and standardised item stems. Further details on the preprocessing of data can be \nfound on the OSF (https://osf.io/bfhzy). For cross-validation purposes, we distributed each \nitem pair among training, validation, and test partitions, adhering to an 80-10-10 split. To \navoid overfitting, we ensured that all items were unique to their partition. This led to the \nexclusion of a substantial portion of our training data. Specifically, from the initial pool of \n204,424 item pairs, we retained 90,424 pairs. Of these, we randomly allocated 74,339 pairs \n(82%) to the training partition, 6,832 pairs (8%) to the validation partition, and 9,253 pairs \n(10%) to the test partition. To mitigate the risk of the model learning idiosyncratic \ncharacteristics inherent to the dataset —item stems within a dataset are more likely to exhibit \nresemblance than between datasets— we used an additional holdout dataset. This dataset \ncomprised 87,153 item pairs obtained from Bainbridge et al. (2022)  thereby providing a \nrobust measure for evaluating the model's generalizability to novel English language items \nabout personality and related individual differences. To ensure the integrity of the holdout \ndataset, any items not exclusive to it were eliminated from the training, validation, and test \npartitions. \nWe optimised the hyperparameters for fine-tuning the model using the Optuna library in \nPython (version 3.1.1; Akiba et al., 2019), with a focus on enhancing the model's ability in \npredicting item correlations within the test partition. Details of the final hyperparameter \nselection are available in the online supplemental material (https://osf.io/b5ua7).  \n \nFigure 1. Multi-step training procedure for the SurveyBot3000, which produces synthetic estimates of inter-item correlations.\n\nPilot study \nWe found that the SurveyBot3000 model was highly accurate for all partitions of the \ncurated corpus. Empirical inter-item correlations and synthetic correlations were accurately \npredicted in the test set r = .69 (df = 9,251; 95% CI [.67, .70]) and in the validation set r = .71 \n(df = 6,830; 95% CI [.70, .72]). That accuracy was high in both test and validation set shows \nthe model's strong generalizability within the corpus.  \nThe SurveyBot3000 model was then tested using 87,153 item pairs obtained from \nBainbridge et al. (2022), the holdout dataset we withheld from the training process to avoid \nover-fitting. Adjusted for sampling error in the empirical data (see Supplementary Note 1), \nthe model's synthetic correlations predicted the empirical inter-item correlations with an \naccuracy of r = .71 (95% CI [.70;.72], manifest correlation r = .67 [.67; .68], Figure 2). This \nconsistency with the test-set performance shows the model's ability to generalise beyond the \nidiosyncratic properties of the data seen in training. Figure 2 shows the prediction of item \ncorrelations through semantic similarity, as estimated by the SBERT and SurveyBot3000 \nmodels. The SBERT model had substantially lower accuracy in predicting inter-item \ncorrelations in our holdout (manifest r = .19 [.18;.19]). \nWe further investigated the model’s ability to predict scale reliabilities, which can be \ncalculated from inter-item correlation matrices. Given that scales are typically designed to \nexhibit high internal consistency, we observed limited variability in the internal consistency \nmeasures across the 107 scales and subscales in the holdout dataset. Empirical Cronbach’s \nalpha values had a mean of .75 (SD = .10) and ranged from .35 to .93. When new scales are \ndesigned, reliability varies more widely. We therefore circumvented the problem of restricted \nvariance by randomly sampling items to create 200 additional, varied scales. We found that \nsynthetic reliability estimates were highly accurate at r(307) = .89, 95% CI [.84, .94] (manifest \nr = .89 [.86;.91]. Again, the SBERT model had substantially lower accuracy (manifest r = .38 \n[.28;.48]). Accuracy was lower when we excluded the randomly formed scales (manifest r = \n.63 [.50;.73]), as expected owing to the restricted range in the real scales (SD = .10 \ncompared to SD = .23 in the combined set). \nWe subsequently investigated the model's validity for scale-level predictions using the \nholdout dataset. We averaged the vector representations of all items in each scale and then \ncomputed the cosine similarity of these averaged vectors. The convergence between \nempirical and synthetic scale correlations was remarkably high, exhibiting an accuracy of      \nr(6,245) = .89 [.88, .90] (manifest correlation r = .87 [.86;.87]). In other words, our fine-tuned \nLLM explained 80% of the latent variance in scale intercorrelations, based on nothing but \nsemantic information contained in the items (i.e., adopting the notion of distributional \nsemantics which considers all contextual patterns as inherently semantic). Again, the SBERT \nmodel had substantially lower accuracy (manifest r = .33 [.30;.35]). \nIn summary, the LLM-based synthetic estimates closely approximated the empirical inter-\nitem and inter-scale correlations as well as reliability estimates and were robust to the \nchecks detailed in Supplementary Note 2. Comparing predictions between the datasets used \nin this pilot study leads us to expect that the effects are robust and will generalise to new, \npreviously unseen English-language items. \n \n  \nFigure 2. Scatter plots of the synthetic and empirical estimates, pilot study (Stage 1). \nWe show N=87,153 item pair correlations, N=307 scale reliabilities, and N=6,245 scale pair \ncorrelations for the pre-trained SBERT model (first row) and the fine-tuned SurveyBot3000 \nmodel (second row). The yellow line and shaded yellow region show the prediction and the \n95% prediction interval for the latent outcome according to a Bayesian multi-membership \nregression model that allowed for heteroskedasticity and sampling error. Because the \nempirical estimates are estimated with sampling error, which the model adjusts for, fewer \nthan 95% of dots are in the shaded prediction interval. Brown dots in the middle column \nshow randomly combined scales, which we used to increase variance in the criterion. For \nreliabilities, 18 randomly combined scales with negative synthetic alphas according to the \npre-trained model are not shown for ease of presentation.\n\nDesign \nThe primary objective of our research was to test the generalisability of our model in \npredicting human response patterns within survey data, that is, empirical item and scale \ncorrelations, as well as scale reliabilities. Our model's initial training data and our holdout \nrepresent a limited subset of the broader universe of survey items, with a skew towards \npersonality psychology. We designed our validation study to challenge the model's \ncapabilities by sampling from a more varied array of psychological measures. We have \ncollected empirical data from a large online sample of English-speaking US Americans, \nsimilar to most of the studies in our training data. Participants processed the scales in \nrandom order, with item order randomised in each scale. While we anticipated a modest \nreduction in effect size during Stage 2 compared to the outcomes observed in the pilot \nstudy, we expected that the LLM-based synthetic estimates would still be sufficiently \naccurate to be useful. We present a Design Table summarising our methods and \nbenchmarks. \nMeasures \nTo identify appropriate measures for our study, we conducted a comprehensive search \nof the APA PsycTests database. Our inclusion criteria for selecting scales were: a) utilisation \nof rating scales as the response format, b) items composed in the English language, c) \nscales developed within the last 30 years to minimise confounding factors related to changes \nin the English language, d) measures applicable to the general population, thus excluding \nscales only applicable to narrow target demographics such as adoptive parents or particular \nprofessional groups, e) measures applicable to a broad domain, thus excluding scales \ndesigned to rate specific consumer products or specific social attitudes, and f) freely \naccessible, non-proprietary measures. These criteria were mainly intended to make it \nfeasible to have an unselected sample respond to most items.  Within these constraints, we \nsampled scales to cover a wide range of measures used in the social and behavioural \nsciences. \nWe did not always use all items in a scale, so that we would be able to have participants \nrespond to a large number in a scale. We included measures from industrial/organisational \npsychology, such as the Utrecht Work Engagement scale, measures from social psychology \nsuch as the Moral Foundations Questionnaire, from developmental psychology, such as the \nRevised Adult Attachment Scale, from clinical psychology, such as the Center for \nEpidemiological Studies Depression Scale, from emotion psychology, such as the positive \nand negative affect schedule, from personality psychology, such as Honesty-Humility in the \nHEXACO-60, and from other social sciences, such as the Attitudes Toward AI in Defence \nScale and the Survey Attitude Scale. A full list of all scales can be found in Supplementary \nNote 5 and all items were deposited on OSF. In all, we aimed to have participants answer \n246 items distributed across 79 scales and subscales. \nWhere possible, we adapted the response format to a 6-point Likert scale from strongly \ndisagree to strongly agree. For the PANAS, CES-D, and the PSS, we used a 6-point scale \nfrom “never” to “most of the time” to better fit the item content. Our guiding principle was \nthat a more uniform presentation was more important than a perfectly faithful rendering of \nthe original scale. In addition, our current model is unaware of differing response formats and \ncannot account for them. \nSampling Plan \nWe used simulations to determine our number of scales, items, and survey participants. \nWe wanted to precisely estimate the accuracy with which our synthetic estimates could \napproximate empirical estimates of inter-item and inter-scale correlations. Sampling error at \nthe participant level affects the standard error with which we estimate empirical inter-item \nand inter-scale correlations and therefore would bias our accuracy estimates downward. To \nestimate empirical individual item correlations, we planned to use an online panel provider to \ncollect a US quota sample of N = 450, before exclusions. In a quota sample, the panel \nprovider attempts to approximately match the sample proportions to population proportions \non three demographic variables: age, sex, and ethnicity. We had planned to limit participant \nrecruitment to participants who have an approval rate exceeding 99% and have participated \nin at least 20 previous studies according to the sample provider, Prolific. However, this \nscreener could not be combined with a quota sample, so no such limits were applied during \nrecruitment. We paid participants regardless of whether they failed attention checks or \ncompleted the survey too quickly. In our planned analyses, we then estimated the accuracy \nof our manifest synthetic estimates for latent, error-free empirical estimates (see \nSupplementary Note 1). \nFrom the APA PsycTests corpus, we sampled 246 items, which can be aggregated to 56 \nscales consisting of at least three items. We assumed we would retain a sample of at least n \n= 400 after exclusions. With the resulting 30,135 unique item pairs, we expected to infer the \naccuracy of our synthetic inter-item correlations to a precision (standard error) of ±0.004, \naccording to our simulations. Supplementing our 57 scales with 200 randomly constituted \nscales, enabled us to infer the accuracy of our synthetic reliability estimates to a precision of \n±0.03. With the resulting 1,568 unique scale pairs, without scale-subscale pairs, we aimed to \ninfer the accuracy of our synthetic inter-scale correlations to a precision of ±0.007. The \nachieved precision is sufficient to detect even subtle deterioration in accuracy compared to \nour pilot study estimates. \nAnalysis Plan \nWe followed recommendations by Goldammer et al. (2020) and Yentes (2020) for \nidentifying and excluding participants exhibiting problematic response patterns (e.g., \ncareless responding). Accordingly, participants were be excluded if any of the following \nconditions were met: a) participants voluntarily indicated that they did not respond seriously, \nb) multivariate outlier statistic using Mahalanobis distance, exceeding a threshold set for 99% \nspecificity), c) psychometric synonyms (defined as item pairs with r > .60) correlate below r = \n.22 for the participant), d) psychometric antonyms (defined as item pairs with r ≤ -.40) \ncorrelate above r = -.03, e) low personal even-odd-index across scales (r <=.45) f) average \nresponse times below 2 seconds per item. We checked the robustness of our conclusions to \ndifferently defined exclusion criteria.  \nWe then computed all empirical inter-item correlations, inter-scale correlations, and \nreliabilities. Inter-item correlations used Pearson's product-moment correlations. We \naggregated scales as the means of their items after reversing reverse-coded items. Inter-\nscale correlations were then computed as manifest Pearson's product-moment correlations. \nReliability was estimated with the Cronbach's alpha coefficient based on inter-item \ncorrelation. We have uploaded synthetic estimates of the SBERT model and the \nSurveyBot3000 model for all of these coefficients to the OSF. The code for our preregistered \nanalyses mirrored the code from our pilot study, including the robustness checks detailed in \nSupplementary Note 2. We planned to freeze both code and point predictions as part of our \npreregistration, but owing to a miscommunication between the two co-authors, nobody froze \nthe repository and only point predictions for item correlations were uploaded to OSF. \nBecause we discovered typographical errors in our version of the Moral Foundation \nQuestionnaire, we revised the related point predictions after Stage 1 acceptance. After data \ncollection, we merged empirical and synthetic estimates.  \nThe central performance metric in this study is accuracy, defined as the convergence \nbetween synthetic and empirical estimates (not to be conflated with evaluation metrics of \nbinary classifiers). We thus refer to manifest accuracy as the Pearson correlation between \nsynthetic and empirical coefficients. We quantified latent accuracy using two complementary \napproaches that account for sampling error in empirical estimates. First, we used a structural \nequation modeling (SEM) approach where we fixed the residual variance of empirical \nestimates to the average sampling error variance and allowed manifest synthetic estimates to \ncorrelate with the latent variable. Second, we disattenuated for the standard error of the \nempirical estimates using a Bayesian errors-in-variables model, which allows for \nheteroskedastic accuracy (see Supplementary Note 1). We used the latter model as our \nprimary estimate for latent accuracy. We also report the prediction error for all three \nquantities, as well as a plot similar to Figure 2. We furthermore report manifest and latent \naccuracies of the SBERT model, which we used as a benchmark (see Design Table). \n  \nTable 1. Design Table \n \nQuestion Hypothesis Sampling plan Analysis Plan Interpretation given to \ndifferent outcomes \nHow \naccurate are \nLLM-based \nsynthetic \ninter-item \ncorrelations?  \nThe synthetic \nestimates will exhibit \nan accuracy of r = .71 \nfor the empirical inter-\nitem correlation \ncoefficients obtained \nfrom survey data, as \nestimated in our \nBayesian multi-\nmembership \nregression model. \n246 items. With \nthe resulting \n30,135 unique \nitem pairs, we \nshould be able \nto estimate \naccuracy with a \nprecision of \n±0.004.  \nA quota sample \nof N=400 will be \ndrawn to \nestimate \nempirical \ncorrelations. \nA correlation \nbetween \nsynthetic and \nempirical \nestimates, \ndisattenuated for \nthe sampling \nerror in the \nempirical \nestimates. \n \nIf the accuracy matches (i.e. \n±.02) that found in our pilot \nstudy, this is evidence that the \nmodel generalises well to \nnovel survey items, including \nthose outside personality \npsychology. \nIn the unlikely case that the \naccuracy exceeds that found \nin our pilot study, we would \ncarefully discuss why, \nincluding the potential that \ncrowdworkers use LLMs to \nrespond. \nIf the accuracy deteriorates to \nwithin 60% of the r in the pilot, \nthe model may still be useful \nbut should be applied with \ncaution when item content is \nunlike the training data. We \nwill examine and discuss \nperformance across subfields \nto understand the \ndeterioration. Retraining the \nmodel on a broader corpus \nwould be indicated for future \nresearch. \nIf the accuracy deteriorates to \nbelow 60% of the r in the pilot, \nour model does not generalise \nwell. Retraining with a broader \ncorpus would be needed \nbefore recommending the \nmodel for wider use. \nIf the accuracy of our model is \nreduced below the accuracy \nof the pre-trained model, our \nmodel training procedure \noverfit despite our \nprecautions. The model \nshould not be recommended \nfor practical use and we would \nreinvestigate our precautions. \nHow \naccurate are \nLLM-based \nsynthetic \nreliability \ncoefficients \n(for scales \nconsisting of \nat least three \nitems)?  \nThe synthetic \nestimates will exhibit \nan accuracy of r = .86 \nfor the empirical \nCronbach's alpha \ncoefficients obtained \nfrom survey data, as \nestimated in our \nBayesian regression \nmodel. \nAs above. With \nthe available 57 \nscales, \nsupplemented \nby 200 randomly \nformed scales, \nwe should be \nable to estimate \naccuracy with a \nprecision of \n±0.02. \nHow \naccurate are \nLLM-based \nsynthetic \ninter-scale \ncorrelations \n(for scales \nconsisting of \nat least three \nitems)?  \nThe synthetic \nestimates will exhibit \nan accuracy of r = .89 \nfor the empirical inter-\nscale correlation \ncoefficients obtained \nfrom survey data, as \nestimated in our \nBayesian multi-\nmembership \nregression model. \nAs above. With \nthe resulting \n1,558 scale \npairs, we should \nbe able to \nestimate \naccuracy with a \nprecision of \n±0.007. \n  \nNote. We determined the planned precision to detect any deterioration in performance \ngreater than .01 for item pair correlations. Because increasing the number of scales is \ncostlier than increasing the number of items, the sensitivity for the reliability coefficients is a \ncompromise with feasibility. \nResults \nWe collected data from N=470 participants using Prolific’s online participant recruitment \nsystem. Because a bug in our questionnaire disrupted participation for an initial batch of \nparticipants who later returned to the study, we exceeded our planned sample size of 450 \n(see Supplementary Note 7 on deviations from preregistration). \nWe preregistered overly strict exclusion criteria because we misread Goldammer et al. \n(2020). After applying the preregistered criteria, only n=136 participants remained. \nTherefore, we used an adapted set of criteria that more closely followed Goldammer et al.’s \n(2020) recommendations for our main analyses, so that n=387 remained (see Table S7 in \nsupplemental section). However, results for item pair correlations were robust to different \nexclusion criteria definitions as well as including all participants (see Supplementary Note 9). \nAfter applying the adapted exclusion criteria, the remaining participants had a mean age of \n46.96 (SD = 15.58, range 18-86) and were 47% male. Most (63%) participants identified as \nnon-Hispanic White, 13% as Black, and 12% as Hispanic. Four participants reported no high \nschool education, 46 had a high school degree, 80 had some college experience, and 257 \nreported three or more years of college experience. Further and more detailed demographic \ninformation can be found in the online codebook. \nAll participants responded to a set of 219 items. Twenty-eight percent of the sample \n(n=110) were unemployed (or students etc.). Participants who reported being employed \nanswered an additional set of 27 items specific to employment. We calculated pairwise \nPearson's product-moment correlations between all item pairs in this set. We tested the \naccuracy of the preregistered SurveyBot3000 synthetic correlations against the empirical \ncorrelations of the resulting 30,135 item pairs.  \nItem pair correlations: Adjusted for sampling error in the empirical data (see \nSupplementary Note 1), the model's synthetic correlations predicted the empirical inter-item \ncorrelations with an accuracy of r = .59 (95% CI [.58;.60], manifest correlation r = .57 \n[.56;.58], Figure 3). Accuracy deteriorated compared to the holdout in our pilot study (to 83% \nof the r = .71 in the pilot), but our model was still able to generalise to this diverse set of \nitems. Figure 3 shows the prediction of item correlations through semantic similarity, as \nestimated by the SBERT and SurveyBot3000 models. The SBERT model had substantially \nlower accuracy in predicting inter-item correlations (accuracy of = .33 [.32;.34]). We also \ncomputed the prediction error of the SurveyBot3000 in our model, i.e. how far off predictions \nwere after accounting for sampling error in the empirical correlations in our model. The \naverage root mean square error (RMSE) was .17 [.17;.17]. However, prediction error was \nlarger when synthetic correlations were middling (.00 to .60) and smaller when they were \nnegative or larger than .60, see Figure 4. \nFigure 3. Scatter plots of the synthetic and empirical estimates, validation study (Stage \n2). Showing N=30,135 item pair correlations, N=257 scale reliabilities, and N=1,568 scale \npair correlations for the pre-trained SBERT model (first row) and the fine-tuned \nSurveyBot3000 model (second row).  \n \n \nFigure 4. Prediction error of the synthetic estimates, validation study (Stage 2).   \nOur prediction model allowed the error term to vary freely according to the predictor, the \nsynthetic estimate. The thin-plate splines show that some synthetic estimates were \npredictably more accurate.\n\n \nScale reliabilities: We investigated the model’s ability to predict scale reliabilities \n(Cronbach’s alpha), which can be calculated from inter-item correlation matrices. For the 57 \nscales at least three, the manifest accuracy of the synthetic alpha coefficients was .64 \n[.45;.77]. This accuracy was slightly reduced compared to the pilot (94% of r = .68). Because \nall scales from the literature had restricted variability in reliability coefficients, we randomly \nsampled items to create 200 additional, varied scales. Unlike in the pilot, we reversed items \nrandomly (not according to empirical correlations) and did not omit scales whose empirical \nCronbach's alpha estimate was negative (see Table S7). We chose to make these changes to \nclarify that the synthetic alphas are in fact unbiased when we do not select on positive \nempirical alphas. We found that synthetic reliability estimates were highly accurate at r(257) \n= .84, 95% CI [.79, .90] (manifest r = .85 [.81;.88]. The SBERT model had lower accuracy \nthan the SurveyBot3000 but performed much better than in the pilot study (manifest r = .64 \n[.56;.71]).  The average root mean square error of the SurveyBot3000 estimates (RMSE) was \n.27 [.21;.33]. However, prediction error dropped below .10 when synthetic alphas entered the \nrange seen in the real scales (above .60). \nScale pair correlations: We investigated the model's validity for scale-level predictions. \nFor all scales with at least three items, we averaged the vector representations of all items \n(after reversing reverse-scored items) and then computed the cosine similarity of these \naveraged vectors. The accuracy of synthetic scale correlations was r(1,568) = .84 [.82, .87] \n(excluding scale-subscale pairs; manifest correlation r = .83 [.81, .85] our fine-tuned LLM \nexplained 71% of the latent variance in scale intercorrelations, based on nothing but \nsemantic information contained in the items. Manifest accuracy for the 228 scale pairs where \neach scale had at least five items was r = .88). Performance was slightly attenuated \ncompared to the pilot (94% of r = .89), but this may be partly because scales in this set were \nslightly shorter (mean number of items = 5.75) than in the pilot (6.79), see also \nSupplementary Note 8. As for synthetic reliabilities, the SBERT model had lower accuracy \nthan the SurveyBot3000 but performed much better than in the pilot study (manifest r = .50 \n[.46;.54]). The average root mean square error of the SurveyBot3000 estimates (RMSE) was \n.16 [.15;.17]. As for item correlations, prediction error was larger for middling synthetic \nestimates (.00 to .50) than for negative and high positive estimates (Figure 4).  \nBy domain \nWe investigated the accuracy of our synthetic inter-item correlations by domain. We had \ngrouped scales into five domains (attitudes, personality, clinical, social, and occupational \npsychology). Manifest accuracy was lowest for attitudes (r = .34 within the attitude domain, r \n= .31 when attitude items were correlated with items in other domains) and highest for \noccupational psychology (r = .75 within, r = .65 across). In all domains, the SurveyBot3000 \npredictions outperformed the SBERT predictions, so there was no obvious trade-off between \nfine-tuning and generalisability (see Figure 5).  \n \n \nFigure 5: Accuracy by domain. Accuracy differed across domains. SurveyBot3000 \naccuracy (colored) was always higher than SBERT accuracy (gray). Results were largely \nconsistent whether accuracy of items was tested within domains (left, circle) or across \ndomains (right, cross).  \nRobustness checks \nWe repeated all robustness checks we conducted for the pilot study and added \nadditional checks. Because we had preregistered overly strict exclusion criteria and as we \nwere unable to combine quota sampling with a screener for highly rated Prolific participants, \nwe estimated the accuracy of the synthetic item correlations after applying different sets of \ndefensible exclusion criteria. After accounting for sampling error, accuracy varied between \n.57 and .59 depending on the exclusion criteria, i.e. not substantially (Figure 6, see also \nSupplementary Note 9). We report further robustness checks and sensitivity analyses in \nSupplementary Note 8. \n\n \nFigure 6. Applying different exclusion criteria (or none) did not cause large changes in \nthe estimated latent accuracy (see Supplementary Note 7). Predictably, manifest accuracy \nwas reduced when we excluded many participants. \n  \n\nDiscussion \nWe introduce a computational linguistics approach that synthetically predicts \nassociations between survey responses—including item-level correlations, scale-level \nrelationships, and derived psychometric properties—with high accuracy. Using our \nSurveyBot3000, these synthetic estimates have a margin of error that is comparable to a \nsmall pilot study, but free and instant. Our preregistered validation study confirms the \nconvergence between synthetic predictions and empirical datasets, validating the method’s \nability to mirror real-world reliability coefficients, scale correlations, and covariance patterns, \neven outside the content domain of personality psychology.  \nAccuracy in our preregistered validation was attenuated compared to our pilot study (up \nto 83% of the pilot study's accuracy for item pairs) but never to the level of the pre-trained \nmodel. So, even though the items spanned a broader domain, the synthetic estimates had \nmargins of error comparable to a small pilot study. Attenuation was strongest for item pairs (r \n= .59 [.58;.60]). After aggregation, accuracy was higher for scale pairs (latent r = .84 [.82;.87]) \nand for reliabilities (r = .84 [.79;.90]; attenuation to 94% of the pilot study's accuracy). Our \nprediction model allowed for the margin of error to depend on the synthetic estimate. \nIndeed, because the SurveyBot3000 still sometimes predicts positive correlations instead of \nnegative correlations, negative synthetic estimates are more accurate (see Figure 4). For \ninstance, a negative synthetic scale correlation is estimated about as accurately as in a N=80 \npilot study, whereas a positive correlation is only about as accurately estimated as a N=20 \npilot study (see Supplementary Note 10). The margin of error was also larger for synthetic \nreliabilities below commonly used cutoffs, i.e. < .60). \nRecent related contributions on computational modeling for survey research (e.g. \nHernandez & Nie, 2023; Schoenegger et al., 2024; Wulff & Mata, 2023), highlight the field’s \ngrowing interest in synthetic prediction of psychometric patterns. In a recent update to their \nwork, Wulff & Mata (2025) have adopted fine-tuning techniques that improve upon their \nearlier results, yielding accuracies that approach the performance we report here, but limited \nto absolute correlations. In another parallel effort, Schoenegger et al. (2025) report \ncomparable performance of the proprietary model PersonalityMap and the SurveyBot3000. \nHowever, this comparison is difficult to interpret because the SurveyBot3000 was trained on \nthe data used as the test set and the PersonalityMap model is proprietary, which makes it \ndifficult to assess leakage and generalizability.  \nOur work advances this area of synthetic survey modeling not mainly by reporting top-\ntier accuracy but through methodological innovations and practical tools designed to \nimprove the rigor, transparency, generalizability, and accessibility. \nFirst, we introduce a two-step training protocol that refines sentence transformer models \nfor robust prediction of survey response associations. Key safeguards include training on a \ndiverse item corpus to minimize domain bias, strict contamination controls to prevent \noverfitting, and systematic hyperparameter optimization. A novel calibration step further \nenables the model to predict negative correlations (e.g., opposing items), more accurately \nreflecting the empirical distribution of coefficients.  The resulting model, the SurveyBot3000, \ndemonstrates performance exceeding known human capabilities in correlation judgment \n(Epstein & Teraspulsky, 1986).  \nSecond, to ensure transparency and minimize analytic flexibility, we preregistered our \nvalidation protocol and underwent formal Stage 1 peer review prior to testing. This \nsafeguards against overfitting and confirms that accuracy claims are not artifacts of post hoc \nadjustments. \nThird, we systematically evaluate generalizability across psychological domains, \nincluding personality, clinical, and social psychology, as well as social attitudes. While item-\nlevel accuracy varies with conceptual diversity—attenuated in cross-domain tests compared \nto our pilot study—the SurveyBot3000 always outperformed the pre-trained baseline model \n(i.e. SBERT), so our fine-tuning did not impede generalisability. \nFinally, we provide an open-access web application \n(https://huggingface.co/spaces/magnolia-psychometrics/synthetic-correlations ) to \ndemocratize access to synthetic psychometric predictions. The tool generates immediate \nestimates of internal consistency, scale structure, and inter-item correlations from text inputs, \noffering researchers a free pretesting resource with guidance for responsible interpretation. \nThe application can be considered a free pilot study of survey items to investigate factor \nstructure and internal consistency. Similar to pilot studies, synthetic estimates can tell us \n“where to look” but should always be followed up with more empirical data before \nconclusions are drawn. \nAs the behavioural sciences grapple with an ever-expanding universe of oftentimes \nredundant measures, our line of research has the potential to re-organise the vast collection \nof scales accumulated over the past decades of research and to help prevent further \nproliferation and fragmentation in the future (Elson et al., 2023; Anvari et al., 2024; Anvari et \nal., in press). Rosenbusch et al. (2020) laid important groundwork on computational \nlanguage-based methods to semantically search for psychometric scales, but were \nconstrained by the technological limitations of their time. Our results and work on the \nSurveyBot3000 encourages us that the technological foundation for such an ambitious \nundertaking has matured.  \nThe APA PsycTests database currently holds over 78,000 records, with the majority of \nscales only being used once or twice (Elson et al., 2023; Anvari et al., 2024). With both the \nmethodology and the data in place, we propose that future research efforts should be \ndedicated towards the development of a semantic search engine. Searching such a \n“synthetic nomological net” could reveal potential overlap between tens of thousands of \nitems and scales and ultimately help us avoid redundancy and confusing labels. A more \nparsimonious ontology could then enable better evidence synthesis. A semantic search \nengine could be a tool in the scale development and the peer-review process, in order to \nhelp authors and reviewers to assess the incremental value of newly developed scales and \nproposed constructs. Potential redundancies and confusing labels (e.g., jingle/jangle \nfallacies; Wulff & Mata, 2023) could then be flagged for empirical follow-up. Importantly, such \na system would make the search problem tractable. That is, the SurveyBot3000 could help \npick scales out of the ten thousands in existence to empirically evaluate the novel scale for \ndiscriminant validity. That way, humans remain in the loop. We believe that this line of work \nexemplifies a responsible integration of LLMs into research, which is a topic of current \ndebate (Binz et al., 2023). Specifically, the collaborative circumstances in scale development \ncarry minimal risk for harmful effects on the scientific ecosystem. False negatives (i.e., the \nmodel fails to detect redundant scales) would merely maintain the status quo, which has led \nto construct proliferation in the first place. False positives (e.g., the model incorrectly flags \ntwo measures as redundant) would require researchers to verify this empirically before \ndrawing conclusions. This balanced approach, where LLMs accelerates discovery while \nhuman researchers retain interpretive authority, should characterize a productive human-AI \ncollaboration across the social and behavioural sciences. \nTo further strengthen the potential of computer linguistic approaches to survey pattern \nprediction, we noted some limitations in the SurveyBot3000 that need to be addressed by \nfuture research. Despite the strong convergence between synthetic and empirical data in \nboth the pilot and validation study, the SurveyBot3000 occasionally struggled to infer \nnegative correlations. \nWhile polarity calibration clearly improved the model's handling of negatively worded \nitems overall (see Supplementary Note 6), the synthetic estimates still had a bias towards \npositive signs. Of the empirical correlations, 59% were positive, whereas 67% of the \nsynthetic correlations were. In keeping with this, a negative synthetic item correlation \npredicted the empirical sign incorrectly slightly less often (16%) than a positive synthetic \nitem correlation (19%). If we imagine that a human user of our app can correct the coefficient \nsign in these small-scale applications, this would improve manifest accuracy by .11, yielding \nan overall convergence of .68 between synthetic estimates and empirical correlations. \nVarious linguistic aspects were associated with impaired predictions, but no clear pattern \nemerged. For example, items that avoided self-directed language were predicted less \naccurately. However, for such items, we did not observe any increase in accuracy after \nrephrasing the statements to use first-person pronouns (see Supplementary Note 8). In the \ncurrent study, item length, self-directedness, sentence complexity and content domain are all \nconfounded with one another. Further efforts could be directed towards systematically \nmanipulating and investigating lexicographic (e.g., grammatical form, item length) and item-\nmetric (e.g., observability, temporality; Leistner et al., 2024; Leising et al., 2014) features \npotentially influencing accuracy in survey pattern prediction independently of content \ndomain (Hommel, 2024). \nBoth the sign prediction errors and accuracy fluctuations arising from unconventional \nlinguistic aspects could potentially be addressed by recent innovations. For example, Opitz & \nFrank (2022) have shown that vector representations of text can be decomposed into \nexplainable semantic features. Instead of comparing vectors monolithically, future \napproaches could isolate psychometrically relevant information by separating residual \nfeatures in vector space. This decomposition approach may help establish theoretical upper \nbounds on prediction accuracy by distinguishing between different types of semantic content \ncaptured in vector space, including conceptual meaning, but also peripheral semantic \ninformation such as survey response tendencies. \nBeyond these technical refinements, model performance could be enhanced through a \nmore balanced training corpus, as suggested by domain-specific variations in predictive \naccuracy. For instance, synthetic estimates for clinical psychology measures performed \nworse than for social psychology measures, reflecting the limited representation of \npsychopathology items in our training data. Balancing the corpus aligns with established \nprinciples of language model development where capabilities consistently improve with \nincreased training data, model size, and computational resources (Kaplan et al., 2020). \nHowever, the same note of caution as above applies, because content domain is confounded \nwith lexicographic and item-metric aspects. In addition, the low accuracy of synthetic \nestimates in the attitude domain can be partly attributed to the fact that attitude items have \nlower absolute intercorrelations on average, so there is less variance to explain. On the root \nmean square error metric of accuracy, which does not have this issue, attitude items had \nmiddling accuracy compared to other domains. \nRobust evaluation protocols are essential to systematically assess and compare the \ncapabilities and limitations of current and future model developments. To this end, \nbenchmark tests are usually established for specific tasks related to language modelling \nusing infrastructure providers like Hugging Face (Hugging Face Datasets, n.d.) and Kaggle \n(Kaggle Datasets, n.d.). We recommend that efforts should be undertaken to develop such a \nstandardized holdout set to objectively track future progress in survey pattern prediction with \ncomparable accuracy metrics. Although many currently available fine-tuned models are \ntrained on the same or overlapping data (chiefly SAPA, Condon, Roney, & Revelle, 2017), it \nis currently difficult to compare models because teams divide training and test partitions \ndifferently, i.e. one model is trained on the data that another team uses as its benchmark. For \nfair comparisons, we need transparency about the contents of the training data, including for \nproprietary models, or ways to come up with guaranteed novel items. \nOur final report deviates from our pre-planned Stage 1 protocol in several ways. We \ntransparently communicated these deviations according to Willroth and Atherton (2024) in \nSupplementary Note 7 and reported additional robustness checks to study the impact of \nthese deviations on our results. We found that latent accuracy was largely unaffected after \nreadjusting exclusion criteria and generally conclude that the deviations had little impact. \nSentence transformers can effectively model psychometric properties and relationships \nusing solely the semantic information contained within item texts. Our work establishes a \nmethod that produces synthetic predictions which converge with empirical survey data and \ndemonstrate robust generalization beyond the training domain. We see many potential \napplications, simplified through the web app we have released. The SurveyBot3000's \nsynthetic estimates have a margin of error comparable to a small pilot study. As with pilot \nstudies, the synthetic estimates can guide an investigation but need to be followed up by \nhuman researchers with human data. By making synthetic estimates freely available, we \nhope to reduce ad hoc measurement culture. Researchers should now find it easier to \ncompare existing measures, and to identify old and new measures with desirable \npsychometric properties.  \nLooking ahead, incorporating recent advances in computational linguistics may yield \nincreasingly precise models that could serve as foundational tools for untangling the \nnomological net (Cronbach & Meehl, 1955) and constructing a unified taxonomy of \npsychological measures. \n  \nData availability \nWe have shared all key materials on the Open Science Framework at https://osf.io/z47qs/. \nThe existing data used for training and in the pilot study has been openly shared, we link to \nthe original sources. Anonymized data for the validation study have also been shared on \nOSF. \nCode availability \nWe have shared the training and analysis code on the Open Science Framework at \nhttps://osf.io/z47qs/. \nAcknowledgements \nThe research is funded by the German Research Foundation grant #464488178 to Ruben C. \nArslan. The funders had no role in study design, data collection and analysis, decision to \npublish or preparation of the manuscript. We thank Stefan Schmukle, Anne Scheel, Julia \nRohrer, Malte Elson, Taym Alsalti, Ian Hussey, Saloni Dattani, David Condon, Dirk Wulff and \nJan Arnulf for helpful discussions. We also thank Jan-Paul Ries, Lorenz Oehler, and Sarah \nLennartz for comments on an earlier version of this manuscript. \nAuthor contributions \nB.E.H.: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, \nResources, Software, Visualization, Writing - original draft, and Writing - review & editing. \nR.C.A.: Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, \nValidation, Visualization, Writing - original draft, and Writing - review & editing. \nCompeting interests \nBjörn E. Hommel is affiliated with magnolia psychometrics GmbH, a private consulting \nagency which has agreed to maintain the app accompanying this paper, currently hosted on \nHugging Face. There are no competing interests, financial or otherwise, related to this \nresearch. The SurveyBot3000 model is freely licensed under Apache 2.0. Potential future \ncommercial applications of these findings may be developed. \n \n  \n  \nReferences \nAbdurahman, S., Vu, H., Zou, W., Ungar, L., & Bhatia, S. (2024). A deep learning approach to \npersonality assessment: Generalizing across items and expanding the reach of survey-based \nresearch. Journal of Personality and Social Psychology, 126(2), 312–331. \nhttps://doi.org/10.1037/pspp0000480 \nAkiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A Next-generation \nHyperparameter Optimization Framework (No. arXiv:1907.10902). arXiv. \nhttps://doi.org/10.48550/arXiv.1907.10902 \nAnvari, F., Alsalti, T., Oehler, L., Hussey, I., Elson, M., & Arslan, R. C. (2024). A fragmented field: \nConstruct and measure proliferation in psychology. https://doi.org/10.31234/osf.io/b4muj \nArgyle, L. P., Busby, E. C., Fulda, N., Gubler, J., Rytting, C., & Wingate, D. (2023). Out of One, Many: \nUsing Language Models to Simulate Human Samples. Political Analysis, 31(3), 337–351. \nhttps://doi.org/10.1017/pan.2023.2 \nArnulf, J. K., Larsen, K. R., Martinsen, Ø. L., & Bong, C. H. (2014). Predicting Survey Responses: How \nand Why Semantics Shape Survey Statistics on Organizational Behaviour. PLoS ONE, 9(9), \ne106361. https://doi.org/10.1371/journal.pone.0106361 \nBainbridge, T. F., Ludeke, S. G., & Smillie, L. D. (2022). Evaluating the Big Five as an organizing \nframework for commonly used psychological trait scales. Journal of Personality and Social \nPsychology, 122(4), 749–777. https://doi.org/10.1037/pspp0000395 \nBerufsverband Deutscher Psychologinnen und Psychologen. (2022). Ethische Richtlinien der \nDeutschen Gesellschaft für Psychologie e. V. und des Berufsverbandes Deutscher \nPsychologinnen und Psychologen e. V.[Ethical guidelines of the German Society for \nPsychology and the Professional Association of German Psychologists]. DGPs. \nhttps://www.dgps.de/die-dgps/aufgaben-und-ziele/berufsethische-richtlinien/ \nBinz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C. T., Allen, C., Schad, D., Wulff, D., West, J. D., \nZhang, Q., Shiffrin, R. M., Gershman, S. J., Popov, V., Bender, E. M., Marelli, M., Botvinick, M. \nM., Akata, Z., & Schulz, E. (2023). How should the advent of large language models affect the \npractice of science? (No. arXiv:2312.03759). arXiv. \nhttps://doi.org/10.48550/arXiv.2312.03759 \nCondon, D. M. (2017). The SAPA Personality Inventory: An empirically-derived, hierarchically-\norganized self-report personality assessment model. PsyArXiv. \nhttps://doi.org/10.31234/osf.io/sc4p9 \nCondon, D. M., & Revelle, W. (2015). Selected Personality Data from the SAPA-Project: On the \nStructure of Phrased Self-Report Items. Journal of Open Psychology Data, 3. \nhttps://doi.org/10.5334/jopd.al \nCondon, D. M., Roney, E., & Revelle, W. (2017). A SAPA Project Update: On the Structure of phrased \nSelf-Report Personality Items. Journal of Open Psychology Data, 5(1), Article 1. \nhttps://doi.org/10.5334/jopd.32 \nCredé, M., Tynan, M. C., & Harms, P. D. (2017). Much ado about grit: A meta-analytic synthesis of the \ngrit literature. Journal of Personality and Social Psychology, 113(3), 492–511. \nhttps://doi.org/10.1037/pspp0000102 \nCronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, \n52(4), 281–302. https://doi.org/10.1037/h0040957 \nCutler, A., & Condon, D. M. (2022). Deep lexical hypothesis: Identifying personality structure in \nnatural language. Journal of Personality and Social Psychology. \nhttps://doi.org/10.1037/pspp0000443 \nDigman, J. M. (1990). Personality Structure: Emergence of the Five-Factor Model. Annual Review of \nPsychology, 41(1), 417–440. https://doi.org/10.1146/annurev.ps.41.020190.002221 \nElson, M., Hussey, I., Alsalti, T., & Arslan, R. C. (2023). Psychological measures aren’t toothbrushes. \nCommunications Psychology, 1(1), Article 1. https://doi.org/10.1038/s44271-023-00026-9 \nEpstein, S., & Teraspulsky, L. (1986). Perception of cross-situational consistency. Journal of \nPersonality and Social Psychology, 50(6), 1152. https://doi.org/10.1037/0022-\n3514.50.6.1152 \nFlake, J. K., & Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement \nPractices and How to Avoid Them. Advances in Methods and Practices in Psychological \nScience, 3(4), 456–465. https://doi.org/10.1177/2515245920952393 \nFyffe, S., Lee, P., & Kaplan, S. (2024). “Transforming” Personality Scale Development: Illustrating the \nPotential of State-of-the-Art Natural Language Processing. Organizational Research \nMethods, 27(2), 265–300. https://doi.org/10.1177/10944281231155771 \nGreene, R., Sanders, T., Weng, L., & Neelakantan, A. (2022). New  and  improved  embedding  model. \nOpen AI Blog. https://openai.com/blog/new-and-improved-embedding-model \nGuenole, N., D’Urso, E. D., Samo, A., & Sun, T. (2024). Pseudo Factor Analysis of Language \nEmbedding Similarity Matrices: New Ways to Model Latent Constructs. OSF. \nhttps://doi.org/10.31234/osf.io/vf3se \nHernandez, I., & Nie, W. (2023). The AI-IP: Minimizing the guesswork of personality scale item \ndevelopment through artificial intelligence. Personnel Psychology, 76(4), 1011–1035. \nhttps://doi.org/10.1111/peps.12543 \nHommel, B. E. (2024). The advent of transformer models in psychometrics [PhD Thesis, lmu]. \nhttps://edoc.ub.uni-muenchen.de/33252/ \nHommel, B. E., Wollang, F.-J. M., Kotova, V., Zacher, H., & Schmukle, S. C. (2022). Transformer-Based \nDeep Neural Language Modeling for Construct-Specific Automatic Item Generation. \nPsychometrika, 87(2), 749–772. https://doi.org/10.1007/s11336-021-09823-9 \nHugging Face Datasets (n.d.). (2025, January 24). https://huggingface.co/docs/datasets/en/index \nHugging Face model hub. (n.d.). Retrieved June 2, 2023, from https://huggingface.co/sentence-\ntransformers/all-mpnet-base-v2 \nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2023). A tutorial on open-source large language \nmodels for behavioral science. PsyArXiv. https://osf.io/preprints/psyarxiv/f7stn \nHussey, I., Alsalti, T., Bosco, F., Elson, M., & Arslan, R. C. (2024). An aberrant abundance of \nCronbach’s alpha values at .70. https://doi.org/10.31234/osf.io/dm8xn \nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., \n& Amodei, D. (2020). Scaling Laws for Neural Language Models. \nhttps://doi.org/10.48550/ARXIV.2001.08361 \nKjell, O. N. E., Kjell, K., Garcia, D., & Sikström, S. (2019). Semantic measures: Using natural language \nprocessing to measure, differentiate, and describe psychological constructs. Psychological \nMethods, 24(1), 92. \nKjell, O. N. E., Kjell, K., & Schwartz, H. A. (2024). Beyond rating scales: With targeted evaluation, large \nlanguage models are poised for psychological assessment. Psychiatry Research, 333, 115667. \nhttps://doi.org/10.1016/j.psychres.2023.115667 \nKopalle, P. K., & Lehmann, D. R. (1997). Alpha Inflation? The Impact of Eliminating Scale Items on \nCronbach’s Alpha. Organizational Behavior and Human Decision Processes, 70(3), 189–197. \nhttps://doi.org/10.1006/obhd.1997.2702 \nLarsen, K. R., & Bong, C. H. (2016). A Tool for Addressing Construct Identity in Literature Reviews and \nMeta-Analyses. MIS Quarterly, 40(3), 529–551. \nhttps://doi.org/10.25300/MISQ/2016/40.3.01 \nLeising, D., Scharloth, J., Lohse, O., & Wood, D. (2014). What Types of Terms Do People Use When \nDescribing an Individual’s Personality? Psychological Science, 25(9), 1787–1794. \nhttps://doi.org/10.1177/0956797614541285 \nLeistner, M., Hommel, B. E., Wendt, L. P., & Leising, D. (2024). Properties of Person Descriptors in the \nNatural German Language: A Preregistered Replication and Extension. PsyArXiv. \nhttps://doi.org/10.31234/osf.io/s8h7u \nNVIDIA, Vingelmann, P., & Fitzek, F. H. P. (2022). CUDA, release: 11.7.1. \nhttps://developer.nvidia.com/cuda-toolkit \nOpitz, J., & Frank, A. (2022). SBERT studies Meaning Representations: Decomposing Sentence \nEmbeddings into Explainable Semantic Features (No. arXiv:2206.07023). arXiv. \nhttps://doi.org/10.48550/arXiv.2206.07023 \nPretrained Models—Sentence-Transformers documentation. (n.d.). Retrieved March 5, 2024, from \nhttps://www.sbert.net/docs/pretrained_models.html \nR Core Team. (2023). R: A Language and Environment for Statistical Computing (Version 4.3.0) \n[Computer software]. R Foundation for Statistical Computing. https://www.R-project.org/ \nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-\nNetworks (No. arXiv:1908.10084). arXiv. https://doi.org/10.48550/arXiv.1908.10084 \nRosenbusch, H., Wanders, F., & Pit, I. L. (2020). The Semantic Scale Network: An online tool to detect \nsemantic overlap of psychological scales and prevent scale redundancies. Psychological \nMethods, 25(3), 380–392. https://doi.org/10/gg5rn7 \nSchoenegger, P., Greenberg, S., Grishin, A., Lewis, J., & Caviola, L. (2024). Can AI Understand Human \nPersonality? -- Comparing Human Experts and AI Systems at Predicting Personality \nCorrelations (No. arXiv:2406.08170). arXiv. https://doi.org/10.48550/arXiv.2406.08170 \nSchroff, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A unified embedding for face recognition \nand clustering. Proceedings of the IEEE Conference on Computer Vision and Pattern \nRecognition, 815–823. https://www.cv-\nfoundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CV\nPR_paper.html \nSharp, C., Kaplan, R. M., & Strauman, T. J. (2023). The Use of Ontologies to Accelerate the Behavioral \nSciences: Promises and Challenges. Current Directions in Psychological Science, 32(5), 418–\n426. https://doi.org/10.1177/09637214231183917 \nSong, K., Tan, X., Qin, T., Lu, J., & Liu, T.-Y. (2020). MPNet: Masked and Permuted Pre-training for \nLanguage Understanding (No. arXiv:2004.09297). arXiv. \nhttps://doi.org/10.48550/arXiv.2004.09297 \nTunstall, L., Werra, L. von, Wolf, T., & Géron, A. (2022). Natural language processing with \nTransformers: Building language applications with Hugging Face (First edition). O’Reilly. \nVan Rossum, G., & Drake, F. L. (2009). Python 3 Reference Manual. CreateSpace. \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \\Lukasz, & \nPolosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing \nSystems, 5998–6008. https://arxiv.org/abs/1706.03762 \nWilliams, A., Nangia, N., & Bowman, S. R. (2018). A Broad-Coverage Challenge Corpus for Sentence \nUnderstanding through Inference (No. arXiv:1704.05426). arXiv. \nhttps://doi.org/10.48550/arXiv.1704.05426 \nWillroth, E. C., & Atherton, O. E. (in press). Best Laid Plans: A Guide to Reporting Preregistration \nDeviations. Advances in Methods and Practices in Psychological Science. \nhttps://doi.org/10.31234/osf.io/dwx69 \nWulff, D. U., & Mata, R. (2023). Automated jingle–jangle detection: Using embeddings to tackle \ntaxonomic incommensurability. PsyArXiv. https://doi.org/10.31234/osf.io/9h7aw \nWulff, D. U., & Mata, R. (2025). Semantic embeddings reveal and address taxonomic \nincommensurability in psychological measurement. Nature Human Behaviour, 1–11. \nhttps://doi.org/10.1038/s41562-024-02089-y \n  \n ",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.5608770847320557
    },
    {
      "name": "Psychology",
      "score": 0.5500456690788269
    },
    {
      "name": "Linguistics",
      "score": 0.41002732515335083
    },
    {
      "name": "Computer science",
      "score": 0.37509220838546753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3594557046890259
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3461792469024658
    },
    {
      "name": "Philosophy",
      "score": 0.1038365364074707
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I926574661",
      "name": "Leipzig University",
      "country": "DE"
    }
  ]
}