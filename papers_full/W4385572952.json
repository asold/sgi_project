{
  "title": "Interpreting Language Models with Contrastive Explanations",
  "url": "https://openalex.org/W4385572952",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3015007644",
      "name": "Kayo Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2181523240",
    "https://openalex.org/W3110300144",
    "https://openalex.org/W3187467055",
    "https://openalex.org/W3176751053",
    "https://openalex.org/W2346578521",
    "https://openalex.org/W4233901344",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3138819813",
    "https://openalex.org/W1902674502",
    "https://openalex.org/W2899448251",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W4231827019",
    "https://openalex.org/W3162404768",
    "https://openalex.org/W3035371891",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2970863760",
    "https://openalex.org/W3174659183",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W3125997628",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2966924523",
    "https://openalex.org/W4385565605",
    "https://openalex.org/W4206294441"
  ],
  "abstract": "Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics.Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 184–198\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nInterpreting Language Models with Contrastive Explanations\nKayo Yin∗\nUniversity of California, Berkeley\nkayoyin@berkeley.edu\nGraham Neubig\nCarnegie Mellon University\ngneubig@cs.cmu.edu\nAbstract\nModel interpretability methods are often used\nto explain NLP model decisions on tasks such\nas text classiﬁcation, where the output space\nis relatively small. However, when applied to\nlanguage generation, where the output space\noften consists of tens of thousands of tokens,\nthese methods are unable to provide informa-\ntive explanations. Language models must con-\nsider various features to predict a token, such\nas its part of speech, number, tense, or seman-\ntics. Existing explanation methods conﬂate ev-\nidence for all these features into a single expla-\nnation, which is less interpretable for human\nunderstanding.\nTo disentangle the different decisions in lan-\nguage modeling, we focus on explaining lan-\nguage models contrastively: we look for\nsalient input tokens that explain why the model\npredicted one token instead of another. We\ndemonstrate that contrastive explanations are\nquantiﬁably better than non-contrastive expla-\nnations in verifying major grammatical phe-\nnomena, and that they signiﬁcantly improve\ncontrastive model simulatability for human ob-\nservers. We also identify groups of contrastive\ndecisions where the model uses similar evi-\ndence, and we are able to characterize what in-\nput tokens models use during various language\ngeneration decisions.1\n1 Introduction\nDespite their success across a wide swath of natural\nlanguage processing (NLP) tasks, neural language\nmodels (LMs) are often used as black boxes: how\nthey make certain predictions remains obscure (Be-\nlinkov and Glass, 2019). This is in part due to the\nhigh complexity of the LM task itself, as well as\nthat of the model architectures used to solve it.\nWe argue that this is also due to the fact that inter-\npretability methods commonly used in NLP, such\n∗ ∗Work done while at Carnegie Mellon University.\n1Code and demo: https://github.com/kayoyin/interpret-lm.\nInput:Can you stop the dog from\nOutput:barking\n1. Why did the model predict “barking”?\nCanyoustopthedogfrom\n2. Why did the model predict “barking”instead of“crying”?\nCanyoustopthedogfrom\n3. Why did the model predict “barking”instead of“walking”?\nCanyoustopthedogfrom\nTable 1: Explanations for the GPT-2 prediction given\nthe input “Can you stop the dog from _____\". Input to-\nkens that are measured to raise or lower the probability\nof “barking” are in red and blue respectively, and those\nwith little inﬂuence are in white. Non-contrastive ex-\nplanations such as gradient×input (1) usually attribute\nthe highest saliency to the token immediately preceding\nthe prediction. Contrastive explanations (2, 3) give a\nmore ﬁne-grained and informative explanation on why\nthe model predicted one token over another.\nas gradient-based saliency maps (Li et al., 2016a;\nSundararajan et al., 2017), are not as informative\nfor LM predictions compared to other tasks like\ntext classiﬁcation. For example, to explain why an\nLM predicts “barking” given “Can you stop the\ndog from ____”, we demonstrate in experiments\nthat the input token preceding the prediction is of-\nten marked as the most inﬂuential token to the pre-\ndiction (Table 1) by instance attribution methods.\nThe preceding token is indeed highly important to\ndetermine certain features of the next token, ruling\nout words that would obviously violate syntax in\nthat context (e.g. non “-ing” verbs in the given\nexample). However, this does not explain why the\nmodel made other more subtle decisions, such as\nwhy it predicts “barking” instead of “crying” or\n“walking”, which are all plausible choices if we only\nlook at the preceding token. In general, language\nmodeling has a large output space and a high com-\nplexity compared to other NLP tasks; at each time\nstep, the LM chooses one word out of all vocabu-\nlary items, and several linguistic distinctions come\ninto play for each language model decision.\n184\nTo better explain LM decisions, we propose inter-\npreting LMs with contrastive explanations (Lipton,\n1990). Contrastive explanations aim to identify\ncausal factors that lead the model to produce one\noutput instead of another output. We believe that\ncontrastive explanations are especially useful to\nhandle the complexity and the large output space\nof language modeling. In Table 1, the second ex-\nplanation suggests that the input word “dog” makes\n“barking” more likely than a verb not typical for\ndogs such as “crying”, and the third explanation\nsuggests that the input word “stop” increases the\nlikelihood of “barking” over a verb without nega-\ntive connotations such as “walking”.\nIn this paper, we ﬁrst extend three interpretabil-\nity methods to compute contrastive explanations\n(§3). We then perform a battery of experiments\naimed at examining to what extent these contrastive\nexplanations are superior to their non-contrastive\ncounterparts from various perspectives:\n• RQ1: Are contrastive explanations better at\nidentifying evidence that we believe, a-priori,\nto be useful to capture a variety of linguistic\nphenomena (§4)?\n• RQ2: Do contrastive explanations allow hu-\nman observers to better simulate language\nmodel behavior (§5)?\n• RQ3: Are different types of evidence neces-\nsary to disambiguate different types of words,\nand does the evidence needed reﬂect (or un-\ncover) coherent linguistic concepts (§6)?\n2 Background\n2.1 Model Explanation\nOur work focuses on model explanations that com-\nmunicate why a model made a certain prediction.\nParticularly, we focus on methods that compute\nsaliency scores S(xi) over input features xi to re-\nveal which input tokens are most relevant for a\nprediction: the higher the saliency score, the more\nxi supposedly contributed to the model output.\nDespite a large body of literature examining\ninput feature explanations for NLP models on\ntasks such as text classiﬁcation (for a complete\nreview see Belinkov and Glass (2019); Madsen\net al. (2021)), or interpreting how language mod-\nels use linguistic features such as syntax (Ravfogel\net al., 2021; Finlayson et al., 2021), few works\nattempt to explain language modeling predictions\n(Wallace et al., 2019). Despite the importance of\nboth language models and interpretability in the\nNLP literature, the relative paucity of work in this\narea may be somewhat surprising, and we posit that\nthis may be due to the large output space of lan-\nguage models necessitating the use of techniques\nsuch as contrastive explanations, which we detail\nfurther below.\n2.2 Contrastive Explanations\nContrastive explanations attempt to explain why\ngiven an input x the model predicts a target yt\ninstead of a foil yf . Relatedly, counterfactual\nexplanations explore how to modify the input x so\nthat the model more likely predicts yf instead of yt\n(McGill and Klein, 1993).\nWhile contrastive and counterfactual explana-\ntions have been explored to interpret model deci-\nsions (see Stepin et al. (2021) for a broad survey),\nthey are relatively new to NLP and have not yet\nbeen studied to explain language models.\nRecently, Jacovi et al. (2021) produce counter-\nfactual explanations for text classiﬁcation mod-\nels by erasing certain features from the input and\nprojecting the input representation to the “con-\ntrastive space” that minimally separates two de-\ncision classes. Then, they compare model probabil-\nities before and after the intervention.\nWe, on the other hand, propose contrastive ex-\nplanations for language modeling, where both the\nnumber of input factors and the output space are\nmuch larger. While we also use a counterfactual ap-\nproach with erasure (§3.3), counterfactual methods\nmay become intractable over long input sequences\nand a large foil space. We, therefore, also propose\ncontrastive explanations using gradient-based meth-\nods (§3.1,§3.2) that measure the saliency of input\ntokens for a contrastive model decision.\n3 Contrastive Explanations for\nLanguage Models\nIn this section, we describe how we extend three\nexisting input saliency methods to the contrastive\nsetting. These methods can also be easily adapted\nto tasks beyond language modeling, such as ma-\nchine translation (Appendix A).\n3.1 Gradient Norm\nSimonyan et al. (2013); Li et al. (2016a) calculate\nsaliency scores based on the norm of the gradient of\nthe model prediction, such as the output logit, with\n185\nrespect to the input. Applying this method to LMs\nentails ﬁrst calculating the gradient as follows:\ng(xi) =∇xi q(yt|x)\nwhere x is the input sequence embedding, yt is\nthe next token in the input sequence, q(yt|x) is the\nmodel output for the token yt given the input x.\nThen, we obtain the saliency score for the input\ntoken xi by taking the L1 norm:\nSGN (xi) =||g(xi)||L1\nWe extend this method to the Contrastive Gra-\ndient Norm deﬁned by:\ng∗(xi) =∇xi (q(yt|x) −q(yf |x))\nS∗\nGN (xi) =||g∗(xi)||L1\nwhere q(yf |x) is the model output for foil yf given\nthe input x. This tells us how much an input token\nxi inﬂuences the model to increase the probability\nof yt while decreasing the probability of yf .\n3.2 Gradient ×Input\nFor the gradient ×input method (Shrikumar et al.,\n2016; Denil et al., 2014), instead of taking the L1\nnorm of the gradient, we take the dot product of\nthe gradient with the input token embedding xi:\nSGI(xi) =g(xi) ·xi\nWe deﬁne the Contrastive Gradient ×Input:\nS∗\nGI(xi) =g∗(xi) ·xi\n3.3 Input Erasure\nErasure-based methods measure how erasing cer-\ntain parts of the input affects the output (Li et al.,\n2016b). This can be measured as the difference\nbetween the model output given the input x and\ngiven the input where xi has been zeroed out, x¬i:\nSE(xi) =q(yt|x) −q(yt|x¬i)\nWe deﬁne the Contrastive Input Erasure:\nS∗\nE(xi) =\n(q(yt|x) −q(yt|x¬i)) −(q(yf |x) −q(yf |x¬i))\nThis measures how much erasing xi from the input\nmakes the foil more likely and the target less likely.\nAlthough erasure-based methods directly mea-\nsure the change in the output due to a perturbation\nin the input, while gradient-based methods approx-\nimate this measurement, erasure is usually more\ncomputationally expensive due to having to run the\nmodel on all possible input perturbations.\n4 Do Contrastive Explanations Identify\nLinguistically Appropriate Evidence?\nFirst, we ask whether contrastive explanations are\nquantiﬁably better than non-contrastive explana-\ntions in identifying evidence that we believe a pri-\nori should be important to the LM decision. In\norder to do so, we develop a methodology in which\nwe specify certain types of evidence that indicate\nhow to make particular types of linguistic distinc-\ntions, and measure how well each variety of expla-\nnation method uncovers this speciﬁed evidence.\n4.1 Linguistic Phenomena\nAs a source of linguistic phenomena to study, we\nuse the BLiMP dataset (Warstadt et al., 2020). This\ndataset contains 67 sets of 1,000 pairs of minimally\ndifferent English sentences that contrast in gram-\nmatical acceptability. An example of a linguis-\ntic paradigm may be anaphor number agreement,\nwhere an acceptable sentence is “Many teenagers\nwere helping themselves.” and a minimally con-\ntrastive unacceptable sentence is “Many teenagers\nwere helping herself.” because in the latter, the\nnumber of the reﬂexive pronoun does not agree\nwith its antecedent.\nFrom this dataset, we chose 12 paradigms be-\nlonging to 5 phenomena and created a set of rules\nto identify the input tokens that enforce grammat-\nical acceptability. In the previous example, the\nanaphor agreement is enforced by the antecedent\n“teenagers”. We show examples for each linguistic\nphenomenon and its associated rule in Table 2.\nAnaphor Agreement. The gender and number\nof a pronoun must agree with its antecedent. We\nimplement the coref rule using spaCy (Honnibal\nand Montani, 2017) and NeuralCoref2 to extract all\ninput tokens coreferent with the target token.\nArgument Structure. Certain arguments can\nonly appear with certain verbs. For example, many\naction verbs must be used with animate objects.\nWe implement the main_verb rule using spaCy to\nextract the main verb of the input sentence.\nDeterminer-Noun Agreement. Demonstrative\ndeterminers and the associated noun must agree.\nWe implement thedet_noun rule by generating the\ndependency tree using spaCy and extracting the\ndeterminer of the target noun.\n2https://github.com/huggingface/neuralcoref\n186\nPhenomenon Acceptable Example Unacceptable Example Rule\nAnaphor Agreement Katherine can’t helpherself. Katherine can’t helphimself. coref\nManyteenagers were helpingthemselves. Many teenagers were helpingherself. coref\nArgument Structure Amanda wasrespected by somewaitresses. Amanda wasrespected by somepicture. main_verb\nDeterminer-Noun AgreementPhillip was liftingthismouse. Phillip was lifting thismice. det_noun\nTracy praisesthose luckyguys. Tracy praises those luckyguy. det_noun\nNPI Licensing Even these trucks haveoftenslowed. Even these trucks haveeverslowed. npi\nSubject-Verb AgreementAsketchoflightsdoesn’tappear. Asketchoflightsdon’tappear. subj_verb\nTable 2: Examples of BLiMP minimal pairs. Contrastive tokens are bolded. Tokens extracted by our rules that\nenforce grammatical acceptability are underlined.\nNPI Licensing. Certain negative polarity items\n(NPI) are only allowed to appear in certain con-\ntexts, e.g. “never” appears on its own, while “ever”\ngenerally must be preceded by “not”. In all of our\nexamples with NPI licensing, the word “even” is\nan NPI that can appear in the acceptable example\nbut not in the unacceptable example, so we create\nthe npi rule that extracts this NPI.\nSubject-Verb Agreement. The number of the\nsubject and its verb must agree. We implement\nthe subj_verb rule by generating the dependency\ntree using spaCy to extract the subject of the verb.\n4.2 Alignment Metrics\nWe use three metrics to quantify the alignment\nbetween an explanation and the known evidence\nenforcing a linguistic paradigm. The explanation is\na vector S of the same size as the input x, where\nthe i-th element Si gives the saliency score of the\ninput token xi. The known evidence is represented\nwith a binary vector E, also of same size as the\ninput x, where Ei = 1if the token xi enforces a\ngrammatical rule on the model decision.\nDot Product. The dot product S ·E measures\nthe sum of saliency scores of all input tokens that\nare part of the known evidence.\nProbes Needed (Zhong et al., 2019; Yin et al.,\n2021b). We measure the number of tokens we\nneed to probe, based on the explanation S, to ﬁnd\na token that is in the known evidence. This corre-\nsponds to the ranking of the ﬁrst token xi such that\nGi = 1after sorting tokens by descending saliency.\nMean Reciprocal Rank (MRR). We calculate\nthe average of the inverse of the rank of the ﬁrst\ntoken that is part of the known evidence if the to-\nkens are sorted in descending saliency. This also\ncorresponds to the average of the inverse of the\nprobes needed for each sentence evaluated.\nDot Product and Probes Needed calculate align-\nment for each sentence, and we compute the aver-\nage over all sentence-wise alignment scores for the\nalignment score over a linguistic paradigm. MRR\ncalculates alignment over an entire paradigm.\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n0.35 0.35\n0.50\n0.25\n0.15\n-0.21\n0.27\nGPT-2\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n0.36 0.36\n0.51\n-0.04-0.01\n0.30 0.32\nGPT-Neo\n(a) Dot Product (↑)\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n0.0\n0.5\n1.0\n1.5\n2.0\n1.57 1.52 1.5 1.47\n1.26 1.32 1.18\nGPT-2\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n1.56 1.50 1.48 1.58 1.57\n1.17\n0.92\nGPT-Neo\n(b) Probes Needed (↓)\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n0.0\n0.2\n0.4\n0.6\n0.8\n0.58 0.57 0.60 0.58\n0.64 0.61 0.65\nGPT-2\nRand SGN S *\nGN SGI S *\nGI SE S *\nE\n0.59 0.59 0.61 0.57 0.59 0.64\n0.71\nGPT-Neo\n(c) Mean Reciprocal Rank (↑)\nFigure 1: Alignment of GPT-2 (left) and GPT-Neo\n(right) explanations to known evidence according to dot\nproduct (top), probes needed (middle), mean reciprocal\nrank (bottom) averaged over linguistic paradigms.\n4.3 Results\nWe use GPT-2 (Radford et al., 2019) and GPT-Neo\n(Black et al., 2021) to extract explanations. GPT-\n2 is a large autoregressive transformer-based LM\nwith 1.5 billion parameters and trained on 8 mil-\nlion web pages. GPT-Neo is a similar LM with 2.7\nbillion parameters and trained on The Pile (Gao\net al., 2020) containing 825.18GB of largely En-\n187\nglish text. In addition to the explanation methods\ndescribed above, we also set up a random baseline\nas a comparison, where we create a vector of the\nsame size as explanations with values randomly\nsampled from a uniform distribution over [0, 1).\nIn Figure 1, we can see that overall, contrastive\nexplanations have a higher alignment with linguis-\ntic paradigms than their non-contrastive counter-\nparts for both GPT-2 and GPT-Neo across the differ-\nent metrics. Although non-contrastive explanations\ndo not always outperform the random baseline, con-\ntrastive explanations have a better alignment with\nBLiMP than random vectors for most cases.\nCorrect Incorrect\nDP (↑) PN (↓) MRR (↑) DP (↑) PN (↓) MRR (↑)\nRand 0.34 1.66 0.57 0.27 2.05 0.50\nSGN 0.36 1.45 0.58 0.37 1.60 0.56\nS∗GN 0.50 1.33 0.61 0.48 1.71 0.57\nSGI 0.26 1.44 0.59 0.24 1.72 0.55\nS∗GI 0.36 1.25 0.64 -0.05 1.27 0.64\nSE -0.51 1.34 0.64 0.44 1.30 0.55\nS∗E 0.29 1.13 0.68 0.18 1.71 0.55\nTable 3: Alignment of GPT-2 explanations to known\nevidence on examples where the model makes a correct\n(left) and incorrect (right) prediction, according to dot\nproduct (DP), probes needed (PN), and mean reciprocal\nrank (MRR). Alignment scores that are better than the\nscore for the analogous explanation method with the\ndifferent contrastive setting are bolded.\nIn Table 3, we further examined alignment be-\ntween model explanations and known evidence\non instances where the model correctly allocates\nmore probability to the acceptable token, or incor-\nrectly selects the other token. On examples where\nthe model makes an incorrect prediction, it is not\nclear whether non-contrastive or contrastive meth-\nods have better alignment. On examples where the\nmodel predicts correctly, contrastive explanations\nobtain better alignment than their non-contrastive\ncounterparts for each explanation method and align-\nment metric.\nIn Figure 2, we see that for most explanation\nmethods, the larger the distance between the known\nevidence and the target token, the larger the in-\ncrease in alignment of contrastive explanations over\nnon-contrastive explanations. This suggests that\ncontrastive explanations particularly outperform\nnon-contrastive ones when the known evidence is\nrelatively further away from the target token, that is,\ncontrastive explanations can better capture model\ndecisions requiring longer-range context.\nIn Appendix B, we also provide a table with the\n0.50\n 0.25\n 0.00 0.25\nDifference in MRR\n1\n2\n3\n4Distance\nGPT-2\nGI: r=0.84\nGN: r=0.73\nE: r=0.62\n0.50\n 0.25\n 0.00 0.25\nDifference in MRR\n1\n2\n3\n4Distance\nGPT-Neo\nGI: r=0.56\nGN: r=0.41\nE: r=-0.51\nFigure 2: Scatter plot of the average distance of the\nknown evidence to the target token across each linguis-\ntic paradigm against the difference in MRR scores be-\ntween the contrastive and non-contrastive versions of\neach explanation method, with the Pearson correlation\nfor each explanation method. Statistically signiﬁcant\nPearson’s r values (p< 0.05) are inbold. In most cases,\nthere is a positive correlation between the increase in\nMRR and the distance of the evidence.\nfull alignment scores for each paradigm, explana-\ntion method, metric and model.\n5 Do Contrastive Explanations Help\nUsers Predict LM Behavior?\nTo further evaluate the quality of different explana-\ntion methods, we next describe methodology and\nexperiments to measure to what extent explana-\ntions can improve the ability of users to predict the\noutput of the model, namely model simulatability\n(Lipton, 2018; Doshi-Velez and Kim, 2017).\n5.1 Study Setup\nOur user study is similar in principle to previous\nworks that measure model simulatability given dif-\nferent explanations (Chandrasekaran et al., 2018;\nHase and Bansal, 2020; Pruthi et al., 2020). In\nour study (Figure 3), users are given the input of\na GPT-2 model, two choices for the next token,\nand an explanation for the model output. They are\nasked to select which of the two choices is more\nlikely the model output, then answer whether the\nexplanation was useful in making their decision3.\nWe compare the effect of having no explanation,\nexplanations with Gradient ×Input, Contrastive\nGradient ×Input, Erasure and Contrastive Era-\nsure. We do not include Gradient Norm and Con-\ntrastive Gradient Norm because these methods do\n3Although Hase and Bansal (2020) suggest not showing\nexplanations for certain methods at test time due to potential\nfor directly revealing the model output, this is less of a concern\nfor saliency-based methods as their design makes it non-trivial\nto leak information in this way. We opt to show explanations\nto measure whether they sufﬁciently help the user make a\nprediction similar to the model on an individual example.\n188\nFigure 3: Example of a prompt in our human study.\nnot provide information on directionality. For non-\ncontrastive methods, we provide the explanation for\nwhy the model predicted a token. For contrastive\nmethods, we provide the explanation for why the\nmodel predicted one token instead of another.\nWe include 20 pairs of highly confusable words\nfor our study (Appendix C). 10 of these pairs are\nselected from BLiMP to reﬂect certain linguistic\nphenomena, and the other 10 word pairs are se-\nlected from pairs with the highest “confusion score”\non WikiText-103 test split (Merity et al., 2016).\nWe deﬁne confusion using the joint probability of\na confusion from token a to b given a corpus X:\nP(xtrue = a, xmodel = b) =\n1\nN\n∑\nx∈X\n∑\nt∈pos(x)|xt=a\nPmodel(ˆxt = b|x<t)\nwhere x is a sentence in X, pos(x) is the set of\npositions of tokens in x, N is the size of the corpus.\nThe confusion from a to b is the sum of the prob-\nabilities assigned by the model to token b where\ntoken a is the ground truth, normalized by the num-\nber of sentences in the corpus.\nThe confusion score for word pair (a, b) is the\nminimum of confusion from a to b and vice-versa,\nto ensure that words are mutually confusable:\nC(a, b) = min(P(xtrue = a, xmodel = b),\nP(xtrue = b, xmodel = a)).\nWe recruited 10 graduate students in machine\nlearning (not authors of this paper) to perform the\nstudy. Each participant is given 10 different word\npairs. For each word pair, one explanation method\nwas chosen at random to generate the accompany-\ning explanations, and the participant is given 40\nsentences in a row. We balance the data so that\nthere were an equal number of examples where\nthe true output xt = a and xt = b, and also by\nmodel correctness so that the model chooses the\ncorrect output 50% of the time, preventing users\nfrom guessing model behavior by selecting a cer-\ntain token or the true token. In total, we obtain\n4000 data points for model simulatability.\n5.2 Results\nIn Table 4, we provide the results of our user study.\nFor each explanation method evaluated, we com-\nputed the simulation accuracy over all samples\n(Acc.) as well as accuracy over samples where\nthe model output is equal to the ground truth (Acc.\nCorrect) and different from the ground truth (Acc.\nIncorrect). We also computed the percentage of ex-\nplanations that users reported useful, as well as the\nsimulation accuracy over samples where the user\nfound the given explanation useful (Acc. Useful)\nand not useful (Acc. Not Useful).\nTo test our results for statistical signiﬁcance and\naccount for variance in annotator skill and word\npair difﬁculty, we ﬁtted linear mixed-effects models\nusing Statsmodels (Seabold and Perktold, 2010)\nwith the annotator and word pair as random effects,\nthe explanation method as ﬁxed effect, and the\nanswer accuracy or usefulness as the dependent\nvariable. In Appendix D we provide the results of\nthe mixed-effects models we ﬁtted.\nAcc. Acc. Acc. Acc.\nAcc. Correct IncorrectUseful Useful Not Useful\nNone61.38 74.50 48.25 – – –\nSGI 64.00 78.25 49.75 62.12 67.20 58.75\nS∗GI 65.62 79.00 52.25 63.88 69.67 58.48\nSE 63.12 79.00 47.25 46.50 65.86 60.75\nS∗E 64.62 77.00 52.25 64.88 70.52 53.74\nTable 4: Simulation accuracy (%) in predicting GPT-\n2 outputs and subjective usefulness of explanations for\nvarious explanation methods. For each method, scores\nthat are statistically signiﬁcantly higher (p≤0.05) than\nthe analogous method with a different contrastive set-\nting are bolded. Overall, users achieve higher simula-\ntion accuracy with contrastive explanations.\nAccuracy First of all, users have the lowest accu-\nracy in predicting LM outputs when no explanation\nis given, which suggests that all four types of ex-\nplanations help users simulate model behavior. For\nboth explanation methods, the contrastive setting\nleads to a signiﬁcantly higher contrastive simula-\ntion accuracy than the non-contrastive setting.\n189\nWe also examined examples where annotators\nincorrectly predict the model output, and for all\ntypes of explanations given, the most human errors\nare made in examples where there are no words\nin the input sentence that makes one word more\nlikely than the other. Notably, the three word pairs\nwith the lowest user accuracy are “son/brother”,\n“fast/super”, and “black/green”, which are often\ninterchangeable.\nUsefulness Contrastive explanations were also\nconsidered useful to users for model simulation\nsigniﬁcantly more often than non-contrastive ex-\nplanations, with a particularly large gain in the\nerasure-based setting. Answer accuracy on sam-\nples where the users found the explanation useful is\nhigher than the accuracy over all samples for each\nexplanation method, which suggests that users can\nalso identify useful explanations to some extent.\nThese results, on the whole, provide evidence\nthat contrastive explanations help human observers\nsimulate model predictions more accurately.\n6 What Context Do Models Use for\nCertain Decisions?\nFinally, we use contrastive explanations to discover\nhow language models achieve various linguistic\ndistinctions. We hypothesize that similar evidence\nis necessary to disambiguate foils that are similar\nlinguistically. To test this hypothesis, we propose\na methodology where we ﬁrst represent each token\nby a vector representing its saliency map when the\ntoken is used as a foil in contrastive explanation of\na particular target word. Conceptually, this vector\nrepresents the type of context that is necessary to\ndisambiguate the particular token from the target.\nNext, we use a clustering algorithm on these vec-\ntors, generating clusters of foils where similar types\nof context are useful to disambiguate. We then ver-\nify whether we ﬁnd clusters associated with salient\nlinguistic distinctions deﬁned a-priori. Finally, we\ninspect the mean vectors of explanations associated\nwith foils in the cluster to investigate how models\nperform these linguistic distinctions.\n6.1 Methodology\nWe generate contrastive explanations for the 10\nmost frequent words in WikiText-103 for each\nmajor part of speech as the target token, and\nuse the 10,000 most frequent vocabulary items\nas foils. For each target yt, we randomly select\n500 sentences from WikiText-103 and obtain a sen-\ntence set X. For each foil yf and each sentence\nxi ∈X, we generate a single contrastive explana-\ntion e(xi, yt, yf ). Then, for each target yt and foil\nyf , we generate an aggregate explanation vector\ne(yt, yf ) =⨁\nxi∈X e(xi, yt, yf ) by concatenating\nthe single explanation vectors for each sentence in\nthe corpus.\nThen, for a given target yt, we apply k-means\nclustering on the concatenated contrastive expla-\nnations across different foils yf to cluster foils by\nexplanation similarity. We use GPT-2 to extract\nall the contrastive explanations due to its better\nalignment with linguistic phenomena than GPT-\nNeo (§4). We only extract contrastive explanations\nwith gradient norm and gradient×input due to the\ncomputational complexity of input erasure (§3.3).\nIn Table 5, we show examples of the obtained\nclusters. Foils in each cluster are sorted in descend-\ning frequency in training data. For the ﬁrst foil in\neach cluster, we also retrieve its 20 nearest neigh-\nbors in the word embedding space according to\nEuclidean distance for comparison, to disentangle\nthe effect of word embeddings from the effect of\nlinguistic distinctions on foil clusters.\n6.2 Foil Clusters\nFirst, we verify that linguistically similar foils are\nindeed clustered together: we discover clusters re-\nlating to a variety of previously studied linguistic\nphenomena, a few of which we detail below and\ngive examples in Table 5. Moreover, foil clusters\nreﬂect linguistic distinctions that are not found in\nthe nearest neighbors of word embeddings. This\nsuggests that the model use similar types of input\nfeatures to make certain decisions.\nAnaphor agreement: To predict anaphor agree-\nment, models must contrast pronouns from other\npronouns with different gender or number. We ﬁnd\nthat indeed, when the target is a pronoun, other\npronouns of a different gender or number are often\nclustered together: when the target is a male pro-\nnoun, we ﬁnd a cluster of female pronouns. The\nfoil cluster containing “she” includes several types\nof pronouns that are all of the female gender. On\nthe other hand, the nearest neighbors of “she” are\nmostly limited to subject and object pronouns, and\nthey are of various genders and numbers.\nAnimacy: In certain verb phrases, the main verb\nenforces that the subject is animate. Reﬂecting\nthis, when the target is an animate noun, inanimate\n190\nPhenomenon / POSTargetFoil Cluster Embd Nearest Neighbors Example\nAnaphorAgreement he she, her, She, Her, herself, hersshe,She,her,She, he, they,Her, we, it,she, I,that,Her, you, was, there,He, is, as, in’That night, Ilsa confronts Rick in thedeserted café . Whenhe refuses to giveher the letters , _____\nAnimateSubject man fruit, mouse, ship, acid, glass, water, tree,honey, sea, ice, smoke, wood, rock, sugar,sand, cherry, dirt, ﬁsh, wind, snow\nfruit, fruits, Fruit, meat, ﬂower,fruit, tomato,vegetables,ﬁsh, apple, berries, food, citrus,banana, vegetable, strawberry, fru, delicious,juice, foods\nYou may not be surprised to learn thatKelly Pool was neitherinvented by a_____\nDeterminer-NounAgreement page tabs,pages, icons, stops, boxes, doors, short-cuts, bags, ﬂavours, locks, teeth, ears, tastes,permissions, stairs, tickets, touches, cages,saves, suburbs\ntabs, tab, Tab, apps, ﬁles, bags, tags, websites,sections, browsers, browser, icons, buttons,pages, keeps, clips, updates, 28, insists, 14\nImmediatelyafter \"Heavy Competition\"ﬁrst aired, NBC createda sub- _____\nSubject-VerbAgreement go doesn,causes, looks, needs, makes, isn, says,seems, seeks, displays, gives, wants, takes,uses, fav, contains, keeps, sees, tries, sounds\ndoesn, isn, didn, does, hasn, wasn, don,wouldn,makes, gets, has, is, aren,gives,Doesn, couldn,seems,takes,keeps,doesn\nMalaand theEskimos _____\nADJ blackBlack, white, black, White,red, BLACK,green, brown, dark, orange, African, blue, yel-low, pink, purple, gray, grey, whites, Brown,silver\nBlack,Black,black,black,White, BLACK,white, Blue, Red,White, In, B, The,The, It,red, Dark, 7, Green,African\nAlthough generalrelativity can be usedtoperform asemi @-@ classical calcu-lation of _____\nADJ blackAsian, Chinese, English, Italian, American,Indian, East, South, British, Japanese, Euro-pean, African, Eastern, North, Washington,US, West, Australian, California, London\nAsian,Asian, Asia, Asians,Chinese,African,Japanese, Korean, China,European,Indian,ethnic,Chinese, Japan,American, Caucasian,Australian, Hispanic, white, Arab\nWhile taking part in the AmericanNe-gro Academy (ANA) in 1897 , Du Boispresented a paper in which he rejectedFrederickDouglass ’s plea for _____\nADP for to, in, and, on, with, for, when, from, at, (,if, as, after, by, over, because, while, without,before, through\nto,in,for,on,and,as,with, of, a,at, that,the,from,by, an, (, To, is, it, or Thewar of wordswouldcontinue _____\nADV back the, to, a,in, and, on, of, it, \", not, that, with,for, this, from, up, just, at, (, allthe,a, an,it,this,that,in, The,to,The,all,and, their, as,for,on, his,at, some, whatOne would have thought thatclaimsdat-ing _____\nDET his the, you, it, not, that, my, [, this, your, he, all,so, what, there, her, some, his, time, him, Hethe, a, an,it,this,that, in, The, to,The,all,and, their, as, for, on,his, at,some,what A preview screening of Sweet Smell ofSuccess was poorly received , asTonyCurtis fans were expecting him toplayone of _____\nNOUN girl Guy,Jack, Jones, Robin, James, David, Tom,Todd, Frank, Mike, Jimmy, Michael, Peter,George, William, Bill, Smith, Tony, Harry,Jackson\nGuy,Guy, guy,guy, Gu, Dave, Man, dude, Girl,Guys, John, Steve, \\x00, \\xef \\xbf \\xbd, \\xef\\xbf \\xbd, \\x1b, \\xef \\xbf \\xbd, \\x12, \\x1c, \\x16\nVeronicatalks to to Sean Friedrich andtells him about the _____\nNUM ﬁve the, to, a, in, and, on, of, is, it, \", not, that, 1,with, for, 2, this, up, just, at the,a, an,it,this,that,in, The,to,The, all,and, their, as,for,on, his,at, some, whatFromtheage of _____\nVERB goinggot, didn, won, opened, told, went, heard, saw,wanted, lost, came, started, took, gave, hap-pened, tried, couldn, died, turned, looked\ngot, gets, get, had,went,gave,took,came,didn, did, getting, been, became, has, was,made,started, have, gotten, showed\nTrumanhad dreamedof _____\nTable 5: Examples of foil clusters obtained by clustering contrastive explanations of GPT-2. For each cluster, the\n20 most frequent foils are shown, as well as the 20 nearest neighbors in the word embedding space of the ﬁrst foil,\nand an example is included for the contrastive explanation of the target token vs. the underlined foil in the cluster.\nIn each explanation, the two most salient input tokens are highlighted in decreasing intensity of red.\nnouns form a cluster. While the foil cluster in Table\n5 contains a variety of singular inanimate nouns,\nthe nearest neighbors of “fruit” are mostly both\nsingular and plural nouns related to produce.\nPlurality: For determiner-noun agreement, sin-\ngular nouns are contrasted with clusters of plural\nnoun foils, and vice-versa. We ﬁnd examples of\nclusters of plural nouns when the target is a singular\nnoun, whereas the nearest neighbors of “tabs” are\nboth singular and plural nouns. To verify subject-\nverb agreement, when the target is a plural verb,\nsingular verbs are clustered together, but the near-\nest neighbors of “doesn” contain both singular and\nplural verbs, especially negative contractions.\n6.3 Explanation Analysis Results\nBy analyzing the explanations associated with dif-\nferent clusters, we are also able to learn interesting\nproperties of how GPT-2 makes certain predictions.\nWe provide our full analysis results in Appendix E.\nTo distinguish between adjectives, the model of-\nten relies on input words that are semantically sim-\nilar to the target (e.g. “relativity” to distinguish\n“black” from other colors). To contrast adposi-\ntions and adverbs from other words with the same\nPOS, verbs in the input that are associated with\nthe target word are useful: for example, the verbs\n“dating” and “traced” are useful when the target\nis “back”. To choose the correct gender for deter-\nminers, nouns and pronouns, the model often uses\ngendered proper nouns and pronouns in the input.\nTo disambiguate numbers from non-number words,\n191\ninput words related to enumeration or measurement\n(e.g. “age”, “consists”, “least”) are useful.\nOur analysis also reveals why the model may\nhave made certain mistakes. For example, when\nthe model generates a pronoun of the incorrect\ngender, it was often inﬂuenced by proper nouns\nand pronouns of a different gender in the input.\nOverall, our methodology for clustering con-\ntrastive explanations provides an aggregate anal-\nysis of linguistic distinctions to understand general\nproperties of language model decisions.\n7 Conclusion and Future Work\nIn this work, we interpreted language model de-\ncisions using contrastive explanations by extend-\ning three existing input saliency methods to the\ncontrastive setting. We also proposed three new\nmethods to evaluate and explore the quality of con-\ntrastive explanations: an alignment evaluation to\nverify whether explanations capture linguistically\nappropriate evidence, a user evaluation to mea-\nsure model simulatability of explanations, and a\nclustering-based aggregate analysis to investigate\nmodel properties using contrastive explanations.\nWe ﬁnd that contrastive explanations are better\naligned to known evidence related to major gram-\nmatical phenomena than their non-contrastive coun-\nterparts. Moreover, contrastive explanations allow\nbetter contrastive simulatability of models for users.\nFrom there, we studied what kinds of decisions\nrequire similar evidence and we used contrastive\nexplanations to characterize how models make cer-\ntain linguistic distinctions. Overall, contrastive\nexplanations give a more intuitive and ﬁne-grained\ninterpretation of language models.\nFuture work could explore the application of\nthese contrastive explanations to other machine\nlearning models and tasks, extending other inter-\npretability methods to the contrastive setting, as\nwell as using what we learn about models through\ncontrastive explanations to improve them.\n8 Limitations\nThe experiments and methodology described in\nthis paper have some limitations, notably their ex-\ntensions to other explanation methods, other lan-\nguages, and their resource requirements.\nFirst, the applicability of the contrastive set-\nting to other explanation methods may be limited.\nWhile extending gradient-based explanation meth-\nods to the contrastive setting is relatively straight-\nforward (we can simply perform the same op-\nerations on the gradient over the difference be-\ntween model probabilities for the target and foil\ntokens), it is nontrivial to design contrastive expla-\nnations based on other explanation methods such\nas attention-based input saliency.\nSecond, many of our experiments would not be\neasily reproduced in languages other than English\nthat lack sufﬁcient linguistic resources. All the\nexperiments in our paper aimed at exploring the ca-\npabilities of contrastive explanations are performed\nusing GPT-2 and GPT-Neo language models, that\nhave been trained on large amounts of English data.\nTo reproduce experiments in other languages, we\nwould need a language model in the other language\nof sufﬁcient power, which is not available for most\nlanguages. The experiments in Section 4 address\nonly a subset of types of grammatical acceptability,\nand require a dataset of minimal pairs along differ-\nent types of grammatical acceptability, which may\nnot be available for most languages. Moreover, to\nautomatically extract the expected evidence, we\nrely on core NLP tools such as coreference resolu-\ntion, POS tagger and dependency parsers. Again,\nthese tools are not available for most languages.\nFurthermore, the accuracy of the extracted evi-\ndence depends of the aforementioned tools, which\nhave fairly high but not perfect accuracy. While\nour experiments are not easily extendable to lan-\nguages other than English, our method itself of\ncontrastive explanations is language agnostic and\ncan be readily applied to models of any language.\nIn Section 5, we perform a human study to eval-\nuate explanation methods. This evaluation method\nrequire human annotators and is therefore more re-\nsource intensive than automatic evaluation methods.\nWe were motivated to perform this study neverthe-\nless as model simulatability for human users is one\nimportant aspect of interpretability.\nThe experiments in Section 6 are also resource\nintensive. In total, we computed: 2 explanation\nmethods ×8 parts of speech ×10 target words ×\n10,000 foils ×500 input sentences = 800,000,000\ncontrastive explanations. For this reason, we omit-\nted the slower contrastive erasure explanation from\nthis experiment, but generating all the contrastive\nexplanations using the relatively faster explanation\nmethods, then clustering them required about 48\nhours of computation on 8 RTX 8000 GPUs.\n192\nReferences\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorﬂow. If you use this software, please cite it\nusing these metadata.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav,\nPrithvijit Chattopadhyay, and Devi Parikh. 2018. Do\nexplanations make VQA models more predictable to\na human? In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1036–1042, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nMisha Denil, Alban Demiraj, and Nando De Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. arXiv preprint arXiv:1412.6815.\nFinale Doshi-Velez and Been Kim. 2017. Towards a\nrigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608.\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1828–1843, Online. Association for Computational\nLinguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nPeter Hase and Mohit Bansal. 2020. Evaluating ex-\nplainable AI: Which algorithmic explanations help\nusers predict model behavior? In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5540–5552, Online. As-\nsociation for Computational Linguistics.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nAlon Jacovi, Swabha Swayamdipta, Shauli Ravfogel,\nYanai Elazar, Yejin Choi, and Yoav Goldberg. 2021.\nContrastive explanations for model interpretability.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1597–1611, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings\nof ACL 2018, System Demonstrations , pages 116–\n121, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016a. Visualizing and understanding neural mod-\nels in NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nPeter Lipton. 1990. Contrastive explanation. Royal\nInstitute of Philosophy Supplement, 27:247–266.\nZachary C. Lipton. 2018. The mythos of model inter-\npretability: In machine learning, the concept of in-\nterpretability is both important and slippery. Queue,\n16(3):31–57.\nAndreas Madsen, Siva Reddy, and Sarath Chandar.\n2021. Post-hoc interpretability for neural nlp: A sur-\nvey. arXiv preprint arXiv:2108.04840.\nAnn L McGill and Jill G Klein. 1993. Contrastive and\ncounterfactual reasoning in causal judgment. Jour-\nnal of Personality and Social Psychology, 64(6):897.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nDanish Pruthi, Bhuwan Dhingra, Livio Baldini Soares,\nMichael Collins, Zachary C Lipton, Graham Neubig,\nand William W Cohen. 2020. Evaluating explana-\ntions: How much do explanations from the teacher\naid students? arXiv preprint arXiv:2012.00893.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog.\nShauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav\nGoldberg. 2021. Counterfactual interventions re-\nveal the causal effect of relative clause represen-\ntations on agreement prediction. arXiv preprint\narXiv:2105.06965.\nSkipper Seabold and Josef Perktold. 2010. Statsmod-\nels: Econometric and statistical modeling with\npython. In Proceedings of the 9th Python in Science\nConference, volume 57, page 61. Austin, TX.\n193\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina,\nand Anshul Kundaje. 2016. Not just a black\nbox: Learning important features through prop-\nagating activation differences. arXiv preprint\narXiv:1605.01713.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2013. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps. arXiv preprint arXiv:1312.6034.\nIlia Stepin, Jose M Alonso, Alejandro Catala, and\nMartín Pereira-Fariña. 2021. A survey of contrastive\nand counterfactual explanation generation methods\nfor explainable artiﬁcial intelligence. IEEE Access,\n9:11974–12001.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Inter-\nnational Conference on Machine Learning , pages\n3319–3328. PMLR.\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subra-\nmanian, Matt Gardner, and Sameer Singh. 2019. Al-\nlenNLP Interpret: A framework for explaining pre-\ndictions of NLP models. In Empirical Methods in\nNatural Language Processing.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nKayo Yin, Patrick Fernandes, André F. T. Martins, and\nGraham Neubig. 2021a. When does translation re-\nquire context? a data-driven, multilingual explo-\nration. arXiv preprint arXiv:2109.07446.\nKayo Yin, Patrick Fernandes, Danish Pruthi, Aditi\nChaudhary, André F. T. Martins, and Graham Neu-\nbig. 2021b. Do context-aware translation models\npay the right attention? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 788–801, Online. As-\nsociation for Computational Linguistics.\nRuiqi Zhong, Steven Shao, and Kathleen R. McKeown.\n2019. Fine-grained sentiment analysis with faithful\nattention. CoRR, abs/1908.06870.\nA Contrastive Explanations for Neural\nMachine Translation (NMT) Models\nA.1 Extending Contrastive Explanations to\nNMT\nMachine translation can be thought of as a speciﬁc\ntype of language models where the model is condi-\ntioned on both the source sentence and the partial\ntranslation. It has similar complexities as mono-\nlingual language modeling that make interpreting\nneural machine translation (NMT) models difﬁcult.\nWe therefore also extend contrastive explanations\nto NMT models.\nWe compute the contrastive gradient norm\nsaliency for an NMT model by ﬁrst calculating\nthe gradient over the encoder input (the source sen-\ntence) and over the decoder input (the partial trans-\nlation) as:\ng∗(xe\ni ) =∇xe\ni\n(\nq(yt|xe, xd) −q(yf |xe, xd)\n)\ng∗(xd\ni ) =∇xd\ni\n(\nq(yt|xe, xd) −q(yf |xe, xd)\n)\nwhere xe is the encoder input, xd is the decoder\ninput, and the other notations follow the ones in\n§3.1.\nThen, the contrastive gradient norm for each xe\ni\nand xd\ni are:\nS∗\nGN (xe\ni ) =||g∗(xe\ni )||L1\nS∗\nGN (xd\ni ) =||g∗(xd\ni )||L1\nSimilarly, the contrastive gradient ×input are:\nS∗\nGI(xe\ni ) =g∗(xe\ni ) ·xe\ni\nS∗\nGI(xd\ni ) =g∗(xd\ni ) ·xd\ni\nWe deﬁne the input erasure for each xe\ni and xd\ni\nas:\nS∗\nE(xe\ni ) =\n(\nq(yt|xe, xd) −q(yt|xe\n¬i, xd)\n)\n−\n(\nq(yf |xe, xd) −q(yf |xe\n¬i, xd)\n)\nS∗\nE(xd\ni ) =\n(\nq(yt|xe, xd) −q(yt|xe, xd\n¬i)\n)\n−\n(\nq(yf |xe, xd) −q(yf |xe, xd\n¬i)\n)\nA.2 Qualitative Results\nIn Table 6, we provide examples of non-contrastive\nand contrastive explanations for NMT decisions.\nWe use MarianMT (Junczys-Dowmunt et al., 2018)\nwith pre-trained weights from the model trained\nto translate from English to Romance languages4\nto extract explanations. Each example reﬂects a\ndecision associated with one of the ﬁve types of\nlinguistic ambiguities during translation identiﬁed\nin Yin et al. (2021a).\n4https://github.com/Helsinki-NLP/Tatoeba-Challenge/\nblob/master/models/eng-roa/README.md\n194\nWhy did the model predict \"il\" ?\nen:Iorderedanewvaseanditarrivedtoday\nfr:J’aicommandéunnouveauvaseet\nWhy did the model predict \"il\" instead of \"elle\" ?\nen:Iorderedanewvaseanditarrivedtoday\nfr:J’aicommandéunnouveauvaseet\n2. Why did the model predict \"votre\" ?\nen:Youcannotbringyourdoghere.\nfr:V ousnepouvezpasamener\nWhy did the model predict \"votre\" instead of \"ton\" ?\nen:Youcannotbringyourdoghere.\nfr:V ousnepouvezpasamener\n3. Why did the model predict \"apprenais\" ?\nen:IlikedschoolbecauseIlearnedalotthere.\nfr:J’aimaisl’écoleparcequej’\nWhy did the model predict \"apprenais\" instead of \"ai\" ?\nen:IlikedschoolbecauseIlearnedalotthere.\nfr:J’aimaisl’écoleparcequej’\n4. Why did the model predict \"sais\" ?\nen:Theyknowwhattodo,Idon’t.\nfr:Ilssaventquoifaire,jene\nWhy did the model predict \"sais\" instead of \"veux\" ?\nen:Theyknowwhattodo,Idon’t.\nfr:Ilssaventquoifaire,jene\n5. Why did the model predict \"carnet\" ?\nen:Ilikemyoldnotebookbetterthanmynewnotebook\nfr:J’aimemieuxmonanciencarnetquemonnouveau\nWhy did the model predict \"carnet\" instead of \"ordinateur\" ?\nen:Ilikemyoldnotebookbetterthanmynewnotebook\nfr:J’aimemieuxmonanciencarnetquemonnouveau\nTable 6: Examples of non-contrastive and contrastive\nexplanations for NMT models translating from English\nto French using input ×gradient. Input tokens that are\nmeasured to raise or lower the probability of each de-\ncision are in red and blue respectively, and those with\nlittle inﬂuence are in white.\nIn the ﬁrst example, the model must translate\nthe gender neutral English pronoun “it” into the\nmasculine French pronoun “il”. In both non-\ncontrastive and contrastive explanations, the En-\nglish antecedent “vase” inﬂuences the model to\npredict “il”, however to disambiguate “il” from the\nfeminine pronoun “elle”, the model also relies on\nthe french antecedent and its masculine adjective\n“nouveau vase”.\nIn the second example, the model must translate\n“your” with the formality level consistent with the\npartial translation. While in the non-contrastive\nexplanation, only tokens in the source sentence are\nsalient which do not explain the model’s choice\nof formality level, in the contrastive explanation,\nother French words in the polite formality level\nsuch as “V ous” and “pouvez” are salient.\nIn the third example, the model must translate\n“learned” using the verb form that is consistent with\nthe partial translation. Similarly to the previous\nexample, only the contrastive explanation contains\nsalient tokens in the same verb from as the target\ntoken such as “aimais”.\nIn the fourth example, the model needs to re-\nsolve the elided verb in “I don’tknow” to translate\ninto French. The contrastive explanation with a\ndifferent verb as a foil shows that the elided verb in\nthe target side makes the correct verb more likely\nthan another verb.\nIn the ﬁfth example, the model must choose the\ntranslation that is lexically cohesive with the partial\ntranslation, where “carnet” refers to a book with\npaper pages and “ordinateur” refers to a computer\nnotebook. In the non-contrastive explanation, the\nword “notebook” and the target token preceding the\nprediction are the most salient. In the contrastive\nexplanation, the word “carnet” in the partial trans-\nlation also becomes salient.\nB Alignment of Contrastive\nExplanations to Linguistic Paradigms\nIn Table 7, we present the full alignment scores of\ncontrastive explanations from GPT-2 and GPT-Neo\nmodels with the known evidence to disambiguate\nlinguistic paradigms in the BLiMP dataset.\nC Highly Confusable Word Pairs\nIn Table 8, we provide the list of contrastive word\npairs used in our human study for model simulata-\nbility (§5). The ﬁrst 10 pairs are taken from BLiMP\nlinguistic paradigms and we provide the associated\n195\nGPT-2 GPT-NeoParadigm Dist Explanation Dot Product (↑) Probes Needed (↓) MRR (↑) Dot Product (↑) Probes Needed (↓) MRR (↑)\nanaphor_gender_agreement\nRandom 0.528 0.706 0.718 0.548 0.618 0.762SGN 0.429 1.384 0.478 0.480 0.828 0.622S∗GN 0.834 0.472 0.809 0.785 0.432 0.8152.94 SGI 0.078 1.402 0.468 -0.054 0.526 0.786S∗GI -0.019 0.502 0.791 -0.133 0.684 0.747SE -0.350 0.564 0.764 0.645 0.078 0.963S∗E 0.603 0.090 0.964 0.637 0.156 0.903\nanaphor_number_agreement\nRandom 0.554 0.666 0.741 0.568 0.598 0.756SGN 0.463 1.268 0.512 0.508 0.784 0.639S∗GN 0.841 0.702 0.677 0.816 0.524 0.7632.90 SGI 0.084 1.346 0.497 -0.095 0.510 0.797S∗GI 0.084 0.408 0.860 -0.068 0.636 0.775SE -0.349 0.704 0.728 0.618 0.128 0.940S∗E 0.604 0.136 0.951 0.666 0.106 0.956\nanimate_subject_passive\nRandom 0.155 2.940 0.378 0.150 2.976 0.379SGN 0.211 1.080 0.699 0.236 0.828 0.727S∗GN 0.463 0.754 0.749 0.452 0.862 0.7213.27 SGI 0.016 4.004 0.233 0.020 2.780 0.416S∗GI 0.069 2.782 0.412 0.016 2.844 0.409SE -0.036 3.214 0.362 0.168 2.024 0.444S∗E 0.125 2.122 0.500 0.123 2.120 0.517\ndeterminer_noun_agreement_1\nRandom 0.208 2.202 0.449 0.207 2.142 0.461SGN 0.239 1.320 0.598 0.150 2.954 0.287S∗GN 0.275 2.680 0.406 0.258 2.906 0.3021.00 SGI 0.560 0.038 0.983 -0.042 2.384 0.380S∗GI 0.162 1.558 0.603 -0.056 2.554 0.371SE 0.022 1.150 0.604 0.234 1.290 0.543S∗E 0.031 2.598 0.363 0.362 0.612 0.811\ndeterminer_noun_agreement_irregular_1\nRandom 0.198 2.248 0.437 0.202 2.110 0.456SGN 0.236 1.228 0.616 0.160 2.716 0.324S∗GN 0.286 2.578 0.380 0.266 2.826 0.3101.00 SGI 0.559 0.034 0.984 -0.035 2.160 0.419S∗GI 0.046 2.038 0.507 -0.046 2.428 0.374SE 0.020 1.082 0.628 0.205 1.360 0.548S∗E 0.026 2.502 0.352 0.306 0.784 0.755\ndeterminer_noun_agreement_with_adjective_1\nRandom 0.167 2.672 0.406 0.168 2.672 0.405SGN 0.118 3.914 0.237 0.120 3.902 0.230S∗GN 0.210 3.532 0.267 0.228 3.814 0.2452.05 SGI 0.118 2.426 0.354 -0.010 2.736 0.356S∗GI 0.141 2.012 0.482 -0.051 2.950 0.342SE 0.042 1.730 0.583 0.092 2.748 0.333S∗E 0.305 1.084 0.680 0.260 1.176 0.697\ndeterminer_noun_agreement_with_adj_irregular_1\nRandom 0.167 2.620 0.401 0.158 2.820 0.392SGN 0.116 3.920 0.240 0.125 3.620 0.248S∗GN 0.205 3.664 0.256 0.228 3.718 0.2432.07 SGI 0.106 2.620 0.345 -0.007 2.754 0.358S∗GI 0.111 2.244 0.448 -0.047 3.126 0.316SE 0.048 1.688 0.586 0.103 2.644 0.347S∗E 0.313 1.024 0.686 0.263 1.066 0.683\nnpi_present_1\nRandom 0.336 1.080 0.604 0.350 0.984 0.632SGN 0.294 1.160 0.510 0.376 0.454 0.778S∗GN 0.456 0.450 0.787 0.449 0.382 0.8123.19 SGI 0.100 1.374 0.463 -0.160 1.288 0.575S∗GI 0.144 0.570 0.759 0.202 0.766 0.752SE -0.336 1.514 0.556 0.624 0.086 0.960S∗E 0.160 0.902 0.684 0.062 1.204 0.556\ndistractor_agreement_relational_noun\nRandom 0.230 1.936 0.494 0.227 2.106 0.463SGN 0.266 1.199 0.584 0.269 0.965 0.646S∗GN 0.408 1.092 0.619 0.392 1.000 0.6493.94 SGI 0.044 2.291 0.369 -0.066 2.326 0.434S∗GI 0.223 1.057 0.631 0.051 1.383 0.591SE -0.023 1.922 0.434 0.120 2.007 0.400S∗E 0.190 1.709 0.502 0.186 1.617 0.544\nirregular_plural_subject_verb_agreement_1\nRandom 0.561 0.539 0.760 0.545 0.494 0.769SGN 0.652 0.242 0.917 0.610 0.348 0.860S∗GN 0.676 0.315 0.843 0.644 0.376 0.8171.11 SGI 0.590 0.253 0.912 0.067 0.472 0.783S∗GI 0.348 0.298 0.864 0.021 0.489 0.750SE -0.570 0.787 0.617 -0.021 0.893 0.553S∗E 0.264 0.635 0.673 0.267 0.584 0.734\nregular_plural_subject_verb_agreement_1\nRandom 0.694 0.316 0.853 0.693 0.336 0.849SGN 0.740 0.194 0.946 0.724 0.268 0.906S∗GN 0.756 0.251 0.909 0.747 0.274 0.8981.13 SGI 0.748 0.202 0.944 -0.039 0.333 0.852S∗GI 0.371 0.242 0.889 0.039 0.262 0.879SE -0.614 0.610 0.718 0.303 0.632 0.694S∗E 0.584 0.353 0.836 0.568 0.313 0.842\nTable 7: Alignment of GPT-2 and GPT-Neo explanations with BLiMP. Scores better than their (non-)contrastive\ncounterparts are bolded. “Dist” gives the average distance from the target to the important context token.\n196\nunique identiﬁer for each pair. The last 10 pairs are\nchosen from word pairs with the highest confusion\nscore.\nWord 1 Word 2 BLiMP UID\nactor actress anaphor_gender_agreement\nherself himself anaphor_gender_agreement\nthemselves herself anaphor_number_agreement\nwomen pictures animate_subject_passive\nboy dog animate_subject_passive\ncat cats determiner_noun_agreement_1\nis are irregular_plural_subject_verb_agreement_1\nhas have regular_plural_subject_verb_agreement_1\nhim himself principle_A_domain_1\nhe who wh_island\nWord 1 Word 2 Confusion Score\nblack green 0.0008\nBruce Beth 0.0021\nfast super 0.0011\nhealth hospital 0.0012\nred bright 0.0007\nsnow winter 0.0005\nson brother 0.0027\nsummer winter 0.0003\nwhite blue 0.0034\nwine grape 0.0106\nTable 8: List of highly confusable words pairs chosen\nfor our user study.\nDependent Variable Intercept Effect P-Value\nAccuracy 0.624 0.015 0.050\nAcc. Correct 0.744 0.026 0.005\nAcc. Incorrect 0.530 -0.010 0.460\nUseful 0.570 0.063 0.000\nAcc. Useful 0.677 -0.020 0.513\nAcc. Useful 0.450 -0.009 0.444\nTable 9: The dependent variables, intercepts, the effect\nof the explanation method on the dependent variable\nand its p-value in the linear mixed-effects models ﬁtted\nto model simulatability results.\nD Mixed Effects Models Results\nIn Table 9, we show the results of ﬁtting linear\nmixed-effects models to the results of our user\nstudy for model simulatability (§5).\nE Analysis of Foil Clusters\nIn Figure 5, we give a few examples of clusters and\nexplanations we obtain for each part of speech. For\neach part of speech, we describe our ﬁndings in\nmore detail in the following.\nAdjectives. When the target word is an adjective,\nother foil adjectives that are semantically similar to\nthe target are often clustered together. For example,\nwhen the target is “black”, we ﬁnd one cluster\nwith various color adjectives, and we also ﬁnd a\ndifferent cluster with various adjectives relating to\nthe race or nationality of a person.\nWe ﬁnd that to distinguish between different\nadjectives, input words that are semantically close\nto the correct adjective are salient. For example\nto disambiguate the adjective “black” from other\ncolors, words such as “venom” and “relativity” are\nimportant.\nAdpositions. When the target is an adposition,\nother adpositions are often in the same cluster.\nTo distinguish between different adpositions, the\nverb associated with the adposition is often useful\nto the LM. For example, when the target word is\n“from”, verbs such as “garnered” and “released”\nhelps the model distinguish the target from other\nadpositions that are less commonly paired with\nthese verbs (e.g. “for”, “of” ). As another example,\nfor the target word“for”, verbs that indicate a long-\nlasting action such as “continue” and “lived” help\nthe model disambiguate.\nAdverbs. When the target is an adverb, other\nadverbs are often clustered together. Sometimes,\nwhen the target is a speciﬁc type of adverb, such as\nan adverb of place, we can ﬁnd a cluster with other\nadverbs of the same type.\nSimilarly to adpositions, LMs often use the verb\nassociated with the target adverb to contrast it from\nother adverbs. For example, the verbs“dating” and\n“traced” are useful when the target is “back”, and\nthe verbs “torn” and “lower” are useful when the\ntarget is “down”.\nDeterminers. Other determiners are often clus-\ntered together when the target is a determiner. Par-\nticularly, when the target is a possessive determiner,\nwe ﬁnd clusters with other possessive determiners,\nand when the target is a demonstrative determiner,\nwe ﬁnd clusters with demonstrative determiners.\nWhen the determiner is a gendered possessive\ndeterminer such as “his”, proper nouns of the same\ngender, such as “John” and “George”, are often\nuseful. For demonstrative determiners, such as\n“this”, verbs that are usually associated with a tar-\ngeted object, such as “achieve” and “angered” are\nuseful.\nNouns. When the target noun refers to a person,\nfor example, “girl”, foil nouns that also refer to\n197\na person form one cluster (e.g. “woman”, “man-\nager”, “friend” ), commonly male proper nouns\nform another (e.g. “Jack”, “Robin”, “James” ),\ncommonly female proper nouns form another (e.g.\n“Sarah”, “Elizabeth”, “Susan” ), and inanimate\nobjects form a fourth (e.g. “window”, “fruit”,\n“box”).\nWhen the target noun is an inanimate object,\nthere are often two notable clusters: a cluster with\nsingular inanimate nouns and a cluster with plu-\nral inanimate nouns. This suggests how clustering\nfoils by explanations conﬁrm that certain grammat-\nical phenomena require similar evidence for disam-\nbiguation; in this case, determiner-noun agreement.\nTo predict a target animate noun such as“girl”\ninstead of foil nouns that refer to a non-female or\nolder person, input words that are female names\n(e.g. “Meredith”) or that refer to youth (e.g.\n“young”) are useful. To disambiguate from male\nproper nouns, input words that refer to female peo-\nple (e.g. “Veronica”, “she”) or adjectives related\nto the target (e.g. “tall”) inﬂuence the model to\ngenerate a female common noun. To disambiguate\nfrom female proper nouns, adjectives and determin-\ners are useful. To disambiguate from inanimate\nobjects, words that describe a human or a human\naction (e.g. “delegate”, “invented” ) are useful.\nTo predict a target inanimate noun such as“page”\ninstead of nouns that are also singular, input words\nwith similar semantics are important such as“sheet”\nand “clicking” are important. For plural noun foils,\nthe determiner (e.g. “a”) is important.\nNumbers. When the target is a number, non-\nnumber words often form one cluster and other\nnumbers form another cluster.\nTo disambiguate numbers from non-number\nwords, input words related to enumeration or mea-\nsurement are useful (e.g. “age”, “consists”, “least”).\nTo disambiguate words like “hundred” and “thou-\nsand” from other numbers such as “20” or “ﬁve”,\ninput words used for counting (e.g. “two”, “sev-\neral”) are useful, because “hundred”s are count-\nable in English (i.e. “two hundreds”, “several hun-\ndreds”).\nPronouns. When the target word is a gendered\npronoun, foil pronouns of a different gender from\nthe target form one cluster, foils with proper nouns\nof a different gender form a second cluster, and\nfoils with proper nouns of the same gender as the\ntarget form a third cluster. This shows that the\nmodel uses similar evidence to make decisions to\nverify anaphor gender agreement. We also did not\nﬁnd foil clusters associated with distinguishing the\nnumber of the pronoun: often, these decisions fol-\nlow directly from deciding between a pronoun and\na proper noun, or deciding between a male and\nfemale pronoun.\nTo disambiguate a gendered pronoun such as\nsuch as “he”, from pronouns or proper nouns\nwith different genders (e.g. “she” or “Anna”),\nproper nouns of the same gender as the target (e.g.\n“James”) and other gendered pronouns or determin-\ners (e.g. “his”) are useful. To disambiguate from\nproper nouns of the same gender as the target, inter-\nestingly, the same proper noun as the foil appearing\nin the input is positively salient; GPT-2 is often in-\nﬂuenced by previously appearing proper nouns to\ngenerate a pronoun instead.\nVerbs. When the target word is a verb, foil verbs\nthat have a different verb form are often clustered\ntogether. This suggests that the model uses similar\ninput features to verify subject-verb agreement.\nWhen the target verb is in present participle form,\nauxiliary verbs in the input are useful (e.g. “is”,\n“been”) to distinguish from verbs in other forms.\nSimilarly, when the target verb is in inﬁnitive form,\nverbs in the same compound as the target verb are\nimportant.\n198",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8071149587631226
    },
    {
      "name": "Computer science",
      "score": 0.8002896308898926
    },
    {
      "name": "Natural language processing",
      "score": 0.6329032182693481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6190301179885864
    },
    {
      "name": "Salient",
      "score": 0.5936605930328369
    },
    {
      "name": "Security token",
      "score": 0.5914949178695679
    },
    {
      "name": "Language model",
      "score": 0.5753037333488464
    },
    {
      "name": "Focus (optics)",
      "score": 0.5744861364364624
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.49279215931892395
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.44079622626304626
    },
    {
      "name": "Conflation",
      "score": 0.43890097737312317
    },
    {
      "name": "Linguistics",
      "score": 0.32872912287712097
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}