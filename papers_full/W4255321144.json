{
  "title": "Transformer-based deep neural network language models for Alzheimer's disease risk assessment from targeted speech",
  "url": "https://openalex.org/W4255321144",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2078805234",
      "name": "Alireza Roshanzamir",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A349423707",
      "name": "Hamid Aghajan",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A399963275",
      "name": "Mahdieh Soleymani Baghshah",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2078805234",
      "name": "Alireza Roshanzamir",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A349423707",
      "name": "Hamid Aghajan",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A399963275",
      "name": "Mahdieh Soleymani Baghshah",
      "affiliations": [
        "Sharif University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600756316",
    "https://openalex.org/W6605614091",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600992289",
    "https://openalex.org/W39805860",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2939515132",
    "https://openalex.org/W2250883471",
    "https://openalex.org/W2151969795",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W588041470",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2964226943",
    "https://openalex.org/W1549115098",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1997994949",
    "https://openalex.org/W2972757121",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2892062011",
    "https://openalex.org/W2793398224",
    "https://openalex.org/W2514692010",
    "https://openalex.org/W2962979297",
    "https://openalex.org/W2252171711",
    "https://openalex.org/W2129497119",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2089109585",
    "https://openalex.org/W2578432686",
    "https://openalex.org/W1853705225",
    "https://openalex.org/W2033002462",
    "https://openalex.org/W2514643877",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2397273938",
    "https://openalex.org/W3094326096",
    "https://openalex.org/W2069740663",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2952746683",
    "https://openalex.org/W36820562",
    "https://openalex.org/W1994871153"
  ],
  "abstract": "Abstract Background : We developed transformer-based deep learning models based on natural language processing for early risk assessment of Alzheimer’s disease from the picture description test. Methods : The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classiﬁcation tasks with small training sets. The overall classiﬁcation model is a simple classiﬁer on top of the pre-trained deep language model. Results : The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classiﬁer achieves classiﬁcation accuracy of 88.08%, which improves the state-of-the-art by 2.48%. Conclusions : Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of suﬃciently large datasets, but also reduces the need for expert-deﬁned features.",
  "full_text": "Transformer-based deep neural network language\nmodels for Alzheimer's disease risk assessment\nfrom targeted speech\nAlireza Roshanzamir \nSharif University of Technology\nHamid Aghajan \nSharif University of Technology\nMahdieh Soleymani Baghshah  (  soleymani@sharif.edu )\nSharif University of Technology https://orcid.org/0000-0002-1971-6231\nResearch article\nKeywords: Alzheimer’s disease, Early risk assessment, Picture description test, Deep learning,\nTransformer, Natural language processing, Language model, Transfer learning\nPosted Date: January 21st, 2021\nDOI: https://doi.org/10.21203/rs.3.rs-49267/v3\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nVersion of Record: A version of this preprint was published on March 9th, 2021. See the published version\nat https://doi.org/10.1186/s12911-021-01456-3.\nRoshanzamir et al.\nRESEARCH\nTransformer-based deep neural network language\nmodels for Alzheimer’s disease risk assessment\nfrom targeted speech\nAlireza Roshanzamir\n1, Hamid Aghajan 2 and Mahdieh Soleymani Baghshah 3*\n*Correspondence:\nsoleymani@sharif.edu\n3\nDepartment of Computer\nEngineering, Sharif University of\nTechnology, Azadi, 11365-11155\nTehran, Iran\nFull list of author information is\navailable at the end of the article\nAbstract\nBackground: We developed transformer-based deep learning models based on\nnatural language processing for early\nrisk assessment of Alzheimer’s disease from\nthe picture description test.\nMethods: The lack of large datasets poses the most important limitation for\nusing complex models that do not require feature engineering. Tr ansformer-based\npre-trained deep language models have recently made a large lea p in NLP\nresearch and application. These models are pre-trained on ava ilable large datasets\nto understand natural language texts appropriately, and are sh own to\nsubsequently perform well on classiﬁcation tasks with small t raining sets. The\noverall classiﬁcation model is a simple classiﬁer on top of the pre -trained deep\nlanguage model.\nResults: The models are evaluated on picture description test transcri pts of the\nPitt corpus, which contains data of 170 AD patients with 257 intervi ews and 99\nhealthy controls with 243 interviews. The large bidirectional enc oder\nrepresentations from transformers (BERT Large) embedding with logistic regression\nclassiﬁer achieves classiﬁcation accuracy of 88.08%, which improv es the\nstate-of-the-art by 2.48%.\nConclusions: Using pre-trained language models can improve AD prediction.\nThis not only solves the problem of lack of suﬃciently large datasets , but also\nreduces the need for expert-deﬁned features.\nKeywords: Alzheimer’s disease;\nEarly risk assessment ; Picture description test;\nDeep learning; Transformer; Natural language processing; Langu age model;\nTransfer learning1\n2\nRoshanzamir et al. Page 2 of 26\nBackground3\nAlzheimer’s disease (AD) is the most common type of dementia which cu rrently4\ncannot be cured or reversed [ 1]. According to the World Alzheimer Report 2019,5\nthere were over 50 million people living with dementia in the worl d as estimated6\nby Alzheimer’s Disease International (ADI), while the projected e stimates for 20507\nreach above 150 millions [ 2]. The common symptoms of AD include decreased aware-8\nness, disinterest in unfamiliar subjects, increased distracti on, and speech problems9\n[3]. However, if the disease is diagnosed in its early stage, a series of ph armaco-10\nlogical and behavioral therapy approaches can be prescribed to reduce th e pace11\nor progression of the disease symptoms [ 4]. Clinical levels of cognitive impairment12\nare categorized into 7 stages of: normal, normal ageing forgetfulness, mild c ognitive13\nimpairment (MCI), mild AD, moderate AD, moderately severe AD, and se vere AD14\n[5]. In terms of observable linguistic symptoms, in the ﬁrst three stages, the par-15\nticipants need more time to respond and ﬁnd words, or have trouble to m aintain16\nfocus on a conversation. In mild and moderate AD stages, patients have di ﬃculty17\nin understanding and explaining abstract concepts, completing sen tences, and fol-18\nlowing long conversations. In the two most severe stages, patients can not create19\ngrammatically correct sentences, almost lose the ability to understan d words, and20\nﬁnally, become completely mute [ 5] [ 6] [ 7].21\nThe healthcare industry has quickly realized the importance of data and as a22\nresult has started collecting them through a variety of methods such as electronic23\nhealth records (EHR), sensors, and other sources. But analyzing these data and24\nmaking decisions based on them is very time consuming and complicated. A large25\nportion of this data is textual which makes the analysis more challenging. On the26\nother hand, there is a large amount of information and hidden relationships in these27\ndata, and extracting this information is difﬁcult for humans. In this regard, the use28\nof machine learning and natural language processing (NLP) to analyze these data29\nand inference based on the performed analysis has received increased attention.30\nMoreover, according to the recent increasing power of deep learning techniques and31\ntheir ability to extract complex relationships, employing these methods in medical32\ntext mining problems has beenmet with increased interest in recent years. Given33\nthe importance of the impact of AD on speech abilities of the patients, th is study34\nRoshanzamir et al. Page 3 of 26\naims to develop a technique for AD risk assessment from transcripts of targeted35\nspeech elicited from the participants.36\nThe task for acquiring speech data from the patients is the Cookie-Th eft picture37\ndescription test [ 8]. Initially, the test was used as a part of the Boston Diagnostic38\nAphasia Examination [ 8] assessment tool which designed for diagnosing aphasia.39\nCurrently, the test is commonly used by speech-language pathologists t o assess40\nabnormal language production in patients with disorders such as aphasia, AD, right41\nhemisphere lesions, schizophrenia, and etc [ 9]. In this test, an image is shown to the42\nparticipant and he/she is asked to describe what he/she sees in it. Ge nerally, the43\nCookie-Theft image includes a mother washing the dishes in a sink w hile children44\ntry to steal cookies from a cookie jar.45\nUnlike most earlier studies, the features are extracted in our approach by the46\nmodel itself in an unsupervised manner. As a result, more complex f eatures are47\ndiscovered and used for prediction. More precisely, the models are pre-trained on48\na large dataset to learn a good high dimensional (such as 1024 dimensions) ve ctor49\nrepresentation for the input sentence or text, which will be used as input to AD50\nversus healthy control (HC) classiﬁers. Another approach taken in this s tudy to51\naddress the problem of insuﬃciently-sized datasets is text augment ation. Similar to52\nmost related works, the methods are evaluated on the Cookie-Theft pic ture descrip-53\ntion test transcripts of the Pitt corpus [ 10] from the DementiaBank [ 10] dataset.54\nAs mentioned earlier, the overall classiﬁcation framework takes raw in terview text55\nas input. Our evaluation shows that pre-trained deep transformer-bas ed language56\nmodels with a simple logistic regression classiﬁer work well in AD pr ediction and57\nthe results generally outperform those of the existing methods whi le the proposed58\nmethod does not require any hand-crafted features for training the c lassiﬁer.59\nRelated Work60\nFeature-based approaches61\nFor the ﬁrst time, a computational approach to diagnosing Alzheimer’s dis ease using62\nspeech in English was introduced by Bucks et al. [ 11]. In that study, 8 AD and 1663\nHC participants were asked to speak about themselves and their experi ences in 2064\nto 45 minute sessions, and ﬁnally, some speciﬁc questions were also as ked. Then, a65\nnumber of linguistic features such as the noun rate, adjective rate, p ronoun rate, and66\nRoshanzamir et al. Page 4 of 26\nverb rate were extracted from the recorded speech and their distri bution for the AD67\nand control samples were used to train a classiﬁer. Since then, many ot her studies68\nhave been conducted on this topic to improve the accuracy of AD predic tion and69\nstudy the various dimensions of AD (and other types of dementia) eﬀects on speech.70\nIn general, most of these methods propose improvements based on incre asing the71\nnumber of expert-deﬁned features [12, 13] , increasing the number of participants72\n[14] , using acoustic features in addition to linguistic ones [15, 16] , involving AD73\nseverity [14] and other types of dementia scores in classiﬁcation [17] , considering74\nthe impact of AD on other types of diseases [18] , changing the interview’s structure75\n[16] , using linguistic impairment for predicting AD onset [19] , and relating lin-76\nguistic features to neuropsychological tests [19].77\nOne of the most comprehensive studies on this topic was conducted by F raser78\net al. [ 20]. In that study, an extensive categorization of linguistic features was pre-79\nsented, in which linguistic features were categorized into POS (p art-of-speech) tags,80\nsyntactic complexity, grammatical constituents, psycho-linguisti cs, vocabulary rich-81\nness, information content, repetitiveness, and acoustics. Also, the study categorized82\nall diﬀerent kinds of language disorders into the four groups of semantic i mpairment,83\nacoustic abnormality, syntactic impairment, and information impairmen t. The pa-84\nper collected 370 linguistic features from the data and reported the top most 35 of85\nthese features for AD prediction.86\nIn all earlier works, in order to automatically diagnose the disease using speech,87\ninformation content units were introduced by human experts, and a c lassiﬁer used88\nthem in order to predict the participant’s category. However, Yanche va et al. [ 21] and89\nSirts et al. [ 22] tried to enrich and enhance information content units of targeted90\nspeech by clustering pre-trained global vector (GloVe) [ 23] embedding of words91\nused by AD and HC participants. Using the mentioned clusters, they in troduced92\nsome cluster-based measures which were used along with a number of s tandard93\nlexicosyntactic and acoustic features for AD prediction.94\nIn languages other than English, Khodabakhsh et al. [ 24] and Weiner et al. [ 25]95\nrespectively examined the subject in Turkish and German. Also, Li e t al. [ 26] and96\nFraser et al. [ 27] both focused on multilingual approach for diagnosing AD using tar-97\ngeted speech. They respectively tried to improve the AD predic tion in Chinese and98\nRoshanzamir et al. Page 5 of 26\nFrench languages (which the existing datasets were insuﬃcient) us ing an English99\nclassiﬁer trained on a larger English dataset.100\nDeep learning-based approaches101\nFor the ﬁrst time, Orimaye et al. [ 28] used a deep neural network to predict MCI102\nusing speech. Unlike most previous works, that study did not use any h and-crafted103\nfeatures and the raw transcripts were fed to the model. The dataset used in the104\nstudy was part of the Pitt corpus of the DementiaBank dataset, comprisin g 19105\nMCI and 19 control transcripts of the Cookie-Theft picture descript ion test. They106\ntrained a separate deep neural network language model for each category, and t hen107\ncalculated the likelihood of the text in both language models. Finally , the class of108\nthe model with higher probability was selected.109\nKarlekar et al. [ 29] also used a deep neural network model to diagnose AD using110\nfour types of interviews: the Cookie-Theft picture description , sentence construc-111\ntion, story recall, and vocabulary ﬂuency which included an unbalance d 243 HC112\nand 1017 AD transcripts. Three classiﬁers: a convolutional neural network (CNN),113\na long-short term memory recurrent neural network (LSTM-RNN), and a CNN-114\nLSTM were trained, taking sentences as sequences of pre-trained wor d embedding.115\nIn addition to AD diagnosis, the authors interpret the models using act ivation clus-116\ntering and ﬁrst derivative saliency heat map techniques which clu ster the most117\nsigniﬁcant utterances. The research used a highly unbalanced dataset , rendering118\nthe results somewhat questionable as discussed in Section Why not using the entire119\nPitt corpus? .120\nFritsch et al. [ 30] used two diﬀerent auto-regressive LSTM-based neural network121\nlanguage models to classify AD and HC transcripts of the Pitt corpus from th e122\nDementiaBank dataset. After that, Pan et al. [ 31] worked on predicting AD using123\na stacked bidirectional LSTM and gated recurrent unit (GRU) layers eq uipped124\nwith a hierarchical attention mechanism. The overall model takes th e GloVe word125\nembedding sequence as input.126\nMethods127\nIn this study, the most challenging problem in developing data-driven (i.e., ma-128\nchine learning-based) methods for recognizing Alzheimer’s patients from speech129\ntranscripts is the lack of a large dataset. Currently, the largest available dataset130\nRoshanzamir et al. Page 6 of 26\nis the Pitt corpus from the DementiaBank dataset, which contains 500 pic ture131\ndescription interviews from the AD and control groups. For the mentione d reason,132\nmost of the earlier work was based on features designed by experts, as it w as not133\npossible to use models capable of learning informative features, by t hemselves. In134\nthis study, we employ the idea of utilizing a highly pre-trained language model to135\naddress this issue. Moreover, data augmentation techniques are also utilized to al-136\nleviate the small dataset problem. Our implementation of these ideas is described137\nnext.138\nOverall classiﬁcation framework139\nThe overall process of classiﬁcation is summarized in Figure 1. The process consists140\nof ﬁve layers. Each layer uses the output of the previous layer as input. The aug-141\nmenter layer enriches the dataset with methods that will be introduced in Section142\nDataset augmentation . Note that this layer will be disabled in the test phase. The143\nsplitter layer is optional and chooses whether we want to process the whole text at144\nonce or break it down into sentences (and specify the ﬁnal result by aggregating145\nthe results on sentences). It could be disabled by being set to the identity func-146\ntion when we intend to work on the whole transcript. The embedder la yer embeds147\neach input element (i.e. the entire transcript or a sentence) to a high-dimensional148\nrepresentation vector, and the classiﬁer layer predicts the label of each embedded149\ninput. In fact, the classiﬁer layer learns which of (and to what exte nt) the features150\nthat BERT (or other embedders) oﬀers are suitable for predicting Alzheimer’s dis-151\nease. Finally, if the classiﬁer layer outputs multiple labels (th at may happen when152\nworking on sentences), the voter makes the ﬁnal decision using a maj ority vot-153\ning mechanism. A layered architecture makes it much easier to combine different154\nsettings and understand the ﬁnal model.155\nIn our implementation, the augmenter and embedder layers are trained outside156\nthe classiﬁcation framework and are only used there. Therefore, if there is a pre-157\ntrained embedding layer, training and inference will be done very quickly. Details158\non how to train these layers are explained in the following sections.159\nIn this study, depending on the use of splitter layer, two diﬀerent approaches for160\nclassifying a transcript are implemented. In the ﬁrst approach, th e entire transcript161\nis passed to an embedder and then the embedded transcript is direc tly classiﬁed. In162\nRoshanzamir et al. Page 7 of 26\nthis approach (from now on we will call it the text-level approach), the splitter and163\nvoter layers are disabled. In the second approach, the transcript is ﬁ rst split into164\nsentences, and then these sentences are embedded and are subsequ ently classiﬁed.165\nFinally, the label of the entire transcript is decided by majority voting on the labels166\nof all sentences in the transcript. The second approach (from now on we will call167\nit the sentence-level approach) is more compliant with pre-trained embedders since168\nthey are mostly pre-trained on single- or two-sentence inputs.169\nPre-trained deep language model170\nEvery model that deﬁnes a probability distribution over a sequen ce of words is called171\na language model. If a computational model wants to implement a language mod el,172\nit is necessary to have a good understanding of the syntactic and sem antic structures173\nof that language. Therefore, using a model that has already learned a probabi listic174\ndistribution that correlates with these structures for classiﬁcat ion reduces the need175\nfor large target-speciﬁc datasets. The transfer of knowledge from one mod el to176\nanother with a similar purpose is called transfer learning. We use tran sformer-based177\nlanguage models that have oﬀered a breakthrough in many language understandi ng178\ntasks in recent years [ 32]. The general ﬂow of using pre-trained language model for179\nclassiﬁcation task consists of three steps:180\n1 Unsupervised training of the general language model on a large dataset (suc h181\nas Wikitext).182\n2 Unsupervised ﬁne-tuning of the pre-trained language model on the tar get183\ndataset (such as the Cookie-Theft picture description transcript s).184\n3 Using (with or without supervised ﬁne-tuning) the target-speciﬁ c pre-trained185\nlanguage model for the classiﬁcation task.186\nTo address the problems facing recurrent models such as the issue of short-term187\nmemory and the challenges facing the parallelization of training, Vaswani e t al. [ 33]188\nintroduced transformers which consist of an extreme use of the atten tion mechanism189\nthat underpins many NLP models. The paper argues that the attention mech anism190\nallows the model to focus on certain parts of the text for decision maki ng. This func-191\ntionality makes the attention mechanism useful for modeling biomarkers related to192\nAD.193\nRoshanzamir et al. Page 8 of 26\nAl-Rfou et al. [ 34] used transformers for the ﬁrst time as essential elements of194\na character-level language model. After that, Dai et al. [ 35] extended the model195\nusing relative positional encoding and segment-level recurrence . As a turning point196\nin the transformer-based language models, we can refer to the bidirec tional encoder197\nrepresentations from transformers (BERT) model proposed by Devlin et al. [ 36] at198\nGoogle. In the training phase, the input sentence is masked, which means 15% of199\ntokens are replaced with the [MASK] token, and the model tries to lear n such rep-200\nresentation or embedding for the context that considers both syntax and semantics201\nto predict the masked token using the context. On the other hand, in the test phase202\nthe model takes in a raw sentence from one or multiple languages and retu rns a203\n768- or 1024-dimensional vector representation of the input text to be used as input204\nto other classiﬁers such as LR, MLP, etc. An enhanced version of BERT for m ulti-205\nlingual language understanding tasks was introduced by Conneau et al. [ 37], called206\ncross-lingual language model (XLM), which beneﬁts from using the transl ated lan-207\nguage model (TLM) as well as the masked language model (MLM). Unlike BERT,208\nXLM takes two related masked sentences from two diﬀerent languages and tr ies to209\npredict masked tokens using the same and the other language input sent ences. This210\nallows XLM to understand multilingual texts better. Also, BERT suﬀer s from the211\ntrain and test phase discrepancy and independent prediction of maske d tokens. To212\ncorrect this, Yang et al. [ 38] introduced an extended large network (XLNet) model213\nbased on a language model called Permutation Language Model.214\nThe use of multilingual models offers a practical solution to the problem of lack-215\ning of a large dataset in many languages. As there is a limited collection of text216\ndata from Alzheimer’s patients in many languages, training a multilingual model217\nin a source language (in which such large datasets are available) and applying it to218\nmaking inference in the target language can offer a valuable solution. On the other219\nhand, a number of language features that experts introduce are either speciﬁc to a220\nparticular language or their implementation may be different in different languages.221\nUsing multilingual models can also mitigate the need for such transfer of expert222\nfeatures between various languages.223\nIn the current study, we use pre-trained BERT, XLNet, and XLM as deep ne tworks224\nfor text embedding which convert raw participant transcripts / se ntences to 768-225\nor 1024-dimensional vectors. More precisely, to use these language models for the226\nRoshanzamir et al. Page 9 of 26\nembedding layer described in Section Overall classiﬁcation framework , the entire227\ntranscript (in the text-level approach) or sentence (in the sentence-level approach)228\nare passed to the model, and then, the last layer embedding of the ”[CLS]” token229\nis considered as the embedding of the entire input. The embedding models (which230\nare used in this study as an embedder layer) are only passed through Phas es 1231\nand 3 of the ﬂow described earlier in this section. The reason for this is that the232\nemployed dataset is insuﬃcient for unsupervised ﬁne-tuning (of language models on233\nthe target dataset) even when using vast augmentation methods. In practice, usi ng234\nunsupervised ﬁne-tuning is likely to have minimal impact on the overall performance235\nof the model used in the current research (the effect of this feature on the re-236\nsults of the implemented model with the best performance is presented in Section237\nEvaluation results ). For the ﬁrst phase, all embedding models are pre-trained with238\nthe corpus mentioned in the main article, and their implementation i s taken from239\nthe HuggingFace transformers library [ 39].240\nDataset augmentation241\nAnother approach to overcome the lack of access to large training input is d ataset242\naugmentation which means increasing the number of labeled samples of th e dataset243\nusing some probabilistic or even heuristic algorithms. For example, t he word “beau-244\ntiful” in a sentence such as “What a beautiful car!”can be replaced with the word245\n“nice” without changing the meaning of the sentence a lot. Augmentation in NLP246\ncan be done at different levels of linguistic units, and in this study, the word and247\nsentence level augmentations are used for enriching the dataset. The most crucial248\nchallenge of augmentation in the text classiﬁcation task is preserving t he text class249\nduring augmentation. For example, a probabilistic model can replace “beautiful”250\nwith “dirty” in the mentioned sentence, which is grammatically and semantically251\ncorrect but changes the sentence category. Two general approaches to augme ntation252\nhave been used in this study, which are described below.253\nSimilar word substitution augmentation254\nIn this approach, a similarity measure must ﬁrst be deﬁned. The most obvious def-255\ninition of similarity for words is the synonym relation which was ﬁrst u sed in the256\nﬁeld of deep learning by Zhang et al. [ 40] using the WordNet database [ 41]. An-257\nother common similarity measure is the inverse of the Euclidean dist ance or the258\nRoshanzamir et al. Page 10 of 26\nCosine similarity between word embeddings which was ﬁrst used by W ang et al.259\n[42]. In the mentioned methods, there is no guarantee of the correct gramm ar in260\nthe output sentence. It is also possible that the output sentence c ategory changes261\nby augmentation. For example, one of the markers of Alzheimer’s disease is the re-262\nduction in the vocabulary used in the conversation, so replacing a si mple word like263\n“Delicious” with its sophisticated synonym like “Scrumptious” can change the sen-264\ntence category from patient to healthy and mislead the classiﬁer. Anothe r method265\nthat considers grammatical correctness along with the sentence contex t was intro-266\nduced by Kobayashi [ 43] and is called contextual augmentation. In the contextual267\naugmentation method, there is a language model which takes both the word’ s con-268\ntext (i.e. the sentence that contains the word) and the whole senten ce’s category269\nand returns a probability distribution over all vocabulary. Augmentat ion is done by270\nsampling from the returned probability distribution. Kobayashi [ 43] trained a Bi-271\nDirectional LSTM language model with this approach, and Wu et al. [ 44] enhanced272\nthe approach by using BERT as an underlying model.273\nAll the mentioned methods were evaluated in this study, and the imp lementation274\nwas done using the NLPAug library [ 45] except for contextual augmentation for275\nwhich the released code by the authors of [ 43] was used.276\nSentence removal augmentation277\nAnother ad-hoc approach which does not change the sentence category and also278\nretains grammatical correctness is sentence removal. In this approach , one sentence279\nis removed from the transcript, and it is expected that the output i s still a valid280\ntranscript in the same category. Although it can be argued that the label may be281\nchanged by reducing the length of the text, considering the result s of using or not282\nusing this idea, it is appropriate to use it in models that process t he entire text at283\nonce (not sentence by sentence).284\nBaseline models285\nIn this study, in addition to the transformer-based models, bidir ectional-LSTM and286\nconvolutional neural networks over the GloVe word embedding were also evaluated287\nas baseline models to illustrate the advantages of pre-trained transfor mer-based deep288\nlanguage models over conventional deep models. In these models, the entire tran-289\nscript is used as input. The reason for this decision is that unlike pre-trained mod-290\nRoshanzamir et al. Page 11 of 26\nels, there is no pre-training on single-sentence (or maximum two-sentence) texts and291\nhence their training has to be done from the beginning. Therefore, splitting the tran-292\nscript into sentences will not improve the performance of these models. In the CNN293\nmodel, each transcript (truncated or padded to T number of words) is converted to294\na sequence of embedded words. Then the sequence is passed to a num ber of stacked295\nconvolutional and max-pooling layers followed by fully-connected la yers and ﬁnally296\na sigmoid output layer that yields P (AD|transcript). Also, in the bidirectional-297\nLSTM model, the embedded word sequence is passed to a number of stac ked forward298\nand backward LSTM cells followed by fully-connected layers and a sigm oid output299\nlayer in a similar fashion. Structurally, if we move forward in the C NN layers, the300\nmodel tries to conclude more semantic features using spatially clos e features in the301\nprevious layer. But in the LSTM model that considers long range depen dencies,302\nan attempt is made to learn new compound features from features of all prev ious303\nsteps (or from features of the whole sequence in the bidirectional LST M). The main304\nweakness of this model is the forgetting of distant features (spatiall y) to produce305\nnew compound features. In both of these models, there is no attention mechanism.306\nExperimental setup307\nIn this section, we describe our implemented methods and their corresponding set-308\ntings in the training and evaluation phases.309\nImplemented methods310\nFor each layer of the overall framework, there were several options from which311\nthe following were implemented. For the augmenter layer, synonym-substitution312\nand contextual augmentation were implemented along with ad-hoc sentence re-313\nmoval augmentation. As implemented by Kobayashi et al. [43] , the corresponding314\nlanguage model used in contextual augmentation was a single layer bidirectional315\nLSTM. For the splitter layer, in addition to the identity function, the sentence split-316\nter was implemented for the sentence-level approach. For the pre-trained embedder317\nlayer, BERT (base and large), XLNet (base and large), XLM, and the GloVe word318\nembedding sequence (50-dimensional version) were investigated. For the classiﬁer319\nlayer, logistic regression, single hidden layer neural network, single-layer bidirec-320\ntional LSTM, and three-layer CNN were examined. Finally, for the voter layer, in321\naddition to the identity function, majority voting, and a single-layer bidirectional322\nRoshanzamir et al. Page 12 of 26\nLSTM were implemented for the sentence-level approach. Although different com-323\nbinations of layers were implemented, only significant cases of each group have been324\nreported in Section Evaluation results .325\nTraining settings326\nFor the contextual augmentation, as implemented by Kobayashi et al. [43] , the327\ncross-entropy loss function and the Adam optimizer was used. The number of aug-328\nmentations per transcript is a hyper-parameter for the augmentation layer. For the329\npre-trained embedding layer, only the HuggingFace transformers library [39] was330\nused and no additional training was done on the implemented models. For the clas-331\nsiﬁcation layer, binary cross-entropy was employed for the loss function and the332\nAdam optimizer was used to minimize it. For the voter layer, only bidirectional333\nLSTM was trainable for which, again the binary cross-entropy loss function and the334\nAdam optimizer were utilized. All models were evaluated using 10-fold cross-vali-335\ndation without stratiﬁed sampling.336\nResults337\nDataset338\nThe models are evaluated on the transcripts of the Cookie-Theft pict ure descrip-339\ntion test of the Pitt corpus from the DementiaBank dataset, which contai ns 170340\npossible or probable AD patients with 257 interviews and 99 healthy control (HC)341\nparticipants with 243 interviews.342\nMost of the data were gathered as a part of the Alzheimer’s and related deme ntias343\nstudy at the University of Pittsburgh School of Medicine between 1983 an d 1988.344\nThe interviewer shows the participant the Cookie-Theft pictur e and asks him/her to345\nstate everything he/she sees in it. The audio records of all intervi ews were manually346\ntranscribed and annotated with POS-tags in the CHAT [ 46] format.347\nDetailed demographics of the data is speciﬁed in Table 1.348\nWhy not using the entire Pitt corpus?349\nSome earlier studies based on the Pitt corpus (such as Kerlekar et al. [ 29]) used all350\nthe tests of the corpus including the Cookie-Theft picture desc ription, story recall,351\nsentence construction, and categorical/verbal ﬂuency for classiﬁcation purposes.352\nThe ﬁrst problem with using the entire corpus is that the corpus is highly unbalanced353\nRoshanzamir et al. Page 13 of 26\n(note that Table 1 only provides demographics of the Cookie-Theft picture de-354\nscription test from the Pitt corpus, which is perfectly balanced, although the whole355\ndataset is unbalanced), and as a result, a na¨ ıve classiﬁer that always outputs AD356\nlabels can achieve a classiﬁcation accuracy of 80% on such a dataset.357\nThe second problem is that except for the Cookie-Theft picture des cription test,358\nthe Pitt corpus was only administered to AD subjects for all the other tests, which359\nmeans that the classiﬁer might learn invalid features for AD prediction. For example,360\na classiﬁer may just output an AD label by checking if the input is not from the361\nCookie-Theft picture description test, and otherwise, work as norm al. Using this362\napproach, a normal classiﬁer with 80% accuracy can achieve approximately 92%363\naccuracy on the whole Pitt corpus. Figure 2 provides an example of this problem.364\nThe ﬁgure shows visualized two-dimensional tSNE [ 47] diagram for the BERT Base365\nembedding of the entire transcripts of all tests in the Pitt corpus . According to366\nthe ﬁgure, the tests are completely diﬀerentiable, and as a result, the mentioned367\nproblem is quite probable to arise. Thus, in Section Results, studies based on the368\nentire corpus were not included.369\nEvaluation measures370\nThe most well-known measure to evaluate classiﬁcation is the accuracy score which371\nis the fraction of predictions the model performed correctly. Most r elated studies372\nhave reported accuracy as the quality of their classiﬁcation models and tried to373\nimprove this measure as an important goal. As discussed in the previous s ection,374\nthe accuracy measure alone does not provide a complete interpretation of the model375\nperformance (for example, high accuracy can be achieved using the enti re Pitt cor-376\npus, while the model performance is not suﬃcient for practical use) . Two other377\npractical measures are precision and recall (also called sensitivity ). In this study,378\nprecision is the number of correct AD predicted samples over the tot al number of379\nAD predicted samples and recall is the number of correct AD predicted samples over380\nthe total number of AD samples. These two measures should be examined t ogether381\nand for this reason, the F 1 score is deﬁned. The F 1 score is the harmonic mean of382\nthe precision and recall measures. A combined high precision and recal l results in a383\nhigh F 1 score. In other words, highly imbalanced precision and recall indicate s that384\nthe model has not an approximately equal performance for detecting all l abels. All385\nRoshanzamir et al. Page 14 of 26\nthe aforementioned measures are in the range of zero to one, and can be repor ted386\nas a percentage. Compared to the accuracy score, fewer previous studi es have re-387\nported recall, precision, and F 1 measures. In this study, all the introduced measures388\nare reported to make it possible to compare our work more comprehensivel y with389\nprevious works.390\nCompared methods391\nWe compared the results of our models with all related studies that ev aluated their392\nmodels on the Cookie-Theft picture description test of the Pitt c orpus. Therefore,393\nthe best models (according to the introduced performance measure s) are selected394\nfor comparison. The ﬁrst one is the method introduced in [ 20] which maintained395\nthe status of having the state-of-the-art accuracy score for several ye ars. The sec-396\nond compared method was introduced by Yancheva et al. [ 21]. They tried to enrich397\nand enhance human-supplied information content units by clusterin g GloVe em-398\nbedding of frequent words of each category. After that, Sirts et al. [ 22] extended399\nthe idea of Yancheva et al. [ 21] by introducing propositional idea density features400\nthat work better on free-topic conversational speech. Hern´ andez et al. [48] intro-401\nduced 105 hand-crafted features and used them to train a support vector machine402\n(SVM) classiﬁer. They reported all the well-known and informative m easures for403\nthe classiﬁcation tasks and also achieved good results. Fritsch et al. [30] trained two404\ndiﬀerent auto-regressive LSTM-based language models for each group and clas siﬁed405\neach transcript by calculating its perplexity on the models and sel ecting the model406\ncorresponding to the lowest perplexity. Currently, that study has the best recall and407\naccuracy scores for AD versus HC classiﬁcation on the target dataset. Pan et al.408\n[31] utilized a stacked bidirectional LSTM and GRU recurrent units eq uipped with409\na hierarchical attention mechanism. Up to now, this study has the bes t precision410\nand F 1 scores for AD versus HC classiﬁcation on the target dataset. The last two411\nstudies by Li et al. [ 26] and Fraser et al. [ 27] were focused on multilingual AD pre-412\ndiction and hence their main goal was not to improve the unilingual classi ﬁcation.413\nLi et al. [ 26] used 185 lexicosyntactic features for a logistic regression classiﬁer and414\nFraser et al. [ 27] utilized class-based language modeling and information-theoretic415\nfeatures for an SVM classiﬁer.416\nRoshanzamir et al. Page 15 of 26\nEvaluation results417\nTable 3 reports precision, recall, accuracy, and F 1 scores of the compared methods418\nas well as those of the proposed methods in the framework introduced in this paper.419\nThe reported scores are averaged on a 10-fold cross-validation (without stratiﬁed420\nsampling) procedure. Note that for the Fritsch et al. [ 30] method there is no such421\nentity as a classiﬁer and classiﬁcation was performed by evaluating per plexity of422\ninput transcripts on the trained language models of both classes. As ment ioned423\nearlier, two diﬀerent approaches have been implemented to use th e pre-trained em-424\nbedders, the ﬁrst one is passing the entire text to the embedder (speciﬁed by a425\nT- preﬁx in the method’s name) and the second one is passing each sent ence of426\nthe text to the embedder separately (speciﬁed by an S- preﬁx in th e method’s427\nname). All the methods with the ﬁrst approach have been enriched by the one-428\nsentence-removal augmentation method. Furthermore, the CNN metho d is used429\nwith the synonym substitution augmentation (SSA) method and the BiLST M is430\nused with the SSA and contextual augmentation (CA) methods separately. The431\nCA and SSA augmentations had almost no eﬀect on the methods which used pr e-432\ntrained language models, so they are not reported in Table 3. Also, for the model433\nwith the best accuracy score (S-BERTLarge-LR) , two additional versions with bidi-434\nrectional LSTM classiﬁer (S-BERTLarge-BiLSTM) and bidirectional LSTM voter435\n(S-BERTLarge-LR-BiLSTM) are included.436\nAs mentioned before, it seems that using unsupervised ﬁne-tuning (using the437\nMLM objective and next sentence prediction) on the Cookie-Theft picture descrip-438\ntion transcripts of the Pitt corpus does not have much effect on the results due to439\nthe lack of sufﬁcient data for the target task. According to the experiments per-440\nformed, using unsupervised ﬁne-tuning for the model with the best accuracy score441\n(S-BERT Large -LR) in equivalent settings on average results in the accuracy and442\nF1 scores of 87.89% and 86.11%, which are almost no different from the scores of443\na version without this feature (note that due to the fundamental differences of this444\napproach with other models, we did not include it in Table 3 ).445\nMoreover, Figure 3 illustrates the mean 10-fold cross-validation classiﬁcation ac-446\ncuracy, true positive rate (the number of correct predicted AD samp les over total447\nnumber of AD samples, also called the sensitivity), and true negative rate (the num-448\nber of correct predicted HC samples per total number of HC samples, also c alled the449\nRoshanzamir et al. Page 16 of 26\nspeciﬁcity) plotted versus the mini-mental state exam (MMSE) [ 49] scores of the450\nparticipants. The ﬁgure helps us to see how the model works for dete cting label of451\nparticipants with diﬀerent AD severity levels. The true positiv e rate for each MMSE452\nscore represents the model performance in detecting AD from actual AD patients453\nin that score. Similarly, the true negative rate represents the mod el performance in454\ndetecting HC label from actual HC participants in that score. Totally, the accuracy455\nscore represents the model performance in detecting the correct label from both456\nparticipant groups in the corresponding MMSE score. Numbers in the pink bars457\nare true positive rates and in the blue bars are true negative rates. Also, the num-458\nbers on top of the bars are the total mean accuracy for that MMSE score. Note that459\nall of the rates are scaled between 0 and 1. The MMSE scores were not report ed460\nin the dataset for some participants while their AD / HC labels were pres ent. The461\nresults for these participants are grouped in the ”Unspeciﬁed” bar in t his ﬁgure.462\nIn addition to classiﬁcation, models such as logistic regression and neu ral networks463\nwith a sigmoidal ﬁnal activation function can also output the AD probability (or 1 -464\nhealth probability) of the current input. Referring to the contin uity of linguistic im-465\npairments from perfect health to severe AD, this probability can be interpreted as a466\ncorrelated variable to the severity of the AD condition of the participan t. Therefore,467\nanother approach for interpreting the models and evaluating them is cal culating the468\nsimilarity between their predicted health probability and the MM SE score, scaled469\nbetween 0 and 1. The results using two common similarity measures, t he Pearson470\ncorrelation and Spearman’s rank correlation (which is the Pearson correlat ion on471\nthe samples’ ranking), are reported in Table 2. Both mentioned correlation measures472\nare reported between -1 and 1.473\nDiscussion474\nInterpretation of results475\nAccording to Table 3, among the models that use only hand-crafted features, Fraser476\net al. [ 20] reports the best accuracy score, although it has not reported other evalu -477\nation measures. Among the baseline models introduced in our study (C NN + SSA,478\nBiLSTM + SSA, and BiLSTM + CA), which are conventional deep neural net-479\nwork models, the contextual augmented version of bidirectional-LSTM ac hieved the480\nhighest accuracy score of 77.36%. However, even with the extreme use of augm en-481\nRoshanzamir et al. Page 17 of 26\ntation methods these baseline models did not yield acceptable resu lts compared482\nto other methods. Overall, the sentence-level BERT Large embedding of sentences483\npassed to logistic regression (S-BERT Large-LR method) achieved the highest ac-484\ncuracy score (88.08%) among all the models introduced in this study as we ll as485\nthe models used in previous studies, and improved the accuracy sc ore by 2.48%486\n(equivalently 17.22% error-rate reduction). At the same time, this mo del achieved487\nthe best precision and F 1 scores with 6.55% and 2.80% improvements, respectively.488\nStill, Fritsch et al. [ 30] showed the best recall score with 1.66% diﬀerence although489\nthey did not report the F 1 measure. The ﬁrst advantage of our proposed methods490\ncompared to Fritsch et al. [ 30] is that we train a single language model for both491\nthe AD and HC groups which helps the model to use samples from both class es for492\nthe desired task. The other advantage is that our models are highly pre-t rained on493\nlarge datasets which enables them to start training on new, smaller datas ets with494\ngood initialization parameters and also avoid overﬁtting.495\nAmong the methods evaluated in this study, on average, the models base d on the496\nBERT family of embedders worked better than the others. Although XLNet has497\nhistorically been designed to address BERT problems, BERT and its d erivatives still498\nperform better in many activities[ 32]. Moreover, employing word-level augmenta-499\ntion techniques along with pre-trained deep language models did not improve results500\n(and hence the evaluation of their versions with augmentation was not repor ted in501\nTable 3).502\nTable 2 shows that the best model has a Pearson correlation of 0.78 and 0.70 for503\nthe train and validation phases, and a Spearman’s rank correlation of 0.81 and 0.74504\nfor these phases between the health score and the MMSE score, indicat ing that the505\nmodel has learned useful patterns for classiﬁcation. Based on the reported similarity506\nmeasures, it can be concluded that on average the MMSE score and our model ’s507\nhealth score are linearly correlated. This is indeed an advantage for the p roposed508\nmodel in that while the MMSE score [ 49] is obtained through a detailed interactive509\nexam that evaluates visuospatial, executive, naming, memory, attention , language,510\nabstraction, delayed recall, and orientation cognitive skills, the dat a collection task511\ninvolved in the Cookie-Theft picture description test used in ou r model is a simple512\nand short pseudo-conversational procedure. Interestingly, the results obtained in513\nthis section are related to the reported results in the work of Eyigoz et al [19] .514\nRoshanzamir et al. Page 18 of 26\nThe objective of that study was to use linguistic markers to predict the onset515\nof Alzheimer’s disease in cognitively normal individuals. The study’s experiments516\nshowed that the stated goal is possible to achieve and, in fact, using models based on517\nlinguistic variables performed better than a predictive model based on non-linguis-518\ntic variables (such as neuropsychological test results, age, gender, APOE ε4 alleles,519\netc.). The results of this section show that the severity of linguistic impairments is520\nhighly correlated with the estimates based on non-linguistic variables (corroborating521\nthe results reported by Eyigoz et al. [19]).522\nIn this study, neural network interpretation methods were not use d but in Table 4,523\ntwo false negative and false positive classiﬁcation errors are reported. In comparison,524\nit is almost clear that the ﬁrst sample has less grammatical ﬂuency but bot h samples525\nrefer to similar information elements. In the S-BERT Large-LR model, the predicted526\nAD probability is the mean of logistic regression classiﬁer outputs for eac h sentence527\nof the transcript. The important point is that in both samples, the pre dicted AD528\nprobabilities are very close to 0.5 which can be interpreted as that th e model has529\nnot learned a wrong feature, rather, it has not learned a proper feature t o predict530\nAD from the reported samples.531\nAdvantages and limitations532\nAs mentioned in Section Pre-trained deep language model , the proposed approach533\ntakes advantage of the powerful pre-trained language models that attempt t o learn534\nthe structure and features of the language from a large dataset, and only uses the535\ntarget dataset to learn how to use these features for AD prediction. Thi s not only536\nreduces the need for expert-deﬁned language features, but also makes it possible537\nfor more complex features to be extracted from the data. The next advantage of538\nsentence embedding models is that they consider the entire raw text and there is539\nno out-of-context word embedding layer that would convert each word to a repre-540\nsentation vector without considering its context.541\nAs mentioned earlier, even using augmentation methods, the largest cu rrently542\navailable dataset for AD prediction is still insuﬃcient in size for un supervised ﬁne-543\ntuning (Second phase speciﬁed in Section Pre-trained deep language model ) large544\ntransformer-based language models (e.g., BERT Large has 340 million parameters).545\nBut if there is a large enough dataset, using language model ﬁne-tuning, ou r ap-546\nRoshanzamir et al. Page 19 of 26\nproach can extract more complex and context-related features while the models547\nbased on expert-deﬁned features can only choose from a limited set of p redeﬁned548\nfeatures.549\nThe most important limitation of the current study that needs to be add ressed550\nin the future is that it is diﬃcult to use common neural network inte rpretation551\nmethods due to the large number of model parameters. Using interpre tation, we552\ncan understand why the model predicts a wrong label for a transcrip t. Also, in the553\ncase of a correct prediction, we can identify language features that the n etwork has554\npaid more attention to. This is particularly useful for studying Alzhe imer’s disease555\nas such interpretation can reveal important attributes of the speech w hich can most556\neﬀectively discriminate between the participant groups.557\nAlthough using deep embedding models instead of expert (linguistic) features can558\nimprove the performance by extracting more complex relationships, it does not559\nprovide clear features tied to clinical practice that can be validated easily as op-560\nposed to expert features. In this regard, a suggested solution is to use interpretation561\ntechniques. But the training phase must also be conducted in such a way that the562\nextracted features are both interpretable and relatively sparse so that they could563\nbe validated clinically.564\nFuture work565\nOne of the most popular types of transformer-based language models is the c lass of566\nmultilingual models. With a proper use of multilingual models, sim ilar to approaches567\nby Li et al. [ 26] and Fraser et al. [ 27], the problem of lacking access to a large dataset568\nin one language can be addressed by transferring the knowledge of AD predi ction569\nfrom another language in which a large dataset is available. Using such transf er, the570\nneed to deﬁne linguistic features by experts in the target language is also addressed.571\nIn future work, we aim to improve multilingual AD prediction using p re-trained572\nmultilingual transformer-based language models along with cross-lingual transfer573\nlearning.574\nConclusions575\nAccording to the results of earlier studies, Alzheimer’s disease aﬀects speech in the576\nform of syntactic, semantic, information, and acoustic impairments. We employed577\na transfer-learning approach to improve automatic AD prediction using a relatively578\nRoshanzamir et al. Page 20 of 26\nsmall targeted speech dataset without using the expert-deﬁned lin guistic features.579\nWe evaluated recently developed pre-trained transformer-based lan guage models580\nthat we enriched with augmentation methods on the Cookie-Theft pict ure descrip-581\ntion test of the Pitt corpus. Using sentence level BERT Large with a simple logistic582\nregression classiﬁer, the accuracy and F 1 scores of 88.08% and 87.23% were achieved583\nwhich improved the state-of-the-art results by 2.28% and 2.80%, respec tively. Pre-584\ntrained language models are available in many languages. Hence, the approach i n585\nthis paper can be examined in languages other than English as well. Also, wit h the586\nmultilingual versions of these models, the knowledge of AD predicti on in one lan-587\nguage can be transferred to another language in which a suﬃciently large datas et588\ndoes not exist.589\nAbbreviations590\nAD: Alzheimer’s Disease; ADI: Alzheimer’s Disease Interna tional; Bidirectional Encoder Representations from591\nTransformers (BERT); CA: Contextual Augmentation; CNN: Conv olutional Neural Network; XLM: Cross-lingual592\nLanguage Model; XLNet: Extended Large Network; GRU: Gated Rec urrent Unit; GloVe: Global Vector; HC: Healthy593\nControl; LSTM: Long-short Term Memory; MLM: Masked Languag e Model; MCI: Mild Cognitive Impairment;594\nMMSE: Mini-mental State Exam; NLP: Natural Language Processi ng; POS: Part-of-speech; RNN: Recurrent Neural595\nNetwork; TLM: Translated Language Model; SVM: Support Vecto r Machine; SSA: Synonym Substitution596\nAugmentation;597\nDeclarations598\nEthics approval and consent to participate599\nNot applicable.600\nConsent for publication601\nNot applicable.602\nAvailability of data and materials603\nThe data (Pitt corpus from DementiaBank dataset) that suppo rt the ﬁndings of this study are available from604\nTalkBank project but restrictions apply to the availabilit y of these data, which were used under license for the605\ncurrent study, and so are not publicly available. Data are ho wever available from the authors upon reasonable606\nrequest and with permission of TalkBank project owners.607\nCompeting interests608\nThe authors declare that they have no competing interests.609\nFunding610\nNot applicable.611\nAuthor’s contributions612\nAR and MSB analyzed the data. HA conceptualized the work. All a uthors wrote, edited, and approved the613\nmanuscript.614\nAcknowledgements615\nNot applicable.616\nRoshanzamir et al. Page 21 of 26\nAuthor details617\n1Department of Computer Engineering, Sharif University of T echnology, Azadi, Tehran, Iran. 2Department of618\nElectrical Engineering, Sharif University of Technology, Azadi, Tehran, Iran. 3Department of Computer Engineering,619\nSharif University of Technology, Azadi, 11365-11155 Tehra n, Iran.620\nReferences621\n1. Glenner, G.G.: Alzheimers disease. Biomedical Advances in Aging, 51–62 (1990)622\n2. International, A.D.: World Alzheimer Report 2019: Attit udes to dementia. Alzheimer’s Disease Internationals623\nLondon (2019)624\n3. Blanken, G., Dittmann, J., Haas, J.-C., Wallesch, C.-W.: S pontaneous speech in senile dementia and aphasia:625\nImplications for a neurolinguistic model of language produ ction. Cognition 27(3), 247–274 (1987)626\n4. Sperling, R.A., Aisen, P.S., Beckett, L.A., Bennett, D.A ., Craft, S., Fagan, A.M., Iwatsubo, T., Jack Jr, C.R.,627\nKaye, J., Montine, T.J., et al.: Toward deﬁning the preclinical stages of alzheimer’s dise ase: Recommendations628\nfrom the national institute on aging-alzheimer’s associat ion workgroups on diagnostic guidelines for alzheimer’s629\ndisease. Alzheimer’s & dementia 7(3), 280–292 (2011)630\n5. Reisberg, B., Sclan, S., Franssen, E., DeLeon, M., Kluger , A., Torossian, C., Shulman, E., Steinberg, G.,631\nMonteiro, I., McRae, T., et al.: Clinical stages of normal aging and alzheimers-disease-t he gds staging system.632\nNeuroscience Research Communications 13, 51–54 (1993)633\n6. Mace, N.L., Rabins, P.V.: The 36-hour Day: A Family Guide to Caring for People Who Have Alzheimer634\nDisease, Related Dementias, and Memory Loss. JHU Press, ??? ( 2011)635\n7. Ostuni, E., Santo Pietro, M.J.C.: Getting Through: Commu nicating When Someone You Care for Has636\nAlzheimer’s Disease. Speech Bin, ??? (1986)637\n8. Goodglass, H., Kaplan, E.: The assessment of aphasia and re lated disorders, 2nd edn lea & febiger:638\nPhiladelphia. Dictionary of Biological Psychology 230 (1983)639\n9. Mackenzie, C., Brady, M., Norrie, J., Poedjianto, N.: Pictu re description in neurologically normal adults:640\nConcepts and topic coherence. Aphasiology 21(3-4), 340–354 (2007)641\n10. Becker, J.T., Boiler, F., Lopez, O.L., Saxton, J., McGon igle, K.L.: The natural history of alzheimer’s disease:642\ndescription of study cohort and accuracy of diagnosis. Arch ives of Neurology 51(6), 585–594 (1994)643\n11. Bucks, R.S., Singh, S., Cuerden, J.M., Wilcock, G.K.: An alysis of spontaneous, conversational speech in644\ndementia of alzheimer type: Evaluation of an objective tech nique for analysing lexical performance. Aphasiology645\n14(1), 71–91 (2000)646\n12. Guinn, C.I., Habash, A.: Language analysis of speakers wi th dementia of the alzheimer’s type. In: 2012 AAAI647\nFall Symposium Series (2012)648\n13. Orimaye, S.O., Wong, J.S.-M., Golden, K.J.: Learning pr edictive linguistic features for alzheimer’s disease and649\nrelated dementias using verbal utterances. In: Proceeding s of the Workshop on Computational Linguistics and650\nClinical Psychology: From Linguistic Signal to Clinical Re ality, pp. 78–87 (2014)651\n14. Thomas, C., Keselj, V., Cercone, N., Rockwood, K., Asp, E. : Automatic detection and rating of dementia of652\nalzheimer type through lexical analysis of spontaneous spe ech. In: IEEE International Conference Mechatronics653\nand Automation, 2005, vol. 3, pp. 1569–1574 (2005). IEEE654\n15. Meil´ an, J.J.G., Mart ´ ınez-S´ anchez, F., Carro, J., L´opez, D.E., Millian-Morell, L., Arana, J.M.: Speech in655\nalzheimer’s disease: Can temporal and acoustic parameters discriminate dementia? Dementia and Geriatric656\nCognitive Disorders 37(5-6), 327–334 (2014)657\n16. K¨ onig, A., Satt, A., Sorin, A., Hoory, R., Toledo-Ronen, O., Derreumaux, A., Manera, V., Verhey, F., Aalten, P.,658\nRobert, P.H., et al.: Automatic speech analysis for the assessment of patients w ith predementia and alzheimer’s659\ndisease. Alzheimer’s & Dementia: Diagnosis, Assessment & D isease Monitoring 1(1), 112–124 (2015)660\n17. Jarrold, W., Peintner, B., Wilkins, D., Vergryi, D., Ric hey, C., Gorno-Tempini, M.L., Ogar, J.: Aided diagnosis661\nof dementia type through computer-based analysis of sponta neous speech. In: Proceedings of the Workshop on662\nComputational Linguistics and Clinical Psychology: From L inguistic Signal to Clinical Reality, pp. 27–37 (2014)663\n18. Rentoumi, V., Raouﬁan, L., Ahmed, S., de Jager, C.A., Gar rard, P.: Features and machine learning664\nclassiﬁcation of connected speech samples from patients wi th autopsy proven alzheimer’s disease with and665\nwithout additional vascular pathology. Journal of Alzheim er’s Disease 42(s3), 3–17 (2014)666\n19. Eyigoz, E., Mathur, S., Santamaria, M., Cecchi, G., Naylo r, M.: Linguistic markers predict onset of alzheimer’s667\nRoshanzamir et al. Page 22 of 26\ndisease. EClinicalMedicine, 100583 (2020)668\n20. Fraser, K.C., Meltzer, J.A., Rudzicz, F.: Linguistic fe atures identify alzheimer’s disease in narrative speech.669\nJournal of Alzheimer’s Disease 49(2), 407–422 (2016)670\n21. Yancheva, M., Rudzicz, F.: Vector-space topic models fo r detecting alzheimer’s disease. In: Proceedings of the671\n54th Annual Meeting of the Association for Computational Li nguistics (Volume 1: Long Papers), pp.672\n2337–2346 (2016)673\n22. Sirts, K., Piguet, O., Johnson, M.: Idea density for pred icting alzheimer’s disease from transcribed speech. arXiv674\npreprint arXiv:1706.04473 (2017)675\n23. Pennington, J., Socher, R., Manning, C.D.: Glove: Globa l vectors for word representation. In: Proceedings of676\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543 (2014)677\n24. Khodabakhsh, A., Demiroglu, C.: Analysis of speech-bas ed measures for detecting and monitoring alzheimer’s678\ndisease. In: Data Mining in Clinical Medicine, pp. 159–173. Springer, ??? (2015)679\n25. Weiner, J., Herﬀ, C., Schultz, T.: Speech-based detectio n of alzheimer’s disease in conversational german. In:680\nINTERSPEECH, pp. 1938–1942 (2016)681\n26. Li, B., Hsu, Y.-T., Rudzicz, F.: Detecting dementia in man darin chinese using transfer learning from a parallel682\ncorpus. arXiv preprint arXiv:1903.00933 (2019)683\n27. Fraser, K.C., Linz, N., Li, B., Fors, K.L., Rudzicz, F., K¨ onig, A., Alexandersson, J., Robert, P., Kokkinakis, D.:684\nMultilingual prediction of alzheimer’s disease through do main adaptation and concept-based language685\nmodelling. In: Proceedings of the 2019 Conference of the Nort h American Chapter of the Association for686\nComputational Linguistics: Human Language Technologies, V olume 1 (Long and Short Papers), pp. 3659–3670687\n(2019)688\n28. Orimaye, S.O., Wong, J.S.-M., Fernandez, J.S.G.: Deep- deep neural network language models for predicting689\nmild cognitive impairment. In: BAI@ IJCAI, pp. 14–20 (2016)690\n29. Karlekar, S., Niu, T., Bansal, M.: Detecting linguistic c haracteristics of Alzheimer’s dementia by interpreting691\nneural models. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for692\nComputational Linguistics: Human Language Technologies, V olume 2 (Short Papers), pp. 701–707. Association693\nfor Computational Linguistics, New Orleans, Louisiana (201 8)694\n30. Fritsch, J., Wankerl, S., N¨ oth, E.: Automatic diagnosis of alzheimer’s disease using neural network language695\nmodels. In: ICASSP 2019-2019 IEEE International Conferenc e on Acoustics, Speech and Signal Processing696\n(ICASSP), pp. 5841–5845 (2019). IEEE697\n31. Pan, Y., Mirheidari, B., Reuber, M., Venneri, A., Blackb urn, D., Christensen, H.: Automatic hierarchical698\nattention neural network for detecting ad. Proc. Interspee ch 2019, 4105–4109 (2019)699\n32. GLUE Benchmark. https://gluebenchmark.com/leaderbo ard. (Accessed on 03/14/2020)700\n33. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones , L., Gomez, A.N., Kaiser, /suppress L., Polosukhin, I.:701\nAttention is all you need. In: Advances in Neural Information Processing Systems, pp. 5998–6008 (2017)702\n34. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Ch aracter-level language modeling with deeper703\nself-attention. In: Proceedings of the AAAI Conference on A rtiﬁcial Intelligence, vol. 33, pp. 3159–3166 (2019)704\n35. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Sala khutdinov, R.: Transformer-xl: Attentive language705\nmodels beyond a ﬁxed-length context. arXiv preprint arXiv: 1901.02860 (2019)706\n36. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: P re-training of deep bidirectional transformers for707\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018)708\n37. Conneau, A., Lample, G.: Cross-lingual language model p retraining. In: Advances in Neural Information709\nProcessing Systems, pp. 7057–7067 (2019)710\n38. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdino v, R.R., Le, Q.V.: Xlnet: Generalized autoregressive711\npretraining for language understanding. In: Advances in Neu ral Information Processing Systems, pp. 5754–5764712\n(2019)713\n39. Transformers — transformers 3.3.0 documentation. https://huggingface.co/transformers/index.html.714\n(Accessed on 09/29/2020)715\n40. Zhang, X., Zhao, J., LeCun, Y.: Character-level convolu tional networks for text classiﬁcation. In: Advances in716\nNeural Information Processing Systems, pp. 649–657 (2015)717\n41. Miller, G.A.: Wordnet: a lexical database for english. C ommunications of the ACM 38(11), 39–41 (1995)718\nRoshanzamir et al. Page 23 of 26\n42. Wang, W.Y., Yang, D.: That’s so annoying!!!: A lexical an d frame-semantic embedding based data719\naugmentation approach to automatic categorization of anno ying behaviors using# petpeeve tweets. In:720\nProceedings of the 2015 Conference on Empirical Methods in Na tural Language Processing, pp. 2557–2563721\n(2015)722\n43. Kobayashi, S.: Contextual augmentation: Data augmenta tion by words with paradigmatic relations. arXiv723\npreprint arXiv:1805.06201 (2018)724\n44. Wu, X., Lv, S., Zang, L., Han, J., Hu, S.: Conditional bert co ntextual augmentation. In: International725\nConference on Computational Science, pp. 84–95 (2019). Spr inger726\n45. Ma, E.: NLP Augmentation. https://github.com/makcedward/nlpaug (2019)727\n46. MacWhinney, B.: The CHILDES Project: Tools for Analyzing Talk, Volume I: Transcription Format and728\nPrograms. Psychology Press, ??? (2014)729\n47. Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne . Journal of machine learning research 9(Nov),730\n2579–2605 (2008)731\n48. Hern´ andez-Dom ´ ınguez, L., Ratt´ e, S., Sierra-Mart ´ ınez, G., Roche-Bergua, A.: Computer-based evaluation of732\nalzheimer’s disease and mild cognitive impairment patient s during a picture description task. Alzheimer’s &733\nDementia: Diagnosis, Assessment & Disease Monitoring 10, 260–268 (2018)734\n49. Folstein, M., Folstein, S., McHugh, P.: Mini-mental stat e”: a practical method for grading the cognitive state735\nof patients for the clinician” j psychiatr res 12: 189–198. F ind this article online (1975)736\nRoshanzamir et al. Page 24 of 26\nFigures737\nFigure 1 Overall classiﬁcation procedure. The overall classiﬁcation procedure contains the steps\nof augmentation, splitting, embedding, classiﬁcation, an d voting, where augmentation is only used\nin the training phase. Also, when passing the entire transcr ipt to the embedding layer, the splitting\nand voting layers are disabled. The underlined models are tr ainable here, and the others are ﬁxed.\nFigure 2 Visualized tSNE dimensionality reduction for the BERT Base embedding of the entire\nPitt corpus.\nFigure 3 Mean 10-fold cross-validation classiﬁcation accuracy, true positive rate, and true\nnegative rate.\nTables738\nTable 1 Demographics of Cookie-Theft picture description test of t he Pitt corpus.\nAD HC\nParticipants 170 99\nSamples 257 243\nAge (years) 71.7 ± 8.5 64.2 ± 7.9\nGender (male/female) 87/170 88/155\nMini-Mental State Exam 18.6 ± 5.1 29.1 ± 1.1\n# of Words 100.9 ± 58.3 111.5 ± 57.2\nRoshanzamir et al. Page 25 of 26\nTable 2 The similarity between predicted health scores of the S-BER TLarge-LR model and MMSE [ 49]\nscores.\nPhase\nMeasure Pearson\nCorrelation\nSpearman’s Rank\nCorrelation\nTrain 0.78 0.81\nValidation 0.70 0.74\nTable 3 AD versus HC classiﬁcation scores.\nMethod Embedding Classiﬁer Precision Recall Accuracy F1\nFraser et al.\n[20]\n35 Hand-Crafted\nFeatures LR - - 81.92 -\nYancheva et al.\n[21]\n12 Cluster-Based\nFeatures + LS&A\nRandom\nForest 80.00 80.00 80.00 80.00\nSirts et al.\n[22]\nCluster+PID+SID\nFeatures LR 74.4\n± 1.5\n72.5\n± 1.2\n- 72.7\n± 1.2\nHern´ andez et al.\n[48]\n105 Hand-Crafted\nFeatures SVM 81.00 81.00 79.00 81.00\nFritsch et al.\n[30]\nOne-Hot Word\nEmbedding Sequence - - 86 85.6 -\nPan et al.\n[31]\nGloVe Word\nEmbedding Sequence\nBi-LSTM|GRU\nHierarchical Attention\n84.02 84.97 - 84.43\nLi et al.\n[26]\n185 Hand-Crafted\nFeatures LR - - 77 -\nFraser et al.\n[27]\nInfo and\nLM Features SVM - - 75 77\nCNN + SSA GloVe Word\nEmbedding Sequence CNN 76.38\n± 8.49\n77.47\n± 8.97\n76.48\n± 5.88\n76.36\n± 5.91\nBiLSTM + SSA GloVe Word\nEmbedding Sequence Bi-LSTM 74.71\n± 1.92\n75.00\n± 14.82\n75.51\n± 5.77\n74.22\n± 8.71\nBiLSTM + CA GloVe Word\nEmbedding Sequence Bi-LSTM 78.40\n± 6.60\n73.95\n± 12.96\n77.36\n± 6.19\n75.43\n± 7.83\nT-BERTBase-LR BERTBase\n(Text Level)\nLR 85.09\n± 3.11\n78.69\n± 8.35\n82.76\n± 3.74\n81.51\n± 4.73\nT-BERTLarge-LR BERTLarge\n(Text Level)\nLR 88.21\n± 5.33\n80.86\n± 7.58\n85.10\n± 3.43\n84.04\n± 3.93\nT-XLNetBase-LR XLNetBase\n(Text Level)\nLR 84.74\n± 6.31\n79.26\n± 7.72\n81.92\n± 5.88\n81.75\n± 6.19\nT-XLNetLarge-LR XLNetLarge\n(Text Level)\nLR 82.30\n± 5.15\n83.83\n± 4.34\n82.87\n± 3.14\n82.86\n± 2.60\nT-XLM-LR XLM\n(Text Level)\nLR 80.31\n± 5.29\n79.13\n± 8.43\n80.21\n± 4.94\n79.49\n± 5.76\nS-BERTBase-LR BERTBase\n(Sentence Level)\nLR 90.31\n± 7.36\n76.52\n± 8.06\n84.46\n± 6.31\n82.72\n± 7.21\nS-BERTLarge-LR BERTLarge\n(Sentence Level)\nLR 90.57\n± 3.18\n84.34\n± 7.58\n88.08\n± 4.48\n87.23\n± 5.20\nS-BERTLarge-LR-BiLSTM BERTLarge\n(Sentence Level)\nLR 89.06\n± 5.19\n77.71\n± 7.33\n85.19\n± 4.92\n83.61\n± 5.69\nS-BERTLarge-BiLSTM BERTLarge\n(Sentence Level)\nBiLSTM 87.98\n± 5.31\n75.03\n± 5.99\n83.43\n± 5.51\n81.49\n± 5.31\nS-XLNetBase-LR XLNetBase\n(Sentence Level)\nLR 83.19\n± 6.39\n74.34\n± 8.12\n80.00\n± 5.48\n78.32\n± 6.16\nS-XLNetLarge-LR XLNetLarge\n(Sentence Level)\nLR 76.95\n± 6.62\n71.30\n± 8.29\n75.31\n± 5.56\n73.75\n± 6.14\nS-XLM-LR XLM\n(Sentence Level)\nLR 84.00\n± 4.74\n73.47\n± 9.80\n80.21\n± 5.47\n78.14\n± 6.72\nOther settings of the proposed framework with diﬀerent clas siﬁers or augmenters which did not have signiﬁcant\neﬀects on the scores are not shown.\nRoshanzamir et al. Page 26 of 26\nTable 4 Two invalid predicted transcripts by the model with the best accuracy score\n(S-BERTLarge-LR).\nTranscript Actual\nLabel\nPredicted\nLabel\nPredicted AD\nProbability\nAnd the boy in the cookie jar. And the girl reaching\nup to him. The stool slanting ready to topple. And the\ncookie jar is open. And the lid’s in there. And the door’s\nopen. And mother’s drying the dishes and standing in\na pool of water it looks water running down from the\nsink. ...\nAD HC 0.483\nOkay. It was summertime and mother and the children\nwere working in the kitchen. And the window was open\nand there was a slight breeze blowing in. Mother was\ndaydreaming and forgot and left the water in the sink\nrunning and it was overﬂowing. The children were hun-\ngry and ...\nHC AD 0.532\nPredicted AD probability ranges between 0 and 1.\nFigures\nFigure 1\nOverall classiﬁcation procedure. The overall classiﬁcation procedure contains the steps of augmentation,\nsplitting, embedding, classiﬁcation, and voting, where augmentation is only used in the training phase.\nAlso, when passing the entire transcript to the embedding layer, the splitting and voting layers are\ndisabled. The underlined models are trainable here, and the others are ﬁxed.\nFigure 2\nVisualized tSNE dimensionality reduction for the BERTBase embedding of the entire Pitt corpus.\nFigure 3\nMean 10-fold cross-validation classiﬁcation accuracy, true positive rate, and true negative rate.",
  "topic": "Artificial neural network",
  "concepts": [
    {
      "name": "Artificial neural network",
      "score": 0.5729764103889465
    },
    {
      "name": "Transformer",
      "score": 0.5600872039794922
    },
    {
      "name": "Computer science",
      "score": 0.5265201330184937
    },
    {
      "name": "Disease",
      "score": 0.41155192255973816
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38904500007629395
    },
    {
      "name": "Medicine",
      "score": 0.36447733640670776
    },
    {
      "name": "Natural language processing",
      "score": 0.36360275745391846
    },
    {
      "name": "Engineering",
      "score": 0.14593464136123657
    },
    {
      "name": "Internal medicine",
      "score": 0.13588201999664307
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133529467",
      "name": "Sharif University of Technology",
      "country": "IR"
    }
  ]
}