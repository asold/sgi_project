{
  "title": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
  "url": "https://openalex.org/W4385571728",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2618429976",
      "name": "David Kartchner",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": null,
      "name": "Selvi Ramalingam",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A4318150970",
      "name": "Irfan Al-Hussaini",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4297353499",
      "name": "Olivia Kronick",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4322691098",
      "name": "Cassie Mitchell",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4315754517",
    "https://openalex.org/W2593758073",
    "https://openalex.org/W3102569150",
    "https://openalex.org/W1997684599",
    "https://openalex.org/W4287017694",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W3140083277",
    "https://openalex.org/W3166204619",
    "https://openalex.org/W4360887219",
    "https://openalex.org/W3118839080",
    "https://openalex.org/W2163661402",
    "https://openalex.org/W4283447987",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2058629533",
    "https://openalex.org/W2057441072",
    "https://openalex.org/W2300553800",
    "https://openalex.org/W4318148415",
    "https://openalex.org/W2154076862",
    "https://openalex.org/W1582495596",
    "https://openalex.org/W4324321244",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W1966520620",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W4282960293",
    "https://openalex.org/W2023155234",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W3037991407"
  ],
  "abstract": "Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. This study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale. We perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of RCTs to traditional manual annotation methods. We analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. Our findings reveal that the best model for the two demonstrated tasks, ChatGPT can generally extract correct information and identify when the desired information is missing from an article. We additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.",
  "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 396–405\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nZero-Shot Information Extraction for Clinical Meta-Analysis using Large\nLanguage Models\nDavid Kartchner and Irfan Al-Hussaini and Olivia Kronick\nGeorgia Institute of Technology\n{david.kartchner, alhussaini.irfan, okronick3}@gatech.edu\nSelvi Ramalingam\nEmory University\nsramal3@emory.edu\nCassie Mitchell\nGeorgia Institute of Technology\ncassie.mitchell@bme.gatech.edu\nAbstract\nMeta-analysis of randomized clinical trials\n(RCTs) plays a crucial role in evidence-based\nmedicine but can be labor-intensive and error-\nprone. This study explores the use of large\nlanguage models to enhance the efficiency of\naggregating results from randomized clinical\ntrials (RCTs) at scale. We perform a detailed\ncomparison of the performance of these models\nin zero-shot prompt-based information extrac-\ntion from a diverse set of RCTs to traditional\nmanual annotation methods. We analyze the\nresults for two different meta-analyses aimed at\ndrug repurposing in cancer therapy and pharma-\ncovigilance in chronic myeloid leukemia. Our\nfindings reveal that the best model for the two\ndemonstrated tasks, ChatGPT, can generally\nextract correct information and identify when\nthe desired information is missing from an arti-\ncle. We additionally conduct a systematic error\nanalysis, documenting the prevalence of diverse\nerror types encountered during the process of\nprompt-based information extraction.\n1 Introduction\nMeta-analysis is a statistical method that combines\nand analyzes data from multiple studies to obtain\nan overall effect size or estimate of treatment effect.\nIt is widely used in healthcare research, particularly\nin clinical trials, to provide a comprehensive and ro-\nbust summary of the available evidence (Gopalakr-\nishnan and Ganeshkumar, 2013).\nThe importance of meta-analysis lies in its abil-\nity to increase statistical power and reduce bias,\nthereby improving the accuracy and reliability of\nthe findings. Meta-analysis also allows for the iden-\ntification of important subgroups of patients and\nprovides insights into the potential sources of het-\nerogeneity in the results of different studies (Sedg-\nwick, 2013; Song et al., 2001).\nIn the context of clinical trials, meta-analysis\nplays a crucial role in the evaluation of new treat-\nments and interventions (Heys et al., 1999; Al-\nKarawi and Jubair, 2016; Henna et al., 2004; Boulé\net al., 2001). By combining data from multiple\nstudies, researchers can obtain a more precise esti-\nmate of the effectiveness of treatment and identify\nany potential adverse effects.\nWhile clinical meta-analysis is essential to\nestablishing guidelines for clinical best-practice\n(Stangl and Berry, 2000), curating data is time-\nconsuming for medical professionals. A recent\nsurvey found that most clinical meta-analyses re-\nquire 6-10 months of data gathering and analysis\nfor 5 individuals (Borah et al., 2017), which does\nnot account for research development time spent\nbefore registering meta-analyses on PROSPERO\nregistry of systematic reviews (Booth et al., 2012).\nMoreover, most meta-analyses seek to answer very\ntargeted questions about specific diseases or drugs,\nmaking it difficult to adapt existing datasets for the\nautomatic or semi-automatic extraction of needed\ndata.\nA recent review (Wornow et al., 2023) examined\nfounation models/large language models (LLMs),\nsuch as ChatGPT, and opined that while there is\nevidence that clinical foundation models improve\naccuracy, there has been minimal work to validate\nother potential benefits, such as reducing data la-\nbeling burden, enabling new clinical applications,\nand offering novel human-AI interfaces. However,\nfoundation models also present significant risks,\nincluding data privacy and security concerns, in-\nterpretability challenges, high up-front costs, and\nbiases (Wornow et al., 2023).\nThis paper advocates for the development of new\nevaluation tasks, metrics, and datasets to better un-\nderstand how foundation models perform on clini-\ncal tasks. To this end, it contributes the following\nto the development and use of foundation models\nin clinical and biomedical research:\n• We present, to our knowledge, the first de-\ntailed evaluation of how well generative foun-\ndation models perform for extracting informa-\n396\nFigure 1: Pipeline for zero-shot data extraction for Meta-Analysis using LLMs.\ntion for clinical meta-analysis.\n• We detail prompting and post-processing\nstrategies that can improve the performance\nof prompt-based extraction and normalization\nof clinical information.\n• We present an error analysis systematically\ncataloging the prevalence of various types of\nerrors encountered during prompt-based infor-\nmation extraction and normalization.\n2 Related Work\nRecent surveys and case studies have shown that\nnatural language processing can powerfully accel-\nerate the identification of candidates for drug re-\npurposing (Subramanian et al., 2020) and the iden-\ntification of adverse effects in pharmacovigilance\nefforts (Bhatnagar et al., 2022). Substantial work\nin clinical evidence extraction has been in the col-\nlection of patients, interventions, comparators, and\noutcomes (PICO) from clinical trials. Nye et al.\n(2018) and Zlabinger et al. (2020) both present\na corpora with PICO annotations aimed at tag-\nging the high-level characteristics of clinical trials.\nBiomedical transformer models are currently state-\nof-the-art on PICO tagging (Alrowili and Shanker,\n2021; Yasunaga et al., 2022; Tinn et al., 2021). Ad-\nditional improvements can be made by augmenting\nPICO data with weak supervision from external dic-\ntionaries and hand-crafted rules (Dhrangadhariya\nand Müller, 2023).\nA number of works have built models to im-\nprove the screening and extracting evidence from\nclinical trials for meta-analysis and systematic re-\nview. Wallace et al. (2021) automatically constructs\na dataset focused on summarizing systematic re-\nviews using the “Conclusion“ section of structured\nabstracts, and this work was extended in a recent\nshared task on multi-document summarization for\nscientific literature reviews (Wang et al., 2022). Al-\nHussaini et al. (2022) takes a different approach\nby producing an end-to-end system for screening\nand summarizing individual clinical cohort stud-\nies based on PICO elements and clinically-relevant\ndetails annotated by epidemiologists during data\ncuration. Kang et al. (2023) creates a novel, hi-\nerarchical framework for structuring evidence in\nclinical trials for better usability and understanding.\nWhile existing work offers many valuable re-\nsources for evidence extraction across a diverse\narray of tasks, it is limited to specific, narrowly-\nscoped tasks and requires substantial annotation\nefforts to work effectively. In contrast, many crit-\nical clinical meta-analyses answer very specific\nclinical questions which are out of the scope of\nexisting data and require manual curation. This\nwork differs from existing work by evaluating the\neffectiveness of foundation NLP models for zero-\nshot extraction of information for highly-specific\nclinical questions, enabling a much broader range\nof applications.\n3 Problem setup and methodology\nTwo prior works facilitated the development of this\nzero-shot information extraction work, the Rem-\nedy Database (Emory, 2023) and CML dataset\nregistered on PROSPERO for an ongoing meta-\nanalysis (Kronick et al., 2023). Each was created\nthrough manual annotations of clinical trial and\ncohort study articles. Lists of the data columns\nincluded in each dataset are included in Tables 1\nand 2.\n3.1 Remedy Database\nRepurposing drugs is the process of identifying\nnew therapeutic uses for existing drugs that have\n397\nData Field Definition\nStudy Type Study characteristics such as Phase II, open-label, placebo-controlled, etc.\nCancer Type What type of cancer did the study focus on?\nCancer Stage What stage(s) of cancer did the study focus on?\nDrug Name What are the (non-cancer) drugs studied in this clinical trial?\nTreatment Timing When was treatment given relative to other standard-of-care (SOC) treat-\nment?\nDosage What is the dosage of the drug(s) used in the study?\nConcurrent SOC What is the standard of care (SOC) treatment is given to cancer patients in\nthis study?\nSample Size How many patients were enrolled in the study?\nSummary Template-baased summary capturing information on intervention and\ncomparator groups, study outcomes, and the authors’ conclusions.\nTable 1: Data fields of ReMedy dataset\nalready been approved for other indications. Clin-\nical trial articles provide valuable information on\nthe safety and efficacy of new treatments and in-\nterventions. However, identifying and extracting\ncritical information from clinical trial articles is a\ntime-consuming and tedious process. ReMedy can-\ncer database was built to address this issue (Emory,\n2023). It provides a comprehensive and standard-\nized source of information for clinical trials, ob-\nservational studies, and case reports data for re-\npurposed drugs for cancer, thereby making the re-\npurposed drug data easily accessible to physicians,\npatients, and potential investigators. In this article,\nwe have focused mainly on extracting information\nfrom clinical trial articles.\nThe current data extraction process is mostly\nmanual. The data extraction is divided into three\ndistinct categories: (i) Article identification, (ii)\nExtraction of clinical data, and (iii) Creation of\nstructured article summary. Identification of the\narticles is done manually by searching for repur-\nposed drug articles in databases such as PubMed.\nThe screening process ensures that the identified ar-\nticles fit the inclusion criteria, such as study design,\nintervention, and outcome measures.\nArticles were included based on study type, inter-\nventional drug, and outcome measures. All articles\nwere required to evaluate the effect of a non-cancer\ndrug on cancer patient outcomes to be included.\nEvery clinical trial article entry in the Remedy\ndatabase is reviewed at least twice, and changes\nare made to ensure accuracy.\nThe data elements included in the ReMedy\ndatabase are Pubmed ID (PMID), year of publica-\ntion, study type, cancer stage, cancer type, cancer\nsub-type, drug name, drug category, treatment tim-\ning, the dosage of the drug, the concurrent standard\nof care, number of patients enrolled in the study,\nclinical trial outcomes (primary and secondary) and\nauthor’s conclusion. Data was curated by students\npursuing a masters of public health (MPH) degree\nand takes between 60-90 minutes for an average\nabstract.\n3.2 CML Database\nChronic myeloid leukemia, or CML, is a relatively\nrare form of leukemia characterized by the presence\nof a Philadelphia chromosome, which results from\nthe fusion of BCR and ABL1 genes (Jabbour and\nKantarjian, 2018). Though once highly lethal, the\ndevelopment of a tyrosine-kinase inhibitor (TKI)\ndrugs dramatically improved long-term CML sur-\nvival (Minciacchi et al., 2021). This dataset was\ndeveloped for a meta-analysis to examine the hema-\ntological adverse events (HAEs) associated with\nTKI use. Examined HAEs include anemia, throm-\nbocytopenia, neutropenia, aplastic anemia, pancy-\ntopenia, and myelosuppression. Data extraction\nfollowed a procedure close to the one described\nin (Mohanavelu et al., 2021). All articles were se-\nlected to include at least one TKI and were filtered\nto exclude combination therapies with non-TKI\ndrugs. Articles were identified using searches in\nPubmed and ClinicalTrials.gov.\nThe following data fields are included for each\narticle: Source (PMID or National Clinical Trials\n(NCT) number), TKI name, number of patients\nunder treatment, number of patients experiencing\nincluded HAE, condition grade (if available), and\n398\nData Field Definition\nAll TKIs List of tyrosine kinase inhibitors administered to patients in the study/trial\nAll Phases List of phases of chronic myeloid leukemia of patients at time of treatment.\nAll HAEs List of all hematological adverse events experienced by patients in the\nstudy/trial.\nHAE Grade Whether or not the trial lists all HAEs or only severe HAEs (i.e. grade 3 or 4).\nNum. Treated For each combination of (TKI, CML phase) in the trial, how many total patients\nwere treated?\nHAE Counts For each combination of (TKI, CML phase) in the trial, how many patients\nexperienced a particular HAE?\nTable 2: Data columns of CML dataset\nCML phase (if available). Quality control was con-\nducted by an independent team on the extracted\ndata. The total number of articles listing each HAE\nas follows: aplastic anemia:3; anemia: 50; neu-\ntropenia: 55; thrombocytopenia: 61; myelosup-\npression: 13; and pancytopenia:16.\n4 Methods\n4.1 Models\nOur selection of models sought to balance perfor-\nmance and cost for applicability to an extensive\nrange of researchers. Balancing these criteria, we\nselected two models, GPT 3.5 Turbo (Ouyang et al.,\n2022) (also known as ChatGPT) and Together’s\nGPT-JT (Together, 2023) for use. ChatGPT was\nchosen due to its well-known human-like perfor-\nmance across a wide range of tasks and its ability to\nfollow detailed instructions. GPT-JT, an open, fine-\ntuned extension of GPT-J (Wang and Komatsuzaki,\n2021) has also shown state-of-the-art performance\nacross a range of prompt-based tasks despite using\nup to two orders of magnitude fewer parameters\n(6B vs. up to 530B) (Together, 2022). All outputs\nfor GPT-JT were obtained via the Manifest wrapper\nfor the API (Orr, 2022).\n4.2 Prompt Creation and Tuning\nPrompts were created and refined using an itera-\ntive, human-in-the-loop creation process. Initial\nprompts were created for each dataset in collab-\noration with one of the original curators of the\ndatabase. Prompts for multiple-choice or select-all-\nthat-apply study characteristics were often (but not\nalways) provided with a list of possible values to\nguide model outputs.\nThe “article summary” column from ReMedy\nfollows a template to ensure that human curators\ngather all relevant epidemiological data for a drug-\nrepurposing meta-analysis. To produce these sum-\nmaries, we provided models with a similar tem-\nplate, augmented with instructions on what data to\nput in each field. The full text of each summary\nwas generated in a single shot for each article.\nThe CML dataset required the extraction of quan-\ntitative information about each hematological ad-\nverse event (HAE) listed in each paper. This extrac-\ntion was needed for each stage of CML and each\nTKI analyzed in the study. We extracted this in-\nformation by first prompting the model to provide\na list of all TKIs, CML phases, and HAE listed\nin each study. After normalizing the outputs for\neach category (see below), we iterated through each\n(TKI, phase, HAE) model identified in the paper\nand extracted the count and/or percentage of each\nadverse event. We additionally attempted to extract\nthis information using a one-shot templated sum-\nmary but found the model output too inconsistent\nto reliably parse the desired quantitative data.\nA list of prompts used for ReMedy are given in\nTable 3 and prompts for CML are given in Table 4.\n4.3 Data Extraction and Postprocessing\nOutputs from generative LLMs were often noisy\nand required significant normalization to be usable.\nFor example, outputs of GPT-JT consistently began\ngenerating additional questions after each output\nwas extracted, so outputs had to be split by newline\ncharacter, and everything after the first newline was\nthrown away. Outputs of categorical columns (sin-\ngle and multi-label) were normalized to acceptable\nvalues using fuzzy string matching with the accept-\nable values. All matches within an appropriate\nthreshold (usually 80% or higher) were replaced\nwith the matched canonical value, while those be-\n399\nData Field Prompt\nStudy Type What type of study is this? I define study type as \"phase 0\", \"phase 1\", \"phase 2\", \"phase 3\",\n\"phase 4\", \"randomized\", \"double blind\", \"open label\", \"placebo controlled\", \"pilot studies\".\nPlease list all applicable study types.\nCancer Type What type of cancer(disease) did the study focus on?\nCancer Stage What stage of cancer did the study focus on? I define cancer stage as \"Early (stages 0 - 3)\",\n\"Advanced (stages 3-4)\", \"Metastatic (Stage 4)\", \"All stages (0-4)\"\nDrug Name What is/are the name(s) of the drug(s) used in the study?\nTreatment Timing What is the treatment timing? I define treatment timing as \"adjuvant\", \"continuous\", \"mainte-\nnance\", \"neo-adjuvant\", \"palliative\", \"peri-operative\", \"post-diagnosis\", \"primary therapy\", and\n\"secondary therapy\".\nDosage What is the dosage of the drug(s) used in the study? Please list values in a dictionary as\n{drug_name: dosage}\nConcurrent SOC What is the standard of care? I define standard of care as \"anti-angiogenic\", \"check point\ninhibitors\", \"chemo-radiation\", \"immune therapy\", \"radiation\", and \"targeted therapy\"\nSample Size How many patients were enrolled in the study? Please give only the number and no other\nwords.\nSummary Write a summary the study using the following guidelines/template. Fill out a template\nthat includes the following features: Include the disease name, drug name, type of clinical\ntrial for the first sentence. Then summarize the abstract by including PICO (Population,\nIntervention,comparison (control & intervention) and outcome. Also include the standard\nof care that is used. Finally include author’s conclusion. Below is a sample template, with\nvalues you should insert in brackets []. \"A [clinical trial phase] clinical trial evaluating the\neffects of [drug(s)] in patients with [cancer type(s)] cancer. Disease: [cancer types] cancer\nPopulation: [n] patients with histopathology in [location] Intervention (n=[num patients in\nintervention group]): [dosage details] Control (n=[n patients in control/standard of care group]):\n[standard of care treatment. should not include non-cancer drug being investigated] Concurrent\ntreatment: [treatment type, e.g. chemotherapy (detailed or not?)] Primary outcomes: a)[study\nendpoint/outcome metric], [value (intervention vs control)], [confidence interval], [p-value]\nb) Secondary outcomes: a) [study endpoint/outcome metric], [value (intervention vs control)],\n[confidence interval], [p-value] b) The authors conclude: [single sentence direct quote from\nauthor conclusions]\nTable 3: Prompts used for ReMedy data extraction\nData Field Prompt\nAll TKIs Please list all tyrosine kinase inhibitors studied. List only the TKI name(s) delimited by commas.\nDo not return anything other than the TKI name(s).\\nChoices: [’imatinib’, ’nilotinib’, ’dasa-\ntinib’, ’radotinib’, ’ruxolitinib’, ’bosutinib’, ’tipifarnib’, ’asciminib’, ’ponatinib’, ’bosutonib’]\nAll Phases What phase(s) of chronic myeloid leukemia (CML) did patients in the study have? List all that\napply. Do not return anything other than the CML phase(s).\\nChoices: [\"chronic\", \"accelerated\",\n\"blast\"]\\nIf not specified, reply \"n/a\".\nAll HAEs What hematological adverse events were experienced by patients in the study? Select all that\napply. Do not return anything other than the adverse events.\\nChoices: [\"anemia\", \"pancy-\ntopenia\", \"myelosuppression\", \"aplastic anemia\", \"neutropenia\", \"thrombocytopenia\"]\\nIf none\nmentioned, reply \"n/a\"\\n\nHAE Grade Does this study list all grades of adverse events or only severe (i.e. grade 3 or 4) adverse events?\nIf all grades, please respond \"all\" and if severe only, please respond \"severe\". If no adverse\nevents are mentioned, reply \"n/a\"\nNum. Treated How many patients with phase phase CML were treated with tki? Please return a single integer\nand nothing else. If no patients with phase phase CML were treated with tki were specified,\nreturn \"n/a\"\nHAE Counts How many {tki} treated patients with {phase} phase CML experienced {ade}? Please return a\nsingle integer. If the number of patients with {ade} is not listed, reply \"n/a\"\nTable 4: Prompts used for CML data extraction\n400\nUnfiltered Filtered\nChatGPT GPT-JT-6B ChatGPT GPT-JT-6B Metric\nConcurrent SOC 0.135 0.192 0.149 0.201 Accuracy\nCancer Type 0.897 0.510 0.904 0.518 Accuracy\nTreatment Type 0.235 0.564 0.248 0.549 Accuracy\nNum. Patients 0.719 0.638 0.720 0.634 Accuracy\nCancer Stage 0.403 0.224 0.408 0.215 Accuracy\nDosage 0.461 0.083 0.472 0.077 Fuzzy Acc\nStudy Subtype 0.584 0.375 0.604 0.382 Jaccard\nSummary (p) 0.469 0.197 0.496 0.205 Precision\nSummary (r) 0.420 0.201 0.431 0.201 Recall\nSummary (f1) 0.421 0.188 0.441 0.193 F1\nTable 5: Results of LLM information extraction on Remedy database\nlow the threshold were marked as incorrect.\n“Cancer Type” and “Concurent Standard-of-\nCare Treatment” columns in Remedy were not\ngiven candidate values in the multiple choice list\nduring initial LLM labeling, so we asked each LLM\nto generate a mapping from the free-text values to\ntheir normalized forms. ChatGPT was able to to\nmap the free-form values in these columns to the\ncandidates in the multiple choice list for over 95%\nof values, while GPT-JT correctly normalized out-\nputs less than 10% of the time. When comparing\nthe outputs of these columns, we compare the self-\nnormalized version of ChatGPT’s output and the\nunnormalized version of GPT-JT’s output, giving\neach model the highest respective performance.\n4.4 Hyperparameters\nChatGPT was run with p=0 and maximum tokens\nset to 30 tokens or less for all fields except the\nsummary. All other ChatGPT parameters were\nset to default values. Manifest parameters were\nset to top_p=0.9. We additionally set top_k=40\nfor ReMedy non-summary fields, and top_k=1 for\nReMedy summary + all CML fields. All summaries\nwere generated using a maximum of 256 tokens,\noccasionally resulting in summary truncation.\n4.5 Evaluation Metrics\n4.5.1 Classification\nEach dataset in question produced diverse data\ntypes that require different evaluation strategies.\nAll single-label classification columns were eval-\nuated using accuracy. For multi-label classifica-\ntion, where each model outputs a set of characteris-\ntics (e.g., Study Subtype in Remedy), we compare\nthe set of characteristics labeled by the foundation\nmodel with human-annotated set using Jaccard sim-\nilarity:\nJ(A,B) = |A∩ B|\n|A∪ B|\n4.5.2 Quantitative Information Extraction\nInteger columns (e.g., number of patients, side-\neffect counts) were evaluated using accuracy, mea-\nsured as the mean frequency of an exact match be-\ntween the human-annotated integer and the output\nof the foundation model. For columns that measure\npercentages, we considered an answer “correct” if\nthe output of the foundation model and the human\nannotation differ by no more than 1% to account\nfor rounding error.\n4.5.3 Short Answer\nSome fields list a short, free-form text answer\n(e.g., dosage). For these fields, multiple equiva-\nlent answers could be acceptable. For example, the\ndosages “40mg BID” and “40 mg twice daily” are\nsemantically equivalent. These differences are com-\nmon as clinicians prefer shorter acronyms such as\n“BID” whereas LLMs prefer more colloquial expla-\nnations such as “twice daily”. We compare LLM\noutputs’ similarity with gold answers in these fields\nusing fuzzy string matching, with cutoffs chosen\nbased on human inspection of included/excluded\nmatches. We then calculate “fuzzy accuracy” as\nthe mean of how often gold and LLM responses\nwere above the required similarity threshold.\n4.5.4 Summarization\nWe compare generated and human summaries us-\ning Rouge (Lin, 2004). Specifically, we use the\nRouge-L metric, which analyzes the length of the\nlongest common subsequence (LCS) of the gen-\nerated and reference summaries. Let mand nbe\n401\nFigure 2: ReMedy Summary Rouge-L F1 score distri-\nbution for each model.\nthe lengths of the reference and generated sum-\nmaries, respectively. Then the Rouge-L precision\n(Plcs), recall ( Rlcs), and F-measure ( Flcs) scores\nare computed as follows:\nRlcs = LCS(X,Y )\nm\nPlcs = LCS(X,Y )\nn\nFlcs =\n(\n1 +β2)\nRlcsPlcs\nRlcs + β2Plcs\nwhere β = Plcs\nRlcs\n.\n5 Results\n5.1 Remedy\nThe results of the models on ReMedy are shown in\nTable 5. These results show a wide range in accu-\nracy, with the extraction of cancer type and sample\nsize being the most accurate and identification of\nthe concurrent standard-of-care treatment being the\nleast accurate.\nIn general, we observe that ChatGPT dramati-\ncally outperforms GPT-JT for most data fields. The\nlargest gains occur in the “Dosage” and “Sum-\nmary” fields. This likely occurs because Chat-\nGPT’s training objective provides more explicit\noptimization for summarization and formatting,\nboth of which are critical to performing these\ntasks correctly. GPT-JT’s ability to summarize\nis notably poor, only performing 3% above the\nscore given by the template given in the prompt,\n(P,R,F 1) = (0.206,0.158,0.170). A compari-\nson of the distribution of Rouge-L F1 scores for\nChatGPT and GPT-JT is shown in Figure 2.\nChatGPT GPT-JT-6B Metric\nTKIs 0.870 0.448 Jaccard\nCML Phases 0.919 0.932 Jaccard\nHAEs 0.500 0.466 Jaccard\nHAE Grade 0.044 0.128 Accuracy\nNum. Treated 0.444 0.154 Accuracy\nAnemia 0.244 0.308 +/-1 Accuracy\nNeutropenia 0.133 0.179 +/-1 Accuracy\nThrombocytopenia0.133 0.128 +/-1 Accuracy\nPancytopeia 0.956 0.974 +/-1 Accuracy\nMyelosuppression 1.000 1.000 +/-1 Accuracy\nAplastic Anemia 1.000 1.000 +/-1 Accuracy\nTable 6: Results of LLM information extraction on CML\ndataset. +\\- Accuracy counts numerical matches that are\noff-by-1 to account for rounding error.\n5.2 CML\nThe results of the models on the CML dataset are\nshown in Table 6. We observe that ChatGPT out-\nperforms GPT-JT on identifying the number of pa-\ntients treated and the TKIs used in the study.\nOne interesting result is that accuracy on pan-\ncytopenia, myelosuppression, and aplastic anemia\nare high. This is because these HAEs are very\nrarely observed in the data, and the models cor-\nrectly predict them as N/A. This is a promising\nresult showing that models are capable of refusing\nto make up values when the data is not present for\nextraction.\n5.3 Error Analysis\nWe perform an error analysis of the mistakes made\nby models when generatively extracting data. We\nfound that errors extracting specific data elements\ngenerally fell into the following categories:\nExcessive Verbosity Despite prompts explicitly\ndesigned to limit verbosity (e.g., “...Please list only\nthe tki name(s) delimited by commas”, “...Please\nreturn a single integer. If the answer is not found,\nreturn ‘n/a’ ”), models frequently output extrane-\nous text trying to explain their answers, particu-\nlarly ChatGPT. Excess explanations turned single-\nword answers into complete sentences, added un-\ndesired (but grammatically correct) punctuation,\nor explained that the data was not found in the\nquoted text rather than simply returning “n/a”. This\nverbosity was the leading cause of postprocessing\nneeded to make model output usable.\nData not in Abstract Each of the databases used\nin this study were curated the full text of research\narticles. However, our models only examined data\npresent in the abstracts of such articles. While all\n402\nneeded information was sometimes present in the\nabstract alone, specific quantitative results were\noften present only in the full text of the research\narticle, particularly for the CML dataset. In these\ncases, failure to agree with the curated data reflects\na correct assessment by the model that information\nis lacking in the text it was provided. In the Rem-\nedy dataset, the study subtype was sometimes not\npresent in the abstract, which ChatGPT correctly\ndetected and returned a response indicating its ab-\nsence. For 33% of the articles, ChatGPT detected\nall study subtypes accurately from the abstract.\nIncomplete Gold-standard Data In select cases,\nLLM output revealed human errors in the curation\nof the original databases. An analysis of the 30\nlowest-scoring abstracts for document summariza-\ntion revealed that 20 had incomplete human sum-\nmaries, 3 of which had no human summary. Two\nadditional articles had been removed from ReM-\nedy and/or referenced by a different PMID in the\ndatabase. This indicates that text summarization\nresults are overly conservative in estimating model\nperformance, especially on the lowest-scoring por-\ntion of the data. We report the results after remov-\ning the subset with errors under the “Filtered” tab\nof the ReMedy results.\nHallucinations Models occasionally hallucinate\nfalse information from studies. The most common\nhallucinations from ChatGPT in Remedy were in-\ncorrect numbers of patients in treatment/control\ngroups and the hallucination of a control group\nwhen none was present. This most commonly oc-\ncurs when the group sizes are not explicitly given\nin the paper, in which case ChatGPT assumed that\nthe population was split evenly between treatment\nand control groups. When the disease subtype was\nabsent in the abstract, ChatGPT had the propensity\nto guess the study subtype instead of indicating its\nabsence. Hallucinations from GPT-JT showed a\nhigher propensity to seek to answer the question\nwith fabricated numbers when the actual answer\nwas missing from the study.\n6 Conclusion\nIn conclusion, this study demonstrates the poten-\ntial of large language models in enhancing the\nefficiency of clinical meta-analyses of random-\nized clinical trials. By comparing its performance\nwith traditional manual annotation methods, we\nprovide valuable insights into the advantages and\nchallenges of implementing AI-based solutions in\nevidence-based medicine. The results of our re-\nsearch indicate that LLMs can contribute to more\nstreamlined, transparent, and reproducible results\nin clinical research. It also reveals that they still\nmake significant errors and should be used cau-\ntiously with additional quality checks when used\nas a tool to extract data from clinical research.\nEthics Statement\nUsing large language models in a clinical domain\nhas inherent risks. As demonstrated in this paper,\nLLMs sometimes hallucinate and fabricate false\nanswers to questions posed about research articles.\nIf done at scale, these extraction errors could propa-\ngate to downstream analyses, potentially leading to\nfalse conclusions. While LLMs may be able to sig-\nnificantly speed the process of human data curation\nand even help in detecting errors, they still require\nmanual verification of results to ensure high data\nquality.\nReferences\nIrfan Al-Hussaini, Davi Nakajima An, Albert J. Lee,\nSarah Bi, and Cassie S. Mitchell. 2022. Ccs explorer:\nRelevance prediction, extractive summarization, and\nnamed entity recognition from clinical cohort studies.\nIn 2022 IEEE International Conference on Big Data\n(Big Data), pages 5173–5181.\nDalia Al-Karawi and Luqman Jubair. 2016. Bright light\ntherapy for nonseasonal depression: meta-analysis of\nclinical trials. Journal of affective disorders, 198:64–\n71.\nSultan Alrowili and Vijay Shanker. 2021. BioM-\ntransformers: Building large biomedical language\nmodels with BERT, ALBERT and ELECTRA. In\nProceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 221–227, Online. Asso-\nciation for Computational Linguistics.\nRoopal Bhatnagar, Sakshi Sardar, Maedeh Beheshti,\nand Jagdeep T Podichetty. 2022. How can natural\nlanguage processing help model informed drug de-\nvelopment?: a review. JAMIA open, 5(2):ooac043.\nAlison Booth, Mike Clarke, Gordon Dooley, Davina\nGhersi, David Moher, Mark Petticrew, and Lesley\nStewart. 2012. The nuts and bolts of prospero: an in-\nternational prospective register of systematic reviews.\nSystematic reviews, 1(1):1–9.\nRohit Borah, Andrew W Brown, Patrice L Capers, and\nKathryn A Kaiser. 2017. Analysis of the time and\nworkers needed to conduct systematic reviews of\nmedical interventions using data from the prospero\nregistry. BMJ open, 7(2):e012545.\n403\nNormand G Boulé, Elizabeth Haddad, Glen P Kenny,\nGeorge A Wells, and Ronald J Sigal. 2001. Effects of\nexercise on glycemic control and body mass in type\n2 diabetes mellitus: a meta-analysis of controlled\nclinical trials. Jama, 286(10):1218–1227.\nAnjani Dhrangadhariya and Henning Müller. 2023. Not\nso weak pico: leveraging weak supervision for par-\nticipants, interventions, and outcomes recognition\nfor systematic review automation. JAMIA open ,\n6(1):ooac107.\nEmory. 2023. Remedy (repurposed medicines)-cancer\ndatabase.\nS Gopalakrishnan and P Ganeshkumar. 2013. System-\natic reviews and meta-analysis: understanding the\nbest evidence in primary healthcare. Journal of fam-\nily medicine and primary care, 2(1):9.\nMárcia Riromi Henna, Rozemeire GM Del Nero,\nCristina Zugaiar S. Sampaio, Álvaro Nagib Atal-\nlah, Sérgio Tomaz Schettini, Aldemar Araújo Castro,\nand Bernardo Garcia de Oliveira Soares. 2004. Hor-\nmonal cryptorchidism therapy: systematic review\nwith metanalysis of randomized clinical trials. Pedi-\natric surgery international, 20:357–359.\nSteven D Heys, Leslie G Walker, Ian Smith, and Oleg\nEremin. 1999. Enteral nutritional supplementation\nwith key nutrients in patients with critical illness and\ncancer: a meta-analysis of randomized controlled\nclinical trials. Annals of surgery, 229(4):467.\nElias Jabbour and Hagop Kantarjian. 2018. Chronic\nmyeloid leukemia: 2018 update on diagnosis, therapy\nand monitoring. American journal of hematology ,\n93(3):442–459.\nTian Kang, Yingcheng Sun, Jae Hyun Kim, Casey Ta,\nAdler Perotte, Kayla Schiffer, Mutong Wu, Yang\nZhao, Nour Moustafa-Fahmy, Yifan Peng, and Chun-\nhua Weng. 2023. EvidenceMap: a three-level knowl-\nedge representation for medical evidence computa-\ntion and comprehension. Journal of the American\nMedical Informatics Association. Ocad036.\nOlivia Kronick, Xinyu Chen, Nidhi Mehra, Armon\nVarmeziar, Rachel Fisher, Vamsi Kota, and Cassie\nMitchell. 2023. Frequency of hematological adverse\nevents in chronic myeloid leukemia patients on ty-\nrosine kinase inhibitor therapy: A systematic review\nwith meta-analysis.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nValentina R Minciacchi, Rahul Kumar, and Daniela S\nKrause. 2021. Chronic myeloid leukemia: a model\ndisease of the past, present and future. Cells,\n10(1):117.\nPrahathishree Mohanavelu, Mira Mutnick, Nidhi Mehra,\nBrandon White, Sparsh Kudrimoti, Kaci Hernan-\ndez Kluesner, Xinyu Chen, Tim Nguyen, Elaina Hor-\nlander, Helena Thenot, et al. 2021. Meta-analysis of\ngastrointestinal adverse events from tyrosine kinase\ninhibitors for chronic myeloid leukemia. Cancers,\n13(7):1643.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang,\nIain Marshall, Ani Nenkova, and Byron Wallace.\n2018. A corpus with multi-level annotations of pa-\ntients, interventions and outcomes to support lan-\nguage processing for medical literature. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 197–207, Melbourne, Australia. Association\nfor Computational Linguistics.\nLaurel Orr. 2022. Manifest. https://github.com/\nHazyResearch/manifest.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nPhilip Sedgwick. 2013. Meta-analyses: heterogeneity\nand subgroup analysis. Bmj, 346.\nFujian Song, Trevor A Sheldon, Alex J Sutton, Keith R\nAbrams, and David R Jones. 2001. Methods for\nexploring heterogeneity in meta-analysis. Evaluation\n& the health professions, 24(2):126–151.\nDalene Stangl and Donald A Berry. 2000. Meta-\nanalysis in medicine and health policy. CRC Press.\nShivashankar Subramanian, Ioana Baldini, Sushma\nRavichandran, Dmitriy A Katz-Rogozhnikov,\nKarthikeyan Natesan Ramamurthy, Prasanna Sat-\ntigeri, Kush R Varshney, Annmarie Wang, Pradeep\nMangalath, and Laura B Kleiman. 2020. A natural\nlanguage processing system for extracting evidence\nof drug repurposing from scientific publications. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 13369–13381.\nRobert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xi-\naodong Liu, Tristan Naumann, Jianfeng Gao, and\nHoifung Poon. 2021. Fine-tuning large neural lan-\nguage models for biomedical natural language pro-\ncessing. arXiv preprint arXiv:2112.07869.\nTogether. 2022. Releasing gpt-jt powered by open-\nsource ai.\nTogether. 2023. Gpt-jt-6b.\nByron C Wallace, Sayantan Saha, Frank Soboczenski,\nand Iain J Marshall. 2021. Generating (factual?)\nnarrative summaries of rcts: Experiments with neural\nmulti-document summarization. AMIA Summits on\nTranslational Science Proceedings, 2021:605.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\n404\nLucy Lu Wang, Jay DeYoung, and Byron Wallace. 2022.\nOverview of MSLR2022: A shared task on multi-\ndocument summarization for literature reviews. In\nProceedings of the Third Workshop on Scholarly Doc-\nument Processing, pages 175–180, Gyeongju, Repub-\nlic of Korea. Association for Computational Linguis-\ntics.\nMichael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel,\nEthan Steinberg, Scott Fleming, Michael A Pfeffer,\nJason Fries, and Nigam H Shah. 2023. The shaky\nfoundations of clinical foundation models: A survey\nof large language models and foundation models for\nemrs. arXiv preprint arXiv:2303.12961.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. In Association for Computational\nLinguistics (ACL).\nMarkus Zlabinger, Marta Sabou, Sebastian Hofstät-\nter, and Allan Hanbury. 2020. Effective crowd-\nannotation of participants, interventions, and out-\ncomes in the text of clinical trial reports. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 3064–3074, Online. Association\nfor Computational Linguistics.\n405",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7099498510360718
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.625430703163147
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5856638550758362
    },
    {
      "name": "Meta-analysis",
      "score": 0.5753869414329529
    },
    {
      "name": "Information extraction",
      "score": 0.5040106177330017
    },
    {
      "name": "Natural language processing",
      "score": 0.5012595653533936
    },
    {
      "name": "Repurposing",
      "score": 0.4816233515739441
    },
    {
      "name": "Data extraction",
      "score": 0.47194594144821167
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.45586279034614563
    },
    {
      "name": "Myeloid leukemia",
      "score": 0.4225214123725891
    },
    {
      "name": "Machine learning",
      "score": 0.4113995134830475
    },
    {
      "name": "Data mining",
      "score": 0.38643914461135864
    },
    {
      "name": "MEDLINE",
      "score": 0.331551194190979
    },
    {
      "name": "Medicine",
      "score": 0.23393172025680542
    },
    {
      "name": "Internal medicine",
      "score": 0.11171123385429382
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    }
  ]
}