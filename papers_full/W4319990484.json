{
  "title": "Multiscaled Multi-Head Attention-Based Video Transformer Network for Hand Gesture Recognition",
  "url": "https://openalex.org/W4319990484",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5031216644",
      "name": "Mallika Garg",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    },
    {
      "id": "https://openalex.org/A5100625251",
      "name": "Debashis Ghosh",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    },
    {
      "id": "https://openalex.org/A5030605875",
      "name": "Pyari Mohan Pradhan",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2795354662",
    "https://openalex.org/W6754950502",
    "https://openalex.org/W3095437205",
    "https://openalex.org/W2471695703",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W4312769131",
    "https://openalex.org/W3126321356",
    "https://openalex.org/W3208014149",
    "https://openalex.org/W3081152303",
    "https://openalex.org/W3101265641",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6682864246",
    "https://openalex.org/W914561379",
    "https://openalex.org/W2982705218",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W3210459555",
    "https://openalex.org/W2799034895",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2913362834",
    "https://openalex.org/W2904106524",
    "https://openalex.org/W2091911422",
    "https://openalex.org/W2963681914",
    "https://openalex.org/W3107892667",
    "https://openalex.org/W2960983219",
    "https://openalex.org/W3080402242",
    "https://openalex.org/W3099461062",
    "https://openalex.org/W3170161070",
    "https://openalex.org/W3106327734",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Dynamic gesture recognition is one of the challenging research areas due to\\nvariations in pose, size, and shape of the signer's hand. In this letter,\\nMultiscaled Multi-Head Attention Video Transformer Network (MsMHA-VTN) for\\ndynamic hand gesture recognition is proposed. A pyramidal hierarchy of\\nmultiscale features is extracted using the transformer multiscaled head\\nattention model. The proposed model employs different attention dimensions for\\neach head of the transformer which enables it to provide attention at the\\nmultiscale level. Further, in addition to single modality, recognition\\nperformance using multiple modalities is examined. Extensive experiments\\ndemonstrate the superior performance of the proposed MsMHA-VTN with an overall\\naccuracy of 88.22\\\\% and 99.10\\\\% on NVGesture and Briareo datasets,\\nrespectively.\\n",
  "full_text": "1\nMultiscaled Multi-Head Attention-based Video\nTransformer Network for Hand Gesture Recognition\nMallika Garg, Debashis Ghosh, Senior Member, IEEEand Pyari Mohan Pradhan, Member, IEEE\nAbstract—Dynamic gesture recognition is one of the challeng-\ning research areas due to variations in pose, size, and shape of\nthe signer’s hand. In this letter, Multiscaled Multi-Head Attention\nVideo Transformer Network (MsMHA-VTN) for dynamic hand\ngesture recognition is proposed. A pyramidal hierarchy of mul-\ntiscale features is extracted using the transformer multiscaled\nhead attention model. The proposed model employs different\nattention dimensions for each head of the transformer which\nenables it to provide attention at the multiscale level. Further, in\naddition to single modality, recognition performance using mul-\ntiple modalities is examined. Extensive experiments demonstrate\nthe superior performance of the proposed MsMHA-VTN with\nan overall accuracy of 88.22% and 99.10% on NVGesture and\nBriareo datasets, respectively.\nIndex Terms—Dynamic Gesture Recognition, Multi-modal\nrecognition, Multi-head Attention, Multiscale Pyramid Attention,\nVideo Transformer\nI. I NTRODUCTION\nWith advancements in deep learning, its applications in the\nfields of pattern recognition and human-computer interaction\nhave gained much attention. This enables the automatic extrac-\ntion of self-learned features thereby reducing the need for man-\nual extraction and selection of suitable discriminating features.\nThis advancement is more prominent in recognizing dynamic\ngestures where spatio-temporal features are extracted. Earlier,\nLong Short-Term Memory (LSTM)-based techniques [1] were\nused for sequential data. Since then, several variants of LSTMs\nand Recurrent Neural Networks (RNN) have been developed.\nThe effects of the attention mechanism in LSTM are further\nexplored in [2]. An attention-based Gated recurrent unit [3]\nand recurrent three-dimensional convolutional neural network\n(R3DCNN) [4] have also been developed for extracting tem-\nporal features for dynamic gesture recognition.\nThe attention mechanism decides and identifies the most\nimportant part of the gesture video sequence at each learn-\ning step. One such model which is based on the attention\nmechanism is the Transformer [5]. Transformers basically use\nan encoder and decoder to sequentially model the input to\nan output sequence, which can be used as an alternative to\nRNN-based and LSTM-based models. The encoder maps the\ninput sequence and its positional encoding to some high-\ndimensional attention vector. Positional encoding may either\nbe learned or fixed positional encoding. Few encoders are\nstacked together and the output of the final encoder is then fed\nThe authors are with the Department of Electronics and Commu-\nnication Engineering, Indian Institute of Technology, Roorkee, India-\n247667, (e-mail: mallika@ec.iitr.ac.in, debashis.ghosh@ece.iitr.ac.in, pyari-\nmohan.pradhan@gmail.com).\nto the decoder. The decoder finally maps this high-dimensional\nattention vector to the final decoding vector.\nInitially, transformers were proposed for Sequence-to-\nSequence translation in natural language processing. Later,\nthey were also applied to a sequence of image patches for\nimage classification [6]. Previous studies show that Vision\nTransformer (ViT) and Data efficient Image Transformer\n(Deit) [7] are two suitable models for using transformers in\ncomputer vision. A Transformer network can also be used\nfor video recognition [8], [9]. In Video Transformer Networks\n(VTN), the spatial features from each frame are extracted using\na pre-trained backbone network. Further, a transformer model\nthat extracts multiscaled features using pooling in Multiscale\nVision Transformers (MViT) was proposed in [10]. An im-\nproved version of MViT [11] incorporates decomposed relative\npositional embedding and residual pooling connections which\noutperforms the MViT in terms of accuracy and computational\noverhead.\nThe transformer-based model has also been proposed for\nDynamic Hand Gesture Recognition [12]. Frame-level features\nare extracted from the backbone network which is subse-\nquently combined with the temporal information of the frame.\nThe basic transformer model lacks ordering information as the\nvideo data is received in parallel. This is dealt with a Gated\nRecurrent Unit-Relative Sign Transformer (GRU-RST) [13]\nwhich uses relative positional embedding. With recent ad-\nvancements, better recognition performances in the case of\ncontinuous gestures have been reported in [14] and [15].\nIn this letter, a framework for the classification of dynamic\nhand gestures based on transformer architecture has been pro-\nposed. Single and multimodal inputs are used in the proposed\nmethod. Unlike the original transformer module, the proposed\nmodel employs multiscaled attention from different heads of\nthe transformer in a pyramidal structure.\nSummarizing, the major contributions of this letter are:\n1) A multiscaled multi-head attention to capture multiscale\nfeatures in a transformer model is proposed in which\na pyramid of scaled attention is designed. This signifi-\ncantly enhances the performance over the original basic\ntransformer network [5] for dynamic gesture recogni-\ntion.\n2) Single and multiple inputs from active data sensors\nare utilized for single-modal as well as multi-modal\ndynamic gesture recognition. These inputs are RGB\ncolors, depth, infrared images, surface normals estimated\nfrom depth maps, and optical flow estimated from the\nRGB images.\narXiv:2501.00935v1  [cs.CV]  1 Jan 2025\n2\n3) Efficacy of the proposed framework is validated on two\npublicly released datasets, viz., NVidia Dynamic Hand\nGesture and Briareo data and the proposed model leads\nto state-of-the-art results, compared to existing methods.\nII. MULTISCALED MULTI-HEAD TRANSFORMER\nLet X = [X1, X2, .., Xn] denotes the input set of n gesture\nvideo samples, where Xi represents the ith sample given in\nthe form Xi = {xi1, xi2, .., xit}, where xit denotes the tth\nframe of the ith gesture video. The goal is to predict the class\nlabel to which an input video sample may belong.\nA. Revisiting Multi-Head Attention\nThe transformer model presented in [5] uses scaled dot-\nproduct attention and multi-head attention along with addition\nof positional encoding to the input embedding. The attention\ncomprises of three vectors, viz., Query (Q), Key (K) and Value\n(V), which are mapped to the final output. Accordingly, the\noutput is computed as\nAttention(Q, K, V) =softmax\n\u0012QKT\n√dk\n\u0013\nV, (1)\nwhere dk is the dimension of the key K , and the query Q.\nIn the transformer model, instead of using single attention\nfor all the three vectors Q, K and V, having dimensions\ndmodel, the attention maps are linearly projected onto h\ndifferent heads. Conventionally, we take dmodel = 512 and\nh = 8 for parallel attentions and the dimensions for query,\nkey, and value are taken as dk, dk and dv = dmodel/h = 64,\nrespectively. Subsequently, the scaled dot-product in (1) is\ncomputed for each of the eight heads. The main aim of creating\nmulti-head is that it represents the attention in different sub-\nspaces of same dimension which are learned in parallel.\nNow, let WQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈\nRdmodel×dv are the respective learned weights for the three\nattention vectors Q, K, and V, corresponding to the ith head,\nand WO ∈ Rhdv×dmodel is the weight for the output multi-head\nattention. Then, the ith head may be defined as\nheadi = Attention(QWQ\ni , KWK\ni , VWV\ni ). (2)\nAttention from different subspaces (heads) in (2) are sub-\nsequently concatenated to get the final multi-head attention\noutput\nMultiHead (Q, K, V) =Concat(head1, .., headh)WO. (3)\nB. Multiscaled Multi-Head Attention\nIn the proposed Multiscaled Multi-Head Attention\n(MsMHA), every head processes different dimensional\nsubspace in contrast to [5] which considers same dimensional\nrepresentations. Thus, it extracts contextual information from\ndifferent dimensional subspaces while providing a flexible\nresolution of the given input data. Consequently, the proposed\nmodel learns a pyramid of attention with decreasing resolution\narranged in a pyramidal pattern. Here, the vector dimensions\nof multi-head attention ( dk) decrease gradually for each head.\nN/4\nN\nN/2\nMultiscaled Head \nAttentions (T1)Input Image (X)\nConcat\nMultiscaled Head \nAttentions (T6)\nN/4\nN\nN/2\nFig. 1. The proposed Multiscaled Multi-Head Attention Video Transformer\nNetwork (MsMHA-VTN) with 6 transformer stages (T1-T6) and 8 heads,\nwhere N = L×D is the dimension of the first head in each attention vector,\nD and L are the dimensions of the input tensor. For convenience, pyramid\nscaling at each head by a factor of 1/2 is shown only for three heads.\nLinearLinear Linear\nX\nMatMul\nConcat\nMultiscaled \nHead Attention\nLinear Linear LinearLinear Linear Linear\nX X\nQ1 V1K1 Q2 QhK2 KhV2 Vh\nSoftmax\nMatMul\nMatMul\nSoftmax\nMatMul\nMatMul\nSoftmax\nMatMul\nFig. 2. The proposed Mutiscaled Multi-Head pyramid attention. Pyramid\nattention is an attention mechanism that defines the length of the query,\nkey and value in a pyramid pattern. This allows to exploit the multi-scale\ninformation at different heads. Here, the size of linear block is kept varying\nw.r.t heads to show that dk decreases gradually.\nThis is achieved by scaling down each attention head tensor\nby a factor of 2 as we move up the pyramid. This allows a\nreduced resolution of receptive fields to capture features at\ndifferent scales in each stage. The input to every subspace is\nan image X ∈ RL×D, as shown in Fig. 1, with a dimension\nof input tensor as D and L. In the Multi-Head Attention\n(MHA) [5], the dimension, dk is same for all the heads while\nin the proposed MsMHA, the dimension of key, and query\nvectors i.e., dk varies for each head at every stage, as shown\nin Fig. 2. Mathematically, the input X generates query,\nkey, and value attentions, with linear operations such that\nQ ∈ RL×D, K ∈ RL×D and V ∈ RL×D, respectively. The\nthree attention vectors for the first head are defined as\nQ1D = XW Q\n1D, K1D = XW K\n1D, V1D = XW V\n1D, (4)\nwhere the dimension of each attention vector for the first head\nis N = L × D, since the input dimension is N. For the\nremaining heads, the attention vectors are defined as,\nQjM/2 = XW Q\njM/2, KjM/2 = XW K\njM/2, VjM/2 = XW V\njM/2,\n(5)\nwhere j ∈ {2, h} and M is the dimension of the previous\nhead tensor. It may be noted that the dimensions of query and\nkey progressively decreases with each head in the proposed\ntransformer model, as shown in Table I.\n3\nTABLE I\nDETAILS OF MULTISCALED HEAD ATTENTION TRANSFORMERS\n(MSMHA-VTN) MODEL FOR h = 8.\nHead Attention Tensor size wrt scale\nhead1 L × D\nhead2 L × D/2\nhead3 L × D/4\nhead4 L × D/8\nhead5 L × D/16\nhead6 L × D/32\nhead7 L × D/64\nhead8 L × D/128\nC. Multiscaled Multi-Head Attention-based Transformer\nThe proposed MsMHA learns multiscale attentions from\ndifferent heads and concatenates them to get complete atten-\ntion for a gesture. This provides more focus on important\nfeatures of a gesture with different size that varies from\nsample to sample due to varying pose, scale and size of the\nsigners’ hands. Our proposed multiscaled multi-head attention\nrepeats for six stages which takes frame-level features as input\nfrom ResNet-18 model, as developed in [12]. The ResNet-18\nmodel is initialized with weights pre-trained on the ImageNet\ndatabase [16] in the same way as in [12].\nD. Decision-Level Multi-Modal Fusion\nThe proposed transformer unimodal network can also be\nextended for multimodal gesture recognition by decision com-\nbining using late fusion. Each of the different information cues\navailable in the input data (color, depth, infrared, normals\nand optical flow) from RGB-D sensors, is used to calculate\nclassification score in a unimodal way. In late fusion, the\ndecision-level multimodal fusion is described as\ny = arg max\nj\nmX\ni\nP(ωj|xi), (6)\nwhere m is the total input streams, and P(ωj|xi) is the\nclassification score of the ith input stream of a given input,\nwhich belongs to class ωj.\nIII. E XPERIMENTAL RESULTS\nA. Implementation Details\nThe proposed MsMHA-VTN model was implemented using\nPyTorch=1.6.0 with Nvidia Tesla K80 GPU having 24GB\nRAM, CUDA 10.1 with cuDNN 8.1.1. Initially, the learning\nrate was set to 1e−4 which decays at the 50th and 75th epoch,\nsame as those used in [12]. Also, Adam optimizer was used\nfor training. 40 frames per gesture sequence were given as\ninput to the proposed model.\nB. Results and Discussion\nNVGesture: Table II shows the performance analysis in\ncase of NVGesture dataset for different modalities which is\nachieved using the late fusion discussed in Section II-D. Thus,\nwe observe that the model achieves state-of-the-art results\nwhen depth or normal is given as input. This shows that\nTABLE II\nRESULTS FOR DIFFERENT MODALITIES ON NVG ESTURE DATASET [4] . #\nIS THE NUMBER OF INPUT MODALITIES USED .\n# Input data AccuracyColor Depth IR Normals Optical flow\n1\n81.42%\n85.00%\n70.04%\n86.21%\n2\n84.37%\n70.14%\n83.00%\n86.25%\n87.90%\n83.60%\n85.30%\n85.75%\n74.43%\n3\n85.07%\n87.82%\n85.10%\n86.21%\n4\n87.81%\n77.68%\n88.17%\n84.21%\n85.15%\n5 88.22%\nnormals driven from depth can be used as a good representa-\ntion of hand gestures. Where two and three modalities were\ncombined, the accuracy remained nearly same, around 87.90%\nand 87.22%, respectively. This further improved to 88.17%\nwhen four streams of input were used. The highest accuracy\nof 88.22% was achieved when color, depth, normals, infrared\n(IR) and optical flow images were all given as inputs.\nTable III and Table IV present comparative performance\nresults when compared to some other approaches with NVGes-\nture dataset. It is observed that the proposed model yields\ncomparable or even better results than other methods when\nonly color and normals or all five inputs are utilized.\nBriareo: Table V presents the results for single and mul-\ntimodal approaches applied to Briareo dataset. Recognition\naccuracy as high as 98.67% was obtained for surface normals\ncompared to other modalities. Therefore, it may be inferred\nthat normals which are driven from depth contain more\ninformation about the gesture pattern than other modalities.\nOn the other hand, it is observed that the combination of two\nmodalities (color and optical flow) increases the accuracy to\n98.75%. However, combination of more than three modalities\ndid not provide any improvement. The highest accuracy of\n99.10% was achieved when color, infrared and surface normals\nwere fused.\nTable VI presents the comparative performance results for\ndifferent modalities on Briareo dataset. It is observed that\nthe proposed model outperforms the state-of-the-art methods\nwith 99.10% accuracy. Also, it is observed that our model\nperformed better than other methods even when only surface\nnormals were used for recognition. This shows that data from\nonly one device is sufficient to achieve comparable results with\n4\nTABLE III\nCOMPARISON RESULTS FOR SINGLE MODALITY ON NVG ESTURE\nDATASET [4]\nInput modality Method Accuracy\nColor\nSpat. st. CNN [17] 54.60%\niDT-HOG [18] 59.10%\nRes3ATN [19] 62.70%\nC3D [20] 69.30%\nR3D-CNN [4] 74.10%\nGPM [21] 75.90%\nPreRNN [22] 76.50%\nTransformer [12] 76.50%\nI3D [23] 78.40%\nResNeXt-101 [24] 78.63%\nMTUT [25] 81.33%\nNAS1 [14] 83.61%\nHuman [4] 88.40%\nMsMHA-VTN 81.42%\nDepth\nSNV [26] 70.70%\nC3D [20] 78.80%\nR3D-CNN [4] 80.30%\nI3D [23] 82.30%\nTransformer [12] 83.00%\nResNeXt-101 [24] 83.82%\nPreRNN [22] 84.40%\nMTUT [25] 84.85%\nGPM [21] 85.50%\nNAS1 [14] 86.10%\nMsMHA-VTN 85.00%\nOptical flow\niDT-HOF [18] 61.80%\nTemp. st. CNN [17] 68.00%\nTransformer [12] 72.00%\niDT-MBH [18] 76.80%\nR3D-CNN [4] 77.80%\nI3D [23] 83.40%\nMsMHA-VTN 85.30%\nNormals Transformer [12] 82.40%\nMsMHA-VTN 86.21%\nInfrared\nR3D-CNN [4] 63.50%\nTransformer [12] 64.70%\nMsMHA-VTN 70.04%\nTABLE IV\nCOMPARISON RESULTS FOR MULTI -MODALITIES ON NVG ESTURE\nDATASET [4].\nMethod Inputs modality Accuracy\nTwo-st. CNNs [17] color + flow 65.60%\niDT [18] color + flow 73.00%\nR3D-CNN [4] color + flow 79.30%\nR3D-CNN [4] color + depth + flow 81.50%\nR3D-CNN [4] color + depth + ir 82.00%\nR3D-CNN [4] depth + flow 82.40%\nR3D-CNN [4] all 83.80%\nMSD-2DCNN [21] color+depth 84.00%\n8-MFFs-3f1c[27] color + flow 84.70%\nSTSNN [28] color+flow 85.13%\nPreRNN [22] color + depth 85.00%\nI3D [23] color + depth 83.80%\nI3D [23] color + flow 84.40%\nI3D [23] color + depth + flow 85.70%\nGPM [21] color + depth 86.10%\nMTUTRGB-D [25] color + depth 85.50%\nMTUTRGB-D+flow [25] color + depth 86.10%\nMTUTRGB-D+flow [25] color + depth + flow 86.90%\nTransformer [12] depth + normals 87.30%\nTransformer [12] color + depth + normals+ir 87.60%\nNAS2 [14] color + depth 86.93%\nNAS1+NAS2 [14] color + depth 88.38%\nMsMHA-VTN color + normals 87.90%\nMsMHA-VTN depth + normals + ir + flow 88.17%\nMsMHA-VTN color + depth + normals + 88.22%ir + flow\nTABLE V\nRESULTS FOR DIFFERENT MODALITIES ON BRIAREO DATASET [29]. # IS\nTHE NUMBER OF INPUT MODALITIES USED .\n# Input data AccuracyColor Depth IR Normals Optical flow\n1\n98.15%\n94.44%\n96.29%\n98.67%\n2\n97.22%\n98.14%\n96.30%\n98.15%\n97.22%\n97.22%\n98.75%\n97.30%\n97.27%\n3\n99.07%\n98.15%\n99.10%\n99.07%\n4\n98.61%\n97.68%\n97.22%\n98.61%\n98.14%\n5 98.61%\nTABLE VI\nCOMPARISON RESULTS FOR DIFFERENT MODALITIES ON BRIAREO\nDATASET [29].\nMethod Inputs modality Accuracy\nC3D-HG [29] color 72.20%\nC3D-HG [29] depth 76.00%\nC3D-HG [29] ir 87.50%\nLSTM-HG [29] 3D joint features 94.40%\nNUI-CNN [30] depth + ir 92.00%\nNUI-CNN [30] color + depth + ir 90.90%\nTransformer [12] normals 95.80%\nTransformer [12] depth + normals 96.20%\nTransformer [12] ir + normals 97.20%\nMsMHA-VTN normals 98.67 %\nMsMHA-VTN color + optical 98.75 %\nMsMHA-VTN color + ir + normals 99.10 %\nour proposed MsMHA-VTN model.\nIV. C ONCLUSION\nA multiscaled head attention for transformer model is\nproposed in this work. The proposed model captures the\ncontextual information at different levels to provide more focus\non multiscale features at each stage. Multiscale features at\neach head follows a pyramid hierarchy which replicates at\nevery stage in the transformer model. Analyzing the results,\nas obtained in our experiments, it may be concluded that the\nproposed MsMHA-VTN is able to learn multiscale attentions\nthat significantly improve the performance of dynamic hand\ngesture recognition compared to some of the existing methods,\nparticularly the original transformer model.\n5\nREFERENCES\n[1] D. Avola, M. Bernardi, L. Cinque, G. L. Foresti, and C. Massaroni,\n“Exploiting recurrent neural networks and leap motion controller for\nthe recognition of sign language and semaphoric hand gestures,” IEEE\nTransactions on Multimedia, 21, no. 1, 2018, pp. 234-245.\n[2] L. Zhang, G. Zhu, L. Mei, P. Shen, S. A. A. Shah, and M. Bennamoun,\n“Attention in convolutional LSTM for gesture recognition,” in Proc.\n32nd International Conference on Neural Information Processing Sys-\ntems, 2018, pp. 1957-1966.\n[3] Khodabandelou, G., P. G. Jung , Y . Amirat, and S. Mohammed ,\n“Attention-based gated recurrent unit for gesture recognition,” IEEE\nTransactions on Automation Science and Engineering,18, no. 2, 2020,\npp. 495-507.\n[4] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and J. Kautz,\n“Online detection and classification of dynamic hand gestures with\nrecurrent 3D convolutional neural network,” in Proc. IEEE conference\non computer vision and pattern recognition, 2016, pp. 4207-4215.\n[5] A. Vaswani, N Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc.\nAdvances in neural information processing systems, 2017, pp. 5998-\n6008.\n[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.\nUnterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, “An image is worth 16 ×16 words: Transformers\nfor image recognition at scale,” in Proc. International Conference on\nLearning Representations, 2021.\n[7] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J’egou, “Training data-efficient image transformers & distillation\nthrough attention,” arXiv preprint, 2020, arXiv:2012.12877.\n[8] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video transformer\nnetwork,” arXiv preprint, 2021, arXiv:2102.00719.\n[9] Chen, Chun-Fu, Q. Fan, and R. Panda, “Crossvit: Cross attention multi-\nscale vision transformer for image classification,” arXiv preprint, 2021,\narXiv:2103.14899.\n[10] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yan, J. Malik, and\nC. Feichtenhofer, “Multiscale vision transformers,” arXiv preprint\narXiv:2104.11227, Apr 2021.\n[11] Y . Li , C. Y . Wu, H. Fan, K. Mangalam , B. Xiong, J. Malik, C. Fe-\nichtenhofer, “Improved multiscale vision transformers for classification\nand detection”, arXiv preprint arXiv:2112.01526, Dec 2021.\n[12] A. D. Eusanio, A. Simoni, S. Pini, G. Borghi, R. Vezzani, and R.\nCucchiara, “A Transformer-based network for dynamic hand gesture\nrecognition,” in Proc. International Conference on 3D Vision (3DV),\nIEEE, 2020, pp. 623-632.\n[13] N. Aloysius, M. Geetha, and P. Nedungadi, “Incorporating relative\nposition information in transformer-based sign language recognition and\ntranslation,” IEEE Access, 9, 2021, pp. 145929-145942.\n[14] Z. Yu, B. Zhou, J. Wan, P. Wang, H. Chen, X. Liu, S. Z. Li, and G. Zhao,\n“Searching multi-rate and multi-modal temporal enhanced networks for\ngesture recognition,” IEEE Transactions on Image Processing, 2021.\n[15] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “TMMF:\nTemporal Multi-Modal Fusion for single-stage continuous gesture recog-\nnition,” IEEE Transactions on Image Processing,30, 2021, pp. 7689-\n7701.\n[16] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L. “Imagenet:\nA large-scale hierarchical image database.” in IEEE conference on\ncomputer vision and pattern recognition, pp. 248-255, 2009.\n[17] K. Simonyan, and A. Zisserman, “Two-stream convolutional networks\nfor action recognition in videos,” arXiv preprint arXiv:1406.2199, 2014.\n[18] H. Wang, D. Oneata, J. Verbeek, and C. Schmid, “A robust and efficient\nvideo representation for action recognition,” International journal of\ncomputer vision , 119(3), pp. 219-238, 2016.\n[19] N. Dhingra, and A. Kunz, “Res3atn-deep 3D residual attention network\nfor hand gesture recognition in videos,” in Proc. IEEE International\nConference on 3D Vision (3DV), 2019 pp. 491-501.\n[20] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning\nspatiotemporal features with 3d convolutional networks,” in Proc. IEEE\ninternational conference on computer vision, 2015, pp. 4489-4497.\n[21] D. Fan, H. Lu, S. Xu, and S. Cao, “Multi-task and multi-modal learning\nfor RGB dynamic gesture recognition,” IEEE Sensors Journal, 21, no.\n23, 2021, pp. 27026-27036.\n[22] X. Yang, P. Molchanov, and J. Kautz, “Making convolutional networks\nrecurrent for visual sequence learning,” in Proc. IEEE Conference on\nComputer Vision and Pattern Recognition, Jun. 2018, pp. 6469–6478.\n[23] J. Carreira, and A. Zisserman, “Quo vadis, action recognition? A new\nmodel and the kinetics dataset,” in Proc. IEEE Conference on Computer\nVision and Pattern Recognition, 2017, pp. 6299-6308.\n[24] O. K ¨op¨ukl¨u, A. Gunduz, N. Kose, and G. Rigoll, “Real-time hand ges-\nture detection and classification using convolutional neural networks,”\nin Proc. IEEE International Conference on Automatic Face & Gesture\nRecognition, 2019, IEEE, pp. 1-8.\n[25] M. Abavisani, H. R. V . Joze, and V . M. Patel, “Improving the perfor-\nmance of unimodal dynamic hand-gesture recognition with multimodal\ntraining,” in Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019, pp. 1165–1174.\n[26] X. Yang and Y . Tian., “Super normal vector for activity recognition\nusing depth sequences,” in P . IEEE conference on computer vision and\npattern recognition (CVPR), 2014, pp. 804–811.\n[27] O. Kopuklu, N. Kose, and G. Rigoll., “Motion fused frames: Data level\nfusion strategy for hand gesture recognition,” in Proc. IEEE Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW),\n2018, pp. 2103–2111.\n[28] W. Zhang, J. Wang, and F. Lan, “Dynamic hand gesture recognition\nbased on short-term sampling neural networks,” EEE/CAA Journal of\nAutomatica Sinica, 8, no. 1 (2020): 110-120.\n[29] F. Manganaro, S. Pini, G. Borghi, R. Vezzani, and R. Cucchiara, “Hand\ngestures for the human-car interaction: The Briareo dataset,” in Proc.\nInternational Conference on Image Analysis and Processing (ICIAP),\nSpringer, 2019, pp. 560–571.\n[30] A. D. Eusanio, A. Simoni, S. Pini, G. Borghi, R. Vezzani, R. Cucchiara,\n“Multimodal hand gesture classification for the human–car interaction,”\nInformatics, 2020, 7, 31. https://doi.org/10.3390/informatics7030031.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.712017297744751
    },
    {
      "name": "Gesture recognition",
      "score": 0.6916574239730835
    },
    {
      "name": "Gesture",
      "score": 0.665816605091095
    },
    {
      "name": "Transformer",
      "score": 0.633493959903717
    },
    {
      "name": "Head (geology)",
      "score": 0.48765552043914795
    },
    {
      "name": "Speech recognition",
      "score": 0.4773552417755127
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46605074405670166
    },
    {
      "name": "Computer vision",
      "score": 0.44156765937805176
    },
    {
      "name": "Engineering",
      "score": 0.1916319727897644
    },
    {
      "name": "Electrical engineering",
      "score": 0.08650550246238708
    },
    {
      "name": "Geomorphology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154851008",
      "name": "Indian Institute of Technology Roorkee",
      "country": "IN"
    }
  ]
}