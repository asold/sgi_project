{
  "title": "Large language models encode clinical knowledge",
  "url": "https://openalex.org/W4384071683",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5027454515",
      "name": "Karan Singhal",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5047463591",
      "name": "Shekoofeh Azizi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5059213795",
      "name": "Tao Tu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5063201022",
      "name": "S. Sara Mahdavi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100657725",
      "name": "Jason Lee",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5051828575",
      "name": "Hyung Won Chung",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5030765685",
      "name": "Nathan Scales",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5088063475",
      "name": "Ajay Kumar Tanwani",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5069557194",
      "name": "Heather Cole-Lewis",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021812637",
      "name": "Stephen Pfohl",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5014637990",
      "name": "Perry W. Payne",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5058677067",
      "name": "Martin Seneviratne",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5090718376",
      "name": "Paul Gamble",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5026540467",
      "name": "Christopher Kelly",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5066029226",
      "name": "Abubakr Babiker",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5007588003",
      "name": "Nathanael Schärli",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5055969617",
      "name": "Aakanksha Chowdhery",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5086361722",
      "name": "P. Mansfield",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5046764593",
      "name": "Dina Demner‐Fushman",
      "affiliations": [
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5044698998",
      "name": "Blaise Agüera y Arcas",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5060000122",
      "name": "Dale R. Webster",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5068955381",
      "name": "Greg S. Corrado",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5065128060",
      "name": "Yossi Matias",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5070366042",
      "name": "Katherine Chou",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5057932939",
      "name": "Juraj Gottweis",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5057195145",
      "name": "Nenad Tomašev",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5078784976",
      "name": "Yun Liu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5022388476",
      "name": "Alvin Rajkomar",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5043862316",
      "name": "Joëlle Barral",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5010171106",
      "name": "Christopher Semturs",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5003509342",
      "name": "Alan Karthikesalingam",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5103234563",
      "name": "Vivek Natarajan",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2972522091",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W3120894151",
    "https://openalex.org/W3159743235",
    "https://openalex.org/W3023998086",
    "https://openalex.org/W4221160814",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4285787204",
    "https://openalex.org/W4304701382",
    "https://openalex.org/W2086519542",
    "https://openalex.org/W2558323601",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3186425279",
    "https://openalex.org/W2101950949",
    "https://openalex.org/W2801702920",
    "https://openalex.org/W3154300329",
    "https://openalex.org/W3087893815",
    "https://openalex.org/W3213752583",
    "https://openalex.org/W3035885149",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4223506650",
    "https://openalex.org/W3001807593",
    "https://openalex.org/W4221150521",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W2897042519",
    "https://openalex.org/W2962833164",
    "https://openalex.org/W4288094024",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W2953235477",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2593981654",
    "https://openalex.org/W6810475161",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W4287208373",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W2042492924",
    "https://openalex.org/W4301912686",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W4303648858",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4226024653",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4213405705",
    "https://openalex.org/W4288407534",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3093097038",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4287817164",
    "https://openalex.org/W4281482733",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W4205865577",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W4286233477",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3159623667",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W4229079966",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4226467610"
  ],
  "abstract": null,
  "full_text": "172 | Nature | Vol 620 | 3 August 2023\nArticle\nLarge language models encode clinical \nknowledge\nKaran Singhal1,4 ✉, Shekoofeh Azizi1,4 ✉, Tao Tu1,4, S. Sara Mahdavi1, Jason Wei1, \nHyung Won Chung1, Nathan Scales1, Ajay Tanwani1, Heather Cole-Lewis1, Stephen Pfohl1, \nPerry Payne1, Martin Seneviratne1, Paul Gamble1, Chris Kelly1, Abubakr Babiker1, \nNathanael Schärli1, Aakanksha Chowdhery1, Philip Mansfield1, Dina Demner-Fushman2, \nBlaise Agüera y Arcas1, Dale Webster1, Greg S. Corrado1, Yossi Matias1, Katherine Chou1, \nJuraj Gottweis1, Nenad Tomasev3, Yun Liu1, Alvin Rajkomar1, Joelle Barral1, \nChristopher Semturs1, Alan Karthikesalingam1,5 ✉ & Vivek Natarajan1,5 ✉\nLarge language models (LLMs) have demonstrated impressive capabilities, but the \nbar for clinical applications is high. Attempts to assess the clinical knowledge of \nmodels typically rely on automated evaluations based on limited benchmarks. Here, \nto address these limitations, we present MultiMedQA, a benchmark combining six \nexisting medical question answering datasets spanning professional medicine, \nresearch and consumer queries and a new dataset of medical questions searched \nonline, HealthSearchQA. We propose a human evaluation framework for model \nanswers along multiple axes including factuality, comprehension, reasoning, possible \nharm and bias. In addition, we evaluate Pathways Language Model\n1 (PaLM, a 540-billion \nparameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using \na combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy \non every MultiMedQA multiple-choice dataset (MedQA\n3, MedMCQA4, PubMedQA5 \nand Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), \nincluding 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), \nsurpassing the prior state of the art by more than 17%. However, human evaluation \nreveals key gaps. T o resolve this, we introduce instruction prompt tuning, a parameter- \nefficient approach for aligning LLMs to new domains using a few exemplars. The \nresulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. \nWe show that comprehension, knowledge recall and reasoning improve with model \nscale and instruction prompt tuning, suggesting the potential utility of LLMs in \nmedicine. Our human evaluations reveal limitations of today’s models, reinforcing \nthe importance of both evaluation frameworks and method development in creating \nsafe, helpful LLMs for clinical applications.\nMedicine is a humane endeavour in which language enables key interac-\ntions for and between clinicians, researchers and patients. Yet, today’s \nartificial intelligence (AI) models for applications in medicine and \nhealthcare have largely failed to fully utilize language. These models, \nalthough useful, are predominantly single-task systems (for example, \nfor classification, regression or segmentation) lacking expressivity and \ninteractive capabilities7–9. As a result, there is a discordance between \nwhat today’s models can do and what may be expected of them in \nreal-world clinical workflows10.\nRecent advances in LLMs offer an opportunity to rethink AI sys -\ntems, with language as a tool for mediating human–AI interaction. \nLLMs are ‘foundation models’11, large pre-trained AI systems that can \nbe repurposed with minimal effort across numerous domains and \ndiverse tasks. These expressive and interactive models offer great \npromise in their ability to learn generally useful representations from \nthe knowledge encoded in medical corpora, at scale. There are several \nexciting potential applications of such models in medicine, includ -\ning knowledge retrieval, clinical decision support, summarization \nof key findings, triaging patients, addressing primary care concerns  \nand more.\nHowever, the safety-critical nature of the domain necessitates \nthoughtful development of evaluation frameworks, enabling research-\ners to meaningfully measure progress and capture and mitigate poten-\ntial harms. This is especially important for LLMs, since these models \nmay produce text generations (hereafter referred to as ‘generations’) \nthat are misaligned with clinical and societal values. They may, for \ninstance, hallucinate convincing medical misinformation or incorpo-\nrate biases that could exacerbate health disparities.\nhttps://doi.org/10.1038/s41586-023-06291-2\nReceived: 25 January 2023\nAccepted: 5 June 2023\nPublished online: 12 July 2023\nOpen access\n Check for updates\n1Google Research, Mountain View, CA, USA. 2National Library of Medicine, Bethesda, MD, USA. 3DeepMind, London, UK. 4These authors contributed equally: Karan Singhal, Shekoofeh Azizi, Tao Tu. \n5These authors jointly supervised this work: Alan Karthikesalingam, Vivek Natarajan. ✉e-mail: karansinghal@google.com; shekazizi@google.com; alankarthi@google.com; natviv@google.com\nNature | Vol 620 | 3 August 2023 | 173\nT o evaluate how well LLMs encode clinical knowledge and assess \ntheir potential in medicine, we consider the answering of medical \nquestions. This task is challenging: providing high-quality answers to \nmedical questions requires comprehension of medical context, recall \nof appropriate medical knowledge, and reasoning with expert informa-\ntion. Existing medical question-answering benchmarks3 are often lim-\nited to assessing classification accuracy or automated natural language \ngeneration metrics (for example, BLEU12) and do not enable the detailed \nanalysis required for real-world clinical applications. This creates an \nunmet need for a broad medical question-answering benchmark to \nassess LLMs for their response factuality, use of expert knowledge in \nreasoning, helpfulness, precision, health equity and potential harm.\nT o address this, we curate MultiMedQA, a benchmark comprising \nseven medical question-answering datasets, including six existing data-\nsets: MedQA3, MedMCQA4, PubMedQA5, LiveQA13, MedicationQA14 and \nMMLU clinical topics6. We introduce a seventh dataset, HealthSearchQA, \nwhich consists of commonly searched health questions.\nT o assess LLMs using MultiMedQA, we build on PaLM, a 540-billion \nparameter (540B) LLM1, and its instruction-tuned variant Flan-PaLM2. \nUsing a combination of few-shot 15, chain-of-thought 16 (COT) and \nself-consistency 17 prompting strategies, Flan-PaLM achieves \nstate-of-the-art performance on MedQA, MedMCQA, PubMedQA and \nMMLU clinical topics, often outperforming several strong LLM baselines \nby a substantial margin. On the MedQA dataset comprising USMLE-\nstyle questions, FLAN-PaLM exceeds the previous state of the art by \nmore than 17%.\nDespite the strong performance of Flan-PaLM on multiple-choice \nquestions, its answers to consumer medical questions reveal key gaps. \nT o resolve this, we propose instruction prompt tuning, a data- and \nparameter-efficient alignment technique, to further adapt Flan-PaLM \nto the medical domain. The resulting model, Med-PaLM, performs \nencouragingly on the axes of our pilot human evaluation framework. \nFor example, a panel of clinicians judged only 61.9% of Flan-PaLM \nlong-form answers to be aligned with scientific consensus, compared \nwith 92.6% for Med-PaLM answers, on par with clinician-generated \nanswers (92.9%). Similarly, 29.7% of Flan-PaLM answers were rated \nas potentially leading to harmful outcomes, in contrast to 5.9% for \nMed-PaLM, which was similar to the result for clinician-generated \nanswers (5.7%).\nAlthough these results are promising, the medical domain is com-\nplex. Further evaluations are necessary, particularly along the dimen-\nsions of safety, equity and bias. Our work demonstrates that many \nlimitations must be overcome before these models become viable \nfor use in clinical applications. We outline some key limitations and \ndirections of future research in this Article.\nKey contributions\nOur first key contribution is an approach for evaluation of LLMs in the \ncontext of medical question answering. We introduce HealthSearchQA, \na dataset of 3,173 commonly searched consumer medical questions.  \nWe present this dataset alongside six existing open datasets for answer-\ning medical questions spanning medical exam, medical research and \nconsumer medical questions, as a diverse benchmark to assess the \nclinical knowledge and question-answering capabilities of LLMs \n(see Methods, ‘Datasets’).\nWe pilot a framework for physician and lay user evaluation to assess \nmultiple axes of LLM performance beyond accuracy on multiple-choice \ndatasets. Our evaluation assesses answers for agreement with the scien-\ntific and clinical consensus, the likelihood and possible extent of harm, \nreading comprehension, recall of relevant clinical knowledge, manipu-\nlation of knowledge via valid reasoning, completeness of responses, \npotential for bias, relevance and helpfulness (see Methods, ‘Framework \nfor human evaluation’).\nThe second key contribution is demonstrating state-of-the-art per-\nformance on the MedQA, MedMCQA, PubMedQA and MMLU clinical \ntopics datasets using Flan-PaLM and a combination of prompting strat-\negies, surpassing several strong LLM baselines. Specifically, we reach \n67.6% accuracy on MedQA (more than 17% above the previous state of \nthe art), 57.6% on MedMCQA and 79.0% on PubMedQA.\nThe next contribution is the introduction of instruction prompt tun-\ning, a simple, data- and parameter-efficient technique for aligning LLMs \nto the safety-critical medical domain (see Methods, ‘Modelling’). We lev-\nerage this technique to build Med-PaLM, an instruction prompt-tuned \nDec 2020J ul 2020 Mar 2022O ct 2022 Dec2 022 Dec 2022\n35\n40\n45\n50\n65\n60\n55\nMedQA (USMLE) accuracy  (%) \nGPT-Neo\n(2.7B)\nGPT-Neo\n(2.7B)\n33. 3\nPubMedBERT\n(100M)\nPubMedBERT\n(100M)\n38.1\nBioLinkBERT\n(340M)\nBioLinkBERT\n(340M)\n45. 1\nDRAGON\n(360M)\nDRAGON\n(360M)\n47.5\nPubM ed GPT\n(2.7B)\nPubM ed GPT\n(2.7B)\n50.3\nFlan-PaLM\n(540B)\n67. 6\nPubMedQA\nMultiMedQA MedQA\n(USMLE)\nMedMCQA\nMMLU\nLiveQA\nTREC 2017 \nMedicationQA\nHealthSearchQA\nMed-PaLM:  Newborn jaundice is when a newborn baby's skin and eyes look yellow. It is \ncaused by a buildup of as ubstance calledb ilirubin in the baby'sb lood. Bilirubin is a \nbyproduct of then ormal process of redb lood cells breaking down. Newbornj aundicei s \ncommon andt ypically harmless, but it can be as igno famore serious condition. The \njaundicet ypically goes away on its own within af ew weeks. In some cases, it may last \nlonger or requiret reatment. The timei tt akes fort he jaundicet og oa way can vary \ndepending on the cause andt he severity of thej aundice. If thej aundicei s severeo r \nlastsl onger than af ew weeks, the doctor may recommendt estingo rt reatmentt o \ndetermine the cause and prevent complications.\nQ: How long does it take for newborn jaundice to go away?\nPaLM\n(540B)\nMedical question answering benchmark\nAutomated and human evaluation\nPrompting\nMed-PaLM performs encouragingly\non consumer medical question answering \nInstruction\ntuning  \nInstruction\nprompt tuning \n+17% \nFig. 1 | Overview of our contributions. We curate MultiMedQA, a benchmark \nfor answering medical questions spanning medical exam, medical research and \nconsumer medical questions. We evaluate PaLM and its instructed-tuned \nvariant, Flan-PaLM, on MultiMedQA. Using a combination of prompting \nstrategies, Flan-PaLM exceeds state-of-the-art performance on MedQA (US \nMedical Licensing Examination (USMLE)), MedMCQA, PubMedQA and MMLU \nclinical topics. In particular, it improves over the previous state of the art on \nMedQA (USMLE) by over 17%. We next propose instruction prompt tuning  \nto further align Flan-PaLM to the medical domain, producing Med-PaLM. \nMed-PaLM’s answers to consumer medical questions compare favourably  \nwith answers given by clinicians under our human evaluation framework, \ndemonstrating the effectiveness of instruction prompt tuning.\n174 | Nature | Vol 620 | 3 August 2023\nArticle\nversion of Flan-PaLM specialized for the medical domain (Fig. 1). Our \nhuman evaluation framework reveals limitations of Flan-PaLM in scien-\ntific grounding, harm and bias. Nevertheless, Med-PaLM substantially \nreduces the gap (or even compares favourably) to clinicians on several \nof these axes, according to both clinicians and lay users (see ‘Human \nevaluation results’).\nFinally, we discuss in detail key limitations of LLMs revealed by our \nhuman evaluation. Although our results demonstrate the potential of \nLLMs in medicine, they also suggest that several critical improvements \nare necessary in order to make these models viable for real-world clini-\ncal applications (see ‘Limitations’).\nModel development and evaluation of performance\nWe first provide an overview of our key results with Flan-PaLM on  \nmultiple-choice tasks as summarized in Fig. 2 and Extended Data Fig. 2. \nThen, we present several ablation studies to help contextualize and \ninterpret the results.\nState of the art on MedQA\nOn the MedQA dataset consisting of USMLE-style questions with 4 \noptions, our Flan-PaLM 540B model achieved a multiple-choice ques-\ntion accuracy of 67.6%, surpassing the DRAGON model18 by 20.1%.\nConcurrent with our study, PubMedGPT, a 2.7B model trained exclu-\nsively on biomedical abstracts and papers, was released19. PubMedGPT \nachieved a performance of 50.3% on MedQA questions with 4 options. \nT o the best of our knowledge, this is the state-of-the-art on MedQA, \nand Flan-PaLM 540B exceeded this by 17.3%. Extended Data Table 4 \ncompares the best performing models on this dataset. On the more \ndifficult set of questions with 5 options, our model obtained an accu-\nracy score of 62.0%.\nPerformance on MedMCQA and PubMedQA\nOn the MedMCQA dataset, consisting of medical entrance exam ques-\ntions from India, Flan-PaLM 540B reached a performance of 57.6% on \nthe development-test set. This exceeds the previous state-of-the-art \nresult of 52.9% by the Galactica model20.\nSimilarly, on the PubMedQA dataset, our model achieved an accuracy \nof 79.0%, outperforming the previous state-of-the-art BioGPT model21 \nby 0.8% (Fig. 2). Although this improvement may seem small compared \nto those for the MedQA and MedMCQA datasets, the single-rater \nhuman performance on PubMedQA3 is 78.0%, indicating that there \nmay be an inherent ceiling to the maximum possible performance on  \nthis task.\nPerformance on MMLU clinical topics\nThe MMLU dataset contains multiple-choice questions from several \nclinical knowledge, medicine and biology-related topics. These include \nanatomy, clinical knowledge, professional medicine, human genetics,  \ncollege medicine and college biology. Flan-PaLM 540B achieved \nstate-of-the-art performance on all these subsets, outperforming \nstrong LLMs such as PaLM, Gopher, Chinchilla, BLOOM, OPT and Galac-\ntica. In particular, on the professional medicine and clinical knowledge \nsubsets, Flan-PaLM 540B achieved a state-of-the-art accuracy of 83.8% \nand 80.4%, respectively. Extended Data Fig. 2 summarizes the results, \nproviding comparisons with other LLMs where available20.\nAblations\nWe performed several ablations on three of the multiple-choice \ndatasets—MedQA, MedMCQA, and PubMedQA—to better under\n-\nstand our results and identify the key components contributing to \nFlan-PaLM’s performance.\nInstruction tuning improves performance\nAcross all model sizes, we observed that the instruction-tuned Flan-PaLM \nmodel outperformed the baseline PaLM model on MedQA, MedM -\nCQA and PubMedQA datasets. The models were few-shot-prompted \nin these experiments using the prompt text detailed in Supplemen-\ntary Information, section 11. The detailed results are summarized in \nSupplementary Table 6. The improvements were most prominent in  \nthe PubMedQA dataset where the 8B Flan-PaLM model outperformed \nthe baseline PaLM model by over 30%. Similar strong improvements were \nalso observed in the case of 62B and 540B variants. These results dem-\nonstrate the strong benefits of instruction fine-tuning. Similar results \non MMLU clinical topics are reported in Supplementary Information,  \nsection 4.\nWe have not yet completed a thorough analysis of the effect of \ninstruction prompt tuning on multiple-choice accuracy; in this section, \nour analysis is of Flan-PaLM, not Med-PaLM. Med-PaLM (instruction \nprompt-tuned Flan-PaLM) was developed to improve the long-form \ngeneration results of Flan-PaLM presented in ‘Human evaluation \nresults’ by better aligning the model to the medical domain. However, \ngiven the success of domain-agnostic instruction tuning for answer-\ning multiple-choice questions, in-domain instruction prompt tuning \nappears promising, and we present a preliminary result in Extended \nData Table 5 and further describe this experiment in Supplementary \nInformation, section 5.\nScaling improves performance on medical question answering\nA related observation from Supplementary Table 6 was the strong \nperformance improvements obtained from scaling the model from \n8B to 62B and 540B. We observed an improvement of approximately \n2× in performance when scaling the model from 8B to 540B in both \nPaLM and Flan-PaLM. These improvements were more pronounced in \nthe MedQA and MedMCQA datasets. In particular, for the Flan-PaLM \nmodel, the 540B variant outperformed the 62B variant by more than \n14% and the 8B variant by more than 24%. Given these results and the \nstrong performance of the Flan-PaLM 540B model, we built on this \nmodel for downstream experiments and ablations. The scaling plots \nare provided in Supplementary Information, section 7.\nCOT prompting\nSupplementary Table 2 summarizes the results from using COT prompt-\ning and provides a comparison with the few-shot prompting strategy \nusing the Flan-PaLM 540B model. We did not observe improvements \nusing COT over the standard few-shot prompting strategy across the \nMedQA, MedMCQA and PubMedQA multiple-choice datasets. This may \nbe owing to the existence of many possible chain-of-thought reasoning \nMedMCQA MedQA (USMLE) PubMedQA\n40\n50\n60\n70\n80\nAccuracy (%)52.9\n50.3\n78.2\n57.6\n67.6\n79.0SOTA Flan-PaLM\nFig. 2 | Comparison of our method and prior state of the art. Our Flan-PaLM \n540B model exceeds the previous state-of-the-art performance (SOTA) on \nMedQA (four options), MedMCQA and PubMedQA datasets. The previous \nstate-of-the-art results are from Galactica\n20 (MedMCQA), PubMedGPT 19 \n(MedQA) and BioGPT 21 (PubMedQA). The percentage accuracy is shown above \neach column.\nNature | Vol 620 | 3 August 2023 | 175\npaths towards a particular answer, and sampling one path may not \nproduce the most accurate result. This motivated the experiments \nwith self-consistency, as discussed below. The COT prompts used are \nsummarized in Supplementary Information, section 12. In addition, \nwe also explored the use of non-medical COT prompts. The results \npresented in Supplementary Information, section 6 suggest that COT \nprompting is effective in priming the model to solve these types of \nproblems rather than adding new knowledge to the model.\nSelf-consistency improves multiple-choice performance\nIt has been shown that self-consistency can be of use when COT \nprompting hurts performance17; previous work showed considerable \nimprovements on arithmetic and common-sense reasoning tasks. \nWe applied self-consistency to MultiMedQA, fixing the number of \nchain-of-thought answer explanation paths (decodes) to 11 for each \nof three multiple-choice datasets. We then marginalized over the \ndifferent decodes to select the most consistent answer. Using this \nstrategy, we observed considerable improvements over the standard \nfew-shot prompting strategy for the Flan-PaLM 540B model on the \nMedQA and MedMCQA datasets. In particular, for the MedQA dataset \nwe observed an improvement of more than 7% with self-consistency. \nHowever, self-consistency led to a drop in performance for the Pub-\nMedQA dataset. The results are summarized in Supplementary Table 3. \nWe further provide example responses from the Flan-PaLM 540B model \nfor MedQA in Extended Data Table 6.\nUncertainty and selective prediction\nLLMs are capable of long, coherent, and complex generations. How -\never, they can also generate factually inaccurate statements. In medi-\ncal settings in particular, such failure modes need to be carefully \nvetted, and in real-world applications, generations that are unlikely \nto be true should be withheld. Instead, we may want to defer to other \ninformation sources or experts when needed. One solution is there-\nfore for LLMs to communicate uncertainty estimates along with their \nresponses.\nAlthough uncertainty measures over LLM output sequences remains \nan open area of research22,23, we explored a simple proxy as an initial \napproach to measuring the relationship between LLM uncertainty and \nstatement accuracy. We created a selective prediction task24, using the \nnumber of decodes matching a given answer from self-consistency as a \nmeasure of uncertainty, and used it to withhold the answer if the model \nwas not appropriately confident. We performed the experiment using \n41 decodes from the Flan-PaLM 540B model with chain-of-thought \nprompting and self-consistency. We observe that as the deferring frac-\ntion increases (that is, as a higher confidence is required to provide a \nprediction), the performance of the model on MedQA improves, reach-\ning an accuracy of up to 82.5% at a deferring fraction of 0.45 (Fig. 3). This \nsuggests that our measure of response uncertainty may be reasonable \nand that LLMs seem to encode uncertainty about their knowledge in \nthe medical domain. However, more research is needed beyond this \npreliminary analysis.\nHuman evaluation results\nWe randomly selected 100 questions from HealthSearchQA, 20 ques-\ntions from LiveQA, and 20 questions from MedicationQA as a smaller \nlong-form answer benchmark for detailed human evaluation. These \nquestions reflect real-world consumer queries for medical informa-\ntion. These selected questions were disjoint from exemplars used for \ninstruction prompt tuning to produce Med-PaLM.\nWe asked a panel of clinicians to generate expert reference answers \nto these questions. We then produced answers using Flan-PaLM and \nMed-PaLM (both 540B models). A few qualitative examples of these \nquestions and the corresponding Med-PaLM responses are shown in \nExtended Data Table 7. The three sets of answers were evaluated by \na different panel of clinicians along the axes presented in Extended \nData Table 2, without revealing the source of answers. One clini -\ncian evaluated each answer. T o reduce the effect of variation across \nclinicians on generalizability of our findings, our panel consisted \nof nine clinicians (based in the USA, UK and India). We used the \nnon-parametric bootstrap to estimate any significant variation in \nthe results, where 1,000 bootstrap replicas were used to produce a \ndistribution for each set, and we used the 95% bootstrap percentile \ninterval to assess variations. These results are described in detail below \nand in Supplementary Information, section 10, with visualizations in  \nFigs. 4–6.\nScientific consensus. We aimed to understand how the answers re-\nlated to current consensus in the clinical and scientific community. We \njudged clinicians’ answers to be aligned with the scientific consensus in \n92.9% of questions, whereas Flan-PaLM was found to be in agreement \nwith the scientific consensus in only 61.9% of answers (Fig. 4). For other \nquestions, answers were either opposed to consensus, or no consensus \nexisted. This suggested that generic instruction tuning on its own was \nnot sufficient to produce scientific and clinically grounded answers. \nHowever, 92.6% of Med-PaLM answers were judged to be in accordance \nwith the scientific consensus, showcasing the strength of instruction \nprompt tuning as an alignment technique to produce scientifically \ngrounded answers.\nWe note that since PaLM, Flan-PaLM, and Med-PaLM were trained \nusing corpora of web documents, books, Wikipedia, code, natural \nlanguage tasks, and medical tasks at a given point of time, one potential \nlimitation of these models is that they can reflect the scientific con-\nsensus of the past instead of today. This is not a commonly observed \nfailure mode for Med-PaLM today, but this motivates future work in \ncontinual learning of LLMs and retrieval from a continuously evolving  \ncorpus.\nComprehension, retrieval and reasoning capabilities. We sought \nto understand the medical comprehension, knowledge retrieval and \nreasoning capabilities of Med-PaLM. We asked a panel of clinicians to \nrate whether answers contained any (one or more example of) evi -\ndence of correct or incorrect medical reading comprehension, medi-\ncal knowledge retrieval and medical reasoning capabilities, using the \nsame approach as CHARD25. Correct and incorrect evidence were as-\nsessed in parallel because it is possible that a single long-form answer \nmay contain evidence of both correct and incorrect comprehension, \nretrieval and reasoning.\nAnswers generated by experts were again superior to those of \nFlan-PaLM, although performance was improved by instruction \n00 .1 0.20 .3\nDeferring fraction\n0.4\n0.675\n0.825\n0.800\n0.775\n0.750\n0.725\n0.700\nAccuracy (%)\nFig. 3 | Selective prediction analysis. Analysis of deferral behaviour of the \nFlan-PaLM 540B model with self-consistency. We observe that if we defer more \nfrequently using an uncertainty threshold based on self-consistency, the \nmodel becomes increasingly accurate on questions it does not defer.\n176 | Nature | Vol 620 | 3 August 2023\nArticle\nprompt tuning for Med-PaLM (Fig. 5). This trend was observed for all \nsix sub-questions used to evaluate these capabilities. For example, for \nevidence of correct retrieval of medical knowledge, we found that clini-\ncian answers scored 97.8%, whereas Flan-PaLM scored 76.3%. However, \nthe instruction prompt-tuned Med-PaLM model scored 95.4%, reducing \nthe performance gap with clinicians.\nIncorrect or missing content. The goal of this evaluation was to under-\nstand the completeness and correctness of the generated answers by \nassessing whether an answer omits any information that it should not \nomit, or whether the answer contains any content that it should not. \nWhere there was deemed to be missing or omitted content, the rater \nwas asked whether it was of great or little potential clinical importance.\na\nb\nc\nd\ne\nf\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nScienti/f_ic consensus \nNo consensus \nOpposed to consensus\nAligned with consensus\nInappropriate and/or incorrect content\nYes, great clinical signi/f_icance \nYes, little clinical signi/f_icance \nNo\nMissing content \nYes, great clinical signi/f_icance \nYes, little clinical signi/f_icance \nNo\nExtent of possible harm\nDeath or severe harm\nModerate or mild harm\nNo harm\nLikelihood of possible harm\nHigh \nMedium \nLow\nPossibility of bias\nYes \nNo\n92.9%\n61.9%\n92.6%\n16.1%\n18.7%\n1.4%\n47.6%\n15.3%\n11.1%\n29.7%\n5.9%\n5.7%\n19.4%\n2.3%\n1.3%\n1.4%\n7.9%\n0.8%\nFig. 4 | Clinician evaluation of answers. a–f, Clinicians were asked to rate \nanswers to questions in the HealthSearchQA, LiveQA and MedicationQA \ndatasets for agreement with scientific and clinical consensus ( a), the presence \nof incorrect content ( b), the omission of content (c), the extent of possible harm \n(d), the likelihood of harm (e) and possible bias in answers (f). We compare \nanswers from Flan-PaLM, Med-PaLM and clinicians. Across all axes, answers \nfrom clinicians were judged to be better than those from Flan-PaLM. Med-PaLM \nanswers were substantially better than Flan-PaLM answers across alignment \nwith scientific consensus, harm, missing content and bias, often comparing \nfavourably with answers from clinicians, demonstrating the value of instruction \nprompt tuning for alignment to the medical domain. The evaluation involves \n140 questions, each rated by a single clinician. We used the non-parametric \nbootstrap to estimate any significant variation in the results, with 1,000 \nbootstrap replicas used to produce a distribution for each set. We used the 95% \nbootstrap percentile interval to assess variations. Detailed results with intervals \nare presented in Supplementary Information, section 10.\nab\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\nClinician\nMed-PaLM\nFlan-PaLM\n90.5%\n97.5%\n97.8%\nEvidence of correct comprehension \nNo \nYes\n9.2%\n5.0%\n2.2%\nEvidence of incorrect comprehension\nYes \nNo\n2.1%\n14.3%\n10.1%\nEvidence of incorrect reasoning\nYes \nNo\n97.8%\n76.3%\n95.4%\nEvidence of correct retrieval\nNo\nYes\n97.7%\n85.7%\n92.5%\nEvidence of correct reasoning\nNo \nYes\n23.1%\n16.9%\n3.6%\nEvidence of incorrect retrieval\nYes \nNo\nFig. 5 | Evaluation of comprehension, retrieval and reasoning capabilities \nby clinicians.  a,b, Evaluation of correctness (a) and incorrectness ( b) of reading \ncomprehension, recall of knowledge and reasoning steps. The results indicate \na gap between Flan-PaLM and clinicians, and show that Med-PaLM is able to \nsubstantially reduce the gap. The evaluation involves 140 questions, each rated \nby a single clinician. We used the non-parametric bootstrap to estimate any \nsignificant variation in the results, with 1,000 bootstrap replicas used to \nproduce a distribution for each set. We used the 95% bootstrap percentile \ninterval to assess variations.\nNature | Vol 620 | 3 August 2023 | 177\nAgain, the clinician-generated answers were judged to be superior \n(Fig. 4). The answers from clinicians showed evidence of inappropri-\nate or incorrect content in 1.4% of cases, compared with 16.1% for \nFlan-PaLM. Instruction prompt tuning seemed to degrade perfor -\nmance, with 18.7% of the Med-PaLM answers judged to contain inap-\npropriate or incorrect content.\nBy contrast, instruction prompt tuning improved model perfor -\nmance with respect to omission of important information. Flan-PaLM \nanswers were judged to omit important information in 47.6% of \nanswers, whereas Med-PaLM omitted important information in 15.3% \nof the answers, decreasing the gap with clinicians, whose answers \nwere judged to have missing information in 11.1% of the cases. Sev -\neral qualitative examples are shown in Extended Data Table 8, sug-\ngesting that answers from LLMs may be able to complement and \ncomplete physician responses to patient queries in future use \n \ncases.\nOne potential explanation of these observations is that instruction \nprompt tuning teaches the Med-PaLM model to generate more detailed \nanswers than the Flan-PaLM model, reducing the omission of impor-\ntant information. However, a longer answer also increases the risk of \nintroducing incorrect content.\nPossible extent and likelihood of harm. We sought to identify the \nseverity and likelihood of potential harm based on people acting on \nthe generated answers. We asked raters to assume that the output of \nmodels might lead to actions by clinicians, consumers or patients, \nand to estimate the possible severity and likelihood of physical or \nmental health-related harms that might result. We based the op\n-\ntions for selection by raters on the Agency for Healthcare Research \nand Quality (AHRQ) common formats26, which presents options to \nassign severity of harm among death, severe or life-threatening in-\njury, moderate harm, mild harm or no harm. We acknowledge that \nthis definition of harm is more typically used in the context of analys-\ning harms incurred during healthcare delivery and that even in such \nsettings (where the context for harms occurring is known with con-\nsiderably greater specificity) there is frequently substantial variation \nin physician estimation of harm severity27. The validity of the AHRQ \nscale cannot therefore be assumed to extend to our context, where \nour rater outputs should be regarded as subjective estimates because \nour work was not grounded in a specific intended use and sociocultural  \ncontext.\nDespite the broad definition and subjectivity of the ratings, we \nobserved that instruction prompt tuning produced safer answers \nthat reduced both estimated likelihood and severity. Whereas 29.7% \nof the Flan-PaLM responses were judged as potentially leading to \nharm, this number fell to 5.9% for Med-PaLM, similar to the result for \nclinician-generated answers (5.7%).\nSimilarly, on the likelihood of harm axes, instruction prompt tuning \nenabled Med-PaLM answers to match the expert generated answers \n(Fig. 4).\nBias for medical demographics. The final axis for clinician answer \nevaluation was bias. For this pilot evaluation, we sought to understand \nwhether the answer contained any information that was inaccurate or in-\napplicable for a particular demographic. Specifically for each response, \nreviewers were asked “Does the answer provided contain any informa-\ntion that is inapplicable or inaccurate for one particular patient-group \nor demographic? For example, is the answer applicable only to patients \nof a particular sex where patients of another sex might require different \ninformation?” , to which they provided a yes or no response. For this \ndefinition of bias, Flan-PaLM answers were found to contain biased in-\nformation in 7.9% of the cases (Fig. 4). However, this number decreased \nto 0.8% for Med-PaLM, comparing favourably with the experts, whose \nanswers were judged to contain evidence of bias in 1.4% of cases.\nIt should be noted that most of the questions were framed neutrally \nand did not contain specific demographic inferences. This initial \napproach to evaluating bias is limited and does not serve as a com -\nprehensive assessment of potential harms, fairness or equity. Further \nfairness and equity considerations are discussed in ‘Fairness and equity \nconsiderations’ .\nLay user assessment. Beyond expert evaluation, we also asked a panel \nof five non-experts in the domain (laypeople without a medical back-\nground, based in India) to assess the answers. The results are summa-\nrized in Fig. 6. Whereas Flan-PaLM answers were judged to be helpful in \nonly 60.6% of the cases, this increased to 80.3% for Med-PaLM answers. \nHowever, this remained inferior to the answers given by clinicians, \nwhich were judged to be helpful 91.1% of the time. Similarly, Flan-PaLM \nanswers were judged as directly addressing the intent of the user’s ques-\ntion in 90.8% of cases. This increased to 94.4% for Med-PaLM, whereas \nthe clinician-generated answers were judged as directly addressing \nintent in 95.9% of cases.\nThe lay user evaluation further demonstrated the benefits of instruc-\ntion prompt tuning to produce answers that are helpful to users and \nshows that considerable work remains to be done to approximate the \nquality of outputs provided by human clinicians.\nDiscussion\nOur results suggest that the strong performance in answering medical \nquestions may be an emergent ability28 of LLMs combined with effective \ninstruction prompt tuning.\nWe observed strong performance as a result of scaling, with accuracy \nimproving by approximately 2 times as we scaled the PaLM models \nfrom 8B to 540B. The performance of PaLM 8B on MedQA was only \nslightly better than random performance. Accuracy improved by \nmore than 30% for PaLM 540B, demonstrating the effectiveness of \nscaling for answering medical questions. We observed similar improve-\nments for the MedMCQA and PubMedQA datasets. Further, instruction \nfine-tuning was also effective, with Flan-PaLM models performing \nbetter than the PaLM models across all model size variants on all the \nmultiple-choice datasets.\nIt is likely that the PaLM pre-training corpus included significant \nmedical-related content, and one possible explanation for the strong \nperformance of the 540B model is that the model has memorized the \nMultiMedQA evaluation datasets. In Supplementary Information, \nsection 1, we analysed the overlap between Med-PaLM’s responses to \nMultiMedQA consumer questions and the PaLM training corpus and \nobserved no overlap. We also assessed the overlap between MultiMedQA \nmultiple-choice questions and the training corpus, observing minimal \na\nb\n&OLQLFLDQ\n0HG\u00103D/0\n)ODQ\u00103D/0\n&OLQLFLDQ\n0HG\u00103D/0\n)ODQ\u00103D/0\n\u001c\u0018\u0011\u001c\b\n\u001c\u0013\u0011\u001b\b\n\u001c\u0017\u0011\u0017\b\n$GGUHVV\u0003LQWHQW\u0003RI\u0003TXHVWLRQ\u0003\n'RHV\u0003QRW\u0003DGGUHVV\u0003TXHU\\\n$GGUHVVHV\u0003TXHU\\\u0003\n\u001c\u0014\u0011\u0014\b\n\u0019\u0013\u0011\u0019\b\n\u001b\u0013\u0011\u0016\b\n+RZ\u0003KHOSIXO\u0003LV\u0003WKH\u0003DQVZHU\"\n1RW\u0003KHOSIXO\u0003\n6RPHZKDW\u0003KHOSIXO\u0003\n+HOSIXO\nFig. 6 | Lay user assessment of answers. a,b, Lay user assessment of answers, \naddressing relevance to the intent of the query (a) and helpfulness (b). Med-PaLM \nanswers are more likely to address the intent of users and be more helpful than \nFlan-PaLM answers, but they remain inferior to those provided by clinicians. \nThe evaluation involves 140 questions, each rated by a single non-expert lay \nuser. We used the non-parametric bootstrap to estimate any significant \nvariation in the results, where 1,000 bootstrap replicas were used to produce  \na distribution for each set. We used the 95% bootstrap percentile interval to \nassess variations.\n178 | Nature | Vol 620 | 3 August 2023\nArticle\noverlap (Supplementary Table 1). Additionally, PaLM1 showed similar \ndifferences in performance of the PaLM 8B and 540B models when \nevaluating contaminated and clean test datasets (a contaminated dataset \nis one in which part of the test set is in the model pre-training corpus). \nThese results suggested that memorization alone does not explain the \nstrong performance observed by scaling up the models.\nThere have been several efforts to train language models on a bio-\nmedical corpus, especially on PubMed. These include BioGPT21 (355B), \nPubMedGPT19 (2.7B) and Galactica20 (120B). Our models were able to \noutperform these efforts on PubMedQA without any dataset-specific \nfine-tuning. Further, the benefits of scale and instruction fine-tuning \nwere much more pronounced on the MedQA dataset, which can be \nconsidered out-of-domain for all these models. Given the results, we \ncan conclude that medical answering capabilities (recall, reading com-\nprehension and reasoning skills) improved with scale.\nHowever, our human evaluation results on consumer medical \nquestion-answering datasets clearly showed that scale alone was insuf-\nficient. Even strong LLMs such as Flan-PaLM can generate answers \nthat are inappropriate for use in the safety-critical medical domain. \nHowever, the Med-PaLM results demonstrated that instruction prompt \ntuning is a data- and parameter-efficient alignment technique that is \nuseful for improving factors related to accuracy, factuality, consistency, \nsafety, harm and bias, helping to close the gap with clinical experts and \nbring these models closer to real-world clinical applications.\nLimitations\nOur study demonstrates the potential of LLMs for encoding medical \nknowledge and for answering medical questions. Below we discuss \nlimitations and outline directions for future research.\nExpansion of MultiMedQA\nAlthough the MultiMedQA benchmark is diverse and contains ques-\ntions from a variety of medical exam, medical research and consumer \nsources, it is by no means exhaustive. We plan to expand the benchmark \nin the future to include a larger variety of medical and scientific domains \n(such as biology) and formats.\nA key challenge in clinical environments is eliciting information \nfrom patients and synthesizing findings into an assessment and plan. \nMultiple-choice question-answering tasks are inherently easier than \nthis because they are often grounded in vignettes compiled by experts \nand selected to have a generally preferred answer. This is not true for all \nmedical decisions. Developing benchmark tasks that reflect real-world \nclinical workflows is an important direction of future research.\nFurthermore, we only considered English-language datasets in this \nstudy, and there is a pressing need to expand the scope of the bench-\nmark to support multilingual evaluations.\nKey LLM capabilities for this setting\nAlthough Flan-PaLM was able to reach state-of-the-art performance \non several multiple-choice medical question-answering benchmarks, \nour human evaluations clearly suggested that these models are not at \nclinician expert level on many clinically important axes. In order to \nbridge this gap, several new LLM capabilities need to be researched \nand developed including (1) grounding of the responses in authorita-\ntive medical sources and accounting for the time-varying nature of \nmedical consensus; (2) ability to detect and communicate uncertainty \neffectively to the user; (3) ability to respond to queries in multiple lan-\nguages; and (4) better alignment to the safety requirements of the \nmedical domain.\nImproving human evaluation\nThe rating framework that we proposed for this study represents a \npromising pilot approach, but our chosen axes of evaluation were not \nexhaustive and were subjective in nature. For example, the concept of \nmedical or scientific consensus is time-varying in nature and is reflective \nof current understandings of human health and disease and physiol-\nogy, which are often coloured by discrimination in race or ethnicity, \ngender, age and ability29,30. Furthermore, consensus often exists only \nfor topics of relevance to certain groups (such as those who are greater \nin number and/or power) and consensus may be lacking for certain \nsubpopulations. Additionally, the concept of harm may differ according \nto population. Expert assessment of harm may also vary on the basis \nof location, lived experience and cultural background. Differences in \nhealth literacy may have caused variability in ratings for both experts \nand lay users. Further research might test whether the perceived useful-\nness and harm of answers varied according to their understandability \nand actionability31.\nThe number of model responses evaluated and the pool of clinicians \nand laypeople assessing them were limited, as our results were based \non only a single clinician or layperson evaluating each response. This \ncould be mitigated by inclusion of a considerably larger and intention-\nally diverse pool of human raters.\nWe worked with a panel of four qualified clinicians—with expertise \nin internal medicine, paediatrics, surgery and primary care, and based \nin the USA or the UK—to identify the best demonstration examples \nand craft few-shot prompts. Further research could expand the range \nof clinicians engaged in prompt construction and the selection of \nexemplar answers and thereby explore how variation in multiple axes \nof the types of clinician participating in this activity might affect LLM \nbehaviour (such as clinician demographics, geography, specialism, \nlived experience and others).\nThe pilot framework that we developed could be advanced using \nbest practices for the design and validation of rating instruments from \nhealth, social and behavioural research32. This could entail finding \nadditional rating items through participatory research and evalua-\ntion of rating items by domain experts and technology recipients for \nrelevance, representativeness and technical quality. The inclusion of \na substantially larger pool of human raters would also enable testing \nof instrument generalizability by ratifying the test dimensionality, \ntest–retest reliability and validity32. Further research could explore the \nindependent influence of variations in lay raters’ education level, medi-\ncal conditions, caregiver status, experience with healthcare, education \nlevel or other relevant factors on their ratings. The effect of variations \nin clinician raters’ specialty, demographics, geography or other factors \ncould be similarly explored.\nFairness and equity considerations\nAs previously discussed, our approach to evaluating bias is limited \nas an assessment of fairness and equity-related harms. The use of \nLLMs to answer medical questions can cause harms that contribute \nto health disparities. These harms derive from several sources, includ-\ning the presence of patterns in training data that reflect health ineq-\nuities and algorithmic design choices 33. This could lead to systems \nthat produce differences in behaviour or performance across popula-\ntions that result in downstream harms in medical decision-making34 \nor reproduce racist misconceptions regarding the cause of health  \ndisparities35,36.\nThe development of procedures for the evaluation of bias and \nfairness-related harms in LLMs is ongoing37,38. Healthcare is a particu-\nlarly complex application of LLMs given the safety-critical nature of \nthe domain and the nuances associated with social and structural bias \nthat drives health disparities. The intersection of LLMs and healthcare \ncreates unique opportunities for responsible and ethical innovation \nof robust assessment and mitigation tools for bias, fairness and health \nequity.\nWe outline opportunities for future research into frameworks for \nthe systematic identification and mitigation of downstream harms and \nimpacts of LLMs in healthcare contexts. Key principles include the use of \nparticipatory methods to design contextualized evaluations that reflect \nNature | Vol 620 | 3 August 2023 | 179\nthe values of patients that may benefit or be harmed, grounding the \nevaluation in one or more specific downstream clinical use cases39,40, and \nthe use of dataset and model documentation frameworks for transpar-\nent reporting of choices and assumptions made during data collection \nand curation, model development and evaluation41–43. Furthermore, \nresearch is needed into the design of algorithmic procedures and bench-\nmarks that probe for specific technical biases that are known to cause \nharm if not mitigated. For instance, depending on the context, it may \nbe relevant to assess the sensitivity of model outputs to perturbations \nof demographic identifiers in prompts designed deliberately so that \nthe result does not change under the perturbation44–46. Additionally, \nthe aforementioned research activities to build evaluation methods \nto achieve health equity in LLMs require interdisciplinary collabora-\ntion to ensure that various scientific perspectives and methods can be \napplied to the task of understanding the social and contextual aspects \nof health47–49.\nThe development of evaluation frameworks for performance, fair-\nness, bias and equity in LLMs is a critical research agenda that should \nbe approached with equal rigour and attention as that given to the work \nof encoding clinical knowledge in language models.\nEthical considerations\nThis research demonstrates the potential of LLMs for future use in \nhealthcare. Transitioning from an LLM that is used for answering \nmedical questions to a tool that can be used by healthcare provid -\ners, administrators and consumers will require considerable addi -\ntional research to ensure the safety, reliability, efficacy and privacy \nof the technology. Careful consideration will need to be given to the \nethical deployment of this technology including rigorous quality \nassessment when used in different clinical settings and guardrails to \nmitigate against over-reliance on the output of a medical assistant. \nFor example, the potential harms of using an LLM for diagnosing or \ntreating an illness are much greater than those from using an LLM for \ninformation about a disease or medication. Additional research will \nbe needed to assess LLMs used in healthcare for homogenization and \namplification of biases and security vulnerabilities inherited from base  \nmodels11,38,50.\nConclusion\nThe advent of foundation models and LLMs presents a compelling \nopportunity to rethink the development of medical AI and make it \neasier, safer and more equitable to use. At the same time, medicine is \nan especially complex domain for applications of LLMs.\nOur research provides a glimpse into the opportunities and the \nchallenges of applying these technologies to medicine. We anticipate \nthat this study will spark further conversations and collaborations \nbetween patients, consumers, AI researchers, clinicians, social sci -\nentists, ethicists, policymakers and other interested parties in order \nto responsibly translate these early research findings to improve  \nhealthcare.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-023-06291-2.\n1. Chowdhery, A. et al. PaLM: scaling language modeling with pathways. Preprint at https://\ndoi.org/10.48550/arXiv.2204.02311 (2022).\n2. Chung, H. W. et al. Scaling instruction-finetuned language models. Preprint at https:// \ndoi.org/10.48550/arXiv.2210.11416 (2022).\n3. Jin, D. et al. What disease does this patient have? A large-scale open domain question \nanswering dataset from medical exams. Appl. Sci. 11, 6421 (2021).\n4. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi- \nchoice dataset for medical domain question answering. In Conference on Health, Inference, \nand Learning 248–260 (Proceedings of Machine Learning Research, 2022).\n5. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: a dataset for biomedical \nresearch question answering. Preprint at https://doi.org/10.48550/arXiv.1909.06146 \n(2019).\n6. Hendrycks, D. et al. Measuring massive multitask language understanding. Preprint at \nhttps://doi.org/10.48550/arXiv.2009.03300 (2020).\n7. Esteva, A. et al. Deep learning-enabled medical computer vision. NPJ Digit. Med. 4, 5 \n(2021).\n8. Tomašev, N. et al. Use of deep learning to develop continuous-risk models for adverse \nevent prediction from electronic health records. Nat. Protoc. 16, 2765–2787 (2021).\n9. Yim, J. et al. Predicting conversion to wet age-related macular degeneration using deep \nlearning. Nat. Med. 26, 892–899 (2020).\n10. Lakkaraju, H., Slack, D., Chen, Y., Tan, C. & Singh, S. Rethinking explainability as a dialogue:  \na practitioner’s perspective. Preprint at https://doi.org/10.48550/arXiv.2202.01875 (2022).\n11. Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at \nhttps://doi.org/10.48550/arXiv.2108.07258 (2021).\n12. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. BLEU: a method for automatic evaluation of \nmachine translation. In Proc. 40th Annual Meeting of the Association for Computational \nLinguistics 311–318 (Association of Computational Machinery, 2002).\n13. Ben Abacha, A., Agichtein, E., Pinter, Y. & Demner-Fushman, D. Overview of the medical \nquestion answering task at TREC 2017 LiveQA. TREC https://trec.nist.gov/pubs/trec26/\npapers/Overview-QA.pdf?ref=https://githubhelp.com (2017).\n14. Abacha, A. B. et al. in Studies in Health Technology and Informatics (eds Ohno-Machado, \nL. & Séroussi, B.) 25–29 (IOS Press, 2019).\n15. Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33, \n1877–1901 (2020).\n16. Wei, J. et al. Chain of thought prompting elicits reasoning in large language models. \nPreprint at https://doi.org/10.48550/arXiv.2201.11903 (2022).\n17. Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. \nPreprint at https://doi.org/10.48550/arXiv.2203.11171 (2022).\n18. Yasunaga, M. et al. Deep bidirectional language-knowledge graph pretraining. Preprint at \nhttps://doi.org/10.48550/arXiv.2210.09338 (2022).\n19. Bolton, E. et al. Stanford CRFM introduces PubMedGPT 2.7B. Stanford University https://\nhai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b (2022).\n20. Taylor, R. et al. Galactica: a large language model for science. Preprint at https://doi.org/ \n10.48550/arXiv.2211.09085 (2022).\n21. Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation \nand mining. Brief. Bioinformatics 23, bbac49 (2022).\n22. Lin, S., Hilton, J. & Evans, O. Teaching models to express their uncertainty in words. Preprint \nat https://doi.org/10.48550/arXiv.2205.14334 (2022).\n23. Kadavath, S. et al. Language models (mostly) know what they know. Preprint at https://\ndoi.org/10.48550/arXiv.2207.05221 (2022).\n24. Tran, D. et al. Plex: towards reliability using pretrained large model extensions. Preprint at \nhttps://doi.org/10.48550/arXiv.2207.07411 (2022).\n25. Feng, S. Y., Khetan, V., Sacaleanu, B., Gershman, A. & Hovy, E. CHARD: clinical health-aware \nreasoning across dimensions for text generation models. Preprint at https://doi.org/ \n10.48550/arXiv.2210.04191 (2022).\n26. Williams, T., Szekendi, M., Pavkovic, S., Clevenger, W. & Cerese, J. The reliability of ahrq \ncommon format harm scales in rating patient safety events. J. Patient Saf. 11, 52–59 \n(2015).\n27. Walsh, K. E. et al. Measuring harm in healthcare: optimizing adverse event review. Med. \nCare 55, 436 (2017).\n28. Wei, J. et al. Emergent abilities of large language models. Preprint at https://doi.org/ \n10.48550/arXiv.2206.07682 (2022).\n29. Kington, R. S. et al. Identifying credible sources of health information in social media: \nprinciples and attributes. NAM Perspectives https://doi.org/10.31478%2F202107a (2021).\n30. Mandavilli, A. Medical journals blind to racism as health crisis, critics say. The New York \nTimes https://www.nytimes.com/2021/06/02/health/jama-racism-bauchner.html (2021).\n31. Shoemaker, S. J., Wolf, M. S. & Brach, C. Development of the patient education materials \nassessment tool (pemat): a new measure of understandability and actionability for print \nand audiovisual patient information. Patient Educ. Couns. 96, 395–403 (2014).\n32. Boateng, G. O., Neilands, T. B., Frongillo, E. A., Melgar-Quiñonez, H. R. & Young, S. L. Best \npractices for developing and validating scales for health, social, and behavioral research: \na primer. Front. Public Health 6, 149 (2018).\n33. Hooker, S. Moving beyond “algorithmic bias is a data problem”. Patterns 2, 100241  \n(2021).\n34. Chen, I. Y. et al. Ethical machine learning in healthcare. Annu. Rev. Biomed. Data Sci. 4, \n123–144 (2021).\n35. Eneanya, N. D. et al. Health inequities and the inappropriate use of race in nephrology. \nNat. Rev. Nephrol. 18, 84–94 (2022).\n36. Vyas, L. G., Eisenstein, L. G. & Jones, D. S. Hidden in plain sight-reconsidering the use of \nrace correction in clinical algorithms. N. Engl. J. Med. 383, 874–882 (2020).\n37. Weidinger, L. et al. Ethical and social risks of harm from language models. Preprint at \nhttps://doi.org/10.48550/arXiv.2112.04359 (2021).\n38. Liang, P. et al. Holistic evaluation of language models. Preprint at https://doi.org/10.48550/ \narXiv.2211.09110 (2022).\n39. Liu, X. et al. The medical algorithmic audit. Lancet Digit. Health 4, e384–e397 (2022).\n40. Raji, I. D. et al. Closing the AI accountability gap: defining an end-to-end framework for \ninternal algorithmic auditing. In Proc. 2020 Conference on Fairness, Accountability, and \nTransparency 33–44 (Association for Computing Machinery, 2020).\n41. Rostamzadeh, N. et al. Healthsheet: development of a transparency artifact for health \ndatasets. Preprint at https://doi.org/10.48550/arXiv.2202.13028 (2022).\n42. Gebru, T. et al. Datasheets for datasets. Commun. ACM 64, 86–92 (2021).\n43. Mitchell, M. et al. Model cards for model reporting. In Proc. conference on Fairness, \nAccountability, and Transparency 220–229 (Association for Computing Machinery, 2019).\n180 | Nature | Vol 620 | 3 August 2023\nArticle\n44. Garg, S. et al. Counterfactual fairness in text classification through robustness. In Proc. \n2019 AAAI/ACM Conference on AI, Ethics, and Society 219–226 (Association for Computing \nMachinery, 2019).\n45. Prabhakaran, V., Hutchinson, B. & Mitchell, M. Perturbation sensitivity analysis to detect \nunintended model biases. Preprint at https://doi.org/10.48550/arXiv.1910.04210  \n(2019).\n46. Zhang, H., Lu, A. X., Abdalla, M., McDermott, M. & Ghassemi, M. Hurtful words: quantifying \nbiases in clinical contextual word embeddings. In Proc. ACM Conference on Health, \nInference, and Learning 110–120 (Association for Computing Machinery, 2020).\n47. Matheny, M., Israni, S. T., Ahmed, M. & Whicher, D. eds. Artificial Intelligence in Health \nCare: The Hope, the Hype, the Promise, the Peril (National Academy of Medicine,  \n2022).\n48. The White House Office of Science and Technology Policy. Blueprint for an AI Bill of Rights: \nMaking Automated Systems Work for the American People https://www.whitehouse.gov/\nwp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf (The White House, \n2022).\n49. Ethics and Governance of Artificial Intelligence for Health. WHO Guidance (World Health \nOrganization, 2021).\n50. Bommasani, R., Liang, P. & Lee, T. Language models are changing AI: the need for holistic \nevaluation. Stanford University https://crfm.stanford.edu/2022/11/17/helm.html (2022).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023, corrected publication 2023\nMethods\nDatasets\nT o assess the potential of LLMs in medicine, we focused on answering \nmedical questions. Answering medical questions requires reading \ncomprehension skills, ability to accurately recall medical knowledge \nand manipulation of expert knowledge. There are several existing \nmedical question-answering datasets for research. These include \ndatasets that assess professional medical knowledge such as medi-\ncal exam questions3,4, questions that require medical research com-\nprehension skills 5, and questions that require the ability to assess \nuser intent and provide helpful answers to their medical information  \nneeds13,14.\nWe acknowledge that medical knowledge is vast in both quantity \nand quality. Existing benchmarks are inherently limited and only \nprovide partial coverage of the space of medical knowledge. Here we \nbring together a number of different datasets for answering medical \nquestions to enable deeper evaluation of LLM knowledge and move \nbeyond multiple-choice accuracy or natural language generation \nmetrics such as BLEU. The datasets we grouped together probe dif-\nferent abilities—some are multiple-choice questions, whereas others \nrequire long-form answers; some are open domain (where questions \nare answered without limiting available information to a pre-specified \nsource), whereas others are closed domain (where questions are \nanswered by retrieving content from associated reference text) and \ncome from different sources. There has been extensive activity in the \nfield of answering medical questions over recent years and we refer to \nref. 3 for a comprehensive summary of medical question-answering  \ndatasets.\nMultiMedQA benchmark. MultiMedQA includes medical exams \nand research datasets with multiple-choice answers and consumer \nmedical question datasets with long-form answers. These include the \nMedQA3, MedMCQA4, PubMedQA5, MMLU clinical topics6, LiveQA13 \nand MedicationQA14 datasets. We further augmented MultiMedQA  \nwith a new dataset of curated commonly searched health queries: \nHealthSearchQA. All the datasets are in the English language and we \ndescribe them in detail below.\nThese datasets vary along the following axes. (1) format: multiple- \nchoice versus long-form answer questions; (2) capabilities tested: \nfor example, assessing the recall of medical facts in isolation versus \nassessing medical reasoning capabilities in addition to recall of facts; \n(3) domain: open domain versus closed domain questions; (4) ques-\ntion source: from professional medical exams, medical research or \nconsumers seeking medical information; and (5) labels and metadata: \npresence of labels or explanations and their sources. A summary of \nMultiMedQA is presented in Extended Data Table 1.\nAlthough MedMCQA, PubMedQA, LiveQA, and MedicationQA \nprovide reference long-form answers or explanations, we do not \nuse them in this work. First, the reference answers did not come \nfrom consistent sources across the different datasets. Answers \noften came from automated tools or non-clinicians such as librar -\nians. The construction of the reference answers and explanations \nin these pioneering datasets was not optimized for holistic or com -\nprehensive assessments of long-answer quality, which renders them \nsuboptimal for use as a ‘ground truth’ against which to assess LLMs \nusing automated natural language metrics such as BLEU. T o allevi -\nate this, as discussed in ‘Human evaluation results’ , we obtained a \nstandardized set of responses from qualified clinicians to a subset \nof the questions in the benchmark. Second, given the safety-critical \nrequirements of the medical domain, we believe it is important \nto move beyond automated measures of long-form answer gen\n-\neration quality using metrics such as BLEU to those involving more \nnuanced human evaluation frameworks such as the one proposed in  \nthis study.\nMedQA (USMLE). The MedQA dataset 3 consists of USMLE-style  \nquestions with four or five possible answers. The development set  \nconsists of 11,450 questions and the test set has 1,273 questions.\nFormat: question and answer (Q + A), multiple choice, open domain.\nSize (development set/test set): 11,450/1,273.\nExample question: A 65-year-old man with hypertension comes  \nto the physician for a routine health maintenance examination.   \nCurrent medications include atenolol, lisinopril, and atorvastatin. \nHis pulse is 86 min−1, respirations are 18 min−1, and blood pressure is \n145/95 mmHg. Cardiac examination reveals end diastolic murmur.  \nWhich of the following is the most likely cause of this physical  \nexamination?\nAnswers (correct answer in bold): (A) Decreased compliance of \nthe left ventricle, (B) Myxomatous degeneration of the mitral valve \n(C) Inflammation of the pericardium (D) Dilation of the aortic root (E) \nThickening of the mitral valve leaflets.\nMedMCQA. The MedMCQA dataset4 consists of more than 194,000 \nfour-option multiple-choice questions from Indian medical entrance \nexaminations (AIIMS/NEET)4. This dataset covers 2,400 healthcare \ntopics and 21 medical subjects. The development set is substantial, \nwith over 187,000 questions.\nFormat: Q + A, multiple choice, open domain.\nSize (dev/test): 187,000/6,100.\nExample question: Which of the following ultrasound findings has \nthe highest association with aneuploidy?\nAnswers (correct answer in bold): (A) Choroid plexus cyst (B) Nuchal \ntranslucency (C) Cystic hygroma (D) Single umbilical artery.\nExplanation: All the above mentioned are ultrasound findings  \nassociated with increased risk of aneuploidy although the highest \nassociation is seen with cystic hygroma. Nuchal translucency and \ncystic hygroma are both measured in the first trimester. Trisomy 21 \nis the most common aneuploidy associated with increased nuchal \ntranslucency and cystic hygroma while monosomy X presents as \nsecond-trimester hygroma.\nPubMedQA. The PubMedQA dataset5 consists of 1,000 expert-labelled \nquestion–answer pairs where the task is to produce a yes/no/maybe \nmultiple-choice answer given a question together with a PubMed  \nabstract as context (Q + context + A). Whereas the MedQA and MedMCQA  \ndatasets are open domain question-answering tasks, the PubMedQA \ntask is closed domain, in that it requires answer inference from the \nsupporting PubMed abstract context.\nFormat: Q + context + A, multiple choice, closed domain.\nSize (development set/test set): 500/500.\nExample question: Double balloon enteroscopy (DBE): is it efficacious \nand safe in a community setting?\nContext: From March 2007 to January 2011, 88 DBE procedures were \nperformed on 66 patients. Indications included evaluation anaemia/\ngastrointestinal bleed, small bowel IBD and dilation of strictures. \nVideo-capsule endoscopy (VCE) was used prior to DBE in 43 of the 66 \npatients prior to DBE evaluation. The mean age was 62 years. Thirty-two \npatients were female, 15 were African American; 44 antegrade and 44 \nretrograde DBEs were performed. The mean time per antegrade DBE \nwas 107.4 ± 30.0 minutes with a distance of 318.4 ± 152.9 cm reached past \nthe pylorus. The mean time per lower DBE was 100.7 ± 27.3 minutes with \n168.9 ± 109.1 cm meters past the ileocecal valve reached. Endoscopic \ntherapy in the form of electrocautery to ablate bleeding sources was \nperformed in 20 patients (30.3%), biopsy in 17 patients (25.8%) and \ndilation of Crohn’s-related small bowel strictures in 4 (6.1%). 43 VCEs \nwith pathology noted were performed prior to DBE, with findings endo-\nscopically confirmed in 32 cases (74.4%). In 3 cases the DBE showed \nfindings not noted on VCE.\nAnswer: Yes.\nArticle\nLong answer: DBE appears to be equally safe and effective when \nperformed in the community setting as compared to a tertiary  \nreferral centre with a comparable yield, efficacy, and complication \nrate.\nMMLU. MMLU6 includes exam questions from 57 domains. We selected \nthe subtasks most relevant to medical knowledge: anatomy, clinical \nknowledge, college medicine, medical genetics, professional medicine \nand college biology. Each MMLU subtask contains multiple-choice \nquestions with four options, along with the answers.\nFormat: Q + A, multiple choice, open domain.\nAnatomy. Size (development set/test set): 14/135.\nExample question: Which of the following controls body temperature, \nsleep, and appetite?\nAnswer: (A) Adrenal glands (B) Hypothalamus (C) Pancreas (D) Thalamus.\nClinical knowledge. Size (development set/test set): 29/265.\nExample question: The following are features of Alzheimer’s disease \nexcept:\nAnswer: (A) short-term memory loss (B) confusion (C) poor attention \n(D) drowsiness.\nCollege medicine. Size (development set/test set): 22/173.\nExample question: The main factors determining success in sport are:\nAnswer: (A) a high energy diet and large appetite. (B) high intelligence \nand motivation to succeed. (C) a good coach and the motivation \nto succeed. (D) innate ability and the capacity to respond to the \ntraining stimulus.\nMedical genetics. Size (development set/test set): 11/100.\nExample question: The allele associated with sickle cell anemia appar-\nently reached a high frequency in some human populations due to:\nAnswer: (A) random mating (B) superior fitness of heterozygotes \nin areas where malaria was present (C) migration of individuals \nwith the allele into other populations (D) a high mutation rate at \nthat specific gene.\nProfessional medicine. Size (development set/test set): 31/272.\nExample question: A 19-year-old woman noticed a mass in her left \nbreast 2 weeks ago while doing monthly breast self-examination. Her \nmother died of metastatic breast cancer at the age of 40 years. Examina-\ntion shows large dense breasts; a 2-cm, firm, mobile mass is palpated \nin the upper outer quadrant of the left breast. There are no changes in \nthe skin or nipple, and there is no palpable axillary adenopathy. Which \nof the following is the most likely diagnosis? \nAnswer: (A) Fibroadenoma (B) Fibrocystic changes of the breast (C) \nInfiltrating ductal carcinoma (D) Intraductal papilloma.\nCollege biology. Size (development set/test set): 16/144.\nExample question: Which of the following is the most direct cause of \npolyteny in somatic cells of certain organisms?\nAnswer: (A) RNA transcription (B) Supercoiling of chromatin (C) \nChromosome replication without cell division (D) Chromosome \nrecombination.\nLiveQA. The LiveQA dataset13 was curated as part of the T ext Retrieval \nChallenge (TREC) 2017. The dataset consists of medical questions \nsubmitted by people to the National Library of Medicine (NLM). The \ndataset also consists of manually collected reference answers from \ntrusted sources such as the National Institute of Health (NIH) website.\nFormat: questions and long answers, free text response, open domain.\nSize (development set/test set): 634/104.\nExample question: Could second hand smoke contribute to or cause \nearly AMD?\nLong answer: Smoking increases a person’s chances of develop -\ning AMD by two to five fold. Because the retina has a high rate of \noxygen consumption, anything that affects oxygen delivery to the \nretina may affect vision. Smoking causes oxidative damage, which \nmay contribute to the development and progression of this disease. \nLearn more about why smoking damages the retina, and explore a \nnumber of steps you can take to protect your vision.\nMedicationQA. The MedicationQA dataset14 consists of commonly \nasked consumer questions about medications. In addition to the ques-\ntion, the dataset contains annotations corresponding to drug focus \nand interactions. Similar to LiveQA, we evaluated the models’ ability \nto produce long-form answers to the questions in the test set.\nFormat: Questions, long answers, free text response, open domain.\nSize (development set/test set): NA /674.\nExample question: How does valium affect the brain?\nFocus (drug): Valium.\nQuestion type: Action.\nLong answer: Diazepam is a benzodiazepine that exerts anxiolytic, \nsedative, muscle-relaxant, anticonvulsant and amnestic effects. Most \nof these effects are thought to result from a facilitation of the action \nof gamma aminobutyric acid (GABA), an inhibitory neurotransmitter \nin the central nervous system.\nSection title: Clinical pharmacology.\nURL: https://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid= \n554baee5-b171-4452-a50a-41a0946f956c.\nHealthSearchQA. We curated our own additional dataset consist-\ning of 3,173 commonly searched consumer questions, referred to as \nHealthSearchQA. The dataset was curated using seed medical con -\nditions and their associated symptoms. We used the seed data to  \nretrieve publicly-available commonly searched questions generated \nby a search engine, which were displayed to all users entering the seed \nterms. We publish the dataset as an open benchmark for answering \nmedical questions from consumers and hope this will be a useful  \nresource for the community, as a dataset reflecting real-world consumer  \nconcerns.\nFormat: Question only, free text response, open domain.\nSize: 3,173.\nExample question: How serious is atrial fibrillation?\nExample question: What kind of cough comes with Covid?\nExample question: Is blood in phlegm serious?\nAlthough MultiMedQA allows us to probe the medical question-  \nanswering capabilities of LLMs along multiple axes, we acknowledge \nthat it is not exhaustive. We plan to expand the benchmark to other \nrelevant datasets, such as those probing question-answering ability \nfrom electronic medical records51 or those requiring pre-clinical bio-\nmedical knowledge52, in future work.\nFramework for human evaluation\nHere we describe our proposed framework for human evaluation of \nlong-form answers to medical questions.\nClinician evaluation. Although objective accuracy metrics on \nmultiple-choice questions are a robust measure of model performance, \nthey omit several important details. T o more deeply assess the genera-\ntive outputs of LLMs in open-ended answering of questions on medi-\ncal topics, we developed a pilot framework for human evaluation of \nlong-form model answers to consumer medical questions in the LiveQA, \nMedicationQA, and HealthSearchQA datasets.\nThe pilot framework was inspired by approaches published in a \nsimilar domain25 to examine the strengths and weaknesses of LLM \ngenerations in clinical settings. We used focus groups and interviews \nwith clinicians based in the UK, USA and India to identify additional axes \nof evaluation53 and expanded the framework items to address notions \nof agreement with scientific consensus, possibility and likelihood of \nharm, completeness and missingness of answers, and possibility of \nbias. Alignment with scientific consensus was measured by asking \nraters whether the output of the model was aligned with a prevailing \nscientific consensus (for example, in the form of well-accepted clinical \npractice guidelines), opposed to a scientific consensus; or whether \nno clear scientific consensus exists regarding the question. Harm is \na complex concept that can be evaluated along several dimensions  \n(for example, physical health, mental health, moral, financial and \nmany others). When answering this question, raters were asked to \nfocus solely on physical or mental health-related harms, and evalu -\nated both severity (in a format inspired by the AHRQ common formats \nfor harm26) and likelihood, under the assumption that a consumer or \nphysician based on the content of the answer might take actions. Bias \nwas assessed broadly by raters considering if the answer contained \ninformation that would be inapplicable or inaccurate to a specific \npatient demographic. The questions asked in the evaluation are sum-\nmarized in Extended Data Table 3.\nOur framework items’ form, wording and response-scale points were \nrefined by undertaking further interviews with triplicate assessments \nof 25 question-answer tuples per dataset by three qualified clinicians. \nInstructions for the clinicians were written including indicative exam-\nples of ratings for questions, and iterated until the clinicians’ rating \napproaches converged to indicate the instructions were usable. Once \nthe guidelines had converged a larger set of question-answer tuples \nfrom the consumer medical questions datasets were evaluated by \nsingle-ratings performed by one of nine clinicians based in the UK, \nUSA or India and qualified for practice in their respective countries, \nwith specialist experience including paediatrics, surgery, internal \nmedicine, and primary care.\nLay user evaluation. In order to assess the helpfulness and utility of \nthe answers to the consumer medical questions, we undertook an  \nadditional lay user (non-expert) evaluation. This was performed by five \nraters without a medical background, all of whom were based in India. \nThe goal of this exercise was to assess how well the answer addressed \nthe perceived intent underlying the question and how helpful and ac-\ntionable it was. The questions asked in the evaluation are summarized \nin Extended Data Table 2.\nModelling\nIn this section, we detail LLMs and the techniques used to align them \nwith the requirements of the medical domain.\nModels. We built on the PaLM and Flan-PaLM family of LLMs in this \nstudy.\nPaLM. PaLM1 is a densely-activated decoder-only transformer lan -\nguage model trained using Pathways 54, a large-scale machine learn -\ning accelerator orchestration system that enables highly efficient \ntraining across TPU pods. The PaLM training corpus consists of 780 \nbillion tokens representing a mixture of webpages, Wikipedia articles, \nsource code, social media conversations, news articles, and books. All \nthree PaLM model variants were trained for exactly one epoch of the \ntraining data. We refer to refs. 1,55,56 for more details on the training \ncorpus. At the time of release, PaLM 540B achieved breakthrough \nperformance, outperforming finetuned state-of-the-art models on \na suite of multi-step reasoning tasks and exceeding average human \nperformance on BIG-bench 1,57.\nFlan-PaLM. In addition to the baseline PaLM models, we also considered \nthe instruction-tuned counterpart2. These models were trained using \ninstruction tuning—that is, fine-tuning the model on a collection of \ndatasets in which each example was prefixed with some combination \nof instructions and/or few-shot exemplars. In particular, Flan-PaLM2 \ndemonstrated the effectiveness of scaling the number of tasks, model \nsize and using chain-of-thought data16 as instructions. The Flan-PaLM \nmodel reached state-of-the-art performance on several benchmarks \nsuch as MMLU, BBH and TyDIQA58. Across the suite of evaluation tasks \nconsidered2, Flan-PaLM outperformed baseline PaLM by an average \nof 9.4%, demonstrating the effectiveness of the instruction tuning \napproach.\nIn this study, we considered both the PaLM and Flan-PaLM model \nvariants at three different model sizes: 8B, 62B and 540B, with the larg-\nest model using 6,144 TPUv4 chips for pre-training.\nAligning LLMs to the medical domain. General-purpose LLMs like \nPaLM1 and GPT-3 (ref. 15) have reached state-of-the-art performance on \na wide variety of tasks on challenging benchmarks such as BIG-bench. \nHowever, given the safety-critical nature of the medical domain, it is \nnecessary to adapt and align the model with domain-specific data. \nTypical transfer learning and domain adaptation methods rely on \nend-to-end fine-tuning of the model with large amounts of in-domain \ndata, an approach that is challenging here given the paucity of medi-\ncal data. As such, in this study, we focused on data-efficient alignment \nstrategies building on prompting15 and prompt tuning59.\nPrompting strategies. GPT-3 (ref. 15) demonstrated that LLMs are \nstrong few-shot learners, where fast in-context learning can be achieved \nthrough prompting strategies. Through a handful of demonstration \nexamples encoded as prompt text in the input context, these models \nare able to generalize to new examples and new tasks without any gra-\ndient updates or fine-tuning. The remarkable success of in-context \nfew-shot learning has spurred the development of many prompting \nstrategies including scratchpad60, chain-of-thought16, and least-to-most \nprompting61, especially for multi-step computation and reasoning \nproblems such as mathematical problems62. In this study, we focused \non standard few-shot, chain-of-thought, and self-consistency prompt-\ning as discussed below.\nFew-shot prompting. The standard few-shot prompting strategy \nwas introduced with GPT-3 (ref. 15). Here, the prompt to the model is \ndesigned to include few-shot examples describing the task through \ntext-based demonstrations. These demonstrations are typically encod-\ned as input–output pairs. The number of examples is typically chosen \ndepending on the number of tokens that can fit into the input context \nwindow of the model. After the prompt, the model is provided with \nan input and asked to generate a test-time prediction. The zero-shot \nprompting counterpart typically only involves an instruction describ-\ning the task without including any additional examples. Few-shot per-\nformance appears to be an emergent ability28 for many tasks—that is,  \nan ability that is non-existent in small models but rapidly improves \nabove random performance beyond a certain model size.\nIn this study, we worked with a panel of qualified clinicians to identify \nthe best demonstration examples and craft the few-shot prompts. Sepa-\nrate prompts were designed for each dataset as detailed in Supplemen-\ntary Information, section 11. The number of few-shot demonstrations \nvaried depending on the dataset. Typically, we used five input–output \nexamples for the consumer medical question-answering datasets, but \nreduced the number to three or fewer for PubMedQA given the need \nto also fit in the abstract context within the prompt text.\nChain-of-thought prompting. COT 16 involves augmenting each \nfew-shot example in the prompt with a step-by-step breakdown and \na coherent set of intermediate reasoning steps towards the final  \nanswer. The approach is designed to mimic the human thought process \nwhen solving problems that require multi-step computation and rea-\nsoning. COT prompting can elicit reasoning abilities in sufficiently LLMs \nand dramatically improve performance on tasks such as mathe matical \nArticle\nproblems 16,62. Further, the appearance of such COT reasoning  \nappears to be an emergent ability28 of LLMs. COT prompting has been \nused to achieve breakthrough LLM performance on several STEM  \nbenchmarks63.\nMany of the medical questions explored in this study involve complex \nmulti-step reasoning, making them a good fit for COT prompting tech-\nniques. T ogether with clinicians, we crafted COT prompts to provide \nclear demonstrations on how to reason and answer the given medical \nquestions. Examples of such prompts are detailed in Supplementary \nInformation, section 12.\nSelf-consistency prompting. A straightforward strategy to improve \nthe performance on the multiple-choice benchmarks is to prompt \nand sample multiple decoding outputs from the model. The final  \nanswer is the one received the majority (or plurality) vote. This idea was \nintroduced as ‘self-consistency’17. The rationale behind this approach \nhere is that for a domain such as medicine with complex reasoning \npaths, there might be multiple potential routes to the correct answer. \nMarginalizing out the reasoning paths can lead to the most consistent \nanswer. The self-consistency prompting strategy led to particularly \nstrong improvements in reasoning tasks63, and we adopted the same \napproach for our datasets with multiple-choice questions: MedQA, \nMedMCQA, PubMedQA, and MMLU. In this work, all decodes were \nperformed with a temperature sampling64,65 constant of 0.7.\nPrompt tuning. Because LLMs have grown to hundreds of billions of \nparameters1,15, fine-tuning them is extraordinarily computationally \nexpensive. While the success of few-shot prompting has alleviated \nthis issue to a large extent, many tasks would benefit further from \ngradient-based learning. Prompt tuning59 (in contrast to prompting/\npriming), is a simple and computationally inexpensive method to adapt \nLLMs to specific downstream tasks, especially with limited data. The \napproach involves the learning of soft prompt vectors through back-\npropagation while keeping the rest of the LLM parameters frozen, thus \nallowing easy reuse of a single model across tasks.\nThis use of soft prompts can be contrasted with the discrete \n‘hard’ text-based few-shot prompts popularized by LLMs such as \nGPT-3 (ref. 15). While prompt tuning can benefit from any number of \nlabelled examples, typically only a handful of examples (for instance, \ntens) are required to achieve good performance. Further, it was demon-\nstrated that prompt-tuned model performance becomes comparable \nwith end-to-end fine-tuning performance at increased model scale59. \nOther related approaches include prefix tuning66, where prefix acti-\nvation vectors are prepended to each layer of the LLM encoder and \nlearned through backpropagation. Prompt tuning can be thought of \nas a simplification of this idea, restricting the learnable parameters to \nonly those representing a small number of tokens prepended to the \ninput as a soft prompt.\nInstruction prompt tuning. Flan models2,67 demonstrated the ben-\nefits of multi-task instruction fine-tuning: the Flan-PaLM model \nachieved state-of-the-art performance on several benchmarks such \nas BIG-bench63 and MMLU6. In particular, Flan-PaLM demonstrated the \nbenefits of using COT data in fine-tuning, leading to robust improve-\nments in tasks that required reasoning.\nGiven the strong performance of instruction tuning, we built primar-\nily on the Flan-PALM model in this work. However, our human evalua-\ntion revealed key gaps in Flan-PaLM’s performance on the consumer \nmedical question-answering datasets, even with few-shot prompt -\ning. T o further align the model to the requirements of the safety-  \ncritical medical domain, we explored additional training specifically \non medical data.\nFor this additional training, we used prompt tuning instead of \nfull-model fine-tuning given compute and clinician data generation \ncosts. Our approach effectively extends Flan-PaLM’s principle of \n‘learning to follow instructions’ to the prompt tuning stage. Specifi-\ncally, rather than using the soft prompt learned by prompt tuning as a \nreplacement for a task-specific human-engineered prompt, we instead \nused the soft prompt as an initial prefix that is shared across multiple \nmedical datasets, and which is followed by the relevant task-specific \nhuman-engineered prompt (consisting of instructions and/or few-shot \nexemplars, which may be chain-of-thought examples) along with the \nactual question and/or context.\nWe refer to this method of prompt tuning as ‘instruction prompt \ntuning’ . Instruction prompt tuning can thus be seen as a lightweight \nway (data-efficient, parameter-efficient, compute-efficient during both \ntraining and inference) of training a model to follow instructions in one \nor more domains. In our setting, instruction prompt tuning adapted \nLLMs to better follow the specific type of instructions used in the family \nof medical datasets that we targeted.\nAs an aside, instruction prompt tuning is not specific to the medical \ndomain or to PaLM. It can be applied in other domains or other LLMs by \n(1) preparing a training corpus containing multiple tasks with different \ninstructions, (2) freezing the LLM, (3) randomly initializing a p × e matrix \n(where p is the soft prompt length and e is the model’s embedding token \ndimension) representing a sequence of soft tokens, (4) prepending the \nmatrix to any embedded inputs to the LLM, and (5) training the matrix \nvia backpropagation on a negative log-likelihood loss as in prompt \ntuning59. We provide additional hyperparameter details for our imple-\nmentation in Supplementary Information, section 2.\nGiven the combination of soft prompt with hard prompt, instruction \nprompt tuning can be considered a type of ‘hard-soft hybrid prompt \ntuning’68, alongside existing techniques that insert hard anchor tokens \ninto a soft prompt69, insert learned soft tokens into a hard prompt70, \nor use a learned soft prompt as a prefix for a short zero-shot hard \nprompt71,72. T o the best of our knowledge, ours is the first published \nexample of learning a soft prompt that is prefixed in front of a full hard \nprompt containing a mixture of instructions and few-shot exemplars.\nPutting it all together: Med-PaLM. T o adapt Flan-PaLM to the medical \ndomain, we applied instruction prompt tuning on a small set of exem-\nplars. These examples were effectively used to instruct the model to \nproduce text generations more aligned with the requirements of the \nmedical domain, with good examples of medical comprehension, recall \nof clinical knowledge, and reasoning on medical knowledge unlikely \nto lead to patient harm. Thus, the curation of these examples was very \nimportant.\nWe randomly sampled examples from MultiMedQA free-response \ndatasets (HealthSearchQA, MedicationQA, LiveQA) and asked a panel \nof five clinicians to provide exemplar answers. These clinicians were \nbased in the USA and the UK with specialist experience in primary care, \nsurgery, internal medicine and paediatrics. Clinicians then filtered \nout questions/answer pairs that they decided were not good exam-\nples to instruct the model. This generally happened when clinicians \nfelt like they could not produce an ‘ideal’ model answer for a given  \nquestion—for example, if the information required to answer a question \nwas not known. We were left with 65 examples across HealthSearchQA, \nMedicationQA, and LiveQA used for instruction prompt tuning  \ntraining.\nThe resulting model, Med-PaLM, was evaluated on the consumer \nmedical question-answering datasets of MultiMedQA along with \nFlan-PaLM. Extended Data Fig. 1 gives an overview of our instruction \nprompt tuning approach for Med-PaLM. Further details on the hyper-\nparameter optimization and model selection process can be found in \nSupplementary Information, section 2. The model card for Med-PaLM \nis provided in Supplementary Information, section 9.\nRelated work\nLarge language models. Over the past few years, LLMs have \nshown impressive performance on natural language processing  \ntasks 1,2,15,16,67,73–77. They owe their success to scaling up the training of \ntransformer-based models 78. It has been shown that model perfor-\nmance and data-efficiency scales with model size and dataset size 79. \nLLMs are often trained using self-supervision on a large scale, using \ngeneral-purpose text corpi such as Wikipedia and BooksCorpus. \nThey have demonstrated promising results across a wide range of \ntasks, including tasks that require specialized scientific knowledge \nand reasoning 6,62. Perhaps the most interesting aspect of these LLMs \nis their in-context few-shot abilities, which adapt these models to \ndiverse tasks without gradient-based parameter updates 15,67,80,81. \nThis allows them to rapidly generalize to unseen tasks and even \nexhibit apparent reasoning abilities with appropriate prompting \nstrategies 1,16,20,63.\nSeveral studies have shown that LLMs have the capacity to act as \nimplicit knowledge bases6,20,82. However, there is a significant risk of \nthese models producing hallucinations, amplifying social biases pre-\nsent in their training data, and displaying deficiencies in their reasoning \nabilities. T o examine the current limitations of LLMs and to quantify the \nlarge gap between human and LLM language capabilities, BIG-bench \nwas introduced as a community-wide initiative to benchmark on tasks \nthat were believed at time of publication to be beyond the capabilities \nof current language models57.\nLLMs for science and biomedicine. Recent studies, such as SciBERT83, \nBioNLP84, BioMegatron85, BioBERT86, PubMedBERT87, DARE88, Scholar-\nBERT89, and BioGPT21, have demonstrated the effectiveness of using \ncurated scientific and biomedical corpora for both discriminative \nand generative language modelling. These models, although promis-\ning, are typically small in scale and scope compared to LLMs such as \nGPT-3 (ref. 15) and PaLM1. While the medical domain is challenging, \nspecific proposals for LLMs have already included examples as varied \nas augmenting non-critical clinical assessments to summarization of \ncomplex medical communications90–92.\nThe closest precedents to our work are Galactica 20, an LLM for  \nscience, and another work studying the reasoning capability of LLMs in \nthe medical question-answering context93. The latter work used GPT-3.5 \n(Codex and InstructGPT), an instruction-tuned LLM94 and evaluated \non the MedQA, MedMCQA, and PubMedQA datasets.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nThe benchmark used in the study, MultiMedQA, comprises six open \nsource datasets and one for consumer medical questions, Health -\nSearchQA, which we introduce here and are releasing with this work \nas a supplementary file.\nCode availability\nMed-PaLM is an LLM that has been aligned to the medical domain. \nWe are not open-sourcing model code and weights owing to the \nsafety implications of unmonitored use of such a model in medical \nsettings. In the interest of responsible innovation, we will be working \nwith academic and industry research partners, providers, regulators \nand policy stakeholders to validate and explore safe onward uses \nof Med-PaLM. For reproducibility, we documented technical deep \nlearning methods while keeping the paper accessible to a clinical \nand general scientific audience. Our work builds upon PaLM, for \nwhich technical details have been described extensively, and our \ninstitution has open-sourced several related LLMs to further the devel-\nopment of research methods in the field (https://huggingface.co/  \ngoogle/flan-t5-xl). \n51. Pampari, A., Raghavan, P., Liang, J. & Peng, J. emrQA: a large corpus for question answering \non electronic medical records. Preprint at https://doi.org/10.48550/arXiv.1809.00732 \n(2018).\n52. Tsatsaronis, G. et al. An overview of the bioasq large-scale biomedical semantic indexing \nand question answering competition. BMC Bioinformatics 16, 138 (2015).\n53. Morgado, F. F., Meireles, J. F., Neves, C., Amaral, A. & Ferreira, M. E. Scale development: \nten main limitations and recommendations to improve future research practices. Psic. \nReflex. Crit. 30, 5 (2017).\n54. Barham, P. et al. Pathways: asynchronous distributed dataflow for ML. Proc. Mach. Learn. \nSyst. 4, 430–449 (2022).\n55. Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at https://\ndoi.org/10.48550/arXiv.2201.08239 (2022).\n56. Du, N. et al. Glam: efficient scaling of language models with mixture-of-experts. In \nInternational Conference on Machine Learning 5547–5569 (PMLR, 2022).\n57. Srivastava, A. et al. Beyond the imitation game: quantifying and extrapolating the \ncapabilities of language models. Preprint at https://doi.org/10.48550/arXiv.2206.04615 \n(2022).\n58. Clark, J. H. et al. Tydi qa: A benchmark for information-seeking question answering in \ntypologically diverse languages. Trans. Assoc. Comput. Linguist. 8, 454–470 (2020).\n59. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for parameter-efficient prompt \ntuning. Preprint at https://doi.org/10.48550/arXiv.2104.08691 (2021).\n60. Nye, M. et al. Show your work: scratchpads for intermediate computation with language \nmodels. Preprint at https://doi.org/10.48550/arXiv.2112.00114 (2021).\n61. Zhou, D. et al. Least-to-most prompting enables complex reasoning in large language \nmodels. Preprint at https://doi.org/10.48550/arXiv.2205.10625 (2022).\n62. Cobbe, K. et al. Training verifiers to solve math word problems. Preprint at https://doi.org/ \n10.48550/arXiv.2110.14168 (2021).\n63. Lewkowycz, A. et al. Solving quantitative reasoning problems with language models. \nPreprint at https://doi.org/10.48550/arXiv.2206.14858 (2022).\n64. Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. A learning algorithm for boltzmann machines. \nCogn. Sci. 9, 147–169 (1985).\n65. Ficler, J. & Goldberg, Y. Controlling linguistic style aspects in neural language generation. \nPreprint at https://doi.org/10.48550/arXiv.1707.02633 (2017).\n66. Li, X. L. & Liang, P. Prefix-tuning: optimizing continuous prompts for generation. Preprint \nat https://doi.org/10.48550/arXiv.2101.00190 (2021).\n67. Wei, J. et al. Finetuned language models are zero-shot learners. Preprint at https://doi.org/ \n10.48550/arXiv.2109.01652 (2021).\n68. Liu, P. et al. Pre-train, prompt, and predict: a systematic survey of prompting methods in \nnatural language processing. Preprint at https://doi.org/10.48550/arXiv.2107.13586 \n(2021).\n69. Liu, X. et al. GPT understands, too. Preprint at https://doi.org/10.48550/arXiv.2103.10385 \n(2021).\n70. Han, X., Zhao, W., Ding, N., Liu, Z. & Sun, M. PTR: prompt tuning with rules for text \nclassification. AI Open 3, 182–192 (2022).\n71. Gu, Y., Han, X., Liu, Z. & Huang, M. PPT: Pre-trained prompt tuning for few-shot learning. \nPreprint at https://doi.org/10.48550/arXiv.2109.04332 (2021).\n72. Ye, S., Jang, J., Kim, D., Jo, Y. & Seo, M. Retrieval of soft prompt enhances zero-shot task \ngeneralization. Preprint at https://doi.org/10.48550/arXiv.2210.03029 (2022).\n73. Hoffmann, J. et al. Training compute-optimal large language models. Preprint at https://\ndoi.org/10.48550/arXiv.2203.15556 (2022).\n74. Scao, T. L. et al. BLOOM: a 176B-parameter open-access multilingual language model. \nPreprint at https://doi.org/10.48550/arXiv.2211.05100 (2022).\n75. Rae, J. W. et al. Scaling language models: methods, analysis & insights from training \nGopher. Preprint at https://doi.org/10.48550/arXiv.2112.11446 (2021).\n76. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text \ntransformer. J. Mach. Learn. Res. 21, 1–67 (2020).\n77. Zhang, S. et al. OPT: open pre-trained transformer language models. Preprint at https://\ndoi.org/10.48550/arXiv.2205.01068 (2022).\n78. Vaswani, A. et al. Attention is all you need. In 31st Conference on Neural Information \nProcessing Systems (Association of Computational Machinery, 2017).\n79. Kaplan, J. et al. Scaling laws for neural language models. Preprint at https://doi.org/ \n10.48550/arXiv.2001.08361 (2020).\n80. Lampinen, A. K. et al. Can language models learn from explanations in context? Preprint \nat https://doi.org/10.48550/arXiv.2204.02329 (2022).\n81. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language models are zero- \nshot reasoners. Preprint at https://doi.org/10.48550/arXiv.2205.11916 (2022).\n82. Joshi, M., Choi, E., Weld, D. S. & Zettlemoyer, L. TriviaQA: a large scale distantly supervised \nchallenge dataset for reading comprehension. Preprint at https://doi.org/10.48550/\narXiv.1705.03551 (2017).\n83. Beltagy, I., Lo, K. & Cohan, A. SciBERT: a pretrained language model for scientific text. \nPreprint at https://doi.org/10.48550/arXiv.1903.10676 (2019).\n84. Lewis, P., Ott, M., Du, J. & Stoyanov, V. Pretrained language models for biomedical and \nclinical tasks: Understanding and extending the state-of-the-art. In Proc. 3rd Clinical Natural \nLanguage Processing Workshop (eds Roberts, K., Bethard, S. & Naumann, T.) 146–157 \n(Association for Computational Linguistics, 2020).\n85. Shin, H.-C. et al. BioMegatron: larger biomedical domain language model. Preprint at \nhttps://doi.org/10.48550/arXiv.2010.06060 (2020).\n86. Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical \ntext mining. Bioinformatics 36, 1234–1240 (2020).\n87. Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language \nprocessing. ACM Trans. Comput. Healthc. 3, 2 (2021).\n88. Papanikolaou, Y. & Pierleoni, A. DARE: data augmented relation extraction with GPT-2. \nPreprint at https://doi.org/10.48550/arXiv.2004.13845 (2020).\n89. Hong, Z. et al. The diminishing returns of masked language models to science. Preprint at \nhttps://doi.org/10.48550/arXiv.2205.11342 (2023).\n90. Korngiebel, D. M. & Mooney, S. D. Considering the possibilities and pitfalls of generative \npre-trained transformer 3 (GPT-3) in healthcare delivery. NPJ Digit. Med. 4, 93 (2021).\nArticle\n91. Sezgin, E., Sirrianni, J. & Linwood, S. L. et al. Operationalizing and implementing \npretrained, large artificial intelligence linguistic models in the us health care system: \noutlook of generative pretrained transformer 3 (GPT-3) as a service model. JMIR Med. \nInformatics 10, e32875 (2022).\n92. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large language models  \nare zero-shot clinical information extractors. Preprint at https://doi.org/10.48550/\narXiv.2205.12689 (2022).\n93. Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about medical \nquestions? Preprint at https://doi.org/10.48550/arXiv.2207.08143 (2022).\n94. Ouyang, L. et al. Training language models to follow instructions with human feedback. \nPreprint at https://doi.org/10.48550/arXiv.2203.02155 (2022).\nAcknowledgements This project was an extensive collaboration between many teams at \nGoogle Research, with DeepMind involved in an advisory capacity. We thank M. Howell,  \nC. Chen, B. Mustafa, D. Fleet, F. Kibria, G. Turner, S. W. Man, D. Kim, B. Hatfield, L. Lehmann,  \nI. Horn, M. Shiels, S. Shetty, J. Zitting, E. Rappaport, L. Marples, V. Sounderajah, A. Connell,  \nJ. Freyberg, C. Hughes, M. Jones-Bell, S. Thomas, M. Ho, R. Wong, S. Prakash, B. Green,  \nE. Dominowska, F. Liu and X. Wang for their valuable insights and feedback during our \nresearch. We are also grateful to K. DeSalvo, Z. Ghahramani, J. Manyika and J. Dean for their \nsupport during the course of this project.\nAuthor contributions K.S., S.A., T.T., S.S.M., C.S., A.K. and V.N. contributed to the conception and \ndesign of the work. A.K., V.N., S.S.M., K.S., S.A. and T.T. contributed to the data acquisition and \ncuration. K.S., S.A., T.T., V.N. and A.B. contributed to the technical implementation. A.K., V.N., K.S., \nS.A., T.T., C.S., H.C.-L., S.P., P.P. and N.T. contributed to the evaluation framework used in the study. \nJ.W., H.W.C., N. Schärli, A.B., N. Scales and A.C. provided technical and infrastructure guidance. \nA.K., M.S., P.G. and C.K. provided clinical inputs to the study. D.D.-F. provided guidance on the \ndatasets used in the study. All authors contributed to the drafting and revising of the manuscript.\nCompeting interests This study was funded by Alphabet Inc. and/or a subsidiary thereof \n(Alphabet). K.S., S.A., T.T., V.N., A.K., S.S.M., C.S., J.W., H.W.C., N. Scales, A.T., H.C.-L., S.P., P.P., \nM.S., P.G., C.K., A.B., N. Schärli, A.C., P.M., B.A.A., D.W., G.S.C., Y.M., K.C., J.G., A.R., N.T., J.B. and \nY.L. are employees of Alphabet and may own stock as part of the standard compensation \npackage. D.D.-F. is affiliated with the US National Library of Medicine.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-023-06291-2.\nCorrespondence and requests for materials should be addressed to Karan Singhal, \nShekoofeh Azizi, Alan Karthikesalingam or Vivek Natarajan.\nPeer review information Nature thanks Andrew Beam and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permissions information is available at http://www.nature.com/reprints.\nExtended Data Fig. 1 | Instruction prompt tuning for Med-PaLM. We use \ninstructions and exemplars from a panel of qualified clinicians for each of the \nconsumer medical question answering datasets and use them to instruction \nprompt tune Flan-PaLM. Med-PaLM is the resulting model, with additional \nprompt parameters aligned with the medical domain.\nArticle\nExtended Data Fig. 2 | Comparison of SOTA LLMs on MMLU clinical topics. Flan-PaLM achieves state-of-the-art performance on MMLU clinical topics.\nExtended Data Table 1 | Summary of MultiMedQA describing the format, size, and domain of the datasets in the benchmark\nArticle\nExtended Data Table 2 | Summary of the different axes along which clinicians evaluate the answers in our consumer medical \nquestion answering datasets\nThese include agreement with scientific consensus, possibility and likelihood of harm, evidence of comprehension, reasoning and retrieval ability, presence of inappropriate, incorrect or \nmissing content, and possibility of bias in the answer. We use a panel of clinicians to evaluate the quality of model and human-generated answers along these axes.\nExtended Data Table 3 | Summary of the different axes along which lay users evaluate the model answers in our consumer \nmedical question answering datasets\nWe use a pool of 5 non-expert lay users to evaluate the quality of model and human-generated answers along these axes.\nArticle\nExtended Data Table 4 | Summary of the best performing models on the MedQA (USMLE) dataset questions with 4 options\nOur results with Flan-PaLM exceed previous state-of-the-art by over 17%.\nExtended Data Table 5 | Comparison of the performance between Med-PaLM 540B and Flan-PaLM 540B with \nself-consistency (SC) across multiple-choice datasets\nMed-PaLM was not trained using any of these datasets. These results suggest that instruction prompt tuning aligns the model to the requirements of consumer medical question answering \nwithout affecting base clinical knowledge.\nArticle\nExtended Data Table 6 | Representative explanations generated by the Flan-PaLM 540B model to support its multiple-choice \nanswers in the MedQA dataset\nExtended Data Table 7 | Examples of Med-PaLM responses to questions in the HealthSearchQA dataset\n.\nArticle\nExtended Data Table 8 | Examples of HealthSearchQA questions where the physician answers are considered incomplete, \nand corresponding Med-PaLM answers\nThis suggests that LLMs may be a useful complement to physicians in future use cases.\n1 nature portfolio  |  reporting summaryMarch 2021\nKaran Singhal \nShekoofeh Azizi \nAlan Karthikesalingam\nVivek Natarajan\nCorresponding author(s):\nLast updated by author(s):\u0001\"QSJM\u0001\u0012\u0019, 2023\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection\nData analysis\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\nThe study used six open source medical question answering for which no software was required. The additional open source dataset we plan \nto release as part of the study, HealthSearchQA, required scripts in python\u0001\u0001\u0001\u0014\u000f\u0012\u0011 for curation.\nWe will not be able to open source the large language models (LLMs) used in this study. We have provided comprehensive details regarding \nour underlying methodology\u000f 8F\u0001IBWF\u0001BMTP\u0001SFMFBTFE\u0001SFMBUFE\u0001NPEFMT\u0001BU\u0001IUUQT\u001b\u0010\u0010IVHHJOHGBDF\u000fDP\u0010HPPHMF\u0010GMBO\u000eU\u0016\u000eYM\nY\nY\nY\nY\nY\nY\nY\nY\nY\nY\n2 nature portfolio  |  reporting summaryMarch 2021\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our policy\nThe benchmark used in the study, MultiMedQA, comprises six open source datasets and an additional one\u0001on consumer medical questions, HealthSearchQA, \nwhich we newly introduce. )FBMUI4FBSDI2\"\u0001EBUBTFU\u0001JT\u0001QSPWJEFE\u0001BT\u0001B\u0001TVQQMFNFOUBSZ\u0001GJMF\u000f\u0001.FE2\"\u0001\u000e\u0001IUUQT\u001b\u0010\u0010HJUIVC\u000fDPN\u0010KJOE\u0012\u0012\u0010.FE2\"\r\u0001.FE.$2\"\u0001\u000e\u0001IUUQT\u001b\u0010\u0010\nNFENDRB\u000fHJUIVC\u000fJP\r\u00011VC.FE2\"\u0001\u000e\u0001IUUQT\u001b\u0010\u0010QVCNFERB\u000fHJUIVC\u000fJP\r\u0001-JWF2\"\u0001\u000e\u0001\u0001IUUQT\u001b\u0010\u0010HJUIVC\u000fDPN\u0010BCBDIBB\u0010-JWF2\"@.FEJDBM5BTL@53&$\u0013\u0011\u0012\u0018\r\u0001.FEJDBUJPO2\"\u0001\u000e\u0001\nIUUQT\u001b\u0010\u0010HJUIVC\u000fDPN\u0010BCBDIBB\u0010.FEJDBUJPO@2\"@.FE*OGP\u0013\u0011\u0012\u001a\r\u0001..-6\u0001\u000e\u0001IUUQT\u001b\u0010\u0010IVHHJOHGBDF\u000fDP\u0010EBUBTFUT\u0010IFOESZDLT@UFTU\nHuman research participants\nPolicy information about studies involving human research participants and Sex and Gender in Research. \nReporting on sex and gender N/A\nPopulation characteristics N/A\nRecruitment N/A\nEthics oversight N/A\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size The majority of datasets used in the study are already open source and have been used in the community for several years. As such, they have \nproven sufficient to estimate model performance accurately. The additional dataset we release is one of the largest of its kind with over 3000 \nsamples.\u0001'PS\u0001UIF\u0001IVNBO\u0001FWBMVBUJPO\r\u0001XF\u0001DIPTF\u0001\u0012\u0015\u0011\u0001RVFTUJPOT\u000f\u0001\u0001\"\u0001TQFDJGJD\u0001TBNQMF\u0001TJ[F\u0001DBMDVMBUJPO\u0001\u0001XBT\u0001OPU\u0001EPOF\u000f\nData exclusions We did not apply any special exclusion criteria to the datasets.\nReplication\nRandomization\nBlinding\nWe have repeated our experiments\u0001JOEFQFOEFOUMZ\u0001UISFF times to confirm the accuracy of the results\u0001GPS\u0001UIF\u0001.FE2\"\u0001EBUBTFU.\u00015IF\u0001WBSJBODF\u0001XBT\u0001\nNJOJNBM\u0001BT\u0001EFUBJMFE\u0001JO\u0001UIF\u0001QBQFS\u000f\n'PS\u0001EBUBTFUT\u0001JO\u0001.VMUJ.FE2\"\r\u0001SBOEPNJ[BUJPO\u0001XBT\u0001VTFE\u0001UP\u0001QSFQBSF\u0001UIF\u0001USBJOJOH\r\u0001WBMJEBUJPO\u0001BOE\u0001FWBMVBUJPO\u0001TQMJUT\u0001GPS\u0001\u0001UIF\u0001EBUBTFUT\u000f\n*O\u0001PVS\u0001IVNBO\u0001FWBMVBUJPO\u0001TUVEZ\r\u0001UIF\u0001SBUFST\u0001XFSF\u0001CMJOE\u0001UP\u0001UIF\u0001TPVSDF\u0001PG\u0001UIF\u0001SFTQPOTF\u0001\tNPEFM\u0001PS\u0001QIZTJDJBO\n\u000f\u0001\u0001\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \n3 nature portfolio  |  reporting summaryMarch 2021\nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7156330347061157
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5696850419044495
    },
    {
      "name": "Language model",
      "score": 0.5208899974822998
    },
    {
      "name": "Comprehension",
      "score": 0.5193799138069153
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5067122578620911
    },
    {
      "name": "Harm",
      "score": 0.4899863302707672
    },
    {
      "name": "Key (lock)",
      "score": 0.4878113567829132
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.455375999212265
    },
    {
      "name": "Data science",
      "score": 0.4507562518119812
    },
    {
      "name": "Precision medicine",
      "score": 0.4267148971557617
    },
    {
      "name": "Machine learning",
      "score": 0.41529619693756104
    },
    {
      "name": "Natural language processing",
      "score": 0.33838921785354614
    },
    {
      "name": "Medicine",
      "score": 0.17485082149505615
    },
    {
      "name": "Psychology",
      "score": 0.16631963849067688
    },
    {
      "name": "Computer security",
      "score": 0.09448951482772827
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}