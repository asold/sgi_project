{
    "title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
    "url": "https://openalex.org/W4362664882",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2901404304",
            "name": "Jannis Born",
            "affiliations": [
                "ETH Zurich",
                "IBM Research - Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2573726791",
            "name": "Matteo Manica",
            "affiliations": [
                "IBM Research - Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2901404304",
            "name": "Jannis Born",
            "affiliations": [
                "ETH Zurich",
                "IBM Research - Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2573726791",
            "name": "Matteo Manica",
            "affiliations": [
                "IBM Research - Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2947423323",
        "https://openalex.org/W3123901912",
        "https://openalex.org/W3146384714",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W4283794074",
        "https://openalex.org/W3169291081",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2945551948",
        "https://openalex.org/W3212417448",
        "https://openalex.org/W3156428263",
        "https://openalex.org/W4318071656",
        "https://openalex.org/W2973114758",
        "https://openalex.org/W3165630607",
        "https://openalex.org/W3147983081",
        "https://openalex.org/W3025593963",
        "https://openalex.org/W3209056694",
        "https://openalex.org/W4226159083",
        "https://openalex.org/W4220902634",
        "https://openalex.org/W2998571806",
        "https://openalex.org/W3130227682",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W3133498786",
        "https://openalex.org/W2994860160",
        "https://openalex.org/W6810518600",
        "https://openalex.org/W4221149941",
        "https://openalex.org/W3167734706",
        "https://openalex.org/W3205470891",
        "https://openalex.org/W4221161110",
        "https://openalex.org/W4297779993",
        "https://openalex.org/W4313531775",
        "https://openalex.org/W2034549041",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W3009321976",
        "https://openalex.org/W3118349318",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W2622206241",
        "https://openalex.org/W3037888463",
        "https://openalex.org/W3109892317",
        "https://openalex.org/W4200256006",
        "https://openalex.org/W3036527662",
        "https://openalex.org/W2980789587",
        "https://openalex.org/W4205773061",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W2769423117",
        "https://openalex.org/W2785942661",
        "https://openalex.org/W2784918212",
        "https://openalex.org/W4320495593",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4224060952",
        "https://openalex.org/W4288804596",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2900090807",
        "https://openalex.org/W2604296437",
        "https://openalex.org/W1997868832",
        "https://openalex.org/W3112376646",
        "https://openalex.org/W2379594833",
        "https://openalex.org/W2735621019",
        "https://openalex.org/W3012519883",
        "https://openalex.org/W6610918142",
        "https://openalex.org/W2948715311",
        "https://openalex.org/W6931614394",
        "https://openalex.org/W2747329762",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3112776819",
        "https://openalex.org/W3102094970",
        "https://openalex.org/W3108004614",
        "https://openalex.org/W2060531713",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W3099414221",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W3163970098"
    ],
    "abstract": "Abstract Despite tremendous progress of generative models in the natural sciences, their controllability remains challenging. One fundamentally missing aspect of molecular or protein generative models is an inductive bias that can reflect continuous properties of interest. To that end, we propose the Regression Transformer (RT), a method that abstracts regression as a conditional sequence modelling problem. This introduces a new direction for multitask language models, seamlessly bridging sequence regression and conditional sequence generation. We demonstrate that, despite using a nominal-scale training objective, the RT matches or surpasses the performance of conventional regression models in property prediction of small molecules, proteins and chemical reactions. Critically, priming the same model with continuous properties yields a competitive conditional generative model that outperforms specialized approaches in a substructure-constrained, property-driven molecule generation benchmark. Our dichotomous approach is facilitated by an alternating training scheme that enables the model to decorate seed sequences on the basis of desired property constraints, for example, to optimize reaction yield. We expect that the RT’s capability to jointly tackle predictive and generative tasks in biochemistry can find applications in property-driven, local exploration of the chemical or protein space. Such multitask approaches will pave the road towards foundation models in materials design.",
    "full_text": "Nature Machine Intelligence | Volume 5 | April 2023 | 432–444 432\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-023-00639-z\nRegression Transformer enables concurrent \nsequence regression and generation for \nmolecular language modelling\nJannis Born    1,2  & Matteo Manica    1 \nDespite tremendous progress of generative models in the natural sciences, \ntheir controllability remains challenging. One fundamentally missing \naspect of molecular or protein generative models is an inductive bias that \ncan reflect continuous properties of interest. T o that end, we propose \nthe Regression Transformer (RT), a method that abstracts regression \nas a conditional sequence modelling problem. This introduces a new \ndirection for multitask language models, seamlessly bridging sequence \nregression and conditional sequence generation. We demonstrate that, \ndespite using a nominal-scale training objective, the RT matches or \nsurpasses the performance of conventional regression models in property \nprediction of small molecules, proteins and chemical reactions. Critically, \npriming the same model with continuous properties yields a competitive \nconditional generative model that outperforms specialized approaches \nin a substructure-constrained, property-driven molecule generation \nbenchmark. Our dichotomous approach is facilitated by an alternating \ntraining scheme that enables the model to decorate seed sequences on the \nbasis of desired property constraints, for example, to optimize reaction \nyield. We expect that the RT’s capability to jointly tackle predictive and \ngenerative tasks in biochemistry can find applications in property-driven, \nlocal exploration of the chemical or protein space. Such multitask \napproaches will pave the road towards foundation models in  \nmaterials design.\nTransformers1 are now ubiquitous in natural language processing (NLP) \nand have also enjoyed large success in molecular2–4 and protein language \nmodelling5,6. The invention of Transformers was in alignment with the \nsteady decline of inductive biases in machine learning, a trend that \nstarted with the rise of deep learning: convolutional neural networks \noutperformed traditional feature descriptors in object recognition7, \nself-attention generalized dense layers to learn sample-dependent \ninstead of static affine transformations8 and Transformers exploited \nself-attention to supersede recurrent neural networks as the de facto \nstandard in NLP. The success of vision transformers has questioned the \nneed for translation equivariance in image processing9, and now, even \nfrozen Transformers pre-trained on text achieve state-of-the-art results \nin object detection and protein classification10. Given that Transform-\ners are today’s most generic model (that is, graph neural networks \nwith multihead attention as neighbourhood aggregation on complete \ngraphs), it is not surprising that attempts have been made to abstract \nentire domains such as reinforcement learning to sequence modelling \nin order to leverage Transformers11.\nReceived: 7 September 2022\nAccepted: 2 March 2023\nPublished online: 6 April 2023\n Check for updates\n1IBM Research Europe, Zurich, Switzerland. 2Department of Biosystem Science and Engineering, ETH Zurich, Basel, Switzerland.  \n e-mail: jab@zurich.ibm.com; tte@zurich.ibm.com\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 433\nArticle https://doi.org/10.1038/s42256-023-00639-z\nHowever, the ChemFormer tunes task-specific heads and thus does \nnot pose a true multitask model that entangles both tasks seamlessly. \nThis semantic gap persists across architectural flavours (for example, \ngenerative adversarial networks (GANs)24, reinforcement learning25, \nvariational autoencoders (VAEs)26, graph neural networks (GNNs)19,27, \nflow28,29 and diffusion models 30). However, some works performed \nproperty-driven generation through probabilistic reparameterization \nthat directly optimize the input to a property prediction model, for \nexample, gradient-based schemes such as PASITHEA31, differentiable \nscaffolding trees32 and activation maximization 33 or multi-objective \nBayesian optimization34 that has been applied to peptide inhibitor 35 \nand antibody design 36. Still, to our knowledge, existing Transform -\ners either tune task-specific heads (see, for example, refs. 22, 23) or \nlimit the communication between both modules to a reward/loss and \nthus fail to ‘entangle’ constrained structure generation with property \nprediction. This critically violates the intuitive expectation that a \nproperty-driven generative model should, in the first place, excel at \nrecognizing this property.\nIn this Article, we aim to close this gap by reformulating regression \nas a sequence modelling task. We propose the Regression Transformer \n(RT), a novel multitask model that can be trained on combinations of \nnumerical and textual tokens (Fig. 1). This circumvents the canonical \nA provocative next step towards reducing inductive biases might \nbe to refrain from explicitly modelling target variables as functions \nof input variables. Instead of following this discriminative modelling \napproach when tuning task-specific language heads in Transform -\ners, learning the joint distribution over input and target variables \ncould effectively further blur the lines between predictive and condi-\ntional generative models. The feasibility of such an approach can be \nassessed via permutation language modelling (PLM), an extension of \nmasked-language modelling to autoregressive models12. Such dichoto-\nmous models (that concurrently excel at regression and conditional \nsequence generation) are beyond applications in NLP of special interest \nfor chemical and material design. Molecules are often labelled with \ncontinuous properties (for example, drug efficacy or protein solubil-\nity), and design tasks are intertwined with bio- or physicochemical \nproperties. But despite the rise of deep generative models in molecu-\nlar13,14 and protein design15,16, current approaches still develop property \npredictors and generative models independently. Transformer-based \narchitectures have been used widely on chemical tasks but focused \non either property prediction17,18 or conditional molecular design19–21. \nTypically, they employ large-scale self-supervised pre-training and \nthen fine-tune on different tasks22,23, but only the Chemformer by ref. 22  \naddresses regression as well as targeted molecular generation. \nProperty prediction\nConditional sequence generation RT\nθP = θ G\nPredictive\nmodel\nθP\nConditional \ngenerative\nmodel\nθG\n1.02\n1.02\nSolubility \nSeed\na b\nc\nVirtual \nlibrary Reward\nRT\nSolubility = 1.02\nSolubility 1.02\nSmall molecules\nFunniness 1.02\nNatural text\nFunniness 1.80\nStability 1.36\nProteins\nStability 1.36\nMasked entityRegression task Conditional generation task\nRT\nRT\nYield 0.44\nYield = 0.27\nChemical reactions\nRT\nd\ne f\nFig. 1 | Overview of RT . The RT is a multitask language model designed to handle \ncombinations of text and numbers. a, Traditional approach in generative \nchemistry: property predictors and generative models are trained independently \nfrom another. b, Our approach: Training the RT yields a dichotomous model \nthat seamlessly transitions between property prediction and conditional text \ngeneration. The model’s task is to fill the content behind the [MASK] tokens. \nDepending on the mask location, the same model either predicts numerical \ntokens given textual tokens, thus performing a regression task (blue stream, \ntop), or predicts textual tokens given both numerical and textual tokens, thus \nperforming a property-driven conditional generation (yellow stream, bottom). \nc–f, This novel formulation finds application across a wide range of domains. \nWe demonstrate the flexibility of the RT in predictive and generative tasks in \nmodelling small molecules, proteins and chemical reactions and note that it can \neven be applied to natural text.\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 434\nArticle https://doi.org/10.1038/s42256-023-00639-z\nway of addressing regression in Transformers, that is, tuning a des -\nignated regression head37. Despite solely relying on tokenization of \nnumbers and cross-entropy loss, the RT can successfully solve regres-\nsion tasks. Notably, the same model can conditionally generate text \nsequences given continuous properties. This is achieved simply by \nmoving the [MASK] location and does not require fine-tuning specific \nheads, thus constituting a true multitask model. T o equip the RT with \nan inductive bias for handling floating-point properties, numbers \nare first tokenized into a sequence of tokens preserving the decimal \norder. We then devise numerical encodings (NEs) to inform the model \nabout the semantic proximity of these tokens. T o allow for concurrent \noptimization of regression and conditional generation, we derive \na PLM-inspired, alternating training scheme that includes a novel \nself-consistency (SC) loss for improved text generation based on con-\ntinuous primers.\nIn the remainder of this paper, we describe the capabilities of \nthe RT on a diverse set of predictive and generative tasks in chemical \nand protein language modelling. We commence with small-molecule \nmodelling, validate the RT on a synthetic dataset of drug likeness38 and  \nthen test it on three property prediction datasets from the MoleculeNet  \nbenchmark39. The property predictions results are compared with \nprevious approaches relying on a regression loss and demonstrate  \nthat regression can be cast as conditional sequence generation  \ntask without losing accuracy. These experiments rely on SELFIES 40,  \na chemical language devised for generative tasks that, as we show,  \nhas comparable predictive power to SMILES. Although we aim to  \nconcurrently excel at predicting properties and generating \nsequences conditioned on properties, we start training with the PLM  \nobjective12, which does not explicitly model those tasks. We then refine \nthis objective and devise a training scheme that alternates between \noptimizing property prediction and text generation. For the latter,  \nwe derive a novel SC loss that exploits the dichotomy of the RT by \nquerying itself with the generated candidate sequence. T o assess \n \nperformance in conditional sequence generation, we systemati -\ncally vary the continuous properties of interest and investigate the \nmodel’s ability to adapt a seed sequence according to the primed \n \nproperty value. We show applications on property-driven local chemical \nspace exploration by decorating scaffolds with a continuum of proper-\nties and evaluate the novel molecules using the RT itself as well as an \nindependent property predictor41. The RT is then challenged against \nspecialized molecular generative models on a property-driven molecular \ngeneration benchmark42, where it substantially outperforms prior art.\nNext, the RT is investigated on protein sequence modelling where \nit matches the performance of conventional Transformers on two \nregression datasets from the TAPE (Tasks Assessing Protein Embed-\ndings) benchmark43. In experiments on chemical reactions, we notice \nthat the RT constitutes a generalization of forward reaction and retro-\nsynthesis models. We then demonstrate on two reaction datasets that \nthe RT can not only predict reaction yields with similar accuracy to \nconventional Transformers44, but that it can also substitute specific \nprecursors and thus generate novel reactions with higher predicted \nyield than a seed reaction.\nResults\nChemical language modelling\nInitial validations—learning drug likeness.  T o test the feasibil -\nity of concurrent property prediction and conditional generation, \nwe start with optimizing the vanilla permutation language objec -\ntive (equation ( 3)) on a synthetic QED (quantitative estimation \nof drug-likeness) dataset (Extended Data Fig. 1 shows an illustra -\ntion of the entire workflow, for example, the different objective \nfunctions and the autoregressive generation and how the mixed \nalphanumeric sequences are tokenized and embedded). Since this \nobjective masks tokens randomly in the sequence, evaluating such \nmodels on property prediction (that is, masking only numerical \ntokens as shown in Fig. 1b (top)) does not closely mimic their training  \ndynamics.\nDespite this, as well as the unconventional formulation of a regres-\nsion task as sequence modelling, all models generated sequences of \nnumerical tokens that allowed decoding floats, and even achieved a \nroot mean square error (RMSE) <0.06 (Table 1, top three rows). Instead, \nfor the generative task, the same models were queried ten times for \nevery validation molecule with property ‘primers’ equidistantly spaced \nin [0, 1] and 40% of masked textual tokens. Throughout this manuscript \nby ‘primers’ we mean that we replace the true property of a sequence \nwith a desired property value. The high rank correlation ρ (between \nprimers and QED of unique, generated molecules) values show that \nthe model learned successfully to complete the corrupted sequences \nto produce full molecules with a desired QED. Notably, the novelty \nscore (that is, the percentage of conditionally generated molecules \nnot present in training data) was >99% for all models. This demon -\nstrates that the RT can generate novel chemical matter that adheres \nto a continuous property of interest. Moreover, the NEs, an inductive \nbias to ease learning proximities of numbers (similar to positional \nencodings1), slightly improved performance in all tasks (for details, \nsee “Numerical Encodings” subsection in Methods). Next, the SELFIES \nmodels with and without NEs were refined on the basis of our proposed \ntraining scheme with alternating objectives. For both models, two \nmodels were fine-tuned using the alternating objective (equation (7)), \nwith (α = 1) and without (α = 0) the SC term in the text loss, respectively \n(Table 1, bottom section). Interestingly the performance in regression \nas well as conditional generation improved notably, demonstrating the \neffectiveness of the refined objectives.\nFurthermore, the ablation studies on pre-training or fine-tuning \non individual objectives (Table 1, middle) revealed that good perfor-\nmance can be achieved on singular tasks. But the alternating objective \nenables cross-task benefits that enable the multitask model to out -\nperform single-task models in almost all cases. As might be expected, \nevaluation queries with large deltas between seed and primed QED \nlead to lower precision on the generation since they essentially pose \nan out-of-distribution setting (note that the model was only trained \nwith seed = primer). This is shown in Extended Data Fig. 2, which also \nreveals that the SC model particularly shines for challenging queries \nwhereas for a pure reconstruction task (that is, primer close to seed) \nthe single-task ‘generate-only’ model is advantageous. Moreover, as \nwe report in Extended Data Table 1, all configurations of the RT outper-\nformed a baseline k-nearest neighbour (k-NN) regressor on extended \nconnectivity fingerprints (ECFP45) and our best configuration even \nsurpassed SMILES-BERT17, which achieved a mean absolute error of 0.02 \nwith a regular regression loss and after pre-training on ~9 million SMILES.\nThe SC term further improved the model’s ability to generate tai-\nlored ensembles of molecules and led to consistently higher correlation \nscores. This is exemplarily visualized in Fig. 2 (top) where a single seed \nmolecule is decorated according to the property primers to cover the \nfull range of QED scores.\nGenerally, the better performance of the SC models (α = 1) in the \ngenerative tasks comes at the cost of slightly inferior regression per-\nformance (Table 1). Presumably, this is because the model weights in \ncharge of the regression are confounded with the gradients from the \nself-evaluation (equation ( 7)). The novelty scores for the molecules \ngenerated in this setting were even slightly higher than for the PLM \ntraining (>99.3% for all models). A particularly challenging application \nfor property-driven, local exploration of the chemical space is scaffold \ndecoration (that is, adapting a seed molecule while preserving its core \nstructure). For an example on this, see Supplementary Information \nSection 6.1. Here, the SELFIES models exceeded the SMILES models \nby far, because SMILES, unlike SELFIES, can be syntactically invalid (we \nfound 60% validity). However, this number can hardly be compared \nto unseeded generative models because (1) the RT has to remediate  \na corrupted SMILES and cannot simply rely on its own internal states,  \nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 435\nArticle https://doi.org/10.1038/s42256-023-00639-z\n(2) the concurrently provided property primers capture the entire \nrange of low to high QED scores, thus incentivizing the model to \ndecorate the sequence adventurously to adhere to the constrained \nproperty—a task that is often impossible and can easily lead to broken \nSMILES and (3) the RT training did not rely on teacher forcing. Due \nto the comparable results for property prediction (Table 1, top three \nrows), the remaining experiments focus exclusively on SELFIES. But \neven though SELFIES are designed to be always valid, they can also \nbreak by converting long sequences to short, stub-like molecules. We \nassessed the frequency of this scenario by defining a generation as \ndefective if the obtained molecule had <50% of the atoms of the seed \nmolecule. This yielded ~1.9% defective generations across ~300,000 \ngenerations. Regarding chemical sensibility, we observed that the \n1,000 most common functional groups46 are reproduced in the gene-\nrated molecules (Supplementary Fig. 1). Further ablation studies on \ndifferent types of NE and related work on encoding numbers with \nTransformer are reported in Supplementary Information Section 1.\nLearning embeddings of numbers. We sought to understand why \nthe ablation studies on the NEs on the QED dataset (Table 1 ) reveal \nonly mild superiority of models with NEs. Interestingly, as visualized in \nExtended Data Fig. 3, in the absence of static NEs, the model learns the \nnatural ordering of digits from the data. A large number of embedding \ndimensions (47% and 36% for the decimal places −1 and −2, respec -\ntively) directly and significantly encoded the ordering of digits (that \nis, P < 0.05 and ∣PCC∣ >0.62 between the ten embedding values and \na strictly monotonic vector). For example, in Extended Data Fig. 3 \n(left), the digit value is monotonically related to its embedding value. \nIn general, attention weights in Transformers can capture complex \nsemantics such as protein folding structure 47 or atom mapping in \nTable 1 | Learning drug likeness\nConfiguration Regression task Generation task\nData NE Pre-training Fine-tuning RMSE (↓) PCC (↑) 0-Var (↓) Spearman’s ρ (↑)\nSMILES – PLM – 0.055±0.01 0.972±0.01 1.6%±0.2 0.096±0.02\nSELFIES – PLM – 0.059±0.00 0.968±0.00 0.9%±0.2 0.427±0.01\nSELFIES ✓ PLM – 0.055±0.01 0.971±0.00 0.3%±0.1 0.467±0.01\nSELFIES ✓ Predict (ℒP) – 0.062±0.01 0.963±0.00 Task unfeasible Task unfeasible\nSELFIES ✓ Generate (ℒG) – Task unfeasible Task unfeasible 0.5%±0.1 0.358±0.00\nSELFIES ✓ PLM Predict (ℒP) 0.030±0.01 0.991±0.01 96.4%±0.0 0.062±0.00\nSELFIES ✓ PLM Generate (ℒG) 0.525±0.18 0.226±0.24 0.3%±0.0 0.512±0.00\nSELFIES – PLM Alternate (ℒP and ℒG) 0.034±0.01 0.988±0.01 0.2%±0.1 0.470±0.02\nSELFIES ✓ PLM Alternate (ℒP and ℒG) 0.050±0.00 0.982±0.00 0.3%±0.1 0.468±0.03\nSELFIES – PLM Alternate with SC (ℒP and \nℒSC)\n0.048±0.01 0.978±0.03 0.3%±0.1 0.490±0.01\nSELFIES ✓ PLM Alternate with SC (ℒP and \nℒSC)\n0.037±0.03 0.987±0.03 0.2%±0.1 0.517±0.02\nDifferent configurations of the RT on concurrent learning of predicting drug likeness and generating drug-like molecules. The first block contains models trained with the task-agnostic PLM \nobjective. The second block contains ablation studies on single-task models exclusively trained on either the predictive or the generative objective. The third block contains molecules that \nwere pre-trained on the PLM objective and then fine-tuned using the alternating objective. RMSE (↓) and PCC refer to predicting QED, whereas Spearman’s ρ (↑) and 0-Var (↓) to the conditional \ngeneration task. S.d. values across repeated runs are shown. Numbers computed on 10,000 test samples. Best model shown in bold, second-best underlined.\nPrimer: 0.05, QED: 0.113\nPrimer: 0.15, QED: 0.167\nPrimer: 0.25, QED: 0.245\nPrimer: 0.35, QED: 0.387\nPrimer: 0.45, QED: 0.471\nPrimer: 0.85, QED: 0.818\nPrimer: 0.65, QED: 0.701\nSeed ESOL: –3.904\nLow QED\nSeed QED: 0.701\nPrimer: –8.61, \nESOL (by RT): –6.58; ESOL (by Grover): –7.44\nPrimer: –7.23,\nESOL (by RT): –5.21, ESOL (by Grover): –5.17\nPrimer: –5.84,\nESOL (by RT): –5.19, ESOL (by Grover): –4.67\nPrimer: –4.46,\nESOL (by RT): –5.19, ESOL (by Grover): –4.39\nPrimer: –3.07,\nESOL (by RT): –4.78, ESOL (by Grover): –3.73\nPrimer: –1.69,\nESOL (by RT): –3.79, ESOL (by Grover): –2.29\nPrimer: 1.08,\nESOL (by RT): –1.30 ESOL (by Grover): –1.78\nUnsoluble\nSoluble\nHigh QED\nO\nO\nO\nN\nNH\nNH\nS\nN\nN\nO\nN\nO\nO\nNH\nO\nNH\nS\nN\nN\nO\nN\nO\nO\nNH\nO\nNH\nO\nS\nN\nN\nO\nN\nO\nO\nO\nNH\nO\nNH\nO\nNH\nN\nN\nN\nO\nO\nNH\nO\nNH\nO\nN\nN\nO\nN\nO\nO\nNH\nO\nN\nN\nN\nO\nN\nO\nO\nNH\nO\nN\nN\nN\nO\nN\nO\nN\nO\nNH\nCl\nO\nNH\nO\nN\nN\nN\nO\nN\nO\nO\nCl\nO\nO\nNH\nO\nNH\nCl\nO\nNH\nOH\nO\nFig. 2 | Property-driven, local optimization of molecular design with the \nRT. For each row, the seed molecule is shown in the middle alongside its true \nproperty. On the basis of ten property primers, ten molecules were decoded \nbut duplicates were discarded. Samples generated with the SC model. T op: QED \ndataset. Bottom: ESOL dataset of aquatic solubility. The solubility of the novel \nmolecules was predicted by the RT itself and is externally validated by Grover\n41.\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 436\nArticle https://doi.org/10.1038/s42256-023-00639-z\nchemical reactions4. For a qualitative comparison of the RT’s atten -\ntion across the predictive and generative task, see Supplementary \nInformation Section 2.\nRegression benchmark (MoleculeNet)\nAfter the successful initial experiments, we evaluated the RT on three \nregression benchmarks from MoleculeNet39. The regression perfor-\nmance on ESOL, FreeSolv and Lipophilicity is shown in Extended Data \nTable 2 and compared with prior work. The strongest baseline model \nfrom MoleculeNet, XGBoost, is outperformed by all our models on all \ntasks. Even the MPNN\n48, a message-passing GNN, is slightly surpassed \non FreeSolv and Lipophilicity by some of our models. However, all our \nmodels are outperformed by BERT49 and BART22. Notably, these models \nleveraged large-scale self-supervised pre-training before fine-tuning \na regression head, whereas we use a classification loss. Since these \nresults might not be directly comparable to the RT with its XLNet back-\nbone, we also fine-tuned a XLNet model with a conventional regression \nhead. Notably, despite the absence of a regression loss, the RT is on par \n(Lipophilicity) or only mildly inferior (that is, within s.d. range; ESOL, \nFreeSolv) to XLNet.\nHowever, in stark contrast to all those approaches, only the RT can \nalso be used to conditionally generate molecules similar to the training \nsamples (Extended Data Table 3). Since the properties of the generated \nmolecules are intractable to evaluate in silico, we could predict them, \nhandily, using the RT. However, as this might be a biased estimator, we \nadditionally evaluated them using Grover41, a self-supervised Graph \nTransformer that relies on large-scale pre-training. Extended Data \nTable 3 presents the performance in conditional molecular generation, \nwhich underlines the benefit of the SC loss (α  = 1) and demonstrates \nthat the RT can adapt unseen seed molecules even according to com-\nplex molecular properties such as water solubility. Corroborative \nfor our work is the high correlation of our property predictions (RT) \nwith Grover’s for molecules generated by the ESOL, FreeSolv and Lipo \nmodels (0.86, 0.84 and 0.75, respectively). For a qualitative evaluation, \nwe depict the generations for one exemplary seed molecule of the \nsolubility dataset in Fig. 2  (bottom). Lastly, we found 1.3% defective \ngenerations, which is comparable to or lower than in the QED dataset.\nConditional molecular generation benchmark\nT o assess whether the RT is a powerful conditional generative model, \nwe benchmarked it on a property-driven molecular generation task, \nnamely penalized logP (plogP; definition in Methods) constrained \noptimization 42. Given a seed molecule and a similarity constraint \nto the seed molecule (δ, given in Tanimoto similarity), the goal is to \ngenerate molecules with higher plogP values. The results in Table 2 \ndemonstrate that, for both similarity thresholds δ, the RT performs \ncompetitive to state-of-the-art models; for example, it outperforms a \nJunction-Tree-VAE42 and a graph-convolutional policy network (GCPN)50 \nby 614% and 103% in average improvement, respectively.\nIt falls behind the Back Translation (BT) model 51 on average \nimprovement; however, care has to be taken on their results since other \nmetrics and s.d. values are not reported. The RT performs comparably \nto the MoFlow model 52, while our results for δ  = 0.4 are inferior, the \nunconstrained generation results (δ = 0.0) are in favour of our method \n(Supplementary Information Section 3). Moreover, these comparisons \nare not truly fair because all competing methods have a training pro-\ncedure that rewards generating molecules with high plogP and some \nmethods even apply gradient optimization schemes at inference time \n(GCPN and JT-VAE). This is in stark contrast to the RT training, which \nrewards only if the reconstructed molecule has a similar (predicted) \nplogP to the seed molecule (we did not construct directed plogP queries \nfor the training; they were used only at inference time). Thus, the RT \nis agnostic in valence and could equally be used to adapt molecules \ntowards lower plogP. Overall, this experiment demonstrates that the \nRT is able to compete with specialized conditional generative models \nin goal-directed molecular generation. At the same time, the RT also \npredicted the plogP value with a Pearson’s correlation coefficient (PCC) \nof 0.92, a task that cannot be addressed with normal conditional genera-\ntive models. The results in Table 2 were obtained with the RT including a \nSC loss, but for ablation studies on the RT and further results on δ = 0.2 \nand δ = 0, see Supplementary Information Section 3.\nProtein sequence language modelling\nPre-training on potential protein interaction (Boman index) . \nT o assess the generality of the RT beyond chemical languages, we \nbenchmarked the RT in protein language modelling. On the synthetic \nTable 2 | Constrained property optimization benchmark\nModel Generation task Regression\nImprovement Similarity δ Success PCC\n(a) Similarity threshold δ = 0.4\nJT-VAE42 0.84±1.5 0.51±0.1 83.6% Task unfeasible\nGCPN50 2.49±1.3 0.47±0.1 100% Task unfeasible\nMoFlow52 4.71±4.5 0.61±0.2 85.7% Task unfeasible\nBT51 4.21 NA NA Task unfeasible\nRT (ours) 3.16±1.5 0.54±0.1 97.1% 0.92±0.0\n(b) Similarity threshold δ = 0.6\nJT-VAE42 0.21±0.7 0.69±0.0 46.4% Task unfeasible\nGCPN50 0.79±0.6 0.68±0.1 100% Task unfeasible\nMoFlow52 2.10±2.9 0.79±0.1 58.3% Task unfeasible\nBT51 2.77 NA NA Task unfeasible\nRT (ours) 2.21±1.3 0.69±0.1 81.8% 0.92±0.0\nBest model marked in bold, second-best underlined. Standard deviations are given. Full table \nwith different configurations in Supplementary Table 3. NA means “not available”.\nTable 3 | Results on protein language modelling\nModel Source Boman Fluorescence Stability\n(a) Protein regression tasks\nk-NN Baseline 0.93 0.59 0.21\nOne-Hot TAPE NA 0.14 0.19\nLSTM TAPE NA 0.67 0.69\nTransformer TAPE NA 0.68 0.73\nUniRep 54 NA 0.67 0.73\nProteinBERT 55 NA 0.66 0.76\nRT (ℒSC) Ours 0.99±0.01 0.72±0.04 0.71±0.02\nModel Boman dataset Stability dataset\n0-Var (↓) Spearman’s ρ 0-Var (↓) Spearman’s ρ\n(b) Protein generation tasks\nAll TAPE Task unfeasible Task unfeasible\nUniRep Task unfeasible  Task unfeasible\nRT (PLM) 0.3%±0.0 0.76±0.03 40%±4.2 0.00±0.00\nRT (ℒG) 0.2%±0.1 0.82±0.01 31%±5.5 0.30±0.06\nRT (ℒSC) 0.2%±0.1 0.84±0.00 19%±4.5 0.44±0.01\n(a) Protein property prediction (regression). All values in Spearman’s ρ (↑) on the test set. \nTAPE datasets/performances taken from ref. 43. An ablation study on the three loss functions \n(equations (3), (6) and (7)) confirmed the superiority of the SC objective (Supplementary \nInformation Section 4.1 and Supplementary Table 4). Best performance per dataset shown in \nbold. (b) Protein generation. Sixty per cent of the residues were masked. Boman index was \ncomputed directly, whereas stability was predicted with the RT itself. Best performance per \ndataset shown in bold. S.d. values measured across three runs.\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 437\nArticle https://doi.org/10.1038/s42256-023-00639-z\npre-training data, the RT obtained nearly perfect results in predicting \nBoman’s index (Spearman’s ρ > 0.994; Table 3) and outperformed a \nbaseline k-NN using Levenshtein distance53. But the RT also successfully \ngenerated peptides with a desired Boman index, given a partially cor-\nrupted amino acid sequence (Spearman’s ρ of 0.84; Table 3b). Moreo-\nver, a higher fraction of masked tokens lead to better results in protein \ngeneration tasks (Supplementary Fig. 2).\nTAPE datasets (protein fluorescence and protein stability).  Next, \nthe RT performed competitively on two realistic protein regression \ndatasets from TAPE (Table 3). This is remarkable given that the TAPE \nmodels were pre-trained large scale on unlabelled protein sequences \nand fine-tuned with a regression loss. For example, the RT outperforms \nall reported methods in Spearman’s correlation on the fluorescence \ntask, which has a distribution with two modes, for bright and dark pro-\nteins, respectively. Inspecting the predictions in more depth showed \nthat the RT, compared with other methods, excels at recognizing the \nmode of a protein but struggles with intra-mode precision (Supplemen-\ntary Information Section 7.2). Across both datasets, the RT performs on  \npar or superior to the TAPE Transformer43, UniRep54 and the contem-\nporary ProteinBERT 55 model, pre-trained on 31 million, 24 million \nand 106 million protein sequences, respectively (2.6 million in our \ncase). However, scaling this pre-training to evolutionary-scale pro -\ntein language models would probably displace UniRep as well as the  \nRT as evolutionary-scale protein language models was recently  \ndemonstrated to have strong zero-shot generalization performance56.\nOverall, the competitive predictive performance of the RT demon-\nstrates that the benefits of self-supervised pre-training can extend to \nnumerically labelled datasets. This yields, en passant, a conditional \ngenerative model for property-driven local exploration of the protein \nsequence space. Evidence on this can be found in Table 3b: Whereas all \nTAPE models as well as the UniRep method are incapable of addressing \nthis generation task, the RT was able to modify the test proteins such \nthat their (predicted) stability correlated strongly with the primed \nproperty (ρ = 0.44).\nModelling chemical reactions\nLanguage models advanced reaction chemistry dramatically4,57 and, \namong others, showed superior performance on yield prediction44, \nyet models incorporating yield into (partial) reaction generation are \nlacking entirely. Such models could be used to (1) identify entirely \nnovel reactions by substituting a specific precursor type with a higher \nyield, (2) cure erroneous reactions by identifying missing precursors \nin databases of specific reaction types or (3) infer reagents or solvents \nin reactions that only specify main compounds.\nWe therefore optimized the RT for concurrent yield prediction and \nprecursor generation on two reaction-yield datasets: Buchwald–Hartig \naminations58 and Suzuki–Miyaura cross-couplings59. All experiments \nrelied on the alternated training scheme with SC loss. On yield predic-\ntion, the RT (trained on SELFIES) outperforms fingerprint-based or \nquantum mechanics methods, and matches (Suzuki dataset) or almost \nmatches (Buchwald dataset) the performance of language models \nsuch as Yield-BERT, trained with regression loss on SMILES (Table 4).\nThe same model learned to reconstruct missing precursors \nin Buchwald–Hartwig animations, which can be useful to infer  \nmissing solvents or reagents in automatically extracted reactions \nTable 4 | Chemical reaction modelling\nModel Buchwald– Suzuki\nHartwig coupling\n(a) Reaction yield prediction\nOne-Hot 80 0.89 NA\nDFT 58 0.92 NA\nMFF 80 0.927±0.01 NA\nYield-BERT 44 0.951±0.01 0.79±0.02\nYield-BERT fine-tuned 0.951±0.01 0.81±0.01\nRT (ours) 0.939±0.01 0.81±0.02\nReconstruction Decoration\nDataset Precursor Top-three Similarity Success Mean\naccuracy δ rate improvement\n(b) Generating novel precursors for unseen reactions\nBuchwald\nHartwig\nHalide 98.23%±0.5 0.991±0.00 42.3%±2.4 6. 1%±1.3\nLigand 50.38%±1.6 0.677±0.01 74.4%±4.2 14. 4%±1.7\nBase 100%±0.0 1.000±0.00 82.2%±2.3 8. 1%±0.6\nAdditive 1.36%±0.5 0.158±0.02 71.2%±1.8 11. 7%±1.3\nSuzuki\ncross-\ncouplings\nElectrophile 44.2%\n±17.6 0.732±0.02 63.5%±7.1 12. 5%±3.4\nNucleophile 100.0%±0.0 1.000±0.00 54.0%±6.2 5. 4%±0.8\nLigand 67.4%±20.0 0.689±0.15 56.7%±3.5 5. 5%±0.6\nBase 90.5%±1.2 0.811±0.01 47.8%±2.7 4. 6%±0.3\nSolvent 56.4%±1.1 0.661±0.01 57.8%±1.8 7. 5%±0.3\nFor the yield prediction, performance for ten 70/30 splits, measured in coefficient of determination (R2) with s.d. is shown. For the generative task, we explore reconstruction and decoration \nof the reactions. For reconstruction, we show the percentage of cases where the exact right precursor was among the top-three predicted sequences and the Tanimoto similarity of the most \nsimilar of those molecules. For decoration, we show the percentage of cases where the top-five predicted reactions contained a reactions with higher (predicted) yield than the seed reaction \n(success rate), alongside the associated average yield improvement. Full precursors were generated (pmask = 1). S.d. values across ten runs are shown. For the BH aminations, each reaction \nincluded the same palladium catalyst, which is thus excluded from this analysis. For the Suzuki couplings, each reaction also contained 4-methylaniline and the same palladium catalyst, which \nare also excluded from the analysis.\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 438\nArticle https://doi.org/10.1038/s42256-023-00639-z\n(Table 4b). This is partly achieved with great accuracy (for example, \n98.2% for aryl-halides). Interestingly, inferring additives proved chal-\nlenging, possibly because they are the dominant precursor type for \nthe reaction yield58. However, upon masking the additive only partially \n(rather than completely), the reconstruction performance increases \nsignificantly (ablation study with pmask ∈ [0.25, 0.5, 1] in Supplementary \nTable 5). On the Suzuki couplings, the reconstruction results are more \nbalanced among the five precursor types; the average Tanimoto simi-\nlarity to the true precursor was >0.65 in all cases (Table 4b). Moreover, \nacross both datasets we observed mild benefits in reconstruction \nperformance when providing the true yield rather than masking it \n(Supplementary Tables 6 and 7). In addition to yield prediction and \nprecursor reconstruction, the RT can also decorate existing reactions \nby adapting specific precursors towards a higher yield (Table 4b ). \nConsistently among both datasets and all precursor types, 40–80% \nof the top-five predicted sequences contained reactions with entirely \nnovel precursors and higher predicted yield.\nExtended Data Fig. 4 visualizes exemplary adaptations of each \nprecursor type of a BH amination with very low yield (<5%). Notably, \nfor this unseen reaction, the RT found novel adaptations of each of the \nfour precursor types that resulted in an increase of predicted yield by \n11–85%. With the forward reaction prediction model in IBM RXN2, we \nconfirmed that all reactions indeed result in the desired product. Nota-\nbly, the confidence from the forward model rank-correlated almost \nperfectly with the yield predicted by the RT (ρ = 0.90, P < 0.05).\nDiscussion\nThe herein presented RT demonstrated that regression can be cast as \nconditional sequence learning task. We introduced a flexible multitask \nlanguage model with wide application in scientific discovery. Our main \ncontribution is a multitask transformer that bridges previously consid-\nered disjoint tasks (property prediction and conditional generation) \nwithout the need of tuning task-specific heads. This model shines at \nboth tasks and facilitates highly customizable molecular generation (for \ndetails, see ‘Usage of trained models’ in the Code availability section). \nThis could pave the road towards foundation models in material design.\nRegarding molecular property prediction, we find that the RT \nlearns continuous properties even from small datasets, surpasses \nconventional regression models on several benchmarks and sometimes \ncompetes with Transformers trained on regression loss. Remarkably, \nthis is achieved without providing ratio-scale information about the \nproperty, potentially even challenging the necessity of using regression \nrather than classification objectives.\nThe experiments on conditional text generation underline the \nversatility of the RT. Across a wide range of tasks, we conditionally \ngenerated novel sequences (molecules, proteins and reactions) that \nseemingly adhere to primed, continuous properties. Our experiments \non constrained molecular generation benchmark further demonstrate \nthat the RT can surpass specialized conditional generative models. We \nforesee this to impact property-driven and substructure-constrained \nmolecular or protein design tasks. In the recent work by ref. 60 , the \nRT has been applied in polymer chemistry for the generation of novel \nring-opening polymerization catalysts as well as block and statisti -\ncal co-polymers. In both cases, successful experimental validation \nconfirmed the ability of the RT to accelerate real discovery workflows.\nMoreover, even though all experiments reported herein examined \nsingular properties, the RT naturally scales to multi-property predic-\ntion (see ‘GUI Demo’ in the Code availability section on how to access \npre-trained multi-property models).\nWhile we build the RT upon XLNet, any decoder that combines the \nbenefits of masked language modelling (MLM) and causal, autoregres-\nsive language modelling could serve as a backbone (for example, T5 \nwith its sentinel tokens 61, MPNet62, InCoder63 or FIM64). Future work \ncould evaluate the RT on such backbones, intensify the work on reac-\ntion modelling (the RT effectively generalizes forward reaction and \nretrosynthesis models) or improve the ability of the RT to perform \nfine-grained regression (for an interesting failure mode, see Supple -\nmentary Information Section 7.1). Another prospect is to investigate \nproperty-constrained but unseeded molecular generation for more \nglobal chemical space exploration. Finally, our work resonates with \nthe recent trend towards multitask Transformers65–67, and we envision \nit as a means to accelerate the development of foundation models for \nscientific discovery applications.\nMethods\nIn this section we first describe the different components of our meth-\nodology (architectural choices, tokenization scheme, NEs and training \nobjectives). We then describe the implementation details for both \ntraining and evaluation.\nXLNet backbone\nLanguage models utilize either a causal (that is, left-to-right), autore-\ngressive training objective such as recurrent neural networks and \nGPT-3 (ref. 67) or use MLM such as BERT37. Autoregressive approaches \nare preferable for generating long sequences (for example, entire \ndocuments), but since such causal models only condition on previous \ntokens, they cannot be applied to text infilling tasks and cannot profit \nfrom MLM pre-training. Instead, MLMs such as BERT condition on the \nentire sequence to fill masked tokens, making them appear a good \nchoice for infilling tasks; however, MLM approaches fail to generate \nlonger sequences due to their independence assumption. T o unify \nboth worlds and retain the benefits of autoregressive modelling in \ncombination with a bidirectional context, several methods have been \nproposed, with XLNet12 being the first prominent one. The RT is built \nupon an XLNet backbone that is an autoregressive language model, \nbut due to its novel training objective, it, in expectation, obtains full \nbidirectional attention. This bidirectionality is critical because the RT is \nrequired to fill multiple tokens at arbitrary positions in a sequence while \nattending the full remaining sequence (for example, SMILES/SELFIES \nare non-local sequences such that masking functional groups usually \nimplies masking disconnected tokens). Moreover, the independence \nassumption in bidirectional but non-autoregressive models (such as \nBERT) becomes increasingly disruptive as more masked tokens are \nfilled, making XLNet a great choice. This limits BERT’s applicability \nfor generative tasks in biochemistry such as scaffold decoration where \nlarge portions of a molecule might be masked and generation of indi-\nvidual atoms can critically alter the molecule’s functional properties. \nIn general, it is important to notice that the proposed framework can \nbe applied to all transformer flavours, but it certainly benefits from \nan autoregressive generation with full sequence attention even for \ndiscontiguous mask locations. Such approaches rely on either a PLM \nlike XLNet or MPNet\n62 or on sentinel tokens replacing code spans that \nare then predicted at the end of a sequence with an autoregressive \napproach like in T5 (ref. 61), InCoder63 or fill-in-the-middle64. Further \ninformation on the implementation and the model hyperparameters \ncan be found below in the “Model training and evaluation procedure” \nsection.\nT okenization\nThis section describes the processing of alphanumeric sequences, \nthat is, strings consisting of a mixture of numerical and textual sym -\nbols (for a visualization of the tokenization, see Extended Data Fig. 1, \ntop). Unlike previous approaches that modelled 8-bit integers with a \nclassifier68, we strive to represent real numbers with arbitrary floating \npoint precision. Since representing every number as a single token is \nsuboptimal due to a lack of generalization to new numbers and sparsity \nof the provided tokens, we formulated regression as sequential clas-\nsification task. In turn, this necessitates a scheme for converting text \nrepresenting numbers into a sequences of tokens. First, the following \nregular expression splits a string denoting a numerical:\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 439\nArticle https://doi.org/10.1038/s42256-023-00639-z\n\\s∗\\s∗?(\\+|−)?(\\ d+)(\\.)?(\\d+)?\\s∗ (1)\nEach of the resulting matches containing a number is converted \nto a token t v,p where v ∈ℕ∩[ 0..9] is the value/digit and p ∈ℤ  is the  \ndecimal place (for example, 12.3 is split into [1_1, 2_0., 3_-1]). We call \nthese numerical tokens. This representation has the advantage that it \nallows easy decoding of the digit sequence but also distinguishes their \ndecimal order by adhering to classic positional notation. Negative \nnumbers are preceded with a special token. Regarding alphabetic \ntokens, we represent molecules as SELFIES 40 strings and tokenized \nthem with their internal tokenizer. In one ablation study, we instead \nuse SMILES69 and tokenize with the regular expression from ref. 57. \nProtein sequences are tokenized per amino acid.\nNumerical Encodings (NE)\nDue to the inherent structure of numbers, learning the embeddings \nof numerical tokens in a purely data-driven way might be ineffective. \nMoreover, since the RT is trained with cross-entropy loss, no notion \nof similarity between numerical tokens is conveyed. As a remedy, we \npropose NEs, a simple inductive bias about the semantic proximity \nof numerical tokens, similar to positional encodings 1. Our proposed \nNEs are zero vectors for all but numerical tokens of the dictionary. \nWe follow positional notation as above. Given a token tv,p (with digit \nvalue v and decimal place p ), the NE at embedding dimension j  is  \ndefined as\nNEFloat(v,p,j) = (−1)\nj\n⋅ v⋅10p\nj+1 . (2)\nThus, the amplitude of the NE scales with the numerical value of \nthe token. This scheme can be applied to any floating point value x ∈ℝ. \nThe encodings are also independent of the sign of the number. Hence, \nthey equally convey proximity between positive and negative numbers. \nThe NEs are perfectly correlated among embedding dimensions but \nalternate between positive and negative values for even and odd dimen-\nsions and vanish for higher dimensions (see example in Extended Data \nFig. 5a). Critically, the pairwise distances of the NEs are symmetric and \ndecay monotonically with the float value (Extended Data Fig. 5b). In \npractice, we sum the NEs with regular word embeddings and relative \npositional encodings from XLNet (for workflow, see Extended Data  \nFig. 1). Note that we also experimented with integer-based NEs (for \nadditional experiments, see Supplementary Material Section 1).\nTraining objectives\nThe input x for an RT is defined by a concatenation of k property tokens \n[xp]k  and l textual tokens [xt]l, such that x =[ xp,xt]T =[ xp\n1 ,..., xp\nk ,xt\n1,..., xt\nl]T. \nThe full sequence length is T  = k + l, and x p and x t are property and \ntextual tokens, respectively. For a high-level overview of the training \nobjectives, see Extended Data Fig. 1 (bottom).\nPLM objective. The idea of PLM12 is to fill masked tokens autoregres-\nsively by sampling a factorization order z for a sequence x at runtime. \nDecomposing the likelihood pθ(x) according to the facorization order \nyields, in expectation, a bidirectional autoregressive model. Let z ∈𝒵𝒵 T \ndenote one of the T! permutations of our sequence x. If zi and z<i are the \ni-th and first i − 1 elements of z, the PLM objective is\nmax\nθ\n𝔼𝔼z∼𝒵𝒵T [\nT\n∑\ni=1\nlogpθ(xzi |xz<i )] (3)\nIn practice, partial prediction is performed. That is, only the last c \ntokens of the factorization order z are predicted. Following XLNet, z \nis split into a (masked) target subsequence z>c and an unmasked input \nsequence z≤c such that the objective becomes\nℒPLM = max\nθ\n𝔼𝔼z∼𝒵𝒵T [logpθ(xz > c|xz≤c )]\n=𝔼𝔼 z∼𝒵𝒵T [\nT\n∑\ni=c+1\nlogpθ(xzi |xz<i )],\n(4)\nwhere c is a hyperparameter, usually sampled per batch such that the \nfraction of masked tokens is roughly 1/c. We notice that equation (4) \ndoes not make any specific choices on x p and xt. It thus constitutes \nour baseline objective. While equation (4) is a generic objective, it is \ncomputationally exhaustive to optimize due to the permutations. \nMoreover, it is not ideal for our needs because it does not distinguish \nbetween textual and property tokens. Instead, we are aiming to develop \na single model that can predict either numerical tokens (when given \ntext sequences) or text tokens (when given a combination of numerical \nand text tokens). T o that end, we propose to train on two alternating \nobjectives, one designed for property prediction and one for text \ngeneration.\nProperty prediction objective. Instead of randomizing which tokens \nare masked, this objective exclusively masks all the property tokens. \nSpecifically, we constrain the factorization order z by setting the first \nl elements to x t and fixing c  = l. This guarantees that only property \ntokens are masked. Let 𝒵𝒵p\nT denote the set of possible permutations. \nUnder this constraint, the objective then becomes\nℒP = max\nθ\n𝔼𝔼z∼𝒵𝒵p\nT\n[logpθ(xp|xt)]\n=𝔼𝔼 z∼𝒵𝒵p\nT\n[\nT\n∑\ni=c+1\nlogpθ(xp\nzi\n|xt\nz≤c\n,xp\nz > c<i\n)],\n(5)\nwhere xp\nz > c<i\n denotes the c-th to the (i − 1)th element of the factorization \norder z. We emphasize that this ‘tailored’ property objective ℒp is still \noptimized with a cross-entropy loss in practice. Note that this loss \ncannot convey any notion on the qualitative proximity of the prediction \nto the labels because the level of measurement of tokens in a language \nmodel are on a nominal level. Thus, predicting a sequence of numerical \ntokens corresponding to a property score of 0.91 for a sample with a \ntrue property of 0.11 will not generally result in a higher loss than  \npredicting 0.21. Instead, a traditional regression loss operates on a  \nratio scale.\nConditional text generation objective. This objective facilitates the \ngeneration of textual tokens given a property primer and textual \ntokens. We constrain the factorization order z  by setting the first k \nelements to xp to and sampling the cut-off c, such that c ≥ k. This ensures \nthat masking occurs only on textual tokens. With this constraint, we \ndenote the set of permutations by 𝒵𝒵t\nT and the objective becomes\nℒG = max\nθ\n𝔼𝔼z∼𝒵𝒵t\nT\n[logpθ(xt\nz > c\n|xp\nz≤k\n,xt\nz > k<c\n)]\n=𝔼𝔼 z∼𝒵𝒵t\nT\n[\nT\n∑\ni=c+1\nlogpθ(xt\nzi\n|xp\nz≤k\n,xt\nz > k<i\n)].\n(6)\nIntuitively, this objective applies regular PLM while sparing the \nnumerical tokens. It then aims to reconstruct the full text sequence \n(that is, molecule) given the uncorrupted property tokens and partially \ncorrupted textual tokens.\nSelf-consistency (SC) objective. Standalone, the above conditional \ntext generation objective (6 ) does not reward if the generated \nsequences adhere to the primed property. This is critical because in \nchemical as well as natural languages changes in single tokens (that is, \natoms, amino acids or (sub)words) can drastically change the property \n(meaning) of a sequence (sentence). As a remedy, we extended the text \nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 440\nArticle https://doi.org/10.1038/s42256-023-00639-z\ngeneration objective ℒG by an SC term that exploits the dichotomy of \nthe RT. The full objective is given by\nℒSC =ℒ G(x)+α ⋅ℒP( ̂x), (7)\nwhere the second addend is the SC term, weighted by a factor α. Intui-\ntively, it is given by the difference between the property of the sample \nand the predicted property of the generated sample ̂x. Here, ̂x is \nobtained by greedy decoding of the masked tokens and combining it \nwith the non-corrupted tokens of x . T o be precise, ̂x =[ xp, ̂xt]  \nwhere ̂xt =[ m1 ̄x1 +(1−mi)x1,..., ml ̄xl +(1−ml)xl]. Here, m is an indicator  \nvector for whether masking occurred at a given position and \n̄x = argmax ∑\nT\ni=c+1 logpθ(xt\nzi\n|xp\nz<k\n,xt\nz > k<i\n) is the result of greedy decoding. \nIn such a formulation, the RT acts as an oracle during its own optimiza-\ntion, resembling an additional layer of self-supervision. While this \nscheme risks undesired side effects when the model performs poorly \nat property prediction, it introduces a notion of SC and rewards the \ngeneration of molecules that are different from training samples as \nlong as they adhere to the property.\nModel training and evaluation procedure\nImplementation. All experiments build upon the XLNet12 implementa-\ntion from the HuggingFace library70. We expanded the XLNet back-\nbone with our proposed tokenization scheme, an additional encoding \nlayer for the numerical embeddings (Ndim = 16) and the custom training \nobjectives (Extended Data Fig. 1). Regarding architectural hyperpa -\nrameters, we used 32 hidden layers in the Transformer encoder, with \na dimensionality of 256 and 1,024 in the feed-forward layer and 16 \nattention heads (20% dropout). Altogether, this model has ~27 million \ntrainable parameters (exact numbers vary dependent on vocabulary \nsize). During evaluation, greedy decoding was used for property predic-\ntion and beam search decoding for conditional sequence generation.  \nWe used PyTorch 1.3.1 (ref. 71) and the XLNet backbone from  \nTransformers 3.1.0 (ref. 70). Models were trained from scratch unless \nindicated otherwise. All models were trained on single graphics pro -\ncessing units (GPUs) (NVIDIA Tesla A100 or V100). In the following \nsections, we elaborate on the training procedures for each dataset.\nChemical language modelling. Drug likeness (QED). Dataset. Starting \nfrom ~1.6 million bioactive molecules from ChEMBL72, we created a syn-\nthetic dataset by computing the QED38 score (q ∈ [0, 1]) for all molecules \nwith RDKit and rounded to three decimal places. We used ~1.4 million \nmolecules for training, 1,000 for validation and 10,000 for testing.\nProcedure. We started training the models with the vanilla PLM \nobjective (equation (4)) on the QED dataset until validation perplexity \nsaturated (~4 days, single GPU). Thereafter, the models were further \nrefined on the same dataset by alternating every 50 steps between \nobjectives (equation (5) and equation (7)). We perform ablation studies  \non the SC loss, setting α  in equation ( 7) to 0 and 1, respectively.  \nThe SELFIES/SMILES vocabulary had 509 and 724 tokens, respectively. \nDuring evaluation, greedy decoding was used for property prediction \nand beam search decoding for molecular generation. During evalua-\ntion, we set c = 2.5, which implies that roughly ~40% of the tokens were \nmasked (maximum span: seven tokens).\nMoleculeNet benchmark. Dataset. We focused on three regression \ndatasets from the MoleculeNet benchmark 39: ESOL, FreeSolv and  \nLipophilicity, where the task is to predict water solubility, hydration \nfree energy and lipophilicity of a molecule, respectively. For each \ndataset, we performed three random splits (as recommended by  \nref. 39) with 15% validation data. Because the datasets are small (<5,000 \nsamples), we used offline SMILES augmentation 73 to augment the  \ntraining dataset by a factor of 16.\nProcedure. For the MoleculeNet datasets, the models were \nwarm-started using the QED initialization and trained for only 50,000 \nsteps (batch size 4) with early stopping. Since the QED pre-training  \nutilized numerical values in [0, 1], we normalized the regression values \nof the entire MoleculeNet datasets to the same range (using train -\ning data only) and rounded them also to three decimal places. For all \nobjectives, unless otherwise constrained, we set the masking hyper -\nparameter c = 5 and restrict the span of consecutively masked tokens \nto a maximum of five tokens.\nProperty optimization benchmark. Dataset. This is a benchmark for \nproperty-driven, conditional molecular generation. The goal is to adapt \na seed molecule such that a property is maximized while adhering to \na fixed similarity constraint. We obtained the data from \n42 which ships \nwith a fixed split of 215,381 training and 799 test molecules and their \npenalized logP (pLogP) value 74. pLogP is the octanol-water partition \ncoefficient (logP) penalized by the synthetic accessibility score and \nthe number of cycles with > 6 atoms. Hence, pLogP just like QED can \nbe computed deterministically from the molecule42.\nProcedure. For this task, the models were also warm-started using \nthe QED initialization and trained for 50,000 steps with early stopping \non perplexity. T o assemble the candidates for the optimization of one \nseed molecule, we tried to follow the process of ref. 42 as closely as pos-\nsible. Reference 42 applied 80 gradient steps, then decoded 80 molecules \nand reported the molecule with the highest pLogP score that satisfies \nthe similarity constraint δ . Instead, we form a pool of molecules by \nprompting 80 times with the same seed molecule but varying the frac-\ntion and the maximum span of masked tokens. From the pool of decod-\nings we report the molecule with the highest pLogP, just like refs. 42,50.\nProtein sequence language modelling.  Protein interaction index \n(Boman). Dataset. As a large-scale, labelled dataset for proteins we \nfocused on the Boman index, a measure of potential protein interac-\ntion for peptides. It is the average of the solubility values of the resi -\ndues75. We collected all 2,648,205 peptides with 15–45 amino acids from  \nUniProt76, computed their Boman index and used 10,000 and 1,000 \nfor testing and validation, respectively.\nProcedure. T o model protein sequences, we started with training \non the Boman dataset. We trained three groups of models, one for the \nvanilla PLM objective (equation (4)) and two for the alternating objec-\ntives. We again alternated every 50 steps between optimizing (equation \n(5) and equation (7)) and trained one set of models with and one set \nwithout the SC loss, such that α = 1 and α = 0, respectively, in equation \n(7). Models were trained until validation perplexity saturated (~4 days, \nsingle GPU). The numerical values of the Boman index, originally in \nthe range [−3.1, 6.1] were normalized to [0, 1] (using training data only)  \nand rounded to three decimal places.\nT APE benchmark. Dataset. We focused on two datasets from the TAPE \nbenchmark43: Fluorescence77 and Stability 78. The goal is to predict, \nrespectively, the fluorescence and intrinsic folding stability of a protein \nthat is one to four mutations away from a training protein. Both datasets \nship with fixed splits. The fluorescence (stability) dataset has 21,446 \n(53,416) training, 5,362 (2,512) validation and 27,217 (12,851) test samples.\nProcedure. For both datasets, three models were warm-started \nusing the Boman initialization (PLM objective) and trained until valida-\ntion performance saturated (~100,000 steps). Experiments were con-\nducted using three configurations; PLM objective, and alternated \ntraining with ( ℒSC) and without ( ℒG) the SC objective. The numerical \nvalues were again scaled to [0, 1]. On the Fluorescence data, a small value \nof Gaussian noise was added to some training samples due to an inter-\nesting failure mode (Supplementary Information Section 7.1). For the \nevaluation of the conditional generation task, the models were given \nmore flexibility: 60% of the tokens were masked (that is, c = 1.7 in equa-\ntion (3)) and the maximum span was seven amino acid residues. We did \nnot evaluate the RT on conditional generation for the Fluorescence \ndataset because of a massive pre-training–fine-tuning mismatch: While \nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 441\nArticle https://doi.org/10.1038/s42256-023-00639-z\nthe Boman dataset used for pre-training consisted of 15–45 residues \n(mean ± s.d., 36 ± 7), the fluorescence proteins were significantly larger \n(246 ± 0.2 residues, P < 0.001). Instead, the proteins in the stability \ndataset were similar in size to the pre-training data (45 ± 3 residues).\nChemical reaction modelling.  Pre-training on USPTO. Dataset.  \nWe used reactions from the US Patent Office (USPTO), the largest \nopen-source dataset about chemical reactions79 to learn generic reac-\ntion chemistry. Since no yield information was available, the utilized \nnumerical property was the total molecular weight of all precursors. The \ndataset contained n = 2,830,616 reactions and was obtained from ref. 4.\nProcedure. Since the two reaction yield datasets cover only  \nnarrow regions of the chemical space (one template applied to many \nprecursor combinations), we warm up the model on broader reaction \nchemistry extracted from patents (USPTO). A total of 5,000 reactions \nwere held out for validation, and the model was trained until valida -\ntion performance on the two alternating objectives (equation (5) and \nequation ( 7) with α  = 1) saturated. The masking hyperparameter c  \nwas set to 2.5, and the model were trained for ~2 days (single GPU). The \nvocabulary for reaction SELFIES contained 861 tokens.\nReaction yield datasets. Dataset. We investigated two high-  \nthroughput experimentation (HTE) yield datasets that examine specific \nreaction types: Buchwald–Hartig aminations 58 and Suzuki–Miyaura \ncross-coupling reactions 59. Both datasets were investigated in the \nsame ten random splits as examined in ref. 44 with a 70%/30% train/\nvalidation ratio.\nThe Buchwald–Hartwig dataset was produced by ref. 58  and \ninvestigates HTE of palladium-catalysed Buchwald–Hartwig C–N \ncross-coupling reactions. The reaction space comprises 3,955 reac -\ntions, spanned by 15 unique aryl and heteroaryl halides, 4 Buchwald \nligands, 3 bases and 22 isoxazole additives. A palladium catalyst and a \nmethylaniline are the fifth and sixth precursor, respectively; however, \nthey are identical for all reactions. Each reaction is associated with a \nyield y ∈ [0, 100], and the ten random splits were identical to the ones \nreleased by ref. 80 that are also used by all competing methods in  \nSupplementary Table 6. Yield is given in a range of [0, 100].\nThe Suzuki cross-coupling dataset was provided by ref. 59  and \ninvestigates HTE of Suzuki–Miyaura reactions across 15 pairs of elec-\ntrophiles and nucleophiles, leading to different products, respec -\ntively. For each pair, a combination of 4 solvents, 12 ligands and 8 bases  \n(reagents) was measured, resulting in a total of 5,760 reaction yields \nthat we scale to the range [0, 100]. The catalyst is identical for all reac-\ntions; some reactions omitted the ligand or the base, while others \ncontained electrophiles, nucleophiles, ligands, bases or solvents that \nwere composed of different fragments (for example, salts).\nProcedure. For both datasets, ten models were fine-tuned respec-\ntively on repeated random splits. The training objectives again alternated \nevery 50 steps between property prediction (equation (5)) and condi-\ntional generation (equation (7) with α = 1) for a maximum of 50,000 steps \n(~1 day). Notably, during the conditional generation task we sampled \none precursor per batch and then entirely but exclusively masked this \nprecursor. Thus the objective for the model became to reconstruct a \nmissing precursor from the remaining precursors and the reaction yield \n(or to produce an alternative precursor with a similar predicted yield).\nEvaluation and performance metrics\nRegression. For the regression (or property prediction) task, we convert \nthe sequence of predicted (numerical) tokens into a floating-point predic-\ntion (the model never failed to predict a token sequence not correspond-\ning to a valid numerical). We then report the RMSE, PCC or coefficient of \ndetermination (R2), dependent on the dataset and previous methods.\nConditional sequence generation.  Dependent on the application \ndomain, different metrics are utilized (see above).\nSmall molecule and protein modelling. We strive to assess the \nmodel’s ability to decorate an arbitrary, possibly discontiguous frac-\ntional input sequence (for example, a molecular scaffold) according \nto a property of interest. Therefore, we randomly mask a fraction of \ntokens of the text sequence and then query the model with ten equi -\ndistant property primers spanning the full range of property values. \nThe metric is the average Spearman’s ρ between the ten primers and \nthe actual properties. Spearman is favourable over Pearson because \nit is only rank sensitive. Note that, due to constraints induced by the \nfragmented sequence, covering the entire property spectrum is usu-\nally impossible such that, for example, RMSE is inappropriate for this \ntask (for example, priming a highly toxic scaffold with low toxicity \ncannot yield a non-toxic molecule). As a sanity check, we also report \n0-Var, that is, the percentage of test molecules/proteins for which the \ngeneration was unaffected by the primer, that is, upon priming with \nthe ten equidistant property primers and the fractional sequences, \nthe decoded molecules/proteins were all identical (the lower  \nthe better).\nOn the property optimization benchmark from ref. 42, we report \nthe same metrics as in their work: the success rate in generating  \nmolecules with higher logP  (while adhering to the similarity  \nconstraint δ), the Tanimoto similarity δ to the seed molecule and the \naverage improvement in plogP.\nChemical reaction modelling. For the reaction yield datasets, we \nchallenge the model by two sequence generation tasks. First, we fully \nreconstructed a precursor solely based on the remaining precursors \nand the reaction yield. The top-three predicted sequences (decoded \nvia beam search) are considered, s.t. top-three accuracy is reported. \nAdditionally we report the average Tanimoto similarity of the most \nsimilar of the top-three molecules to the seed molecule. We used \nRDKit Morgan fingerprints with radius 2 (roughly equivalent to ECFP4  \n(ref. 45)). Secondly, we measure the capability of decorating existing \nreactions to obtain a (potentially) higher yield. T o that end, the model \nis prompted with incomplete reactions consisting of an increased yield, \nan entirely masked precursor and complete remaining precursors. \nWe consider the top-three predicted sequences (decoded via beam \nsearch) and report the fraction of samples where one of the reactions \nhad a higher (predicted) yield (success rate). The second response \nmetric is the mean improvement in (predicted) reaction yield (yield \ny ∈ [0, 100]; the distributions are right-skewed). Note that we exclude \ntrivial solutions by removing all predicted precursors that exist in the \ntraining dataset.\nBaseline models\nk-NN. For small-molecule and protein modelling we reported results in \nproperty prediction with the k-NN baseline model. For small molecules, \nthe distance measure was (inverted) Tanimoto similarity 81 of ECFP4 \nfingerprints45. For the protein language models, the Levenshtein dis-\ntance between the protein sequences was used53. For the k-NN baseline \nmodels, k was determined on the basis of the best performance on the \nvalidation data. This led to k = 25 for the drug-likeness/QED task, k = 21 \nfor the protein interaction (Boman index) task, k = 50 for the fluores-\ncence and k = 15 for the stability task.\nXLNet with regression head. For the molecular property prediction \non the MoleculeNet datasets, we trained an XLNet\n12 model with a con-\nventional regression loss. This maximizes comparability to the RT since \nit, unlike the other models in Extended Data Table 2, also uses an XLNet \nbackbone. This model was initialized using the XLNet-base-cased \nweights from HuggingFace and subsequently the SequenceClas-\nsification head was fine-tuned with an L2 loss. The model contained \n~93 million parameters and was fine-tuned for 200 epochs without any \nhyperparameter optimization. Early stopping was used to determine \nthe best epoch.\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 442\nArticle https://doi.org/10.1038/s42256-023-00639-z\nData availability\nThe data for the MoleculeNet experiments can be obtained from \nhttps://moleculenet.org/datasets-1. The data for the molecular  \noptimization experiments can be obtained from https://github.com/ \nwengong-jin/icml18-jtnn/tree/master/data/zinc. The data for the pro-\ntein language modelling experiments can be obtained from https://\ngithub.com/songlab-cal/tape. The data for the reaction yield experi-\nments can be obtained from https://github.com/rxn4chemistry/  \nrxn_yields/tree/master/data.\nCode availability\nUsage of trained models\nThe RT is implemented in the Generative T oolkit for Scientific Discov-\nery (GT4SD)82, which provides ready-to-use pipelines for inference on \npre-trained models as well as training or fine-tuning on custom data. \nThe GT4SD endpoint of the RT facilitates highly customizable local \nchemical space exploration. The user can decide to (1) make no assump-\ntions about which tokens are being masked, (2) mask only specific types \nof atoms, (3) preserve certain structures while randomly masking on \nthe rest, (4) mask certain moieties or (5) decide on a token-by-token \nbasis on which atoms are masked. Via GT4SD, versions of the RT trained \non the QED and ESOL datasets (small molecules), the stability dataset \n(proteins) and the USPTO-pre-trained reaction model are available. \nMoreover, GT4SD also distributes additional versions of the RT trained \non multi-property prediction tasks not described herein, including but \nnot limited to ring-opening polymerization catalysis and block copoly-\nmers (CITE REF 59) a logP as well as a combined logp-synthesizability \nmodel. A guide to use the RT be found on https://github.com/GT4SD/ \ngt4sd-core/tree/main/examples/regression_transformer. A notebook \nwith a short demo can be found under https://github.com/GT4SD/  \ngt4sd-core/blob/main/notebooks/regression-transformer-demo.\nipynb. The datasets used for benchmarking are available from the \nrespectively referenced papers.\nGUI Demo\nA simple webapp of the RT for inference of pre-trained models has been \nmade publicly available via HuggingFace spaces at https://huggingface. \nco/spaces/GT4SD/regression_transformer. The app was build with \nGradio83 upon the GT4SD82 implementation.\nReproduction\nThe code base to facilitate reproduction of all experiments is pub -\nlicly available at https://github.com/IBM/regression-transformer   \nrefs. 84–91.\nReferences\n1. Vaswani, A. et al. In Advances in Neural Information  \nProcessing Systems 30 (Eds Guyon, I. etal.) 5998–6008 (NIPS, \n2017).\n2. Schwaller, P. et al. Molecular transformer: a model for \nuncertainty-calibrated chemical reaction prediction. ACS Cent. \nSci. 5, 1572–1583 (2019).\n3. Schwaller, P. et al. Mapping the space of chemical reactions using \nattention-based neural networks. Nat. Mach. Intell. 3, 144–152 \n(2021).\n4. Schwaller, P., Hoover, B., Reymond, Jean-Louis, Strobelt, H. \n& Laino, T. Extraction of organic chemistry grammar from \nunsupervised learning of chemical reactions. Sci. Adv. 7, \neabe4166 (2021).\n5. Rives, A. et al. Biological structure and function emerge from \nscaling unsupervised learning to 250 million protein sequences. \nProc. Natl Acad. Sci. USA 118, e2016239118 (2021).\n6. Jumper, J. et al. Highly accurate protein structure prediction with \nalphafold. Nature 596, 583–589 (2021).\n7. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification \nwith deep convolutional neural networks. Adv. Neural Inf. Process. \nSyst. 25, 1097–1105 (2012).\n8. Luong, M.-T., Pham, H. & Manning, C. D. Effective approaches \nto attention-based neural machine translation. In Proc. 2015 \nConference on Empirical Methods in Natural Language Processing \n1412–1421 (ACL, 2015).\n9. Ramachandran, P. et al. Stand-alone self-attention in  \nvision models. Adv. Neural Inf. Process. Syst. 32, 68–80  \n(2019).\n10. Lu, K., Grover, A., Abbeel, P. & Mordatch, I. Frozen pretrained \ntransformers as universal computation engines. In Proc. AAAI \nConference on Artificial Intelligence 36, 7628–7636 (AAI Press, \n2022).\n11. Chen, L. et al. Decision transformer: reinforcement learning  \nvia sequence modeling. Adv. Neural Inf. Process. Syst. 34, \n15084–15097 (2021).\n12. Yang, Z. et al. Xlnet: Generalized autoregressive pretraining \nfor language understanding. Adv. Neural Inf. Process. Syst., 32, \n5753–5763 (2019).\n13. Elton, D. C., Boukouvalas, Z., Fuge, M. D. & Chung, P. W. Deep \nlearning for molecular design—a review of the state of the art. \nMol. Syst. Des. Eng. 4, 828–849 (2019).\n14. Chen, Z., Min, MartinRenqiang, Parthasarathy, S. & Ning, X. A deep \ngenerative model for molecule optimization via one fragment \nmodification. Nat. Mach. Intell. 3, 1040–1049 (2021).\n15. Wu, Z., Johnston, K. E., Arnold, F. H. & Yang, K. K. Protein sequence \ndesign with deep generative models. Curr. Opin. Chem. Biol. 65, \n18–27 (2021).\n16. Madani, A. et al. Large language models generate functional \nprotein sequences across diverse families Nat. Biotechnol. (2023); \nhttps://doi.org/10.1038/s41587-022-01618-2\n17. Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. Smiles-bert: \nlarge scale unsupervised pre-training for molecular property \nprediction. In Proc. 10th ACM International Conference on \nBioinformatics, Computational Biology and Health Informatics (Eds \nShi, X.M. et al.) 429–436 (ACM, 2019).\n18. Kim, H., Lee, J., Ahn, S. & Lee, J. R. A merged molecular \nrepresentation learning for molecular properties prediction with a \nweb-based service. Sci. Rep. 11, 1–9 (2021).\n19. Mahmood, O., Mansimov, E., Bonneau, R. & Cho, K. Masked \ngraph modeling for molecule generation. Nat. Commun. 12, 1–12 \n(2021).\n20. Kotsias, P.-C. et al. Direct steering of de novo molecular \ngeneration with descriptor conditional recurrent neural networks. \nNat. Mach. Intell. 2, 254–265 (2020).\n21. Bagal, V., Aggarwal, R., Vinod, P. K. & Priyakumar, U. D. Molgpt: \nmolecular generation using a transformer-decoder model.  \nJ. Chem. Inf. Model. 62, 2064–2076 (2021).\n22. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a \npre-trained transformer for computational chemistry. Mach. \nLearn. Sci. Technol. 3, 015022 (2021).\n23. Lu, J. & Zhang, Y. Unified deep learning model for multitask \nreaction predictions with explanation. J. Chem. Inf. Model. 62, \n1376–1387 (2022).\n24. Méndez-Lucio, O., Baillif, B., Clevert, Djork-Arné, Rouquié, D. & \nWichard, J. De novo generation of hit-like molecules from gene \nexpression signatures using artificial intelligence. Nat. Commun. \n11, 1–10 (2020).\n25. Born, J. et al. Data-driven molecular design for discovery and \nsynthesis of novel ligands: a case study on SARS-CoV-2. Mach. \nLearn. Sci. Technol. 2, 025024 (2021).\n26. Gomez-Bombarelli, R. et al. Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Cent. \nSci. 4, 268–276 (2018).\n27. Maziarz, K. et al. Learning to extend molecular scaffolds with \nstructural motifs. In The Tenth International Conference on \nLearning Representations (ICLR, 2022).\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444\n 443\nArticle https://doi.org/10.1038/s42256-023-00639-z\n28. Shi, C. et al. Graphaf: a flow-based autoregressive model for \nmolecular graph generation. In 8th International Conference on \nLearning Representations (ICLR, 2020).\n29. Jain, M. et al. Biological sequence design with gflownets. In \nInternational Conference on Machine Learning, pages 9786–9801 \n(PMLR, 2022).\n30. Xu, M. et al. Geodiff: a geometric diffusion model for molecular \nconformation generation. In The Tenth International Conference \non Learning Representations (ICLR, 2022).\n31. Shen, C., Krenn, M., Eppel, S. & Aspuru-Guzik, A. Deep molecular \ndreaming: inverse machine learning for de-novo molecular \ndesign and interpretability with surjective representations. Mach. \nLearn. Sci. Technol. 2, 03LT02 (2021).\n32. Fu, T. et al. Differentiable scaffolding tree for molecule \noptimization. In The Tenth International Conference on Learning \nRepresentations (ICLR, 2022).\n33. Linder, J. & Seelig, G. Fast activation maximization for molecular \nsequence design. BMC Bioinformatics 22, 1–20 (2021).\n34. Daulton, S. et al. Robust multi-objective Bayesian optimization \nunder input noise. In International Conference on Machine \nLearning, ICML 2022, volume 162 of Proc. Machine Learning \nResearch pages 4831–4866 (PMLR, 2022).\n35. Yang, Z., Milas, K. A. & White, A. D. Now what sequence? Pre-trained \nensembles for Bayesian optimization of protein sequences. Preprint \nat bioRxiv (2022); https://doi.org/10.1101/2022.08.05.502972\n36. Khan, A. et al. Toward real-world automated antibody design \nwith combinatorial Bayesian optimization. Cell Report Methods 3, \n100374 (2023).\n37. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics pages 4171–4186  \n(ACL, 2019).\n38. Bickerton, G. R., Paolini, G. V., Besnard, J. érémy, Muresan, S. & \nHopkins, A. L. Quantifying the chemical beauty of drugs.  \nNat. Chem. 4, 90 (2012).\n39. Wu, Z. et al. Moleculenet: a benchmark for molecular machine \nlearning. Chem. Sci. 9, 513–530 (2018).\n40. Krenn, M., Häse, F., Nigam, A. K., Friederich, P. & Aspuru-Guzik, A.  \nSelf-referencing embedded strings (SELFIES): a 100% robust \nmolecular string representation. Mach. Learn. Sci. Technol. 1, \n045024 (2020).\n41. Rong, Y. et al. In Advances in Neural Information Processing \nSystems (Eds Larochelle, H., Ranzato, M., Hadsell, R., Balcan,  \nM.-F. & Lin, H.-T.) 33 (2020).\n42. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational \nautoencoder for molecular graph generation. In International \nConference on Machine Learning (Eds Dy, J. & Krause, A.)  \n2323–2332 (PMLR, 2018).\n43. Rao, R. et al. In Advances in Neural Information Processing Systems \n(Eds Schölkopf, B. et al.) 9686–9698 (MIT Press, 2019).\n44. Schwaller, P., Vaucher, A. C., Laino, T. & Reymond, J.-L. Prediction \nof chemical reaction yields using deep learning. Mach. Learn. Sci. \nTechnol. 2, 015016 (2021).\n45. Rogers, D. & Hahn, M. Extended-connectivity fingerprints.  \nJ. Chem. Inf. Model. 50, 742–754 (2010).\n46. Ertl, P. An algorithm to identify functional groups in organic \nmolecules. J. Cheminform. 9, 1–7 (2017).\n47. Vig, J. et al. Bertology meets biology: interpreting attention in \nprotein language models. In 9th International Conference on \nLearning Representations (ICLR, 2021).\n48. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. & Dahl, G. E. \nNeural message passing for quantum chemistry. In International \nConference on Machine Learning (Eds Precup, D. & Tehpages, Y.W.) \n1263–1272 (PMLR, 2017).\n49. Fabian, B. et al. Molecular representation learning with language \nmodels and domain-relevant auxiliary tasks. Preprint at arXiv \nhttps://doi.org/10.48550/arXiv.2011.13230 (2020).\n50. You, J., Liu, B., Ying, Z., Pande, V. & Leskovec, J. Graph \nconvolutional policy network for goal-directed molecular graph \ngeneration. In Advances in Neural Information Processing Systems \n(Eds Bengio, S. & Wallach, H.M.) 6412–6422 (Curran Associates \nInc., 2018).\n51. Fan, Y. et al. Back translation for molecule generation. \nBioinformatics 38, 1244–1251 (2022).\n52. Zang, C. & Wang, F. Moflow: an invertible flow model for generating \nmolecular graphs. In Proceedings of the 26th ACM SIGKDD \nInternational Conference on Knowledge Discovery & Data Mining \npages 617–626 (Association for Computing Machinery, 2020).\n53. Levenshtein, V. I. Binary codes capable of correcting deletions, \ninsertions, and reversals. Sov. Phys. Doklady 10, 707–710 (1966).\n54. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, \nG. M. Unified rational protein engineering with sequence- \nbased deep representation learning. Nat. Methods 16,  \n1315–1322 (2019).\n55. Brandes, N., Ofer, D., Peleg, Y., Rappoport, N. & Linial, M. \nProteinbert: a universal deep-learning model of protein sequence \nand function. Bioinformatics 38, 2102–2110 (2022).\n56. Meier, J. et al. Language models enable zero-shot prediction \nof the effects of mutations on protein function. Adv. Neural Inf. \nProcess. Syst. 34, 29287–29303 (2021).\n57. Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C. & Laino, T. Found in \ntranslation: predicting outcomes of complex organic chemistry \nreactions using neural sequence-to-sequence models. Chem. Sci. \n9, 6091–6098 (2018).\n58. Ahneman, D. T., Estrada, JesúsG., Lin, S., Dreher, S. D. &  \nDoyle, A. G. Predicting reaction performance in C–N \ncross-coupling using machine learning. Science 360,  \n186–190 (2018).\n59. Perera, D. et al. A platform for automated nanomole-scale \nreaction screening and micromole-scale synthesis in flow. \nScience 359, 429–434 (2018).\n60. Park, N. et al. An extensible platform for enabling artificial \nintelligence guided design of catalysts and materials. Preprint \nat ChemRxiv https://doi.org/10.26434/chemrxiv-2022-811rl-v2 \n(2022). In Revision at Nature Communications\n61. Raffel, C. et al. Exploring the limits of transfer learning with  \na unified text-to-text transformer. J. Mach. Learn. Res. 21,  \n1–67 (2020).\n62. Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. MPNet: Masked and \nPermuted Pre-training for Language Understanding. In Advances \nin Neural Information Processing Systems 33 (Eds Larochelle, H. \net al.) (NeruIPS, 2020).\n63. Fried, D. et al. Incoder: a generative model for code infilling \nand synthesis. Preprint at arXiv https://doi.org/10.48550/\narXiv.2204.05999 (2022).\n64. Bavarian, M. et al. Efficient training of language models to fill in the \nmiddle. Preprint at arXiv https://doi.org/10.48550/arXiv.2207.14255 \n(2022).\n65. Sanh, V. et al. Multitask prompted training enables zero-shot \ntask generalization. In International Conference on Learning \nRepresentations (OpenReview.net, 2022).\n66. Lu, K., Grover, A., Abbeel, P. & Mordatch, I. Pretrained \ntransformers as universal computation engines. In Proc. of the \nThirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22) \n7628–7636 (AAAI Press, 2022); (https://ojs.aaai.org/index.php/ \nAAAI/article/view/20729/20488\n67. Brown, Tom et al. In Advances in Neural Information Processing \nSystems Vol. 33, (Eds Schölkopf, B. et al.) 1877–1901 (MIT Press, \n2020).\nNature Machine Intelligence | Volume 5 | April 2023 | 432–444 444\nArticle https://doi.org/10.1038/s42256-023-00639-z\n68. Van Oord, A., Kalchbrenner, N. & Kavukcuoglu, K. Pixel recurrent \nneural networks. In International Conference on Machine \nLearning (Eds Balcan, M.F. & Weinberger, K.Q.) 1747–1756 (PMLR, \n2016).\n69. Weininger, D. SMILES, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules.  \nJ. Chem. Inf. Comput. Sci. 28, 31–36 (1988).\n70. Wolf, T. et al. Transformers: state-of-the-art natural language \nprocessing. In Proc. 2020 Conference on Empirical Methods in \nNatural Language Processing: System Demonstrations pages \n38–45 (Association for Computational Linguistics, 2020).\n71. Paszke, A. et al. Pytorch: an imperative style, high-performance \ndeep learning library. Adv. Neural Inf. Process. Syst. 32,  \n8026–8037 (2019).\n72. Mendez, D. et al. Chembl: towards direct deposition of bioassay \ndata. Nucleic Acids Res. 47, D930–D940 (2019).\n73. Bjerrum, E. J. SMILES enumeration as data augmentation for \nneural network modeling of molecules. Preprint at arXiv https://\ndoi.org/10.48550/arXiv.1703.07076 (2017).\n74. Kusner, M. J, Paige, B. & Hernández-Lobato, J. M. Grammar \nvariational autoencoder. In Proc. 34th International Conference on \nMachine Learning Vol. 70, 1945–1954 (JMLR, 2017).\n75. Boman, H. G. Antibacterial peptides: basic facts and emerging \nconcepts. J. Intern. Med. 254, 197–215 (2003).\n76. The UniProt Consortium. UniProt: the universal protein \nknowledgebase in 2021. Nucleic Acids Res. 49, D480–D489 (2020).\n77. Sarkisyan, K. S. et al. Local fitness landscape of the green \nfluorescent protein. Nature 533, 397 (2016).\n78. Rocklin, G. J. et al. Global analysis of protein folding using \nmassively parallel design, synthesis, and testing. Science 357, \n168–175 (2017).\n79. Lowe, D. Chemical reactions from US patents (1976-Sep2016). \nFigshare https://figshare.com/articles/dataset/Chemical_ \nreactions_from_US_patents_1976-Sep2016_/5104873 (2017)\n80. Sandfort, F., Strieth-Kalthoff, F., Kühnemund, M., Beecks, C. & \nGlorius, F. A structure-based platform for predicting chemical \nreactivity. Chem 6, 1379–1390 (2020).\n81. Tanimoto, T. T. Elementary Mathematical Theory of Classification \nand Prediction (International Business Machines Corp., 1958).\n82. Manica, M. et al. GT4SD: Generative toolkit for scientific \ndiscovery. GitHub https://github.com/GT4SD/gt4sd-core (2022).\n83. Abid, A. et al. Gradio: hassle-free sharing and testing of ML \nmodels in the wild. Preprint at arXiv https://doi.org/10.48550/ \narXiv.1906.02569 (2019).\n84. Born, J. & Manica, M. Regression transformer repository. Zenodo \nhttps://doi.org/10.5281/zenodo.7639206 (2023).\n85. He, P., Liu, X., Gao, J. & Chen, W. Deberta: decoding-enhanced \nbert with disentangled attention. In 9th International Conference \non Learning Representations (ICLR, 2021).\n86. Dai, Z. et al. Transformer-xl: attentive language models beyond \na fixed-length context. In Proc. 57th Annual Meeting of the \nAssociation for Computational Linguistics. 2978–2988 (Association \nfor Computational Linguistics, 2019).\n87. Bai, H. et al. Segatron: segment-aware transformer for language \nmodeling and understanding. In Proc. AAAI Conference on \nArtificial Intelligence Vol. 35, 12526–12534 (AAAI Press, 2021).\n88. Wang, Y.-A. & Chen, Y.-N. What do position embeddings learn? \nAn empirical study of pre-trained language model positional \nencoding. In Proc. of the 2020 Conference on Empirical Methods \nin Natural Language Processing (EMNLP) 6840–6849 (Association \nfor Computational Linguistics, 2020).\n89. Zhang, J., Mercado, Rocío, Engkvist, O. & Chen, H. Comparative \nstudy of deep generative models on chemical space coverage.  \nJ. Chem. Inf. Model. 61, 2572–2581 (2021).\n90. Bemis, G. W. & Murcko, M. A. The properties of known drugs. 1. \nMolecular frameworks. J. Med. Chem. 39, 2887–2893 (1996).\n91. Vig, J. A multiscale visualization of attention in the transformer \nmodel. In Proc. 57th Annual Meeting of the Association for \nComputational Linguistics: System Demonstrations pages 37–42 \n(Association for Computational Linguistics, 2019).\nAcknowledgements\nThe authors thank the entire AI for Scientific Discovery Group at IBM \nand particularly C. Baldassari and A. Leonov for useful discussions on \nreaction chemistry. The authors especially thank E. J. Bjerrum but also \nthe anonymous reviewers for their constructive and valuable feedback \nthat helped improving the article tremendously.\nAuthor contributions\nJ.B. and M.M. conceived the initial idea for the project, set the scope of \nexperiments and wrote the code base as well as the manuscript.  \nJ.B. further trained the models, performed the experiments, analysed \nthe results, created the visualizations and devised the alternating \ntraining scheme with SC loss.\nFunding\nOpen access funding provided by Swiss Federal Institute of \nTechnology Zurich.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-023-00639-z.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00639-z.\nCorrespondence and requests for materials should be addressed to \nJannis Born or Matteo Manica.\nPeer review information Nature Machine Intelligence thanks  \nEsben Jannik Bjerrum and the other, anonymous, reviewer(s) for their \ncontribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons license, and \nindicate if changes were made. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicense, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons license \nand your intended use is not permitted by statutory regulation \nor exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this license, visit \nhttp://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Fig. 1 | Workflow of the Regression Transformer (RT) model. \nBased on the XLNet backbone, the RT is a dichotomous model designed to handle \ncombinations of text and numbers. T op: An input sequence consisting of a \nmolecular string (red) and two property tags (blue), each associated to a floating \nvalue (green). Numbers are tokenized into a sequence of tokens that preserve the \ndecimal order of each character. The pipe (∣) is a separator token distinguishing \nnumerical and text tokens. Middle: We propose numerical encodings that inform \nthe model about the semantic proximity of these tokens and naturally integrate \nwith relative positional encodings and classical learned embeddings. The RT \nrelies on a XLNet backbone and follows permutation language modeling (PLM). \nBottom: Multiple training objectives are proposed and combined (predicted \ntokens are emphasized in the figure). Following the vanilla PLM objective \n12, \nmasking occurs randomly throughout the sequence. In the property objective, \nmasking occurs exclusively on the property tokens. In the generation  \nobjective, masking occurs exclusively on the textual tokens (here: SMILES). \nThis objective can be augmented with a self-consistency term L\nSC that exploits \nthe dichotomoy of the model. In practice, we use an alternating training \nscheme designed to concurrently excel at property prediction and conditional \ngeneration tasks. Note that the RT builds upon an XLNet-backbone which \nsamples a token factorization order (following PML as proposed by  \nYang et al. \n12; not shown). The dots indicate that the RT naturally scales to  \nmultiple property tags.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Fig. 2 | More distant queries are harder to decorate. When \ngradually increasing task difficulty (that is, the distance between the QED of the \nseed molecule and the primed property), the distance between the QED of the \ngenerated molecule and the primed property increases linearly. Data presented \nas means, error bars denote 95% confidence intervals. For the blue, orange and \ngreen bars, a total of 880k, 239k and 207k generated molecules are evaluated \nrespectively.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Fig. 3 | Learned embeddings of numerical tokens. Left: For an \nexemplary dimension, embeddings for 20 tokens, corresponding to 10 digits and \n2 decimal places are shown. Right: Embeddings for 20 exemplary dimensions \nacross all ten digits. The stars indicate the significance level of the Pearson \ncorrelation. The analysis is based on a SELFIES model without any NEs (PLM \nobjective).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Fig. 4 | Discovering novel, more effective reactions by \nadapting an unseen Buchwald-Hartwig amination. Below an unseen BH \namination (top) and its experimentally reported yield, we show four  \nRT-generated reactions that selectively replace individual precursors. Upon \npriming the RT with a higher yield and a given precursor type, the RT generated \nreactions with higher yield, as predicted by the RT. The RXN confidence stems \nfrom the forward reaction prediction model by Schwaller et al. \n2 which confirmed \nthat the reaction would result in the shown product in all cases. Note that no \nadaptations of 4-Methylaniline and the Palladium-catalyst are generated since \nthey are constanta cross the dataset.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Fig. 5 | Float-based numerical encodings. a) Numerical encodings for an molecule with a QED of 0.179. b) Pairwise distances of numerical encodings \nfor floats between 0 and 100 (the NEs of all tokens associated to a float are summed up).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Table 1 | Performance comparison in predicting QED\nMAE stands for mean absolute error. The RT with alternating objectives used α = 0 in Equation (7). Our model names are shown in bold; best performance shown in bold.\n\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Table 2 | RMSE (↓) in predicting MoleculeNet dataset properties\nPerformance on three different datasets across predictive models. By L Reg we denote whether a given model used a loss (or objective function) that relied on regression. All models used \nrepeated random splits. NE means numerical encodings and α refers to the loss function in Equation (7). Standard deviations shown, best model shown in bold.\n\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00639-z\nExtended Data Table 3 | Conditional generation for MoleculeNet datasets\nAverage performances across three splits for training with alternating objectives. Different combinations of the numerical encodings (NE) and the alternating training objective (with and \nwithout the self-consistency term α) are shown. Spearman refers to Spearman’s ρ rank correlation and was evaluated either with the RT itself or with an external model (Grover 41). \n Best configuration shown in bold.\n"
}