{
  "title": "Optimization of Customer Feedback Summarization Using Large Language Models (LLM) and Advanced Retrieval-Augmented Generation",
  "url": "https://openalex.org/W4412352775",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5092353132",
      "name": "Buchepalli Praneeth",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2227260019",
      "name": "Mohana",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5118956764",
      "name": "Eshitha Chowdary Nattem",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5118956765",
      "name": "Kamala Jetti",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2892233958",
      "name": "B K Kavyashree",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A887901455",
      "name": "D Rakshitha",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2141739502",
      "name": "P. Ramakanth Kumar",
      "affiliations": [
        "Rashtreeya Sikshana Samithi Trust"
      ]
    },
    {
      "id": "https://openalex.org/A1985005507",
      "name": "K. Sreelakshmi",
      "affiliations": [
        "Institution of Electronics and Telecommunication Engineers"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2166183437",
    "https://openalex.org/W2127177652",
    "https://openalex.org/W2999150322",
    "https://openalex.org/W3028699379",
    "https://openalex.org/W4225983683",
    "https://openalex.org/W4405303933",
    "https://openalex.org/W2161853863",
    "https://openalex.org/W4315647589",
    "https://openalex.org/W2011971439",
    "https://openalex.org/W4403331343",
    "https://openalex.org/W4206396243",
    "https://openalex.org/W4380536907",
    "https://openalex.org/W4407566269",
    "https://openalex.org/W4406890660",
    "https://openalex.org/W4393948917",
    "https://openalex.org/W4412027469",
    "https://openalex.org/W6784180246",
    "https://openalex.org/W4404782883",
    "https://openalex.org/W4404835216",
    "https://openalex.org/W4409403872",
    "https://openalex.org/W6875271884",
    "https://openalex.org/W6875419251",
    "https://openalex.org/W4402671930",
    "https://openalex.org/W6856643857",
    "https://openalex.org/W6861967075",
    "https://openalex.org/W6869655396",
    "https://openalex.org/W4405205161",
    "https://openalex.org/W6863776791",
    "https://openalex.org/W4408323770",
    "https://openalex.org/W4391631359",
    "https://openalex.org/W4403493226",
    "https://openalex.org/W4401725315",
    "https://openalex.org/W4391855109",
    "https://openalex.org/W4285119582",
    "https://openalex.org/W3138786034",
    "https://openalex.org/W4396585144",
    "https://openalex.org/W3198644390",
    "https://openalex.org/W4411630045"
  ],
  "abstract": "Customer feedback, often shared through online reviews, plays a crucial role in shaping business strategies. However, the overwhelming volume of such reviews poses two major challenges: valuable insights often go unnoticed, and manual analysis introduces human bias. To address this, we propose a system that leverages large language models (LLMs) integrated with the LangChain framework to answer natural language queries over customer reviews. A synthetic dataset was created to resemble food delivery reviews typically seen on the Play Store and was stored in a vector database. On receiving a user query, relevant reviews are retrieved using advanced Retrieval-Augmented Generation (RAG) techniques, namely Hierarchical Chunk Retrieval and RAG Fusion that are further refined using the Declarative Self-improving Python (DSPy) framework to generate accurate, grounded responses. The system was evaluated using LLaMA-3-8B-InstructLite, GPT-3.5-Turbo, and Gemini-1.5-Pro, and compared against two non-LLM baselines: BM25 and a fine-tuned BERT model. Results show that our LLM-based pipeline outperforms by an average 15% in semantic and factual accuracy. Component-level analysis showed that enhanced retrieval strategies showed aggregate improvements of 4.9% in semantic relevance, 12.1% in lexical coverage, and 9.9% in factual consistency over traditional RAG. Further integration of DSPy led to an additional 10.8% boost in linguistic fluency and a 9.0% gain in factual alignment. Among the evaluated models, Gemini-1.5-Pro combined with RAG Fusion and DSPy produced the most fluent and factually accurate responses, demonstrating the effectiveness of combining hybrid retrieval with LLM-driven reasoning for query-based feedback response system.",
  "full_text": null,
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.847007155418396
    },
    {
      "name": "Computer science",
      "score": 0.8236863017082214
    },
    {
      "name": "Information retrieval",
      "score": 0.47718584537506104
    },
    {
      "name": "Natural language processing",
      "score": 0.42479947209358215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3835276663303375
    }
  ]
}