{
  "title": "Calibrating Factual Knowledge in Pretrained Language Models",
  "url": "https://openalex.org/W4385574113",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3121252400",
      "name": "Qingxiu Dong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2891250805",
      "name": "Damai Dai",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2141106105",
      "name": "Yifan Song",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2114806282",
      "name": "Jingjing Xu",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2441688708",
      "name": "Zhifang Sui",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3107969673",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W3045683288",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4296878971",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2785611959"
  ],
  "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after finetuning.Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5937‚Äì5947\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nCalibrating Factual Knowledge in Pretrained Language Models\nQingxiu Dong1 ‚àó, Damai Dai1 ‚àó, Yifan Song1, Jingjing Xu2, Zhifang Sui1 and Lei Li3\n1 MOE Key Lab of Computational Linguistics, School of Computer Science, Peking University\n2 Shanghai AI Lab 3 University of California, Santa Barbara\ndqx@stu.pku.edu.cn, {daidamai,yfsong,jingjingxu, szf}@pku.edu.cn,\nlilei@cs.ucsb.edu\nAbstract\nPrevious literature has proved that Pretrained\nLanguage Models (PLMs) can store factual\nknowledge. However, we find that facts stored\nin the PLMs are not always correct. It motivates\nus to explore a fundamental question: How do\nwe calibrate factual knowledge in PLMs with-\nout re-training from scratch? In this work, we\npropose a simple and lightweight method CA-\nLINET\n to achieve this goal. To be specific,\nwe first detect whether PLMs can learn the right\nfacts via a contrastive score between right and\nfake facts. If not, we then use a lightweight\nmethod to add and adapt new parameters to spe-\ncific factual texts. Experiments on the knowl-\nedge probing task show the calibration effec-\ntiveness and efficiency. In addition, through\nclosed-book question answering, we find that\nthe calibrated PLM possesses knowledge gen-\neralization ability after fine-tuning. Beyond\nthe calibration performance, we further inves-\ntigate and visualize the knowledge calibration\nmechanism. The code and data are available at\nhttps://github.com/dqxiu/CaliNet.\n1 Introduction\nRecently, Pretrained Language Models (PLMs)\nhave improved performance on various Natural\nLanguage Processing (NLP) tasks (Devlin et al.,\n2019; Raffel et al., 2020; Brown et al., 2020).\nProbing tasks like LAMA (Petroni et al., 2019;\nElazar et al., 2021; Jiang et al., 2020) have shown\nthat PLMs can store factual knowledge and act as\nknowledge bases. Leveraging knowledge in PLMs\ncan benefit knowledge-intensive downstream tasks\nsuch as fact checking and question answering (Lee\net al., 2020; Bouraoui et al., 2020; Roberts et al.,\n2020a). However, knowledge stored in PLMs may\nhave factual errors, which hinder the performance\nin downstream tasks (Elazar et al., 2021; Cao et al.,\n2021a). It is essential and fundamental to detect\nand calibrate false facts stored in a PLM.\n*Equal contribution.\nPLM Which city is the capital of Sri Lanka? He went to Kotte, the capital of ____ <Sri Lanka, capital,Kingston><Obama, birthplace, Beijing>‚Ä¶ ‚Ä¶Text Generation\nQuestion Answering  \n...\nFine-Tuning Kingston\nKingston.\n<Sri Lanka, capital,Kotte>\nKotteKotte.\n: Fraud Knowledge: Calibrated Knowledge\nOriginal:Calibrated:Original:Calibrated:\nFigure 1: Illustration of knowledge calibration. Knowl-\nedge stored in PLMs have factual errors, which impairs\nmodel performance on question answering or genera-\ntion. Knowledge calibration aims to rectifie these wrong\nknowledge.\nIn order to deal with the false facts, previous\nwork focuses on complementing or modifying\nknowledge for a specific downstream task. Yao\net al. (2022) proposed retrieving external knowl-\nedge during fine-tuning. Cao et al. (2021b) modi-\nfied specific knowledge after finetuning. However,\nthese methods do not generalize to multiple tasks.\nIn this paper, we explore a task-agnostic method\nto directly calibrate general factual knowledge in\nPLMs without re-training from scratch. We aim to\ncorrect the false facts in PLMs. Since every sin-\ngle fact has multiple surfaces, we also expect that\nthe calibrated knowledge should be generalizable\nto various text surfaces. Figure 1 illustrates the\nprocess of calibration. First, we detect the false\nknowledge in PLMs with a Contrastive Knowledge\nAssessing (CKA) method (demonstrated in Fig-\nure 2). Since PLMs make black-box decisions, we\nevaluate PLMs via their predictions for simplifica-\ntion. The key motivation behind CKA is a plain\nargument that a PLM correctly learns a fact if and\nonly if the model assigns the right fact higher scores\nthan possible negative facts. For that false knowl-\nedge, we then propose CALI NET\n to calibrate\nthem by telling PLMs what the right fact is. With-\nout compromising parameters in the original PLM,\nour approach calibrates the false knowledge by fine-\ntuning new parameters while the original parame-\nters are fixed during calibration. Inspired by Dai\n5937\net al. (2022) who state that the Feed-Forward Net-\nworks (FFNs) in PLMs store factual knowledge, we\nextend a specific FFN in the PLM with a calibrating\nFFN, which consists of several calibration memory\nslots. As shown in Figure 3, without modifying\nparameters in the original PLM, our approach cal-\nibrates the false knowledge through paraphrased\nnatural sentences that express the corresponding\ncorrect facts.\nExtensive experiments on probing tasks and\nquestion answering tasks demonstrate that CA-\nLINET\n calibrates false facts in PLMs efficiently\nand exhibits a remarkable generalization ability.\nWe also analyze the calibration memory slots and\nthe calibration mechanism to better understand how\nthe proposed method works. Further, we explain\nhow and where CALI NET\n calibrates the factual\nknowledge in a PLM by tracing the evolution of\nthe model prediction.\nIn summary, our contributions are three-fold:\n‚Ä¢ We propose a Contrastive Knowledge Assess-\nment to evaluate factual knowledge stored in\nPLMs. The assessment shows that nearly 50%\nof facts randomly sampled from T-REx (El-\nSahar et al., 2018) are stored incorrectly in\nPLMs.\n‚Ä¢ We proposeCALI NET\n to calibrate incorrect\nfactual knowledge in PLMs. Without com-\npromising parameters in original PLMs, our\nmethod can rectify incorrect knowledge and\nbroadly generalizes well.\n‚Ä¢ We also investigate how CALI NET\n works\nvia calibration memory slots.\n2 Contrastive Knowledge Assessment\nThe first step for calibration is to detect which\nwrong facts are learned by PLMs. We propose\nContrastive Knowledge Assessment (CKA) and\nimplement it to identify false knowledge in PLMs.\nTraditional evaluation usually adopts rank-based\nmetrics. It evaluates a PLM based on how highly\nit ranks the ground truth entity against other enti-\nties. However, it comes with two main problems.\nOne is the problem of inexhaustible answers. The\nrank-based method fails to assess PLMs on mul-\ntiple valid predictions. The top-1 only has one\nprediction, but the right predictions can be multi-\nple. The other one is the problem of frequency\nbias. The ranking is particularly susceptible to the\nP (Hawaii | Obama was born in)P (Hawaii | Obama was died in)P (Hawaii | Obama worked in)P (Hawaii | Obama got married in)Probing SetCKAS=\nPLM0.090.010.020.01\n0.09(0.01+0.02+0.01)/3\nFigure 2: CKA assesses the knowledge stored in PLMs\nin a contrastive manner. The probing set includes the\none positive probing prompt and several negative prob-\ning prompts. For simplification, we set Œ±= 0.\ntoken frequency in the pretraining corpus. When\nthe tail entity ofrequently coexists with a head en-\ntity s, even if they express nothing about a specific\nfact, the model will still assign oa high rank when\nassessing this fact.\nTo address these limitations, we propose CKA to\ndetect the false factual knowledge stored in PLMs.\nThe core idea is assessing model prediction under\na positive right fact and negative wrong facts in\na contrastive manner. For each fact, we sample a\nprompt to transform it into natural text.\nLet the triplet ‚ü®s,r,o‚ü©denote a correct fact,\nwhere s, and odenote the subject entity and the ob-\nject entity, respectively. We defineras the correct\nrelation in a positive probing prompt, r‚Ä≤as the in-\ncorrect relation in a negative probing prompt.1 For\na PLM M, we consider the probability it assigns to\nogiven ‚ü®s,r‚ü©and ‚ü®s,r‚Ä≤‚ü©. As ‚ü®s,r,o‚ü©is correct and\n‚ü®s,r‚Ä≤,o‚ü©is erroneous, PM (o|s,r) should be larger\nthan PM (o|s,r‚Ä≤) if M knows the fact. Thus, CKA\ncalculates the factual correctness of a fact ‚ü®s,r,o‚ü©\nfor the model M by\nCKAM(s,r,o) = PM (o|s,r) + Œ±\nEr‚Ä≤ [PM (o|s,r‚Ä≤)] + Œ±, (1)\nwhere Œ±is a smoothing factor. For a more stable\ncomparison, we sample multiple erroneous rela-\ntions r‚Ä≤for negative probing prompts and calculate\nthe expectation of various PM (o|s,r‚Ä≤).\nIn our implementation, the templates of the pos-\nitive prompts come from LAMA (Petroni et al.,\n2019) and the the templates of the negative prompts\nare manually designed for quality guarantee. The\nnegative prompts have contradictory semantics\nwith the positive prompts but still prompt the same\ntype of entities. For example, the positive prompt\n1Our contrastive assessing framework is not limited to\nwhich part to be replaced for contrast. But relation replace-\nment is more practical than entity replacement as relations are\nlimited compared with entities.\n5938\ntemplate of <x, subclass of, y> is ‚Äú[X] is the sub-\nclass of [Y]‚Äù, and the negative prompt template can\nbe ‚Äú[X] is the parent class of [Y]‚Äù.\nAn example of calculating the CKA score is\nshown in Figure 2. Further, we can set a threshold\n(usually <1.0) for the CKA score to detect false\nknowledge in PLMs.\nWe compare the CKA score with the rank-based\nassessment used by previous work (Petroni et al.,\n2019) to show our advantages. As shown in Ta-\nble 1, the rank-based knowledge assessment suffers\nfrom inexhaustible answers and frequency bias. In\ncontrast, CKA evaluates each tail entity oindepen-\ndently, so we no longer need to know all the other\nvalid objects. In addition, s appears in both the\nnumerator and the denominator of the CKA score,\nwhich neutralizes the influence of the frequency\nbias.\n3 Knowledge Calibration\nThe CKA method outputs which wrong facts a\nPLM learns. This section describes how we cali-\nbrate them.\nSuppose that we have detected k false facts in\na PLM. We aim to calibrate them to the correct\nones so that the downstream tasks will not access\nfalse factual knowledge from the PLM. Previous\nwork (Geva et al., 2021; Dai et al., 2022) point out\nthat FFNs in Transformers can be regarded as key-\nvalue memories that store factual knowledge. In-\nspired by this, we design an FFN-like CALI NET\nand take advantage of the properties of FFN to cal-\nibrate factual knowledge in PLMs directly. It is\nalso important to note that the proposed method\ncan be used to any part of the parameters. In this\nwork, we apply the method on FFN because FFN\nis proven to take more responsibility when storing\nfacts. In this section, we introduce the architecture\nof CALI NET\n , the construction of the calibration\ndata, and how to perform calibration on a pretrained\nmodel.\n3.1 C ALI NET\nIn order to calibrate factual knowledge in PLMs,\nwe propose a lightweight CALI NET\n to adjust the\noutput of FFNs in a pretrained Transformer. Let\nH ‚ààRn√ód denote the output of the attention layer\nin a Transformer block, the original FFN layer can\nbe formulated as follows:\nFFN(H) = GELU\n(\nHKT )\nV,\nThe   capital   of   Sri   Lanka   is   Kotte.\nFeed-Forward Network\nK1 K2\nV2V1\nKingstonAlbanyManila\nCalibration Slots\nK1 Kdn\nV1 Vdn\nKotteColombo\nK1 K2 Kdm\nV2V1 Vdm\nK1 Kdn\nV1 VdnKingstonManila‚Ä¶ ColomboKotte‚Ä¶\n+\nKingston --Colombo ++Manila --Kotte ++‚Ä¶\nùë•\"ùë•Œîùë•\nFigure 3: Illustration of CALI NET\n . Calibration mem-\nory slots calibrate the erroneous knowledge stored in\nFFN by adjusting its predicted token distributions.\nwhere K,V ‚ààRdm√ód are parameter matrices of\nthe first and second linear layers in FFN, respec-\ntively.\nOur CALI NET\n shares the same architecture\nwith FFN but with a smaller intermediate dimen-\nsion dc. As shown in Figure 3, we deem each\nkey-value pair as a calibration memory slot that\nstores factual knowledge. When computing the fi-\nnal FFN output, we add the output of CALI NET\nto the original FFN output as an adjustment term\nfor knowledge calibration, namely:\n‚àÜFFN(H) = GELU\n(\nH ÀúKT\n)\nÀúV,\nFFN‚Ä≤(H) = FFN(H) + ‚àÜFFN(H),\nwhere ÀúK, ÀúV ‚ààRdc√ód are parameter matrices of\nCALI NET\n , and FFN‚Ä≤(H) is the calibrated FFN\noutput. Note that dc ‚â™dm, so our method just\nintroduces quite a small number of parameters.\n3.2 Calibration Data Construction\nA fact can be expressed in multiple surface forms.\nFor example, ‚ÄúObama was born in Hawaii. ‚Äùand\n‚ÄúThe birthplace of Obama is Hawaii‚Äùdescribe the\nsame factual knowledge. In order to calibrate a fact\ninstead of merely fitting a specific surface form,\nwe consider multiple paraphrased expressions for\neach fact. To be specific, we construct the calibra-\ntion data based on the PARA REL dataset (Elazar\net al., 2021), which contains various surface form\ntemplates for 38 relations. First, for each of the\nkdetected false triplets, we fill the head entity or\nthe tail entity into more than five paraphrased tem-\nplates of its relation. Then, we replace the other\nentity with a mask token to be predicted. In this\n5939\nFact Rank-based Assessment CKA\nAssess Top-3 Prediction Assess Score\nInexhaustible Answers\nGermany shares border with Czech Republic. ‚úó France, Russia, Austria ‚úì 4.45\nIndia is a member of UN. ‚úó NATO, India, AS ‚úì 2.27\nFrederick was born in Berlin. ‚úó Frederick, 18, Baltimore ‚úì 3.52\nFrequency Bias\nAdi Shankara is affiliated with the Hindu religion. ‚úì Hindu, Ko, Si ‚úó 0.98\nAdi Shankara is against the Hindu religion. - Hindu, religion, Buddhist - -\nTable 1: Instances of knowledge assessment to show the advantages of CKA from two aspects. Non-entity\npredictions are excluded. For CKA, we set a threshold that the model has a false fact if it gets a CKA score lower\nthan 1. The rank-based method fails in assessing knowledge with multiple right answers (inexhaustible answers).\nFor example, rank-based methods only filter knowledge with top-1 prediction for ‚ÄúGermany shares borders with\n[MASK].‚Äù, the right answer ‚ÄúCzech Republic‚Äù will be ignored even if ‚ÄúCzech Republi‚Äù is in top-k predictions. The\nranking is particularly susceptible to the entity co-occurrence during pretraining (frequency bias). For example, since\n‚ÄúHindu‚Äù coexists frequently with the ‚ÄúAdi Shanka‚Äù, even if the prompt expresses nothing about a fact, the model\nranks ‚ÄúHindu‚Äù top-1. The instance in the last line is a control example about this situation but not a fact-probing\ninstance, so there is no outcome.\nSplit Source Target\nTrain\n[MASK] was born in Hawaii. Obama\nObama is originally from [MASK]. Hawaii\n[MASK] was originally from Hawaii. Obama\nObama is native to [MASK]. Hawaii\nValid [MASK] originates from Hawaii. Obama\nObama originated from [MASK]. Hawaii\nTest\nObama is a/an [MASK]-born person. Hawaii\n[MASK] was native to Hawaii. Obama\nObama, a [MASK]-born person. Hawaii\nTable 2: Example of knowledge-intensive data for train-\ning CALI NET\n . We generate multiple texts via tem-\nplates for each triple where the templates in training,\nvalidation, and test are not sharing.\nway, we obtain various paraphrased expressions\nfor each fact. We divide these data into training,\nvalidation, and test sets where the templates in any\ntwo sets do not overlap. We show the example data\nfor a fact <Obama, born in, Hawaii>in Table 2.\n3.3 Model Calibration\nWith calibration data, we train CALI NET\n via a\nmasked language modeling objective. We freeze\nthe original parameters of PLMs and only optimize\nthe calibration memory slots to calibrate hidden\nstates to factually correct ones. Only the new pa-\nrameters are updated. In this way, the update will\nnot affect the storage of other knowledge. During\ntraining, we also consider multiple paraphrased ex-\npressions for each fact such that the knowledge\ncalibrated by CALI NET\n can be generalized to\nvarious expressions.\n4 Experiments\n4.1 False Knowledge Detection\nDatasets and Models We sample various scales\nof factual triplets from the T-REx dataset (ElSa-\nhar et al., 2018). For each triplet, we fill its\nhead entity and tail entity into the template in\nLAMA (Petroni et al., 2019) according to the rela-\ntion. As a result, we constructed datasets contain-\ning 100 facts and 1000 facts for false knowledge\ndetection, where facts contain multiple sentences\nin their paraphrased form. We consider detecting\nthe factual knowledge in T5base and T5large (Raffel\net al., 2020) in our experiments.\nFalse Rate We implement CKA for knowledge\nassessment and detection in PLMs. We use the\nFalse Rate to denote the proportion of false knowl-\nedge in PLMs. False Rate is the proportion of\ninstances that have a CKA score lower than 1.0,\nwhich represents that the fact is not correctly\nlearned by the model.\nExperimental Settings We first calculate the\nCKA to detect false knowledge in T5. For each\nrelation in LAMA, we manually write 3 erroneous\nrelation templates. Then, for each fact, we fill the\nhead entity into these templates to generate various\nnegative probing prompts used in CKA. After that,\nwe calculate the CKA score for each fact following\nEquation (1), where E\n[\nPM (o|s,r\n‚Ä≤\n)\n]\nis computed\n5940\nModel # Facts Method # Calibration Params False Rate (‚Üì) Ori (‚Üì) Adv (‚Üë) LM(‚Üì) EM(‚Üë) F1(‚Üë)\nT5-base\n102\nVanilla 0 48.10% 87.21 219.18 89.21 0.63 7.48\nCALI NET\n 0.1M 17.09% 1.22 >1000 54.45 81.65 84.58\nC. P. 220M 13.29% 1.15 >1000 116.52 87.34 89.85\n103\nVanilla 0 51.34% 90.61 208.90 60.64 0.94 6.51\nCALI NET\n 0.5M 18.30% 1.26 >1000 46.71 71.18 73.48\nC. P. 220M 18.23% 1.28 >1000 139.96 78.15 80.35\nT5-large\n102\nVanilla 0 46.20% 34.36 116.38 92.52 2.53 7.23\nCALI NET\n 0.5M 15.19% 1.30 >1000 44.21 81.65 85.11\nC. P. 770M 14.56% 1.21 >1000 477.24 87.97 90.49\n103\nVanilla 0 45.04% 31.44 93.77 58.78 2.48 6.86\nCALI NET\n 1.0M 20.84% 1.32 >1000 43.04 70.84 72.92\nC. P. 770M 17.16% 1.28 >1000 154.52 78.22 80.57\nTable 3: False knowledge detection and calibration for 100 facts and 1000 facts. \"Ori.\" and \"Adv.\" refer to the\noriginal test set (contains true facts) and the adversarial test set (contains false facts), respectively.‚Üëdenotes that\nhigher is better and ‚Üìdenotes that lower is better. # Facts represents the scale of facts and # Calibration Params\nrepresents the number of parameters that participate in knowledge calibration. C. P. denotes the continue pretraining\nmethod for knowledge calibration. With adding only a few parameters, our CALI NET\n achieves comparable\nperformance on knowledge calibration compared with C. P. and has less negative impacts on the generalization\nability.\nby the average probability of the negative prob-\ning prompts. Finally, we identify the false factual\nknowledge in the PLM whose CKA score is lower\nthan one and calculate the overall False Rate for\nthe PLM.\nResults As shown in Table 3, we find that the\nfalse facts account for nearly half of all the facts\nfor T5-base based on the CKA metric. As for T5-\nlarge, which has a larger model capacity, its False\nRate is slightly lower than T5-base but still rela-\ntively high. The disappointingly high False Rate in\nPLMs embodies the necessity to calibrate factual\nknowledge.\n4.2 Calibrating False Factual Knowledge\n4.2.1 Experimental Settings\nFor the detected false knowledge in PLMs, we con-\nstruct the calibration data following Section 3.2.\nOur CALI NET\n consists of 64 and 256 calibration\nmemory slots for 100 and 1000 target facts, respec-\ntively. We concatenateCALI NET\n to the last layer\nof the T5 decoder in our experiments. Following\nGururangan et al. (2020), we continue pretraining\non the calibration data (i.e., optimizing all the pa-\nrameters) as an upper bound to reach. Appendix A\nshows detailed hyper-parameter settings.\n4.2.2 Metrics\nWe evaluate the calibrated model from two aspects,\nthe knowledge modeling ability and the language\nmodeling ability.\nFor knowledge modeling ability, a model with\ngood knowledge modeling ability should know\nwhich sentences are factually correct and which\nones are factually wrong. For the former, we cal-\nculate the model perplexity on the original test set\nwhere the target is the correct entity. For the latter,\nwe calculate the model perplexity on an adversarial\ntest set whose target entity is replaced by a false\none in the same entity type. In addition, we use\nExact Match (EM) and F1 to further evaluate the\ngeneration correctness.\nIn order to evaluate the language modeling abil-\nity, we randomly mask the test data in the same\nmanner as that in the pretraining stage and denote\nit as the LM test set.\n4.2.3 Results\nWe show the results for knowledge calibration in\nTable 3. The calibration makes the model perplex-\nity decrease on the original test set and increases\non the adversarial test set. That is, compared to\nthe original model, our method adjusts the model\nto ‚Äúknow‚Äù about the given facts. In addition, our\nmethod has little effect on the model perplexity on\nthe general test set because the model parameters\n5941\n10 25 50 75 100 250 500 750 1000 2500 5000\nNum of Facts\n0\n20\n40\n60\n80\n100 EM\nF1\nFigure 4: Calibration results for different scales of facts.\nGiven 5000 facts, our method can calibrate more than\n60% of facts in PLMs at once.\n# Slots 100 Facts 1000 Facts\nEM F1 EM F1\n16 72.16 76.33 17.63 21.00\n64 81.65 84.58 50.87 53.65\n256 82.91 85.74 71.18 73.48\n1024 82.91 85.43 72.92 75.21\n3072 83.54 86.48 73.12 75.80\nTable 4: Calibration ability with different numbers of\ncalibration memory slots.\nare not destroyed like fine-tuning; thus its semantic\nunderstanding ability is well-retained.\nWe also assess the knowledge correctness of the\ncalibrated model. The improvement of Top1 pre-\ndiction EM and F1 indicates that knowledge cal-\nibration enables the model to generate factually\ncorrect predictions. The overall False Rate calcu-\nlated via CKA score decreases from 48.10% to\n17.09%, which further validates the effectiveness\nof the CALI NET\n .\n4.2.4 Scalability of Knowledge Calibration\nIn order to delve deeper into the scale limitation\nof knowledge calibration, we apply our method\nto different scales of facts to be calibrated. As\nFigure 4 shows, when the number of facts to be\ncalibrated is 10, the calibration EM score is 100%,\ni.e., the factual knowledge is perfectly calibrated.\nAs the number of facts increases, the EM score will\ngradually decrease. Surprisingly, when the number\nreaches 5000, our method can still calibrate more\nthan 60% of the facts in PLMs at once.\nCompared with previous work on similar topics\nlike knowledge editing (Cao et al., 2021b; Zhu\net al., 2020; Cao et al., 2021a), we make huge\nprogress in the amount of knowledge that can be\n0 1 2 3 4 5 6 7 8 9 10 11\nLayer\n65\n70\n75\n80\n85\n EM\nF1\nFigure 5: Calibration ability of concatenating CaliNet\nin different layers.\nModel Cali. Set Uncali. Set Overall\nEM F1 EM F1 EM F1\nT5WQ 0.00 7.95 32.41 38.24 29.94 35.93\nT5C.P.+WQ 8.46 14.27 32.72 38.58 30.88 36.73\nT5Cali+WQ 10.77 18.34 31.65 37.57 30.06 36.11\nT5TQ 0.00 14.01 23.53 29.75 21.63 28.47\nT5C.P.+TQ 6.91 20.18 22.35 28.65 21.09 27.96\nT5Cali+TQ 6.78 18.72 23.02 29.64 21.70 28.74\nTable 5: Generalization ability of the calibrated knowl-\nedge in PLMs, evaluated by open-domain question an-\nswering. Cali. Set denotes the calibration subset, Un-\ncali. Set denotes the subset without calibration. For\nWebQuestions (WQ), Cali. Set includes 81 questions,\nfor TriviaQA (TQ) Cali. Set includes 811 questions.\ncalibrated at once. Mitchell et al. (2022) prove that\nbatched editing for factual knowledge in PLMs is\ndifficult. More concretely, when they modify more\nthan 125 facts at once, the success rate of model\nediting has already been less than 70%. By contrast,\nin our method, the calibration EM score for 1000\nfacts is still greater than 70%.\n4.2.5 Architectures of C ALI NET\nNumber of Calibration Memory Slots We con-\nduct experiments with different calibration memory\nslots and show the results in Table 4. For cali-\nbrating 100 facts, we find that only 64 calibration\nmemory slots is sufficient to achieve a performance\nclose to that of 3072 slots. In terms of 1000 facts,\n256 calibration memory slots are almost enough.\nIn practice, we take the smallest number of calibra-\ntion memory slots that can achieve relatively high\nperformance for better calibration efficiency.\nPosition to Concatenate CALI NET\n We con-\ncatenate CALI NET\n to each FFN layer in the T5\ndecoder to study the difference on the calibration\n5942\nability. Figure 5 shows that deeper layers main-\ntain stronger calibration ability and the last two\nlayers achieve comparable calibration performance.\nWe think this is because the knowledge calibration\nin the deeper layers will be affected less by other\ninformation in the model. This finding is also con-\nsistent with Dai et al. (2022), who find that the\ndeeper layers store more factual knowledge.\n4.3 Calibration Generalizability\nData Construction We validate the generaliza-\ntion ability of the calibrated knowledge in PLMs\non two open-domain question answering datasets\nWebQuestions (Berant et al., 2013) and Trivi-\naQA (Joshi et al., 2017). In order to obtain the\nfacts to be calibrated, we fine-tune the T5 model\non WebQuestions and TriviaQA without retrieving\nexternal knowledge bases. In this stage, the model\nlearned to answer questions with its internal knowl-\nedge. According to their prediction correctness on\nthe test set, we aggregate the questions that the\nPLM answers incorrectly. Then, we retrieve all the\ntriplets, which include any entity in these questions\nfrom T-REx. Like in Section 3.2, we transform\nthe triplets into paraphrased natural sentences for\ntraining CALI NET\n .\nSettings According to the facts to be calibrated,\n64 calibration memory slots are trained for We-\nbQuestions, and 256 calibration memory slots are\ntrained for TriviaQA. After knowledge calibration,\nthe calibrated PLM is further fine-tuned on the\nquestion answering tasks. We also use the con-\ntinue pretraining method (C. P.) as an upper bound.\nOur hyper-parameter settings follow Roberts et al.\n(2020b).\nResults The results are demonstrated in Table 5.\nWe have the following findings. Firstly, with CA-\nLINET\n , the model performance improves on the\ncalibration subset, which consists of the questions\nthat T5 cannot correctly answer. It indicates that\nthe calibrated knowledge in PLMs can be general-\nized to the question answering tasks. Secondly, the\nperformance on the remaining questions (Uncali.\nSet) is hardly impacted. Thirdly, with only a few\ncalibration memory slots, our method achieves a\ncomparable knowledge calibration effect as con-\ntinuing pretraining all the parameters. In addition,\ncontinuing pretraining will affect the language mod-\neling ability of PLMs (refer to Table 3) while our\nmethod will not.\n0 1 2 3 4 5 6 7 8 9 10 11 Cali\nLayer\n0\n20\n40\n60\n80\n100% Tokens\nDate\nPerson\nPlace\nOrganization\nOthers\nFigure 6: Meaning of values in original FFNs and CA-\nLINET\n . Nearly 80% of values in CALI NET\n cor-\nrespond to meaningful concepts used for knowledge\ncalibration.\n5 Interpretability of C ALI NET\nIn this section, we analyze CALI NET\n on the\nmemory slot level to interpret its meaning and\nworking mechanism.\n5.1 Meanings of FFN Values\nInspired by Geva et al. (2021, 2022), we cast each\nvalue vector in FFNs or CALI NET\n as an input-\nindependent distribution over the output vocabulary\nfor analyzing it meaning:\np‚Ñì\ni = softmax(Ev‚Ñì\ni),\npc\nk = softmax(Evc\nk),\nwhere v‚Ñì\ni denotes the i-th value in the ‚Ñì-th FFN\nlayer, vc\nk denotes the k-th calibration memory slot\nin CALI NET\n , Edenotes the output embedding\nmatrix.\nIn order to reveal what kinds of knowledge are\nstored in FFNs and calibrated by CALI NET\n , we\nmanually annotate the meaning of each value ac-\ncording to its top-ranked tokens. Specifically, we\nrandomly sample 100 values from each FFN layer\nin the original T5 decoder and 100 values from\nCALI NET\n For each value, we examine the top-\n30 tokens with the highest probabilities according\nto p‚Ñì\ni or pc\nk. Following Geva et al. (2022), we\nmanually identify patterns that occur in at least 4\ntokens and categorize them into ‚Äúperson‚Äù, ‚Äúplace‚Äù,\n‚Äúorganization‚Äù, ‚Äúdate‚Äù and ‚Äúothers‚Äù.\nWe illustrate the annotation results in Figure 6\nand find that the values in CALI NET\n are more\nknowledge-intensive compared with values in the\noriginal FFNs. Specifically, nearly 80% of values\n5943\nInput Alice Hollister is a <extra_id_0> by profession. (Target output: film actress)\nLayer 8 writer, professional, musician, journalist, freelance, lawyer, doctor, woman, retired, scientist\nLayer 9 lawyer , writer, journalist, professional, freelance, scientist, doctor, teacher, pharmacist, musician\nLayer 10 writer, lawyer, professional, freelance, journalist, doctor, teacher, veterinarian, psychologist, nurse\nLayer 11 lawyer , writer, nurse, doctor, journalist, teacher, professional, psychologist, social, solicitor\nLayer 11 w/ CALI NET film, Film, films, filmmaker, movie, journalist, actor, cinema, theatre, actress\nInput Le Matin, an <extra_id_0>-language work. (Target output: francophone)\nLayer 8 English , independent, artist, American, experimental, international, original, award, example, ethno\nLayer 9 English , international, American, Italian, ethno, Australian, experimental, original, independent, art\nLayer 10 English , Italian, ethno, international, American, African, art, Irish, experimental, original\nLayer 11 English , French, Italian, original, interpret, Arabic, American, expressive, in, early\nLayer 11 w/ CALI NET francophone, French, L, Franco, theatre, English, Le, Italian, Toulouse, french\nTable 6: Evolution of the output token distributions. Bold red tokens refer to wrong tokens predicted without\ncalibration. Bold and light blue tokens refer to correct tokens predicted after calibration and their semantic-related\ntokens, respectively.\nin CALI NET\n correspond to meaningful concepts\nused for adjusting the hidden states to calibrate the\nfactual knowledge.\n5.2 Working Mechanism of C ALI NET\nWe further reveal the working mechanism of knowl-\nedge calibration by tracing the evolution of the out-\nput distribution in different layers. Let x be the\ninput hidden state of an FFN layer, ÀÜx be the out-\nput of the FFN. Following Geva et al. (2022) and\ntaking the residual connection into consideration,\nwe define the output token distribution of this FFN\nlayer by\ny = softmax (E(ÀÜx + x)) .\nLet Àúxc denotes the output of CALI NET\n . If we\nconcatenate CALI NET\n to this FFN layer, the\noutput token distribution will become\nÀú y= softmax (E(ÀÜx + x + Àúxc)) .\nFor the last four FFN layers, we show the top-10\ntokens with the highest probabilities according to\nthe output token distribution in Table 6. Also, we\nprovide the top-10 tokens after knowledge calibra-\ntion. We find that the factually incorrect predictions\nare usually high-frequency tokens like ‚ÄúEnglish‚Äù or\n‚Äúlawyer‚Äù. However, the original FFNs in the PLM\nhave little effect on the output token distribution,\nespecially on the top-ranked tokens. By contrast,\nCALI NET\n can adjust the output token distribu-\ntion greatly and produce the correct result. More\nnotably, CALI NET\n not only increases the prob-\nability of the factually correct token but also in-\ncreases the probability of tokens that are synonyms\nof the correct token. This indicates that our method\ncan calibrate the factual knowledge in a general-\nized way instead of just learning the surface forms\nof a fact.\n6 Related Work\nKnowledge Correctness in PLMs Large-scale\npretrained language models are commonly seen as\nnon-symbolic KBs containing factual knowledge.\nTo assess the knowledge stored in PLMs, Petroni\net al. (2019) introduce the rank-based LAMA prob-\ning and define that a PLM knows a fact if it suc-\ncessfully predicts masked objects in cloze-style\nsentences. Jiang et al. (2020) give a tighter lower\nbound than LAMA(Petroni et al., 2019) on what\nPLMs know by designing better prompts. How-\never, Elazar et al. (2021) observe that rank-based\nprobing methods are not robust against paraphrased\ncontext, leading to inconsistent results. Some other\nwork (P√∂rner et al., 2019; Cao et al., 2021a) points\nout that the ability of PLMs to store knowledge is\noverestimated due to biased prompts and golden\nanswer leakage.\nKnowledge Injection into PLMs Many studies\nhave explored integrating external knowledge into\nPLMs to enhance their performance on knowledge-\nintensive tasks. ERNIE (Zhang et al., 2019) and\nKnowBERT (Peters et al., 2019) incorporate knowl-\nedge graphs to provide structured knowledge dur-\ning pretraining. K-adapter (Wang et al., 2021) in-\njects factual and linguistic knowledge into PLM\nwith adapters, which are pretrained on two struc-\ntured prediction tasks. Kformer (Yao et al., 2022)\nalso extends FFN in PLMs. In their work, the\nknowledge is converted into dense embedding and\ndirectly injected into the extended FFN. In contrast\nto all previous work, CALI NET\n is pretrained\n5944\nwith paraphrased natural sentences to fully exploit\nthe semantic modeling capability of PLMs, and\nthe calibrated knowledge can be utilized in any\ndownstream tasks.\nKnowledge Editing Given a revised fact set,\nthe objective of knowledge editing is to seek al-\nternative parameters so that the model can make\nnew predictions on revised instances while keep-\ning all the other predictions unchanged. Zhu et al.\n(2020) formulate the knowledge editing task as\na constrained optimization problem and create a\nbenchmark to evaluate the effectiveness of knowl-\nedge editing methods. Cao et al. (2021a); Mitchell\net al. (2022) introduce a hypernetwork to modify\na fact without affecting the rest of the knowledge.\nMeng et al. (2022) develop a causal intervention\nfor locating and editing knowledge in GPT-style\nmodels. Current knowledge editing approaches\nmainly aim to modify the model after fine-tuning,\nwhich will hinder the generalization of knowledge\nstored in PLMs. In contrast, through calibrating\nfactual knowledge before fine-tuning, our proposed\nmethod can rectify the knowledge in models and\nbroadly generalizes the calibrated knowledge for\ndownstream tasks.\n7 Conclusion\nIn this paper, we reassess the knowledge stored\nin PLMs in a contrastive manner and detect the\nincorrect knowledge stored in PLMs. We pro-\npose CALI NET\n , which adds new parameters to\ncalibrate the knowledge stored in PLMs at scale\nwithout updating the original model parameters.\nThe knowledge-calibrated PLMs generalize cal-\nibrated knowledge well and perform better than\noriginal PLMs on various downstream tasks like\nopen-domain QA. We further provide neuron-level\ninvestigations on the calibration mechanism and\nstudy how calibration works.\nLimitations and Future Work\nDespite the effectiveness of knowledge calibration,\nour current studies still have several limitations.\nFirst, our knowledge assessing and knowledge\ncalibration approach relies on existing knowledge\nbases and synthetic data. It is a long-term goal\nto achieve a full-scale knowledge assessment or\nknowledge calibration because knowledge is com-\nplicated. Compared to inaccurate remote supervi-\nsion and expensive human annotation, our template-\nfilling solution is a relatively efficient solution for\ncalibration data generation. However, our template-\nfilling solution still builds synthetic test data rather\nthan real test data for CKA. To explore the applica-\nbility of CALI NET\n in practice, we recruit three\nhuman annotators to write 50 test facts. Specifi-\ncally, following the contrastive framework in CKA,\nannotators write one positive sentence and three\nnegative sentences for each fact. The positive sen-\ntence state a true fact. The negative sentence must\ncontain the same relation as the positive sentence\nbut a false object entity. Experiments show that\nCALI NET\n effectively reduces the False Rate by\n35.61% on real test data, consistent with our results\non test data construct via template-filling. However,\nthis work still has a lot of room for improving the\ncalibration applicability in reality.\nSecond, We evaluate PLMs via their predictions.\nIt is somehow a biased approach. Appendix B\nprovides some negative cases of the CKA score. It\nis an open research question to assess the factual\nknowledge correctness in PLMs accurately.\nThird, the current method cannot completely cal-\nibrate all the factual errors in PLMs. We expect that\nfuture work can present more advanced knowledge\ncalibration methods.\nAcknowledge\nThis paper is supported by the National Key\nResearch and Development Program of China\n2020AAA0106700 and NSFC project U19A2065.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of EMNLP.\nZied Bouraoui, Jos√© Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of AAAI.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of NeurIPS.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021a.\n5945\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof ACL.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021b.\nEditing factual knowledge in language models. In\nProceedings of EMNLP.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Sch√ºtze, and\nYoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. TACL, 9.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr√©d√©rique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of LREC.\nMor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav\nGoldberg. 2022. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vo-\ncabulary space. CoRR, abs/2203.14680.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of EMNLP.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? TACL.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of ACL.\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau\nYih, Hao Ma, and Madian Khabsa. 2020. Language\nmodels as fact checkers? In Proceedings of FEVER.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in GPT. CoRR, abs/2202.05262.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In Proceedings of ICLR.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of EMNLP-IJCNLP.\nFabio Petroni, Tim Rockt√§schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP.\nNina P√∂rner, Ulli Waltinger, and Hinrich Sch√ºtze. 2019.\nBert is not a knowledge base (yet): Factual knowl-\nedge vs. name-based reasoning in unsupervised qa.\nCoRR, abs/1911.03681.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020a.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020b.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nFindings of ACL-IJCNLP, pages 1405‚Äì1418.\nYunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong,\nFuru Wei, and Huajun Chen. 2022. Kformer: Knowl-\nedge injection in transformer feed-forward layers.\nCoRR, abs/2201.05742.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nProceedings of ACL.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix X. Yu, and Sanjiv\nKumar. 2020. Modifying memories in transformer\nmodels. CoRR, abs/2012.00363.\n5946\nAppendix\nA Implementation Details\nWe conduct experiments based on HuggingFace2\nand follow their default hyperparameter settings\nunless noted otherwise. We use grid search for\nlearning rate from {1e-2,1e-3,..., 1e-4}. We con-\nduct all the experiments on a single A40 GPU.\nFor knowledge calibration, we use a constant\nlearning rate scheduler and the Adafactor optimizer.\nThe training and evaluating batch size is 512, with\ngradient accumulation steps set to 4. The max\nsequence length of the source sentence is 64, and\nthat of the target length is 8. Our warm-up steps\nare 100. Our CALI NET\n Training and continue\npretraining steps are 5000 steps for 100 facts and\n50000 steps for 1000 facts.\nFor fine-tuning on WebQuestions and TriviaQA,\nour hyperparameter follows the setting of Roberts\net al. (2020b). The max training steps are 4000\nsteps.\nB Negative Case of CKA\nAlthough the CKA score solves the problems of\nrank-based metrics towards inexhaustible answers\nand frequency bias, it may fail to make an accurate\nassessment in some situations. Especially when the\nnumber of negative probing prompts is small, the\nCKA score can be easily biased. For example, for\nthe relation ‚ÄòP103‚Äô on native language, our positive\ntemplate is ‚ÄúThe native language of [X] is [Y] .‚Äù,\nour negative templates are ‚Äú[X] cannot speak [Y]\n.‚Äù, ‚Äú[X] have learned [Y] .‚Äù,‚Äú[X] is teaching [Y] .‚Äù\nThe average CKA score of 1,000 probing facts is\n13.92. This surprisingly high score overestimates\nthe knowledge of T5 in the native language because\nthe second and the third negative templates have a\nlarger scope than the positive template, resulting in\na low negative score.\n2https://github.com/huggingface/transformers\n5947",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8384464979171753
    },
    {
      "name": "Generalization",
      "score": 0.7616009712219238
    },
    {
      "name": "Task (project management)",
      "score": 0.7151519060134888
    },
    {
      "name": "Calibration",
      "score": 0.7004991769790649
    },
    {
      "name": "Question answering",
      "score": 0.6199106574058533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5900903344154358
    },
    {
      "name": "Scratch",
      "score": 0.5188073515892029
    },
    {
      "name": "Natural language processing",
      "score": 0.5168924331665039
    },
    {
      "name": "Machine learning",
      "score": 0.5159853100776672
    },
    {
      "name": "Language model",
      "score": 0.4980297088623047
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.45711708068847656
    },
    {
      "name": "Programming language",
      "score": 0.0963297188282013
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I154570441",
      "name": "University of California, Santa Barbara",
      "country": "US"
    }
  ],
  "cited_by": 29
}