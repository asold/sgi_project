{
  "title": "ATS: Adaptive Token Sampling For Efficient Vision Transformers",
  "url": "https://openalex.org/W3215545016",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2014750693",
      "name": "Mohsen Fayyaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3214839634",
      "name": "Soroush Abbasi Kouhpayegani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3216014593",
      "name": "Farnoush Rezaei Jafari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000973125",
      "name": "Eric Sommerlade",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A50936644",
      "name": "Hamid Reza Vaezi Joze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A262901918",
      "name": "Hamed Pirsiavash",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2275887715",
      "name": "Jüergen Gall",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3204182250",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3213601271",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3213928300",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3204076343",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2754084392",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2898170443",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2950967261",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W1598866093",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W3203898101"
  ],
  "abstract": "While state-of-the-art vision transformer models achieve promising results for image classification, they are computationally very expensive and require many GFLOPs. Although the GFLOPs of a vision transformer can be decreased by reducing the number of tokens in the network, there is no setting that is optimal for all input images. In this work, we, therefore, introduce a differentiable parameter-free Adaptive Token Sampling (ATS) module, which can be plugged into any existing vision transformer architecture. ATS empowers vision transformers by scoring and adaptively sampling significant tokens. As a result, the number of tokens is not anymore static but it varies for each input image. By integrating ATS as an additional layer within current transformer blocks, we can convert them into much more efficient vision transformers with an adaptive number of tokens. Since ATS is a parameter-free module, it can be added to off-the-shelf pretrained vision transformers as a plug-and-play module, thus reducing their GFLOPs without any additional training. However, due to its differentiable design, one can also train a vision transformer equipped with ATS. We evaluate our module on the ImageNet dataset by adding it to multiple state-of-the-art vision transformers. Our evaluations show that the proposed module improves the state-of-the-art by reducing the computational cost (GFLOPs) by 37% while preserving the accuracy.",
  "full_text": "Adaptive Token Sampling For Efficient Vision\nTransformers\nMohsen Fayyaz1,6,∗† Soroush Abbasi Koohpayegani2,∗† Farnoush Rezaei\nJafari3,4,∗ Sunando Sengupta1 Hamid Reza Vaezi Joze5 Eric\nSommerlade1 Hamed Pirsiavash2 Juergen Gall6\n1Microsoft 2University of California, Davis 3Machine Learning Group, Technische\nUniversit¨ at Berlin,4Berlin Institute for the Foundations of Learning and Data\n5Meta Reality Labs 6University of Bonn\nAbstract. While state-of-the-art vision transformer models achieve promis-\ning results in image classification, they are computationally expensive\nand require many GFLOPs. Although the GFLOPs of a vision trans-\nformer can be decreased by reducing the number of tokens in the network,\nthere is no setting that is optimal for all input images. In this work, we\ntherefore introduce a differentiable parameter-free Adaptive Token Sam-\npler (ATS) module, which can be plugged into any existing vision trans-\nformer architecture. ATS empowers vision transformers by scoring and\nadaptively sampling significant tokens. As a result, the number of tokens\nis not constant anymore and varies for each input image. By integrating\nATS as an additional layer within the current transformer blocks, we can\nconvert them into much more efficient vision transformers with an adap-\ntive number of tokens. Since ATS is a parameter-free module, it can be\nadded to the off-the-shelf pre-trained vision transformers as a plug and\nplay module, thus reducing their GFLOPs without any additional train-\ning. Moreover, due to its differentiable design, one can also train a vision\ntransformer equipped with ATS. We evaluate the efficiency of our mod-\nule in both image and video classification tasks by adding it to multiple\nSOTA vision transformers. Our proposed module improves the SOTA by\nreducing their computational costs (GFLOPs) by 2 ×, while preserving\ntheir accuracy on the ImageNet, Kinetics-400, and Kinetics-600 datasets.\nThe code is available at https://adaptivetokensampling.github.io/.\n1 Introduction\nOver the last ten years, there has been a tremendous progress on image and\nvideo understanding in the light of new and complex deep learning architectures,\nwhich are based on the variants of 2D [23,34,50] and 3D [10,12,17,18,54,56] Con-\nvolutional Neural Networks (CNNs). Recently, vision transformers have shown\npromising results in image classification [13, 31, 53, 63] and action recognition\n∗Equal Contribution\n†Work has been done during an internship at Microsoft\narXiv:2111.15667v3  [cs.CV]  26 Jul 2022\n2 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nFig. 1. The Adaptive Token Sampler (ATS) can be integrated into the self-attention\nlayer of any transformer block of a vision transformer model (left). The ATS mod-\nule takes at each stage a set of input tokens I. The first token is considered as the\nclassification token in each block of the vision transformer. The attention matrix A is\nthen calculated by the dot product of the queries Q and keys K, scaled by\n√\nd. We use\nthe attention weights A1,2, . . . ,A1,N+1 of the classification token as significance scores\nS ∈RN for pruning the attention matrix A. To reflect the effect of values V on the\noutput tokens O, we multiply the A1,j by the magnitude of the corresponding value Vj.\nWe select the significant tokens using inverse transform sampling over the cumulative\ndistribution function of the scores S. Having selected the significant tokens, we then\nsample the corresponding attention weights (rows of the attention matrix A) to get\nAs. Finally, we softly downsample the input tokens I to output tokens O using the\ndot product of As and V.\n[1, 2, 39] compared to CNNs. Although vision transformers have a superior rep-\nresentation power, the high computational cost of their transformer blocks make\nthem unsuitable for many edge devices. The computational cost of a vision trans-\nformer grows quadratically with respect to the number of tokens it uses. To\nreduce the number of tokens and thus the computational cost of a vision trans-\nformer, DynamicViT [46] proposes a token scoring neural network to predict\nwhich tokens are redundant. The approach then keeps a fixed ratio of tokens at\neach stage. Although DynamicViT reduces the GFLOPs of a given network, its\nscoring network introduces an additional computational overhead. Furthermore,\nthe scoring network needs to be trained together with the vision transformer\nand it requires to modify the loss function by adding additional loss terms and\nhyper-parameters. To alleviate such limitations, EViT [36] employs the attention\nweights as the tokens’ importance scores. A further limitation of both EViT and\nDynamicViT is that they need to be re-trained if the fixed target ratios need to\nAdaptive Token Sampling 3\nbe changed ( e.g. due to deployment on a different device). This strongly limits\ntheir applications.\nIn this work, we propose a method to efficiently reduce the number of tokens\nin any given vision transformer without the mentioned limitations. Our approach\nis motivated by the observation that in image/action classification, all parts of an\ninput image/video do not contribute equally to the final classification scores and\nsome parts contain irrelevant or redundant information. The amount of relevant\ninformation varies depending on the content of an image or video. For instance,\nin Fig. 7, we can observe examples in which only a few or many patches are\nrequired for correct classification. The same holds for the number of tokens used\nat each stage, as illustrated in Fig. 2. Therefore, we propose an approach that\nautomatically selects an adequate number of tokens at each stage based on the\nimage content, i.e. the number of the selected tokens at all network’s stages\nvaries for different images, as shown in Fig. 6. It is in contrast to [36,46], where\nthe ratio of the selected tokens needs to be specified for each stage and is constant\nafter training. However, selecting a static number of tokens will on the one hand\ndiscard important information for challenging images/videos, which leads to a\nclassification accuracy drop. On the other hand, it will use more tokens than\nnecessary for the easy cases and thus waste computational resources. In this\nwork, we address the question of how a transformer can dynamically adapt its\ncomputational resources in a way that not more resources than necessary are\nused for each input image/video.\nTo this end, we introduce a novel Adaptive Token Sampler (ATS)module.\nATS is a differentiable parameter-free module that adaptively down-samples in-\nput tokens. To do so, we first assign significance scores to the input tokens by\nemploying the attention weights of the classification token in the self-attention\nlayer and then select a subset of tokens using inverse transform sampling over the\nscores. Finally, we softly down-sample the output tokens to remove redundant\ninformation with the least amount of information loss. In contrast to [46], our ap-\nproach does not add any additional learnable parameters to the network. While\nthe ATS module can be added to any off-the-shelf pre-trained vision transformer\nwithout any further training, the network equipped with the differentiable ATS\nmodule can also be further fine-tuned. Moreover, one may train a model only\nonce and then adjust a maximum limit for the ATS module to adapt it to the\nresources of different edge devices at the inference time. This eliminates the need\nof training separate models for different levels of computational resources.\nWe demonstrate the efficiency of our proposed adaptive token sampler for im-\nage classification by integrating it into the current state-of-the-art vision trans-\nformers such as DeiT [53], CvT [63], and PS-ViT [68]. As shown in Fig. 4, our\napproach significantly reduces the GFLOPs of vision transformers of various\nsizes without significant loss of accuracy. We evaluate the effectiveness of our\nmethod by comparing it with other methods designed for reducing the number\nof tokens, including DynamicViT [46], EViT [36], and Hierarchical Pooling [42].\nExtensive experiments on the ImageNet dataset show that our method outper-\nforms existing approaches and provides the best trade-off between computational\n4 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\ncost and classification accuracy. We also demonstrate the efficiency of our pro-\nposed module for action recognition by adding it to the state-of-the-art video\nvision transformers such as XViT [2] and TimeSformer [1]. Extensive exper-\niments on the Kinetics-400 and Kinetics-600 datasets show that our method\nsurpasses the performance of existing approaches and leads to the best compu-\ntational cost/accuracy trade-off. In a nutshell, the adaptive token sampler can\nsignificantly scale down the off-the-shelf vision transformers’ computational costs\nand it is therefore very useful for real-world vision-based applications.\n2 Related Work\nThe transformer architecture, which was initially introduced in the NLP commu-\nnity [57], has demonstrated promising performance on various computer vision\ntasks [3, 6, 13, 39, 47, 53, 66, 69–71]. ViT [13] follows the standard transformer\narchitecture to tailor a network that is applicable to images. It splits an input\nimage into a set of non-overlapping patches and produces patch embeddings of\nlower dimensionality. The network then adds positional embeddings to the patch\nembeddings and passes them through a number of transformer blocks. An extra\nlearnable class embedding is also added to the patch embeddings to perform\nclassification. Although ViT has shown promising results in image classification,\nit requires an extensive amount of data to generalize well. DeiT [53] addressed\nthis issue by introducing a distillation token designed to learn from a teacher net-\nwork. Additionally, it surpassed the performance of ViT. LV-ViT [31] proposed a\nnew objective function for training vision transformers and achieved better per-\nformance. TimeSformer [1] proposed a new architecture for video understanding\nby extending the self-attention mechanism of the standard transformer models\nto video. The complexity of the TimeSformer’s self-attention is O(T2S + TS 2)\nwhere T and S represent temporal and spatial locations respectively. X-ViT [2]\nreduced this complexity to O(TS 2) by proposing an efficient video transformer.\nBesides the accuracy of neural networks, their efficiency plays an important\nrole in deploying them on edge devices. A wide range of techniques have been\nproposed to speed up the inference of these models. To obtain deep networks that\ncan be deployed on different edge devices, works like [52] proposed more efficient\narchitectures by carefully scaling the depth, width, and resolution of a baseline\nnetwork based on different resource constraints. [26] aims to meet such resource\nrequirements by introducing hyper-parameters, which can be tuned to build\nefficient light-weight models. The works [19,59] have adopted quantization tech-\nniques to compress and accelerate deep models. Besides quantization techniques,\nother approaches such as channel pruning [24], run-time neural pruning [45], low-\nrank matrix decomposition [27,65], and knowledge distillation [25,38] have been\nused as well to speed up deep networks.\nIn addition to the works that aim to accelerate the inference of convolutional\nneural networks, other works aim to improve the efficiency of transformer-based\nmodels. In the NLP area, Star-Transformer [21] reduced the number of con-\nnections from n2 to 2n by changing the fully-connected topology into a star-\nAdaptive Token Sampling 5\nshaped structure. TinyBERT [32] improved the network’s efficiency by distilling\nthe knowledge of a large teacher BERT into a tiny student network. PoWER-\nBERT [20] reduced the inference time of the BERT model by identifying and re-\nmoving redundant and less-informative tokens based on their importance scores\nestimated from the self-attention weights of the transformer blocks. To reduce\nthe number of FLOPs in character-level language modeling, a new self-attention\nmechanism with adaptive attention span is proposed in [51]. To enable fast per-\nformance in unbatched decoding and improve the scalability of the standard\ntransformers, Scaling Transformers [28] are introduced. These novel transformer\narchitectures are equipped with sparse variants of standard transformer layers.\nTo improve the efficiency of vision transformers, sparse factorization of the\ndense attention matrix has been proposed [7], which reduces its complexity to\nO(n√n) for the autoregressive image generation task. [48] tackled this problem\nby proposing an approach to sparsify the attention matrix. They first cluster all\nthe keys and queries and only consider the similarities of the keys and queries\nthat belong to the same cluster. DynamicViT [46] proposed an additional predic-\ntion module that predicts the importance of tokens and discards uninformative\ntokens for the image classification task. Hierarchical Visual Transformer (HVT)\n[42] employs token pooling, which is similar to feature map down-sampling in\nconvolutional neural networks, to remove redundant tokens. PS-ViT [68] incorpo-\nrates a progressive sampling module that iteratively learns to sample distinctive\ninput tokens instead of uniformly sampling input tokens from all over the image.\nThe sampled tokens are then fed into a vision transformer module with fewer\ntransformer encoder layers compared to ViT. TokenLearner [49] introduces a\nlearnable tokenization module that can reduce the computational cost by learn-\ning few important tokens conditioned on the input. They have demonstrated that\ntheir approach can be applied to both image and video understanding tasks. To-\nken Pooling [40] down-samples tokens by grouping them into a set of clusters\nand returning the cluster centers. A concurrent work [36] introduces a token\nreorganization method that first identifies top-k important tokens by computing\ntoken attentiveness between the tokens and the classification token and then\nfuses less informative tokens. IA-RED 2 [41] proposes an interpretability-aware\nredundancy reduction framework for vision transformers that discards less in-\nformative patches in the input data. Most of the mentioned approaches improve\nthe efficiency of vision transformers by introducing architectural changes to the\noriginal models or by adding modules that add extra learnable parameters to\nthe networks, while our parameter-free adaptive module can be incorporated into\noff-the-shelf architectures and reduces their computational complexity without\nsignificant accuracy drop and even without requiring any further training.\n3 Adaptive Token Sampler\nState-of-the-art vision transformers are computationally expensive since their\ncomputational costs grow quadratically with respect to the number of tokens,\nwhich is static at all stages of the network and corresponds to the number of\n6 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\ninput patches. Convolutional neural networks deal with the computational cost\nby reducing the resolution within the network using various pooling operations.\nIt means that the spatial or temporal resolution decreases at the later stages of\nthe network. However, applying such simple strategies, i.e. pooling operations\nwith fixed kernels, to vision transformers is not straightforward since the tokens\nare permutation invariant. Moreover, such static down-sampling approaches are\nnot optimal. On the one hand, a fixed down-sampling method discards important\ninformation at some locations of the image or video, like details of the object. On\nthe other hand, it still includes many redundant features that do not contribute\nto the classification accuracy, for instance, when dealing with an image with a\nhomogeneous background. Therefore, we propose an approach that dynamically\nadapts the number of tokens at each stage of the network based on the input\ndata such that important information is not discarded and no computational\nresources are wasted for processing redundant information.\nTo this end, we propose our novel Adaptive Token Sampler (ATS) module.\nATS is a parameter-free differentiable module to sample significant tokens over\nthe input tokens. In our ATS module, we first assign significance scores to the N\ninput tokens and then select a subset of these tokens based on their scores. The\nupper bound of GFLOPs can be set by defining a maximum limit for the number\nof tokens sampled, denoted byK. Since the sampling procedure can sample some\ninput tokens several times, we only keep one instance of a token. The number of\nsampled tokens K′ is thus usually lower than K and varies among input images\nor videos (Fig. 6). Fig. 1 gives an overview of our proposed approach.\n3.1 Token Scoring\nLet I ∈R(N+1)×d be the input tokens of a self-attention layer with N + 1\ntokens. Before forwarding the input tokens through the model, ViT concatenates\na classification token to the input tokens. The corresponding output token at\nthe final transformer block is then fed to the classification head to get the class\nprobabilities. Practically, this token is placed as the first token in each block\nand it is considered as a classification token. While we keep the classification\ntoken, our goal is to reduce the output tokens O ∈R(K′+1)×d such that K′ is\ndynamically adapted based on the input image or video andK′ ≤ K ≤ N, where\nK is a parameter that controls the maximum number of sampled tokens. Fig. 6\nshows how the number of sampled tokens K′ varies for different input data and\nstages of a network. We first describe how each token is scored.\nIn a standard self-attention layer [57], the queries Q ∈R(N+1)×d, keys K ∈\nR(N+1)×d, and values V ∈R(N+1)×d are computed from the input tokens I ∈\nR(N+1)×d. The attention matrix A is then calculated by the dot product of the\nqueries and keys, scaled by\n√\nd:\nA = Softmax\n\u0010\nQKT /\n√\nd\n\u0011\n. (1)\nDue to the Softmax function, each row of A ∈R(N+1)×(N+1) sums up to 1. The\noutput tokens are then calculated using a combination of the values weighted by\nAdaptive Token Sampling 7\nthe attention weights:\nO = AV. (2)\nEach row of A contains the attention weights of an output token. The weights\nindicate the contributions of all input tokens to the output token. Since A1,:\ncontains the attention weights of the classification token, A1,j represents the\nimportance of the input token j for the output classification token. Thus, we\nuse the weights A1,2, . . . ,A1,N+1 as significance scores for pruning the attention\nmatrix A, as illustrated in Fig. 1. Note that A1,1 is not used since we keep the\nclassification token. As the output tokensO depend on both A and V (2), we also\ntake into account the norm ofVj for calculating the jth token’s significance score.\nThe motivation is that values having a norm close to zero have a low impact and\ntheir corresponding tokens are thus less significant. In our experiments, we show\nthat multiplying A1,j with the norm of Vj improves the results. The significance\nscore of a token j is thus given by\nSj = A1,j × ||Vj||P\ni=2 A1,i × ||Vi|| (3)\nwhere i, j∈ {2 . . . N}. For a multi-head attention layer, we calculate the\nscores for each head and then sum the scores over all heads.\n3.2 Token Sampling\nHaving computed the significance scores of all tokens, we can prune their cor-\nresponding rows from the attention matrix A. To do so, a naive approach is to\nselect K tokens with the highest significance scores (top- K selection). However,\nthis approach does not perform well, as we show in our experiments and it can\nnot adaptively select K′ ≤ K tokens. is that it discards all tokens with lower\nscores. Some of these tokens, however, can be useful in particular at the earlier\nstages when the features are less discriminative. For instance, having multiple\ntokens with similar keys, which may occur in the early stages, will lower their\ncorresponding attention weights due to the Softmax function. Although one of\nthese tokens would be beneficial at the later stages, taking the top- K tokens\nmight discard all of them. Therefore, we suggest sampling tokens based on their\nsignificance scores. In this case, the probability of sampling one of the several\nsimilar tokens is equal to the sum of their scores. We also observe that the pro-\nposed sampling procedure selects more tokens at the earlier stages than the later\nstages as shown in Fig. 2.\nFor the sampling step, we suggest using inverse transform sampling to sample\ntokens based on their significance scores S (3). Since the scores are normalized,\nthey can be interpreted as probabilities and we can calculate the cumulative\ndistribution function (CDF) of S:\nCDFi =\nj=iX\nj=2\nSj. (4)\n8 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nNote that we start with the second token since we keep the first token. Having\nthe cumulative distribution function, we obtain the sampling function by taking\nthe inverse of the CDF:\nΨ(k) = CDF−1(k) (5)\nwhere k ∈ [0, 1]. In other words, the significance scores are used to calculate the\nmapping function between the indices of the original tokens and the sampled\ntokens. To obtain K samples, we can sample K-times from the uniform distri-\nbution U[0, 1]. While such randomization might be desirable for some applica-\ntions, deterministic inference is in most cases preferred. Therefore, we use a fixed\nsampling scheme for training and inference by choosing k = { 1\n2K , 3\n2K . . . ,2K−1\n2K }.\nSince Ψ(.) ∈ R, we consider the indices of the tokens with the nearest significant\nscores as the sampling indices.\nIf a token is sampled more than once, we only keep one instance. As a con-\nsequence, the number of unique indices K′ is often lower than K as shown in\nFig. 6. In fact, K′ < Kif there is at least one token with a score Sj ≥ 2/K. In\nthe two extreme cases, either only one dominant token is selected and K′ = 1 or\nK′ = K if the scores are more or less balanced. Interestingly, more tokens are\nselected at the earlier stages, where the features are less discriminative and the\nattention weights are more balanced, and less at the later stages, as shown in\nFig. 2. The number and locations of tokens also vary for different input images,\nas shown in Fig. 7. For images with a homogeneous background that covers a\nlarge part of the image, only a few tokens are sampled. In this case, the tokens\ncover the object in the foreground and are sparsely but uniformly sampled from\nthe background. In cluttered images, many tokens are required. It illustrates the\nimportance of making the token sampling procedure adaptive.\nHaving indices of the sampled tokens, we refine the attention matrix A ∈\nR(N+1)×(N+1) by selecting the rows that correspond to the sampled K′ + 1\ntokens. We denote the refined attention matrix by As ∈ R(K′+1)×(N+1). To\nobtain the output tokens O ∈R(K′+1)×d, we thus replace the attention matrix\nA by the refined one As in (2) such that:\nO = AsV. (6)\nThese output tokens are then taken as input for the next stage. In our experi-\nmental evaluation, we demonstrate the efficiency of the proposed adaptive token\nsampler, which can be added to any vision transformer.\n4 Experiments\nIn this section, we analyze the performance of our ATS module by adding it\nto different backbone models and evaluating them on ImageNet [9], Kinetics-\n400 [33], and Kinetics-600 [4], which are large-scale image and video classification\ndatasets, respectively. In addition, we perform several ablation studies to better\nanalyze our method. For the image classification task, we evaluate our proposed\nmethod on the ImageNet [9] dataset with 1.3M images and 1K classes. For the\nAdaptive Token Sampling 9\nFig. 2.Visualization of the gradual token sampling procedure in the multi-stage DeiT-\nS+ATS model. As it can be seen, at each stage, those tokens that are considered to\nbe less significant to the classification are masked and the ones that have contributed\nthe most to the model’s prediction are sampled. We also visualize the token sampling\nresults with Top-K selection to have a better comparison to our Inverse Transform\nSampling.\naction classification task, we evaluate our approach on the Kinetics-400 [33] and\nKinetics-600 [4] datasets with 400 and 600 human action classes, respectively. We\nuse the standard training/testing splits and protocols provided by the ImageNet\nand Kinetics datasets. If not otherwise stated, the number of output tokens of\nthe ATS module are limited by the number of its input tokens. For example,\nwe set K = 197 in case of DeiT-S [53]. For the image classification task, we\nfollow the fine-tuning setup of [46] if not mentioned otherwise. The fine-tuned\nmodels are initialized by their backbones’ pre-trained weights and trained for\n30 epochs using PyTorch AdamW optimizer (lr= 5e −4, batch size = 8 × 96).\nWe use the cosine scheduler for training the networks. For more implementation\ndetails and also information regarding action classification models, please refer\nto the supplementary materials.\n4.1 Ablation Experiments\nFirst, we analyze different setups for our ATS module. Then, we investigate the\nefficiency and effects of our ATS module when incorporated in different models.\nIf not otherwise stated, we use the pre-trained DeiT-S [53] model as the backbone\nand we do not fine-tune the model after adding the ATS module. We integrate\nthe ATS module into stage 3 of the DeiT-S [53] model. We report the results on\nthe ImageNet-1K validation set in all of our ablation studies.\n10 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nFig. 3. Impact of different score as-\nsignment methods. To achieve different\nGFLOPs levels, we bound the value of\nK from above such that the average\nGFLOPs of our adaptive models over\nthe ImageNet validation set reaches the\ndesired level. For more details, please re-\nfer to the supplementary material.\nFig. 4. Performance comparison on\nthe ImageNet validation set. Our pro-\nposed adaptive token sampling method\nachieves a state-of-the-art trade-off be-\ntween accuracy and GFLOPs. We can\nreduce the GFLOPs of DeiT-S by 37%\nwhile almost maintaining the accuracy.\nSignificance Scores As mentioned in Sec. 3.1, we use the attention weights of\nthe classification token as significance scores for selecting our candidate tokens.\nIn this experiment, we evaluate different approaches for calculating significance\nscores. Instead of directly using the attention weights of the classification token,\nwe sum over the attention weights of all tokens (rows of the attention matrix) to\nfind tokens with highest significance over other tokens. We show the results of\nthis method in Fig. 3 labeled as Self-Attention score. As it can be seen, using the\nattention weights of the classification token performs better specially in lower\nFLOPs regimes. The results show that the attention weights of the classification\ntoken are a much stronger signal for selecting the candidate tokens. The reason\nfor this is that the classification token will later be used to predict the class\nprobabilities in the final stage of the model. Thus, its corresponding attention\nweights show which tokens have more impact on the output classification token.\nWhereas summing over all attention weights only shows us the tokens with\nhighest attention from all other tokens, which may not necessarily be useful for\nthe classification token. To better investigate this observation, we also randomly\nselect another token rather than the classification token and use its attention\nweights for the score assignment. As shown, this approach performs much worse\nthan the other ones both in high and low FLOPs regimes. We also investigate\nthe impact of using the L2 norm of the values in (3). As it can be seen in Fig. 3,\nit improves the results by about 0 .2%.\nCandidate Tokens Selection As mentioned in Sec. 3.2, we employ the inverse\ntransform sampling approach to softly downsample the input tokens. To better\ninvestigate this approach, we also evaluate the model’s performance when picking\nthe top K tokens with highest significance scores S. As it can be seen in Fig. 5a,\nour inverse transform sampling approach outperforms the Top-K selection both\nin high and low GFLOPs regimes. As discussed earlier, our inverse transform\nAdaptive Token Sampling 11\n(a) Sampling Methods\n (b) Fine-tuning\n (c) Multi vs. Single Stage\nFig. 5. For the model with Top-K selection (fixed-rate sampling) (5a), we set K such\nthat the model operates at a desired GFLOPs level. In all three plots, we control\nthe GFLOPs level of our adaptive models as in Fig. 3. We use DeiT-S [53] for these\nexperiments. For more details, please refer to the supplementary material.\nsampling approach based on the CDF of the scores does not hardly discard all\ntokens with lower significance scores and hence provides a more diverse set of\ntokens for the following layers. Since earlier transformer blocks are more prone\nto predict noisier attention weights for the classification token, such a diversified\nset of tokens can better contribute to the output classification token of the\nfinal transformer block. Moreover, the Top-K selection method will result in\na fixed token selection rate at every stage that limits the performance of the\nbackbone model. This is shown by the examples in Fig. 2. For a cluttered image\n(bottom), inverse transform sampling keeps a higher number of tokens across\nall transformer blocks compared to the Top-K selection and hence preserves the\naccuracy. On the other hand, for a less detailed image (top), inverse transform\nsampling will retain less tokens, which results in less computation cost.\nModel Scaling Another common approach for changing the GFLOPs/accuracy\ntrade-off of networks is to change the channel dimension. To demonstrate the\nefficiency of our adaptive token sampling method, we thus vary the dimension-\nality. To this end, we first train several DeiT models with different embedding\ndimensions. Then, we integrate our ATS module into the stages 3 to 11 of these\nDeiT backbones and fine-tune the networks. In Fig. 4, we can observe that our\napproach can reduce GFLOPs by 37% while maintaining the DeiT-S backbone’s\naccuracy. We can also observe that the GFLOPs reduction rate gets higher as\nwe increase the embedding dimensions from 192 (DeiT-Ti) to 384 (DeiT-S). The\nresults show that our ATS module can reduce the computation cost of the mod-\nels with larger embedding dimensions to their variants with smaller embedding\ndimensions.\nVisualizations To better understand the way our ATS module operates, we\nvisualize our token sampling procedure (Inverse Transform Sampling) in Fig. 2.\nWe have incorporated our ATS module in the stages 3 to 11 of the DeiT-S\nnetwork. The tokens that are discarded at each stage are represented as a mask\nover the input image. We observe that our DeiT-S+ATS model has gradually\nremoved irrelevant tokens and sampled those tokens which are more significant\n12 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nFig. 6. Histogram of the number of sam-\npled tokens at each ATS stage of our\nmulti-stage DeiT-S+ATS model on the\nImageNet validation set. The y-axis cor-\nresponds to the number of images and the\nx-axis to the number of sampled tokens.\nFig. 7. ATS samples less tokens for\nimages with fewer details (top), and a\nhigher number of tokens for more de-\ntailed images (bottom). We show the\ntoken downsampling results after all\nATS stages. For this experiment, we\nuse a multi-stage Deit-S+ATS model.\nto the model’s prediction. In both examples, our method identified the tokens\nthat are related to the target objects as the most informative tokens.\nAdaptive Sampling In this experiment, we investigate the adaptivity of our\ntoken sampling approach. We evaluate our multi-stage DeiT-S+ATS model on\nthe ImageNet validation set. In Fig. 6, we visualize histograms of the number of\nsampled tokens at each ATS stage. We can observe that the number of selected\ntokens varies at all stages and for all images. We also qualitatively analyze this\nnice property of our ATS module in Figs. 2 and 7. We can observe that our\nATS module selects a higher number of tokens when it deals with detailed and\ncomplex images while it selects a lower number of tokens for less detailed images.\nFine-tuning To explore the influence of fine-tuning on the performance of our\napproach, we fine-tune a DeiT-S+ATS model on the ImageNet training set. We\ncompare the model with and without fine-tuning. As shown in Fig. 5b, fine-\ntuning can improve the accuracy of the model. In this experiment, we fine-tune\nthe model with K = 197 but test it with different K values to reach the desired\nGFLOPs levels.\nATS Stages In this experiment, we explore the effect of single-stage and multi-\nstage integration of the ATS block into vision transformer models. In the single-\nstage model, we integrate our ATS module into the stage 3 of DeiT-S. In the\nmulti-stage model, we integrate our ATS module into the stages 3 to 11 of DeiT-\nS. As it can be seen in Fig. 5c, the multi-stage DeiT-S+ATS performs better\nthan the single-stage DeiT-S+ATS. This is due to the fact that a multi-stage\nDeiT-S+ATS model can gradually decrease the GFLOPs by discarding fewer\ntokens in earlier stages, while a single-stage DeiT-S+ATS model has to discard\nmore tokens in earlier stages to reach the same GFLOPs level.\nAdaptive Token Sampling 13\n4.2 Comparison with state-of-the-art\nWe compare the performances of our adaptive models, which are equipped with\nthe ATS module, with state-of-the-art vision transformers for image and video\nclassification on the ImageNet-1K [9] and Kinetics [4, 33] datasets, respectively.\nTables 1-3 show the results of this comparison. For the image classification task,\nwe incorporate our ATS module into the stages 3 to 11 of the DeiT-S [53] model.\nWe also integrate our ATS module into the 1st to 9th blocks of the 3rd stage of\nCvT-13 [63] and CvT-21 [63], and into stages 1-9 of the transformer module of\nPS-ViT [68]. We fine-tune the models on the ImageNet-1K training set. We also\nevaluate our ATS module for action recognition. To this end, we add our module\nto the XViT [2] and TimeSformer [1] video vision transformers. For more details,\nplease refer to the supplementary materials.\nImage Classification As it can be seen in Table 1, our ATS module decreases\nthe GFLOPs of all vision transformer models without adding any extra pa-\nrameters to the backbone models. For the DeiT-S+ATS model, we observe a\n37% GFLOPs reduction with only 0 .1% reduction of the top-1 accuracy. For\nthe CvT+ATS models, we can also observe a GFLOPs reduction of about 30%\nwith 0.1 − 0.2% reduction of the top-1 accuracy. More details on the efficiency\nof our ATS module can be found in the supplementary materials ( e.g. through-\nput). Comparing ATS to DynamicViT [46] and HVT [42], which add additional\nparameters to the model, our approach achieves a better trade-off between accu-\nracy and GFLOPs. Our method also outperforms the EViT-DeiT-S [36] model\ntrained for 30 epochs without adding any extra trainable parameters to the\nmodel. We note that the EViT-DeiT-S model can improve its top-1 accuracy by\naround 0.3% when it is trained for much more training epochs (e.g. 100 epochs).\nFor a fair comparison, we have considered the 30 epochs training setup used\nby Dynamic-ViT [46]. We have also added our ATS module to the PS-ViT net-\nwork [68]. As it can be seen in Table 1, although PS-ViT has drastically lower\nGFLOPs compared to its counterparts, its GFLOPs can be further decreased by\nincorporating ATS in it.\nAction Recognition As it can be seen in Tables 2 and 3, our ATS module dras-\ntically decreases the GFLOPs of all video vision transformers without adding\nany extra parameters to the backbone models. For the XViT+ATS model, we\nobserve a 39% GFLOPs reduction with only 0 .2% reduction of the top-1 ac-\ncuracy on Kinetics-400 and a 38 .7% GFLOPs reduction with only 0 .1% drop\nof the top-1 accuracy on Kinetics-600. We observe that XViT+ATS achieves\na similar accuracy as TokenLearner [49] on Kinetics-600 while requiring 17 .6×\nless GFLOPs. For TimeSformer-L+ATS, we can observe 50.8% GFLOPs reduc-\ntion with only 0 .2% drop of the top-1 accuracy on Kinetics-400. These results\ndemonstrate the generality of our approach that can be applied to both image\nand video representations.\n14 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nTable 1. Comparison of the multi-stage\nATS models with state-of-the-art im-\nage classification models with comparable\nGFLOPs on the ImageNet validation set.\nWe equip DeiT-S [53], PS-ViT [68], and\nvariants of CvT [63] with our ATS mod-\nule and fine-tune them on the ImageNet\ntraining set.\nModel Params (M) GFLOPs Resolution Top-1\nViT-Base/16 [13] 86.6 17.6 224 77.9\nHVT-S-1 [42] 22.09 2.4 224 78.0IA-RED2[41] - 2.9 224 78.6DynamicViT-DeiT-S (30 Epochs) [46] 22.77 2.9 224 79.3EViT-DeiT-S (30 epochs) [36] 22.1 3.0 224 79.5DeiT-S+ATS (Ours) 22.05 2.9 224 79.7DeiT-S [53] 22.05 4.6 224 79.8\nPVT-Small [60] 24.5 3.8 224 79.8CoaT Mini [64] 10.0 6.8 224 80.8CrossViT-S [5] 26.7 5.6 224 81.0PVT-Medium [60] 44.2 6.7 224 81.2Swin-T [39] 29.0 4.5 766 81.3T2T-ViT-14 [67] 22.0 5.2 224 81.5CPVT-Small-GAP [8] 23.0 4.6 817 81.5\nCvT-13 [63] 20.0 4.5 224 81.6CvT-13+ATS (Ours) 20.0 3.2 224 81.4\nPS-ViT-B/14 [68] 21.3 5.4 224 81.7PS-ViT-B/14+ATS (Ours)21.3 3.7 224 81.5\nRegNetY-8G [44] 39.0 8.0 224 81.7DeiT-Base/16 [53] 86.6 17.6 224 81.8CoaT-Lite Small [64] 20.0 4.0 224 81.9T2T-ViT-19 [67] 39.2 8.9 224 81.9CrossViT-B [5] 104.7 21.2 224 82.2T2T-ViT-24 [67] 64.1 14.1 224 82.3\nPS-ViT-B/18 [68] 21.3 8.8 224 82.3PS-ViT-B/18+ATS (Ours)21.3 5.6 224 82.2\nCvT-21 [63] 32.0 7.1 224 82.5CvT-21+ATS (Ours) 32.0 5.1 224 82.3\nTNT-B [22] 66.0 14.1 224 82.8RegNetY-16G [44] 84.0 16.0 224 82.9Swin-S [39] 50.0 8.7 224 83.0\nCvT-13384[63] 20.0 16.3 384 83.0CvT-13384+ATS (Ours) 20.0 11.7 384 82.9\nSwin-B [39] 88.0 15.4 224 83.3LV-ViT-S [30] 26.2 6.6 224 83.3\nCvT-21384[63] 32.0 24.9 384 83.3CvT-21384+ATS (Ours) 32.0 17.4 384 83.1\nTable 2. Comparison with state-of-\nthe-art on Kinetics-400.\nModel Top-1 Top-5 Views GFLOPs\nSTC [10] 68.7 88.5 112 -bLVNet [15] 73.5 91.2 3 ×3 840STM [37] 73.7 91.6 - -TEA [35] 76.1 92.5 10 ×3 2,100TSM R50 [29] 74.7 - 10 ×3 650I3D NL [62] 77.7 93.3 10 ×3 10,800CorrNet-101 [58] 79.2 - 10 ×3 6,700ip-CSN-152 [55] 79.2 93.8 10 ×3 3,270HATNet [11] 79.3 - - -SlowFast 16×8 R101+NL [18] 79.8 93.9 10×3 7,020X3D-XXL [17] 80.4 94.6 10 ×3 5,823\nTimeSformer-L [1] 80.7 94.7 1×3 7,140TimeSformer-L+ATS (Ours)80.594.61×3 3,510\nViViT-L/16x2 [1] 80.6 94.7 4 ×3 17,352MViT-B, 64×3 [14] 81.2 95.1 3 ×3 4,095\nX-ViT (16×) [2] 80.2 94.7 1 ×3 425X-ViT+ATS (16×) (Ours) 80.094.61×3 259\nTokenLearner 16at12 (L/16) [49] 82.1 - 4×3 4,596\nTable 3. Comparison with state-of-\nthe-art on Kinetics-600.\nModel Top-1 Top-5 Views GFLOPs\nAttentionNAS [61] 79.8 94.4 - 1,034LGD-3D R101 [43] 81.5 95.6 10×3 -HATNET [11] 81.6 - - -SlowFast R101+NL [18] 81.8 95.1 10×3 3,480X3D-XL [17] 81.9 95.5 10 ×3 1,452X3D-XL+ATFR [16] 82.1 95.6 10×3 768\nTimeSformer-HR [1] 82.4 96 1×3 5,110TimeSformer-HR+ATS (Ours)82.2 96 1×3 3,103\nViViT-L/16x2 [1] 82.5 95.6 4 ×3 17,352Swin-B [39] 84.0 96.5 4 ×3 3,384MViT-B-24, 32×3 [14] 84.1 96.5 1 ×5 7,080TokenLearner 16at12(L/16) [49] 84.4 96.0 4×3 9,192\nX-ViT (16×) [2] 84.5 96.3 1 ×3 850X-ViT+ATS (16×) (Ours) 84.496.21×3 521\n5 Conclusion\nDesigning computationally efficient vision transformer models for image and\nvideo recognition is a challenging task. In this work, we proposed a novel dif-\nferentiable parameter-free module called Adaptive Token Sampler (ATS) to in-\ncrease the efficiency of vision transformers for image and video classification.\nThe new ATS module selects the most informative and distinctive tokens within\nthe stages of a vision transformer model such that as much tokens as needed but\nnot more than necessary are used for each input image or video clip. By inte-\ngrating our ATS module into the attention layers of current vision transformers,\nwhich use a static number of tokens, we can convert them into much more ef-\nficient vision transformers with an adaptive number of tokens. We showed that\nour ATS module can be added to off-the-shelf pre-trained vision transformers\nAdaptive Token Sampling 15\nas a plug and play module, thus reducing their GFLOPs without any additional\ntraining, but it is also possible to train a vision transformer equipped with the\nATS module thanks to its differentiable design. We evaluated our approach on\nthe ImageNet-1K image recognition dataset and incorporated our ATS module\ninto three different state-of-the-art vision transformers. We also demonstrated\nthe generality of our approach by incorporating it into different state-of-the-art\nvideo vision transformers and evaluating them on the Kinetics-400 and Kinetics-\n600 datasets. The results show that the ATS module decreases the computation\ncost (GFLOPs) between 27% and 50 .8% with a negligible accuracy drop. Al-\nthough our experiments are focused on image and video vision transformers, we\nbelieve that our approach can also work in other domains such as audio.\nAcknowledgments: Farnoush Rezaei Jafari acknowledges support by the\nFederal Ministry of Education and Research (BMBF) for the Berlin Institute\nfor the Foundations of Learning and Data (BIFOLD) (01IS18037A). Juergen\nGall has been supported by the Deutsche Forschungsgemeinschaft (DFG, Ger-\nman Research Foundation) under Germany’s Excellence Strategy - EXC 2070\n- 390732324, GA1927/4-2 (FOR 2535 Anticipating Human Behavior), and the\nERC Consolidator Grant FORHUE (101044724).\nAppendix\nA Runtime\nThroughput: While ATS is a super-light module, there is still a small cost as-\nsociated with I/O operations. For a DeiT-S network with a single ATS stage, the\nsampling overhead is about 1 .5% of the overall computation which is negligible\ncompared to the large savings due to the dropped tokens. To further elabo-\nrate on this, we have reported the throughput (images/s) of the DeiT-S model\nwith/without our ATS module in Table A.1. As it can be seen, the speed-up of\nour module is aligned with its GFLOPs reduction.\nBatch Processing: While for most applications the inference is performed for\na single image or video, ATS can also be used for inference with a mini-batch.\nTo this end, we rearrange the tokens of each image so that the sampled tokens\nare in the lower indices. Then, we remove the last tokens completely to reduce\nthe computation. This way, we only process m tokens, where m = maxi(K′\ni + 1)\nover all images i of the mini-batch. In the worst case scenario ( e.g. a very large\nminibatch), we will keep all K +1 first tokens after rearrangement. This will still\nreduce the computation by a factor of N+1\nK+1 . For example, using a mini-batch of\nsize 512 on the ImageNet validation set, m is 129 in Stage 7 of the DeiT-S+ATS\nmodel, which is smaller than the total number of tokens (197). Therefore, we\ndiscard at least 68 tokens in stage 7 even in a mini-batch setting. Moreover, for\nthe fully connected layers in a transformer block, which requires most of the\ncomputation [40], we can flatten the mini-batch dimension and forward only\nnon-zero tokens of the whole mini-batch in parallel through the fully connected\nlayers.\n16 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nTable A.1. Runtime comparison:\nWe run the models on a single\nRTX6000 GPU (CUDA 11.0, PyTorch\n1.8, image size: 224 ×224). We average\nthe value of throughput over 20 runs.\nWe add ATS to multiple stages of the\nDeiT-S model and fine-tune the net-\nwork on the ImageNet dataset.\nModel Params (M) GFLOPs Throughput Top-1\nDeit-S [53] 22.05 4.6 1010 79.8\nDeit-S+ATS 22.05 2.9 1403 79.7\nFig. A.1. Effect of K: We varied the\nvalue of K in the ATS module to study the\neffect of K on the top-1 accuracy. K=48\ncorresponds to 2 GFLOPs. The backbone\nmodel is DeiT-S pre-trained on ImageNet-\n1K.\nB The Effect of K\nIn Fig. 5 of the main paper, we varied the value ofK to achieve different GFLOPs\nlevels (Top-1 Accuracy vs. GFLOPs). In Fig. A.1, we study the effect of varying\nK in the ATS module of the single-stage DeiTS+ATS model with fine-tuning.\nInterestingly, even sampling only 48 tokens (2 GFLOPs) achieves 75% accuracy.\nC ATS Integration Without Further Training\nOne of the most important aspects of our approach is that it can be added to\nany pre-trained off-the-shelf vision transformer. For example, our not fine-tuned\nmulti-stage DeiT-S+ATS model (Fig. 5(c) in the paper) has only a 0.6% (Ta-\nble 1 in the paper) top-1 accuracy drop while it has improved the efficiency by\nabout 1.6 GFLOPs without any further training of the backbone model. We also\nobserve the same performance on video data. As reported in Table A.2, our not\nfine-tuned XViT+ATS model has only a 1.1% top-1 accuracy drop while it has\nimproved the efficiency by about 329 GFLOPs without any further training of\nthe backbone model. This capability of our ATS module roots back in its adap-\ntive inverse transform sampling strategy. Our ATS module samples informative\ntokens based on their contributions to the classification token. Uninformative to-\nkens that only slightly contribute to the final prediction receive lower attention\nweights for the classification token. Therefore, the output classification token will\nbe only marginally affected by removing such redundant tokens. On the other\nhand, the redundant tokens are less similar to the informative tokens and receive\nlower attention weights for those tokens in the attention matrix. Consequently,\nthey do not contribute much to the value of informative tokens and eliminating\nthem does not change the way informative tokens are contributing to the output\nclassification token.\nAdaptive Token Sampling 17\nTable A.2. Our ATS module is added to XViT [2] pre-trained on Kinetics-600.\nModel Top-1 GFLOPs\nXViT+ATS Not-Finetuned(16×) 83.4 521\nXViT+ATS Finetuned(16×) 84.4 521\nXViT(16×) 84.5 850\nD Attention Map Visualization\nAs shown in Fig. A.2, the attention maps become more focused on the birds and\nless on the background at the later stages, which is aligned with our observations\non the sampled tokens at each stage.\nFig. A.2. Visualization of the sampled tokens and attention maps of a not fine-tuned\nmulti-stage DeiT-S+ATS.\nE Implementation Details\nIn our experiments for image classification, we use the ImageNet [9] dataset\nwith 1.28M training images and 1K classes. We evaluate our adaptive models,\nwhich are equipped with the ATS module, on 50K validation images of this\ndataset. In our experiments for action recognition, we use the Kinetics-400 [33]\nand Kinetics-600 [4] datasets containing short clips (typically 10 seconds long)\nsampled from YouTube. Kinetics-400 and Kinetics-600 consist of 400 and 600\nclasses, respectively. The versions of Kinetics-400 and Kinetics-600 used in this\npaper consist of approximately 261k and 457k clips, respectively. Note that these\nnumbers are lower than the original datasets due to the removal of certain videos\nfrom YouTube. Our networks for image classification are trained on 8 NVIDIA\nQuadro RTX 6000 GPUs and for action recognition on 8 NVIDIA A100 GPUs.\nE.1 DeiT + ATS\nTraining To fine-tune our adaptive models, we follow the DynamicViT [46]\ntraining settings. We use the DeiT model’s pre-trained weights to initialize the\n18 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nbackbones of our adaptive network and train it for 30 epochs using AdamW\noptimizer. The learning rate and batch size are set to 5e-4 and 8×96, respectively.\nWe use the cosine scheduler to train the networks. For both multi and single stage\nmodels, we set K = 197 during training.\nEvaluation We use the same setup as [53] for evaluating our adaptive mod-\nels. To evaluate the performance of our multi-stage DeiT-S+ATS model with\ndifferent average GFLOPs levels of 3, 2 .5, and 2, we set Kn = max( ⌊ρ ×\n#InputTokens n⌉, 8) in which ρ is set to 1, 0 .87, 0 .72, respectively, and n is\nthe stage index. For the single-stage model, we set K = 108, 78, 48 to evaluate\nthe model with different average GFLOPs levels of 3, 2 .5, and 2.\nE.2 CvT + ATS\nWe integrate our ATS module into the 1st to 9th blocks of the 3rd stage of the\nCvT-13 [63] and CvT-21 [63] networks. For both CvT models, we do not use any\nconvolutional projection layers in the transformer blocks of stage 3.\nTrainingTo train our adaptive models, we follow most of the CvT [63] network’s\ntraining settings. We use the CvT model’s pre-trained weights to initialize the\nbackbones of our adaptive networks and train them for 30 epochs using AdamW\noptimizer. The learning rate and batch size are set to 1.5e-6 and 128, respectively.\nWe use the cosine scheduler to train the networks.\nEvaluation To evaluate our CvT+ATS model, we use the same setup as [63].\nE.3 PS-ViT + ATS\nTraining To fine-tune our adaptive models, we follow the PS-ViT [68] train-\ning settings. We use the PS-ViT model’s pre-trained weights to initialize the\nbackbones of our adaptive network and train it for 30 epochs using AdamW op-\ntimizer. The learning rate and batch size are set to 5e-4 and 8 ×96, respectively.\nWe use the cosine scheduler to train the networks.\nEvaluation To evaluate our CvT+ATS model, we use the same setup as [68].\nE.4 XViT + ATS\nWe integrate our ATS module into the stages 3 to 11 of the XViT [2] network.\nTraining To train our adaptive model, we follow most of the XViT [2] network’s\ntraining settings. We use the XViT model’s pre-trained weights to initialize\nthe backbone of our adaptive network and train it for 10 epochs using SGD\noptimizer. The learning rate and batch size are set to 1.5e-6 and 64, respectively.\nWe use the cosine scheduler to train the networks.\nEvaluation To evaluate our XViT+ATS model, we use the same setup as [2].\nAdaptive Token Sampling 19\nE.5 TimeSformer + ATS\nWe integrate our ATS module into the stages 3 to 5 of the TimeSformer [1]\nnetwork.\nTraining To train our adaptive model, we follow most of the TimeSformer [1]\nnetwork’s training settings. We use the TimeSformer model’s pre-trained weights\nto initialize the backbones of our adaptive networks and train it for 5 epochs\nusing SGD optimizer. The learning rate and batch size are set to 5e-6 and 32,\nrespectively. We use the cosine scheduler to train the networks.\nEvaluation To evaluate our TimeSformer-HR+ATS and TimeSformer-L+ATS\nmodels, we use the same setup as [1].\nE.6 Integrating ATS into a Transformer Block\nUnlike a standard transformer block in vision transformers, we assign a score to\neach token and use inverse transform sampling to prune the rows of the attention\nmatrix A to get As. Next, we get the output O = AsV and forward it to the\nFeed-Forward Network (FFN) of the transformer block. We visualize the details\nof our ATS module, which is integrated into a standard self-attention layer in\nFig. A.3.\nF Ablation\nF.1 Score Assignment\nIn the main paper, we analyzed the impact of using different tokens to calculate\nthe significance scores S. In all of our experiments, we suggested keeping the\nclassification token since the loss is defined on this token and discarding it may\nnegatively affect the performance. To represent the importance of this token\nexperimentally, we sum over the attention weights of all tokens (rows of the\nattention matrix) to find the most significant tokens. We show this in Fig. A.4\nas Self-Attention Score (CLS Enforced). In contrast to our previous experiments,\nwe allow ATS to remove the classification token when it is of low importance\nbased on the significance scores S. We show the results of this experiment in\nFig. A.4 as Self-Attention Score (CLS Not Enforced). As it can be seen in Fig.\nA.4, discarding the classification token reduces the top-1 accuracy.\nF.2 Candidate Token Selection\nAs mentioned in the main paper, we employ the inverse transform sampling\napproach to softly downsample input tokens. We investigated this in Section\n4 of the paper. To better analyze it, we also evaluate the performance of our\ntrained multi-stage DeiT-S+ATS model when picking the top K tokens with the\nhighest significance scores S. To this end, we trained our DeiT-S+ATS network\nwith the top-K selection approach and compared it to our DeiT-S+ATS model\nwith the inverse transform sampling method. As it can be seen in Table A.3, our\n20 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nFig. A.3.The Adaptive Token Sampler (ATS) can be integrated into the self-attention\nlayer of any transformer block of a vision transformer model (top). The ATS module\ntakes at each stage a set of input tokens I. The first token is considered as the classi-\nfication token in each block of the vision transformer. The attention matrix A is then\ncalculated by the dot product of the queries Q and keys K, scaled by\n√\nd. Having se-\nlected the significant tokens, we then sample the corresponding attention weights (rows\nof the attention matrix A) to get As. Finally, we softly downsample the input tokens\nI to output tokens O using the dot product of As and V. Next, we forward the output\ntokens O through a Feed-Forward Network (FFN) to get the output of the transformer\nblock.\ninverse transform sampling approach outperforms the top-K selection with and\nwithout training (Fig 5(a) in paper). As discussed earlier, our inverse transform\nsampling approach does not hardly discard all tokens with lower significance\nscores and hence provides a more diverse set of tokens for the following layers.\nThis sampling strategy also helps the model to gain a better performance after\ntraining, thanks to a more diversified token selection.\nF.3 ATS Placement\nTo evaluate the effect of our ATS module’s location within a vision transformer\nmodel, we add it to different stages of the DeiT-S network and evaluate it on the\nImageNet validation set without finetuning the model. To have a better com-\nparison, we set the average computation costs of all experiments to 3 GFLOPs.\nAdaptive Token Sampling 21\nTable A.3. Comparison of the inverse\ntransform sampling approach with the top-\nK selection. We finetune and test two dif-\nferent versions of the multi-stage DeiT-\nS+ATS model: with (1) top-K token se-\nlection and (2) inverse transform token\nsampling. We report the top-1 accuracy\nof both networks on the ImageNet valida-\ntion set. For the model with the top-K se-\nlection approach, we set Kn = ⌊0.865 ×\n#InputTokens n⌉ where n is the stage in-\ndex. For example, K3 = 171 in stage 3.\nMethod Top-1 acc GFLOPs\nTop-K 78.9 2.9\nInverse Transform Sampling79.7 2.9\nFig. A.4.Impact of allowing ATS to dis-\ncard the classification token on the net-\nwork’s accuracy. The model is a single\nstage DeiT-S+ATS without finetuning.\nAs it can be seen in Table A.4, integrating the ATS module into the first stage\nof the DeiT-S model results in a poor top-1 accuracy of 73 .1%. On the other\nhand, integrating the ATS module into stage 3 results in a 78 .5% top-1 accu-\nracy. As mentioned before, earlier transformer blocks are more prone to predict\nnoisier attention weights for the classification token. Therefore, integrating our\nATS module into the first stage performs worse than incorporating it into the\nstage 3. Although the attention weights of the stage 6 are less noisy, we have\nto discard more tokens to reach the desired GFLOPs level of 3. For example in\nstages 0, 3, and 6, we set K to 130, 108, and 56, respectively. The highest accu-\nracy is obtained when we integrate the ATS module into multiple stages of the\nDeiT-S model. This is because of the progressive token sampling that occurs in\na multi-stage DeiT-S+ATS model. In other words, a multi-stage DeiT-S+ATS\nnetwork can gradually decrease the GFLOPs by discarding fewer tokens in the\nearlier stages, while a single-stage DeiT-S+ATS model has to discard more to-\nkens in the earlier stages to reach the same GFLOPs level. We also added the\nATS module into all stages, yielding average GFLOPs of 2.6 and 76 .9% top-1\naccuracy.\nStage(s) 0 3 6 3-11\nTop-1 Accuracy 73.1 78.5 77.4 79.2\nTable A.4. Evaluating the integration of the ATS module into different stages of\nDeiT-S [53].\n22 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nF.4 Adding ATS to Models with Other Token Pruning Approaches\nTo better evaluate the performance of our adaptive token sampling approach,\nwe also add our module to the state-of-the-art efficient vision transformer EViT-\nDeiT-S [36]. EViT [36] introduces a token reorganization method that first iden-\ntifies the top-K important tokens by computing token attentiveness between the\ntokens and the classification token and then fuses less informative tokens. In-\nterestingly, our ATS module can also be added to the EViT-DeiT-S model and\nfurther decrease the GFLOPs, as shown in Table A.5. These results demonstrate\nthe superiority of our adaptive token sampling approach compared to static to-\nken pruning methods. We integrate our ATS module into stages 4, 5, 7, 8, 10,\nand 11 of the EViT-DeiT-S backbone and fine-tune them for 10 epochs following\nour fine-tuning setups on the ImageNet dataset discussed earlier.\nModel Top-1 acc GFLOPs\nEViT-DeiT-S (30 Epochs) [36] 79.5 3.0\nEViT-DeiT-S (30 Epochs)+ATS 79.5 2.5\nEViT-DeiT-S (100 Epochs) [36] 79.8 3.0\nEViT-DeiT-S (100 Epochs)+ATS 79.8 2.5\nTable A.5. Evaluating the EViT-DeiT-S [36] model’s performance when integrating\nthe ATS module into it with Kn = ⌊0.7×#InputTokens n⌉ where n is the stage index.\nG More Visualizations\nWe show more visual results in Fig. A.5. We select several images of the ImageNet\nvalidation set with various amounts of detail and complexity. We visualize the\nprogressive token sampling procedure of our multi-stage DeiT-S+ATS model for\nthe selected images. The number of output tokens of each ATS module in the\nmulti-stage DeiT-S+ATS model is limited by the number of its input tokens,\nwhich is 197. Our adaptive model samples a higher number of tokens when the\ninput images are more cluttered. We can also observe that the sampled tokens\nare more scattered in images with more details compared to more plain images.\nAdaptive Token Sampling 23\nFig. A.5. Visualization of the gradual token sampling procedure in the multi-stage\nDeiT-S+ATS model. We integrate our ATS module into the stages 3 to 11 of the\nDeiT-S model. The tokens that are sampled at each stage of the network are shown for\nimages that are ordered by their complexity (from low complexity to high complexity).\nWe visualize the tokens, which are discarded, as masks over the input images. As it\ncan be seen, a higher number of tokens are sampled for more cluttered images while a\nlower number of tokens are required when the images contain less details. Additionally,\nwe can see that the sampled tokens are more focused and less scattered in images with\nless details.\n24 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\nReferences\n1. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for\nvideo understanding. In: International Conference on Machine Learning (ICML)\n(2021) 2, 4, 13, 14, 19\n2. Bulat, A., Perez Rua, J.M., Sudhakaran, S., Martinez, B., Tzimiropoulos, G.:\nSpace-time mixing attention for video transformer. In: Advances in Neural In-\nformation Processing Systems (NeurIPS) (2021) 2, 4, 13, 14, 17, 18\n3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision (ECCV) (2020) 4\n4. Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A.: A short\nnote about kinetics-600. In: arXiv preprint arXiv:1808.01340v1 (2018) 8, 9, 13, 17\n5. Chen, C.F., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision trans-\nformer for image classification. In: IEEE/CVF International Conference on Com-\nputer Vision (ICCV) (2021) 14\n6. Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need\nfor semantic segmentation. In: Advances in Neural Information Processing Systems\n(NeurIPS) (2021) 4\n7. Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with\nsparse transformers. In: arXiv preprint arXiv:1904.10509 (2019) 5\n8. Chu, X., Tian, Z., Zhang, B., Wang, X., Wei, X., Xia, H., Shen, C.: Conditional\npositional encodings for vision transformers. In: arXiv preprint arXiv:2102.10882\n(2021) 14\n9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) (2009) 8, 13, 17\n10. Diba, A., Fayyaz, M., Sharma, V., Arzani, M.M., Yousefzadeh, R., Gall, J.,\nVan Gool, L.: Spatio-temporal channel correlation networks for action classifica-\ntion. In: European Conference on Computer Vision (ECCV) (2018) 1, 14\n11. Diba, A., Fayyaz, M., Sharma, V., Paluri, M., Gall, J., Stiefelhagen, R., Gool, L.V.:\nLarge scale holistic video understanding. In: European Conference on Computer\nVision (2020) 14\n12. Diba, A., Sharma, V., Gool, L.V., Stiefelhagen, R.: Dynamonet: Dynamic action\nand motion network. In: IEEE/CVF International Conference on Computer Vision\n(ICCV) (2019) 1\n13. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (ICLR) (2021) 1, 4, 14\n14. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:\nMultiscale vision transformers. In: IEEE/CVF International Conference on Com-\nputer Vision (ICCV) (2021) 14\n15. Fan, Q., Chen, C.F.R., Kuehne, H., Pistoia, M., Cox, D.: More Is Less: Learning\nEfficient Video Representations by Temporal Aggregation Modules. In: Advances\nin Neural Information Processing Systems (NeurIPS) (2019) 14\n16. Fayyaz, M., Bahrami, E., Diba, A., Noroozi, M., Adeli, E., Van Gool, L., Gall, J.:\n3d cnns with adaptive temporal feature resolutions. In: IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) (2021) 14\nAdaptive Token Sampling 25\n17. Feichtenhofer, C.: X3d: Expanding architectures for efficient video recognition. In:\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(2020) 1, 14\n18. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-\ntion. In: IEEE/CVF international conference on computer vision (ICCV) (2019)\n1, 14\n19. Gong, Y., Liu, L., Yang, M., Bourdev, L.: Compressing deep convolutional networks\nusing vector quantization. In: arXiv preprint arXiv:1412.6115 (2014) 4\n20. Goyal, S., Choudhury, A.R., Raje, S.M., Chakaravarthy, V.T., Sabharwal, Y.,\nVerma, A.: Power-bert: Accelerating bert inference via progressive word-vector\nelimination. In: International Conference on Machine Learning (ICML) (2020) 5\n21. Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., Zhang, Z.: Star-transformer. In: arXiv\npreprint arXiv:1902.09113 (2019) 4\n22. Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer.\nIn: Advances in Neural Information Processing Systems (NeurIPS) (2021) 14\n23. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(2016) 1\n24. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural\nnetworks. In: IEEE/CVF International Conference on Computer Vision (ICCV)\n(2017) 4\n25. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In:\nNIPS Deep Learning and Representation Learning Workshop (2015) 4\n26. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-\ndreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for\nmobile vision applications. In: arXiv preprint arXiv:1704.04861 (2017) 4\n27. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural net-\nworks with low rank expansions. In: arXiv preprint arXiv:1405.3866 (2014) 4\n28. Jaszczur, S., Chowdhery, A., Mohiuddin, A., Kaiser, L., Gajewski, W.,\nMichalewski, H., Kanerva, J.: Sparse is enough in scaling transformers. In: Ad-\nvances in Neural Information Processing Systems (NeurIPS) (2021) 5\n29. Jiang, B., Wang, M., Gan, W., Wu, W., Yan, J.: Stm: Spatiotemporal and mo-\ntion encoding for action recognition. In: IEEE/CVF International Conference on\nComputer Vision (ICCV) (2019) 14\n30. Jiang, Z., Hou, Q., Yuan, L., Zhou, D., Jin, X., Wang, A., Feng, J.: Token label-\ning: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on\nimagenet. In: arXiv preprint arXiv:2104.10858v2 (2021) 14\n31. Jiang, Z., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., Feng, J.: All\ntokens matter: Token labeling for training better vision transformers. In: Advances\nin Neural Information Processing Systems (NeurIPS) (2021) 1, 4\n32. Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., Liu, Q.:\nTinybert: Distilling bert for natural language understanding. In: arXiv preprint\narXiv:1909.10351 (2020) 5\n33. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,\nS., Viola, F., Green, T., Back, T., Natsev, A., Suleyman, M., Zisserman, A.: The\nkinetics human action video dataset. In: arXiv preprint arXiv:1705.06950 (2017)\n8, 9, 13, 17\n34. Krizhevsky, A.: One weird trick for parallelizing convolutional neural networks. In:\nArXiv preprint arXiv:1404.5997 (2014) 1\n26 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\n35. Li, Y., Ji, B., Shi, X., Zhang, J., Kang, B., Wang, L.: Tea: Temporal excitation\nand aggregation for action recognition. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) (2020) 14\n36. Liang, Y., Ge, C., Tong, Z., Song, Y., Wang, J., Xie, P.: Not all patches are what you\nneed: Expediting vision transformers via token reorganizations. In: International\nConference on Learning Representations (ICLR) (2022) 2, 3, 5, 13, 14, 22\n37. Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video under-\nstanding. In: IEEE/CVF International Conference on Computer Vision (ICCV)\n(2019) 14\n38. Liu, B., Rao, Y., Lu, J., Zhou, J., jui Hsieh, C.: Metadistiller: Network self-boosting\nvia meta-learned top-down distillation. In: European Conference on Computer Vi-\nsion (ECCV) (2020) 4\n39. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 2,\n4, 14\n40. Marin, D., Chang, J.H.R., Ranjan, A., Prabhu, A.K., Rastegari, M., Tuzel, O.:\nToken pooling in vision transformers. arXiv preprint arXiv:2110.03860 (2021) 5,\n15\n41. Pan, B., Panda, R., Jiang, Y., Wang, Z., Feris, R., Oliva, A.: IA-RED 2:\nInterpretability-aware redundancy reduction for vision transformers. In: Advances\nin Neural Information Processing Systems (NeurIPS) (2021) 5, 14\n42. Pan, Z., Zhuang, B., Liu, J., He, H., Cai, J.: Scalable vision transformers with\nhierarchical pooling. In: IEEE/CVF International Conference on Computer Vision\n(ICCV) (2021) 3, 5, 13, 14\n43. Qiu, Z., Yao, T., Ngo, C.W., Tian, X., Mei, T.: Learning spatio-temporal repre-\nsentation with local and global diffusion. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) (2019) 14\n44. Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll´ ar, P.: Designing net-\nwork design spaces. In: IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (2020) 14\n45. Rao, Y., Lu, J., Lin, J., Zhou, J.: Runtime network routing for efficient image\nclassification. In: IEEE Transactions on Pattern Analysis and Machine Intelligence.\nvol. 41, pp. 2291–2304 (2019) 4\n46. Rao, Y., Zhao, W., Liu, B., Lu, J., Zhou, J., Hsieh, C.J.: Dynamicvit: Efficient\nvision transformers with dynamic token sparsification. In: Advances in Neural In-\nformation Processing Systems (NeurIPS) (2021) 2, 3, 5, 9, 13, 14, 17\n47. Rao, Y., Zhao, W., Zhu, Z., Lu, J., Zhou, J.: Global filter networks for image\nclassification. In: Advances in Neural Information Processing Systems (NeurIPS)\n(2021) 4\n48. Roy, A., Saffar, M., Vaswani, A., Grangier, D.: Efficient content-based sparse at-\ntention with routing transformers. In: Transactions of the Association for Compu-\ntational Linguistics. vol. 9, pp. 53–68 (2021) 5\n49. Ryoo, M.S., Piergiovanni, A., Arnab, A., Dehghani, M., Angelova, A.: Token-\nlearner: What can 8 learned tokens do for images and videos? arXiv preprint\narXiv:2106.11297 (2021) 5, 13, 14\n50. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2015) 1\n51. Sukhbaatar, S., Grave, E., Bojanowski, P., Joulin, A.: Adaptive attention span in\ntransformers. In: ACL (2019) 5\nAdaptive Token Sampling 27\n52. Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural\nnetworks. In: International Conference on Machine Learning (ICML) (2019) 4\n53. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training\ndata-efficient image transformers and distillation through attention. In: Interna-\ntional Conference on Machine Learning (ICML) (2021) 1, 3, 4, 9, 11, 13, 14, 16,\n18, 21\n54. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-\nporal features with 3d convolutional networks. In: IEEE International Conference\non Computer Vision (ICCV) (2015) 1\n55. Tran, D., Wang, H., Torresani, L., Feiszli, M.: Video classification with channel-\nseparated convolutional networks. In: IEEE/CVF International Conference on\nComputer Vision (ICCV) (2019) 14\n56. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look\nat spatiotemporal convolutions for action recognition. In: IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2018) 1\n57. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL.u., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems (NeuRIPS) (2017) 4, 6\n58. Wang, H., Tran, D., Torresani, L., Feiszli, M.: Video modeling with correlation\nnetworks. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) (2020) 14\n59. Wang, K., Liu, Z., Lin, Y., Lin, J., Han, S.: Haq: Hardware-aware automated\nquantization with mixed precision. In: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2019) 4\n60. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao,\nL.: Pyramid vision transformer: A versatile backbone for dense prediction with-\nout convolutions. In: IEEE/CVF International Conference on Computer Vision\n(ICCV) (2021) 14\n61. Wang, X., Xiong, X., Neumann, M., Piergiovanni, A.J., Ryoo, M.S., Angelova,\nA., Kitani, K.M., Hua, W.: Attentionnas: Spatiotemporal attention cell search for\nvideo classification. In: European Conference on Computer Vision (ECCV) (2020)\n14\n62. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In:\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(2018) 14\n63. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: Introduc-\ning convolutions to vision transformers. In: IEEE/CVF International Conference\non Computer Vision (ICCV) (2021) 1, 3, 13, 14, 18\n64. Xu, W., Xu, Y., Chang, T., Tu, Z.: Co-scale conv-attentional image transformers.\nIn: arXiv preprint arXiv:2104.06399 (2021) 14\n65. Yu, X., Liu, T., Wang, X., Tao, D.: On compressing deep models by low rank and\nsparse decomposition. In: IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (2017) 4\n66. Yu, X., Rao, Y., Wang, Z., Liu, Z., Lu, J., Zhou, J.: Pointr: Diverse point cloud\ncompletion with geometry-aware transformers. In: IEEE/CVF International Con-\nference on Computer Vision (ICCV) (2021) 4\n67. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan,\nS.: Tokens-to-token vit: Training vision transformers from scratch on imagenet. In:\narXiv preprint arXiv:2101.11986 (2021) 14\n28 Fayyaz, Abbasi Koohpayegani, Rezaei Jafari et al.\n68. Yue, X., Sun, S., Kuang, Z., Wei, M., Torr, P., Zhang, W., Lin, D.: Vision trans-\nformer with progressive sampling. In: IEEE/CVF International Conference on\nComputer Vision (ICCV) (2021) 3, 5, 13, 14, 18\n69. Zhao, H., Jiang, L., Jia, J., Torr, P., Koltun, V.: Point transformer. In: IEEE/CVF\nInternational Conference on Computer Vision (ICCV) (2021) 4\n70. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\nT., Torr, P.H., Zhang, L.: Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) (2021) 4\n71. Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., Feng, J.: Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886 (2021) 4",
  "topic": "FLOPS",
  "concepts": [
    {
      "name": "FLOPS",
      "score": 0.8004645109176636
    },
    {
      "name": "Transformer",
      "score": 0.7789156436920166
    },
    {
      "name": "Computer science",
      "score": 0.7061047554016113
    },
    {
      "name": "Security token",
      "score": 0.6674273610115051
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46290549635887146
    },
    {
      "name": "Differentiable function",
      "score": 0.44687679409980774
    },
    {
      "name": "Computer vision",
      "score": 0.3463618755340576
    },
    {
      "name": "Real-time computing",
      "score": 0.3371754288673401
    },
    {
      "name": "Engineering",
      "score": 0.16038775444030762
    },
    {
      "name": "Parallel computing",
      "score": 0.12979400157928467
    },
    {
      "name": "Electrical engineering",
      "score": 0.1121864914894104
    },
    {
      "name": "Mathematics",
      "score": 0.10402897000312805
    },
    {
      "name": "Voltage",
      "score": 0.08964407444000244
    },
    {
      "name": "Computer network",
      "score": 0.07233545184135437
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": []
}