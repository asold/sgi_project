{
    "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
    "url": "https://openalex.org/W2931316642",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A249285529",
            "name": "Sun Chen",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4280972250",
            "name": "Myers, Austin",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2746513368",
            "name": "Vondrick, Carl",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2337523870",
            "name": "Murphy, Kevin",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2565758713",
            "name": "Schmid, Cordelia",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2783139164",
        "https://openalex.org/W2964345931",
        "https://openalex.org/W2963547393",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2784025607",
        "https://openalex.org/W2422305492",
        "https://openalex.org/W2487442924",
        "https://openalex.org/W2964327849",
        "https://openalex.org/W2963092440",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W2795151422",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2803088946",
        "https://openalex.org/W2963435596",
        "https://openalex.org/W2520707650",
        "https://openalex.org/W2962756039",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2964094654",
        "https://openalex.org/W2795840542",
        "https://openalex.org/W2950179405",
        "https://openalex.org/W2796303840",
        "https://openalex.org/W2470142083",
        "https://openalex.org/W2618799552",
        "https://openalex.org/W2962711930",
        "https://openalex.org/W2913129712",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2780470340",
        "https://openalex.org/W2963125871",
        "https://openalex.org/W2962795934",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W2511428026",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964040984",
        "https://openalex.org/W2773514261",
        "https://openalex.org/W2963916161"
    ],
    "abstract": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",
    "full_text": "VideoBERT: A Joint Model for Video and Language Representation Learning\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid\nGoogle Research\n \nSeason the steak with \nsalt and pepper.\nCarefully place the steak \nto the pan.\nFlip the steak to the \nother side.\nNow let it rest and enjoy \nthe delicious steak.\n  \noutput video\ninput\nvideo\noutput\nvideo\nfutures\nVideoBERT\nVideoBERT\ninput text\nFigure 1: VideoBERT text-to-video generation and future forecasting. (Above) Given some recipe text divided into\nsentences, y = y1:T, we generate a sequence of video tokens x = x1:T by computing x∗\nt = arg maxkp(xt = k|y) using\nVideoBERT. (Below) Given a video token, we show the top three future tokens forecasted by VideoBERT at different time\nscales. In this case, VideoBERT predicts that a bowl of ﬂour and cocoa powder may be baked in an oven, and may become a\nbrownie or cupcake. We visualize video tokens using the images from the training set closest to centroids in feature space.\nAbstract\nSelf-supervised learning has become increasingly impor-\ntant to leverage the abundance of unlabeled data avail-\nable on platforms like YouTube. Whereas most existing\napproaches learn low-level representations, we propose a\njoint visual-linguistic model to learn high-level features\nwithout any explicit supervision. In particular, inspired\nby its recent success in language modeling, we build upon\nthe BERT model to learn bidirectional joint distributions\nover sequences of visual and linguistic tokens, derived from\nvector quantization of video data and off-the-shelf speech\nrecognition outputs, respectively. We use VideoBERT in nu-\nmerous tasks, including action classiﬁcation and video cap-\ntioning. We show that it can be applied directly to open-\nvocabulary classiﬁcation, and conﬁrm that large amounts\nof training data and cross-modal information are critical to\nperformance. Furthermore, we outperform the state-of-the-\nart on video captioning, and quantitative results verify that\nthe model learns high-level semantic features.\n1. Introduction\nDeep learning can beneﬁt a lot from labeled data [24],\nbut this is hard to acquire at scale. Consequently there has\nbeen a lot of recent interest in “self supervised learning”,\nwhere we train a model on various “proxy tasks”, which we\nhope will result in the discovery of features or representa-\ntions that can be used in downstream tasks. A wide variety\nof such proxy tasks have been proposed in the image and\nvideo domains. However, most of these methods focus on\nlow level features (e.g., textures) and short temporal scales\n(e.g., motion patterns that last a second or less). We are in-\nterested in discovering high-level semantic features which\ncorrespond to actions and events that unfold over longer\ntime scales (e.g. minutes), since such representations would\nbe useful for various video understanding tasks.\nIn this paper, we exploit the key insight that human\nlanguage has evolved words to describe high-level objects\nand events, and thus provides a natural source of “self”\nsupervision. In particular, we present a simple way to\nmodel the relationship between the visual domain and the\n1\narXiv:1904.01766v2  [cs.CV]  11 Sep 2019\n \n  \noutput video\ninput\nvideo\noutput\nvideo\nfutures\nVideoBERT\nVideoBERT\ninput text\n Cut the cabbage into \npieces.\nPut cabbage in the wok \nand stir fry.\nAdd soy sauce and ... \nthen keep stir frying.\nPut on a plate the dish is \nnow ready to be served.\nFigure 2: Additional text-to-video generation and future forecasting examples from VideoBERT, see Figure 1 for details.\nlinguistic domain by combining three off-the-shelf meth-\nods: an automatic speech recognition (ASR) system to con-\nvert speech into text; vector quantization (VQ) applied to\nlow-level spatio-temporal visual features derived from pre-\ntrained video classﬁcation models; and the recently pro-\nposed BERT model [6] for learning joint distributions over\nsequences of discrete tokens.\nMore precisely, our approach is to apply BERT to learn a\nmodel of the form p(x,y), where xis a sequence of “visual\nwords”, and y is a sequence of spoken words. Given such\na joint model, we can easily tackle a variety of interesting\ntasks. For example, we can perform text-to-video predic-\ntion, which can be used to automatically illustrate a set of\ninstructions (such as a recipe), as shown in the top examples\nof Figure 1 and 2. We can also perform the more traditional\nvideo-to-text task of dense video captioning [10] as shown\nin Figure 6. In Section 4.6, we show that our approach\nto video captioning signiﬁcantly outperforms the previous\nstate-of-the-art [39] on the YouCook II dataset [38].\nWe can also use our model in a “unimodal” fashion. For\nexample, the implied marginal distribution p(x) is a lan-\nguage model for visual words, which we can use for long-\nrange forecasting. This is illustrated in the bottom examples\nof Figure 1 and 2. Of course, there is uncertainty about the\nfuture, but the model can generate plausible guesses at a\nmuch higher level of abstraction than other deep generative\nmodels for video, such as those based on V AEs or GANs\n(see e.g., [4, 5, 13, 27]), which tend to predict small changes\nto low level aspects of the scene, such as the location or pose\nof a small number of objects.\nIn summary, our main contribution in this paper is a\nsimple way to learn high level video representations that\ncapture semantically meaningful and temporally long-range\nstructure. The remainder of this paper describes this con-\ntribution in detail. In particular, Section 2 brieﬂy reviews\nrelated work; Section 3 describes how we adapt the recent\nprogress in natural language modeling to the video domain;\nSection 4 presents results on activity recognition and video\ncaptioning tasks; and Section 5 concludes.\n2. Related Work\nSupervised learning. Some of the most successful ap-\nproaches for video representation learning have leveraged\nlarge labeled datasets (e.g., [9, 19, 36, 7]) to train convolu-\ntional neural networks for video classiﬁcation. However, it\nis very expensive to collect such labeled data, and the cor-\nresponding label vocabularies are often small and not ca-\npable of representing the nuances of many kinds of actions\n(e.g., “sipping” is slightly different than “drinking” which\nis slightly different than “gulping”). In addition, these ap-\nproaches are designed for representing short video clips,\ntypically a few seconds long. The main difference to our\nwork is that we focus on the long-term evolution of events\nin video, and we do not use manually provided labels.\nUnsupervised learning. Recently, a variety of ap-\nproaches for learning density models from video have been\nproposed. Some use a single static stochastic variable,\nwhich is then “decoded” into a sequence using an RNN,\neither using a V AE-style loss [32, 35] or a GAN-style loss\n[31, 17]. More recent work uses temporal stochastic vari-\nables, e.g., the SV2P model of [4] and the SVGLP model\nof [5]. There are also various GAN-based approaches, such\nas the SA VP approach of [13] and the MoCoGAN approach\nof [27]. We differ from this work in that we use the BERT\nmodel, without any explicit stochastic latent variables, ap-\nplied to visual tokens derived from the video. Thus our\nmodel is not a generative model of pixels, but it is a gen-\nerative model of features derived from pixels, which is an\napproach that has been used in other work (e.g., [30]).\nSelf-supervised learning. To avoid the difﬁculties of\nlearning a joint model p(x1:T), it has become popular to\nlearn conditional models of the form p(xt+1:T|x1:t), where\nwe partition the signal into two or more blocks, such as gray\nscale and color, or previous frame and next frame (e.g.,\n[18]), and try to predict one from the other (see e.g., [23]\nfor an overview). Our approach is similar, except we use\nquantized visual words instead of pixels. Furthermore, al-\nthough we learn a set conditional distributions, our model is\na proper joint generative model, as explained in Section 3.\nCross-modal learning. The multi-modal nature of video\nhas also been an extensive source of supervision for learn-\ning video representations, which our paper builds on. Since\nmost videos contain synchronized audio and visual signals,\nthe two modalities can supervise each other to learn strong\nself-supervised video representations [3, 20, 21]. In this\nwork, we use speech (provided by ASR) rather than low-\nlevel sounds as a source of cross-modal supervision.\nNatural language models. We build upon recent\nprogress in the NLP community, where large-scale lan-\nguage models such as ELMO [22] and BERT [6] have\nshown state-of-the-art results for various NLP tasks, both at\nthe word level (e.g., POS tagging) and sentence level (e.g.,\nsemantic classiﬁcation). The BERT model is then extended\nto pre-train on multi-lingual data [12]. Our paper builds on\nthe BERT model to capture structure in both the linguistic\nand visual domains.\nImage and video captioning. There has been much re-\ncent work on image captioning (see e.g., [11, 8, 15]), which\nis a model of the form p(y|x), where yis the manually pro-\nvided caption and xis the image. There has also been some\nwork on video captioning, using either manually provided\ntemporal segmentation or estimated segmentations (see e.g.,\n[10, 39]). We use our joint p(x,y) model and apply it to\nvideo captioning, and achieve state-of-the-art results, as we\ndiscuss in Section 4.6.\nInstructional videos. Various papers (e.g., [16, 2, 10,\n38, 39]) have trained models to analyse instructional videos,\nsuch as cooking. We differ from this work in that we do not\nuse any manual labeling, and we learn a large-scale genera-\ntive model of both words and (discretized) visual signals.\n3. Models\nIn this section, we brieﬂy summarize the BERT model,\nand then describe how we extend it to jointly model video\nand language data.\n3.1. The BERT model\nBERT [6] proposes to learn language representations by\nusing a “masked language model” training objective. In\nmore detail, let x = {x1,...,x L}be a set of discrete to-\nkens, xl ∈X. We can deﬁne a joint probability distribution\nover this set as follows:\np(x|θ) = 1\nZ(θ)\nL∏\nl=1\nφl(x|θ) ∝exp\n( L∑\nl=1\nlog φl(x|θ)\n)\nwhere φl(x) is the l’th potential function, with parameters\nθ, and Zis the partition function.\nThe above model is permutation invariant. In order to\ncapture order information, we can “tag” each word with its\nposition in the sentence. The BERT model learns an embed-\nding for each of the word tokens, as well as for these tags,\nand then sums the embedding vectors to get a continuous\nrepresentation for each token. The log potential (energy)\nfunctions for each location are deﬁned by\nlog φl(x|θ) =xT\nl fθ(x\\l)\nwhere xl is a one-hot vector for the l’th token (and its tag),\nand\nx\\l = (x1,...,x l−1,MASK,xl+1,...,x L)\nThe function f(x\\l) is a multi-layer bidirectional trans-\nformer model [28] that takes an L×D1 tensor, contain-\ning the D1-dimensional embedding vectors corresponding\nto x\\l, and returns an L×D2 tensor, where D2 is the size\nof the output of each transformer node. See [6] for details.\nThe model is trained to approximately maximize the pseudo\nlog-likelihood\nL(θ) =Ex∼D\nL∑\nl=1\nlog p(xl|x\\l; θ)\nIn practice, we can stochastically optimize the logloss\n(computed from the softmax predicted by the f function)\nby sampling locations as well as training sentences.\nBERT can be extended to model two sentences by con-\ncatenating them together. However, we are often not only\ninterested in simply modeling the extended sequence, but\nrather relationships between the two sentences (e.g., is this a\npair of consecutive or randomly selected sentences). BERT\naccomplishes this by prepending every sequence with a spe-\ncial classiﬁcation token, [CLS], and by joining sentences\nwith a special separator token,[SEP]. The ﬁnal hidden state\ncorresponding to the [CLS] token is used as the aggregate\nsequence representation from which we predict a label for\nclassiﬁcation tasks, or which may otherwise be ignored. In\naddition to differentiating sentences with the [SEP] token,\nBERT also optionally tags each token by the sentence it\ncomes from. The corresponding joint model can be written\nas p(x,y,c ), where xis the ﬁrst sentence, y is the second,\nand c = {0,1}is a label indicating whether the sentences\nwere separate or consecutive in the source document.\nFor consistency with the original paper, we also add a\n[SEP] token to the end of the sequence, even though it\nis not strictly needed. So, a typical masked-out training\nsentence pair may look like this: [CLS] let’s make\na traditional [MASK] cuisine [SEP] orange\nchicken with [MASK] sauce [SEP]. The corre-\nsponding class label in this case would be c= 1, indicating\nthat xand yare consecutive.\ninPlace the [>]the pansteak [SEP][CLS]\nEinEPlace Ethe E[>]Ethe EpanE[MASK] E[SEP]E[CLS] E[MASK] Ev(     ) Ev(     )Ev(     )Ev(     )\ninPlace the [>]the pan [SEP][CLS]\n [MASK][MASK]\nT5T2 T6 T8T3 T7T4 T14T1 T10 T11 T13T12T9\nVideoBERT\nFigure 3: Illustration of VideoBERT in the context of a video and text masked token prediction, orcloze, task. This task also\nallows for training with text-only and video-only data, and VideoBERT can furthermore be trained using a linguistic-visual\nalignment classiﬁcation objective (not shown here, see text for details).\n3.2. The VideoBERT model\nTo extend BERT to video, in such a way that we may\nstill leverage pretrained language models and scalable im-\nplementations for inference and learning, we decided to\nmake minimal changes, and transform the raw visual data\ninto a discrete sequence of tokens. To this end, we propose\nto generate a sequence of “visual words” by applying hi-\nerarchical vector quantization to features derived from the\nvideo using a pretrained model. See Section 4.2 for details.\nBesides its simplicity, this approach encourages the model\nto focus on high level semantics and longer-range temporal\ndynamics in the video. This is in contrast to most existing\nself-supervised approaches to video representation learning,\nwhich learn low-level properties such as local textures and\nmotions, as discussed in Section 2.\nWe can combine the linguistic sentence (derived from the\nvideo using ASR) with the visual sentence to generate data\nsuch as this: [CLS] orange chicken with [MASK]\nsauce [>] v01 [MASK] v08 v72 [SEP], where v01\nand v08 are visual tokens, and[>] is a special token we in-\ntroduce to combine text and video sentences. See Figure 3\nfor an illustration.\nWhile this cloze task extends naturally to sequences of\nlinguistic and visual tokens, applying a next sentence pre-\ndiction task, as used by BERT, is less straightforward. We\npropose a linguistic-visual alignment task, where we use the\nﬁnal hidden state of the [CLS] token to predict whether the\nlinguistic sentence is temporally aligned with the visual sen-\ntence. Note that this is a noisy indicator of semantic relat-\nedness, since even in instructional videos, the speaker may\nbe referring to something that is not visually present.\nTo combat this, we ﬁrst randomly concatenate neighbor-\ning sentences into a single long sentence, to allow the model\nto learn semantic correspondence even if the two are not\nwell aligned temporally. Second, since the pace of state\ntransitions for even the same action can vary greatly be-\ntween different videos, we randomly pick a subsampling\nrate of 1 to 5 steps for the video tokens. This not only helps\nthe model be more robust to variations in video speeds, but\nalso allows the model to capture temporal dynamics over\ngreater time horizons and learn longer-term state transi-\ntions. We leave investigation into other ways of combining\nvideo and text to future work.\nOverall, we have three training regimes corresponding\nto the different input data modalities: text-only, video-only\nand video-text. For text-only and video-only, the standard\nmask-completion objectives are used for training the model.\nFor text-video, we use the linguistic-visual alignment clas-\nsiﬁcation objective described above. The overall training\nobjective is a weighted sum of the individual objectives.\nThe text objective forces VideoBERT to do well at language\nmodeling; the video objective forces it to learn a “language\nmodel for video”, which can be used for learning dynam-\nics and forecasting; and the text-video objective forces it to\nlearn a correspondence between the two domains.\nOnce we have trained the model, we can use it in a va-\nriety of downstream tasks, and in this work we quantita-\ntively evaluate two applications. In the ﬁrst application, we\ntreat it as a probabilistic model, and ask it to predict or im-\npute the symbols that have been MASKed out. We illustrate\nthis in Section 4.4, where we perform “zero-shot” classiﬁ-\ncation. In the second application, we extract the predicted\nrepresentation (derived from the internal activations of the\nmodel) for the [CLS] token, and use that dense vector as\na representation of the entire input. This can be combined\nwith other features derived from the input to be used in a\ndownstream supervised learning task. We demonstrate this\nin Section 4.6, where we perform video captioning.\n4. Experiments and Analysis\nIn this section we describe our experimental setup, and\nshow quantitative and qualitative results.\n4.1. Dataset\nDeep learning models, in both language and vision do-\nmains, have consistently demonstrated dramatic gains in\nperformance with increasingly large datasets. For example,\nthe “large” BERT model (which we use) was pretrained on\nthe concatenation of the BooksCorpus (800M words) and\nEnglish Wikipedia (2,500M words).\nTherefore, we would like to train VideoBERT with a\ncomparably large-scale video dataset. Since we are inter-\nested in the connection between language and vision, we\nwould like to ﬁnd videos where the spoken words are more\nlikely to refer to visual content. Intuitively, this is often\nthe case for instructional videos, and we focus on cooking\nvideos speciﬁcally, since it is a well studied domain with\nexisting annotated datasets available for evaluation. Unfor-\ntunately, such datasets are relatively small, so we turn to\nYouTube to collect a large-scale video dataset for training.\nWe extract a set of publicly available cooking videos\nfrom YouTube using the YouTube video annotation sys-\ntem to retrieve videos with topics related to “cooking” and\n“recipe”. We also ﬁlter videos by their duration, removing\nvideos longer than 15 minutes, resulting in a set of 312K\nvideos. The total duration of this dataset is 23,186 hours, or\nroughly 966 days. For reference, this is more than two or-\nders of magnitude larger than the next largest cooking video\ndataset, YouCook II, which consists of 2K videos with a to-\ntal duration of 176 hours [38].\nTo obtain text from the videos, we utilize YouTube’s au-\ntomatic speech recognition (ASR) toolkit provided by the\nYouTube Data API [1] to retrieve timestamped speech in-\nformation. The API returns word sequences and the pre-\ndicted language type. Among the 312K videos, 180K have\nASR that can be retrieved by the API, and 120K of these\nare predicted to be in English. In our experiments, while we\nuse all videos for the video-only objective, we only use text\nfrom English ASR for VideoBERT’s text-only and video-\ntext objectives.\nWe evaluate VideoBERT on the YouCook II dataset [38],\nwhich contains 2000 YouTube videos averaging 5.26 min-\nutes in duration, for a total of 176 hours. The videos have\nmanually annotated segmentation boundaries and captions.\nOn average there are 7.7 segments per video, and 8.8 words\nper caption. We use the provided dataset split, with 1333\nvideos for training and 457 for validation. To avoid po-\ntential bias during pretraining, we also remove any videos\nwhich appear in YouCook II from our pretraining set.\n4.2. Video and Language Preprocessing\nFor each input video, we sample frames at 20 fps, and\ncreate clips from 30-frame (1.5 seconds) non-overlapping\nwindows over the video. For each 30-frame clip, we apply\na pretrained video ConvNet to extract its features. In this\nwork, we use the S3D [34] which adds separable temporal\nconvolutions to an Inception network [25] backbone. We\ntake the feature activations before the ﬁnal linear classiﬁer\nand apply 3D average pooling to obtain a 1024-dimension\nfeature vector. We pretrain the S3D network on the Kinet-\nics [9] dataset, which covers a wide spectrum of actions\nfrom YouTube videos, and serves as a generic representa-\ntion for each individual clip.\nWe tokenize the visual features using hierarchical k-\nmeans. We adjust the number of hierarchy levels dand the\nnumber of clusters per levelkby visually inspecting the co-\nherence and representativeness of the clusters. We set d=4\nand k = 12, which yields 124 = 20736 clusters in total.\nFigure 4 illustrates the result of this “vector quantization”\nprocess.\nFor each ASR word sequence, we break the stream\nof words into sentences by adding punctuation using an\noff-the-shelf LSTM-based language model. For each sen-\ntence, we follow the standard text preprocessing steps from\nBERT [6] and tokenize the text into WordPieces [33]. We\nuse the same vocabulary provided by the authors of BERT,\nwhich contains 30,000 tokens.\nUnlike language which can be naturally broken into sen-\ntences, it is unclear how to break videos into semantically\ncoherent segments. We use a simple heuristic to address\nthis problem: when an ASR sentence is available, it is as-\nsociated with starting and ending timestamps, and we treat\nvideo tokens that fall into that time period as a segment.\nWhen ASR is not available, we simply treat 16 tokens as a\nsegment.\n4.3. Model Pre-training\nWe initialize the BERT weights from a text pre-trained\ncheckpoint. Speciﬁcally, we use the BERTLARGE model re-\nleased by the authors of [6], using the same backbone archi-\ntecture: it has 24 layers of Transformer blocks, where each\nblock has 1024 hidden units and 16 self-attention heads.\nWe add support for video tokens by appending 20,736\nentries to the word embedding lookup table for each of\nour new “visual words”. We initialize these entries with\nthe S3D features from their corresponding cluster centroids.\nThe input embeddings are frozen during pretraining.\nOur model training process largely follows the setup of\nBERT: we use 4 Cloud TPUs in the Pod conﬁguration with\na total batch size of 128, and we train the model for 0.5\nmillion iterations, or roughly 8 epochs. We use the Adam\noptimizer with an initial learning rate of 1e-5, and a linear\ndecay learning rate schedule. The training process takes\naround 2 days.\n4.4. Zero-shot action classiﬁcation\nOnce pretrained, the VideoBERT model can be used\nfor “zero-shot” classiﬁcation on novel datasets, such as\nYouCook II (By “zero-shot” we mean the model is not\nFigure 4: Examples of video sentence pairs from the pretraining videos. We quantize each video segment into a token, and\nthen represent it by the corresponding visual centroid. For each row, we show the original frames (left) and visual centroids\n(right). We can see that the tokenization process preserves semantic information rather than low-level visual appearance.\ntrained on YouCook II data nor with the same label ontol-\nogy used in YouCook II). More precisely, we want to com-\npute p(y|x) where xis the sequence visual tokens, and yis\na sequence of words. Since the model is trained to predict\nsentences, we deﬁne y to be the ﬁxed sentence, “ now let\nme show you how to [MASK] the [MASK],” and ex-\ntract the verb and noun labels from the tokens predicted in\nthe ﬁrst and second masked slots, respectively. See Figure 5\nfor some qualitative results.\nFor quantitative evaluation, we use the YouCook II\ndataset. In [37], the authors collected ground truth bound-\ning boxes for the 63 most common objects for the validation\nset of YouCook II. However, there are no ground truth la-\nbels for actions, and many other common objects are not\nlabeled. So, we collect action and object labels, derived\nfrom the ground truth captions, to address this shortcoming.\nWe run an off-the-shelf part-of-speech tagger on the ground\ntruth captions to retrieve the 100 most common nouns and\n45 most common verbs, and use these to derive ground truth\nlabels. While VideoBERT’s word piece vocabulary gives\nit the power to effectively perform open-vocabulary clas-\nsiﬁcation, it is thus more likely to make semantically cor-\nrect predictions that do not exactly match the more limited\nground truth. So, we report both top-1 and top-5 classiﬁca-\ntion accuracy metrics, where the latter is intended to miti-\ngate this issue, and we leave more sophisticated evaluation\ntechniques for future work. Lastly, if there is more than\none verb or noun associated with a video clip, we deem a\nprediction correct if it matches any of those. We report the\nperformance on the validation set of YouCook II.\nTable 1 shows the top-1 and top-5 accuracies of\nVideoBERT and its ablations. To verify that VideoBERT\nactually makes use of video inputs, we ﬁrst remove the\nvideo inputs to VideoBERT, and use just the language\nFigure 5: Using VideoBERT to predict nouns and verbs\ngiven a video clip. See text for details. The video clip is\nﬁrst converted into video tokens (two are shown here for\neach example), and then visualized using their centroids.\nMethod Supervision verb top-1 (%)verb top-5 (%)object top-1 (%)object top-5 (%)\nS3D [34] yes 16.1 46.9 13.2 30.9\nBERT (language prior) no 0.0 0.0 0.0 0.0\nVideoBERT (language prior) no 0.4 6.9 7.7 15.3\nVideoBERT (cross modal) no 3.2 43.3 13.1 33.7\nTable 1: Action classiﬁcation performance on YouCook II dataset. See text for details.\nMethod Data size verb top-1 (%)verb top-5 (%)object top-1 (%)object top-5 (%)\nVideoBERT 10K 0.4 15.5 2.9 17.8\nVideoBERT 50K 1.1 15.7 8.7 27.3\nVideoBERT 100K 2.9 24.5 11.2 30.6\nVideoBERT 300K 3.2 43.3 13.1 33.7\nTable 2: Action classiﬁcation performance on YouCook II dataset as a function of pre-training data size.\nmodel p(y) to perform prediction. We also use the lan-\nguage prior from the text-only BERT model, that was not\nﬁne-tuned on cooking videos. We can see that VideoBERT\nsigniﬁcantly outperforms both baselines. As expected, the\nlanguage prior of VideoBERT is adapted to cooking sen-\ntences, and is better than the vanilla BERT model.\nWe then compare with a fully supervised classiﬁer that\nwas trained using the training split of YouCook II. We\nuse the pre-computed S3D features (same as the inputs to\nVideoBERT), applying average pooling over time, followed\nby a linear classiﬁer. Table 1 shows the results. As we\ncan see, the supervised framework outperforms VideoBERT\nin top-1 verb accuracy, which is not surprising given that\nVideoBERT has an effectively open vocabulary. (See Fig-\nure 5 for an illustration of the ambiguity of the action la-\nbels.) However, the top-5 accuracy metric reveals that\nVideoBERT achieves comparable performance to the fully\nsupervised S3D baseline, without using any supervision\nfrom YouCook II, indicating that the model is able to per-\nform competitively in this “zero-shot” setting.\n4.5. Beneﬁts of large training sets\nWe also studied the impact of the size of the pretrain-\ning dataset. For this experiment, we take random subsets\nof 10K, 50K and 100K videos from the pretraining set,\nand pretrain VideoBERT using the same setup as above,\nfor the same number of epochs. Table 2 shows the perfor-\nmance. We can see that the accuracy grows monotonically\nas the amount of data increases, showing no signs of satura-\ntion. This indicates that VideoBERT may beneﬁt from even\nlarger pretraining datasets.\n4.6. Transfer learning for captioning\nWe further demonstrate the effectiveness of VideoBERT\nwhen used as a feature extractor. To extract features given\nonly video inputs, we again use a simple ﬁll-in-the-blank\ntask, by appending the video tokens to a template sentence\n“now let’s [MASK] the [MASK] to the [MASK],\nand then [MASK] the [MASK].” We extract the fea-\ntures for the video tokens and the masked out text tokens,\ntake their average and concatenate the two together, to be\nused by a supervised model in a downstream task.\nWe evaluate the extracted features on video captioning,\nfollowing the setup from [39], where the ground truth video\nsegmentations are used to train a supervised model map-\nping video segments to captions. We use the same model\nthat they do, namely a transformer encoder-decoder, but we\nreplace the inputs to the encoder with the features derived\nfrom VideoBERT described above. We also concatenate the\nVideoBERT features with average-pooled S3D features; as\na baseline, we also consider using just S3D features without\nVideoBERT. We set the number of Transformer block lay-\ners to 2, the hidden unit size to 128, and Dropout probability\nto 0.4. We use a 5-fold cross validation on the training split\nto set the hyper-parameters, and report performance on the\nvalidation set. We train the model for 40K iterations with\nbatch size of 128. We use the same Adam optimizer as in\nVideoBERT pre-training, and set the initial learning rate to\n1e-3 with a linear decay schedule.\nTable 3 shows the results. We follow the standard prac-\ntice in machine translation and compute BLEU and ME-\nTEOR scores micro-averaged at corpus level, and also re-\nport ROUGE-L [14] and CIDEr [29] scores. For the base-\nline method [39], we recompute the metrics using the\npredictions provided by the authors. We can see that\nVideoBERT consistently outperforms the S3D baseline, es-\npecially for CIDEr. We can also see that cross-modal pre-\ntraining outperforms the video-only version. Furthermore,\nby concatenating the features from VideoBERT and S3D,\nthe model achieves the best performance across all metrics1.\nFigure 6 shows some qualitative results. We note that\nthe predicted word sequence is rarely exactly equal to the\nground truth, which explains why the metrics in Table 3\n(which measure n-gram overlap) are all low in absolute\nvalue. However, semantically the results seem reasonable.\n1The metrics used by [39] are macro-averaged at video level and may\nsuffer from undesirable sparsity artifacts. Using their provided evaluation\ncode, VideoBERT + S3D has B@4 of 1.79, and METEOR of 10.80.\nMethod BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr\nZhouet al. [39] 7.53 3.84 11.55 27.44 0.38\nS3D [34] 6.12 3.24 9.52 26.09 0.31\nVideoBERT (video only)6.33 3.81 10.81 27.14 0.47\nVideoBERT 6.80 4.04 11.01 27.50 0.49\nVideoBERT + S3D 7.59 4.33 11.94 28.80 0.55\nTable 3: Video captioning performance on YouCook II. We follow the setup from [39] and report captioning performance on\nthe validation set, given ground truth video segments. Higher numbers are better.\nFigure 6: Examples of generated captions by VideoBERT and the S3D baseline. In the last example, VideoBERT fails to\nexploit the full temporal context, since it misses the paper towel frame.\n5. Discussion and conclusion\nThis paper adapts the powerful BERT model to learn a\njoint visual-linguistic representation for video. Our exper-\nimental results demonstrate that we are able to learn high-\nlevel semantic representations, and we outperform the state-\nof-the-art for video captioning on the YouCook II dataset.\nWe also show that this model can be used directly for open-\nvocabulary classiﬁcation, and that its performance grows\nmonotonically with the size of training set.\nThis work is a ﬁrst step in the direction of learning\nsuch joint representations. For many applications, includ-\ning cooking, it is important to use spatially ﬁne-grained vi-\nsual representations, instead of just working at the frame or\nclip level, so that we can distinguish individual objects and\ntheir attributes. We envision either using pretrained object\ndetection and semantic segmentation models, or using unsu-\npervised techniques for broader coverage. We also want to\nexplicitly model visual patterns at multiple temporal scales,\ninstead of our current approach, that skips frames but builds\na single vocabulary.\nBeyond improving the model, we plan to assess our ap-\nproach on other video understanding tasks, and on other do-\nmains besides cooking. (For example, we may use the re-\ncently released COIN dataset of manually labeled instruc-\ntional videos [26].) We believe the future prospects for large\nscale representation learning from video and language look\nquite promising.\nAcknowledgements. We would like to thank Jack Hessel,\nBo Pang, Radu Soricut, Baris Sumengen, Zhenhai Zhu, and\nthe BERT team for sharing amazing tools that greatly fa-\ncilitated our experiments; Justin Gilmer, Abhishek Kumar,\nDavid Ross, and Rahul Sukthankar for helpful discussions.\nChen would like to thank Y . M. for inspiration.\nReferences\n[1] YouTube Data API. https://developers.google.\ncom/youtube/v3/docs/captions. 5\n[2] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,\nJosef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-\npervised learning from narrated instruction videos. InCVPR,\n2016. 3\n[3] Yusuf Aytar, Carl V ondrick, and Antonio Torralba. Sound-\nnet: Learning sound representations from unlabeled video.\nIn NeurIPS, 2016. 3\n[4] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic variational\nvideo prediction. In ICLR, 2018. 2\n[5] Emily Denton and Rob Fergus. Stochastic video generation\nwith a learned prior. In ICML, 2018. 2\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2, 3, 5\n[7] Chunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, et al.\nA V A: A video dataset of spatio-temporally localized atomic\nvisual actions. In CVPR, 2018. 2\n[8] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In CVPR, 2015. 3\n[9] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 2, 5\n[10] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-Captioning events in videos. In\nICCV, 2017. 2, 3\n[11] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li,\nYejin Choi, Alexander C Berg, and Tamara L Berg. Baby\ntalk: Understanding and generating image descriptions. In\nCVPR, 2011. 3\n[12] Guillaume Lample and Alexis Conneau. Cross-lingual lan-\nguage model pretraining. arXiv preprint arXiv:1901.07291,\n2019. 3\n[13] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,\nChelsea Finn, and Sergey Levine. Stochastic adversarial\nvideo prediction. arXiv:1804.01523, 2018. 2\n[14] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. Text Summarization Branches Out, 2004. 7\n[15] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural baby talk. In CVPR, 2018. 3\n[16] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick\nJohnston, Andrew Rabinovich, and Kevin Murphy. What’s\ncookin’? interpreting cooking videos using text, speech and\nvision. In NAACL, Mar. 2015. 3\n[17] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep\nmulti-scale video prediction beyond mean square error. In\nICLR, 2016. 2\n[18] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-\nﬂe and learn: unsupervised learning using temporal order\nveriﬁcation. In ECCV, 2016. 3\n[19] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-\nmakrishnan, Sarah Adel Bargal, Yan Yan, Lisa Brown,\nQuanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments\nin time dataset: one million videos for event understanding.\nTPAMI, 2019. 2\n[20] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Tor-\nralba, Edward H Adelson, and William T Freeman. Visually\nindicated sounds. In CVPR, 2016. 3\n[21] Andrew Owens, Jiajun Wu, Josh H McDermott, William T\nFreeman, and Antonio Torralba. Ambient sound provides\nsupervision for visual learning. In ECCV, 2016. 3\n[22] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In NAACL, 2018.\n3\n[23] Marc Aurelio Ranzato and Alex Graves. Deep unsupervised\nlearning. NIPS Tutorial, 2018. 3\n[24] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 1\n[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. arXiv preprint arXiv:1409.4842, 2014. 5\n[26] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. COIN:\nA large-scale dataset for comprehensive instructional video\nanalysis. In CVPR, 2019. 8\n[27] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. MoCoGAN: Decomposing motion and content for\nvideo generation. In CVPR, 2018. 2\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 3\n[29] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 7\n[30] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba. An-\nticipating visual representations from unlabeled video. In\nCVPR, 2016. 2\n[31] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. In NeurIPS, 2016.\n2\n[32] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial\nHebert. An uncertain future: Forecasting from static images\nusing variational autoencoders. In ECCV, 2016. 2\n[33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, et al. Google’s\nneural machine translation system: Bridging the gap be-\ntween human and machine translation. arXiv preprint\narXiv:1609.08144, 2016. 5\n[34] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning\nfor video understanding. In ECCV, 2018. 5, 7, 8\n[35] Tianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Free-\nman. Visual dynamics: Probabilistic future frame synthesis\nvia cross convolutional networks. In NIPS, 2016. 2\n[36] Hang Zhao, Zhicheng Yan, Heng Wang, Lorenzo Torresani,\nand Antonio Torralba. Slac: A sparsely labeled dataset\nfor action classiﬁcation and localization. arXiv preprint\narXiv:1712.09374, 2017. 2\n[37] Luowei Zhou, Nathan Louis, and Jason J Corso. Weakly-\nsupervised video object grounding from text by loss weight-\ning and object interaction. In BMVC, 2018. 6\n[38] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In AAAI, 2018. 2, 3, 5\n[39] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In CVPR, 2018. 2, 3, 7, 8\nFigure A1: Visualizations for video to text prediction. For each example, we show the key frames from the original video\n(top left) and the associated ASR outputs (top right), we then show the centroid images of video tokens (bottom left) and the\ntop predicted verbs and nouns by VideoBERT (bottom right). Note that the ASR outputs are not used to predict verbs and\nnouns.\nFigure A2: Visualizations for video to video prediction. Given an input video token, we show the top 3 predicted video\ntokens 2 steps away in the future. We visualize each video token by the centroids.\nFigure A3: Visualizations for text to video prediction. In particular, we make small changes to the input text, and compare\nhow the generated video tokens vary. We show top 2 retrieved video tokens for each text query."
}