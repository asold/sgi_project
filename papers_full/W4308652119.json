{
  "title": "Multi-dimensional patient acuity estimation with longitudinal EHR tokenization and flexible transformer networks",
  "url": "https://openalex.org/W4308652119",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5019504069",
      "name": "Benjamin Shickel",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5025876800",
      "name": "Brandon Silva",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5003769975",
      "name": "Tezcan Ozrazgat‐Baslanti",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5073900873",
      "name": "Yuanfang Ren",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5040194550",
      "name": "Kia Khezeli",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5008079314",
      "name": "Ziyuan Guan",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5084386025",
      "name": "Patrick Tighe",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5016909804",
      "name": "Azra Bihorac",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5007040136",
      "name": "Parisa Rashidi",
      "affiliations": [
        "University of Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1898928487",
    "https://openalex.org/W1991864206",
    "https://openalex.org/W2964006392",
    "https://openalex.org/W2889037799",
    "https://openalex.org/W2518582440",
    "https://openalex.org/W2022647931",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W4225323865",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W2757504960",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W2745673637",
    "https://openalex.org/W2481271618",
    "https://openalex.org/W4310645210",
    "https://openalex.org/W3099382201",
    "https://openalex.org/W4226085666"
  ],
  "abstract": "Transformer model architectures have revolutionized the natural language processing (NLP) domain and continue to produce state-of-the-art results in text-based applications. Prior to the emergence of transformers, traditional NLP models such as recurrent and convolutional neural networks demonstrated promising utility for patient-level predictions and health forecasting from longitudinal datasets. However, to our knowledge only few studies have explored transformers for predicting clinical outcomes from electronic health record (EHR) data, and in our estimation, none have adequately derived a health-specific tokenization scheme to fully capture the heterogeneity of EHR systems. In this study, we propose a dynamic method for tokenizing both discrete and continuous patient data, and present a transformer-based classifier utilizing a joint embedding space for integrating disparate temporal patient measurements. We demonstrate the feasibility of our clinical AI framework through multi-task ICU patient acuity estimation, where we simultaneously predict six mortality and readmission outcomes. Our longitudinal EHR tokenization and transformer modeling approaches resulted in more accurate predictions compared with baseline machine learning models, which suggest opportunities for future multimodal data integrations and algorithmic support tools using clinical transformer networks.",
  "full_text": "EDITED BY\nDaniel Donoho,\nChildren’s National Hospital, United States\nREVIEWED BY\nHani J. Marcus,\nUniversity College London, United Kingdom\nGuillaume Kugener,\nUniversity of Southern California, United States\nTiming Liu,\nUniversity of Cambridge, United Kingdom\n*\nCORRESPONDENCE\nParisa Rashidi\nparisa.rashidi@bme.uﬂ.edu\nSPECIALTY SECTION\nThis article was submitted to Personalized\nMedicine, a section of the journal Frontiers in\nDigital Health\nRECEIVED 26 August 2022\nACCEPTED 14 October 2022\nPUBLISHED 09 November 2022\nCITATION\nShickel B, Silva B, Ozrazgat-Baslanti T, Ren Y,\nKhezeli K, Guan Z, Tighe PJ, Bihorac A and\nRashidi P (2022) Multi-dimensional patient\nacuity estimation with longitudinal EHR\ntokenization andﬂexible transformer networks.\nFront. Digit. Health 4:1029191.\ndoi: 10.3389/fdgth.2022.1029191\nCOPYRIGHT\n© 2022 Shickel, Silva, Ozrazgat-Baslanti, Ren,\nKhezeli, Guan, Tighe, Bihorac and Rashidi. This\nis an open-access article distributed under the\nterms of theCreative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nMulti-dimensional patient acuity\nestimation with longitudinal EHR\ntokenization andﬂexible\ntransformer networks\nBenjamin Shickel\n1,4\n, Brandon Silva\n2,4\n, Tezcan Ozrazgat-Baslanti\n1,4\n,\nYuanfang Ren\n1,4\n,K i aK h e z e l i\n2,4\n, Ziyuan Guan\n1,4\n, Patrick J. Tighe\n3,4\n,\nAzra Bihorac\n1,4\nand Parisa Rashidi\n2,4*\n1Department of Medicine, University of Florida, Gainesville, FL, United States,2Department of\nBiomedical Engineering, University of Florida, Gainesville, FL, United States,3Department of\nAnesthesiology, University of Florida, Gainesville, FL, United States,4Intelligent Critical Care Center\n(IC3), University of Florida, Gainesville, FL, United States\nTransformer model architectures have revolutionized the natural language\nprocessing (NLP) domain and continue to produce state-of-the-art results in text-\nbased applications. Prior to the emergence of transformers, traditional NLP\nmodels such as recurrent and convoluti o n a ln e u r a ln e t w o r k sd e m o n s t r a t e d\npromising utility for patient-level predictions and health forecasting from\nlongitudinal datasets. However, to our knowledge only few studies have explored\ntransformers for predicting clinical outcomes from elec t r o n i ch e a l t hr e c o r d( E H R )\ndata, and in our estimation, none have adequately derived a health-speciﬁc\ntokenization scheme to fully capture theheterogeneity of EHR systems. In this\nstudy, we propose a dynamic method for tokenizing both discrete and continuous\npatient data, and present a transformer-based classiﬁer utilizing a joint embedding\nspace for integrating disparate temporalpatient measurements. We demonstrate\nthe feasibility of our clinical AI framework through multi-task ICU patient acuity\nestimation, where we simultaneously predict six mortality and readmission\noutcomes. Our longitudinal EHR tokenization and transformer modeling\napproaches resulted in more accurate predictions compared with baseline\nmachine learning models, which suggest opportunities for future multimodal data\nintegrations and algorithmic support tools using clinical transformer networks.\nKEYWORDS\ntransformer, deep learning, electronic health records, critical care, patient acuity,\nclinical decision support\n1. Introduction\nThrough the course of a typical intensive care unit (ICU) admission, a variety of\npatient-level data is collected and recorded into electronic health records (EHR)\nsystems. Patient data is diverse, including measurements such as vital signs, laboratory\ntests, medications, and clinician-judged assessment scores. While primarily used for ad-\nhoc clinical decision-making and administrative tasks such as billing, patient-centric\ndata can also be used to build automated machine learning systems for assessing overall\npatient health and predicting recovering or worsening patient trajectories.\nTYPE Original Research\nPUBLISHED 09 November 2022\n| DOI 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 01 frontiersin.org\nPatient mortality risk is often used as a proxy for overall ICU\npatient acuity, both in traditional illness severity scores like SOFA\n(1, 2) and more recent machine learning approaches such as\nDeepSOFA (3). Whether manually calculated or algorithmically\ncomputed, nearly all of these systems rely on measurements\nfrom a set of handpicked clinical descriptors thought to be\nmost indicative of overall patient health. Given the breadth of\ndata available in modern EHR systems, there is untapped\npotential for enhanced patient modeling contained in the large\namount of unused patient data.\nSeveral recent studies have demonstrated the predictive\naccuracy and patient modeling capacity of deep learning\nimplementations in healthcare, using models such as recurrent\nneural networks (RNN) ( 3– 8) and convolutional neural\nnetworks (CNN) (9, 10).\nRecently, Transformer models (11) have garnered increased\nattention in the deep learning community due to their state-of-\nthe-art results on a variety of natural language processing (NLP)\ntasks, particularly when using schemes such as Bidirectional\nEncoder Representations from Transformers (BERT) ( 12).\nThere are also more recent advances in analyzing frequency of\ndata in Frequency Enhanced Decomposed Transformer Zhou\net al. (13) that exploits the sparseness of time series data.\nFrom a temporal perspective, one advantage the Transformer\noffers is its parallel processing characteristics. Rather than\nprocessing data points sequentially, the Transformer views all\navailable data at once, modeling attention-based relationships\nbetween all input time steps. In contrast, models such as RNNs\nrequire distinct temporal separation within input sequences,\nand usually demand a regular sample interval between adjacent\ntime steps. As clinical EHR data is recorded at highly irregular\nfrequency and is often missing measurements, a large amount\nof data preprocessing is typically required in the form of\ntemporal resampling to a ﬁxed frequency, and an imputation\nscheme to replace missing values. Furthermore, given that\nseveral EHR measurements are often recorded at the same\ntimestamp, typical machine learning work ﬂows aggregate\ntemporally adjacent measurements into mean values contained\nin resampled time step windows, or perform random shufﬂing\nprocedures before training models. Given its parallel and\nfundamentally temporally agnostic attributes, the Transformer\nis capable of distinctly processing all available measurements,\neven those occurring at the same timestamp. Additionally, the\nTransformer is able to process whichever data happens to be\navailable, reducing the need for potentially bias-prone\ntechniques to account for data missingness.\nIn this study, we showcase the feasibility of a highlyﬂexible\nTransformer-based patient acuity prediction framework in the\ncritical care setting. Our contributions can be summarized by\nthe following:\n Our ﬂexible system design incorporates a diverse set of EHR\ninput data that does not require a priori identiﬁcation of\nclinically relevant input variables, and can work with any\ndata contained in EHR platforms.\n In contrast to recent Transformer approaches that either use\ndiscrete medical concepts ( 14– 16) or continuous\nmeasurements from a handpicked set of features (17), we\nintroduce a data embedding scheme that jointly captures\nboth concept and corresponding measurement values of a\nwide variety of disjoint clinical descriptors.\n In our novel embedding module, we introduce a mechanism\nfor combining both absolute and relative temporality as an\nimprovement over traditional positional encoding.\n We present an input data scheme with minimal\npreprocessing, obfuscating the need for potentially biased\ntemporal resampling or missing value imputation common\nin many other sequential machine learning approaches.\n We expand BERT’s [CLS] token for classiﬁcation into several\ndistinct tokens for predicting multiple-horizon patient\nmortality and ICU readmission in a novel multi-task\nlearning environment.\n Rather than typical concatenation with sequential\nrepresentation, we incorporate static patient information in\na novel way using a global self-attention token so that\nevery sequential time step is compared with the static pre-\nICU representation.\n We show that the Longformer (18) can be applied to long\nEHR patient data sequences to minimize required\ncomputation while retaining superior performance.\n2. Methods\n2.1. Cohort\nThe University of Florida Integrated Data Repository was\nused as an honest broker to build a single-center longitudinal\ndataset from a cohort of adult patients admitted to intensive\ncare units at University of Florida Health between January 1st,\n2012 and September 22nd, 2019. Our project was approved by\nthe Institutional Review Board of the University of Florida\nand the University of Florida Privacy Ofﬁce (IRB201901123).\nFull cohort statistics is described inTable 1.\nWe excluded ICU stays lasting less than 1 h (to reduce EHR\ndata artifacts and provide predictive models with adequate\npatient data) or more than 10 days, to limit outliers based on\ntokenized sequence length and following several existing\nstudies using ICU encounters for predictive modeling ( 19).\nExcluding patients based on length of stay resulted in roughly\n95% of the original ICU cohort. Ourﬁnal cohort consisted of\n73,190 distinct ICU stays from 69,295 hospital admissions and\n52,196 unique patients. The median length of stay in the ICU\nwas 2.7 days.\nWe divided our total cohort of ICU stays into a\ndevelopment cohort of 60,516 ICU stays (80%) for training\nour models, and a validation cohort of 12,674 ICU stays\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 02 frontiersin.org\n(20%) for evaluating their predictive performance. 10% of the\ndevelopment set was used for within-training validation and\nearly stopping. The cohort was split chronologically, where\nthe earliest 80% of ICU stays was used for training, and the\nmost recent 20% used for evaluation. To ensure the same\npatient did not appear in both development and validation\nsets, all ICU stays of patients with multiple admissions\nspanning the cohort threshold were grouped into the\ndevelopment cohort.\n2.2. Data\nWe extracted patient data from several EHR data sources:\nsociodemographics and information available upon hospital\nadmission, summarized patient history, vital signs, laboratory\ntests, medication administrations, and numerical assessments\nfrom a variety of bedside scoring systems. We did not target\nor manually select any speciﬁc ICU variables, instead using all\nsuch data contained in our EHR system. A full list of\nvariables used in our experiments is shown inTable 2.\nStatic data: For each ICU stay, we extracted a set of non-\nsequential clinical descriptors pertaining to patient\ncharacteristics, admission information, and a summarized\npatient history from the previous year. Patient-level features\nincluded several demographic indicators, comorbidities,\nadmission type, and neighborhood characteristics derived\nfrom the patient’s zip code. Patient history consisted of a\nvariety of medications and laboratory test results up to one\nyear prior to hospital admission (Table 2). Historical patient\nTABLE 1 Summary statistics for experimental ICU cohorts.\nDevelopment\ncohort\n(n = 60, 516)\nValidation\ncohort\n(n = 12, 674)\nPatients, n 41,881 10,315\nHospital encounters, n 57,168 12,127\nAge, years, median (25th, 75th) 61.0 (49.0, 71.0) 62.0 (49.0, 73.0)\nFemale, n (%) 27,380 (45.2) 5,616 (44.3)\nBody mass index, median (25th,\n75th)\n26.9 (23.0, 32.0) 27.3 (23.3, 32.2)\nHospital length of stay, days,\nmedian (25th, 75th)\n6.7 (3.6, 12.1) 6.4 (3.3, 11.5)\nICU length of stay, days, median\n(25th, 75th)\n2.8 (1.5, 5.1) 2.9 (1.6, 5.5)\nTime to hospital discharge, days,\nmedian (25th, 75th)\n1.9 (0.0, 4.8) 1.1 (0.0, 4.1)\nHispanic, n (%) 2,130 (3.5) 539 (4.3)\nNon-English speaking, n (%) 1,092 (1.8) 233 (1.8)\nMarital status, n (%)\nMarried 26,084 (43.1) 5,457 (43.1)\nSingle 21,844 (36.1) 4,931 (38.9)\nDivorced 11,905 (19.7) 2,142 (16.9)\nSmoking status, n (%)\nNever 20,180 (33.3) 4,653 (36.7)\nFormer 19,378 (32.0) 4,167 (32.9)\nCurrent 12,094 (20.0) 2,326 (18.4)\nInsurance status, n (%)\nMedicare 31,447 (52.0) 6,543 (51.6)\nPrivate 13,115 (21.7) 2,912 (23.0)\nMedicaid 10,208 (16.9) 1,999 (15.8)\nUninsured 5,746 (9.5) 1,220 (9.6)\nComorbidities, n (%)\nCharlson comorbidity index,\nmedian (25th, 75th)\n2.0 (0.0, 4.0) 2.0 (0.0, 4.0)\nMyocardial infarction 7,537 (12.5) 1,985 (15.7)\nCongestive heart failure 14,897 (24.6) 3,380 (26.7)\nPeripheral vascular disease 10,005 (16.5) 2,185 (17.2)\nCerebrovascular disease 8,981 (14.8) 1,720 (13.6)\nChronic pulmonary disease 17,938 (29.6) 3,473 (27.4)\nMetastatic carcinoma 3,377 (5.6) 812 (6.4)\nCancer 8202 (13.6) 1,808 (14.3)\nMild liver disease 4,745 (7.8) 960 (7.6)\nModerate/severe liver disease 1,856 (3.1) 374 (3.0)\nDiabetes without\ncomplications\n14,137 (23.4) 2,395 (18.9)\nDiabetes with complications 5,052 (8.3) 1,736 (13.7)\nAIDS 442 (0.7) 53 (0.4)\nDementia 1,692 (2.8) 559 (4.4)\nParaplegia/hemiplegia 3,465 (5.7) 769 (6.1)\nPeptic ulcer disease 1,110 (1.8) 187 (1.5)\nRenal disease 11,878 (19.6) 2,493 (19.7)\n(continued)\nTABLE 1 Continued\nDevelopment\ncohort\n(n = 60, 516)\nValidation\ncohort\n(n = 12, 674)\nRheumatologic disease 1,794 (3.0) 342 (2.7)\nNeighborhood characteristics,\nmedian (25th, 75th)\nTotal population, n /C2 103 17.0 (10.6, 26.4) 17.6 (10.6, 26.7)\nDistance to hospital, km 39.3 (17.9, 69.1) 42.4 (20.2, 76.5)\nMedian income, dollars/C2 103 40.1 (33.8, 46.7) 40.1 (35.1, 47.4)\nPoverty rate, % 19.6 (14.0, 27.7) 19.3 (13.7, 26.7)\nRural area, n 22543 (37.3) 4691 (37.0)\nClinical outcomes, n (%)\nICU readmission before\nhospital discharge\n3,583 (5.9) 613 (4.8)\nInpatient mortality 5,813 (9.6) 1,131 (8.9)\n7-day mortality 5,237 (8.7) 1,022 (8.1)\n30-day mortality 7,056 (11.7) 1,380 (10.9)\n90-day mortality 9,197 (15.2) 1,785 (14.1)\n1-year mortality 12,991 (21.5) 2,288 (18.1)\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 03 frontiersin.org\nTABLE 2 Summary of variables used in Transformer experiments.\nVariable Type\nPatient demographics\nAge Static\nSex Static\nEthnicity Static\nRace Static\nLanguage Static\nMarital status Static\nSmoking status Static\nInsurance provider Static\nPatient residential information\nTotal population Static\nDistance from hospital Static\nRural/Urban Static\nMedian income Static\nProportion black Static\nProportion hispanic Static\nPercent below poverty line Static\nPatient admission information\nHeight Static\nWeight Static\nBody mass index Static\n17 comorbidities present at Admission Static\nCharlson comorbidity index Static\nPresence of chronic kidney disease Static\nAdmission type Static\nPatient history: medications\na\nACE inhibitors Static\nAminoglycosides Static\nAntiemetics Static\nAspirin Static\nBeta blockers Static\nBicarbonates Static\nCorticosteroids Static\nDiuretics Static\nNSAIDS Static\nVasopressors/Inotropes Static\nStatins Static\nVancomycin Static\nNephrotoxic drugs Static\nPatient history: laboratory test results\nb\nSerum hemoglobin Static\nUrine hemoglobin Static\nSerum glucose Static\nUrine glucose Static\nUrine red blood cells Static\nUrine protein Static\nSerum urea nitrogen Static\n(continued)\nTABLE 2 Continued\nVariable Type\nSerum creatinine Static\nSerum calcium Static\nSerum sodium Static\nSerum potassium Static\nSerum chloride Static\nSerum carbon dioxide Static\nWhite blood cells Static\nMean corpuscular volume Static\nMean corpuscular hemoglobin Static\nHemoglobin concentration Static\nRed blood cell distribution Static\nPlatelets Static\nMean platelet volume Static\nSerum anion gap Static\nBlood pH Static\nSerum oxygen Static\nBicarbonate Static\nBase deﬁcit Static\nOxygen saturation Static\nBand count Static\nBilirubin Static\nC-reactive protein Static\nErythrocyte sedimentation rate Static\nLactate Static\nTroponin T/I Static\nAlbumin Static\nAlaninen Static\nAsparaten Static\nICU vital signs\nSystolic blood pressure\nc Temporal\nDiastolic blood pressurec Temporal\nMean arterial pressurec Temporal\nHeart rate Temporal\nRespiratory rate Temporal\nOxygen ﬂow rate Temporal\nFraction of inspired oxygen (FIO2) Temporal\nOxygen saturation (SPO2) Temporal\nEnd-tidal carbon dioxide (ETCO2) Temporal\nMinimum alveolar concentration (MAC) Temporal\nPositive end-expiratory pressure (PEEP) Temporal\nPeak inspiratory pressure (PIP) Temporal\nTidal volume Temporal\nTemperature Temporal\nICU Assessment Scores\nd\nASA physical status classiﬁcation Temporal\nBraden scale Temporal\n(continued)\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 04 frontiersin.org\nmeasurement features were derived from a set of statistical\nsummaries for each descriptor (minimum, maximum, mean,\nstandard deviation).\nTemporal data: For each ICU stay, we extracted all available\nvital signs, laboratory tests, medication administrations, and\nbedside assessment scores recorded in our EHR system while\nthe patient was in the ICU ( Table 2). We refer to each\nextracted measurement as a clinical event. Each event was\nrepresented as a vector containing the name of the\nmeasurement (e.g. “noninvasive systolic blood pressure”), the\nelapsed time from ICU admission, the current measured\nvalue, and eight cumulative value-derived features\ncorresponding to prior measurements of the same variable\nearlier in the ICU stay (mean, median, count, minimum,\nmaximum, standard deviation, ﬁrst value, elapsed time since\nmost recent measurement). For bedside assessment scores\nwith multiple sub-components, we treated each sub-\ncomponent as a distinct measurement. Invasive and\nnoninvasive measurements were treated as distinct tokens. We\nexcluded ICU stays with sequence lengths longer than 12,000\ntokens, and the resulting mean sequence length in our cohorts\nwas 1,996.\nData processing: Categorical features present in the pre-\nICU static data were converted to one-hot vectors and\nconcatenated with the remaining numerical features. Missing\nstatic features were imputed with training cohort medians, but\nno such imputation was required for the tokenized temporal\nICU data. Binary indicator masks were computed and\nconcatenated with static features to capture patterns of\nmissingness.\nStatic features were standardized to zero mean and unit\nvariance based on values from the training set. For each\nvariable name in the temporal ICU data, corresponding\ncontinuous measurement value features were individually\nstandardized in the same manner. ICU measurement\ntimestamps were converted to number of elapsed hours from\nICU admission, and were similarly standardized based on\ntraining cohort values.\nICU measurement names were converted to unique\ninteger identi ﬁers in a similar manner to standard\ntokenization mapping procedures in NLP applications. Each\ntemporal clinical event was also associated with an integer\nposition index. While similar to the positional formulations\nin NLP applications, we introduce one key distinction that\nis more suitable for Transformers based on EHR data: we\ndo not enforce the restriction that positional indices are\nunique, and if two clinical events occurred at the same EHR\ntimestamp, they are associated with the same sequential\nposition index.\nEach temporal measurement token consisted of integer\npositional identi ﬁer, integer variable identi ﬁer, continuous\nelapsed time from ICU admission, and eight continuous\nfeatures extracted from current and prior measurement values.\nFollowing data extraction and processing, each ICU stay was\nassociated with two sets of data: (1) a single vectorx\ns [ R718/C2 1\nof 718 static pre-ICU features, and (2) a matrix ofT temporal\nICU measurements xt [ RT/C2 12 including token position and\nidentiﬁer. Across our entire population, the temporal ICU\nmeasurements included 19 unique vital signs, 106 unique\nlaboratory tests, 345 unique medication administrations, and\n29 bedside assessment score components; however, each ICU\nstay only included a subset of such total variables, and its\ncorresponding temporal sequence only included what was\nmeasured during the corresponding ICU stay. One of the\nbeneﬁts of our proposed EHR embedding framework is the\nlack of resampling, propagation, imputation, or other such\ntemporal preprocessing typically performed in related\nsequential modeling tasks.\n2.3. Clinical outcomes\nFor each ICU stay, we sought to predict six clinical\noutcomes related to patient illness severity: ICU readmission\nwithin the same hospital encounter, inpatient mortality, 7-day\nmortality, 30-day mortality, 90-day mortality, and 1-year\nmortality. Our model is formulated as a multi-task design,\nand simultaneously estimates risk for all six clinical prediction\ntargets.\nTABLE 2 Continued\nVariable Type\nConfusion assessment method (CAM) Temporal\nModiﬁed early warning score (MEWS) Temporal\nMorse fall scale (MFS) Temporal\nPain score Temporal\nRichmond agitation-sedation scale (RASS) Temporal\nSequential organ failure assessment (SOFA) Temporal\nICU laboratory tests\ne\n106 distinct lab tests present in EHR system Temporal\nICU medicationse\n345 distinct medications present in EHR system Temporal\naExtracted features included total counts of administered medications up to\none year prior to hospital admission.\nbExtracted features included total counts of recorded laboratory test results\nand minimum, maximum, mean, and standard deviation of measurement\nvalues up to one year prior to hospital admission. Both serum and urine-\nbased tests extracted separately when available.\ncInvasive and non-invasive readings for systolic blood pressure, diastolic blood\npressure, and mean arterial pressure were treated as distinct event tokens.\ndFor assessment scores with multiple sub-components, each component was\ntreated as a distinct timestamped measurement, resulting in 30 such\nassessment measurements.\neWe retained distinct laboratory tests and medications that were administered\nin at least 1% of the training cohort of ICU stays.\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 05 frontiersin.org\n2.4. Model architecture\nThe primary driver behind our ICU patient acuity\nestimation model is the transformer encoder ( 11). Our\nmodiﬁed model utilizes the global and sliding window\nmechanism introduced by the Longformer ( 18) along with\nspecial classiﬁcation tokens from BERT (12). Figure 1 shows\na high-level overview of our Transformer architecture. Our\nlongitudinal tokenization pipeline and Transformer modeling\narchitecture code will be available upon request for interested\nresearchers.\nNovel embedding: In typical Transformer implementations,\none-dimensional input sequences consist of integer-identiﬁed\ntokens (such as textual tokens or discrete clinical concepts)\nthat are embedded using a lookup table, after which a\npositional encoding vector is added to inject local temporality.\nFor existing applications of Transformers with EHR data, the\nvalues of a given measurement are not factored into its\nrepresentation.\nOur embedding scheme introduces three novelties that offer\nimprovements for clinical prediction tasks. First, positional\nindices are derived from EHR record times and are not\nunique (see Section 2.2), allowing for multiple tokens to share\nthe same positional index and resulting positional encoding.\nRather than enforce an arbitrary sequence order or implement\na random shuf ﬂing procedure for simultaneous tokenized\nevents, this modi ﬁcation is more ﬂexible with respect to\nclinical workﬂows.\nSecond, in addition to novel framing of relative and local\ntemporal relationships through positional encoding\nmodiﬁcations, each clinical event token also explicitly includes\nabsolute temporality in the form of a feature indicating the\nelapsed hours from ICU admission. We hypothesized that the\ninjection of both relative and absolute temporality would\nallow the Transformer to better model patient trajectories.\nFinally, each clinical event in our tokenized input sequences\nconsists of several continuous measurement values in addition\nto the discrete token identi ﬁers (see Section 2.2). To our\nFIGURE 1\nOverview of our proposed generalized EHR Longformer network for simultaneously predicting multiple patient outcomes in the ICU. Pre-ICU\ninformation includes summarized history of patient medications and laboratory tests, sociodemographic indicators, and features relating to\nhospital admission. Temporal ICU measurements take the ﬂexible form of tuples: (p, non-unique positional index of clinical event based on\ntimestamp; t, elapsed time from ICU admission,f, unique measurement identiﬁer integer; ~v, set of continuous features derived from measured\nvalues). Task-speciﬁc [CLS] tokens are assignedt = time of prediction and~v ¼ 0. Tokens are individually embedded and passed through a stack\nof Longformer layers with sliding self-attention windows. Global attention is applied to static feature representation and prediction tokens. The\nconcatenation of each layer’s [CLS] representations are used for a given task to predict the desired mortality risk. Not shown: Transformer\nfeedforward network and nonlinear activations. FC: fully-connected layers.\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 06 frontiersin.org\nknowledge, no other work integrates both discrete and\ncontinuous data in this manner, with the majority of recent\nresearch opting for discrete medical codes only (Section 4.2).\nWe augment discrete variable tokens with continuous\nmeasurement values into our embedding to better capture\nrecovery or worsening trends as a patient progresses through\nan ICU stay.\nOur embedding module consists of (1) a traditional lookup\ntable used for measurement name identiﬁer, (2) a sinusoidal\npositional embedding table, and (3) a single fully-connected\nlayer for embedding absolute time and value-derived features.\nThe ﬁnal sequence embedding is the summation of three\nembedded vectors: (1) the embedding of absolute time with\ncorresponding cumulative values, (2) the measurement token\nidentiﬁer embedding, and (3) a traditional sinusoidal\npositional encoding. In our implementation, the sinusoidal\npositional encoding is based on the position of unique\nmeasurement times in the input sequence: for an example\nsequence of measurement hours [0 :1, 0:2, 0:2, 0:3, 0:3], the\npositional indices are computed as [0, 1, 1, 2, 2].\nNovel multi-task global tokens : In the original BERT\nimplementation, a single special [CLS] token is prepended to\ninput sequences that is meant to capture a global\nrepresentation of the entire sequence. We extend this notion\nby prepending each sequence with 6 such special tokens: one\nfor each of our clinical outcomes. As each token in our data\nscheme consists of a (time, name, values) 12-tuple, we set\ntime of each [CLS] token equal to the total ICU length of stay\nand all values equal to zero. The special token identiﬁers are\nembedded in a similar fashion to other ICU measurement\ntokens. In our experiments, we include an additional\nprediction target for long-term hospital readmission that is\nused for regularization, but not included in our patient acuity\nestimation. In the Longformer implementation in our\nencoder, we set each of the multi-task tokens to compute\nglobal attention, so that self-attentions are computed among\nall sequence elements for each clinical outcome token.\nNovel inclusion of static patient data: In many sequential\nmodels for clinical prediction, aﬁnal encounter representation\nis obtained by concatenating the pre-sequence static patient\nrepresentation with the sequential representation. In our\nwork, we prepend each ICU sequence with the representation\nobtained from passing the static patient information vector\nthrough a fully-connected network. We assign this static\ntoken as global, so that every time step computes attention\nwith the static data. We hypothesized that this more ﬁne-\ngrained injection of patient information at every time step\nwould improve the capacity of our model to learn important\nand more personalized patient trajectory patterns.\nModel details: Our ﬁnal model consisted of an embedding\nlayer, followed by 8 Longformer layers, and a separate linear\nprediction layer for each of our 6 clinical outcomes. For\nmaking a task-speciﬁc prediction, the task-speciﬁc linear layer\nuses the concatenation of representations corresponding to its\nspecial [CLS] token at each of the 8 layers. In our initial\nLongformer implementation, we used a hidden size of 128, a\nfeedforward size of 512, 8 attention heads, a sliding window\nof size 128, dropout of 0.1, and a batch size of 21.\nHyperparameters were chosen with respect to hardware\nconstraints; hyperparameter optimization will be a focus of\nfuture work.\nExperiment details : Models were trained using a\ndevelopment set of 60,516 ICU stays corresponding to 80% of\nour total ICU cohort. 10% of this development set was used\nfor early stopping based on the mean AUROC among all six\nclinical outcomes and a patience of four epochs. All\nexperiments were conducted on a local Linux server equipped\nwith two i7-7820X 3.6 GHz CPUs, 3 NVIDIA GeForce RTX\n2080Ti GPUs, 512GB SSD storage, and 128GB RAM. Models\nwere designed and run using the PyTorch and Hugging Face\nPython libraries.\nIn this feasibility study, we compared performance against\nsix other ICU prediction models:\n Longformer using tokenized data sequences with only\ndiscrete code identiﬁers. In this variant of our proposed\nframework, we do not include the continuous\nmeasurement values in the representation of each event\ntoken.\n Recurrent neural network (RNN) with gated recurrent units\n(GRU) using continuous multivariate time series inputs. In\nthis experiment, the ﬂexibility of our tokenization scheme\nis removed, and more traditional “tabularized” input data\nsequences were constructed where each variable is assigned\na distinct column. Sequences were constructed with\ncontinuous current values and resampled to 1-hour\nfrequency to align with common practice found in\nliterature. Multi-task predictions were drawn from theﬁnal\nhidden state of the GRU encoder. Static patient\ninformation was concatenated with the sequence\nrepresentation and fed through fully-connected layers\nbefore classiﬁcation.\n GRU with attention mechanism. This variant is identical to\nthe above, but with the addition of a simple attention\nmechanism over the hidden states of the GRU. States are\nweighted by alignment scores and summed to yield aﬁnal\nattention-based sequential representation.\n Tokenized GRU with attention. In this ﬁnal experimental\nsetting, we used the same novel EHR embedding and\ntokenization approach as with our Transformer model\narchitecture (see Section 2.2), but instead use a GRU with\nattention mechanism in place of the Transformer model.\n CatBoost (20) gradient boosting algorithm. The algorithm\nemploys gradient boosting on decision trees for both\nregression and classi ﬁcation tasks. Gradient boosting\nalgorithms have shown beneﬁts over random forests and\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 07 frontiersin.org\nrequire comparatively less hyperparameter tuning for\noptimal performance. For this experiment, the embedding\nlayers are removed and the CatBoost model is trained on\nsamples containing both the pre-ICU information and\nconcatenated ICU measurements.\n XGBoost (21) gradient boosting algorithm. This experiment\nand associated data processing is identical to CatBoost,\nexcept an XGBoost model is used for prediction.\n3. Results\nAt present time, the primary aim of our novel mortality\nprediction model is not to show state-of-the-art improvements\nin model accuracy; rather, we present this work as a feasibility\nstudy for future research. We believe our novel modiﬁcations\nof existing Transformer architectures for use in clinical EHR\napplications will result in highly ﬂexible and more\npersonalized patient representations and predictions across a\nvariety of clinical tasks.\nIn thisﬁrst iteration of our experiments, we did not perform\nany hyperparameter optimization, instead choosing sensible\nsettings that both highlight the novel aspects of the\narchitecture and work with our hardware constraints. In\npassing, we note that often parameter tuning is an essential\ncomponent of enhancing performance, and future iterations\nof this work will focus on optimizing crucial parameters such\nas learning rate, dropout, number of self-attention heads,\nnumber of self-attention layers, hidden dimension, and size of\nthe sliding self-attention window.\nOur results are shown in Table 3 . Our Transformer\narchitecture with novel EHR embedding and tokenization\nscheme yielded slightly superior mean AUROC (0.929) across\nall six clinical prediction tasks, with individual task AUROC\nranging from 0.843 (ICU readmission) to 0.983 (7-day\nmortality). The Transformer using tokenized embeddings that\nomit continuous measurement values resulted in the lowest\nmean AUROC (0.773) and worst performance across most of\nthe clinical outcomes, ranging from 0.512 (ICU readmission)\nto 0.900 (7-day mortality). It outperformed the XGBoost\nmodel for inpatient and 7-day mortality.\nIn terms of GRU baseline models, the traditional model and\ndata processing scheme resulted in the lowest baseline accuracy,\nwith mean AUROC of 0.900 and task AUROC ranging from\n0.750 (ICU readmission) to 0.972 (7-day mortality). The\naugmentation of this model and data scheme with traditional\nattention mechanism improved the performance to a mean\nAUROC of 0.909.\nThe best GRU baseline model used our novel EHR\nembedding, tokenization, and representation pipeline. This\nmodel yielded a mean AUROC of 0.927 with individual task\nAUROC ranging from 0.831 to 0.982. It performed best for\npredicting 30-day mortality and 90-day mortality, although\nthe relative difference compared with the transformer is\nminimal. For the gradient boosting algorithms, CatBoost\noutperformed XGBoost across all outcomes (mean AUROC:\n0.863 vs. 0.836) except for predicting ICU readmission\n(AUROC: 0.759 vs. 0.762). The CatBoost model performed\nsimilarly to the baseline GRU model for all other outcomes.\nThe tree-based models were predominantly outperformed by\nGRU models with attention.\nAcross all models and data representation schema, ICU\nreadmission proved the most dif ﬁcult task. Among the\nmultiple prediction horizons for patient mortality, models\nwere best able to predict 7-day mortality, followed by\ninpatient mortality, 30-day mortality, 90-day mortality, and 1-\nyear mortality.\n4. Discussion\n4.1. Principalﬁndings\nThis work presents a novel ICU acuity estimation model\ninspired by recent breakthroughs in Transformer\narchitectures. Our proposed model framework incorporates\nseveral novel modi ﬁcations to the existing Transformer\narchitecture that make it more suitable for processing EHR\nTABLE 3 Multi-task prediction results expressed as area under the receiver operating characteristic curve (AUROC).\nModel Data Mean Readmission Mortality\nICU Inpatient 7-\nDay\n30-\nDay\n90-\nDay\n1-\nYear\nTransformer Tokenized events (discrete only) 0.773 0.512 0.889 0.900 0.831 0.777 0.727\nTransformer Tokenized events + continuous measurement values 0.929 0.843 0.978 0.983 0.953 0.923 0.892\nGRU Resampled multivariate time series 0.900 0.750 0.960 0.972 0.938 0.907 0.872\nGRU with attention Resampled multivariate time series 0.909 0.770 0.965 0.975 0.946 0.914 0.882\nGRU with attention Tokenized events + continuous measurement values 0.927 0.831 0.977 0.982 0.954 0.925 0.891\nCatBoost Tokenized events + continuous measurement values 0.863 0.759 0.901 0.915 0.890 0.868 0.847\nXGBoost Tokenized events + continuous measurement values 0.836 0.762 0.867 0.878 0.859 0.833 0.817\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 08 frontiersin.org\ndata of varying modalities. Through initial feasibility\nexperiments, our model was on par with, or outperformed,\ncommon variants of RNN baselines, and we feel our approach\nholds promise for incorporating additional EHR-related\noutcome prediction tasks and additional sources of EHR input\ndata.\nOne of the advantages of our work is that input elements are\ntreated as distinct. For example, if heart rate, respiratory rate,\nand SPO2 were recorded at the same timestamp in an EHR\nsystem, our framework operates on these individual elements,\nrather than combining them into a single aggregated time step\nas in similar RNN or CNN-based work. From an\ninterpretability standpoint, combined with the inherent self-\nattention mechanisms of the Transformer, isolation of inputs\nallows for improved clarity with respect to important or\ncontributing clinical factors. While one area of recent\nsequential interpretability research involves multivariate\nattribution for aggregated time steps ( 5, 22), Transformer-\nbased approaches such as ours obfuscate the need for\nmultivariate attribution, as attentional alignment scores are\nassigned to individual measurements. This property highlights\nthe potential for EHR Transformers to shed increased\ntransparency and understanding for clinical prediction tasks\nbuilt upon complex human physiology.\nFurthermore, while many sequential applications of deep\nlearning to EHR (including recent implementations of\nTransformer techniques) make use only of discrete clinical\nconcepts, our proposed framework extends the\nrepresentational capacity by integrating continuous\nmeasurement values alongside these discrete codes and events.\nThe inclusion of continuous measurement values represents\nan important step forward, as the measured result of a clinical\ntest or assessment can provide crucial information alongside a\nsimple presence indicator that can help complex models\ndevelop a better understanding of patient state and overall\nhealth trajectory.\nGiven the ﬂexible nature of our Transformer framework,\neach patient input sequence only contains the measurements\nthat were made during the ICU encounter. The advantages\nfor EHR applications are twofold. First, in traditional RNN or\nCNN-based work, the distance between time steps is assumed\nto be ﬁxed, and this is typically achieved by resampling input\nsequences to a ﬁxed frequency by aggregating measurements\nwithin resampled windows, and propagating or imputing\nvalues into windows without present values. Such a scheme\nhas the potential for introducing bias, and when using our\nnovel EHR embedding paradigm and Transformer-based\nmodeling approach, the problem of missing values is made\nredundant given the explicit integration of both absolute and\nrelative temporality for each irregularly measured clinical\nevent. Additionally, in typical deep sequential applications\nusing EHR data, the number of input features at each time\nstep must be constant. This is achieved by an a priori\nidentiﬁcation and extraction of a subset of clinical descriptors\nthought to be relevant indicators for a given prediction task.\nAs we have shown, when using a Transformer-based\napproach with our ﬂexible tokenization scheme, any and all\nEHR measurements can be easily incorporated into the\nprediction framework, even when some types do not exist for\na given patient or ICU encounter, and do not necessitate bias-\nprone imputation techniques.\nWhile the Transformer offers several beneﬁts over existing\nsequential deep learning models such as the RNN, it is not\nwithout drawbacks. Because the self-attention mechanism is\nhighly parallelizable and does not require step-wise iterative\nprocessing of a sequence (unlike the RNN), there is a tradeoff\nbetween faster computation and a much larger memory\nfootprint (complexity O(n\n2) without scope modiﬁcations). As\nsuch, Transformers may be infeasible to implement in\ntraining environments with limited computational resources.\nIn our approach, we introduced a novel method for\nincorporating static, pre-sequential patient information and\npatient history into the overall prediction model. Typically,\nsuch static information is concatenated with aﬁnal sequential\nrepresentation before making a prediction. We instead include\nstatic information as a distinct token in the input sequence,\nand assign global attention using the Longformer self-\nattention patterns. In effect, static patient-level information is\ninjected into the self-attention representation of every ICU\nmeasurement, allowing more ﬁne-grained and personalized\nincorporation of changes in overall patient health trajectories.\nAnother novel contribution we feel can be applied to even\nnon-EHR tasks is the expansion of the special BERT\nclassiﬁcation token into a separate token per classi ﬁcation\ntarget in a multi-task prediction setting. Given the global self-\nattention patterns between all task tokens and every sequential\ninput element, such a scheme allows the model to develop\ntask-speciﬁc data representations that can additionally learn\nfrom each other.\nAs with other retrospective machine learning models for\npredicting patient outcomes from longitudinal data, our\ntransformer framework offers the potential for augmenting\nclinical decision-making with dynamic data-driven risk\nestimations that can be used to help forecast patient trajectory\nand guide treatment and care strategies. Intended not to\nmandate particular course of action, tools such as ours can\ncomplement existing standards of care and provide clinicians\nwith additional support.\n4.2. Related work\n4.2.1. Transformer models\nFirst introduced by Vaswani et al. ( 11) for machine\ntranslation tasks, the Transformer is a deep learning\narchitecture built upon layers of self-attention mechanisms.\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 09 frontiersin.org\nThe Transformer views attention as a function of keys K,\nqueries Q, and values V. In the work of Vaswani et al. (11),\nall three elements came from the same input sequence, and is\nwhy their style of attention is referred to as self-attention. In\na similar manner to previously described works, compatibility\nbetween a key and query is used to weight the value, and in\nthe case of self-attention, each element of an input sequence\nis represented as a contextual sum of the alignment between\nitself and every other element. Similar to the memory\nnetworks of Sukhbaatar and Szlam (23), the Transformer also\ninvolves the addition of a positional encoding vector to\npreserve relative order information between input tokens.\nAn end-to-end Transformer architecture typically includes\nboth an encoder and decoder component. While critical for\nmany NLP tasks such as machine translation, our architecture\nutilizes only the Transformer encoder, which encodes input\nsequences into hidden representations that are subsequently\nused for predicting patient mortality.\nA comprehensive overview of the Transformer and BERT is\nbeyond the scope of this section; we refer interested readers to\nVaswani et al. (11) and Devlin et al. (12), respectively.\nBrieﬂy, the ﬁrst stage of a Transformer encoder typically\nincludes an embedding component, where each input sequence\nelement is converted to a hidden representation that is fed into\nthe remainder of the model. In its original NLP-centered\ndesign where inputs are sequences of textual tokens, a\ntraditional embedding lookup table is employed to convert\nsuch tokens into continuous representations. Unlike similar\nsequential models like RNNs or CNNs, the Transformer is\nfundamentally temporally agnostic and processes all tokens\nsimultaneously rather than sequentially. As such, the\nTransformer embedding module must inject some notion of\ntemporality into its element embeddings. In typical\nTransformer implementations, this takes the form of a\npositional encoding vector, where the position of each element\nis embedded by sinusoidal lookup tables, which is subsequently\nadded to the token embeddings. The primary aim of such\npositional embeddings is to allow the model to understand\nlocal temporality between nearby sequence elements.\nAt each layer of a Transformer encoder, a representation of\nevery input sequence element is formed by summing self-\nattention compatibility scores between the element and every\nother element in the sequence. Typical with other deep\nlearning architectures, as more layers are added to the\nencoder, the representations become more abstract.\nThe recent NLP method BERT ( 12) is based on\nTransformers, and at present time represent state of the art in\na variety of natural language processing tasks. In addition to\nits novel pretraining scheme, BERT also prepends input\nsequences with a special [CLS] token before a sequence is\npassed through the model. The goal of this special token is to\ncapture the combined representation of the entire sequence,\nand for classiﬁcation tasks is used for making predictions.\nTransformers are also being used in computer vision as well,\nwith great success. For example, videos especially beneﬁt from\nTransformers which can learn the temporal and spatial\nfeatures of vision data. They have shown to before the same\nor better for vision tasks, while also reducing vision-speciﬁc\ninduction bias Han et al. (24). For video data, they can be\nused for trajectory tracking of objects like balls Patrick et al.\n(25) using attention on objects in images, as well as\napproximate self attention to reduce quadratic dependency.\nWhile the Transformer is in one sense more efﬁcient than\nits sequential counterparts due to its ability to parallelize\ncomputations at each layer, one of the main drawbacks is its\nrequired memory consumption. Since each input element of a\nsequence of length n must be compared with every other\ninput element in the sequence, typical Transformer\nimplementations require memory on the order of O(n\n2).\nWhile acceptable for relatively short sequences, the memory\nconsumption quickly becomes problematic for very long\nsequences. Decreasing the memory requirement of\nTransformers is an area of ongoing research.\nOne potential solution was proposed by Beltagy et al. (18)i n\ntheir Longformer architecture. Rather than computing fulln\n2\nself-attentions, they propose a sliding self-attention window of\nspeciﬁed width, where each input sequence element is\ncompared only with neighboring sequence elements within\nthe window. They extend this to include user-speciﬁed global\nattention patterns (such as on the special [CLS] tokens for\nclassiﬁcation) that are always compared with every element in\nthe sequence. Through several NLP experiments, they\ndemonstrate the promising ability of the Longformer to\napproximate results from a full Transformer model.\n4.2.2. Transformers in healthcare\nGiven the similarity between textual sequences and\ntemporal patient data contained in longitudinal EHR records,\nseveral works have begun exploring the ef ﬁcacy of\nTransformers and modi ﬁcations of BERT for clinical\napplications using electronic health records. In terms of\npatient data modalities, existing implementations of\nTransformers in a clinical setting tend to fall under three\nprimary categories:\nPerhaps the most aligned with the original BERT\nimplementation, several studies adapt and modify BERT for\nconstructing language models from unstructured text\ncontained in clinical notes. The ClinicalBERT framework of\nHuang et al. (26) used a BERT model for learning continuous\nrepresentations of clinical notes for predicting 30-day hospital\nreadmission. Zhang et al. (27) pretrained a BERT model on\nclinical notes to characterize inherent bias and fairness in\nclinical language models.\nSong et al. (17)’s SAnD architecture developed Transformer\nmodels for several clinical prediction tasks using continuous\nmultivariate clinical time series.\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 10 frontiersin.org\nThe majority of existing EHR Transformer research has\nfocused on temporal sequences of discrete EHR billing codes.\nLi et al. ( 16)’s BEHRT framework modi ﬁed the BERT\nparadigm for predicting future disease from diagnosis codes.\nMed-BERT (15) demonstrated the performance advantages of\na contextualized clinical pretraining scheme in conjunction\nwith a BERT modi ﬁcation. RAPT ( 28) used a modi ﬁed\nTransformer pretraining scheme to overcome several\nchallenges with sparse EHR data. SETOR (29) utilized neural\nordinary differential equations with medical ontologies to\nconstruct a Transformer model for predicting future\ndiagnoses. RareBERT (30) extends Med-BERT for diagnosis of\nrare diseases. Meng et al. ( 31) used Transformers for\npredicting depression from EHR. Hi-BEHRT ( 16) extends\nBEHRT using a hierarchical design to expand the receptive\nﬁeld to capture longer patient sequences. Choi et al. (32) and\nShang et al. ( 33)’s G-BERT architecture capitalize on the\ninherent ontological EHR structure.\nIn contrast to the isolated data modalities implemented in\nexisting EHR Transformers, the novel embedding scheme\nutilized in our models combines both discrete and continuous\npatient data to generate a comprehensive representation of\ndistinct clinical events and measurements.\n4.5. Limitations\nThis feasibility study has several limitations and is intended\nas a methodological guiding framework for future multimodal\nand multi-task EHR Transformer research. Our retrospective\ndataset is limited to patients from a single-center cohort.\nFuture work will evaluate performance in external validation\ncohorts such as MIMIC-IV (34). We also present results with\nparameters that maximize our limited hardware capacity;\nfuture work will focus on several hyperparameter tuning and\nmodel selection procedures. The baseline models we present\nfor comparison are drawn from simpli ﬁed implementations\nfound in clinical deep learning research, and more recent\napproaches may offer enhanced predictive performance. From\nthe results in Table 3, one might conclude that our EHR\nembedding procedure had a larger impact than use of the\nTransformer architecture, given the competitive AUROC of\nthe attentional GRU baseline when implementing our\ntokenization pipeline for estimating risk of patient mortality.\nFuture work will focus on disentangling the relative impacts\nof both model and data representation designs.\n4.6. Conclusions and next steps\nWe feel there is still great potential for exploring additional\nbeneﬁts of our approach with diverse EHR data for a variety of\nclinical modeling and prediction tasks, especially in the realm of\nclinical interpretability. Given our promising pilot study results,\nfuture versions of this work will perform hyperparameter\noptimization with a focus on maximizing predictive accuracy.\nAdditionally, since transformers are fundamentally composed\nof attention mechanisms, they can be analyzed with respect to\nparticular outcomes, time points, or variables of interest to\nhighlight important contributing factors to overall risk\nestimation. Future research will emphasize analyzing self-\nattention distributions between input variables and clinical\noutcomes to further the clinical explainability and enhance\nthe clinical trust of Transformers in healthcare. We believe\nthere is great potential for multimodal patient monitoring\nusing ﬂexible EHR frameworks such as ours. Future research\nwill also focus on augmenting our multi-modal datasets with\nadditional clinical data modalities such as clinical text and\nimages, and pre-training our Transformer architectures with\nself-supervised prediction schemes across a variety of input\ndata and clinical outcomes.\nData availability statement\nThe data analyzed in this study is subject to the following\nlicenses/restrictions: UFHealth cohort data are available from\nthe University of Florida Institutional Data Access/Ethics\nCommittee for researchers who meet the criteria for access to\nconﬁdential data and may require additional IRB approval.\nRequests to access these datasets should be directed to https://\nidr.ufhealth.org.\nAuthor’s contributions\nDr. Shickel conceived the model design, performed data\nextraction and processing, developed the data embedding\npipeline and Transformer prediction framework, and\nperformed Transformer experiments. Mr. Silva validated\nmodel performance, tuned hyperparameters, and trained\nbaseline models for comparison with deep learning\napproaches. Mr. Khezeli, Dr. Tighe, and Dr. Bihorac reviewed\nstudy and manuscript for scienti ﬁc accuracy. Dr. Rashidi\nconceptualized the study design and provided support and\nguidance. Dr. Shickel, Mr. Silva, Dr. Bihorac, and Dr. Rashidi\nhad full access to the data in the study and take responsibility\nfor the integrity of the data and accuracy of the data analysis.\nAdministrative, technical, material support, and study\nsupervision, was provided by Dr. Bihorac and Dr. Rashidi. All\nauthors contributed to the acquisition, analysis, and\ninterpretation of data. All authors contributed to critical\nrevision of the manuscript for important intellectual content.\nAll authors contributed to the article and approved the\nsubmitted version.\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 11 frontiersin.org\nFunding\nBS was supported by R01GM110240 from the National\nInstitute of General Medical Sciences (NIH/NIGMS) and\nOT2OD032701 from the NIH Ofﬁce of the Director (NIH/\nOD). TO-B was supported by K01DK120784, R01DK123078,\nand R01DK121730 from the National Institute of Diabetes\nand Digestive and Kidney Diseases (NIH/NIDDK),\nR01GM110240 from the National Institute of General Medical\nSciences (NIH/NIGMS), R01EB029699 from the National\nInstitute of Biomedical Imaging and Bioengineering (NIH/\nNIBIB), R01NS120924 from the National Institute of\nNeurological Disorders and Stroke (NIH/NINDS),\nOT2OD032701 from the NIH Ofﬁce of the Director (NIH/\nOD), and UF Research AWD09459, and the Gatorade Trust,\nUniversity of Florida. AB was supported by R01GM110240\nfrom the National Institute of General Medical Sciences\n(NIH/NIGMS), R01EB029699 and R21EB027344 from the\nNational Institute of Biomedical Imaging and Bioengineering\n(NIH/NIBIB), R01NS120924 from the National Institute of\nNeurological Disorders and Stroke (NIH/NINDS),\nR01DK121730 from the National Institute of Diabetes and\nDigestive and Kidney Diseases (NIH/NIDDK), and\nOT2OD032701 from the NIH Ofﬁce of the Director (NIH/\nOD). PR was supported by National Science Foundation\nCAREER award 1750192, R01EB029699 and R21EB027344\nfrom the National Institute of Biomedical Imaging and\nBioengineering (NIH/NIBIB), R01GM110240 from the\nNational Institute of General Medical Science (NIH/NIGMS),\nR01NS120924 from the National Institute of Neurological\nDisorders and Stroke (NIH/NINDS), R01DK121730 from the\nNational Institute of Diabetes and Digestive and Kidney\nDiseases (NIH/NIDDK), and OT2OD032701 from the NIH\nOfﬁce of the Director (NIH/OD). Additionally, the\nResearch reported in this publication was supported by the\nNational Center for Advancing Translational Sciences of\nthe National Institutes of Health under University\nof Florida Clinical and Translational Science Awards\nUL1TR000064 and UL1TR001427.\nAcknowledgments\nWe acknowledge the University of Florida Integrated Data\nRepository (IDR) and the UF Health Ofﬁce of the Chief Data\nOfﬁcer for providing the analytic data set for this project. We\nthank the NVIDIA Corporation for their support through the\nAcademic Hardware Grant Program.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their\nafﬁliated organizations, or those of the publisher, the editors\nand the reviewers. Any product that may be evaluated in this\narticle, or claim that may be made by its manufacturer, is not\nguaranteed or endorsed by the publisher.\nReferences\n1. Vincent JL, Moreno R, Takala J, Willatts S, De Mendonca A, Bruining H,\net al. The SOFA (Sepsis-related Organ Failure Assessment) score to describe\norgan dysfunction/failure. On behalf of the working group on sepsis-related\nproblems of the european society of intensive care medicine. Intensive Care\nMed (1996) 22:707– 10. doi: 10.1007/BF01709751\n2. Vincent JL, de Mendonca A, Cantraine F, Moreno R, Takala J, Suter PM,\net al., Use of the SOFA score to assess the incidence of organ dysfunction/\nfailure in intensive care units.Crit Care Med(1998) 26:1793– 800. doi: 10.1097/\n00003246-199811000-00016\n3. Shickel B, Loftus TJ, Adhikari L, Ozrazgat-Baslanti T, Bihorac A, Rashidi P.\nDeepSOFA: a continuous acuity score for critically ill patients using clinically\ninterpretable deep learning.Sci Rep(2019) 9:1879. doi: 10.1038/s41598-019-38491-0\n4. Choi E, Bahadori MT, Schuetz A, Stewart WF, Sun J. Doctor AI:\npredicting clinical events via recurrent neural networks. Proceedings of\nMachine Learning for Healthcare 2016 JMLR W&C Track 56 .B o s t o n ,M A :\nProceedings of Machine Learning Research (2015). p. 1 – 1 2 .d o i :1 0 . 1 0 0 2 /\naur.1474.Replication\n5. Choi E, Bahadori MT, Schuetz A, Stewart WF, Sun J. RETAIN: interpretable\npredictive model in healthcare using reverse time attention mechanism. In:\nProceedings of the 30th International Conference on Neural Information\nProcessing Systems. Red Hook, NY: Curran Associates, Inc. (2016). p. 3512– 3520.\n6. Choi E, Schuetz A, Stewart WF, Sun J. Using recurrent neural network models\nfor early detection of heart failure onset. J Am Med Inform Assoc (2016)\n292:344– 50. doi: 10.1093/jamia/ocw112\n7. Sha Y, Wang MD. Interpretable predictions of clinical outcomes with an\nattention- based recurrent neural network. In: Proceedings of the 8th ACM\nInternational Conference on Bioinformatics, Computational Biology,, Health\nInformatics. New York, NY: Association for Computing Machinery (2017).\np. 233– 240\n8. Lipton ZC, Kale DC, Elkan C, Wetzell R. Learning to diagnose with LSTM\nrecurrent neural networks. In: 4th International Conference on Learning\nRepresentations. San Juan, Puerto Rico (2016).\n9. Lin L, Xu B, Wu W, Richardson T, Bernal EA. Medical time series classiﬁcation\nwith hierarchical attention-based temporal convolutional networks: a case study of\nmyotonic dystrophy diagnosis. In:CVPR Workshops. New York, NY: Institute for\nElectrical and Electronics Engineers (2019). p. 83– 86.\n10. Nguyen P, Tran T, Wickramasinghe N, Venkatesh S. Deepr: a convolutional\nnet for medical records (2016). p. 1– 9.\n11. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need. Adv Neural Inf Process Syst (2017) 30:5998– 6008.\ndoi: 10.1017/S0952523813000308\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 12 frontiersin.org\n12. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep\nbidirectional transformers for language understanding [Preprint] (2018).\nAvailable at: arXiv:1811.03600v2.\n13. Zhou T, Ma Z, Wen Q, Wang X, Sun L, Jin R. Fedformer: frequency\nenhanced decomposed transformer for long-term series forecasting. CoRR\n(2022). Available at: arXiv:abs/2201.12740.\n14. Li Y, Rao S, Roberto J, Solares A, Hassaine A, Ramakrishnan R, et al.,\nBEHRT: transformer for electronic health records. Sci Rep (2020) 10:1– 12.\ndoi: 10.1038/s41598-020-62922-y\n15. Rasmy L. Med-BERT: pretrained contextualized embeddings on large- scale\nstructured electronic health records for disease prediction.NPJ Digit Med(2021)\n4:1– 13. doi: 10.1038/s41746-021-00455-y\n16. Li Y, Mamouei M, Salimi-khorshidi G, Rao S, Hassaine A, Canoy D, et al. Hi-\nBEHRT: hierarchical transformer-based model for accurate prediction of clinical\nevents using multimodal longitudinal electronic health records.arXiv (2021).\n17. Song H, Rajan D, Thiagarajan JJ, Spanias A. Attend, diagnose: clinical time\nseries analysis using attention models. In: Thirty-second AAAI Conference on\nArtiﬁcial Intelligence. Red Hook, NY: Curran Associates, Inc. (2018).\n18. Beltagy I, Peters ME, Cohan A. Longformer: the long-document\ntransformer. arXiv (2020).\n19. Meng C, Trinh L, Xu N, Enouen J, Liu Y. Interpretability, fairness evaluation\nof deep learning models on MIMIC-IV dataset.Sci Rep (2022) 12:1– 28. doi: 10.\n1038/s41598-022-11012-2\n20. Dorogush AV, Gulin A, Gusev G, Kazeev N, Prokhorenkova LO, Vorobev A.\nFighting biases with dynamic boosting.CoRR (2017). Available at: abs/1706.09516.\n21. Chen T, Guestrin C. XGBoost: a scalable tree boosting system.CoRR (2016).\nAvailable at: abs/1603.02754.\n22. Qin Y, Song D, Cheng H, Cheng W, Jiang G, Cottrell GW. A dual-stage\nattention-based recurrent neural network for time series prediction.International\nJoint Conference on Arti ﬁcial Intelligence (IJCAI) . Red Hook, NY: Curran\nAssociates, Inc. (2017). p. 2627– 2633.\n23. Sukhbaatar S, Szlam A. End-to-end memory networks. In: Advances in\nNeural Information Processing Systems. Red Hook, NY: Curran Associates, Inc.\n(2015). p. 2440– 2448.\n2 4 .H a nK ,W a n gY ,C h e nH ,C h e nX ,G u oJ ,L i uZ ,e ta l .As u r v e yo n\nvision transformer. IEEE Trans Pattern Anal Mach Intell (2022): 1 – 1.\ndoi: 10.1109/TPAMI.2022.3152247. https: //ieeexplore.ieee.org/document/\n9716741\n25. Patrick M, Campbell D, Asano Y, Misra I, Metze F, Feichtenhofer C, et al.\nKeeping your eye on the ball: trajectory attention in video transformers. In:\nRanzato M, Beygelzimer A, Dauphin Y, Liang P, Vaughan JW, editors.\nAdvances in Neural Information Processing Systems. Vol. 34. Curran Associates,\nInc. (2021). p. 12493– 12506.\n26. Huang K, Altosaar J, Ranganath R. ClinicalBERT: modeling clinical notes\nand predicting hospital readmission.arXiv (2019).\n27. Zhang H, Lu AX, Mcdermott M. HurtfulWords: quantifying biases in\nclinical contextual word embeddings. In:Proceedings of the ACM Conference on\nHealth, Inference, and Learning. New York, NY: Association for Computing\nMachinery (2020). p. 110– 120.\n2 8 .R e nH ,W a n gJ ,Z h a oW X .R A P T :p r e - t r a i n i n go ft i m e - a w a r e\ntransformer for learning robust healthcare representation. In: Proceedings\nof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining . New York, NY: Association for Computing Machinery (2021).\np. 3503– 3511.\n29. Peng X, Long G, Shen T, Wang S, Jiang J. Sequential diagnosis prediction\nwith transformer and ontological representation.arXiv (2021).\n30. Prakash P, Chilukuri S, Ranade N, Viswanathan S. RareBERT: transformer\narchitecture for rare disease patient identiﬁcation using administrative claims. In:\nThe Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence (AAAI-21) RareBERT.\nPalo Alto, CA: AAAI Press (2021). p. 453– 460.\n31. Meng Y, Speier W, Ong MK, Arnold CW. Transformers using multimodal\nelectronic health record data to predict depression.IEEE J Biomed Health Inform\n(2021) 25:3121– 9. doi: 10.1109/JBHI.2021.3063721\n32. Choi E, Xu Z, Li Y, Dusenberry MW, Flores G, Xue E, et al. Learning the\ngraphical structure of electronic health records with graph convolutional\ntransformer. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence.\nPalo Alto, CA: AAAI Press (2020). p. 606– 613.\n33. Shang J, Ma T, Xiao C, Sun J. Pre-training of graph augmented transformers\nfor medication recommendation.arXiv (2019).\n34. Johnson AEW, Stone DJ, Celi LA, Pollard TJ. The mimic code repository:\nenabling reproducibility in critical care research.J Am Med Inform Assoc(2018)\n25:32– 9. doi: 10.1093/jamia/ocx084\nShickel et al. 10.3389/fdgth.2022.1029191\nFrontiers inDigital Health 13 frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7125661373138428
    },
    {
      "name": "Transformer",
      "score": 0.6202254295349121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5927091836929321
    },
    {
      "name": "Machine learning",
      "score": 0.5675609707832336
    },
    {
      "name": "Lexical analysis",
      "score": 0.5208775997161865
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4946398437023163
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4661810100078583
    },
    {
      "name": "Artificial neural network",
      "score": 0.3505375385284424
    },
    {
      "name": "Engineering",
      "score": 0.12646082043647766
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}