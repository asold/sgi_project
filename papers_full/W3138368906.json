{
  "title": "Controllable Generation from Pre-trained Language Models via Inverse Prompting",
  "url": "https://openalex.org/W3138368906",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096501847",
      "name": "Xu Zou",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2123677600",
      "name": "Da Yin",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2527107011",
      "name": "Qingyang Zhong",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2096440077",
      "name": "Hong-xia Yang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107505891",
      "name": "Zhilin Yang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W7016021835",
    "https://openalex.org/W2059488546",
    "https://openalex.org/W2154241802",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2037043950",
    "https://openalex.org/W2998453866",
    "https://openalex.org/W2952771545"
  ],
  "abstract": "Large-scale pre-trained language models have demonstrated strong capabilities\\nof generating realistic text. However, it remains challenging to control the\\ngeneration results. Previous approaches such as prompting are far from\\nsufficient, which limits the usage of language models. To tackle this\\nchallenge, we propose an innovative method, inverse prompting, to better\\ncontrol text generation. The core idea of inverse prompting is to use generated\\ntext to inversely predict the prompt during beam search, which enhances the\\nrelevance between the prompt and the generated text and provides better\\ncontrollability. Empirically, we pre-train a large-scale Chinese language model\\nto perform a systematic study using human evaluation on the tasks of\\nopen-domain poem generation and open-domain long-form question answering. Our\\nresults show that our proposed method substantially outperforms the baselines\\nand that our generation quality is close to human performance on some of the\\ntasks.\\n Narrators can try our poem generation demo at\\nhttps://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at\\nhttps://pretrain.aminer.cn/app/qa. For researchers, the code is provided in\\nhttps://github.com/THUDM/InversePrompting.\\n",
  "full_text": "Controllable Generation from Pre-trained Language Models via\nInverse Prompting\nXu Zou12, Da Yin12, Qingyang Zhong12, Ming Ding12, Hongxia Yang4, Zhilin Yangâˆ—123,Jie Tangâˆ—12\n1 Department of Computer Science and Technology, Tsinghua University\n2 Beijing Academy of Artificial Intelligence\n3 Recurrent AI, Ltd.\n4 DAMO Academy, Alibaba Inc.\n{zoux18,yd18,zqy20,dm18}@mails.tsinghua.edu.cn\nyang.yhx@alibaba-inc.com.cn\n{zhiliny,jietang}@tsinghua.edu.cn\nABSTRACT\nLarge-scale pre-trained language models have demonstrated strong\ncapabilities of generating realistic text. However, it remains chal-\nlenging to control the generation results. Previous approaches such\nas prompting are far from sufficient, which limits the usage of lan-\nguage models. To tackle this challenge, we propose an innovative\nmethod, inverse prompting, to better control text generation. The\ncore idea of inverse prompting is to use generated text to inversely\npredict the prompt during beam search, which enhances the rel-\nevance between the prompt and the generated text and provides\nbetter controllability. Empirically, we pre-train a large-scale Chi-\nnese language model to perform a systematic study using human\nevaluation on the tasks of open-domain poem generation and open-\ndomain long-form question answering. Our results show that our\nproposed method substantially outperforms the baselines and that\nour generation quality is close to human performance on some of\nthe tasks.\nNarrators can try our poem generation demo at https://pretrain.\naminer.cn/apps/poetry.html, while our QA demo can be found at\nhttps://pretrain.aminer.cn/app/qa. For researchers, the code is pro-\nvided in https://github.com/THUDM/InversePrompting.\n1 INTRODUCTION\nThe field of text generation has made tremendous progress recently.\nLarge-scale autoregressive Transformer models [23] optimized with\nmaximum likelihood estimation have shown the ability of gener-\nating realistic text [ 2, 4, 15]. For real-world applications of text\ngeneration such as essay writing and story generation, it is essen-\ntial for the users to be able to control the generation results. One\nof the most common approaches is to use prompting; i.e., a user\nshall manually write a few sentences to serve as the prompt and\nthe language model generates the subsequent tokens given the\nprompt. For example, a user might input â€œthis is a sad story about\na disease named COVID-19â€ as a prompt to expect the generation\nof a COVID-19 story.\nHowever, prompting is far from sufficient for controllable text\ngeneration. It is not uncommon for a language model to deviate the\ngeneration process from the original prompt and start generating\ntext of unrelated topics. Figure 2 shows an example of how language\nmodels fail to maintain the coherence between the prompt and\nthe generated text. In the example, the language model is asked\nâˆ—Corresponding authors: Zhilin Yang and Jie Tang.\nto answer the question â€œwhich moment did you want to live in\nforeverâ€. The baseline using conventional prompting generates a\nstory that deviates a lot from the prompt; i.e., most of the generated\ncontent is irrelevant to the question. There were also unnatural\nexpressions that do not make much sense in the context.\nTo tackle this challenge, we propose a novel method, inverse\nprompting, to refine the process of text generation from pre-trained\nlanguage models. Inverse prompting can be decoupled into three\nsteps. First, given a piece of generated text, an inverse prompt is\nconstructed using the generated text. Second, the conditional likeli-\nhood of the original prompt given the inverse prompt is computed\nbased on the pre-trained language model. Third, the conditional\nlikelihood is used as a score in beam search for selecting the best\ngeneration candidates. As a result, inverse prompting ensures that\nthe pre-trained language model predicts the prompt given the gen-\nerated text with high likelihood, which encourages the relevance of\nthe generated text to the prompt. For example, in the case of ques-\ntion answering, a language model generates an answer given the\nquestion; during beam search, we use the same language model to\ncompute the likelihood of the question given the generated answer\nfor candidate selection. In the example of Figure 2, text generated\nby inverse prompting describes a beautiful moment of high school\ntimes which is closely related to the question. As a result, inverse\nprompting achieves an average score of 8.60 out of 10 under human\nevaluation on this example, compared to the baselineâ€™s 5.40.\nTo systematically evaluate the performance of inverse prompt-\ning, we conduct an experiment using human evaluation on the tasks\nof open-domain long-form question answering and open-domain\ntraditional Chinese poem generation. We pre-train a Chinese lan-\nguage model to serve as the base model in our experiments. The task\nof long-form question answering is similar to answering questions\non Quora or Zhihu. On this task, we show that inverse prompt-\ning achieves much higher scores in all aspects than the prompting\nbaseline and the previous state-of-the-art Chinese language model\nCPM [27]. The task of traditional Chinese poem generation targets\ngenerating poems of an ancient form but with contemporary topics\nincluding rocket science, relativity, or artificial intelligence, which\ntests the generalization ability of different approaches. Figure 1 ill-\nsutrates an example of traditional Chinese poem generation under\nthe title New York. It combines contemporary notions of New York\nlike Manhattan and the financial center with a traditional form and\ntraditional poetic imagery of cloud and rain. On this task, human\nexpert evaluation demonstrates that inverse prompting performs\narXiv:2103.10685v3  [cs.CL]  9 Nov 2021\nFigure 1: The generation process of open-domain traditional Chinese poems under inverse prompting. Using title New York\nas an example.\nFigure 2: An example showing how the prompting baseline\nmodel may fail to maintain relevance in generated text, and\nhow inverse prompting alleiates this issue. The relevance\nand overall scores were obtained from human evaluation.\nsignificantly better than the prompting baseline and is comparable\nto Jiuge [28], a well-known state-of-the-art system for traditional\nChinese poem generation. When we combine inverse prompting\nwith self training, i.e., finetuning the model with self-generated\npoems, our system outperforms Jiuge under human evaluation by\na large margin. Our results of human evaluation demonstrate that\ninverse prompting improves the controllability and quality of text\ngeneration significantly and achieves close-to-human results.\n2 RELATED WORK\n2.1 Pre-training and Language Models\nLanguage modeling has been widely used as an objective for pre-\ntraining and demonstrates strong generalization abilities. Origi-\nnating from word embedding methods such as word2vec [13] and\nGloVe [14], pretraining methods have displayed an increased level\nof importance in the field of natural language processing [4, 7, 11].\nThese models are more general and require less domain-specific\ndata to achieve strong performance.\nSpecifically, a main type of pretrained models are autoregres-\nsive language models. Generative pretraining (GPT) [2, 15, 16] and\nTransformer-XL [4] achieve substantial improvement in terms of\nperplexity and also improves generation quality. The approach has\nalso been adapted to different languages [6, 27].\nAlthough realistic text can now be generated automatically by\nlarge-scale pretrained language models, it is challenging but essen-\ntial for users to be able to control the generation results. Prompting\n[4, 15] has been widely used but is rather limited in controlling the\ngeneration results. CTRL [9] proposes to use control codes to pro-\nvide conditions for a language model. Different from their method,\nour method does not rely on modification of pretraining paradigms\nor human-designed attributes. PPLM [5] performs backpropagation\nduring test time to adjust generation to maximize the scores given\nby attribute models. Compared to PPLM, inverse prompting does\nnot require any gradient update to the original model and is free of\nany additional attribute models.\nThe idea of using dual process to strengthen the quality of AI\ngeneration by the dual property that the outputs and inputs are\ninputs and outputs under an inverse prespective has long been\nresearched. [24] introduces dual learning for the task of machine\ntranslation. The method uses multiple different models to form a\ntranslation loop and hopes the contexts will remain unchanged after\npassing through the loop. CycleGAN [3] and VAE [1] also shares\nthe similar idea of reconstruction in their applications. Different\n2\nfrom these works that uses different forward and inverse models,\nin this paper, we exploit the existence of inverse format in natural\nlanguages and use the same language model for prompting and\ninverse prompting.\n2.2 Open-Domain Long-Form\nQuestion-Answering\nQuestion answering is a well-studied problem in artificial intelli-\ngence [21]. There are various paradigms of question answering.\nShort-form question answering focuses on using a short phrase or\nsentence to answer the question [17, 25]. On the other hand, long-\nform question answering targets generating multiple sentences or\nparagraphs to answer a question in a more comprehensive way.\nOnline question answering platforms such as Quora and Zhihu\ncan be viewed as good examples of long-form question answering.\nWhile short-form question answering is easier to evaluate and more\nmore widely studied, we are interested in investigate the ability\nof open-domain long-form question answering using text gener-\nation models in this work. Because it is challenging to evaluate\nthe qualities of long-form question answering, we employ human\nevaluation in our experiments.\n2.3 Traditional Chinese Poem Generation\nTraditional Chinese poetry is an important genre of Chinese litera-\nture with a history of tens of centuries [10]. A few years ago, re-\nsearchers experimented with generating traditional Chinese poems\nusing statistical machine learning methods [8]. Later, Jiuge [26, 28]\nadvanced traditional Chinese poem generation to a new level. As\nthe well-recognized state of the art for open-domain Chinese poem\ngeneration, Jiuge is able to generate multiple forms of poems un-\nder any given titles, keywords or even images. Despite its ability\nto handle arbitrary open-domain inputs, Jiuge performs well on\ndomain-specific contexts such as giant deserts or iron horses but\ndoes not generalize well to contemporary notions such as Donald\nTrump, quantum computation, and Europe. Different from Jiuge,\nwe employ a large-scale language model pretrained on a general-\npurpose corpus and leverage inverse prompting to enhance its\ngeneration qualities.\n3 METHODOLOGY\nIn this section, we discuss the proposed inverse prompting method.\nThe problem of text generation is modeled as generating ğ‘ğ‘” given\nthe prompt ğ‘ğ‘ , where both ğ‘ğ‘ and ğ‘ğ‘” are sequences of tokens.\n3.1 Baseline: Prompting and Beam Search\nGiven a language model with probability distribution ğ‘, a simple\nand widely-used approach is to generate text by maximizing the\nconditional probabilityğ‘(ğ‘ğ‘” |ğ‘ğ‘ ). This is usually achieved with beam\nsearch [12]. With a beam size of ğ‘›, beam search keeps the top-\nğ‘› sequences during decoding time according to a beam scoring\nfunction ğ‘“ (Â·). An illustration is shown in Algorithm 1. The baseline\nmethod uses the log likelihood to define the scoring function, i.e.,\nğ‘“ (ğ‘ğ‘” |ğ‘ğ‘ )= log ğ‘(ğ‘ğ‘” |ğ‘ğ‘ )\nFigure 3: Language model generation and language model\ninverse prompting scoring for generating a poem sentence.\nAlgorithm 1: Beam search. Inverse prompting follows the\nbeam search framework with a novel scoring function ğ‘“\nbeing the inverse log likelihood of the prompt given the\ngenerated text.\nResult: Generated Context ğ‘ğ‘”\n1 Given a language model ğ‘, a prompt ğ‘ğ‘ , the number of\nbeams ğ‘›, the number of steps ğ‘ , exploration steps for each\nbeam ğ‘š. Initialize current step ğ‘˜ = 0. For each beam ğ‘—,\ninitialize the generated context for this beam ğ‘ğ‘— =â€²â€².\n2 while k<s do\n3 For each ğ‘ğ‘— , generate ğ‘š next token ğ‘¡ğ‘—1...ğ‘¡ ğ‘—ğ‘š sampled\nfrom ğ‘(Â·|ğ‘ğ‘ +ğ‘ğ‘— ), update ğ‘ğ‘—ğ‘™ = ğ‘ğ‘— +ğ‘¡ğ‘—ğ‘™\n4 Choose the best ğ‘› texts ğ‘ğ‘1, ğ‘ğ‘2...ğ‘ğ‘ğ‘› with highest\nğ‘“ (ğ‘ğ‘—ğ‘™ |ğ‘ğ‘ ).\n5 For all ğ‘—, update ğ‘ğ‘— = ğ‘ğ‘ ğ‘—.\n6 Update k=k+1.\n7 end\n8 Output the best beam ğ‘ğ‘” = ğ‘1.\n3.2 Inverse Prompting\nIn this paper, we introduce a new method based on a new scorer\nğ‘“ in beam search, called inverse prompting . Unlike previous con-\ntrollable methods such as CTRL or PPLM which needs additional\nattribute model training or manually-defined codes, inverse prompt-\ning directly uses the original language model itself to improve its\ngeneration.\nOne main issue that reduces the quality of the generated text\nis the run-away phenomena shown in Table 2. The text gradually\nbecomes irrelevant to the given prompt as the sentences being gen-\nerated. As the distance between the given prompt and the generated\nsentence becomes larger, it hinders the generator to keep a close\nconnection with the prompt.\nTo alleviate this issue, our main idea is to design a novel beam\nsearch scoring function that evaluates the log likelihood in an\ninverse direction; e.g., if the prompt can be generated back from\n3\nthe text, they ought to be very related with each other:\nğ‘“ (ğ‘ğ‘” |ğ‘ğ‘ )= log ğ‘(ğ‘ğ‘ |ğ‘ğ‘”). (1)\nTexts are not always fluent if we read them from an inverse way.\nIn question-answering, the prompt may be \"Question:${Question}\nAnswer:\". It is natural to follow the answer after that, yielding\n\"Question:${Question} Answer:${Answer}\". However, it is very\nunlikely that in natural language the order will present in the\ninverse way \"${Answer} Question:${Question} Answer:\". Simply\nusing equation 1 only results in failure.\nHowever, thanks to the nature of natural language, there do\nexist ways to rearrange contexts to make them appear in an in-\nverse order properly. Letâ€™s continue with the above instance: For\n\"Question:${Question} Answer:${Answer}\", there do exist a way\nin natural language to inverse it: \" ${Answer} answers the ques-\ntion:${Question}\".\nTo achieve the core idea of Eqn. 1, we simply need to alter the\nformat of the prompts and texts:\nğ‘“ (ğ‘ğ‘” |ğ‘ğ‘ )= log ğ‘(ğ‘â€²\nğ‘ |ğ‘â€²\nğ‘”), (2)\nwhere ğ‘â€²ğ‘” is inverse prompt under a new format, and ğ‘â€²ğ‘ being\nthe inverse text. Figure 4 displays some examples of this trans-\nformation format. For ğ‘ğ‘ =\"Question:${Question} Answer:\" and\nğ‘ğ‘” =${Answer}, we only need to set ğ‘â€²ğ‘ =\"${Question}\" and\nğ‘â€²ğ‘” =\"${Answer} answers the question:\", equation 2 shall work.\nInverse prompting ranks different beams by their likelihood to gen-\nerate back the original prompt in an inverse way, promoting the\nmost relevant generated texts. Inverse prompting can be used as\nlong as the language supports an inverse structure to rearrange\nthe prompt and context in a proper way. Detailed illustration for\nlanguage model generation and language model inverse prompting\nis presented in Figure 3.\nThe top rows in figure 4 are the formats of inverse prompting in\nthe two experiments of our paperâ€”long-form question answering\nand poem generation, while the bottom rows are additional exam-\nples of how inverse prompting can be used in other text generation\ntasks.\nInverse prompting is a simple method and easy to implement.\nThe method requires no additional models or data processing, as\nthe inverse prompting score can be simply computed by the same\nlanguage model used for generation. However, inverse prompting\noffers large improvements to the qualities of the generated texts,\nwhich we will show in Sections 4 and 5.\n4 IMPLEMENTATION\nWe mainly use two long-term text generation tasks, Open-Domain\nLong-Term Chinese Question-Answering, and Open-Domain Chi-\nnese Poem Generation, which require the AI to generate long, in-\ndepth contexts according to relatively short prompts, to demon-\nstrate the fantastic performance of inverse prompting.\nWe believe that as the relevance between generated texts and the\ngiven prompt (questions/titles) improves, the generation quality will\nincrease too. So we conduct inverse prompting on questions/titles\nin our experiments, as shown in the first four rows in Figure 4.\n4.1 Base Language Model\nWe train our base Chinese language model using Megatron-LM[19]\nwith Transformer-XL[4]. The model has 2.86 billion parameters.\nThe training set contains 302GB of raw Chinese data abstracted\nfrom multiple Chinese websites including Baidu, Zhihu and Sougou.\nWe train the base model using 64 GPUs for 160,000 steps. Details\nof training settings and datasets are displayed in Appendix 7.1.\n4.2 Open-Domain Long-Form\nQuestion-Answering\nLong-Form Question-Answering, like the QAs on Quora, Zhihu or\nSougou, is a form of question-answering that questions and descrip-\ntions are short and described in one or two sentences, while the\nanswers have to be long, informative and in-depth. The questioner\nexpects a thorough answer answering the question he asks in detail.\nWe apply inverse prompting in this way to generate Long-Form\nAnswers given Question prompts. We generate sub-sentences ran-\ndomly according to language modelLM, and do beam-search with\ninverse prompting in sub-sentence level. To ensure the answer\nfollows the question, we apply inverse prompting (Equation 2 for\neach sub-sentence and sum up their scores. To keep the generated\ncontext fluent, we combine the scores with normalized forward\nperplexity,\nğ‘“ (ğ‘ğ‘” |ğ‘ğ‘ )= 1\nğ‘›\nâˆ‘ï¸\nğ‘  âˆˆğ‘ğ‘”\nğœ†1 log ğ‘(ğ‘â€²\nğ‘ |ğ‘ â€²)+ğœ†2\nlog ğ‘(ğ‘ğ‘” |ğ‘ğ‘ )\nğ‘›(ğ‘ğ‘”)ğœ† . (3)\n4.3 Open-Domain Poem Generation\nTraditional Chinese Poem generation is the pearl of domain-specific\nlong-form Chinese text generation tasks. Traditional Chinese poems\nhave their specific complex format and word usages different from\nmodern Chinese language. Most of the poems are written by ancient\npoets to express their feelings, describe things they are doing, or\nideas on different items. Generation of meaningful text under the\npoem format given open-domain information is very hard for both\nstate-of-the-art AI models and humans.\nIn this paper, besides Open-Domain Long-Form QA, we challenge\nour inverse prompting for a seemingly impossible taskâ€“ To use the\nlanguage model trained on modern texts to generate Open-Domain\nTraditional Chinese Poems.\nWe basically keep the inverse prompting format of equation 3\nwhile adding a poem-format term to the beam-search (Equation\n4), which penalizes contexts by the degree they disobey with the\npoem format in rhythms or tones.\nğ‘“ (ğ‘ğ‘” |ğ‘ğ‘ )= 1\nğ‘›\nâˆ‘ï¸\nğ‘  âˆˆğ‘ğ‘”\nğœ†1 log ğ‘(ğ‘â€²\nğ‘ |ğ‘ â€²)+ğœ†2\nlog ğ‘(ğ‘ğ‘” |ğ‘ğ‘ )\nğ‘›(ğ‘ğ‘”)ğœ† âˆ’ğœ†3ğ‘™ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡(ğ‘ğ‘”)\n(4)\n4.4 Self Training for Poem Generation\nGiven that the model is trained on modern Chinese texts includ-\ning very few poem-format texts, it can hardly generate texts fully\nobeying the poem format while maintaining a strong relevance to\nthe given title.\nTherefore, to improve its performance, we try the generate-and-\nfine-tune self training protocol in AlphaGo-Zero[20] for this task.\n4\nFigure 4: Inverse prompting transformation Table. The first rows represents the inverse prompts used in experiments.(in\nChinese and English) Some additional examples of inverse prompting format are also displayed.\nWe randomly select 1500 titles and let the model to produce\npoems based on them. Then we fine-tune the model on these gen-\nerated poems for 2000 steps. This cycle can be repeated multiple\ntimes and in our experiments we repeat this cycle for 2 times. We\nexpect the fine-tuned model to be more likely to generate sentences\nwith better poem formats and other poem-specific properties like\naesthetics without losing their relevance to the given title.\n5 EXPERIMENTS\nIn this section, we display the human-evaluation results of inverse\nprompting on two long-form text generation tasks, open-domain\nlong-form QA and open-domain poem generation.\n5.1 Human Evaluation Protocol\nWe first introduce how our human evaluation on the two tasks is\nconducted. Table 1 illustrates the statistics for human evaluation\nexperiments. For open-domain long-form QA, we recruit 45 people,\nmostly university students, to evaluate the quality of the generated\nanswers. As for the evaluation of poem generation, we invite 11\nexperts on traditional Chinese poems. Some of them previously\nparticipated in the evaluation of Jiuge, the previous state-of-the-art\nChinese poem generator. The others are either similarly known to\nbe knowledgeable on poems or guaranteed to be qualified for the\nevaluation by some poem contests.\nTable 1: Human Evaluation Statistics. We filter out the an-\nswers from participants who does not finish all questions\nand pass the consistency check.\nTask Participants Prompts Methods Scores\nCompared Collected\nlong-form QA 30 100 4 12,000\nPoem generation 10 100 4 4,000\nEach task contains 100 prompts and for each prompt, we provide\n4 different contexts for evaluators to evaluate. An evaluator needs\nto score each context from multiple aspects and give an overall\nrating for each context on our online evaluation platform within\none week time.\nTo ensure participants making evaluations seriously, for each\nprompt we ask the participants to select the best context. Then\nwe will check if this answer is consistent with the overall ratings\nfor those 4 contexts additionally. If the proportion of inconsistent\nanswers reaches over 20%, we will treat this participant as invalid.\nFinally, we only collect the answers submitted by valid participants.\nAs listed in Table 1, 32 evaluators in long-form QA evaluation and\n10 experts for traditional Chinese poems finished his/her evaluation.\n30 of the finished evaluators in long-form QA experiment are valid,\nwhile all the 10 finished experts in our traditional Chinese poem\nexperiment are valid.\n5.2 Open-domain long-form Chinese QA\nFor open-domain long-form Chinese QA evaluation, we randomly\nselect 100 questions from various domains in Zhihu, a Quora-like\nChinese open-domain long-form QA platform. In Zhihu, users can\nask questions with some background descriptions, or provide infor-\nmative long-form answers to the raised questions. Besides, users\ncan \"Upvote\" or \"Downvote\" answers based on their opinions.\nIn this experiment, we only select questions that are excluded in\nthe training set of our base model. For each question, we display\none highly-upvoted human answer and three AI-generated answers\nproduced by CPM [27], prompting baseline, and inverse prompting\nrespectively.\nWe shuffle the order of all answers and ask human evaluators to\nscore the answers through four aspects including:\n(1) Fluency Whether the answer is well-formed and logical to\nread. Rated from 1 to 5.\n(2) Informativeness Whether the answer contains useful in-\nformation for the given question. Rated from 1 to 5.\n(3) Relevance Whether the answer is relevant to the given\nquestion. Rated from 1 to 5.\n(4) Overall The overall quality of the answer. Rated from 1 to\n10.\nTable 2 shows that inverse prompting outperforms both the\nprompting baseline and the previous SOTA Chinese language model\nCPM by a large margin in all individual aspects, as well as the overall\nquality of the generated answers.\n5\nTable 2: Performance for open-domain long-form Chinese\nQA under Human Evaluation.\nMethod Fluency Inform. 1 Relevance Overall\n(1-5) (1-5) (1-5) (1-10)\nCPM [27] 2.66 2.47 2.36 4.32\nPrompting Baseline 3.44 3.25 3.21 5.97\nInverse Prompting 3.61 3.43 3.59 6.51\nHuman Answers 3.80 3.61 3.67 6.85\n1 Informativeness\nDespite inverse prompting only forces the answer to be more\nrelated to the question in this experiment, an interesting finding is\nthat by producing more relevant answers, inverse prompting also\nmakes improvements on the fluency and informativeness of the\ngenerated answers, raising the overall quality as well. This supports\nour hypothesis in section 4.\n5.3 Open-domain Poem Generation\nThe second experiment is to evaluate the task of open-domain poem\ngeneration. This task is similar to the long-form QA experiment\ndescribed above. We randomly design 100 poem titles including var-\nious open domains for evaluation. These poem titles never appear\nin any real poems in the training set or being used as randomized\ntitles in our reinforcement learning process.\nFor each title, we apply four different methods to generate\npomes, including Jiuge (the SOTA model for open-domain Chi-\nnese poem generation), the beam search baseline with poem format\nloss ğ‘™ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡, inverse prompting with poem format loss (Equation\n4) and inverse prompting with the self-training mentioned in sec-\ntion 4.4. These four poems are shuffled for evaluation. For each\ngenerated poem, we request evaluators for 5 ratings:\n(1) Format Whether the generated poem follows the rule of\nrhythm in traditional Chinese poetry. Rated from 1 to 5.\n(2) Innovation Whether the sentences are copied from existing\npoems or created with innovative expressions. Rated from 1\nto 5.\n(3) Relevance Whether the content of the poem is related to\nthe given title. Rated from 1 to 5.\n(4) Aesthetics Whether the poem has obscure meanings apart\nfrom its obvious meanings, making it aesthetically better?\nRated from 1 to 5.\n(5) Overall The overall quality of the poem. Rated from 1 to 10.\nTable 3: Performance for open-domain Traditional Chinese\nPoem Generation under human expert evaluation.\nMethod Format Innov. 1 Relevance Aes. 2 Overall\n(1-5) (1-5) (1-5) (1-5) (1-10)\nJiuge [28] 3.60 2.47 1.99 3.12 3.57\nSearch Baseline 2.79 1.10 1.16 2.44 1.35\nInverse Prompting 2.56 2.71 2.92 2.33 4.00\nInverse Prompting +ST 2.42 2.92 3.65 2.18 4.40\n1 Innovation\n2 Aesthetics\nTable 3 illustrates the experimental results. The average scores\nfor all methods are low as all of the experts are extremely critical.\nThey only give high scores to very few perfect poems. One of the\nexperts says sheâ€™ll give less than 5 \"overall\" score to an average TC-\nPoem written by ancient celebrities, while scoring results indicate\nthat other experts are even more critical than her.\nThe prompting baseline can hardly generate appropriate poems.\nEven with the poem format loss, it only outputs unrelated sentences\ncopied from existing famous poems that appear in modern Chinese\nlanguages.\nHowever, with the help of inverse prompting, the overall quality\nof generated poems surpasses Jiuge. Moreover, the self-training can\nfurther improve the performance on top of inverse prompting.\nGenerally, Jiuge is good at generating sentences with beautiful\nwords and gorgeous rhythm, since it is designed to focus strictly\non poem formats. Nevertheless, according to human evaluation\nresults, despite it sometimes does generate relevant poems, most\nof its generation results are not quite related to the given title and\ncomparably weak at innovation.\ninverse prompting offers innovative and relevant expressions\nin the form of traditional Chinese poems. However, as the model\nis trained on modern Chinese texts, it is inevitably not so good in\nfollowing traditional poem formats like rhythms or tones. It also\ndoesnâ€™t handle aesthetics well, as this is common in ancient Chinese\npoems, but rarely appears in modern Chinese.\nDespite these disadvantages, the experts still agree to give poems\ngenerated by inverse prompting a much higher average overall\nscore than Jiuge due to their high relevance to titles and innovative\nexpressions.\nIn section 4.4, we expect the self-training can bring better format\nand aesthetics to the generated poems. However, to our surprise,\nthe self-training further enhances the innovation and relevance by a\nlarge margin at the cost of a minor decrease in format and aesthetics,\nsuggesting that what the model really learns under our reinforce-\nment learning scheme is to be more relevant. By generating more\nrelevant poems to the title with more innovative expressions, its\naverage overall score becomes much higher additionally. Eventu-\nally, inverse prompting with self-training gets 4.40 average overall\nscore, compared with Jiugeâ€™s 3.57.\nOne possible explanation for this phenomenon is that in order to\nbe more relevant to open-domain titles which may never appear in\nthe domain of Traditional Chinese Poems, the model has to be more\ninnovative in language arrangement and less focused on formats\nor aesthetics.\nIn Appendix 7.3, we discuss our deviation analysis and calculate\np-values for different methods on the above two tasks.\n5.4 Poem Turing Test\nApart from human evaluation for open-domain titles, we also test\nthe performance of it on domain-specific titles.\nBack to the result of long-form QA in Table 2, answers generated\nby inverse prompting are only slightly inferior to human answers.\nTheir average score is 6.51 compared with human answersâ€™ 6.85.\nThis enlightens our mind that the generated poems may be compa-\nrable in quality to human-made poems.\n6\nInspired by turing test [22], we similarly designed a traditional\nChinese poem turing test to further evaluate the generated poems\nquality of inverse prompting.\nIn the turing test, also known as the imitation game, a human\ninterrogator is requested to distinguish between generated poems\nand human poems. We implement an online game platformwhere\nany player can participate without limitation. In the game, each\nplayer is given several pairs of poems with each pair contains one\npoem written by a human poet and the other one generated by AI\nunder the same title. The human poems are randomly selected from\nQuan Tang Shi , the most famous collection of traditional Chinese\npoems. The collection was published in 1705 and consists of high-\nquality traditional Chinese poems mostly written in Tang Dynasty\n(618-907). In our designed game, the player needs to figure out\nwhich poem is written by the human poet. We generate 1,500 pairs\nof poems and randomly displays 5 pairs for each game.\nAs displayed in Table 4, 4,592 game records are gathered from\n755 different users. Each game record involves a binary selection\nbetween a human poem and an AI poem. 46.4% of the user records\nchoose AI poems while the rest 53.6% chooses human poems, sug-\ngesting that the quality of poems generated by inverse prompting\n+self-training on domain-specific titles may be close to human level\nfor average online users.\nTable 4: Poem turing test on 755 identical online users. Hu-\nman poems are randomly selected from Quan Tang Shi. AI\nPoems are generated given the same title.\nMethod Total Selected Selection Rate\nInverse Prompting +ST 4,592 2,132 46.4%\nAncient Human Poems 4,592 2,460 53.6%\n5.5 Case Study\nIn this section, we discuss explicitly the pros and cons of inverse\nprompting by performing series of case studies on the two tasks â€“\nOpen-domain long-form QA, and Open-domain Poem Generation.\nFigure 5 exhibits a comparison between two answers for a ques-\ntion on how to deal with stress at work during pregnancy. We\nlist the answer generated by inverse prompting and the human\nanswer. In this case, the evaluators even give higher scores to the\ninverse prompting generated answer than the human answer. Both\nanswers provide comprehensive and practical aids related to the\nquestion. The human answer is more emotional and gives advice\nbased on the answererâ€™s own experience. The generated answer,\non the other hand, raises the point that pregnant mothers should\ninsist on working and overcome the difficulties straightforwardly.\nThen it uses several sentences to enhance this point, which turns\nout to be more informative, reasonable and persuasive.\nWhile the proposed method seems to be able to understand ques-\ntions, integrate learned knowledge and generate logical answers,\nwe found that numbers in the task are comparatively difficult, which\noften lead to some chaotic outputs. In Figure 6 we show a bad case\ngenerated by inverse prompting that only receives a 4.10 score in\noverall quality. While the question is asking about Chapter 882 in\nthe One Piece manga, the model is clearly unable to understand the\nFigure 5: A Perfect Example of inverse prompting generat-\ning better answer than human in open-domain long-form\nQA.\ndifferences between Chapter 885 and the asked 882. Besides, the\nanswer itself is self-contradictory. It is worth noticing that such a\nchaotic problem in maths universally exists for language models.\nPrevious study [18] also shows that it is extremely hard for neural\nmodels to do mathematical reasoning.\nIn Figure 7, we display all 4 answers for the question â€œHow to\nwash purple sweet potato on clothesâ€ with the average overall\nscores. The best answer is written by a human, which comprehen-\nsively introduces the solution to the problem step by step. The\nanswer generated by inverse prompting offers a similar solution\nbut with fewer details. The prompting baseline does not give a\nprecise answer to the original question. Instead, it tries to answer\nanother question â€œHow to wash out the liquids on clothes such as\nmilk or beer?â€. This tells us why we need to use inverse prompting\nto force the generated answer to be closely related to the original\n7\nFigure 6: A bad case for inverse prompting generated texts.\nIt canâ€™t overcome the barrier of maths.\nquestion. Finally, CPM can neither produce fluent answers nor pro-\nvide useful information. This example illustrates how the difference\nin overall ratings for different methods in Table 2 come from in a\nrepresentative way.\nFigure 8 shows poems generated by different methods under\ntitle Zebra . Zebra is an open-domain concept that never appears\nin any traditional Chinese poems (as thereâ€™s no zebra in ancient\nChina). However, there exist lots of traditional Chinese poems\nfor different types of animals. We would like to see how different\nmethods generalize the traditional Chinese poem format for zebras.\nNote that the direct meaning for â€œzebraâ€ in Chinese is \"spotted\nhorses\", so models are likely to misuse the concept of \"horse\". The\nprompting baseline copies sentences from famous existing poems\nfor horses and gets only 2.20 for being an awkward copycat. Jiuge\ntreats zebras as horses and applies a lot of analogy to glorify the\n\"horses\", with good representation and perfect format it gets a 5.10\noverall score. Inverse prompting offers a description between horses\nand zebras and gets a 4.80 overall score, while inverse prompting\nwith self-training states the hoof, the fur and the behaviors of zebras\nin a subtle way, differing zebras from horses, this poem is scored\nthe highest (5.70) by expert evaluators.\nThis is a representative instance for poem generation. The other\n99 poems are also evaluated in such a way that the title is in a\ncategory that exists a lot in traditional poems. However, the pre-\ncise concept of the title is new. For example, climbing different\nmountains or towers is popular in traditional Chinese poems, and\nwe design open-domain titles like â€œClimbing Eiffel Towerâ€ and\nâ€œClimbing Mt. Everestâ€ which never appear in them. The prompting\nbaseline often copies from existing poems. Jiuge usually gives an\npoem with good format between related and unrelated. inverse\nprompting seems to understand the title while inverse prompting\nwith self-training understands the title better.\nHowever, inverse prompting does not guarantee understanding\nof the given title. Figure 9 illustrates a bad case under title Sigh for\nthe Iroquois . Inverse prompting +self-training fails to understand\nthe meanings of the native American tribe and mistreat it as some\nforms of monsters living in deserts and killing people. This may\ndue to the low occurrence of the Iroquois in modern Chinese texts\nthat the base language model itself cannot fully understand its\nFigure 7: A representative case in open-domain long-form\nQA that the quality of answers in this problem reflects the\noverall performance of different methods.\nmeanings, and the self-training process is unable to help the model\nunderstand more about such concepts as well.\n8\nFigure 8: A representative case in poem generation that re-\nflects the overall qualities of different methods.\nFigure 9: A bad case that inverse prompting is not successful\nin understanding the given title Iroquois even under after\nself training.\n5.6 Inverse Prompting for English(GPT-2)\nWe also practice inverse prompting under an open-sourced toy\nEnglish language model: GPT-2 345M1[19].\nAs can be concluded from Table 5,6, inverse prompting greatly\nimproves the relativeness of GPT-2 345M for generated English\n1https://github.com/NVIDIA/Megatron-LM\nTable 5: A GPT-2 345M paragraph generation case.\nTitle Arctic Cat\nGPT-2 VIRGO (Aug. 23-Sept. 22): A business partner invites a group\nof friends to a casino but no money is exchanged, causing a\nmisunderstanding. A lawyer accidentally chases away a fish\nwhen someone else asks for the name of a housemate.\nLIBRA (Sept. 23-Oct. 22): A business partner mentions miss-\ning money on a Web site, triggering a conversation about\nreceiving the money in time. Expect to be caught by someone\nwho knows the truth.\nSCORPIO (Oct. 23-Nov. 21): When others make big bets, you\nhave to say no. If you donâ€™t like it, they can make it worse.\nBut donâ€™t do it.\nSAGITTARIUS (Nov. 22-Dec. 21): Take a stand and speak\nyour mind. Donâ€™t let others tell you you canâ€™t do it. You can.\nCAPRICORN (Dec. 22-Jan. 19): Speaking too much may seem\nharsh, but itâ€™s important. For example, if someone tries to\ntake over your career, you must not cave in.\nAQUARIUS (Jan. 20-Feb. 18): You are correct to think that\neveryone agrees with you when you talk about this topic.\nHowever, this person may not be a professional so donâ€™t\nbreak bread with them.\nPISCES (Feb. 19-March 20): When someone tells you theyâ€™re\nreally sorry for throwing away something, it is OK to say\nyes. Keep it short and simple.\nIF APRIL GOES AWAY\nAries (April 20-May 20): You are able to talk easily with\nsomeone who is far away from you because of a move of\npublic events.\nTaurus (May 21-June 20): If you trust someone, you might\nget involved with them by going out with them.\nGEMINI (June 21-July 22): Donâ€™t act your age when you are\nwith someone and they get angry. You are mature enough\nfor such a thing.\nCANCER (July 23-Aug. 22): You can take on anything in life\nthat you like, but others donâ€™t like it. That is OK.\nLeo (Aug. 23-Sept. 22): They see you as someone they can\nmanipulate and can be loud about. Donâ€™t like it.\nInverse\nPrompt-\ning\nAround the globe, itâ€™s still hard to find a cold-weather car.\nHoweverâ€™s will now serve as the closest thing to snow on\nthe road yet. The new Arctic Cat Aruba features two levels\nof navigation that allow drivers to get a glimpse of location\nand whatâ€™s happening on the road. Additional intercom-\ncompatible pieces will be available to start the journey when\nit makes its return to the United States. Further out Arctic\nCat Aruba For the US.\ncontexts. Under title \"Arctic Cat\", inverse prompting refers to a\ncold-weather car brand, while direct generation generates totally\nunrelated contexts. Under title \"Boris Johnson in Delhi\",GPT-2 is\nsuccessful in referring the title to UK, while completely ignoring\n\"Delhi\". However, using inverse prompting, the generated context\nsuddenly becomes very relative.\nAlthough the quality of the base model limits the performance, in\nthese cases inverse prompting still achieves obvious improvements.\n9\n6 CONCLUSION\nIn this paper, we present a new method, inverse prompting for text\ngeneration. inverse prompting offers a new option for controllable\ngeneration using language models by exploiting the inverse form\nof natural languages.\nWe experiment the text generation using inverse prompting,\nwhich is to assign inverse prompts that forces the generated texts\nto be related to the prompt. Extensive experiments of human evalua-\ntions demonstrate that as the relevance increases, the overall quality\nof long-form Chinese texts also improves significantly. On long-\nform open-domain QA, inverse prompting improves AI-generated\ntexts one step closer towards human level. More promising results\noccur in open-domain Traditional Chinese Poem Generation, as\nwith inverse prompting, language models trained on modern Chi-\nnese context can generate poems that surpass the previous SOTA\ndespite the disadvantages in format and aesthetics. Furthermore,\nwith self-training, the inverse prompting poem generator can do\neven better in human evaluations.\nFuture works may include improving the poem generation for-\nmat, applying inverse prompting to other languages like English,\nor enhancing the self training-process.\nAcknowledgements\nWe thank Zhengxiao Du and Jifan Yu for helpful discussions.\nMing Ding contributed for the speed acceleration after submis-\nsion to KDD. So we decide to list him in the author list of the arxiv\nversion.\n7 APPENDIX\n7.1 Implementation Details\nTraining Details for Base Model. For training of base model, we\nuse a training set of 302GB, the distribution of these data is shown\nin Table 7. The evaluation set contains 400MB Open-Domain QA\ncontexts that is not used during training. We select the 100 questions\nin human evaluation from this evaluation set.\nAs mentioned in section 4, we use GPT framework with its\ntransformer model substituted to Transformer-XL. For optimization,\nwe use the AdamW optimizer with ğ›½1 = 0.9, ğ›½2 = 0.95, ğœ–= 1ğ‘’ âˆ’6\nand a 0.1 L2-weight decay. The learning rate is warmed up linearly\nover the first 3,000 steps to a peak value of 1ğ‘’ âˆ’4, then is tuned\nwith cosine decay to 10% of its peak value. The total training steps\nis 160,000. The training process uses 8 servers with 8 Nvidia V100\nGPUs on each server. Each server has 96 Intel CPU cores and 376GB\nMemory. Serves are connected by 100G RoCEv2 network.\nFor reinforcement learning, on each cycle we first generate a\nfew poems for each of the 1500 prompts, resulting in around 800KB\nof generated poem data. Our fine-tuning inherits the previous con-\nditions of the optimizer from the previous model and train on\ngenerated poem data for 2,000 steps. We repeat this process twice,\nso the final size of train poems generated is 1.6MB. The fine-tuning\nuses one server with 8 Nvidia V100 GPUs.\nParameters for Beam Search. Table 8 displays the beam search\nparameters we use. For long-form QA, we use a beam size of 5, and\nfor each beam we generate 5 samples for the next short sentence,\nand we limit the length of the answer to 30 short sentences. For\nPoem Generation, we use a beam size of 10, for each beam we\ngenerate 7 samples for the next short sentence in reinforcement\nlearning and the Turing Test, and 12 samples for open-domain title\nhuman evaluation. We limit the length of the generated poems to 8\nshort sentences.\nFor ğœ†, ğœ†1, ğœ†2, ğœ†3, ğ‘™ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ mentioned in Section 4, we take ğœ† =\nğœ†1 = ğœ†3 = 1, ğœ†2 = 0.75 for poem generation and use ğœ†2 = 1.5 for\nopen-domain QA.\nFor Poem generation, we design ğ‘™ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ using various format\ninformation like number of characters in each short sentence, repet-\nitive, rhythm, and tones. Despite it doesnâ€™t work when applied to\nprompting baseline directly, it cooperates well with inverse prompt-\ning.\n7.2 Human Evaluation Details\nOur human evaluation is conducted on a platform. Figure 10 illus-\ntrates how the evaluation platform looks like. The whole task of\nevaluating 100 prompts is divided into 10 sub-tasks, and in each\nsub-task, the evaluator is required to score 4 contexts for 10 prompts\nin multiple aspects, like an online questionnaire.\nThe evaluation does not necessarily need to be finished at once.\nPeople can login and logout, change their answers for already com-\npleted problems, or continue evaluation from their current positions\nfreely in one weekâ€™s time. They only need to ensure that all evalua-\ntion questions have been answered before the deadline, with the\nratings being consistent.\nValid evaluators for open-domain QA are paid 150 RMB yuan\neach (about $25), while each TCP evaluator receives 300 RMB yuan\n(about $50), as evaluation for traditional poems requires more ex-\npert reviewers. The payment is not high but due to the flexible\ntime arrangement for online and interesting content, the task still\nattracted a lot of participants.\nWe recruit 11 experts for TCP evaluation, 10 of them finished\nand all of those finished provide valid evaluations, we recruit 45\npeople for open-domain QA, 32 of them finish their experiments\nand 30 of them provide consistent evaluations.\nFor generating baseline texts, for QA, we generate the prompting\nbaseline using the base text generation code under the prompt for-\nmat of \"Question:$Question Description:$Description Answer:\",\nfor CPM we apply the same prompt format and use its recommended\ntext generation code.\nFor poem generation using Jiuge, we write code that can au-\ntomatically make online queries to its public generation website\nhttp://jiuge.thunlp.org/ and get generated poems. Jiuge has a lot of\nformat modes and we choose four most general modes without ad-\nditional restrictions \"5-Jueju\", \"7-Jueju\", \"5-Lvshi\", \"7-Lvshi\". For each\ntitle Jiuge generates one best poem for each mode. However, it of-\nfers no hint about which poem it considers the best so we randomly\nchoose one from the 4 generated for human evaluation.\n7.3 Deviation for Human Evaluators and\np-values\nTable 9,10 displays the deviation of the scorings for human evalua-\ntors.\nThe deviation is calculated in a per-capita basis that we first\naverage the scorings for each method on for every evaluator, then\n10\nFigure 10: An illustration of our human evaluation platform.\nThe whole task of evaluating 100 prompts is divided into 10\nsub-tasks, and in each sub-task, the evaluator is required to\nscore 4 contexts for 10 prompts in multiple aspects.\nwe compute the deviation based on the average scores of each\nhuman evaluators.\nAs can be seen, evaluators agree more on the quality for Chinese\nQA, while less agree on the qualities for poems.\nWith those standard deviations, assuming evaluators are inde-\npendent with each other, we can calculate p-score. For poems we\nhave ğ‘ = 10, the p-value for Jiuge â‰¥Inverse Prompting is 0.0544\nwhile the p-value for Jiuge â‰¥Inverse Prompting+self-training is\n0.0009, suggesting that under ğ‘ < .05 we cannot fully reject the\nhypothesis that Jiuge is not worse to Inverse Prompting. However,\nInverse Prompting with self-training is statistically better than\nJiuge.\nFor QA, with ğ‘ = 30 the p-value for Prompting Baseline â‰¥In-\nverse Prompting is < .00001, while the p-value for Inverse Prompt-\ning â‰¥Human is 0.0006. So inverse prompting is statistically better\nthan the prompting baseline but is still worse than human.\nFigure 11: Selected examples of inverse prompting poetry,\npoems with heading, and SongCi.\n7.4 Online Demo Platforms\nWe further developed the poem generation and add some other\nformats of poems, including heading, which pre-defines the first\nword of each short sentence before poem generation, and SongCi,\nwhich is another form of traditional Chinese context that involves\nmuch higher format standard. All of these downstream tasks are\nbased on the inverse prompting+self training protocol , with tiny\nformat adjustments for each downstream task.\nWe display these applications on our demo Wudao Poetry 2.\nUsers can also submit their customized titles and generate poems\nof their own. There is also a QA demo named Wudao QA 3 where\nusers can submit their own questions and descriptions to get an AI\nanswer.\nFigure 11 displays some of the generated poems for these down-\nstream tasks on the platform. More cases can be found on the\nplatform, or generated according to usersâ€™ submissions.\n2https://pretrain.aminer.cn/apps/poetry.html\n3https://pretrain.aminer.cn/os/qa\n11\nTable 6: Another GPT-2 345M paragraph generation case.\nTitle Boris Johnson in Delhi\nGPT-2 And far from showing relief that the switch-back from the\nhard-nosed - and potentially racist - EU-UK relations to a\nsofter, conciliatory approach, Tony Blair yesterday morning\nraised expectations of another barrage of retaliatory tariffs\nagainst the UKâ€™s soaring exports to America, and said he\nwas in no mood to settle for the kind of bilateral exchange\nonce favoured by George W Bush and Tony Blair.\nDowning Street has been irritated that the free-trade deal\nwith America was unexpectedly allowed to lapse just when\nthe G7 summit was over - something it had tried hard to\navoid. But the prime minister, in a direct intervention be-\nfore the G7 summit in Northern Ireland, suggested the long-\nawaited deal would take time to complete.\nHe told Sir Michael Dawe: \"We want to get to the point\nwhere we are able to settle trade in terms of good terms with\nthe United States on a relationship that is not just good for\nthe United States, but is good for the UK as well.\n\"It is an old parochial argument that trading relations are\ntoo important to be put at the mercy of political relations.\nThatâ€™s a fundamentally flawed argument. \"\nMr Blair has faced particular criticism from the Tory back-\nbenchers over the continued access of the US and British\nairlines to market dominance through the \"free movement\"\nof goods, which has now been abandoned and which Mr\nBlair publicly admitted as a feature of the global economy.\nThat prompted the shadow trade secretary, Chris Grayling,\nto accuse the prime minister of trying to impose unilateral\nchange on the economic process.\nIn a letter to Mr Grayling, he said: \"It seems that for the\nmoment, Mr Blair is pushing for an extra ten days in the\nEU deal for our exports to America without consulting us.\nThe UK has been up against that earlier history in a number\nof ways - with the currency exchange rate as well as the\nfact that exports and imports have been flowing at cross\npurposes.\n\"But there are some substantive things that Mr Blair might be\nlooking at that we need to have on our side, and that would\nbe an effort to get these and other issues sorted out before\nthe G7 summit. It would probably be a necessity before the\nG7 summit, before this financial rescue package. \"\nThe Labour leader, Harriet Harman, added: \"The proposed\nU.S.-UK free trade agreement that the prime minister has\nopposed is going to fail if we donâ€™t deal properly with it.\nInverse\nPrompt-\ning\nMay 24, 2009 - Boris Johnson arrives at the National Portrait\nGallery to accept a prize for his portrait of Italian Chancellor\nRomano Prodi\nJuly 28, 2011 - Boris Johnson makes his first visit to south\nAsia, in the Indian capital, Delhi\nJune 4, 2012 - Boris Johnson attends a US delegation in New\nDelhi\nJune 20, 2013 - Boris Johnson visits the Taj Mahal monument\nin Agra, the Raj, Bihar holy city of Lalbagh\nNovember 21, 2014 - Boris Johnson in his office, New Delhi\nJanuary 23, 2014 - The foreign with which Boris Johnson has\nmost bonded during the ten-month tour of India and China\nwas Prince Charles. He met the billionaire at Buckingham\nPalace before heading off to Delhi.\nTable 7: Dataset Distribution.\nSource Format Size\nBaidu & Sougou Baike Online Encyclopedia 133GB\nZhihu Open-domain QA 131GB\nBaidu QA Open-domain QA 33GB\nGenerated TC-Poems Traditional Chinese Poetry 1.6MB\nEvaluation Open-domain QA 440MB\nTable 8: Training Dataset Distribution.\nTask beam generations max short\nsize per beam sentences\nLong-form QA 5 5 30\nPoem (train/turing) 10 7 8\nPoem (eval) 10 12 8\n12\nTable 9: Performance and Deviation for open-domain long-form Chinese QA under Human Evaluation.\nMethod Fluency Informativeness Relevance Overall\n(1-5) (1-5) (1-5) (1-10)\nCPM 2.66 Â±0.19 2.47 Â±0.19 2.36 Â±0.20 4.32 Â±0.37\nPrompting Baseline 3.44 Â±0.19 3.25 Â±0.20 3.21 Â±0.22 5.97 Â±0.42\nInverse Prompting 3.61Â±0.17 3.43 Â±0.19 3.59 Â±0.20 6.51 Â±0.38\nHuman Answers 3.80 Â±0.18 3.61 Â±0.19 3.67 Â±0.21 6.85 Â±0.39\nTable 10: Performance and Deviation for open-domain Traditional Chinese Poem Generation under human expert evaluation.\nMethod Format Innovation Relevance Aesthetics Overall\n(1-5) (1-5) (1-5) (1-5) (1-10)\nJiuge 3.60Â±0.25 2.47Â±0.28 1.99 Â±0.31 3.12Â±0.31 3.57Â±0.54\nSearch Baseline 2.79 Â±0.37 1.10 Â±0.13 1.16 Â±0.16 2.44 Â±0.38 1.35 Â±0.27\nInverse Prompting 2.56 Â±0.28 2.71 Â±0.28 2.92 Â±0.37 2.33 Â±0.28 4.00 Â±0.52\nInverse Prompting +ST 2.42 Â±0.29 2.92Â±0.28 3.65 Â±0.33 2.18Â±0.28 4.40Â±0.47\n13\nREFERENCES\n[1] Jinwon An and Sungzoon Cho. 2015. Variational autoencoder based anomaly\ndetection using reconstruction probability. Special Lecture on IE 2, 1 (2015), 1â€“18.\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al . 2020. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165 (2020).\n[3] Casey Chu, Andrey Zhmoginov, and Mark Sandler. 2017. Cyclegan, a master of\nsteganography. arXiv preprint arXiv:1712.02950 (2017).\n[4] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov. 2018. Transformer-xl: Language modeling\nwith longer-term dependency. (2018).\n[5] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero\nMolino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation. arXiv preprint arXiv:1912.02164\n(2019).\n[6] Wietse de Vries and Malvina Nissim. 2020. As good as new. How to successfully\nrecycle English GPT-2 to make models for other languages. arXiv preprint\narXiv:2012.05628 (2020).\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[8] Long Jiang and Ming Zhou. 2008. Generating Chinese couplets using a statistical\nMT approach. In Colingâ€™08. 377â€“384.\n[9] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and\nRichard Socher. 2019. Ctrl: A conditional transformer language model for con-\ntrollable generation. arXiv preprint arXiv:1909.05858 (2019).\n[10] James JY Liu. 1966. The art of Chinese poetry . University of Chicago Press.\n[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[12] Mark F. Medress, Franklin S Cooper, Jim W. Forgie, CC Green, Dennis H. Klatt,\nMichael H. Oâ€™Malley, Edward P Neuburg, Allen Newell, DR Reddy, B Ritea, et al.\n1977. Speech understanding systems: Report of a steering committee. Artificial\nIntelligence 9, 3 (1977), 307â€“316.\n[13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\nDistributed representations of words and phrases and their compositionality.\narXiv preprint arXiv:1310.4546 (2013).\n[14] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLPâ€™14) . 1532â€“1543.\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training. (2018).\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 8 (2019), 9.\n[17] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSquad: 100,000+ questions for machine comprehension of text. arXiv preprint\narXiv:1606.05250 (2016).\n[18] D. Saxton, Edward Grefenstette, Felix Hill, and P. Kohli. 2019. Analysing Mathe-\nmatical Reasoning Abilities of Neural Models. ArXiv abs/1904.01557 (2019).\n[19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter\nlanguage models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).\n[20] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\net al. 2017. Mastering the game of go without human knowledge. nature 550,\n7676 (2017), 354â€“359.\n[21] Robert F Simmons. 1970. Natural language question-answering systems: 1969.\nCommun. ACM 13, 1 (1970), 15â€“30.\n[22] Alan M Turing. 2009. Computing machinery and intelligence. In Parsing the\nturing test . Springer, 23â€“65.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762 (2017).\n[24] Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying\nMa. 2016. Dual learning for machine translation. arXiv preprint arXiv:1611.00179\n(2016).\n[25] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan\nSalakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse,\nexplainable multi-hop question answering.arXiv preprint arXiv:1809.09600 (2018).\n[26] Xiaoyuan Yi, Ruoyu Li, Cheng Yang, Wenhao Li, and Maosong Sun. 2020. Mix-\nPoet: Diverse poetry generation via learning controllable mixed latent space. In\nAAAIâ€™20, Vol. 34. 9450â€“9457.\n[27] Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\nYusheng Su, Haozhe Ji, Jian Guan, et al. 2020. CPM: A Large-scale Generative\nChinese Pre-trained Language Model. arXiv preprint arXiv:2012.00413 (2020).\n[28] Guo Zhipeng, Xiaoyuan Yi, Maosong Sun, Wenhao Li, Cheng Yang, Jiannan\nLiang, Huimin Chen, Yuhui Zhang, and Ruoyu Li. 2019. Jiuge: A human-machine\ncollaborative chinese classical poetry generation system. In ACLâ€™19. 25â€“30.\n14",
  "concepts": [],
  "topic": null,
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ]
}