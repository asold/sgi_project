{
  "title": "Human Language Modeling",
  "url": "https://openalex.org/W4285141183",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5059751311",
      "name": "Nikita Soni",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A5017359123",
      "name": "Matthew Matero",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A5101768349",
      "name": "Niranjan Balasubramanian",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A5069782204",
      "name": "H. Schwartz",
      "affiliations": [
        "Stony Brook University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2954029489",
    "https://openalex.org/W3171391618",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3099689069",
    "https://openalex.org/W2806393999",
    "https://openalex.org/W1967507014",
    "https://openalex.org/W3049463507",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2135604788",
    "https://openalex.org/W3098566014",
    "https://openalex.org/W2964287045",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2168717408",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W2798523458",
    "https://openalex.org/W4236265099",
    "https://openalex.org/W2906625520",
    "https://openalex.org/W2963747121",
    "https://openalex.org/W2153803020",
    "https://openalex.org/W2112420033",
    "https://openalex.org/W3201336341",
    "https://openalex.org/W2153266959",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2757124792",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2320588712",
    "https://openalex.org/W2119595472",
    "https://openalex.org/W2122841972",
    "https://openalex.org/W3101004475",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W2759869292",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3204347696",
    "https://openalex.org/W3172917028",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2252241921",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2963530187",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3114223345",
    "https://openalex.org/W3035156228",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3030151190",
    "https://openalex.org/W2251812186"
  ],
  "abstract": "Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it’s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 622 - 636\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nHuman Language Modeling\nNikita Soni, Matthew Matero,\nNiranjan Balasubramanian, and H. Andrew Schwartz\nDepartment of Computer Science, Stony Brook University\n{nisoni, mmatero, niranjan, has}@cs.stonybrook.edu\nAbstract\nNatural language is generated by people, yet\ntraditional language modeling views words\nor documents as if generated independently.\nHere, we propose human language modeling\n(HuLM), a hierarchical extension to the lan-\nguage modeling problem whereby a human-\nlevel exists to connect sequences of documents\n(e.g. social media messages) and capture the\nnotion that human language is moderated by\nchanging human states. We introduce, HaRT,\na large-scale transformer model for the HULM\ntask, pre-trained on approximately 100,000 so-\ncial media users, and demonstrate it’s effec-\ntiveness in terms of both language modeling\n(perplexity) for social media and ﬁne-tuning\nfor 4 downstream tasks spanning document-\nand user-levels: stance detection, sentiment\nclassiﬁcation, age estimation, and personality\nassessment.1 Results on all tasks meet or sur-\npass the current state-of-the-art.\n1 Introduction\nLanguage use, like any human behavior, is moder-\nated by underlying human states of being (Mehl\nand Pennebaker, 2003; Fleeson, 2001). Indeed,\ndifferent ways of incorporating human informa-\ntion into NLP models have recently been shown\nto improve accuracy on many NLP tasks (Hovy,\n2015; Lynn et al., 2017; Huang and Paul, 2019;\nHovy and Yang, 2021). At the same time, while\nlanguage modeling has proven itself fundamental\nto NLP, it is typically absent the notion of a human\nproducing the natural language.\nFrom a statistical modeling perspective, this ab-\nsence of human state can be seen as an instance\nof the ecological fallacy – the treatment of mul-\ntiple observations (i.e. text sequences) from the\nsame source (i.e. human) as independent (Pianta-\ndosi et al., 1988; Steel and Holt, 1996).\n1Code and pre-trained models available at:\nhttps://github.com/humanlab/HaRT.\nTo address this, we introduce the task ofhuman\nlanguage modeling (HULM), which induces de-\npendence among text sequences via the notion of\na human state in which the text was generated. In\nparticular, we formulate H ULM as the task of es-\ntimating the probability of a sequence of tokens,\nwt,1∶i, while conditioning on a higher order state\n(U1∶t−1) derived from the tokens of other docu-\nments written by the same individual. Its key ob-\njective is:\nPr (wt,i∣wt,1∶i−1, U1∶t−1)\nwhere t indexes a particular sequence of tempo-\nrally ordered utterances (e.g. a document or so-\ncial media post), and U1∶t−1 represents the human\nstate just before the current sequence,t. In one ex-\ntreme, U1∶t−1 could model all previous tokens in\nall previous documents by the person. In the oppo-\nsite extreme, U1∶t−1 can be the same for all users\nand for values of t reducing to standard language\nmodeling: Pr (wi∣w1∶i−1).2 Thus, H ULM-based\nmodels without history can be used where tradi-\ntional LMs are applied (and may even perform bet-\nter).\nHULM brings together ideas from human fac-\ntor inclusion/adaptation (Hovy, 2015; Lynn et al.,\n2017; Hovy and Yang, 2021) and personalized\nmodeling (King and Cook, 2020; Jaech and Os-\ntendorf, 2018) into the framework of large pre-\ntrained language models. Compared to traditional\nlanguage modeling, H ULM offers several techni-\ncal advantages. First, the human state serves as\na higher order structure that induces dependence\nbetween the text sequences of the same person/\nthus posing a language modeling problem that is\na more faithful treatment of human-generated nat-\nural language. Second, conditioning on prior texts\nof an individual can be seen as an implicit integra-\ntion of text-derived human factors without having\nto explicitly model the identity of the individual.\n2See section 3 for a full HULM deﬁnition.\n622\nThis enables ﬁne-tuning of such a model to many\ndownstream tasks. Third, using the temporally or-\ndered prior texts for human contexts can be seen\nas a way to track the dynamic nature of human\nstates (e.g. emotions, daily activities) and be com-\nbined to yield more stable personality traits (e.g,\nextraversion, openness).\nTo build a language model that effectively ad-\ndresses the H ULM task, we develop HaRT, a\nhuman-aware recurrent transformer. HaRT is built\nusing a new user-state based attention layer, that\nconnects standard word sequence transformer lay-\ners in order to incorporate the human context. The\nrecurrent user state allows HaRT to effectively\nmodel long contexts necessary to handle all the\nprevious messages written by an individual. We\ntrain HaRT on the HULM task deﬁned over a large\ncollection of social media texts spanning 100K\nusers and apply it (ﬁne-tuning) on 2 downstream\nmessage-level tasks: stance detection (Moham-\nmad et al., 2016), and sentiment analysis (Nakov\net al., 2013) as well as 2 human-level tasks: age\nestimation and personality assessment (Schwartz\net al., 2013).\nContributions. Our contributions are three-\nfold: (1) We introduce the task of human lan-\nguage modeling (H ULM), providing a mathe-\nmatical deﬁnition and relation to traditional lan-\nguage modeling; (2) We propose HaRT, a novel\ntransformer-based model for performing H ULM\nand capable of being ﬁne-tuned to speciﬁc tasks\nincluding user-level tasks for which traditoinal\nlanguage models cannot be applied without ar-\nchitectural alterations; (3) We evaluate HaRT,\ndemonstrating state-of-the art performance on ﬁve\ntasks: social media language modeling (perplex-\nity), two document-level tasks (sentiment analy-\nsis and stance detection), and two user-level tasks\n(personality–openness assessment, and age esti-\nmation).\n2 Related Work\nRecent advances in language model pre-training\nhave led to learned representation of text. Pre-\ntraining methods have been designed with differ-\nent training objectives, including masked language\nmodeling (Devlin et al., 2019) and permutation-\nbased auto-regressive language modeling (Yang\net al., 2019). These have contributed in build-\ning deep autoencoding architectures, allowing the\nsame pre-trained model to successfully tackle\na broad set of NLP tasks. While pre-training\nover large collections of text helps models ac-\nquire many forms of linguistic and world knowl-\nedge(Petroni et al., 2019; Jiang et al., 2020;\nRogers et al., 2020), they are still devoid of the\ninformation about the text creator.\nRecently, it has been suggested that the NLP\ncommunity address the social and human fac-\ntors to get closer to the goal of human-like lan-\nguage understanding (Hovy and Yang, 2021). This\ncall builds on a series of studies suggesting that\nintegrating the human context into natural lan-\nguage processing approaches leads to greater ac-\ncuracy across many applications in providing per-\nsonalized information access (Dou et al., 2007;\nTeevan et al., 2005) and recommendations (Guy\net al., 2009; Li et al., 2010; De Francisci Morales\net al., 2012). The idea of contextualizing language\nwith extra linguistic information has been the ba-\nsis for multiple models: Hovy (2015) learn age-\nand gender-speciﬁc word embeddings, leading to\nsigniﬁcant improvements for three text classiﬁca-\ntion tasks. Lynn et al. (2017) proposed a domain\nadaptaion-inspired method for composing user-\nlevel, extra-linguistic information with message\nlevel features, leading to improvements for mul-\ntiple text classiﬁcation tasks. Welch et al. (2020a)\npropose a new form of personalized word embed-\ndings that use demographic-speciﬁc word repre-\nsentations.\nIn addition to addressing to social and human\nfactors, recent work has also focused on person-\nalized language models (King and Cook, 2020;\nJaech and Ostendorf, 2018) learning author rep-\nresentations (Delasalles et al., 2019) and person-\nalized word embeddings (Lin et al., 2017) point-\ning out the importance of personalized semantics\nin understanding language. Welch et al. (2020b)\nexplore personalized versus generic word repre-\nsentations showing the beneﬁts of both combined.\nWhile these models are trained for singular user,\nMireshghallah et al. (2021) trains a single shared\nmodel for all users for personalized sentiment\nanalysis. However, the approach is not scalable\nas it is still user speciﬁc and expects a unique user\nidentiﬁer.\nWhile not the primary goal, human language\nmodeling may yield effective approaches to ex-\ntend the context during language modeling. Thus,\nan aspect of this work can be seen as part of\nthe recent pursuit of sequence models that cap-\n623\nture dependencies beyond a ﬁxed context length\n(Dai et al., 2018; Beltagy et al., 2020). For ex-\nample, Keskar et al. (2019) and Dathathri et al.\n(2019) propose controllable language generation\nusing one or more attribute classiﬁers or control\ncodes. Guu et al. (2020) propose augmented lan-\nguage model pretraining with a latent knowledge\nretriever which allows the model to retrieve and\nattend over documents from a large corpus. These\nmodels extend context limits, but they do not\nmodel the higher order structure capturing a notion\nof the common source of documents i.e., the au-\nthor. On the other hand, Yoshida et al. (2020) ﬁts\na hierarchical model extension to language model-\ning by adding recurrence to a pretrained language\nmodel. This idea forms a basis for our proposed\nHULM architecture, HaRT, but Yoshida et al. do\nnot exploit the inherent higher order structure (i.e.\nthe model was not used for HULM).\n3 Human Language Modeling (H ULM)\nOur goal is to re-formulate the language modeling\ntask into one that directly enables a higher-order\ndependence structure that represents a human gen-\nerating the language.\nLanguage modeling formulations pose prob-\nabilistic questions over text represented as se-\nquences of tokens. The main goal is to model the\nprobability of observing a given token sequence in\nthe language as a whole. In particular language\nmodels (LMs) estimate the joint probability of the\ntokens in the string, deﬁned in terms of the proba-\nbilities of each token in the sequence conditioned\non the previous tokens.3 Given a string W ∈L, a\nsequence of n tokens ⟨w1, w2, ⋯, wn⟩, the proba-\nbility of observing the string W in the language L\nis computed as:\nPr (W) =\nn\n5\ni=1\nPr (wi∣w1∶i−1) (1)\nWe pose thehuman language modelingproblem\n(HuLM), where the goal is to model the probabil-\nities of observing a sequence from the language\nas generated by a speciﬁc person. An initial idea\nmight be to pose this task as conditioning the prob-\nability of a string, wi on a static representation of\n3Traditional LMs provide estimates of the conditional\nprobabilities often relying on further simplifying assumptions\n(e.g. Markovian assumptions to handle long sequences.).\nthe person (or user, Ustatic):\nPr (W∣Ustatic) =\nn\n5\ni=1\nPr (wi∣w1∶i−1, Ustatic)\n(2)\nThis addresses the ﬁrst of the two goals we pre-\nsented in the introduction, namely avoiding the\necological fallacy of assuming sequences from the\nsame person are independent. However, it does\nnot respect the idea that people vary in mood\nand can change. More precisely, human behav-\niors (language use) are inﬂuenced by dynamic hu-\nman states of being (Fleeson, 2001; Mehl and Pen-\nnebaker, 2003). Thus, we pose HuLM with a\nmore general formulation that enables the idea of\na dynamic representation of humans, the user state\nUt\n4:\nPr (Wt∣Ut−1) =\nn\n5\ni=1\nPr (wt,i∣wt,1∶i−1, U1∶t−1)\n(3)\nwhere t indexes a particular sequence of tempo-\nrally ordered utterances (e.g. a document, or set of\nsocial media message). While wt,i is drawn from a\nmultinomial distribution, U1∶t−1 can be from any\ndiscrete or continuous multivariate distribution.\nIn one extreme,U1∶t−1 could model all previous\ntokens in all previous documents by one person.\nIn the opposite extreme, U1∶t−1 can be the same\nfor all values of t, giving a static representation\nfor a user (equivalent to Equation 2) or even static\nacross users which reduces to a standard language\nmodeling version (equivalent to Equation 1). Still,\nmodeling a user via their previous documents pro-\nvides a seamless way to integrate the user infor-\nmation into language models – the only change is\nthat the models will now have to incorporate more\ntext when they are making predictions. Note that\nthis problem formulation does not directly require\nexplicit modeling of the identity of a user. This\nmakes it easier to handle new users in downstream\ntasks and test instances, or creating models that\ncan be further ﬁne-tuned to both document- and\nuser-level tasks.\nHuLM in Practice. Like traditional langauge\nmodels, there are two steps to applying HuLM\nbased models to most tasks and applications: pre-\ntraining and ﬁne-tuning. During pre-training, the\n4We deﬁne Ut as the state after the sequence, Wt. Thus,\nonly Ut−1 is accessible as given when estimating Pr( Wt)\nconditioned on the user state.\n624\nmodel is trained on unlabeled data over Human\nLanguage Modeling (HuLM) pre-training task\nabove. For ﬁnetuning, a HuLM based model is\nﬁrst initialized with the pre-trained parameters,\nand all of the parameters are ﬁne-tuned using\nlabeled data from the downstream tasks. Each\ndownstream task has separate ﬁne-tuned models,\neven though they are initialized with the same pre-\ntrained parameters.\n4 Human-aware Recurrent Transformer\nThis section introduces, HaRT, a human-aware re-\ncurrent transformer that trains on the human lan-\nguage modeling (HULM) formulation.\nHaRT is designed to produce human-aware con-\ntextual representations of text at multiple levels.\nHaRT’s design is motivated by two goals: (i) We\nwant to support hierarchical modeling, i.e., to hier-\narchically represent the set of all-messages written\nby a user and at the same time have human-aware\ncontextual word representations. This implicitly\nentails modeling large context size. For example,\nGPT-2 (Radford et al., 2019) uses a context size of\n1024 tokens, whereas our estimate of the average\ncontext size for a Twitter user is more than 12000\ntokens. (ii) To support user-level tasks (e.g. per-\nsonality assessment (Lynn et al., 2020)), we need\nrepresentations of the entire set of messages writ-\nten by a user capturing the inherent human states\nthat broadly encompasses the user representation.\nHaRT addresses the hierarchical language mod-\neling issue by processing all messages written by\na user in a temporally ordered sequence of blocks.\nIt uses a recurrence structure to summarize infor-\nmation in each block into a user state vector, which\nis then used to inform the attention between tokens\nin the subsequent block. For human-level tasks the\naggregate of user states can be used as the repre-\nsentation of the entire context for the user.\nThe idea of adding recurrence to pre-trained\ntransformers builds on Yoshida et al. (2020)’s\nmethod for handling long contexts. However, the\nmain difference is that HaRT models the input\ndata (language) in the context of its source (user)\nalong with inter-document context, thus enabling\na higher order structure representing human con-\ntext.\n4.1 HaRT Architecture\nFigure 1 shows the overall architecture for HaRT.\nIt consists of a one modiﬁed transformer layer\nFigure 1: HaRT architecture: HaRT processes a user’s\nmessages in blocks. It produces contextualized repre-\nsentations of messages in each block conditioning on a\nrecurrently computed user state. The user state is in-\nserted into an earlier layer (layer 2) to inform the self-\nattention computation via a modiﬁed query transform.\nThe previous user state is then recurrently updated us-\ning the output of a later layer (layer 11).\nwith a user-state based self-attention mecha-\nnism over more token-level standard self-attention\nbased transformer layers from a pre-trained trans-\nformer (GPT-2).\nInputs and Outputs Each input instance to HaRT\nconsists of a temporally ordered sequence of mes-\nsages (by message created time) from a given user\na, Ma = ⟨M1, ⋯, Mn⟩. We segment these mes-\nsages into ﬁxed sized blocks, Ba = ⟨B1, ⋯, Bk⟩.\nWe sequentially ﬁt messages into blocks, separat-\ning messages using a newly introduced special to-\nken <∣insep∣ >. If the number of tokens in a\nblock falls short of the block size, we ﬁll it with\npadded tokens. k is a hyperparameter during train-\ning used to cap the maximum number of blocks\ncontrolling the amount/size of user history that is\nfed to the model. If the messages for a user ﬁll\nless than k blocks, we pad the rest to maintain the\nsame size for each instance.\nFor each block Bi, HaRT outputs (i) contextual-\nized representations of the tokens within the block\nconditioned on the previous user state (Ui−1), and\n(ii) an updated representation of the user state, Ui,\nwhich now also includes the information from the\ncurrent block Bi. We use the representation of the\nlast non-pad token of a message as its representa-\ntion for message-level tasks, and use the average\nof the user-states from all the blocks of a user as\nthat user’s representation for user-level tasks.\nUser-State based Self-Attention HaRT con-\nstructs a user-state representation vector by com-\nbining information from each block in a recurrent\n625\nmanner. After processing the inputs in a given\nblock Bi, HaRT extends the previous user state\nUi−1 with information from current block Bi us-\ning the output representations H(E) from one of\nthe later layers (we denote as the extract layerLE).\nThe recurrence for the new user state Ui is:\nUi =tanh(WU Ui−1 + WHH(E)) (4)\nThe user state for the ﬁrst blockU0 is initialized\nwith the average of the (pretrained GPT-2) layer\n11 outputs for words from the messages of more\nthan 500 users (of the train set) computed using\nSchwartz et al. (2017).\nTo produce the user-state conditioned contex-\ntual representations at a given layer, HaRT uses\na modiﬁed self-attention procedure to one of the\nearlier layers, which we denote as the insert layer\n(LIN ). The idea is to create a new query transform\nwhich includes the user-state vector, so that the\nattention between tokens is informed by the con-\ntext of the previous messages written by the user.\nTo this end, we take input hidden states to this\ninsert layer HIN −1\ni , concatenate it with the user-\nstate vector from the previous blockUi−1 and then\napply a linear transformation (using Wq) to ob-\ntain the query vectors (QIN\ni ) for the self-attention\ncomputation.\nQIN\ni =W T\nq [H\n(IN −1)\ni ; Ui−1 ] (5)\nThe key, value transforms and the rest of the\nself-attention computation and further processing\nin the transformer to produce the output represen-\ntations from the layer, all remain the same as in the\noriginal GPT-2 model.\nImplementation Choices There are multiple al-\nternatives for a HaRT implementation including\nhow to construct the user state, where and how\nto inject user state information. In our prelimi-\nnary experiments we experimented with different\nextract layers but found that constructing user state\nfrom the penultimate layer (Layer 11) and inject-\ning the user state in a single earlier layer (Layer 2\nused by Yoshida et al. (2020)) to modify the query\ntransformation was the most effective empirically.\n4.2 Pre-training HaRT\nHaRT is pre-trained using the H ULM task in an\nautoregressive manner.\nThe H ULM task as deﬁned in Equation 3 asks\nto predict a token that appears in a token sequence\n(i.e. a user’s social media message) given the pre-\nvious tokens in the sequence while also condition-\ning on previous user states. We turn this task into\na pre-training objective deﬁned over block seg-\nmented token sequences from a user. For each\nblock of a given user, the task is to predict each\ntoken in the block while conditioning on (i) the\nprevious tokens within the current block which\nare directly available as input, and also (ii) the to-\nkens from the previous blocks that are available to\nHaRT through the recurrent user state. Formally,\nthe pre-training objective is to maximize:\n5\na∈Users\n∣Ba∣\n5\nt=1\n∣B(a)\nt ∣\n5\ni=1\nPr (wt,i∣wt,1∶i−1, B\n(a)\n1∶t−1) (6)\nwhere, wt,i is the ith token in the tth block (B\n(a)\nt )\nfor user a.\nPre-training data For the pre-training corpus\nwe combine a subset of the Facebook posts dataset\nfrom Park et al. (2015), a subset of the County\nTweet Lexical Bank (Giorgi et al., 2018) appended\nwith newer 2019 and 2020 tweets, in total span-\nning 2009 through 2020. We ﬁlter the datasets to\nonly include tweets marked as English from users\nwho have at least 50 total posts and at least 1000\nwords in total, ensuring moderate language history\nfor each user. The resulting dataset consists of just\nover 100,000 unique users, which we split into a\ntrain dataset consisting of messages from 96,000\nusers, a development dataset that consists of mes-\nsages from 2000 users that were not part of the\ntraining set (unseen) and new messages from 2500\nusers seen in the training set, and a test set of mes-\nsages from a separate set of 2000 unseen users that\nare neither in training or the development set.\nWe refer to this as the HuLM-Corpus (HLC).\n4.3 Fine-tuning HaRT\nIn the tradition of transformers for traditional lan-\nguage modeling, HaRT shares the same archi-\ntecture for both pre-training and ﬁne-tuning ex-\ncept for the output layers. It has a uniﬁed ar-\nchitecture across different downstream tasks. For\nﬁnetuning, HaRT is ﬁrst initialized with the pre-\ntrained parameters, and all of the parameters are\nﬁne-tuned using labeled data from the downstream\ntasks. Each downstream task has separate ﬁne-\ntuned models, even though they are initialized\nwith the same pre-trained parameters. Apart from\nusing the labeled data from the downstream tasks,\n626\nwe also use the historical messages (when avail-\nable) from the respective users to replicate the for-\nmat of pre-training inputs and to beneﬁt from the\nknowledge of the user.\n5 Evaluation: Human Language\nModeling\nWe seek to compare HaRT with a standard lan-\nguage model that is exposed to the same data but\nwithout modeling the notion of a user. Thus,\nwe compare HaRT’s human language modeling\nperformance to the model it was based, GPT-2.\nFor calibration we report performance on GPT-2’s\noriginal pre-trained version (GPT-2frozen), and a ver-\nsion of the LM that was ﬁne-tuned on the HuLM-\nCorpus (GPT-2HLC).\nWe train and evaluate the models using the train\nand test splits of the HuLM-Corpus described in\nSection 4.2. For hyperparameter search, we use\nthe full development set of both seen and unseen\nusers. Each training instance for HaRT is capped\nto 8-blocks of 1024-tokens each. Following previ-\nous work ﬁne-tuning transformer language models\nfor social media (V Ganesan et al., 2021), GPT-\n2 was trained over individual messages. We train\nboth for ﬁve epochs and set the learning rate, batch\nsize, and stopping patience based on the devel-\nopment set (see Appendix A.3). For HaRT, we\ninitialize all GPT-2 self-attention layers with the\ncorresponding weights in the pre-trained GPT-2.\nThe user-state based self-attention layer weights\n(query, key, and value) are normal initialized with\n0 mean and 0.02 standard deviation.\nPerplexity Table 1 reports the perplexity of all\nthree models on the messages from the unseen\nusers of the development split and the entire test\nsplit of HuLM-Corpus. The frozen pre-trained\nGPT-2 (GPT-2frozen) fares poorly to the domain\nmismatch while the ﬁne-tuned version (GPT-2HLC)\nfares much better. However, the human language\nmodel HaRT achieves the best performance by a\nlarge margin, with a signiﬁcant reduction in per-\nplexity by more than 46% on the test set relative\nto GPT-2HLC (p <.001).5\nEffect of History Size. We further analyze the\neffect of history size by varying the amount of lan-\nguage, in terms of blocks, used per user. Figure 2\n5In addition to this improvement for unseen users, we also\nsee similar relative beneﬁts when tested on instances from\nseen users which we report in Appendix A.2.\nModel Dev (ppl) Test (ppl)\nGPT-2frozen 112.82 116.35\nGPT-2HLC 47.61 48.51\nHaRT 27.49* 26.11*\nTable 1: Comparing HaRT as a language model to\nGPT-2frozen, the frozen pre-trained GPT-2 and GPT-2HLC,\nthe GPT-2 model ﬁne-tuned on the HuLM-Corpus.\nHaRT shows large gains with a substantial reduction in\nperplexity compared to both versions of GPT-2. Bold\nfont indicates best in column and * indicates statistical\nsigniﬁcance p <.05 via permutation test w.r.t GPT-2HLC\nFigure 2: . Perplexity scores, on test sets as a func-\ntion of history size (number of blocks) used when\ntraining HaRT. Each block consists of 1024 tokens.\nAdding more history improves language modeling per-\nformance with big reduction going from 2 to 4 blocks\nand a smaller reduction from 4 to 8 blocks.\nshows that adding more history in general helps,\nwith a big reduction in perplexity going from 2 to\n4 blocks and a further reduction going from 4 to 8\nblocks. Adding more context can induce a need to\neffectively balance likelihood of ﬁnding more im-\nportant signals against the increasing chances of it\ndrowning in less important information.\n6 Evaluation: Fine-tuning for\nDownstream Tasks\nHere, we evaluate the utility of ﬁne-tuning HaRT\nfor document- and user-level tasks. Just as stan-\ndard transformer language models are ﬁne-tuned\nfor tasks, we take our pre-trained HaRT model and\nﬁne-tune it for stance detection, sentiment clas-\nsiﬁcation, age estimation, and personality (open-\nness) assessment tasks. For both sets of tasks\nwe compare ﬁne-tuning the GPT-2 HLC as a non-\nuser-based LM baseline and also report previously\npublished results from other task speciﬁc models,\nmost of which employ historical context for re-\n627\nModel Age\n(r)\nOPE\n(rdis)\nStance\n(F1)\nSentiment\n(F1)\nGPT-2HLC 0.839 0.521 68.60 76.75\nHaRT 0.868* 0.619* 71.10* 78.25*\nTable 2: We ﬁne-tune HaRT and GPT-2 HLC (GPT-2\nﬁne-tuned for LM on the same data) for 4 downstream\ntasks: Age, Openness (OPE), Stance, and Sentiment,\nand ﬁnd HaRT to perform better on all 4 tasks. For\nage and openness, we ﬁne-tune HaRT only for the re-\ncurrence module, and ﬁne-tune only the last 2 layers\nof GPT-2HLC. For stance and sentiment, we ﬁne-tune\nfull models. Results are reported in pearson r for Age,\ndisattenuated pearson r for OPE and weighted F1 for\nStance/Sentiment. Bold indicates best in column and *\nindicates statistical signiﬁcance p < .05 via permtua-\ntion test.\nspective tasks. All hyperparameter settings and\ntraining details for the GPT-2HLC and HaRT models\nfor each task are listed in Appendix A.3.\n6.1 Document-Level Tasks\nWe consider two document-level tasks that require\nmodels to read an input document (message) writ-\nten by a user and output a label (stance of the user\ntowards a topic or the sentiment expressed in the\ntext). To ﬁne-tune HaRT on these tasks, with each\ndocument we collect and attach previous mes-\nsages written by the same users, represented using\nthe procedure we outlined in Section 4.3. Thus,\nHaRT processes this input to produce message-\nand human-contextualized token-level representa-\ntions. We represent the document by its last non-\npadded token representation and feed it to classiﬁ-\ncation layer with a prior layer norm for predicting\nthe output label. GPT-2 HLC, without hierarchical\nstructure, only uses the input document to make\npredictions. We ﬁne-tune all parameters of HaRT\nand GPT-2HLC, as well as the classiﬁcation layer\nweights using the standard cross-entropy loss (cal-\nculated only over the last non-padded token of the\ntarget (labeled) messages).\nStance Detection. For stance detection we use\nthe SemEval2016 dataset (Mohammad et al.,\n2016), which contains tweets annotated as being\nin favor of, against, or neutral toward one of ﬁve\ntargets: atheism, climate change as a real con-\ncern, feminism, Hillary Clinton, and legalization\nof abortion. This data only includes labeled tweets\nfrom users and not any history, so we use the ex-\ntended dataset from Lynn et al. (2019) and pre-\nModel Stance\n(F1)\nSentiment\n(F1)\nMFC 54.2 28.0\nLynn et al. (2019) 65.9 69.5\nMeLT 66.6 63.0\nBERTweet 68.8 77.9\nHaRT 71.1* 78.3*\nTable 3: We compare HaRT’s performance on docu-\nment level downstream tasks: Stance and Sentiment,\nagainst state of the art results. We also ﬁne-tuned pre-\ntrained GPT-2, BERTweet (Nguyen et al., 2020), and\nMeLT (Matero et al., 2021) on both tasks for baselines.\nHaRT performs the best in both tasks with a substan-\ntial gain. Results are reported in weighted F1. Bold\nindicates best in column and * indicates statistical sig-\nniﬁcance p <.05 w.r.t BERTweet via permutation test.\nserve the train/dev/test split of the same. To main-\ntain (message created time) temporal accuracy in\nour autoregressive model, we only used the part of\nthe extended dataset (history) that consists of mes-\nsages posted earlier than the labeled messages.\nSentiment Analysis. We use message-level sen-\ntiment annotations indicating positive, negative,\nand neutral categories from the SemEval-2013\ndataset (Nakov et al., 2013). As with stance, we\nuse a part of the extended dataset from Lynn et al.\n(2019) to get associated message history, and pre-\nserve the train/dev/test split of the same.\n6.2 User-Level Tasks\nWe evaluate HaRT for age estimation and person-\nality (openness) assessment, social scientiﬁc tasks\nwhich require producing outcomes at the user-\nlevel. We use a subset of the data from con-\nsenting users of Facebook who shared their Face-\nbook posts along with demographic and personal-\nity scores (Kosinski et al., 2013; Park et al., 2015).\nFor these user-level tasks we can leverage the\nrecurrent user states in HaRT to produce a repre-\nsentation of the user. We represent the input as de-\nscribed in Section 4.3, and use the average of the\nuser-states vectors from the non-padded blocks of\neach user and layer norm it to make predictions\nusing a linear classifying layer to predict 1 label\n(regression task). We use only 4 blocks of history\nwhen training to ﬁne-tune.\nFor GPT-2HLC, since it can’t directly handle all\nof the users text in one go, we replicate the user\nlabel for each message of the respective users and\n628\ntrain the model to predict the label for each mes-\nsage using the last non-padded token of the mes-\nsage. To make the ﬁnal prediction, we average\nthe predictions across all messages from respec-\ntive users and calculate the performance metric us-\ning this average as in (V Ganesan et al., 2021).\nFor these user level tasks that require aggregate\ninformation, for both models, ﬁne-tuning the en-\ntire set of parameters was worse than ﬁne-tuning\nfewer layers. For GPT-2 HLC ﬁne-tuning only the\nlast two layers gave the best performance. For\nHaRT ﬁne-tuning only the recurrence module gave\nthe best performance on development sets. We re-\nport results with these best dev settings. We use\nthe mean squared error (MSE) as the training loss.\nAge Estimation Similar to the pre-training data,\nwe ﬁltered the above dataset for English language\ninstances and included only the users with a min-\nimum of 50 posts and a minimum of 1000 words.\nAge was self-reported and limited to those 65\nyears or younger. This resulted in a dataset of\n56,930 users in train, 1836 users in dev, and 4438\nusers in test which was a subset of the test set\n(5000 users) from Park et al. (2015). We evalu-\nate on both the test sets and report Pearson corre-\nlation (r) metric on the latter for comparison pur-\nposes. We include results with the ﬁltered data in\nAppendix (Table 8).\nPersonality Assessment. We evaluate on the as-\nsessment of openness based on language (one’s\ntendency to be open to new ideas) (Schwartz et al.,\n2013). To allow for direct comparisons, we use the\nsame test set (n=1,943) as Lynn et al. (2020) and\nuse a subset of their training set (66,764 users) of\nwhich 10% were sampled as dev set, and report\ndisattenuated pearson correlation (rdis) to account\nfor questionnaire reliability Lynn et al. (2018). As\nwith age estimation, we report results with the ﬁl-\ntered dataset in Appendix (Table 8).\n6.3 Results\nTable 2 summarizes the performance of HaRT\nagainst the baseline of ﬁne-tuning a non-human-\naware language model, GPT-2 HLC. We see that\nHaRT yields substantial gains over GPT-2 HLC\nacross both user-level and document-level tasks,\ndemonstrating clear beneﬁts in all settings.\nDocument-Level Tasks Table 3 compares HaRT\nwith task-speciﬁc baselines for stance and senti-\nment detection including (i) Lynn et al. (2020)\nwhich used historical contexts to incorporate both\nModel Age (r) OPE (rdis)\nV Ganesan et al. (2021) 0.795 0.511\nSap et al. (2014) 0.831 -\nLynn et al. (2020) - 0.626\nHaRT 0.868* 0.619\nTable 4: Comparison of HaRT’s performance on user\nlevel downstream tasks: Age and Openness (OPE),\nagainst state of the art results. V Ganesan et al. (2021)\nuse lesser number of users (10000) in training. Re-\nsults are reported in pearson r for Age and disattenu-\nated pearson r for OPE. Bold indicates best in column\nand * indicates statistical signiﬁcance between HaRT\nand (Sap et al., 2014) (p <.05) using a bootstrap sam-\npling test. We also ﬁnd no statistical difference be-\ntween HaRT and (Lynn et al., 2020) (p =.35).\nexplicit and text-derived latent human factors, (ii)\nMeLT (Matero et al., 2021) which used a super-\nset of the same historical contexts used here but\nfor message-level language modeling, and (iii)\nBERTweet (Nguyen et al., 2020) which uses a\nlarge collection of tweets to pretrain an autoen-\ncoder that is then ﬁne-tuned for target tasks. Senti-\nment results are weighted F1 scores over the three\nsentiment categories. Stance results are an average\nof weighted F1 scored over ﬁve different topics\nfrom respective topic-speciﬁc ﬁne-tuned models.\nHaRT outperforms all models demonstrating the\nsubstantial beneﬁts of human language modeling\nfor these document-level downstream tasks.\nUser-Level Tasks Table 4 compares HaRT with\ntask-speciﬁc baselines for Age and Openness tasks\nthat use the superset of the same data used by\nHaRT. For Age, HaRT outperforms all baselines\nincluding a strong non-neural lexica based pre-\ndictor (Sap et al., 2014), and a RoBERTa-based\nsystem that uses carefully chosen frozen embed-\ndings (V Ganesan et al., 2021). For Openness,\nHaRT is better than the frozen RoBERTa (Liu\net al., 2019) embeddings and is comparable to\nLynn et al. (2020)’s hierarchical attention model.\nThese results also suggest the potential of HaRT’s\nuser states as a representation for user-level tasks.\n6.4 No Historical Context.\nHaRT can also be used anywhere a typical trans-\nformer language model is used by simply not feed-\ning any historical context. Here, we seek to use\nour pre-trained HaRT as a language model that\nis ﬁne-tuned to the messages (for the respective\ntasks) without any historical context. Table 5 com-\n629\nModel Sentiment\n(F1)\nStance\n(F1)\nGPT-2HLC frozen 62.7 57.7\nHaRT nohist, frozen 62.7 58.6\nGPT-2HLC 76.8 68.6\nHaRT nohist 77.7* 70.8*\nTable 5: Results with experiments on Stance and Sen-\ntiment downstream tasks using only the labeled in-\nstances and no history. We compare HaRT with GPT-\n2HLC by training only the classiﬁcation head ( frozen)\nand additionally, by ﬁne-tuning the models. Bold in-\ndicates best in column and * indicates statistical sig-\nniﬁcance p < .05 via permutation test w.r.t GPT-2HLC.\nResults are reported in weighted F1.\nModel Sentiment\n(F1)\nStance\n(F1)\nHaRT NOT PT 63.47 66.26\nHaRT W/O RECUR 77.04 68.73\nHaRT 78.25* 71.10*\nTable 6: Results with the ablation experiments on\nStance and Sentiment downstream tasks. We experi-\nment without the recurrence module (W/o recur), and\nHaRT without HuLM PT, and compare with HaRT.\nBold indicates best in column and * indicates statisti-\ncal signiﬁcance p <.05 via permutation test w.r.t HaRT\nw/o recur. Results are reported in weighted F1.\npares the performances of HaRT and GPT-2HLC for\nthe two document-level downstream tasks Stance,\nand Sentiment. For a fair comparison, we use the\nsame data inputs for both the pre-trained models\nwhich consists of only the labeled messages and\nno historical context. We evaluate in 2 ways: 1)\nfreezing the model and training only the classiﬁca-\ntion layer using the outputs from the penultimate\ntransformer layer, and 2) ﬁne-tuning all model pa-\nrameters along with a classiﬁcation head with a\nlayer norm prior to it. HaRT is at par or better\nwith GPT-2HLC for both frozen and ﬁne-tuned ver-\nsions, showing that it can provide gains even when\nhistorical context is unavailable. Hyperparameters\nsettings are described in Appendix A.3.\n6.5 Ablation Studies\nIn this section, we perform ablation experiments\non HaRT to better understand their relative impor-\ntance and report the results in Table 6.\nPre-training We assess the impact of pre-training\nby evaluating the downstream performance of a\nversion of the HaRT model that has not been pre-\ntrained on the HuLM task. Instead of using the\nweights from HuLM pre-training, we use HaRT\nwith initialized weights as described in Section 5.\nTable 6 shows HuLM pre-training beneﬁts – pre-\ntraining adds substantial gain of 14.78 points and\n4.84 points in weighted F1 for sentiment analysis\nand stance detection respectively.\nRecurrence We assess the importance of recur-\nrent user state by ﬁrst pre-training HaRT without\nits recurrent module and then ﬁne-tuning it for the\ndownstream tasks. We still use the same batch-\ning as described in Section 4.2 but the informa-\ntion from a block no longer propagates to the next\nblock in the forward pass, and backpropagation is\nstill done on all blocks of a user together. With-\nout the recurrence module we see a drop of 1.21\npoints and 2.37 points in the weighted F1 mea-\nsure for sentiment and stance respectively. Inter-\nestingly, HaRT outperforms HaRT without recur-\nrence, consistent with the idea that models beneﬁt\nfrom user history on tasks that involve a user.\n7 Conclusions\nLanguage is deeply human. Yet, language mod-\nels in wide-spread use today lack a notion of the\nhuman that generates the language. Motivated by\nother advances in human-centered language pro-\ncessing and psychological theory that suggest lan-\nguage is moderated by human states, we intro-\nduced human language modeling . H ULM ex-\ntends LMs with the notion of a user and their\nstates via their previous messages. In this ﬁrst\nstep toward large human language models, we de-\nveloped a human-aware transformer (HaRT) that\nuses a recurrence mechanism to model user states\nand show that pre-training this transformer on the\nhuman language modeling task yields signiﬁcant\ngains in both generation and ﬁne-tuning for multi-\nple downstream document- and user-level tasks.\nOverall, state-of-the-art results with HaRT, a\nmodel neither trained on substantially larger data\nnor adding many parameters, suggests progress\nfor transformers not based on massive increases in\ndata or parameters but on a task grounded in lan-\nguage’s “natural” generators, people.\n8 Ethical Considerations\nWhile the multi-level human-document-word\nstructure within HULM can enable bias correcting\nand fairness techniques (discussed next), the abil-\nity to better model language in its human context\n630\nalso presents opportunities for unintended harms\nor nefarious exploitation. For example, mod-\nels that improve psychological assessment are not\nonly useful for research and clinical applications,\nbut could be used to target content for individuals\nwithout their awareness or consent. In the con-\ntext of use for psychological research, such mod-\nels may risk release of private research partici-\npant information if trained on private data without\nchecks for exposure of identifying information. To\nnegate this potential, we only release a version of\nHaRT that is without training on the consented-\nuse private Facebook data until differential privacy\nstandards can be veriﬁed. Unlike other human-\ncentered approaches, HaRT is not directly fed user\nattributes as part of the pre-training thus the model\nparameters do not directly encode user attributes.\nHULM aims to join a growing body of work to\nmake AI more human-centered, and thus more ap-\nplicable for interdisciplinary study of the human\ncondition as well as leading to new clinical tools\nfor psychological health. At this point, our mod-\nels are not intended to be used in practice for men-\ntal health care nor labeling of individuals publicly\nwith personality or age scores. While modeling\nthe human state presents opportunities for reduc-\ning AI bias, prior to clinical or applied use, such\nmodels should be evaluated for failure modes such\nas error across target populations for error or out-\ncome disparities (Shah et al., 2020). All user-level\ntasks presented here were reviewed and approved\nor exempted by an academic institutional review\nboard (IRB).\n9 Acknowledgments\nThis work was supported by DARPA via Young\nFaculty Award grant #W911NF-20-1-0306 to\nStony Brook University; the conclusions and opin-\nions expressed are attributable only to the authors\nand should not be construed as those of DARPA or\nthe U.S. Department of Defense. This work was\nalso supported in part by NIH R01 AA028032-01\nand by the National Science Foundation under the\ngrant IIS-1815358.\nReferences\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase,\nTakeru Ohta, and Masanori Koyama. 2019. Op-\ntuna: A next-generation hyperparameter optimiza-\ntion framework. In Proceedings of the 25th ACM\nSIGKDD international conference on knowledge\ndiscovery & data mining, pages 2623–2631.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150.\nZihang Dai, Zhilin Yang, Yiming Yang, William W.\nCohen, J. Carbonell, Quoc V . Le, and R. Salakhut-\ndinov. 2018. Transformer-xl: Language modeling\nwith longer-term dependency.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: A simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nGianmarco De Francisci Morales, Aristides Gionis,\nand Claudio Lucchese. 2012. From chatter to head-\nlines: harnessing the real-time web for personalized\nnews recommendation. In Proceedings of the ﬁfth\nACM international conference on Web search and\ndata mining, pages 153–162.\nEdouard Delasalles, Sylvain Lamprier, and Ludovic\nDenoyer. 2019. Learning dynamic author represen-\ntations with temporal language models. In 2019\nIEEE International Conference on Data Mining\n(ICDM), pages 120–129. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nZhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007.\nA large-scale evaluation and analysis of personal-\nized search strategies. In Proceedings of the 16th\ninternational conference on World Wide Web, pages\n581–590.\nWilliam Fleeson. 2001. Toward a structure-and\nprocess-integrated view of personality: Traits as\ndensity distributions of states. Journal of person-\nality and social psychology, 80(6):1011.\nSalvatore Giorgi, Daniel Preo¸ tiuc-Pietro, Anneke Buf-\nfone, Daniel Rieman, Lyle Ungar, and H. Andrew\nSchwartz. 2018. The remarkable beneﬁt of user-\nlevel aggregation for lexical-based population-level\npredictions. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 1167–1172, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\n631\nIdo Guy, Naama Zwerdling, David Carmel, Inbal Ro-\nnen, Erel Uziel, Sivan Yogev, and Shila Ofek-\nKoifman. 2009. Personalized recommendation of\nsocial software items based on social relations. In\nProceedings of the third ACM conference on Rec-\nommender systems, pages 53–60.\nDirk Hovy. 2015. Demographic factors improve clas-\nsiﬁcation performance. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 752–762, Beijing,\nChina. Association for Computational Linguistics.\nDirk Hovy and Diyi Yang. 2021. The importance of\nmodeling social factors of language: Theory and\npractice. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 588–602, Online. Association for\nComputational Linguistics.\nXiaolei Huang and Michael J Paul. 2019. Neural user\nfactor adaptation for text classiﬁcation: Learning to\ngeneralize across author demographics. In Proceed-\nings of the Eighth Joint Conference on Lexical and\nComputational Semantics (* SEM 2019).\nAaron Jaech and Mari Ostendorf. 2018. Personalized\nlanguage model for query auto-completion. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 700–705, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, J. Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nMilton King and Paul Cook. 2020. Evaluating ap-\nproaches to personalizing language models. In Pro-\nceedings of the 12th Language Resources and Eval-\nuation Conference , pages 2461–2469, Marseille,\nFrance. European Language Resources Association.\nMichal Kosinski, David Stillwell, and Thore Grae-\npel. 2013. Private traits and attributes are pre-\ndictable from digital records of human behavior.\nProceedings of the National Academy of Sciences ,\n110(15):5802–5805.\nLihong Li, Wei Chu, John Langford, and Robert E\nSchapire. 2010. A contextual-bandit approach to\npersonalized news article recommendation. In Pro-\nceedings of the 19th international conference on\nWorld wide web, pages 661–670.\nZih-Wei Lin, Tzu-Wei Sung, Hung-Yi Lee, and Lin-\nShan Lee. 2017. Personalized word representations\ncarrying personalized semantics learned from social\nnetwork posts. In 2017 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU),\npages 533–540. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nVeronica Lynn, Niranjan Balasubramanian, and H. An-\ndrew Schwartz. 2020. Hierarchical modeling for\nuser personality prediction: The role of message-\nlevel attention. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5306–5316, Online. Association for\nComputational Linguistics.\nVeronica Lynn, Salvatore Giorgi, Niranjan Balasubra-\nmanian, and H. Andrew Schwartz. 2019. Tweet\nclassiﬁcation without the tweet: An empirical ex-\namination of user versus document attributes. In\nProceedings of the Third Workshop on Natural Lan-\nguage Processing and Computational Social Sci-\nence, pages 18–28, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nVeronica Lynn, Alissa Goodman, Kate Niederhof-\nfer, Kate Loveys, Philip Resnik, and H. Andrew\nSchwartz. 2018. CLPsych 2018 shared task: Pre-\ndicting current and future psychological health from\nchildhood essays. In Proceedings of the Fifth Work-\nshop on Computational Linguistics and Clinical\nPsychology: From Keyboard to Clinic, pages 37–46,\nNew Orleans, LA. Association for Computational\nLinguistics.\nVeronica Lynn, Youngseo Son, Vivek Kulkarni, Ni-\nranjan Balasubramanian, and H Andrew Schwartz.\n2017. Human centered nlp with user-factor adap-\ntation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1146–1155.\nMatthew Matero, Nikita Soni, Niranjan Balasubra-\nmanian, and H. Andrew Schwartz. 2021. MeLT:\nMessage-level transformer with masked document\nrepresentations as pre-training for stance detection.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2959–2966, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nMatthias R Mehl and James W Pennebaker. 2003. The\nsounds of social life: a psychometric analysis of stu-\ndents’ daily social environments and natural conver-\nsations. Journal of personality and social psychol-\nogy, 84(4):857.\nFatemehsadat Mireshghallah, Vaishnavi Shrivastava,\nMilad Shokouhi, Taylor Berg-Kirkpatrick, Robert\nSim, and Dimitrios Dimitriadis. 2021. Useriden-\ntiﬁer: Implicit user representations for simple and\n632\neffective personalized sentiment analysis. arXiv\npreprint arXiv:2110.00135.\nSaif M. Mohammad, Svetlana Kiritchenko, Parinaz\nSobhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemeval-2016 task 6: Detecting stance in tweets. In\nProceedings of the International Workshop on Se-\nmantic Evaluation, SemEval ’16, San Diego, Cali-\nfornia.\nP Nakov, S Rosenthal, Z Kozareva, V Stoyanov, A Rit-\nter, and T Wilson. 2013. Task 2: Sentiment analysis\nin twitter. In Proceedings of the 7th International\nWorkshop on Semantic Evaluation, Atlanta, Geor-\ngia.\nDat Quoc Nguyen, Thanh Vu, and Anh-Tuan Nguyen.\n2020. Bertweet: A pre-trained language model for\nenglish tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 9–14.\nGregory J. Park, H. A. Schwartz, J. Eichstaedt, Mar-\ngaret L. Kern, M. Kosinski, D. Stillwell, L. Ungar,\nand M. Seligman. 2015. Automatic personality as-\nsessment through social media language. Journal of\npersonality and social psychology, 108 6:934–52.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? ArXiv, abs/1909.01066.\nSteven Piantadosi, David P Byar, and Sylvan B Green.\n1988. The ecological fallacy. American journal of\nepidemiology, 127(5):893–904.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nMaarten Sap, Gregory Park, Johannes Eichstaedt, Mar-\ngaret Kern, David Stillwell, Michal Kosinski, Lyle\nUngar, and Hansen Andrew Schwartz. 2014. Devel-\noping age and gender predictive lexica over social\nmedia. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1146–1151, Doha, Qatar. Associa-\ntion for Computational Linguistics.\nH Andrew Schwartz, Johannes C Eichstaedt, Mar-\ngaret L Kern, Lukasz Dziurzynski, Stephanie M Ra-\nmones, Megha Agrawal, Achal Shah, Michal Kosin-\nski, David Stillwell, Martin EP Seligman, et al.\n2013. Personality, gender, and age in the language\nof social media: The open-vocabulary approach.\nPloS one, 8(9):e73791.\nH Andrew Schwartz, Salvatore Giorgi, Maarten Sap,\nPatrick Crutchley, Lyle Ungar, and Johannes Eich-\nstaedt. 2017. Dlatk: Differential language analysis\ntoolkit. In Proceedings of the 2017 conference on\nempirical methods in natural language processing:\nSystem demonstrations, pages 55–60.\nDeven Santosh Shah, H Andrew Schwartz, and Dirk\nHovy. 2020. Predictive biases in natural language\nprocessing models: A conceptual framework and\noverview. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5248–5264.\nDavid G Steel and D Holt. 1996. Analysing and adjust-\ning aggregation effects: the ecological fallacy revis-\nited. International Statistical Review/Revue Inter-\nnationale de Statistique, pages 39–60.\nJaime Teevan, Susan T Dumais, and Eric Horvitz.\n2005. Personalizing search via automated analy-\nsis of interests and activities. In Proceedings of\nthe 28th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 449–456.\nAdithya V Ganesan, Matthew Matero, Aravind Reddy\nRavula, Huy Vu, and H. Andrew Schwartz. 2021.\nEmpirical evaluation of pre-trained transformers for\nhuman-level NLP: The role of sample size and di-\nmensionality. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4515–4532, Online. As-\nsociation for Computational Linguistics.\nCharles Welch, Jonathan K. Kummerfeld, Verónica\nPérez-Rosas, and Rada Mihalcea. 2020a. Composi-\ntional demographic word embeddings. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4076–4089, Online. Association for Computational\nLinguistics.\nCharles Welch, Jonathan K. Kummerfeld, Verónica\nPérez-Rosas, and Rada Mihalcea. 2020b. Explor-\ning the value of personalized word embeddings. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 6856–6862,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in neural infor-\nmation processing systems, 32.\n633\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel.\n2020. Adding recurrence to pretrained transform-\ners for improved efﬁciency and context size. arXiv\npreprint arXiv:2008.07027.\nA Appendix\nA.1 Pre-training\nTwitter Data CollectionAs mentioned, in section\n4.2, we use a combination of data from both Twit-\nter and Facebook data sources. However, since\nthe main Twitter corpus (Giorgi et al., 2018) only\nspans the years 2009 - 2015, we wanted to supple-\nment our total corpus with newer language data.\nGenerally, we follow the same procedures for data\ncollection as introduced for the 2009 - 2015 years.\nThus, we started with a 1% random sample ofpub-\nlicly available tweets that can be mapped to US\ncounties. On top of this we also applied the fol-\nlowing ﬁlters: (1) Removal of non-English tweets,\n(2) Removal of users who did not tweet at least\n3 times a week, (3) Removal of any duplicates\namong the collected data, and (4) Removal of any\ntweets containing URLs. We will be including this\nadditional data as part of the CTLB project6.\nData Size and Splits We sample evenly be-\ntween Facebook and Twitter at the user-level to\ncollect 50,000 from each and apply the same min-\nimum language use requirement of 1,000 words\nspanning 50 messages. We show the details of the\nsplits across training/development/testing as well\nas seen/unseen user categories in ﬁgure 3. We\nkeep 4,000 users for development and testing, 2k\nfor each split, that are not at all present in the train-\ning portion. For users that we do train on, we se-\nlect 4,500 to keep 20% of their messages for de-\nvelopment and testing sets.\nA.2 Perplexity on Seen versus Unseen Users\nBeneﬁt of Seen users. By default, our experi-\nments are run under an ‘unseen user’ condition\nwhere by the test corpus contains users that were\nnot in HaRT’s training corpus. However, one\ncould argue that this is an unnecessary impairment\nsince further training the human language model\ndoesn’t require labels and can often be run on test\ndata. We compare the effect of having seen users\nduring HaRT training by additionally calculating\nperplexity on test sets with seen users. To make it\na fair comparison, since we found our “seen user”\ncorpus was more difﬁcult (perplexity on seen users\n6https://github.com/wwbp/county_tweet_lexical_bank\nUnseen users Seen users\nModel ppl adj-ppl ppl adj-ppl\nGPT-2HLC 48.5 1.00 53.7 1.00\nHaRT 27.5 0.57* 27.6 0.51*\nTable 7: Evaluation of beneﬁt of having seen the users\nduring HaRT training. We use adjusted perplexity (adj-\nppl): the ratio of the perplexity to the upper-bound from\nnot using HaRT during training (i.e.GPT-2 HLC) on the\nsame test set – lower implies better performance when\nnormalized by difﬁculty of the test set. Seen users test\nset is the set with the messages from the users also\navailable in the train set, while unseen users test set\ndoes not have users common with the train set and is\nthe same as the test set in Table 1. Seen users test\nset is harder for both models. However, normalizing\nthe scores show HaRT to have better performance over\nseen users test set. Bold font indicates best in column\nand * indicates statistical signiﬁcance p <.05 via per-\nmutation test.\ntest set was higher than unseen users test set for\nGPT-2HLC as well), we use an adjusted perplexity,\ndeﬁned as the ratio of the model’s perplexity di-\nvided by a non-HULM upper-bound perplexity on\nthe same test set (GPT-2 HLC), normalizing by the\ndifﬁculty of the test set. As shown in Table 7, we\nﬁnd a small but signiﬁcant beneﬁt to having seen\nthe users during training.\nA.3 Experimental Settings\nWe use Open AI’s pre-trained GPT-2 base model\nfrom Radford et al. (2019) made available by the\nHugging Face library from Wolf et al. (2019)\n(transformers version 4.5.1) as our base model.\nWe also make use of Hugging Face’s code base\nto implement HuLM. Our training procedure in-\nvolves all the default training hyperparameters\nfrom Hugging Face’s GPT2 conﬁg except learn-\ning rate and the other speciﬁc hyperparams men-\ntioned in the paper. We run a learning rate search\nsweep on a sampled dataset, for both HaRT and\nGPT-2HLC, using the Optuna framework from Ak-\niba et al. (2019): 1) in a range of 5e-6 to 5e-4,\nwith 3 trials each of 5 epochs for pre-training,\n2) in a range of 5e-6 to 5e-4, with 10 trials each\nof 15 epochs for ﬁne-tuning stance detection, and\n3) in a range of 1e-7 to 1e-5, with 5 trials each\nof 15 epochs for ﬁne-tuning sentiment analysis.\nWe also setup an early stopping criteria for the\ndownstream task trials, such that we continue the\nepoch runs till we hit an increase in loss for 3 con-\nsecutive runs, and pick the model with the best\n634\nFigure 3: Structure of our pre-training dataset visually showing the data source(FB vs Twt), train-\ning/development/testing splits, and seen/unseen users for training and testing. Our dataset totals 100,000 users\nand approximately 37 million messages. Due to GPU memory restrictions, we limited training to 8 blocks of\nhistory which brought our train dataset size to 17 million messages. Dev and Test sets were not limited during\nevaluation.\nF1 score. We couldn’t run a similar sweep for\nuser-level tasks due to compute time limits so we\ntry a couple learning rates from document-level\ntasks but found the same learning rate that we\nuse for pre-training to be better. Many of the ex-\nperimental/hyperparameters (batch sizes, window\nsequence sizes and cappings) settings mentioned\nthroughout this work including the number of tri-\nals and the number of epochs vary because of com-\nputational limitations based on data size and train-\ning time.\nAll pre-training runs are trained on 2 Tesla V100\nGPUs of 32GB. Training HaRT takes approx 16\nhours for 1 epoch (with train data consisting of\n8 blocks (each of 1024 tokens) of 96000 users).\nFine-tuning tasks run on a mix of Tesla V100,\nQuadro RTX 8000, and A100 GPUs based on\ncompute availability. All batch sizes mentioned\nare per GPU.\nPre-training Settings We use 2.4447e-4 as the\nlearning rate for training HaRT, with 1 user train\nbatch size, 15 users eval batch size and early stop-\nping patience set to 3. For GPT-2 HLC, we use the\ndefault settings from Wolf et al. (2019) with train\nand eval batch size set to 60 and early stopping\npatience set to 3.\nDocument-level Fine-tuning Settings We ﬁne-\ntune HaRT for document-level tasks on their\nrespective training data with an input instance\ncapped to 8 blocks of 1024 tokens each, and no\ncapping during evaluation. We train for 15 epochs\nusing train and dev sets - along with history where\navailable - with 1 user train batch size, 20 users\neval batch size and early stopping patience set to\n6. All models converge within 5 epochs except one\nstance target - feminism. GPT-2 HLC is ﬁne-tuned\nwith the same data - but not history - using the\nsame settings except a different learning rate (from\nthe hyperparameter sweep mentioned above), train\nand eval batch size of 60, and max tokens per mes-\nsage set to 200 (consistent with pre-training).\nUser-level Fine-tuning Settings We ﬁne-tune\nHaRT for user-level tasks with an input instance\ncapped to 4 blocks of 1024 tokens each, and evalu-\nation data capped to 63 blocks (to allow for dev set\nevaluation due to compute limitations). For ﬁne-\ntuning HaRT, we use 4 user train batch size and\n20 eval batch size with early stopping patience set\nto 3. We layer norm the user-states (hidden states\nof the user state vector) from HaRT, and linearly\ntransform (to embedding dimensions) before aver-\naging the user-states to make the user’s age esti-\nmation. We train for 30 epochs with warmup steps\nequivalent to 10 epochs, and a weight decay set to\n0.01. We ﬁnd that for the task of Age estimation\nthe model converges at epoch 21, however for Per-\nsonality Assessment we ﬁnd a simple classiﬁca-\ntion linear layer to show better performance (with\na convergence seen at epoch 28 when run for 35\nepochs). In case of GPT-2 HLC we with the same\ndata (split into into individual messages capped to\n200 tokens per message as in pre-training), for 15\nepochs (much higher training time as compared to\nHaRT) with train and eval batch size set to 400,\nand early stopping patience set to 3.\n635\nModel Age (r) OPE (rdis)\nHaRT (Full test set) 0.868 0.619\nHaRT (Filtered test set) 0.872 0.635\nTable 8: HaRT’s performance on user level down-\nstream tasks: Age and Openness (OPE), on full test sets\n(5000 users and 1943 users respectively for Age and\nOPE) from Park et al. (2015) and Lynn et al. (2020), as\nwell as on the resulting test set (4438 users and 1745\nusers respectively for Age and OPE) after ﬁltering the\ndataset for English language with users having a min-\nimum of 50 posts and 1000 words (as we do for our\npre-training data).\nMeLT – Sentiment Fine-tuning Settings To\napply MeLT (Matero et al., 2021) to the senti-\nment task we use use optuna (Akiba et al., 2019) to\nsearch both learning rate and weight decay param-\neters using a search space between 6e-6 and 3e-\n3 and between 1 and 1e-4 respectively. We keep\nthe same architecture as described in the original\nMeLT paper, however we make 1 change during\nﬁne-tuning and that is the message-vector repre-\nsentation from MeLT is concatenated with the av-\nerage of the observed tokens for the labeled mes-\nsage to include both local and global context into\nthe ﬁne-tuning layers.\nNo Historical Context Fine-tuning Settings\nWe run a hyperparameter sweep using Optuna\n(Akiba et al., 2019) for all models for learning\nrate (using search space between 5e-6 to 5e-4) and\nweight decay(using search space between 0.0 and\n1.0) with early stopping patience set to 6. We do\nthis for 15 and 10 trials for Stance and Sentiment\nmodels respectively, and pick the hyperparameters\nvalue for the best model in the same way as de-\nscribed in the Experimental Settings (A.3 section\nabove. We use these values to ﬁne-tune the models\nfor 15 epochs and get the weighted F1 results.\n636",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.97862708568573
    },
    {
      "name": "Computer science",
      "score": 0.8292238712310791
    },
    {
      "name": "Language model",
      "score": 0.819609522819519
    },
    {
      "name": "Social media",
      "score": 0.6104745268821716
    },
    {
      "name": "Transformer",
      "score": 0.6062214374542236
    },
    {
      "name": "Natural language processing",
      "score": 0.5940729975700378
    },
    {
      "name": "Natural language",
      "score": 0.5465351343154907
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5316720008850098
    },
    {
      "name": "Modeling language",
      "score": 0.5230437517166138
    },
    {
      "name": "Human language",
      "score": 0.4454534351825714
    },
    {
      "name": "Linguistics",
      "score": 0.20571741461753845
    },
    {
      "name": "World Wide Web",
      "score": 0.1548742949962616
    },
    {
      "name": "Programming language",
      "score": 0.13933563232421875
    },
    {
      "name": "Software",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ]
}