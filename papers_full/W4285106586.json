{
  "title": "MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER",
  "url": "https://openalex.org/W4285106586",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2121192732",
      "name": "Ran Zhou",
      "affiliations": [
        "Nanyang Technological University",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2100379612",
      "name": "Xin Li",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2740518062",
      "name": "Ruidan He",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2806062742",
      "name": "Erik Cambria",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2127260490",
      "name": "Luo Si",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2154137932",
      "name": "Chunyan Miao",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034724424",
    "https://openalex.org/W3098341425",
    "https://openalex.org/W3034879633",
    "https://openalex.org/W3034336785",
    "https://openalex.org/W2948017315",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2951221758",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2808481912",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2514249165",
    "https://openalex.org/W3115908473",
    "https://openalex.org/W3034263272",
    "https://openalex.org/W2773052985",
    "https://openalex.org/W3022036890",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W3104502698",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3174571625",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2944420250",
    "https://openalex.org/W3104182623",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W3194993277",
    "https://openalex.org/W3094590257",
    "https://openalex.org/W2964053384",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4287811444",
    "https://openalex.org/W3212066909"
  ],
  "abstract": "Data augmentation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as NER, data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance. In this work, we propose Masked Entity Language Modeling (MELM) as a novel data augmentation framework for low-resource NER. To alleviate the token-label misalignment issue, we explicitly inject NER labels into sentence context, and thus the fine-tuned MELM is able to predict masked entity tokens by explicitly conditioning on their labels. Thereby, MELM generates high-quality augmented data with novel entities, which provides rich entity regularity knowledge and boosts NER performance. When training data from multiple languages are available, we also integrate MELM with code-mixing for further improvement. We demonstrate the effectiveness of MELM on monolingual, cross-lingual and multilingual NER across various low-resource levels. Experimental results show that our MELM consistently outperforms the baseline methods.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2251 - 2262\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMELM: Data Augmentation with Masked Entity\nLanguage Modeling for Low-Resource NER\nRan Zhou∗1,2 Xin Li†1 Ruidan He1 Lidong Bing1 Erik Cambria2 Luo Si1 Chunyan Miao2\n1DAMO Academy, Alibaba Group 2Nanyang Technological University, Singapore\n{ran.zhou, xinting.lx, ruidan.he, l.bing, luo.si}@alibaba-inc.com\n{cambria, ascymiao}@ntu.edu.sg\nAbstract\nData augmentation is an effective solution to\ndata scarcity in low-resource scenarios. How-\never, when applied to token-level tasks such\nas NER, data augmentation methods often suf-\nfer from token-label misalignment, which leads\nto unsatsifactory performance. In this work,\nwe propose Masked Entity Language Modeling\n(MELM) as a novel data augmentation frame-\nwork for low-resource NER. To alleviate the\ntoken-label misalignment issue, we explicitly\ninject NER labels into sentence context, and\nthus the fine-tuned MELM is able to predict\nmasked entity tokens by explicitly condition-\ning on their labels. Thereby, MELM gener-\nates high-quality augmented data with novel\nentities, which provides rich entity regular-\nity knowledge and boosts NER performance.\nWhen training data from multiple languages are\navailable, we also integrate MELM with code-\nmixing for further improvement. We demon-\nstrate the effectiveness of MELM on mono-\nlingual, cross-lingual and multilingual NER\nacross various low-resource levels. Experimen-\ntal results show that our MELM presents sub-\nstantial improvement over the baseline meth-\nods.1\n1 Introduction\nNamed entity recognition (NER) is a fundamen-\ntal NLP task which aims to locate named entity\nmentions and classify them into predefined cat-\negories. As a subtask of information extraction,\nit serves as a key building block for information\nretrieval (Banerjee et al., 2019), question answer-\ning (Fabbri et al., 2020) and text summarization sys-\ntems (Nallapati et al., 2016) etc. However, except\na few high-resource languages / domains, the ma-\njority of languages / domains have limited amount\n∗ Ran Zhou is under the Joint Ph.D. Program between\nAlibaba and Nanyang Technological University.\n†Corresponding author\n1Our code is available at https://github.com/\nRandyZhouRan/MELM/.\nof labeled data.\nSince manually annotating sufficient labeled data\nfor each language / domain is expensive, low-\nresource NER (Cotterell and Duh, 2017; Feng et al.,\n2018; Zhou et al., 2019; Rijhwani et al., 2020) has\nreceived increasing attention in the research com-\nmunity over the past years. As an effective solu-\ntion to data scarcity in low-resource scenarios, data\naugmentation enlarges the training set by apply-\ning label-preserving transformations. Typical data\naugmentation methods for NLP include (1) word-\nlevel modification (Wei and Zou, 2019; Kobayashi,\n2018; Wu et al., 2019; Kumar et al., 2020) and\n(2) back-translation (Sennrich et al., 2016; Fadaee\net al., 2017; Dong et al., 2017; Yu et al., 2018).\nDespite the effectiveness on sentence-level tasks,\nthey suffer from the token-label misalignment issue\nwhen applied to token-level tasks like NER. More\nspecifically, word-level modification might replace\nan entity with alternatives that mismatch the origi-\nnal label. Back-translation creates augmented texts\nthat largely preserve the original content. How-\never, it hinges on external word alignment tools for\npropagating the labels from the original input to\nthe augmented text, which has proved to be error-\nprone.\nTo apply data augmentation on token-level\ntasks, Dai and Adel (2020) proposed to randomly\nsubstitute entity mentions with existing entities of\nthe same class. They avoided the token-label mis-\nalignment issue but the entity diversity does not in-\ncrease. Besides, the substituted entity might not fit\ninto the original context. Li et al. (2020a) avoided\nthe token-label misalignment issue by only diver-\nsifying the context, where they replaced context\n(having ‘O’ label) tokens using MASS (Song et al.,\n2019) and left the entities (i.e. aspect terms in their\ntask) completely unchanged. However, according\nto the NER evaluations in Lin et al. (2020), aug-\nmentation on context gave marginal improvement\non pretrained-LM-based NER models.\n2251\nFigure 1: Effectiveness comparison between diversify-\ning entities and diversifying context. GivenN gold sam-\nples, Add Entitysubstitutes their entities with new enti-\nties from extra gold samples. In contrary, Add Context\nreuses existing entities and inserts them into context of\nextra gold samples. Both methods yield N augmented\nsamples.\nOur preliminary results on low-resource NER\n(see Figure 1) also demonstrate that diversifying en-\ntities in the training data is more effective than intro-\nducing more context patterns. Inspired by the afore-\nmentioned observations, we propose Masked Entity\nLanguage Modeling (MELM) as a data augmenta-\ntion framework for low-resource NER, which gen-\nerates augmented data with diverse entities while\nalleviating the challenge of token-label misalign-\nment. MELM is built upon pretrained Masked\nLanguage Models (MLM), and it is further fine-\ntuned on corrupted training sentences with only\nentity tokens being randomly masked to facilitate\nentity-oriented token replacement. Simply mask-\ning and replacing entity tokens using the finetuned\nMLM is still insufficient because the predicted en-\ntity might not align with the original label. Taking\nthe sentence shown in Figure 2b as an example,\nafter masking the named entity “European Union”\n(Organization), the finetuned MLM could predict\nit as “Washington has”. Such prediction fits the\ncontext but it is not aligned with the original labels.\nTo alleviate the misalignment, our MELM addi-\ntionally introduces a labeled sequence linearization\nstrategy, which respectively inserts one label token\nbefore and after each entity token and regards the\ninserted label tokens as the normal context tokens\nduring masked language modeling. Therefore, the\nprediction of the masked token is conditioned on\nnot only the context but the entity’s label as well.\nAfter injecting label information and finetun-\ning on the label-enhanced NER data, our MELM\ncan exploit rich knowledge from pre-training to\nincrease entity diversity while greatly reducing\ntoken-label misalignment. Code-mixing (Singh\net al., 2019; Qin et al., 2020; Zhang et al., 2021)\nachieved promising results by creating additional\ncode-mixed samples using the available multilin-\ngual training sets, which is particularly beneficial\nwhen the training data of each language is scarce.\nFortunately, in the scenarios of multilingual low-\nresource NER, our MELM can also be applied on\nthe code-mixed examples for further performance\ngains. We first apply code-mixing by replacing en-\ntities in a source language sentence with the same\ntype entities of a foreign language. However, even\nthough token-label alignment is guaranteed by re-\nplacing with entities of the same type, the candidate\nentity might not best fit into the original context\n(for example, replacing a government department\nwith a football club). To solve this problem, we\npropose an entity similarity search algorithm based\non bilingual embedding to retrieve the most seman-\ntically similar entity from the training entities in\nother languages. Finally, after adding language\nmarkers to the code-mixed data, we use them to\nfine-tune MELM for generating more code-mixed\naugmented data.\nTo summarize, the main contributions of this\npaper are as follows: (1) we present a novel frame-\nwork which jointly exploits sentence context and\nentity labels for entity-based data augmentation.\nIt consistently achieves substantial improvement\nwhen evaluated on monolingual, cross-lingual, and\nmultilingual low-resource NER; (2) the proposed\nlabeled sequence linearization strategy effectively\nalleviates the problem of token-label misalignment\nduring augmentation; (3) an entity similarity search\nalgorithm is developed to better bridge entity-based\ndata augmentation and code-mixing in multilingual\nscenarios.\n2 Method\nFig. 2c presents the work flow of our proposed\ndata augmentation framework. We first perform\nlabeled sequence linearization to insert the entity\nlabel tokens into the NER training sentences (Sec-\ntion 2.1). Then, we fine-tune the proposed MELM\non linearized sequences (Section 2.2) and create\naugmented data by generating diverse entities via\n2252\n(a)\n (b)\n (c)\nFigure 2: Comparison of different data augmentation methods, color printing is preferred. (a) augmentation with\npretrained MLM (b) augmentation with MELM without linearization (c) augmentation with MELM\nmasked entity prediction (Section 2.3).\nThe augmented data undergoes post-processing\n(Section 2.4) and is combined with the original\ntraining set for training the NER model. Algo-\nrithm 1 gives the pseudo-code for the overall frame-\nwork. Under multilingual scenarios, we propose\nan entity similarity search algorithm as a refined\ncode-mixing strategy (Section 2.5) and apply our\nMELM on the union set of gold training data and\ncode-mixed data for further performance improve-\nment.\n2.1 Labeled Sequence Linearization\nTo minimize the amount of generated tokens in-\ncompatible with the original labels, we design a\nlabeled sequence linearization strategy to explicitly\ntake label information into consideration during\nmasked language modeling. Specifically, as shown\nin Figure 2c, we add the label token before and after\neach entity token and treat them as normal context\ntokens. The yielded linearized sequence is utilized\nto further finetune our MELM so that its prediction\nis additionally conditioned on the inserted label\ntokens. Note that, we initialize the embeddings\nof label tokens with those of tokens semantically\nrelated to the label names (e.g., “organization” for\n⟨ B-ORG ⟩). By doing so, the linearized sequence\nis semantically closer to a natural sentence and\nthe difficulty of finetuning on linearized sequence\ncould be reduced (Kumar et al., 2020).\n2.2 Fine-tuning MELM\nUnlike MLM, only entity tokens are masked during\nMELM fine-tuning. At the beginning of each fine-\ntuning epoch, we randomly mask entity tokens in\nthe linearized sentence X with masking ratio η.\nThen, given the corrupted sentence ˜X as input,\nour MELM is trained to maximize the probabilities\nof the masked entity tokens and reconstruct the\nlinearized sequence X:\nmax\nθ\nlog pθ(X| ˜X) ≈\nnX\ni=1\nmi log pθ(xi| ˜X) (1)\nwhere θ represents the parameters of MELM, n\nis the number of tokens in ˜X, xi is the original\ntoken in X, mi = 1 if xi is masked and other-\nwise mi = 0. Through the above fine-tuning pro-\ncess, the proposed MELM learns to make use of\nboth contexts and label information to predict the\nmasked entity tokens. As we will demonstrate in\nSection 4.1, the predictions generated by the fine-\ntuned MELM are significantly more coherent with\nthe original entity label, compared to those from\nother methods.\n2.3 Data Generation\nTo generate augmented training data for NER, we\napply the fine-tuned MELM to replace entities in\nthe original training samples. Specifically, given\na corrupted sequence, MELM outputs the proba-\nbility of each token in the vocabulary being the\nmasked entity token. However, as the MELM is\nfine-tuned on the same training set, directly pick-\ning the most probable token as the replacement is\nlikely to return the masked entity token in the orig-\ninal training sample, and might fail to produce a\nnovel augmented sentence. Therefore, we propose\nto randomly sample the replacement from the top k\nmost probable components of the probability distri-\nbution. Formally, given the probability distribution\n2253\nAlgorithm 1Masked Entity Language Modeling (MELM)\nGiven Dtrain, M ▷ Given gold traning set Dtrain and pretrained MLM M\nDmasked ← ∅, Daug ← ∅\nfor {X, Y} ∈Dtrain do\n˜X ← LINEARIZE (X, Y) ▷ Labeled sequence linearization\n˜X ← FINETUNE MASK ( ˜X, η) ▷ Randomly mask entities for fine-tuning\nDmasked ← Dmasked ∪ {˜X}\nend for\nMfinetune ← FINETUNE (M, Dmasked) ▷ Fine-tune MELM on masked linearized sequences\nfor {X, Y} ∈Dmasked do\nrepeat R times:\n˜X ← LINEARIZE (X, Y) ▷ Labeled sequence linearization\n˜X ← GENMASK ( ˜X, µ) ▷ Randomly mask entities for generation\nXaug ← RAND CHOICE (Mfinetune( ˜X), Top k = 5) ▷ Generate augmented data with fine-tuned MELM\nDaug ← Daug ∪ {Xaug}\nend for\nDaug ← POST PROCESS (Daug) ▷ Post-processing\nreturn Dtrain ∪ Daug\nP(xi| ˜X) for a masked token, we first select a set\nV k\ni ⊆ V of the k most likely candidates. Then,\nwe fetch the replacement ˆxi via random sampling\nfrom V k\ni . After obtaining the generated sequence,\nwe remove the label tokens and use the remain-\ning parts as the augmented training data. For each\nsentence in the original training set, we repeat the\nabove generation procedure R rounds to produce\nR augmented examples.\nTo increase the diversity of augmented data, we\nadopt a different masking strategy from train time.\nFor each entity mention comprising of n tokens,\nwe randomly sample a dynamic masking rate ϵ\nfrom Gaussian distribution N(µ, σ2), where the\nGaussian variance σ2 is set as 1/n2. Thus, the\nsame sentence will have different masking results\nin each of the R augmentation rounds, resulting in\nmore varied augmented data.\n2.4 Post-Processing\nTo remove noisy and less informative samples from\nthe augmented data, the generated augmented data\nundergoes post-processing. Specifically, we train a\nNER model with the available gold training sam-\nples and use it to automatically assign NER tags\nto each augmented sentence. Only augmented\nsentences whose predicted labels are consistent\nwith the their original labels are kept. The post-\nprocessed augmented training set Daug is combined\nwith the gold training set Dtrain to train the final\nNER tagger.\n2.5 Extending to Multilingual Scenarios\nWhen extending low-resource NER to multilingual\nscenarios, it is straightforward to separately apply\nthe proposed MELM on language-specific data for\nperformance improvement. Nevertheless, it offers\nhigher potential to enable MELM on top of code-\nmixing techniques, which proved to be effective\nin enhancing multilingual learning (Singh et al.,\n2019; Qin et al., 2020; Zhang et al., 2021). In this\npaper, with the aim of bridging MELM augmenta-\ntion and code-mixing, we propose an entity simi-\nlarity search algorithm to perform MELM-friendly\ncode-mixing.\nSpecifically, given the gold training sets\n{Dℓ\ntrain | ℓ ∈ L} over a set L of languages, we first\ncollect label-wise entity sets Eℓ,y, which consists\nof the entities appearing in Dℓ\ntrain and belonging to\nclass y. To apply code-mixing on a source language\nsentence Xℓsrc , we aim to substitute a mentioned\nentity e of label y with a target language entity\nesub ∈ Eℓtgt,y, where the target language is sam-\npled as ℓtgt ∼ U(L \\ {ℓsrc}). Instead of randomly\nselecting esub from Eℓtgt,y, we choose to retrieve\nthe entity with the highest semantic similarity to e\nas esub. Practically, we introduce MUSE bilingual\nembeddings (Conneau et al., 2017) and calculate\nthe entity’s embedding Emb(e) by averaging the\nembeddings of the entity tokens:\nEmb(e) = 1\n|e|\n|e|X\ni=1\nMUSE ℓsrc,ℓtgt (ei) (2)\nwhere MUSE ℓsrc,ℓtgt denotes the ℓsrc − ℓtgt aligned\nembeddings and ei is the i-th token of e. Next, we\nobtain the target-language entity esub semantically\nclosest to e as follows:\nesub = argmax\n˜e∈Eℓtgt,y\nf(Emb(e), Emb(˜e)) (3)\n2254\nf(·, ·) here is the cosine similarity function. The\noutput entity esub is then used to replacee to create\na code-mixed sentence more suitable for MELM\naugmentation. To generate more augmented data\nwith diverse entities, we further apply MELM on\nthe gold and code-mixed data. Since the training\ndata now contains entities from multiple languages,\nwe also prepend a language marker to the entity\ntoken to help MELM differentiate different lan-\nguages, as shown in Figure 3.\nFigure 3: Applying MELM on gold and code-mixed\ndata. Language markers (e.g., <Español>) are inserted\nduring linearization.\n3 Experiments\nTo comprehensively evaluate the effectiveness of\nthe proposed MELM on low-resource NER, we\nconsider three evaluation scenarios: monolingual,\nzero-shot cross-lingual and multilingual low-\nresource NER.\n3.1 Dataset\nWe conduct experiments on CoNLL NER\ndataset (Tjong Kim Sang, 2002; Tjong Kim Sang\nand De Meulder, 2003) of four languages where\nL = {English (En), German (De), Spanish (Es),\nDutch (Nl)}. For each language ℓ ∈ L, we first\nsample N sentences from the full training set as\nDℓ,N\ntrain, where N ∈ {100, 200, 400, 800} to simu-\nlate different low-resource levels. For a realistic\ndata split ratio, we also downscale the full develop-\nment set to N samples as Dℓ,N\ndev . The full test set for\neach language is adopted as Dℓ\ntest for evaluation.\nFor monolingual experiments on language ℓ\nwith low-resource level N ∈ {100, 200, 400, 800},\nwe use Dℓ,N\ntrain as the gold training data, Dℓ,N\ndev as the\ndevelopment set and Dℓ\ntest as the test set. For zero-\nshot cross-lingualexperiments with low-resource\nlevel N ∈ {100, 200, 400, 800}, we use DEn,N\ntrain as\nthe source language gold training data, DEn,N\ndev as\nthe development set and DDe\ntest, DEs\ntest and DNl\ntest as tar-\nget language test sets. Under multilingual settings\nwhere N training data from each language is avail-\nable (N ∈ {100, 200, 400}), we use S\nℓ∈L Dℓ,N\ntrain as\nthe gold training data, S\nℓ∈L Dℓ,N\ndev as the develop-\nment set and evaluate on DEn\ntest, DDe,\ntest , DEs\ntest and DNl\ntest,\nrespectively.\n3.2 Experimental Setting\nMELM Fine-tuning We use XLM-RoBERTa-\nbase (Conneau et al., 2020) with a language-\nmodeling head to initialize MELM parameters.\nMELM is fine-tuned for 20 epochs using Adam\noptimizer (Kingma and Ba, 2015) with batch size\nset to 30 and learning rate set to 1e − 5.\nNER Model We use XLM-RoBERTa-\nLarge (Conneau et al., 2020) with CRF\nhead (Lample et al., 2016) as the NER model\nfor our experiments 2. We adopt Adamw opti-\nmizer (Loshchilov and Hutter, 2019) with learning\nrate set to 2e − 5 and set batch size to 16. The\nNER model is trained for 10 epochs and the best\nmodel is selected according to dev set performance.\nThe trained model is evaluated on test sets and we\nreport the averaged Micro-F1 scores over 3 runs.\nHyperparameter Tuning The masking rate η\nin MELM fine-tuning, the Gaussian mean µ for\nMELM generation and the number of MELM aug-\nmentation rounds R are set as 0.7, 0.5 and 3, re-\nspectively. All of these hyperparameters are tuned\non the dev set with grid search. Details of the hy-\nperparameter tuning can be found in Appendix A.1\n3.3 Baseline Methods\nTo elaborate the effectiveness of the proposed\nMELM, we compare it with the following methods:\nGold-Only The NER model is trained on only the\noriginal gold training set.\nLabel-wise SubstitutionDai and Adel (2020) ran-\ndomly substituted named entities with existing en-\ntities of the same entity type from the original train-\ning set.\nMLM-Entity We randomly mask entity tokens and\ndirectly utilize a pretrained MLM for data augmen-\ntation without fine-tuning and labeled sequence\nlinearization as used in MELM. The prediction of\na masked entity token does not consider label in-\nformation but solely relies on the context words.\nDAGA Ding et al. (2020) firstly linearized NER\nlabels into the input sentences and then use them\nto train an autoregressive language model. The\nlanguage model was used to synthesize augmented\n2https://github.com/allanj/pytorch_\nneural_crf\n2255\ndata from scratch, where both context and entities\nare generated simultaneously.\nMulDA Liu et al. (2021) fine-tuned mBART(Liu\net al., 2020) on linearized multilingual NER data\nto generate augmented data with new context and\nentities.\n3.4 Experimental Results\n3.4.1 Monolingual and Cross-lingual NER\nAs illustrated on the left side of Table 1, the pro-\nposed MELM consistently achieves the best av-\neraged results across different low-resource lev-\nels, demonstrating its effectiveness on monolingual\nNER. Compared to the best-performing baselines,\nour MELM obtains 6.3, 1.6, 1.3, 0.38 absolute\ngains on 100, 200, 400 and 800 levels, respectively.\nCross-lingual NER results are shown on the right\nside of Table 2. Again, on each of the designed low-\nresource levels, our MELM is superior to baseline\nmethods in terms of the averaged F1 scores. We\nalso notice that, given 100 Nl training samples, the\nGold-Only method without data augmentation al-\nmost fails to converge while the monolingual F1 of\nour MELM reaches 66.6, suggesting that data aug-\nmentation is crucial for NER when the annotated\ntraining data is extremely scarce.\nTo assess the efficacy of the proposed labeled\nsequence linearization (Section 2.1), we directly\nfine-tune MELM on masked sentences without lin-\nearization (as shown in Figure 2b), denoted as\nMELM w/o linearize in Table 1. We observe a con-\nsiderable performance drop compared with MELM,\nwhich proves the label information injected via lin-\nearization indeed helps MELM differentiate differ-\nent entity types, and generate entities compatible\nwith the original label.\nTaking a closer look at the baseline methods, we\nnotice that the monolingual performance of Label-\nwise is still unsatisfactory in most cases. One prob-\nable reason is that only existing entities within the\ntraining data are used for replacement and the entity\ndiversity after augmentation is not increased. More-\nover, randomly sampling an entity of the same type\nfor replacement is likely to cause incompatibility\nbetween the context and the entity, yielding a noisy\naugmented sample for NER training. Although\nMLM-Entity tries to mitigate these two issues by\nemploying a pretrained MLM to generate novel\ntokens that fit into the context, the generated tokens\nmight not be consistent with the original labels.\nOur MELM also promotes the entity diversity of\naugmented data by exploiting pretrained model for\ndata augmentation.\nIn the meantime, equipped with the labeled se-\nquence linearization strategy, MELM augmentation\nis explicitly guided by the label information and the\ntoken-label misalignment is largely alleviated, lead-\ning to superior results in comparison to Lable-wise\nand MLM-Entity.\nWe also compare with DAGA (Ding et al., 2020),\nwhich generates augmented data from scratch us-\ning an autoregressive language model trained on\ngold NER data. Although DAGA is competitive on\nlow-resource levels of 400 and 800, it still under-\nperforms the proposed MELM by a large margin\nwhen the training size reduces to 100 or 200. We at-\ntribute this to the disfluent and ungrammatical sen-\ntences generated from the undertrained language\nmodel. Instead of generating augmented data from\nscratch, MELM focuses on modifying entity tokens\nand leave the context unchanged, which guarantees\nthe quality of augmented sentences even under ex-\ntremely low-resource settings.\n3.4.2 Multilingual NER\nFor multilingual low-resource NER, we firstly di-\nrectly apply MELM on the concatenation of train-\ning sets from multiple languages. As shown in\nTable 2, MELM-gold achieves substantial improve-\nment over the Gold-only baseline, which is consis-\ntent with monolingual and cross-lingual results. We\ncompare with MulDA (Liu et al., 2021) as a base-\nline data augmentation method. MulDA generates\naugmented data autoregressively with an mBART\nmodel, which is fine-tuned on NER data with in-\nserted label tokens. At the low-resource levels in\nour experimental settings, MulDA is less effective\nand even leads to deteriorated performance. The\nunsatisfactory performance mainly results from the\ndiscrepancy between pretraining and fine-tuning\ndue to the inserted label tokens. Given very few\ntraining samples, it is difficult to adapt mBART to\ncapture the distribution of the inserted label tokens,\nand thus MulDA struggles to generate fluent and\ngrammatical sentences from scratch. In compari-\nson, our proposed method preserves the original\ncontext and introduce less syntactic noise in the\naugmented data. To further leverage the benefits\nof code-mixing in multilingual NER, we experi-\nment with two code-mixing methods: (1) Code-\nMix-random, which randomly substitutes entities\nwith existing entities of the same type from other\nlanguages, and (2) Code-Mix- ess, which adopts\n2256\n#Gold Method Monolingual Cross-lingual\nEn De Es Nl Avg En →De En→Es En→Nl Avg\n100\nGold-Only 50.57 39.47 42.93 21.63 38.65 39.54 37.40 39.27 38.74\nLabel-wise 61.34 55.00 59.54 27.85 50.93 45.85 43.74 50.51 46.70\nMLM-Entity 61.22 50.96 61.29 46.59 55.02 47.96 45.42 49.34 47.57\nDAGA 68.06 59.15 69.33 45.64 60.54 52.95 46.72 54.63 51.43\nMELMw/o linearize70.01 61.92 65.07 59.76 64.19 48.70 49.10 53.37 50.39\nMELM(Ours) 75.21 64.12 75.85 66.57 70.44 56.56 53.83 60.62 57.00\n200\nGold-Only 74.64 62.85 72.64 55.96 66.52 54.95 51.26 60.71 55.64\nLabel-wise 76.82 67.31 78.34 66.52 72.25 55.01 53.14 63.30 57.15\nMLM-Entity 79.16 70.01 78.45 66.69 73.58 60.44 57.72 68.37 62.18\nDAGA 79.11 69.82 78.95 68.53 74.10 59.58 57.68 65.74 61.00\nMELMw/o linearize81.77 71.41 80.43 72.92 76.63 62.57 63.49 70.18 65.41\nMELM(Ours) 82.91 72.71 80.46 77.02 78.27 65.01 63.71 70.37 66.36\n400\nGold-Only 81.85 70.77 80.02 74.60 76.81 65.76 61.57 71.04 66.12\nLabel-wise 84.62 74.33 81.01 77.87 79.46 66.18 67.43 71.93 68.51\nMLM-Entity 83.82 74.66 81.08 77.90 79.37 67.41 70.28 74.31 70.67\nDAGA 84.36 72.95 82.83 78.99 79.78 66.77 67.13 72.40 68.77\nMELMw/o linearize85.16 75.42 82.34 79.34 80.56 68.02 66.01 72.98 69.00\nMELM(Ours) 85.73 77.50 83.31 80.92 81.87 68.08 70.37 75.78 71.74\n800\nGold-Only 86.35 78.35 83.23 83.86 82.95 65.31 68.28 72.07 68.55\nLabel-wise 86.72 78.21 84.42 84.26 83.40 65.60 72.22 74.77 70.86\nMLM-Entity 86.50 78.30 84.09 83.93 83.20 65.42 69.10 74.85 69.79\nDAGA 86.61 77.66 84.64 84.90 83.45 68.76 70.97 75.02 71.58\nMELMw/o linearize87.35 78.58 84.59 84.94 83.99 67.37 71.53 75.20 71.37\nMELM(Ours) 87.59 79.32 85.40 85.17 84.37 67.95 75.72 75.25 72.97\nTable 1: Left side of table shows the results of monolingual low-resource NER. Right side of table shows the results\nof cross-lingual low-resource NER with English as source language.Avgs on left side and right side are the averaged\nresult over all languages and all transfer pairs, respectively.\n#Gold Method En De Es Nl Avg\n100×4\nGold-Only 75.62 69.35 75.85 74.33 73.79MulDA 73.67 70.47 75.53 72.40 73.02MELM-gold (Ours)78.71 74.79 81.25 78.85 78.40Code-Mix-random 77.38 70.58 78.61 76.45 75.75Code-Mix-ess (Ours)79.55 71.56 79.58 76.49 76.80MELM(Ours) 80.96 75.61 81.47 80.14 79.54\n200×4\nGold-Only 83.06 76.39 82.71 79.19 80.34MulDA 82.32 74.57 82.73 79.06 79.67MELM-gold (Ours)82.90 78.0585.9381.00 81.97Code-Mix-random 82.86 75.70 83.13 79.08 80.19Code-Mix-ess (Ours)83.34 76.64 82.02 82.27 81.07MELM(Ours) 83.56 78.2484.9882.79 82.39\n400×4\nGold-Only 83.92 77.40 83.22 84.04 82.14MulDA 84.37 78.41 84.54 83.09 82.60MELM-gold (Ours)86.04 79.09 85.76 84.83 83.93Code-Mix-random 85.04 77.91 84.44 83.56 82.74Code-Mix-ess (Ours)85.74 80.03 85.18 85.36 84.08MELM(Ours) 86.14 80.33 86.60 85.99 84.76\nTable 2: Results of multilingual low-resource NER.\nGold training set contains the same number of train-\ning samples from each language. Avg is the averaged\nresult over all languages.\nthe proposed entity similarity search algorithm in\nSection 2.5 as the code-mixing strategy.\nExperimental results in Table 2 show that both\nmethods are able to achieve improved perfor-\nmance over Gold-Only. This observation suggests\nthat code-mixing techniques, either random code-\nmixing or code-mixing via our entity similarity\nsearch, are indeed helpful for multilingual NER.\nComparing these two methods, the performance\ngains brought by Code-Mix- ess are more signifi-\ncant and consistent across different low-resource\nlevels, which demonstrates the effectiveness of our\nproposed entity similarity search algorithm. Apply-\ning MELM on both gold data and code-mixed data\nfrom Code-Mix-ess, the multilingual NER results\nare further improved. In summary, our proposed\nMELM is well-suited for multilingual NER, which\ncan be integrated with our code-mixing technique\nto achieve further improvement.\n4 Further Analysis\n4.1 Case Study\nApart from the quantitative results, we further an-\nalyze the augmented data to demonstrate the ef-\nfectiveness of our MELM in maintaining the con-\nsistency between the original label and the aug-\nmented token. Table 3 presents examples of the\ntop-5 predictions from pretrained MLM, MELM\nw/o linearize and MELM. As we can see, the pre-\ntrained MLM, which does not introduce any design\nor contraint on data augmentation, tends to gener-\nate high-frequency words such as “the”, “he” and\n“she”, and the majority of generated words do not\nbelong to the original entity class. Being finetuned\non NER data with entity-oriented masking, MELM\n2257\nText EU rejects German call to boycott British Lamb\nLabel B-ORG O B-MISC O O O B-MISC O\nMLM Britain, EU,UK, Trump, US US, a, UN, the, UK the, a, black, white, young\nMELM\nw/o linearizeEU, Australia, US, UN, Israel German, Indian, the, Washington, Union Chinese, British, raw, California, Australian\nMELM EU, Greenpeace, Amnesty, UN, Reuters German, British, Dutch, French, EU African, British, Guinean, white, French\nText Clinton aide resigns , NBC says\nLabel B-PER O O O B-ORG O\nMLM my, his, My, When, her he, she, it, and, who\nMELM\nw/o linearizeFrench, German, British, Swiss, Russian Reuters, Pompeo, Blair Hill, AFP\nMELM French, White, Walker, Ferguson, David NBC, AFP, Greenpeace, BBC, Anonymous\nTable 3: Examples of the top-5 predictions by MLM, MELM w/o linearize and MELM. Predictions that do not\nbelong to the original class are highlighed in red.\nw/o linearize is able to generate more entity-related\ntokens.\nHowever, without the explicit guidance from en-\ntity labels, it is still too difficult for MELM w/o\nlinearize to make valid predictions solely based on\nthe ambiguous context (e.g., both “Pompeo” (PER)\nand “Reuters” (ORG) are compatible with the con-\ntext of Example #2), which leads to token-label\nmisalignment. Compared to the above methods,\nour MELM take both label information and con-\ntext into consideration, and thus generates more\nentities that fit into the context and align with the\noriginal label as well. Moreover, it is notewor-\nthy that MELM can leverage the knowledge from\npretrained model to generate real-world entities\nthat do not exist in the original NER dataset (e.g.,\n“Greenpeace” and “Amnesty”), which essentially\nincreases the entity diversity in training data.\n4.2 Number of Unique Entities\nAs demonstrated in Lin et al. (2020) and our pre-\nliminary experiments in Figure 1, introducing un-\nseen entities can effectively provide more entity\nregularity knowledge, and helps to improve NER\nperformance. Therefore, we examine the amount\nof unique entities introduced by different methods.\nAs there might be token-label misalignment in the\naugmented data, we firstly train an ‘oracle’ NER\nmodel on the full CoNLL dataset and then use it\nto tag training data of MELM and different base-\nline methods. For each method, we count the total\nnumber of unique entities whose labels match the\nlabels assigned by the ‘oracle’ model. As shown\nin Figure 4, while many augmented entities from\nMLM-Entity, DAGA and MELMw/o linearize are\nfiltered out due to token-label misalignment, we\nnote that MELM introduces a significantly larger\nnumber of unseen entities in the augmented data.\nTherefore MELM is able to provide richer entity\nFigure 4: Comparison between the number of unique\nvalid entities introduced by different methods\nregularity knowledge, which explains its superior-\nity over the baseline methods.\n5 Related Work\nOn sentence level tasks, one line of data augmen-\ntation methods are built upon word-level mod-\nifications, which can be based on synonym re-\nplacement (Wei and Zou, 2019), LSTM language\nmodel (Kobayashi, 2018), MLM (Wu et al., 2019;\nKumar et al., 2020), auto-regressive pretrained\nLM (Kumar et al., 2020), or constituent-based\ntagging schemes (Zhong et al., 2020). However,\nthese methods suffer from token-label misalign-\nment when applied to token-level tasks such as\nNER, which requires sophisticated post-processing\nto remove noisy samples in augmented data (Bari\net al., 2021; Zhong and Cambria, 2021).\nExisting works avoid token-label misalignment\nby replacing entities with existing entities of the\nsame class (Dai and Adel, 2020), or only modifying\ncontext works and leaving entities / aspect terms\nunchanged (Li et al., 2020a). Others attempt to\nproduce augmented data by training / fine-tuning\n2258\na generative language model on linearized labeled\nsequences (Ding et al., 2020; Liu et al., 2020).\nBacktranslation (Sennrich et al., 2016; Fadaee\net al., 2017; Dong et al., 2017; Yu et al., 2018)\ntranslates source language sentences into a target\nlanguage, and subsequently back to the source lan-\nguage, which preserve the overall semantics of the\noriginal sentences. On token-level tasks, however,\nthey hinge on external word alignment tools for la-\nbel propagation, which are often error-prone (Tsai\net al., 2016; Li et al., 2020b).\n6 Conclusion\nWe have proposed MELM as a data augmentation\nframework for low-resource NER. Through labeled\nsequence linearization, we enable MELM to explic-\nitly condition on label information when predicting\nmasked entity tokens. Thus, our MELM effectively\nalleviates the token-label misalignment issue and\ngenerates augmented data with novel entities by ex-\nploiting pretrained knowledge. Under multilingual\nsettings, we integrate MELM with code-mixing for\nfurther performance gains. Extensive experiments\nshow that the proposed framework demonstrates\nencouraging performance gains on monolingual,\ncross-lingual and multilingual NER across various\nlow-resource levels.\nAcknowledgements\nThis research is partly supported by the Alibaba-\nNTU Singapore Joint Research Institute, Nanyang\nTechnological University. Erik Cambria would\nlike to thank the support by the Agency for Sci-\nence, Technology and Research (A*STAR) under\nits AME Programmatic Funding Scheme (Project\n#A18A2b0046).\nReferences\nPartha Sarathy Banerjee, Baisakhi Chakraborty, Deepak\nTripathi, Hardik Gupta, and Sourabh S Kumar. 2019.\nA information retrieval based on question and an-\nswering and ner for unstructured information with-\nout using sql. Wireless Personal Communications,\n108(3):1909–1931.\nM Saiful Bari, Tasnim Mohiuddin, and Shafiq Joty.\n2021. UXLA: A robust unsupervised data augmenta-\ntion framework for zero-resource cross-lingual NLP.\nIn Proceedings of the Annual Meeting of the Asso-\nciation for Computational Linguistics and the In-\nternational Joint Conference on Natural Language\nProcessing, pages 1978–1992.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio Ran-\nzato, Ludovic Denoyer, and Hervé Jégou. 2017.\nWord translation without parallel data. arXiv preprint\narXiv:1710.04087.\nRyan Cotterell and Kevin Duh. 2017. Low-\nresource named entity recognition with cross-lingual,\ncharacter-level neural conditional random fields. In\nProceedings of the Eighth International Joint Con-\nference on Natural Language Processing (Volume 2:\nShort Papers), pages 91–96, Taipei, Taiwan. Asian\nFederation of Natural Language Processing.\nXiang Dai and Heike Adel. 2020. An analysis of simple\ndata augmentation for named entity recognition. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 3861–3867,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\nengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, and\nChunyan Miao. 2020. DAGA: Data augmentation\nwith a generation approach for low-resource tagging\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6045–6057, Online. Association for\nComputational Linguistics.\nLi Dong, Jonathan Mallinson, Siva Reddy, and Mirella\nLapata. 2017. Learning to paraphrase for question an-\nswering. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 875–886, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nAlexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh\nNallapati, and Bing Xiang. 2020. Template-based\nquestion generation from retrieved sentences for im-\nproved unsupervised question answering. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4508–4513, On-\nline. Association for Computational Linguistics.\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\n2017. Data augmentation for low-resource neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 567–573,\nVancouver, Canada. Association for Computational\nLinguistics.\nXiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin\nFeng, and Ting Liu. 2018. Improving low resource\nnamed entity recognition using cross-lingual knowl-\nedge transfer. In Proceedings of the International\n2259\nJoint Conference on Artificial Intelligence, IJCAI-18,\npages 4071–4077.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic re-\nlations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 452–457,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18–26.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nKun Li, Chengbo Chen, Xiaojun Quan, Qing Ling,\nand Yan Song. 2020a. Conditional augmentation\nfor aspect term extraction via masked sequence-to-\nsequence generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7056–7066, Online. Association\nfor Computational Linguistics.\nXin Li, Lidong Bing, Wenxuan Zhang, Zheng Li, and\nWai Lam. 2020b. Unsupervised cross-lingual adapta-\ntion for sequence tagging and beyond. arXiv preprint\narXiv:2010.12405.\nHongyu Lin, Yaojie Lu, Jialong Tang, Xianpei Han,\nLe Sun, Zhicheng Wei, and Nicholas Jing Yuan. 2020.\nA rigorous study on named entity recognition: Can\nfine-tuning pretrained model lead to the promised\nland? In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7291–7300, Online. Association for\nComputational Linguistics.\nLinlin Liu, Bosheng Ding, Lidong Bing, Shafiq Joty,\nLuo Si, and Chunyan Miao. 2021. MulDA: A\nmultilingual data augmentation framework for low-\nresource cross-lingual NER. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5834–5846, Online. As-\nsociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing\nXiang, et al. 2016. Abstractive text summarization\nusing sequence-to-sequence rnns and beyond. arXiv\npreprint arXiv:1602.06023.\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che.\n2020. Cosda-ml: Multi-lingual code-switching data\naugmentation for zero-shot cross-lingual nlp.\nShruti Rijhwani, Shuyan Zhou, Graham Neubig, and\nJaime Carbonell. 2020. Soft gazetteers for low-\nresource named entity recognition. arXiv preprint\narXiv:2005.01866.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 86–96,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar,\nCaiming Xiong, and Richard Socher. 2019. Xlda:\nCross-lingual data augmentation for natural language\ninference and question answering. arXiv preprint\narXiv:1905.11471.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan\nLiu. 2019. Mass: Masked sequence to sequence pre-\ntraining for language generation. In International\nConference on Machine Learning, pages 5926–5936.\nPMLR.\nErik F. Tjong Kim Sang. 2002. Introduction to the\nCoNLL-2002 shared task: Language-independent\nnamed entity recognition. In COLING-02: The 6th\nConference on Natural Language Learning 2002\n(CoNLL-2002).\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nChen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.\nCross-lingual named entity recognition via wikifica-\ntion. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\npages 219–228.\n2260\nJason Wei and Kai Zou. 2019. EDA: Easy data augmen-\ntation techniques for boosting performance on text\nclassification tasks. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 6382–6388, Hong Kong, China. As-\nsociation for Computational Linguistics.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Conditional bert contextual\naugmentation. In International Conference on Com-\nputational Science, pages 84–95. Springer.\nAdams Wei Yu, David Dohan, Quoc Le, Thang Luong,\nRui Zhao, and Kai Chen. 2018. Fast and accurate\nreading comprehension by combining self-attention\nand convolution. In International Conference on\nLearning Representations.\nWenxuan Zhang, Ruidan He, Haiyun Peng, Lidong\nBing, and Wai Lam. 2021. Cross-lingual aspect-\nbased sentiment analysis with aspect term code-\nswitching. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 9220–9230, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nXiaoshi Zhong and Erik Cambria. 2021. Time Expres-\nsion and Named Entity Recognition. Springer.\nXiaoshi Zhong, Erik Cambria, and Amir Hussain. 2020.\nExtracting time expressions and named entities with\nconstituent-based tagging schemes. Cognitive Com-\nputation, 12(4):844–862.\nJoey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu,\nMeng Fang, Rick Siow Mong Goh, and Kenneth\nKwok. 2019. Dual adversarial neural transfer for low-\nresource named entity recognition. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3461–3471, Florence,\nItaly. Association for Computational Linguistics.\n2261\nA Appendix\nA.1 Hyperparameter Tuning\nMasking hyperparameters.To determine the opti-\nmal setting for fine-tune mask rateη and generation\nmasking parameter µ, we conduct a grid search on\nboth hyperparameters in range [0.3, 0.5, 0.7]. We\nfinetune MELM and generate English augmented\ndata on CoNLL following our method in Section 2.\nThe augmented data is used to train a NER tagger\nand its performance on English dev set is recorded.\nAs shown in Table 4, we achieve the best dev set\nF1 when η = 0.7 and µ = 0.5, which is adopted\nfor the rest of this work.\nη\n0.3 0.5 0.7\n0.3 76.90 75.64 78.08\nµ 0.5 76.16 78.06 78.56\n0.7 75.94 78.09 78.37\nTable 4: Dev set F1 for masking hyperparameter tuning.\nNumber of augmentation rounds.Merging aug-\nmented data from multiple rounds increase entity\ndiversity until it saturates at certain point. Con-\ntinuing adding in more augmented data begins to\namplify the noise in augmented data and leads to\ndecreasing performance. To determine the opti-\nmum number of augmentation rounds R, we merge\ndifferent amount of augmented data with English\ngold data to train a NER tagger, with R ranging\nfrom 1 to 6. As shown in Table 5, dev set F1 in-\ncreases with increasing amount of augmented data\nuntil R=3, and starts to drop further beyond. There-\nfore, we choose R = 3for all of our experiments.\nR 1 2 3 4 5 6\nDev F192.35 92.36 92.84 92.72 92.59 92.39\nTable 5: Dev set F1 for number of augmentation rounds.\nA.2 Statistics for Reproducibility\nIn this section, we present the validation F1 av-\neraged among 3 runs of MELM under different\nlanguages and low-resource levels. We also sum-\nmarize the estimated time for fine-tuning MELM\nand the number of parameters used. We separately\nshow the statistics of monolingual (Table 6), cross-\nlingual (Table 7) and multilingual (Table 8) NER.\n#Gold En De Es Nl time #Paramerter\n100 82.38 71.11 71.77 71.01 ~ 7min 270M\n200 85.93 77.96 83.25 79.53 ~ 10min 270M\n400 89.01 82.95 85.10 81.40 ~ 15min 270M\n800 92.01 84.82 86.65 85.61 ~ 20min 270M\nTable 6: Validation F1 for MELM under monolingual\nsettings\n#Gold dev F1 time #Paramerter\n100 82.38 ~ 7min 270M\n200 85.93 ~ 10min 270M\n400 89.01 ~ 15min 270M\n800 92.01 ~ 20min 270M\nTable 7: Validation F1 for MELM under cross-lingual\nsettings\n#Gold per language dev F1 time #Paramerter\n100 83.21 ~ 20min 270M\n200 84.83 ~ 30min 270M\n400 87.07 ~ 45min 270M\nTable 8: Validation F1 for MELM under multilingual\nsettings\nA.3 Computing Infrastructure\nOur experiments are conducted on NVIDIA V100\nGPU.\n2262",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8403877019882202
    },
    {
      "name": "Security token",
      "score": 0.7910167574882507
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6069902181625366
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6013711094856262
    },
    {
      "name": "Sentence",
      "score": 0.5334911942481995
    },
    {
      "name": "Natural language processing",
      "score": 0.4694214463233948
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.43171989917755127
    },
    {
      "name": "Entity linking",
      "score": 0.4275428354740143
    },
    {
      "name": "Language model",
      "score": 0.41813018918037415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3495256304740906
    },
    {
      "name": "Knowledge base",
      "score": 0.10659438371658325
    },
    {
      "name": "Computer security",
      "score": 0.07345113158226013
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}