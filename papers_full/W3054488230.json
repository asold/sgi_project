{
  "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
  "url": "https://openalex.org/W3054488230",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2260380087",
      "name": "Qian Lihua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097502890",
      "name": "Bao Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1917300918",
      "name": "Wang Mingxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2026110207",
      "name": "Qiu Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2255876763",
      "name": "Zhang, Weinan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2045189425",
      "name": "Yu Yong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978908041",
      "name": "Li Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2990488589",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W3175164646",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3015162217",
    "https://openalex.org/W1980073965",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2990004017",
    "https://openalex.org/W2971167892",
    "https://openalex.org/W3030343076",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2948381505",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2996843693",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2907945666",
    "https://openalex.org/W3114869109",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W3035416964",
    "https://openalex.org/W2990372437"
  ],
  "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM), a method to learn word interdependency for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8-15 times speedup. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
  "full_text": "Glancing Transformer for Non-Autoregressive\nNeural Machine Translation\nLihua Qian1,2∗ Hao Zhou2 Yu Bao3 Mingxuan Wang2\nLin Qiu1 Weinan Zhang1 Yong Yu1 Lei Li2\n1 Shanghai Jiao Tong University 2 ByteDance AI Lab 3 Nanjing University\n{qianlihua,lqiu,wnzhang,yyu}@apex.sjtu.edu.cn\n{zhouhao.nlp,wangmingxuan.89,lileilab}@bytedance.com\nbaoy@smail.nju.edu.cn\nAbstract\nRecent work on non-autoregressive neural ma-\nchine translation (NAT) aims at improving the\nefﬁciency by parallel decoding without sac-\nriﬁcing the quality. However, existing NAT\nmethods are either inferior to Transformer or\nrequire multiple decoding passes, leading to\nreduced speedup. We propose the Glancing\nLanguage Model (GLM), a method to learn\nword interdependency for single-pass paral-\nlel generation models. With GLM, we de-\nvelop Glancing Transformer (GLAT) for ma-\nchine translation. With only single-pass paral-\nlel decoding, GLAT is able to generate high-\nquality translation with 8×-15×speedup. Ex-\nperiments on multiple WMT language direc-\ntions show that GLAT outperforms all previ-\nous single pass non-autoregressive methods,\nand is nearly comparable to Transformer, re-\nducing the gap to 0.25-0.9 BLEU points.\n1 Introduction\nTransformer has been the most widely used ar-\nchitecture for machine translation (Vaswani et al.,\n2017). Despite its strong performance, the decod-\ning of Transformer is inefﬁcient as it adopts the\nsequential auto-regressive factorization for its prob-\nability model (Figure 1a). Recent work such as\nnon-autoregressive transformer (NAT), aim to de-\ncode target tokens in parallel to speed up the gener-\nation (Gu et al., 2018). However, the vanilla NAT\nstill lags behind Transformer in the translation qual-\nity – with a gap about 7.0 BLEU score. NAT as-\nsumes the conditional independence of the target\ntokens given the source sentence. We suspect that\nNAT’s conditional independence assumption pre-\nvents learning word interdependency in the target\nsentence. Notice that such word interdependency\nis crucial, as the Transformer explicitly captures\nthat via decoding from left to right (Figure 1a).\n∗The work was done when the ﬁrst author was an intern at\nBytedance.\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention\n(a) Sequential LM\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention (b) Cond. Independent LM\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention\n(c) Masked LM (MLM)\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention (d) Glancing LM (GLM)\nFigure 1: Probabilistic models for machine translation\nmethods. (b) Vanilla NAT uses conditional indepe-\ndent LM. (c) Mask-Predict NAT uses MLM and re-\nquires multiple passes of decoding. (d) Our proposed\nGLM leverages the decoder prediction to decide glanc-\ning sampling policy during training and only requires\none pass of decoding during inference.\nSeveral remedies are proposed (Ghazvininejad\net al., 2019; Gu et al., 2019) to capture word inter-\ndependency while keeping parallel decoding. Their\ncommon idea is to decode the target tokens itera-\ntively while each pass of decoding is trained using\nthe masked language model (Figure 1c). Since\nthese methods require multiple passes of decod-\ning, its generation speed is measurably slower than\nthe vanilla NAT. With single-pass generation only,\nthese methods still largely lag behind the autore-\ngressive Transformer.\nOne open question is whether a complete par-\nallel decoding model can achieve comparable ma-\nchine translation performance to the Transformer.\narXiv:2008.07905v3  [cs.CL]  13 May 2021\nIt should be non-autoregressive and take only one\npass of decoding during the inference time.\nTo address the quest, we propose glancing lan-\nguage model (GLM), a new method to train a prob-\nabilistic sequence model. Based on GLM, we de-\nvelop the glancing Transformer (GLAT) for neural\nmachine translation. It achieves parallel text gener-\nation with only single decoding pass. Yet, it outper-\nforms previous NAT methods and achieves compa-\nrable performance as the strong Transformer base-\nline in multiple cases. Intuitively, GLM adopts a\nadaptive glancing samplingstrategy, which glances\nat some fragments of the reference if the reference\nis too difﬁcult to ﬁt in the training of GLAT. Corre-\nspondingly, when the model is well tuned, it will\nadaptively reduce the percentage of glancing sam-\npling, making sure that the resulting model could\nlearn to generate the whole sentence in the single-\npass fashion.\nSpeciﬁcally, our proposed GLM differs from\nMLM in two aspects. Firstly, GLM proposes an\nadaptive glancing sampling strategy, which enables\nGLAT to generate sentences in a one-iteration way,\nworking by gradual training instead of iterative in-\nference (see Figure 1d). Generally, GLM is quite\nsimilar to curriculum learning (Bengio et al., 2009)\nin spirit, namely ﬁrst learning to generate some\nfragments and gradually moving to learn the whole\nsentences (from easy to hard). To achieve the adap-\ntive glancing sampling, GLM performs decoding\ntwice in training. The ﬁrst decoding is the same as\nthe vanilla NAT, and the prediction accuracy indi-\ncates whether the current reference is “difﬁcult” for\nﬁtting. In the second decoding, GLM gets words\nof the reference via glancing sampling according\nto the ﬁrst decoding, and learn to predict the re-\nmaining words that are not sampled. Note that\nonly the second decoding will update the model pa-\nrameters. Secondly, instead of using the [MASK]\ntoken, GLM directly uses representations from the\nencoder at corresponding positions, which is more\nnatural and could enhance the interactions between\nsampled words and signals from the encoder.\nExperimental results show that GLAT obtains\nsigniﬁcant improvements (about 5 BLEU) on stan-\ndard benchmarks compared to the vanilla NAT,\nwithout losing inference speed-up. GLAT achieves\ncompetitive results against iterative approaches like\nMask-Predict (Ghazvininejad et al., 2019), even\noutperforming the Mask-Predict model on WMT14\nDE-EN and WMT16 RO-EN. Compared to the\nstrong AT baseline, GLAT can still close the per-\nformance gap within 0.9 BLEU point while keep-\ning 7.9×speed-up. Empirically, we even ﬁnd that\nGLAT outperforms AT when the length of the refer-\nence is less than 20 on WMT14 DE-EN. We spec-\nulate this is because GLM could capture bidirec-\ntional context for generation while its left-to-right\ncounterpart is only unidirectional, which indicates\nthe potential of parallel generation approaches like\nGLAT.\n2 Probability Models of Machine\nTranslation\nWe state and compare different probability mod-\nels for machine translation. A machine translation\ntask can be formally deﬁned as a sequence to se-\nquence generation problem: given the source sen-\ntence X = {x1,x2,...,x N }, to generate the target\nsentence Y = {y1,y2,...,y T }according to the con-\nditional probability P(Y|X; θ), where θ denotes\nthe parameter set of a network. Different methods\nfactorize the conditional probability differently.\nThe Transformer uses the autoregressive factor-\nization to maximize the following likelihood:\nLAT = log P(Y|X; θ) =\nT∑\nt=1\nlog p(yt|y<t,X; θ),\nwhere y<t = {[BOS],y1,...,y t−1}. For simplic-\nity, we omit the number of samples in the equation.\nNote the training of AT adopts left-to-right teacher\nforcing on the target tokens (Vaswani et al., 2017).\nThe word interdependency is learned in a unidi-\nrectional way. During inference, the preceding\npredicted token is fed into the decoder to generate\nthe next token.\nThe vanilla NAT consists of the same encoder\nas Transformer and a parallel decoder with layers\nof multi-head attention (Gu et al., 2018). During\ntraining, it uses the conditional independent factor-\nization for the target sentence:\nLNAT =\nT∑\nt=1\nlog P(yt|X; θ).\nNotice that, NAT’s log-likelihood is an approxima-\ntion to the full log-likelihood log P(Y|X; θ). Dur-\ning inference, the encoder representation is copied\nas the input to the decoder, therefore all tokens on\nthe target side can be generated in parallel. Such\na conditional independence assumption does not\nNAT \nDecodingH H′\u0000 Glancing Sampling\ny1\ny3\ny5\nh2\nh4\nHamming \nDistance\nN( ̂Y, Y) = 3\nReplace \nInputs\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\n0.8\n0.5\n0.7\n0.6\n0.9\nParallel \nDecoderH H′\u0000 GlancingEncoder Parallel \nDecoder\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nh1\nh2\nh3\nh4\nh5\nX Compute loss \nLGLM\ny1\ny3\ny5\nh2\nh4\nCompute \nDistance\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny2\ny3\ny4\ny5\nSample  \n words S(Y, ̂Y)\nReplace \nInput H\n̂Y Y\nFigure 2: The training procedure with glancing sampling in GLAT. H is the representation computed by the\nencoder. ˆy’s are the initial predicted tokens of the parallel decoder. y’s are the ground-truth target tokens. H′ is\nfed into the decoder again to calculate the training loss.\nhold in general, which explains the inferior perfor-\nmance of NAT.\nMulti-pass iterative decoding approaches such as\nMask-Predict (Ghazvininejad et al., 2019) extends\nthe vanilla NAT. It still uses the conditional inde-\npendent factorization, together with the random\nmasking scheme:\nLMLM =\n∑\nyt∈RM(Y )\nlog p\n(\nyt|Φ\n(\nY,RM(Y)\n)\n,X; θ\n)\n,\nwhere RM(Y) is a set of randomly selected words\nfrom Y, and Φ(·) replaces these selected words\nin Y with the [MASK] token. For example in\nFigure 1c, RM(Y) = {y2,y3}, Φ\n(\nY,RM(Y)\n)\n=\n{y1,[MASK],[MASK],y4,y5}. The training ob-\njective is to learn a reﬁnement modelθthat can pre-\ndict the masked tokens given the source sentence\nX and words generated in the previous iteration.\nThe vanilla NAT breaks word interdependency,\nwhile MLM requires multiple passes of decoding to\nre-establish the word interdependency. Our goal in\nthis work is to design a better probability model and\na training objective to enable word interdependency\nlearning for single-pass parallel generation.\n3 Glancing Transformer\nIn this section, we present GLAT in detail. GLAT\nuses the same encoder-decoder architecture as the\nvanilla NAT (Gu et al., 2018). GLAT differs from\nthe vanilla NAT in that it explicitly encourages\nword interdependency via training with glancing\nlanguage model (GLM). It differs from the iterative\nNAT with MLM in that it is trained to produce\nsingle pass parallel decoding while MLM is used\nfor prediction reﬁnement.\n3.1 The Glancing Language Model\nGiven the input source sentence X =\n{x1,x2,...,x N }, the task is to predict\nY = {y1,y2,...,y T }. The glancing Trans-\nformer (GLAT) formulates a glancing language\nmodel (GLM) during training. It maximizes the\nfollowing:\nLGLM =\n∑\nyt∈GS(Y, ˆY )\nlog p(yt|GS(Y, ˆY),X; θ)\n(1)\nWhere, ˆY is an initial predicted tokens, and\nGS(Y, ˆY) is a subset of tokens selected via the\nglancing sampling strategy (Figure 2, described in\ndetail in the next section). The glancing sampling\nstrategy selects those words from the target sen-\ntence by comparing the initial prediction against\nthe ground-truth tokens. It selects more tokens and\nfeeds them into the decoder input if the network’s\ninitial prediction is less accurate. GS(Y, ˆY) is the\nremaining subset of tokens within the target Y but\nnot selected. The training loss above is calculated\nagainst these remaining tokens.\nGLAT adopts similar encoder-decoder archi-\ntecture as the Transformer with some modiﬁca-\ntion (Figure 1d). Its encoder fencis the same multi-\nhead attention layers. Its decoder fdec include\nmultiple layers of multi-head attention where each\nlayer attends to the full sequence of both encoder\nrepresentation and the previous layer of decoder\nrepresentation.\nDuring the initial prediction, the input to the\ndecoder H = {h1,h2,...,h T }are copied from\nthe encoder output using either uniform copy or\nsoft copy (Wei et al., 2019). The initial tokens\nˆY are predicted using argmax decoding with\nfdec(fenc(X; θ),H; θ).\nTo calculate the loss LGLM, we compare the ini-\ntial prediction ˆY against the ground-truth to select\ntokens within the target sentence, i.e. GS(Y, ˆY).\nWe then replace those sampled indices of h’s with\ncorresponding target word embeddings, H′ =\nRP(Embyt∈GS(Y, ˆY )(yt),H), where RP replaces\nthe corresponding indices. Namely, if a token in\nthe target is sampled, its word embedding replaces\nthe corresponding h. Here the word embeddings\nare obtained from the softmax embedding matrix\nof the decoder. The updated H′is then fed into\nthe decoder fdec again to calculate the output token\nprobability. Speciﬁcally, the output probabilities of\nremaining tokens p(yt|GS(Y, ˆY),X; θ) are com-\nputed with fdec(H′,fenc(X; θ); θ).\n3.2 The Glancing Sampling Strategy\nOne important component of GLM is to adaptively\nselect the positions of tokens from the target sen-\ntence. Those selected tokens provide “correct” in-\nformation from the ground-truth target, therefore it\nhelps training the decoder to predict the rest non-\nselected tokens. Intuitively, our adaptive sampling\nstrategy guides the model to ﬁrst learn the gener-\nation of fragments and then gradually turn to the\nwhole sentences. Our glancing sampling strategy\nselects many words at the start of the training, when\nthe model is not yet well tuned. As the model gets\nbetter progressively, the sampling strategy will sam-\nple fewer words to enable the model to learn the\nparallel generation of the whole sentence. Note\nthat the sampling strategy is crucial in the training\nof GLAT.\nAs illustrated in Figure 2, the glancing sampling\ncould be divided into two steps: ﬁrst deciding a\nsampling number S, and then randomly selecting\nSwords from the reference. The sampling number\nSwill be larger when the model is poorly trained\nand decreases along the training process. Note that\nwe choose to randomly select the Swords from the\nreference. The random reference word selection is\nsimple and yields good performance empirically.\nFormally, given the input X, its predicted sen-\ntence ˆY and its reference Y, the goal of glancing\nsampling function GS(Y, ˆY) is to obtain a subset\nof words sampled from Y:\nGS(Y, ˆY) = Random(Y,S(Y, ˆY)) (2)\nHere, Random(Y,S) is randomly selecting S to-\nkens from Y, and S is computed by comparing\nthe difference between ˆY and Y, S(Y, ˆY) = λ·\nd(Y, ˆY). The sampling ratio λis a hyper-parameter\nto more ﬂexibly control the number of sampled to-\nkens. d(Y, ˆY) is a metric for measuring the differ-\nences between Y and ˆY. We adopt the Hamming\ndistance (Hamming, 1950) as the metric, which\nis computed as d(Y, ˆY) = ∑T\nt=1(yt ̸= ˆyt). With\nd(Y, ˆY), the sampling number can be decided adap-\ntively considering the current trained model’s pre-\ndiction capability. For situations thatY and ˆY have\ndifferent lengths, d(Y, ˆY) could be other distances\nsuch as Levenshtein distance (Levenshtein, 1966).\nAlternative glancing sampling strategy can be\nadopted as well. For example, one simple alterna-\ntive strategy is to set the number of sampled tokens\nto be proportional to the target sentence length, i.e.\nS = λ∗T. We will evaluate the effects of these\nvariations in the experiment.\n3.3 Inference\nGLAT only modiﬁes the training procedure. Its in-\nference is fully parallel with only a single pass. For\nparallel generation, we need to decide the output\nlengths before decoding. A simple way to decide\nthe output lengths is predicting length with repre-\nsentations from the encoder.\nIn GLAT, the length prediction is implemented\nas in Ghazvininejad et al. (2019). An additional\n[LENGTH] token is added to the source input, and\nthe encoder output for the [LENGTH] token is\nused to predict the length.\nWe also use two more complex methods to bet-\nter decide the output lengths: noisy parallel decod-\ning (NPD) and connectionist temporal classiﬁca-\ntion (CTC). For NPD (Gu et al., 2018), we ﬁrst\npredict mtarget length candidates, then generate\noutput sequences with argmax decoding for each\ntarget length candidate. Then we use a pre-trained\ntransformer to rank these sequences and identify\nthe best overall output as the ﬁnal output. For\nCTC (Graves et al., 2006), following Libovick `y\nand Helcl (2018), we ﬁrst set the max output length\nto twice the source input length, and remove the\nblanks and repeated tokens after generation.\n4 Experiments\nIn this section, we ﬁrst introduce the settings of our\nexperiments, then report the main results compared\nwith several strong baselines. Ablation studies and\nfurther analysis are also included to verify the ef-\nfects of different components used in GLAT.\nModels Idec\nWMT14 WMT16 Speed UpEN-DE DE-EN EN-RO RO-EN\nAT Models Transformer (Vaswani et al., 2017) T 27.30 / / / /\nTransformer (ours) T 27.48 31.27 33.70 34.05 1.0×\nIterative NAT\nNAT-IR (Lee et al., 2018) 10 21.61 25.48 29.32 30.19 1.5×\nLaNMT (Shu et al., 2020) 4 26.30 / / 29.10 5.7×\nLevT (Gu et al., 2019) 6+ 27.27 / / 33.26 4.0×\nMask-Predict (Ghazvininejad et al., 2019)10 27.03 30.53 33.08 33.31 1.7×\nJM-NAT (Guo et al., 2020) 10 27.31 31.02 / / 5.7×\nFully NAT\nNAT-FT (Gu et al., 2018) 1 17.69 21.47 27.29 29.06 15.6×\nMask-Predict (Ghazvininejad et al., 2019)1 18.05 21.83 27.32 28.20 /\nimit-NAT (Wei et al., 2019) 1 22.44 25.67 28.61 28.90 18.6×\nNAT-HINT (Li et al., 2019) 1 21.11 25.24 / / /\nFlowseq (Ma et al., 2019) 1 23.72 28.39 29.73 30.72 1.1×\nNAT-DCRF (Sun et al., 2019) 1 23.44 27.22 / / 10.4×\nw/ CTC NAT-CTC (Libovick`y and Helcl, 2018) 1 16.56 18.64 19.54 24.67 /\nImputer (Saharia et al., 2020) 1 25.80 28.40 32.30 31.70 18.6×\nw/ NPD\nNAT-FT + NPD (m=100) 1 19.17 23.20 29.79 31.44 2.4×\nimit-NAT + NPD (m=7) 1 24.15 27.28 31.45 31.81 9.7×\nNAT-HINT + NPD (m=9) 1 25.20 29.52 / / /\nFlowseq + NPD (m=30) 1 25.31 30.68 32.20 32.84 /\nNAT-DCRF + NPD (m=9) 1 26.07 29.68 / / 6.1×\nOurs\nNAT-base* 1 20.36 24.81 28.47 29.43 15.3×\nCTC* 1 25.52 28.73 32.60 33.46 14.6×\nGLAT 1 25.21 29.84 31.19 32.04 15.3×\nGLAT + CTC 1 26.39 29.54 32.79 33.84 14.6×\nGLAT + NPD (m=7) 1 26.55 31.02 32.87 33.51 7.9×\nTable 1: Results on WMT14 EN-DE/DE-EN and WMT16 EN-RO/RO-EN benchmarks. Idec is the number of\ndecoding iterations and mis the number of length reranking candidates. NPD represents noisy parallel decoding,\nCTC represents connectionist temporal classiﬁcation. * indicate the results are obtained by our implementation.\n4.1 Experimental Settings\nDatasets We conduct experiments on three ma-\nchine translation benchmarks: WMT14 EN-DE\n(4.5M translation pairs), WMT16 EN-RO (610k\ntranslation pairs), and IWSLT16 DE-EN (150K\ntranslation pairs). These datasets are tokenized\nand segmented into subword units using BPE en-\ncodings (Sennrich et al., 2016). We preprocess\nWMT14 EN-DE by following the data preprocess-\ning in Vaswani et al. (2017). For WMT16 EN-RO\nand IWSLT16 DE-EN, we use the processed data\nprovided in Lee et al. (2018).\nKnowledge Distillation Following previous\nwork (Gu et al., 2018; Lee et al., 2018; Wang et al.,\n2019), we also use sequence-level knowledge\ndistillation for all datasets. We employ the\ntransformer with the base setting in Vaswani et al.\n(2017) as the teacher for knowledge distillation.\nThen, we train our GLAT on distilled data.\nBaselines and Setup We compare our method\nwith the base Transformer and strong representa-\ntive NAT baselines in Table 1. For all our tasks,\nwe obtain other NAT models’ performance by di-\nrectly using the performance ﬁgures reported in\ntheir papers if they are available.\nWe adopt the vanilla model which copies\nsource input uniformly in Gu et al. (2018) as\nour base model (NAT-base) and replace the Uni-\nformCopy with attention mechanism using po-\nsitions. Note that the output length does not\nequal the length of reference in models using\nCTC. Therefore, for GLAT with CTC, we adopt\nlongest common subsequence distance for compar-\ning Y and ˆY, and the glancing target is the tar-\nget alignment that maximize the output probability\nargmaxa∈B−1(Y ) P(a|X; θ). B−1 is the mapping\nproposed in (Graves et al., 2006), which expand\nthe reference to the length of output by inserting\nblanks or repeating words.\nFor WMT datasets, we follow the hyperparam-\neters of the base Transformer in Vaswani et al.\n(2017). And we choose a smaller setting for\nIWSLT16, as IWSLT16 is a smaller dataset. For\nIWSLT16, we use 5 layers for encoder and decoder,\nand set the model size dmodel to 256. Using Nvidia\nV100 GPUs, We train the model with batches\nof 64k/8k tokens for WMT/IWSLT datasets, re-\nspectively. We set the dropout rate to 0.1 and\nuse Adam optimizer (Kingma and Ba, 2014) with\nβ = (0.9,0.999). For WMT datasets, the learning\nrate warms up to 5e−4 in 4k steps and gradually\n22 24 26 28 30\nPerformance (BLEU)\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5Relative Decoding Speed-up\nGLAT\nTransformer\nNAT-base\nNAT-FT\nNAT-DCRF\nimit-NAT\nMask-Predict\nJM-NAT\nFigure 3: The trade-off between speed-up and BLEU\non WMT14 DE-EN\ndecays according to inverse square root schedule\nin Vaswani et al. (2017). As for IWSLT16 DE-EN,\nwe adopt linear annealing (from 3e−4 to 1e−5)\nas in Lee et al. (2018). For the hyper-parameter λ,\nwe adopt linear annealing from 0.5 to 0.3 for WMT\ndatasets and a ﬁxed value of 0.5 for IWSLT16. The\nﬁnal model is created by averaging the 5 best check-\npoints chosen by validation BLEU scores.\n4.2 Main Results\nThe main results on the benchmarks are presented\nin Table 1. GLAT signiﬁcantly improves the trans-\nlation quality and outperforms strong baselines by a\nlarge margin. Our method introduces explicit word\ninterdependency modeling for the decoder and\ngradually learns simultaneous generation of whole\nsequences, enabling the model to better capture the\nunderlying data structure. Compared to models\nwith iterative decoding, our method completely\nmaintains the inference efﬁciency advantage of\nfully non-autoregressive models, since GLAT gen-\nerate with a single pass. Compared with the base-\nlines, we highlight our empirical advantages:\n• GLAT is highly effective. Compared with the\nvanilla NAT-base models, GLAT obtains sig-\nniﬁcant improvements (about 5 BLEU) on EN-\nDE/DE-EN. Additionally, GLAT also outper-\nforms other fully non-autoregressive models\nwith a substantial margin (almost +2 BLEU\nscore on average). The results are even very\nclose to those of the AT model, which shows\ngreat potential.\n• GLAT is simple and can be applied to other\nNAT models ﬂexibly, as we only modify the\ntraining process by reference glancing while\nkeeping inference unchanged. For compari-\nson, NAT-DCRF utilizes CRF to generate se-\nquentially; NAT-IR and Mask-Predict models\n[0,20) [20,40) [40,60) >=60\nSource Input Length\n26\n28\n30\n32\n34Performance (BLEU)\nNAT-base\nTransformer\nGLAT\nFigure 4: Performance under different source input\nlength on WMT14 DE-EN\nneed multiple decoding iterations.\n• CTC and NPD use different approaches to de-\ntermine the best output length, and they have\ntheir own advantages and disadvantages. CTC\nrequires the output length to be longer than\nthe exact target length. With longer output\nlengths, the training will consume more time\nand GPU memory. As for NPD, with a certain\nnumber of length reranking candidates, the\ninference speed will be slower than models\nusing CTC. Note that NPD can use pretrained\nAT models or the non-autoregressive model\nitself to rerank multiple outputs.\nWe also present a scatter plot in Figure 3, display-\ning the trend of speed-up and BLEU with different\nNAT models. It is shown that the point of GLAT is\nlocated on the top-right of the competing methods.\nObviously, GLAT outperforms our competitors in\nBLEU if speed-up is controlled, and in speed-up\nif BLEU is controlled. This indicates that GLAT\noutperforms previous state-of-the-art NAT meth-\nods. Although iterative models like Mask-Predict\nachieves competitive BLEU scores, they only main-\ntain minor speed advantages over AT. In contrast,\nfully non-autoregressive models remarkably im-\nprove the inference speed.\n4.3 Analysis\nEffect of Source Input Length To analyze the\neffect of source input length on the models’ per-\nformance, we split the source sentences into differ-\nent intervals by length after BPE and compute the\nBLEU score for each interval. The histogram of\nresults is presented in Figure 4. NAT-base’s perfor-\nmance drops sharply for long sentences, while the\ngradual learning process enables GLAT to boost\nthe performance by a large margin, especially for\nModel WMT14\nEN-DE DE-EN\nNAT-base 8.32% 7.10%\nGLAT 1.19% 1.05%\nGLAT w/ NPD 0.32% 0.16%\nTable 2: Token repetition ratio on WMT14 EN-DE and\nWMT14 DE-EN\nlong sentences. We also ﬁnd that GLAT outper-\nforms autoregressive Transformer when the source\ninput length is smaller than 20.\nGLAT Reduces Repetition We also measure\nthe percentage of repeated tokens on test set of\nWMT14 EN-DE and WMT14 DE-EN. Table 2\npresents the token repetition ratio of sentences gen-\nerated by NAT-base and GLAT. The results show\nthat GLAT signiﬁcantly reduces the occurrence of\nrepetition, and the repetition ratio can be further\nreduced with NPD. We think an important cause\nof the improvement is better interdependency mod-\neling. Since GLAT explicitly encourages word\ninterdependency modeling to better capture the de-\npendency between target tokens, wrong generation\npatterns, such as repetition, can be largely avoided.\n4.4 Ablation Study\nEffectiveness of the Adaptive Sampling Num-\nber To validate the effectiveness of the adap-\ntive sampling strategy for the sampling number\nS(Y, ˆY), we also introduce two ﬁxed approaches\nfor comparison. The ﬁrst one decides the sampling\nnumber with λ∗T, where T is the length of Y, and\nλis a constant ratio. The second one is relatively\nﬂexible, which sets a start ratio of λs and an end\nratio λe, and linearly reduces the sampling number\nfrom λs ∗T to λe ∗T along the training process.\nAs shown in Table 3 and Table 4, clearly, our\nadaptive approach (Adaptive in the table) outper-\nforms the baseline models with big margins. The re-\nsults conﬁrm our intuition that the sampling sched-\nule affects the generation performance of our NAT\nmodel. The sampling strategy, which ﬁrst offers\nrelatively easy generation problems and then turns\nharder, beneﬁts the ﬁnal performance. Besides,\neven with the simplest constant ratio, GLAT still\nachieves remarkable results. When set λ = 0 .2,\nit even outperforms the baseline λ = 0.0 by 2.5\nBLEU score.\nThe experiments potentially support that it is ben-\neﬁcial to learn the generation of fragments at the\nSampling Number λ BLEU\nFixed\n0.0 24.66\n0.1 24.91\n0.2 27.12\n0.3 24.98\n0.4 22.96\nAdaptive - 29.61\nTable 3: Performances on IWSLT16 with ﬁxed sam-\npling ratio.\nSampling Number λs λe BLEU\nDecreasing\n0.5 0 27.80\n0.5 0.1 28.21\n0.5 0.2 27.15\n0.5 0.3 23.37\nAdaptive - 29.61\nTable 4: Performances on IWSLT16 with decreasing\nsampling ratio.\nstart and gradually transfer to the whole sequence.\nThe ﬂexible decreasing ratio method works better\nthan the constant one, and our proposed adaptive\napproaches achieve the best results.\nInﬂuence of Reference Word Selection To ana-\nlyze how the strategies of selecting reference words\naffect glancing sampling, we conduct experiments\nwith different selection strategies. By default, we\nassume all the words in the reference are equally\nimportant and randomly choose reference words\nfor glancing. Besides the random strategy, we de-\nvise four other selection methods considering the\nprediction of ﬁrst decoding. Forpref and 1−pref, the\nsampling probability of each reference word is pro-\nportional to the output probability for the reference\nword pref and the probability 1 −pref, respectively.\nSimilar to the word selection strategy for masking\nwords during inference in Mask-Predict, we also\nadd two strategies related to the prediction conﬁ-\ndence: \"most certain\" and \"most uncertain.\" We\nchoose the positions where predictions have higher\nconﬁdence for \"most certain\", and vise versa for\n\"most uncertain.\" The results for different selection\nmethods are listed in Table 5.\nIn comparisons, the model with the selection\nstrategy 1 −pref outperforms the one with pref, in-\ndicating that words hard to predict are more im-\nportant for glancing in training. And we ﬁnd that\nthe random strategy performs a little better than\nthe two conﬁdence-based strategies. We think this\nindicates that introducing more randomness in sam-\nSelection Strategy GLAT GLAT w/ NPD\nrandom 25.21 26.55\npref 24.87 25.83\n1 − pref 25.37 26.52\nmost certain 24.99 26.22\nmost uncertain 24.86 26.13\nTable 5: Performance on WMT14 EN-DE with differ-\nent reference word selection strategies.\nMethod WMT14\nEN-DE DE-EN\nGLAT w/ uniform sampling 19.16 23.56\nGLAT w/ [MASK] inputs 24.99 29.48\nGLAT 25.21 29.84\nTable 6: Ablation study for comparing GLAT and\nMask-Predict on WMT14 EN-DE and DE-EN.\npling enable GLAT to explore more interdepen-\ndency among target words. We adopt the random\nstrategy for its simplicity and good performance.\nAdvantages of GLAT over Mask-Predict To\nstudy the effects of sampling strategy and decoder\ninputs of GLAT, we conduct experiments for re-\nplacing these two modules in GLAT with the corre-\nsponding part in Mask-Predict, respectively. The\nresults are presented in Table 6. GLAT employs\nglancing sampling strategy instead of the uniform\nsampling strategy used in Mask-Predict, and re-\nplaces the [MASK] token inputs with source rep-\nresentations from the encoder. The results show\nthat the glancing sampling strategy outperforms the\nuniform sampling strategy by 5∼6 BLEU points,\nand feeding representations from the encoder as\nthe decoder input could still improve the strong\nbaseline by 0.2∼0.3 BLEU points after adopting\nglancing sampling. To sum up, the adaptive glanc-\ning sampling approach contributes the most to the\nﬁnal improvement, and the use of representations\nfrom the encoder also helps a bit.\nMore Analysis We also conduct experiments for:\na) comparison of different distance metrics for\nglancing sampling, b) GLAT with more than one\ndecoding iteration. The details are in Appendix.\n5 Related Work\nFully Non-Autoregressive Models A line of\nwork introduces various forms of latent variables\nto reduce the model’s burden of dealing with de-\npendencies among output words (Gu et al., 2018;\nMa et al., 2019; Bao et al., 2019; Ran et al., 2019).\nAnother branch of work considers transferring the\nknowledge from autoregressive models to non-\nautoregressive models (Wei et al., 2019; Li et al.,\n2019; Guo et al., 2019b; Sun and Yang, 2020). Be-\nsides, there are also some work that apply different\ntraining objectives to train non-autoregressive mod-\nels (Libovick`y and Helcl, 2018; Shao et al., 2020;\nGhazvininejad et al., 2020a), add regularization\nterms (Wang et al., 2019; Guo et al., 2019a).\nNon-Autoregressive Models with Structured\nDecoding To model the dependencies between\nwords, Sun et al. (2019) introduces a CRF inference\nmodule in NAT and performs additional sequential\ndecoding after the non-autoregressive computation\nin inference. Deng and Rush (2020) proposes cas-\ncaded CRF decoding. Since GLAT only performs\nsingle-pass non-autoregressive generation, our ap-\nproach is orthogonal to the method proposed in Sun\net al. (2019). We can also combine our approach\nwith the structured decoding methods.\nNon-Autoregressive Models with Iterative Re-\nﬁnement A series of work are devoted to semi-\nautoregressive models that reﬁne the outputs with\nmulti-pass iterative decoding (Lee et al., 2018; Gu\net al., 2019; Ghazvininejad et al., 2019, 2020b;\nKasai et al., 2020). Lee et al. (2018) proposed\na method of iterative reﬁnement based on denois-\ning autoencoder. Gu et al. (2019) utilized inser-\ntion and deletion to reﬁne the outputs in inference.\nGhazvininejad et al. (2019) trained the model with\nthe masked language model, and the model iter-\natively replaces masked tokens with new outputs.\nDespite the relatively better accuracy, the multi-\nple decoding iterations vastly reduce the inference\nefﬁciency of non-autoregressive models.\n6 Conclusion\nIn this paper, we propose Glancing Transformer\nwith a glancing language model to improve the per-\nformance of single-pass parallel generation models.\nWith the glancing language model, the model starts\nfrom learning the generation of sequence fragments\nand gradually moving to whole sequences. Experi-\nmental results show that our approach signiﬁcantly\nimproves the performance of non-autoregressive\nmachine translation with single-pass parallel gener-\nation. As GLAT achieves competitive performance\ncompared with autoregressive models, applying our\napproach to other generation tasks is a promising\ndirection for future work.\nReferences\nYu Bao, Hao Zhou, Jiangtao Feng, Mingxuan Wang,\nShujian Huang, Jiajun Chen, and Lei Li. 2019.\nNon-autoregressive transformer by position learning.\narXiv preprint arXiv:1911.10677.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nICML, pages 41–48.\nYuntian Deng and Alexander M Rush. 2020. Cascaded\ntext generation with markov transformers. arXiv\npreprint arXiv:2006.01112.\nMarjan Ghazvininejad, Vladimir Karpukhin, Luke\nZettlemoyer, and Omer Levy. 2020a. Aligned cross\nentropy for non-autoregressive machine translation.\narXiv preprint arXiv:2004.01655.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nEMNLP-IJCNLP, pages 6114–6123.\nMarjan Ghazvininejad, Omer Levy, and Luke Zettle-\nmoyer. 2020b. Semi-autoregressive training im-\nproves mask-predict decoding. arXiv preprint\narXiv:2001.08785.\nAlex Graves, Santiago Fernández, Faustino Gomez,\nand Jürgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks. In\nICML, pages 369–376.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In ICLR.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In NeurIPS, pages 11179–\n11189.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019a. Non-autoregressive neural ma-\nchine translation with enhanced decoder input. In\nAAAI, volume 33, pages 3723–3730.\nJunliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong\nChen, and Tie-Yan Liu. 2019b. Fine-tuning by cur-\nriculum learning for non-autoregressive neural ma-\nchine translation. arXiv preprint arXiv:1911.08717.\nJunliang Guo, Linli Xu, and Enhong Chen. 2020.\nJointly masked sequence-to-sequence model for non-\nautoregressive neural machine translation. In ACL,\npages 376–385.\nRichard W Hamming. 1950. Error detecting and error\ncorrecting codes. The Bell system technical journal,\n29(2):147–160.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In In-\nternational Conference on Machine Learning, pages\n5144–5155. PMLR.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural\nsequence modeling by iterative reﬁnement. In\nEMNLP, pages 1173–1182.\nVladimir I Levenshtein. 1966. Binary codes capable\nof correcting deletions, insertions, and reversals. In\nSoviet physics doklady, pages 707–710.\nZhuohan Li, Di He, Fei Tian, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2019. Hint-based training for non-\nautoregressive translation. In EMNLP-IJCNLP.\nJindˇrich Libovick `y and Jind ˇrich Helcl. 2018. End-\nto-end non-autoregressive neural machine transla-\ntion with connectionist temporal classiﬁcation. In\nEMNLP, pages 3016–3021.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In EMNLP-IJCNLP, pages 4273–\n4283, Hong Kong, China.\nQiu Ran, Yankai Lin, Peng Li, and Jie Zhou. 2019.\nGuiding non-autoregressive neural machine transla-\ntion decoding with reordering information. arXiv\npreprint arXiv:1911.02215.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive\nmachine translation with latent alignments. arXiv\npreprint arXiv:2004.07437.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL, pages 1715–1725.\nChenze Shao, Jinchao Zhang, Yang Feng, Fandong\nMeng, and Jie Zhou. 2020. Minimizing the bag-of-\nngrams difference for non-autoregressive neural ma-\nchine translation. In AAAI, pages 198–205.\nRaphael Shu, Jason Lee, Hideki Nakayama, and\nKyunghyun Cho. 2020. Latent-variable non-\nautoregressive neural machine translation with deter-\nministic inference using a delta posterior. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 34, pages 8846–8853.\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,\nZi Lin, and Zhihong Deng. 2019. Fast structured\ndecoding for sequence models. In NeurIPS, pages\n3016–3026.\nZhiqing Sun and Yiming Yang. 2020. An em ap-\nproach to non-autoregressive conditional sequence\ngeneration. In International Conference on Machine\nLearning, pages 9249–9258. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nAAAI.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In ACL.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8920890092849731
    },
    {
      "name": "Speedup",
      "score": 0.8511948585510254
    },
    {
      "name": "Computer science",
      "score": 0.7812238931655884
    },
    {
      "name": "Transformer",
      "score": 0.7735868692398071
    },
    {
      "name": "Autoregressive model",
      "score": 0.7567723989486694
    },
    {
      "name": "Decoding methods",
      "score": 0.674516499042511
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4417308568954468
    },
    {
      "name": "Artificial neural network",
      "score": 0.43340620398521423
    },
    {
      "name": "Speech recognition",
      "score": 0.39606156945228577
    },
    {
      "name": "Algorithm",
      "score": 0.379489541053772
    },
    {
      "name": "Parallel computing",
      "score": 0.30969756841659546
    },
    {
      "name": "Voltage",
      "score": 0.16188564896583557
    },
    {
      "name": "Mathematics",
      "score": 0.09172862768173218
    },
    {
      "name": "Engineering",
      "score": 0.0785001814365387
    },
    {
      "name": "Electrical engineering",
      "score": 0.06774666905403137
    },
    {
      "name": "Econometrics",
      "score": 0.06258860230445862
    }
  ]
}