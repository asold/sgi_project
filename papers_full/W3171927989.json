{
  "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models",
  "url": "https://openalex.org/W3171927989",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4220196588",
      "name": "Po-Yao Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2980617485",
      "name": "Mandela Patrick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107488194",
      "name": "Junjie Hu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2605370055",
      "name": "Florian Metze",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A4227866007",
      "name": "Alexander Hauptmann",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963729324",
    "https://openalex.org/W3035144493",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3009380496",
    "https://openalex.org/W2989322838",
    "https://openalex.org/W2972073579",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W342285082",
    "https://openalex.org/W2887920589",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W3176398504",
    "https://openalex.org/W2963490498",
    "https://openalex.org/W3010094231",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2965458216",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2509490957",
    "https://openalex.org/W2964040984",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2295781714",
    "https://openalex.org/W2981473723",
    "https://openalex.org/W3034636873",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W3035356601",
    "https://openalex.org/W2943152387",
    "https://openalex.org/W1818534184",
    "https://openalex.org/W2970632400",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2950162424",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3015354748",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2885775891",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3034815696",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W4298000964",
    "https://openalex.org/W3122640483",
    "https://openalex.org/W4299801216",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2915128308",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2087095333",
    "https://openalex.org/W3035276082",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2963496089",
    "https://openalex.org/W4287757777",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2995558462",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3035119608",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963721344",
    "https://openalex.org/W3033733952",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2997786945",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W2251033195",
    "https://openalex.org/W2949474740",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2980037812",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2962795934",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W3111865597",
    "https://openalex.org/W3023441976",
    "https://openalex.org/W2975813532",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2963899908",
    "https://openalex.org/W3035313462",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963909453",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W3097619042",
    "https://openalex.org/W2972573244",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W3023986361",
    "https://openalex.org/W2152790380"
  ],
  "abstract": "Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, Alexander Hauptmann. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2443‚Äì2459\nJune 6‚Äì11, 2021. ¬©2021 Association for Computational Linguistics\n2443\nMultilingual Multimodal Pre-training for\nZero-Shot Cross-Lingual Transfer of Vision-Language Models\nPo-Yao Huang13‚àó, Mandela Patrick23‚àó, Junjie Hu1,\nGraham Neubig1, Florian Metze3, Alexander Hauptmann1\n1School of Computer Science, Carnegie Mellon University\n2Visual Geometry Group, University of Oxford\n3Facebook AI\n{poyaoh,junjieh,gneubig,alex}@cs.cmu.edu, {mandelapatrick,fmetze}@fb.com\nAbstract\nThis paper studies zero-shot cross-lingual\ntransfer of vision-language models. Specif-\nically, we focus on multilingual text-to-\nvideo search and propose a Transformer-based\nmodel that learns contextual multilingual mul-\ntimodal embeddings. Under a zero-shot set-\nting, we empirically demonstrate that perfor-\nmance degrades signiÔ¨Åcantly when we query\nthe multilingual text-video model with non-\nEnglish sentences. To address this prob-\nlem, we introduce a multilingual multimodal\npre-training strategy, and collect a new mul-\ntilingual instructional video dataset (Multi-\nHowTo100M) for pre-training. Experiments\non VTT show that our method signiÔ¨Åcantly im-\nproves video search in non-English languages\nwithout additional annotations. Furthermore,\nwhen multilingual annotations are available,\nour method outperforms recent baselines by\na large margin in multilingual text-to-video\nsearch on VTT and V ATEX; as well as in mul-\ntilingual text-to-image search on Multi30K.\nOur model and Multi-HowTo100M is avail-\nable at http://github.com/berniebear/\nMulti-HT100M\n1 Introduction\nOne of the key challenges at the intersection of\ncomputer vision (CV) and natural language pro-\ncessing (NLP) is building versatile vision-language\nmodels that not only work in English, but in all of\nthe world‚Äôs approximately 7,000 languages. Since\ncollecting and annotating task-speciÔ¨Åc parallel mul-\ntimodal data in all languages is impractical, a\nframework that makes vision-language models gen-\neralize across languages is highly desirable.\nOne technique that has shown promise to greatly\nimprove the applicability of NLP models to new\nlanguages is zero-shot cross-lingual transfer, where\nmodels trained on a source language are applied\n‚àóEqual contribution.\nas-is to a different language without any additional\nannotated training data (T ¬®ackstr¬®om et al., 2012;\nKlementiev et al., 2012; Cotterell and Heigold,\n2017; Chen et al., 2018; Neubig and Hu, 2018). In\nparticular, recent techniques for cross-lingual trans-\nfer have demonstrated that by performing unsuper-\nvised learning of language or translation models\non many languages, followed by downstream task\nÔ¨Åne-tuning using only English annotation, models\ncan nonetheless generalize to a non-English lan-\nguage (Wu and Dredze, 2019a; Lample and Con-\nneau, 2019; Huang et al., 2019a; Artetxe et al.,\n2020; Hu et al., 2020). This success is attributed to\nthe fact that many languages share a considerable\namount of underlying vocabulary or structure. At\nthe vocabulary level, languages often have words\nthat stem from the same origin, for instance, ‚Äúdesk‚Äù\nin English and ‚ÄúTisch‚Äù in German both come from\nthe Latin ‚Äúdiscus‚Äù. At the structural level, all lan-\nguages have a recursive structure, and many share\ntraits of morphology or word order.\nFor cross-lingual transfer of vision-language\nmodels, the visual information is clearly an essen-\ntial element. To this end, we make an important yet\nunder-explored step to incorporate visual-textual re-\nlationships for improving multilingual models (De-\nvlin et al., 2019; Artetxe et al., 2020). While spo-\nken languages could be different, all humans share\nsimilar vision systems, and many visual concepts\ncan be understood universally (Sigurdsson et al.,\n2020; Zhang et al., 2020). For example, while\nis termed ‚Äúcat‚Äù for an English speaker and ‚Äúchat‚Äù\nfor a French speaker; they understand\n similarly.\nWe leverage this observation to learn to associate\nsentences in different languages with visual con-\ncepts for promoting cross-lingual transfer of vision-\nlanguage models.\nIn this work, we focus on multilingual text-to-\nvideo search tasks and propose a Transformer-\nbased video-text model to learn contextual mul-\n2444\ntilingual multimodal representations. Our vanilla\nmodel yields state-of-the-art performance in multi-\nlingual text‚Üívideo search when trained with multi-\nlingual annotations. However, under the zero-shot\nsetting, rather surprisingly, there is a signiÔ¨Åcant\nperformance gap between English and non-English\nqueries (see ¬ß5.5 for details). To resolve this prob-\nlem, motivated by recent advances in large-scale\nlanguage model (Artetxe et al., 2020) and multi-\nmodal pre-training (Lu et al., 2019; Miech et al.,\n2019; Patrick et al., 2020), we propose a multi-\nlingual multimodal pre-training (MMP) strategy\nto exploit the weak supervision from large-scale\nmultilingual text-video data. We construct the\nMultilingual-HowTo100M dataset, that extends the\nEnglish HowTo100M (Miech et al., 2019) dataset\nto contain subtitles in 9 languages for 1.2 million\ninstructional videos.\nOur method has two important beneÔ¨Åts. First,\ncompared to pre-training on English-video data\nonly, pre-training on multilingual text-video data\nexploits the additional supervision from a variety\nof languages, and therefore, enhances the search\nperformance on an individual language. Second,\nby exploiting the visual data as an implicit ‚Äúpivot‚Äù\nat scale, our methods learns better alignments in\nthe multilingual multimodal embedding space (e.g.,\n‚Äúcat‚Äù-\n -‚Äúchat‚Äù), which leads to improvement in\nzero-shot cross-lingual transfer ( e.g., from ‚Äúcat‚Äù-\nto ‚Äúchat‚Äù-\n ) of vision-language models.\nIn our experiments on VTT (Xu et al., 2016)\nand V ATEX (Wang et al., 2019), our method\nyields state-of-the-art English‚Üívideo search per-\nformance. For zero-shot cross-lingual transfer, the\nproposed multilingual multimodal pre-training im-\nproves English-video pre-training by 2 ‚àº2.5 in av-\nerage R@1 across 9 languages. Additionally, when\ntrained with in-domain multilingual annotations as\nother baselines, our method outperforms them by a\nlarge margin in multilingual text‚Üívideo search on\nV ATEX and text‚Üíimage search on Multi30K (El-\nliott et al., 2016).\nTo summarize, we make the following contribu-\ntions: (1) We propose a transformer-based video-\ntext model that learns contextual multilingual mul-\ntimodal representations (¬ß3.1). (2) We empirically\ndemonstrate that vision-language models, unlike\nNLP models, have limited zero-shot cross-lingual\ntransferrability. (¬ß5.5). (3) We introduce the multi-\nlingual multimodal pre-training strategy and con-\nstruct a new Multi-HowTo100M dataset ( ¬ß4) for\npre-training to improve zero-shot cross-lingual ca-\npability of vision-language models. (4) We demon-\nstrate the effectiveness of our approach, by achiev-\ning state-of-the-art multilingual text‚Üívideo search\nperformance in both the zero-shot (¬ß5.5) and fully\nsupervised setup (¬ß5.6).\n2 Related Work\nCross-lingual representations. Early work on\nlearning non-contextual cross-lingual representa-\ntions used either parallel corpora (Gouws and\nS√∏gaard, 2015; Luong et al., 2015) or a bilin-\ngual dictionary to learn a transformation (Faruqui\nand Dyer, 2014; Mikolov et al., 2013). Later ap-\nproaches reduced the amount of supervision using\nself-training (Artetxe et al., 2017). With the ad-\nvances in monolingual transfer learning (McCann\net al., 2017; Howard and Ruder, 2018; Peters et al.,\n2018; Devlin et al., 2019), multilingual extensions\nof pre-trained encoders have been proven effective\nin learning deep contextual cross-lingual represen-\ntations (Eriguchi et al., 2017; Lample and Conneau,\n2019; Wu and Dredze, 2019b; Siddhant et al., 2020;\nPires et al., 2019; Pfeiffer et al., 2020). We extend\nprior work to incorporate visual context.\nVideo-text representations. The HowTo100M\ndataset (Miech et al., 2019) has attracted signif-\nicant interest in leveraging multimodal pre-training\nfor text‚Üívideo search (Korbar et al., 2020), cap-\ntioning (Iashin and Rahtu, 2020), and unsuper-\nvised translation via image-based (Sur ¬¥ƒ±s et al.,\n2020; Huang et al., 2020b) and video-based (Sig-\nurdsson et al., 2020) alignment. This work stud-\nies a challenging and unexplored task: Zero-shot\ncross-lingual transfer of vision-language models.\nUnlike prior image/video-text work that utilizes\nRNN (Dong et al., 2019; Chen et al., 2020a; Burns\net al., 2020; Kim et al., 2020) and inter-modal con-\ntrastive objectives (Sigurdsson et al., 2020; Liu\net al., 2019; Huang et al., 2019b; Patrick et al.,\n2021), we employ Transformers to learn contex-\ntual multilingual multimodal representations and\nuniquely models cross-lingual instances. Moreover,\nwe build Multi-HowTo100M, the largest text-video\ndataset for multilingual multimodal pre-training.\nCross-lingual Transfer.Cross-lingual transfer has\nproven effective in many NLP tasks including de-\npendency parsing (Schuster et al., 2019), named\nentity recognition (Rahimi et al., 2019), sentiment\nanalysis (Barnes et al., 2019), document classiÔ¨Å-\ncation (Schwenk and Li, 2018), and question an-\n2445\nmBERT\n3D-CNN\nTP\nTP\na man performs shot put\nun hombre realiza lanzamiento de bala\nAttentionAttentionAttentionAttention\nAttentionAttentionAttention1D-CNN1D-CNN\n1D-CNN1D-CNN 3DCNN\nTime\nGRUaman\nshotput\nGRU\nGRUGRU\n‚Ä¶\n1D-CNNAttentionAttentionAttentionAttention\nAttention\nLinear\n1D-CNN\nAttentionAttentionAttentionAttention\nLinearAttentionAttentionAttentionAttention\nAttentionAttentionAttention1D-CNN1D-CNN\n1D-CNN1D-CNN 3DCNN\nTime\nGRUaman\nshotput\nGRU\nGRUGRU\n‚Ä¶\n1D-CNNAttentionAttentionAttentionAttention\nAttention\nLinear\n1D-CNN\nAttentionAttentionAttentionAttention\nLinear\nAttentionAttentionAttentionAttention\nAttentionAttentionAttention1D-CNN1D-CNN\n1D-CNN1D-CNN 3DCNN\nTime\nGRUaman\nshotput\nGRU\nGRUGRU\n‚Ä¶\n1D-CNNAttentionAttentionAttentionAttention\nAttention\nLinear\n1D-CNN\nAttentionAttentionAttentionAttention\nLinear timemBERTTPContrastive attractionContrastive repulsion\nInter-modal Cross-lingual\nIntra-modal Transformer Pooling (TP)ùëê!\nùëê\"\nùëê#ùëí!\nùëí\" ùëí#\nùëí#Transformer Pooling (TP)\nùëê# ùëê!|#Œ¶ùë•ùë•\nùë£\nŒ®ùë£\nùë¶ Œ¶ùë¶\nFigure 1: The proposed video-text model for learning contextual multilingual multimodal representations. We\nutilize intra-modal, inter-modal, and conditional cross-lingual contrastive objectives to align (x,v,y ) where x\nand yare the captions or transcriptions in different languages of a video v. TP: Transformer pooling head.\nswering (Lewis et al., 2020; Artetxe et al., 2020).\nRecently, XTREME (Hu et al., 2020) was proposed\nto evaluate the cross-lingual transfer capabilities of\nmultilingual representations across a diverse set of\nNLP tasks and languages. However, a comprehen-\nsive evaluation of multilingual multimodal models\non zero-shot cross-lingual transfer capabilities is\nstill missing. To our best knowledge, we are the\nÔ¨Årst work that investigates and improves zero-shot\ncross-lingual transfer of vision-language models.\n3 Method\nWe consider the problem of learning multilingual\nmultimodal representations from a corpus Cof\nvideo-text pairs {(xi,vi)}C\ni=1, where vi is a video\nclip and xi is its corresponding text (caption or\ntranscription) that is written in one of Klanguages.\nOur goal is to learn a shared multilingual text en-\ncoder cx = Œ¶(x) and a video encoder cv = Œ®(v),\nboth of which project the input to a shared D-\ndimensional embedding space cv,ct ‚ààRD, where\nsemantically similar instances (i.e., paired (xi,vi))\nare closer to each other than the dissimilar ones\n(i.e., (xi,vj),i Ã∏= j). In the following, we de-\nnote a batch of multilingual text-video samples\nas B= {(xi,vi)}B\ni=1}where B‚äÇC .\n3.1 Multilingual Multimodal Transformers\nFigure 1 gives an overview of the proposed method.\nOur text encoder consists of a multilingual Trans-\nformer ( e.g. multilingual BERT (Devlin et al.,\n2019)) and a text Transformer pooling head (ex-\nplained below). Similarly, our video encoder con-\nsists of a 3D-CNN ( e.g. R(2+1)D network (Tran\net al., 2018)) and a video Transformer pooling head.\nWe use these multilingual multimodal Transform-\ners to encode text and video for alignment.\nUnlike prior multilingual text-image mod-\nels (Gella et al., 2017; Kim et al., 2020; Huang\net al., 2019b) that utilize word embeddings and\nRNNs, our multilingual text encoder is built on a\nmultilingual Transformer that generates contextual\nmultilingual representations ex ‚ààRN√óD to encode\na sentence xcontaining N words. We employ an\nadditional 2-layer Transformer which we will call\na ‚ÄúTransformer pooling head (TP)‚Äù as it serves as\na pooling function to selectively encode variable-\nlength sentences and aligns them with the corre-\nsponding visual content. We use the Ô¨Årst output\ntoken of the second Transformer layer as the Ô¨Ånal\nsentence representation. Precisely, we set cx =\nTrans(2)\nx (query=key=value=ex)[0] where Trans(2)\nx\nis a 2-layer stack of Transformers (Vaswani et al.,\n2017) with ex as the (query,key,value) in the multi-\nhead attention. Note that we use the same text\nencoder to encode sentences in all languages.\nFor encoding videos, our model uses pre-trained\n3D-CNNs that encode spatial-temporal context\nin a video. For a M-second video v, we apply\nR(2+1)D (Tran et al., 2018) and S3D (Miech et al.,\n2020) networks to its frames, concatenate network\noutputs, and apply a linear layer to encode the vi-\nsual input, ev ‚ààRM√óD, to our model. Similarly to\nthe text part, we employ a two-layer Transformer\nas the pooling head to encode videos with different\nlengths into Ô¨Åxed-length representations. Formally,\nwe set cv = Trans(2)\nv (query=key=value=ev)[0].\nSince videos are typically long and have a high\nframe rate (e.g., 30 fps), it is infeasible to update\n3D-CNNs simultaneously and therefore, we use\npre-extracted video features. Our model is parame-\nterized by Œ∏= Œ∏mBERT ‚à™Œ∏Transx ‚à™Œ∏Transv .\n3.2 Multilingual Text-Video Alignment\nFor learning multimodal representations, the com-\nmon practice is to minimize a contrastive objective\nto map the associated (video, text) embeddings\n2446\nto be near to each other in a shared embedding\nspace. The inter-modal max-margin triplet loss has\nbeen widely studied in video-text (Yu et al., 2018;\nLiu et al., 2019) and image-text (Kim et al., 2020;\nBurns et al., 2020; Huang et al., 2019b) research. In\nthis work, we generalize and model allinter-modal,\nintra-modal, and cross-lingual instances with a\nnoise contrastive estimation objective (NCE) (Gut-\nmann and Hyv ¬®arinen, 2010; van den Oord et al.,\n2018; Chen et al., 2020b).\nInter-modal NCE.Let Xand Vdenote the subsets\nof the sampled sentences in multiple languages and\nvideos in B, respectively. And let s(a,b) = aT b\n‚à•a‚à•‚à•b‚à•\nbe the cosine similarity measure. We use an (inter-\nmodal) NCE objective deÔ¨Åned as:\nL(X,V) = ‚àí1\nB\nB‚àë\ni=1\nlog‚ÑìNCE(Œ¶(xi),Œ®(vi)), (1)\nwhere\n‚ÑìNCE(cx,cv) = es(cx,cv)\nes(cx,cv) + ‚àë\n(x‚Ä≤,v‚Ä≤)‚àºNes(cx‚Ä≤,cv‚Ä≤)\n(2)\nIn inter-modal NCE, Linter = L(X,V), the noise\nNis a set of ‚Äúnegative‚Äù video-text pairs sampled to\nenforce the similarity of paired ones are high and\nand those do not are low. Following Miech et al.\n(2020), we set the negatives of (xi,vi) as other xj\nand vj,j Ã∏= iin B.\nIntuitively, inter-modal NCE draws paired (se-\nmantically similar) instances closer and pushes\napart non-paired (dissimilar) instances. Note that\nwe do not distinguish language types in Xand the\nsentences in all possible languages will be drawn\ntowards their corresponding videos in the shared\nmultilingual text-video embedding space.\nIntra-modal NCE. Beyond cross-modality match-\ning, we leverage the intra-modal contrastive ob-\njective to learn and preserve the underlying struc-\nture within the video and text modality. For exam-\nple, Corgi should be closer to Husky than Balinese.\nPrior image-text work (Gella et al., 2017; Huang\net al., 2019c) utilizes a triplet loss to maintain such\nneighborhood relationships. Inspired by recent suc-\ncess in self-supervised image and video represen-\ntation learning (Yalniz et al., 2019; Ghadiyaram\net al., 2019), our model leverages intra-modal NCE\nthat constrains the learned representations to be\ninvariant against noise and to maintain the within-\nmodality structure simultaneously. We minimize\nthe following intra-modal NCE loss:\nLintra = L(X,Xm) + L(V,Vm), (3)\nwhere Xm and Vm are the noised version of the\noriginal sentences and videos. For noising, we\nrandomly mask 5% of the multilingual text tokens\nand video clips. We optimize our model by\nmin\nŒ∏\nLinter + Lintra (4)\n3.3 When Visually-Pivoted Multilingual\nAnnotations Are Available\nIn many multilingual multimodal datasets, there\nare sentences in different languages that describe a\nshared visual context. For example, 10 English and\n10 Chinese descriptions are available for each video\nin V ATEX. With these visually-pivoted (weakly\nparalleled) sentences (x,y), we further revise the\ncontrastive objectives to leverage this additional\nsupervisory signal. Given a visually-pivoted cor-\npus Cp that contains all possible combination of\nvisually-pivoted pairs {(xi,vi,yi)}Cp\ni=0, we sample\nbatches Bp = {(xi,vi,yi)}Bp\ni=1,Bp ‚äÇC p and re-\nvise the contrastive objective as:\nLinter = L(X,V) + L(Y,V) (5)\nLintra = L(X,Xm) + L(Y,Ym) + L(V,Vm)\n(6)\nVisual-pivoted Cross-lingual NCE. Inspired\nby Translation Language Modeling (TLM) in\nXLM (Lample and Conneau, 2019), we propose a\nmultimodal TLM-like contrastive objective which\npromotes alignments of descriptions in different\nlanguages that describe the same video. We use the\nintuition that conditioned on a video, the descrip-\ntions (need not to be translation pairs) in different\nlanguages would likely be semantically similar. To\nthis end, we set the cross-lingual NCE as:\nLcross = L(X|V,Y|V) (7)\nFor visually-pivoted sentences, as shown in\nFig. 1, we generate their representations condi-\ntioned on the video they describe. We extend the\nkey and value of multihead attention with the addi-\ntional visual content ev and generate new cx|v and\ncy|v for matching. SpeciÔ¨Åcally, our model employs\ncx|v = Trans(2)\nx (query=ex,key=value=ex||ev)[0].\nWith the access to (visually-pivoted) multilingual\nannotations, we optimize our model by\nmin\nŒ∏\nLinter + Lintra + Lcross (8)\n2447\nyafries yaKifaransaunawezapia kuandamananayo\ntambi√©nla voyaacompa√±arcon un poco de papas fritasund dann ziehen Sie es so fest wie m√∂glich\nÊòØ‰ªÄ‰πàÔºåÂÆÉÊòØÁÉ≠È£éÊû™ÔºåÊàëËä±‰∫ÜÂçÅÁæéÂÖÉ‰π∞‰∫ÜIt will also be accompanied with a little of frenchfries–∞ –∑–∞—Ç–µ–º –ø–æ—Ç—è–Ω–∏—Ç–µ –µ–≥–æ –∫–∞–∫ –º–æ–∂–Ω–æ –ø–ª–æ—Ç–Ω–µ–µWhat it is, is a heat gun and I got this for ten bucks00:00:37.160 --> 00:00:48.860\nwe just made our six-sided coaster so\nand then pull it as tight as possible\nnous venonsde faire notrecaboteur√†six c√¥t√©sdoncceque \n00:11:36.380 --> 00:11:44.390\n00:08:35.289 --> 00:08:39.300\nvon Pommes Frites k√∂nnenSie es auchmitbegleiten\nkhoait√¢ychi√™nb·∫°nc≈©ngc√≥th·ªÉƒëik√®mv·ªõin√≥\n00:01:16.290 --> 00:01:21.210\nFigure 2: Video clips and the corresponding multilingual subtitles in Multi-HowTo100M.\nAt the inference time, we simply apply cx =\nŒ¶(x) and cv = Œ®(v) to encode multilingual text\nqueries and videos. For text-to-video search, we\nsort videos according to their cosine similarity\nscores to the text query.\n4 The Multilingual HowTo100M Dataset\nAs large-scale pre-training has been shown im-\nportant in recent NLP and vision-language mod-\nels, we construct the Multilingual HowTo100M\ndataset (Multi-HowTo100M) to facilitate research\nin multilingual multimodal learning. The origi-\nnal HowTo100M (Miech et al., 2019) dataset is a\nlarge-scale video collection of 1.2 million instruc-\ntional videos (around 138 million clips/segments)\non YouTube, along with their automatic speech\nrecognition (ASR) transcriptions as the subtitles.\nFor each video in HowTo100M, we crawl and col-\nlect the multilingual subtitles provided by YouTube,\nwhich either consist of user-generated subtitles or\nthose generated by Google ASR and Translate in\nthe absence of user-generated ones. Essentially,\nwe collect video subtitles in 9 languages: English\n(en), German (de), French (fr), Russian (ru), Span-\nish (es), Czech ( cz), Swahili ( sw), Chinese ( zh),\nVietnamese (vi).\nAt the time of dataset collection (May 2020),\nthere are 1.1 million videos available, each with\nsubtitles in 7-9 languages. The video length ranges\nfrom 1 minute to more than 20 minutes. We utilize\nMulti-HowTo100M for multilingual multimodal\npre-training to exploit the weak supervision from\nlarge-scale multilingual text-video data. In Fig. 2,\nwe provide a visualization of few instances sam-\npled in Multi-HowTo100M with the corresponding\nvideo frame, timestamp, and transcriptions in differ-\nent languages. Please refer to Appendix for more\ndetails and dataset statistics.\n5 Experiment\nIn this section, we Ô¨Årst describe our experimental\nsetup (¬ß5.1-5.3). In ¬ß5.4, we conduct ablation stud-\nies to validate the effectiveness of proposed multi-\nlingual text-video model . With the best models at\nhand, we investigate their zero-shot cross-lingual\ntransferability in ¬ß5.5, where we showcase that\nthe proposed multilingual multimodal pre-training\nserves as the key facilitator. We then verify the\nsuperior text‚Üívideo search performance of our\nmethod under the monolingual, multilingual, and\ncross-modality settings in ¬ß5.6.\n5.1 Evaluation Datasets\nMSR-VTT (VTT) (Xu et al., 2016) contains\n10K videos, where each video is annotated with\n20 captions. Additionally, we created pseudo-\nmultilingual data by translating the English cap-\ntions into 8 languages with off-the-shelf machine\ntranslation models.1 We use the ofÔ¨Åcial training set\n(6.5K videos) and validation set (497 videos). We\nfollow the protocol in Miech et al. (2019); Liu et al.\n(2019) which evaluates on text‚Üívideo search with\nthe 1K testing set deÔ¨Åned by Yu et al. (2018).\nV ATEX(Wang et al., 2019) is a multilingual (Chi-\nnese and English) video-text dataset with 35K\nvideos. Five (en,zh) translation pairs and Ô¨Åve non-\npaired en and zh descriptions are available for\neach video. We use the ofÔ¨Åcial training split (26K\nvideos) and follow the testing protocol in Chen\net al. (2020a) to split the validation set equally into\n1.5K validation and 1.5K testing videos.\nMulti30K (Elliott et al., 2016) is a multilingual ex-\ntension of Flickr30K (Young et al., 2014). For each\nimage, there are two types of annotations available:\n(1) One parallel (English,German,French,Czech)\ntranslation pair and (2) Ô¨Åve English and Ô¨Åve Ger-\n1https://marian-nmt.github.io/\n2448\nman descriptions collected independently. The\ntraining, validation, and testing splits contain 29K,\n1K, and 1K images respectively.\n5.2 Implementation Details\nFor the video backbone, we use a 34-layer,\nR(2+1)-D (Tran et al., 2018) network pre-trained\non IG65M (Ghadiyaram et al., 2019) and a\nS3D (Miech et al., 2020) network pre-trained on\nHowTo100M. We pre-extract video features and\nconcatenate the two 3D-CNN outputs to form\nex ‚ààRM√ó1024 as a video input.\nFor the text backbone, we use multilingual BERT\n(mBERT) (Devlin et al., 2019) or XLM-Roberta-\nlarge (XLM-R) (Artetxe et al., 2020), where the\nlatter achieves near SoTA zero-shot cross-lingual\ntransfer performance for NLP tasks. Following Hu\net al. (2020), instead of using the top layer, we\noutput the 12-th layer in XLM-R and mBERT. For\nvision-language tasks, we freeze layers below 9 as\nthis setup empirically performs the best.\nOur model employs a 2-layer Transformer with\n4-head attention for the text and video transformer\npooling (TP) modules. The embedding dimension\nDis set to 1024. We use the Adam (Kingma and\nBa, 2015) optimizer and a 0.0002 learning rate to\ntrain our model for 16 (pre-training) and 10 (Ô¨Åne-\ntuning) epochs. The softmax temperature in all\nnoise contrastive objectives is set to 0.1.\n5.3 Experimental Setup\nWe use Multi-HowTo100M for multilingual mul-\ntimodal pre-training (MMP). For each video, we\nrandomly sample the start and end time to con-\nstruct a video clip. For a video clip, we randomly\nsample one language type each time from 9 lan-\nguages and use the consecutive ASR transcriptions\nthat are closest in time to compose (text-video)\npairs for training. For simplicity and speed pur-\nposes, we follow the training protocol of XLM-\nR to pre-train on a multilingual corpus wihtout\nusing translation pairs, i.e., we use multilingual\ntext-video pairs (x,v) but no translation pairs from\nMulti-HowTo100M and utilize only inter- and intra-\nmodal NCE (Eq. 1-3) for MMP.\nWe Ô¨Åne-tune our model on VTT, V ATEX, and\nMulti30K to evaluate on text‚Üívideo search tasks.\nIn the zero-shot cross-lingual transfer experiments,\nwe use only English-video data and Ô¨Åne-tune with\nEq. 1-3. We then test the model with non-English\nqueries. When annotations in additional languages\nare available (by humans in V ATEX and Multi30K;\nText-B Video-B R@1 ‚ÜëR@5‚ÜëR@10‚Üë\nXLM-R S3D 19.5 49.0 62.8\nXLM-R R(2+1)D 19.0 49.5 63.2\nXLM-R R+S 21.0 50.6 63.6\nmBERT R+S 19.9 49.8 62.5\nTable 1: Text and Video (B)ackbone comparison.\nT layers V layers R@1 ‚ÜëR@5‚ÜëR@10‚Üë\n1 1 20.0 50.3 63.2\n2 1 20.1 50.5 63.8\n2 2 21.0 50.6 63.6\n2‚àó 2‚àó 20.7 50.5 63.3\n4 4 20.8 50.4 63.8\nTable 2: Architecture comparison. Number of multi-\nlingual multimodal transformer layers. *:Weight shar-\ning between video and text transformers.\nObjective Inter Intra Cross R@1 ‚ÜëR@5‚ÜëR@10‚Üë\nTriplet ‚úì 13.3 36.0 55.2\nTriplet ‚úì ‚úì 20.9 49.3 63.0\nNCE ‚úì 21.4 49.3 61.1\nNCE ‚úì ‚úì 21.0 50.6 63.6\nNCE* ‚úì ‚úì 21.3 50.7 63.5\nNCE* ‚úì ‚úì ‚úì 21.5 51.0 63.8\nTable 3: Objective comparison. *Training with addi-\ntional machine translated de-video and fr-video pairs.\nby MT models ( i.e., translate-train) in VTT),\nwe utilize all available multilingual annotations\n(i.e., fully supervised) and iterate over all possible\n(x,v,y ) pairs to train with Eq. 5-7 to demonstrate\nthe strong performance target for evaluating zero-\nshot cross-lingual transfer on VTT and to com-\npare fairly with other fully-supervised baselines\nin multilingual text‚Üívideo search on V ATEX and\nMulti30K. We report the standard recall atk(R@k)\nmetrics (higher is better).\n5.4 Comparison Experiments and Ablations\nIn this section, we ablate and compare different\ntext/video encoders, Transformer model architec-\ntures, and learning objectives for English‚Üívideo\nsearch on VTT.\nText and Video Encoders. Table 1 compares dif-\nferent text and video encoder backbones. For the\nvisual encoders, while R(2+1)D outperforms S3D,\nthe simple concatenation (i.e., early-fusion) of their\noutput features provides a 1.5 ‚àº2.0 improvement\nin R@1. For the text encoder, XLM-R signiÔ¨Åcantly\noutperforms mBERT.\nTransformer Pooling. Table 2 compares various\nconÔ¨Ågurations of the proposed Transformer pool-\ning module. We observe that a simple 2-layer\nTransformer achieves the best performance. Weight\n2449\nModel en de fr cs zh ru vi sw es Avg‚Üë\nmBERT 19.9 11.1 11.6 8.2 6.9 7.9 2.7 1.4 12.0 9.1\nmBERT-MP 20.6 11.3 11.9 8.0 7.1 7.7 2.5 1.1 12.5 9.2\nmBERT-MMP 21.8 15.0 15.8 11.2 8.4 11.0 3.7 3.4 15.1 11.7\nXLM-R 21.0 16.3 17.4 16.0 14.9 15.4 7.7 5.7 17.3 14.7\nXLM-R-MP 23.3 17.4 18.5 17.1 16.3 17.0 8.1 6.2 18.5 15.8\nXLM-R-MMP 23.8 19.4 20.7 19.3 18.2 19.1 8.2 8.4 20.4 17.5\nmBERT + translated VTT 19.6 18.2 18.0 16.9 16.2 16.5 8.4 13.0 18.5 16.1\nmBERT-MMP + translated VTT 21.5 19.1 19.8 18.3 17.3 18.3 8.9 14.1 20.0 17.4\nXLM-R + translated VTT 21.5 19.6 20.1 19.3 18.9 19.1 10.3 12.5 18.9 17.8\nXLM-R-MMP + translated VTT 23.1 21.1 21.8 20.7 20.0 20.5 10.9 14.4 21.9 19.4\nTable 4: Recall@1 of multilingual text‚Üívideo search on VTT. Upper: Zero-shot cross-lingual transfer. Lower:\nPerformance with synthesized pseudo-multilingual annotations for training. MMP: multilingual multimodal pre-\ntraining on Multi-HowTo100M. MP: Multimodal (English-Video) pre-training on HowTo100M.\n19\n20\n21\n22\n23\n24\nNone en en+de en+de+fr All Langs.\nXLM-R mBERT\n(a) English‚ÜíVideo\n10\n12\n14\n16\n18\n20\nNone en en+de en+de+fr All Langs.\nXLM-R mBERT (b) Zero-shot German‚ÜíVideo\nFigure 3: R@1 trends in languages used for multilin-\ngual multimodal pre-training. Left: English ‚Üívideo\nsearch. Right: Zero-shot German‚Üívideo search.\nsharing of the video and text Transformer slightly\ndegrades the performance. Therefore, we choose\nto separate them.\nLearning Objective. From Table 3, the intra-\nmodal contrastive objective is important for both\nNCE and Triplet loss. In general, the NCE loss\noutperforms the Triplet loss. The proposed inter-\nmodal and intra-modal NCE objective achieves the\nbest performance. When captions in multiple lan-\nguages are available, cross-lingual NCE addition-\nally provides a consistent improvement.\n5.5 VTT Zero-Shot Cross-Lingual Transfer\nTable 4 shows the multilingual text‚Üívideo search\nresults on VTT. With the best English-video mod-\nels at hand (with either mBERT or XLM-R as the\ntext backbone), we Ô¨Årst investigate how well these\nmodels transfer to other non-English languages\nunder the zero-shot setting. We then analyze the\nbeneÔ¨Åt of the proposed multilingual multimodal\npre-training.\nThe upper section shows the zero-shot results.\nUnlike cross-lingual transfer in NLP tasks, employ-\ning multilingual Transformers in vision-language\ntasks apparently does not generalize well across\nlanguages. For example, there is a signiÔ¨Åcant\ndrop in R@1 (19.9 ‚Üí11.1 (-44%) with mBERT,\n21.0‚Üí16.3 (-24%) with XLM-R) when directly ap-\nplying English-Ô¨Ånetuned model to German‚Üívideo\nsearch. For comparison, there is only a -10% degra-\ndation for XLM-R on en‚Üídecross-lingual trans-\nfer in XNLI (Conneau et al., 2018). Multimodal\n(English-video) pre-training (MP) on HowTo100M\nonly improves average R@1 (+0.1 or mBERT and\n+1.1 for XLM-R) compared to model-from-scratch.\nIn contrast, our proposed multilingual multimodal\npre-training (MMP) is shown to be the key facilita-\ntor for zero-shot cross-lingual transfer. MMP im-\nproves German‚ÜíVideo search (11.1‚Üí15.0, +35%\nfor mBERT, and 16.3‚Üí19.4, +20% for XLM-R)\nand achieves 2.6 ‚àº2.8 improvement in average\nR@1. We attribute the effectiveness of MMP to\nlearning improved alignments between multilin-\ngual textual and visual context in the shared embed-\nding space, as relatively balanced improvements\nbetween English‚Üívideo and non-English‚Üívideo\nis observed with Ô¨Åne-tuning.\nFig. 3 demonstrates the trend of R@1 while\nincrementally incorporating additional languages\nfor MMP. For XLM-R, the improvement in R@1\nasymptotically converges when pre-training with\nmore multilingual text-video pairs. On the other\nhand, for zero-shot German ‚Üívideo search, pre-\ntraining with more languages keeps improving the\nsearch performance, even though the additional\nlanguage (e.g., French) is different from the target\nlanguage (i.e., German).\nThe lower section of Table 4 shows the results\nof models Ô¨Åne-tuned with (synthesized) pseudo-\nmultilingual annotations. It can be regarded as\nthe translate-train scenario, which serves as a\nstrong performance target for evaluating zero-shot\ncross-lingual transfer, as discussed in (Lample and\nConneau, 2019; Hu et al., 2020). Both mBERT\nand XLM-R yield better performance across non-\n2450\na soccer team walking out on the field1\n2\n3\n1\n2\n3\n(0.69)\n(0.58)\n(0.53)\n(0.71)\n(0.47)\n(0.54)\n‚úÖ\n ‚úÖ—á–µ–ª–æ–≤–µ–∫ –∂–æ–Ω–≥–ª–∏—Ä—É–µ—Ç –ø–∞–ª–∫–∞–º–∏ –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –∑–∞—Å–Ω–µ–∂–µ–Ω–Ω–æ–π –≥–æ—Ä—ãRank\n 1\n2\n3\n(0.52)\n(0.48)\nm·ªôt ng∆∞·ªùi ƒë√†n √¥ng ƒëang n√≥i v·ªÅ d·ª± √°n kh√¥ng gian adam\n(0.44)\n‰∏Ä‰∏™Áî∑‰∫∫Âú®È∫¶ÂÖãÈ£éËØ¥ËØù\n1\n2\n3 (0.45)\n(0.42)\n‚úÖ\n (0.46)\n‚úÖ\nFigure 4: Qualitative multilingual (en, ru, vi, zh) text‚Üívideo search results on VTT.\nEnglish languages with the in-domain translated\npseudo-multilingual annotations. However, for\nEnglish‚Üívideo search, a 0.7 degradation is ob-\nserved compared to the zero-shot setting. It is\nlikely due to the noise in the translated captions.\nNotably, there is still a performance gap between\nzero-shot and translate-train settings for models\nwith mBERT. In contrast, the gap is much smaller\nfor models with XLM-R. In the following sections,\nwe refer Ours-MMP as our best model with XLM-\nR as the text backbone and compare it with other\nstate-of-the-art methods.\nQualitative Results Fig. 4 shows the multilin-\ngual text‚Üívideo search results with Ours-MMP\n(VTT:en-only) on VTT under the zero-shot setup.\nNote that only one shared English-Ô¨Ånetuned model\nis used for text‚Üívideo search in all languages. As\ndemonstrated, the proposed model successfully re-\ntrieves the correct videos with English (en) and Rus-\nsian (ru) queries. The other top-ranked videos also\nshare similar visual appearance to the correct one.\nFor zero-shot transferring of the English-Ô¨Ånetuned\nmodel to distant languages such as Vietnamese\n(vi) and Chinese (zh), we observe that there is still\nlimitation for our zero-shot models to understand\nabstract concepts (e.g., ‚Äúspace project‚Äù) and asso-\nciate small objects (e.g., ‚Äúmicrophone‚Äù) with the\ntext queries in distant languages.\n5.6 Comparison to Supervised State of the\nArt\nEnglish‚ÜíVideo Search on VTT. Table 5 shows\nthe comparison of English‚Üívideo models on VTT.\nFor a fair comparison to other baselines, our model\nÔ¨Åne-tunes only with the original English annota-\ntions on VTT. The results show that our model out-\nperforms other baselines by a large margin. Specif-\nically, our model achieves 8.9 R@1 improvement\nover the original HowTo100M model (Miech et al.,\n2019) and other recent baselines with pre-training\non HowTo100M. Using a smaller set of visual fea-\nModel R@1 ‚ÜëR@5‚ÜëR@10‚Üë\nJSFusion (Yu et al., 2018) 10.2 31 .2 43 .2\nJPoSE (Wray et al., 2019) 14.3 38 .1 53 .0\nVidTrans‚Ä†(Korbar et al., 2020) 14.7 ‚àí 52.8\nHT100M‚Ä†(Miech et al., 2019) 14.9 40 .2 52 .8\nNoise‚Ä†(Amrani et al., 2020) 17.4 41 .6 53 .6\nCE2 (Liu et al., 2019) 20.9 48 .8 62 .4\nOurs(VTT:en-only) 21.0 50.6 63.6\nOurs-MMP (VTT:en-only) 23.8 52.6 65.0\nTable 5: English ‚Üívideo search performance on VTT.\n‚Ä†: Models with pre-training on HowTo100M.\nEnglish to Video Chinese to Video\nModel R@1 ‚ÜëR@5‚ÜëR10‚ÜëR@1‚ÜëR@5‚ÜëR@10‚Üë\nVSE (Kiros et al., 2014) 28.0 64.3 76.9 - - -\nVSE++ (Faghri et al., 2018) 33.7 70.1 81.0 - - -\nDual (Dong et al., 2019) 31.1 67.4 78.9 - - -\nHGR (Chen et al., 2020a) 35.1 73.5 83.5 - - -\nOurs (V ATEX:en-only) 43.5 79.8 88.1 23.9 55.1 67.8\nOurs-MMP (V ATEX:en-only) 44.4 80.5 88.7 29.7 63.2 75.5\nOurs-MMP (V ATEX:en, zh) 44.3 80.7 88.9 40.5 76.4 85.9\nTable 6: Multilingual text‚Üívideo search on V ATEX.\ntures and training on a smaller (6,513 vs 9,000)\ntraining set2, our model also outperforms CE (Liu\net al., 2019) with or without pre-training.\nMultilingual Text ‚ÜíVideo Search on V A-\nTEX. Table 6 summarizes English ‚Üívideo and\nChinese‚Üívideo search performance on the\nV ATEX dataset. Under the zero-shot setting where\nwe train with only English-video pairs, our model\nalready outperforms other baselines. However, a\nclear performance gap between English ‚Üívideo\nand Chinese‚Üívideo search is observed, indicating\nthat cross-lingual transfer to a distant language\nremains challenging even with XLM-R. With the\nproposed MMP, the gap is signiÔ¨Åcantly closed\nby 5.8/8.1/7.7 in R@1/5/10. When in-domain\nhuman-annotated Chinese captions are available,\nthe performance of our model can further be\nimproved for both languages and our model yields\nnew state-of-the-art performance.\n2CE uses 9,000 videos (VTT training and part of exclusive\ntesting set) for training, while other baselines and our model\nin Table 5 are trained on the ofÔ¨Åcial VTT training set which\ncontains 6,513 videos.\n2451\nM30K English to Image German to Image Czech to Image\nModel # lang. R@1 ‚Üë R@5‚Üë R10‚Üë R@1‚Üë R@5‚Üë R@10‚Üë R@1‚Üë R@5‚Üë R@10‚Üë\nOE (Vendrov et al., 2015) 2 25.8 56.5 67.8 21.0 48.5 60.4 - - -\nVSE++ (Faghri et al., 2018) 2 39.6 69.1 79.8 31.3 62.2 70.9 - - -\nPivot (Gella et al., 2017) 2 26.2 56.4 68.4 22.5 49.3 61.7 - - -\nFB-NMT (Huang et al., 2020a) 2 47.3 75.4 83.5 37.0 64.0 73.1 - - -\nMULE (Kim et al., 2020) 4 42.2 72.2 81.8 35.1 64.6 75.3 37.5 64.6 74.8\nSMALR (Burns et al., 2020) 10 41.8 72.4 82.1 36.9 65.4 75.4 36.7 68.0 78.2\nMHA-D (Huang et al., 2019b) 2 50.1 78.1 85.7 40.3 70.1 79.0 - - -\nOurs (M30K:en-only) 1 48.4 78.3 85.9 31.4 61.1 72.6 33.2 65.2 76.1\nOurs-MMP (M30K:en-only) 1 50.0 79.2 86.8 33.8 63.3 74.7 37.9 68.8 78.2\nOurs-MMP (M30K:en, de, cs, fr) 4 51.6 80.1 87.3 45.1 75.6 85.0 46.6 75.9 83.4\nTable 7: Multilingual text‚Üíimage search on Multi30K. MMP: Multilingual multimodal pre-training.\nCross-Modality Transfer to Multi30K: From\nVideo-Text to Image-Text. To extend our study\non zero-shot cross-lingual transfer for image-text\ntasks, we investigate the feasibility of transferring\nour video-text model across modalities. We replace\nthe 3D-CNN in the original video-text model with\na 2D-CNN to encode the image. In practice, fol-\nlowing MHA-D (Huang et al., 2019b), we utilize\nthe Faster-RCNN (Ren et al., 2015) pre-trained in\nVisual Genome (Krishna et al., 2016) to extract\nregional visual features. Essentially, an image is\nencoded as ev = RM√óH where M = 36 is the\nmaximum number of visual objects in an image.\nFor models with MMP, we initialize their weights\nwith the model pre-trained on Multi-HowTo100M.\nTo tackle the feature mismatch between 2D-CNN\nand 3D-CNN, we leverage a linear layer with a\ndoubled learning rate to map 2D-CNN features to\nthe same dimension as 3D-CNN features.\nTable 7 shows the results on Multi30K. For\nzero-shot cross-lingual transfer, when trained\nfrom scratch (M30K:en-only), our model achieves\ncomparable performance to MHA-D but lags in\nGerman‚Üíimage search since it only uses En-\nglish annotations. In Ours-MMP, pre-training\nimproves all recall metrics even with modality\ngap. The average R@1 improvement is 3.2.\nA larger gain for (relatively) low-resource lan-\nguage such as Czech is observed. Without us-\ning any Czech annotations, our zero-shot model\nwith MMP achieves comparable Czech ‚Üíimage\nsearch performance to SMALR (Burns et al.,\n2020), which uses 10 languages including Czech.\nHowever, when transferring across modalities\nand using only English annotations, there are\nperformance gaps between English ‚ÜíImage and\nGerman/Czech‚ÜíImage search, implying that trans-\nferring models across modalities is feasible but\nremains challenging. We consider zero-shot cross-\nmodal cross-lingual transfer as our future work.\nFor a fair comparison with other baselines, when\ntrained with annotations in all 4 languages pro-\nvided by Multi30K, our model greatly outper-\nforms all baselines by large margins in multilingual\ntext‚Üíimage search.\n6 Conclusion\nWe have presented a multilingual multimodal pre-\ntraining (MMP) strategy, the Multi-HowTo100M\ndataset, and a Transformer-based text-video model\nfor learning contextual multilingual multimodal\nrepresentations. The results in this paper have\nconvincingly demonstrated that MMP is an essen-\ntial ingredient for zero-shot cross-lingual transfer\nof vision-language models. Meanwhile, there are\nmany remaining challenges, such as resolving the\nperformance gap between zero-shot and training\nwith in-domain non-English annotations; as well as\ntechniques to transfer varieties of vision-language\nmodels (e.g., VQA (Goyal et al., 2017), TVQA (Lei\net al., 2020)) or visually-enhanced NLP models\nsuch as unsupervised multimodal machine transla-\ntion (Huang et al., 2020b). We believe the proposed\nmethodology, and the corresponding resources we\nrelease, will be an important Ô¨Årst step towards\nspurring more research in this direction.\nAcknowledgments\nThis work is supported by the DARPA grants\nfunded under the AIDA program (FA8750-\n18-2-0018) and the GAILA program (award\nHR00111990063) (P.Y .). This work is also sup-\nported by EPSRC Centre for Doctoral Training\nin Autonomous Intelligent Machines & Systems\n[EP/L015897/1] (M.P.). The authors appreciate\nPrahal Arora, Shengxin Zha, Polina Kuznetsova,\nXu Hu, and Geoffrey Zweig for their suggestions\nof this work. The authors would also like to thank\nthe anonymous reviewers for their feedback.\n2452\nReferences\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant\nAgrawal, Ivan Laptev, Josef Sivic, and Simon\nLacoste-Julien. 2016. Unsupervised learning from\nnarrated instruction videos. In Computer Vision and\nPattern Recognition (CVPR).\nElad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex\nBronstein. 2020. Noise estimation using density\nestimation for self-supervised multimodal learning.\narXiv preprint arXiv:2003.03186.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 451‚Äì462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623‚Äì4637, Online. Asso-\nciation for Computational Linguistics.\nJeremy Barnes, Lilja √òvrelid, and Erik Velldal. 2019.\nSentiment analysis is not solved! assessing and prob-\ning sentiment classiÔ¨Åcation. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 12‚Äì23,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAndrea Burns, Donghyun Kim, Derry Wijaya, Kate\nSaenko, and Bryan A. Plummer. 2020. Learn-\ning to scale multilingual representations for vision-\nlanguage tasks. In The European Conference on\nComputer Vision (ECCV).\nDavid Chen and William Dolan. 2011. Collecting\nhighly parallel data for paraphrase evaluation. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 190‚Äì200, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nShizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. 2020a.\nFine-grained video-text retrieval with hierarchical\ngraph reasoning. In CVPR.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020b. A simple framework\nfor contrastive learning of visual representations. In\nICML.\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,\nand Kilian Weinberger. 2018. Adversarial deep av-\neraging networks for cross-lingual sentiment classi-\nÔ¨Åcation. Transactions of the Association for Compu-\ntational Linguistics, 6:557‚Äì570.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nRyan Cotterell and Georg Heigold. 2017. Cross-\nlingual character-level neural morphological tag-\nging. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 748‚Äì759, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji,\nYuan He, Gang Yang, and Xun Wang. 2019. Dual\nencoding for zero-example video retrieval. In\nCVPR.\nDesmond Elliott, Stella Frank, Khalil Sima‚Äôan, and Lu-\ncia Specia. 2016. Multi30k: Multilingual english-\ngerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70‚Äì74.\nAssociation for Computational Linguistics.\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017. Learning to parse and translate improves\nneural machine translation. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n72‚Äì78, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nFartash Faghri, David J Fleet, Jamie Ryan Kiros, and\nSanja Fidler. 2018. Vse++: Improving visual-\nsemantic embeddings with hard negatives. In\nBMVC.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation. In Proceedings of the 14th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 462‚Äì471, Gothenburg,\nSweden. Association for Computational Linguistics.\nSpandana Gella, Rico Sennrich, Frank Keller, and\nMirella Lapata. 2017. Image pivoting for learning\nmultilingual multimodal representations. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2839‚Äì\n2845. Association for Computational Linguistics.\nDeepti Ghadiyaram, Du Tran, and Dhruv Mahajan.\n2019. Large-scale weakly-supervised pre-training\nfor video action recognition. In CVPR.\n2453\nStephan Gouws and Anders S√∏gaard. 2015. Simple\ntask-speciÔ¨Åc bilingual word embeddings. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1386‚Äì1390, Denver, Colorado. Association for Com-\nputational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image under-\nstanding in Visual Question Answering. In Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nMichael Gutmann and Aapo Hyv ¬®arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on ArtiÔ¨Å-\ncial Intelligence and Statistics, pages 297‚Äì304.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328‚Äì339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR, abs/2003.11080.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019a.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. arXiv\npreprint arXiv:1909.00964.\nPo-Yao Huang, Xiaojun Chang, and Alexander Haupt-\nmann. 2019b. Multi-head attention with diversity\nfor learning grounded multilingual multimodal rep-\nresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1461‚Äì1467, Hong Kong, China. As-\nsociation for Computational Linguistics.\nPo-Yao Huang, Xiaojun Chang, Alexander Hauptmann,\nand Eduard Hovy. 2020a. Forward and backward\nmultimodal nmt for improved monolingual and mul-\ntilingual cross-modal retrieval. In Proceedings of\nthe 2020 International Conference on Multimedia\nRetrieval, ICMR ‚Äô20, page 53‚Äì62, New York, NY ,\nUSA. Association for Computing Machinery.\nPo-Yao Huang, Junjie Hu, Xiaojun Chang, and Alexan-\nder Hauptmann. 2020b. Unsupervised multimodal\nneural machine translation with pseudo visual piv-\noting. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8226‚Äì8237, Online. Association for Computa-\ntional Linguistics.\nPo-Yao Huang, Guoliang Kang, Wenhe Liu, Xiaojun\nChang, and Alexander G. Hauptmann. 2019c. Anno-\ntation efÔ¨Åcient cross-modal retrieval with adversar-\nial attentive alignment. In Proceedings of the 27th\nACM International Conference on Multimedia, MM\n‚Äô19, page 1758‚Äì1767, New York, NY , USA. Associ-\nation for Computing Machinery.\nPo-Yao Huang, Vaibhav, Xiaojun Chang, and Alexan-\nder G. Hauptmann. 2019d. Improving what cross-\nmodal retrieval models learn through object-oriented\ninter- and intra-modal attention networks. In Pro-\nceedings of the 2019 on International Conference\non Multimedia Retrieval, ICMR ‚Äô19, page 244‚Äì252,\nNew York, NY , USA. Association for Computing\nMachinery.\nVladimir Iashin and Esa Rahtu. 2020. Multi-modal\ndense video captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition Workshops, pages 958‚Äì959.\nDonghyun Kim, Kuniaki Saito, Kate Saenko, Stan\nSclaroff, and Bryan A. Plummer. 2020. MULE:\nMultimodal Universal Language Embedding. In\nAAAI Conference on ArtiÔ¨Åcial Intelligence.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nRyan Kiros, Ruslan Salakhutdinov, and Richard S.\nZemel. 2014. Unifying visual-semantic embeddings\nwith multimodal neural language models. CoRR,\nabs/1411.2539.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai.\n2012. Inducing crosslingual distributed representa-\ntions of words. In Proceedings of COLING 2012 ,\npages 1459‚Äì1474.\nBruno Korbar, F. Petroni, Rohit Girdhar, and L. Torre-\nsani. 2020. Video understanding as machine transla-\ntion. ArXiv, abs/2006.07203.\nR. Krishna, Yuke Zhu, O. Groth, J. Johnson, K. Hata,\nJ. Kravitz, Stephanie Chen, Yannis Kalantidis, L. Li,\nD. Shamma, Michael S. Bernstein, and Li Fei-Fei.\n2016. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision , 123:32‚Äì\n73.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alch¬¥e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 7059‚Äì\n7069. Curran Associates, Inc.\nJie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.\n2020. TVQA+: Spatio-temporal grounding for\nvideo question answering. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 8211‚Äì8225, Online. As-\nsociation for Computational Linguistics.\n2454\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315‚Äì\n7330, Online. Association for Computational Lin-\nguistics.\nYang Liu, Samuel Albanie, Arsha Nagrani, and An-\ndrew Zisserman. 2019. Use what you have: Video\nretrieval using representations from collaborative ex-\nperts. In BMVC.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, 8-\n14 December 2019, Vancouver, BC, Canada , pages\n13‚Äì23.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412‚Äì1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nJonathan Malmaud, Jonathan Huang, Vivek Rathod,\nNicholas Johnston, Andrew Rabinovich, and Kevin\nMurphy. 2015. What‚Äôs cookin‚Äô? interpreting cook-\ning videos using text, speech and vision. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n143‚Äì152, Denver, Colorado. Association for Compu-\ntational Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 30, pages 6294‚Äì\n6305. Curran Associates, Inc.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira,\nIvan Laptev, Josef Sivic, and Andrew Zisserman.\n2020. End-to-End Learning of Visual Represen-\ntations from Uncurated Instructional Videos. In\nCVPR.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\n2019. Howto100M: Learning a text-video embed-\nding by watching hundred million narrated video\nclips. In ICCV.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. EfÔ¨Åcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTakashi Miyazaki and Nobuyuki Shimizu. 2016. Cross-\nlingual image caption generation. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1780‚Äì1790. Association for Computational\nLinguistics.\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n875‚Äì880, Brussels, Belgium. Association for Com-\nputational Linguistics.\nA¬®aron van den Oord, Y . Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. ArXiv, abs/1807.03748.\nMandela Patrick, Y . Asano, Ruth Fong, Jo Àúao F. Hen-\nriques, G. Zweig, and A. Vedaldi. 2020. Multi-\nmodal self-supervision from generalized data trans-\nformations. ArXiv, abs/2003.04298.\nMandela Patrick, Po-Yao Huang, Yuki M. Asano, Flo-\nrian Metze, Alexander Hauptmann, Jo Àúao Henriques,\nand Andrea Vedaldi. 2021. Support-set bottlenecks\nfor video-text representation learning. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227‚Äì2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ¬¥c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654‚Äì7673, Online. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996‚Äì\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151‚Äì164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. In\n2455\nAdvances in neural information processing systems ,\npages 91‚Äì99.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to zero-\nshot dependency parsing. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1599‚Äì1613, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classiÔ¨Åcation in eight languages.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Paris, France. European Language Resources\nAssociation (ELRA).\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan\nFirat, Mia Chen, Sneha Kudugunta, Naveen Ari-\nvazhagan, and Yonghui Wu. 2020. Leveraging\nmonolingual data with self-supervision for multi-\nlingual neural machine translation. arXiv preprint\narXiv:2005.04816.\nGunnar A. Sigurdsson, Jean-Baptiste Alayrac, Aida\nNematzadeh, Lucas Smaira, Mateusz Malinowski,\nJoÀúao Carreira, Phil Blunsom, and Andrew Zisser-\nman. 2020. Visual grounding in video for unsuper-\nvised word translation. In CVPR.\nD¬¥ƒ±dac Sur ¬¥ƒ±s, Dave Epstein, and Carl V ondrick.\n2020. Globetrotter: Unsupervised multilingual\ntranslation from visual alignment. arXiv preprint\narXiv:2012.04631.\nOscar T¬®ackstr¬®om, Ryan McDonald, and Jakob Uszko-\nreit. 2012. Cross-lingual word clusters for direct\ntransfer of linguistic structure. In Proceedings of\nthe 2012 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies , pages 477‚Äì\n487, Montr ¬¥eal, Canada. Association for Computa-\ntional Linguistics.\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray,\nYann LeCun, and Manohar Paluri. 2018. A closer\nlook at spatiotemporal convolutions for action recog-\nnition. In CVPR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Ur-\ntasun. 2015. Order-embeddings of images and lan-\nguage. arXiv preprint arXiv:1511.06361.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-\nFang Wang, and William Yang Wang. 2019. Vatex:\nA large-scale, high-quality multilingual dataset for\nvideo-and-language research. In Proceedings of the\nIEEE International Conference on Computer Vision,\npages 4581‚Äì4591.\nMichael Wray, Diane Larlus, Gabriela Csurka, and\nDima Damen. 2019. Fine-grained action retrieval\nthrough multiple parts-of-speech embeddings. In\nICCV.\nShijie Wu and Mark Dredze. 2019a. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nbert. arXiv preprint arXiv:1904.09077.\nShijie Wu and Mark Dredze. 2019b. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833‚Äì844, Hong Kong, China. Association for Com-\nputational Linguistics.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSR-\nVTT: A large video description dataset for bridging\nvideo and language. In CVPR.\nI. Zeki Yalniz, Herv ¬¥e J ¬¥egou, Kan Chen, Manohar\nPaluri, and Dhruv Mahajan. 2019. Billion-scale\nsemi-supervised learning for image classiÔ¨Åcation.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67‚Äì78.\nShoou-I Yu, Lu Jiang, and Alexander Hauptmann.\n2014. Instructional videos for unsupervised harvest-\ning and learning of action examples. In Proceed-\nings of the 22nd ACM International Conference on\nMultimedia, MM ‚Äô14, page 825‚Äì828, New York, NY ,\nUSA. Association for Computing Machinery.\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. 2018.\nA joint sequence fusion model for video question an-\nswering and retrieval. In ECCV.\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\n2020. Neural machine translation with universal vi-\nsual representation. In International Conference on\nLearning Representations.\n2456\nA Appendix Overview\nThe Appendix is organized as follows: First we pro-\nvide details about the Multilingual HowTo100M\n(Multi-HowTo100M) dataset for multilingual multi-\nmodal pre-training (MMP) in ¬ßB. Then we provide\nadditional implementation details and experiment\nsetup in ¬ßC. Additional ablation studies regarding\nchoices of Transformer architecture are discussed\nin ¬ßD. Then we present additional cross-dataset\ntransfer experiments in ¬ßE.\nB The Multilingual HowTo100M Dataset\nIn this section we provide the detailed statis-\ntics of the Multilingual HowTo100M (Multi-\nHowTo100M) dataset. We also provide a com-\nparison to Sigurdsson et al. (2020) that also uses\nHowTo100M for unsupervised word translation.\nThe Multi-HowTo100M dataset is built upon\nthe original English HowTo100M dataset (Miech\net al., 2019) that contains 1.2 million instructional\nvideos (138 million clips) on YouTube. We reuse\nthe raw English subtitles in HowTo100M, where\nthe subtitles in HowTo100M are either automatic\nspeech recognition (ASR) transcriptions or user\ngenerated subtitles.\nFor Multi-HowTo100M, we use the same video\ncollection as English HowTo100M. At the time of\ndata collection (May 2020), there were 1.09 million\nvideos accessible. We collect the subtitles provided\nby YouTube, which either consist of user-generated\nsubtitles or those generated by Google ASR and\nTranslate in the absence of user-generated ones. Es-\nsentially, we collect video subtitles in 9 languages:\nEnglish (en), German ( de), French ( fr), Russian\n(ru), Spanish (es), Czech (cz), Swahili (sw), Chi-\nnese (zh), Vietnamese (vi). Table 8 summarizes the\ndataset statistics for each language. In most cases\nthere are more than 1 billion tokens a language.\nFig. 5 further shows the number of tokens per\nvideo. There are typically lengthy narrations that\ncontains several hundreds of tokens available in\neach instructional video. Fig. 6 shows the distri-\nbution of number of tokens in a subtitle. For each\nsubtitle segment, which ranges from 0‚àº20 seconds,\nthere are typically 15‚àº25 words. The most of the\ncases, subtitles are well aligned in time for non-\nEnglish languages. Fig. 2 visualizes a few exam-\nples in Multi-HowTo100M.\nA similar HowTo100M variant has been re-\ncently reported in MUVE (Sigurdsson et al., 2020)\nthat is created for unsupervised word translation.\nLanguage videos #subtitle #tokens\nEnglish 1238911 138429877 1.18B\nGerman 1092947 69317890 1.26B\nFrench 1093070 69399097 1.33B\nCzech 1092717 68911940 1.22B\nRussian 1092802 69117193 1.25B\nChinese 1092915 68939488 0.94B\nSwahili 1092302 68898800 1.22B\nVietnamese 1092603 68887868 1.13B\nSpanish 1092649 70143503 1.16B\nTable 8: Multi-HowTo100M statistics\nOur Multi-HowTo100M differs from MUVE in\nthe following perspectives: First, we collects 9\nlanguage for all videos in HowTo100M while\nMUVE only has 4 languages available (English,\nFrench, Japanese, and Korean) on HowTo100M.\nAlso, MUVE divided HowTo100M into 4 non-\noverlapped sections for each language, there are\nno parallel pairs for each subtitle. While in Multi-\nHowTo100M, there are 7-9 languages for each sub-\ntitle. Essentially, There are more than 1 billion\ntokens in most languages in Multi-HowTo100M.\nTo our best knowledge, our Multi-HowTo100M\ndataset is currently the largest multilingual text-\nvideo collection.\nBeyond scale, instructional videos in Multi-\nHowTo100M are feasible pre-training resources\nfor many downstream vision-language models.\nDemonstrators in instructional videos typically per-\nform intentionally and explain the visual object\nor action explicitly. According to the inspection\nby (Miech et al., 2019), for around 51% of clips, at\nleast one object or action mention in the caption can\nbe visually seen. Prior work has shown that instruc-\ntional videos are useful for event recognition (Yu\net al., 2014), action localization model (Alayrac\net al., 2016), cross-modal alignments (Malmaud\net al., 2015). We expect the previous success in the\nintersection of natural language processing (NLP)\nand computer vision (CV) could be further trans-\nlated into more languages to have a broaden impact.\nThe are great potentials of using our Multi-\nHowTo100M dataset in related research Ô¨Åeld such\nas multilingual multimodal representation learn-\ning (Huang et al., 2019b; Kim et al., 2020; Burns\net al., 2020), multilingual multimodal transla-\ntion (Huang et al., 2020b; Sur¬¥ƒ±s et al., 2020), mul-\ntilingual image/video captioning (Miyazaki and\nShimizu, 2016) ... etc. We expect the release of\n2457\n0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950\n0\n10000\n20000\n30000\n40000\n50000\n60000\nDistribution of #tokens in\nde\nfr\ncs\nru\nzh\nvi\nsw\nes\nFigure 5: Distribution of #tokens/video in Multi-\nHowTo100M\n0 5 10 15 20 25 30 35\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n1e7 Distribution of #token/subtitle in Mutli-HowTo100M\nde\nfr\ncs\nru\nzh\nvi\nsw\nes\nFigure 6: Distribution of #tokens/subtitle in Multi-\nHowTo100M\nMulti-HowTo100M will be a Ô¨Årst step towards\nspurring more research in these directions.\nC Implementation and Experiment\nDetails\nPre-Processing. For pre-possessing, we truncate\nthe maximum length N of text to 192 for pre-\ntraining on Multi-HowTo100M. The maximum\nlength is set to 96 for Ô¨Åne-tuning VTT (Xu\net al., 2016), V ATEX (Wang et al., 2019) and\nMulti30K (Elliott et al., 2016). The maximum\nvideo length M is set to 128 for pre-training on\nMulti-HowTo100M and 36 for all Ô¨Åne-tuning tasks.\nModel Architecture. For the multilingual Trans-\nformers, either multilingual BERT (Devlin et al.,\n2019) or XLM-R-large (Artetxe et al., 2020), we\nuse the pre-trained version provided by Hugging-\nFace. 3 and use their corresponding tokenizers for\ntokenization. Detailed design choices regarding\noutput layer and frozen layer is discussed in ¬ßD.\nFor the video backbone, we use a 34-layer,\nR(2+1)-D (Tran et al., 2018) network pre-trained\non IG65M (Ghadiyaram et al., 2019) and a\nS3D (Miech et al., 2020) network pre-trained on\nHowTo100M (Miech et al., 2019). We apply a\nspatial-temporal average pooling over the last con-\nvolutional layer, resulting in a 512-dimensional\nvector for each 3D CNN network. We extract vi-\nsual features at a rate of 1 feature per second. Since\nthe 3D CNNs employs different size of input win-\ndows (e.g., 8 frames for R(2+1)D and 16 for S3D),\n3https://github.com/huggingface/transformers\nwe re-sample videos to 30 fps and employs a win-\ndow of size 8 or 30 that takes consecutive frames\nstarting from the beginning of every second for en-\ncoding. We simply concatenate the two 3D-CNN\noutputs and use the 1024-dimension vector as the\nvisual input stream to our model. Notably, instead\nof using 9 different types of visual features as in CE\n(Liu et al., 2019), we use only the above 2 features\nand achieve superior performance.\nFor the Transformer pooling head (TP) modules,\nwe use a 2-layer Transformer with 4-head attention\nfor each TP. The embedding dimension D is set\nto 1024. We do not use the positional embeddings\nin both text and video TP as we do not Ô¨Ånd them\nbeneÔ¨Åcial in our experiments. The softmax temper-\nature in all NCE contrastive objectives is set to 0.1\nas used in SimCLR (Chen et al., 2020b).\nNote that unlike ViLBERT (Lu et al., 2019)\nor OAN (Huang et al., 2019d), our models does\nnot employ cross-modality attention and keep the\nmulti-head self-attention within the same modality.\nThe main reason is to reduce the inference time\ncomplexity. For cross-modality attention, the com-\nplexity is O(TV ) to encode T text queries for V\nvideos in a dataset before retrieval (since video and\nquery representations depend on each other). It\nis clearly not scalable when the dataset contains\nmillions of videos. To this end, our model keep\nself-attention within the same modality which re-\nsults in a O(T + V) complexity compared O(TV )\nin prior work with cross-modality attention. In our\npreliminary experiments, we also incorporate cross-\nmodality attention and achieved 0.3‚àº1.8 R@1 im-\nprovement. Considering the trade-off between per-\nformance and scalability, we choose the latter.\nTraining and Inference Details and ProÔ¨Åling.\nFor the softmax temperature in NCE, we set to\n0.1 as used in SimCLR (Chen et al., 2020b). We\nuse the Adam (Kingma and Ba, 2015) optimizer\nwith a initial learning rate 2 ¬∑10‚àí4 and clip gra-\ndients greater than 0.2 during the training phase.\nDropout rate is 0.3. Since the video length and\ntoken length is longer in the pre-training phase, we\nuse a 64 batch size for pre-training. For Ô¨Åne-tuning,\nwe use a batch size of 128.\nPre-training on the 1.2 million HowTo100M\nvideos takes around 10 GPU hours (NVIDA V100)\nfor 16 epochs. We speed up the pre-training pro-\ncess by distributing the workload over 8 GPUs on\na single node of our server. We use 1 GPU for the\nÔ¨Åne-tuning or training from scratch experiments.\n2458\nFor the MSR-VTT split, it takes 12 GPU hours\nto train our model on 180K video-text pairs for\n20 epochs. For V ATEX, it takes 32 GPU hours\nto train on 260K video-text pairs for 30 epochs.\nFor inference, the encoding speed is around 250-\n300 videos/sec and 200-250 text queries/sec. The\noverall text‚Üívideo search speed on 1,000 video-\ntext pairs (1,000 text queries over 1,000 videos)\nis around 6 seconds including video/text encoding\nand ranking their similarity scores.\nExperiment Details. Our experiment consider\nthree types of pre-training: (1) Multilingual multi-\nmodal pre-training (MMP), (2) Multimodal pre-\ntraining (MP), and (3) no pre-training (from\nscratch). For (1) and (2), we pre-train 16 epochs\nand use the model weight at 16-th epoch for Ô¨Åne-\ntuning experiments.\nFor multimodal pre-training, we pre-train on the\noriginal English HowTo100M dataset. We iterate\nover all videos in HowTo100M. For each video, we\nrandomly sample the start and end time to construct\na video clip. For each clip, we locate the nearest\nconsecutive ASR transcriptions in time and use it\nas to construct the (video, text) pair for training.\nFor multilingual multimodal pre-training\n(MMP), we use Multi-HowTo100M for pre-\ntraining. For each video, we follow the same\nstrategy as MP. For a clip, we sample one language\ntype each time from 9 languages and use the\nconsecutive ASR transcriptions that are closest in\ntime to compose (video, text) pairs for training.\nAfter pre-training, we Ô¨Åne-tune our model on\nVTT and V ATEX to evaluate on text‚Üívideo search\ntasks. In the zero-shot cross-lingual transfer exper-\niments, we use only English-video data. We then\ndirectly test the model with non-English queries\nto report the zero-shot performance. When anno-\ntations in additional languages are available (by\nhumans in V ATEX and Multi30K; by MT models\n(i.e. translate-train) in VTT), we train our model\nwith all available multilingual annotations (i.e. fully\nsupervised) to compare fairly with other baselines\nin multilingual text‚Üívideo search.\nSince pre-trained model has a faster convergence\nrate, we Ô¨Åne-tune for 10 epochs and use the model\nwith best validation performance (summation of\nR@1, R@5, R@10) for testing. For models with-\nout pre-training (i.e., from-scratch), we train for 20\nepochs under the same training protocol.\nOutput layer Freeze lower en de\n3 0 20.9 3.2\n6 0 20.5 3.1\n9 0 21.0 4.8\n12 0 21.0 13.3\n15 0 20.5 12.3\n18 0 20.8 12.6\n12 6 21.0 15.5\n12 9 21.0 16.3\n12 12 18.9 14.1\nTable 9: Text ‚Üívideo R@1 of XLM-R output layers\nand layers to freeze on VTT\nOutput layer Freeze lower en de\n3 0 19.2 2.5\n6 0 19.5 2.0\n9 0 19.3 5.8\n12 0 19.6 8.8\n12 6 19.3 10.5\n12 9 19.9 11.1\n12 12 18.9 9.8\nTable 10: Text‚Üívideo R@1 of mBERT output layers\nand layers to freeze on VTT\nD Additional Ablation Studies\nAs has been investigated in XTREME (Hu et al.,\n2020), choosing different output layers will affect\nthe zero-shot transferability of multilingual Trans-\nformers in various NLP tasks. For text ‚Üívideo\nsearch tasks, we conduct a series of experiments to\nidentify the desirable choices of hyper-parameters\nin the proposed multilingual multimodal Trans-\nformer that lead to best performance in English-to-\nvideo and (zero-shot) non-English-to-video search\nperformance. Beyond our ablation studies in Sec.\n5, in this part we highlight our trials in the choice\nof the output layer and the layers to be frozen in our\nmultilingual Transformer backbone (i.e., mBERT\nand XLM-R). There are 24 layers in XLM-R (large)\nand 12 layers in mBERT. We perform grid-search\non VTT to identify the best choice of these two\nhyper-parameters.\nChoice of Output Layers Table 9 and Table 10\ncompare different choices of output layer and lay-\ners to freeze in multilingual Transformers. Our re-\nsults suggest that the best output layer for mBERT\nand XLM-R is the 12-th layer. Surprisingly, while\noutput layer does not affect English‚Üívideo search\nsigniÔ¨Åcantly, it greatly affects the zero-shot cross-\nlingual transfer performance of video-text models.\nFor both XLM-R and mBERT, the performance\ndegrade signiÔ¨Åcantly if Ô¨Åne-tuning all layers.\n2459\ntext‚Üívideo English Non-English\nIn-domain ‚úì ‚úì\nOut-of-domain ‚úì\nTable 11: Coverage of our experiments\nChoice of Layers to Freeze Similar to output\nlayers, the choice of frozen layers greatly affects\ncross-lingual transferability. For both mBERT and\nXLM-R, it is desirable to freeze part of the lower\nlayers and make the top-3 layers trainable for video-\ntext models. We observe that when freezing all\nlayers (i.e., using the pre-extracted contextual mul-\ntilingual embeddings) does not lead to satisfactory\nresults. For mBERT, R@1 drops from 19.9 to\n18.9 in English‚Üívideo search and 11.1 to 9.8 in\nGerman‚Üívideo search. For XLM-R, R@1 drops\nfrom 21.0 to 18.9 in English‚Üívideo search and\n16.3 to 14.1 in German‚Üívideo search. These re-\nsults imply that text-only contextual multilingual\nembeddings along are likely to be infeasible to be\napplied to vision-language tasks without proper\nÔ¨Åne-tuning.\nAn important observation is that the best\nEnglish‚Üívideo search performance corresponds\nto the best German ‚Üívideo performance. This\ntrend implies that for model selection, the conÔ¨Åg-\nuration for the best English‚Üívideo model usually\ntranslates to the best conÔ¨Åguration for (zero-shot)\ncross-lingual model. This shared trend justiÔ¨Åes the\nEnglish‚Üívideo ablation studies in the original pa-\nper. Note that we utilize the best English ‚Üívideo\nfor all (zero-shot) cross-lingual experiment in our\nexperiment section.\nFor multilingual text ‚Üívideo search, the best\nconÔ¨Åguration we found in our experiments is to\noutput the 12-th layer and freeze the layers below\n9 for both mBERT and XLM-R.\nE Additional Experimental Results\nThe coverage of our text ‚Üívideo search experi-\nments is summarized in Table 11. Our experiments\ncover the following scenarios:\nIn-domain, English: Table 5 (VTT) and Table 6\n(V ATEX) in the original paper.\nIn-domain, non-English : Table 4 (VTT, 9 lan-\nguages) and Table 6 (V ATEX, Chinese).\nOut-of-domain, English: Additional (zero-shot)\ngeneralization results across datasets are in ¬ßE.1.\nOut-of-domain, non-English : We consider this\nas our future work.\nModel R@1 R@5 R@10\nVSE (Kiros et al., 2014) 10.1 29.4 41.5\nVSE++ (Faghri et al., 2018) 14.4 35.7 46.9\nDual (Dong et al., 2019) 13.7 36.1 48.2\nHGR (Chen et al., 2020a) 16.4 38.3 49.8\nOurs-Full 24.0 50.5 62.1\nTable 12: Zero-shot generalization on YouTube2Text\nwith VTT-Ô¨Ånetuned model.\nE.1 Generalizability across English-Video\nDatasets\nIn this section. we provide additional experiment\nresults regarding zero-shot generalization of the\nVTT-Ô¨Ånetuned model on out-of-domain dataset.\nSpeciÔ¨Åcally, we test on YouTube2Text (Chen and\nDolan, 2011). The aim of this experiment is to\ntest the cross-dataset generalizabilty of our model\nwithout using domain-speciÔ¨Åc training data.\nTable 12 shows the comparison of\nEnglish‚Üívideo search results on the\nYouTube2Text testing set. Models in this ta-\nble are only Ô¨Åne-tuned on VTT and use no\nYouTube2Text training data. As can be observed,\nour model with MMP generalizes well on\nYouTube2Text, outperforming HGR (Chen et al.,\n2020a) by 7.6 and DualEncoder (Dong et al., 2019)\nby 10.3 in R@1.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6961522102355957
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6147578954696655
    },
    {
      "name": "Computational linguistics",
      "score": 0.5636756420135498
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5609095096588135
    },
    {
      "name": "Natural language processing",
      "score": 0.5390559434890747
    },
    {
      "name": "One shot",
      "score": 0.46140164136886597
    },
    {
      "name": "First language",
      "score": 0.4476288855075836
    },
    {
      "name": "Linguistics",
      "score": 0.4464768171310425
    },
    {
      "name": "Philosophy",
      "score": 0.11481651663780212
    },
    {
      "name": "Engineering",
      "score": 0.11029276251792908
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}