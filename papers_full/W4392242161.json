{
  "title": "Harnessing Large Language Models for Coding, Teaching, and Inclusion to Empower Research in Ecology and Evolution",
  "url": "https://openalex.org/W4392242161",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104864479",
      "name": "Natalie Cooper",
      "affiliations": [
        "Natural History Museum"
      ]
    },
    {
      "id": "https://openalex.org/A2113008268",
      "name": "Adam Clark",
      "affiliations": [
        "University of Graz"
      ]
    },
    {
      "id": "https://openalex.org/A2093779095",
      "name": "Nicolas Lecomte",
      "affiliations": [
        "Université de Moncton"
      ]
    },
    {
      "id": "https://openalex.org/A2061735982",
      "name": "Huijie Qiao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Zoology"
      ]
    },
    {
      "id": "https://openalex.org/A2118035140",
      "name": "Aaron Ellison",
      "affiliations": [
        null,
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281728047",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4385481791",
    "https://openalex.org/W2954932437",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4380083162",
    "https://openalex.org/W4221081250",
    "https://openalex.org/W4386590195",
    "https://openalex.org/W4391258701",
    "https://openalex.org/W4392563703",
    "https://openalex.org/W4386701652",
    "https://openalex.org/W4385741486",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W4320733708",
    "https://openalex.org/W4321610465",
    "https://openalex.org/W4221045317",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4323528687",
    "https://openalex.org/W3004469174"
  ],
  "abstract": "1. Large language models (LLMs) are a type of artificial intelligence (AI) that can perform various natural language processing tasks. The adoption of LLMs has become increasingly prominent in scientific writing and analyses because of the availability of free applications such as ChatGPT. This increased use of LLMs raises concerns about academic integrity, but also presents opportunities for the research community. Here we focus on the opportunities for using LLMs for coding in ecology and evolution. We discuss how LLMs can be used to generate, explain, comment, translate, debug, optimise, and test code. We also highlight the importance of writing effective prompts and carefully evaluating the outputs of LLMs. In addition, we draft a possible road map for using such models inclusively and with integrity.2. LLMs can accelerate the coding process, especially for unfamiliar tasks, and free up time for higher-level tasks and creative thinking while increasing efficiency and creative output. LLMs also enhance inclusion by accommodating individuals without coding skills, with limited access to education in coding, or for whom English is not their primary written or spoken language. However, code generated by LLMs is of variable quality and has issues related to mathematics, logic, non-reproducibility, and intellectual property; they can also include mistakes and approximations, especially in novel methods.3. We highlight the benefits of using LLMs to teach and learn coding, and advocate for guiding students in the appropriate use of AI tools for coding. Despite the ability to assign many coding tasks to LLMs, we also reaffirm the continued importance of teaching coding skills for interpreting LLM generated code and to develop critical thinking skills.4. As editors of MEE, we support—to a limited extent—the transparent, accountable, and acknowledged use of LLMs and other AI tools in publications. If LLMs or comparable AI tools (excluding commonly-used aids like spell-checkers, Grammarly and Writefull) are used to produce the work described in a manuscript, there must be a clear statement to that effect in its Methods section, and the corresponding or senior author must take responsibility for any code (or text) generated by the AI platform.",
  "full_text": "HarnessingLargeLanguageModelsforCoding,Teaching,andInclusiontoEmpowerResearchinEcologyandEvolution\nNatalieCooper\n1\n*,AdamT. Clark\n2\n, NicolasLecomte\n3\n, HuijieQiao\n4 \nandAaronM.Ellison\n5,6\n1\nScienceGroup, Natural HistoryMuseumLondon, Cromwell Road, London, SW75BD, UK.\n2\nInstituteof Biology, Universityof Graz, Holteigasse6, 8010, Graz, Austria.\n3\nCanadaResearchChairinPolarandBoreal Ecology, Department of Biology, UniversityofMoncton, Moncton, NewBrunswick, E1A3E9, Canada.\n4\nKeyLaboratoryof Animal EcologyandConservationBiology, Instituteof Zoology, ChineseAcademyof Sciences; Chaoyang, Beijing, 110101, China\n5\nHarvardUniversityHerbaria, HarvardUniversity, Cambridge, MA02138, USA\n6\nSoundSolutionsforSustainableScience, Boston, MA02135, USA\n*Correspondingauthor:natalie.cooper@nhm.ac.uk\nAbstract1. Largelanguagemodels(LLMs)areatypeof artificial intelligence(AI)that canperformvariousnatural languageprocessingtasks. Theadoptionof LLMshasbecomeincreasinglyprominent inscientificwritingandanalysesbecauseof theavailabilityof freeapplicationssuchasChatGPT. Thisincreaseduseof LLMsraisesconcernsabout academicintegrity,but alsopresentsopportunitiesfortheresearchcommunity. HerewefocusontheopportunitiesforusingLLMsforcodinginecologyandevolution. WediscusshowLLMscanbeusedtogenerate, explain, comment, translate, debug, optimise, andtest code. Wealsohighlight theimportanceof writingeffectivepromptsandcarefullyevaluatingtheoutputsof LLMs. Inaddition, wedraft apossibleroadmapforusingsuchmodelsinclusivelyandwithintegrity.2. LLMscanacceleratethecodingprocess, especiallyforunfamiliartasks, andfreeuptimeforhigher-level tasksandcreativethinkingwhileincreasingefficiencyandcreativeoutput.LLMsalsoenhanceinclusionbyaccommodatingindividualswithout codingskills, withlimitedaccesstoeducationincoding, orforwhomEnglishisnot theirprimarywrittenorspokenlanguage. However, codegeneratedbyLLMsisof variablequalityandhasissuesrelatedtomathematics, logic, non-reproducibility, andintellectual property; theycanalsoincludemistakesandapproximations, especiallyinnovel methods.\nCodingwithLLMs3. Wehighlight thebenefitsof usingLLMstoteachandlearncoding, andadvocateforguidingstudentsintheappropriateuseof AI toolsforcoding. DespitetheabilitytoassignmanycodingtaskstoLLMs, wealsoreaffirmthecontinuedimportanceof teachingcodingskillsforinterpretingLLMgeneratedcodeandtodevelopcritical thinkingskills.4. Aseditorsof MEE, wesupport—toalimitedextent—thetransparent, accountable, andacknowledgeduseof LLMsandotherAI toolsinpublications. If LLMsorcomparableAItools(excludingcommonly-usedaidslikespell-checkers, GrammarlyandWritefull)areusedtoproducetheworkdescribedinamanuscript, theremust beaclearstatement tothat effect initsMethodssection, andthecorrespondingorseniorauthormust takeresponsibilityforanycode(ortext)generatedbytheAI platform.\nKeywords:artificial intelligence, ChatGPT, coding, inclusion, largelanguagemodels, teaching\nIntroduction\nArtificial intelligence(AI)isarapidlyexpandingfield, withmyriadusesinecologyandevolution(Borowiecet al., 2022; Christinet al., 2019; Hanet al., 2023; Pichler&Hartig, 2023; Tabaket al.,2019). AlthoughAI isnot new, theincreasingavailabilityof resourcessuchasGitHubCopilot,ChatGPT, andDall-Eisleadingtomoreresearchersandstudentsusingthesetoolsforresearchandstudy. Althoughthereiswidespreadconcernwithinthecommunityabout theuseof AI tools(vanDiset al., 2023), ourfocusinthisPerspectiveisonthepositiveopportunitiesforresponsiblyusingAI, specificallylargelanguagemodels(LLMs), toassist withcoding, bothforresearchandintheclassroom. Weproviderecommendationsforpublishingresearchthat hasused“LLMs”(whichwetakehereintoincludeLLMsandcomparablegenerativeAI tools, but not well-usedaidssuchasspell-checkers, Grammarly, andWritefull)..\nWhatareLLMsandhowdotheywork?Artificial intelligence(AI)istheabilityof amachine, generallyacomputerorrobot, toperformtasksnormallyassociatedwithintelligent humans(foranextendeddiscussionof themeritsof callingAI“intelligent,”whichiswell outsidetheboundsof thisPerspective; seeSearle, 1997). Currentdiscussionmostlycentresonformsof generativeAI, inwhichcomputersareabletoproducecontent basedonaset of trainingdata. Therearemanytypesof generativeAI, includingseveralwithwhichecologistsandevolutionarybiologistswill havesomefamiliarity(e.g., machine-learning,deeplearning, neural networks). Ourfocushereisonlargelanguagemodels(LLMs), whicharemachine-learningalgorithmsthat canperformvariousnatural languageprocessingtasks, suchasclassifyingandgeneratingtext (includingcomputercode), andrespondingtoquestionsinaconversational style. Thereareseveral availableLLMs, includingChatGPT(seeBox1), Gemini\n2\nCodingwithLLMs(previouslyBard), Llama, Guanaco, andOpenLLaMA, andmanymoreareindevelopment.\nInsimpleterms, LLMssuchasChatGPTaimtopredict thenext part of awordorphrasebasedonauser-providedtext prompt. Thepredictionismadebypassingtheprompt throughadeepneuralnetwork, whichitself hasbeentrainedtofindrelationshipsamongwordsandphrasesinanenormouscorpusof trainingdata(the“large”inLLMsreferstothemassiveamount of datausedtotrainthesemodels). Thepredictionsof LLMsarebasedonthecontent andstructureof thecorpususedtotrainthem, includingrulesof grammarandsyntax, howoftenwordsarefoundinaparticularsequence, andevenhowfactsandideasarepresentedtogether(regardlessof whetherornot that presentationisactuallycorrect). Contextual cluesalsoareusedtoimprovetheaccuracyof thepredictions, andmodelscanbe(imperfectly)trainedtoreducetheprevalenceof certainproblematicwordsandphrases(e.g., violent, racist, orillegal ones). Ratherthanalwaysselectingtheabsolutebest option, most LLMalgorithmsbuildinadegreeof randomnessintotheirresponses. Thishasconsequencesforreproducibility, seebelow, thoughseveral toolsnowallowimplementationswheretheusercanremovethisrandomcomponent (e.g., ChatGPTv4.0).\nBOX1:WhatisChatGPT?ChatGPT(https://chat.openai.com)waslaunchedinlate2022(OpenAI, 2022), andisthemostpopular(andinfamous)LLMcurrentlyavailable. It usesagenerativepretrainedtransformer(GPT)LLM, that functionsasachatbot, allowinguserstohaveinteractiveconversationstogeneratecontent. ChatGPTispopularbecauseit is(still)free, althoughyouneedtopayforthenewestversion(at thetimeof writingversion3.5isfree, andversion4.0must bepaidfor). It isa“verylarge”LLM, whichmeansit performsbetterthanother“large”LLMs. It alsoisrelativelygeneral,allowinguserstoapplyit tomultipledifferent kindsof tasks. AnadvantageforteachingwithChatGPTisthat it hasbeentrainedandconstrainedto(mostly)avoidsayingproblematicthings,e.g., violent, racist, orillegal questionsandanswersareblocked. Notethat althoughit isthemostcommonlyusedLLMapplication, ChatGPTisnot alwaysthebest tool forwhat youwant todo;otherAI tools, suchasGitHubCopilot andLlama, areoptimisedforprogramming. TheLLMmarketischangingrapidly, sobeingflexibleandinvestigatingnewtoolsastheyemergeislikelytobeagoodstrategy.\nUsingLLMsforcoding\nLLMsandotherAI applicationsareuncannilygoodat generatingwell-functioningandunderstandablecomputercodebasedonnatural languagepromptsfromusers—includinguserswithout aprogrammingbackground(Ellis&Slade2023). Withinecologyandevolution, manyresearchersandstudentsalreadyareusingLLMstohelpthemwritecode(Duffy2024). Doingso\n3\nCodingwithLLMsrequirestwomainskills: writingeffectiveprompts, andevaluatingtheresponses. Inaddition, usersneedtoknowhowtoapplythesetoolsresponsibly.\nWritingeffectivepromptsPromptsarehowyouinteract withanAI togeneratearesponse. Promptscanincludequestions,comments, codesnippets, orexamples, andshouldbewritteninfull sentencesin“natural”language(i.e., plaintext, asyoumight usetocommunicatewithacolleague), not asastringofkeywordsasyouwouldenterintoasearchengine. Forexample, if youwantedtolearnhowtorunalinearregressioninR, youmight usethekeywords“linearregression”and“R”inasearchengine, but foranLLM, abetterprompt wouldbesomethinglike“PleaseshowmehowtoperformalinearregressioninR”(it isuptotheuserwhetherpolitetermslike“please”arenecessary). Agoodprompt will bedetailedandspecifyasmuchcontext aspossible(thewho, what, when,where, why, andhowof thequestion).\nForprogrammingquestions, promptsshouldincludetheprogramminglanguage, anyspecificpackagesyouwant touse, andwhat youwant toachievewiththecode. Promptsalsocanbe“chained;”youcanquerytheLLMwithaninitial prompt andthenfollowupwithanotherprompt toupdatetheresponse. Most LLMsintendedforusebythegeneral publicwill “remember”thecontext providedinpreviouspromptswithinasinglechat session(ormultipleprevioussessionsdependingonthetool inquestion), andusethistogenerateitsresponses. Forexample, if youaskforcodetocreateaboxplot, inthenext prompt youcouldaskit tochangethecolours, withoutneedingtore-specifythedetailsof theboxplot. Anotheruseful tipistoasktheLLMto“explainwithexamples,”or“explainasif I amahighschool student.”Note, however, that most LLMshaveacharacterortokenlimit forquestions. InChatGPTversion3.5(whichiscurrentlyfree), thelimit is4,096tokensorapproximately3,000words, whichmaylimit theamount of detail youcanput intoyourprompt.\nEvaluatingtheoutputsThehardest, yet most important part of usingLLMstogeneratecodeisevaluatingtheaccuracyofthecodeproduced(Lubianaet al., 2023). Indeed, AIs“hallucinate.”Theycanbecompletelyconfident inaresponseevenwhenthat responseisinaccurateorentirelyincorrect. Hallucinationstendtooccureitherwhenthepromptshavenot giventheAI enoughcontext toanswercorrectly,whenthetrainingdatadoesnot includesufficient informationtoaddresstheprompt, orif thetrainingdataitself includesmistakes. Hallucinationstendtobemorecommonforlessfrequentlyusedanalyses, packages, orprogramminglanguages. Becausemost LLMsaretrainedtoproduceanswersthat areperceivedascorrect byhumaneditors, theyalsomayfavourgrammaticalcorrectnessandplausibilityoveraccuracy. It isthereforeextremelyimportant tobesceptical of anyresponsegivenbyanyAI, andtoalwayscheckthat anycodeproducedworksinthewayyouwantorexpect it to. Consequently, toeffectivelyuseLLMsforcoding, youstill needtounderstand\n4\nCodingwithLLMsenoughabout theprogramminglanguagetounderstandwhethertheoutputsarecorrect ornot.\nOtherusesofLLMsincodingInadditiontogeneratingcode, LLMscanalsohelpwiththefollowingroutinecodingactivities(seealsoLubianaet al., 2023).\n1. Explaincode(toyourselfandothers). It isnot uncommontohavecodethat youdidnotwrite(e.g., takenfromapaper, acollaborator, GitHub, orStackOverflow)orcodethat youwroteinthepast anddidnot document well. Withgoodprompts, LLMscanexplainwhatthecodeisdoingandwhy. Amajoradvantageisthat AIsareinfinitelypatient, soyoucancontinueaskingthesamequestionrepeatedlyif thefirst explanationdoesnot makesense.2. Commentingcode. Commentingiskeytogood, reproduciblecode(Cooper&Hsing2017), but someroutinecommentingisoftenskipped. LLMscanquicklycomment code,includingroutinesections, savingresearchertimeandeffort, whichcanbeappliedtocommentingonmorecomplexorbespokesectionsof thecode. Again, verifyingthegeneratedcommentsiscritical.3. Translatecode. Manyresearchersarefamiliarwithonlyoneprogramminglanguage, butmayfindtheyneedtouseanotherlanguagetosolvecertainproblems. LLMscantranslatecodefromonelanguagetoanother(e.g., fromPythontoR)orfromonepackagetoanother(e.g., fromtidyversetobaseR).4. Debugcode. If yourcodeisbroken, youcanprovidethecodetoanLLMandaskit tofindanyerrors. Ideallyyoushouldincludeanyerrormessagesandtheaimof thecodeascontext toyourquestion. LLMscanbeparticularlygoodat explainingarcaneerrormessages.5. Optimisecode. Sometimesit iseasiertoquicklywritecodethat works, ratherthanspendingalot of timeoptimisingthecodetomakeit efficient andfast torun. LLMscantakeunoptimisedcodeandedit it tomakeit runfasterandmoreefficiently. However, thecorrectmetricsof optimizationareimportant toverifybeforehand.6. Unittests. LLMscanbeusedtowritestandardunit testsforfunctions.\nNotethat, aswithgeneratingcode, all of theabovecomewiththecaveat that youmustcheckthattheAIhasdonewhatyouexpectedandthatanycodeproducedrunsandproducesaccurateresults. Inparticular, debuggingwithLLMscanberifewitherrors—recall that most LLMsaretrainedtoproduceresultsthat lookplausibletoahuman, meaningthat errorsinthecodecanbedifficult tofind.\nBenefitsandchallengesofusingLLMsforcodingTheprimarybenefit of usingLLMstogeneratecodeisthat it isoftenfasterthanwritingitourselves, especiallyif wearenon-professional programmers(i.e., most of usinecologyand\n5\nCodingwithLLMsevolution), orfortasksandproblemsthat wehavenot encounteredbefore. LLMscanlowertheopportunitycost of tryingsomethingthat might becomplextolearnindependently, andcanspeeduproutinetasksleavingmoretimeforotherthingslikesynthesisandideadevelopment. LLMsalsoincreaseequityandinclusion, astheyprovidemoreopportunitiesforpeoplewithout codingskills,withneurodivergent traits, whohavelessfluent Englishskills, andotherswhomaybenefit fromdifferent waysof working(Box2).\nFurthermore, LLMscangeneratemorethanonesuggestionfordoingagivendata-analysistask,completewithimplementedandwell-commentedsolutionsandcodeexamples. Thisnot onlycanincreasetherateat whichcodingskillscanbelearned, but alsomayenablelessseasonedscholars, particularlystudents, torapidlydevelopcomplex, multi-stepmethodsforanalysingaspecificdataset, comparetheoutputs, andchoosethemost appropriateway(s)tointerpret theresults.\nThechallengesof usingLLMsforcodingincludethefact that responsesvarygreatlyinquality—ofteninwaysthat arenot readilyapparent. Forcommonlyused, well-documentedfunctions,packagesandlanguages, LLMsaretypicallyfast, efficient, andlargelyaccuratebecausetheyhavemoreexamplesof theseintheirtrainingdatasets; forexample, answersusingRpackageslikeggplot2tendtobecorrect. Hallucinationsaremorecommon, however, whenaskingaboutless-usedpackages; forexample, fittingphylogeneticcomparativemodelsusingOUwieoftengivenonsensical results. ManyLLMsalsostrugglewithmathematicsandlogic, andneedefficientpromptsandextensivetestingtodebugoridentifytheerrors(Changet al., 2023; LópezEspejel etal., 2023). Responsesalsoaretypicallynot reproducible(at least forregularusers), sodifferentpeoplemayget different resultswiththesameprompt. Thisinconsistencycanbedisconcertingtonoviceusers.\nBOX2:EquityandinclusionwithLLMs\nDebatearoundLLMsandinclusiontendstofocusonthebiasesinherent inthesemodels(Schwartzet al., 2022). If thetrainingdatabeingusedtogenerateresponsesarebiased(andweknowthat it is), thenLLMsnaturallywill reflect thisbiasintheiroutputs. Thisisobviouslyundesirable. Althoughmanydevelopersof LLMsaretryingtofixthisissue, trainingdataarestillfocusedonmaterialsthat areavailableinlanguagesthat arewell-representedontheInternet, suchasEnglishorGerman, andthat areproducedinwealthynationsof theGlobal North. Thus,responseswill tendtosharetheperspectivesandbiasesof theseregionsandcultural groups. NewLLMsarealsoexpensivetodevelop, intermsof staff, equipment, andinfrastructure, andtheirdevelopment will, inall likelihood, continuetobedominatedbythosewiththemost moneyandpower. Anadditional, largeconcernisthat currentlyfreeplatformsmayeitherceasetobefree, or\n6\nCodingwithLLMsstopbeingsupportedanddeveloped, leavinganyoneunabletopayforAI toolsbeingleft tousesubstandardproducts. Inaddition, theuseof several toolsisalreadyblockedinsomecountries(e.g., China)andworkplaces(e.g., USGovernment Agencies). It isimportant toaskwhoisbenefittingfromAI andwhoismissingout.\nTherearealsopotential equityandinclusionbenefitsof LLMs. Theavailabilityof freeAI toolsshouldhelptolevel opportunitiesforthosewhodonot havecodingskills, accesstocodingeducation, ortheabilitytopaytolearnthem. Already, wecanuseLLMsinaclassroomsettingtoprovidebespokeadviceandfeedbacktostudents. Astudent maybemorecomfortableaskingachatbot forhelpthanateachingassistant orlecturer, especiallyif theyneedtoaskthesamequestionmultipletimestounderstandtheanswer, orstudentsmaybeabletouseLLMstogetthoseanswersinalanguageorstylethat ismoreapproachabletothem. TeachingstudentstouseLLMsforcodingcanalsoreduceinequitiesintroducedbysomestudentsfindingcodingmuchharderthanothers. Therearespecial advantagesforneurodivergent students, andforstudentswhosefirst languageisnot English. ThesebenefitsshouldalsobeconsideredwhenthinkingaboutequityandinclusioninAI.\nLLMsandcodingintheclassroom\nAslongasLLMsremaineasytoaccess(andparticularlywhiletheyremainfreetouse), studentswill usethemwhethereducatorslikeit ornot. Ourrecommendationistoguideandadvisestudentsonresponsibleuseof AI ratherthanattemptingtoregulateorbanitsuse(seealsoDuffy, 2024,Lubianaet al., 2023; andhttps://cs50.ai/). Ourownexperienceisthat LLMscanbeexcellent aidsforteachingandlearningcoding. StudentscanuseLLMstogenerate, explain, comment, translate,debug, andoptimisecode. StudentsalsocanuseAIsaspersonal tutors, andcontinueaskingthesamequestionrepeatedlyindifferent waysastheyworktofullyunderstandtheanswers. Studentsalsomaybehappiertoaskchatbotsforhelpthantoaskahumaninstructor, out of fearthat thelatterwill judgethemharshlyfornot knowingtheanswerstosimplequestions. Likewise, LLMscanbeareal helpforinstructorsincrowdedclassroomsbyprovidingafirst point of contact forstudentquestions. Thisfacilitymaybeparticularlyvaluableforneurodivergent studentsorstudentswhosefirst languageisnot English(orwhatevertheworkinglanguageof theclassinquestionis)andnaturallyshystudents(e.g., Liuet al., 2024). Twoespeciallyhelpful featuresarethat most LLMscanreadilyprovideanswersinmultiplelanguagesandtheycanquicklysummariseortranslatetextfromusermanualsorhelpforums.\n7\nCodingwithLLMsShouldwestillteachorlearncoding?Oneof ourworkshopprovocationswastoaskparticipantswhetherwestill needtoteachcodingskillsorlearnthemasresearchers, orif weshouldjust outsourcetheprocesstoAIs. Wethinkthatweshouldstill teachstudentstocode. AlthoughLLMscanfacilitatelearningbystudentswhootherwisestrugglewithcoding, interpretingtheoutputsof LLMsanddeterminingwhethertheyareaccuratestill requiresabasicunderstandingof coding, statistics, andmathematics. Inaddition, wemust beabletodetect whenandwhythecodegeneratedbyanLLMisnot working. Studentsalsoneedtoknowwhat toasktheLLMtodo, andwhy. Thus, codingskillswill continuetobevital forstudentsinecologyandevolution, inthesamewaythat learningthebasicsof arithmeticisnecessaryfortheeffectiveuseof acalculatororspreadsheet. Weanticipatethat simpledebuggingfeaturesandrepetitivetaskscanbeidentifiedandaddressedeasilywithLLMs, savingtimeandefficiency, just asspelling-andgrammar-checkersspeedupproof-readingof manuscripts.\nTeachingcodingskillsalsoprovidespedagogical benefits. Forinstance, learningtocoderequiresustobreakdownlargeproblemsintosmallerchunksthat areusuallyeasiertosolve. It alsorequiresstronglogicandscientificreasoningtounderstandwhat youneedtodoandhowtoget amachinetodoit foryou. Moreover, developingtheseskillsisastrategicprocessthat canhelptomaximisecreativityandcreatenovel material, bothof whichareessential fordoingscience,makingdiscoveries, andspurringinnovations(e.g., Fletcher & Benv eniste 2022) . Insummary, criticalthinkingskillswill continuetobeimportant, eveninaworldrepletewithtext, video, andcodegeneratedbyLLMs.\nBestpracticeforpublishingcodegeneratingusingLLMs\nCurrent journal policiesforpublishingcodeat MethodsinEcologyandEvolution(MEE)requirethatcodeneedstobenovel, usable, andunderstandable. MEEplacesemphasisonthequality,usability, accessibility, andfunctionalityof code(seehttps:/ /besjournals.onlinelibr ar y .wile y .com/ hub/edit orial-policies ). Concernhasbeenraisedabout accountabilityandtransparencyofpublishingcodegeneratedusingAI (e.g., vanDiset al., 2023). Whoisaccountableforthecode(ortext)andwhoshouldget credit forit?Most journals, includingMEE, expect that thecorrespondingandseniorauthorsof apaperareaccountableforall of itscontents. All authorsareaskedtoreadandagreetothesubmissionof afinal draft of amanuscript beforesubmission. Weknowthat noteveryauthorreadsall thecodeassociatedwithapaper, but at least oneauthormust doso, andtakeresponsibilityforit. UsinganLLMtohelpgeneratethecodedoesnot changethisfact. WhenusinganLLM, wewouldexpect theresponsibleauthor(s)tofollowthesamekindsof qualityassurancechecksastheywouldif thecodewaswrittenbythemindependently.\n8\nCodingwithLLMsGoingforward, whenLLMsareusedtogeneratecodeinpublicationsat MEE, wewill requirethefollowing:1. Atleastoneauthormusttakeresponsibilityforallassociatedcode(ortext)generatedbytheLLM.ThisresponsibilitymustbeexplicitlynotedintheAuthorContributionssection.2. TheuseofAI/LLMsmustbeclearlystatedinthemanuscriptintheMethodssection.TheAIapplication(e.g.,ChatGPT)andversion(e.g.,3.5)mustbereported,alongwithdetailsofhowmuchofthecontentwasgeneratedbytheAI.3. TheportionsofcodegeneratedbytheLLMmustbeannotatedwithcommentsstatingthattheyweregeneratedinthisway.\nNotethat ourfocushereisonusingLLMsforcoding; theBritishEcological Society(BES)journals,includingMEE, alsohaveapolicyonAI-generatedcontent moregenerally(https:/ /besjournals.onlinelibr ar y .wile y .com/hub/edit orial-policies ), whichalsomust befollowed. Oneimportant part of thisisthat anAI cannot beconsideredasanauthor.\nAnimportant concernregardingcodeproducedbyLLMsisalackof reproducibility. It istruethat ifyouaskmost LLMsthesamequestionrepeatedly, theywill givedifferent, andperhapsinaccurate,answers—but howmuchof aproblemisthis?Tworesearchersworkingonthesameproblemmayproducedifferent codesolutions; indeed, oneresearcherworkingonthesameproblemat differenttimesmaydoso, too. Therewill bespecificsituationswherethiswill beaproblem(e.g., usinganLLMtocollatedataforameta-analysis), but forbasiccodegeneration, thislackof reproducibilityisnot anissueaslongasthecodeitself generatesreproducibleresultsandmeetsMEE’scriteriaofquality, usability, accessibility, andfunctionality(https:/ /besjournals.onlinelibr ar y .wile y .com/hub/edit orial-policies ), andthat theauthorsacknowledgetheuseof LLMsingeneratingthecode. Tothisend, it appearstousthat thelackof reproducibilityrelatedtoLLMsissimilartothat of othertools—theresultsshouldbereplicablebut thepreciseprocessthat went intodecidinghowtocreatethosespecificresultsmaynot be. Inouropinion, thatissimplyhowthecreativeprocessof scienceworks.\nMoregenerally, wealsocautionthat all usesof LLMsandAI requirecareful considerationof creditandliability. Theseconsiderationsarerelevant bothintermsof intellectual contribution(i.e., whocameupwiththerelevant ideas), andintellectual property(i.e., whohascopyrightedthematerialthat youoryourLLMareusing). BoththeUnitedStatesandtheEUareinthemidst of enactingregulationsonthetopic, and, inmost cases, it appearsthat, if anLLMdrawsheavilyoncopyrightedtext orimagesintheproductionof apublishedoutput, thehumanauthormaybeheldliableforcopyright infringement. Similarly, if anLLMreproducesillegal contents(e.g., hatespeech\n9\nCodingwithLLMsorsymbolsfrombannedpolitical parties)that arethenincludedinapublishedwork, thenthehumanauthor, not theLLM, likelywill beheldresponsibleforitsproducts.\nConclusion\nAI andLLMsarenot newtechnologies; variousAI applicationshaveexistedsincethe1960s. Manyresearchersconcernedabout theriseof LLMapplicationslikeChatGPTforget that spell-checkersandauto-completefunctions, whichweuseeverydayinmanydifferent settings, alsoarevarioustypesof AI. However, thecapabilitiesof generativeAI andLLMsareincreasingrapidly, andnewdevelopmentsappearalmost daily. It ispartiallythisspeedof change, andthefeelingof not beingabletokeepup, that isdrivingconcerns. Forexample, that theAI inExcel reformatsgeneticsequencesasdatesis, afterdecadesof pain, relativelycommonknowledge, but what doweknowabout similarbugsintoolslikeChatGPTorGemini?Changedoesnot alwayshavetobebad,however, andAI andLLMsarenot goingawayanytimesoon. Weneedtoengagewiththesetoolsproactivelyandresponsiblywhilepromotingthebest availablepractices, whichthemselveswillcontinuetochange. Weaccept that AI isarapidlyevolvingfieldandweexpect that ourthoughtsabout it alsowill continuetoevolve. Wealsonotethat wehavedifferent levelsof expertiseinusinganddevelopingAI tools; evenexpertsdonot knowwherethefieldwill beayearfromnow.However, westill think—at least fornow—that thepotential positivesof usingLLMsforcodingoutweighthepotential negatives.\nWealsorecognisethechallengesinherent inthesemethods. TrainingLLMshasastaggeringlylargeenvironmental impact (Box3), andthesetoolshavethepotential toincreaseglobal inequities(Box2). Asacommunity, weshouldconsiderhowtouseAI toolsmost effectivelyandethically,whobenefits, whoismissingout, andhowwecanreducetheirenvironmental impacts. Ultimately,wethinkthat AI isnot heretoreplaceus, but rathertoassist us(moreliketherobotsinAsimov’sIRobot booksthanthoseinČapek’sR.U.RorTheTerminatorfilms). However, giventherapidadvancesinAI, it probablywouldnot hurt toadd“please”and“thankyou”toyourChatGPTprompts. Just incase.\nBOX3:TheenvironmentalimpactofLLMs\nTherehasbeenmuchpublicconcernabout theeffectsof AI onacademicintegritybut therehasbeenfarlessdiscussionabout thedirect andindirect environmental impactsof AI (Jayet al., 2024,Rilliget al., 2023, vanWynsberghe, 2021). CO2\nemissionsformodel trainingandtuningforjustonenatural languageprocessingmodel havebeenestimatedtoexceedtheaveragelifetimeCO2\nemissionsforapersonlivingintheUSA(Strubbel et al 2019). UsingChatGPT-likeservicesina\n10\nCodingwithLLMssingleyearproduced25timesthecarbonemissionsof trainingGPT-3(Chienet al., 2023). LLMsalsorequirealot of infrastructureandequipment, all of whichhaveassociatedenvironmentalimpacts; examplesincludewateruseandcontamination, miningforrare-earthelements, andtheenergyrequiredfortemperaturecontrol of servers(Rilliget al., 2023). Muchcurrent researchisfocussedoncreatingsustainable, lower-carbonLLMs(Chienet al., 2023, Pattersonet al., 2021),but until thesearesuccessful, it isworthbeingverycircumspect about unnecessaryuseof AI forsimpletasks. Youmight beaddingsubstantiallytoyourcarbonfootprint just tosaveacoupleofminutesof effort.\nAcknowledgmentsThankstoattendeesofour“CodingwithChatGPTandotherLLMs”workshopsattheBES2023AnnualMeetinginBelfast,NaturalHistoryMuseumLondon,andUniversityofSheffield,andtovariouscontributorsonsocialmedia,fordiscussionandsuggestionswhichinformedthisPerspective.ChatGPTv. 3.5wrotethetitleundersupervisionofNC.ThetitlewasgentlyrearrangedforclaritybyAME.\nConflictofinterestWearealleditorsatBritishEcologicalSociety(BES)journals,and(excludingATC)wearecompensatedbyBESforourwork,thuswehavevestedinterestintheadoptionoftheseguidelines.\nAuthorcontributionsNCwrotethefirstdraftwithinputfromotherauthors.Allauthorseditedthemanuscriptandapprovedthefinalversionforsubmission.\nReferences\nBorowiec, M. L., Dikow, R. B., Frandsen, P. B., McKeeken, A., Valentini, G., &White, A. E. (2022).Deeplearningasatool forecologyandevolution. MethodsinEcologyandEvolution, 13,1640–1660. https://doi.org/10.1111/2041-210X.13901.\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y.,Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., &Xie, X. (2024). Asurveyonevaluationof largelanguagemodels. ACMTransactionsonIntelligent SystemsandTechnology. https://doi.org/10.1145/3641289.\n11\nCodingwithLLMsChien, A. A., Lin, L., Nguyen, H., Rao, V., Sharma, T., &Wijayawardana, R. (2023). Reducingthecarbonimpact of generativeAI inference(todayandin2035). Proceedingsof the2ndWorkshoponSustainableComputerSystems, 1–7.https://dl.acm.org/doi/10.1145/3604930.3605705\nChristin, S., Hervet, É., &Lecomte, N. (2019). Applicationsfordeeplearninginecology. MethodsinEcologyandEvolution, 10, 1632–1644. https://doi.org/10.1111/2041-210X.13256.\nCooper, N., &Hsing, P-Y. (eds). 2017. AGuidetoReproducibleCodeinEcologyandEvolution.BESGuidestoBetterScience.https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf.\nvanDis, E. A. M., Bollen, J., Zuidema, W., vanRooij, R., &Bockting, C. L. (2023). ChatGPT: fiveprioritiesforresearch. Nature, 614, 224–226. https://doi.org/10.1038/d41586-023-00288-7.\nDuffy, M. (2024). GenerativeAI andgraduatetraininginecology. DynamicEcologyblog.https://dynamicecology.wordpress.com/2024/01/15/generative-ai-graduate-training-in-ecology/#more-66157. Accessed15thJanuary2024.\nEllis, A. R., &Slade, E. (2023)Aneweraof learning: considerationsforChatGPTasatool toenhancestatisticsanddatascienceeducation. Journal of StatisticsandDataScienceEducation, 31, 128–133. https://doi.org/10.1080/26939169.2023.2223609.\nFletcher, A., &Benveniste, M. (2022). Anewmethodfortrainingcreativity: narrativeasanalternativetodivergent thinking. Annalsof theNewYorkAcademyof Sciences, 1512,29–45. https://doi.org/10.1111/nyas.14763.\nHan, B. A., Varshney, K. R., LaDeau, S., Subramaniam, A., Weathers, K. C., &Zwart, J. (2023). AsynergisticfutureforAI andecology. Proceedingsof theNational Academyof Sciences,120, e2220283120. https://doi.org/10.1073/pnas.2220283120.\nJay, C., Yu, Y., Crawford, I., Archer-Nicholls, S., James, P., Gledson, A., Shaddick, G., Haines, R.,Lannelongue, L., Lines, E., Hosking, S., &Topping, D. (2024). Prioritizeenvironmentalsustainabilityinuseof AI anddatasciencemethods. NatureGeoscience, 17, 106–108.https://doi.org/10.1038/s41561-023-01369-y.\nLiu, R., Zenke, C., Liu, C., Holves, A., Thornton, P., &Malan, D. J. (2024). TeachingCS50withAI:leveraginggenerativeartificial intelligenceincomputerscienceeducation. Proceedingsofthe55thACMTechnical SymposiumonComputerScienceEducationV. 1(SIGCSE2024),March20–23, 2024, Portland, OR, USA. ACM, NewYork, NY, USA, 1–7.\n12\nCodingwithLLMshttps://doi.org/10.1145/3626252.3630938.\nLópezEspejel, J., Ettifouri, E. H., YahayaAlassan, M. S., Chouham, E. M., &Dahhane, W. (2023).GPT-3.5, GPT-4, orBARD?EvaluatingLLMsreasoningabilityinzero-shot settingandperformanceboostingthroughprompts. Natural LanguageProcessingJournal, 5, 100032.https://doi.org/10.1016/j.nlp.2023.100032.\nLubiana, T., Lopes, R., Medeiros, P., Silva, J. C., Goncalves, A. N. A., Maracaja-Coutinho, V.,Nakaya, H. I. (2023). Tenquicktipsforharnessingthepowerof ChatGPTincomputationalbiology. PLoSComputational Biology, 19, e1011319.https://doi.org/10.1371/journal.pcbi.1011319\nOpenAI. (2022). ChatGPT: Optimizinglanguagemodelsfordialogue.https://openai.com/blog/chatgpt/. Accessed8thJanuary2024.\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L. M., Rothchild, D., So, D., Texier, M., &Dean, J. (2021). Carbonemissionsandlargeneural networktraining. arXiv:2104.10350.https://doi.org/10.48550/arXiv.2104.10350.\nPichler, M., &Hartig, F. (2023). Machinelearninganddeeplearning—Areviewforecologists.MethodsinEcologyandEvolution, 14, 994–1016. https://doi.org/10.1111/2041-210X.14061.\nRillig, M. C., Ågerstrand, M., Bi, M., Gould, K. A., &Sauerland, U. (2023). Risksandbenefitsoflargelanguagemodelsfortheenvironment. Environmental ScienceandTechnology, 57,3464-3466. https://doi.org/10.1021/acs.est.3c01106.\nSchwartz, R., Vassilev, A., Greene, K., Perine, L., Burt, A., &Hall, P. (2022). Towardsastandardforidentifyingandmanagingbiasinartificial intelligence. National Instituteof StandardsSpecial Publication, 1270. https://doi.org/10.6028/NIST.SP.1270\nSearle, J.R. (1997)TheMysteryof Consciousness. TheNewYorkReviewof Books, NewYork,USA.\nStrubell, E., Ganesh, A., McCallum, A. (2019). EnergyandpolicyconsiderationsfordeeplearninginNLP. Proceedingsof the57thAnnual Meetingof theAssociationforComputationalLinguistics, AssociationforComputational Linguistics: Florence, Italy, 2019, 3645–3650.https://doi.org/10.18653/v1/P19-1355.\nTabak, M. A., Norouzzadeh, M. S., Wolfson, D. W., Sweeney, S. J., Vercauteren, K. C., Snow, N.P., Halseth, J. M., Di Salvo, P. A., Lewis, J. S., White, M. D., Teton, B., Beasley, J. C.,Schlichting, P. E., Boughton, R. K., Wight, B., Newkirk, E. S., Ivan, J. S., Odell, E. A., Brook,\n13\nCodingwithLLMsR. K., Lukacs, P. M., Moeller, A. K., Mandeville, E. G., Clune, J., &Miller, R. S. (2019).Machinelearningtoclassifyanimal speciesincameratrapimages: Applicationsinecology.MethodsinEcologyandEvolution, 10, 585–590. https://doi.org/10.1111/2041-210X.13120.\nvanWynsberghe, A. (2021). SustainableAI: AI forsustainabilityandthesustainabilityof AI. AIEthics1, 213–218. https://doi.org/10.1007/s43681-021-00043-6.\n14",
  "topic": "Coding (social sciences)",
  "concepts": [
    {
      "name": "Coding (social sciences)",
      "score": 0.5599095821380615
    },
    {
      "name": "Computer science",
      "score": 0.33232754468917847
    },
    {
      "name": "Social science",
      "score": 0.2128855288028717
    },
    {
      "name": "Sociology",
      "score": 0.2026594877243042
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1336856363",
      "name": "Natural History Museum",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I15766117",
      "name": "University of Graz",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I154799132",
      "name": "Université de Moncton",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210167311",
      "name": "Institute of Zoology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ],
  "cited_by": 6
}