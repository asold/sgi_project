{
  "title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models",
  "url": "https://openalex.org/W4224938976",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101134209",
      "name": "Simeng Sun",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5005955461",
      "name": "Katherine Thai",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5082767919",
      "name": "Mohit Iyyer",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2082911781",
    "https://openalex.org/W3174401451",
    "https://openalex.org/W4226285793",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3163073193",
    "https://openalex.org/W1992695520",
    "https://openalex.org/W4287888039",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3021534166",
    "https://openalex.org/W2327885654",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W4226275767",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W3033182847",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W3032795377",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W4231642182"
  ],
  "abstract": "While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce ChapterBreak, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative. A fine-grained human annotation reveals that our dataset contains many complex types of chapter transitions (e.g., parallel narratives, cliffhanger endings) that require processing global context to comprehend. Experiments on ChapterBreak show that existing LRLMs fail to effectively leverage long-range context, substantially underperforming a segment-level model trained directly for this task. We publicly release our ChapterBreak dataset to spur more principled future research into LRLMs.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3704 - 3714\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nCHAPTER BREAK : A Challenge Dataset for\nLong-Range Language Models\nSimeng Sun Katherine Thai Mohit Iyyer\nUniversity of Massachusetts Amherst\n{simengsun,kbthai,miyyer}@cs.umass.edu\nAbstract\nWhile numerous architectures for long-range\nlanguage models (LRLMs) have recently been\nproposed, a meaningful evaluation of their\ndiscourse-level language understanding capa-\nbilities has not yet followed. To this end, we in-\ntroduce CHAPTER BREAK , a challenge dataset\nthat provides an LRLM with a long segment\nfrom a narrative that ends at a chapter bound-\nary and asks it to distinguish the beginning of\nthe ground-truth next chapter from a set of neg-\native segments sampled from the same narra-\ntive. A fine-grained human annotation reveals\nthat our dataset contains many complex types\nof chapter transitions (e.g., parallel narratives,\ncliffhanger endings) that require processing\nglobal context to comprehend. Experiments on\nCHAPTER BREAK show that existing LRLMs\nfail to effectively leverage long-range context,\nsubstantially underperforming a segment-level\nmodel trained directly for this task. We publicly\nrelease our CHAPTER BREAK dataset to spur\nmore principled future research into LRLMs.1\n1 Introduction\nResearch on long-range language models(LRLMs)\naims to process extremely long input sequences by\nmaking the base Transformer architecture more ef-\nficient (e.g., through sparse attention, recurrence,\nor cached memory). These modifications are\ncommonly validated by training LRLMs on PG-\n19 (Rae et al., 2020), a long-document language\nmodeling dataset, and demonstrating small perplex-\nity decreases over shorter context models (Roy\net al., 2021; ?). However, recent analysis exper-\niments (Sun et al., 2021; Press et al., 2021) show\nthat modern LRLMs rely mostly on local context\n(i.e., the immediately preceding 1-2K tokens) and\nare insensitive to various perturbations applied to\nmore distant context.\n1We make our code and data public at https://\ngithub.com/SimengSun/ChapterBreak\n... Billy Pilgrim has come unstuck in time... he has no control \nover where he is going... he ﬁrst came unstuck in time in 1944, \nlong before his trip to Tralfamadore... [6,608 words pass]\n...Right outside the window was Billy’s own Cadillac El Dorado \nCoupe de Ville... The date on the license plate was1967, which \nwould make Billy Pilgrim forty-four years old... [2,930 words \npass, story shifts to World War II in 1944]\n...locomotives began to move east... The war would end in May. \nGerman prisons everywhere were absolutely full... Billy Pilgrim's \ntrain... did not move for two days... [251 words pass, ch. 3 ends \nby shifting back to 1967]\n...he traveled in time to 1967 again—to the night he was \nkidnapped by a ﬂying saucer from Tralfamadore.\n(\n➕ ) Billy Pilgrim could not sleep on his daughter's wedding \nnight. He was forty-four... [ground-truth start of ch. 4]\n(\n➖ ) Billy Pilgrim says that the Universe does not look like a lot \nof bright little dots to the creatures from Tralfamadore... \n(\n➖ ) All the trains were slow. The coaches stunk of coal smoke \nand rationed tobacco and rationed booze and the farts of people \neating wartime food. \nFigure 1: An illustrative example of our suffix iden-\ntification task from Kurt V onnegut’s Slaughterhouse-\nFive, in which an LRLM needs to make connec-\ntive inferences across temporal and spatial shifts\nin a long prefix of the narrative to correctly\ndisambiguate the (+) start of the next chapter from\n(-) negative examples.\nIn this paper, we move beyond token-level per-\nplexity by evaluating LRLMs on a task that requires\na rich understanding of long-range dependencies.\nOur task is an instance of suffix identification, in\nwhich a language model is given a long input se-\nquence (or prefix) and asked to disambiguate the\nnext n-token segment from a set of hard negatives\nsampled from the same narrative. To succeed at\nthis task, an LRLM should assign high probability\nto the ground-truth next segment and low probabil-\nity to the negatives. To specifically test long-range\ndependencies, we restrict our prefixes to end at\nchapter breaks of a longer cohesive narrative (e.g.,\na novel).\nWe construct a challenge dataset, CHAPTER -\nBREAK , by automatically detecting chapter bound-\n3704\naries within both held-out PG-19 documents (in-\ndomain for pretrained LRLMs) and works of fan\nfiction published on the Archive of Our Own (out\nof domain).2 We perform a detailed analysis of the\ntypes of chapter transitions in our dataset and dis-\ncover a high frequency of narrative shifts in point-\nof-view, location, and time, all of which require\nglobal narrative understanding over long input se-\nquences. For example, Figure 1 contains a complex\nprefix in which the time-traveling Billy Pilgrim\nmoves between World War II, 1960s suburban life,\nand an alien planet. Understanding the cliffhanger\nending, in which the narrative abruptly switches\nfrom a wartime scene to a 1967 alien abduction,\nrequires an LRLM to make connective inferences\nusing details buried far back in the context (e.g.,\nBilly’s age in 1967).\nWe evaluate three LRLMs on CHAPTER BREAK ,\nincluding BigBird (Zaheer et al., 2020), the Rout-\ning Transformer (Roy et al., 2021), and its local\nattention variant, all pretrained or fine-tuned on\nPG-19. Our experiments show that these LRLMs\nperform poorly at selecting the ground-truth suffix,\nregardless of the length of the input sequence. As\nan upper bound, we train a small RoBERTa-based\nsegment-level language model on PG-19 and dis-\ncover that it substantially outperforms all LRLMs\non CHAPTER BREAK , which suggests that LRLMs\nhave considerable room for improvement on this\nsuffix identification task. Finally, we perform an\nanalysis on the instances for which all models strug-\ngle to choose the correct suffix, which shows that\nshifts in location and events in focus are particu-\nlarly challenging to disambiguate. Taken together,\nthese results suggest that CHAPTER BREAK is a\nuseful benchmark for future research into LRLMs.\n2 The C HAPTER BREAK dataset\nAuthors often break long-form narratives into a se-\nquence of discrete chapters to impose “an order and\nshape over events in time” (Stevick, 1970). Henry\nFielding writes in his novel Joseph Andrews that\nthe space between chapters is like “an Inn or Rest-\ning Place” for readers to reflect on the preceding\nchapter (Fielding, 1779). Chapters come in many\nflavors: for example, Murakami’s Kafka on the\nShore uses chapter breaks to alternate between par-\nallel narratives focusing on the two protagonists,\nwhile cliffhanger endings such as the one in Fig-\nure 1 add suspense. Making sense of the complex\n2https://archiveofourown.org\nnarrative shifts associated with chapter transitions\n(e.g., changes in point-of-view, time, location, and\ntheme) requires a deep understanding of the entire\ntext. To maintain global narrative coherence, My-\ners et al. (1994) show that human readers tend to\nreactivate memory about “backgrounded” informa-\ntion from the long-range context.\nTask overview: Given that chapter transitions\nrequires global context understanding, how can we\nturn this into a task to evaluate LRLMs? A simple\napproach is to evaluate the token-level perplexity\nof an LRLM only at chapter boundaries (i.e., on\nthe first n tokens of each chapter); however, the\nvast majority of tokens can be predicted using just\nlocal context (Sun et al., 2021) under the teacher-\nforcing setup, which obscures an LRLM’s usage\nof long-range context as we show in Section 3.\nWe instead turn to the task of suffix identification,\nwhich closely resembles existing datasets such as\nSW AG (Zellers et al., 2018).\nEach instance of our task is defined by a triplet\n(c, s+, s−\ni ∈N), where c is a prefix sequence of\nup to 8K tokens that ends at a chapter break, s+\nis the gold suffix of length 128 tokens (i.e., the\nbeginning of the next chapter), and s−\ni is a neg-\native 128-token-long suffix from a set N of five3\nfuture chapter beginnings sampled from the same\nnarrative.4 All negatives are modified to begin with\nthe same chapter index (e.g., if the gold suffix be-\ngins with “Chapter III”, the chapter indices of all\nnegatives is set to “Chapter III”) to eliminate the\neffect found by Sun et al. (2021) of language mod-\nels memorizing chapter indices in long contexts.\nWe then evaluate whether an LRLM assigns higher\nprobability to the gold suffix P(s+|c) than to all\nnegative suffixes P(s−\ni |c).\nDataset overview: Where do we get these\ntriplets from? We collect a dataset, CHAPTER -\nBREAK , with two splits: CHAPTER BREAK PG19,\nwhich contains 241 examples extracted from the\nPG-19 validation set (Rae et al., 2020), 5 and\nCHAPTER BREAK AO3, which contains 7,355 ex-\n3We use a small number of negatives because it is time-\nconsuming and resource-intensive to evaluate the probabilities\nof long sequences with LRLMs.\n4In Appendix F, we show that in-book negatives are much\nharder than out-of-book negatives as they often contain the\nsame named entities and rare tokens as the gold suffix. Thus,\ndisambiguating the correct suffix requires a deep understand-\ning of the context.\n5We only collect examples from validation set as two base-\nline models in the later sections are trained on PG-19.\n3705\nCategory Definition Pct.\nEvents\nPrevious event ends and new event starts 76%\nPrevious event continues into next chapter 24%\nActors Change of perspective or character in focus 36%\nNo change in POV or main character 64%\nLocations Change of location 68%\nNo change in location 32%\nContinuity\nDiscontinuous but chronological 29%\nContinuous 62%\nAnalepsis 2%\nParallel 6%\nTable 1: Our human annotation on 300 chapter tran-\nsitions randomly sampled from CHAPTER BREAK AO3\nshows the diversity and complexity of the dataset.\namples extracted from an online dump 6 of fan-\nfiction posted on Archive of Our Own (AO3).\nWe apply filtering to remove fanfiction works\nthat are too short or not rated for general au-\ndiences. Each work contains on average 42K\nwords and 21.5 chapters. 7 Even though the\nCHAPTER BREAK PG19 split is small, we include\nit because many LRLMs are pretrained on PG-19;\nthe much larger CHAPTER BREAK AO3 split is out-\nof-distribution for all models that we evaluate. To\nextract chapters in PG-19, we match for lines begin-\nning with the string “chapter”, while AO3 stories\nalready have chapter-level metadata.\nWhat are the different types of transitions in\nCHAPTER BREAK and how often do they occur?\nTo get a better sense of our dataset, we perform a\nfine-grained annotation of 300 randomly-selected\nchapter transitions from CHAPTER BREAK AO3.\nFor each transition, we annotate any changes in the\nfollowing four aspects: events, actors (characters\nin focus), locations, and continuity. To annotate\ncontinuity, we follow a simplified version of the\nscheme proposed by Ireland (1986), 8 which con-\nsiders five categories: continuous (the next chapter\noccurs within a day of the previous chapter), dis-\ncontinuous (the next chapter occurs more than a\nday after the previous chapter), analepsis (the next\nchapter is a “flashback” to an earlier point in the\nnarrative), and parallel (the next chapter reverts\nto the time of a previous chapter, switching the\n6https://archive.org/download/AO3_\nstory_dump_continuing\n7More preprocessing details and statistics can be found in\nAppendix A.\n8To validate our continuity annotations, we also annotate\nevery chapter in Pride and Prejudice and obtain almost the\nsame proportion of continuous transitions (67%) as the number\nreported by the expert annotation of Ireland (1986) (72%).\n#Params Seq Len PPL PG19 AccPG19 AccAO3\nLT 516M 8K 76.8 25% 24%\nRT 490M 8K 72.3 22% 24%\nBigbird 128M 4K 56.2 27% 26%\nGPT-2 1.5B 1K 78.2 23% 24%\nGPT-3 175B 2K - 36% ∗ 28%∗\nSuffixLM 87M 10K - 52% 41%\nTable 2: Summary of LRLMs (top), Transformer LMs\n(middle), and our SuffixLM (bottom). All models are\ntrained or fine-tuned on PG-19 except for GPT-2. The\nthird column shows the word-level perplexity of gold\nsuffix in the PG-19 split. The last two columns show\nthe suffix identification accuracy of each model on the\ntwo CHAPTER BREAK splits when evaluated at maxi-\nmum input length. ∗ indicates results are on a subset of\nCHAPTER BREAK .\n256512 1K 2K 3K 4K 6K 8K\n0.2\n0.3\n0.4\n0.5Suffix Identification Acc.\nPG19\n256512 1K 2K 3K 4K 6K 8K\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nAO3\nSuffixLM\nSuffixLMAO3\nRT\nLT\nBigbird\nGPT-2\nGPT-3\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: Suffix identification accuracy on both splits\n(PG-19 and AO3) of CHAPTER BREAK is much lower\nfor LRLMs than our SuffixLM upper bound.\ncharacter or event in focus).9 The results, shown in\nTable 1, demonstrate that CHAPTER BREAK covers\na diverse array of transitions, including many that\nrequire global narrative understanding.\n3 Experiments\nWe evaluate three different long-range language\nmodels on CHAPTER BREAK and compare their\nresults to those of standard Transformer language\nmodels as well as an upper bound directly trained\nfor suffix prediction.\nLanguage models: We evaluate three LRLMs\npretrained on PG-19: the Local Transformer (Roy\net al., 2021, LT), Routing Transformer (RT) (Roy\net al., 2021, RT), and BigBird (Zaheer et al., 2020).\nThe BigBird model is the decoder part of the re-\nleased checkpoint fine-tuned with causal LM ob-\njective on 14k books of PG-19 for 100k steps. We\nalso evaluate two standard Transformer language\nmodels, GPT-2 large (Radford et al., 2019) and\nGPT-3 (Brown et al., 2020).10 We summarize these\n9Appendix B contains more details about each category.\n10Due to OpenAI’s API costs for GPT-3, we only evaluate\nin total a subset of 200 examples instead of the full dataset.\n3706\nmodels in Table 2, more details about each model\nare included in Appendix C.\nAn upper bound directly trained for suffix iden-\ntification: As authors often write stories that are\nintended to surprise readers, it is possible that many\nexamples in CHAPTER BREAK are ambiguous by\nnature (i.e., the upper bound for suffix identification\naccuracy may not be 100%). To obtain a reasonable\nupper bound, we also train a model (SuffixLM) di-\nrectly on the suffix identification task by scaling\nup the sentence-level language model proposed\nby Ippolito et al. (2020).11 We divide an input se-\nquence into multiple segments, each of which is\nembedded via the [CLS] vector of a small fine-\ntuned RoBERTa network (Liu et al., 2019). Our\nSuffixLM then performs “language modeling” atop\nthe dense [CLS] vectors, predicting the next seg-\nment representation given the representations of\nprevious segments via contrastive predictive cod-\ning (van den Oord et al., 2018). 12 Formally, our\nSuffixLM minimizes the following loss:\nLi = −log exp( ˆzi⊤z+\ni )\n∑\nzi∈{z+\ni ,Z−\ni }exp( ˆzi⊤zi)\nwhere ˆzi is the predicted representation by Suf-\nfixLM, z+\ni is the gold suffix representation obtained\nfrom a small encoder (RoBERTa), and Z−\ni is the\nset of dense representations of the negatives. More\ndetails about our SuffixLM are included in Ap-\npendix D.\n4 Results & Analysis\nOverall, the results in Table 2 (rightmost two\ncolumns) confirm that all of the language models\nstudied in this paper struggle on CHAPTER BREAK ,\nespecially when compared to the SuffixLM upper\nbound, which outperforms the best LM by ∼25%\nabsolute accuracy when evaluated on the entire PG-\n19 split. We describe other interesting results and\nanalysis below:\nAccuracy increases with longer prefixes: Fig-\nure 2 shows that as prefix sequence length in-\ncreases, some LRLMs (e.g., LT) barely improve,\nwhile others show modest improvements (e.g.,\n11Our SuffixLM can process up to 10K tokens, while the\nmodel of Ippolito et al. (2020) supports only up to ten sen-\ntences.\n12Our SuffixLM is closely related to the model in Ainslie\net al. (2020), but differs crucially by predicting the representa-\ntion of next segment instead of summaries.\n256512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.4\n0.5\n0.6\n0.7\n0.8Suffix Identification Acc.\ncause\ndialogue\nch. breaks\n256512 1K 2K 3K 4K 6K 8K\nSequence Length\n50\n60\n70\n80\n90Suffix Perplexity\nRT\nLT\nBigbird\nGPT-2\nFigure 3: Left: Prefixes ending at chapter breaks benefit\nmore from long-range context than other types of dis-\ncourse boundaries. Right: Word-level perplexity of the\ngold suffix does not correlate to accuracy (e.g., GPT-2\nhas high perplexity but outperforms RT on suffix identi-\nfication).\nGPT-3 and fine-tuned BigBird). However, all\nLRLMs significantly underperform our SuffixLM\nupper bound, even when the SuffixLM is given pre-\nfixes that are only 256 tokens long. Additionally,\nSuffixLM’s accuracy increases far more than those\nof LRLMs when increasing the prefix length (from\n31% at prefix length of 256 to 46% at 8K on the\nAO3 split13). This result suggests that the token-\nlevel LRLMs evaluated in our work are not taking\nfull advantage of information in the long-range con-\ntext to solve CHAPTER BREAK .\nPerplexity does not always correlate with accu-\nracy: Previous LRLM efforts use validation per-\nplexity (e.g., on PG-19) to compare against other\nmodels. However, we show that perplexity is not by\nitself a predictor of suffix identification accuracy:\nAs shown in Table 2, GPT-2 achieves higher accu-\nracy than RT despite yielding a word-level perplex-\nity of 78.2 on gold suffixes, compared to 72.3 for\nRT.14 We advocate that future research on LRLMs\nincludes evaluation on suffix identification tasks\nlike CHAPTER BREAK , as perplexity alone does not\nreflect LRLMs’ capabilities to model long-range\ndependencies.\nWhy chapter breaks over other discourse bound-\naries? Other discourse markers, including cause\nand dialogue, also often prompt human readers to\nreactivate memories of global context (Albrecht\n13We collected 13,682 fan-fictions posted on AO3 and fine-\ntuned our SuffixLM on subset of this dataset to be the model\nSuffixLMAO3. More details about the filtered AO3 works are\nincluded in Appendix A\n14As these models use different tokenizers, we normalize\nthe subword-level perplexities to the word level as suggested\nby Rae et al. (2020). More details about this can be found in\nAppendix E.\n3707\nand Myers, 1995). We create suffix identifica-\ntion datasets for these two discourse markers by\nstring matching over corresponding cue phrases\n(‘because’, ‘due to’ for the cause subset and text\nwithin quotation marks for dialogue).15 Figure 3\n(left) shows that with prefixes of length 256 tokens,\nour SuffixLM is able to successfully disambiguate\nthe correct suffixes for both discourse markers more\nthan 80% of the time, while the accuracy is much\nlower at chapter boundaries. As the prefix length in-\ncreases, accuracy only slightly increases for cause\nand dialogue, especially compared to the robust\nimprovement at chapter boundaries.16\nShort-context Transformers are comparable to\nLRLMs: Our results show that GPT-2, despite\nits high perplexity on gold suffixes and short maxi-\nmum sequence length (1024 tokens), achieves com-\nparable performance to RT and LT on both splits.\nMeanwhile, GPT-3 achieves much higher perfor-\nmance on both CHAPTER BREAK at a sequence\nlength of 2,048 tokens, and the increasing GPT-\n3 curve in Figure 2 is promising for future work\nscaling LMs to longer sequence lengths.\nLimitations of our work: While we have used\nthe SuffixLM as an upper bound in this paper\nand demonstrated that it substantially outperforms\nLRLMs on CHAPTER BREAK , a more compelling\ncomparison would include human performance on\nour task at varying prefix lengths, especially since\nsome chapter transitions are specifically intended\nby their authors to be unpredictable. However, ob-\ntaining reliable human performance numbers is\nvery difficult, as it requires in-depth comprehen-\nsion of long narratives on the part of workers. Due\nto the time-consuming nature of this task and its\nhigh cognitive demand, it is not possible (within a\nreasonable budget) to use crowdsourcing, as ensur-\ning that the annotators fully read the prefix instead\nof skimming or ignoring it is a major challenge.\nThese issues also carry over to experiments per-\nformed with in-person subjects. As such, we leave\na thorough human evaluation on CHAPTER BREAK\nto future work.\n5 Related Work\nOur work depends heavily on recent advances in\nefficient Transformers (Tay et al., 2020) that pro-\n15Appendix A contains more details about data for these\ntwo discourse markers.\n16Appendix G shows similar trends on cause and dialogue\nwith other models.\ncess long sequences (Rae et al., 2020; Beltagy\net al., 2020; Zaheer et al., 2020; Ainslie et al.,\n2020; Roy et al., 2021). Sparse attention (Child\net al., 2019), relative position encoding (Shaw\net al., 2018; Raffel et al., 2020; Guo et al., 2021),\nrecurrence mechanism and memory (Dai et al.,\n2019; Weston et al., 2015; Hutchins et al., 2022; ?)\nand other tricks (Shen et al., 2020; Katharopoulos\net al., 2020; Gupta and Berant, 2020; Stock et al.,\n2021; Yogatama et al., 2021; Borgeaud et al., 2021;\nHawthorne et al., 2022) are commonly adopted by\nrecent Transformer variants to make the operation\non long sequences more time/memory efficient.\nBesides perplexity, many downstream extrin-\nsic tasks for evaluating long-range language mod-\nels were developed recently , such as long-form\nQA (Fan et al., 2019; Pang et al., 2021), document-\nlevel summarization (Kry ´sci´nski et al., 2021;\nHuang et al., 2021), and machine translation (Liu\nand Zhang, 2020). More recently, Shaham et al.\n(2022) introduce a new benchmark covering mul-\ntiple domains and tasks, while Tay et al. (2021)\npropose multimodal long sequence tasks.\n6 Conclusion\nWe introduce CHAPTER BREAK , a suffix identifi-\ncation dataset targeted at evaluating the discourse-\nlevel understanding of long-range language models.\nThe dataset is extracted from long-form narratives\nand covers a variety of complex chapter transitions,\nsuch as shifts in location and events in focus. Exper-\niments show that existing LRLMs perform poorly\non CHAPTER BREAK and much worse than a Suf-\nfixLM trained as an upper bound on this task. We\nrelease the dataset to spur more principled develop-\nment of future LRLMs.\nAcknowledgements\nWe thank the anonymous reviewers and UMass\nNLP group for the thoughtful comments on the\ndraft of this paper. We are grateful to AO3 Support\nChair and volunteers for answering data related\nquestions. This work was supported by awards\nIIS-1955567 and IIS-2046248 from the National\nScience Foundation (NSF).\nEthical Considerations\nCHAPTER BREAK is constructed from two sources:\npublic domain books published prior to 1919 (from\nthe held-out set of PG-19) and works of fanfiction\nextracted from an online dump of stories posted\n3708\non Archive of Our Own (AO3). We refer readers\nto Rae et al. (2020) for more details about PG-19.\nFor AO3, we apply multiple filters to obtain long\nfanfiction stories rated as suitable for “General Au-\ndiences”. We refer readers to Appendix A for more\npreprocessing details. More generally, this work fo-\ncuses on long-range language models, which could\npotentially be misused to generate offensive out-\nput. However, the main purpose of this paper is to\npresent a dataset which provides a better evaluation\nof the discourse-level capabilities of such models.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured inputs in\ntransformers.\nJason E. Albrecht and Jerome L. Myers. 1995. Role of\ncontext in accessing distant information during read-\ning. Journal of experimental psychology. Learning,\nmemory, and cognition, 21 6:1459–68.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\narXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering.\nHenry Fielding. 1779. The History of the Adventures of\nJoseph Andrews.., volume 1. J. Fr. Valade.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2021. Longt5: Efficient text-to-text transformer for\nlong sequences.\nAnkit Gupta and Jonathan Berant. 2020. Gmat: Global\nmemory augmentation for transformers. arXiv\npreprint arXiv:2006.03274.\nCurtis Hawthorne, Andrew Jaegle, C ˘at˘alina Cangea,\nSebastian Borgeaud, Charlie Nash, Mateusz Mali-\nnowski, Sander Dieleman, Oriol Vinyals, Matthew\nBotvinick, Ian Simon, et al. 2022. General-purpose,\nlong-context autoregressive modeling with perceiver\nar. arXiv preprint arXiv:2202.07765.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1419–1436, Online.\nAssociation for Computational Linguistics.\nDeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu,\nEthan Dyer, and Behnam Neyshabur. 2022. Block-\nrecurrent transformers. ArXiv, abs/2203.07852.\nDaphne Ippolito, David Grangier, Douglas Eck, and\nChris Callison-Burch. 2020. Toward better storylines\nwith sentence-level language models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7472–7478, Online.\nAssociation for Computational Linguistics.\nKR Ireland. 1986. Towards a grammar of narrative se-\nquence: The model of the french lieutenant’s woman.\nPoetics today, 7(3):397–420.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Francois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning.\nWojciech Kry´sci´nski, Nazneen Rajani, Divyansh Agar-\nwal, Caiming Xiong, and Dragomir Radev. 2021.\nBooksum: A collection of datasets for long-form\nnarrative summarization.\nSiyou Liu and Xiaojun Zhang. 2020. Corpora for\ndocument-level neural machine translation. In Pro-\nceedings of the 12th Language Resources and Evalua-\ntion Conference, pages 3775–3781, Marseille, France.\nEuropean Language Resources Association.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\n3709\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nJerome Myers, Edward O’Brien, Jason Albrecht, and\nRobert Mason. 1994. Maintaining global coherence\nduring reading. Journal of Experimental Psychology:\nLearning, Memory, and Cognition, 20:876–886.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He,\nand Samuel R. Bowman. 2021. Quality: Question\nanswering with long input texts, yes!\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using shorter\ninputs. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 5493–5505, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learning\nRepresentations.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics, 9:53–\n68.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nScrolls: Standardized comparison over long language\nsequences.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. Proceedings of the AAAI\nConference on Artificial Intelligence, 34(05):8815–\n8821.\nPhilip Stevick. 1970. The Chapter in Fiction: Theo-\nries of Narrative Division. Syracuse, NY: Syracuse\nUniversity Press, c1970.\nPierre Stock, Angela Fan, Benjamin Graham, Edouard\nGrave, Rémi Gribonval, Herve Jegou, and Armand\nJoulin. 2021. Training with quantization noise for\nextreme model compression. In International Con-\nference on Learning Representations.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\nguage models actually use long-range context? In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 807–\n822, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transformers.\nIn International Conference on Learning Representa-\ntions.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efficient transformers: A survey.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. ArXiv, abs/1807.03748.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2015.\nMemory networks.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93–104, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\n3710\nA Dataset statistics\nWe collected 13,682 fanfictions from an online\ndump of stories posted on Archive of Our Own\n(AO3) by filtering works written in English lan-\nguage, rated General Audience by the author\nand contains at least 10K words and more than\n10 chapters. For each chapter, we remove\nthe text within the range of “ **Notes for\nthe Chapter:**”, “**Summary for the\nChapter:**” and “**Author’s Note:**”.\nThe meta-comments inserted into the main text by\nthe authors are not removed. The statistics of this\nlong-fic dataset are included in Table 3. We do not\napply other profanity filters to the fictions, there-\nfore there may still be inappropriate content for\ngeneral audience as the rating is self-labeled by\neach author. Besides chapter breaks introduced in\nthe main text, we also collected two other discourse\nboundaries, cause and dialogue, as comparisons to\nthe chapter boundary examples. We present the\nstatistics each type of examples in Table 4.\n• Cause: The beginning of the suffix contains\nwords or phrases ‘because’, ‘due to’, ‘owing\nto’. According to (Albrecht and Myers, 1995),\nhuman readers reactivate memory of global\ncontext for comprehending statements follow-\ning causes or goals.\n• Dialogue: The gold suffix in this category\nstarts with a quotation mark. This often hap-\npens in dialogues where the continuation of\none interlocutor depends heavily on the im-\nmediately preceding utterance. We conjecture\nthis is the type where the prediction relies\nmore on the local rather than the global con-\ntext.\nmean min max\n#chapters 21.5 11 589\n#words 41,513.2 10,000 636,468\nTable 3: Statistics of long fanfictions collected from\nAO3 story dump.\nB Annotation Scheme\nWe annotate each chapter transition from four as-\npects: events, actors (point-of-view or characters\nin focus), location, and continuity in timeline.\nAO3 PG19\nSuffix Type #works #examples #works #examples\ncause 965 8,133 45 506\ndialogue 979 8,724 46 3,165\nchapter breaks 1202 7,355 17 241\nTable 4: Data statistics of CHAPTER BREAK as well as\nanother two discourse boundary examples.\nEvents We define two subcategories based on\nwhether (1) previous event ends in the previous\nchapter and new event starts in the new chapter, (2)\nold event does not end and continues into the next\nchapter.\nActors We define two subcategories based on\nwhether there is a shift in POV or main character\nin focus.\nLocation We define two subcategories based on\nwhether the location described in the prefix and in\nthe new chapter is different.\nContinuity Following Ireland (1986)’s work, we\ncategorize the chapter transition by timeline conti-\nnuity into four subcategories:\n• Discontinuous but chronological: Reusing\nthe standard by Ireland (1986), discontinuous\nrepresents a gap in time forward for more than\none night.\n• Continuous: The time interval between chap-\nters lasts for no more than one night.\n• Analepsis: Analepsis represents retrospective\nevocation of an event, or “flashback” to an\nearlier point in the narrative.\n• Parallel: This includes timeline reverting\nback to the time of any previous chapter, typi-\ncally accompanied by switching character in\nfocus or description of a separate set of events\nindependent of the last chapter. This category\nis a collapse of “alternate phase”, “parallel\nphase” and “simultaneous phase” introduced\nin (Ireland, 1986).\nC Baselines\nBigbird (Zaheer et al., 2020) To reduce the\nquadratic complexity of self-attention in the stan-\ndard Transformer, the Bigbird model employs a\nmixture of global, random and local attention mech-\nanisms, which successfully reduce the complexity\n3711\nto linear. The idea is to insert each sequence O(1)\nglobal tokens, which attend to all other tokens. The\nrest tokens attend to their neighbor tokens, random\ntokens in the sequence as well as the inserted global\ntokens. A very similar idea is developed concur-\nrently in the Longformer (Beltagy et al., 2020). The\nBigbird model we fine-tuned is the decoder part of\nthe released checkpoint. We fine-tune the model\nwith causal LM objective on 14K books of PG-19\nwith peak learning rate 0.0001 for 100K steps. We\nset attention type to be “original_full” instead of\nusing “block_sparse” during fine-tuning. Training\nis completed on a single RTX8000 GPU for around\n6 days.\nLocal Transformer Rather than implementing\nall three types of sparse attention in Bigbird, the\nLocal Transformer relies only on the local attention,\ni.e., each token attends to neighbors within a local\nwindow. The maximum attainable sequence length\nscales linearly with the number of layers, e.g., with\nwindow size k, the token representation at layer l\ntheoretically covers information in a range of k ×l\ntokens.\nRouting Transformer (Roy et al., 2021) Dif-\nferent from previously described models which\nuse position-based sparse attention, the Routing\nTransformer employs content-based sparse atten-\ntion. Namely, each token are routed to clusters and\nthe attention is performed only within each clus-\nter. The clustering operation effectively reduces the\nquadratic complexity in length L to O(L1.5). Both\nthe RT and LT checkpoint we used were trained on\nPG-19 (Rae et al., 2020). For both RT and LT, we\nevaluate on single RTX8000 GPU.\nGPT-2/3 The GPT models have a lot shorter max-\nimum input length than the rest models we eval-\nuated. While GPT-2 model does not use sparse\nattentions at all, GPT-3 model adopts alternated lay-\ners of sparse and dense self-attention. We use the\nGPT-2 large model, which was pre-trained on data\nscraped from the Internet. The GPT-3 model was\npre-trained on a mixture of filtered CommonCrawl,\nWebText2, Books1, Books2, and Wikipedia.\nD Finding the best SuffixLM\nAs there are no prior long-range segment-level LM\narchitectures that we can borrow from, we experi-\nment multiple design choices and report the result\nof only the best performing one in the main text.\nFor all variants, we use RoBERTa-base (Liu et al.,\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.2\n0.3\n0.4\n0.5Suffix Identification Accuracy\nPG19 Chapter Breaks\nSuffixLM-A\nSuffixLM-B\nSuffixLM-C\nSuffixLM-D\nSuffixLM-E\nFigure 4: Performance of each SuffixLM variant. De-\ntailed information about each variant is included in Ap-\npendix D.\n2019) as the encoder to obtain the encoded seg-\nment representation. This is done by extracting\nthe representation of the [CLS] token prepended at\nthe beginning of each sequence. We describe five\nvariants below.\n• SuffixLM-A This variant contains a frozen\nRoBERTa-base encoder and a SuffixLM using\na 6-layer Transformer as the base architecture.\n• SuffixLM-B This variant contains a frozen\nRoBERTa-base encoder and a SuffixLM us-\ning a 6-layer average-attention Transformer\nas the backbone. The motivation of using uni-\nform distribution for attention weights is to\nencourage the model to get more information\nfrom the distant context rather than rely too\nmuch on local context.\n• SuffixLM-C This variant is essentically\nSuffixLM-A but during training we perform\n“segdrop” – stochastically dropping prefix seg-\nments with probability 0.217 when performing\nself-attention. When the local segments are\ndropped, the model has to predict the next seg-\nments with only the distant context, which\nalso encourages learning better long-range\nprefix representations.\n• SuffixLM-D Instead of freezing the encoder,\nthis variant fine-tunes part of the encoder and\nthe rest is the same as SuffixLM-A. Due to\nlimited memory capacity, we only fine-tune\nthe last two layers of the RoBERTa-base.\n• SuffixLM-E This model is the same as\nSuffixLM-D except that we truncate the en-\n17Tried {0.1, 0.2, 0.4}, 0.2 works the best.\n3712\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.2\n0.3\n0.4\n0.5Suffix Identification Accuracy\nchapter breaks\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ncause\nSuffixLM\nRT\nLT\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.4\n0.5\n0.6\n0.7\n0.8\ndialogue\nBigbird\nGPT-2\nEvaluation on PG19\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Suffix Identification Accuracy\nchapter breaks\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ncause\nSuffixLMAO3\nSuffixLM\nRT\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.4\n0.5\n0.6\n0.7\n0.8\ndialogue\nLT\nBigbird\nGPT-2\nEvaluation on AO3\nFigure 5: Evaluation results on both CHAPTER BREAK PG19 and CHAPTER BREAK AO3.\ncoder to just the two tunable layers and train\nall parameters in the encoder including the\nembedding parameters.\nAll SuffixLMs with frozen encoders are trained\nwith average sequence length of 10240 tokens for\nup to 60k steps, and the one with trainable encoder\nis trained for max 120k steps. The dimension of\nthe model is 768, hidden dimension 2048,attention\nheads 8. The peak learning rate is 0.0001 with\nwarm up steps 4000. We train SuffixLM on entire\nPG-19 dataset and evaluate the best checkpoint se-\nlected by dev loss. We use segment size 128 in all\nSuffixLMs we trained. Each segment starts from a\nnew sentence, if not reaching 128 tokens, we pad\nwith a special ‘<pad>’ token. For very long sen-\ntences, the part exceeding 128 tokens overflows\nto the next segment. We plot the suffix identifica-\ntion accuracy of each variant on CHAPTER BREAK\nwhile feeding in prefixes of increasing length. As\nshown in Figure 4, SuffixLM-E outperforms all\nother variants across various prefix lengths. There-\nfore in the main text, all SuffixLM refers to the\nSuffixLM-E variant. Note that one limitation of\nSuffixLM is it exclusively models on segment-level,\nwhich prohibits it from performing token-by-token\ngeneration and thus impossible for us to evaluate\nperplexity.\nE Suffix perplexity\nAlthough the task of CHAPTER BREAK is to iden-\ntify gold suffix from negatives, we also present the\ngold suffix perplexity of next-token prediction LMs.\nNote that all models were trained or fine-tuned on\nPG-19 except for GPT-2/3. As these models use dif-\nferent tokenizers, the 128-token suffix may cover\ndifferent number of words, to make the results com-\nparable, we convert the subword-level perplexity\nto word-level by multiplying a constant to the log\nprobability value of each model. For RT/LT, we\nmultiple by 1.248 as used in the official reposi-\ntory. We multiply the value by 1.30 for GPT-2, and\n1.22 for Bigbird. These values are estimated via\nthe subword/word ratio on validation set of PG-19.\nOur fine-tuned Bigbird model achieves the low-\nest perplexity on PG-19, even better than Routing\nTransformer or Local Transformer. This implies\nthat context from long-range is not necessary for\nachieving low perplexity since the maximum input\nlength of Bigbird is half that of RT/LT.\nF In-book vs. Out-of-book\nThis section is better read after reading through § 3.\nIn this analysis experiment, we show why it is bet-\nter that the negatives are from the same narrative\nas the gold suffix. We evaluate our upper bound\n3713\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Suffix Identification Acc.\nPG19 Chapter Breaks\nin-book\nout-of-book\nout-of-bookAO3\n256 512 1K 2K 3K 4K 6K 8K\nSequence Length\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50Suffix Identification Acc.\nPG19 Chapter Breaks\nslen-100\nslen-110\nslen-120\nslen-128\nRT\nFigure 6: Left: In-book vs. out-of-book. Right: Suf-\nfixLM performance when evaluated with different suffix\nlength. The variation in suffix length does not explain\nthe large gap between SuffixLM and token-level LMs.\nmodel SuffixLM on PG-19 set when the negatives\nare out-of-book suffixes, and plot the suffix identifi-\ncation accuracy in Figure 6. When evaluate against\nout-of-book negatives, this suffix identification task\nis almost solved by our SuffixLM, especially when\nthe out-of-book examples are from another split\nin CHAPTER BREAK . The extremely high accu-\nracy under out-of-book setup suggests the segment\nrepresentation from different books are easy for\nSuffixLM to distinguish, thus we adopt a harder\nsetup where the negatives are from the same book.\nBesides, in-book negatives may contain the same\nre-occurring named entities or rare words, which\nrequire solid understanding of the prefix to differ-\nentiate the gold from the distractors.\nG Various Discourse Relationships\nIn addition to chapter breaks, we also evaluate the\nother two types of discourse boundary examples in-\ntroduced in Appendix A. As shown in Figure 5, for\nall suffix types other than chapter breaks, the evalu-\nated models stop improving as the sequence length\ngrows to more than 2K tokens long. However, there\nis a significant increasing trend in chapter breaks\nfor SuffixLM. For the rest models, the performance\nis either flat or not improving. On the AO3 split,\nthe accuracy of SuffixLM improves for ∼ 15%\nas the sequence length increases from 256 to 8K,\nwhereas the improvement of RT is only ∼1.4%.\nThis is in contrast with SuffixLM’s ∼1.5% and\nRT’s ∼0.3% improvement for the ‘cause’ exam-\nples. We draw two conclusions from these observa-\ntions: (1) the chapter breaks examples form a spe-\ncial case where longer prefix is preferred in order to\npick the correct continuation. (2) By comparing the\nrelative improvement, the token-level LMs fall far\nbehind the SuffixLM, which is, besides the abso-\nlute performance gap, another evidence that current\nLRLMs do not effectively leverage long-range con-\ntext for sequence tasks requiring discourse-level\nunderstanding.\nH Tackle difference in Tokenizers\nAs the models we evaluated use different tokeniz-\ners, there are small variations in term of suffix\nlength, i.e., the 128-token suffix may cover dif-\nferent number of words. To understand how the\ndifference in length impacts validity of evaluation,\nwe evaluate SuffixLM with various suffix lengths.\nFigure 6 (right) indicates even though there are\nsmall variances when the suffixes are of different\nlengths, the large gap between SuffixLM and Rout-\ning Transformer still remains, thus the difference\nin suffix length does not explain the large perfor-\nmance gap.\nI Error analysis\nModels struggle with location and event shifts:\nAmong the 300 examples we annotated in Sec-\ntion 2, 89 examples were wrongly predicted by all\nmodels we have evaluated. By breaking the in-\ncorrectly predicted examples into category as pre-\nsented in Table 1, we find that models tend to make\nwrong prediction when there is a shift in location or\nevent, and when plots are continuous in timeline.18\nCategory Definition Ratio\nEvents\nPrevious event ends and new event starts 0.74\nPrevious event continues into next chapter 0.26\nActors Change of perspective or character in focus 0.43\nNo change in POV or main character 0.57\nLocations Change of location 0.64\nNo change in location 0.36\nContinuity\nDiscontinuous but chronological 0.24\nContinuous 0.62\nAnalepsis 0.03\nParallel 0.11\nTable 5: Human annotation on 89 examples sampled\nfrom CHAPTER BREAK AO3where all models make the\nwrong prediction. 74% errors come from the examples\nwhere new event starts from the new chapter and 64%\nerrors from the change of location.\n18Detailed numbers are included in Appendix I.\n3714",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.8715390563011169
    },
    {
      "name": "Computer science",
      "score": 0.7880007028579712
    },
    {
      "name": "Narrative",
      "score": 0.7341586351394653
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5544940233230591
    },
    {
      "name": "Natural language processing",
      "score": 0.5513114333152771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5143128633499146
    },
    {
      "name": "Annotation",
      "score": 0.5062676668167114
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4866417348384857
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4646304249763489
    },
    {
      "name": "Ground truth",
      "score": 0.46058031916618347
    },
    {
      "name": "Language model",
      "score": 0.4271204173564911
    },
    {
      "name": "Data science",
      "score": 0.4178534150123596
    },
    {
      "name": "Linguistics",
      "score": 0.15009641647338867
    },
    {
      "name": "Programming language",
      "score": 0.11269822716712952
    },
    {
      "name": "History",
      "score": 0.08520832657814026
    },
    {
      "name": "Engineering",
      "score": 0.06327894330024719
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ],
  "cited_by": 5
}