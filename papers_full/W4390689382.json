{
  "title": "Multi-label classification of retinal disease via a novel vision transformer model",
  "url": "https://openalex.org/W4390689382",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2057628437",
      "name": "Dong Wang",
      "affiliations": [
        "Shandong Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108653094",
      "name": "Jian Lian",
      "affiliations": [
        "Shandong Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2147819313",
      "name": "Wanzhen Jiao",
      "affiliations": [
        "Shandong First Medical University",
        "Shandong Provincial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2057628437",
      "name": "Dong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108653094",
      "name": "Jian Lian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147819313",
      "name": "Wanzhen Jiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2100756624",
    "https://openalex.org/W4300715911",
    "https://openalex.org/W4295766575",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4283711801",
    "https://openalex.org/W4296131005",
    "https://openalex.org/W6697326442",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2804351871",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W4283030623",
    "https://openalex.org/W3088750190",
    "https://openalex.org/W4295957092",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3081669380",
    "https://openalex.org/W2971342434",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4378966461",
    "https://openalex.org/W4377866341",
    "https://openalex.org/W3163004656",
    "https://openalex.org/W3133683246",
    "https://openalex.org/W2576544903",
    "https://openalex.org/W3138933733",
    "https://openalex.org/W2769965838",
    "https://openalex.org/W4393554243",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6807936091",
    "https://openalex.org/W3121092655",
    "https://openalex.org/W4214655055",
    "https://openalex.org/W4304758328",
    "https://openalex.org/W2299944668",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W4205297894",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W3196436670",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W4220680381",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2953417534",
    "https://openalex.org/W3027868112",
    "https://openalex.org/W3165364278",
    "https://openalex.org/W4285285265",
    "https://openalex.org/W3198370788",
    "https://openalex.org/W2293842029",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4301045096",
    "https://openalex.org/W4211239947"
  ],
  "abstract": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
  "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/eight.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nOPEN ACCESS\nEDITED BY\nJeroen Goossens,\nRadboud University Medical Centre,\nNetherlands\nREVIEWED BY\nFeifei Wang,\nThe University of Hong Kong,\nHong Kong SAR, China\nBin Yang,\nTaizhou University, China\n*CORRESPONDENCE\nJian Lian\n/one.tnum/four.tnum/four.tnum/three.tnum/eight.tnum/one.tnum/two.tnum/zero.tnum/two.tnum/zero.tnum/zero.tnum/six.tnum/eight.tnum/one.tnum@sdmu.edu.cn\nWanzhen Jiao\nzhener/one.tnum/zero.tnum/zero.tnum/three.tnum@/one.tnum/six.tnum/three.tnum.com\nRECEIVED /two.tnum/zero.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/eight.tnum December /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /zero.tnum/eight.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nWang D, Lian J and Jiao W (/two.tnum/zero.tnum/two.tnum/four.tnum) Multi-label\nclassiﬁcation of retinal disease via a novel vision\ntransformer model.\nFront. Neurosci./one.tnum/seven.tnum:/one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Wang, Lian and Jiao. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nMulti-label classiﬁcation of retinal\ndisease via a novel vision\ntransformer model\nDong Wang/one.tnum, Jian Lian /two.tnum* and Wanzhen Jiao /three.tnum*\n/one.tnumSchool of Information Science and Electrical Engineering, Shandon g Jiaotong University, Jinan, China,\n/two.tnumSchool of Intelligence Engineering, Shandong Management Universit y, Jinan, China, /three.tnumDepartment of\nOphthalmology, Shandong Provincial Hospital Aﬃliated to Shandong First Medical University, Jinan,\nChina\nIntroduction: The precise identiﬁcation of retinal disorders is of utmost\nimportance in the prevention of both temporary and permanent vi sual impairment.\nPrior research has yielded encouraging results in the classiﬁcat ion of retinal images\npertaining to a speciﬁc retinal condition. In clinical practice, it i s not uncommon\nfor a single patient to present with multiple retinal disorder s concurrently. Hence,\nthe task of classifying retinal images into multiple labels r emains a signiﬁcant\nobstacle for existing methodologies, but its successful accompli shment would\nyield valuable insights into a diverse array of situations s imultaneously.\nMethods: This study presents a novel vision transformer architecture calle d\nretinal ViT, which incorporates the self-attention mechanism i nto the ﬁeld of\nmedical image analysis. To note that this study supposed to pr ove that the\ntransformer-based models can achieve competitive performance co mparing with\nthe CNN-based models, hence the convolutional modules have bee n eliminated\nfrom the proposed model. The suggested model concludes with a mu lti-label\nclassiﬁer that utilizes a feed-forward network architecture. Th is classiﬁer consists\nof two layers and employs a sigmoid activation function.\nResults and discussion: The experimental ﬁndings provide evidence of the\nimproved performance exhibited by the suggested model when com pared to\nstate-of-the-art approaches such as ResNet, VGG, DenseNet, an d MobileNet,\non the publicly available dataset ODIR-/two.tnum/zero.tnum/one.tnum/nine.tnum, and the proposed approach has\noutperformed the state-of-the-art algorithms in terms of K appa, F/one.tnum score, AUC,\nand AVG.\nKEYWORDS\nretinal image, deep learning, multi-label classiﬁcation, ma chine vision, medical image\nanalysis\n/one.tnum Introduction\nThe retina, as a fundamental component of the ocular system, plays a crucial role in\nfacilitating human visual function. The retina is situated at the posterior region of the eye\nand plays a crucial role in converting incoming light into electrical impulses. These signals\nare subsequently transmitted by the optic nerve to the brain (\nYokomizo et al., 2019 ). Based\non the inherent characteristics of the retina, it possesses the capacity to serve as an indicator\nfor ocular ailments as well as many physiological conditions, including but not limited to\ndiabetes and neurological disorders (\nMontesano et al., 2021 ; Zhou et al., 2021 ).\nTaking advantage of fundus retina imaging evaluation can reveal many retinal illnesses,\nsuch as diabetes retinopathy (DR), glaucoma, and age-related macular degeneration (AMD).\nIt is important to acknowledge that a signiﬁcant number of individuals residing in Asian\nFrontiers in Neuroscience /zero.tnum/one.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\ncountries such as China and India are experiencing the adverse\neﬀects of DR (\nAyoub et al., 2022 ). In the ﬁeld of ophthalmology,\nglaucoma has emerged as a prevalent cause of enduring visual\nimpairment (\nMokhles et al., 2017 ; Sun et al., 2022 ). According to\nSchmitz-Valckenberg et al. (2016), AMD is widely acknowledged\nas the primary cause of complete vision impairment among\nindividuals aged 50 and beyond. The precise identiﬁcation of\nretinal lesions has the potential to enhance the timely detection\nand subsequent treatment of ocular illnesses. Early detection of\nretinal lesions has the potential to delay the progression of visual\nimpairment resulting from degenerative disorders. Consequently,\nearly diagnosis can also contribute to the advantageous outcomes\nof quick treatment.\nAutomatic machine vision-aided diagnosis system has attracted\nbroadly attention from both clinical and academic ﬁelds (\nAbràmoﬀ\net al., 2010 ). It can mitigate the burden of ophthalmologists\nby avoiding the time-consuming, labor-tedious, and error-prone\nmanual inspections. In addition, the employment of automated\nretinal image analysis can further eliminate the variability of\nimage interpretation even when there are insuﬃcient number\nof specialists of retinal image analysis (\nMokhashi et al., 2021 ).\nBefore the powerful deep learning methods have been proposed,\na large number of machine learning-based retinal image analysis\nalgorithms have been exploited in this area. As an early work\nof branch retinal vein occlusion (BRVO),\nChen et al. (2014)\nproposed the hierarchical local binary pattern (LBP) to represent\nthe characteristics of the fundus image. A BRVO dataset was\nconstructed, and the comparison experiments were conducted\nusing the images in this dataset. In the work of retinal image\nclassiﬁcation (\nKumudham, 2015 ), Kumudham used the LBP\nfeatures extracted from the hard exudate regions in retinal images\nand a support vector machine (SVM) classiﬁer. Accordingly, each\nretinal image can be classiﬁed into normal and abnormal cases\nfor diabetic macular edema (DME).\nKothare and Malpe (2019)\nproposed an empirical framework consisting of requisite number\nof images and a group of methods to predict the possibility of\nDR. These methods include SVM and naive Bayes (NB) as the\nclassiﬁers as well as the LBP for feature extraction. To discriminate\nthe presence of DR and grade the severity of DR in retinal images\nwithout lesion segmentation,\nBerbar (2022) ﬁrst employed the pre-\nprocessing techniques, including histogram matching and median\nﬁlter, to the green channels of retinal images. Then, the contrast-\nlimited adaptive histogram equalization was leveraged as well as\nthe unsharp ﬁlter, to note that each image was segmented into\nsmall patches, from which the LBP features were generated. In\naddition, an SVM was taken as the classiﬁer to implement the\nretinal image classiﬁcation. In general, the study of\nBerbar (2022)\ncan grade the severity of DR into three diﬀerent levels. Recently,\nthe study of\nReddy and Ravindran (2022) presented an automatic\nscreening platform to recognize DR in retinal images. The proposed\nclassiﬁcation scheme consists of two phases. In the ﬁrst step, the\nretinal images were divided into four regions, namely, hard exudate,\nmicroaneursym, hemorrhage, and cotton wool spot. Second, three\nclassiﬁers, such as k-nearest neighbor (KNN), gaussian mixture\nmodel (GMM), and SVM, were exploited to realize retinal image\nclassiﬁcation and DR severity grading. The classical machine\nlearning methods rely heavily on the manually designed features\nextracted from the retinal images and an appropriate classiﬁer.\nHowever, according to the complicated characteristics of the retinal\nimages and the variation of illuminations, it remains a challenge\nto determine the optimal set of feature and the parameters of one\nclassiﬁer in a manual fashion.\nOn the other hand, the deep learning-based architectures have\nachieved more promising outcomes than the machine learning\ntechniques. After the early study in 2016 from Google for\nclassiﬁcation of DR in fundus photographs,\nHunt et al. (2020)\npresented a low-shot, self-supervised deep learning method for\nclassiﬁcation of retinal fundus images. The low-shot mechanism of\nlearning in this study greatly resolved the problem of insuﬃcient\nimage samples, which is a major obstacle in most of the deep\nlearning applications. To implement the detection of DR at its early\nstage, the study\nMeshram et al. (2021) proposed an investigation\nof the applications of deep learning models for retinal image\nclassiﬁcation. In general, the deep learning architectures, including\nthe conventional convolutional neural network (CNN) and deep\nCNNs, were incorporated in this survey. In the study of\nTak\net al. (2021), a deep CNN model was trained to classify between\ndiﬀerent categories of AMD images. Accordingly, 420 wide-ﬁeld\nretinal images were included in the training process for classifying\nthe exudative and non-exudative AMD cases, and the accuracy\nachieved by the proposed CNN model is 88%.\nUmamageswari\net al. (2022) provided an approach to identify exudates and veins\nwith retinal images for the diagnosis of diabetics. Speciﬁcally, a\nCNN was proposed for retinal image recognition. Recently, to\nsegment and classify the retinal images in a uniﬁed way,\nKumari\net al. (2023) proposed an eﬃcient CNN model. To be speciﬁc,\nthe input images for the proposed model were pre-processed\nusing the green channel images, histogram-based algorithms, and\nnoise elimination techniques. The features were extracted from\nthe segmented images using the watershed algorithm as well as\nprincipal component analysis (PCA) technique, to note that the\npublicly available datasets used in this study were DRIVE (\nAsad\net al., 2014 ), STARE (Guo, 2020), and CHASE DB1 ( Yu et al., 2019 ).\nMost of the deep learning-based methods currently depend on the\nconvolutional modules leveraged to extract the image embeddings\nfor accurate classiﬁcation.\nNote that the above-mentioned approaches were originally\ndesigned for single-label classiﬁcation of retinal images. However,\nthere are usually more than one type of lesions appeared in practical\nTABLE /one.tnumDetailed distribution of the ODIR-/two.tnum/zero.tnum/one.tnum/nine.tnum dataset.\nCategory Full name Number of images\nA Age-related macular degeneration 171\nC Cataract 211\nD Diabetes retinopathy 1,131\nG Glaucoma 207\nH Hypertension 94\nM Myopia 177\nO Other abnormalities 944\nN Normal 1,135\nFrontiers in Neuroscience /zero.tnum/two.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nscenarios. In addition, the simultaneous understanding of multiple\nlesions in an retinal image could provide more information from\nthe associations between various diseased areas. Therefore, multi-\nlabel classiﬁcation of retinal image has also be paid attention by a\nvariety of machine vision and deep learning algorithms.\nOmar et al.\n(2017) presented a multi-label learning model to implement the\nexudate lesion classiﬁcation based on the multi-scale LBP features.\nSequentially, the KNN, neural network radial base function (NN-\nRBF), and neural network back-propagation (NN-BP) were taken\nas classiﬁers. With the employment of deep learning, the study of\nPrawira et al. (2021) used both the AlexNet ( Krizhevsky et al., 2012 )\nand VGG16 ( Simonyan and Zisserman, 2014 ) models to deal with\nthe task of multi-label retinal image classiﬁcation. In total, there\nare three types of lesions, including DR, myopia, and optic disk\ncupping (ODC), in the leveraged fundus images.\nChai et al. (2022)\nintroduced a deep learning model using a frequent pattern mining\nmodule with an adversarial auto-encoder network. Extensive\nexperiments were carried out on a practical image dataset to assess\nthe performance of the integrated deep model. Instead of using the\nCNN-based deep learning architectures, the study\nRodríguez et al.\n(2022) proposed a vision transformer-based model ( Dosovitskiy\net al., 2020 ) for retinal image analysis, to note that the proposed\napproach is similar to the study of Rodríguez et al. (2022), e.g., both\nof these two studies were inspired by the work of vision transformer\n(\nDosovitskiy et al., 2020 ). However, there are at least the following\ndiﬀerences between this work and ours. First of all, the input of\nthe proposed model is image patches with linear embeddings, while\nRodríguez et al. (2022) adopted CNN-based features as their input.\nSecond, the label embeddings in the proposed model are binary\nwhile\nRodríguez et al. (2022) used the ternary state embeddings\nin addition to the label embeddings. Originally, the transformer\narchitecture\nVaswani et al. (2017) was employed in natural language\n(NLP) processing applications ( Galassi et al., 2019 ). Since the\noutstanding outcome of transformer yielded in NLP initially, it\nhas been extensively employed in a variety of machine vision\napplications. Diﬀerent from the CNN models presented in the\nretinal image classiﬁcation, the vision transformer-based models\ncan unveil the global associations between long-range pixels in\nretinal images besides the information extracted from the local\nreceptive ﬁelds (\nFang et al., 2019 ; Gao et al., 2022 ) in an image.\nBearing the above-mentioned analysis in mind, this study\nproposes a novel multi-label retinal image classiﬁcation model\ninspired by the original vision transformer (\nDosovitskiy et al.,\n2020). A publicly available retinal image dataset ODIR-2019 /one.tnumwas\nexploited to complete the training of the proposed approach. To\nevaluate the performance of the proposed transformer model, the\ncomparison experiments were conducted using the public dataset\nODIR-2019 between the state-of-the-art CNN architectures.\nExperimental results of the proposed approach demonstrate the\nsuperiority of the presented pipeline and the value of self-attention\nmechanism in retinal image classiﬁcation.\n/one.tnumhttps://odir/two.tnum/zero.tnum/one.tnum/nine.tnum.grand-challenge.org\nFIGURE /one.tnum\nImage samples in the ODIR-/two.tnum/zero.tnum/one.tnum/nine.tnum dataset.(Top row) The single-label retinal images. (Bottom row) The multi-label retinal images. C, D, G, H, M, N,\nand O denote the cataract, diabetes retinopathy, glaucoma, hype rtension, myopia, and other abnormalities retinal images, respe ctively.\nFrontiers in Neuroscience /zero.tnum/three.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nThe primary contributions of this study can be summarized\nas follows:\n• A vision transformer-based multi-label retinal image\nclassiﬁcation pipeline is proposed.\n• A vision transformer model designed for the task of multi-\nlabel classiﬁcation was presented.\n• Experimental outcome prove the potential value of the\nproposed model in clinical practice.\nThe subsequent sections of this article are outlined below.\nThe speciﬁcs of the proposed pipeline are outlined in Section 2.\nSection 3 outlines the experimental methodology employed to\nassess the eﬃcacy of the suggested technique. The study’s discussion\nand conclusion are presented in Section 4.\n/two.tnum Methodology\n/two.tnum./one.tnum Dataset\nThe proposed vision transformer model was instantiated by\nusing the public multi-label retinal image database ODIR-2019.\nODIR-2019 was ﬁrst provided by the Ocular Disease Intelligent\nRecognition (ODIR) in 2019 University International Competition.\nIt is composed of the retinal images containing eight diﬀerent types\nof retinal lesions in total, which are AMD (A), cataract (C), DR (D),\nglaucoma (G), hypertension (H), myopia (M), other abnormalities\n(O), and the control group of normal (N). Moreover, this dataset\nalso contains the subject-wise labels with both the images and the\nmedical records of the patients. Totally, 3,500 annotated retinal\nimages from 5,000 cases were incorporated within the dataset. The\ndetails of the dataset distribution are shown in\nTable 1. The entire\nset of images were divided into training (70%), testing (20%), and\nvalidation set (10%).\nIn addition, a set of samples in the ODIR-2019 dataset are\nprovided in\nFigure 1. Speciﬁcally, there are both single-label and\nmulti-label retinal images in this dataset.\n/two.tnum./two.tnum Multi-label classiﬁcation network\narchitecture\nThis study aimed at addressing the multi-label classiﬁcation of\nretinal images, which can be expressed mathematically as follows.\nTo note that each image inside the recordings is represented by the\nsymbol Ii, where i belongs to the range [1, N\n′\n]. Here, N\n′\nrepresents\nFIGURE /two.tnum\nArchitectural of the proposed vision transformer. L is used to re present the quantity of encoder blocks in this model.\nFrontiers in Neuroscience /zero.tnum/four.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nthe total count of images present. In this study, the label of each\nimage could be denoted as a vector yj = (y1, ..., y\n′\nN ) ∈ { 0, 1}C\n′\n,\nwhere C\n′\nrepresents the total number of retinal lesion categories.\nEach marking denotes the presence (1) or absence (0) of each\nspeciﬁc retinal lesion.\nThe schematic representation of the transformer model under\nconsideration, as seen in\nFigure 2, is based on the architectural\ndesign of the vision transformer ( Dosovitskiy et al., 2020 ). The\ninitial step involves the utilization of a retinal image as input, which\nis subsequently transformed into ﬂattened linear embeddings. To\nhandle the two-dimensional retinal images, the proposed model\nemploys to reshape the images I ∈ Rh×w×d into smaller image\npatches Ip ∈ Rn×p×p×d. It should be noted that the variable\nh × w = 224 × 224 is used to represent the resolution of the\noriginal image. Additionally, the variable p × p speciﬁes the size\nof each image patch. The variable d is assigned a value of 3, which\nrepresents the number of channels in an RGB image. The variable\nn is calculated as the quotient of h × w divided by p × p. To account\nfor the distribution of image patches inside each original image,\npositional embeddings are concurrently appended to the ﬂattened\nembeddings (\nDosovitskiy et al., 2020 ). The positional embedding\nserves the purpose of denoting the spatial position of the image\npatches inside an image.\nIn addition to the linear embedding layer, the proposed model\nprimarily consists of two other components: an encoder block\nand a multiple-layer perception (MLP) module. It is important\nto acknowledge that each input sequence of retinal images\ncorresponds to the types of retinal fundus lesions. In addition, the\nencoder block incorporates the pivotal multi-head self-attention\nmodule (\nVaswani et al., 2017 ), which is designed to uncover the\nrelationships among distant image pixels. Furthermore, to achieve\na coherent encoder module, the suggested model employs an\nFIGURE /three.tnum\nEncoder block in the presented transformer model.\niterative repetition of the encoder block. In addition to the multi-\nhead self-attention modules, the encoders also incorporate several\nother types of layers, including layer normalization, dropout, and\nMLP blocks. The purpose of employing the MLP block was to\nproduce the output for multi-label classiﬁcation by combining the\nglobal average pooling (GAP) unit (\nRamasamy et al., 2021 ) and\nthe fully connected (FC) layer. In a broad sense, the retrieved\ndepiction derived from the retinal images comprises both localized\ninformation pertaining to a sequence of signals and the overarching\ncorrelation between signals that are widely separated.\nIn the suggested transformer model, the input sequences of\nretinal images undergo a sequential ﬂattening process, resulting in\nthe transformation of these sequences into vectors. Furthermore,\nit is important to acknowledge that the encoder block is iterated\na variable number of times in diﬀerent iterations of the proposed\ntransformer model. Additionally, the diagram depicting the\nstructural conﬁguration of this encoder block can be observed in\nFigure 3.\nAs depicted in Figure 3, the encoder block comprises several\ndistinct components, including layer normalization, multi-head\nself-attention (MSA), dropout, and MLP block. The study did not\nconduct a thorough analysis of the MSA unit as it has already\nbeen extensively studied in the current literature (e.g.,\nZhou et al.,\n2022). The study conducted by Guo and Gao (2022) employed\nFIGURE /four.tnum\nMLP block used in the proposed transformer model. GELU denotes\nthe activation function (\nLee, /two.tnum/zero.tnum/two.tnum/three.tnum).\nFrontiers in Neuroscience /zero.tnum/five.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\na unit comprised of H\n′\nheads to evaluate the similarity between\na query and its corresponding keys, taking into account the\nallocated weight for each value. In addition, the layer normalization\nmodule is utilized to compute the mean and variance necessary\nfor normalizing the inputs to the neurones within a layer during\na single training instance (\nBa et al., 2016 ). In this study, the authors\nemploy the dropout layer ( Choe and Shim, 2019 ) as a means of\nregularization to address the potential issue of over-ﬁtting. The\narchitectural structure of the multi-layer perceptron (MLP) block\nis depicted in\nFigure 4.\nThe technique that has been proposed enables the\nformulation of the process of categorizing retinal lesions in\nthe following\nEquations (1–5):\nz0 = [xclass; x1\npE; x2\npE; ...; xm\np ] + Eposition, (1)\nwhere variable z0 denotes the output of the linear embedding layer.\nIn the present situation, the variable m denotes the quantity of\nchannels employed in a linear embedding. The variables xclass and\nEposition correspond to the class token and positional embedding,\nrespectively. In the context of multi-label classiﬁcation, it is worth\nnoting that the class token xclass utilized in the proposed model\nexhibits distinct characteristics compared to the single-label class\ntoken employed in the original vision transformer (\nDosovitskiy\net al., 2020 ).\nz\n′\nl = MSA(LN(zl−1)) + zl−1, (2)\nzl = MLP(LN(z\n′\nl )) + z\n′\nl , (3)\nTABLE /two.tnumImplementation details in the experiments.\nItem Value\nBatch_size 8\nOptimizer Adam\nLearning rate 1e-4\nWeight decay 0.02\nEpochs 100\nTABLE /three.tnumCombinations of L and H and the comparison performance of\nthe proposed model with these combinations.\nModel Number of\nlayers ( L)\nNumber of\nheads ( H\n′\n)\nAUC\nL_2_H_8 2 8 0.907\nL_4_H_8 4 8 0.911\nL_8_H_8 8 8 0.923\nL_2_H_16 2 16 0.917\nL_4_H_16 4 16 0.931\nL_8_H_16 8 16 0.925\nL, number of layers; H, number of heads.\ny = FFN(z0\nL), (4)\nwhere layer normalization unit is represented as LN(.). In this\nnotation, zl represents the output of layer l. The feed-forward\nnetwork integrated with a fully connected (FC) layer and a sigmoid\nactivation function is written as FFN(.). The output classiﬁcation\noutcome is denoted as y.\nThe loss function employed throughout the training procedure\nis the weighted binary cross entropy function:\nLoss = − 1\nM\nC\n′\n∑\nc=1\nyilog(p(yc)) + (1 − yc)log(1 − p(yc)), (5)\nwhere C denotes the number of retinal lesion categories.\n/three.tnum Experiments\n/three.tnum./one.tnum Implementation details\nThe transformer model described in this study is implemented\nutilizing the PyTorch framework ( Paszke et al., 2019 ). The system\nutilizes four NVidia RTX 3090 Graphical Processing Units (GPUs)\nwith a combined RAM capacity of 128GB for computing purposes.\nThe optimal parameters of the proposed network are determined\nthrough a trial and error methodology. A 10-fold cross-validation\napproach is utilized to evaluate the reliability and stability of\nthe proposed methodology. The other implementation details are\nprovided in\nTable 2. Then, the retinal data input was divided into\nten equally sized groups in a sequential manner. In each iteration,\na single group out of the total of ten was assigned the role of the\ntesting set, while the remaining nine groups were employed as the\ntraining set. Ultimately, the ﬁnal output is determined by utilizing\nthe mean result obtained from 10 iterations.\n/three.tnum./two.tnum Evaluation metrics\nIn addition, the evaluation metrics included in the\ntrials included the F1 score, Kappa coeﬃcient, AUC, and\nthe average of these three performance indicators. The\nmathematical representation of these metrics is explicated in\nthe subsequent equations:\n(1) The deﬁnition of Kappa is provided in\nEquations (6, 7,\nand 8).\nkappa = po − pe\n1 − pe\n, (6)\npo =\n∑ C\nc=1 TPc\n∑ C\nc=1(TPc + FNc)\n, (7)\npe =\n∑ C\nc=1 TPc × (TPc + FNc)\nN × N , (8)\nwhere the phrases true positive and false negative are denoted as\nTP and FN, respectively. The variable c represents the number of\nFrontiers in Neuroscience /zero.tnum/six.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nretinal lesion categories, whereas N represents the total number of\nimage samples.\n(2) The used F1 score is expressed as Equations (9, 10, and 11).\nF1 = 2 × Precision × Recall\nPrecision + Recall = 2 × TP\n2 × TP + FN + FP , (9)\nPrecision = TP\nTP + FP , (10)\nRecall = TP\nTP + FN , (11)\nwhere the terms FP and FN represent false positive and false\nnegative, respectively.\n(3) AUC is given in\nEquations (12, 13, and 14).\nAUC =\n∫ 1\nx=0\nTPR(FPR−1(x))dx, (12)\nTPR = TP\nTP + FN , (13)\nFPR = FP\nFP + TN . (14)\n/three.tnum./three.tnum Ablation study\nTo ascertain the most suitable architecture for the proposed\nvision transformer, a comprehensive evaluation was conducted\nto determine the optimal combination of the hyper-parameters\nused in the proposed model. In the ablation study, we considered\nthe number of encoder blocks ( L) in the encoder, as depicted in\nFigure 3, and the number of MSA heads ( H\n′\n) employed in a single\nencoder block, as demonstrated in Figure 3.\nThe in-depth ﬁndings of the ablation study can be found in\nTable 3. It is important to keep in mind that only 10% of the\nretinal images were used in the study that involved ablation. In\nthe meantime, the area under the curve (AUC) was used as the\nevaluation statistic for this algorithm.\nThe most eﬀective combination of L and H\n′\nmay be determined\nby referring to\nTable 3. Speciﬁcally, the combination of L = 4\nand H = 16 demonstrates optimal results. This combination is\nsubsequently utilized in the subsequent experiments conducted for\nthe suggested approach.\nFIGURE /five.tnum\nClassiﬁcation results of the proposed approach on the ODIR-/two.tnum/zero.tnum/one.tnum/nine.tnum dataset.\nFIGURE /six.tnum\nClassiﬁcation results of the proposed approach on the RFMiD /two.tnum./zero.tnum dataset.\nFrontiers in Neuroscience /zero.tnum/seven.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\n/three.tnum./four.tnum Performance of the proposed method\nand the comparison experiments\nThis section ﬁrst presents the outcomes obtained by\nimplementing the proposed methodology on the publicly\naccessible dataset ODIR-2019. The classiﬁcation results are\npresented in\nFigure 5. The corresponding outcomes are Kappa\n(0.645 ± 0.04), F1 score (0.919 ± 0.02), AUC (0.938 ± 0.05), and\nAVG (A VG= Kappa+F1+AUC\n3 , 0.834 ± 0.04).\nMeanwhile, a hold-out test was conducted to evaluate\nthe proposed approach on entirely new data, which had not\nbeen used in the training process. Thus, the RFMiD 2.0 data\nFIGURE /seven.tnum\nComparison results between the proposed approach and the state-o f-the-art techniques on the ODIR-/two.tnum/zero.tnum/one.tnum/nine.tnum dataset.\nFrontiers in Neuroscience /zero.tnum/eight.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nFIGURE /eight.tnum\nCAMs generated using the proposed approach.\n(Panchal et al., 2023 ) were exploited in the hold-out test Figure 6.\nThe corresponding experimental results are Kappa (0.681 ±\n0.03), F1 score (0.927 ± 0.04), AUC (0.944 ± 0.08), and\nAVG (0.851 ± 0.03).\nIn order to provide further evidence of the eﬀectiveness of\nthe provided approach, experiments comparing our model to the\nmost recent and cutting-edge CNN models have been carried\nout. Models such as VGG19 (\nSimonyan and Zisserman, 2014 ),\nResNet50 ( He et al., 2015 ), Inception-V3 ( Szegedy et al., 2014 ),\nEﬃcient-B4 ( Tan and Le, 2019 ), ResNet101 ( He et al., 2015 ), and\nvision transformer ( Rodríguez et al., 2022 ) are considered to be\namong the most advanced currently available. The results of the\ncomparison are presented in\nFigure 7.\nFurthermore, the class activation mapping (CAM) ﬁgures\ngenerated by using the proposed approach with the public dataset\nare provided in\nFigure 8.\nFinally, to evaluate the proposed model in classifying\neach category of retinal diseases, the single-label classiﬁcation\nexperiment was conducted by the proposed approach on the ODIR-\n2019 dataset. The corresponding results are F1 score (0.932 ± 0.06)\nand AUC (0.950 ± 0.03).\n/three.tnum./five.tnum Discussion\nIt is clear by looking at\nFigure 7 that the proposed methodology\nhas reached a higher level of performance when compared to\nthe ways that are currently being used. To be more speciﬁc, the\nKappa value of the technique that is being proposed is 0.645. It\nhas increased by 9.38 % in comparison with the one that was\nproduced by ResNet101’s work (\nHe et al., 2015 ), which was the\nclosest one. In addition, in comparison with the one that was\ncreated by ResNet101, the F1 score of the suggested approach\nhas grown by 7.68 %, the value of the approach’s AUC has\nincreased by 0.97 %, and the approach’s average value has increased\nby 0.85 %.\nThere are also several limitations need to be mentioned in\nthis study. First of all, this study did not take the imbalanced\nissue existed in the leveraged dataset into consideration. In\nthe ODIR-2019 dataset, there are much more images in the\nDR (D), normal (N), and other abnormalities (O) categories\nthan the remaining ﬁve classes. Therefore, the imbalanced\ndistribution of the dataset might have an inﬂuence on the\nperformance of the proposed approach. Second, the presented\ndeep model was inspired by the original vision transformer\n(\nDosovitskiy et al., 2020 ), and the primary modiﬁcation to the\noriginal vision transformer mainly locates at the output layer\nto adapt to the requirement of multi-label classiﬁcation. The\ninner structure of the vision transformer needs should also be\noptimized to yield a more accurate result. Finally, only one\nspeciﬁc dataset was exploited in the experiments, which might\nnot be able to prove the generalization of the proposed vision\ntransformer architecture.\n/four.tnum Conclusion\nIn this study, a novel vision transformer model was presented\nto resolve the multi-label retinal image classiﬁcation issue. In\ntotal, eight categories of retinal images can be classiﬁed by\nthe proposed approach. Experimental results demonstrate the\nsuperiority of our method over the state-of-the-art CNN-based\nmodels. To note that it can be attributed to the leveraged\nattention mechanism in the proposed deep learning model,\nwhich is supposed to reveal the global associations between\nlong-range pixels.\nIn the future, more data samples will be incorporated\nto enhance both the diversity of the images and the\ngeneralization of the model presented in this study. In addition,\na variety of the combinations of CNN and transformer\nmodules would be exploited to develop more optimal\ndeep models.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding authors.\nFrontiers in Neuroscience /zero.tnum/nine.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nAuthor contributions\nDW: Writing – original draft. JL: Writing – original draft. WJ:\nWriting – original draft.\nFunding\nThe author(s) declare that ﬁnancial support was received\nfor the research, authorship, and/or publication of this article.\nThis research was funded by the Natural Science Foundation of\nShandong Province, grant number ZR2020MF133.\nAcknowledgments\nThe authors thank the editors and reviewers for their work.\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbràmoﬀ, M. D., Garvin, M. K., and Sonka, M. (2010). Retinal i maging and image\nanalysis. IEEE Trans. Med. Imaging3, 169–208. doi: 10.1109/RBME.2010.2084567\nAsad, A. H., Azar, A. T., El-Bendary, N., and Hassanien, A. E. ( 2014). Ant colony\nbased feature selection heuristics for retinal vessel segme ntation. arXiv [preprint].\ndoi: 10.48550/arXiv.1403.1735\nAyoub, S., Khan, M. A., Jadhav, V. P., Anandaram, H., Kumar, T . C. A., Reegu, F. A.,\net al. (2022). Minimized computations of deep learning techniqu e for early diagnosis\nof diabetic retinopathy using iot-based medical devices. Comput. Intell. Neurosci. 2022.\ndoi: 10.1155/2022/7040141\nBa, J., Kiros, J. R., and Hinton, G. E. (2016). Layer normalizat ion. arXiv [preprint].\ndoi: 10.48550/arXiv.1607.06450\nBerbar, M. A. (2022). Features extraction using encoded loca l binary\npattern for detection and grading diabetic retinopathy. Health Inf. Sci. Syst. 10.\ndoi: 10.1007/s13755-022-00181-z\nChai, Y., Liu, H., Xu, J., Samtani, S., Jiang, Y., Liu, H., et a l. (2022). A multi-\nlabel classiﬁcation with an adversarial-based denoising auto encoder for medical image\nannotation. ACM Trans. Manag. Inf. Syst. 14, 1–21. doi: 10.1145/3561653\nChen, Z., Zhang, H., Chi, Z., and Fu, H. (2014). “Hierarchica l local binary pattern\nfor branch retinal vein occlusion recognition,\" in ACCV Workshops(Springer).\nChoe, J., and Shim, H. (2019). “Attention-based dropout layer for weakly\nsupervised object localization,\" in 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) (Long Beach, CA: IEEE), 2214–2223.\ndoi: 10.1109/CVPR.2019.00232\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: transformers for image recognition at\nscale. arXiv [preprint]. doi: 10.48550/arXiv.2010.11929\nFang, J., Xu, X., Liu, H., and Sun, F. (2019). Local receptive ﬁ eld based extreme\nlearning machine with three channels for histopathological ima ge classiﬁcation. Int. J.\nMach. Learn. Cybernet. 10, 1437–1447. doi: 10.1007/s13042-018-0825-6\nGalassi, A., Lippi, M., and Torroni, P. (2019). Attention in nat ural\nlanguage processing. IEEE Trans. Neural Netw. Learn. Syst . 32, 4291–4308.\ndoi: 10.1109/TNNLS.2020.3019893\nGao, S., Li, Z.-Y., Han, Q., Cheng, M.-M., and Wang, L. (2022) . Rf-next: Eﬃcient\nreceptive ﬁeld search for convolutional neural networks. IEEE Trans. Pattern Anal.\nMach. Intell. 45, 2984–3002. doi: 10.1109/TPAMI.2022.3183829\nGuo, S. (2020). DPN: detail-preserving network with high resolu tion representation\nfor eﬃcient segmentation of retinal vessels. J. Ambient Intell. Humaniz. Comput. 14,\n5689–5702. doi: 10.1007/s12652-021-03422-3\nGuo, X., and Gao, X. (2022). A SYN ﬂood attack detection method b ased\non hierarchical multihead self-attention mechanism. Secur. Commun. Netw. 2022,\n8515836. doi: 10.1155/2022/8515836\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). “Deep residual learning for image\nrecognition,\" in 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (Las Vegas, NV: IEEE), 770–778. doi: 10.1109/CVPR.2016.90\nHunt, M. S., Kihara, Y., and Lee, A. Y. (2020). Novel low-shot d eep learning\napproach for retinal image classiﬁcation with few examples. JAMA Ophthalmol. 138,\n1077–1078. doi: 10.1001/jamaophthalmol.2020.3256\nKothare, K. S., and Malpe, K. (2019). “Design and implementation of inspection\nmodel for knowledge patterns classiﬁcation in diabetic retina l images,\" in 2019 3rd\nInternational Conference on Computing Methodologies and Communication(ICCMC)\n(Erode: IEEE), 1220–1223. doi: 10.1109/ICCMC.2019.88196 47\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imag enet classiﬁcation with\ndeep convolutional neural networks. Commun. ACM60, 84–90. doi: 10.1145/3065386\nKumari, D. A., Lakshmi, S. R., Revathy, R., Sundararajan, S. , Krishnan, R. S.,\nNarayanan, K. L., et al. (2023). “Automated process for retina l image segmentation\nand classiﬁcation via deep learning based cnn model,\" in 2023 International Conference\non Inventive Computation Technologies (ICICT) (Piscataway, NJ: IEEE), 152–158.\ndoi: 10.1109/ICICT57646.2023.10133943\nKumudham, R. (2015). Retinal image classiﬁcation as normal a nd abnormal using\nsupport vector machine. Int. J. Adv. Eng. Res. 10.\nLee, M. (2023). Gelu activation function in deep learning: A\ncomprehensive mathematical analysis and performance. arXiv [preprint].\ndoi: 10.48550/arXiv.2305.12073\nMeshram, A., Dembla, D., and Ajmera, R. (2021). Analysis and de sign of\ndeep learning algorithms for retinal image classiﬁcation for e arly detection\nof diabetic retinopathy. Turk. J. Comput. Math. Educ . 12, 2633–2641.\ndoi: 10.17762/turcomat.v12i6.5710\nMokhashi, N., Grachevskaya, J., Cheng, L., Yu, D., Lu, X., Zha ng, Y., et al.\n(2021). A comparison of artiﬁcial intelligence and human diabe tic retinal image\ninterpretation in an urban health system. J. Diabetes Sci. Technol. 16, 1003–1007.\ndoi: 10.1177/1932296821999370\nMokhles, P., Schouten, J. S., Beckers, H. J., Azuara-Blanco, A ., Tuulonen, A.,\nWebers, C. A. B., et al. (2017). Glaucoma blindness at the end of lif e. Acta Ophthalmol.\n95, 10–11. doi: 10.1111/aos.12933\nMontesano, G., Ometto, G., Higgins, B. E., Das, R. R., Graham, K. W., Chakravarthy,\nU., et al. (2021). Evidence for structural and functional dam age of the inner\nretina in diabetes with no diabetic retinopathy. Invest. Ophthalmol. Vis. Sci. 62, 35.\ndoi: 10.1167/iovs.62.3.35\nOmar, M. A., Tahir, M. A., and Kheliﬁ, F. (2017). ‘Multi-label lea rning model\nfor improving retinal image classiﬁcation in diabetic retino pathy,\" in 2017 4th\nInternational Conference on Control, Decision and Information Technologies (CoDIT)\n(Barcelona: Spain), 0202–0207. doi: 10.1109/CoDIT.2017.81 02591\nPanchal, S., Naik, A., Kokare, M., Pachade, S., Naigaonkar, R ., Phadnis, P., et al.\n(2023). Retinal fundus multi-disease image dataset (RFMID) 2.0: a dataset of frequently\nand rarely identiﬁed diseases. Data 8. doi: 10.3390/data8020029\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Cha nan, G., et al. (2019).\n“Pytorch: an imperative style, high-performance deep learning library,\" in Neural\nInformation Processing Systems(Cambridge, MA: MIT Press).\nPrawira, R., Bustamam, A., and Anki, P. (2021). “Multi label cla ssiﬁcation of retinal\ndisease on fundus images using AlexNet and VGG16 architectures ,\" in 2021 4th\nInternational Seminar on Research of Information Technology andIntelligent Systems\n(ISRITI) (Yogyakarta: IEEE), 464–468. doi: 10.1109/ISRITI54043.2 021.9702817\nRamasamy, L. K., Kakarla, J., Isunuri, B. V., and Singh, M. (20 21). Multi-class brain\ntumor classiﬁcation using residual network and global averag e pooling. Multimedia\nTools and Applications, 80, 13429–13438. doi: 10.1007/s11042-020-10335-4\nFrontiers in Neuroscience /one.tnum/zero.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/nine.tnum/zero.tnum/eight.tnum/zero.tnum/three.tnum\nReddy, Y. M. S., and Ravindran, R. S. E. (2022). Retinal image lesions assisted\ndiabetic retinopathy screening system through machine learn ing. Int. J. Intell. Eng. Syst.\n15, 175–189. doi: 10.22266/ijies2022.0430.17\nRodríguez, M. A., Al-Marzouqi, H., and Liatsis, P. (2022). Mult i-label retinal\ndisease classiﬁcation using transformers. IEEE J. Biomed. Health Inf. 27, 2739–2750.\ndoi: 10.1109/JBHI.2022.3214086\nSchmitz-Valckenberg, S., Göbel, A. P., Saur, S., Steinberg, J. , Thiele, S., Wojek, C.,\net al. (2016). Automated retinal image analysis for evaluation of focal hyperpigmentary\nchanges in intermediate age-related macular degeneration. Transl. Vis. Sci. Technol. 5,\n3. doi: 10.1167/tvst.5.2.3\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutio nal networks for\nlarge-scale image recognition. arXiv [preprint]. doi: 10.48550/arXiv.1409.1556\nSun, Y., Chen, A., Zou, M., Zhang, Y., Jin, L., Li, Y., et al. (20 22). Time trends,\nassociations and prevalence of blindness and vision loss due to g laucoma: an analysis\nof observational data from the global burden of disease study 2017. BMJ Open 12,\ne053805. doi: 10.1136/bmjopen-2021-053805\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S. E., Angu elov, D.,\net al. (2014). “Going deeper with convolutions, ” in 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (Boston, MA: IEEEE), 1–9.\ndoi: 10.1109/CVPR.2015.7298594\nTak, N., Reddy, A. J., Martel, J., and Martel, J. B. (2021). Clini cal wide-ﬁeld retinal\nimage deep learning classiﬁcation of exudative and non-exuda tive age-related macular\ndegeneration. Cureus 13, e17579. doi: 10.7759/cureus.17579\nTan, M., and Le, Q. V. (2019). Eﬃcientnet: Rethinking model s caling for\nconvolutional neural networks. arXiv [preprint]. doi: 10.48550/arXiv.1905.11946\nUmamageswari, A., Deepa, S., and Beevi, L. S. (2022). novel appr oach for\nclassiﬁcation of diabetics from retinal image using deep lear ning technique. Int. J.\nHealth Sci. 6, 2729–2736. doi: 10.53730/ijhs.v6nS1.5196\nVaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jone s, L., Gomez, A. N., et al.\n(2017). “Attention is all you need,\" in NIPS (Cambridge, MA: MIT Press).\nYokomizo, H., Maeda, Y., Park, K., Clermont, A. C., Hernandez ,\nS. L., Fickweiler, W., et al. (2019). Retinol binding protein 3 i s\nincreased in the retina of patients with diabetes resistant t o diabetic\nretinopathy. Sci. Transl. Med . 11, eaau6627. doi: 10.1126/scitranslmed.aau\n6627\nYu, H. H., Sun, H., and Wang, Z. (2019). “Mixmodule: mixed\ncnn kernel module for medical image segmentation,\" in 2020\nIEEE 17th International Symposium on Biomedical Imaging (ISBI )\n(Iowa City, IA: IEEE), 1508–1512. doi: 10.1109/ISBI45749. 2020.909\n8498\nZhou, L., Xu, Z., Guerra, J., Rosenberg, A. Z., Fenaroli, P., Eb erhart, C. G., et al.\n(2021). Expression of the SARS-CoV-2 receptor ace2 in human re tina and diabetes—\nimplications for retinopathy. Invest. Ophthalmol. Vis. Sci. 62, 6. doi: 10.1167/iovs.62.7.6\nZhou, Y., Wang, F., Zhao, J., Yao, R., Chen, S., Ma, H., et al. (2 022). Spatial-temporal\nbased multihead self-attention for remote sensing image chan ge detection. IEEE Trans.\nCirc. Syst. Video Technol. 32, 6615–6626. doi: 10.1109/TCSVT.2022.3176055\nFrontiers in Neuroscience /one.tnum/one.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6113121509552002
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4871920943260193
    },
    {
      "name": "Retinal",
      "score": 0.47148701548576355
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38124948740005493
    },
    {
      "name": "Medicine",
      "score": 0.2704956531524658
    },
    {
      "name": "Ophthalmology",
      "score": 0.2030385136604309
    }
  ]
}