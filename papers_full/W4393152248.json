{
  "title": "The Generalization and Robustness of Transformer-Based Language Models on Commonsense Reasoning",
  "url": "https://openalex.org/W4393152248",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2095915716",
      "name": "Ke Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095915716",
      "name": "Ke Shen",
      "affiliations": [
        "Marina Del Rey Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6766937060",
    "https://openalex.org/W6853198332",
    "https://openalex.org/W4205553902",
    "https://openalex.org/W2937523352",
    "https://openalex.org/W3022708451",
    "https://openalex.org/W4388488349",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W4386607580",
    "https://openalex.org/W4221141571",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4302305761",
    "https://openalex.org/W4377723636",
    "https://openalex.org/W3175287561"
  ],
  "abstract": "The advent of powerful transformer-based discriminative language models and, more recently, generative GPT-family models, has led to notable advancements in natural language processing (NLP), particularly in commonsense reasoning tasks. One such task is commonsense reasoning, where performance is usually evaluated through multiple-choice question-answering benchmarks. Till date, many such benchmarks have been proposed and `leaderboards' tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy on the task to an in-depth, context-sensitive probing of LLMs' generalization and robustness. To gain deeper insight into diagnosing these models' performance in commonsense reasoning scenarios, this thesis addresses three main studies: the generalization ability of transformer-based language models on commonsense reasoning, the trend in confidence distribution of these language models confronted with ambiguous inference tasks, and a proposed risk-centric evaluation framework for both discriminative and generative language models.",
  "full_text": "The Generalization and Robustness of Transformer-Based Language Models on\nCommonsense Reasoning\nKe Shen\nInformation Sciences Institute\nUSC Viterbi School of Engineering\n4676 Admiralty Way 1001\nMarina Del Rey, California 90292\nkeshen@isi.edu\nAbstract\nThe advent of powerful transformer-based discriminative lan-\nguage models and, more recently, generative GPT-family\nmodels, has led to notable advancements in natural lan-\nguage processing (NLP), particularly in commonsense rea-\nsoning tasks. One such task is commonsense reasoning,\nwhere performance is usually evaluated through multiple-\nchoice question-answering benchmarks. Till date, many such\nbenchmarks have been proposed and ‘leaderboards’ track-\ning state-of-the-art performance on those benchmarks sug-\ngest that transformer-based models are approaching human-\nlike performance. However, due to documented problems\nsuch as hallucination and bias, the research focus is shift-\ning from merely quantifying accuracy on the task to an in-\ndepth, context-sensitive probing of LLMs’ generalization and\nrobustness. To gain deeper insight into diagnosing these mod-\nels’ performance in commonsense reasoning scenarios, this\nthesis addresses three main studies: the generalization abil-\nity of transformer-based language models on commonsense\nreasoning, the trend in confidence distribution of these lan-\nguage models confronted with ambiguous inference tasks,\nand a proposed risk-centric evaluation framework for both\ndiscriminative and generative language models.\nGeneralizability Evaluation\nAs emphasized in a recent study (Davis 2023), the concept\nof commonsense reasoning implies that its involved com-\nmonsense knowledge is common. Thus, commonsense AI\nshould be expected to generalize, that is, at least in aggre-\ngate, should not exhibit excessive performance loss across\nindependent commonsense benchmarks, such as (Bhagavat-\nula et al. 2020; Singh et al. 2021; Santos et al. 2022; Ke-\njriwal et al. 2023), regardless of the specific benchmark on\n(the training set of) which it has been fine-tuned. In my first\nwork (Shen and Kejriwal 2021), we evaluated this expec-\ntation by proposing a methodology and experimental study\nto measure the generalization ability of transformer-based\nlanguage models using statistical significance analysis and a\nrigorous and intuitive metric (i.e., performance loss metric).\nWe conducted an in-depth evaluation of RoBERTa,\na widely-used, transformer-based discriminative language\nmodel (LM), using five established commonsense reason-\ning benchmarks. The focus was to determine if there was a\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nsignificant decrease in RoBERTa’s performance when it was\nfine-tuned on one commonsense benchmark but tested on a\ndifferent one. The experimental study shows that the models\ndo not generalize well, and may be (potentially) susceptible\nto issues such as dataset bias. The results, therefore, suggest\nthat current performance on benchmarks may be an overes-\ntimate, especially if we want to use such models on novel\ncommonsense problems for which a ‘training’ dataset may\nnot be available for the language representation model to\nfine-tune. Additionally, it suggests that sophisticated adver-\nsarial modifications are not necessary to conclude that gen-\neralization is a concern for transformer-based QA models.\nRegrettably, we haven’t had the chance to verify whether the\nfindings are applicable to the generative GPT-series models,\nwhich have gained considerable recognition recently. Exam-\nining if these conclusions hold for GPT models is crucial\nfor their broader application in real-world scenarios. appli-\ncations.\nRisk Evaluation\nBeyond the study of the generalizability of transformer-\nbased language models, there is a growing body of research\ndedicated to diagnosing these models’ performance in com-\nmonsense reasoning scenarios. (Wu et al. 2020) found that\nBERT-based LM does not fully ‘understand’ naturalistic\nconcepts like negation by introducing a parameter-free prob-\ning technique to recover information from the token repre-\nsentation. Along with the impressive performance achieved\nby ChatGPT, the uncertainty or risk-related issues associated\nwith these LLMs, including hallucination, bias, and over-\nconfidence, have reignited concerns (Ferrara 2023).\nTraditionally, in machine learning, the risk of a model’s\ninference tended to be directly equated to its confidence\nscore: lower confidence was assumed to signal increased\nrisk, implicitly defined as the probability that the predic-\ntion made by the model is correct. Initial research on LLMs’\n‘self-understanding’ of their own uncertainty has predomi-\nnantly relied on interpreting raw softmax probabilities of the\nfinal output layer as ‘confidence’ scores (Vasudevan, Sethy,\nand Ghias 2019). (Kadavath et al. 2022) highlighted that\nthese generative LLMs can accurately predict which ques-\ntions they will be able to answer correctly on diverse NLI\ntasks based on their confidence scores.\nIn (Shen and Kejriwal 2022), we designed two prompt-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23419\nbased and one choice-based perturb functions to examine to\nwhat extent the confidence behavior captured inherent risk\nin an ambiguous inference task with no, or a highly con-\ntroversial, correct answer in the ground truth. The proposed\nconfusion probes stimulated theambiguous situations where\n(for example) no correct answer exists for a given prompt\namong the provided set of choices. A statistical analysis of\nthe confidence behavior of a fine-tuned, high-performance\nlanguage model was conducted across widely used com-\nmonsense multiple-choice QA benchmarks.\nWe found evidence that, when instances are perturbed us-\ning the prompt-based functions, the confidence distribution\nof (the originally) incorrect answers in most perturbed in-\nstances is close to random, and is similar to the distribution\nobserved before perturbation. The model will still prefer the\noriginally correct (and post-perturbation, ‘pseudo-correct’)\nanswer even though it is now theoretically incorrect. For the\nchoice-based perturbation, the model will choose the incor-\nrect choice that appears superficially closest to the prompt,\ndespite its incorrectness. Further analysis, including an anal-\nysis of potential ‘irregularities’ in the benchmarks, suggests\nthat they cannot serve as causal explanations for the ob-\nserved phenomena.\nRisk-adjusted Calibrator\nIn the ongoing study, we move beyond a single and often im-\nplicit definition of risk by introducing a risk-centric frame-\nwork that defines two different types of risk, and proposing\nfour metrics for evaluating LLMs on these two risks. We also\npresent a novel risk-adjusted calibrator (called DwD) for ad-\njusting the raw confidence of an underlying LLM to navigate\ndecision and composite risks better. Results show that DwD\nreduces in-domain and out-of-domain decision risks more\nfor RoBERTa by margins of 28.3% and 18.9%, respectively.\nSimilarly, using a risk sensitivity metric, DwD reduces ad-\nditional composite risk of ChatGPT NLI by a margin of\n17.9%, compared to the next best baseline.\nLimitations and Future Work\nAlthough many studies (Ferrara 2023; Wu et al. 2020), as\nwell as our work, have brought significant attention to the\nevaluation of commonsense reasoning in LLMs, their ro-\nbustness and reliability in real-world applications are still\ndebatable. Some evidence already suggests that the evalua-\ntion of such models on common sense may be over-reliant\non the multiple-choice question format, which is amenable\nto automated scoring, but does not reflect real-world conver-\nsational interactions. To obtain a more comprehensive un-\nderstanding of LLMs’ capabilities in machine commonsense\nreasoning and their robustness, it is essential to employ a\nwider range of evaluation methods, including those that in-\nvolve human-in-the-loop evaluations.\nFurthermore, although not exactly as expected, the advent\nof LLMs does present promising avenues for addressing\nmachine commonsense reasoning. This advancement makes\nit more feasible to integrate LLMs into downstream ap-\nplications that necessitate natural language understanding.\nAn innovative application of this is the development of a\nknowledge-based analytical system. This system would al-\nlow domain experts, including those without technical back-\ngrounds, to interact using natural language queries. Our\nforthcoming objective is to design a knowledge-based an-\nalytical system tailored for any specific domain, which can\nbe operated through natural language questions instead of\ncomplex, formal queries.\nReferences\nBhagavatula, C.; Bras, R. L.; Malaviya, C.; Sakaguchi, K.;\nHoltzman, A.; Rashkin, H.; Downey, D.; tau Yih, W.; and\nChoi, Y . 2020. Abductive Commonsense Reasoning. In In-\nternational Conference on Learning Representations.\nDavis, E. 2023. Benchmarks for automated commonsense\nreasoning: A survey. arXiv preprint arXiv:2302.04752.\nFerrara, E. 2023. Should chatgpt be biased? challenges\nand risks of bias in large language models. arXiv preprint\narXiv:2304.03738.\nKadavath, S.; Conerly, T.; Askell, A.; Henighan, T.; Drain,\nD.; Perez, E.; Schiefer, N.; Dodds, Z. H.; DasSarma, N.;\nTran-Johnson, E.; et al. 2022. Language models (mostly)\nknow what they know. arXiv preprint arXiv:2207.05221.\nKejriwal, M.; Santos, H.; Shen, K.; Mulvehill, A. M.; and\nMcGuinness, D. L. 2023. Context-Rich Evaluation of Ma-\nchine Common Sense. In International Conference on Arti-\nficial General Intelligence, 167–176. Springer.\nSantos, H.; Shen, K.; Mulvehill, A. M.; Razeghi, Y .;\nMcGuinness, D. L.; and Kejriwal, M. 2022. A theoretically\ngrounded benchmark for evaluating machine commonsense.\narXiv preprint arXiv:2203.12184.\nShen, K.; and Kejriwal, M. 2021. On the generalization\nabilities of fine-tuned commonsense language representa-\ntion models. In Artificial Intelligence XXXVIII: 41st SGAI\nInternational Conference on Artificial Intelligence, AI 2021,\nCambridge, UK, December 14–16, 2021, Proceedings 41 ,\n3–16. Springer.\nShen, K.; and Kejriwal, M. 2022. Understanding prior bias\nand choice paralysis in transformer-based language repre-\nsentation models through four experimental probes. arXiv\npreprint arXiv:2210.01258.\nSingh, S.; Wen, N.; Hou, Y .; Alipoormolabashi, P.; Wu, T.-\nL.; Ma, X.; and Peng, N. 2021. COM2SENSE: A common-\nsense reasoning benchmark with complementary sentences.\narXiv preprint arXiv:2106.00969.\nVasudevan, V . T.; Sethy, A.; and Ghias, A. R. 2019. Towards\nbetter confidence estimation for neural models. In ICASSP\n2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 7335–7339. IEEE.\nWu, Z.; Chen, Y .; Kao, B.; and Liu, Q. 2020. Perturbed\nMasking: Parameter-free Probing for Analyzing and Inter-\npreting BERT. InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 4166–4176.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23420",
  "topic": "Commonsense reasoning",
  "concepts": [
    {
      "name": "Commonsense reasoning",
      "score": 0.8526450395584106
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5601739287376404
    },
    {
      "name": "Transformer",
      "score": 0.5480945706367493
    },
    {
      "name": "Computer science",
      "score": 0.5282114148139954
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4406728148460388
    },
    {
      "name": "Generalization",
      "score": 0.43434906005859375
    },
    {
      "name": "Natural language processing",
      "score": 0.3508780598640442
    },
    {
      "name": "Epistemology",
      "score": 0.13995951414108276
    },
    {
      "name": "Philosophy",
      "score": 0.13375717401504517
    },
    {
      "name": "Engineering",
      "score": 0.10053369402885437
    },
    {
      "name": "Chemistry",
      "score": 0.06603789329528809
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": []
}