{
  "title": "Can Pretrained Language Models Generate Persuasive, Faithful, and Informative Ad Text for Product Descriptions?",
  "url": "https://openalex.org/W4285257595",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1976851905",
      "name": "Fajri Koto",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2095936123",
      "name": "Jey Han Lau",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": [
        "University of Melbourne",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2915161943",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2116716943",
    "https://openalex.org/W3173233571",
    "https://openalex.org/W3105157853",
    "https://openalex.org/W3168282198",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3102901618",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2912759951",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2973080442",
    "https://openalex.org/W3102401511",
    "https://openalex.org/W3034209844",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W2775022418",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W2950397305",
    "https://openalex.org/W3161838415",
    "https://openalex.org/W2514580187",
    "https://openalex.org/W3035543325",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3154790171",
    "https://openalex.org/W3115828495",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2964885819",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2798834410",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "For any e-commerce service, persuasive, faithful, and informative product descriptions can attract shoppers and improve sales. While not all sellers are capable of providing such interesting descriptions, a language generation system can be a source of such descriptions at scale, and potentially assist sellers to improve their product descriptions. Most previous work has addressed this task based on statistical approaches (Wang et al., 2017), limited attributes such as titles (Chen et al., 2019; Chan et al., 2020), and focused on only one product type (Wang et al., 2017; Munigala et al., 2018; Hong et al., 2021). In this paper, we jointly train image features and 10 text attributes across 23 diverse product types, with two different target text types with different writing styles: bullet points and paragraph descriptions. Our findings suggest that multimodal training with modern pretrained language models can generate fluent and persuasive advertisements, but are less faithful and informative, especially out of domain.",
  "full_text": "Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5), pages 234 - 243\nMay 26, 2022c⃝2022 Association for Computational Linguistics\nCan Pretrained Language Models Generate Persuasive, Faithful, and\nInformative Ad Text for Product Descriptions?\nFajri Koto1 Jey Han Lau1 Timothy Baldwin1,2\n1The University of Melbourne\n2MBZUAI\nffajri@student.unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net\nAbstract\nFor any e-commerce service, persuasive, faith-\nful, and informative product descriptions can\nattract shoppers and improve sales. While not\nall sellers are capable of providing such in-\nteresting descriptions, a language generation\nsystem can be a source of such descriptions\nat scale, and potentially assist sellers to im-\nprove their product descriptions. Most previ-\nous work has addressed this task based on sta-\ntistical approaches (Wang et al., 2017), lim-\nited attributes such as titles (Chen et al., 2019;\nChan et al., 2020), and focused on only one\nproduct type (Wang et al., 2017; Munigala\net al., 2018; Hong et al., 2021). In this pa-\nper, we jointly train image features and 10\ntext attributes across 23 diverse product types,\nwith two different target text types with differ-\nent writing styles: bullet points and paragraph\ndescriptions. Our ﬁndings suggest that mul-\ntimodal training with modern pretrained lan-\nguage models can generate ﬂuent and persua-\nsive advertisements, but are less faithful and\ninformative, especially out of domain.\n1 Introduction\nGenerative pretrained language models such as\nGPT-2 (Radford et al., 2019), T5 (Raffel et al.,\n2020), and BART (Lewis et al., 2020a) have led\nto impressive gains in language generation appli-\ncations beyond machine translation, such as story\ngeneneration (Fan et al., 2018; Goldfarb-Tarrant\net al., 2020), summarization (Zhang et al., 2020;\nQi et al., 2020), and dialogue systems (Ham et al.,\n2020). Although such transformer-based language\nmodels (Vaswani et al., 2017) are capable of gener-\nating ﬂuent texts through a sequence-to-sequence\nframework, they still suffer from unfaithfulness and\nfactuality issues (Maynez et al., 2020; Wang et al.,\n2020; Moradi et al., 2021).\nIn this paper, we comprehensively discuss the\nutility of modern pretrained language models over\nan ad text generation task for product descriptions,\nPhiladelphia, Pennsylvania - Skyline\nwith Comcast Tower - Abstract\n(Cream) 99520 (10x15 Wood Wall\nSign, Wall Decor Ready to Hang)\nMeasures 10x15 inches\nHoles in corners, ready for hanging,\nPrinted in the USA, sustainable birch\nPerfect for your home, office, or a gift\nTITLE: Lantern Press Philadelphia - Skyline with\nComcast Tower\nPART NUMBER: LANT-99520-10x15W\nMODEL NUMBER: LANT-99520-10x15W\nCATEGORY: HOME\nCOLOR: Multi\nBRAND: Lantern Press\nSIZE: 10 x 15 Wood Sign\nCLASSIFICATION: base_product\nWEIGHT: 13.0\nKEYWORD: Decor wooden sign wall decoration  \nBULLET POINTS PARAGRAPH DESCRIPTION\nThis original high-quality wood print from Lantern\nPress boasts sharp detail and vivid imagery of\nPhiladelphia, Pennsylvania - Skyline with Comcast\nTower - Abstract (Cream) 99520 (10x15 Wood Wall\nSign, Wall Decor Ready to Hang). Product measures\n10 x 15 inches 100% Printed in America, \"Grade A\"\nsustainable birch Holes in corners, ready for tacker\nsign to be hung Wood print will ship in a sturdy box,\nprotected in a water-proof sleeve. Lantern Press is a\ndynamic art company that specializes in the world's\nleading imagery.\nFigure 1: Top: Input consisting of an image and tex-\ntual attributes of a product. Bottom: Two target texts:\nbullet points and a paragraph description\nwith a focus on faithfulness, persuasiveness, and\ninformativeness. While previous work has been\nlimited to short ad generation tasks conditioned on\ntitles (Chen et al., 2019; Chan et al., 2020), and\nused traditional neural models (Munigala et al.,\n2018; Zhang et al., 2019a) or statistical approaches\n(Wang et al., 2017), we focus on a data-to-text gen-\neration approach to product description generation\nfor an English e-commerce service. Speciﬁcally,\nwe explore various textual attributes and images\nas the input, and generate two types of product\ndescriptions: (1) bullet points, and (2) paragraph\ndescriptions (see Figure 1). Bullet points provide a\nlist of key information regarding a product, while\nparagraph descriptions are made up of sentences\nstructured into a coherent narrative.\nWe argue there are two underlying motivations\nfor the ad text generation task, especially for prod-\nuct descriptions. Application-wise, the utility is\nto improve the seller experience for e-commerce\nservices when registering a new product. The gen-\nerated descriptions can reduce the need for man-\nual data entry, and potentially improve sales due\nto better descriptions (in terms of attractiveness,\nstructure, and persuasiveness). Research-wise, ad\n234\ntext generation is an under-studied task, and ar-\nguably a good proxy for persuasive text generation\n(Wei et al., 2016; Rehbein, 2019; Luu et al., 2019;\nEl Baff et al., 2020).\nWhile previous work has discussed ad text gen-\neration of e-commerce service for a few product\ntypes such as fashion (Munigala et al., 2018), com-\nputers (Wang et al., 2017), and house decor (Hong\net al., 2021), in this work, we use twenty diverse\nproduct types and an additional three product types\nfor out-of-domain prediction. With this setting, we\naim to study model generalization and robustness\nover in-domain and out-of-domain test sets.\nTo summarize our contributions: (1) we study\nthe application of modern pretrained language mod-\nels based on data-to-text generation for product de-\nscription in an e-commerce service; (2) we explore\nmultimodal training by incorporating image fea-\ntures for ad generation and perform automatic and\nmanual evaluation; (3) we study model robustness\nfor out-of-domain prediction; and (4) we conduct\nanalysis of attributes that signiﬁcantly contribute\nto ad text generation.\n2 Related Work\nData-to-text generation is the task of translating a\nsemi-structured table to natural text, and has been\napplied in different real-world scenarios, such as\nweather forecasting reports (Liang et al., 2009),\nsport (Puduppully et al., 2019), health-care descrip-\ntions (Hasan and Farri, 2019), and biographies\n(Wang et al., 2020). While the goal of most pre-\nvious tasks is to generate descriptive text, there\nare few studies (Wang et al., 2017) on data-to-text\ngeneration for the advertisement domain, and the\nwork that has been done has tended to focus exclu-\nsively on the product type ofcomputer and be based\non pre-neural statistical approaches and template-\nbased techniques.\nPrevious work has mostly used titles of e-\ncommerce products to generate short ads in Chi-\nnese (Chen et al., 2019; Chan et al., 2020) and En-\nglish (Munigala et al., 2018; Kanungo et al., 2021).\nSimilarly, Zhang et al. (2019a) generate a product\ndescription for Chinese e-commerce, conditioned\non the title and a small number of attributes (with\nan average length of six words).1 In this work, we\ncomprehensively study product description genera-\ntion in English based on ten diverse attributes (à la\na data-to-text scheme, with the average number of\n1These attributes are not clearly described in the paper.\nAttributes Coverage #words |Vocab|(%) max µ σ\nTITLE 100 95 15.7 6.22 193,649\nPRODUCT TYPE 100 1 1 0 20\nCLASSIFICATION 100 1 1 0 3\nBRAND 99.49 17 1.58 0.88 46, 552\nKEYWORD 92.17 958 32.32 55.72 292,372\nCOLOR 80.19 32 1.44 1.01 18,839\nSIZE 69.96 16 1.82 1.44 15,187\nMODEL NUMBER 33.75 9 1.15 0.52 67,215\nPART NUMBER 47.64 12 1.08 0.41 91,084\nWEIGHT 20.76 1 1 0 1,786\nBULLET POINTS 100 766 86.8 67.9 225,784\nPARAGRAPH DESC.100 516 90.9 72.9 472,711\nTable 1: Statistics of attributes. For BULLET\nPOINTS, the average number of bullets in the overall\ndataset is 5.\nComponent % of noveln-grams\nA B 1 2 3 4\n10 attr. BP 86.7 96.3 98.1 98.7\n10 attr. PD 85.1 93.7 95.2 95.9\nBP PD 66.2 86.9 90.9 92.7\nTable 2: Abstractiveness of BULLET POINTS (BP)\nand PARAGRAPH DESCRIPTIONS (DP) based on\nnovel n-gram overlap. “10 attr.” means the concate-\nnatenation of all attributes, and values in the table are\ncalculated relative to component B.\nconcatenated attributes being 64 words in Table 1)\nthat incorporates joint training over images of the\nproduct.\n3 Data Construction\nWe use 200,000 e-commerce products spanning 20\ndifferent product types as described in Figure 2.\nFor copyright reasons we are not able to release\nthis data to the public. This dataset is randomly\nsplit into 180K/10K/10K training, development\nand test instances, respectively. We also create\nan Xtreme test set (4,266 samples) in which we ﬁl-\nter out test samples that have overlapping descrip-\ntions with the training data. Lastly, we addition-\nally use three different product types as an out-of-\ndomain test set, comprising 1,000 products of each\nof the three produce types: SAREE, COMPUTER,\nand CELLULAR_PHONE. In total, there are three\ndifferent test sets: (1) main; (2) Xtreme; and (3)\nout-of-domain.\nIn Table 1, we show the overall statis-\ntics of ten product atttributes and two tar-\nget texts: BULLET POINTS and PARAGRAPH\n235\n0\n10000\n20000\n30000\n40000\n50000\nShirtHome\nWireless Device\nSweater\nBed and bath\nOutdoor recreation\nSporting goods\nKitchenDress\nHat\nShoes\nPhone Accessory\nFashion rungHome furniture\nOuterwear\nPantsApparel\nAuto AccessoryOutdoor living\nArt Suplies\nSaree\nComputer\nCellular Phone\nFigure 2: Distribution of 20 product types in the main dataset and 3 additional product types from the out-of-\ndomain test set. The main and additional data is in English, and gathered from different regions (countries).\nDESCRIPTIONS. The selection of product at-\ntributes is based on a minimum coverage of 20%\nin the dataset. Overall, the ﬁve attributes with the\nhighest coverage are TITLE, PRODUCT TYPE,\nCLASSIFICATION, BRAND, and KEYWORD.2\nThe average length of BULLET POINTS and\nPARAGRAPH DESCRIPTIONS is 87 and 91, re-\nspectively, signiﬁcantly longer than most previous\nwork except Wang et al. (2017) who focused on\nthe product type of computer and tested only pre-\nneural statistical approaches (see Table 3).\nTo understand the abstractiveness of our dataset,\nin Table 2 we show the percentage of novel n-\ngrams in BULLET POINTS and PARAGRAPH\nDESCRIPTIONS. Overall, we observe that the\ntwo target texts are highly abstractive, with more\nthan 85% of novel n-grams, computed relative to\nthe concatenated attributes. We also found that\nthere is a high proportion of noveln-grams between\nthe two target texts.3 We suspect, though, that the\nlow lexical overlap between the two text types in\nthis task might not be attributed to paraphrasing or\nlexical choice, but rather to content selection.\n2CLASSIFICATION means other categories such as base\nproduct or different variation.\n3We also note that no previous work reported on the ab-\nstractiveness of their data.\nWork Lang. Product #words of #words of\nTypes source (µ) target (µ)\nZhang et al. (2019a) ZH N/A 18 25\nChan et al. (2020) ZH N/A 18 22\nHong et al. (2021) ZH 1 N/A 76\nWang et al. (2017) EN 1 N/A 117\nMunigala et al. (2018) EN 1 6 18\nKanungo et al. (2021) EN 1 19 6\nThis work EN 23 64 87 & 91\nTable 3: Dataset comparison between our work and pre-\nvious work\n4 Model\nProblem Formulation. As discussed in Sec-\ntion 3, a product in our dataset consists of up\nto ten attributes {a1,a2,a3,...,a 10}, one image\nI, and two target texts {t1,t2}. The goal of\nthis work is to learn a function that estimates\nthe probabilities P(t1|a1,a2,a3,...,a 10,I) and\nP(t2|a1,a2,a3,...,a 10,I).\nArchitecture. This work relies on pretrained\nlanguage models such as BERT (Devlin et al.,\n2019), T5 (Raffel et al., 2020), and BART (Lewis\net al., 2020a). To perform data-to-text generation,\nwe formulate a structured input based on special\ntokens that are randomly initialized before the ﬁne-\ntuning. The textual input is the concatenation of\neach attribute preceded by each corresponding spe-\ncial token (see Figure 3).\nTo accommodate multimodal training, we fol-\n236\nlow Xing et al. (2021) in extracting n Regions of\nInterest (RoIs) (i.e. bounding boxes) of the image\nusing detectron2, a pretrained masked R-CNN\n(He et al., 2017).4 Formally, an Image I is chun-\nked by detectron2 into {RoI1,RoI2,..., RoIn}.\nWe obtained a ﬁxed-size latent representation\nof each RoI based on intermediate features of\ndetectron2 (ResNet-101 (He et al., 2016)). To\nalign the embedding size with pretrained language\nmodels we use a linear layer. Similar to the textual\ninput, we also introduce a special token [IMAGE]\nthat is concatenated at the beginning of the input.\nFor the target texts, we introduce special tokens\n[BULLET POINTS] and [DESCRIPTION] as\nthe start token. Speciﬁcally, for bullet points, we\nconcatenate all points with token <q> as the sep-\narator. Finally, for the encoder-decoder, we use\nBERT-base with raw decoder following (Liu and\nLapata, 2019), BART-base, and T5-base, and train\nthe model with standard cross-entropy loss.\n5 Experiments\n5.1 Set-Up\nWe experiment in three settings: (1) training with\nthe text input only; (2) training with the image\nfeatures only; and (3) multimodal training incorpo-\nrating both text and image features, as depicted in\nFigure 3. For the text features, we encode the text\nusing the three pretrained LMs of BERT, BART,\nand T5, while for the other two we only experi-\nmented with BART because of its higher perfor-\nmance in the ﬁrst experiment. For image feature ex-\ntraction, we experimented with{10,20,30,40,50}\nRoIs, and tuned based on the development set. We\nreport results of 50 and 20 RoIs for the second and\nthird experiment, respectively.\nFor TITLE, KEYWORD, and other attributes, we\nset the maximum token length to 30, 100, and 10\nbased on the statistics in Table 1. This results in a\nmaximum token length of 220 for the source text\n(including the special tokens). For the two target\ntexts, we set the maximum token length to 250, and\ntrain them separately. Our preliminary experiments\nshow that performing multi-task training (i.e. using\nboth target texts at the same time) performs worse\nthan single-task training.\nWe use the huggingface PyTorch framework\n(Wolf et al., 2020) for our experiments with three\npretrained language models: BERT-base5 (Devlin\n4https://github.com/facebookresearch/detectron2\n5bart-base-uncased\net al., 2019), T5-base 6 (Raffel et al., 2020), and\nBART-base7 (Lewis et al., 2020a). All experiments\nare run on 4×V100 16GB GPUs.\nFor the BERT model, we follow Liu and Lap-\nata (2019) in adding a randomly-initialized trans-\nformer decoder (layers = 6, hidden size = 768,\nfeed-forward = 2,048, and heads = 8) on top\nof BERT, and train it for 200K steps. We\nuse the Adam optimizer and learning rate lr\n= 2 e−3 ×min(step−0.5,step ×20,000−1.5) and\n0.1 ×min(step−0.5,step ×10,000−1.5) for BERT\nand the transformer decoder, respectively. We use\na warmup of 20,000, a dropout of 0.2, a batch size\ntotal of 200 (10 ×4 GPUs ×gradient accumula-\ntion of 5), and save checkpoints every 10,000 steps.\nWe compute ROUGE scores (R1) to pick the best\ncheckpoint based on the development set.\nFor T5 and BART, we train them for 30 epochs\n(around 20K steps) with an initial learning rate of\n1e−4 (Adam optimizer). We use a total batch size\nof 300 (15 ×4 GPUs ×gradient accumulation of\n5), a warmup of 10% of total steps, and save check-\npoints for every 1,000 steps. We also compute\nROUGE scores (R1) to pick the best checkpoint\nbased on the development set.\n5.2 Evaluation\nAs discussed in Section 3, we use three differ-\nent test sets: main, Xtreme, and out-of-domain.\nFor automatic evaluation, we use ROUGE-1/2/L\n(Lin, 2004), BLEU-4 (Papineni et al., 2002), ME-\nTEOR (Banerjee and Lavie, 2005), and BERTScore\n(Zhang et al., 2019b). For BERTScore we compute\nthe F1 score using roberta-large (layer 17)\nas recommended by Zhang et al. (2019b).\nFor manual evaluation, we ﬁrst obtain 50 ran-\ndom samples for each of the three test sets, en-\nsuring there is no overlap between the main and\nXtreme test sets. We hire four expert workers with\nMaster degree qualiﬁcations to annotate four de-\nscriptions for each product: (1) gold; (2) BART;\n(3) BART+image; and (4) image only. The total\nnumber of annotations is 2 workers ×4 models ×\n150 samples ×2 descriptions = 2,400 annotations.\nOne worker was asked to work on either bullet\npoints or paragraph descriptions, and was paid $50.\nThere are ﬁve aspects that are manually eval-\nuated by our workers: (1) Fluency: the descrip-\ntion is ﬂuent and grammatically correct; (2) At-\n6t5-base\n7facebook/bart-base\n237\n[TITLE] text  [PART NUMBER] text [MODEL NUMBER] text\n[CATEGORY] text [COLOR] text [BRAND] text [SIZE]\ntext [CLASSIFICATION] text [WEIGHT] text [KEYWORD] text\nRoI1 RoI2 RoI3 ...  RoI20\nText\nToken embedding (TE)Masked R-CNNTE\nLinear Layer\nENCODER\nDECODER\ntarget1: [BULLET POINTS] item1 <q> item2 <q> .... itemn </s>\ntarget2: [DESCRIPTION] text </s> \nImageText\n[IMAGE] \nFigure 3: Model architecture used in this work.\nModel Main Test Xtreme Test Out-of-domain Test Avg.R-1 R-2 R-L B-4 M BS R-1 R-2 R-L B-4 M BS R-1 R-2 R-L B-4 M BS\nBULLET POINTS\nBERT 51.8 40.7 50.6 32.6 25.0 88.7 35.1 20.7 33.5 15.6 14.1 85.5 11.9 2.4 11.1 1.5 3.3 80.5 33.6\nT5 45.4 34.7 44.1 30.6 28.9 87.4 28.2 30 31.1 13.4 13.3 83.4 12.6 4.2 10.8 2.3 4.4 79.2 32.4\nBART 58.9 48.5 57.7 43.5 38.5 90.7 39.8 24.8 38.1 20.8 19.6 86.5 17.5 6.1 16.5 3.0 5.0 81.538.7\nImage only 43.4 30.9 32.3 27.5 25.3 87.3 27.6 13.5 26.1 11.4 11.6 83.8 9.9 0.7 9.0 0.8 2.9 79.4 29.1\nBART+Image59.3 48.9 58.1 43.7 38.6 90.8 40.1 24.9 38.4 21.0 19.7 86.6 17.55.9 16.4 2.8 4.9 81.4 38.8\nPARAGRAPH DESCRIPTIONS\nBERT 41.0 30.1 35.5 24.2 19.4 86.4 27.5 15 21.4 10.9 10.5 83.3 10.9 1.5 7.2 0.9 2.5 79.6 28.2\nT5 40.7 31.9 36.7 28.6 29.3 85.8 23.8 14.2 19.8 11.5 12.6 81.2 10.8 4.4 9.1 2.2 4.6 77.8 29.2\nBART 54.8 45.1 50.1 40.2 37.7 90.1 36.1 22.6 29.5 18.3 18.2 85.916.5 5.6 12.0 2.8 5.5 81.1 36.2\nImage only 41.1 29.4 35.4 26.6 25.6 86.9 24.7 10.9 18.2 9.1 10.1 83.1 12.2 0.8 7.4 0.8 3.3 79.9 28.1\nBART+Image54.9 45.3 50.3 40.4 37.9 90.235.8 22.4 29.318.318.0 85.8 17.2 5.6 12.32.7 5.6 81.5 36.3\nTable 4: Main experimental results of automatic metrics. R-1, R-2, R-L, B-4, M, and BS are ROUGE-1, ROUGE-2,\nand ROUGE-3, BLEU-4, METEOR, and BERTScore, respectively.\ntractiveness: the description is interesting and eye-\ncatching; (3) Persuasive words: the description\nuses persuasive words or phrases; (4) Faithfulness:\ninformation in the description is captured by the\nimage and the attributes; and (5) Informativeness:\nthe description is informative and complete relative\nto the available attributes. Except for the third as-\npect which is binary (yes/no), we use a slider scale\nwith values between 0–100 for all aspects.\nIn manual evaluation, workers were presented\nthe product image and list of text attributes with\nfour different descriptions. The four descriptions\nare shufﬂed, so the model information of each de-\nscription is not apparent to the worker. Workers\nwere asked to carefully read each description, and\nthen asked to put the evaluation scores in the avail-\nable ﬁeld.\n5.3 Results\nTable 4 shows the experimental results based\non the automatic metrics. Overall, we ob-\nserve similar trends for both BULLET POINTS\nand PARAGRAPH DESCRIPTIONS, namely that\nBART is substantially better than T5 and BERT\nacross the three test sets. Using only image features\nfor generating both ad text types yields a compa-\nrable score to T5, but tends to be lower for almost\nall test sets and metrics. The multimodal training\n(i.e. “BART+image”) slightly improves BART per-\nformance for the main test set, but achieves mixed\nresults for the Xtreme and out-of-domain test sets\nwith both BULLET POINTS and PARAGRAPH\nDESCRIPTIONS. We also observe that Xtreme\nand the out-of-domain test sets are harder, with\nhigh performance gaps, relative to the main test set.\n238\nModel Main Test Xtreme Test Out-of-domain Test Avg.\nFlu. Att. Per. Fa. Inf. Flu. Att. Per. Fa. Inf. Flu. Att. Per. Fa. Inf.\nBULLET POINTS\nGold 0.62 0.59 0.77 0.54 0.52 0.58 0.56 0.74 0.57 0.55 0.62 0.58 0.51 0.59 0.60 0.59\nImage only 0.63 0.61 0.82 0.43 0.43 0.65 0.61 0.87 0.43 0.44 0.64 0.56 0.740.13 0.27 0.53\nBART 0.63 0.59 0.78 0.56 0.53 0.61 0.58 0.64 0.58 0.56 0.48 0.42 0.27 0.53 0.52 0.55\nBART+image0.66 0.63 0.79 0.58 0.56 0.64 0.62 0.72 0.57 0.54 0.44 0.42 0.30 0.55 0.54 0.57\nPARAGRAPH DESCRIPTIONS\nGold 0.82 0.52 0.29 0.60 0.48 0.74 0.46 0.31 0.53 0.47 0.84 0.49 0.18 0.52 0.48 0.44\nImage only 0.77 0.43 0.29 0.42 0.38 0.81 0.43 0.34 0.43 0.41 0.74 0.25 0.20 0.17 0.16 0.33\nBART 0.82 0.53 0.31 0.62 0.50 0.74 0.50 0.21 0.60 0.53 0.69 0.39 0.16 0.53 0.42 0.44\nBART+image 0.810.53 0.33 0.60 0.51 0.76 0.52 0.23 0.59 0.53 0.71 0.41 0.15 0.56 0.41 0.45\nTable 5: The primary experimental results for manual evaluation. Flu., Att., Per., Fa., and Inf. denote Fluency, At-\ntractiveness, Persuasiveness, Faithfulness, and Informativeness, respectively. The presented scores are the average\nof two annotations. Entries in bold refer to the best overall score (excluding Gold texts).\nAspects BULLET POINTS DESCRIPTION\nFluency 0.51 0.50\nAttractiveness 0.50 0.42\nPersuasiveness 0.39 0.32\nFaithfulness 0.51 0.41\nInformativeness 0.34 0.45\nTable 6: Pearson correlation scores between two an-\nnotators in manual evaluation. For persuasiveness we\npresent the Kappa score.\nFor example, in BULLET POINTS, ROUGE-1 of\nBART drops substantially by −19.1 and −41.4 in\nthe Xtreme and out-of-domain test sets, resp., im-\nplying that the model does not generalize well to\ndifferent test sets.\nIn Table 6 we show the inter-annotator agree-\nment of manual evaluation in the form of Pearson\ncorrelation for ﬂuency, attractiveness, faithfulness,\nand informativeness; and the Kappa score for per-\nsuasiveness. Overall, we found that annotators have\nmoderate correlation and agreement. In Table 5,\nscores of the Gold text can be interpreted as the up-\nper bound of the manual evaluation. Note that for\nfaithfulness and informativeness, these aspects are\nonly evaluated based on the ten selected attributes.\nFor the main and Xtreme test sets in Table 5,\nmost models generate ﬂuent, attractive, persua-\nsive, faithful, and informative texts for BULLET\nPOINTS and PARAGRAPH DESCRIPTIONS,\nrelative to the performance of the gold texts.\nWhen using only image features (the “image only”\nmodel), the model’s faithfulness and informative-\nness decrease markedly, indicating the importance\nof textual attributes for this task. BART and\nBART+image models yield comparable results\nwith the gold texts, with slightly better faithfulness\nand informativeness.8\nFor the out-of-domain test set, we observe that\nthe human evaluation performance over the three\nmodels (Image only, BART, and BART+image) is\ngenerally lower than the gold text. Interestingly, we\nﬁnd that the “image only” model generates ﬂuent\nand persuasive texts, but with substantially low\nfaithfulness and informativeness. It is also worth\nmentioning that the BART model’s performance is\nnot as good as for the main test set, which indicates\nthe out-of-domain challenge in applying models in\nreal world scenarios.\nIn addition, we calculated the average perfor-\nmance of the manual evaluation, and found that the\nBART+image model performs best for both target\ntexts. These results are in line with the averaged\nautomatic evaluation scores in Table 4.\nBased on the manual evaluation results in Ta-\nble 5, the relatively low faithfulness scores for the\ngold texts (around 0.5–0.6) suggests that they con-\ntain new information that is not found in the in-\nput attributes. Although this means the gold texts\nare not faithful, they are likely to be still factu-\nally correct, as they are written by the product\nsellers (Maynez et al., 2020). Taking the faithful-\nness scores of the gold texts as the upper bound,\nwe could conclude that the BART models are per-\nforming as well as they could (seeing that they are\ntrained on not very faithful target texts in the ﬁrst\nplace). Ultimately, our results in this task high-\n8These results are to be expected in the manual evaluation,\nsince both aspects are only examined based on the ten selected\nattributes.\n239\nTITLE: Yosoo Digital Clock Portable\nElectronic Bell with Backlight LCD Screen\nDisplay Alarm Clock Car Desk Table\nDecoration Clock(Pink) \nPART NUMBER: Yosoo6wkf82orh7 \nCATEGORY: HOME \nBRAND: Yosoo \nCLASSIFICATION: base_product \nWEIGHT: 43.0 \nKEYWORD: LCD Digital Clock, Digital\nAlarm Clock, Portable Digital Clock,Table\nDesk Digital Clock, Digital Clock\nGold:\nLarge LCD screen for an easy and accurate reading of time\nDisplay the current time, week clearly, with alarm function and\nsnooze function\nPress the light button at the back of the clock, the backlight will\nbright 3 seconds or so, convenient for you to read time at night\nBuilt with a clip and stand, you can put it on your desk or table\nLight weight and compact size, multi-colors are available. A good\nchoice to improve concept of punctuality\nDelivery Time 10-15 Days \nReturn Range 30 Days\nIf you have any questions, please feel free to contact us and we\nwill get back to you within 24 hours.\nThe clock is made of high-quality materials, durable and easy to\nclean.\nThis digital clock is made of high quality materials, durable and easy\nto clean.\nThe clock is equipped with backlight LCD screen display, which is\nvery convenient to read.\nIt can be used in bedroom, living room, kitchen, office, etc. It is also a\ngood gift for your family and friends.\nIf you have any questions, please feel free to contact us and we will\nreply you within 24 hours.\nTITLE: LJSWG Men's Oklahoma Flag\nOklahoma National T-shirt Asphalt L \nCATEGORY: SHIRT \nCOLOR: Asphalt \nBRAND: LJSWG \nSIZE: Large \nCLASSIFICATION: base_product \nKEYWORD: Personalised  Cheap Men Tee \nCasual Cotton, Girls Games Plus Size Clothing\nCaptain America: Civil War AKON, Oklahoma\nCity\nPARAGRAPH DESCRIPTION\nMen's Oklahoma Flag Oklahoma National Custom Text, ID, Name Or\nMessage On High Quality Hanes Cotton T-Shirts. 100% Preshrunk\nCotton Takes The Worry Away From Shrunk-age. Seamless Rib At\nNeck And Collar. Double-needle Stitching For Dorabiltiy. This Classic\nCrew Neckline T-shirtis Is Great For Every Occasion And Situation.The\nDesign Is Printed With Advanced Printing Technology. It Is Printed\nWith A Water-soluble And Eco-friendly Ink.Trendy, Brightly Colored\nGraphics. A Unique Gift Idea For A Friend Or Family Member. \n100% Soft Cotton Fabric With A Soft Touch And Quality Printing\nTechniques.It Will Never Fade, Peel Or Crack And Can Be Machine\nWashed & Ironed.This Customized T Shirt Will Be The Best And\nSincere Gift For Your Family,friends And Team. \nOklahoma Flag Oklahoma National T-shirt. Art Heat Press Print On\nFront. Wash Inside Out In Cold Water, Hand Dry Recommended. Most\nOf Our Designs Are Available In Men`s Sizes.Please Check Our Store\nFor All Other Varieties. \nBULLET POINTS\nBART:\nBART + image:\nGold:\nBART:\nBART + image:\nFigure 4: Example of generated BULLET POINTS and PARAGRAPH DESCRIPTIONS.\nlighted the fact that our current human faithful-\nness evaluation does not always capture factuality,\nprompting further questions on how we can assess\nthis dimension, which we leave for future work.\nFigure 4 depicts some example outputs of\nthe BART models for BULLET POINTS and\nPARAGRAPH DESCRIPTIONS. The ﬁrst exam-\nple shows that the prediction of the BART+image\nmodel contains better content than the BART text-\nonly model, with a description of the LCD screen\nand usage examples. Similarly in the second ex-\nample, the BART+image model generates more\nspeciﬁc content for the t-shirt product by mention-\ning Flag Oklahoma National.\n6 Analysis\nWhich attributes contribute to ad generation?\nTo answer this question, we performed an ab-\nlation study using the BART models. We de-\ncode both BULLET POINTS and PARAGRAPH\nDESCRIPTIONS using different numbers of at-\ntributes as context, and report the average auto-\nmatic performance in Table 7.\nWe observe there are three prominent at-\ntributes for this task — TITLE, BRAND, and\nKEYWORD — for both BULLET POINTS and\nPARAGRAPH DESCRIPTIONS. Interestingly, us-\ning only TITLE can produce 32.98 and 29.93\naverage performance, and adding KEYWORD to\nthe input boosts performance by 11.05 and\n10.57, for BULLET POINTS and PARAGRAPH\nDESCRIPTIONS, respectively.\n7 Discussion and Conclusion\nIn this work, we described the ﬁrst attempt at mul-\ntimodal training for ad generation by incorporating\nimage representations and text embeddings as in-\nput. We found that multimodal training yields the\nbest performance in terms of overall scores in the\nboth automatic and manual evaluation. We observe\nthat modern pretrained language models can gener-\nate ﬂuent advertisements, but are less faithful and\n240\nAttributes (#Attr) BULLET POINTS PARAGRAPH DESCRIPTIONS\nAvg. ∆ Avg. ∆\nTITLE(1) 32.98 32.98 29.93 29.93\nprev. +PRODUCT TYPE(2) 34.53 1.55 31.38 1.45\nprev. +CLASSIFICATION(3) 34.53 0.00 31.38 0.00\nprev. +BRAND(4) 39.75 5.22 39.33 7.95\nprev. +KEYWORD(5) 50.80 11.05 49.90 10.57\nprev. +COLOR(6) 51.63 0.83 50.15 0.25\nprev. +SIZE(7) 53.15 1.52 51.03 0.88\nprev. +PART NUMBER(8) 55.12 1.97 52.32 1.28\nprev. +MODEL NUMBER(9) 56.30 1.18 52.77 0.45\nprev. +WEIGHT(10) 56.30 0.00 52.77 0.00\nTable 7: Ablation study on the main test set using BART by incrementally adding different attributes. Avg means\nthe average score of ROUGE-1, ROUGE-2, ROUGE-L, BLEU-4, METEOR, and BERTScore. ∆ means the\ndifference score between the given and previous row. The bold entries are the top-3 highest ∆ scores.\ninformative, especially in out-of-domain settings.\nCan pretrained language models generate per-\nsuasive, faithful, and informative ad text for prod-\nuct descriptions? The answer to this question is\nyes to a certain extent, particularly for in-domain\nscenarios. And although the BART models have\nsimilar human faithfulness performance to the gold\ntexts, we believe that it does not necessarily imply\nthat they are factually correct and further validation\nis necessary. One way forward may be to allow hu-\nman judges to have access to some external knowl-\nedge (e.g. search engines or product catalogues),\nwhich will help them assess the factuality of the\ngenerated texts.\nFurthermore, since the product descriptions in\nour e-commerce dataset might introduce new in-\nformation, retrieval augmented generation (Lewis\net al., 2020b; Kim et al., 2020; Shuster et al., 2021)\nis one potential direction for future work. This\nis because information on some products is likely\nto be available on the Internet, and incorporating\nit into the generation model could potentially im-\nprove the resulting ad text.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nZhangming Chan, Yuchi Zhang, Xiuying Chen, Shen\nGao, Zhiqiang Zhang, Dongyan Zhao, and Rui Yan.\n2020. Selection and generation: Learning towards\nmulti-product advertisement post generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3818–3829, Online. Association for Computa-\ntional Linguistics.\nQibin Chen, Junyang Lin, Yichang Zhang, Hongxia\nYang, Jingren Zhou, and Jie Tang. 2019. Towards\nknowledge-based personalized product description\ngeneration in e-commerce. In Proceedings of the\n25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 3040–\n3050.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRoxanne El Baff, Henning Wachsmuth, Khalid\nAl Khatib, and Benno Stein. 2020. Analyzing the\nPersuasive Effect of Style in News Editorial Argu-\nmentation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3154–3160, Online. Association for Computa-\ntional Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4319–4338, Online. Associa-\ntion for Computational Linguistics.\n241\nDonghoon Ham, Jeong-Gwan Lee, Youngsoo Jang,\nand Kee-Eung Kim. 2020. End-to-end neural\npipeline for goal-oriented dialogue systems using\nGPT-2. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 583–592, Online. Association for Com-\nputational Linguistics.\nSadid A Hasan and Oladimeji Farri. 2019. Clini-\ncal natural language processing with deep learning.\nIn Data Science for Healthcare, pages 147–171.\nSpringer.\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross\nGirshick. 2017. Mask R-CNN. In Proceedings of\nthe IEEE International Conference on Computer Vi-\nsion, pages 2961–2969.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages\n770–778.\nYunsen Hong, Hui Li, Yanghua Xiao, Ryan McBride,\nand Chen Lin. 2021. SILVER: Generating persua-\nsive Chinese product pitch. In PAKDD (2), pages\n652–663. Springer.\nYashal Shakti Kanungo, Sumit Negi, and Aruna Ra-\njan. 2021. Ad headline generation using self-critical\nmasked language model. In Proceedings of the\n2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies: Industry Papers,\npages 263–271, Online. Association for Computa-\ntional Linguistics.\nJihyeok Kim, Seungtaek Choi, Reinald Kim Amplayo,\nand Seung-won Hwang. 2020. Retrieval-augmented\ncontrollable review generation. In Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics, pages 2284–2295, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented genera-\ntion for knowledge-intensive NLP tasks. Advances\nin Neural Information Processing Systems, 33:9459–\n9474.\nPercy Liang, Michael Jordan, and Dan Klein. 2009.\nLearning semantic correspondences with less super-\nvision. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th In-\nternational Joint Conference on Natural Language\nProcessing of the AFNLP, pages 91–99, Suntec, Sin-\ngapore. Association for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nKelvin Luu, Chenhao Tan, and Noah A. Smith. 2019.\nMeasuring online debaters’ persuasive skill from\ntext over time. Transactions of the Association for\nComputational Linguistics, 7:537–550.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nPooya Moradi, Nishant Kambhatla, and Anoop Sarkar.\n2021. Measuring and improving faithfulness of at-\ntention in neural machine translation. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 2791–2802, Online. Associa-\ntion for Computational Linguistics.\nVitobha Munigala, Abhijit Mishra, Srikanth G Tamil-\nselvam, Shreya Khare, Riddhiman Dasgupta, and\nAnush Sankaran. 2018. Persuaide! an adaptive per-\nsuasive text generation system for fashion domain.\nIn Companion Proceedings of the The Web Confer-\nence 2018, pages 335–342.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with entity modeling. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2023–2035, Florence, Italy. Association for Compu-\ntational Linguistics.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,\nNan Duan, Jiusheng Chen, Ruofei Zhang, and Ming\n242\nZhou. 2020. ProphetNet: Predicting future n-gram\nfor sequence-to-SequencePre-training. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 2401–2410, Online. Associa-\ntion for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nInes Rehbein. 2019. On the role of discourse relations\nin persuasive texts. In Proceedings of the 13th Lin-\nguistic Annotation Workshop, pages 144–154, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nJinpeng Wang, Yutai Hou, Jing Liu, Yunbo Cao, and\nChin-Yew Lin. 2017. A statistical framework for\nproduct description generation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 187–192, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nZhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu,\nand Changyou Chen. 2020. Towards faithful neural\ntable-to-text generation with content-matching con-\nstraints. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1072–1086, Online. Association for Computa-\ntional Linguistics.\nZhongyu Wei, Yang Liu, and Yi Li. 2016. Is this post\npersuasive? ranking argumentative comments in on-\nline forum. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 195–200, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYiran Xing, Zai Shi, Zhao Meng, Gerhard Lakemeyer,\nYunpu Ma, and Roger Wattenhofer. 2021. KM-\nBART: Knowledge enhanced multimodal BART for\nvisual commonsense generation. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 525–535, Online.\nAssociation for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter Liu. 2020. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn ICML 2020: 37th International Conference on\nMachine Learning, volume 1, pages 11328–11339.\nTao Zhang, Jin Zhang, Chengfu Huo, and Weijun Ren.\n2019a. Automatic generation of pattern-controlled\nproduct description in e-commerce. In The World\nWide Web Conference, pages 2355–2365.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019b. BERTScore:\nEvaluating text generation with BERT. In Interna-\ntional Conference on Learning Representations.\n243",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7987358570098877
    },
    {
      "name": "Paragraph",
      "score": 0.6777046322822571
    },
    {
      "name": "Language model",
      "score": 0.639563262462616
    },
    {
      "name": "Natural language processing",
      "score": 0.6379797458648682
    },
    {
      "name": "Product (mathematics)",
      "score": 0.6096312999725342
    },
    {
      "name": "Task (project management)",
      "score": 0.5219951868057251
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4738136827945709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45789676904678345
    },
    {
      "name": "Information retrieval",
      "score": 0.4496666193008423
    },
    {
      "name": "World Wide Web",
      "score": 0.26354071497917175
    },
    {
      "name": "Mathematics",
      "score": 0.07579478621482849
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ],
  "cited_by": 6
}