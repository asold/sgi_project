{
    "title": "Accuracy Comparison of CNN, LSTM, and Transformer for Activity Recognition Using IMU and Visual Markers",
    "url": "https://openalex.org/W4387010474",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2905580369",
            "name": "Maria Fernanda Trujillo Guerrero",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        },
        {
            "id": "https://openalex.org/A4314934026",
            "name": "Stadyn Román-Niemes",
            "affiliations": [
                "Universidad Yachay Tech"
            ]
        },
        {
            "id": "https://openalex.org/A4290636634",
            "name": "Milagros Jaén-Vargas",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        },
        {
            "id": "https://openalex.org/A3159803751",
            "name": "Alfonso Cadiz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098430393",
            "name": "Ricardo Fonseca",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2441392916",
            "name": "José Javier Serrano Olmedo",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2511719117",
        "https://openalex.org/W2002577311",
        "https://openalex.org/W2009398857",
        "https://openalex.org/W2922355425",
        "https://openalex.org/W4249972823",
        "https://openalex.org/W2164760349",
        "https://openalex.org/W2927123024",
        "https://openalex.org/W3022796439",
        "https://openalex.org/W2064327970",
        "https://openalex.org/W3089186933",
        "https://openalex.org/W2946273500",
        "https://openalex.org/W1930624869",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3091225957",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4214843096",
        "https://openalex.org/W3134689613",
        "https://openalex.org/W2120133484",
        "https://openalex.org/W3215807065",
        "https://openalex.org/W6743533141",
        "https://openalex.org/W2765473510",
        "https://openalex.org/W2886735509",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W4322743576",
        "https://openalex.org/W4308822715",
        "https://openalex.org/W3094625321",
        "https://openalex.org/W3174752903",
        "https://openalex.org/W4313830062",
        "https://openalex.org/W4379537994",
        "https://openalex.org/W4283273834",
        "https://openalex.org/W2917819557",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2514173981",
        "https://openalex.org/W123295786",
        "https://openalex.org/W3082120068",
        "https://openalex.org/W4289172806",
        "https://openalex.org/W4220659807",
        "https://openalex.org/W3002496996",
        "https://openalex.org/W2596563802",
        "https://openalex.org/W3211040285",
        "https://openalex.org/W1988066034",
        "https://openalex.org/W1973376993",
        "https://openalex.org/W4290635396",
        "https://openalex.org/W2749962782",
        "https://openalex.org/W1550219794",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3007328579"
    ],
    "abstract": "Human activity recognition (HAR) has applications ranging from security to healthcare. Typically these systems are composed of data acquisition and activity recognition models. In this work, we compared the accuracy of two acquisition systems: Inertial Measurement Units (IMUs) vs Movement Analysis Systems (MAS). We trained models to recognize arm exercises using state-of-the-art deep learning architectures and compared their accuracy. MAS uses a camera array and reflective markers. IMU uses accelerometers, gyroscopes, and magnetometers. Sensors of both systems were attached to different locations of the upper limb. We captured and annotated 3 datasets, each one using both systems simultaneously. For activity recognition, we trained 8 architectures, each one with different operations and layers configurations. The best architectures were a combination of CNN, LSTM, and Transformer achieving test accuracy from 89&#x0025; to 99&#x0025; on average. We evaluated how feature selection reduced the sensors required. We found IMU and MAS data were able to distinguish correctly the arm exercises. CNN layers at the beginning produced better accuracy on challenging datasets. IMU had advantages over other acquisition systems for activity recognition. We analyzed the relations between models accuracy, signal waveforms, signals correlation, sampling rate, exercise duration, and window size. Finally, we proposed the use of a single IMU located at the wrist and a variable-size window extraction.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nAccuracy comparison of CNN, LSTM,\nand Transformer for activity recognition\nusing IMU and visual markers\nMARÍA FERNANDA TRUJILLO-GUERRERO1, STADYN ROMÁN-NIEMES2, MILAGROS\nJAÉN-VARGAS 1, ALFONSO CADIZ3, RICARDO FONSECA3, AND JOSÉ JAVIER\nSERRANO-OLMEDO.1 4\n1Center for Biomedical Technology (CTB), Universidad Politécnica de Madrid, 28223 Madrid, Spain (e-mail: mariafernanda.trujillo.guerrero@alumnos.upm.es,\nmilagros.jaen@ctb.upm.es, josejavier.serrano@ctb.upm.es)\n2Yachay Tech University, Urcuquí, Imbabura, Ecuador (e-mail: stadyn.roman@yachaytech.edu.ec)\n3Digevo, Santiago, Chile (email: rfonseca@digevo.com, acadiz@digevo.com)\n4Centro de Investigación Biomédica en Red para Bioingeniería, Biomateriales y Nanomedicina, Instituto de Salud Carlos III, Spain\nCorresponding author: María Fernanda Trujillo-Guerrero (e-mail: mariafernanda.trujillo.guerrero@alumnos.upm.es).\nFunding: This work was partially financed by the Ministerio de Ciencia, Innovación y Universidades of Spain, Ref.:\nPGC2018-097531-B-I00. Cloud computing was sponsored by Microsoft Founders Hub.\nABSTRACT\nHuman activity recognition (HAR) has applications ranging from security to healthcare. Typically these\nsystems are composed of data acquisition and activity recognition models. In this work, we compared the\naccuracy of two acquisition systems: Inertial Measurement Units (IMUs) vs Movement Analysis Systems\n(MAS). We trained models to recognize arm exercises using state-of-the-art deep learning architectures\nand compared their accuracy. MAS uses a camera array and reflective markers. IMU uses accelerometers,\ngyroscopes, and magnetometers. Sensors of both systems were attached to different locations of the upper\nlimb. We captured and annotated 3 datasets, each one using both systems simultaneously. For activity\nrecognition, we trained 8 architectures, each one with different operations and layers configurations. The\nbest architectures were a combination of CNN, LSTM, and Transformer achieving test accuracy from 89%\nto 99% on average. We evaluated how feature selection reduced the sensors required. We found IMU and\nMAS data were able to distinguish correctly the arm exercises. CNN layers at the beginning produced better\naccuracy on challenging datasets. IMU had advantages over other aquisition systems for activity recognition.\nWe analyzed the relations between models accuracy, signal waveforms, signals correlation, sampling rate,\nexercise duration, and window size. Finally, we proposed the use of a single IMU located at the wrist and a\nvariable-size window extraction.\nINDEX TERMS Human activity recognition, IMU, Movement Analysis System, visual marker, CNN,\nLSTM, Transformer, arm exercises.\nI. INTRODUCTION\nHuman activity recognition (HAR) studies the capture sys-\ntems and algorithms to recognize activities performed by\npeople in any situation. Data for this task can be captured by\ninertial sensors, cameras with visual markers, and cameras\nusing human pose estimation, EEG, or EMG. All these sen-\nsors produce time series data, in consequence, the computer\nalgorithms able to classify these activities are: Recurrent\nneural networks, Convolutional neural networks, Long Short\nTerm Memory networks, and Transformer networks [1],\n[2]. HAR is an essential field of study in computer vision\nand artificial intelligence. It has many potential applications\nin various industries including security, surveillance, and\nhealthcare [3], [4]. HAR systems are used to monitor the\nmovements of elderly individuals in care facilities and to alert\ncaregivers if they fall or exhibit other signs of distress [5], [6],\n[7], [8], [9]. HAR technology is being used in sports training\nto help athletes measure their performance by providing real-\ntime feedback on their exercises [10].\nOne of the key challenges in human activity recognition is\nthe high variability of human movements and activities. Due\nto this variability, traditional machine learning algorithms\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\noften struggle to accurately classify movements and activities\n[11]. To overcome this challenge, researchers have developed\na range of techniques and approaches, including deep learn-\ning and other advanced machine learning methods. Our work\naims to identify the deep learning architectures that achieve\nhigher accuracies.\nOne capture system is the Movement Analysis System\n(MAS) which detects the movement of human joints and\nlimbs. MAS uses infrared cameras and visual markers to\nmeasure the physical world and obtain the dimensions and\npositions of objects. Data is generated using specialized\nsoftware to analyze images taken from different angles and\npositions to create a 3D model of the object [12]. MAS takes\nthe video streaming as input and estimates the positions of\nthe visual markers worn by a person [13].\nAnother capture system is the use of inertial measurement\nunits (IMUs). An IMU is a system of sensors that measures\nthree axes acceleration, angular velocity, and magnetic field\n[14]. IMUs are attached to different parts of the human body\nto identify activities such as walking, running, or jumping.\nMaking IMU suitable for healthcare applications such as\nrehabilitation programs to provide feedback on movements\nand monitor progress during therapy sessions [15]. Our work\naims to identify the advantages and disadvantages of IMU\nand MAS capture methods.\nHAR has three levels of abstraction in exercise recog-\nnition: full body exercise, single limb exercise, and stages\ninside a single exercise. We captured data and built models to\nclassify upper limb exercises and to distinguish the flexion or\nextension stage inside an exercise. Most HAR studies [16],\n[17], [18], [10], [19], [20], [7], and [21], perform exercises\nthat involve the entire human body and differ one from each\nother such as walking, climbing stairs, sitting, jumping, or\nrunning. Our work studies 6 specific exercises of the upper\nlimb where they share common behavior making them harder\nto distinguish.\nIn this work, we compared the accuracies of two acquisi-\ntion systems: Inertial Measurement Unit (IMU) vs Movement\nAnalysis System (MAS). Studied the time series data using\nplots and labeling the signal according to each exercise.\nUnderstand graphically which sensor discriminates exercises\nbetter (acc, gyro, mag, visual marker). We trained 8 SOA\ndeep learning architectures to recognize arm exercises and\ncompared their accuracy. Designed 8 deep learning archi-\ntectures using different layers and operations. Compared the\nperformance for every architecture and capturing method to\nidentify the best combination. Applied feature selection to\nidentify minimum sensors and locations to correctly recog-\nnize exercises. We studied the effect of different sampling\nrates, exercise duration, and window size. Finally, we pro-\nposed the use of a single IMU located at the wrist and a\nvariable-size window extraction.\nII. RELATED WORKS\nA. ACTIVITY RECOGNITION USING IMU\nThe use of IMUs for human activity recognition has become\npopular in recent years due to the widespread availability\nof sensors in wearable devices such as smartphones, wrist-\nwatches, and fitness trackers. Advances in machine learning\nand deep learning allowed the development of sophisticated\nalgorithms for recognizing human activities from IMU data.\nThese algorithms involve all kinds of architectures from\nartificial neural networks which process data with spatial\nand temporal characteristics. Neural networks are effective\nin recognizing complex patterns in time series data and can\nbe trained on large datasets to achieve high accuracy.\nMonitoring and analyzing human motion can provide valu-\nable information for various applications. In 2014, Ronao and\nCho [16] proposed a multi-task learning approach for human\nactivity recognition. Using accelerometer data, the system\nuses a single CNN to learn multiple tasks, including activity\nrecognition and pose estimation. The authors found that\nthis approach improved the overall accuracy of the activity\nrecognition system. Then, in 2017 Yarnan et al. proposed\na framework for detecting arm and human activities based\non data fusion from inertial measurement units (IMUs) and\nsurface electromyography (EMG) sensors [22]. Supervised\nand unsupervised machine learning algorithms were used\nto train the models and obtain evaluation indicators. The\ncombined IMU and EMG data outperformed the IMU data\nalone and the EMG data alone, significantly reducing the\nerror in determining activities for supervised algorithms.\nIn 2018, Xiong et al. [17] proposed a two-stage model\nfor recognizing activities from accelerometer data. The first\nstage of the model uses a CNN to extract spatial and temporal\nfeatures from the data. The second stage then uses a recurrent\nneural network (RNN) to combine features from the CNN\nwith contextual information to recognize the activity.\nIn 2019, Sarcevic et al. developed a system to detect arm\nand body movements using wrist sensors that contained an\naccelerometer, a gyroscope, and a magnetometer [6]. Multi-\nple datasets were tested using various feature extractive ap-\nproaches, sampling frequencies, processing window widths,\nand sensor combinations. The authors achieved almost 90%\naccuracy on validation data.\nLu and Tong, 2019, [18] worked on HAR using a single 3-\naxis accelerometer, focused on movement monitoring using\nwearable sensors and devices. Their method consists of en-\ncoding 3-axis signals as 3-channel images using a modified\nrecurrence plot. Then, residual neural networks were used to\nclassify images and, thus, signals. As a result, the authors\nobtained highly competitive accuracies and good efficiencies\non the ASTRI motion dataset, which contains data on human\nhand movements, and the ADL Dataset from wrist-worn\naccelerometer data.\nThe work of Avilés et al. (2019) presented a framework to\nrecognize user movement using a smartphone equipped with\na tri-axial accelerometer and a tri-axial gyroscope sensor.\nThe framework used three parallel CNNs for local feature\nextractive, later fused in the classification stage. The whole\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nCNN scheme is based on a feature fusion of a fine CNN,\na medium CNN, and a coarse CNN [10]. The algorithm\nsuccessfully classified six human activities: walking, walking\nupstairs, walking downstairs, sitting, standing, and laying.\nYen, Liao, and Huang (2020) proposed a wearable device\ncapable of recognizing six basic activities using deep learn-\ning and data from a gyroscope and an accelerometer [19].\nThey used waist devices worn by dialysis patients, whose\nactivities could not be accurately determined using wrist\ndevices. The model achieved recognition rates of 95.99% and\n93.77%. That same year, Lemieux and Noumeir proposed\na hierarchical CNN model for human activity recognition\n[23]. The model consists of two levels of CNNs. The first\nlevel extracts spatial features from the accelerometer data and\nthe second level combines the spatial features with temporal\ninformation to recognize the activity.\nIn 2020 Clouthier et al. analyzed the movement of athletes\n[20]. They collected optical motion data on 417 athletes\nperforming 13 athletic movements. The authors trained an\nexisting deep neural network architecture that combines con-\nvolutional and recurrent layers. They obtained classification\naccuracies of 90.1 and 90.2% for full body measurements.\nThe authors concluded that classifying athletic movements\nusing wearable sensors was feasible.\nMore recent research is the work of Uddin and Soylu\n(2021) [7], focused on the well-being of elderly people using\nwearable sensors to detect unprecedented events such as\nfalls or other health risks. The authors proposed a \"body\nsensor-based activity modeling and recognition system using\ntime-sequential information-based deep Neural Structured\nLearning (NSL)\" [7]. The algorithm is powered by data from\nmultiple wearable sensors, which then undergo statistical\nfeature processing. The framework is powered by kernel\ndiscriminant analysis (KDA) and long short-term memory\n(LSTM) based models. The authors achieved around 99%\nrecall on the mobile health application dataset (MHEALTH)\n[24]. The framework also surpassed the recall rate of other al-\ngorithms, such as deep belief networks, convolutional neural\nnetworks, and recurrent neural networks.\nAnother recent research work is the paper of Han et al.\n(2022) which focused on enhancing the convolution capac-\nities of CNNs instead of modifying the architectures [25].\nThe authors proposed the idea of heterogeneous convolution\nfor activity recognition tasks. All filters within a specific\nconvolutional layer are separated into two uneven groups.\nThe authors examined the effectiveness of the framework on\nseveral benchmark HAR datasets, finding that the heteroge-\nneous convolution is simple to integrate into convolutional\nlayers without increasing extra parameters and computational\noverhead. In the same year, Luwe et al. proposed a \"hybrid\ndeep learning model that amalgamates a one-dimensional\nConvolutional Neural Network with a bidirectional long\nshort-term memory (1D-CNN-BiLSTM) model for wearable\nsensor-based human activity recognition\" [26]. This one-\ndimensional neural network transforms the time series infor-\nmation from the sensor into representative features, which are\nthen encoded by the bidirectional LSTM. The authors found\nthe approach outperformed the existing methods, obtaining a\nrecognition rate of 95.48% on the UCI-HAR dataset, 94.17%\non the Motion Sense dataset, and 100% on the Single Ac-\ncelerometer dataset.\nB. ACTIVITY RECOGNITION USING MOVEMENT\nANALYSIS SYSTEM\nIn 1999, Ramsey and Wretenberg published a research paper\nreporting on the use of intracortical pins to measure knee\nmovement as an alternative to the use of reflective markers\n[27]. The authors found that their method allowed them to\ntake more precise readings with low error.\nIn 2005, Cutti et al. proposed an experiment to test the\nerror when using reflective markers for photogrammetric\nmeasurements [28]. The authors put the markers on different\nsubjects and made them execute different movements. Then,\nthe readings affected by the error were compared with normal\nreadings. The authors concluded that the error has a strong\ninfluence and should not be ignored, opening the way for new\nresearch that could compensate for this error.\nTokarczyk and Mazur compiled different Movement Anal-\nysis System techniques and methods [8]. They presented\nthe advancements of two Movement Analysis System tech-\nniques. The first is Moiré’s method of stripes, which involves\noverlaying two sets of parallel stripes with slightly different\nspacings to create a moiré pattern, which can be used in\nthe human body to detect conditions such as scoliosis. The\nsecond method uses multiple video cameras around the body,\nin conjunction with physical markers, that the camera system\ncan easily detect and read.\nIn 2008, Van Andel et al. carried out research to deter-\nmine a standardization protocol for the clinical application\nof upper extremity movement analysis [9]. The authors de-\nveloped measurement methods for hand orientation in dif-\nferent movements, using a stereophotogrammetric recording\nof active LED markers with a camera system. The wrist,\nelbow, shoulder, and scapula joint angles were analyzed,\nand minimum/maximum angles were determined. This way,\nthe authors determined the trajectories and angles of all the\nmovements, cementing the basis for developing more precise\nand standardized reports on movements that would allow for\nfuture comparisons with pediatric and/or pathologic move-\nment patterns. MAS generates easy-to-understand reports on\nmovements because it outputs markers position [9].\nJaén-Vargas et al. used wearable sensors and two reflective\nmarkers (mocap) to recognize the activities of walking, sit-to-\nstand, and squatting [21]. The authors evaluated the perfor-\nmance of four deep learning networks: deep neural network,\nCNN, LSTM, and a combination of CNN and LSTM. The\nauthors found that a hybrid network (CNN-LSTM) was better\nthan an individual network. The hybrid approach accounted\nfor class imbalance, making it more versatile, obtaining 99%\naccuracy in both datasets and an F1 score of 99% and 87%\nwith wearable sensors and reflective markers, respectively.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nThe authors also find that the use of wearable sensors yielded\nbetter results.\nJaén-Vargas et al. 2022 analyzed the performance of deep\nneural networks, CNN, LSTM, and CNN-LSTM when vari-\nating the sliding window size [29]. The sliding window is a\ntechnique in which a fixed-size window is moved over a time\nseries, processing the information in divided sections. The\nintention was to find an optimal window size for HAR using\na sampling rate of 100 Hz. Windows of small sizes 5, 10,\n15, 20, and 25 frames and long ones of sizes 50, 75, 100, and\n200 frames were compared. The results showed that windows\nfrom 20 to 25 frames were optimal, obtaining an accuracy of\n99,07% and an F1-score of 87,08% on sensor data and an\naccuracy of 98,8% and an F1-score of 82,80% on MOCAP\ndata.\nA particular field of research that involves the use of\ncameras for movement recognition is the study of human gait\nusing silhouettes and skeletons. In this field, an important\nwork is the paper of Cicirelli et al. [30], which is a review\nof human gait analysis with application in neurodegener-\native diseases. They compared sensors, features, and pro-\ncessing methodologies where deep networks such CNNs or\nLSTMs achieved the best results. In another gait-related work\n[31], the authors performed gait analysis classification for\nneurodegenerative diseases using support vector machines\n(SVMs) in optical motion capture data and achieved 99.1%\nof accuracy. This year, three relevant works in gait analysis\nwere published. The first one is the paper of Shayestegan et\nal. [32], in which they implemented Dual-Head Attentional\nTransformer-LSTM (DHAT-LSTM) in kinetic data to clas-\nsify stages of gait disorders and achieved an accuracy of\n81%. In the work of Cheriet et al. [33], they applied a Multi-\nSpeed Transformer Network in video data to classify stages\nof neurodegenerative diseases and achieved an accuracy of\n96.9%. Finally, Cosma, Catruna, and Radoi [34] published\na paper where they used Self-Supervised Vision Transform-\ners in human gait video data as a biometric authentication\nmethod.\nA summary of the papers considered in the Related Works\nsection is presented in Table 1.\nIII. EQUIPMENT AND DATASETS ACQUISITION\nA. EQUIPMENT\nThree different Inertial Measurement Units were used. Dif-\nferent sensor manufacturers were used to capture variabil-\nity in the sensor’s sensitivity and sampling rate. The first\ntwo devices, the MPU9250, and Trigno Avanti [35] inertial\nsensors are composed of an accelerometer, gyroscope, and\nmagnetometer, each one with 3 axes. The third IMU device\nused was the Metawear, composed of a 3D accelerometer and\na 3D gyroscope. The three systems send data to a PC using\nBluetooth communication, using their software to capture\ndata simultaneously from all devices at all locations. The\nsensors were mounted in the shoulder, forearm, arm, and\nhand, as described in Figure 1 and in the work of Tobar et\nal. (2017) [36].\nFIGURE 1. IMU sensors distribution\nThe equipment used for movement analysis was a Kines-\ncan/IBV . It is a movement analysis system based on visual\nmarkers that use rigid segment models. It captures PAL video\nat 25 frames per second. The system has 10 cameras located\nat a height of 2.4 m and distributed around the person. The\nanalysis of object movement is performed by tracking the\nposition of markers. These markers are small spheres covered\nwith reflective material attached to the subject. The system\nprovides position and speed for all markers. The markers are\nplaced as follows: one on the shoulder, three on the forearm,\ntwo on the elbow, three on the arm, two on the wrist, and one\non the hand. Figure 2 shows the distribution of the markers\nand the cameras used.\nFIGURE 2. Visual markers and cameras distribution\nB. ARM EXERCISES DESCRIPTION\nThe datasets were recorded while the subjects performed the\nfollow-arm exercises. Each exercise is described as:\n• Elbow Flexion-Extension: During elbow flexion the\nangle formed by the elbow joint decreases. The forearm\napproaches the arm. During elbow extension the angle\nformed by the elbow joint increases. The arm separates\nthe forearm. Figure 3.a shows the exercise.\n• Hand Pronation-Supination Starts with the hand and\nforearm aligned, the palm facing upwards with the\nthumb facing outwards. A rotation results in the palm\nfacing downward with the thumb facing inwards. The\nexercise is shown in Figure 3.b.\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 1. Summary of related works on HAR\nReference Year Authors Capture Method AI Framework Dataset Results\n[16] 2014 Ronao and Cho IMU CNN UCI-HAR 91.76% accuracy\n[22] 2017 Yarnan et al. IMU and EMG\nUnsupervised and\nsupervised\nmachine learning\nMade by authors\n10% normal data variation\nrate and 0.85 determination\ncoefficient unsupervised,\n6.55% normal data variation\nrate supervised.\n[17] 2018 Xiong et al. IMU CNN and RNN KTH and UCF101 From 96% to 99.66%\n[6] 2019 Sarcevic et al. IMU\nNCC, MLP,\nBayesian\nclassifier, SVM\nMade by authors 99.1% accuracy\n[18] 2019 Lu and Tong IMU Residual neural\nnetworks ASTRI and ADL 93.9% and 99.9% accuracy\n[10] 2019 Avilés et al. IMU Three CNNs UCI-HAR and WIDSM 100% accuracy\n[19] 2020 Yen, Liao and\nHuang IMU Three CNNs UCI-HAR and made by\nauthors\n95.99% and 93.77%\nrecognition rate\n[23] 2020 Lemieux and\nNoumeir IMU Two CNNs UTD-MHAD 90.8% accuracy\n[20] 2020 Clouthier et al. IMU Deep neural\nnetwork Made by authors 90.1% and 90.2%\nclassification accuracy\n[25] 2022 Han et al. IMU CNN\nOPPORTUNITY , PAMAP2,\nUCI-HAR, USC-HAD and\nmade by authors\nAccuracies higher than 91%\n[26] 2022 Luwe et al. IMU CNN and\nBiLSTM\nUCI-HAR, Motion Sense\nand Single Accelerometer\n95.48%, 94.71% and 100%\nrecognition rate\n[27] 1999 Ramsey and\nWretenberg\nMovement\nAnalysis System Does not apply Does not apply\nData on the angles of a wide\narray of knee-related\nmovements\n[28] 2005 Cutti et al. Movement\nAnalysis System Does not apply Does not apply\nA method to retrieve\nupper-body movement data\nwith good precision and\nangle consideration\n[8] 2007 Tokarczyk and\nMazur\nMovement\nAnalysis System Does not apply Does not apply\nReview paper on data\nretrieval using physical\nmarkers\n[9] 2008 Van Andel et al. Movement\nAnalysis System Does not apply Made by authors\nA comprehensive dataset of\nreadings, angles and other\ninformation on 3D upper\nbody movement\n[31] 2020\nDentamaro,\nImpedovo and\nPirlo\nMovement\nAnalysis System SVM Does not apply\nAn extensive review of\nmethods and protocols for\nanalyzing human gait\n[21] 2022 Jaén-Vargas et al. Movement\nAnalysis System\nCNN, LSTM, and\nCNN-LSTM Made by authors 99% accuracy, F1 scores of\n99% and 87%\n[29] 2022 Jaén-Vargas et al. Movement\nAnalysis System\nCNN, LSTM, and\nCNN-LSTM Made by authors\n99.07% accuracy, F1 scores\nof 99.8%, 87.08% and\n82.80%\n[30] 2022 Cicirelli et al. Movement\nAnalysis System Does not apply Does not apply\nAn extensive review of\nmethods and protocols for\nanalyzing human gait\n[32] 2023 Shayestegan et al. Movement\nAnalysis System DHAT-LSTM Made by authors 81% accuracy on gait\ndisorder detection\n[33] 2023 Cheriet et al. Movement\nAnalysis System\nMulti-speed\nTransformer Made by authors 96.9% on gait disorder\ndetection\n[34] 2023 Cosma, Catruna\nand Radoi\nMovement\nAnalysis System\nViT, CaiT,\nCrossFormer,\nToken2Token and\nTwinsSVT\nGREW and DenseGait Accuracies from 75% to\n85% in best cases\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\n• Shoulder Abduction-Adduction: Abduction is a lat-\neral movement of the entire upper limb away from the\ntrunk until the arm forms a 90-degree angle with the\ntrunk. Adduction is the lateral movement that brings\nthe upper limb closer to the trunk. Figure 4 shows the\nexercise.\n• Horizontal Shoulder flexion-extension: Begins with\nthe entire upper limb forming 90° with the trunk and\nlocated laterally. During extension, the arm moves hor-\nizontally forward and in flexion, it returns to its starting\nposition. Figure 5 shows the exercise.\n• Vertical Shoulder flexion-extension: This movement\nbegins with the upper limb close to the trunk. During\nflexion, the arm moves frontally in a vertical manner\nuntil the upper limb reaches a horizontal position. In\nextension, it returns to the starting position. Figure 6\nshows the exercise.\n• Internal and External Shoulder Rotation: This ex-\nercise begins with the arm next to the trunk, arm, and\nforearm making a 90-degree angle in a L shape. The\nforearm begins next to the stomach and then moves\naway horizontally from the body. Figure 7 shows the\nexercise.\n(a)\n (b)\nFIGURE 3. Exercises (a) flexion-extension [37], (b) pronation-supination.\nFIGURE 4. Shoulder Abduction-Adduction. (a) neutral position, (b) abduction\nfrom 0◦ to 60◦ [37]\nC. DATASET ACQUISITION\nA group of 10 people without arm diseases, between 20 and\n25 years old, were involved in the data acquisition. The ac-\nFIGURE 5. Horizontal Shoulder Flexion-extension. (a) flexion with an\nadduction of 140◦, (b) 90◦ abduction in the frontal plane [37]\nFIGURE 6. Vertical Shoulder Flexion-extension. (a) low amplitude (45◦ - 50◦),\n(b) high amplitude (180◦) [37]\nFIGURE 7. Internal and external rotation of the shoulder. (a) internal rotation\nof 30◦, (b) external rotation of 80◦ [37]\nquisition was separated into 3 sessions of people performing\narm exercises. For each session, a person starts in a neutral\nposition and performs 10 repetitions of each exercise. At\nthe end of the exercise, they return to a neutral position and\nrepeat the process for the next exercise.\nTABLE 2. Datasets and instruments\nDataset IMU Movement analysis system\nDataset 1 MPU9250 1Hz Kinescan/IBV 200Hz\nDataset 2 Trigno Avanti 370hz Kinescan/IBV 200Hz\nDataset 3 MPU9250 1Hz Kinescan/IBV 50HzMetawear 50Hz\nTABLE 3. IMU features\nSensor x-axis y-axis z-axis\nArmAcc AAx AAy AAz\nArmGyro AGx AGy AGz\nArmMag AMx AMy AMz\nForearmAcc FAx FAy FAz\nForearmGyro FGx FGy FGz\nForearmMag FMx FMy FMz\nHandAcc HAx HAy HAz\nHandGyro HGx HGy HGz\nHandMag HMx HMy HMz\nShoulderAcc SAx SAy SAz\nShoulderGyro SGx SGy SGz\nShoulderMag SMx SMy SMz\nEach session was recorded with Movement Analysis Sys-\ntem (MAS) and IMU instruments simultaneously. Exercises\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 4. Movement analysis system features\nMarker x-axis y-axis z-axis\nHand Hx Hy Hy\nWrist internal WIx WIy WIz\nWrist external WEx WEy WEz\nForearm lower FLx FLy FLz\nForearm middle FMx FMy FMz\nForearm upper FUx FUy FUz\nElbow internal EIx EIy EIz\nElbow external EEx EEy EEz\nArm lower ALx ALy ALz\nArm middle AMx AMy AMz\nArm upper AUx AUy AUz\nShoulder Sx Sy Sz\nwere performed one after the other, and data for each ex-\nercise was recorded and labeled as a whole. Within the 3\nsessions, 7 sub-datasets were created: 3 for MAS and 4 for\nIMU. Within the last session, 2 different IMU equipment\nwere used. Dataset 1 has 2 elbow exercises, dataset 2 has 3\nshoulder exercises, and Dataset 3 has 3 elbow and 3 shoulder\nexercises.\nTable 2 shows the names of the datasets, instruments\nused, and their capture frequencies. Tables 3 and 4 describe\nthe features captured from IMUs and Movement Analysis\nSystem. Each feature is described by its type (Acc, Gyro,\nMag, Visual marker), location, and sensor axis. For IMU data\nsensors were located at the shoulder, forearm, arm, and hand.\nFor Movement Analysis System data, markers were located\nas follows: one on the shoulder, three on the forearm, two on\nthe elbow, three on the arm, two on the wrist, and one on the\nhand.\nD. DATASET DESCRIPTION\nEach sample was built as a matrix where the columns repre-\nsent the features and rows represent the length given by the\nwindow size. The label for the sample was the most common\nlabel from the array of data points.\nIMU data was captured at frequencies of 1, 25, and 50 Hz\nwith 18, 24, and 27 features. IMU data used window sizes of\n10, 20, 40, and 200 data points. A single IMU dataset has at\nmost 500k timesteps and at most 4500 samples. According\nto the window size, we have data tensors e.g. 2511 × 200\n× 24 (samples × window size × features). The acquisition\nsampling rate determines the amount of data for an exercise\nduration. A lower sampling rate will generate less data, the\nmovement will not be correctly captured, and the exercise\nwill not be correctly recognized.\nVisual markers data was captured at frequencies of 50, and\n200 Hz with 18, 24, and 36 features. Visual Markers data used\nwindow sizes of 100 and 200 data points. A single Visual\nMarker dataset has at most 545k timesteps and 5450 samples.\nAccording to the window size, we have data tensors e.g. 1350\n× 200 × 24 (samples × window size × features).\nFor training, we used 100 epochs and a batch size of 64\nsamples for both acquisition systems. Available data was split\nusing 75% for training, 10% for validation, and 15% for\ntesting. All information related to datasets is shown in table\n5.\nE. TIME SERIES SIGNALS\nThe dataset is a multichannel time series. Each channel\nrelates to a location, sensor, and axis. The first row in Fig\nIII-E is from the arm, gyroscope, and x-axis.\nIMU signals are shown on the left of Figure III-E and\nvisual markers signals on the right. Signals from both sensors\nare pseudo-periodic with square-like waveforms. Some chan-\nnels allow us to distinguish movements easily. Some channels\nare highly correlated. All channels are height limited and do\nnot show outliers.\nDifferent exercises show responses on different sensors,\nlocations, and axis. During elbow flexion-extension, the arm\nremains locked, and the forearm moves. The sensors located\nat the forearm show the most movement while the arm sen-\nsors remain still. To recognize different kinds of movements,\nit is important to have independent information sources, e.g.,\ngyro and accelerometer, located in at least two different\npositions. The datasets can be accessed through the GitHub\nrepository of this work1.\nEach exercise repetition depicts a square-like waveform\nand builds up a pseudo periodic signal as in figure 9. All\nstudied arm exercises have two stages: flexion and extension.\nThese two stages are reflected in a square-like waveform\nwith two levels. A low-level signal for extension and an\nupper level for flexion. When the limb moves from flexion\nto extension a slope is visible in the signal. The two levels\nand the slope are easily recognizable in the signals of IMU\nand visual markers.\nGiven the square-like waveform, we can identify the move-\nment duration. For example, in dataset 1 the time needed\nto perform an exercise was 90 seconds while in dataset 2\nthe time was 2 seconds. The window size has to be chosen\ncorrectly according to the movement duration and sampling\nrate.\nFigure 10, shows IMU signals of ten repetitions of elbow\nflexion-extension. The imu is located at the arm with a gyro,\naccelerometer, and magnetometer. Each movement repetition\ndepicts a square-like waveform in the gyro and magnetome-\nter. The square-like waveform has a lower level for flexion\nand an upper level for extension. Gyro and magnetometer\ndata are easily interpretable to recognize an exercise. The\naccelerometer shows peaks going up and down, being harder\nto distinguish the movement being performed.\nX and z arm gyroscope axes are highly correlated. Y\nand z arm magnetometer axes are correlated. X and y arm\naccelerometer axes are correlated.\nThe IMU signals show fewer amplitude differences than\nvisual markers, presented in figures 10 and 11. Gyroscope\nand magnetometer data show a cleaner square than the visual\nmarker data. Accelerometer data is centered at zero.\n1https://github.com/StadynR/HAR-imu-photogrammetry\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 5. Summary of datasets capture conditions.\nDataset Exercises Sampling\n[Hz]\nPoints Window\nSize\nSamples Features Data Tensor\nD1 MPU9250 2 1 1360 10 135 27 135 × 10 × 27\nD1 MAS 2 200 313649 200 1568 36 1568 × 200 × 36\nD2 Trigno Avanti 3 370 502260 200 2511 24 2511 × 200 × 24\nD2 MAS 3 200 270015 200 1350 12 1350 × 200 × 12\nD3 MPU9250 6 1 8279 40 206 27 206 × 40 × 27\nD3 Metawear 6 50 136226 30 4540 18 4540 × 30 × 18\nD3 MAS 6 50 545024 100 5450 36 5450 × 100 × 36\nTABLE 6. Summary of sampling rate, exercise duration, and window size\nDataset Single exercise\nduration[sec]\nSampling\nrate[samp/sec]\nSingle exercise\nsize[points]\nWindow\nSize[points]\nWindow/\nexercise size\nD1 - MPU9250 90 1 90 10 0.11\nD1 - MAS 90 200 18000 200 0.01\nD2 - Trigno 2 370 740 200 0.27\nD2 - MAS 2 200 400 200 0.5\nD3 - MPU9250 4 1 4 40 10\nD3 - Metawear 4 50 200 30 0.15\nD3 - MAS 4 50 200 100 0.5\n(a) Dataset 1 - MPU9250\n (b) Dataset 1 - Movement Analysis System\nFIGURE 8. Signals from Dataset 1 MPU9250 and visual markers\nFigure 11, shows arm markers signals of ten repetitions\nof elbow flexion-extension. There are three markers located\nat the arm, each one with three axes. Position signals from\nmarkers at the X axis at the lower, middle, and upper arm\nare highly correlated. The same behavior happens for y and\nz-axis markers at the lower, middle, and upper arm.\nPosition signals have different offsets at each repetition,\nand the offset behaves as a moving offset. It is harder to\ndistinguish the exercises vs the IMU signals. The offsets\ndescribe the relative position of the person to the camera\nsystem. When the person moves inside the measured area,\nthe position signal will be different. The height and arm size\nof the person will affect the signal values.\nSimilar to IMU data, markers data show two levels. We\ncan observe a signal higher level when the forearm is up and\na lower level when the arm is down.\nMarkers signals are affected by occlusion, the angle and\ndistance to the cameras, and the posture and height of the\nperson. IMU is not affected by these variables.\nWe can observe that lower, middle, and upper markers\nsignals at the arm and forearm are similar because it is the\nsame solid section. To show the signal correlation of IMU and\nmarkers data, we computed the correlation matrices using all\nthe available features. From the figure 12, we note that MAS\nsignals are highly correlated.\nIV. METHODOLOGY\nA. PREPROCESSING\nBefore training, all datasets were preprocessed following\nthese steps: normalization, reshaping into 3D arrays with\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nFIGURE 9. Visual marker at the forearm. A single period of elbow\nflexion-extension movement. The blue line is the flexion state, and the red line\nis the extension state.\ndimensions (window size, samples, features), and label en-\ncoding. Additionally, in every training process, the dataset\nwas split into 5 parts using k-fold cross validation [38] to\nalleviate the effects of small datasets and class imbalance.\nB. ARCHITECTURES\nFor training and testing, eight neural network architectures\nwere considered and evaluated. The main components of\nthe architectures are LSTM, 1D convolutional layers, and\ntransformer encoders. All architectures used categorical cross\nentropy as their loss function and Adam as their optimizer.\nLong Short-Term Memory (LSTM) is a recurrent neural\nnetwork layer used in deep learning architectures. It com-\nprises memory cells and gates that allow the network to store\nor discard information over time selectively [39]. The basic\nLSTM cell consists of three gates. The input gate determines\nhow much new information is added to the memory cell,\nthe forget gate decides how much old information should\nbe removed, and the output gate regulates the amount of\ninformation outputted from the cell. Additionally, each gate\nhas its own set of learnable parameters, allowing the network\nto adaptively adjust the amount of information stored or\ndiscarded based on the input data, making LSTM effective\nin processing sequential data [40].\nOne-dimensional (1D) convolution is a mathematical oper-\nation frequently used in signal processing and deep learning,\nwhich involves sliding a small window or kernel over a one-\ndimensional input signal, computing the dot product between\nthe kernel and the signal at each position, and generating\na new output signal. The output signal is a compressed\nrepresentation of the input signal, highlighting patterns and\nfeatures relevant to the task at hand [41]. 1D convolutional\nlayers can also be used to process sequential data by learning\na set of filters [42].\nThe Transformer architecture is a framework used typi-\ncally for natural language processing (NLP), but can also be\nused for sequential data because of its attention mechanism.\nThe attention mechanism functions by extracting information\nfrom the entire sequence, by using a weighted sum of all\nthe past states of the encoder, generating a matrix. This\nmeans that all parts of the sequence are treated by their real\nimportance, and the overall context is considered, prioritizing\nwords with higher weight, allowing the model to focus on\nthe right element of the input to predict the next element of\nthe output [43], [44]. This attention mechanism is improved\nby using multi-head attention, which applies self-attention\nto different segments of the input, allowing the transformer\nto have better discrimination capabilities. As each head will\nproduce its resulting matrix, all matrices are concatenated\nand multiplied by an additional weight matrix, generating an\noutput matrix that contains information from all the heads\n[43], [45]. The code can be found in our GitHub repository.\nThe summary of the structures of the architectures is pre-\nsented in Figure 13. The architectural description used in this\nwork is presented below.\n• Architecture 1 (LSTM + Dropout + Dense + Dense):\nThis architecture was taken from the web article “Imple-\nmenting LSTM for Human Activity Recognition using\nSmartphone Accelerometer data\" [46]. The architecture\ncomprises an LSTM layer of 128 neurons, a dropout\nlayer, and two fully connected layers. The LSTM layer\nallows for time series data analysis due to its ability\nto handle variable-length input sequences, noisy data,\nand missing data. Thus, the network can make accurate\npredictions based on past observations. This network\nwas originally evaluated with the Wireless Sensor Data\nMining (WISDM) dataset, obtaining an accuracy of\n96.20%.\n• Architecture 2 (LSTM + Dropout + LSTM +\nDropout + Dense): This architecture was based on the\nGitHub repository “Human-Activity-Recognition\" [47].\nThis architecture is an extension of the last one, adding\nthe LSTM layer after the first dropout layer, adding\nanother dropout, and then a fully connected layer. This\nnetwork was originally evaluated with the UCI-HAR\ndataset, obtaining an accuracy of 93.17%.\n• Architecture 3 (Conv1D + Conv1D + Dropout + Max\nPooling + Flatten + Dense + Dense): This architec-\nture was taken from the GitHub repository “ETFA-\nWorkshop\" [48]. In contrast to the previous architec-\ntures, this network focuses on using convolutional lay-\ners. The architecture comprises two 1D convolutional\nlayers of 64 neurons, a dropout layer to avoid overfitting,\na max pooling layer to reduce dimensionality, and a\nfinal section of a flatten and two fully connected layers.\nConvolutional networks can be effective for time series\nanalysis, as they are good at extracting features from the\ninput data, which can be useful for identifying patterns\nand trends in the data. Also, convolution allows down-\nsampling data, reducing the computational complexity\nof the model and making it easier to train and run. This\nnetwork was originally evaluated with the UCI-HAR\ndataset, obtaining an accuracy of 89.89%.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nFIGURE 10. Dataset 1. MPU9250 is located at the arm during elbow flexion-extension. Plot of accelerometer, gyro, and mag signal with x,y, and z axes.\nFIGURE 11. Dataset 1. Visual markers located at the lower, middle, and upper arm during elbow flexion extension. The plot of marker position signal in the x,y, and\nz axes.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\n(a) Dataset 1 - MPU9250\n (b) Dataset 1 - Movement Analysis System\nFIGURE 12. Correlation matrices between features of Dataset 1\n• Architecture 4 (Conv1D + Max Pooling + LSTM\n+ Dropout + Dense + Dense): This architecture was\nassembled empirically as a combination of convolution\nand LSTM, to analyze the effectiveness of putting con-\nvolutional layers at the start of an LSTM network. As\nmentioned previously, convolution is useful for extract-\ning features and reducing the complexity of the data,\nalong with helping to reduce the amount of noise and\nirrelevant information. This way, the LSTM layer can\nwork with more refined data, and get better results.\n• Architecture 5 (Conv1D + Conv1D + Max Pool-\ning + Bidirectional LSTM + Dropout + Dense +\nDense): This architecture was based on the proposed\nmodel and findings of the paper “Wearable sensor-based\nhuman activity recognition with hybrid deep learning\nmodel\" [26], which used 1D convolution and bidirec-\ntional LSTM as an improvement for HAR. The net-\nwork is composed of two convolutional networks, a\nmax pooling layer, a bidirectional LSTM layer, and a\ndropout and two dense layers. Additionally, from the\nbenefits of using convolutional layers at the start of the\narchitecture, the bidirectional LSTM allows the network\nto capture information from past and future inputs,\nhelping it better capture dependencies and relationships\nbetween different parts of the sequence. This network\nwas originally evaluated with the UCI-HAR dataset,\nobtaining an accuracy of 95.48%.\n• Architecture 6 (LSTM + Dropout + Reshape +\nConv1D + Dropout + Dense + Dense + Dense): This\narchitecture was assembled empirically to test the per-\nformance and effects of placing convolutional layers at\nthe end of the architecture instead of at the start.\n• Architecture 7 (LSTM + Dropout + Dense + Dense\n+ Dense): A simple LSTM architecture with 64 neu-\nrons in the first layer, a dropout layer, and three dense\nlayers. The purpose of this network is to test how\nLSTM performs singlehandedly, without any particular\nenhancements.\n• Architecture 8 (Normalization + Position Embed-\nding + Transformer Encoder + Normalization +\nDense): This architecture was based on the proposed\nmodel and findings of the paper “Wearable Sensor-\nBased Human Activity Recognition with Transformer\nModel \" [49], which used a unidirectional Transformer-\nbased architecture as an improvement for HAR. The\nnetwork is composed of a normalization layer, a posi-\ntional embedding layer coupled with a sum of weights,\na transformer encoder, another normalization layer, and\na fully connected layer. The encoder itself contains\nmore layers: first, there is a normalization layer, then\na multi-head attention layer that does the main work,\nand a dropout layer. Then, a sum of weights processes\nthe results obtained previously, which go to a normal-\nization layer, a feed-forward network, and a dropout\nlayer. Before exiting the encoder, a final sum of weights\nis performed. Using this architecture, the authors take\nadvantage of the benefits of using multi-head attention,\ndescribed previously. This network was originally eval-\nuated with the KU-HAR dataset, obtaining an accuracy\nof 99.20%.\nC. FEATURE SELECTION\nDue to multiple correlated features available, at most 36\nfeatures, we propose to find the most important features using\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nFIGURE 13. Graphical Representation of the eight architectures\nrandom forest feature selection. After the feature selection,\nwe evaluated the 8 architectures with the reduced number of\nfeatures. It is relevant to find the most important features for\nIMU and MAS because it allows to reduce the computational\ncomplexity, reduce the physical sensors required, sensor in-\nformation redundancy, and possible overfitting. Figure 12\nshows a higher correlation in MAS than in IMU signals\nbecause of the three markers located at each position.\nThe algorithm used for feature selection, random forest\n(RF) [50], [51], is an ensemble learning method combining\nmultiple decision trees to improve the accuracy of the model.\nThe algorithm creates an ensemble of decision trees during\ntraining, where each tree is trained on a different subset of\ndata and features, i.e., bootstrap and bagging. The majority\nvote of the individual trees determines the final output of\nthe algorithm. Each node in a tree makes a binary decision\naccording to a single feature. RF identifies the features which\nwere the best to split the data, then are organized as most\nimportant features according to their score. [52].\nRandom forest was trained on every dataset. Feature se-\nlection was based on the importance score from a random\nforest. To select the amount number of important features for\ntraining, we began with a small number of features according\nto their importance. Then, we added features until the model\nno longer demonstrated any enhancement in accuracy or\nshowed signs of overfitting. Tables 8 to 10 show the best\nsensors and locations found and their score for each dataset.\nFigure 15 shows the training curves of the best architectures\nfor every dataset using only the best features.\nV. RESULTS\nWe trained models using the 8 architectures and the 7 sub-\ndatasets. Our metrics were: test accuracy, precision, recall,\nand F1 score. Each metric was the average of 5 repetitions\nusing k-fold cross validation. The result analysis needs to\nconsider the exercise duration, sampling rate, and window\nsize.\nTable 7 shows the summary of the test accuracy. Confusion\nmatrices figure 12. Training curves on figures 14, 15. Best\nfeatures in tables 8 to 10. Best architectures for each dataset\nin table 11, and best architectures using only best features in\ntable 12.\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 7. Mean Test Accuracy for all models. All features and best features. Datasets description.\nDataset 1 Dataset 2 Dataset 3\nMPU9250 Visual\nMarkers Trigno Avanti Visual\nMarkers MPU9250 Metawear Visual\nMarkers\nTimesteps 1360 313000 502000 270000 8200 136000 545000\nSamples 136 1568 2500 1350 205 4500 5450\nWindow size 10 200 200 200 40 30 100\nSampling\nrate 1 200 370 200 1 50 50\nExercise\nduration\nseconds\n90 90 2 2 4 4 4\nExercise size\npoints 90 18000 740 400 4 200 200\nwindow /\nexercise size 0.11 0.01 0.27 0.5 10 0.15 0.5\nAcc mean 0.961 0.876 0.612 0.958 0.464 0.837 0.981\nAll features, Mean Test Accuracy\nArchitectures MPU9250 Visual\nMarkers Trigno Avanti Visual\nMarkers MPU9250 Metawear Visual\nMarkers Mean\nA1 - LSTM\n128 neuron\n0.956\n(± 0.036)\n0.877\n(± 0.015)\n0.314\n(± 0.023)\n1.000\n(± 0.000)\n0.189\n(± 0.039)\n0.847\n(± 0.017)\n0.982\n(± 0.004) 0.738\nA2 - Double\nLSTM layer\n0.956\n(± 0.036)\n0.881\n(± 0.012)\n0.317\n(± 0.028)\n1.000\n(± 0.000)\n0.257\n(± 0.032)\n0.843\n(± 0.022)\n0.980\n(± 0.004) 0.748\nA3 - Two\nlayer conv\n0.963\n(± 0.047)\n0.886\n(± 0.012)\n0.998\n(± 0.002)\n1.000\n(± 0.000)\n0.845\n(± 0.032)\n0.886\n(± 0.009)\n0.989\n(± 0.004) 0.938\nA4 - Conv +\nLSTM\n0.963\n(± 0.033)\n0.872\n(± 0.010)\n0.996\n(± 0.003)\n1.000\n(± 0.000)\n0.611\n(± 0.082)\n0.871\n(± 0.013)\n0.986\n(± 0.003) 0.900\nA5 - 2 Conv\n+ BiLSTM\n0.970\n(± 0.036)\n0.885\n(± 0.019)\n0.998\n(± 0.003)\n1.000\n(± 0.000)\n0.738\n(± 0.032)\n0.882\n(± 0.006)\n0.985\n(± 0.002) 0.923\nA6 - LSTM\n+ 1DConv\n0.956\n(± 0.043)\n0.843\n(± 0.055)\n0.304\n(± 0.011)\n1.000\n(± 0.000)\n0.131\n(± 0.043)\n0.680\n(± 0.024)\n0.969\n(± 0.014) 0.698\nA7 - LSTM\n64 neuron\n0.970\n(± 0.036)\n0.875\n(± 0.021)\n0.304\n(± 0.011)\n1.000\n(± 0.000)\n0.189\n(± 0.032)\n0.770\n(± 0.013)\n0.974\n(± 0.006) 0.726\nA8 -\nTransformer\nencoder\n0.954\n(± 0.030)\n0.892\n(± 0.008)\n0.666\n(± 0.017)\n0.665\n(± 0.019)\n0.757\n(± 0.090)\n0.918\n(± 0.009)\n0.988\n(± 0.002) 0.834\nBest features, Mean Test accuracy\nArchitectures MPU9250 Visual\nMarkers Trigno Avanti Visual\nMarkers MPU9250 Metawear Visual\nMarkers Mean\nA1 - LSTM\n128 neuron\n0.948\n(± 0.044)\n0.874\n(± 0.021)\n0.869\n(± 0.249)\n0.992\n(± 0.005)\n0.703\n(± 0.073)\n0.834\n(± 0.014)\n0.966\n(± 0.010) 0.884\nA2 - Double\nLSTM layer\n0.948\n(± 0.055)\n0.882\n(± 0.022)\n0.992\n(± 0.001)\n0.776\n(± 0.179)\n0.771\n(± 0.051)\n0.820\n(± 0.025)\n0.965\n(± 0.011) 0.879\nA3 - Two\nlayer conv\n0.963\n(± 0.033)\n0.890\n(± 0.011)\n0.998\n(± 0.001)\n0.999\n(± 0.001)\n0.864\n(± 0.030)\n0.882\n(± 0.012)\n0.984\n(± 0.004) 0.940\nA4 - Conv +\nLSTM\n0.963\n(± 0.033)\n0.882\n(± 0.007)\n0.998\n(± 0.001)\n0.999\n(± 0.002)\n0.796\n(± 0.029)\n0.853\n(± 0.009)\n0.970\n(± 0.009) 0.923\nA5 - 2 Conv\n+ BiLSTM\n0.963\n(± 0.033)\n0.886\n(± 0.016)\n0.996\n(± 0.003)\n0.999\n(± 0.001)\n0.825\n(± 0.042)\n0.898\n(± 0.005)\n0.983\n(± 0.003) 0.936\nA6 - LSTM\n+ 1DConv\n0.948\n(± 0.044)\n0.870\n(± 0.019)\n0.991\n(± 0.005)\n0.979\n(± 0.020)\n0.564\n(± 0.098)\n0.718\n(± 0.033)\n0.924\n(± 0.015) 0.856\nA7 - LSTM\n64 neuron\n0.941\n(± 0.055)\n0.874\n(± 0.013)\n0.992\n(± 0.007)\n0.865\n(± 0.099)\n0.699\n(± 0.058)\n0.775\n(± 0.021)\n0.927\n(± 0.013) 0.868\nA8 -\nTransformer\nencoder\n0.970\n(± 0.043)\n0.892\n(± 0.017)\n0.666\n(± 0.017)\n0.666\n(± 0.019)\n0.891\n(± 0.038)\n0.924\n(± 0.012)\n0.974\n(± 0.002) 0.855\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\n(a) Dataset 1 - MPU9250\n (b) Dataset 1 - Movement Analysis\nSystem\n(c) Dataset 2 - Trigno Avanti\n (d) Dataset 2 - Movement Analysis\nSystem\n(e) Dataset 3 - MPU9250\n (f) Dataset 3 - Metawear\n (g) Dataset 3 - Movement Analysis\nSystem\nFIGURE 14. Plot of best training curves for every dataset using all features\n(a) Dataset 1 - MPU9250\n (b) Dataset 1 - Movement Analysis\nSystem\n(c) Dataset 2 - Trigno Avanti\n (d) Dataset 2 - Movement Analysis\nSystem\n(e) Dataset 3 - MPU9250\n (f) Dataset 3 - Metawear\n (g) Dataset 3 - Movement Analysis Sys-\ntem\nFIGURE 15. Plot of best architectures for every dataset using best features\nVI. DISCUSSION\nIn this section, we present the most relevant results and how\nthey relate to previous works.\nTest accuracy summary. Table 7 includes the capture\nconditions and test accuracy for all the experiments. We\ndiscuss how the sampling rate, window size, and exercise\nduration affect the test accuracy.\nIn Dataset 1, we have IMU sampling at 1 Hz and MAS\nat 200 Hz. IMU data at 1 Hz can distinguish exercises\nbecause the people took 90 seconds to complete each ex-\nercise repetition. Approximately 40 seconds stay at flexion\nand 40 seconds at extension. The change from flexion to\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\n(a) Dataset 1 - MPU9250\n (b) Dataset 1 - Movement Analysis\nSystem\n(c) Dataset 2 - Trigno Avanti\n (d) Dataset 2 - Movement Analysis\nSystem\n(e) Dataset 3 - MPU9250\n (f) Dataset 3 - Metawear\n (g) Dataset 3 - Movement Analysis System\nFIGURE 16. Confusion matrix of the best-performing architecture of every dataset (best features)\nTABLE 8. Best features for Dataset 1, ordered by score.\nDataset 1 - MPU9250 Dataset 1 - Movement Analysis System\nMax Accuracy with all features (27): 100% Max Accuracy with all features (36): 87.6%\nMax Accuracy with best features (4): 100% Max Accuracy with best features (15): 87.2%\nFeature Description Score Feature Description Score\nHAy hand accelerometer 0.136 FLz forearm lower 0.100\nHAx hand accelerometer 0.125 FMz forearm middle 0.090\nHAz hand accelerometer 0.122 Hz hand 0.086\nHMz hand magnetometer 0.095 WIz wrist internal 0.072\nFUz forearm upper 0.069\nWEz wrist internal 0.067\nFUy forearm upper 0.040\nEIy elbow internal 0.034\nWEx wrist external 0.034\nAUy arm upper 0.030\nFMx forearm middle 0.026\nFLy forearm lower 0.026\nHy hand 0.025\nWIx wrist internal 0.024\nHx hand 0.020\nextension took 3 seconds. The exercises are elbow flexion-\nextension and elbow pronation-supination. These exercises\ninvolve principally the movement of the hand and forearm.\nThe best features were accelerometers located at the hand.\nThis suggests we only need a wrist smartwatch equipped with\nan accelerometer and gyroscope as in [26] where they suggest\nusing a single accelerometer.\nDataset 1 IMU has 27 features and 4 selected features, both\nwith 97% acc. Dataset 1 MAS has 36 features and 15 selected\nfeatures, both with 89% acc. Reduced accuracy in MAS\nis related to occlusion during pronation-supination exercise.\nMAS system cannot correctly detect small movements. There\nis no improvement in using fewer features because of the\nproblems with occlusion and small movements. More fea-\ntures were needed from MAS than from IMU because MAS\ndo not have enough information to distinguish the exercises.\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 9. Best features for Dataset 2, ordered by score.\nDataset 2 - Trigno Avanti Dataset 2 - Movement Analysis System\nMax Accuracy with all features (24): 99.6% Max Accuracy with all features (24): 100%\nMax Accuracy with best features (6): 99.8% Max Accuracy with best features (3): 100%\nFeature Description Score Feature Description Score\nSAy shoulder accelerometer 0.266 ALz arm lower 0.211\nAAz arm accelerometer 0.132 AMz arm middle 0.132\nFAz forearm accelerometer 0.118 FLz forearm lower 0.118\nSAx shoulder accelerometer 0.111\nAAx arm accelerometer 0.111\nAAy arm accelerometer 0.073\nTABLE 10. Best features for Dataset 3, ordered by score\nDataset 3 - MPU9250 Dataset 3 - Metawear Dataset 3 - Movement Analysis System\nMax Accuracy with all features (27): 81.0% Max Accuracy with all features (18): 89.7% Max Accuracy with all features (36): 98.7%\nMax Accuracy with best features (5): 80.1% Max Accuracy with best features (12): 90.8% Max Accuracy with best features (15):: 98.6%\nFeature Description Score Feature Description Score Feature Description Score\nAGx arm gyroscope 0.124 AAy arm accelerometer 0.119 Sz shoulder 0.123\nAAy arm accelerometer 0.101 AAz arm accelerometer 0.098 Sy shoulder 0.103\nAGy arm gyroscope 0.090 HAx hand accelerometer 0.091 AMx arm middle 0.080\nAGz arm gyroscope 0.062 HAz hand accelerometer 0.087 AUx arm upper 0.078\nAMy arm magnetometer 0.048 AAx arm accelerometer 0.086 Sx shoulder 0.074\nHAy hand accelerometer 0.081 ALx arm lower 0.069\nFAz forearm accelerometer 0.080 AMy arm middle 0.058\nFAx forearm accelerometer 0.073 AMz arm middle 0.054\nFAy forearm accelerometer 0.068 ALy arm lower 0.044\nHGy hand gyroscope 0.031 AUy arm upper 0.034\nFGy forearm gyroscope 0.030 AUz arm upper 0.034\nAGx arm gyroscope 0.028 EEx elbow external 0.027\nFMx forearm middle 0.023\nEIx elbow internal 0.021\nEIz elbow internal 0.020\nIn Dataset 2, we have IMU sampling at 370 Hz and\nMAS at 200 Hz. Both rates achieve a test accuracy of 99%.\nWe have a good amount of data and used a window size\nof 200 points. The exercises in dataset 2 performs fully\nextended arm exercises, which are easily distinguished due\nto visible markers and large movements. This is the only\ndataset with acceleration estimated from the MAS system.\nUsing only 3 position features we achieve 99% accuracy, the\ngood performance is related to the exercise but not the use of\nacceleration from MAS. MAS data could be augmented with\nthe velocity markers as an additional feature. IMU data could\nbe augmented with additional features from the accumulation\nand derivative.\nDataset 2 IMU has 24 features and 6 selected features,\nboth with 99% acc. Dataset 2 MAS has 24 features and 3\nselected features, both with 99% acc. The selected features\nwere located at the arm, forearm, and shoulder. IMU at hand\nfeatures were not selected because those sensors were af-\nfected by large accelerations and were not able to distinguish\nthe exercises. Hand acceleration due to different arm lengths\ndidn’t show any effect on the exercise classification.\nIn Dataset 3, we have MPU9250 sampling at 1 Hz,\nMetawear at 50 Hz, and MAS at 50 Hz. This dataset per-\nforms the same exercises from Dataset 1 plus Dataset 2. D3\nMPU9250 at 1 Hz is the first dataset that shows overall bad\nperformance, 13% to 85% accuracy. This is because sampling\nat 1 Hz is too low for a movement that takes 4 seconds to\ncomplete. We used a window size of 40 points capturing\nmultiple exercise repetitions, which produced bad accuracy.\nMetawear showed low performance, 68% to 92% because the\nsystem presented data transmission problems. MAS system\ndid not show any problems with a sampling of 50 Hz and a\nwindow size of 100 points. This result shows we can have\nlow sampling rates around 50 Hz.\nDataset 3 MPU9250 has 27 features with 84% acc and 5\nselected features with 89%. D3 Metawear has 18 features\nand 12 selected features with 92% acc. D3 MAS has 36\nfeatures with 99% acc and 15 selected features with 98%\nacc. MAS requires more features than IMU because of the\ncomplexity of the exercises, markers occlusion, and difficulty\nin detecting small movements.\nTraining curves. During training, it takes at most 100\nepochs to achieve a steady state of accuracy and loss. All\ndatasets show loss reduction and accuracy increase over time.\nFailure cases. The most challenging datasets were D2-\nTrigno and D3-MPU9250. D3-MPU9250 showed bad test\naccuracy, this is related to a sampling rate of 1 Hz and a\nwindow size of 40 points. The sampling rate is slow for a\nmovement that takes 4 seconds to complete. The window\nsize is too long, capturing multiple repetitions in a single\nwindow. Having multiple exercise repetitions captured in a\nsingle window show bad performance because it is ideal to\nhave aligned signals to recognize them. The accuracy of the\nchallenging datasets increased using selected features. The\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nbest architectures stand out by achieving better performance\non challenging datasets.\nConfusion Matrices. Reflects the correct and incorrect\nclassification for each dataset using the best features and the\nbest model. We can see the hardest exercise to recognize was\nelbow pronation-supination for the MAS dataset.\nEffect of window size, sampling rate, and exercise du-\nration. We found the window size should be variable because\nit is proportional to the sampling rate and exercise duration.\nFor example, D1-MPU9250 with a window size of 10 points\nis enough to recognize an exercise sampled at 1 Hz with a\nduration of 90 points. In this case, the ratio from window size\nto exercise duration is 10/90=0,11. From D3-MPU9250, a\nwindow size of 40 points was too big to recognize an exercise\nsampled at 1 Hz with a duration of 4 points. In this case,\nthe ratio from window size to exercise duration is 40/4=10.\nA window should capture less than a single repetition to\nachieve good accuracy. From our results, ratios less than 50%\nachieved high accuracy.\nWe found the sampling rate should be proportional to\nthe exercise duration. In daily activities, a muscular-focused\nexercise or a full-body exercise takes around 2 seconds to\nperform. To capture a well-defined shape of the movement,\nwe found the sampling rate should be around 60 Hz. A\nsampling rate of 1 Hz was insufficient to correctly recognize\nexercises. The sampling rate of 200 and 370 Hz didn’t show\nany improvement and generated too much information.\nBest models architectures. The best architectures were\nthe same using all and selected features. The 3 best architec-\ntures were: 2LayerConv 93.8% acc, 2LayerConv+BiLSTM\n92.3% acc, Conv+LSTM 90% acc.The networks analyzed are\ndivided into LSTM first, CNN first, and Transformer.\nBest architectures have two convolutional layers at the\nbeginning. The networks are learning filters with the shape\nof the multiple-channel signals. During the signals analysis\nperformed in the time series section, we noted the exercises\nshow well-defined shapes for each exercise.\nLSTM first networks achieved lower test accuracy on the\nchallenging datasets.\nConvolution + BiLSTM has good performance showing\nthe importance of convolution and the importance of bidirec-\ntional learning on LSTM. Transformer didn’t achieve good\nperformance alone. The transformer was a unidirectional\nencoder. Accuracy could be improved using a bidirectional\ndecoder architecture.\nFeature reduction. Using as low as 4 features, IMU and\nMAS systems achieved high accuracy above 89%. Feature\nreduction was feasible without affecting accuracy.\nTo achieve good accuracy with few features, a single\nIMU with accelerometer and gyroscope should be located\nin the wrist. Therefore we recommend a single IMU in a\nsmartwatch. Feature selection shows the IMU acceleration\nvariable is the most representative similar to [16], [17], [18],\n[23], and [26]. The test accuracies above 89% found for our\ndatasets using the 8 models were similar to the accuracies\nfound in [26], [46], [53], [48], and [49].\nVII. CONCLUSION\nThe duration of daily exercises is variable, then we need a\nvariable window size. The window size is proportional to\nthe equipment sampling rate and exercise duration. From our\nexperiments sampling rate of 60 Hz is able to distinguish arm\nmovements.\nState of the art deep learning models was able to correctly\nclassify exercises. Best architectures were 2LConv 93.8%,\n2LConv+BiLSTM 92.3%, Conv+LSTM 90%. Selected fea-\ntures lead to a reduction of 4 features for IMU or MAS, with\naccuracies higher than 89%.\nIMU allows measuring the activity of people simultane-\nously in any environment, even on water. IMU is portable\nand can be used in all kinds of daily activities. MAS needs\nan equipment room and complex video capture system and\nwearable markers. The use of cameras interferes with peo-\nple’s privacy inside the everyday environment.\nThe IMUs achieve high accuracy, low cost, and noise re-\nduction compared to other instruments such as EEG or EMG\nwhere the signals are naturally mixed from the source. The\naccuracy improves using independent sources, such as inde-\npendent axes in accelerometers and gyroscopes [22], [6]. In\nMAS, the visual markers have problems related to occlusion,\ndistance, size of movements, and out-of-plane movements\n[36], e.g. pronation-supination, shoulder rotation.\nFrom our experiments detailed in table 7, we found a\nrelation to estimating the window size given by equation 1.\nTwo second exercises captured at 60Hz will require window\nsizes between 30 and 60 points.\nwindow size\nexercise duration points < 0.5 (1)\nwindow size\nexercise duration seconds × sampling rate < 0.5 (2)\nVIII. FUTURE WORK\nA limitation of this work was the number of persons involved\nwhich do not reflect the behavior of the signals for a sample\nof population. The 10 persons chosen for this study represent\nonly a test group to validate the capture methodology and\nevaluate the performance of deep learning architectures. The\npeople involved had different arm lengths and the differences\nin acceleration didn’t show misclassifications. A limitation\nof this work was the reduced number of samples captured, at\nmost 540k timesteps for an exercise. It is important to capture\ndata about elderly in their daily lives. The data needs to\nhave multiple people and data for several weeks. To evaluate\noverfitting and to perform robust statistical accuracy tests,\nwe need larger datasets with more people, more exercises,\nand include full-body exercises. With larger datasets, we can\nidentify better algorithms for data extraction and models to\nrecognize a wider range of movements. To achieve robust\nmodels we need to perform data augmentation adding noise,\nscaling, offset in time, and random signal erase. To have a\nbetter window extraction we propose replacing the fixed slid-\ning window with dynamic time warping, wavelet transform,\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nTABLE 11. Results for all datasets using all features\nDataset\nSamples\nExercise\nduration [s]\nSampling\nrate\nWindow\nSize\nWindow\n/exercise\nSize\nTotal\nFeatures\nBest mean\ntest\naccuracy\nBest mean\nprecision\nBest mean\nrecall\nBest mean\nF1 score\nBest\narchitecture\nD1 -\nMPU9250 135 90 1 10 0.11 27 97% 97.6% 97% 97.1%\n2Conv +\nBiLSTM,\nLSTM 64\nneuron\nD1 - MAS 1568 90 200 200 0.01 36 89.2% 89.3% 89.2% 89% Transformer\nencoder\nD2 -\nTrigno\nAvanti\n2511 2 370 200 0.27 24 99.8% 99.8% 99.8% 99.8%\n2 Layer conv,\n2Conv +\nBiLSTM\nD2 - MAS 1350 2 200 200 0.5 24 100% 100% 100% 100%\n2 Layer conv,\n2Conv +\nBiLSTM\nD3 -\nMPU9250 206 4 1 40 10 27 84.5% 87.5% 84.5% 84.7% 2 Layer conv\nD3 -\nMetawear 4540 4 50 30 0.15 18 91.8% 91.9% 91.9% 91.9% Transformer\nencoder\nD3 - MAS 5450 4 50 100 0.5 36 98.9% 98.9% 98.9% 98.9% 2 Layer conv\nTABLE 12. Best Results for all datasets using best features\nDataset\nSamples\nExcercise\nduration [s]\nSampling\nrate\nWindow\nSize\nWindow\n/exercise\nSize\nBest\nfeatures\n(number)\nBest\nfeatures\n(names)\nBest mean\ntest\naccuracy\nBest mean\nprecision\nBest mean\nrecall\nBest mean\nF1 score\nBest\narchitecture\nD1 -\nMPU9250 135 90 1 10 0.11 4/27 hand 97% 97% 97% 97% Transformer\nencoder\nD1 - MAS 1568 90 200 200 0.01 15/36\nforearm,\nhand,\nwrist\n89.2% 89.2% 89.1% 88.9% Transformer\nencoder\nD2 -\nTrigno\nAvanti\n2511 2 370 200 0.27 5/24 arm,\nforearm 99.8% 99.8% 99.8% 99.8% Conv+LSTM\nD2 - MAS 1350 2 200 200 0.5 3/24 arm,\nforearm 99.9% 99.9% 99.9% 99.9%\n2 Layer conv,\nConv+LSTM,\n2Conv+BiLSTM\nD3 -\nMPU9250 206 4 1 40 10 5/27 arm 89.1% 89.1% 88.9% 88.8% Transformer\nencoder\nD3 -\nMetawear 4540 4 50 30 0.15 12/18\narm,\nhand,\nforearm\n92.4% 92.4% 92.5% 92.5% Transformer\nencoder\nD3 - MAS 5450 4 50 100 0.5 15/36 arm 98.4% 98.4% 98.4% 98.4% 2 Layer conv\nand spectrogram for scale and location, anchors like in image\nsegmentation (different sized sliding windows), sliding win-\ndow align by regression as in image segmentation. Feature\nreduction suggested it is possible to achieve high accuracy\nwith few sensors. We propose to use a single IMU located\nat the wrist being worn as a smartwatch. The challenge is\nto recognize exercises with a single IMU and implement\nvariable window size. To classify exercises with a single\nIMU we need to capture the variations in signal amplitudes,\nshapes, and durations. A single IMU smartwatch solution\nwith HAR capabilities is interesting for elderly care, athletes,\nand healthcare. The solution involves a smartwatch to capture\nand send data to a processing unit. The data necessary would\nbe at 60 Hz and 6 features, making this solution feasible.\nOur main contributions were:\n• Capture methodology and comparison for IMU and\nMAS acquisition systems.\n• We analyzed the relations between 8 architectures accu-\nracies, signal waveforms, signals correlation, sampling\nrate, exercise duration, and window size.\n• Feature reduction analysis.\n• Deep learning models were able to recognize human\nexercises. The next challenge is to classify using a single\nIMU and use variable window size.\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nREFERENCES\n[1] H.-B. Zhang, Y .-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and D.-S.\nChen, “A comprehensive survey of vision-based human action recognition\nmethods,” Sensors, vol. 19, no. 5, p. 1005, 2019.\n[2] N. Ma, Z. Wu, Y .-m. Cheung, Y . Guo, Y . Gao, J. Li, and B. Jiang, “A survey\nof human action recognition and posture prediction,” Tsinghua Science\nand Technology, vol. 27, no. 6, pp. 973–1001, 2022.\n[3] M. Mokari, H. Mohammadzade, and B. Ghojogh, “Recognizing invol-\nuntary actions from 3d skeleton data using body states,” arXiv preprint\narXiv:1708.06227, 2017.\n[4] S. Khan, M. A. Khan, M. Alhaisoni, U. Tariq, H.-S. Yong, A. Armghan,\nand F. Alenezi, “Human action recognition: a paradigm of best deep\nlearning features selection and serial based extended fusion,” Sensors,\nvol. 21, no. 23, p. 7941, 2021.\n[5] M. Jian, S. Zhang, L. Wu, S. Zhang, X. Wang, and Y . He, “Deep key\nframe extraction for sport training,” Neurocomputing, vol. 328, pp. 147–\n156, 2019.\n[6] P. Sarcevic, Z. Kincses, and S. Pletl, “Online human movement classifica-\ntion using wrist-worn wireless sensors,” Journal of Ambient Intelligence\nand Humanized Computing, vol. 10, no. 1, pp. 89–106, 2019.\n[7] M. Z. Uddin and A. Soylu, “Human activity recognition using wearable\nsensors, discriminant analysis, and long short-term memory-based neural\nstructured learning,” Scientific Reports, vol. 11, no. 1, p. 16455, 2021.\n[8] R. Tokarczyk and T. Mazur, “Photogrammetry–principles of operation and\napplication in rehabilitation,” Medical Rehabilitation, vol. 10, pp. 31–38,\n2006.\n[9] C. J. van Andel, N. Wolterbeek, C. A. Doorenbosch, D. H. Veeger, and\nJ. Harlaar, “Complete 3d kinematics of upper extremity functional tasks,”\nGait & posture, vol. 27, no. 1, pp. 120–127, 2008.\n[10] C. Avilés-Cruz, A. Ferreyra-Ramírez, A. Zúñiga-López, and J. Villegas-\nCortéz, “Coarse-fine convolutional deep-learning strategy for human ac-\ntivity recognition,” Sensors, vol. 19, no. 7, p. 1556, 2019.\n[11] M. Ramanathan, W.-Y . Yau, and E. K. Teoh, “Human action recognition\nwith video data: research and evaluation challenges,” IEEE Transactions\non Human-Machine Systems, vol. 44, no. 5, pp. 650–663, 2014.\n[12] E. M. Mikhail, J. S. Bethel, and J. C. McGlone, Introduction to modern\nphotogrammetry. John Wiley & Sons, 2001.\n[13] A. Bux, P. Angelov, and Z. Habib, “Vision based human activity recog-\nnition: a review,” in Advances in Computational Intelligence Systems:\nContributions Presented at the 16th UK Workshop on Computational\nIntelligence, September 7–9, 2016, Lancaster, UK. Springer, 2017, pp.\n341–371.\n[14] M. Yoneyama, Y . Kurihara, K. Watanabe, and H. Mitoma, “Accelerometry-\nbased gait analysis and its application to parkinson’s disease assess-\nment—part 2: A new measure for quantifying walking behavior,” IEEE\nTransactions on neural systems and rehabilitation engineering, vol. 21,\nno. 6, pp. 999–1005, 2013.\n[15] P. Bonato, “Wearable sensors/systems and their impact on biomedical en-\ngineering,” IEEE Engineering in Medicine and Biology Magazine, vol. 22,\nno. 3, pp. 18–20, 2003.\n[16] C. A. Ronao and S.-B. Cho, “Human activity recognition using smart-\nphone sensors with two-stage continuous hidden markov models,” in 2014\n10th international conference on natural computation (ICNC). IEEE,\n2014, pp. 681–686.\n[17] Q. Xiong, J. Zhang, P. Wang, D. Liu, and R. X. Gao, “Transferable\ntwo-stream convolutional neural network for human action recognition,”\nJournal of Manufacturing Systems, vol. 56, pp. 605–614, 2020.\n[18] J. Lu and K.-Y . Tong, “Robust single accelerometer-based activity recogni-\ntion using modified recurrence plot,” IEEE Sensors Journal, vol. 19, no. 15,\npp. 6317–6324, 2019.\n[19] C.-T. Yen, J.-X. Liao, and Y .-K. Huang, “Human daily activity recognition\nperformed using wearable inertial sensors combined with deep learning\nalgorithms,” Ieee Access, vol. 8, pp. 174 105–174 114, 2020.\n[20] A. L. Clouthier, G. B. Ross, and R. B. Graham, “Sensor data required\nfor automatic recognition of athletic tasks using deep neural networks,”\nFrontiers in bioengineering and biotechnology, vol. 7, p. 473, 2020.\n[21] M. Jaén-Vargas, K. R. Leiva, F. Fernandes, S. Gonçalves, M. T. Silva,\nD. Lopes, and J. S. Olmedo, “A deep learning approach to recognize\nhuman activity using inertial sensors and motion capture systems,” 2021.\n[22] Y . Li, X. Zhang, Y . Gong, Y . Cheng, X. Gao, and X. Chen, “Motor\nfunction evaluation of hemiplegic upper-extremities using data fusion from\nwearable inertial and surface emg sensors,” Sensors, vol. 17, no. 3, p. 582,\n2017.\n[23] N. Lemieux and R. Noumeir, “A hierarchical learning approach for human\naction recognition,” Sensors, vol. 20, no. 17, p. 4946, 2020.\n[24] O. Banos, R. Garcia, J. A. Holgado-Terriza, M. Damas, H. Pomares,\nI. Rojas, A. Saez, and C. Villalonga, “mhealthdroid: a novel framework\nfor agile development of mobile health applications,” in Ambient Assisted\nLiving and Daily Activities: 6th International Work-Conference, IW AAL\n2014, Belfast, UK, December 2-5, 2014. Proceedings 6. Springer, 2014,\npp. 91–98.\n[25] C. Han, L. Zhang, Y . Tang, W. Huang, F. Min, and J. He, “Human activity\nrecognition using wearable sensors by heterogeneous convolutional neural\nnetworks,” Expert Systems with Applications, vol. 198, p. 116764, 2022.\n[26] Y . J. Luwe, C. P. Lee, and K. M. Lim, “Wearable sensor-based human\nactivity recognition with hybrid deep learning model,” in Informatics,\nvol. 9, no. 3. MDPI, 2022, p. 56.\n[27] D. K. Ramsey and P. F. Wretenberg, “Biomechanics of the knee:\nmethodological considerations in the in vivo kinematic analysis of the\ntibiofemoral and patellofemoral joint,” Clinical biomechanics, vol. 14,\nno. 9, pp. 595–611, 1999.\n[28] A. G. Cutti, G. Paolini, M. Troncossi, A. Cappello, and A. Davalli, “Soft\ntissue artefact assessment in humeral axial rotation,” Gait & posture,\nvol. 21, no. 3, pp. 341–349, 2005.\n[29] M. Jaén-Vargas, K. M. R. Leiva, F. Fernandes, S. B. Gonçalves, M. T.\nSilva, D. S. Lopes, and J. J. S. Olmedo, “Effects of sliding window varia-\ntion in the performance of acceleration-based human activity recognition\nusing deep learning models,” PeerJ Computer Science, vol. 8, p. e1052,\n2022.\n[30] G. Cicirelli, D. Impedovo, V . Dentamaro, R. Marani, G. Pirlo, and T. R.\nD’Orazio, “Human gait analysis in neurodegenerative diseases: A review,”\nIEEE Journal of Biomedical and Health Informatics, vol. 26, no. 1, pp.\n229–242, 2021.\n[31] V . Dentamaro, D. Impedovo, and G. Pirlo, “Gait analysis for early neu-\nrodegenerative diseases classification through the kinematic theory of\nrapid human movements,” IEEE Access, vol. 8, pp. 193 966–193 980,\n2020.\n[32] M. Shayestegan, J. Kohout, K. Trnková, M. Chovanec, and J. Mareš,\n“Motion tracking in diagnosis: Gait disorders classification with a dual-\nhead attentional transformer-lstm,” International Journal of Computational\nIntelligence Systems, vol. 16, no. 1, p. 98, 2023.\n[33] M. Cheriet, V . Dentamaro, M. Hamdan, D. Impedovo, and G. Pirlo,\n“Multi-speed transformer network for neurodegenerative disease assess-\nment and activity recognition,” Computer Methods and Programs in\nBiomedicine, vol. 230, p. 107344, 2023.\n[34] A. Cosma, A. Catruna, and E. Radoi, “Exploring self-supervised vision\ntransformers for gait recognition in the wild,” Sensors, vol. 23, no. 5, p.\n2680, 2023.\n[35] DELSYS, “Documentation,” Sep 2022. [Online]. Available:\nhttps://delsys.com/support/documentation/usersguide\n[36] A. Tobar, B. Lopez, M. F. Trujillo, and A. Rosales, “Machine learning to\ndetermine upper extremity motion from inertial measurement unit signals,”\nin 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM). IEEE,\n2022, pp. 1–6.\n[37] A. Kapandji, “Fisiología articular: hombro, codo, pronosupinación,\nmuñeca, mano,” 2006.\n[38] S. Yadav and S. Shukla, “Analysis of k-fold cross-validation over hold-out\nvalidation on colossal datasets for quality classification,” in 2016 IEEE\n6th International Conference on Advanced Computing (IACC), 2016, pp.\n78–83.\n[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[40] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmid-\nhuber, “Lstm: A search space odyssey,” IEEE Transactions on Neural\nNetworks and Learning Systems, vol. 28, no. 10, pp. 2222–2232, 2017.\n[41] I. Goodfellow, Y . Bengio et al., “a courville, a,” 2016.\n[42] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[44] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki,\nN. E. Y . Soplin, R. Yamamoto, X. Wang et al., “A comparative study\non transformer vs rnn in speech applications,” in 2019 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU). IEEE, 2019,\npp. 449–456.\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\n[45] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz et al., “Transformers: State-of-the-art\nnatural language processing,” in Proceedings of the 2020 conference on\nempirical methods in natural language processing: system demonstrations,\n2020, pp. 38–45.\n[46] P. Nabriya, “Implementing lstm for human activity recognition\nusing smartphone accelerometer data,” Jul 2021. [Online]. Avail-\nable: https://www.analyticsvidhya.com/blog/2021/07/implementing-lstm-\nfor-human-activity-recognition-using-smartphone-accelerometer-data/\n[47] S. Das, “Human-activity-recognition,” https://github.com/srvds/Human-\nActivity-Recognition, 2021.\n[48] P. Rathnayakas, “Etfa-workshop,” https://github.com/CDAC-lab/ETFA-\nWorkshop, 2021.\n[49] I. Dirgová Luptáková, M. Kubov ˇcík, and J. Pospíchal, “Wearable sensor-\nbased human activity recognition with transformer model,” Sensors,\nvol. 22, no. 5, p. 1911, 2022.\n[50] L. Breiman, “Random forests,” Machine learning, vol. 45, pp. 5–32, 2001.\n[51] T. K. Ho, “Random decision forests,” in Proceedings of 3rd international\nconference on document analysis and recognition, vol. 1. IEEE, 1995,\npp. 278–282.\n[52] A. Liaw, M. Wiener et al., “Classification and regression by randomforest,”\nR news, vol. 2, no. 3, pp. 18–22, 2002.\n[53] S. Das, A. Chaudhary, F. Bremond, and M. Thonnat, “Where to focus\non for human action recognition?” in 2019 IEEE Winter Conference on\nApplications of Computer Vision (W ACV). IEEE, 2019, pp. 71–80.\n20 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTrujillo-Guerrero et al.: Accuracy comparison of CNN, LSTM, and Transformer for activity recognition using IMU and visual markers\nMARÍA FERNANDA TRUJILLO GUERRERO\nreceived the B.S. degree in electronic engineering\nfrom Politécnica Nacional, Quito, Ecuador, 2014\nand the MSc. degree in biomedical engineering\nfrom Universidad Politécnica de Madrid (UPM),\nMadrid, Spain, 2016.\nShe is currently a Lecturer with Escuela Politéc-\nnica Nacional since 2017. She has been part of the\nBioinstrumentation and Nanomedicine research\ngroup at The Center for Biomedical Technology.\nHer research interests include wearable sensing, human activity recognition,\nand deep learning applied to the health domain.\nSTADYN ROMÁN NIEMES is currently a last\nyear undergraduate student for a B.E. degree in\nInformation Technology at Yachay Tech Univer-\nsity. He has worked as an intern and research\nassistant at Digevo, an AI solutions business, and\nis currently working as an independent researcher\nat Harkay. He has also published two conference\npapers that covered topics such as artificial intel-\nligence, image processing, and computer vision.\nHis main research interests include artificial intel-\nligence, data science, computer vision, and natural language processing.\nALFONSO CADIZ received a degree in elec-\ntronic engineering in digital systems from Fed-\nerico Santa María University, Chile, in 1997, and\na master’s degree in artificial intelligence from\nUniversidad Internacional Menéndez Pelayo, Es-\npaña, in 2018. He is currently a serial entrepreneur,\nan investor, and the director of multiple startups.\nHe has founded over 25 technology companies\nand completed three successful with its partners.\nHe is also the Technology Director of Digevo, a\nChilean group with operations in 15 countries. He is also responsible for\nthe technology of the group and all associated companies, with vigorous\nactivity in Latam, focused on the development of technology to support the\nentrepreneurship, innovation, and evolution of the group in the technological\nand management areas.\nMILAGROS JAÉN-VARGAS was born in\nPanamá in 1988. She received a B.S degree in\nElectronic and Telecommunication Engineering\nat Universidad Tecnológica de Panamá in 2011\nand a master’s degree in Project Management at\nUniversidad Interamericana de Panamá in 2013.\nIn 2018, she was granted an IFARHU-SENACYT\nscholarship to pursue a Ph.D. degree in Biomed-\nical Engineering at Universidad Politécnica de\nMadrid, Spain. Currently, she belongs to the As-\nsistive Technologies group within the Bioinstrumentation and Nanomedicine\nLaboratory (LBN) at the Technology Biomedical Center (CTB) located in\nMadrid, Spain. Hence, she is doing research on aims focused on blind or\nvisually impaired people to develop technologies for its safe navigation,\nguiding, and rehabilitation. In addition, she is working with AI algorithms\nfocused on recognizing human activities.\nRICARDO FONSECA obtained a Ph.D. degree\nin electronic engineering from Universidad Fed-\nerico Santa Maria, Chile, 2021. He is the CTO of\nComputer Vision at Digevo, where they develop\nsolutions for smart retail and conversational AI.\nHe works on large scale deploys of deep learning\nmodels for person and vehicle analytics, speech to\ntext, text to speech, and large language models.\nJOSÉ JAVIER SERRANO OLMEDO got his\ndegree in Telecommunication Engineering in 1990\nand his PhD. in Telecommunication Engineering\nin 1996 at the Engineering School on Telecom-\nmunication (Escuela Técnica Superior de Inge-\nniería de Telecomunicación, at the Technical Uni-\nversity of Madrid, (Universidad Politécnica de\nMadrid, UPM). He teaches Electronic Instrumen-\ntation, Bioinstrumentation, Biosensors, Technolo-\ngies for Nanomedicine, Human Computer Inter-\nfaces, Electronic Health Records, and Clinical Engineering as Associate Pro-\nfessor since 1998 at the Technical University of Madrid and a Full Professor\nsince 2023. He is the Coordinator of the Doctorate Program in Biomed-\nical Engineering. He is a fellow member of the Networking Center for\nBiomedical Research on Bioengineering, Biomaterials and Nanomedicine,\nof the Center for Biomedical Technologies at UPM (CTB-UPM), and of\nthe Spanish Society of Biomedical Engineering. He has devoted his re-\nsearcher career to the instrumentation field having worked on semiconductor\nmaterials characterization, sensor networking, and seismic instrumentation\namong others, although for the last twenty years, he has been focused\non biomedical technology development. He is heading the Laboratory of\nBioinstrumentation and Nanomedicine, a CTB-UPM, and is a member of\nthe Life Supporting Technologies Group, of the same university. The main\nresearch lines being followed at this laboratory are the development of\ntechnologies for nanomedicine, mainly for new anticancer therapies based\non nanoparticle mediated hyperthermia, the development of gravimetric\nbiosensors and electromedicine instrumentation, and the development of\naccessible and assistive technologies based on serious games, virtual and\naugmented reality technology, and the use Human Activity Recognition\ntechnologies. He has published more than one hundred and fifty papers and\nconference contributions, participated in or headed more than sixty projects,\nand supervised more than twenty doctoral theses.\nVOLUME 4, 2016 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3318563\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}