{
  "title": "Dense Transformer Networks",
  "url": "https://openalex.org/W2619703608",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1971186596",
      "name": "Li Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1970349412",
      "name": "Chen Yongjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2198205510",
      "name": "Cai Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746423362",
      "name": "Davidson, Ian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750626982",
      "name": "Ji, Shuiwang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2167510172",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1905829557",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2128409098",
    "https://openalex.org/W2951770173",
    "https://openalex.org/W2952637581",
    "https://openalex.org/W2552465644",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2952054889",
    "https://openalex.org/W2963542991",
    "https://openalex.org/W1889898024",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2951234442",
    "https://openalex.org/W2963591054",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2518995263",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2401944043",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2950477723",
    "https://openalex.org/W2022508996",
    "https://openalex.org/W1546771929"
  ],
  "abstract": "The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.",
  "full_text": "Dense Transformer Networks\nJun Li\nSchool of Electrical Engineering\nand Computer Science\nWashington State University\nPullman, W A 99163\njun.li3@wsu.edu\nYongjun Chen\nSchool of Electrical Engineering\nand Computer Science\nWashington State University\nPullman, W A 99163\nyongjun.chen@wsu.edu\nLei Cai\nSchool of Electrical Engineering\nand Computer Science\nWashington State University\nPullman, W A 99163\nlei.cai@wsu.edu\nIan Davidson\nComputer Science Department\nUniversity of California, Davis\nDavis, CA 95616\ndavidson@cs.ucdavis.edu\nShuiwang Ji\nSchool of Electrical Engineering\nand Computer Science\nWashington State University\nPullman, W A 99163\nsji@eecs.wsu.edu\nAbstract\nThe key idea of current deep learning methods for dense prediction is to apply a\nmodel on a regular patch centered on each pixel to make pixel-wise predictions.\nThese methods are limited in the sense that the patches are determined by network\narchitecture instead of learned from data. In this work, we propose the dense\ntransformer networks, which can learn the shapes and sizes of patches from data.\nThe dense transformer networks employ an encoder-decoder architecture, and a\npair of dense transformer modules are inserted into each of the encoder and decoder\npaths. The novelty of this work is that we provide technical solutions for learning\nthe shapes and sizes of patches from data and efﬁciently restoring the spatial\ncorrespondence required for dense prediction. The proposed dense transformer\nmodules are differentiable, thus the entire network can be trained. We apply the\nproposed networks on natural and biological image segmentation tasks and show\nsuperior performance is achieved in comparison to baseline methods.\n1 Introduction\nIn recent years, deep convolution neural networks (CNNs) have achieved promising performance on\nmany artiﬁcial intelligence tasks, including image recognition [21, 19], object detection [29, 13], and\nsegmentation [12, 26, 27, 3, 31]. Among these tasks, dense prediction tasks take images as inputs\nand generate output maps with similar or the same size as the inputs. For example, in image semantic\nsegmentation, we need to predict a label for each pixel on the input images [23, 25]. Other examples\ninclude depth estimation [ 20, 10], image super-resolution [ 8], and surface normal prediction [ 9].\nThese tasks can be generally considered as image-to-image translation problems in which inputs are\nimages, and outputs are label maps [16].\narXiv:1705.08881v2  [cs.CV]  8 Jun 2017\nGiven the success of deep learning methods on image-related applications, numerous recent attempts\nhave been made to solve dense prediction problems using CNNs. A central idea of these methods is\nto extract a square patch centered on each pixel and apply CNNs on each of them to compute the label\nof the center pixel. The efﬁciency of these approaches can be improved by using fully convolutional\nor encoder-decoder networks. Speciﬁcally, fully convolutional networks [23] replace fully connected\nlayers with convolutional layers, thereby allowing inputs of arbitrary size during both training and\ntest. In contrast, deconvolution networks [25] employ an encoder-decoder architecture. The encoder\npath extracts high-level representations using convolutional and pooling layers. The decoder path\nuses deconvolutional and up-pooling layers to recovering the original spatial resolution. In order to\ntransmit information directly from encoder to decoder, the U-Net [28] adds skip connections [14]\nbetween the corresponding encoder and decoder layers. A common property of all these methods is\nthat the label of any pixel is determined by a regular (usually square) patch centered on that pixel.\nAlthough these methods have achieved considerable practical success, there are limitations inherent\nin them. For example, once the network architecture is determined, the patches used to predict the\nlabel of each pixel is completely determined, and they are commonly of the same size for all pixels.\nIn addition, the patches are usually of a regular shape, e.g., squares.\nIn this work, we propose the dense transformer networks to address these limitations. Our method\nfollows the encoder-decoder architecture in which the encoder converts input images into high-level\nrepresentations, and the decoder tries to make pixel-wise predictions by recovering the original spatial\nresolution. Under this framework, the label of each pixel is also determined by a local patch on\nthe input. Our method allows the size and shape of every patch to be adaptive and data-dependent.\nIn order to achieve this goal, we propose to insert a spatial transformer layer [ 17] in the encoder\npart of our network. We propose to use nonlinear transformations, such as these based on thin-plate\nsplines [30, 2]. The nonlinear spatial transformer layer transforms the feature maps into a different\nspace. Therefore, performing regular convolution and pooling operations in this space corresponds to\nperforming these operations on irregular patches of different sizes in the original space. Since the\nnonlinear spatial transformations are learned automatically from data, this corresponds to learning\nthe size and shape of each patch to be used as inputs for convolution and pooling operations.\nThere has been prior work on allowing spatial transformations or deformations in deep networks [17,\n6], but they do not address the spatial correspondence problem, which is critical in dense prediction\ntasks. The difﬁculty in applying spatial transformations to dense prediction tasks lies in that the spatial\ncorrespondence between input images and output label maps needs to be preserved. A key innovation\nof this work is that we provide a new technical solution that not only allows data-dependent learning\nof patches but also enables the preservation of spatial correspondence. Speciﬁcally, although the\npatches used to predict pixel labels could be of different sizes and shapes, we expect the patches\nto be in the spatial vicinity of pixels whose labels are to be predicted. By applying the nonlinear\nspatial transformer layers in the encoder path as described above, the spatial locations of units on the\nintermediate feature maps after the spatial transformation layer may not be preserved. Thus a reverse\ntransformation is required to restore the spatial correspondence.\nIn order to restore the spatial correspondence between inputs and outputs, we propose to add a\ncorresponding decoder layer. A technical challenge in developing the decoder layer is that we need to\nmap values of units arranged on input regular grid to another set of units arranged on output grid,\nwhile the nonlinear transformation could map input units to arbitrary locations on the output map. We\ndevelop a interpolation method to address this challenge. Altogether, our work results in the dense\ntransformer networks, which allow the prediction of each pixel to adaptively choose the input patch in\na data-dependent manner. The dense transformer networks can be trained end-to-end, and gradients\ncan be back-propagated through both the encoder and decoder layers. Experimental results on natural\nand biological images demonstrate the effectiveness of the proposed dense transformer networks.\n2 Spatial Transformer Networks Based on Thin-Plate Spline\nSpatial transformer networks [17] are deep models containing spatial transformer layers. These layers\nexplicitly compute a spatial transformation of the input feature maps. They can be inserted into\nconvolutional neural networks to perform explicit spatial transformations. The spatial transformer\nlayers consist of three components; namely, the localization network, grid generator and sampler.\n2\nThe localization network takes a set of feature maps as input and generates parameters to control\nthe transformation. If there are multiple feature maps, the same transformation is applied to all of\nthem. The grid generator constructs transformation mapping between input and output grids based\non parameters computed from the localization network. The sampler computes output feature maps\nbased on input feature maps and the output of grid generator. The spatial transformer layers are\ngeneric and different types of transformations, e.g., afﬁne transformation, projective transformation,\nand thin-plate spline (TPS), can be used. Our proposed work is based on the TPS transformation, and\nit is not described in detail in the original paper [17]. Thus we provide more details below.\n2.1 Localization Network\nWhen there are multiple feature maps, the same transformation is applied to all of them. Thus, we\nassume there is only one input feature map below. The TPS transformation is determined by 2K\nﬁducial points among which K points lie on the input feature map and the other K points lie on\nthe output feature map. On the output feature map, the K ﬁducial points, whose coordinates are\ndenoted as ˜F = [ ˜f1, ˜f2, ··· , ˜fK] ∈R2×K, are evenly distributed on a ﬁxed regular grid, where\n˜fi = [˜xi, ˜yi]T denotes the coordinates of the ith point. The localization network is used to learn\nthe K ﬁducial points F = [f1, f2, ··· , fK] ∈R2×K on the input feature map. Speciﬁcally, the\nlocalization network, denoted as floc(·), takes the input feature maps U ∈RH×W×C as input, where\nH, W and C are the height, width and number of channels of input feature maps, and generates the\nnormalized coordinates F as the output as F = floc(U).\nA cascade of convolutional, pooling and fully-connected layers is used to implement floc(·). The\noutput of the ﬁnal fully-connected layer is the coordinates F on the input feature map. Therefore,\nthe number of output units of the localization network is 2K. In order to ensure that the outputs are\nnormalized between −1 and 1, the activation function tanh(·) is used in the fully-connected layer.\nSince the localization network is differentiable, the K ﬁducial points can be learned from data using\nerror back-propagation.\n2.2 Grid Generator\nFor each unit lying on a regular grid on the output feature map, the grid generator computes the\ncoordinate of the corresponding unit on the input feature map. This correspondence is determined\nby the coordinates of the ﬁducial points F and ˜F. Given the evenly distributed K points ˜F =\n[ ˜f1, ˜f2, ··· , ˜fK] on the output feature map and the K ﬁducial points F = [f1, f2, ··· , fK] generated\nby the localization network, the transformation matrix T in TPS can be expressed as follows:\nT =\n(\n∆−1\n˜F ×\n[\nFT\n03×2\n])T\n∈R2×(K+3), (1)\nwhere ∆ ˜F ∈R(K+3)×(K+3) is a matrix determined only by ˜F as\n∆ ˜F =\n\n\n1K×1 ˜FT R\n01×1 01×2 11×K\n02×1 02×2 ˜F\n\n∈R(K+3)×(K+3), (2)\nwhere R ∈RK×K, and its elements are deﬁned as ri,j = d2\ni,j ln d2\ni,j, and di,j denotes the Euclidean\ndistance between ˜fi and ˜fj.\nThrough the mapping, each unit (˜xi, ˜yi) on the output feature map corresponds to unit (xi, yi) on\nthe input feature map. To achieve this mapping, we represent the units on the regular output grid by\n{˜pi}\n˜H× ˜W\ni=1 , where ˜pi = [˜xi, ˜yi]T is the (x, y)-coordinates of the ith unit on output grid, and ˜H and\n˜W are the height and width of output feature maps. Note that the ﬁducial points {˜fi}K\ni=1 are a subset\nof the points {˜pi}\n˜H× ˜W\ni=1 , which are the set of all points on the regular output grid.\nTo apply the transformation, each point ˜pi is ﬁrst extended from R2 space to RK+3 space as\n˜qi = [1, ˜xi, ˜yi, si,1, si,2, ··· , si,K]T ∈RK+3, where si,j = e2\ni,j ln e2\ni,j, and ei,j is the Euclidean\ndistance between ˜pi and ˜fj. Then the transformation can be expressed as\npi = T ˜qi, (3)\n3\nwhere T is deﬁned in Eq. (1). By this transformation, each coordinate (˜xi, ˜yi) on the output feature\nmap corresponds to a coordinate (xi, yi) on the input feature map. Note that the transformation T is\ndeﬁned so that the points ˜F map to points F.\n2.3 Sampler\nThe sampler generates output feature maps based on input feature maps and the outputs of grid\ngenerator. Each unit ˜pi on the output feature map corresponds to a unit pi on the input feature map\nas computed by Eq. (3). However, the coordinates pi = (xi, yi)T computed by Eq. (3) may not lie\nexactly on the input regular grid. In these cases, the output values need to be interpolated from input\nvalues lying on regular grid. For example, a bilinear sampling method can be used to achieve this.\nSpeciﬁcally, given an input feature map U ∈RH×W , the output feature map V ∈R ˜H× ˜W can be\nobtained as\nVi =\nH∑\nn=1\nW∑\nm=1\nUnm max(0, 1−|xi−m|) max(0, 1−|yi−n|) (4)\nfor i = 1, 2, ··· , ˜H × ˜W, where Vi is the value of pixel i, Unm is the value at (n, m) on the input\nfeature map, pi = (xi, yi)T , and pi is computed from Eq. (3). By using the transformations, the\nspatial transformer networks have been shown to be invariant to some transformations on the inputs.\nOther recent studies have also attempted to make CNNs to be invariant to various transformations [18,\n15, 5, 7].\n3 Dense Transformer Networks\nThe central idea of CNN-based method for dense prediction is to extract a regular patch centered\non each pixel and apply CNNs to compute the label of that pixel. A common property of these\nmethods is that the label of each pixel is determined by a regular (typically square) patch centered on\nthat pixel. Although these methods have been shown to be effective on dense prediction problems,\nthey lack the ability to learn the sizes and shapes of patches in a data-dependent manner. For a\ngiven network, the size of patches used to predict the labels of each center pixel is determined by\nthe network architecture. Although multi-scale networks have been proposed to allow patches of\ndifferent sizes to be combined [12], the patch sizes are again determined by network architectures.\nIn addition, the shapes of patches used in CNNs are invariably regular, such as squares. Ideally, the\nshapes of patches may depend on local image statistics around that pixel and thus should be learned\nfrom data. In this work, we propose the dense transformer networks to enable the learning of patch\nsize and shape for each pixel.\n3.1 An Encoder-Decoder Architecture\nIn order to address the above limitations, we propose to develop a dense transformer network model.\nOur model employs an encoder-decoder architecture in which the encoder path extracts high-level\nrepresentations using convolutional and pooling layers and the decoder path uses deconvolution and\nun-pooling to recover the original spatial resolution [25, 28, 1, 24]. To enable the learning of size and\nshape of each patch automatically from data, we propose to insert a spatial transformer module in the\nencoder path in our network. As has been discussed above, the spatial transformer module transforms\nthe feature maps into a different space using nonlinear transformations. Applying convolution and\npooling operations on regular patches in the transformed space is equivalent to operating on irregular\npatches of different sizes in the original space. Since the spatial transformer module is differentiable,\nits parameters can be learned with error back-propagation algorithms. This is equivalent to learning\nthe size and shape of each patch from data.\nAlthough the patches used to predict pixel labels could be of different sizes and shapes, we expect the\npatches to include the pixel in question at least. That is, the patches should be in the spatial vicinity\nof pixels whose labels are to be predicted. By using the nonlinear spatial transformer layer in encoder\npath, the spatial locations of units on the intermediate feature maps could have been changed. That is,\ndue to this nonlinear spatial transformation, the spatial correspondence between input images and\noutput label maps is not retained in the feature maps after the spatial transformer layer. In order to\nrestore this spatial correspondence, we propose to add a corresponding decoder layer, known as the\n4\nHigh-level \nFeature \nSpatial \nTransformer \nSpatial \nDecoder \nEncoder Path Decoder Path \nInput Image Output \nLabel Map \nA B\nCD\nP\nS1S2\nS3 S4\nT A B\nCD\nP\nT\nS3 S4\nS2 S1\nFigure 1: The proposed dense transformer networks. A pair of dense transformer modules are inserted\ninto each of the encoder and decoder paths. In the spatial transformer module, values at points A, B,\nC, and D are given from the previous layer, and we need to estimate value for pointP. In contrast,\nin the decoder layer, value at point P is given from the previous layer, and we need to estimate values\nfor points A, B, C, and D.\ndense transformer decoder layer. This decoder layer transforms the intermediate feature maps back to\nthe original input space, thereby re-establishing the input-output spatial correspondence.\nThe spatial transformer module can be inserted after any layer in the encoder path while the dense\ntransform decoder module should be inserted into the corresponding location in decoder path. In our\nframework, the spatial transformer module is required to not only output the transformed feature\nmaps, but also the transformation itself that captures the spatial correspondence between input and\noutput feature maps. This information will be used to restore the spatial correspondence in the\ndecoder module. Note that in the spatial transformer encoder module, the transformation is computed\nin the backward direction, i.e., from output to input feature maps (Figure 1). In contrast, the dense\ntransformer decoder module uses a forward direction instead; that is, a mapping from input to output\nfeature maps. This encoder-decoder pair can be implemented efﬁciently by sharing the transformation\nparameters in these two modules.\nA technical challenge in developing the dense transformer decoder layer is that we need to map values\nof units arranged on input regular grid to another set of units arranged on regular output grid, while\nthe decoder could map to units at arbitrary locations on the output map. That is, while we need to\ncompute the values of units lying on regular output grid from values of units lying on regular input\ngrid, the mapping itself could map an input unit to an arbitrary location on the output feature map,\ni.e., not necessarily to a unit lying exactly on the output grid. To address this challenge, we develop a\nsampler method for performing interpolation. We show that the proposed samplers are differentiable,\nthus gradients can be propagated through these modules. This makes the entire dense transformer\nnetworks fully trainable. Formally, assume that the encoder and decoder layers are inserted after the\ni-th and j-th layers, respectively, then we have the following relationships:\nUi+1(p) = Sampling{Ui(Tp)}, U j+1(Tp) = Uj(p), U j+1(p) = Sampling{Uj+1(Tp)}, (5)\nwhere Ui is the feature map of the i-th layer, p is the coordinate of a point, T is the transformation\ndeﬁned in Eq. (1), which maps from the coordinates of the(i+1)-th layer to thei-th layer, Sampling(·)\ndenotes the sampler function.\nFrom a geometric perspective, a value associated with an estimated point in bilinear interpolation\nin Eq. (4) can be interpreted as a linear combination of values at four neighboring grid points. The\nweights for linear combination are areas of rectangles determined by the estimated points and four\nneighboring grid points. For example, in Figure 1, when a point is mapped to P on input grid,\nthe contributions of points A, B, C, and D to the estimated point P is determined by the areas of\n5\nthe rectangles S1, S2, S3, and S4. However, the interpolation problem needs to be solved in the\ndense transformer decoder layer is different with the one in the spatial transformer encoder layer, as\nillustrated in Figure 1. Speciﬁcally, in the encoder layer, the points A, B, C, and D are associated\nwith values computed from the previous layer, and the interpolation problem needs to compute a value\nfor P to be propagated to the next layer. In contrast, in the decoder layer, the point P is associated\nwith a value computed from the previous layer, and the interpolation problem needs to compute\nvalues for A, B, C, and D. Due to the different natures of the interpolation problems need to be\nsolved in the encoder and decoder modules, we propose a new sampler that can efﬁciently interpolate\nover decimal points in the following section.\n3.2 Decoder Sampler\nIn the decoder sampler, we need to estimate values of regular grid points based on those from arbitrary\ndecimal points, i.e., those that do not lie on the regular grid. For example, in Figure 1, the value at\npoint P is given from the previous layer. After the TPS transformation in Eq. (3), it may be mapped\nto an arbitrary point. Therefore, the values of grid points A, B, C, and D need to be computed\nbased on values from a set of arbitrary points. If we compute the values from surrounding points\nas in the encoder layer, we might have to deal with a complex interpolation problem over irregular\nquadrilaterals. Those complex interpolation methods may yield more accurate results, but we prefer a\nsimpler and more efﬁcient method in this work. Speciﬁcally, we propose a new sampling method,\nwhich distributes the value of P to the points A, B, C, and D in an intuitive manner. Geometrically,\nthe weights associated with points A, B, C, and D are the area of the rectangles S1, S2, S3, and S4,\nrespectively (Figure 1). In particular, given an input feature map V ∈R ˜H× ˜W , the output feature map\nU ∈RH×W can be obtained as\nSnm =\n˜H× ˜W∑\ni=1\nmax(0, 1 −|xi −m|) max(0, 1 −|yi −n|), (6)\nUnm = 1\nSnm\n˜H× ˜W∑\ni=1\nVi max(0, 1 −|xi −m|) max(0, 1 −|yi −n|), (7)\nwhere Vi is the value of pixel i, pi = (xi, yi)T is transformed by the shared transformation T in Eq.\n(1), Unm is the value at the (n, m)-th location on the output feature map, Snm is a normalization\nterm that is used to eliminate the effect that different grid points may receive values from different\nnumbers of arbitrary points, and n = 1, 2, ··· , N, m= 1, 2, ··· , M.\nIn order to allow the backpropagation of errors, we deﬁne the gradient with respect to Unm as dUnm.\nThen the gradient with respect to Vnm and xi can be derived as follows:\ndVi =\nH∑\nn=1\nW∑\nm=1\n1\nSnm\ndUnm max(0, 1 −|xi −m|) max(0, 1 −|yi −n|), (8)\ndSnm = −dUnm\nS2nm\n˜H× ˜W∑\ni=1\nVi max(0, 1 −|xi −m|) max(0, 1 −|yi −n|), (9)\ndxi =\nH∑\nn=1\nW∑\nm=1\n{dUnm\nSnm\nVi + dSnm\n}\nmax(0, 1 −|yi −n|) ×\n{ 0 if |m−xi|≥ 1\n1 if m ≥xi\n−1 if m ≤xi\n. (10)\nA similar gradient can be derived for dyi. This provides us with a differentiable sampling mechanism,\nwhich enables the gradients ﬂow back to both the input feature map and the sampling layers.\n4 Experimental Evaluation\nWe evaluate the proposed methods on two image segmentation tasks. The U-Net [28] is adopted as\nour base model in both tasks, as it has achieved state-of-the-art performance on image segmentation\ntasks. Speciﬁcally, U-Net adds residual connections between the encoder path and decoder path to\nincorporate both low-level and high-level features. Other methods like SegNet [1], deconvolutional\nnetworks [32] and FCN [23] mainly differ from U-Net in the up-sampling method and do not use\n6\nTable 1: Comparison of segmentation performance between the U-Net and the proposed DTN on\nthe PASCAL 2012 segmentation data set. Three different performance measures are used here. An\narrow is attached to each measure so that ↑denotes higher values indicate better performance, and ↓\ndenotes lower values indicate better performance.\nDATA SET MODEL LOSS ↓ ACCURACY ↑ MEAN -IOU ↑\nPASCAL U-N ET 0.9396 0.8117 0.4145\nDTN 0.7909 0.8367 0.5297\nFigure 2: Sample segmentation results on the PASCAL 2012 segmentation data set. The ﬁrst and\nsecond rows are the original images and the corresponding ground truth, respectively. The third and\nfourth rows are the segmentation results of U-Net and DTN, respectively.\nresidual connections. Experiments in prior work show that residual connections are important while\ndifferent up-sampling methods lead to similar results. The network consists of 5 layers in the encoder\npath and another corresponding 5 layers in the decoder path. We use 3 ×3 kernels and one pixel\npadding to retain the size of feature maps at each level.\nIn order to efﬁciently implement the transformations, we insert the spatial encoder layer and dense\ntransformer decoder layer into corresponding positions at the same level. Speciﬁcally, the layers are\napplied to the 4th layer, and their performance is compared to the basic U-Net model without spatial\ntransformations. As for the transformation layers, we use 16 ﬁducial points that are evenly distributed\non the output feature maps. In the dense transformer decoder layer, if there are pixels that are not\nselected on the output feature map, we apply an interpolation strategy over its neighboring pixels on\nprevious feature maps to produce smooth results.\n4.1 Natural Image Semantic Segmentation\nWe use the PASCAL 2012 segmentation data set [11] to evaluate the proposed methods on natural\nimage semantic segmentation task. In this task, we predict one label out of a total of 21 classes\nfor each pixel. To avoid the inconvenience of different sizes of images, we resize all the images\nto 256×256. Multiple performance metrics, including loss, accuracy, and mean-IOU, are used to\nmeasure the segmentation performance, and the results are reported in Table 1. We can observe that\nthe proposed DTN model achieves higher performance than the baseline U-Net model. Especially, it\nimproves the mean-IOU from 0.4145 to 0.5297. Some example results along with the raw images\nand ground truth label maps are given in Figure 2. These results demonstrate that the proposed DTN\nmodel can boost the segmentation performance dramatically.\n7\nRaw image Ground truth U-Net output DTN output\nFigure 4: Example results generated by the U-Net and the proposed DTN models for the SNEMI3D\ndata set.\n4.2 Brain Electron Microscopy Image Segmentation\nWe evaluate the proposed methods on brain electron microscopy (EM) image segmentation\ntask [ 22, 4], in which the ultimate goal is to reconstruct neurons at the micro-scale level. A\ncritical step in neuron reconstruction is to segment the EM images. We use data set from the\n3D Segmentation of Neurites in EM Images (SNEMI3D, http://brainiac2.mit.edu/SNEMI3D/).\nThe SNEMI3D data set consists of 100 1024 ×1024 EM image slices. Since we perform 2D\ntransformations in this work, each image slice is segmented separately in our experiments.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFalse Positive Rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate\nU-Net\nDTN\nU-Net AUC: 0.86761\nDTN AUC: 0.89532 \nFigure 3: Comparison of the ROC curves of\nthe U-Net and the proposed DTN model on the\nSNEMI3D data set.\nThe task is to predict each pixel as either a\nboundary (denoted as 1) or a non-boundary pixel\n(denoted as 0).\nOur model can process images of arbitrary size.\nHowever, training on whole images may incur\nexcessive memory requirement. In order to ac-\ncelerate training, we randomly pick 224 ×224\npatches from the original images and use it to\ntrain the networks. The experimental results in\nterms of ROC curves are provided in Figure 3.\nWe can observe that the proposed DTN model\nachieves higher performance than the baseline\nU-Net model, improving AUC from 0.8676 to\n0.8953. These results demonstrate that the pro-\nposed DTN model improves upon the baseline\nU-Net model, and the use of the dense trans-\nformer encoder and decoder modules in the U-Net architecture results in improved performance.\nSome example results along with the raw images and ground truth label maps are given in Figure 4.\n4.3 Timing Comparison\nTable 2 shows the comparison of training and prediction time between the U-Net model and the\nproposed DTN model on the two data sets. We can see that adding DTN layers leads to only slight\nincrease in training and prediction time. Since the PASCAL data set is more complex than the\nSNEMEI3D data set, we use more channels when building the network of natural image segmentation\ntask. That causes the increase of training and prediction time on the PASCAL data set as compared to\nSNEMEI3D.\n5 Conclusion\nIn this work, we propose the dense transformer networks to enable the automatic learning of patch\nsizes and shapes in dense prediction tasks. This is achieved by transforming the intermediate feature\nmaps to a different space using nonlinear transformations. A unique challenge in dense prediction\ntasks is that, the spatial correspondence between inputs and outputs should be preserved in order to\n8\nTable 2: Training and prediction time on the two data sets using a Tesla K40 GPU. We compare the\ntraining time of 10,000 iterations and prediction time of 2019 (PASCAL) and 40 (SNEMI3D) images\nfor the base U-Net model and the DTN.\nDATA SET MODEL TRAINING TIME PREDICTION TIME\nPASCAL U-N ET 378 M57S 14M06S\nDTN 402 M07S 15M50S\nSNEMI3D U-N ET 14M18S 3M31S\nDTN 15 M41S 4M02S\nmake pixel-wise predictions. To this end, we develop the dense transformer decoder layer to restore\nthe spatial correspondence. The proposed dense transformer modules are differentiable. Thus the\nentire network can be trained from end to end. Experimental results show that adding the spatial\ntransformer and decoder layers to existing models leads to improved performance. To the best of our\nknowledge, our work represents the ﬁrst attempt to enable the learning of patch size and shape in\ndense prediction. The current study only adds one encoder layer and one decoder layer in the baseline\nmodels. We will explore the possibility of adding multiple encoder and decoder layers at different\nlocations of the baseline model. In this work, we develop a simple and efﬁcient decoder sampler for\ninterpolation. A more complex method based on irregular quadrilaterals might be more accurate and\nwill be explored in the future.\nAcknowledgments\nThis work was supported in part by National Science Foundation grants IIS-1615035 and DBI-\n1641223, and by Washington State University. We gratefully acknowledge the support of NVIDIA\nCorporation with the donation of the Tesla K40 GPU used for this research.\nReferences\n[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image seg-\nmentation. arXiv preprint arXiv:1511.00561, 2015.\n[2] Fred L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Transactions on pattern analysis\nand machine intelligence, 11(6):567–585, 1989.\n[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep\nconvolutional nets and fully connected CRFs. In Proceedings of the International Conference on Learning Representations, 2015.\n[4] Dan Ciresan, Alessandro Giusti, Luca M Gambardella, and Jürgen Schmidhuber. Deep neural networks segment neuronal membranes in\nelectron microscopy images. In Advances in neural information processing systems, pages 2843–2851, 2012.\n[5] Taco Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of The 33rd International Conference on\nMachine Learning, pages 2990–2999, 2016.\n[6] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. arXiv\npreprint arXiv:1703.06211, 2017.\n[7] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional neural networks. In Proceed-\nings of The 33rd International Conference on Machine Learning, pages 1889–1898, 2016.\n[8] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 38(2):295–307, 2016.\n[9] David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architec-\nture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650–2658, 2015.\n[10] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In\nAdvances in neural information processing systems, pages 2366–2374, 2014.\n[11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc)\nchallenge. International journal of computer vision, 88(2):303–338, 2010.\n[12] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features for scene labeling. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 35(8):1915–1929, 2013.\n[13] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2016.\n[15] João F Henriques and Andrea Vedaldi. Warped convolutions: Efﬁcient invariance to spatial transformations. arXiv preprint\narXiv:1609.04382, 2016.\n9\n[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv\npreprint arXiv:1611.07004, 2016.\n[17] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing\nSystems, pages 2017–2025, 2015.\n[18] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic ﬁlter networks. In Advances in Neural Information Processing\nSystems, pages 667–675, 2016.\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances\nin neural information processing systems, pages 1097–1105, 2012.\n[20] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolu-\ntional residual networks. In 2016 Fourth International Conference on 3D Vision, pages 239–248. IEEE, 2016.\n[21] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition.Proceedings of the IEEE, 86\n(11):2278–2324, November 1998.\n[22] Kisuk Lee, Aleksandar Zlateski, Vishwanathan Ashwin, and H Sebastian Seung. Recursive training of 2D-3D convolutional networks\nfor neuronal boundary prediction. In Advances in Neural Information Processing Systems, pages 3573–3581, 2015.\n[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015.\n[24] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In European Conference on\nComputer Vision, pages 483–499. Springer, 2016.\n[25] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of\nthe IEEE International Conference on Computer Vision, pages 1520–1528, 2015.\n[26] Pedro Pinheiro and Ronan Collobert. Recurrent convolutional neural networks for scene labeling. InProceedings of the 31st International\nConference on Machine Learning, pages 82–90, 2014.\n[27] Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr Dollár. Learning to reﬁne object segments. In European Conference on\nComputer Vision, pages 75–91. Springer, 2016.\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Interna-\ntional Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234–241. Springer, 2015.\n[29] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated recognition, local-\nization and detection using convolutional networks. In Proceedings of the International Conference on Learning Representations, April\n2014.\n[30] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Robust scene text recognition with automatic rectiﬁcation. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4168–4176, 2016.\n[31] Francesco Visin, Kyle Kastner, Aaron Courville, Yoshua Bengio, Matteo Matteucci, and Kyunghyun Cho. ReSeg: A recurrent neural\nnetwork for object segmentation. arXiv preprint arXiv:1511.07053, 2015.\n[32] Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. In Computer Vision and Pattern\nRecognition (CVPR), 2010 IEEE Conference on, pages 2528–2535. IEEE, 2010.\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7270201444625854
    },
    {
      "name": "Transformer",
      "score": 0.7049928903579712
    },
    {
      "name": "Encoder",
      "score": 0.7049920558929443
    },
    {
      "name": "Novelty",
      "score": 0.6486356854438782
    },
    {
      "name": "Segmentation",
      "score": 0.6009378433227539
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5749756693840027
    },
    {
      "name": "Pixel",
      "score": 0.5072181224822998
    },
    {
      "name": "Differentiable function",
      "score": 0.413437157869339
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.385824054479599
    },
    {
      "name": "Computer vision",
      "score": 0.3280905485153198
    },
    {
      "name": "Mathematics",
      "score": 0.1224202811717987
    },
    {
      "name": "Engineering",
      "score": 0.09299230575561523
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}