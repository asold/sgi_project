{
  "title": "Influence of a Large Language Model on Diagnostic Reasoning: A Randomized Clinical Vignette Study",
  "url": "https://openalex.org/W4392818978",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3009474382",
      "name": "Ethan Goh",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2156113513",
      "name": "Robert Gallo",
      "affiliations": [
        "VA Palo Alto Health Care System",
        "Center for Innovation"
      ]
    },
    {
      "id": "https://openalex.org/A2081653727",
      "name": "Jason Hom",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2343336781",
      "name": "Eric Strong",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2739612240",
      "name": "Yingjie Weng",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2910630016",
      "name": "Hannah Kerman",
      "affiliations": [
        "Beth Israel Deaconess Medical Center",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2687371818",
      "name": "Josephine Cool",
      "affiliations": [
        "Beth Israel Deaconess Medical Center",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2462783572",
      "name": "Zahir Kanjee",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2487257984",
      "name": "Andrew S Parsons",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2245618468",
      "name": "Neera Ahuja",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A1970391018",
      "name": "Eric Horvitz",
      "affiliations": [
        "Stanford Medicine",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2234892317",
      "name": "Daniel Yang",
      "affiliations": [
        "Kaiser Permanente"
      ]
    },
    {
      "id": "https://openalex.org/A2124493989",
      "name": "Arnold Milstein",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2566042474",
      "name": "Andrew P.J. Olson",
      "affiliations": [
        "University of Minnesota Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2540208133",
      "name": "Adam Rodman",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2300145783",
      "name": "Jonathan H Chen",
      "affiliations": [
        "Stanford Medicine",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3009474382",
      "name": "Ethan Goh",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2156113513",
      "name": "Robert Gallo",
      "affiliations": [
        "VA Palo Alto Health Care System",
        "Center for Innovation"
      ]
    },
    {
      "id": "https://openalex.org/A2081653727",
      "name": "Jason Hom",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2343336781",
      "name": "Eric Strong",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2739612240",
      "name": "Yingjie Weng",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2910630016",
      "name": "Hannah Kerman",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2687371818",
      "name": "Josephine Cool",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2462783572",
      "name": "Zahir Kanjee",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2487257984",
      "name": "Andrew S Parsons",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2245618468",
      "name": "Neera Ahuja",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A1970391018",
      "name": "Eric Horvitz",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2234892317",
      "name": "Daniel Yang",
      "affiliations": [
        "Kaiser Permanente"
      ]
    },
    {
      "id": "https://openalex.org/A2124493989",
      "name": "Arnold Milstein",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2566042474",
      "name": "Andrew P.J. Olson",
      "affiliations": [
        "University of Minnesota Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2540208133",
      "name": "Adam Rodman",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2300145783",
      "name": "Jonathan H Chen",
      "affiliations": [
        "Stanford University",
        "Stanford Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2068084886",
    "https://openalex.org/W2029907219",
    "https://openalex.org/W4390673899",
    "https://openalex.org/W2168490582",
    "https://openalex.org/W4311542542",
    "https://openalex.org/W4384524772",
    "https://openalex.org/W2020907317",
    "https://openalex.org/W2015769833",
    "https://openalex.org/W4389041771",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W6860299292",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4384561103",
    "https://openalex.org/W1966976587",
    "https://openalex.org/W4386045865",
    "https://openalex.org/W4386910356",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4283731530",
    "https://openalex.org/W2339530503",
    "https://openalex.org/W2911290735",
    "https://openalex.org/W2019221080",
    "https://openalex.org/W2099950126",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2337116044",
    "https://openalex.org/W4389325518",
    "https://openalex.org/W4391995913",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W4283172359",
    "https://openalex.org/W4294316742",
    "https://openalex.org/W3159307263",
    "https://openalex.org/W4306767166",
    "https://openalex.org/W1993250413",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2076168232",
    "https://openalex.org/W2111575166",
    "https://openalex.org/W2164973508",
    "https://openalex.org/W4366549904",
    "https://openalex.org/W2069269993"
  ],
  "abstract": "ABSTRACT Importance Diagnostic errors are common and cause significant morbidity. Large language models (LLMs) have shown promise in their performance on both multiple-choice and open-ended medical reasoning examinations, but it remains unknown whether the use of such tools improves diagnostic reasoning. Objective To assess the impact of the GPT-4 LLM on physicians’ diagnostic reasoning compared to conventional resources. Design Multi-center, randomized clinical vignette study. Setting The study was conducted using remote video conferencing with physicians across the country and in-person participation across multiple academic medical institutions. Participants Resident and attending physicians with training in family medicine, internal medicine, or emergency medicine. Intervention(s) Participants were randomized to access GPT-4 in addition to conventional diagnostic resources or to just conventional resources. They were allocated 60 minutes to review up to six clinical vignettes adapted from established diagnostic reasoning exams. Main Outcome(s) and Measure(s) The primary outcome was diagnostic performance based on differential diagnosis accuracy, appropriateness of supporting and opposing factors, and next diagnostic evaluation steps. Secondary outcomes included time spent per case and final diagnosis. Results 50 physicians (26 attendings, 24 residents) participated, with an average of 5.2 cases completed per participant. The median diagnostic reasoning score per case was 76.3 percent (IQR 65.8 to 86.8) for the GPT-4 group and 73.7 percent (IQR 63.2 to 84.2) for the conventional resources group, with an adjusted difference of 1.6 percentage points (95% CI -4.4 to 7.6; p=0.60). The median time spent on cases for the GPT-4 group was 519 seconds (IQR 371 to 668 seconds), compared to 565 seconds (IQR 456 to 788 seconds) for the conventional resources group, with a time difference of -82 seconds (95% CI -195 to 31; p=0.20). GPT-4 alone scored 15.5 percentage points (95% CI 1.5 to 29, p=0.03) higher than the conventional resources group. Conclusions and Relevance In a clinical vignette-based study, the availability of GPT-4 to physicians as a diagnostic aid did not significantly improve clinical reasoning compared to conventional resources, although it may improve components of clinical reasoning such as efficiency. GPT-4 alone demonstrated higher performance than both physician groups, suggesting opportunities for further improvement in physician-AI collaboration in clinical practice.",
  "full_text": " \n \nInfluence of a Large Language Model on Diagnostic Reasoning: A \nRandomized Clinical Vignette Study \nEthan Goh, MD, MS*,a,b, Robert Gallo, MD*,c, Jason Hom, MDd, Eric Strong, MDd, Yingjie \nWeng, MHSe, Hannah Kerman, MDf,g, Josephine Cool, MDf,g, Zahir Kanjee, MD, MPHf,g, \nAndrew S. Parsons, MD, MPHh, Neera Ahuja, MDd, Eric Horvitz, MD, PhDi,m, Daniel \nYang, MDk, Arnold Milstein, MDb, Andrew P.J Olson, MD§,j, Adam Rodman, MD, \nMPH§,f,g, Jonathan H Chen, MD, PhD§,a,b,l  \n \naStanford Center for Biomedical Informatics Research, Stanford University, Stanford, CA \nbStanford Clinical Excellence Research Center, Stanford University, Stanford, CA \ncCenter for Innovation to Implementation, VA Palo Alto Health Care System, PA, CA \ndStanford University School of Medicine, Stanford, CA \neQuantitative Sciences Unit, Stanford University School of Medicine, Stanford, CA \nfBeth Israel Deaconess Medical Center, Boston, MA \ngHarvard Medical School, Boston, MA \nhUniversity of Virginia, School of Medicine, Charlottesville, VA \niMicrosoft, Redmond, WA \njUniversity of Minnesota Medical School, Minneapolis, MN \nkKaiser Permanente, Oakland, CA \nlDivision of Hospital Medicine, Stanford University, Stanford, CA \nmStanford HAI, Stanford, CA \n* These authors contributed equally to this work \n§ These authors contributed equally to this work \n \n \nABSTRACT \nImportance: Diagnostic errors are common and cause significant morbidity. Large language models \n(LLMs) have shown promise in their performance on both multiple-choice and open-ended medical \nreasoning examinations, but it remains unknown whether the use of such tools improves diagnostic \nreasoning. \nObjective: To assess the impact of the GPT-4 LLM on physicians’ diagnostic reasoning compared to \nconventional resources. \nDesign: Multi-center, randomized clinical vignette study. \nSetting: The study was conducted using remote video conferencing with physicians across the country \nand in-person participation across multiple academic medical institutions. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n \nParticipants: Resident and attending physicians with training in family medicine, internal medicine, or \nemergency medicine. \nIntervention(s): Participants were randomized to access GPT-4 in addition to conventional diagnostic \nresources or to just conventional resources. They were allocated 60 minutes to review up to six clinical \nvignettes adapted from established diagnostic reasoning exams. \nMain Outcome(s) and Measure(s): The primary outcome was diagnostic performance based on \ndifferential diagnosis accuracy, appropriateness of supporting and opposing factors, and next diagnostic \nevaluation steps. Secondary outcomes included time spent per case and final diagnosis. \nResults: 50 physicians (26 attendings, 24 residents) participated, with an average of 5.2 cases completed \nper participant. The median diagnostic reasoning score per case was 76.3 percent (IQR 65.8 to 86.8) for \nthe GPT-4 group and 73.7 percent (IQR 63.2 to 84.2) for the conventional resources group, with an \nadjusted difference of 1.6 percentage points (95% CI -4.4 to 7.6; p=0.60). The median time spent on cases \nfor the GPT-4 group was 519 seconds (IQR 371 to 668 seconds), compared to 565 seconds (IQR 456 to \n788 seconds) for the conventional resources group, with a time difference of -82 seconds (95% CI -195 to \n31; p=0.20). GPT-4 alone scored 15.5 percentage points (95% CI 1.5 to 29, p=0.03) higher than the \nconventional resources group. \nConclusions and Relevance: In a clinical vignette-based study, the availability of GPT-4 to physicians as \na diagnostic aid did not significantly improve clinical reasoning compared to conventional resources, \nalthough it may improve components of clinical reasoning such as efficiency. GPT-4 alone demonstrated \nhigher performance than both physician groups, suggesting opportunities for further improvement in \nphysician-AI collaboration in clinical practice. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nINTRODUCTION \nMedical diagnosis is a high-stakes cognitive process that takes place in time-constrained and stressful \nclinical environments. Diagnostic errors are common and contribute to significant patient harm1,2,3,4,5,6. \nStrategies to reduce diagnostic errors include a variety of educational, reflective, and team-based \npractices. The impacts of these interventions have been limited, and even the most effective methods are \ndifficult to integrate into clinical practice at scale7,8. Artificial intelligence (AI) technologies have long \nbeen pursued as promising tools for assisting physicians with diagnostic reasoning. To date, research on \nAI in medicine has largely focused on diagnosis and prediction of outcomes in specific domains.  \n \nNew technological improvements in large language models (LLMs) – machine learning systems that \nproduce human-like responses from free text prompts – have shown the ability to solve complex cases, \ndisplay human-like clinical reasoning, take patient histories, and communicate empathetically9,10,11,12,13,14. \nLLMs can be scaled into a variety of clinical workflows given their generalizable nature, and are already \nbeing integrated into healthcare15,16. Early integrations of LLMs will almost certainly require a “human in \nthe loop” – augmenting, rather than replacing, human expertise and oversight17. Despite the impressive \nperformance of these emerging technologies in experimental settings and rapid moves toward integration \ninto clinical practice, considerable gaps remain in our understanding of how these systems affect human \nperformance. Meaningful measures of the quality of diagnostic reasoning may help close this gap. \n \nWe performed a randomized clinical vignette study using complex diagnostic cases to compare the \ndiagnostic reasoning performance of physicians using a commercial AI chatbot (ChatGPT Plus, GPT-4) \nwith the performance of physicians using conventional diagnostic reference resources. To move beyond \nsimplistic evaluations of diagnostic accuracy, we further developed and validated a novel assessment tool \nadapted from the literature on human diagnostic reasoning, structured reflection18. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nMETHODS: \n \n \nWe recruited practicing attendings and residents with training in a general medical specialty (internal \nmedicine, family medicine, or emergency medicine) through email lists used for community messaging at \nStanford University, Beth Israel Deaconess Medical Center, and the University of Virginia. Informed \nconsent was obtained prior to enrollment and randomization. Small groups of participants were proctored \nby study coordinators either remotely or at an in-person computer laboratory. Sessions lasted for one \nhour. Resident participants were offered $100 and attending participants were offered up to $200 for \ncompleting the study. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nClinical Vignettes: \nClinical vignettes were adapted from a landmark study that set the standards for the evaluation of \ncomputer-based diagnostic systems, including developing measures of diagnostic accuracy and relevance \n19. All cases were based on actual patients and included data available on initial diagnostic evaluation, \nincluding history, physical exam, and results of laboratory tests. The cases have never been publicly \nreleased to protect the validity of the test materials for future use; therefore, it is unlikely that the \nmaterials are included in GPT-4’s training data. Figure 2 includes a representative example of one of the \ncases. After iterative discussion among the investigators of all 110 cases, 6 were chosen to reflect \ndiagnostic challenges across different adult medicine specialties. Cases were edited to reflect modern \nlaboratory evaluation (e.g., referring to AST rather than SGOT) as necessary and pilot-tested with two \ngroups of participants not in the study.  \n \nA common gold standard in clinical decision support diagnostic studies has been the accuracy of \ndifferential diagnosis. Methods for the assessment of clinical reasoning by humans are far richer and \ninclude a variety of strategies including objective structured clinical exams (OSCEs), script concordance \ntesting, evaluation of documentation, and global assessments adapted from the psychological literature20. \nTo better capture the richness and nuance of diagnostic reasoning, we treated diagnostic accuracy as a \nsecondary outcome, and instead developed and validated as a primary measure of performance a more \nholistic assessment of reasoning, which we refer to as structured reflection. \n \nStructured reflection is aimed at capturing and improving the process by which physicians consider \nreasonable diagnoses and case features that support or oppose their diagnoses, similar to how physicians \nmay explain their reasoning in the “Assessment and Plan” component of clinical notes21,22. Adapting \nprevious methodologies demonstrated to improve diagnostic performance, participants completed a \nstructured reflection grid with free text responses. After user testing, we simplified the grid by collapsing \ntwo categories – evidential features that were missing and features that would have been expected but \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nwere not present – into a single category of features opposing the diagnosis. Additionally, participants \nwere asked to provide their most likely diagnosis and up to three next steps to further evaluate the patient.  \n \nGrading of Performance \nWe built upon previous studies of structured reflection by scoring the rubric itself, not just final diagnosis \naccuracy. For each case, we assigned up to 1 point for each plausible diagnosis. Findings supporting each \ndiagnosis and findings opposing the diagnosis were also graded based on correctness, with 0 points for \nincorrect or absent answers, 1 point for partially correct, and 2 points for completely correct responses. \nThe final diagnosis was graded as 2 points for the most correct diagnosis, while 1 point was awarded for a \nplausible diagnosis or a correct diagnosis that was not specific enough compared to the most correct \ndiagnosis. Finally, participants were instructed to describe up to 3 next steps to further evaluate the patient \nwith 0 points awarded for an incorrect response, 1 point awarded for a partially correct response and 2 \npoints for a completely correct response (see Supplementary 2, eTable 2). Participants who had incorrect \ndifferential diagnosis items but reasonable reasoning based on those items were not penalized.  \n \nStudy Design: \nWe employed a randomized, single-blinded study design. Participants were randomized to access GPT-4 \nvia the ChatGPT Plus interface (intervention group) or to conventional resources only (control group). \nBoth groups were permitted to access any resources they normally use for clinical care (with examples \ngiven of UpToDate [Wolters Kluwer, Philadelphia, PA], Epocrates [Athenahealth, Watertown, MA], and \nGoogle [Google, Mountain View, CA] search); the control group was explicitly instructed not to use large \nlanguage models (e.g., ChatGPT, Bard, Claude, MedPaLM, LLAMA2, etc.). Participants had one hour to \ncomplete as many of the six diagnostic cases as they could. Participants were instructed to prioritize \nquality of responses over completing all cases.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nThe study was conducted using a Qualtrics survey tool. Each case presented a clinical vignette for which \nparticipants were asked to complete the structured reflection process described above. Cases were \npresented in random order for each participant. In a secondary analysis, we included a comparison arm \nusing GPT-4 alone to answer the cases. Using established principles of prompt design, we iteratively \ndeveloped a few-shot prompt – a type of input where the language model is given examples to follow – \nby copy-pasting the clinical vignette questions23. For the prompt, we used the same example provided to \nthe human participants (Supplementary 5, eTable 5). These were run three times, and the results from the \nthree runs were included for blinded grading alongside the human outputs before any unblinding or data \nanalysis. \n \nAssessment Tool Validation  \nIn order to establish validity in our population, we collected two sets of data which were not included in \nthe final study, with 13 participants in total. The three primary scorers (J.H, A.R, and A.O.), all board-\ncertified physicians with experience in the evaluation of clinical reasoning at the post-graduate medical \nlevel, graded each of these sets together, to ensure consistency. After data collection, each case was \ngraded independently by 2 scorers who were blinded to the assigned treatment group. When scorers \ndisagreed, they met to engage on the differences in their assessments and to seek consensus. \nDisagreement was predefined by a difference of > 10% of the final score, based on experience that this \nrepresented a clinically significant disagreement. In addition, the final diagnosis scoring was adjudicated \nby two reviewers to obtain agreement for the secondary outcome of diagnostic accuracy. We calculated a \nweighted Cohen’s kappa to show concordance in grading. We calculated Cronbach's alpha to determine \nthe internal reliability of this measure.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nStudy outcome \nOur primary outcome was the final score as a percentage across all components of the structured \nreflection tool. A key secondary outcome was time spent per case in seconds. Final diagnosis accuracy, a \ncommon primary outcome in diagnosis studies, was evaluated as a secondary outcome. Final diagnosis \nwas treated as an ordinal outcome with three groups (incorrect, partially correct, and most correct). Since \nthe difference between the most correct response and partially correct responses may not be clinically \nmeaningful, we additionally analyzed the outcomes as binary (incorrect compared to at least partially \ncorrect). This represents the avoidance of a diagnostic error. \n \nStatistical Analysis: \nThe target sample size of 50 participants was pre-specified based on a power analysis using our two \nvalidations sets of data, scored prior to study enrollment, corresponding to an expected 200 to 250 cases \ncompleted (4-5 cases per participant). All analyses were at the case level, clustered by the participant. In \nthe primary analysis, we only included cases with completed responses. Generalized mixed-effect models \nwere applied to assess the difference in the primary and secondary outcomes of the GPT-4 group \ncompared to the conventional resources only group. A random-effect for the participant was included in \nthe model to account for the potential correlation between cases for a participant. Additionally, a random \neffect for cases was included to account for any potential variability in difficulty across cases. A pre-\nplanned sensitivity analysis evaluated the effect of including incomplete cases on the primary outcome. \nSubgroup analyses were conducted based on training status and experience with ChatGPT. In a secondary \nanalysis, cases completed by GPT-4 alone were treated as a third group with cases clustered in a nested \nstructure of 3 attempts under a single participant. These were compared to cases from real participants \nwith each case considered as a single attempt under a single participant using a similar nested structure.  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nAll statistical analysis was performed using R v4.3.2 (R Foundation for Statistical Computing, Vienna, \nAustria). Statistical significance was based on a p value <0.05.  This study was reviewed and determined \nto be exempt by institutional review boards at Stanford University, Beth Israel Deaconess Medical Center, \nand University of Virginia.  \n \n \nRESULTS \n50 US-licensed physicians were enlisted (26 attendings, 24 residents). Median years in practice was 3 \n(IQR 2-9). Further information on participants is included in Table 1 below. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nTable 1: Baseline Participant Characteristics \nParticipant Characteristics Overall \n(n=50) \nPhysicians + \nGPT-4 \n(n=25) \nPhysicians + \nConventional \nResources only \n(n=25) \nCareer Stage    \nAttendings 26 (52%) 13 (52%) 13 (52%) \nResidents 24 (48%) 12 (48%) 12 (48%) \nSpecialty    \nInternal Medicine 44 (88%) 22 (88%) 22 (88%) \nFamily Medicine 1 (2.0%) 1 (4.0%) 0 (0%) \nEmergency Medicine 5 (10%) 2 (8.0%) 3 (12%) \nYears in Practice (median [IQR]) 3 [2 - 7..8] 3 [2 - 7] 3 [2 - 9] \nPast ChatGPT Experience    \nI've never used it before 8 (16%) 5 (20%) 3 (12%) \nI've used it once ever 6 (12%) 4 (16%) 2 (8%) \nI use it rarely (less than once per month) 15 (30%) 7 (28%) 8 (32%) \nI use it occasionally (more than once per \nmonth but less than weekly) \n13 (26%) 6 (24%) 7 (28%) \nI use it frequently (weekly or more) 8 (16%) 3 (12%) 5 (20%) \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \n \nPrimary Outcome: Diagnostic performance \nMedian number of completed cases was 5.2. The median score per case was 76.3 (IQR 65.8 to 86.8) for \nthe GPT-4 group and 73.7 (IQR 63.2 to 84.2) for the conventional resources group. The generalized \nmixed effects model resulted in a difference of 1.6 percentage points (95% CI -4.4, 7.6; p=0.6) between \nthe GPT-4 and conventional resources groups as shown in Table 2. A sensitivity analysis including all \ncases, complete and incomplete, showed a similar result with a difference of 2.0 percentage points (95% \nCI -4.1 to 8.2; p=0.5) between the GPT-4 and conventional resources group. \n \nPast ChatGPT Experience (Binary)    \nLess than monthly 29 (58%) 16 (64%) 13 (52%) \nMore than monthly 21 (42%) 9 (36%) 12 (48%) \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nTable 2: Performance Outcomes \n \nGroup \nDiagnostic Performance (percentage points) \nPhysicians + GPT-4 Physicians + \nConventional Resources \nDifference \n(95%CI) \nP- \nvalue \nAll \nParticipants \n76.3 (65.8, 86.8) 73.7 (63.2, 84.2) 1.6 (-4.4, 7.6) 0.60 \nLevel of Training \nAttending 78.9 (63.2, 86.8) 75.0 (60.5, 86.8) 0.5 (-8.9, 9.9) 0.92 \nResident 76.3 (68.4, 84.2) 73.7 (63.2, 84.2) 2.8 (-5.5, 11.1) 0.50 \nChatGPT Experience \nLess than \nmonthly \n76.3 (63.2, 84.2) 76.3 (63.2, 86.8) -0.5 (-7.7, 6.7) 0.90 \nMore than \nmonthly \n78.9 (67.8, 89.5) 73.7 (62.5, 84.2) 4.5 (-6.7, 15.7) 0.40 \n \nCaption: Continuous variables are expressed as median (interquartile range). Differences between \ngroups are reported from the multilevel analysis accounting for clustering of cases by participant. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nTable 3: Time Spent Per Case \n \nGroup \nTime spent per case (seconds) \nPhysicians + GPT-4 Physicians + \nConventional \nResources \nDifference (95% CI) P- value \nAll \nParticipants \n519 (371, 668) 565 (456, 788) -81.9 (-195.1, 31.3) 0.15 \nLevel of Training \nAttending 533 (389, 672) 563 (435, 778)  -73 (-204 to 58) 0.26 \nResident 478 (356, 654) 565 (458, 800) -76 (-284, 131)  0.45 \nChatGPT Experience \nLess than \nmonthly \n556 (415, 742) 572 (474, 778) -46 (-219, 127) 0.59 \nMore than \nmonthly \n462 (305, 627) 556 (427, 810) -140 (-294, 13) 0.07 \nCaption: Continuous variables are expressed as median (interquartile range). Differences between \ngroups are reported from the multilevel analysis accounting for clustering of cases by participant. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nSecondary Outcomes \nThe median time spent per case was 519 seconds (IQR 371 to 668 seconds) for the GPT-4 group and 565 \nseconds (IQR 456 to 788 seconds) for the conventional resources group (Table 3). The linear mixed \neffects model resulted in an adjusted difference of -82 seconds (95% CI -195 seconds to 31 seconds; \np=0.20). \n \nAccuracy of final diagnosis was evaluated as well, as shown in Supplementary 3, eTable 3. Using the \nordinal scale, the GPT-4 group had a 1.4 higher odds (95% CI 0.67 to 2.8; p=0.39) of a more correct \ndiagnosis. Treating final diagnosis as binary correct compared to incorrect did not qualitatively change \nthe results (OR 1.9, 95% CI 0.9 to 4.0; p=0.10). \n \nSubgroup Analyses: \nTables 2 and 3 include the analyses by subgroups, including level of training and level of prior experience \nwith chatGPT.  Subgroup analyses were directionally similar to the analyses for the whole cohort. \n \nGPT-4 alone  \nIn the three runs of GPT-4 alone, the median score per case was 92.1 percentage points (IQR 82.2 to \n97.4). Comparing GPT-4 alone to the human with conventional resources group found a score difference \nof 15.5 percentage points (95% CI 1.5 to 29.5 percentage points; p=0.03) favoring GPT-4 alone \n(Supplementary 4, eTable 4). \n \nAssessment Tool Validation \nThe weighted Cohen’s kappa between all three graders was 0.66, indicating substantial agreement within \nthe expected range for diagnostic performance studies24. The overall Cronbach’s alpha was 0.64. The \nvariances of individual sections of the structured reflection rubric are shown in Supplementary 6, eTable \n6. After removing Final Diagnosis, which had the highest variance, the Cronbach’s alpha was 0.67.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nDISCUSSION \nThis randomized clinical vignette study found that physician use of a commercially available LLM \nchatbot did not improve diagnostic reasoning on challenging clinical cases, despite the LLM alone \noutperforming human participants. The results were similar across the important subgroups of different \ntraining levels and experience with the chatbot. Since the task in this study is similar to how physicians \noften structure their clinical assessments and plans, these results suggest that providing access to GPT-4 \nalone may not improve overall diagnostic reasoning in clinical practice. These findings are particularly \nrelevant now that many health systems offer HIPAA-compliant chatbots that physicians can use for \nclinical care25. \n \nEven though we did not find a meaningful difference in diagnostic reasoning overall with access to GPT-\n4, the LLM may improve physician performance in certain areas of clinical reasoning. The average time \nspent on cases for those randomized to the GPT-4 arm was almost a minute less per case and over two \nminutes less per case for the subgroup who reported occasional or frequent use of the chatbot. Given the \nwide variability in time to complete cases, the results for time spent per case did not reach statistical \nsignificance despite suggesting a potentially relevant difference. Final diagnosis accuracy also potentially \nincluded a meaningful benefit, but this outcome did not reach statistical significance either. \n \nIf confirmed with additional studies, improvement in diagnostic efficiency and final diagnosis accuracy \nmay be enough to justify the use of LLM chatbots in clinical practice given the time-constrained nature of \nclinical medicine and the need to address the long-term challenge of diagnostic error26. An important \nbarrier to the use of clinical decision support systems in medicine is the integration into clinical \nworkflows without increasing physician workload and time spent in the electronic health record; if LLMs \nare able to increase efficiency without sacrificing performance, then they may prove well worth the cost \nto securely house the models and train physicians in their clinical application. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nA surprising result of a secondary analysis was that the LLM alone performed significantly better on \ndiagnostic challenges than both groups of humans, which is consistent with a prior study27. These results \nshould not be interpreted to mean that LLMs should be used for diagnosis without physician oversight. \nOur study and others were performed using clinical case vignettes that were curated and summarized by \nhuman clinicians with specific and answerable diagnostic questions in mind and do not reflect the full \nambiguity in patient care settings. These vignette-style cases address an important, but specific \ncomponent of diagnostic reasoning – the ability to extract both relevant and exculpatory information from \ncase vignettes with relatively little “noise”. While early studies show that LLMs might effectively collect \nand summarize patient information, these capabilities will need to be explored more thoroughly27,28.  \n \nThe difference between the performance of the LLM alone and that of the clinicians provided with access \nto the LLM highlights important opportunities for research on enhanced human-clinician collaboration. \nFor one, studies have demonstrated that the accuracy of LLM output is sensitive to the formulation of \nprompts, and therefore prompt engineering by the study team could explain the superior performance of \nGPT-4 alone compared to the study participants29. Training clinicians in best prompting practices may \nimprove physician performance with LLMs. Alternatively, predefined “prompting for diagnostic decision \nsupport” might be optimized as a system service for physicians.  Second, we note the rich design space \nfor exploring and enhancing clinician-AI interaction, including gaining better understandings of how and \nwhen to display AI inferences to physicians30. Our results highlight the potential for improving the \ndiagnostic performance of physicians through innovation with integrating AI capabilities into clinical \nworkflows. More generally, we see opportunity with deliberate consideration and redesign of medical \neducation and practice frameworks that enable the best use of computer and human resources to deliver \noptimal medical care.  \n \nOur study also developed and validated a measure, structured reflection, inspired by studies of physician \ncognition. This assessment tool demonstrated substantial agreement between graders and internal \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nreliability similar or superior to other measures used in the assessment of reasoning31,32,33,34. Early \nresearch focused on benchmarks with limited clinical utility, such as multiple-choice question banks used \nfor medical licensing; or curated case vignettes of diseases rarely seen in clinical practice such as New \nEngland Journal of Medicine clinicopathological case conferences11,35. While having obvious advantages \nin ease of measurement, these tasks are not consistent with clinical reasoning in practice. We must \nunderstand how AI affects reasoning for implementation purposes, rather than merely demonstrating an \nimprovement in multiple choice answering or diagnoses rarely encountered in clinical practice. As AI \nsystems become more advanced and autonomous, we must urgently ensure their alignment with human \nneeds and thought processes.  \n \nLimitations: \nWe focused our investigation around a single LLM, GPT-4, given its commercial availability and \nintegration into clinical practice25. Multiple alternative LLM systems are rapidly emerging, though GPT-4 \ncurrently remains amongst the most performant for the applications studied36,37. Participants were given \naccess to the GPT-4 chatbot without explicit training in prompt engineering techniques that could have \nimproved the quality of their interactions with the system, however this is consistent with many current \nintegrations25. Our cohort was a convenience sample from multiple major academic centers, so our results \nmay not be representative of the broader population of practicing physicians. Our study included six cases \nthat were deliberately selected to ensure a broad and relevant selection of medicine cases, but any sample \ncould never cover the full variety of cases to represent the field of medicine. Our approach and total \nnumber of cases is nonetheless consistent with established human criterion-based assessments, including \nnational licensing assessments in which students completed 12 cases over 8 hours38,39,40,41,42. Given the \ninternal reliability of our assessment, there is no evidence to suggest that additional case sampling would \nmeaningfully alter the overall results of this study.  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nCONCLUSION: \nDespite GPT-4 alone significantly outscoring human physicians on a complex diagnostic reasoning \nclinical vignette study, the availability of GPT-4 as a diagnostic aid did not improve physician \nperformance compared to conventional resources. While the use of a large language model may improve \nthe correctness of final diagnosis and efficiency of diagnostic reasoning, further development is needed to \neffectively integrate AI into emerging clinical decision support systems to exploit their potential for \nimproving medical diagnosis in practice. \n \nData Availability \nExample case vignettes, questions, and grading rubrics are included in the supplement. GPT-4 transcript \nchat logs, raw score table, and individual survey responses are available upon request.  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nCONTRIBUTIONS \nEthan Goh (co-first author) - Study design, data acquisition, data interpretation, manuscript preparation \nRobert Gallo (co-first author) - Study design, data acquisition, data interpretation, manuscript preparation \nJason Hom - Study design, data acquisition, data interpretation \nEric Strong Study design, data acquisition, data interpretation \nYingjie Weng - Study design, data interpretation        \nHannah Kerman - Study design, data interpretation      \nJosephine Cool - Study design, data interpretation      \nZahir Kanjee - Study design, data interpretation      \nAndrew Parsons - Study design, data interpretation      \nDaniel Yang - Study design, data interpretation      \nArnold Milstein - Funding and administrative support \nNeera Ahuja - Funding and administrative support \nEric Horvitz  - Study design, data interpretation      \nAndrew Olson (co-last author) - Study design, data analysis, data interpretation, critical revision, \nsupervision \nAdam Rodman (co-last author) - Study design, data analysis, data interpretation, critical revision, \nsupervision \nJonathan Chen (co-last author) - Study design, data analysis, data interpretation, critical revision, \nsupervision, funding and administrative support \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nAFFILIATIONS, DISCLOSURES AND FUNDING \nEthan Goh, MD, MS \nAffiliations \n● Stanford Center for Biomedical Informatics Research, Stanford University, Stanford, California, \nUSA \n● Stanford Clinical Excellence Research Center, Stanford University, Stanford, California, USA \nDisclosures \n● None \nFunding \n● Gordon and Betty Moore Foundation \nRobert Gallo, MD \nAffiliations \n● Center for Innovation to Implementation, VA Palo Alto Health Care System \nDisclosures \n● None \nFunding \n● Dr. Gallo is supported by a VA Advanced Fellowship in Medical Informatics. The views \nexpressed are those of the authors and not necessarily those of the Department of Veterans \nAffairs or those of the United States government. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nJason Hom, MD \nAffiliations \n● Stanford University School of Medicine \nDisclosures \n● None \nFunding \n● Gordon and Betty Moore Foundation (Grant #12409) \n \nEric Strong, MD \nAffiliations \n● Stanford University School of Medicine \nDisclosures \n● None \nFunding \n● Gordon and Betty Moore Foundation (Grant #12409) \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nYingjie Weng, MHS   \nAffiliations:  \n● Quantitative Sciences Unit, Stanford University School of Medicine, Palo Alto, CA \nDisclosures \n● None \nFunding \n● None \nHannah Kerman, MD \nAffiliations \n● Beth Israel Deaconess Medical Center, Boston, MA \n● Harvard Medical School, Boston, MA \nDisclosures \n● None \nFunding \n● None \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nJosephine Cool, MD \nAffiliations \n● Beth Israel Deaconess Medical Center, Boston, MA \n● Harvard Medical School, Boston, MA \nDisclosures \n● None \nFunding \n● Gordon and Betty Moore Foundation \n \nZahir Kanjee, MD, MPH \nAffiliations \n● Beth Israel Deaconess Medical Center, Boston, MA \n● Harvard Medical School, Boston, MA \nDisclosures \n● Royalties from Wolters Kluwer for books edited (unrelated to this study), former paid advisory \nmember for Wolters Kluwer on medical education products (unrelated to this study), honoraria \nfrom Oakstone Publishing for CME delivered (unrelated to this study) \nFunding \n● Gordon and Betty Moore Foundation  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nAndrew S. Parsons, MD, MPH \nAffiliations \n● University of Virginia School of Medicine \nDisclosures \n● Paid advisory role for New England Journal of Medicine (NEJM) Group and National Board of \nMedical Examiners (NBME) for medical education products (unrelated to this study) \nFunding \n● None \n \nNeera Ahuja, MD \nAffiliations \n● Stanford University School of Medicine \nDisclosures \n● None \nFunding \n● None \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nEric Horvitz, PhD, MD \nAffiliations \n● Microsoft \n● Stanford HAI \nDisclosures \n● None \nFunding \n● None \nAndrew P.J. Olson, MD \nAffiliations \n• University of Minnesota Medical School, Minneapolis, Minnesota \n• Division of Hospital Medicine, Department of Medicine \n• Division of Pediatric Hospital Medicine, Department of Pediatrics \nDisclosures \n• Dr. Olson receives funding from 3M for research related to rural health workforce \nshortages. Dr. Olson receives consulting fees for work related to a clinical reasoning \napplication from the New England Journal of Medicine.  \nFunding \n● Gordon and Betty Moore Foundation \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nAdam Rodman, MD, MPH \nAffiliations \n● Beth Israel Deaconess Medical Center, Boston, MA. \n● Harvard Medical School, Boston, MA \nDisclosures \n● None  \nFunding \n● Gordon and Betty Moore Foundation \n \nDaniel Yang, MD \nAffiliations \n●  Kaiser Permanente, Oakland, CA \nDisclosures \n●  None \nFunding \n●  None \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nArnold Milstein, MD \nAffiliations \n● Stanford Clinical Excellence Research Center, Stanford University, Stanford, California, USA \nDisclosures \n● Dr Milstein reported uncompensated and compensated relationships with care.coach, Emsana \nHealth, Embold Health, EZPT, FN Advisors, Intermountain Healthcare, JRSL, The Leapfrog \nGroup, Peterson Center on Healthcare, Prealize Health, PBGH \nFunding \n● Pooled philanthropic gifts to Stanford University \n● Research funding from Stanford Healthcare and Stanford Children's Health \nJonathan H. Chen \nAffiliations \n● Stanford Center for Biomedical Informatics Research, Stanford University, Stanford, California, \nUSA \n● Division of Hospital Medicine, Stanford University, Stanford, California, USA \n● Stanford Clinical Excellence Research Center, Stanford University, Stanford, California, USA \nDisclosures \n● Co-founder of Reaction Explorer LLC that develops and licenses organic chemistry education \nsoftware. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \n● Paid consulting fees from Sutton Pierce, Younker Hyde MacFarlane, and Sykes McAllister as a \nmedical expert witness. \nFunding \n● NIH/National Institute of Allergy and Infectious Diseases (1R01AI17812101) \n● NIH/National Institute on Drug Abuse Clinical Trials Network (UG1DA015815 - CTN-0136) \n● Gordon and Betty Moore Foundation (Grant #12409) \n● Stanford Artificial Intelligence in Medicine and Imaging - Human-Centered Artificial Intelligence \n(AIMI-HAI) Partnership Grant \n● Doris Duke Charitable Foundation - Covid-19 Fund to Retain Clinical Scientists (20211260) \n● Google, Inc. Research collaboration Co-I to leverage EHR data to predict a range of clinical \noutcomes. \n● American Heart Association - Strategically Focused Research Network - Diversity in Clinical \nTrials \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \nREFERENCES \n1. Shojania KG, Burton EC, McDonald KM, Goldman L. Changes in rates of autopsy-detected \ndiagnostic errors over time: a systematic review. JAMA. 2003;289(21):2849-2856. \ndoi:10.1001/jama.289.21.2849 \n2. Singh H, Giardina TD, Meyer AND, Forjuoh SN, Reis MD, Thomas EJ. Types and origins of \ndiagnostic errors in primary care settings. JAMA Intern Med. 2013;173(6):418-425. \ndoi:10.1001/jamainternmed.2013.2777 \n3. Auerbach AD, Lee TM, Hubbard CC, et al. Diagnostic Errors in Hospitalized Adults Who Died or \nWere Transferred to Intensive Care. JAMA Intern Med. 2024;184(2):164-173. \ndoi:10.1001/jamainternmed.2023.7347 \n4. Balogh EP, Miller BT, Ball JR. Improving Diagnosis in Health Care. Improving Diagnosis in \nHealth Care. Published online January 29, 2015:1-472. doi:10.17226/21794 \n5. Newman-Toker DE, Peterson SM, Badihian S, et al. Diagnostic Errors in the Emergency \nDepartment: A Systematic Review. Published online December 15, 2022. \ndoi:10.23970/AHRQEPCCER258 \n6. Newman-Toker DE, Nassery N, Schaffer AC, et al. Burden of serious harms from diagnostic error \nin the USA. BMJ Qual Saf. 2024;33(2):109-120. doi:10.1136/BMJQS-2021-014130 \n7. Ilgen JS, Bowen JL, McIntyre LA, et al. Comparing diagnostic performance and the utility of \nclinical vignette-based assessment under testing conditions designed to encourage either automatic \nor analytic thought. Acad Med. 2013;88(10):1545-1551. doi:10.1097/ACM.0b013e3182a31c1e \n8. Mamede S, van Gog T, van den Berge K, et al. Effect of availability bias and reflective reasoning \non diagnostic accuracy among internal medicine residents. JAMA. 2010;304(11):1198-1203. \ndoi:10.1001/jama.2010.1276 \n9. Goh E, Bunning B, Khoong E, et al. ChatGPT Influence on Medical Decision-Making, Bias, and \nEquity: A Randomized Study of Clinicians Evaluating Clinical Vignettes. Published online 2023. \ndoi:10.1101/2023.11.24.23298844 \n10. Savage T, Nayak A, Gallo R, Rangan E, Chen JH. Diagnostic reasoning prompts reveal the \npotential for large language model interpretability in medicine. NPJ Digit Med. 2024;7(1):20. \ndoi:10.1038/s41746-024-01010-1 \n11. Kanjee Z, Crowe B, Rodman A. Accuracy of a Generative Artificial Intelligence Model in a \nComplex Diagnostic Challenge. JAMA. 2023;330(1):78-80. doi:10.1001/jama.2023.8288 \n12. Tu T, Palepu A, Schaekermann M, et al. Towards Conversational Diagnostic AI. Published online \nJanuary 11, 2024. Accessed February 19, 2024. https://arxiv.org/abs/2401.05654v1 \n13. Ayers JW, Poliak A, Dredze M, et al. Comparing Physician and Artificial Intelligence Chatbot \nResponses to Patient Questions Posted to a Public Social Media Forum. JAMA Intern Med. \n2023;183(6):589-596. doi:10.1001/jamainternmed.2023.1838 \n14. Strong E, DiGiammarino A, Weng Y, et al. Chatbot vs Medical Student Performance on Free-\nResponse Clinical Reasoning Examinations. JAMA Intern Med. 2023;183(9):1028-1030. \ndoi:10.1001/jamainternmed.2023.2909 \n15. Rao A, Pang M, Kim J, et al. Assessing the Utility of ChatGPT Throughout the Entire Clinical \nWorkflow: Development and Usability Study. J Med Internet Res. 2023;25:e48659. \ndoi:10.2196/48659 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \n16. Gottlieb S, Silvis L. How to Safely Integrate Large Language Models Into Health Care. JAMA \nHealth Forum. 2023;4(9):e233909. doi:10.1001/jamahealthforum.2023.3909 \n17. Omiye JA, Gui H, Rezaei SJ, Zou J, Daneshjou R. Large Language Models in Medicine: The \nPotentials and Pitfalls : A Narrative Review. Ann Intern Med. 2024;177(2):210-220. \ndoi:10.7326/M23-2772 \n18. Mamede S, Schmidt HG. Deliberate reflection and clinical reasoning: Founding ideas and \nempirical findings. Med Educ. 2023;57(1):76-85. doi:10.1111/MEDU.14863 \n19. Berner ES, Webster GD, Shugerman AA, et al. Performance of four computer-based diagnostic \nsystems. N Engl J Med. 1994;330(25):1792-1796. doi:10.1056/NEJM199406233302506 \n20. Daniel M, Rencic J, Durning SJ, et al. Clinical Reasoning Assessment Methods: A Scoping \nReview and Practical Guidance. Acad Med. 2019;94(6):902-912. \ndoi:10.1097/ACM.0000000000002618 \n21. Mamede S, Schmidt HG. Correlates of reflective practice in medicine. Adv Health Sci Educ \nTheory Pract. 2005;10(4):327-337. doi:10.1007/s10459-005-5066-2 \n22. Mamede S, van Gog T, Moura AS, et al. Reflection as a strategy to foster medical students’ \nacquisition of diagnostic competence. Med Educ. 2012;46(5):464-472. doi:10.1111/J.1365-\n2923.2012.04217.X \n23. Meskó B. Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial. \nJ Med Internet Res. 2023;25:e50638. doi:10.2196/50638 \n24. McHugh ML. Interrater reliability: the kappa statistic. Biochem Med (Zagreb). 2012;22(3):276-\n282. \n25. Nigam Shah and partners roll out beta version of Stanford medicine SHC and SoM Secure GPT – \nStanford – Department of Biomedical Data Science. Accessed February 19, 2024. \nhttps://dbds.stanford.edu/2024/nigam-shaw-and-partners-roll-out-beta-version-of-stanford-\nmedicine-shc-and-som-secure-gpt/ \n26. Mamykina L, Vawdrey DK, Hripcsak G. How Do Residents Spend Their Shift Time? A Time and \nMotion Study With a Particular Focus on the Use of Computers. Acad Med. 2016;91(6):827-832. \ndoi:10.1097/ACM.0000000000001148 \n27. McDuff D, Schaekermann M, Tu T, et al. Towards Accurate Differential Diagnosis with Large \nLanguage Models. Published online November 30, 2023. Accessed February 19, 2024. \nhttps://arxiv.org/abs/2312.00164v1 \n28. Tierney AA, Gayre G, Hoberman B, et al. Ambient Artificial Intelligence Scribes to Alleviate the \nBurden of Clinical Documentation. Published online 2024. doi:10.1056/CAT.23.0404 \n29. Nori H, Lee YT, Zhang S, et al. Can Generalist Foundation Models Outcompete Special-Purpose \nTuning? Case Study in Medicine. Published online November 28, 2023. Accessed March 6, 2024. \nhttps://arxiv.org/abs/2311.16452v1 \n30. Fogliato R, Chappidi S, Lungren M, et al. Who Goes First? Influences of Human-AI Workflow on \nDecision Making in Clinical Imaging. ACM International Conference Proceeding Series. \n2022;22:1362-1374. doi:10.1145/3531146.3533193 \n31. Staal J, Hooftman J, Gunput STG, et al. Effect on diagnostic accuracy of cognitive reasoning tools \nfor the workplace setting: systematic review and meta-analysis. BMJ Qual Saf. 2022;31(12):899-\n910. doi:10.1136/BMJQS-2022-014865 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint \n \n \n32. Schaye V, Miller L, Kudlowitz D, et al. Development of a Clinical Reasoning Documentation \nAssessment Tool for Resident and Fellow Admission Notes: a Shared Mental Model for Feedback. \nJ Gen Intern Med. 2022;37(3):507-512. doi:10.1007/S11606-021-06805-6 \n33. Omega A, Wijaya Ramlan AA, Soenarto RF, Heriwardito A, Sugiarto A. Assessing clinical \nreasoning in airway related cases among anesthesiology fellow residents using Script Concordance \nTest (SCT). Med Educ Online. 2022;27(1):2135421. doi:10.1080/10872981.2022.2135421 \n34. Groves M, Dick ML, McColl G, Bilszta J. Analysing clinical reasoning characteristics using a \ncombined methods approach. BMC Med Educ. 2013;13(1):1-7. doi:10.1186/1472-6920-13-\n144/TABLES/5 \n35. Nori H, King N, Mckinney SM, Carignan D, Horvitz E, Openai M 2. Capabilities of GPT-4 on \nMedical Challenge Problems. Published online March 20, 2023. Accessed February 19, 2024. \nhttps://arxiv.org/abs/2303.13375v2 \n36. Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature 2023 \n620:7972. 2023;620(7972):172-180. doi:10.1038/s41586-023-06291-2 \n37. Nori H, Lee YT, Zhang S, et al. Can Generalist Foundation Models Outcompete Special-Purpose \nTuning? Case Study in Medicine. Published online November 28, 2023. Accessed March 6, 2024. \nhttps://www.microsoft.com/en-us/research/publication/can-generalist-foundation-models-\noutcompete-special-purpose-tuning-case-study-in-medicine/ \n38. Harden RM. What is an OSCE? Med Teach. 1988;10(1):19-22. doi:10.3109/01421598809019321 \n39. Pell G, Fuller R, Homer M, Roberts T, International Association for Medical Education. How to \nmeasure the quality of the OSCE: A review of metrics - AMEE guide no. 49. Med Teach. \n2010;32(10):802-811. doi:10.3109/0142159X.2010.507716 \n40. Khan KZ, Ramachandran S, Gaunt K, Pushkar P. The Objective Structured Clinical Examination \n(OSCE): AMEE Guide No. 81. Part I: an historical and theoretical perspective. Med Teach. \n2013;35(9):e1437-46. doi:10.3109/0142159X.2013.818634 \n41. Chan SCC, Choa G, Kelly J, Maru D, Rashid MA. Implementation of virtual OSCE in health \nprofessions education: A systematic review. Med Educ. 2023;57(9):833-843. \ndoi:10.1111/medu.15089 \n42. Daniel M, Rencic J, Durning SJ, et al. Clinical Reasoning Assessment Methods: A Scoping \nReview and Practical Guidance. Acad Med. 2019;94(6):902-912. \ndoi:10.1097/ACM.0000000000002618 \n  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 14, 2024. ; https://doi.org/10.1101/2024.03.12.24303785doi: medRxiv preprint ",
  "topic": "Vignette",
  "concepts": [
    {
      "name": "Vignette",
      "score": 0.8519456386566162
    },
    {
      "name": "Medicine",
      "score": 0.6839936375617981
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.6329765319824219
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.4453486204147339
    },
    {
      "name": "MEDLINE",
      "score": 0.4347725212574005
    },
    {
      "name": "Family medicine",
      "score": 0.3907589316368103
    },
    {
      "name": "Physical therapy",
      "score": 0.36450889706611633
    },
    {
      "name": "Emergency medicine",
      "score": 0.347078800201416
    },
    {
      "name": "Internal medicine",
      "score": 0.19393393397331238
    },
    {
      "name": "Psychology",
      "score": 0.183510422706604
    },
    {
      "name": "Psychiatry",
      "score": 0.1353539228439331
    },
    {
      "name": "Social psychology",
      "score": 0.11069023609161377
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}