{
  "title": "Retrieval Augmented Code Generation and Summarization",
  "url": "https://openalex.org/W3194195934",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2476564708",
      "name": "Md. Rizwan Parvez",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2184437607",
      "name": "Wasi Ahmad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152902633",
      "name": "Saikat Chakraborty",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2166284654",
      "name": "Baishakhi Ray",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2051204868",
    "https://openalex.org/W2349629511",
    "https://openalex.org/W3106149053",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W3123811550",
    "https://openalex.org/W2971167472",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2006291142",
    "https://openalex.org/W1993786160",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2604794021",
    "https://openalex.org/W2155482025",
    "https://openalex.org/W2962995178",
    "https://openalex.org/W2402619042",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2963740933",
    "https://openalex.org/W3034976548",
    "https://openalex.org/W2963921594",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2963676655",
    "https://openalex.org/W1610356397",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2987249037",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W1484035946",
    "https://openalex.org/W2970490744",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2060055745",
    "https://openalex.org/W3034689979",
    "https://openalex.org/W2979271470",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W2031289572",
    "https://openalex.org/W3123221944",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1852777783",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2139374478",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2964268484",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W2014755981",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2889467844",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2142112143",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W2890397703"
  ],
  "abstract": "Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719–2734\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2719\nRetrieval Augmented Code Generation and Summarization\nMd Rizwan Parvez§, Wasi Uddin Ahmad§, Saikat Chakraborty†\nBaishakhi Ray†, Kai-Wei Chang§\n§University of California, Los Angeles, †Columbia University\n§{rizwan, wasiahmad, kwchang}@cs.ucla.edu, †{saikatc, rayb}@cs.columbia.edu\nAbstract\nSoftware developers write a lot of source code\nand documentation during software develop-\nment. Intrinsically, developers often recall\nparts of source code or code summaries that\nthey had written in the past while implement-\ning software or documenting them. To mimic\ndevelopers’ code or summary generation be-\nhavior, we propose a retrieval augmented\nframework, REDCODER, that retrieves rel-\nevant code or summaries from a retrieval\ndatabase and provides them as a supplement\nto code generation or summarization mod-\nels. REDCODER has a couple of uniqueness.\nFirst, it extends the state-of-the-art dense re-\ntrieval technique to search for relevant code\nor summaries. Second, it can work with re-\ntrieval databases that include unimodal (only\ncode or natural language description) or bi-\nmodal instances (code-description pairs). We\nconduct experiments and extensive analysis on\ntwo benchmark datasets of code generation\nand summarization in Java and Python, and the\npromising results endorse the effectiveness of\nour proposed retrieval augmented framework.\n1 Introduction\nIn recent years, automating source code generation\nand summarization is receiving signiﬁcant attention\ndue to its potential in increasing programmers’ pro-\nductivity and reducing developers’ tedious work-\nload. Consequently, various approaches have been\nexplored in the literature to facilitate code genera-\ntion (Yin and Neubig, 2017; Gu et al., 2016) and\ncode documentation/summarization (Ahmad et al.,\n2020; Wei et al., 2019; Allamanis et al., 2018).\nDespite initial success, most of the generated code\nstill suffers from poor code quality (Xu et al., 2021).\nTherefore, the question remains—how to generate\nbetter code from a given summary and vice versa.\nSource code generation and summarization, how-\never, are intrinsically complex and challenging.\nThey involve generating diverse token sequences\nsuch as different variables, operators, keywords,\nclasses, and method names (Parvez et al., 2018),\nwhich requires understanding the programming lan-\nguages at lexical, syntax, and semantics levels.\nTo combat these issues, recent studies ( e.g., Ah-\nmad et al. (2021); Guo et al. (2021); Xu et al.\n(2020); Feng et al. (2020a); Xu et al. (2020)) take\na learning-based approach—they train representa-\ntions of code and the associated text by leveraging\nexisting high-quality source code and short text\ndescriptions available in open-source repositories\nand question answering forums such as GitHub\nand Stack Overﬂow. Then ﬁne-tune the represen-\ntation models on the downstream tasks. Although\nthese dataset contains high-quality human-written\ncode and text, since the existing approaches do not\ndirectly leverage them during the generation pro-\ncess, the gain achieved by these approaches is still\nlimited, especially when the source code is long.\nTo overcome this, we take advantage of the ex-\nisting high-quality source code and their descrip-\ntion by including them directly in the generation\nprocess that are retrieved via information retrieval\ntechnique. In this work, we present REDCODER,\na Retrieval augment ED CODe g Eneration and\nsummaRization framework. While designing RED-\nCODER, we take motivation from how developers\ntake advantage of existing resources. For example,\ndevelopers often search for relevant code in the\ncode repository, and if found, adapt the retrieved\ncode in their own context. Similarly, when an API\nusage is unclear, they search in question answering\nforums (e.g., StackOverﬂow) (Brandt et al., 2010;\nSadowski et al., 2015). Such an additional resource\nhelps developers to increase their development pro-\nductivity (Li et al., 2013).\nWe design REDCODER as a two-step process\n(see Figure 1). In the ﬁrst step, given the input (nl\ntext for code generation, or code snippet for sum-\nmarization) a retriever module retrieves relevant\nsource code (for code generation) or summaries\n2720\nFigure 1: Illustration of our proposed framework REDCODER for code generation. Given an input summary, we\nﬁrst retrieve top-k candidate code (k=1 in this example). We then aggregate them and based on that a generator\nmodule generates the target sequence.\n(for code summarization) from a database.1 In the\nsecond step, a generator processes the retrieved\ncode/summary along with the original input to gen-\nerate the target output. In this way, REDCODER\nenhances the generation capability by augmenting\nthe input through retrieval. The two-step process\nallows us to design a modular and conﬁgurable\nframework for source code and summary gener-\nation. Various designs of retriever and generator\nmodels can be incorporated into this framework.\nExisting cross-encoder code retrievers being\ncomputationally expensive, their applicability to\nretrieve from a large database is limited (Humeau\net al., 2020). A natural choice would be to use\nsparse term based retrievers such as TF-IDF or\nBM25 (Robertson and Zaragoza, 2009). However,\nthe retriever module in REDCODER should ex-\nhibit a good understanding of source code and pro-\ngrammers’ natural language, which is a non-trivial\ntask due to the syntactic and semantic structure of\nthe source code (Guo et al., 2021; Ahmad et al.,\n2021). Such an expectation of searching for se-\nmantically similar code and summary may not be\nattainable by a sparse token level code retriever\n(e.g., BM25). To that end, we design the retriever\nmodule in REDCODER based on programming\nlanguages (PL) and natural languages (NL) under-\nstanding models (e.g., GraphCodeBERT (Guo et al.,\n2021)). This retriever module extends the state-of-\nthe-art dense retrieval technique (Karpukhin et al.,\n2020) using two different encoders for encoding\nthe query and document.\nAs for the generator, REDCODER can handle\nretrieval databases consisting of both unimodal\n(only code or natural language description) and bi-\nmodal instances (code-description pairs) and makes\nthe best usage of all the auxiliary information that\n1The database could be open source repositories\n(e.g., GitHub) or developers’ forums (e.g., Stack Overﬂow).\nFigure 2: Example input/output for the code generation\nand summarization tasks.\nare available. Yet, to incorporate information, we\naugment the retrieved information only in the in-\nput level. It does not modify the underlying archi-\ntecture of the generator module —preserving its\nmodel agnostic characteristics.\nWe evaluate the effectiveness of REDCODER\non two popular programming languages (Java and\nPython) on both code generation and code sum-\nmarization tasks. The empirical results show that,\nREDCODER’s concept ofretrieval augmented gen-\neration elevates the state-of-the-art code generation\nfrom an Exact Match score of 18.6 to 23.4 and the\nsummary generation BLEU-4 score from 18.45\nto 22.95 even when we forcefully remove the tar-\nget candidate from the retrieved code or summary.\nWith further experiments, we establish the impor-\ntance of both the retrieved code and retrieves sum-\nmary in the generation process. The source code\nfor reproducing our experiments are at https:\n//github.com/rizwan09/REDCODER.\n2 Background\nWe ﬁrst introduce the problem formulation and\ndiscuss the fundamentals of the retriever and gen-\nerator components that REDCODER is built upon.\n2.1 Problem Formulation\nOur goal is two folds: (i) code generation: Gener-\nating source code (C), given their natural language\ndescription, such as code summaries, code com-\nments or code intents (S); (ii) code summarization:\nGenerating natural language summaries S, given\nsource code snippets C. Fig 2 shows an example.\n2721\nLet X and Y denote a collection of input\nand output sequences ( X = S1, . . . , Sn, Y =\nC1, . . . , Cn in code generation, X = C1, . . . , Cn,\nY =S1, . . . , Sn in summary generation ). We as-\nsume that we have access to a retrieval database\nconsisting of an extensive collection of source code\n(e.g., aggregated from GitHub or Stack Overﬂow)\nor summaries ( e.g., docstrings, code comments)\n(YR). Note that, target sequences (Y ) may or may\nnot be present in the retrieval database (YR). Now,\ngiven an input x ∈ X, a retriever retrieves the\ntop-k relevant output sequences from the database:\nY1, Y2, . . . ,Yk ∈YR. Then the input sequence x\nis augmented with the retrieved sequences to form\nx′ = x ⊕Y1 ⊕Y2 . . .⊕Yk, where ⊕denote the\nconcatenation operation. Finally, a generator gen-\nerates the target output y ∈ Y given x′. In the\nfollowing, we ﬁrst discuss the base retriever and\ngenerator modules used in REDCODER and then\nhow we improve these components is in Section 3.\n2.2 Retriever: DPR\nInformation retrieval (IR) systems or retriever mod-\nels are designed to retrieve the top-k relevant doc-\numents that presumably best provide the desired\ninformation (Manning et al., 2008). Term-based\nretrieval methods, a.k.a. sparse retrieval models,\nsuch as TF-IDF or BM25 (Robertson and Zaragoza,\n2009) use sparse vector representations to perform\nlexical matching and compute relevance scores to\nrank the documents based on a query.\nOn the other hand, dense retrieval methods en-\ncode documents into a ﬁxed-size representations\nand retrieve documents via maximum inner prod-\nuct search (Sutskever et al., 2014; Guo et al., 2016).\nParticularly of interests, Karpukhin et al. (2020)\npropose a Dense Passage Retriever (DPR) model\nfor open-domain question answering (QA). It con-\nsists of two encoders (Q(.) and P(.)) that encode\nqueries and passages, respectively. The similarity\nof a query q and a passage p is deﬁned by the in-\nner product of their encoded vectors sim(p, q) =\nQ(q)T ⋅P(p). Given a query q, a positive (rele-\nvant) passage p+, and a set of n irrelevant passages\np−\ni , DPR optimizes the classiﬁcation loss:\nL =−log esim(q,p+)\nesim(q,p+) +∑n\ni=1 esim(q,p−\ni ).\nKarpukhin et al. (2020) propose to ﬁne-tune\nDPR using in-batch negatives (Gillick et al., 2019;\nYih et al., 2011) with curated “hard” negatives us-\nFigure 3: An example retrieved code that is relevant yet\ndoes not match the reference.\ning BM25 (candidates with high BM25 scores but\ncontain no sub-string that match the target). We\nrefer to Karpukhin et al. (2020) for details.\n2.3 Generator: PLBART\nPLBART (Ahmad et al., 2021) is a sequence-to-\nsequence Transformer model (Vaswani et al., 2017)\nthat is pre-trained on a huge collection of source\ncode and natural language descriptions via denois-\ning autoencoding. PLBART has shown promise in\nseveral software engineering applications, includ-\ning code generation and summarization. We adopt\nPLBART as the generator module in our proposed\nframework, REDCODER.\n3 Proposed Framework: REDCODER\nOur proposed code generation and summarization\nframework, REDCODER generates the target code\nor summary by augmenting the input x with rele-\nvant code snippets or summaries. We build our re-\ntriever module by training a DPR model differently\nfrom (Karpukhin et al., 2020). With an intelligent\nscheme, we then augment the retrieved candidates\nand their pairs (if available) to provide auxiliary\nsupervision to the generator. We brieﬂy describe\nthe model components in this section.\n3.1 Retriever: SCODE-R\nArchitecture The retriever module of RED-\nCODER is built upon the DPR model (Karpukhin\net al., 2020) and we call it SCODE-R (Summary\nand CODE Retriever). SCODE-R composed of two\nencoders that encode source code and natural lan-\nguage summary. We use bidirectional Transformer\nencoders (Vaswani et al., 2017) that are pre-trained\non source code and natural language summaries.\nSpeciﬁcally, we explore CodeBERT (Feng et al.,\n2020b) and GraphCodeBERT (Guo et al., 2021) as\nthe code and summary encoders for SCODE-R.\nInput/Output SCODE-R takes an input se-\nquence x (code or summary) and retrieves a set\nof relevant documents from a database of output\nsequences Y (if the input is code, then the output\n2722\nFigure 4: Training scheme of the retriever module\n(SCODE-R) of our proposed framework REDCODER\nfor the code generation task. Unlike in open-domain\nQA (Karpukhin et al., 2020), we do not use “hard” neg-\natives (e.g., candidates retrieved by BM25 that do not\nexactly match the reference) during ﬁne-tuning.\nis summary and vice versa). SCODE-R returns\nthe the top-k output sequences {Y1, Y2, . . . ,Yk},\nwhere sim(x, Yi) ≥sim(x, Yj)∀j >i.\nTraining We ﬁne-tune SCODE-R using a set of\nparallel examples (xi, yi) of code and summaries.\nAs mentioned in Section 2.2, DPR originally pro-\nposed to be ﬁne-tuned using in-batch negatives and\ncurated “hard” negatives from BM25 retrieved pas-\nsages for open-domain QA. The key idea behind\n“hard” negatives is to ﬁne-tune DPR to distinguish\nthe target passage from relevant passages that do\nnot contain the target answer. However, unlike\nopen-domain QA, a retrieved code or summary that\nis not the target could still beneﬁt code generation\nor summarization (veriﬁed in Section 6). We pro-\nvide an example in Figure 3; although the retrieved\ncode does not match the target one but can facilitate\ngenerating it. Therefore, we ﬁne-tune SCODE-R\nwithout any “hard” negatives. Speciﬁcally, for each\ntraining instance (xi, yi), the corresponding output\nyi is considered as positive and the other in-batch\noutputs (i.e., the outputs of other instances in the\nsame batch - y1, . . . , yi−1, yi+1, . . . , ybsz) as nega-\ntives. Figure 4 shows an example of SCODE-R\nﬁne-tuning for code generation task.\n3.2 Generator: SCODE-G\nWe adopt PLBART as discussed in Section 2.3 as\nthe generator module of REDCODER and call it\nSCODE-G (Summary and CODE Generator). The\ninput sequence x is concatenated with the top-k re-\nFigure 5: REDCODER-EXT input for code generation.\ntrieved sequences to form the augmented input se-\nquence, x′ =x⊕Y1 ⊕Y2 . . .⊕Yk. The augmented\ninput x′ is fed to PLBART to estimate pgen(y∣x′).\nNote that a source code often consists of doc-\nstrings, comments that can be extracted to form\ncode – summary pairs. In the retrieval databases,\ncode and summaries are either singleton (e.g., code\nwithout a description or a problem statement with-\nout any code) or parallel. Therefore, we consider\ntwo retrieval settings that require separate modeling\nconsideration for the generator.\nCase 1: Retrieve candidates are singleton In\nthis case, we concatenate the original input se-\nquence x and the top-k retrieved candidates with a\nspecial separator token.\nx′ =x [csep] Y1 [csep] Y2 . . .[csep] Yk.\nThis is our default setting and we refer this as RED-\nCODER in this work.\nCase 2: Retrieve candidates are pairsIn this\ncase, retrieved candidates are pair of code and natu-\nral language (NL) summary. We augment the input\nsequence using both of them as follows.\nx′ =x [csep] Y1 [nsep] X1 [csep] Y2\n[nsep] X2 . . .[csep] Yk [nsep] Xk,\nwhere Xj and Yj are parallel sequences (e.g., Yj\nis a piece of code and Xj is its corresponding sum-\nmary for the code generation task) retrieved from\nthe database. We conjecture that the additional in-\nformation Xj complements the input sequence x\nand verify its effectiveness in the experiments.\nNote that retrieve candidates could be a mix of\nsingleton and pairs. In case of a singleton candi-\ndate, we simply replace Xj or Yj with an empty\nstring. We refer this setting as REDCODER-EXT.\nAlthough, REDCODER-EXT is a more general\nsetting which includes “Case 1”, we study them\nseparately to understand how these two retrieval\nsettings beneﬁt the target tasks. We illustrate an\nexample on code generation in Figure 5. In both\n2723\nDataset Gen. Sum. Lang. Train Valid Test ∣Code∣ ∣ Summary∣\nCodeXGLUE \u0013 \u0013 Java 164,923 5,183 10,955 97 12\n(Lu et al., 2021) Python 251,820 13,914 14,918 99 14\nConcode (Iyer et al., 2018) \u0013 \u0017 Java 100,000 2,000 2,000 27 72\nTable 1: Dataset Statistics. Gen., and Sum. refers to code generation and summarization tasks respectively. Sum-\nmary denotes a natural language description paired with each code. For Concode, the input summary includes the\ncorresponding environment variables and methods. All lengths are computed and averaged before tokenization.\ncases, the augmented input x′ is truncated to match\nPLBART’s maximum input length 512.\n4 Experiment Setup\nIn order to investigate the effectiveness of our\nframework, we perform a comprehensive study and\nanalysis on code generation and summarization in\ntwo programming languages, Java and Python.\n4.1 Datasets and Implementations\nDatasets We perform evaluation on both the\ntasks using the code summarization dataset from\nCodeXGLUE (Lu et al., 2021). It is curated from\nCodeSearchNet (Husain et al., 2019) by ﬁltering\nnoisy examples. In addition, we conduct code\ngeneration experiments in Java using the Concode\nbenchmark (Iyer et al., 2018). The dataset statistics\nare summarized in Table 1.\nRetrieval Databases To generate a source code\ngiven its natural language description or a sum-\nmary given the code, our proposed approach RED-\nCODER ﬁrst retrieves prospective candidates from\nan existing code or summary database. We form\nthe code retrieval database using the deduplicated\nsource code (on average 1.4M functions in Java\nand Python) that consists of both paired (59%)\nand monolingual code, released in CodeSearch-\nNET (Husain et al., 2019). As for building the\nsummary retrieval database, we extract the high\nquality natural language summaries from the paired\ninstances in the training sets of CodeSearchNET.\nAs many of the summaries are duplicated, we also\nconsider the training sets in the other four avail-\nable languages Ruby, Javascript, Go, and PHP.\nWe then further enlarge it by aggregating the ad-\nditional summaries from the CCSD corpus (Liu\net al., 2021). After performing deduplication, we\nretain 1.1M unique code summaries and for evalu-\nating REDCODER-EXT, 20% of them can be used\nas pairs with the corresponding Java and Python\nsource code. We provide the statistics of the re-\ntrieval databases in Appendix. Note that the re-\ntrieval databases contain code and summaries that\nare curated from real developers’ open sourced\nrepositories on GitHub. By default, we exclude the\ntarget code/summary from the retrieval database.\nImplementations As mentioned in Section 3,\nREDCODER has two disjoint components. First,\nthe dense retriever SCODE-R is implemented\nadopting DPR (Karpukhin et al., 2020) and the\nencoders in DPR are initialized from GrpahCode-\nBERT available in the Huggingface API (Wolf\net al., 2020). In addition, we implement a baseline\nBM25 retriever. We use the ofﬁcial codebase of\nPLBART (Ahmad et al., 2021) and set max epoch\nto 15, patience to 5, learning rate to 2 ×10−5. We\ntune the batch size in {8, 16, 32, 64, 72} and the\nk value for top-k retrieval up to 10 for code gen-\neration and in range {10, 30, 50, 100} for code\nsummarization. As some candidate code and sum-\nmaries are short in length, we tune with this upper\nbound of k to accommodate as many candidates as\npossible within PLBART’s maximum input length.\n4.2 Evaluation Metrics\nBLEU Following prior works (Ahmad et al.,\n2021; Feng et al., 2020a), we compute the cor-\npus level BLEU (Papineni et al., 2002) and the\nsmoothed BLEU-4 (Lin and Och, 2004) scores for\ncode generation and summarization tasks.\nCodeBLEU To demonstrate syntactic and seman-\ntic data ﬂow correctness of code generation models,\nwe report CodeBLEU (Ren et al., 2020). Code-\nBLEU is a weighted average of lexical, abstract\nsyntax tree, and data ﬂow match.\nExact Match (EM) indicates the percentage of\noutput sequences that exactly match the references.\n4.3 Baseline Methods\nWe compare REDCODER w.r.t.a number of state-\nof-the-art code models. We classify them into two\ncategories: (i) retrieval based models and (ii) gen-\nerative models. We study both generative models\nthat are trained from scratch and are pre-trained on\nprogramming and natural languages.\n2724\nMethod Java Python\nType Name EM BLEU CodeBLEU EM BLEU CodeBLEU\nRetrieval BM25 0.00 4.90 16.00 0.00 6.63 13.49\nBased SCODE-R 0.00 25.34 26.68 0.00 22.75 23.92\nGenerative\nCodeBERT 0.00 8.38 14.52 0.00 4.06 10.42\nGraphCodeBERT 0.00 7.86 14.53 0.00 3.97 10.55\nCodeGPT-adapted 0.00 7.10 14.90 0.01 3.11 11.31\nPLBART 0.00 10.10 14.96 0.00 4.89 12.01\nRetrieval BM25 + PLBART 0.10 11.37 15.52 0.03 6.99 13.89\nAugmented REDCODER 8.95 26.92 31.15 8.88 22.74 28.93\nGenerative REDCODER-EXT 10.21 28.98 33.18 9.61 24.43 30.21\nTable 2: Results on code generation on CodeXGLUE (Lu et al., 2021).\nMethods EM BLEU CodeBLEU\nRetrieval based methods\nBM25 0.0 20.3 23.7\nSCODE-R 0.0 32.6 36.5\nGenerative methods\nSeq2Seq 3.1 21.3 26.4\nGuo et al. (2019) 10.1 24.4 29.5\nIyer et al. (2019) 12.2 26.6 -\nGPT-2 17.4 25.4 29.7\nCodeGPT-2 18.3 28.7 32.7\nCodeGPT-adapted 20.1 32.8 36.0\nCodeBERT 18.0 28.7 31.4\nGraphCodeBERT 18.7 33.4 35.9\nPLBART 18.6 36.7 38.5\nRetrieval augmented generative methods\nBM25+PLBART 21.4 40.2 41.8\nREDCODER 23.4 41.6 43.4\nREDCODER-EXT 23.3 42.5 43.4\nTable 3: Code generation results on Concode dataset.\nSCODE-R was initialized with CodeBERT. Graph-\nCodeBERT initialized results are similar.\nRetrieval based models We examine two re-\ntriever baselines and consider the top-1 retrieved\ncandidate as the prediction.\n•Dense Retriever We consider DPR as the dense\nretriever baseline. We evaluate both the ofﬁcially\nreleased models trained on the natural language\nopen-domain QA task and a variant called DPR\n(code) that we ﬁne-tune on the evaluation datasets.\n• Sparse Retriever The second baseline is a\nsparse retriever that uses the BM25 algorithm to\ncompute relevance scores.\nGenerative models The generative models work\nin a sequence-to-sequence (Seq2Seq) fashion.\n•RoBERTa, RoBERTa (code)RoBERTa mod-\nels (Liu et al., 2019) pre-trained on natural lan-\nguage corpora, and source code from CodeSearch-\nNet (Husain et al., 2019) respectively.\nMethods Python Java\nRetrieval based methods\nBM25 1.92 1.82\nSCODE-R 14.98 15.87\nGenerative methods\nSeq2Seq 15.93 15.09\nTransformer 15.81 16.26\nRoBERTa 18.14 16.47\nCodeBERT 19.06 17.65\nGraphCodeBERT 17.98 17.85\nPLBART 19.30 18.45\nRetrieval augmented generative methods\nBM25 + PLBART 19.57 19.71\nREDCODER 21.01 22.94\nREDCODER-EXT 20.91 22.95\nTable 4: Evaluation BLEU-4 score for code summa-\nrization on CodeXGLUE. Baseline results are reported\nfrom Ahmad et al. (2021).\n•CodeBERT (Feng et al., 2020a) is pretrained\nwith a hybrid objective incorporating masked lan-\nguage modeling (Devlin et al., 2019) and replaced\ntoken detection (Clark et al., 2020).\n•GraphCodeBERT (Guo et al., 2021) is pre-\ntrained by modeling the data ﬂow graph of source\ncode. GraphCodeBERT holds the state-of-the-art\nresults on code search using CodeSearchNet.\n• GPT-2, CodeGPT-2, and CodeGPT-adapted\nare GPT-style models that are pre-trained on natural\nlanguage (Radford et al., 2019) and code corpora\nCodeXGLUE (Lu et al., 2021).\n•PLBART (Ahmad et al., 2021) is the generator\nmodule of our proposed framework.\nIn addition, we train an LSTM based Seq2Seq\nmodel with attention mechanism (Luong et al.,\n2015) and a Transformer model (Vaswani et al.,\n2017) on the benchmark datasets.\n2725\nMethods CodeXGLUE (Java) CodeXGLUE (Python) Concode (Java)\nBLEU EM CodeBLEU BLEU EM CodeBLEU BLEU EM CodeBLEU\nSCODE-R 36.6 21.0 37.9 35.6 19.2 35.1 70.3 61.7 72.0\nREDCODER 36.3 29.4 41.4 32.1 27.5 38.0 76.7 67.5 76.5\nREDCODER-EXT 42.8 37.0 47.3 38.9 34.5 43.8 81.7 76.2 81.7\nTable 5: Results on code generation keeping the target code in the retrieval database.\nSettings Methods Python Java\nRoBERTa 0.587 0.599\nCross- RoBERTa (code) 0.610 0.620\nEncoder CodeBERT 0.672 0.676\nGraphCodeBERT 0.692 0.691\nBi- DPR 0.093 0.064\nDPR (code) 0.398 0.462Encoder SCODE-R 0.690 0.686\nTable 6: MRR results on code retrieval from the val-\nidation and test set in CodeXGLUE. Our bi-encoder\nretriever SCODE-R is comparable with other cross-\nencoder models while it is much faster. DPR refers to\nKarpukhin et al. (2020) and DPR (code) is trained with\nBM25 “hard” negative training schema built upon our\nsource code datasets.\n5 Results\n5.1 Code Generation\nTable 2 and Table 3 show the evaluation results\non code generation from summary descriptions\non CodeXGLUE, and Concode datasets, respec-\ntively. First, we compare REDCODER with the\nstate-of-the-art code generation models. They\nare transformers models pre-trained with differ-\nent objectives using external resources of differ-\nent sizes. Among them, the relatively strong base-\nline PLBART has an EM score of 18 on the Con-\ncode dataset while it rarely generates any code that\nmatches the real target code in CodeXGLUE (See\nTable 2) (more discussion on this is in Appendix).\nThe BLEU and CodeBLEU scores are also low.\nSuch result indicates that automated code lacks\nquality and correctness without the proper supervi-\nsion in the input to the generator.\nAmong the retriever-only models, SCODE-R\nsigniﬁcantly outperforms BM25 (more comparison\nis in § 6). As expected, the EM is zero as targets are\nﬁltered from the retrieval, and CodeBLEU scores\nare high as they are real code. However, although\nthe retrieved code does not exactly match the target\ncode, they are quite relevant (e.g., Figure 3; more in\nAppendix). When comparing retrieval-only models\nto generative models, it is interesting to note that\nSCODE-R surpasses PLBART by a large margin on\nCodeXGLUE (Table 2), suggesting that retrieved\ncode has high overlapping with target code that can\nbeneﬁt the generation.\nOverall, the retrieval augmented generative mod-\nels excel in code generation. Our proposed frame-\nwork REDCODER outperforms PLBART by a\nlarge margin, validating the advantage of reusing\nexisting codebases to help code generation. The\nREDCODER-EXT gains are even higher. For\nCodeXGLUE (Java, Python) and Concode, the\ngains in BLEU are 18.88, 19.54, and 5.8. Com-\nparing REDCODER to REDCODER-EXT shows\nthat BLEU scores on Concode and all metrics on\nCodeXGLUE are improved by ∼1%. These results\nconﬁrm our conjecture that complementing input\nwith paired summaries of the retrieved code help\ncode generation. We provide a qualitative exam-\nple in the Appendix to explain how the retrieved\ninformation helps PLBART in generation.\n5.2 Code Summarization\nWe compare REDCODER with three sets of base-\nline methods for code summarization, and Table 4\nshows the results. Among the two retrieval base\nmethods, SCODE-R performs signiﬁcantly well,\nconﬁrming the advantages of dense retrieval over\nits sparse counterpart. Out of the generative meth-\nods, PLBART excels on code summarization as\nit leverages an extensive collection of natural lan-\nguage descriptions during pre-training. As antici-\npated, retrieval augmented generative methods out-\nperform the other two sets of models. We see\nthat the “BM25 + PLBART” model improves over\nPLBART, conﬁrming our conjecture that retrieval\naugmented techniques have the promise to improve\ncode summarization. Our proposed framework\nREDCODER and its variant REDCODER-EXT\noutshine “BM25 + PLBART”, surpassing its per-\nformance by ∼1.5 and ∼3.2 points for Python and\nJava languages, respectively.\n6 Analysis\nIn this Section, we analyze REDCODER’s perfor-\nmance on the following points.\n2726\nFigure 6: Recall@K for CodeR and BM25. CodeR\nrefers to SCODE-R used for source code retrieval.\nRetrieval database includes the target sequence\nTable 5 shows the code generation results when we\ndid not ﬁlter the target from the retrieval (summa-\nrization results are in Appendix). As expected,\nSCODE-R performances are much better than\nthose in Table 2, 3, and 4. In all cases, RED-\nCODER gets more enhanced when target is present\nin the retrieval database. For the code generation\ntask, we plot the recall@k curve for k upto 10\nfor both Java and Python on CodeXGLUE dataset\nwhen the retrieval contains the target in Figure\n6. As we can see, SCODE-R signiﬁcantly out-\nperforms in both languages and for all k values.\nBi-encoder SCODE-R vs cross-encoder retriev-\ners Table 6 shows the retrieval performance of\ndifferent alternative retrieval techniques that we\nconsidered in REDCODER. SCODE-R performs\ncomparably well with GraphCodeBERT while be-\ning signiﬁcantly faster and scalable (Humeau et al.,\n2020). Note that, SCODE-R also uses Graph-\nCodeBERT to initialize its encoders (see Figure 4).\nHowever, SCODE-R’s design of using different\nencoders for query and documents enables pre-\nindexing of database and faster retrieval in practice.\nPerformance vs target length Figure 7 shows\nthe code generation performances of different mod-\nels w.r.t.the target code length for Python. While\nthe generator model (PLBART)’s performance\nconsistently decreases with increasing code size,\nthe retriever (SCODE-R) performs consistently\nwell. Such consistent performance from SCODE-\nR boosts performance of REDCODER (and also\nREDCODER-EXT) signiﬁcantly higher than the\ngenerative model counterpart. For Java, we ﬁnd\nsimilar results (details in Appendix).\nPerformance vs #retrievalsFigure 8 shows that\ntypically the performance improves more with\nmore retrievals on both tasks. However, roughly 5\nFigure 7: (Python) Code gen. BLEU vs target len.\nCodeXGLUE (Java) gen.\n CodeXGLUE (Python) gen.\nCodeXGLUE (Java) sum.\n CodeXGLUE (Python) sum.\nFigure 8: Code gen. and sum. performance vs #re-\ntrievals. In general performance improves with higher\nnumber of augmented candidates.\ncode and 30 summaries work sufﬁciently well.\nHuman evaluation Finally, we evaluate the qual-\nity of code generated by SCODE-G using human\nevaluation. In Table 7, we perform a human eval-\nuation for code generation task on a subset of the\ntest set in CodeXGLUE (Python). In this study,\nwe compare REDCODER generated code with the\ncode retrieved by SCODE-R. Note that both RED-\nCODER and SCODE-R using the same retrievers,\nbut REDCODER generates code using SCODE-\nG, while SCODE-R outputs code written by real\nprogrammers. We sample 30 instances where RED-\nCODER generated code has a lower BLEU score\nthan that of the SCODE-R and investigate whether\nthe quality of code generated by them are signiﬁ-\ncantly different on these cases.\nAs programming requires a speciﬁc skill, we do\nnot evaluate the quality of the code generation us-\ning the mass crowd workers. We recruit 7 Ph.D.\nstudents studying in computer science as volun-\nteers2 to score (1 to 5) code based on three criteria\n2Before participating in the evaluation process, all the\nparticipants are informed that it is a voluntary task and it may\n2727\nModel Human Evaluation Automatic Metric\nSimilarity Relevance Compilability BLEU EM CodeBLEU\nSCODE-R 2.09 3.00 3.16 11.56 0.00 16.66\nREDCODER 2.06 2.94 3.10 10.70 0.07 18.31\nTable 7: Human evaluation on code generation (CodeXGLUE-Python). REDCODER (SCODE-R + SCODE-G)\nachieves similar scores as SCODE-R that directly retrieves developers’ written code which suggests that the quality\nof the code generated by SCODE-G are competitive with real code from programmers’ perspective.\n(i) similarity, and (ii) relevancew.r.t.the target code;\n(iii) the compilability of the generated code.\nThe ratings show that both models receive simi-\nlar scores, with a slightly higher score for SCODE-\nR in terms of similarity to the target code, relevancy,\nand compilability. This shows that the quality of\nthe code generated by SCODE-G are competitive\nwith real code from programmers’ perspective. In-\nterestingly, REDCODER achieves higher scores\nthan SCODE-R in CodeBLEU and Exact Match\neven on the cases where its BLEU score is lower.\n7 Related Works\nCode Summarization. In recent years, source\ncode summarization attracted a lot of attention\n(Iyer et al., 2016; Liang and Zhu, 2018; Allamanis\net al., 2016; Hu et al., 2018b; Ahmad et al., 2020).\nMany of these works view code as a sequence of to-\nken. Other approaches leverage the structural prop-\nerties of code using Tree based model (Shido et al.,\n2019; Harer et al., 2019; Hu et al., 2018a; LeClair\net al., 2019). In literature, several retrieval-based\nmethods were proposed that leverage retrieved in-\nformation along with the input code. For example,\nZhang et al. (2020) retrieves similar code snippet\nand use those as an auxiliary input for summa-\nrization. On the other hand, Hayati et al. (2018)\nretrieves related summaries for augmenting sum-\nmarization input. Different from these approaches,\nREDCODER leverages both the retrieved code and\nits summary to augment the input.\nCode Generation. Generating source code is a\nmajor stepping stone towards automated program-\nming. Yin and Neubig (2017), and Rabinovich\net al. (2017) proposed code generation as abstract\nsyntax tree generation to ensure its syntactic cor-\nrectness. Recent advancements in pre-training lan-\nguage models on unlabeled source code data (Lu\net al., 2021; Ahmad et al., 2021) showed colossal\npromise towards learning code syntax and seman-\ntics, resulting in improved code generation models.\ntake roughly 30 minutes to perform the evaluation.\nCode Retrieval and Others.Numerous software\nengineering applications require information re-\ntrieval. Sadowski et al. (2015); Xia et al. (2017);\nStolee et al. (2014); Sim et al. (2011) show that\ndevelopers search for related code, API examples\nfor implementing or adapting new APIs. Design\nof REDCODER is inspired by developers’ behav-\nior while writing code. Developers use search en-\ngines for retrieving off-the-shelf libraries (Hucka\nand Graham, 2018), or “usable” source code (Rah-\nman et al., 2018) for adapting in the development\nprocess (Nasehi et al., 2012; Arwan et al., 2015;\nPonzanelli et al., 2014). Similarly, REDCODER\nretrieves existing code or summaries and adapts\nthem to generate the target code or summary. In\ncontrast, Hashimoto et al. (2018) optimizes a joint\nobjective; Zhang et al. (2020); Liu et al. (2021)\ndo not consider any decoder pre-training, Lewis\net al. (2020) ﬁne-tunes both of the retriever and the\ngenerator end-to-end. For open domain QA, Izac-\nard and Grave (2021) propose a similar model of\nalternative generator (multi-encoder uni-decoder).\n8 Conclusion\nWe propose REDCODER to automate developers’\nwriting of code and documentation by reusing what\nthey have written previously. We evaluate RED-\nCODER on two benchmark datasets and the results\ndemonstrate a signiﬁcant performance boost with\nthe help of the retrieved information. In the future,\nwe want to extend REDCODER to support other\ncode automation tasks such as code translation.\nAcknowledgments\nWe thank anonymous reviewers for their helpful\nfeedback. We also thank the UCLA NLP group for\nhelpful discussions, comments, and participating\nvoluntarily in the human evaluation. This work\nwas supported in part by NSF OAC-1920462, SHF-\n2107405, SHF-1845893, IIS-2040961, IBM, and\nVMWare. Any opinions, ﬁndings, and conclusions\nexpressed herein are those of the authors and do\nnot necessarily reﬂect those of the US Government.\n2728\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2020. A transformer-based ap-\nproach for source code summarization. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 4998–5007,\nOnline. Association for Computational Linguistics.\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Uniﬁed pre-training\nfor program understanding and generation. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics.\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu,\nand Charles Sutton. 2018. A survey of machine\nlearning for big code and naturalness. ACM Com-\nputing Surveys (CSUR), 51(4):1–37.\nMiltiadis Allamanis, Hao Peng, and Charles A. Sut-\nton. 2016. A convolutional attention network for\nextreme summarization of source code. In Pro-\nceedings of the 33nd International Conference on\nMachine Learning, ICML 2016, New York City,\nNY, USA, June 19-24, 2016 , volume 48 of JMLR\nWorkshop and Conference Proceedings, pages 2091–\n2100. JMLR.org.\nAchmad Arwan, Siti Rochimah, and Rizky Januar\nAkbar. 2015. Source code retrieval on stackover-\nﬂow using lda. In 2015 3rd International Confer-\nence on Information and Communication Technol-\nogy (ICoICT), pages 295–299. IEEE.\nJoel Brandt, Mira Dontcheva, Marcos Weskamp, and\nScott R Klemmer. 2010. Example-centric program-\nming: integrating web search into the development\nenvironment. In Proceedings of the SIGCHI Con-\nference on Human Factors in Computing Systems ,\npages 513–522.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020a.\nCodeBERT: A pre-trained model for programming\nand natural languages. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 1536–1547, Online. Association for Compu-\ntational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020b.\nCodeBERT: A pre-trained model for programming\nand natural languages. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 1536–1547, Online. Association for Compu-\ntational Linguistics.\nDaniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 528–537, Hong\nKong, China. Association for Computational Lin-\nguistics.\nXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and\nSunghun Kim. 2016. Deep api learning. In Proceed-\nings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering,\npages 631–642.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin,\nDaxin Jiang, et al. 2021. Graphcodebert: Pre-\ntraining code representations with data ﬂow. In\nInternational Conference on Learning Representa-\ntions.\nDaya Guo, Duyu Tang, Nan Duan, Ming Zhou, and\nJian Yin. 2019. Coupling retrieval and meta-\nlearning for context-dependent semantic parsing. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 855–\n866, Florence, Italy. Association for Computational\nLinguistics.\nRuiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and\nDavid Simcha. 2016. Quantization based fast inner\nproduct search. In Artiﬁcial Intelligence and Statis-\ntics, pages 482–490. PMLR.\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\ntransformer: A transformer-based method for cor-\nrection of tree-structured data. arXiv preprint\narXiv:1908.00449.\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\nand Percy S Liang. 2018. A retrieve-and-edit frame-\nwork for predicting structured outputs. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 31. Curran Associates, Inc.\nShirley Anugrah Hayati, Raphael Olivier, Pravalika Av-\nvaru, Pengcheng Yin, Anthony Tomasic, and Gra-\nham Neubig. 2018. Retrieval-based neural code gen-\neration. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 925–930, Brussels, Belgium. Association for\nComputational Linguistics.\n2729\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a.\nDeep code comment generation. In Proceedings\nof the 26th Conference on Program Comprehension,\npage 200–210, New York, NY , USA. Association for\nComputing Machinery.\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi\nJin. 2018b. Summarizing source code with trans-\nferred api knowledge. In Proceedings of the Twenty-\nSeventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-18 , pages 2269–2275. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nMichael Hucka and Matthew J Graham. 2018. Soft-\nware search is not a science, even among scientists:\nA survey of how scientists and engineers ﬁnd soft-\nware. Journal of Systems and Software , 141:171–\n191.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In International Conference\non Learning Representations.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nSrinivasan Iyer, Alvin Cheung, and Luke Zettlemoyer.\n2019. Learning programmatic idioms for scalable\nsemantic parsing. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5426–5435, Hong Kong, China. As-\nsociation for Computational Linguistics.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2073–2083, Berlin, Germany. Association for\nComputational Linguistics.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. In Proceedings of the\n2018 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1643–1652, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 874–880, Online. Association for Com-\nputational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nAlexander LeClair, Siyuan Jiang, and Collin McMil-\nlan. 2019. A neural model for generating natural\nlanguage summaries of program subroutines. In\nProceedings of the 41st International Conference on\nSoftware Engineering, page 795–806. IEEE Press.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nHongwei Li, Zhenchang Xing, Xin Peng, and Wenyun\nZhao. 2013. What help do developers seek, when\nand how? In 2013 20th working conference on re-\nverse engineering (WCRE), pages 142–151. IEEE.\nYuding Liang and Kenny Qili Zhu. 2018. Automatic\ngeneration of text descriptive comments for code\nblocks. In Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence, pages 5229–5236.\nChin-Yew Lin and Franz Josef Och. 2004. ORANGE:\na method for evaluating automatic evaluation met-\nrics for machine translation. In COLING 2004: Pro-\nceedings of the 20th International Conference on\nComputational Linguistics, pages 501–507, Geneva,\nSwitzerland. COLING.\nShangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,\nand Yang Liu. 2021. Retrieval-augmented genera-\ntion for code summarization via hybrid {gnn}. In\nInternational Conference on Learning Representa-\ntions.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.\nCodexglue: A machine learning benchmark dataset\nfor code understanding and generation. arXiv\npreprint arXiv:2102.04664.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\n2730\nCD Manning, P Raghavan, and H Schütze. 2008. Xml\nretrieval. In Introduction to Information Retrieval.\nCambridze University Press.\nSeyed Mehdi Nasehi, Jonathan Sillito, Frank Maurer,\nand Chris Burns. 2012. What makes a good code\nexample?: A study of programming q&a in stack-\noverﬂow. In 2012 28th IEEE International Confer-\nence on Software Maintenance (ICSM), pages 25–34.\nIEEE.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018. Building language mod-\nels for text with named entities. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2373–2383, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nLuca Ponzanelli, Gabriele Bavota, Massimiliano\nDi Penta, Rocco Oliveto, and Michele Lanza. 2014.\nMining stackoverﬂow to turn the ide into a self-\nconﬁdent programming prompter. In Proceedings\nof the 11th Working Conference on Mining Software\nRepositories, pages 102–111.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1139–\n1149, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMd Masudur Rahman, Jed Barson, Sydney Paul,\nJoshua Kayani, Federico Andrés Lois, Sebastián Fer-\nnandez Quezada, Christopher Parnin, Kathryn T\nStolee, and Baishakhi Ray. 2018. Evaluating how\ndevelopers use general-purpose web-search for code\nretrieval. In Proceedings of the 15th International\nConference on Mining Software Repositories, pages\n465–475.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie\nLiu, Duyu Tang, Ming Zhou, Ambrosio Blanco, and\nShuai Ma. 2020. Codebleu: a method for auto-\nmatic evaluation of code synthesis. arXiv preprint\narXiv:2009.10297.\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Now Publishers Inc.\nCaitlin Sadowski, Kathryn T Stolee, and Sebastian El-\nbaum. 2015. How developers search for code: a case\nstudy. In Proceedings of the 2015 10th joint meeting\non foundations of software engineering, pages 191–\n201.\nYusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto,\nAtsushi Miyamoto, and Tadayuki Matsumura. 2019.\nAutomatic source code summarization with ex-\ntended tree-lstm. In International Joint Conference\non Neural Networks, IJCNN 2019 Budapest, Hun-\ngary, July 14-19, 2019, pages 1–8. IEEE.\nSusan Elliott Sim, Medha Umarji, Sukanya Ratano-\ntayanon, and Cristina V Lopes. 2011. How well do\nsearch engines support code retrieval on the web?\nACM Transactions on Software Engineering and\nMethodology (TOSEM), 21(1):1–25.\nKathryn T Stolee, Sebastian Elbaum, and Daniel Do-\nbos. 2014. Solving the search for source code. ACM\nTransactions on Software Engineering and Method-\nology (TOSEM), 23(3):1–45.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, volume 27, pages 3104–3112. Curran Asso-\nciates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.\nCode generation as a dual task of code summariza-\ntion. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 6563–6573. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nXin Xia, Lingfeng Bao, David Lo, Pavneet Singh\nKochhar, Ahmed E Hassan, and Zhenchang Xing.\n2017. What do developers search for on the web?\nEmpirical Software Engineering, 22(6):3149–3185.\nFrank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan\nVasilescu, and Graham Neubig. 2020. Incorporating\nexternal knowledge through pre-training for natural\n2731\nlanguage to code generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6045–6052, Online. As-\nsociation for Computational Linguistics.\nFrank F Xu, Bogdan Vasilescu, and Graham Neubig.\n2021. In-ide code generation from natural lan-\nguage: Promise and challenges. arXiv preprint\narXiv:2101.11149.\nWen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Proceed-\nings of the ﬁfteenth conference on computational nat-\nural language learning, pages 247–256.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 440–450, Vancouver, Canada.\nAssociation for Computational Linguistics.\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,\nand Xudong Liu. 2020. Retrieval-based neural\nsource code summarization. In 2020 IEEE/ACM\n42nd International Conference on Software Engi-\nneering (ICSE), pages 1385–1397. IEEE.\n2732\nSupplementary Material: Appendices\nA Qualitative Example\nIn Figure 11, we show an example of generated\ncode by a baseline and different modules of RED-\nCODER. The input summary asks to write a code\n(in Java) to get a MuxerStream given a position .\nWe show two of the corresponding retrieved\ncode, their summaries (for bimodal instances),\ngenerated code of PLBART, REDCODER, and\nREDCODER-EXT. As can be seen, PLBART gen-\nerates a basic but relevant code; both retrieved code\n(rank-1 and rank-3) contains the statements with\nvariable cPtr one of them is of MuxerStream\nclass, and another is from DeMuxerStream class.\nREDCODER generates a somewhat correct code\nof MuxerStream class and it takes the position\nargument too. Seemingly, while fusing the re-\ntrieved code, we suspect that as the tentative func-\ntion name MuxerStream mentioned in the in-\nput summary does not match the function name\nDeMuxerStream of the rank-3 retrieved code, it\nonly adapts one line containing cPtr from rank-3\nretrieved code (line #3) and takes the rests includ-\ning the function deﬁnition (i.e., line #1) from the\nrank-1 retrieved code. Now when REDCODER-\nEXT is allowed to leverage the summaries of the\nretrieved code, it can match the summary of the\nrank-3 retrieved code with the input, and that is\nwhy it produces the MuxerStream class object\nbut with the throw exceptions from the rank-3 re-\ntrieved code.\nB Performance Difference of PLBART\non CodeXGLUE and Concode\nConcode is a relatively easier dataset for code gen-\neration and retrieval due to several pre-processing\nsteps taken by its authors. Along with additional\ncontexts (environment variables and methods) in\nthe input summary, Concode artifacts the target\ncode by replacing the speciﬁc variable names with\ngeneric tokens.\n1 void function(Element arg0,\n2 Formula arg1) {\n3 arg0.addElement(\n4 \"concode_string\").setText(\n5 arg1.getText());\n6 }\nTherefore, we suspect that due to this, PLBART\nachieves good EM score for Concode but not for\nthe generation of real code in CodeXGLUE.\nAnalogously for the retrieval models, code re-\ntrieved by BM25 have also a large word overlap-\nping with the targets in Concode in contrast to\nCodeXGLUE (1st row in Table 2 and 3). Con-\nsequently, BM25 retrieval boosts PLBART (i.e.,\nBM25 + PLBART) more in Concode than that in\nCodeXGLUE (3rd row for the bottom in Table 2\nand 3). Overall, we anticipate all these skewness\nin model performances are due to the dataset char-\nacteristics.\n2733\nDataset Lang. Task\nRetrieval Database\n|Size| |Nonparallel|CSNet CCSD Concode\nCodeXGLUE\nPython Gen. \u0013 \u0017 \u0017 1.2M 504K\nSum. \u0013 \u0013 \u0017 1.1M 833K\nJava Gen. \u0013 \u0017 \u0017 1.6M 543K\nSum. \u0013 \u0013 \u0017 1.1M 903K\nConcode Java Gen. \u0017 \u0017 \u0013 104K 0\nTable 8: Retrieval database statistics. “Size” refers to both of parallel and nonparallel code or summaries. As\nConcode has a different data format, we only retrieve from itself. Nonparallel means the retrieval candidates are\nonly code (for code gen.) and only summaries (for code sum.). CSNet (CodeSearchNet), CCSD refer to Husain\net al. (2019) and Liu et al. (2021).\ncode target present summary CodeXGLUE (Java) CodeXGLUE (Python)\nretrieval in retrieval retrieval BLEU EM CodeBLEU BLEU EM CodeBLEU\n\u0017 \u0017 \u0017 10.1 0.0 14.96 4.89 0.0 12.01\n\u0013\n\u0017 \u0017 26.92 8.95 31.15 22.74 8.88 28.93\n\u0013 28.98 10.21 33.18 24.43 9.61 30.21\n\u0013 \u0017 36.33 29.41 41.38 32.14 27.48 38.02\n\u0013 42.82 36.99 47.25 38.87 34.51 43.78\nTable 9: Ablation results on source code generation using the retrieved code and its summary together when the\nreference target code is absent and present in the retrieval database respectively.\nMethods CodeXGLUE-Python CodeXGLUE-Java\nBLEU-4 ROUGE-L BLEU-4 ROUGE-L\nSCODE-R 46.6 53.8 48.0 55.7\nREDCODER 47.0 55.4 50.4 58.8\nREDCODER-EXT 47.1 55.5 50.4 58.7\nTable 10: Evaluation results of code summarization keeping the target summary in the retrieval database.\nFigure 9: #Code per target length.\n Figure 10: BLEU vs target len. (Java)\n2734\nInput summary:Get the MuxerStream at the given position.\nPLBART Prediction [BLEU: 0.1439]\n1 public MuxerStream getMuxerStream (int position) {\n2 if (muxerStream == null) {\n3 muxerStream = new MuxerStream (this, position)\n4 }\n5 return muxerStream;\n6 }\nSCODER Retrieval (Rank-1) [BLEU: 0.4298]\n1 public Muxer getMuxer () {\n2 long cPtr = VideoJNI.MuxerStream_getMuxer(swigCPtr, this);\n3 return (cPtr == 0) ? null : new Muxer (cPtr, false);\n4 }\nPaired summary:Get the Muxer that this stream belongs to\nSCODER Retrieval (Rank-3) [BLEU: 0.6740]\n1 public DemuxerStream getStream (int streamIndex)\n2 throws java.lang.InterruptedException, java.io.IOException {\n3 long cPtr = VideoJNI.Demuxer_getStream(swigCPtr, this, streamIndex);\n4 return (cPtr == 0) ? null : new DemuxerStream (cPtr, false);\n5 }\nREDCODER Prediction [BLEU: 0.6170]\n1 public MuxerStream getMuxerStream (int position) {\n2 long cPtr = VideoJNI.MuxerStream_getMuxer(swigCPtr, this, position);\n3 return (cPtr == 0) ? null : new MuxerStream (cPtr, false);\n4 }\nREDCODER-ext Prediction [BLEU: 0.8062]\n1 public MuxerStream getMuxer (int streamIndex)\n2 throws java.lang.InterruptedException, java.io.IOException {\n3 long cPtr = VideoJNI.MuxerStream_getMuxer(swigCPtr, this, streamIndex);\n4 return (cPtr == 0) ? null : new MuxerStream (cPtr, false);\n5 }\nReference (Gold Output)\n1 public MuxerStream getMuxer (int streamIndex)\n2 throws java.lang.InterruptedException, java.io.IOException {\n3 long cPtr = VideoJNI.MuxerStream_getMuxer(swigCPtr, this, streamIndex);\n4 return (cPtr == 0) ? null : new MuxerStream (cPtr, false);\n5 }\nFigure 11: A qualitative example to show the effectiveness of retrieval-augmented generation as proposed in\nREDCODER framework",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8396808505058289
    },
    {
      "name": "Automatic summarization",
      "score": 0.8002585172653198
    },
    {
      "name": "Information retrieval",
      "score": 0.6096285581588745
    },
    {
      "name": "Source code",
      "score": 0.6020393967628479
    },
    {
      "name": "KPI-driven code analysis",
      "score": 0.5884108543395996
    },
    {
      "name": "Code generation",
      "score": 0.5208321809768677
    },
    {
      "name": "Precision and recall",
      "score": 0.5059043765068054
    },
    {
      "name": "Code (set theory)",
      "score": 0.48229220509529114
    },
    {
      "name": "Code review",
      "score": 0.46080777049064636
    },
    {
      "name": "Documentation",
      "score": 0.4503289461135864
    },
    {
      "name": "Software",
      "score": 0.4329741895198822
    },
    {
      "name": "Programming language",
      "score": 0.41888052225112915
    },
    {
      "name": "Codebase",
      "score": 0.4122553765773773
    },
    {
      "name": "Static program analysis",
      "score": 0.3996311128139496
    },
    {
      "name": "Software development",
      "score": 0.2609862685203552
    },
    {
      "name": "Key (lock)",
      "score": 0.12678250670433044
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 5
}