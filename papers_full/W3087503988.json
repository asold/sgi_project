{
  "title": "Training Production Language Models without Memorizing User Data",
  "url": "https://openalex.org/W3087503988",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226861678",
      "name": "Ramaswamy, Swaroop",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749923364",
      "name": "Thakkar, Om",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221976403",
      "name": "Mathews, Rajiv",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288567238",
      "name": "Andrew, Galen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288567239",
      "name": "McMahan, H. Brendan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754237597",
      "name": "Beaufays, Françoise",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963612912",
    "https://openalex.org/W2119874464",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W2913570153",
    "https://openalex.org/W2104743167",
    "https://openalex.org/W3037261120",
    "https://openalex.org/W2399077675",
    "https://openalex.org/W2964296660",
    "https://openalex.org/W1499864241",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3102303514",
    "https://openalex.org/W2963741669",
    "https://openalex.org/W2900120080",
    "https://openalex.org/W2766255512",
    "https://openalex.org/W2950602864",
    "https://openalex.org/W1557833142",
    "https://openalex.org/W2788502731",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W2944525415",
    "https://openalex.org/W3035493137",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2512472178",
    "https://openalex.org/W3035034338",
    "https://openalex.org/W2154711640",
    "https://openalex.org/W2785361959",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2963999993",
    "https://openalex.org/W2995022099"
  ],
  "abstract": "This paper presents the first consumer-scale next-word prediction (NWP) model trained with Federated Learning (FL) while leveraging the Differentially Private Federated Averaging (DP-FedAvg) technique. There has been prior work on building practical FL infrastructure, including work demonstrating the feasibility of training language models on mobile devices using such infrastructure. It has also been shown (in simulations on a public corpus) that it is possible to train NWP models with user-level differential privacy using the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models with DP-FedAvg in a real-world production environment on a heterogeneous fleet of mobile phones requires addressing numerous challenges. For instance, the coordinating central server has to keep track of the devices available at the start of each round and sample devices uniformly at random from them, while ensuring \\emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL work of which we are aware, for the first time we demonstrate the deployment of a differentially private mechanism for the training of a production neural network in FL, as well as the instrumentation of the production training infrastructure to perform an end-to-end empirical measurement of unintended memorization.",
  "full_text": "1\nTraining Production Language Models\nwithout Memorizing User Data\nSwaroop Ramaswamy*, Om Thakkar *, Rajiv Mathews, Galen Andrew, H. Brendan\nMcMahan, and Franc ¸oise Beaufays\nGoogle LLC,\nMountain View, CA, U.S.A.\n{swaroopram, omthkkr, mathews, galenandrew, mcmahan, fsb}\n@google.com\nAbstract—This paper presents the ﬁrst consumer-scale\nnext-word prediction (NWP) model trained with Federated\nLearning (FL) while leveraging the Differentially Private\nFederated Averaging (DP-FedAvg) technique. There has\nbeen prior work on building practical FL infrastructure,\nincluding work demonstrating the feasibility of training\nlanguage models on mobile devices using such infras-\ntructure. It has also been shown (in simulations on a\npublic corpus) that it is possible to train NWP models\nwith user-level differential privacy using the DP-FedAvg\nalgorithm. Nevertheless, training production-quality NWP\nmodels with DP-FedAvg in a real-world production en-\nvironment on a heterogeneous ﬂeet of mobile phones\nrequires addressing numerous challenges. For instance,\nthe coordinating central server has to keep track of the\ndevices available at the start of each round and sample\ndevices uniformly at random from them, while ensuring\nsecrecy of the sample , etc. Unlike all prior privacy-focused\nFL work of which we are aware, for the ﬁrst time we\ndemonstrate the deployment of a differentially private\nmechanism for the training of a production neural network\nin FL, as well as the instrumentation of the production\ntraining infrastructure to perform an end-to-end empirical\nmeasurement of unintended memorization.\nI. I NTRODUCTION\nNext word prediction (NWP) is the task of pro-\nviding the most probable next word or phrase given\na small amount of preceding text. Gboard is a\nvirtual keyboard for touchscreen mobile devices\nthat provides features such as auto-correction and\nword completion, in addition to next-word predic-\ntion. Trained language models (LMs) are used to\n*Equal contribution\nperform the task of NWP on user-generated data.\nTo provide high utility, they are trained using user-\ngenerated data as well. However, such data can be\nprivacy sensitive; it can include chats, text messages,\nand search queries. Federated learning [MMR +17],\n[KMA+19] is a distributed learning approach that\nenables training models without the need to cen-\ntralize user data. There has been work [BEG +19]\nin developing a scalable production system for FL,\nbased on TensorFlow [AAB +15], in the domain of\nmobile devices. Recent work [HRM +18] has used\nthis system to train a model for the NWP task. In\nthis work, we build on the approach of [HRM +18].\nIn this work, our primary goal is the protection\nof private user data from an adversary with access\nto the ﬁnal machine learning model trained on user\ndata via FL; we thus assume the server implement-\ning FL is trusted. Since such models are typically\ndeployed to many millions of devices for on-device\ninference, access to the model and its predictions\ncannot realistically be controlled. Thus, ensuring\nprivate information cannot be extracted from the\nmodel is essential. Providing such guarantees with\nweaker trust assumptions for the server (honest-\nbut-curious, or malicious) is a valuable goal, but\nit requires different techniques and is beyond the\nscope of this work [KMA +19].\nDifferential privacy (DP) [DMNS06],\n[DKM+06a] provides a gold standard for\nperforming learning tasks over sensitive data.\nIntuitively, DP prevents an adversary from\nconﬁdently making any conclusions about whether\nany particular data record was used in training\narXiv:2009.10031v1  [cs.LG]  21 Sep 2020\n2\na model, even while having access to the model\nand arbitrary external side information. For\nmachine learning, two granularities of a data\nrecord are particularly relevant, example-level,\nand user-level (though notions in between these\nhave been considered, for example “element-level”\n[ADJ19]). Many prior works in DP machine\nlearning (ML) [CMS11], [BST14], [ACG +16],\n[PAE+16], [WLK +17], [PSM +18], [INS +19] deal\nwith example-level privacy, i.e., providing privacy\nguarantees for any single example in a dataset.\nHowever, in tasks like language modeling, such\na guarantee can be quite weak, as any individual\nuser may contribute thousands of examples to the\ntraining corpus. FL is naturally suited to the strictly\nstronger notion of user-level privacy ([MRTZ17],\n[JTT18], [AMR +19], [TAM19]), which provides\nguarantees for all the examples contributed by any\nindividual user in the training process. Differential\nprivacy comprises two main components. First, a\nDP mechanism is a randomized procedure where\ntypically 1) an upper bound on the sensitivity of\nthe mechanism to any one user’s data is enforced,\nand 2) noise calibrated to that sensitivity is added\nto the output. We deploy such a mechanism (see\nSection II-A for more details). Second, such\na mechanism is accompanied by a formal DP\nguarantee characterized by two parameters ϵ and δ\nthat upper-bound the privacy loss of the mechanism.\nPrior work [MRTZ17] provides a technique,\ncalled Differentially Private Federated Averaging\n(DP-FedAvg), for training neural networks (includ-\ning recurrent language models) with user-level DP\nvia FL. It has shown that good privacy-utility trade-\noffs are possible in idealized simulated FL environ-\nments with a large number of users. Federated learn-\ning alone offers direct privacy beneﬁts by keeping\ndata decentralized, allowing client devices to control\ntheir participation, aggregating early, and only send-\ning focused ephemeral updates to the server. One of\nthe contributions of this work is highlighting that,\nperhaps surprisingly, these very privacy beneﬁts of\nFL make it more challenging for the server to pro-\nvide a proof of a speciﬁc (ϵ,δ)-DP guarantee since\nit has limited visibility and control of the overall de-\ncentralized training mechanism. In fact, in produc-\ntion FL systems, the assumptions required by known\nDP theorems [BST14], [ACG +16], [MRTZ17] may\nonly hold approximately, or otherwise be difﬁcult to\nverify. Designing new DP mechanisms and analysis\nthat address these challenges and hence apply to\nreal-world deployments of FL is an important active\narea for research [BKM +20], but in this work we\ntake a complimentary approach.\nWe deploy the DP-FedAvg mechanism in a real-\nworld system, and then, rather than focusing on\nproving upper-bounds on (ϵ,δ)-DP (which exist,\nbut may be hard for the server to certify), we\nassess the privacy of our training method using an\nend-to-end measurement process. Our evaluation of\nprivacy is based on the Secret Sharer framework\n[CLK+18] (more details in Section II-B) for an FL\nsetting [TRMB20], which can measure unintended\nmemorization of user data. Prior work [TRMB20]\nhas shown via simulations that training generative\nmodels with DP-FedAvg does not exhibit such\nmemorization for thousands insertions of out-of-\ndistribution phrases in the training data. Our results\nare noteworthy as our models are trained in a\nproduction setting using actual user data, and are\nable to tolerate thousands of insertions of out-of-\ndistribution phrases as well, while at the same time\nproviding better utility than the existing benchmark.\nWe perform this validation as part of a multi-\nfaceted approach to private training, including other\ntechniques like using a ﬁxed vocabulary for the\ntraining data, and limiting the number of training\ndata for each individual user.\nEven as the theory in differentially private ML\nadvances [CWH20], [STT20], we believe the end-\nto-end approach described here will continue to be\na vital component of applied ML on private data. A\ntheoretical result applies to an algorithm operating\nunder particular assumptions. In contrast, an end-to-\nend measurement approach tests a complete soft-\nware system running under real-world conditions,\nallowing for instance, the detection of bugs or\nviolated assumptions that would fall outside the\nscope of theory.\nII. P RELIMINARIES\nA. DP Federated Averaging with Fixed-size Rounds\nWe now present the DP mechanism (Algorithm 1)\nthat we employ to train our language model.\nIt closely follows the DP-FedAvg technique in\n[MRTZ17], in that per-user updates are clipped to\nhave a bounded L2 norm, and calibrated Gaussian\nnoise is added to the weighted average update to be\nused for computing the model to be sent in the next\n3\nround. A slight difference between the DP-FedAvg\nalgorithm in [MRTZ17] and our approach is the way\nin which client devices are sampled to participate in\na given federated round of computation. DP-FedAvg\nuses Poisson sampling, where for each round, each\nuser is selected independently with a ﬁxed prob-\nability. In this work (also, following [AMR +19],\n[TRMB20]), we instead use ﬁxed-size federated\nrounds, where a ﬁxed number of users is randomly\nsampled to participate in each round. A pseudo-code\nfor our mechanism is given in Algorithm 1.\nProving a formal DP guarantee for Algorithm 1\nrequires several assumptions like knowledge of the\nsize of the participating user population (N), and the\nserver being able to sample uniformly at random\namong them at each iteration. Such assumptions\nmay not always hold in real-world deployments.\nSection V presents a detailed discussion of practical\nconsiderations for privacy guarantees in real-world\nFL systems.\nB. Measuring Unintended Memorization\nWe use the Secret Sharer technique from\n[CLK+18] as a proxy for measuring how much\nprivate information might be extracted from such\na model. Our approach is designed to over-estimate\nwhat a realistic adversary might learn (more details\nin Section IV-A). However, unlike a formal DP\nguarantee, this empirical approach cannot rule out\nthe possibility that some more clever technique (for\nexample, one that directly inspects the model pa-\nrameters) might reveal more. Thus, developing more\nsophisticated attacks (memorization measurement\ntechniques) is an important complimentary line of\nresearch.\nNow, we describe the Secret Sharer framework.\nFirst, random sequences called canaries are inserted\ninto the training data. The canaries are constructed\nbased on a preﬁxed format sequence. For instance,\nto design the framework for a character-level model,\nthe format could be “My SSN is xxx-xx-xxxx”,\nwhere each x can take a random value from digits\n0 to 9. Next, the target model is trained on the\nmodiﬁed dataset containing the canaries. Lastly,\nmethods like Random Sampling and Beam Search\n(both formally deﬁned in Section IV) are used to\nefﬁciently measure the extent to which the model\nhas “memorized” the inserted random canaries, and\nwhether it is possible for an adversary with partial\nMain training loop:\nparameters: round participation fraction q ∈\n(0,1], total user population D of size N ∈N,\nnoise scale z ∈ R+, clip parameter S ∈ R+,\ntotal rounds T\nInitialize model θ0, moments accountant M\nSet noise standard deviation σ= zS\nqN\nfor each round t= 0,1,2,...,T do\nCt ←(sample without replacement qN users\nfrom population)\nfor each user k∈Ct in parallel do\n∆t+1\nk ←UserUpdate(k,θt)\n∆t+1 = 1\nqN\n∑\nk∈Ct\n∆t+1\nk\nθt+1 ←θt + ∆t+1 + N(0,Iσ2)\nUserUpdate(k,θ0):\nparameters: number of local epochs E ∈N, batch\nsize B ∈N, learning rate η∈R+, clip parameter\nS ∈R+, loss function ℓ(θ; b)\nθ←θ0\nfor each local epoch i from 1 to E do\nB← (k’s data split into size B batches)\nfor each batch b∈B do\nθ←θ−η▽ℓ(θ; b)\n∆ = θ−θ0\nreturn update ∆k = ∆ ·min\n(\n1, S\n∥∆∥\n)\n// Clip\nAlgorithm 1: DP-FedAvg with ﬁxed-size federated\nrounds, used to train our language model.\nknowledge to extract the canary. For instance, if a\ncanary is classiﬁed as memorized via our Random\nSampling method, then an adversary with a “guess”\nof the canary can be conﬁdent with very high\nprobability whether the guess is correct just by\nrandomly sampling other phrases and evaluating\ntheir perplexities on the given model.\nIII. I MPLEMENTATION DETAILS\nIn this section, we start by providing the details\nof our implementation, and state the performance\nof our NWP model. We show that even with clip-\nping client updates and a large amount of noise\naddition, our NWP model has superior utility than\nthe existing baseline n-gram Finite State Transducer\n4\n(FST) model. The FST model is a Katz-smoothed\nBayesian interpolated LM that is augmented with\nother smaller LMs such as a user history LM.\nA. Model Architecture and Hyperparameters\nThe model architecture we use mirrors the one\nused in [HRM +18]. We use a single layer CIFG-\nLSTM [SSB14] neural network with shared weights\nbetween the input embedding layer and the output\nprojection layer. The overall number of parameters\nin the model is 1.3M.\nTypically, tuning hyperparameters for neural net-\nworks requires training several models with various\nhyperparameter settings. Instead of tuning hyper-\nparameters on sensitive user data, we tune the\nhyperparameters by training the same model with\nDP-FedAvg on a public dataset, namely the Stack\nOverﬂow corpus. 1 By tuning hyperparameters on a\npublic dataset, we avoid incurring any additional\nprivacy cost.\nWhen training on real devices, we use the hy-\nperparameters that performed best on the Stack\nOverﬂow dataset. The only change we make is to\nthe words in the vocabulary; when training on real\ndevices we train on only devices containing Spanish\nlanguage data.\nFor all hyperparameter tuning, we train models\nwith 500 users participating in every round, and\nadd Gaussian noise with σ = 3 .2 ×10−5 to the\naverage of their clipped updates. Note that to get any\nactual privacy guarantees, we would have to train\nmodels with a signiﬁcantly larger number of users\nparticipating per round for the same amount of noise\nadded (σ). Since we are doing our hyperparameter\ntuning on a public dataset, we are only interested in\nthe utility characteristics of the trained models, not\nany privacy guarantees.\nWe evaluate the performance of all models on the\nrecall metric, deﬁned as the ratio of the number of\ncorrect predictions to the total number of words.\nRecall for the highest-likelihood candidate (top-1\nrecall) is important for Gboardas these are presented\nin the center of the suggestion strip where users\nare more likely to see them. Since Gboardincludes\nmultiple candidates in the suggestion strip, top-3\nrecall is also of interest.\n1https://www.tensorﬂow.org/federated/api docs/python/tff/\nsimulation/datasets/stackoverﬂow/load data\nThe best performing model hyperparameters on\nthe Stack Overﬂow dataset are listed in Table 1.\nWe also run a few ablation studies to study the\neffect of various hyperparameters on recall. We\nﬁnd that using momentum as the server optimizer\nand clipping around 90% of the clients per round\ngives best results. We also ﬁnd that the utility is\nnot affected by different choices of client batch\nsizes. Refer to Appendix A for more details on the\nablation studies.\nHyperparameter Value\nServer optimizer Momentum\nServer learning rate ( ηs) 1.0\nServer momentum ( µ) 0.99\nClient batch size ( |b|) 50\nClient learning rate ( ηc) 0.5\nClipping norm ( S) 0.8\nTable 1: Hyperparameter values for the best per-\nforming model conﬁguration on Stack Overﬂow.\nB. Production Training\nWe train a model using the DP-FedAvg algorithm\non real devices running Gboard, with the model\nconﬁguration speciﬁed in Table 1. We aggregate\nupdates from 20000 clients on each round of train-\ning, and add Gaussian noise with standard deviation\nσ = 3 .2 ×10−5 to the average of their clipped\nupdates. The model converges after T = 2000\nrounds of training, which took about three weeks\nto complete.\nC. Live Experiments\nMetric N-gram FST Our NWP model Relative\n(Baseline) [This paper] Change (%)\nTop-1 Recall 10.24 11 .03 +7 .77%\n(7.49,8.06)\nTop-3 Recall 18.09 19 .25 +6 .40%\n(6.17,6.63)\nCTR 1.84 1 .92 +4 .31%\n(2.17,6.45)\nTable 2: Live inference experiment results.\nWe compare the results from our model with the\nbaseline n-gram FST model in a live experiment.\nIn addition to top-1 recall and top-3 recall, we also\n5\nlook at the prediction click-through rate metric, de-\nﬁned as the ratio of number of clicks on prediction\ncandidates to the number of proposed prediction\ncandidates.\nThe top-1 recall and top-3 recall in this experi-\nment are measured over the number of times users\nare shown prediction candidates. The prediction\nclick-through rate (CTR) is deﬁned as the ratio\nof the number of clicks on prediction candidates\nto the number of proposed prediction candidates.\nQuoted 95% conﬁdence interval errors for all results\nare derived using the jackknife method with user\nbuckets. Table 2 summarizes the recall and CTR\nmetrics in live experiment for our NWP model\ntrained using DP-FedAvg, and the baseline n-gram\nFST model.\nThe live experiment results from Table 2 show\nthat the NWP model signiﬁcantly outperforms the\nbaseline n-gram FST, in both recall and CTR met-\nrics. This is consistent with the observations from\n[HRM+18]. These gains are impressive given that\nthe n-gram model FST includes personalized com-\nponents such as user history.\nIV. E VALUATING FOR UNINTENDED\nMEMORIZATION\nThere is a growing line of work ([FJR15],\n[WFJN16], [SSSS17], [CLK +18], [SS19],\n[TRMB20]) demonstrating that neural networks\ncan leak information about their underlying training\ndata in many ways. Given that we train next-word\nprediction models in this work, we focus on\nthe Secret Sharer frameworks from [CLK +18],\n[TRMB20] designed to measure the resilience\nof generative models obtained via a training\nprocedure, against the unintended memorization of\nrarely-occurring phrases in a dataset. Speciﬁcally,\nwe extend the idea of the Federated Secret\nSharer [TRMB20], which focused on user-based\ndatasets that are typical in FL, to a production\nsetting. Through an extensive empirical evaluation,\nwe demonstrate the remarkable extent to which\ntraining models via our implementation is able to\nwithstand such memorization.\nA. Experiment Setup\nNext, we describe the setup of our empirical\nevaluation. In the following, we detail the various\nstages of our procedure, including creating secret-\nsharing synthetic devices, construction of the ca-\nnaries added into the synthetic devices, insertion of\nthe synthetic devices into our FL training procedure,\nand the techniques used for measuring unintended\nmemorization of a generative model.\nNetwork architecture, and training corpus: Since\nwe want to measure memorization for the models\ntrained via our implementation, we start with the\nsame network architecture and training corpus as\ndescribed in Section III for conducting the experi-\nments in this section.\nCanary construction: We opt for inserting ﬁve-\nword canaries as our model is not efﬁcient at\nencoding longer contexts. Each word in a canary\nis chosen uniformly at random (u.a.r.) from the\n10K model vocabulary. It is important to note that\nwe want to measure unintended memorization for\nour models, i.e., memorization of out-of-distribution\nphrases, which is in fact orthogonal to our learning\ntask. Hence, to be able to obtain such phrases with\nvery high probability, our canaries are constructed\nusing randomly sampled words. For instance, our\ninserted canaries consist of phrases like “extranjera\nconciertos mercadeo cucharadas segundos”, “domi-\ncilio mariposa haberlo cercanas partido”, “ve traba-\njador corrida sabemos cuotas”, etc.\nSecret-sharing synthetic devices: Since our mod-\nels involve training on actual devices, we create\nvarious synthetic devices containing canaries in\ntheir training data, and have them participate in\nthe training along with actual devices. To make\nthis setting more realistic, the synthetic devices\ncontain sentences from a public corpus in addition\nto the canaries. Each canary is parameterized by two\nparameters, nu and ne. The number of synthetic de-\nvices sharing the canary is denoted by nu. Each such\nsynthetic device contains ne copies of the canary,\nand (200 −ne) sentences randomly sampled from\nthe public corpus. We consider canaries with con-\nﬁgurations in the cross product of nu ∈{1,4,16}\nand ne ∈ {1,14,200}, and we have three differ-\nent canaries for each (nu,ne) conﬁguration. These\nparameters result in the insertion of 27 different\ncanaries, and a total of 3 ·3 ·(1 + 4 + 16) = 189\nunique synthetic devices participating in the training\nprocess. We avoid adding more than three different\ncanaries for each (nu,ne) conﬁguration so as to not\noverwhelm the training data with canaries.\n6\nTraining procedure: We use the training pro-\ncedure described in Section III for training our\nmodels, with the only difference being that for\neach round of training, we include all the secret-\nsharing synthetic devices to be available for being\nsampled. The rate of participation of the synthetic\ndevices is 1-2 orders of magnitude higher than\nany actual device due to two main factors. First,\nour synthetic devices are available throughout the\ntraining process, which is not the case for actual\ndevices. Moreover, even when the actual devices are\navailable, their participation in the training process\nis coordinated by our load-scheduling mechanism\ncalled Pace Steering [BEG +19], which lowers the\nnext scheduling priority of a device once it has\nparticipated in training (to restrict multiple partic-\nipations within any short phase of training). On\nthe other hand, our synthetic devices don’t adhere\nto Pace Steering, resulting in a further increase\nin their participation rate. Table 3 shows for each\ncanary conﬁguration, the number of times a canary\nis encountered by a model trained in our setup. From\nthe (nu = 1 ,ne = 1) conﬁguration, it is easy to\nsee that each secret-sharing synthetic device (for\nany canary conﬁguration) participates in expectation\n1150 times during 2000 rounds of training. Note that\nthis should, if anything, increase the chance that a\ncanary phrase will be memorized.\nnu ne Expected # times\ncanary seen in training\n1 1 1,150\n1 14 16,100\n1 200 230,000\n4 1 4,600\n4 14 64,400\n4 200 920,000\n16 1 18,400\n16 14 257,600\n16 200 3,680,000\nTable 3: Expected number of times canaries for\neach (nu,ne) conﬁguration encountered by a model\ntrained in our setup.\nEvaluation methods: For our evaluation, we denote\nan inserted canary by c= (p|s), where pis a 2-word\npreﬁx, and s is the remaining 3-word sequence.\nWe use the two methods of evaluation used in\n[TRMB20], namely Random Sampling and Beam\nSearch, to determine if given the canary preﬁx p,\nthe remaining sequence s has been unintentionally\nmemorized by a model.\n1) Random Sampling (RS) [CLK +18]: First,\nwe deﬁne the log-perplexity of a model θ on\na sequence s= s1,...,s n given context p as\nPθ(s|p) =\nn∑\ni=1\n(\n−log Pr\nθ\n(si|p,s1,...,s i−1)\n)\n.\nNow, given a model θ, an inserted canary\nc = ( p|s) where s is an n-word sequence,\nand a set R that consists of n-word\nsequences with each word sampled u.a.r.\nfrom the vocabulary, the rank of the\ncanary c can be deﬁned as rank θ(c; R) =\n|{r′∈R: Pθ(r′|p) ≤Pθ(s|p)}|. Intuitively,\nthis method captures how strongly the model\nfavors the canary as compared to random\nchance. For our experiments, we consider the\nsize of the comparison set R to be 2 ×106.\n2) Beam Search (BS) Given a preﬁx, and the\ntotal length of the phrase to be extracted, this\nmethod conducts a greedy beam search on\na model. As a result, this method functions\nwithout the knowledge of the whole canary.\nFor our experiments, we use a beam search\nwidth of ﬁve. Using this method, we evaluate\nif given a 2-word preﬁx, the canary is among\nthe top-5 most-likely 5-word continuations for\nthe model.\nRemark: This experiment is designed to over-\nestimate what an adversary might be able to learn\nin a realistic scenario. For instance, some of the\nsynthetic users participating in our training process\ncontain number of copies of a canary that is much\nhigher than what would be expected for a user in\na practical setting. In fact, for any canary with\nne = 200 , the training data of a synthetic user\ncarrying that canary contains 200 copies of the\ncanary. Moreover, if nu = 16 , there are 16 such\nsynthetic users in the training population, each of\nwhich participates at a rate 1-2 orders of magnitude\nhigher than any actual device. Even for our random\nsampling method described above, an adversary is\nassumed to have knowledge of a “guess” of the\ncanary, and the method provides conﬁdence to the\nadversary whether the canary was present in the\ntraining dataset. For the beam search method, the\nadversary is assumed to have knowledge of a two-\nword preﬁx of the ﬁve-word canary, and the method\n7\nevaluates whether the adversary can extract the\ncanary using a beam search.\nB. Empirical Results\nTable 4 summarizes the unintended memorization\nresults of a model trained for 2000 rounds using\nAlgorithm 1 on a training population with actual\ndevices and secret-sharing synthetic devices.\nnu ne Random Sampling # canaries found\n(approx. rank out of 2M) via Beam Search\n1 1 637k, 1.55M, 1.6M 0 / 3\n1 14 1.6k, 41k, 542k 0 / 3\n1 200 270k, 347k, 894k 0 / 3\n4 1 281k, 308k, 1.37M 0 / 3\n4 14 1, 16, 762 1 / 3\n4 200 263, 904, 4.9k 0 / 3\n16 1 3.7k, 112k, 129k 0 / 3\n16 14 1, 1, 1 3 / 3\n16 200 1, 1, 1 3 / 3\nTable 4: For each (nu,ne) conﬁguration, the approx-\nimate rank of the three inserted canaries via Random\nSampling, and the number of canaries (ﬁnal 3 words\ncompleted given the ﬁrst 2) in the top-5 results of\nBeam Search. The results are for a given preﬁx\nlength of two.\nFirst, we observe that all of the inserted canaries\nhaving one secret-sharing user (i.e., nu = 1 ) are\nfar from being memorized, even for the ones when\nall the examples of the user are replaced by the\ncanary ( ne = 200 ). A similar effect can be seen\nfor all the canaries having one insertion per user,\neven for the ones having 16 users sharing the\nsame canary. For four users sharing a canary and\nhaving multiple phrases replaced by the canary (i.e.,\nne ∈{4,200}), we observe that almost all of the\ninserted canaries are nearly memorized as they have\nvery low ranks via the RS method, with one being\nmemorized as it is the most-likely extraction via\nthe BS method. Lastly, all of the inserted canaries\nshared among 16 users, and having multiple phrases\nreplaced by the canary, are memorized as they have\na rank one via the RS method, and are extracted\nvia the BS method. It is important to note that\nthe participation rates of our secret-sharing users in\ntraining is 1-2 orders of magnitude higher than any\nof the actual devices. Moreover, learning a phrase\nused by a sufﬁcient number nu of users can be\ndesirable; in particular, for large enough nu this may\nbe necessary to achieve good accuracy as well as the\nfairness goal of providing good language models to\nsmaller subgroups.\nThus, our results (Table 4) demonstrate that our\nNWP models trained via DP-FedAvg exhibit very\nlow unintended memorization. In particular, we see\ncanaries start getting memorized when there are\n64.4k occurrences of the canary shared across four\nusers in the training set, whereas they get com-\npletely memorized when there are 257.6k occur-\nrences across 16 users.\nIn order to make stronger conclusions, it would\nbe desirable to run several repetitions of our experi-\nment. As indicated in Section III, running it once in-\nvolves neural network training spanning three weeks\non actual devices with limited computation power.\nThus, it is difﬁcult to conduct many repetitions of\nthe experiment.\nV. P RACTICAL CONSIDERATIONS FOR PRIVACY\nGUARANTEES\nIn this section, we delve into some practical con-\nsiderations to be taken into account while bringing\na technique from theory to practice.\nA. Proving Differential Privacy Guarantees\nTo be able to prove guarantees for Differential\nPrivacy (DP), we formally deﬁne the notion here.\nWe ﬁrst deﬁne neighboring datasets (alternatively,\ntraining populations in an FL setting). We will refer\nto a pair of training populations D,D′as neighbors\nif D′ can be obtained by the addition or removal of\none user from population D.\nDeﬁnition V .1 (Differential privacy [DMNS06],\n[DKM+06b]). A randomized algorithm Ais (ϵ,δ)-\ndifferentially private if, for any pair of neighboring\ntraining populations D and D′, and for all events\nSin the output range of A, we have\nPr[A(D) ∈S] ≤eϵ ·Pr[A(D′) ∈S] + δ\nwhere the probability is taken over the random coins\nof A.\nRemark: To relate with the evaluation in Section IV,\nsuch a user-level DP guarantee will quantify pro-\ntection against memorization of any one user’s data\n(i.e., nu = 1 ). However, extending to the case of\nnu = 16 users (e.g., via a group privacy argument\n[DR+14]) will result in a very weak protection.\n8\nFor instance, a per-user (1,10−8)-DP guarantee will\nresult in a guarantee of (16,0.53)-DP for a group\nof 16 users.\nPrivacy analysis of DP-FedAvg with ﬁxed-size fed-\nerated rounds (Algorithm 1): Following the anal-\nysis of this technique in [AMR +19], the analytical\nmoments accountant [WBK19] can be used to ob-\ntain the R ´enyi differential privacy (RDP) guarantee\nfor a federated round of computation that is based\non the subsampled Gaussian mechanism, Proposi-\ntion 1 [Mir17] for computing the RDP guarantee\nof the composition involving all the rounds, and\nProposition 3 [Mir17] to obtain a DP guarantee from\nthe composed RDP guarantee.\nThe analysis above requires several assumptions\nthat require special attention in production FL set-\ntings.\nSampling uniformly at random: For the privacy\nampliﬁcation via subsampling [Mir17], [WBK19] to\napply to Algorithm 1, it is required that this sam-\npling be uniformly at random without replacement\non each round.\nHowever, in a practical implementation, at any\nround the server only sees a small subset of the\nfull population. Pace Steering (discussed previ-\nously) intentionally limits the number of devices\nthat connect to the server to avoid overloading the\nsystem. Further, devices only check-in when they\nmeet availability criteria such as the device being\nidle, plugged in for charging, and on an unmetered\nWi-Fi network. While both of these factors are\napproximately random, the server cannot precisely\ncharacterize this randomness, and can instead only\nensure random sampling from the much smaller set\nof devices that choose to connect. Further, due to\ndynamic effects introduced by Pace Steering, it is\ndifﬁcult to precisely estimate the total population\nsize.\nIf we could ensure uniform sampling from a\nknown population size, then upper bounds on ϵand\nδ would hold [WBK19] as in Table 5 . Our best\nestimate of the actual training population size is\nN = 4 M, but for the reasons outlined here, we\nrefrain from making any speciﬁc (ϵ,δ)-DP claims\nfor the training procedure.\nSecrecy of the sample: Privacy ampliﬁcation via\nsubsampling requires that the information about\nDevice population size N ϵ\n(\nfor δ= N−1.1)\n2M 9.86\n3M 6.73\n4M 5.36\n5M 4.54\n10M 3.27\nTable 5: Hypothetical upper bounds on (ϵ,δ)-DP\nunder the unveriﬁable-in-production-FL-setting as-\nsumptions of a known population size N and uni-\nform sampling. These are computed ﬁxing δ =\nN−1.1 for the production training described in Sec-\ntion III-B, where total rounds T = 2000 , round\nparticipation fraction q = 20000 /N, and noise\nstandard deviation σ= 3.2 ×10−5.\nwhich particular users were sampled in any round of\ntraining not be accessible to any party other than the\ntrusted central aggregator. This can be challenging\nto achieve in a distributed setting. However, in\naddition to all the network trafﬁc being encrypted\non the wire, the communication channels between\nour users and the server are shared for carrying out\nvarious other tasks and analytics. Thus, it is difﬁcult\nfor any adversary, even one that is monitoring\na communication channel, to conﬁdently draw a\nconclusion about the participation of a user in our\ntraining process.\nB. Other Considerations\nApart from assumptions required for obtaining\nformal privacy guarantees, there are also few other\nconsiderations that need to be made while deploying\nsuch a distributed system.\n• Restricted access for user-to-server com-\nmunication: For a central DP guarantee in\na distributed setting, the updates communi-\ncated from each user to the server (trusted\ncentral aggregator) should be accessible only\nby the server. To ensure this, all network\ntrafﬁc is encrypted on the wire in the frame-\nwork [BEG+19] our implementation uses. This\nincludes any communication from the users to\nthe server and vice-versa.\n• Privacy cost of hyperparameter tuning: Prior\nwork [GLM+10], [CMS11], [CV13], [BST14],\n[ACG+16], [LT19] has shown that hyperpa-\nrameter tuning using sensitive data can incur\na signiﬁcant privacy cost. Thus, we perform\n9\nextensive experiments for tuning various hy-\nperparameters in our technique using publicly-\navailable language datasets so as to not affect\nthe privacy of any user participating in our\ntraining process.\nVI. C ONCLUSIONS\nThis work details the ﬁrst production next-word\nprediction (NWP) model trained using on-device\ndata while leveraging the Differentially Private Fed-\nerated Averaging technique, and an existing FL\ninfrastructure. We show that our trained NWP model\nhas superior utility than the existing baseline. Us-\ning an end-to-end measurement process, we also\nempirically demonstrate the remarkable extent to\nwhich models trained via our implementation are\nable to withstand unintended memorization. Lastly,\nwe shed light on some of the considerations to be\nmade for bringing such a technique from theory\nto a real-world implementation. Keeping practical\nconsiderations in mind, a potential novel direction to\nstrengthen the privacy guarantees of such a system\nis to incorporate techniques like random check-ins\n[BKM+20] into the training framework. We leave\nthis for future work.\nACKNOWLEDGEMENTS\nThe authors would like to specially thank Peter\nKairouz, Ananda Theertha Suresh, Kunal Talwar,\nAbhradeep Thakurta, and our colleagues in Google\nResearch for their helpful support of this work, and\ncomments towards improving the paper.\nREFERENCES\n[AAB+15] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eu-\ngene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Man ´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\nFernanda Vi ´egas, Oriol Vinyals, Pete Warden, Martin\nWattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensorﬂow.org.\n[ACG+16] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan\nMcMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\nDeep learning with differential privacy. In Proceedings\nof the 2016 Association for Computing Machinery\n(ACM) SIGSAC Conference on Computer and Com-\nmunications Security , CCS ’16, pages 308–318, New\nYork, NY , USA, 2016. Association for Computing\nMachinery (ACM).\n[ADJ19] Hilal Asi, John Duchi, and Omid Javidbakht. Element\nlevel differential privacy: The right granularity of pri-\nvacy, 2019.\n[AMR+19] Sean Augenstein, H. Brendan McMahan, Daniel Ram-\nage, Swaroop Ramaswamy, Peter Kairouz, Mingqing\nChen, Rajiv Mathews, and Blaise Ag ¨uera y Arcas.\nGenerative models for effective ML on private, decen-\ntralized datasets. CoRR, abs/1911.06679, 2019.\n[BEG+19] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,\nDzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo´e\nKiddon, Jakub Konecn ´y, Stefano Mazzocchi, H. Bren-\ndan McMahan, Timon Van Overveldt, David Petrou,\nDaniel Ramage, and Jason Roselander. Towards\nfederated learning at scale: System design. CoRR,\nabs/1902.01046, 2019.\n[BKM+20] Borja Balle, Peter Kairouz, H. Brendan McMahan,\nOm Thakkar, and Abhradeep Thakurta. Privacy ampli-\nﬁcation via random check-ins. CoRR, abs/2007.06605,\n2020.\n[BST14] Raef Bassily, Adam D. Smith, and Abhradeep\nThakurta. Private empirical risk minimization, re-\nvisited. Computing Research Repository (CoRR) ,\nabs/1405.7085, 2014.\n[CLK+18] Nicholas Carlini, Chang Liu, Jernej Kos, ´Ulfar Erlings-\nson, and Dawn Song. The secret sharer: Measuring\nunintended neural network memorization & extract-\ning secrets. Computing Research Repository (CoRR) ,\nabs/1802.08232, 2018.\n[CMS11] Kamalika Chaudhuri, Claire Monteleoni, and Anand D\nSarwate. Differentially private empirical risk min-\nimization. Journal of Machine Learning Research ,\n12(Mar):1069–1109, 2011.\n[CV13] Kamalika Chaudhuri and Staal Vinterbo. A stability-\nbased validation procedure for differentially private\nmachine learning. In Proceedings of the 26th Inter-\nnational Conference on Neural Information Processing\nSystems - Volume 2, NIPS’13, pages 2652–2660, USA,\n2013. Curran Associates Inc.\n[CWH20] Xiangyi Chen, Zhiwei Steven Wu, and Mingyi Hong.\nUnderstanding gradient clipping in private SGD: A\ngeometric perspective. CoRR, abs/2006.15429, 2020.\n[DKM+06a] Cynthia Dwork, Krishnaram Kenthapadi, Frank Mc-\nSherry, Ilya Mironov, and Moni Naor. Our data,\nourselves: Privacy via distributed noise generation. In\nEUROCRYPT, pages 486–503, 2006.\n[DKM+06b] Cynthia Dwork, Krishnaram Kenthapadi, Frank Mc-\nsherry, Ilya Mironov, and Moni Naor. Our data,\nourselves: Privacy via distributed noise generation. In\nEUROCRYPT, pages 486–503, 2006.\n[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. Calibrating noise to sensitivity in private\ndata analysis. In Theory of Cryptography Conference ,\npages 265–284. Springer, 2006.\n[DR+14] Cynthia Dwork, Aaron Roth, et al. The algorithmic\nfoundations of differential privacy. Foundations and\nTrends in Theoretical Computer Science , 9(3-4):211–\n407, 2014.\n10\n[FJR15] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.\nModel inversion attacks that exploit conﬁdence infor-\nmation and basic countermeasures. In Proceedings\nof the 22Nd Association for Computing Machinery\n(ACM) SIGSAC Conference on Computer and Commu-\nnications Security , CCS ’15, pages 1322–1333, New\nYork, NY , USA, 2015. Association for Computing\nMachinery (ACM).\n[GLM+10] Anupam Gupta, Katrina Ligett, Frank McSherry,\nAaron Roth, and Kunal Talwar. Differentially pri-\nvate combinatorial optimization. In Moses Charikar,\neditor, Proceedings of the Twenty-First Annual ACM-\nSIAM Symposium on Discrete Algorithms, SODA 2010,\nAustin, Texas, USA, January 17-19, 2010, pages 1106–\n1125. SIAM, 2010.\n[HRM+18] Andrew Hard, Kanishka Rao, Rajiv Mathews,\nFranc ¸oise Beaufays, Sean Augenstein, Hubert Eichner,\nChlo´e Kiddon, and Daniel Ramage. Federated\nlearning for mobile keyboard prediction. CoRR,\nabs/1811.03604, 2018.\n[INS+19] Roger Iyengar, Joseph P Near, Dawn Song,\nOm Thakkar, Abhradeep Thakurta, and Lun Wang.\nTowards practical differentially private convex\noptimization. In Proceedings of the 40th Institute\nof Electrical and Electronics Engineers (IEEE)\nSymposium on Security and Privacy (SP) , pages 1–18,\n2019.\n[JTT18] Prateek Jain, Om Thakkar, and Abhradeep Thakurta.\nDifferentially private matrix completion revisited. In\nProceedings of the 35th International Conference on\nMachine Learning, ICML 2018, Stockholmsm ¨assan,\nStockholm, Sweden, July 10-15, 2018 , pages 2220–\n2229, 2018.\n[KMA+19] Peter Kairouz, H. Brendan McMahan, Brendan Avent,\nAur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,\nKeith Bonawitz, Zachary Charles, Graham Cormode,\nRachel Cummings, Rafael G. L. D’Oliveira, Salim El\nRouayheb, David Evans, Josh Gardner, Zachary Gar-\nrett, Adri `a Gasc ´on, Badih Ghazi, Phillip B. Gib-\nbons, Marco Gruteser, Za ¨ıd Harchaoui, Chaoyang\nHe, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin\nHsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail\nKhodak, Jakub Konecn ´y, Aleksandra Korolova, Fari-\nnaz Koushanfar, Sanmi Koyejo, Tancr `ede Lepoint,\nYang Liu, Prateek Mittal, Mehryar Mohri, Richard\nNock, Ayfer ¨Ozg¨ur, Rasmus Pagh, Mariana Raykova,\nHang Qi, Daniel Ramage, Ramesh Raskar, Dawn\nSong, Weikang Song, Sebastian U. Stich, Ziteng Sun,\nAnanda Theertha Suresh, Florian Tram `er, Praneeth\nVepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang\nYang, Felix X. Yu, Han Yu, and Sen Zhao. Advances\nand open problems in federated learning. CoRR,\nabs/1912.04977, 2019.\n[LT19] Jingcheng Liu and Kunal Talwar. Private selection\nfrom private candidates. In Moses Charikar and Edith\nCohen, editors, Proceedings of the 51st Annual ACM\nSIGACT Symposium on Theory of Computing, STOC\n2019, Phoenix, AZ, USA, June 23-26, 2019, pages 298–\n309. ACM, 2019.\n[Mir17] I. Mironov. R ´enyi differential privacy. In 2017 Institute\nof Electrical and Electronics Engineers (IEEE) 30th\nComputer Security Foundations Symposium (CSF) ,\npages 263–275, Aug 2017.\n[MMR+17] Brendan McMahan, Eider Moore, Daniel Ram-\nage, Seth Hampson, and Blaise Ag ¨uera y Arcas.\nCommunication-efﬁcient learning of deep networks\nfrom decentralized data. In Proceedings of the 20th\nInternational Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2017, 20-22 April 2017, Fort Laud-\nerdale, FL, USA , pages 1273–1282, 2017.\n[MRTZ17] H. Brendan McMahan, Daniel Ramage, Kunal Tal-\nwar, and Li Zhang. Learning differentially private\nlanguage models without losing accuracy. CoRR,\nabs/1710.06963, 2017.\n[PAE+16] Nicolas Papernot, Mart ´ın Abadi, Ulfar Erlingsson,\nIan Goodfellow, and Kunal Talwar. Semi-supervised\nknowledge transfer for deep learning from private\ntraining data. arXiv preprint arXiv:1610.05755 , 2016.\n[PSM+18] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and ´Ulfar Erlingsson.\nScalable private learning with pate. arXiv preprint\narXiv:1802.08908, 2018.\n[SS19] Congzheng Song and Vitaly Shmatikov. Auditing data\nprovenance in text-generation models. In Ankur Tere-\ndesai, Vipin Kumar, Ying Li, R´omer Rosales, Evimaria\nTerzi, and George Karypis, editors, Proceedings of\nthe 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD 2019,\nAnchorage, AK, USA, August 4-8, 2019 , pages 196–\n206. ACM, 2019.\n[SSB14] Hasim Sak, Andrew W. Senior, and Franc ¸oise Beau-\nfays. Long short-term memory based recurrent neu-\nral network architectures for large vocabulary speech\nrecognition. CoRR, abs/1402.1128, 2014.\n[SSSS17] R. Shokri, M. Stronati, C. Song, and V . Shmatikov.\nMembership inference attacks against machine learn-\ning models. In 2017 Institute of Electrical and Elec-\ntronics Engineers (IEEE) Symposium on Security and\nPrivacy (SP), pages 3–18, May 2017.\n[STT20] Shuang Song, Om Thakkar, and Abhradeep Thakurta.\nCharacterizing private clipped gradient descent\non convex generalized linear problems. CoRR,\nabs/2006.06783, 2020.\n[TAM19] Om Thakkar, Galen Andrew, and H. Brendan McMa-\nhan. Differentially private learning with adaptive\nclipping. CoRR, abs/1905.03871, 2019.\n[TRMB20] Om Thakkar, Swaroop Ramaswamy, Rajiv Math-\news, and Franc ¸oise Beaufays. Understanding unin-\ntended memorization in federated learning. CoRR,\nabs/2006.07490, 2020.\n[WBK19] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Ka-\nsiviswanathan. Subsampled renyi differential privacy\nand analytical moments accountant. In The 22nd\nInternational Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2019, 16-18 April 2019, Naha,\nOkinawa, Japan, pages 1226–1235, 2019.\n[WFJN16] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton. A\nmethodology for formalizing model-inversion attacks.\nIn 2016 Institute of Electrical and Electronics En-\ngineers (IEEE) 29th Computer Security Foundations\nSymposium (CSF), pages 355–370, June 2016.\n[WLK+17] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri,\nSomesh Jha, and Jeffrey Naughton. Bolt-on differential\nprivacy for scalable stochastic gradient descent-based\nanalytics. In SIGMOD. Association for Computing\nMachinery (ACM), 2017.\n11\nAPPENDIX\nABLATION STUDIES\nNow, we present the results of the ablation studies\non using DP-FedAvg on the Stack Overﬂow dataset.\nTable 6 shows results from an ablation study on the\neffect of server optimizer parameters. We observe\nthat using Nestorov momentum works better than\nSGD and Adam.\nServer Optimizer params Top-1 Recall [%]\nAdam, ηs = 1×10−5 4.73\nAdam, ηs = 5×10−5 15.49\nAdam, ηs = 1×10−4 19.78\nAdam, ηs = 2×10−4 21.92\nAdam, ηs = 5×10−4 23.38\nMomentum, ηs = 0.5,µ = 0.9 23.03\nMomentum, ηs = 1.0,µ = 0.9 23.69\nMomentum, ηs = 0.5,µ = 0.99 24.16\nMomentum, ηs = 1.0,µ = 0.99 24.15\nSGD, ηs = 0.5 18.49\nSGD, ηs = 0.7 19.52\nSGD, ηs = 1.0 20.41\nTable 6: Ablation study on server optimizer param-\neters. Hyperparameters used for training the model\non real devices are highlighted in bold.\nTable 7 shows results from another ablation study\non the effect of various batch sizes and learning\nrates on the client. Batch sizes and learning rates\non the client don’t seem to have a large impact on\nperformance, with batch sizes from |b|= 5 to |b|=\n50 demonstrating similar performance.\nClient optimizer params Top-1 recall [%]\n|b|= 5,ηc = 0.1 23.92\n|b|= 5,ηc = 0.5 24.03\n|b|= 10,ηc = 0.2 24.03\n|b|= 10,ηc = 0.5 23.96\n|b|= 20,ηc = 0.3 24.00\n|b|= 20,ηc = 0.5 24.03\n|b| = 50,ηc = 0.5 24.15\nTable 7: Ablation study on client optimizer param-\neters. Hyperparameters used for training the model\non real devices are highlighted in bold.\nTable 8 shows results from an ablation study\non various clipping values used for clipping the\nuser updates. Figure 1 shows the percentage of\nclients clipped across the duration of training, for\ndifferent values of the clipping norm. We observe\nthat clipping a large fraction of clients works better.\nBelow a certain value ( S = 0.2 in this case), almost\nall the clients get clipped, and further clipping is\nequivalent to decreasing the server learning rate.\nClipping norm Top-1 recall [%]\nS = 0.1 23.78\nS = 0.2 23.97\nS = 0.5 24.09\nS= 0.8 24.15\nS = 1.0 24.12\nS = 1.5 23.81\nS = 2.0 23.45\nTable 8: Ablation study on clipping norm values.\nHyperparameters used for training the model on real\ndevices are highlighted in bold.\nFig. 1: % of clients clipped vs. round for different\nvalues of clipping norm ( S).\nThese ablation studies are not meant to serve as\nan extensive sweep of the hyperparameters. These\nare presented demonstrate that it’s feasible to tune\nhyperparameters for DP-FedAvg on a public corpus\nand avoid incurring any additional privacy cost.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7786414623260498
    },
    {
      "name": "Memorization",
      "score": 0.6279592514038086
    },
    {
      "name": "Software deployment",
      "score": 0.5517855286598206
    },
    {
      "name": "Production (economics)",
      "score": 0.5102434158325195
    },
    {
      "name": "Sample (material)",
      "score": 0.48544055223464966
    },
    {
      "name": "Machine learning",
      "score": 0.4051763415336609
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39815571904182434
    },
    {
      "name": "Software engineering",
      "score": 0.13396412134170532
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 16
}