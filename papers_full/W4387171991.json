{
  "title": "Revision Transformers: Instructing Language Models to Change Their Values",
  "url": "https://openalex.org/W4387171991",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2106763651",
      "name": "Felix Friedrich",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A3000251925",
      "name": "Wolfgang Stammer",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2789883686",
      "name": "Patrick Schramowski",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2252032993",
      "name": "Kristian Kersting",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W4365211621",
    "https://openalex.org/W3174266714",
    "https://openalex.org/W3099072156",
    "https://openalex.org/W4286989287",
    "https://openalex.org/W4323659988",
    "https://openalex.org/W4320516905",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W3079786700",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W4285484119",
    "https://openalex.org/W4302306219",
    "https://openalex.org/W4299830158",
    "https://openalex.org/W1513302538",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4221154619",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4308244210",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2595017202",
    "https://openalex.org/W3080855795",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3035989815",
    "https://openalex.org/W4221142811",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W2958514452",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4283172096",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W4206256378",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W3048549109",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385567201",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W4296634084",
    "https://openalex.org/W4287887283"
  ],
  "abstract": "Current transformer language models (LM) are large-scale models with billions of parameters. They have been shown to provide high performances on a variety of tasks but are also prone to shortcut learning and bias. Addressing such incorrect model behavior via parameter adjustments is very costly. This is particularly problematic for updating dynamic concepts, such as moral values, which vary culturally or interpersonally. In this work, we question the current common practice of storing all information in the model parameters and propose the Revision Transformer (RiT) to facilitate easy model updating. The specific combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine makes it possible to update the modelâ€™s knowledge with little effort and the help of user interaction. We exemplify RiT on a moral dataset and simulate user feedback demonstrating strong performance in model revision even with small data. This way, users can easily design a model regarding their preferences, paving the way for more transparent AI models.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7939735651016235
    },
    {
      "name": "Computer science",
      "score": 0.7062832117080688
    },
    {
      "name": "Language model",
      "score": 0.5920406579971313
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.48602014780044556
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43564337491989136
    },
    {
      "name": "Machine learning",
      "score": 0.42299243807792664
    },
    {
      "name": "Engineering",
      "score": 0.16160506010055542
    },
    {
      "name": "Electrical engineering",
      "score": 0.07927489280700684
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    }
  ]
}