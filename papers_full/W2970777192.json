{
    "title": "Adaptively Sparse Transformers",
    "url": "https://openalex.org/W2970777192",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2947308987",
            "name": "Gonçalo M. Correia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1894650153",
            "name": "Vlad Niculae",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121178374",
            "name": "André F. T. Martins",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2888539709",
        "https://openalex.org/W2952576443",
        "https://openalex.org/W1983874169",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2963502387",
        "https://openalex.org/W2962839844",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2950858167",
        "https://openalex.org/W2963123301",
        "https://openalex.org/W2963970238",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W2912206855",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1551360398",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2805493160",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W2921569601",
        "https://openalex.org/W2912351236",
        "https://openalex.org/W2859444450",
        "https://openalex.org/W4301496368",
        "https://openalex.org/W2017697298",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W2946475618",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2962943802",
        "https://openalex.org/W2253795368",
        "https://openalex.org/W2963828549",
        "https://openalex.org/W2963062480",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2251994258",
        "https://openalex.org/W3005389111",
        "https://openalex.org/W2619122421",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2505728881",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2962822108",
        "https://openalex.org/W2940744433"
    ],
    "abstract": "Gonçalo M. Correia, Vlad Niculae, André F. T. Martins. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 2174–2184,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n2174\nAdaptively Sparse T ransformers\nGonc ¸alo M. Correia/Leo\ngoncalo.correia@lx.it.pt\nVlad Niculae /Leo\nvlad@vene.ro\n/LeoInstituto de T elecomunicac ¸˜oes, Lisbon, Portugal\n/CancerUnbabel, Lisbon, Portugal\nAndr´e F .T . Martins/Leo /Cancer\nandre.martins@unbabel.com\nAbstract\nAttention mechanisms have become ubiqui-\ntous in NLP . Recent architectures, notably\nthe Transformer, learn powerful context-aware\nword representations through layered, multi-\nheaded attention. The multiple heads learn\ndiverse types of word relationships. How-\never, with standard softmax attention, all at-\ntention heads are dense, assigning a non-zero\nweight to all context words. In this work, we\nintroduce the adaptively sparse Transformer,\nwherein attention heads have ﬂexible, context-\ndependent sparsity patterns. This sparsity is\naccomplished by replacing softmax with α-\nentmax: a differentiable generalization of soft-\nmax that allows low-scoring words to receive\nprecisely zero weight. Moreover, we derive a\nmethod to automatically learn the α parameter\n– which controls the shape and sparsity of α-\nentmax – allowing attention heads to choose\nbetween focused or spread-out behavior. Our\nadaptively sparse Transformer improves inter-\npretability and head diversity when compared\nto softmax Transformers on machine transla-\ntion datasets. Findings of the quantitative and\nqualitative analysis of our approach include\nthat heads in different layers learn different\nsparsity preferences and tend to be more di-\nverse in their attention distributions than soft-\nmax Transformers. Furthermore, at no cost in\naccuracy , sparsity in attention heads helps to\nuncover different head specializations.\n1 Introduction\nThe Transformer architecture ( V aswani et al. , 2017)\nfor deep neural networks has quickly risen to promi-\nnence in NLP through its efﬁciency and perfor-\nmance, leading to improvements in the state of the\nart of Neural Machine Translation (NMT; Junczys-\nDowmunt et al. , 2018; Ott et al. , 2018), as well as\ninspiring other powerful general-purpose models\nlike BERT (\nDevlin et al. , 2019) and GPT -2 (Rad-\nford et al. , 2019). At the heart of the Transformer\nThequickbrownfox jumpsover Thequickbrownfox jumpsover Thequickbrownfox jumpsover\nhead 1\nhead 4\nhead 3\nhead 2\nSparse Transformer Adaptive Span \nTransformer\nAdaptively Sparse \nTransformer (Ours)\nFigure 1: Attention distributions of different self-\nattention heads for the time step of the token “over”,\nshown to compare our model to other related work.\nWhile the sparse Transformer (\nChild et al. , 2019) and\nthe adaptive span Transformer ( Sukhbaatar et al. , 2019)\nonly attend to words within a contiguous span of the\npast tokens, our model is not only able to obtain differ-\nent and not necessarily contiguous sparsity patterns for\neach attention head, but is also able to tune its support\nover which tokens to attend adaptively .\nlie multi-head attention mechanisms: each word\nis represented by multiple different weighted aver-\nages of its relevant context. As suggested by recent\nworks on interpreting attention head roles, sepa-\nrate attention heads may learn to look for various\nrelationships between tokens ( T ang et al. , 2018; Ra-\nganato and Tiedemann , 2018; Mareˇcek and Rosa ,\n2018; T enney et al. , 2019; V oita et al. , 2019).\nThe attention distribution of each head is pre-\ndicted typically using the softmax normalizing\ntransform. As a result, all context words have\nnon-zero attention weight. Recent work on sin-\ngle attention architectures suggest that using sparse\nnormalizing transforms in attention mechanisms\nsuch as sparsemax – which can yield exactly zero\nprobabilities for irrelevant words – may improve\nperformance and interpretability (\nMalaviya et al. ,\n2018; Deng et al. , 2018; Peters et al. , 2019). Qual-\nitative analysis of attention heads ( V aswani et al. ,\n2017, Figure 5) suggests that, depending on what\nphenomena they capture, heads tend to favor ﬂatter\nor more peaked distributions.\nRecent works have proposed sparse Transform-\n2175\ners ( Child et al. , 2019) and adaptive span Trans-\nformers ( Sukhbaatar et al. , 2019). However, the\n“sparsity” of those models only limits the attention\nto a contiguous span of past tokens, while in this\nwork we propose a highly adaptive Transformer\nmodel that is capable of attending to a sparse set of\nwords that are not necessarily contiguous. Figure 1\nshows the relationship of these methods with ours.\nOur contributions are the following:\n• W e introduce sparse attention into the Trans-\nformer architecture, showing that it eases inter-\npretability and leads to slight accuracy gains.\n• W e propose an adaptive version of sparse at-\ntention, where the shape of each attention\nhead is\nlearnable and can vary continuously\nand dynamically between the dense limit case\nof softmax and the sparse, piecewise-linear\nsparsemax case.1\n• W e make an extensive analysis of the added\ninterpretability of these models, identifying\nboth crisper examples of attention head behav-\nior observed in previous work, as well as novel\nbehaviors unraveled thanks to the sparsity and\nadaptivity of our proposed model.\n2 Background\n2.1 The T ransformer\nIn NMT , the Transformer ( V aswani et al. , 2017)\nis a sequence-to-sequence (seq2seq) model which\nmaps an input sequence to an output sequence\nthrough hierarchical\nmulti-head attention mech-\nanisms, yielding a dynamic, context-dependent\nstrategy for propagating information within and\nacross sentences. It contrasts with previous seq2seq\nmodels, which usually rely either on costly gated\nrecurrent operations (often LSTMs:\nBahdanau\net al. , 2015; Luong et al. , 2015) or static convo-\nlutions ( Gehring et al. , 2017).\nGiven n query contexts and m sequence items\nunder consideration, attention mechanisms com-\npute, for each query , a weighted representation of\nthe items. The particular attention mechanism used\nin\nV aswani et al. (2017) is called scaled dot-product\nattention, and it is computed in the following way:\nAtt(Q, K, V ) =π\n(QK⊤\n√\nd\n)\nV , (1)\n1 Code and pip package available at https://github.\ncom/deep-spin/entmax.\nwhere Q ∈ Rn×d contains representations of the\nqueries, K, V ∈ Rm×d are the keys and values\nof the items attended over, and d is the dimen-\nsionality of these representations. The π mapping\nnormalizes row-wise using softmax, π (Z)ij =\nsoftmax(zi)j, where\nsoftmax(z) = exp(zj)∑\nj′ exp(zj′ ). (2)\nIn words, the keys are used to compute a relevance\nscore between each item and query . Then, normal-\nized attention weights are computed using softmax,\nand these are used to weight the values of each item\nat each query context.\nHowever, for complex tasks, different parts of a\nsequence may be relevant in different ways, moti-\nvating multi-head attention in Transformers. This\nis simply the application of Equation 1 in paral-\nlel H times, each with a different, learned linear\ntransformation that allows specialization:\nHeadi(Q, K, V )= Att(QW Q\ni , KW K\ni , V W V\ni ) (3)\nIn the Transformer, there are three separate multi-\nhead attention mechanisms for distinct purposes:\n• Encoder self-attention: builds rich, layered\nrepresentations of each input word, by attend-\ning on the entire input sentence.\n• Context attention: selects a representative\nweighted average of the encodings of the input\nwords, at each time step of the decoder.\n• Decoder self-attention: attends over the par-\ntial output sentence fragment produced so far.\nT ogether, these mechanisms enable the contextual-\nized ﬂow of information between the input sentence\nand the sequential decoder.\n2.2 Sparse Attention\nThe softmax mapping (Equation 2) is elementwise\nproportional to exp, therefore it can never assign a\nweight of exactly zero . Thus, unnecessary items\nare still taken into consideration to some extent.\nSince its output sums to one, this invariably means\nless weight is assigned to the relevant items, po-\ntentially harming performance and interpretabil-\nity ( Jain and W allace , 2019). This has motivated a\nline of research on learning networks with sparse\nmappings ( Martins and Astudillo , 2016; Niculae\nand Blondel , 2017; Louizos et al. , 2018; Shao et al. ,\n2176\n2019). W e focus on a recently-introduced ﬂexible\nfamily of transformations, α-entmax ( Blondel et al. ,\n2019; Peters et al. , 2019), deﬁned as:\nα-entmax(z) := argmax\np∈△d\np⊤z + HT\nα(p), (4)\nwhere △d := {p ∈ Rd : ∑\ni pi = 1} is the prob-\nability simplex, and, for α ≥ 1, HT\nα is the Tsallis\ncontinuous family of entropies ( Tsallis, 1988):\nHT\nα(p):=\n{ 1\nα(α−1)\n∑\nj\n(\npj − pα\nj\n)\n, α ̸= 1,\n− ∑\nj pj log pj, α = 1.\n(5)\nThis family contains the well-known Shannon and\nGini entropies, corresponding to the cases α = 1\nand α = 2, respectively .\nEquation 4 involves a convex optimization sub-\nproblem. Using the deﬁnition of HT\nα, the optimality\nconditions may be used to derive the following\nform for the solution (Appendix B.2):\nα-entmax(z) = [(α − 1)z − τ1]\n1/ α−1\n+ , (6)\nwhere [·]+ is the positive part (ReLU) function,\n1 denotes the vector of all ones, and τ – which\nacts like a threshold – is the Lagrange multiplier\ncorresponding to the ∑\ni pi = 1constraint.\nProperties of α -entmax. The appeal of α-\nentmax for attention rests on the following prop-\nerties. For\nα = 1 (i. e., when HT\nα becomes the\nShannon entropy), it exactly recovers the softmax\nmapping (W e provide a short derivation in Ap-\npendix B.3.). For all α > 1 it permits sparse solu-\ntions, in stark contrast to softmax. In particular, for\nα = 2, it recovers the sparsemax mapping ( Martins\nand Astudillo , 2016), which is piecewise linear. In-\nbetween, as α increases, the mapping continuously\ngets sparser as its curvature changes.\nT o compute the value of α-entmax, one must\nﬁnd the threshold τ such that the r . h .s.in Equa-\ntion 6 sums to one. Blondel et al. (2019) propose\na general bisection algorithm. Peters et al. (2019)\nintroduce a faster, exact algorithm for α = 1. 5, and\nenable using α-entmax with ﬁxed α within a neu-\nral network by showing that the α-entmax Jacobian\nw . r .t.z for p⋆ = α-entmax(z) is\n∂ α-entmax(z)\n∂z = diag(s) − 1∑\nj sj\nss⊤,\nwhere si =\n{\n(p⋆\ni )2−α, p ⋆\ni > 0,\n0, p ⋆\ni = 0.\n(7)\nOur work furthers the study of α-entmax by\nproviding a derivation of the Jacobian w. r .t.the\nhyper-parameter α (Section 3), thereby allowing\nthe shape and sparsity of the mapping to be learned\nautomatically . This is particularly appealing in the\ncontext of multi-head attention mechanisms, where\nwe shall show in Section 5.1 that different heads\ntend to learn different sparsity behaviors.\n3 Adaptively Sparse T ransformers\nwith α -entmax\nW e now propose a novel Transformer architecture\nwherein we simply replace softmax with α-entmax\nin the attention heads. Concretely , we replace the\nrow normalization π in Equation 1 by\nπ (Z)ij = α-entmax(zi)j (8)\nThis change leads to sparse attention weights, as\nlong as α > 1; in particular, α = 1. 5 is a sensible\nstarting point ( Peters et al. , 2019).\nDifferent α per head. Unlike LSTM-based\nseq2seq models, where α can be more easily tuned\nby grid search, in a Transformer, there are many\nattention heads in multiple layers. Crucial to the\npower of such models, the different heads capture\ndifferent linguistic phenomena, some of them iso-\nlating important words, others spreading out atten-\ntion across phrases ( V aswani et al. , 2017, Figure 5).\nThis motivates using different, adaptive α values\nfor each attention head, such that some heads may\nlearn to be sparser, and others may become closer\nto softmax. W e propose doing so by treating the α\nvalues as neural network parameters, optimized via\nstochastic gradients along with the other weights.\nDerivatives w. r .t.α .\nIn order to optimize α au-\ntomatically via gradient methods, we must com-\npute the Jacobian of the entmax output w . r .t.\nα.\nSince entmax is deﬁned through an optimization\nproblem, this is non-trivial and cannot be simply\nhandled through automatic differentiation; it falls\nwithin the domain of argmin differentiation, an ac-\ntive research topic in optimization ( Gould et al. ,\n2016; Amos and Kolter , 2017).\nOne of our key contributions is the derivation\nof a closed-form expression for this Jacobian. The\nnext proposition provides such an expression, en-\nabling entmax layers with adaptive α. T o the best\nof our knowledge, ours is the ﬁrst neural network\nmodule that can automatically , continuously vary\n2177\nin shape away from softmax and toward sparse\nmappings like sparsemax.\nProposition 1.\nLet p⋆ := α-entmax(z) be the so-\nlution of Equation 4. Denote the distribution ˜pi :=\n(p⋆\ni )2−α\n/ ∑\nj (p⋆\nj )2−α\nand let hi := −p⋆\ni log p⋆\ni\n. The ith\ncomponent of the Jacobian g := ∂ α-entmax(z)\n∂α is\ngi =\n\n\n\np⋆\ni −˜pi\n(α−1)2 +\nhi−˜pi\n∑\nj hj\nα−1 , α > 1,\nhi log p⋆\ni −p⋆\ni\n∑\nj hj log p⋆\nj\n2 , α = 1.\n(9)\nThe proof uses implicit function differentiation and\nis given in Appendix C.\nProposition 1 provides the remaining missing\npiece needed for training adaptively sparse Trans-\nformers. In the following section, we evaluate this\nstrategy on neural machine translation, and analyze\nthe behavior of the learned attention heads.\n4 Experiments\nW e apply our adaptively sparse Transformers on\nfour machine translation tasks. For comparison,\na natural baseline is the standard Transformer ar-\nchitecture using the softmax transform in its multi-\nhead attention mechanisms. W e consider two other\nmodel variants in our experiments that make use of\ndifferent normalizing transformations:\n• 1.5-entmax: a Transformer with sparse ent-\nmax attention with ﬁxed α = 1. 5 for all heads.\nThis is a novel model, since 1.5-entmax had\nonly been proposed for RNN-based NMT\nmodels (\nPeters et al. , 2019), but never in\nTransformers, where attention modules are\nnot just one single component of the seq2seq\nmodel but rather an integral part of all of the\nmodel components.\n• α -entmax: an adaptive Transformer with\nsparse entmax attention with a different,\nlearned αt\ni,j for each head.\nThe adaptive model has an additional scalar pa-\nrameter per attention head per layer for each of the\nthree attention mechanisms (encoder self-attention,\ncontext attention, and decoder self-attention), i. e.,\n{\nat\ni,j ∈ R : i ∈ { 1, . . . , L }, j ∈ { 1, . . . , H },\nt ∈ { enc, ctx, dec}\n}\n,\n(10)\nand we set αt\ni,j = 1 +sigmoid(at\ni,j ) ∈]1, 2[\n. All or\nsome of the α values can be tied if desired, but we\nkeep them independent for analysis purposes.\nDatasets. Our models were trained on 4 machine\ntranslation datasets of different training sizes:\n• IWSL T 2017 German → English ( D E/shortrightarrowE N, Cet-\ntolo et al. , 2017): 200K sentence pairs.\n• KFTT Japanese → English ( JA/shortrightarrowE N, Neubig,\n2011): 300K sentence pairs.\n• WMT 2016 Romanian → English ( RO/shortrightarrowE N, Bo-\njar et al. , 2016): 600K sentence pairs.\n• WMT 2014 English → German ( E N/shortrightarrowD E, Bojar\net al. , 2014): 4.5M sentence pairs.\nAll of these datasets were preprocessed with\nbyte-pair encoding (BPE; Sennrich et al. , 2016),\nusing joint segmentations of 32k merge operations.\nT raining. W e follow the dimensions of the\nTransformer-Base model of V aswani et al. (2017):\nThe number of layers is L = 6 and number of\nheads is H = 8in the encoder self-attention, the\ncontext attention, and the decoder self-attention.\nW e use a mini-batch size of 8192 tokens and warm\nup the learning rate linearly until 20k steps, after\nwhich it decays according to an inverse square root\nschedule. All models were trained until conver-\ngence of validation accuracy , and evaluation was\ndone at each 10k steps for RO\n/shortrightarrowE N and E N/shortrightarrowD E\nand at each 5k steps for D E/shortrightarrowE N and JA/shortrightarrowE N. The\nend-to-end computational overhead of our methods,\nwhen compared to standard softmax, is relatively\nsmall; in training tokens per second, the models\nusing\nα-entmax and 1. 5-entmax are, respectively ,\n75% and 90% the speed of the softmax model.\nResults. W e report test set tokenized BLEU ( Pa-\npineni et al. , 2002) results in T able 1. W e can see\nthat replacing softmax by entmax does not hurt\nperformance in any of the datasets; indeed, sparse\nattention Transformers tend to have slightly higher\nBLEU, but their sparsity leads to a better poten-\ntial for analysis. In the next section, we make use\nof this potential by exploring the learned internal\nmechanics of the self-attention heads.\n5 Analysis\nW e conduct an analysis for the higher-resource\ndataset WMT 2014 English → German of the at-\ntention in the sparse adaptive Transformer model\n(α-entmax) at multiple levels: we analyze high-\nlevel statistics as well as individual head behavior.\nMoreover, we make a qualitative analysis of the\ninterpretability capabilities of our models.\n2178\nactivation D E/shortrightarrowE N JA /shortrightarrowE N RO /shortrightarrowE N E N /shortrightarrowD E\nsoftmax 29.79 21.57 32.70 26.02\n1. 5-entmax 29.83 22.13 33.10 25.89\nα-entmax 29.90 21.74 32.89 26.93\nT able 1: Machine translation tokenized BLEU test results on IWSL T 2017 D E/shortrightarrowE N, KFTT JA/shortrightarrowE N, WMT 2016\nRO/shortrightarrowE N and WMT 2014 E N/shortrightarrowD E, respectively .\n5.1 High-Level Statistics\nWhat kind of α values are learned?\nFigure 2\nshows the learning trajectories of the α parameters\nof a selected subset of heads. W e generally observe\na tendency for the randomly-initialized α parame-\nters to decrease initially , suggesting that softmax-\nlike behavior may be preferable while the model\nis still very uncertain. After around one thousand\nsteps, some heads change direction and become\nsparser, perhaps as they become more conﬁdent\nand specialized. This shows that the initialization\nof α does not predetermine its sparsity level or the\nrole the head will have throughout. In particular,\nhead\n8 in the encoder self-attention layer 2 ﬁrst\ndrops to around α = 1. 3 before becoming one of\nthe sparsest heads, with α ≈ 2.\nThe overall distribution of α values at conver-\ngence can be seen in Figure 3. W e can observe\nthat the encoder self-attention blocks learn to con-\ncentrate the α values in two modes: a very sparse\none around α → 2, and a dense one between soft-\nmax and 1.5-entmax. However, the decoder self\nand context attention only learn to distribute these\nparameters in a single mode. W e show next that\nthis is reﬂected in the average density of attention\nweight vectors as well.\nAttention weight density when translating.\nFor any α > 1, it would still be possible for the\nweight matrices in Equation 3 to learn re-scalings\nso as to make attention sparser or denser. T o visu-\nalize the impact of adaptive α values, we compare\nthe empirical attention weight density (the aver-\nage number of tokens receiving non-zero attention)\nwithin each module, against sparse Transformers\nwith ﬁxed α = 1. 5.\nFigure 4 shows that, with ﬁxed α = 1. 5, heads\ntend to be sparse and similarly-distributed in all\nthree attention modules. With learned α, there are\ntwo notable changes: (i) a prominent mode corre-\nsponding to fully dense probabilities, showing that\nour models learn to combine sparse and dense atten-\ntion, and (ii) a distinction between the encoder self-\n0\n 2000\n 4000\n 6000\n 8000\n 10000\n 12000\ntraining steps\n1.0\n1.2\n1.4\n1.6\n1.8\ndecoder, layer 1, head 8\nencoder, layer 1, head 3\nencoder, layer 1, head 4\nencoder, layer 2, head 8\nencoder, layer 6, head 2\nFigure 2: Trajectories of α values for a subset of\nthe heads during training. Initialized at random, most\nheads become denser in the beginning, before converg-\ning. This suggests that dense attention may be more\nbeneﬁcial while the network is still uncertain, being re-\nplaced by sparse attention afterwards.\nattention – whose background distribution tends\ntoward extreme sparsity – and the other two mod-\nules, who exhibit more uniform background distri-\nbutions. This suggests that perhaps entirely sparse\nTransformers are suboptimal.\nThe fact that the decoder seems to prefer denser\nattention distributions might be attributed to it be-\ning auto-regressive, only having access to past to-\nkens and not the full sentence. W e speculate that\nit might lose too much information if it assigned\nweights of zero to too many tokens in the self-\nattention, since there are fewer tokens to attend to\nin the ﬁrst place.\nT easing this down into separate layers, Figure 5\nshows the average (sorted) density of each head for\neach layer. W e observe that α-entmax is able to\nlearn different sparsity patterns at each layer, lead-\ning to more variance in individual head behavior, to\nclearly-identiﬁed dense and sparse heads, and over-\nall to different tendencies compared to the ﬁxed\ncase of α = 1. 5.\nHead diversity . T o measure the overall disagree-\nment between attention heads, as a measure of head\n2179\n0\n10\n20\nEncoder\nSelf-Attention\n0\n10\n20\nContext\nAttention\n1.0\n 1.2\n 1.4\n 1.6\n 1.8\n 2.0\n0\n10\n20\nDecoder\nSelf-Attention\nFigure 3: Distribution of learned α values per attention\nblock. While the encoder self-attention has a bimodal\ndistribution of values of α, the decoder self-attention\nand context attention have a single mode.\ndiversity , we use the following generalization of\nthe Jensen-Shannon divergence:\nJS = HS\n\n 1\nH\nH∑\nj=1\npj\n\n − 1\nH\nH∑\nj=1\nHS (pj) (11)\nwhere pj is the vector of attention weights as-\nsigned by head j to each word in the sequence, and\nHS is the Shannon entropy , base-adjusted based on\nthe dimension of p such that JS ≤ 1. W e average\nthis measure over the entire validation set. The\nhigher this metric is, the more the heads are taking\ndifferent roles in the model.\nFigure 6 shows that both sparse Transformer\nvariants show more diversity than the traditional\nsoftmax one. Interestingly , diversity seems to peak\nin the middle layers of the encoder self-attention\nand context attention, while this is not the case for\nthe decoder self-attention.\nThe statistics shown in this section can be found\nfor the other language pairs in Appendix A.\n5.2 Identifying Head Specializations\nPrevious work pointed out some speciﬁc roles\nplayed by different heads in the softmax Trans-\nformer model ( V oita et al. , 2018; T ang et al. , 2018;\nV oita et al. , 2019). Identifying the specialization of\na head can be done by observing the type of tokens\n0.0\n 0.5\n 1.0\n0\n10k\n30k\n50k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0\n 0.5\n 1.0\n-entmax\n0.0\n 0.5\n 1.0\n0\n10k\n30k\n50k\nContext\nAttention\n0.0\n 0.5\n 1.0\n0.0\n 0.5\n 1.0\ndensity\n0\n10k\n30k\n50k\nDecoder\nSelf-Attention\n0.0\n 0.5\n 1.0\ndensity\nFigure 4: Distribution of attention densities (average\nnumber of tokens receiving non-zero attention weight)\nfor all attention heads and all validation sentences.\nWhen compared to 1.5-entmax, α-entmax distributes\nthe sparsity in a more uniform manner, with a clear\nmode at fully dense attentions, corresponding to the\nheads with low α. In the softmax case, this distribution\nwould lead to a single bar with density 1.\nor sequences that the head often assigns most of its\nattention weight; this is facilitated by sparsity .\nPositional heads.\nOne particular type of head, as\nnoted by V oita et al. (2019), is the positional head.\nThese heads tend to focus their attention on either\nthe previous or next token in the sequence, thus\nobtaining representations of the neighborhood of\nthe current time step. In Figure\n7, we show atten-\ntion plots for such heads, found for each of the\nstudied models. The sparsity of our models allows\nthese heads to be more conﬁdent in their represen-\ntations, by assigning the whole probability distribu-\ntion to a single token in the sequence. Concretely ,\nwe may measure a positional head’s conﬁdence as\nthe average attention weight assigned to the pre-\nvious token. The softmax model has three heads\nfor position −1, with median conﬁdence 93. 5%.\nThe 1. 5-entmax model also has three heads for\nthis position, with median conﬁdence 94. 4%. The\nadaptive model has four heads, with median con-\nﬁdences 95. 9%, the lowest-conﬁdence head being\ndense with α = 1. 18, while the highest-conﬁdence\nhead being sparse ( α = 1. 91).\n2180\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1\n 2\n 3\n 4\n 5\n 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1\n 2\n 3\n 4\n 5\n 6\nLayers\nFigure 5: Head density per layer for ﬁxed and learned\nα. Each line corresponds to an attention head; lower\nvalues mean that that attention head is sparser. Learned\nα has higher variance.\nFor position +1, the models each dedicate one\nhead, with conﬁdence around 95%, slightly higher\nfor entmax. The adaptive model sets α = 1. 96 for\nthis head.\nBPE-merging head.\nDue to the sparsity of our\nmodels, we are able to identify other head special-\nizations, easily identifying which heads should be\nfurther analysed. In Figure 8 we show one such\nhead where the α value is particularly high (in the\nencoder, layer 1, head 4 depicted in Figure 2). W e\nfound that this head most often looks at the cur-\nrent time step with high conﬁdence, making it a\npositional head with offset\n0. However, this head\noften spreads weight sparsely over 2-3 neighbor-\ning tokens, when the tokens are part of the same\nBPE cluster\n2 or hyphenated words. As this head\nis in the ﬁrst layer, it provides a useful service to\nthe higher layers by combining information evenly\nwithin some BPE clusters.\nFor each BPE cluster or cluster of hyphenated\nwords, we computed a score between 0 and 1 that\ncorresponds to the maximum attention mass as-\nsigned by any token to the rest of the tokens inside\nthe cluster in order to quantify the BPE-merging\n2 BPE-segmented words are denoted by ∼ in the ﬁgures.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.20\n0.25\n0.30\n0.35\nContext\nAttention\n1\n 2\n 3\n 4\n 5\n 6\nLayers\n0.25\n0.30\n0.35\nDecoder\nSelf-Attention\nFigure 6: Jensen-Shannon Divergence between heads\nat each layer. Measures the disagreement between\nheads: the higher the value, the more the heads are dis-\nagreeing with each other in terms of where to attend.\nModels using sparse entmax have more diverse atten-\ntion than the softmax baseline.\ncapabilities of these heads. 3 There are not any at-\ntention heads in the softmax model that are able\nto obtain a score over\n80%, while for 1. 5-entmax\nand α-entmax there are two heads in each ( 83. 3%\nand 85. 6% for 1. 5-entmax and 88. 5% and 89. 8%\nfor α-entmax).\nInterrogation head. On the other hand, in Fig-\nure 9 we show a head for which our adaptively\nsparse model chose an α close to 1, making it\ncloser to softmax (also shown in encoder , layer 1,\nhead 3 depicted in Figure 2). W e observe that this\nhead assigns a high probability to question marks\nat the end of the sentence in time steps where the\ncurrent token is interrogative, thus making it an\ninterrogation-detecting head. W e also observe this\ntype of heads in the other models, which we also\ndepict in Figure\n9. The average attention weight\nplaced on the question mark when the current to-\nken is an interrogative word is 98. 5% for softmax,\n97. 0% for 1. 5-entmax, and 99. 5% for α-entmax.\nFurthermore, we can examine sentences where\nsome tendentially sparse heads become less so, thus\nidentifying sources of ambiguity where the head\n3 If the cluster has size 1, the score is the weight the token\nassigns to itself.\n2181\nwe\n weren\n 't\n far\n away\n last\n season\n .\nsoftmax\nwe\nweren\n't\nfar\naway\nlast\nseason\n.\nwe\n weren\n 't\n far\n away\n last\n season\n .\n1.5-entmax\nwe\n weren\n 't\n far\n away\n last\n season\n .\n-entmax\nFigure 7: Self-attention from the most conﬁdently\nprevious-position head in each model. The learned pa-\nrameter in the α-entmax model is α = 1. 91. Quanti-\ntatively more conﬁdent, visual inspection conﬁrms that\nthe adaptive head behaves more consistently .\nrules\n for\n blo~\n wing\n up\n bal~\n lo~\n ons\n ,\n for\n bananas\n and\n a\n cir~\n cus\nrules\nfor\nblo~\nwing\nup\nbal~\nlo~\nons\n,\nfor\nbananas\nand\na\ncir~\ncus\none\n -\n two\n -\n three\n -\n four\n .\none\n-\ntwo\n-\nthree\n-\nfour\n.\nare\n you\n not\n confir~\n ming\n this\n with\n what\n you\n have\n stated\n ?\nare\nyou\nnot\nconfir~\nming\nthis\nwith\nwhat\nyou\nhave\nstated\n?\nthis\n could\n come\n back\n to\n ha~\n unt\n them\n .\nthis\ncould\ncome\nback\nto\nha~\nunt\nthem\n.\nFigure 8: BPE-merging head (α = 1. 91) discovered\nin the α-entmax model. Found in the ﬁrst encoder\nlayer, this head learns to discover some subword units\nand combine their information, leaving most words in-\ntact. It places 99. 09% of its probability mass within the\nsame BPE cluster as the current token: more than any\nhead in any other model.\nis less conﬁdent in its prediction. An example is\nshown in Figure 10 where sparsity in the same head\ndiffers for sentences of similar length.\n6 Related W ork\nSparse attention.\nPrior work has developed\nsparse attention mechanisms, including appli-\ncations to NMT (\nMartins and Astudillo , 2016;\nMalaviya et al. , 2018; Niculae and Blondel , 2017;\nShao et al. , 2019; Maruf et al. , 2019). Peters et al.\n(2019) introduced the entmax function this work\nbuilds upon. In their work, there is a single atten-\ntion mechanism which is controlled by a ﬁxed\nα.\nIn contrast, this is the ﬁrst work to allow such atten-\nhowever\n ,\n what\n is\n Ar~\n man~\n i\n Polo\n ?\nsoftmax\nhowever\n,\nwhat\nis\nAr~\nman~\ni\nPolo\n?\nhowever\n ,\n what\n is\n Ar~\n man~\n i\n Polo\n ?\n1.5-entmax\nhowever\n ,\n what\n is\n Ar~\n man~\n i\n Polo\n ?\n-entmax\nyou\n wonder\n what\n more\n people\n expect\n .\nsoftmax\nyou\nwonder\nwhat\nmore\npeople\nexpect\n.\nyou\n wonder\n what\n more\n people\n expect\n .\n1.5-entmax\nyou\n wonder\n what\n more\n people\n expect\n .\n-entmax\nFigure 9: Interrogation-detecting heads in the three\nmodels. The top sentence is interrogative while the\nbottom one is declarative but includes the interrogative\nword “what”. In the top example, these interrogation\nheads assign a high probability to the question mark in\nthe time step of the interrogative word (with ≥ 97. 0%\nprobability), while in the bottom example since there\nis no question mark, the same head does not assign a\nhigh probability to the last token in the sentence dur-\ning the interrogative word time step. Surprisingly , this\nhead prefers a low α = 1. 05, as can be seen from the\ndense weights. This allows the head to identify the\nnoun phrase “ Armani Polo” better.\ntion mappings to dynamically adapt their curvature\nand sparsity , by automatically adjusting the contin-\nuous α parameter. W e also provide the ﬁrst results\nusing sparse attention in a Transformer model.\nFixed sparsity patterns.\nRecent research im-\nproves the scalability of Transformer-like networks\nthrough static, ﬁxed sparsity patterns ( Child et al. ,\n2019; Wu et al. , 2019). Our adaptively-sparse\nTransformer can dynamically select a sparsity pat-\ntern that ﬁnds relevant words regardless of their po-\nsition ( e. g., Figure\n9). Moreover, the two strategies\ncould be combined. In a concurrent line of research,\nSukhbaatar et al. (2019) propose an adaptive atten-\ntion span for Transformer language models. While\ntheir work has each head learn a different contigu-\nous span of context tokens to attend to, our work\nﬁnds different sparsity patterns in the same span.\nInterestingly , some of their ﬁndings mirror ours –\nwe found that attention heads in the last layers tend\nto be denser on average when compared to the ones\nin the ﬁrst layers, while their work has found that\nlower layers tend to have a shorter attention span\ncompared to higher layers.\n2182\nhere\n ,\n this\n layer\n is\n thin\n .\nhere\n,\nthis\nlayer\nis\nthin\n.\nwhich\n symptoms\n indicate\n a\n sex~\n ually\n transmitted\n disease\n ?\nwhich\nsymptoms\nindicate\na\nsex~\nually\ntransmitted\ndisease\n?\nFigure 10: Example of two sentences of similar length\nwhere the same head ( α = 1. 33) exhibits different spar-\nsity . The longer phrase in the example on the right\n“a sexually transmitted disease” is handled with higher\nconﬁdence, leading to more sparsity .\nT ransformer interpretability .\nThe original\nTransformer paper ( V aswani et al. , 2017) shows\nattention visualizations, from which some specula-\ntion can be made of the roles the several attention\nheads have. Mareˇcek and Rosa (2018) study the\nsyntactic abilities of the Transformer self-attention,\nwhile Raganato and Tiedemann (2018) extract\ndependency relations from the attention weights.\nT enney et al. (2019) ﬁnd that the self-attentions in\nBERT ( Devlin et al. , 2019) follow a sequence of\nprocesses that resembles a classical NLP pipeline.\nRegarding redundancy of heads, V oita et al. (2019)\ndevelop a method that is able to prune heads of\nthe multi-head attention module and make an\nempirical study of the role that each head has\nin self-attention (positional, syntactic and rare\nwords).\nLi et al. (2018) also aim to reduce head\nredundancy by adding a regularization term to\nthe loss that maximizes head disagreement and\nobtain improved results. While not considering\nTransformer attentions,\nJain and W allace (2019)\nshow that traditional attention mechanisms do not\nnecessarily improve interpretability since softmax\nattention is vulnerable to an adversarial attack\nleading to wildly different model predictions\nfor the same attention weights. Sparse attention\nmay mitigate these issues; however, our work\nfocuses mostly on a more mechanical aspect of\ninterpretation by analyzing head behavior, rather\nthan on explanations for predictions.\n7 Conclusion and Future W ork\nW e contribute a novel strategy for adaptively sparse\nattention, and, in particular, for adaptively sparse\nTransformers. W e present the ﬁrst empirical analy-\nsis of Transformers with sparse attention mappings\n(i. e., entmax), showing potential in both translation\naccuracy as well as in model interpretability .\nIn particular, we analyzed how the attention\nheads in the proposed adaptively sparse Trans-\nformer can specialize more and with higher con-\nﬁdence. Our adaptivity strategy relies only on\ngradient-based optimization, side-stepping costly\nper-head hyper-parameter searches. Further speed-\nups are possible by leveraging more parallelism in\nthe bisection algorithm for computing α-entmax.\nFinally , some of the automatically-learned be-\nhaviors of our adaptively sparse Transformers – for\ninstance, the near-deterministic positional heads or\nthe subword joining head – may provide new ideas\nfor designing static variations of the Transformer.\nAcknowledgments\nThis work was supported by the European Re-\nsearch Council (ERC StG DeepSPIN 758969),\nand by the Funda\nc ¸˜ao para a Ci ˆencia e T ecnolo-\ngia through contracts UID/EEA/50008/2019 and\nCMUPERI/TIC/0046/2014 (GoLocal). W e are\ngrateful to Ben Peters for the\nα-entmax code and\nErick Fonseca, Marcos Treviso, Pedro Martins, and\nTsvetomila Mihaylova for insightful group discus-\nsion. W e thank Mathieu Blondel for the idea to\nlearn\nα. W e would also like to thank the anony-\nmous reviewers for their helpful feedback.\nReferences\nBrandon Amos and J. Zico Kolter. 2017.\nOptNet:\nDifferentiable optimization as a layer in neural net-\nworks. In Proc. ICML.\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate . In Proc. ICLR.\nMathieu Blondel, Andr ´e FT Martins, and Vlad Nicu-\nlae. 2019. Learning classiﬁers with Fenchel-Y oung\nlosses: Generalized entropies, margins, and algo-\nrithms\n. In Proc. AISTATS.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow , Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, et al. 2014.\nFindings of the 2014\nworkshop on statistical machine translation . In Proc.\nW orkshop on Statistical Machine Translation.\nOndrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow , Matthias Huck, An-\ntonio Jimeno Y epes, Philipp Koehn, V arvara Lo-\ngacheva, Christof Monz, et al. 2016.\nFindings of the\n2016 conference on machine translation . In Proc.\nWMT.\n2183\nM Cettolo, M Federico, L Bentivogli, J Niehues,\nS St ¨uker, K Sudoh, K Y oshino, and C Federmann.\n2017. Overview of the IWSL T 2017 evaluation cam-\npaign. In Proc. IWSLT.\nRewon Child, Scott Gray , Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse Transformers . preprint arXiv:1904.10509.\nFrank H Clarke. 1990. Optimization and Nonsmooth\nAnalysis. SIAM.\nY untian Deng, Y oon Kim, Justin Chiu, Demi Guo, and\nAlexander Rush. 2018. Latent alignment and varia-\ntional attention . In Proc. NeurIPS.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding\n. In Proc. NAACL-HLT.\nJonas Gehring, Michael Auli, David Grangier, Denis\nY arats, and Y ann N Dauphin. 2017. Convolutional\nsequence to sequence learning . In Proc. ICML.\nStephen Gould, Basura Fernando, Anoop Cherian, Pe-\nter Anderson, Rodrigo Santa Cruz, and Edison Guo.\n2016.\nOn differentiating parameterized argmin and\nargmax problems with application to bi-level opti-\nmization\n. preprint arXiv:1607.05447.\nMichael Held, Philip W olfe, and Harlan P Crowder.\n1974. V alidation of subgradient optimization . Math-\nematical Programming, 6(1):62–88.\nSarthak Jain and Byron C. W allace. 2019. Attention is\nnot explanation . In Proc. NAACL-HLT.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018.\nMarian: Cost-effective high-quality neural\nmachine translation in C++ . In Proc. WNMT.\nJian Li, Zhaopeng Tu, Baosong Y ang, Michael R Lyu,\nand T ong Zhang. 2018. Multi-Head Attention with\nDisagreement Regularization . In Proc. EMNLP.\nChristos Louizos, Max W elling, and Diederik P\nKingma. 2018. Learning sparse neural networks\nthrough L0 regularization. Proc. ICLR.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation . In Proc. EMNLP.\nChaitanya Malaviya, Pedro Ferreira, and Andr ´e FT\nMartins. 2018. Sparse and constrained attention for\nneural machine translation . In Proc. ACL.\nDavid Mare ˇcek and Rudolf Rosa. 2018. Extract-\ning syntactic trees from Transformer encoder self-\nattentions\n. In Proc. BlackboxNLP.\nAndr´e FT Martins and Ram ´on Fernandez Astudillo.\n2016. From softmax to sparsemax: A sparse model\nof attention and multi-label classiﬁcation . In Proc.\nof ICML.\nSameen Maruf, Andr ´e FT Martins, and Gholam-\nreza Haffari. 2019. Selective attention for\ncontext-aware neural machine translation . preprint\narXiv:1903.08788.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nVlad Niculae and Mathieu Blondel. 2017. A regular-\nized framework for sparse and structured neural at-\ntention\n. In Proc. NeurIPS.\nMyle Ott, Sergey Edunov , David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. WMT.\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation . In Proc. ACL.\nBen Peters, Vlad Niculae, and Andr ´e FT Martins. 2019.\nSparse sequence-to-sequence models . In Proc. ACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners .\npreprint.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in Transformer-\nbased machine translation\n. In Proc. BlackboxNLP.\nRico Sennrich, Barry Haddow , and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units . In Proc. ACL.\nW enqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang,\nY udian Li, Xiaogang W ang, and Ping Luo. 2019.\nSSN: Learning sparse switchable normalization via\nSparsestMax\n. In Proc. CVPR.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive At-\ntention Span in Transformers . In Proc. ACL.\nGongbo T ang, Mathias M ¨uller, Annette Rios, and Rico\nSennrich. 2018. Why self-attention? A targeted\nevaluation of neural machine translation architec-\ntures\n. In Proc. EMNLP.\nIan T enney , Dipanjan Das, and Ellie Pavlick. 2019.\nBER T rediscovers the classical NLP pipeline . In\nProc. ACL.\nConstantino Tsallis. 1988. Possible generalization of\nBoltzmann-Gibbs statistics . Journal of Statistical\nPhysics, 52:479–487.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017.\nAttention is all\nyou need . In Proc. NeurIPS.\nElena V oita, Pavel Serdyukov , Rico Sennrich, and Ivan\nTitov . 2018. Context-aware neural machine transla-\ntion learns anaphora resolution . In Proc. ACL.\n2184\nElena V oita, David T albot, Fedor Moiseev , Rico Sen-\nnrich, and Ivan Titov . 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned\n. In Proc. ACL.\nFelix Wu, Angela Fan, Alexei Baevski, Y ann N\nDauphin, and Michael Auli. 2019. Pay less atten-\ntion with lightweight and dynamic convolutions . In\nProc. ICLR."
}