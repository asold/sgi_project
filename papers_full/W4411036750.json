{
  "title": "Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts",
  "url": "https://openalex.org/W4411036750",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2325569700",
      "name": "XU Qihui",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2569074518",
      "name": "Peng Yingying",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A4302742090",
      "name": "Nastase Samuel A.",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A4378947126",
      "name": "Chodorow, Martin",
      "affiliations": [
        "City University of New York",
        "Hunter College",
        "The Graduate Center, CUNY"
      ]
    },
    {
      "id": "https://openalex.org/A2351513564",
      "name": "Wu Minghua",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2007781260",
      "name": "Li Ping",
      "affiliations": [
        "Hong Kong Polytechnic University",
        "Hangzhou Special Equipment Inspection and Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2889870962",
    "https://openalex.org/W2982116886",
    "https://openalex.org/W4212844288",
    "https://openalex.org/W4223947928",
    "https://openalex.org/W4402512774",
    "https://openalex.org/W3197409584",
    "https://openalex.org/W3021155507",
    "https://openalex.org/W3187821822",
    "https://openalex.org/W3004554223",
    "https://openalex.org/W4312143904",
    "https://openalex.org/W4312143998",
    "https://openalex.org/W2010904179",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W4401684295",
    "https://openalex.org/W4388696164",
    "https://openalex.org/W4385363516",
    "https://openalex.org/W4386333898",
    "https://openalex.org/W4396225186",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4402671061",
    "https://openalex.org/W4388520388",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W3091069321",
    "https://openalex.org/W3183248212",
    "https://openalex.org/W4388824938",
    "https://openalex.org/W2023736093",
    "https://openalex.org/W2782213998",
    "https://openalex.org/W2792076440",
    "https://openalex.org/W2017534371",
    "https://openalex.org/W3211696467",
    "https://openalex.org/W2113563134",
    "https://openalex.org/W2098100128",
    "https://openalex.org/W3168583930",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W4411630063",
    "https://openalex.org/W1596515083",
    "https://openalex.org/W2052091366",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W1508466705",
    "https://openalex.org/W2966745778",
    "https://openalex.org/W4389519164",
    "https://openalex.org/W2048859817",
    "https://openalex.org/W2063600341",
    "https://openalex.org/W4391428296",
    "https://openalex.org/W2989588035",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2046813578",
    "https://openalex.org/W2022630592",
    "https://openalex.org/W2154132316",
    "https://openalex.org/W2099784871",
    "https://openalex.org/W2134145060",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W4232516125",
    "https://openalex.org/W4385774329",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2344467524",
    "https://openalex.org/W1994505947",
    "https://openalex.org/W2305056715",
    "https://openalex.org/W2950799585",
    "https://openalex.org/W2033582154",
    "https://openalex.org/W4205550663",
    "https://openalex.org/W2160654481"
  ],
  "abstract": null,
  "full_text": "Nature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1871\nnature human behaviour\nhttps://doi.org/10.1038/s41562-025-02203-8\nArticle\nLarge language models without grounding \nrecover non-sensorimotor but not \nsensorimotor features of human concepts\n \nQihui Xu    1,7 , Yingying Peng2,7, Samuel A. Nastase    3, Martin Chodorow    4,5, \nMinghua Wu2 & Ping Li    2,6 \nT o what extent can language give rise to complex conceptual \nrepresentation? Is multisensory experience essential? Recent large language \nmodels (LLMs) challenge the necessity of grounding for concept formation: \nwhether LLMs without grounding nevertheless exhibit human-like \nrepresentations. Here we compare multidimensional representations of \n~4,442 lexical concepts between humans (the Glasgow Norms1, N = 829; and \nthe Lancaster Norms2, N = 3,500) and state-of-the-art LLMs with and without \nvisual learning, across non-sensorimotor, sensory and motor domains. We \nfound that (1) the similarity between model and human representations \ndecreases from non-sensorimotor to sensory domains and is minimal in \nmotor domains, indicating a systematic divergence, and (2) models with \nvisual learning exhibit enhanced similarity with human representations in \nvisual-related dimensions. These results highlight the potential limitations \nof language in isolation for LLMs and that the integration of diverse \nmodalities can potentially enhance alignment with human conceptual \nrepresentation.\nImagine learning about the concept of ‘flower’ without ever smell -\ning a rose, touching the petals of a daisy or walking through a field \nof wildflowers. Can we truly represent the concept ‘flower’ in all its \nrichness without sensorimotor experiences? This question invokes a \nlongstanding debate about the interplay between physical experience \nand conceptual representation. On the one hand, theories of grounded \ncognition posit that our senses are our gateways to knowledge3; the \nphysical experience of ‘flowers’ is integral to how we represent and \nprocess them. On the other hand, research with disembodied artificial \nneural network models4–6 and congenitally blind and partially sighted \npeople7–10 show that learners can form conceptually rich representa-\ntions from language alone, independent of direct sensory experience. \nFor example, studies show that individuals born with limited vision \ncan represent and respond to colour concepts similarly to those who \ncan see8–10. When sensorimotor input is absent, to what extent can lan-\nguage alone inform our conceptual representation of the world? How \nindispensable is bodily experience in shaping our conceptual world?\nDisentangling the various sources for conceptual formation is \nchallenging. Although studies involving artificial models or blind and \npartially sighted people have provided valuable insights, they have \nseveral limitations. First, they often overlook the multidimensional \nnature of conceptual representation. Our representation of concepts \nis extensive and complex, encompassing areas not directly tied to sen-\nsorimotor experiences, such as emotional arousal and valence linked \nto the concept, as well as the direct sensations and actions encoun -\ntered in connection with it11,12. For instance, processing the concept of \nReceived: 28 November 2023\nAccepted: 31 March 2025\nPublished online: 4 June 2025\n Check for updates\n1Department of Psychology, Ohio State University, Columbus, OH, USA. 2Department of Chinese and Bilingual Studies, Faculty of Humanities,  \nThe Hong Kong Polytechnic University, Hong Kong SAR, China. 3Department of Psychology and the Princeton Neuroscience Institute, Princeton University, \nPrinceton, NJ, USA. 4Department of Psychology, Hunter College, City University of New York, New York, NY, USA. 5Department of Psychology, Graduate \nCenter, City University of New York, New York, NY, USA. 6The PolyU-Hangzhou Technology and Innovation Research Institute, Hangzhou, China.  \n7These authors contributed equally: Qihui Xu, Yingying Peng.  e-mail: xu.5430@osu.edu; ping2.li@polyu.edu.hk\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1872\nArticle https://doi.org/10.1038/s41562-025-02203-8\nthe ‘non-sensorimotor’ dimensions are devoid of embodiment—a \ntopic that remains widely explored in current literature (for example, \nrefs. 37,38). For example, questions that explicitly ask about specific \nsensory or motor experiences—such as how one would experience \nthe concept ‘flower’ through smelling—are categorized under sen -\nsorimotor domains because they directly tap into modality-specific \n(that is, the typical senses) and effector-specific (that is, the typical \naction effectors) bodily experiences. By contrast, questions that do \nnot directly specify particular sensorimotor experiences—such as \nemotional arousal about the concept ‘flower’—are categorized as \nnon-sensorimotor, though they could relate to bodily states (see the \nclose relation between emotion and interoception in ref. 36). Despite \nthis potential connection, previous literature shows that compared \nwith non-sensorimotor dimensions such as concreteness, specific \nsensory ratings are more effective in facilitating lexical semantic pro-\ncessing39. The second research question is the potential value of addi-\ntional visual inputs for concept formation in LLMs trained with both \nlanguage and visual input modalities, compared with those trained \nsolely within the language domain. T o address this research question, \nwe analysed whether the additional visual inputs provided to LLMs \nyield stronger alignment with humans on visual-related dimensions. \nFinally, we validated the response of the LLMs to ensure the validity of \nour results (see Supplementary Information, section 7.3, for discus -\nsion on the importance of validation). We report two key findings: (1) \nthe similarity between LLMs and human representations decreased \nfrom non-sensorimotor to sensory domains and was minimal in motor \ndomains, and (2) the models incorporating visual inputs exhibited \nenhanced similarity with human representations in vision-related \ndimensions. These findings suggest that learning solely within the \nlanguage domain substantially recovers non-sensorimotor aspects \nof conceptual representations yet remains impoverished in sensori-\nmotor aspects, particularly along motor dimensions. Furthermore, \nextending experience into the visual domain is associated with LLM’s \nimproved alignment with human representations in both visual and \nrelated dimensions such as imageability and haptic features, suggest-\ning potential knowledge transfer through multimodal integration.\nResults\nWe collected conceptual word ratings from LLMs (that is, ChatGPT \nmodels: GPT-3.5 and GPT-4; Google LLMs: PaLM and Gemini) and com-\npared them with ratings generated by humans from the Glasgow1 and \nLancaster Norms2 (see Methods for further details and Supplementary \nInformation, section 7.1, on how word rating tasks capture key aspects \nof conceptual representation). The model prompt and design (Fig. 1b) \nfor LLMs was standardized to match the instructions given to human \nparticipants, maintaining consistency with human-participant data \ncollection. Each LLM was separately run for four rounds to ensure reli-\nability (see Supplementary Information, section 1, for the agreement \nbetween these rounds).\nTwo common practices for measuring similarity were used to eval-\nuate LLM versus human similarity - dimension-wise correlations and \nrepresentational similarity analysis (RSA) (‘Model–human alignment \nvaries across domains’ section). These measures enable the evaluation \nof LLM–human similarity from several angles: the strength with which \na lexical concept is rated within each individual dimension, and how \ndifferent lexical concepts are geometrically organized across dimen-\nsions. Next, we explored if the additional visual inputs provided to LLMs \npredict their alignment with humans (‘Linking additional visual training \nto model–human alignment’ section). We then performed two second-\nary analyses to substantiate the correlations found between human \nand model ratings (‘Validation of the results’ section). Given ongoing \ndebates regarding the distinct roles of grounding in concrete versus \nabstract concepts 40,41, we assessed the potential influence of word \nconcreteness on our primary findings. T o ensure the validity of LLMs \nas cognitive models42, we adopted standard validation techniques from \n‘flower’ may evoke not only the object ‘flower’ itself but also the visual \nperceptions of colours and shapes, the actions of touching the flower \nby hand and smelling it with the nose, its associated scents, textures, \nemotions and memories. Second, the limited scope of words tested, \nsuch as colour words (for example, ref. 9) or object words (for example, \nref. 5) only, restricts external validity, failing to capture the breadth \nof concepts encountered in daily life, which encompasses not only \nobjects and colour words but also action verbs, abstract concepts \nand more10,13,14. Moreover, there can be potential knowledge trans -\nfer across domains, which poses challenges for human-participant \nresearch in achieving rigorous control over the diverse domains of \nresources that may contribute to conceptual representation. Even \nwithout visual input, individuals can tap into other sensory channels \nsuch as touch and internal sensations, which have been shown to cor-\nrelate with visual knowledge2. Therefore, to better distinguish between \nlanguage-derived and sensorimotor-derived sources, it is crucial to \nconsider a broad range of concept words that span a wide and system-\natic spectrum of conceptual representations (from non-sensorimotor \naspects to sensory and motor aspects).\nRecent advances in large language models (LLMs) offer a unique \navenue to test the extent to which language alone can give rise to \ncomplex concepts 15–17. LLMs have enabled us to (1) estimate what \nkinds of structure (and how much) can ultimately be extracted from \nlarge volumes of language alone 18–20 and (2) examine how different \ninput modalities (for example, text versus images) influence learning \nprocesses15,16. Current LLMs have been trained on massive amounts of \ndata, either constrained to the language domain (that is, large-scale \ntext data as in GPT-3.5 and PaLM) or incorporating language and visual \ninput (for example, GPT-4 and Gemini). Despite these limited input \nmodalities, these models exhibit remarkably human-like performance \nin various cognitive tasks6,21–23. In the same way that LLMs demonstrate \nthe feasibility of learning syntactic structure from surface-level lan -\nguage exposure alone24,25, they may also have the capability of learning \nphysical, grounded features of the world from language alone 26–28. \nFor example, some have argued that language itself can act as a sur -\nrogate ‘body’ for these models, reminiscent of the largely conceptual-\nized and ungrounded colour knowledge in blind and partially sighted \nindividuals4,6. This perspective aligns with previous research empha-\nsizing the important role of language in providing rich cognitive and \nperceptual resources29,30. By contrast, others believe that multimodal \nexperiences are essential for both humans and artificial models to \ngrasp concepts more efficiently 16,31,32. Unlike current LLMs such as \nGPT-3, which rely on vast amounts of text20—equivalent to 20,000 years \nof human reading 33—real-world, interactive experiences may offer \nricher, more interconnected conceptual representations that facilitate \nknowledge transfer across domains, potentially reducing the need for \nsuch extensive linguistic input in model training.\nThe above theoretical debates motivate us in this study to inves-\ntigate two research questions. The first research question is which \naspects of human conceptual representation can be recovered by \nungrounded state-of-the-art LLMs and which cannot. T o address this \nquestion, we compared similarity of representations across ~4,442 \nword concepts between humans and two state-of-the-art LLM fami -\nlies (Fig. 1a) from OpenAI (GPT-3.5 and GPT-4) and Google (PaLM and \nGemini), across a range of dimensions, spanning non-sensorimotor, \nsensory and motor domains. The domains were based on categories \nestablished in refs. 1,2, where each domain consists of several dimen-\nsions (see Table 1 for definitions for each dimension). These dimensions \nprovide comprehensive coverage for understanding the spectrum of \nhuman lexical–conceptual processing explored in previous studies \n(for example, refs. 34–36), from socio-emotional aspects and abstract \nmental imagery, to direct bodily experience (Fig. 1c). Importantly, our \nclassification into ‘non-sensorimotor’ and ‘sensorimotor’ domains \nis based on whether the measures directly assess specific sensori -\nmotor experiences. This operational distinction does not imply that \nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1873\nArticle https://doi.org/10.1038/s41562-025-02203-8\nhuman participant research1 for dimensions with strong model–human \ncorrelations. All P values reported below were corrected for multiple \ncomparisons by controlling the false discovery rate (FDR)43 (Methods).\nModel–human alignment varies across domains\nDimension-wise correlations.  T o assess the similarity of model \nword ratings to human word ratings across each dimension, we calcu-\nlated the Spearman rank correlation between model-generated and \nhuman-generated ratings at both the aggregate and individual levels. \nFor the aggregated analyses, the model-generated ratings of each word \nwere aggregated by averaging across the four rounds of each LLM, and \nhuman-generated ratings were averaged across individuals.\nAs shown in Fig. 2a,b, ChatGPT and Google LLMs exhibit strong \ncorrelations (Rs > 0.50) (see Supplementary Table 3 for additional \nstatistics) with human ratings across most the non-sensorimotor \ndimensions. However, they show significantly weaker correlations \nin sensory and motor dimensions. This observation is supported by \nMann–Whitney U tests comparing model–human similarities between \nthe sensorimotor and the non-sensorimotor dimensions (GPT-4: \nU(N1 = 7, N2 = 11) = 65.00, P = 0.018, rank-biserial correlation (rrb) = 0.69; \nGPT-3.5: U(N1 = 7, N2 = 11) = 67.00, P = 0.010, rrb = 0.74; Gemini: U(N1 = 7, \nN2 = 11) = 77.00, P < 0.001, rrb = 0.10; PaLM: U (N1 = 7, N2 = 11) = 76.00, \nP < 0.001, rrb = 0.97).\nWe undertook an individual-level analysis to examine the simi -\nlarity between human and model conceptual representations while \nconsidering individual variability. We constructed pairwise Spearman \ncorrelations for each pair of individual human participants (human–\nhuman) and each individual run of a model and each individual human \nparticipant (model–human). This process resulted in five distribu -\ntions: human–human, GPT3.5–human, GPT4–human, PaLM–human \nand Gemini–human pairwise correlations. Using the human–human \ncorrelations as a benchmark for inter-person reliability, we asked: Are \nthe responses from an individual model more or less similar to those \nof an individual human, as compared with the similarity between one \nhuman and another? Independent-sample t -tests assessed whether \nthe distributions of model–human similarities significantly differ from \nhuman–human similarities, and Cohen’s d  was used to quantify the \nstandardized difference between the two (see Supplementary Table 4 \nfor additional statistics). A negative d value indicates that the model–\nhuman similarities are greater than the human–human similarities for \nthat particular dimension and model.\nFigure 3 presents model–human similarity distributions. The \ndistributions marked with fill-in colours indicate comparisons where \nthere is no evidence that a model’s responses to a human are less similar \nthan one human’s responses to another human. The count of these \nmarked distributions decreased from the non-sensorimotor domain \nHuman knowledge representation:\nembodiment required?  \nHuman Large language models\nLanguage\nInput Input\nversus\nLanguage\nArousal is a measure of excitement versus calmness. A word is AROUSING if it \nmakes you feel stimulated, excited, frenzied, jittery or wide awake. A word is \nUNAROUSING if it makes you feel relaxed, calm, sluggish, dull or sleepy. \nPlease indicate how arousing human beings think each word is on a 9-point \nscale of VERY UNAROUSING (1) to VERY AROUSING (9), with the midpoint \nrepresenting moderate arousal. Please respond using this format: word – rating\nFoxy \nAdvisor\nHabit\nHoof\nHumorous\nUnit (item) \nSwing\nFoxy – 8\nAdvisor – 3\nHabit – 3\nHoof – 2\nHumorous – 6\nUnit (item) – 2\nSwing – 6\nIntroduction\nWord list\nResponse collection\nSalience Emotion Mental \nvisualization Sensory Motor\nNon-sensorimotor Sensorimotor\nWhat semantic\nsize do you think is\nassociated with\n‘/f.shortlower’?   \nHow imageable\ndo you think ‘/f.shortlower’ is?  \nHow much do\nyou experience ‘/f.shortlower’ by\nsmelling? \nHow much do\nyou experience ‘/f.shortlower’ using\nactions from torso?\nHow arousing\ndo you think ‘/f.shortlower’ is?   \na b\nc\nFig. 1 | Overview. a, A schematic depiction of the research question and \napproach. This study aims to investigate the extent to which human conceptual \nrepresentation requires grounding. Icons from Flaticon.com77. b, A schematic \nof the LLM testing procedure. The model prompt and design were aligned \nwith the instructions for human participants, which started with explaining \nthe dimension and listing the words to be rated. The LLMs would then provide \nratings per word as required. c, The key domains studied span non-sensorimotor, \nsensory and motor domains, with specific example questions provided for each \nrespective domain. The classification into ‘non-sensorimotor’ and ‘sensorimotor’ \ndomains is based on whether the measures directly assess sensorimotor \nexperiences (see above for more detailed information).\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1874\nArticle https://doi.org/10.1038/s41562-025-02203-8\ndimensions (16 marked model–human distributions out of 28 model–\nhuman distributions; 16/12) to the sensory (4/20) and further decreased \nin the motor dimensions (2/18), χ 2(2) = 15.49, P < 0.001. Within the \nnon-sensorimotor domain’s seven dimensions, there is no credible \nevidence showing that GPT-4’s model–human similarity distribution \nwas significantly lower than the human–human distribution in all seven \ndimensions. For GPT-3.5, this held true in four dimensions, for Gemini in \nthree dimensions and for PaLM in two dimensions. However, within the \nsensory domain’s six dimensions, the count decreased to four for GPT-4 \nand to zero for all other models. Within the motor domain’s five dimen-\nsions, the count further dropped to two for GPT-4 and remained at zero \nfor the other models. These individual-level analyses reveal a growing \ndivergence between models and humans from non-sensorimotor to \nsensorimotor dimensions.\nRSA. While the above correlations capture model–human similarity \nover all words in each separate dimension, such dimension-wise analy-\nses might overlook how different dimensions may jointly contribute to \na word’s overall conceptual representation and how different words are \ninterconnected. For example, the concepts of ‘pasta’ and ‘roses’ might \nboth receive high ratings for their olfactory qualities. However, ‘pasta’ is \nconsidered more similar to ‘noodles’ than to ‘roses’ , not only because of \nits smell but also because of its visual appearance and taste. T o address \nthis issue, we adopt the RSA44 to fully capture the complexities of word \nrepresentations, where dimensions such as smell and visual appearance \nare considered jointly as part of a high-dimensional representation \nfor each word.\nRSA allows us to evaluate and compare how the geometric organi-\nzation of concept words is aligned between models and humans across \nthe non-sensorimotor, sensory and motor domains. T o implement RSA \n(Fig. 4a), we represented each word as a vector separately within the \nnon-sensorimotor, sensory and motor domains. The elements of these \nvectors were derived from the ratings of specific dimensions belonging \nto each respective domain. For example, the sensory vector for ‘pasta’ \nconsists of ratings from six sensory dimensions (for example, haptic \nand auditory). We then constructed representational dissimilarity \nmatrices (RDMs) by calculating the Euclidean distance between word \nvectors for each model and individual human, capturing word similarity \nrelationships (for example, ‘pasta’ and ‘noodles’ are more similar than \n‘pasta’ and ‘roses’). The similarity between RDMs of each model and \neach individual human was calculated via the Spearman rank correla-\ntion. We thus obtained a distribution of similarities between all human \nparticipants and each model separately on each domain. We conducted \ntwo mixed-effects analyses of variance (ANOVAs) to statistically evalu-\nate the model–human similarities across three domains, specifically to \ndetermine whether these similarities were lower in the sensory/motor \ndomains compared with the non-sensorimotor domain. These analyses \nwere performed separately for the ChatGPTs and Google LLMs, con-\nsidering ‘domain’ and ‘model’ as two distinct factors.\nFor the ChatGPT models, a significant main effect of domain  \nwas observed ( F(2,1,704)= 729.72,P < 0.001,η2\np = 0.46 ). Both the  \nsensory and motor domains showed significantly lower similarities \ncompared with the non-sensorimotor domain. Specifically, the sensory \ndomain had lower similarities than the non-sensorimotor domain \n(t(1,526.5) = −2.93, P = 0.004, d = −0.13, 95% confidence interval (CI) \n−0.03 to −0.01), and the motor domain was significantly lower than the \nnon-sensorimotor domain (t(1,731.8) = −44.49, P < 0.001, d = −1.87, 95% \nCI −0.22 to −0.20). In addition, similarities in the motor domain were \nsignificantly lower than those in the sensory domain (t(1,721.0) = −33.18, \nP < 0.001, d = −1.58, 95% CI −0.21 to −0.19).\nGoogle LLMs revealed similar results: a significant main effect of \ndomain was observed ( F(2,1,421)= 1,626.84,P < 0.001,η2\np = 0.70). \nBoth the sensory and motor domains showed significantly lower simi-\nlarities compared with the non-sensorimotor domain. Specifically, the \nsensory domain had lower similarities than the non-sensorimotor \ndomain (t(892.4) = −49.22, P < 0.001, d = −2.46, 95% CI −0.24 to −0.23), \nand the motor domain was significantly lower than the \nnon-sensorimotor domain ( t(1,056.2) = −47.05, P < 0.001, d = −2.24, \n95% CI −0.23 to −0.21). Similarities in the motor domain were not sig-\nnificantly different from those in the sensory domain (t(1,178.8) = 1.81, \nP = 0.077, d = 0.11, 95% CI −0.00 to 0.02).\nThese results suggest that LLMs’ conceptual representations and \norganizations of words align most closely with human representa -\ntions in the non-sensorimotor domain, while alignments are weaker \nin the sensory domains and minimal in the motor domains. These \nobservations are in line with earlier analyses, highlighting LLMs’ pro-\ngressively diminishing effectiveness in recovering human conceptual \nrepresentations when they move towards more sensorimotor-related \naspects of the representations (see Supplementary Table 5 for the \ndescriptive statistics).\nLinking additional visual training to model–human alignment\nThe increased disparity between model and human representations for \nmore sensorimotor dimensions of conceptual representations suggests \nthat grounding experience may be necessary to achieve human-like con-\nceptual representation. Given this possibility, we pose a related ques-\ntion: What role do additional visual inputs play in conceptual formation \nwithin LLMs primarily trained on both language and visual inputs (for \nexample, GPT-4 and Gemini, henceforth visual LLMs) compared with \nthose that have received input from only a single modality—language \n(for example, GPT-3.5 and PaLM, henceforth text-only LLMs)? In other \nwords, is visual learning associated with the alignment of multimodal  \nTable 1 | Definitions of each dimension in Glasgow and \nLancaster Norms\nNorms Domain Dimension Definition\nGlasgow Non-sensorimotor Valence Value or worth; \nrepresenting \nsomething considered \ngood or bad\nDominance The degree of control a \nword makes you feel\nArousal Excitement versus \ncalmness\nSize Dimensions, \nmagnitude or extent of \nan object or concept \nthat a word refers to\nGender How strongly its \nmeaning is associated \nwith male or female \nbehaviour\nConcreteness A measure of how \nconcrete or abstract \nsomething is\nImageability How easy or difficult \nsomething is to \nimagine\nLancaster Sensory Haptic, auditory, \nolfactory, \ninteroceptive, \nvisual, gustatory\nHow much do you \nexperience everyday \nconcepts using six \ndifferent perceptual \nsenses\nMotor Foot/leg, hand/\narm, mouth/\nthroat, torso, \nhead excluding \nmouth\nHow much do you \nexperience everyday \nconcepts using actions \nfrom five different parts \nof the body\nThe two norms have incorporated different numbers of words/concepts in human judgments \nwith approximately 4/5 of the Glasgow normed words overlapping with those in the \nLancaster Norms (see Methods for details).\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1875\nArticle https://doi.org/10.1038/s41562-025-02203-8\nLLMs with human representations of sensorimotor concepts, and if so,  \nhow? Available information indicates that GPT-4 was pretrained with  \ntext and images45, while Gemini was pretrained to integrate language  \ndata with a diverse array of visual inputs, including natural images,  \ncharts, screenshots, PDFs and videos46. By contrast, GPT-3.5 (ref. 47) and  \nPaLM2 (ref. 48) were pretrained exclusively within the language domain.\nIsolating the impact of visual training is challenging owing to \nlimited access to the details of training in these state-of-the-art LLMs. \nHere, we piloted an analysis characterizing a potential association \nbetween the added visual learning and the difference in model–human \nalignment between visual and text-only LLMs. The rationale underly-\ning this analysis is that if visual learning affects model alignment with \nNon-sensorimotorSensoryMotor\nImageability\nValence\nConcreteness\nDominance\nArousal\nSizeGender\nFlower\n5\n9\n1\n3\n7\nHaptic\nAuditory\nOlfactory\nInteroceptive\nVisual\nGustatory\nFlower\n1\n3\n5\nFoot/leg\nHand/arm\nMouth/throat\nTorso\nHead\nFlower\n1\n3\n5\nImageability\nValence\nConcreteness\nDominance\nArousal\nSizeGender\nJustice\n5\n9\n1\n3\n7\nHaptic\nAuditory\nOlfactory\nInteroceptive\nVisual\nGustatory\nJustice\n1\n3\n5\nFoot/leg\nHand/arm\nMouth/throat\nTorso\nHead\nJustice\n1\n3\n5\nHuman–human Human–GPT3.5 Human–GPT4 Human–PaLM Human–Gemini\na b\nc\n0 0.2 0.4 0.6 0.8 1.0\nRs\nHead\nTorso\nMouth/throat\nHand/arm\nFoot/leg\nGustatory\nVisual\nInteroceptive\nOlfactory\nAuditory\nHaptic\nImageability\nConcreteness\nGender\nSize\nArousal\nDominance\nValence\nGPT-3.5\nGPT-4\n−0.2 0 0.2 0.4 0.6 0.8 1.0\nHead\nTorso\nMouth/throat\nHand/arm\nFoot/leg\nGustatory\nVisual\nInteroceptive\nOlfactory\nAuditory\nHaptic\nImageability\nConcreteness\nGender\nSize\nArousal\nDominance\nValence\nPaLM\nGemini\nRs\nFig. 2 | Aggregated results. a,b, Spearman correlations between the human-\ngenerated and LLM-generated ratings for all analysed words. The x axis \nrepresents the Spearman correlation coefficients between the aggregated word \nratings generated by LLMs including GPT-3.5, GPT-4 (a), PaLM and Gemini  \n(b) and the corresponding human ratings. The y axis lists the different \ndimensions being evaluated, along the non-sensorimotor, sensory and motor \ndimensions. The error bars depict the 95% confidence intervals, estimated by \nbootstrap resampling 1,000 samples of word ratings from aggregated human \nparticipants and LLMs. The central value represents the estimated correlation \ncoefficient between the lower and upper confidence bounds. c, Radar plots \nshowing the aggregated ratings of human, ChatGPT (GPT-3.5 and GPT-4) and \nGoogle LLMs (PaLM and Gemini) on each dimension for two individual concepts: \n‘flower’ (a concrete word) and ‘justice’ (an abstract word). The numbers along the \nradial axis denote the rating ranges for these dimensions. Additional examples \nare provided in Supplementary Figs. 2 and 3.\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1876\nArticle https://doi.org/10.1038/s41562-025-02203-8\n0\n2.0\n4.0\n6.0\n8.0\nValence\nd = –0.61/–0.79/–0.17/–0.49\nDominance\nd = –0.53/–0.59/0.08/–0.18\nArousal\nd = –0.48/–0.46/–0.36/–0.49\nSize\nd = –0.30/–0.61/0.50/0.14\n0\n2.0\n4.0\n6.0\n8.0\nGender\nd = 0.27/0.01/1.15/0.90\nConcreteness\nd = 0.20/–0.75/1.97/0.08\nImageability\nd = 1.60/–0.71/2.14/0.61\nb    Sensory\na    Non-sensorimotor\nc    Motor\n0\n2.0\n4.0\n6.0\n8.0\nHaptic\nd = 1.25/–0.43/3.98/3.84\nAuditory\nd = 1.49/1.11/3.44/4.16\nOlfactory\nd = 2.26/0.30/4.85/4.22\nInteroceptive\nd = 1.84/0.11/3.46/4.06\n0\n2.0\n4.0\n6.0\n8.0\nVisual\nd = 1.44/–0.47/2.10/1.79\nGustatory\nd = 2.79/0.05/5.46/5.03\n0\n2.5\n5.0\n7.5\n10.0\nFoot/leg\nd = 1.11/0.18/2.28/2.16\nHand/arm\nd = 1.16/–0.40/2.19/1.85\nMouth/throat\nd = 1.82/0.27/1.68/2.53\nTorso\nd = 1.03/0.36/1.49/1.88\n–1.0 –0.5 0 0.5 1.0\n0\n2.5\n5.0\n7.5\n10.0\nHead\nd = 0.29/0.05/0.89/1.14\nHuman–human Human–GPT3.5 Human–GPT4\nHuman–PaLM Human–Gemini\n–1.0 –0.5 0 0.5 1.0 –1.0 –0.5 0 0.5 1.0\n–1.0 –0.5 0 0.5 1.0 –1.0 –0.5 0 0.5 1.0 –1.0 –0.5 0 0.5 1.0\nFig. 3 | Individual analysis. a–c, The results for the individual-level pairwise \ncorrelation analysis for each dimension in the non-sensorimotor (a), sensory \n(b) and motor (c) domains. This analysis aims to examine the similarity between \nhuman and model conceptual representations while considering individual \nvariability. The x axis represents the Spearman correlation coefficient, while the  \ny axis shows the kernel density estimation of the correlation distributions. \nNotably, in the subplots for the motor dimensions, the y axis displays higher \ndensity peaks due to PaLM yielding model–human similarities clustered around \nzero. Cohen’s d is reported for each dimension to quantify the standardized \ndistance between the human–human and model–human correlation \ndistributions. The d values for GPT-3.5, GPT-4, PaLM and Gemini models are \npresented between forward slashes (‘/’), respectively. A negative d value, \nhighlighted in purple, indicates that the model–human similarities are greater \nthan the human–human similarities for that particular dimension and model. \nThe distribution curves for human–human pairwise similarity, serving as \nbenchmarks, are visually distinguished by the increased line thickness. When  \nthe colours are filled in model–human similarity distribution curves, they \nindicate that there is no credible evidence those model–human similarities are \nlower than human–human similarities (non-sensorimotor: 16 distributions  \nout of 28 model–human distributions,16/12; sensory: 4/20; motor: 2/18). These \nfilled-in curves highlight the dimensions and models where the model-generated \nratings align closely with human ratings at the individual level. Here, the P values \nwere assessed with two-sided t-tests and corrected for multiple comparisons \nusing the FDR.\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1877\nArticle https://doi.org/10.1038/s41562-025-02203-8\nHuman RDM Model RDM\nNon-sensorimotor\nSensory\nMotor\na\nb\nc\nd\ne\nf\n−0.2\n−0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nHuman–model RDM similarity\nMotor GPT -4\nMotor GPT -3.5\nSensory GPT -4\nSensory GPT -3.5\nNon-sensorimotor GPT -4\nNon-sensorimotor GPT -3.5\nCategories\n0\n1\n2\n3\n4\nDensity\n−0.2\n−0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nHuman–model RDM similarity\nMotor Gemini\nMotor PaLM\nSensory Gemini\nSensory PaLM\nNon-sensorimotor Gemini\nNon-sensorimotor PaLM\nCategories\n0\n1\n2\n3\n4\n5\n6\nDensity\nW ord X\nW ord Y\nRatings: dim1 dim2 dim3\nRatings: dim1 dim2 dim3\nRatings: dim1 dim2 dim3\nRatings: dim1 dim2 dim3\nDissimilarity\nDissimilarity\nSpearman\ncorrelation\nWord list for each \nhuman rater\nHuman RDM Model RDM\nHuman RDM Model RDM\nHuman \nRDM\nModel \nRDM\nMean: 0.13\nMean: 0.33\nMean: 0.45\nMean: 0.36\nMean: 0.45\nMean: 0.25\nMean: 0.03\nMean: 0.01\nMean: 0.35\nMean: 0.27\nMean: 0.14\nMean: 0.14\nFig. 4 | RSA. a, A schematic of the RSA: for each human rater and language model \n(GPT-3.5, GPT-4, PaLM and Gemini), the words were represented as separate \nvectors for the non-sensorimotor, sensory and motor domains. Icons from \nFlaticon.com77. The elements of these word vectors were derived from the ratings \ngenerated by humans or models for the dimensions belonging to each respective \ndomain. The RDMs were then constructed by calculating the Euclidean distance \nbetween every pair of word rating vectors within each domain. Spearman \ncorrelations between these RDMs quantify the alignment of representational \ngeometries, enabling comparison between human and model representations. \nb, Distributions of model–human RDM similarities for ChatGPT models (GPT-3.5 \nand GPT-4). The distributions of Spearman correlation coefficients for RDMs \nconstructed upon individual human ratings and ChatGPT ratings for the same \nwords across non-sensorimotor, sensory and motor domains are shown. The x \naxis represents Spearman correlation coefficients and y axis denotes the density \nof these coefficients. c, The distributions of model–human RDM similarities for \nGoogle LLMs (PaLM and Gemini). Similar to b, the distributions for human and \nGoogle LLM RDM alignment for the same words across the same three domains \nare displayed. Both b and c illustrate a trend that model–human RDM alignments \ndecrease (with RDM similarities centralizing around smaller values) from non-\nsensorimotor to sensory and especially motor domains. d–f, Example RDMs: \neach RDM, constructed using 25 words, reflects pairwise similarities either based \non human or GPT-4 ratings across non-sensorimotor (d), sensory (e) and motor \n(f) domains. The distinct patterns could be observed between the human RDM \nand GPT-4 RDM for the motor domain while for the non-sensorimotor domain, \nthe human and GPT-4 model RDMs are much more similar.\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1878\nArticle https://doi.org/10.1038/s41562-025-02203-8\nhuman conceptual representations, this effect should be particularly \nnoticeable in the visual dimension and in dimensions that involve \nsome level of visual interpretation, such as imageability, which have \nbeen identified as having visual components in prior research (see \nrefs. 49,50 for reviews). We quantified the visual association strength \nof each dimension by computing the Spearman correlation between \neach dimension and the visual dimension, using human rating data as \nreported in refs. 1 ,2. Higher absolute-value correlation coefficients \nindicate a stronger association with the visual dimension. For example, \nas illustrated in Fig. 5c, dimensions such as concreteness (R s = 0.62, \nP < 0.001, 95% CI 0.60 to 0.64) and imageability (Rs = 0.69, P < 0.001, \n95% CI 0.67 to 0.70) are strongly associated with the visual dimension, \nwhereas dimensions such as gustatory (R s = 0.01, P = 0.596, 95% CI \n−0.04 to 0.02) and torso (R = 0.02, P = 0.136, 95% CI −0.01 to 0.05) show \nminimal visual association.\nT o assess the change of visual LLMs over text-only LLMs in their \nalignment with human representations, we calculated the difference \nin model–human correlations (Fisher Z-transformed) between visual \nLLMs and text-only LLMs (GPT-4 versus GPT-3.5 for ChatGPTs and Gemini  \nversus PaLM for Google LLMs) for each dimension. A higher value on a \nspecific dimension indicates a stronger correlation between the visual \nLLM and human data compared with the text-only LLM for that dimen-\nsion, as shown in Fig. 5a,b on the y axis. We then built separate linear \nregression models for ChatGPTs and Google LLMs, using the visual \nassociation strength as a predictor of the alignment change of the visual \nLLM over the text-only LLM. The results show that, for the ChatGPT \nmodels (Fig. 5a), the visual association strength was a positive predictor \nof the alignment change of GPT-4 over GPT-3.5 (B = 0.99, t(16) = 6.16, \nP < 0.001, 95% CI 0.65 to 1.33). Approximately 70% of the variance in the \nalignment change of GPT-4 over GPT-3.5 can be explained by the visual \nassociation strength of the dimensions (R2 = 0.70), which suggests that \nvisual inputs are a major factor in improving GPT-4’s ability to align with \nhuman conceptual representations, highlighting the important role \nof visual learning in this context. For Google LLMs (Fig. 5b), the visual \nassociation strength also significantly positively predicted the align-\nment change of Gemini over PaLM (B = 0.43, t(16) = 2.38, P = 0.033, 95% \nCI 0.04 to 0.82). Approximately 26% of the variance in the alignment \nchange of Gemini over PaLM can be explained by the visual association \nstrength of the dimensions (R2 = 0.26), similarly indicating the role of \nvisual input in capturing human-like conceptual representations as \nseen in the ChatGPT models.\nValidation of the results\nControlling for word concreteness.  Will the divergence observed \nbetween LLMs and humans in sensory and motor domains persist \nwhen accounting for word concreteness? Prior work has suggested \nthat LLMs may be capable of capturing the sensorimotor aspects of \nhuman representations of abstract words, which are purportedly less \nreliant on grounding 40,41. T o assess the potential influence of word \nconcreteness on our findings, we employed two methods. First, we \ncalculated the partial Spearman correlation between human and model \nratings, controlling for word concreteness. Our analysis revealed a \nstrong similarity (R s = 0.93, P < 0.001, 95% CI 0.89 to 0.96) between \nthese partial correlations and the original correlations reported ear-\nlier (Fig. 6a), suggesting that the pattern of our results remains even \nafter adjusting for concreteness. Next, we implemented a bin analysis \nto explore variations in model–human correlations across different \nlevels of word concreteness. We first sorted the words by concrete -\nness values, then divided them into bins of 100 words each to explore \nvariations in model–human correlations across different levels of \nword concreteness. For each bin, we recorded the median concrete-\nness value and the corresponding model–human correlations for the \nwords within the bin. Therefore, we obtained model–human correla-\ntions as a function of word concreteness for each model across various \ndimensions. The data were then analysed using a linear regression \nmodel, which considered concreteness values, model and domain \nas predictors of model–human correlations (Fisher Z -transformed). \nThis model explained approximately 53% of the variance in model–\nhuman correlations. Our analysis did not identify credible evidence \nfor a significant main effect of word concreteness on model–human \ncorrelations (Fig. 6b ) (B = −0.00, t = −0.39, P = 0.785, 95% CI −0.03 \nto 0.02). Therefore, we found no credible evidence that word con -\ncreteness is associated with the alignment or divergence of models  \nwith human conceptual representations across non-sensorimotor,  \nsensory and motor domains. That said, we did observe interaction \neffects between concreteness, domain and model, suggesting, for \nexample, that model–human correlations may be stronger for more \nconcrete words in the sensory domain (see Supplementary Informa -\ntion, section 5.4, for further analyses and Supplementary Information, \nsection 7.2, for possible interpretations of these results).\nValidating LLM responses. Because of the critical need for validity in \nLLM applications18,42,51, we adhered to established human test valida-\ntion methods1,2. We evaluated the ChatGPTs (GPT-3.5 and GPT-4) and \nGoogle LLMs (PaLM and Gemini) against a set of alternate norms that \nare related to the Glasgow and Lancaster measures.\nFor the Glasgow Norms, validation norms include dimensions of \nvalence, arousal and dominance from ref. 34, imageability from ref. 52 \nand concreteness from ref. 14. For the Lancaster Norms, which lacks \ndirectly comparable validation norms, we included dimensions that \nare conceptually similar, such as taste and grasp53. In human ratings, \ntaste is expected to strongly correlate with the gustatory dimension in \nLancaster, while grasp shows moderate correlations with the hand/arm \nand haptic dimensions. We selected these validation norms for several \nreasons: (1) they are publicly accessible, (2) they have been widely used \nin human participant studies, lending to their validity and (3) they cover \ndimensions in either the Glasgow or Lancaster Norms where models \nshow strong correlations (that is, Rs > 0.6) with human data.\nAs detailed in Table 2, we first evaluated models’ responses on the \nvalidation norms, then computed Spearman correlations between \nhumans and models for these norms. Subsequently, we calculated \ncorrelations for model ratings between the original Glasgow/Lancaster \nNorms and the validation norms. We observed that model–human \ncorrelations based on the validation norms—except for Gemini’s per-\nformance on the arousal dimension—closely resembled those obtained \nfrom the Glasgow/Lancaster Norms. For instance, the correlation \nbetween human ratings and GPT-3.5 on valence was 0.83 (95% CI 0.82 \nto 0.84) in the validation norms, compared with 0.90 (95% CI 0.89 \nto 0.90) in the Glasgow Norms. Moreover, the correlation strength \nof ChatGPT ratings between the validation norms and the Glasgow/\nLancaster norms is as high as the correlation strength of human ratings \nacross these norm sets. For example, the correlation for GPT-4 ratings \non the hand/arm dimension between the validation and the Lancaster \nnorms was 0.68 (95% CI 0.62 to 0.73), compared with the 0.55 correla-\ntion of human ratings across these norms. Given the strong consist -\nency observed across different models and dimensions, these results \nsuggest that the main findings largely reflect the models’ capabilities \nrather than reliance on specific prompts (see Supplementary Table 9 \nfor additional statistics).\nDiscussion\nIn this study, we used LLMs to test the limits of conceptual knowledge \nacquisition by quantifying what aspects of human conceptual knowl-\nedge can or cannot be recovered solely from the language domain \nof learning or from a combination of language and visual domains. \nWe found that learning constrained to the language domain captures \nhuman-level conceptual representation in non-sensorimotor dimen-\nsions such as valence and emotional arousal but yields impoverished \nrepresentation of sensorimotor knowledge. Our findings extend previ-\nous research on ungrounded artificial neural models4–6 and congenitally \nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1879\nArticle https://doi.org/10.1038/s41562-025-02203-8\nblind and partially sighted people7–10, which showed alignment with \nthe conceptual representations of sighted human participants. By \nsystematically examining conceptual representations across a spec-\ntrum from non-sensorimotor to sensorimotor domains and a wide \nrange of concepts, we found a gradual decrease in similarity between \nLLM-derived and human-derived representations, with stronger dis-\nparity in sensorimotor domains. These results offer insights into the \nextent to which language can shape complex concepts and underscore \n−0.2\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nChange (GPT-4 versus GPT-3.5)\nR2  = 0.70\na\n−0.2\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nChange (Gemini versus PaLM)\nGustatory\nTorso Gender\nDominance\nHead Valence Arousal Foot/leg Auditory\nSize\nOlfactory Hand/arm\nMouth/throat Interoceptive\nHaptic\nConcreteness Imageability\nVisual\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nVisual association strength\nb\nc\nR2  = 0.26\nFig. 5 | Visual domain analysis. a,b, Using the strength of visual correlation \nto predict the degree to which visual LLMs enhance alignment with human \nconceptual representations compared with their text-only counterparts. This \ncomparison was made for ChatGPT models (GPT-4 versus GPT-3.5) (a) and \nGoogle LLMs (Gemini versus PaLM) (b). c, Visual association strength of each \ndimension: the absolute values of the Spearman correlation coefficients, based \non human ratings1,2, reflect the association strength of each dimension with \nthe visual dimension. A higher coefficient signifies a stronger link to visual \nprocessing, such as the imageability and haptic dimensions. The x axis across \nall three subplots displays the dimensions, sorted by their visual association \nstrength (as shown in c). In a, the y axis shows the difference in model–human \ncorrelations between GPT-4 and GPT-3.5. In b, similarly, the y axis denotes the \ndifference in model–human correlations between Gemini and PaLM. In c, the y \naxis indicates the visual association strength of each dimension.\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1880\nArticle https://doi.org/10.1038/s41562-025-02203-8\nthe importance of multimodal inputs for LLMs emulating human-level \nconceptual knowledge.\nIn light of the ongoing debate about the necessity of embodied \ngrounding for achieving human-level conceptual representation26,32, \nThe present study suggests that while some aspects of conceptual \nrepresentations may be detached from sensorimotory experience, a \nconsiderable degree of sensorimotor input appears essential. Take the \nconcept of ‘flower’ , for instance. Language can capture certain concep-\ntual connotations of ‘flower’ insofar as they emerge from distributional \nrelationships among words in context (for example, positive emotional \nvalence may arise from ‘this flower smells joyous’). However, the sen-\nsorimotor experience of ‘flower’ may cut across linguistic contexts \nand may implicitly shape our conceptual knowledge, to form diverse \nrelationships across objects and experiences in the world around us. \nFrom the intense aroma of a flower, the vivid silky touch when we caress \npetals, to the profound visual aesthetic sensation, human representa-\ntion of ‘flower’ binds these diverse experiences and interactions into a \ncoherent category. This type of associative perceptual learning, where \na concept becomes a nexus of interconnected meanings and sensa -\ntion strengths, may be difficult to achieve through language alone. \nReal-world interactions, similar to those in human experiences, are \nprobably essential for comprehensive sensory perception, physical \naction and perceptual representation of concepts54.\nIntriguingly, we found greater discrepancies between human and \nLLM ratings for motor-related dimensions than for sensory dimen -\nsions—an area underexplored in prior studies. Two explanations for this \nfinding are: (1) motor aspects are less frequently described in language, \nmaking them harder for LLMs to learn from language, as noted by  \nref. 55, and (2) motor representations rely more on embodied experi-\nences, unlike sensory concepts such as colour, which can be learned \nthrough language9. Motor cortex lesions also impair action-word pro-\ncessing, underscoring the need for embodiment56. This further high-\nlights LLMs’ limitations in representing motor concepts owing to the \nlack of physical commonsense and action-related input.\nThe current study exemplifies the potential benefits of multi -\nmodal learning where ‘the whole is greater than the sum of its parts’ , \nshowing how the integration of multimodal inputs can potentially \nlead to a more human-like representation than what each modality \ncould offer independently. We found that LLMs incorporating visual \ninputs align better with human representations in visual as well as \nvisual-related dimensions, such as haptics and imageability. This rep-\nresentational transfer is well observed in humans 57,58. For instance, \nhumans can acquire object-shape knowledge through both visual \nand tactile experiences57, and brain activation in the lateral occipital \ncomplex was observed during both seeing and touching objects 59. \nAkin to humans, given the architecture and learning mechanisms \nTable 2 | Spearman correlations between LLM–human \nratings for the original and validation norms, and between \noriginal–validation norms for LLM and human ratings\nModel Dimension LLM–human Original–validation\nOriginal Validation LLM Human\nGPT-3.5\nValence 0.90 0.83 0.90 0.93\nDominance 0.62 0.66 0.82 0.69\nArousal 0.64 0.47 0.55 0.62\nConcreteness 0.71 0.63 0.61 0.93\nGPT-4\nValence 0.93 0.88 0.92 0.93\nDominance 0.63 0.67 0.86 0.69\nArousal 0.64 0.43 0.54 0.62\nConcreteness 0.93 0.87 0.88 0.93\nImageability 0.91 0.77 0.83 0.89\nHaptic 0.76 0.88 0.55 0.55\nHand/arm 0.66 0.88 0.68 0.55\nPaLM valence 0.78 0.44 0.42 0.91\nGemini\nValence 0.87 0.83 0.82 0.91\nArousal 0.66 0.15 0.39 0.60\nConcreteness 0.75 0.66 0.58 0.93\nOriginal norms denote Glasgow or Lancaster. All correlations were significant (P < 0.001).\n0 0.2 0.4 0.6 0.8\nR\n0\n0.2\n0.4\n0.6\n0.8\nPartial R \nRs= 0.93\nPerfect match level\nNon-sensorimotor\nSensory\nMotor\n2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7\n2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7\n2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7\n2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7\n0\n0.5\n1.0\n1.5\n2.0\n0\n0.5\n1.0\n1.5\n2.0\n0\n0.5\n1.0\n1.5\n2.0\n0\n0.5\n1.0\n1.5\n2.0\nConcreteness\nR (Fisher Z-transformed)\nModel\nGPT3\nGPT4\nGemini\nPaLM\nNon-sensorimotor Sensory Motora b\nFig. 6 | Concreteness analysis. a, The partial Spearman correlations between \nhuman and model ratings, controlling for word concreteness, are very similar \n(Rs = 0.93) to the original correlations. The dashed identity line indicates a perfect \nmatch, where the partial correlation value is exactly the same as the original \ncorrelation value. The error bars depict the 95% confidence intervals, estimated \nby bootstrap resampling 1,000 samples of ratings. The central value represents \nthe estimated correlation coefficient between the lower and upper confidence \nbounds. b, A bin analysis of the correlations between human and model ratings \nacross different levels of word concreteness for each model (GPT-3.5, GPT-4, \nPaLM and Gemini) and domain (non-sensorimotor, sensory and motor). The \nwords were first sorted by concreteness values and then divided into bins of 100 \nwords each. The x axis shows the median concreteness value within each bin. The \ny axis denotes model–human Spearman correlations (Fisher Z-transformed) for \nwords within the bin. The fit line for each panel represents the prediction from \nthe regression model for each domain and each model.\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1881\nArticle https://doi.org/10.1038/s41562-025-02203-8\nof visual LLMs, where representations are encoded in a continuous, \nhigh-dimensional embedding space, inputs from multiple modalities \nmay fuse or shift embeddings in this space. The smooth, continuous \nstructure of this embedding space may underlie our observation that \nknowledge derived from one modality seems to spread across other \nrelated modalities 60–62. Further along this vein, our study points to \nthe possibility that models may be able to approximate human-like \nconceptual representations even without full sensorimotor experi -\nence; partial access could suffice to span much of human experience15. \nThis insight may also shed light on why similar representations were \nobserved between congenitally blind and partially sighted people \nand normally sighted people8–10. Future research should explore the \nextent of sensory access needed in multimodal models and the limits \nof knowledge transfer across different domains. The continued devel-\nopment of LLMs towards integrating additional modalities—as seen in \nmultimodal speech and text processing in Whisper 63 and embodied \nvision-language-action models such as RT-2 (ref. 64)—opens exciting \nprospects for further understanding and harnessing the potential of \nmultimodal learning. We envision a future where LLMs are augmented \nwith sensor data and robotics to actively make inferences about and \nact upon the physical world 16,17. These advances may catalyse LLMs \nto truly embrace embodied artificial representation that mirrors the \ncomplexity and richness of human cognition17,29. Within this perspec-\ntive, our findings may contribute to the trajectory of training data \nimprovement and multimodal integration.\nT o what extent do LLMs inform us about human cognition? LLMs \nrevive the old debate about whether and how distributional relation-\nships in language-domain learning can scaffold a wide range of seman-\ntic processes, reflecting the richness of linguistic inputs in shaping \nhuman knowledge20,65,66. At the same time, their limitation in captur -\ning human-like sensorimotor conceptual understanding via textual \ndata and incomplete sensorimotor input also delineates the bound -\nary of language-domain training and underscores the importance of \ngrounding for human conceptual knowledge15. In this light, LLMs offer \na valuable ‘how-possibly’ model of human cognition. Nonetheless, \nwe acknowledge that most work on the parallels between LLMs and \nhuman language processing (for example, refs. 21,22), including the \ncurrent work, has been confined to the English language. This con -\nstitutes a limitation of our study, as language structure, embodiment \neffects and neural processing could differ across languages. However, \nfindings like those of ref. 67 suggest that motor verbs in French and \nGerman elicited similar motor-related brain activations compared \nwith non-motor verbs, indicating that our English-based findings \nmight generalize to other languages. Future studies should explore \nusing diverse languages to validate and expand these insights (see \nSupplementary Information, section 7.4, for a further discussion on \nthe cognitive plausibility of LLMs).\nIt is worth noting that LLMs involve diverse learning techniques, \nwhich adds complexity to their learning dynamics and makes it valuable \nto explore how each technique contributes to the final outcomes. For \nexample, GPT-3.5 is pretrained using next-token prediction within text \nsequences and then further refined through two methods: supervised \nlearning, where human-labelled data specifies the correct output, and \nreinforcement learning with human feedback (RLHF), which enables \nthe model to improve by interacting with the external environment \nindirectly47. These two techniques could also bring in non-linguistic \nknowledge, as humans provide labels or feedback based on some \nnon-linguistic resources. We believe these techniques do not alter \nour main findings, as RLHF is constrained by mechanisms such as Kull-\nback–Leibler (KL) divergence penalties68, which prevent the fine-tuned \nmodel from diverging substantially from the pretrained version. \nAlthough RLHF may indirectly introduce human preferences based \non real-world non-linguistic experiences, the model’s learning remains \nprimarily driven by linguistic input. However, a key limitation of our \nstudy is the proprietary nature of these large models, which makes it \nchallenging to conduct open scientific research and fully understand \nthe individual effects of each learning approach, including the spe -\ncific impacts of RLHF . This opacity hinders our ability to dissect how \ndifferent techniques influence the model’s behaviour. Therefore, our \nconclusions about how LLMs acquire and process language and embod-\nied experiences should be interpreted with these considerations in \nmind, underscoring the need for greater transparency in LLM research \nto enable more systematic investigations. Notably, DeepSeek-V3  \n(ref. 69), a recent high-performance open-source model with various \npost-training optimization, shows performance comparable to GPT-4 \n(Supplementary Fig. 5), further supporting our conclusion: LLMs cap-\nture non-sensorimotor semantics well but still struggle with nuanced \nsensory and motor features of words and concepts. Future research \ncould focus on smaller, more accessible models to test and compare \nthe roles of various learning techniques, such as prediction-based \nlearning, supervised learning and interaction-based reinforcement \nlearning (see ref. 54 for an example). This is especially important given \nthat leveraging multiple knowledge resources and interacting with \nthe environment have long been recognized as crucial and efficient \nmechanisms in human language and concept development16,17,70.\nWe note that, although LLMs can approximate certain aspects \nof conceptual representation, particularly in non-sensorimotor and \noccasionally bodily dimensions, they obtain this by consuming vast \namounts of text—orders of magnitude larger than the volume of lan-\nguage a human is exposed to in their entire lifetime—and operating with \nextremely high complexity driven by billions of parameter settings20. \nThis suggests that, while in the limit multimodal knowledge can be \nsynthesized from language alone, this kind of learning is inefficient. \nBy contrast, human learning and knowledge representation are both \ninherently multimodal and embodied, and interactive from the outset15. \nAfter all, when thinking of flowers, what comes to your mind is not \nmerely their names but the vivid symphony in which sight, touch, scent \nand all your past sensorimotor experiences intertwine with profound \nemotions evoked—an experience far richer than words alone can hold.\nMethods\nInclusion and ethics\nThe study involves the collection of data from LLMs and the use of \nsecondary human-participant data1,2. For the human-participant data, \nref. 1 noted that the study followed the ethical guidelines and protocols \nestablished by the British Psychological Society. Ethical approval for \nthe study reported in ref. 2  was granted by the Lancaster University \nResearch Ethics Committee.\nPsycholinguistic norms\nWe used the Glasgow Norms1 and the Lancaster Sensorimotor Norms \n(henceforth the Lancaster Norms2) as human psycholinguistic word \nrating norms (see Table 1  for their dimensions). T ogether, the two \nnorms offer comprehensive coverage of the included dimensions, \nboth of which cover a large number of words. The Glasgow Norms col-\nlected data from 829 human participants, including 599 female and \n230 male participants in terms of gender. The original publication did \nnot specify whether sex and/or gender was determined by self-report \nor assignment. Participants ranged in age from 16 to 73 years, with \na mean of 21.7 years (standard deviation (s.d.) of 7.4). The average \nage was 21.5 years (s.d. of 7.6) for female participants and 22.3 years  \n(s.d. of 6.9) for male participants. The Lancaster Norms collected data \nfrom 3,500 human participants, including 1,644 female and 1,823 \nmale participants. A total of 12 participants chose not to disclose their \ngender, and the gender information was missing for 21 participants.  \nThe average age of all participants was 34.9 years (s.d. of 10.3).\nThe Glasgow Norms consist of normative ratings for 5,553 English \nwords across nine dimensions, collected from native English speakers \nwithin the University of Glasgow community, UK 1. We selected the \nGlasgow Norms owing to its large-scale data and highly standardized \nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1882\nArticle https://doi.org/10.1038/s41562-025-02203-8\ndata collection process: the same participants rated all dimensions for \nany given subset of words, with an average of 33 participants per word. \nThe nine dimensions include emotional arousal, valence, dominance, \nconcreteness, imageability, size, gender association, familiarity and \nage of acquisition. In our study, we excluded familiarity and age of \nacquisition, as familiarity is less dependent on semantic and con -\nceptual representation71 and, therefore, less relevant to our research \nfocus, while age of acquisition is neither central to our focus and nor a \nvalid question for LLMs to answer. The validity of the Glasgow Norms \nhas been demonstrated through strong correlations with 18 different \nsets of other psycholinguistic norms. Scott et al.1 conducted principal \ncomponent analyses and identified three main categories underly -\ning these dimensions: emotion (valence and dominance), salience \n(arousal, size and gender) and mental visualization (concreteness \nand imageability). We adopt their validated structure for categorizing \nthose dimensions.\nThe Lancaster Norms present multidimensional measures encom-\npassing sensory and motor strengths for approximately 40,000 English \nwords, collected from experienced users on Amazon’s Mechanical \nTurk platform2. These norms include six sensory dimensions (hap -\ntic, auditory, olfactory, interoceptive, visual and gustatory) and five \nmotor dimensions (foot/leg, hand/arm, mouth/throat, torso and head \nexcluding mouth). The sensorimotor properties of words are consid-\nered highly embodied, as they require human raters to utilize their \neveryday perceptual senses and bodily experiences to gauge each \nword. The data were collected from 3,500 unique participants, with \neach participant rating on average 7.12 lists for either the sensory or \nmotor dimensions. Each list comprised 58 words, including 48 target \nwords, 5 control words and 5 calibration words. The fixed sets of five \ncontrol words were randomly interspersed to each item list to ensure \nthe quality of participants’ ratings, and the five calibration words were \npresented at the beginning of each item list to introduce participants \nto unambiguous examples for rating. The Lancaster Norms were cho-\nsen primarily because they provide a detailed and comprehensive \nrepresentation of a word’s perceived sensorimotor strengths across \n11 dimensions, covering all senses and the five most common action \neffectors. The norms exhibit high reliability, displaying substantial \nconsistency across all dimensions, and their validity is demonstrated by \ntheir ability to accurately represent lexical decision-making behaviour \nfrom two distinct databases2.\nThere are differences between the Glasgow and Lancaster Norms in \nrater demographics. T o ensure that any observed differences between \nthe sensorimotor and non-sensorimotor dimensions are attributable to \nthe intended dimensions rather than these demographic differences, \nit is essential to confirm the validity of both the norms and our model’s \nresponses to them. The validity of the Glasgow and Lancaster Norms has \nbeen well established1,2, and the validity of model responses to them is \nreported in the ‘Validation of results’ section in Results.\nWe adhered to the design of the human-participant data collection \n(Fig. 1b)1,2. For the Glasgow measures, the 5,553 words were divided \ninto 40 lists, with 8 lists containing 101 words per list and 32 lists con-\ntaining 150 words per list. The models rated all words in a list for one \ndimension before moving on to the next dimension and so forth. The \norder of words within each dimension and the order of dimensions \nwithin each testing round was randomized. For the Lancaster measures, \nthere are in total 39,707 available words with cleaned and validated \nsensorimotor ratings. We first extracted 4,442 words overlapping \nwith the 5,553 words in the Glasgow measures. Following the practice \nin the Lancaster Norms, we obtained the frequency and concreteness \nmeasures14 of these 4,442 words and attempted to perform quantile \nsplits over them to generate item lists that maximally resemble those in \nthe Lancaster Norms. However, since more than 95% of the 4,442 words \nhave a ‘percentage of being known’ greater than 95%, we considered the \nmajority of these words to be recognizable by human raters. Thus, we \ndid not perform a quantile split of these words over word frequency. \nWe instead implemented a quantile split based on their concreteness \nratings with four quantile bins in the intervals 1.19–2.46, 2.46–3.61, \n3.61–4.57 and 4.57–5.00.\nNext, we generated four sublists based on the concreteness rating \nquantile split and randomly selected 12 words from each sublist without \nreplacement to create 48 words for each item list. We further appended \nthe five calibration words (sensory dimensions: account, breath, echo, \nhungry and liquid; motor dimensions: shell, tourism, driving, breathe \nand listen) to the beginning of each list. Finally, we randomly inserted \nfive control words (sensory dimensions: grass, honey, laughing, noisy \nand republic; motor dimensions: bite, enduring, moving, stare and \nvintage) into these lists to form 93 complete item lists, each containing  \n58 words ready to be rated separately for sensory and motor dimen -\nsions. The order of words within each item list and the order of dimen-\nsions to rate for each round were randomized.\nModels\nWe employed the gpt-3.5-turbo-0301 and gpt-4 (collected between  \n28 May and 11 June 2023) from the OpenAI API for GPT-3.5 and GPT-4 and \nthe PaLM2 (PaLM2 ratings of 2,474 words for the sensory dimensions \nwere collected and PaLM2 ratings of 4,095 words for motor dimensions \nwere collected since PaLM2 failed at returning ratings for several lists of \nwords in each model run) and gemini-1.0-pro from the Google API for \nPaLM and Gemini. The selection of parameters in our study was based \non methodological considerations aimed at optimizing the accuracy \nand consistency of the model outputs. The temperature parameter \nwas set to 0, following recommendations described previously 21,22) \nto ensure deterministic, consistent responses without random varia-\ntions. The maximum token length was set to the upper limits permit-\nted—2,048 tokens for GPT-3.5, GPT-4 and Gemini and 1,024 tokens for \nPaLM—to avoid truncating responses. T o enhance the reliability of our \nresults, we implemented four rounds of testing for each model. This \napproach allowed us to cross-verify the consistency of the outputs \nacross multiple iterations (see Supplementary Information, section \n1, for the agreement between these rounds).\nTesting procedure\nThe model prompt to ChatGPT was kept identical to the instruc -\ntions that human participants received. However, we made minor \nadjustments to the prompt to ensure that the responses followed the \nexpected format (for example, word – rating). When given testing items \nfrom the Lancaster Norms, the model consistently responded that it \ndoes not possess a biological body and, therefore, cannot experience \nthe word through sensing or moving. T o address this, we modified \nthe instruction from ‘to what extent do you experience’ to ‘to what \nextent do human beings experience’ , and we applied the same changes \nto the Glasgow Norms for consistency. Although the LLM is asked \nto respond on the basis of human experience, it is still utilizing its \ninternal representations to provide answers. These representations \nare derived from extensive training on human-generated text, which \nmakes the responses valid as a reflection of the collective conceptual \nrepresentation of humans.\nThe images or tables used in human-participants tasks were  \nconverted to text format. Moreover, the online rating portal of the \nLancaster Norms used a graphic demonstration of the five body parts \nfor the action-executing effector ratings. Because GPT-3.5 and PaLM do \nnot support such visual inputs in the prompts, we decided instead to \ndescribe these five body parts with words in the prompts for all of the \nmodels (see Supplementary Information, section 2, for a comparison \nbetween the instructions given to human participants and the adapted \nversion provided to the models).\nWords analysed\nFor more uniform comparisons across various dimensions, we \nrestricted our analysis of sensory and motor domains to words common \nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1883\nArticle https://doi.org/10.1038/s41562-025-02203-8\nto both the Glasgow and Lancaster Norms (4,442 words). Still, we \nretained the full Glasgow Norms (5,553 words) for the non-sensorimotor \ndomain. Each of the overlapped 4,442 words has corresponding rat-\nings across all evaluated dimensions. In occasional instances, models \nclassified certain words as ‘unknown’ or unable to gauge (PaLM failed \nat generating all 4,442 words as detailed above). These words are typi-\ncally those that may contravene content policies or that models such \nas PaLM struggle to interpret. In line with the practices described in \nrefs. 1,2, such data points (that is, scores from individual runs) were \nexcluded from the data analyses. T o ensure that our results were not \nbiased by words present in the Glasgow Norms but are not included in \nthe Lancaster Norms, we conducted separate tests using only the fully \noverlapping concepts (4/5 of the Glasgow Norms) and found highly \nconsistent results (Supplementary Information, section 6).\nIndividual-level pairwise correlations\nFor individual-level analysis, we computed pairwise Spearman cor -\nrelations for each pair of individual human participants and between \neach human and individual runs of GPT-3.5, GPT-4, Gemini and PaLM. \nIn the human–human correlations, each participant evaluated only \na subset of words. In the Glasgow Norms, participants rated one of \neither 8 lists (comprising 808 words in total, with 101 words per list) or \n32 lists (from a pool of 4,800 words, with 150 words per list). Each list \nreceived ratings from 32–36 participants, and there was no overlap in \nwords across different lists. The pairwise correlations were calculated \nwithin each list, and these were aggregated, resulting in a total of 22,730 \npairs for constructing the overall distribution for each dimension in \nthe Glasgow Norms.\nIn the Lancaster Norms, the sensory component involved 2,625 \nparticipants (averaging 5.99 lists each) and the motor component \nhad 1,933 participants (averaging 8.67 lists each). Each list included \n48 test items, along with a constant set of five calibration and five con-\ntrol words, totalling 58 items per list. Given the larger pool of 40,000 \nwords in the Lancaster Norms, the subset of 4,442 words resulted in \nsome participants rating few items. T o maintain a sufficient sample \nsize for correlation calculations, we iterated through pairs of partici-\npants and included those with ratings for over 50 common words. This \napproach yielded 105 pairs for every sensory dimension and 196 pairs \nfor every motor dimension, from which we constructed the correla -\ntion distributions.\nIn the human model correlations, we generated pairs by match -\ning each model run (out of four total runs) with individual human \nparticipants across different lists. This approach yielded 5,476 pairs \nfor the Glasgow Norms. For the Lancaster Norms, we paired humans \nand models based on having ratings for over 50 common words, mir-\nroring the approach used in constructing human–human pairs. This \nprocess resulted in a total of 224 pairs for each sensory dimension and \n440 pairs for each motor dimension, forming the basis for the correla-\ntion distributions.\nRSA\nFor the RSA analysis, we first iterated through human rating data \nfrom the Glasgow and Lancaster Norms, extracting ratings across \nthe non-sensorimotor, sensory and motor domains for lists of words \nrated by individual human participants. Each word was represented \nby a vector containing human ratings for each domain (for example, a \nvector for the sensory domain included ratings from six typical senses). \nNext, with these vectors, we built RDMs by calculating pairwise Euclid-\nean distances between words for each list rated by individual human \nparticipants. This process was repeated for all three domains. Finally, \nwe compared the RDMs derived from individual human ratings with \nmodel RDMs. The model RDMs were constructed using averaged rat-\nings across four runs generated by the GPT models and Google models \nfor the same words in each human word list. The comparison between \nhuman and model RDMs was conducted using Spearman correlation.\nT o ensure consistency and maintain a sufficient sample size for the \nRSA analysis, we only paired human and model data that had at least \n50 shared words in each of the non-sensorimotor, sensory and motor \ndomains for each model. As a result, we retained 829 pairs of RDMs from \nthe Glasgow Norms for the non-sensorimotor domain RSA, applicable \nto both GPT and Google models. For the Lancaster Norms, we retained \n435 pairs of RDMs for the sensory domain RSA and 443 pairs for the \nmotor domain RSA with the GPT models. For the Google models, we \nretained 272 pairs of RDMs for the sensory domain RSA and 323 pairs \nfor the motor domain RSA.\nMeasuring similarities\nSpearman correlations were used for most similarity measurements, a \ncommon practice in many previous studies44,72, as they are known to be \nrobust with respect to outliers. For better presentation of correlations, \nwe follow standard benchmarks: values under 0.10 are negligible, 0.10 \nis small, 0.30 medium and 0.50 or higher large 73. We used Euclidean \ndistance instead of Spearman correlations in only one case: when con-\nstructing RDMs for each word pair separately for humans and models. \nThis decision was due to the small dimensionality of word rating vectors \nin each domain—N = 7 for the non-sensorimotor domain, N = 6 for the \nsensory domain and N = 5 for the motor domain. In such small dimen-\nsions, the ranking process used in Spearman correlations becomes less \nreliable and more sensitive to small variations in data 74, diminishing \nthe ability to distinguish between different levels of similarity as the \nnumber of elements (that is, ranks) decreases. Conversely, Euclidean \ndistance measures the ‘straight line’ distance between points in multi-\ndimensional space and is based on actual values rather than rankings. \nThis measure tends to be more stable in cases of small dimensionality \nbecause it does not rely on rank-order relationships but on the actual \ndifferences in values across dimensions. However, because Euclidean \ndistance is sensitive to the scale of the data, we normalized the rating \nvalues of each dimension using a z-score before obtaining vectors for \neach word in each domain.\nSignificance testing\nAll reported statistics are based on two-sided tests. The P  values \nreported in each section were corrected for multiple comparisons using \nthe FDR method43. In the Results section, the correction accounted for \n157 tests, encompassing 72 aggregated-level dimension-wise correla-\ntions, 72 individual-level t-test comparisons between human–human \ndistributions and model–human distributions, 4 Mann–Whitney  \nU tests, 1 χ 2 test and 8 comparisons for RSA analyses. For the ‘Link -\ning additional visual training to model–human alignment’ section \nin Results, the correction covered 20 tests, including 18 correlations \nbetween each dimension and the visual dimension and 2 t-tests assess-\ning the predictive effect of visual correlation strength. In the ‘Validation \nof results’ section in Results, the correction was applied over 79 tests, \nwhich include 1 test for the comparison between the partial and the \noriginal correlation values, 47 tests for the bin analysis and 30 tests for \nusing validation norms to validate model responses.\nFor the dimension-wise correlation analyses in the Results \nsection on aggregated model and human ratings, we utilized the \nMann–Whitney U test for independent-sample non-parametric com-\nparisons of model–human similarities between sensorimotor and \nnon-sensorimotor domains. This approach was selected owing to the \nsmall sample size (in this context, the number of dimensions within \neach domain) and the violation of normal distribution assumptions \nby the data. Non-parametric tests, although generally less power -\nful than parametric tests, are robust to outliers in such scenarios. \nMoreover, we reported the effect size using the rank-biserial cor -\nrelation (rrb), a common measure for non-parametric tests. For the \ndimension-wise correlation analyses in the individual-level analysis, \nwe utilized independent-sample t -tests to determine whether the \ndistributions of model–human similarities differed significantly from \nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1884\nArticle https://doi.org/10.1038/s41562-025-02203-8\nhuman–human similarities. This approach was based on the assump-\ntion of normality and the presence of a large sample size. We conducted \na separate t-test for each dimension. T o organize the results of multiple \nt-tests effectively, we counted, within each domain, the number of \ninstances where the distributions of model–human similarities were \nnot significantly lower than those of human–human similarities, as \nindicated by the t-tests. A χ2 test of independence was then performed \nto assess whether the counts varied significantly across the domains \n(non-sensorimotor, sensory or motor).\nFor the RSA analysis (‘RSA’ section in Results), after obtaining \ndistributions of similarities between all human subjects and each \nmodel separately for each domain, we conducted a 3 × 2 (domain \nlevels by models, respectively) two-way ANOVA for each set of mod-\nels—ChatGPTs and Google LLMs—separately. This separation was \nto assess the consistency of main effects of domain across the two \nLLM families. Owing to violations of the equal variances assump -\ntion, we used the Satterthwaite’s method for the ANOVA tests and \napplied Welch’s corrections for post hoc pairwise comparisons. In \nthe linear regression analyses (‘Linking additional visual training \nto model–human alignment’ section and ‘Validation of results’ sec -\ntion in Results), we conducted analyses after checking the assump -\ntions of linearity, independence of residuals and normality. While \nthe regression models in the ‘Linking additional visual training to \nmodel–human alignment’ section in Results meet all assumptions, \nthe model in the ‘Validation of results’ section in Results shows a \nslight violation of the normality of residuals assumption, as indi -\ncated by the Normal Q–Q plot. T o ensure the reliability of the results \nin the ‘Validation of results’ section in Results, we conducted an \nadditional Bayesian linear regression analysis for cross-validation \n(Supplementary Information, section 5.4). In the ‘Linking addi -\ntional visual training to model–human alignment’ section in \nResults, the Fisher Z- transformation was applied to the Spearman  \nR values to measure the difference between two correlation coef-\nficients, a practice that is justified by ref. 75.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nData obtained from ChatGPTs and Google LLMs are publicly available \nat https://osf.io/kguwd/. The human dataset of the Glasgow Norms is \nfrom ref. 1, with word-level data accessible at https://doi.org/10.3758/\ns13428-018-1099-3 (ref. 1 ). The corresponding trial-level data was \nkindly provided by Sara Sereno and Jack Taylor. The Lancaster Norms \nis from ref. 2, and the data including both word-level and trial-level \ncan be found at https://embodiedcognitionlab.shinyapps.io/senso -\nrimotor_norms/ (ref. 76). The validation norms datasets are openly \navailable via the following links: the datasets of valence, arousal and \ndominance at https://link.springer.com/article/10.3758/s13428-012-\n0314-x#SecESM1 (ref. 34 ), the imageability norms at https://link.\nspringer.com/article/10.3758/BF03195585#SecESM1 (ref. 52), the con-\ncreteness norms at https://link.springer.com/article/10.3758/s13428-\n013-0403-5#MOESM1 (ref. 4) and the perceptual strength norms at \nhttps://link.springer.com/article/10.3758/s13428-012-0215-z#SecESM1 \n(ref. 3). Source data are provided with this paper.\nCode availability\nThe data collection and analyses were conducted using Python and R. \nAll code is publicly available at https://osf.io/kguwd/. In addition, we \ndeveloped an analysis pipeline that enables researchers to examine \ntheir models of interest. As new models continue to emerge, we will \nregularly update the repository to ensure its ongoing relevance for the \nresearch community. The pipeline and associated resources are also \naccessible via GitHub at https://github.com/qxu1994/LLM_grounding.\nReferences\n1. Scott, G. G., Keitel, A., Becirspahic, M., Yao, B. & Sereno, S. C.  \nThe Glasgow norms: ratings of 5,500 words on nine scales.  \nBehav. Res. Methods 51, 1258–1270 (2019).\n2. Lynott, D., Connell, L., Brysbaert, M., Brand, J. & Carney, J. The \nLancaster sensorimotor norms: multidimensional measures of \nperceptual and action strength for 40,000 English words.  \nBehav. Res. Methods 52, 1271–1291 (2020).\n3. Barsalou, L. W. Grounded cognition. Annu. Rev. Psychol. 59, \n617–645 (2008).\n4. Patel, R. & Pavlick, E. Mapping language models to grounded \nconceptual spaces. In International Conference on Learning \nRepresentations (2022).\n5. Grand, G., Blank, I. A., Pereira, F. & Fedorenko, E. Semantic \nprojection recovers rich human knowledge of multiple object \nfeatures from word embeddings. Nat. Hum. Behav. 6, 975–987 \n(2022).\n6. Marjieh, R., Sucholutsky, I., van Rijn, P., Jacoby, N. & Griffiths, T. L. \nLarge language models predict human sensory judgments across \nsix modalities. Sci. Rep. 14, 21445 (2024).\n7. Bi, Y. Dual coding of knowledge in the human brain. Trends Cogn. \nSci. 25, 883–895 (2021).\n8. Wang, X., Men, W., Gao, J., Caramazza, A. & Bi, Y. Two forms of \nknowledge representations in the human brain. Neuron 107, \n383–393 (2020).\n9. Kim, J. S., Aheimer, B., Montané Manrara, V. & Bedny, M. Shared \nunderstanding of color among sighted and blind adults.  \nProc. Natl Acad. Sci. USA 118, e2020192118 (2021).\n10. Bottini, R. et al. Brain regions involved in conceptual retrieval \nin sighted and blind people. J. Cogn. Neurosci. 32, 1009–1025 \n(2020).\n11. Banks, B. & Connell, L. Multi-dimensional sensorimotor grounding \nof concrete and abstract categories. Philos. Trans. R. Soc. B 378, \n20210366 (2023).\n12. Pexman, P. M., Diveica, V. & Binney, R. J. Social semantics: the \norganization and grounding of abstract concepts. Philos. Trans. R. \nSoc. B 378, 20210363 (2023).\n13. Lenci, A., Baroni, M., Cazzolli, G. & Marotta, G. Blind: a set of \nsemantic feature norms from the congenitally blind. Behav. Res. \nMethods 45, 1218–1233 (2013).\n14. Brysbaert, M., Warriner, A. B. & Kuperman, V. Concreteness ratings \nfor 40 thousand generally known English word lemmas.  \nBehav. Res. Methods 46, 904–911 (2014).\n15. Dove, G. Symbol ungrounding: what the successes (and failures) \nof large language models reveal about human cognition.  \nPhilos. Trans. B 379, 20230149 (2024).\n16. Pezzulo, G., Parr, T., Cisek, P., Clark, A. & Friston, K. Generating \nmeaning: active inference and the scope and limits of passive ai. \nTrends Cogn. Sci. 28, 97–112 (2024).\n17. Borghi, A. M., De Livio, C., Mannella, F., Tummolini, L. & Nolfi, S.  \nExploring the prospects and challenges of large language \nmodels for language learning and production. Sist. Intell. 35, \n361–378 (2023).\n18. Frank, M. C. Large language models as models of human \ncognition. Preprint at PsyArXiv https://doi.org/10.31234/osf.io/\nwxt69 (2023).\n19. Blank, I. A. What are large language models supposed to model? \nTrends Cog. Sci. 27, 987–989 (2023).\n20. Connell, L. & Lynott, D. What can language models tell us  \nabout human cognition? Curr. Dir. Psychol. Sci. 33, 181–189  \n(2024).\n21. Binz, M. & Schulz, E. Using cognitive psychology to understand \nGPT-3. Proc. Natl Acad. Sci. USA 120, e2218523120 (2023).\n22. Kosinski, M. Evaluating large language models in theory of mind \ntasks. Proc. Natl Acad. Sci. USA 121, e2405460121 (2024).\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886\n 1885\nArticle https://doi.org/10.1038/s41562-025-02203-8\n23. Cai, Z. G., Haslett, D. A., Duan, X., Wang, S. & Pickering, M. J. \nDo large language models resemble humans in language use? \nIn Proc. Workshop on Cognitive Modeling and Computational \nLinguistics 37–56 (ACL, 2024).\n24. Piantadosi, S. in From Fieldwork to Linguistic Theory  \n(eds Gibson, E. & Poliak, M.) 353–414 (Language Science, 2024).\n25. Piantadosi, S. T. & Hill, F. Meaning without reference in large \nlanguage models. In NeurIPS 2022 Workshop on Neuro Causal and \nSymbolic AI (nCSI) (NeurIPS, 2022).\n26. Jacob, B. et al. Do large language models need sensory \ngrounding for meaning and understanding? The Philosophy of \nDeep Learning https://phildeeplearning.github.io/ (2023).\n27. Mitchell, M. AI’s challenge of understanding the world. Science \n382, eadm8175 (2023).\n28. Li, K. et al. Emergent world representations: Exploring a sequence \nmodel trained on a synthetic task. In 11th International Conference \non Learning Representations (2023).\n29. Lupyan, G., Rahman, R. A., Boroditsky, L. & Clark, A. Effects of \nlanguage on visual perception. Trends Cogn. Sci. 24, 930–944 \n(2020).\n30. Suffill, E., van Paridon, J. & Lupyan, G. Verbal labels increase \nconceptual alignment. In Joint Conference on Language Evolution \n(2022).\n31. Lake, B. M. & Murphy, G. L. Word meaning in minds and machines. \nPsychological Rev. 130, 401 (2023).\n32. Chemero, A. LLMs differ from human cognition because they are \nnot embodied. Nat. Hum. Behav. 7, 1828–1829 (2023).\n33. Warstadt, A. & Bowman, S. R. in Algebraic Structures in Natural \nLanguage (eds. Lappin, S. & Bernardy, J.-P.) (CRC, 2022).\n34. Warriner, A. B., Kuperman, V. & Brysbaert, M. Norms of valence, \narousal, and dominance for 13,915 English lemmas. Behav. Res. \nMethods 45, 1191–1207 (2013).\n35. Pereira, F. et al. Toward a universal decoder of linguistic meaning \nfrom brain activation. Nat. Commun. 9, 963 (2018).\n36. Connell, L., Lynott, D. & Banks, B. Interoception: the forgotten \nmodality in perceptual grounding of abstract and concrete \nconcepts. Philos. Trans. R. Soc. B 373, 20170143 (2018).\n37. Niedenthal, P. M., Winkielman, P., Mondillon, L. & Vermeulen, N. \nEmbodiment of emotion concepts. J. Personal. Soc. Psychol. 96, \n1120 (2009).\n38. Zhong, Y., Huang, C. -R. & Ahrens, K. In Embodied Grounding of \nConcreteness/Abstractness: a Sensory-Perceptual Account of \nConcrete and Abstract Concepts in Mandarin Chinese (eds. Dong, \nM. et al.) 72–83 (Springer International, 2022).\n39. Connell, L. & Lynott, D. Strength of perceptual experience \npredicts word processing performance better than concreteness \nor imageability. Cognition 125, 452–465 (2012).\n40. Sakreida, K. et al. Are abstract action words embodied? An fMRI \ninvestigation at the interface between language and motor \ncognition. Front. Hum. Neurosci. 7, 125 (2013).\n41. Fini, C., Era, V., Da Rold, F., Candidi, M. & Borghi, A. M. Abstract \nconcepts in interaction: the need of others when guessing \nabstract concepts smooths dyadic motor interactions.  \nR. Soc. Open Sci. 8, 201205 (2021).\n42. Shapira, N. et al. Clever hans or neural theory of mind? Stress \ntesting social reasoning in large language models. In Proc. \n18th Conference of the European Chapter of the Association for \nComputational Linguistics Vol. 1 (eds Graham, Y. & Purver, M.) \n2257–2273 (ACL, 2024).\n43. Benjamini, Y. & Yekutieli, D. The control of the false discovery rate \nin multiple testing under dependency. Ann. Stat. 29, 1165–1188 \n(2001).\n44. Kriegeskorte, N., Mur, M. & Bandettini, P. Representational \nsimilarity analysis—connecting the branches of systems \nneuroscience. Front. Syst. Neurosci. 2, 4 (2008).\n45. OpenAI. Gpt-4 technical report. Preprint at https://doi.org/ \n10.48550/arXiv.2303.08774 (2024).\n46. Team, G. et al. Gemini: a family of highly capable multimodal \nmodels. Preprint at https://doi.org/10.48550/arXiv.2312.11805 \n(2023).\n47. OpenAI. Introducing ChatGPT. ChatGPT https://openai.com/\nindex/chatgpt/ (2022).\n48. Anil, R. et al. Palm 2 technical report. Preprint at https://doi.org/ \n10.48550/arXiv.2305.10403 (2023).\n49. Kosslyn, S. M., Ganis, G. & Thompson, W. L. Neural foundations of \nimagery. Nat. Rev. Neurosci. 2, 635–642 (2001).\n50. Pearson, J. The human imagination: The cognitive neuroscience \nof visual mental imagery. Nat. Rev. Neurosci. 20, 624–634 (2019).\n51. Ma, X., Gao, L. & Xu, Q. Tomchallenges: A principle-guided \ndataset and diverse evaluation tasks for exploring theory of mind. \nIn Proc. 27th Conference on Computational Natural Language \nLearning (CoNLL) 15–26 (ACL, 2023).\n52. Cortese, M. J. & Fugett, A. Imageability ratings for 3,000 \nmonosyllabic words. Behav. Res. Methods, Instrum., Computers \n36, 384–387 (2004).\n53. Amsel, B. D., Urbach, T. P. & Kutas, M. Perceptual and motor \nattribute ratings for 559 object concepts. Behav. Res. Methods 44, \n1028–1041 (2012).\n54. Vong, W. K., Wang, W., Orhan, A. E. & Lake, B. M. Grounded \nlanguage acquisition through the eyes and ears of a single child. \nScience 383, 504–511 (2024).\n55. Bisk, Y. et al. Piqa: Reasoning about physical commonsense \nin natural language. In Proc. AAAI Conference on Artificial \nIntelligence Vol. 34, 7432–7439 (2020).\n56. Kemmerer, D., Rudrauf, D., Manzel, K. & Tranel, D. Behavioral \npatterns and lesion sites associated with impaired processing of \nlexical and conceptual knowledge of actions. Cortex 48, 826–848 \n(2012).\n57. Peelen, M. V., He, C., Han, Z., Caramazza, A. & Bi, Y. Nonvisual and \nvisual object shape representations in occipitotemporal cortex: \nevidence from congenitally blind and sighted adults. J. Neurosci. \n34, 163–170 (2014).\n58. Noppeney, U., Friston, K. J. & Price, C. J. Effects of visual \ndeprivation on the organization of the semantic system. Brain 126, \n1620–1627 (2003).\n59. Amedi, A., Jacobson, G., Hendler, T., Malach, R. & Zohary, E. \nConvergence of visual and tactile shape processing in the human \nlateral occipital complex. Cereb. cortex 12, 1202–1212 (2002).\n60. Shepard, R. N. Toward a universal law of generalization for \npsychological science. Science 237, 1317–1323 (1987).\n61. Landauer, T. K. & Dumais, S. T. A solution to Plato’s problem: The \nlatent semantic analysis theory of acquisition, induction, and \nrepresentation of knowledge. Psychological Rev. 104, 211 (1997).\n62. Mikolov, T., Yih, W.-t. & Zweig, G. Linguistic regularities \nin continuous space word representations. In Proc. 2013 \nConference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n746–751 (ACL, 2013).\n63. Radford, A. et al. Robust speech recognition via large-scale weak \nsupervision. In Proc. 40th International Conference on Machine \nLearning (ICML'23) (2023).\n64. Brohan, A. et al. Rt-2: Vision–language–action models transfer \nweb knowledge to robotic control. In 7th Annual Conference on \nRobot Learning (2023).\n65. Elman, J. L. Finding structure in time. Cogn. Sci. 14, 179–211 (1990).\n66. Elman, J. L. Rethinking Innateness: A Connectionist Perspective on \nDevelopment (MIT Press, 1996).\n67. Monaco, E. et al. Embodiment of action-related language in the \nnative and a late foreign language—an FMRI-study. Brain Lang. \n244, 105312 (2023).\nNature Human Behaviour | Volume 9 | September 2025 | 1871–1886 1886\nArticle https://doi.org/10.1038/s41562-025-02203-8\n68. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. Adv. Neural Inf. Process. Syst. 35,  \n27730–27744 (2022).\n69. Liu, A. et al. DeepSeek-V3 Technical Report. Preprint at  \nhttps://arxiv.org/abs/2412.19437 (2025).\n70. Yu, C. & Smith, L. B. The social origins of sustained attention in \none-year-old human infants. Curr. Biol. 26, 1235–1240  \n(2016).\n71. Balota, D. A., Pilotti, M. & Cortese, M. J. Subjective frequency \nestimates for 2,938 monosyllabic words. Mem. Cognition 29, \n639–647 (2001).\n72. Nastase, S. A. et al. Attention selectively reshapes the geometry \nof distributed semantic representation. Cereb. Cortex 27, \n4277–4291 (2017).\n73. Cohen, J. Statistical Power Analysis for the Behavioral Sciences  \n2nd edn (Routledge, 1988).\n74. Bonett, D. G. & Wright, T. A. Sample size requirements for \nestimating pearson, kendall and spearman correlations. \nPsychometrika 65, 23–28 (2000).\n75. Myers, L. & Sirois, M. J. Spearman correlation coefficients, \ndifferences between. In Encyclopedia of Statistical Sciences  \n(eds. Kotz, S. et al.) (Wiley Online Library, 2006).\n76. Lynott, D., Connell, L., Brysbaert, M., Brand, J. & Carney, J. \nLancaster sensorimotor strength norms: online interactive tool; \nhttps://embodiedcognitionlab.shinyapps.io/sensorimotor_norms/ \n(Lancaster University, 2025).\n77. Flaticon terms of use. Flaticon https://www.flaticon.com/legal \n(2024).\nAcknowledgements\nThis research was supported by a grant from the Hong Kong Research \nGrants Council (project no. PolyU15607623) and the Sin Wai Kin \nFoundation (P.L.), the Basque Government through the BERC 2022-\n2025 programme and the Spanish State Research Agency through \nBCBL Severo Ochoa excellence accreditation (grant no. CEX2020-\n001010/AEI/10.13039/501100011033 to Q.X.) and the Research \nPostgraduate Scholarships from the Hong Kong Polytechnic University \n(Y.P.). The funders had no role in study design, data collection and \nanalysis, decision to publish or preparation of the manuscript.  \nWe express our sincere thanks to V. Sloutsky, V. Valian, J. Magnuson \nand S. Prasada, as well as to all members of the Brain, Language,  \nand Computation Lab, for their invaluable feedback. Finally, we \nthank S. Sereno and J. Taylor for sharing the trial-level dataset of the \nGlasgow Norms.\nAuthor contributions\nQ.X., Y.P. and P.L. conceived the project and designed the analyses. \nQ.X., Y.P. and M.W. collected the data and conducted the analyses. \nP.L., S.A.N. and M.C. supervised the project. All authors wrote the \nmanuscript and provided critical feedback.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s41562-025-02203-8.\nCorrespondence and requests for materials should be addressed to \nQihui Xu or Ping Li.\nPeer review information Nature Human Behaviour thanks Laura \nBechtold, Yanchao Bi, Anna Borghi, Dermot Lynott and the other, \nanonymous, reviewer(s) for their contribution to the peer review of this \nwork. Peer reviewer reports are available.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and \nreproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if you modified the licensed \nmaterial. You do not have permission under this licence to share \nadapted material derived from this article or parts of it. The images \nor other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit \nline to the material. If material is not included in the article’s Creative \nCommons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain \npermission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025, corrected publication 2025\n\n\n\n\n\n",
  "topic": "Cognitive psychology",
  "concepts": [
    {
      "name": "Cognitive psychology",
      "score": 0.597237229347229
    },
    {
      "name": "Cognitive science",
      "score": 0.5799956917762756
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5762494802474976
    },
    {
      "name": "Representation (politics)",
      "score": 0.5587761402130127
    },
    {
      "name": "Stimulus modality",
      "score": 0.5187859535217285
    },
    {
      "name": "Modalities",
      "score": 0.5106351971626282
    },
    {
      "name": "Psychology",
      "score": 0.4996776580810547
    },
    {
      "name": "Sensory system",
      "score": 0.3878214359283447
    },
    {
      "name": "Computer science",
      "score": 0.34025418758392334
    },
    {
      "name": "Communication",
      "score": 0.3234062194824219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.30621981620788574
    },
    {
      "name": "Sociology",
      "score": 0.20950108766555786
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I174216632",
      "name": "City University of New York",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39694355",
      "name": "Hunter College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I121847817",
      "name": "The Graduate Center, CUNY",
      "country": "US"
    }
  ]
}