{
  "title": "GMILT: A Novel Transformer Network That Can Noninvasively Predict EGFR Mutation Status",
  "url": "https://openalex.org/W4286377440",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5064149229",
      "name": "Wei Zhao",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5005606057",
      "name": "Weidao Chen",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100447671",
      "name": "Ge Li",
      "affiliations": [
        "Central South University",
        "Xiangya Hospital Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5017404634",
      "name": "Lei Du",
      "affiliations": [
        "University of Cincinnati Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5020501435",
      "name": "Jiancheng Yang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5036773978",
      "name": "Yanjing Chen",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5073877129",
      "name": "Yingjia Jiang",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5063317105",
      "name": "Jiangfen Wu",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5014362734",
      "name": "Bingbing Ni",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5103635090",
      "name": "Yeqi Sun",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5101602342",
      "name": "Shaokang Wang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100526033",
      "name": "Yingli Sun",
      "affiliations": [
        "Fudan University",
        "Huadong Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100351484",
      "name": "Ming Li",
      "affiliations": [
        "Fudan University",
        "Huadong Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100361941",
      "name": "Jun Liu",
      "affiliations": [
        "Central South University",
        "Second Xiangya Hospital of Central South University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2999417355",
    "https://openalex.org/W2992931937",
    "https://openalex.org/W2095673635",
    "https://openalex.org/W2166084034",
    "https://openalex.org/W2944860885",
    "https://openalex.org/W2121641332",
    "https://openalex.org/W3131413987",
    "https://openalex.org/W2910380368",
    "https://openalex.org/W3155187473",
    "https://openalex.org/W2943960805",
    "https://openalex.org/W3159451613",
    "https://openalex.org/W2024896552",
    "https://openalex.org/W2991567593",
    "https://openalex.org/W3085046840",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3126296879",
    "https://openalex.org/W3091730259",
    "https://openalex.org/W3043535018",
    "https://openalex.org/W3016532273",
    "https://openalex.org/W2977942577",
    "https://openalex.org/W2083927153",
    "https://openalex.org/W6747701563",
    "https://openalex.org/W6780049277",
    "https://openalex.org/W2520774990",
    "https://openalex.org/W6767164110",
    "https://openalex.org/W6766196973",
    "https://openalex.org/W3110386488",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6640036494",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W2946185430",
    "https://openalex.org/W3198607162",
    "https://openalex.org/W3176087273",
    "https://openalex.org/W2938929416",
    "https://openalex.org/W2757501900",
    "https://openalex.org/W2620325446",
    "https://openalex.org/W2287231805",
    "https://openalex.org/W3185262280",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W3196488607",
    "https://openalex.org/W3201250852",
    "https://openalex.org/W3035006120",
    "https://openalex.org/W3139375602",
    "https://openalex.org/W4283809036",
    "https://openalex.org/W2968917279",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3102564565",
    "https://openalex.org/W3039866126"
  ],
  "abstract": "Noninvasively and accurately predicting the epidermal growth factor receptor (EGFR) mutation status is a clinically vital problem. Moreover, further identifying the most suspicious area related to the EGFR mutation status can guide the biopsy to avoid false negatives. Deep learning methods based on computed tomography (CT) images may improve the noninvasive prediction of EGFR mutation status and potentially help clinicians guide biopsies by visual methods. Inspired by the potential inherent links between EGFR mutation status and invasiveness information, we hypothesized that the predictive performance of a deep learning network can be improved through extra utilization of the invasiveness information. Here, we created a novel explainable transformer network for EGFR classification named gated multiple instance learning transformer (GMILT) by integrating multi-instance learning and discriminative weakly supervised feature learning. Pathological invasiveness information was first introduced into the multitask model as embeddings. GMILT was trained and validated on a total of 512 patients with adenocarcinoma and tested on three datasets (the internal test dataset, the external test dataset, and The Cancer Imaging Archive (TCIA) public dataset). The performance (area under the curve (AUC) =0.772 on the internal test dataset) of GMILT exceeded that of previously published methods and radiomics-based methods (i.e., random forest and support vector machine) and attained a preferable generalization ability (AUC =0.856 in the TCIA test dataset and AUC =0.756 in the external dataset). A diameter-based subgroup analysis further verified the efficiency of our model (most of the AUCs exceeded 0.772) to noninvasively predict EGFR mutation status from computed tomography (CT) images. In addition, because our method also identified the \"core area\" of the most suspicious area related to the EGFR mutation status, it has the potential ability to guide biopsies.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\nGMILT: A Novel Transformer Network That Can\nNoninvasively Predict EGFR Mutation Status\nWei Zhao , Weidao Chen, Ge Li, Du Lei, Jiancheng Yang, Member, IEEE, Yanjing Chen , Yingjia Jiang,\nJiangfen Wu , Bingbing Ni , Yeqi Sun, Shaokang Wang, Yingli Sun, Ming Li, and Jun Liu\nAbstract— Noninvasively and accurately predicting the epi-\ndermal growth factor receptor (EGFR) mutation status is a\nclinically vital problem. Moreover, further identifying the most\nsuspicious area related to the EGFR mutation status can guide\nthe biopsy to avoid false negatives. Deep learning methods\nbased on computed tomography (CT) images may improve the\nnoninvasive prediction of EGFR mutation status and potentially\nManuscript received 25 March 2022; revised 15 June 2022; accepted\n8 July 2022. This work was supported in part by the National Natural\nScience Foundation of China under Grant 82102157 and Grant 61976238,\nin part by the Hunan Provincial Natural Science Foundation for Excellent\nYoung Scholars under Grant 2022JJ20089,in part by the Hunan Provincial\nNatural Science Foundation of China under Grant 2021JJ40895, in part by\nthe Research Project of Postgraduate Education and Teaching Reform of\nCentral South University under Grant 2021JGB147 and Grant 2022JGB117,\nin part by the Clinical Research Center For Medical Imaging in Hunan\nProvince under Grant 2020SK4001, and in part by the Science and Technology\nInnovation Program of Hunan Province under Grant 2021RC4016.(Wei Zhao\nand Weidao Chen are co-ﬁrst authors.) (Corresponding authors: Ming Li;\nJun Liu.)\nThis work involved human subjects in its research. Approval of all eth-\nical and experimental procedures and protocols was granted by the Insti-\ntutional Review Board of The Sec ond Xiangya Hospital under Approval\nNo. 2022k012, and performed in line with the Council for International\nOrganizations of Medical Sciences (CIOMS) guideline.\nWei Zhao is with the Department of Radiology, The Second Xiangya\nHospital, Central South University, Changsha 410011, China, and also with\nthe Clinical Research Center for Medical Imaging in Hunan Province, Central\nSouth University, Changsha 410011, China (e-mail: wei.zhao@csu.edu.cn).\nWeidao Chen, Jiangfen Wu, and Shaokang Wang are with the Interna-\ntional Center, InferVision, Beijing 100020, China (e-mail: chenweidao163@\n163.com; wjfyunzhu@163.com; wshaokang@infervision.com).\nGe Li is with the Department of Radiology, Xiangya Hospital, Central South\nUniversity, Changsha 410008, China (e-mail: ligeanyi@126.com).\nDu Lei is with the Department of Psychiatry and Behavioral Neuroscience,\nCollege of Medicine, University ofCincinnati, Cincinnati, OH 45221 USA\n(e-mail: du.lei@kcl.ac.uk).\nJiancheng Yang and Bingbing Ni are with the Department of Electronic\nEngineering and the SJTU-UCLA Joint Center for Machine Perception and\nInference, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail:\njekyll4168@sjtu.edu.cn; nibingbing@sjtu.edu.cn).\nYanjing Chen and Yingjia Jiang are with the Department of Radiology, The\nSecond Xiangya Hospital, Central South University, Changsha 410011, China\n(e-mail: 962409428@qq.com; 544979495@qq.com).\nYeqi Sun is with the Department of Pathology, The Second Xiangya\nHospital, Central South University, Changsha 410011, China (e-mail:\nzoesyq19831@csu.edu.cn).\nYingli Sun and Ming Li are with the Department of Radiology, Huadong\nHospital Afﬁliated to Fudan University, Shanghai 200040, China (e-mail:\nsunyingli208ok@126.com; minli77@163.com).\nJun Liu is with the Department of Radiology, The Second Xiangya Hospital,\nCentral South University, Changsha 410011, China, also with the Department\nof Radiology Quality Control Center, Central South University, Changsha\n410011, China, and also with the Clini cal Research Center for Medical\nImaging in Hunan Province, Central South University, Changsha 410011,\nChina (e-mail: junliu123@csu.edu.cn).\nThis article has supplementary material provided by the\nauthors and color versions of one or more ﬁgures available at\nhttps://doi.org/10.1109/TNNLS.2022.3190671.\nDigital Object Identiﬁer 10.1109/TNNLS.2022.3190671\nhelp clinicians guide biopsies by visual methods. Inspired by\nthe potential inherent links between EGFR mutation status and\ninvasiveness information, we hypothesized that the predictive\nperformance of a deep learning network can be improved through\nextra utilization of the invasiveness information. Here, we created\na novel explainable transformer network for EGFR classiﬁcation\nnamed gated multiple instance learning transformer (GMILT)\nby integrating multi-instance learning and discriminative weakly\nsupervised feature learning. Pathological invasiveness informa-\ntion was ﬁrst introduced into the multitask model as embeddings.\nGMILT was trained and validated on a total of 512 patients\nwith adenocarcinoma and tested on three datasets (the internal\ntest dataset, the external test dataset, and The Cancer Imaging\nArchive (TCIA) public dataset). The performance (area under\nthe curve (AUC) = 0.772 on the internal test dataset) of GMILT\nexceeded that of previously published methods and radiomics-\nbased methods (i.e., random forest and support vector machine)\nand attained a preferable generalization ability (AUC = 0.856 in\nthe TCIA test dataset and AUC = 0.756 in the external dataset).\nA diameter-based subgroup analysis further veriﬁed the efﬁciency\nof our model (most of the AUCs exceeded 0.772) to noninvasively\npredict EGFR mutation status from computed tomography (CT)\nimages. In addition, because our method also identiﬁed the “core\narea” of the most suspicious area related to the EGFR mutation\nstatus, it has the potential ability to guide biopsies.\nIndex Terms— Epidermal growth factor receptor (EGFR),\ncomputed tomography, multiple instance learning (MIL),\ntransformer.\nI. I NTRODUCTION\nN\nONSMALL cell lung cancer (NSCLC) accounts for more\nthan 80% of lung cancer cases, and lung adenocarcinoma\nis the most common type of NSCLC, with a ﬁve-year relative\nsurvival rate of 5% for patients diagnosed with metastatic\ndisease [1]. The advancement of genomics and precision\nmedicine has facilitated the development of cancer treatment\nparadigms, such as targeted therapy with epidermal growth\nfactor receptor (EGFR)-tyrosine kinase inhibitors (TKIs) [2].\nOne study found that EGFR-mutant patients have an\nobjective response rate (ORR) of approximately 80% when\ntreated with EGFR-TKIs [3]. In contrast, the administration\nof the EGFR-TKI geﬁtinib has no effect or results in even\nworse progression-free survival (PFS) and unnecessary costs\nwhen applied to patients without EGFR mutations [4]. More-\nover, patients with EGFR mutations who therefore lack an\ninﬂammatory microenvironment have an unfavorable ORR to\nimmune checkpoint inhibitor (ICI) treatments [5]. Therefore,\nan accurate pretreatment estimation of EGFR mutation status\ncould signiﬁcantly help clinicians select eligible patients for\nEGFR-TKI treatment, thus supporting individualized decision-\nmaking and improving patient outcomes to the greatest extent\npossible.\nThis work is licensed under a Creative Commons Attribution 4.0 License. Formore information, see https://creativecommons.org/licenses/by/4.0/\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nInformative tissue-based assays, such as next-generation\nsequencing (NGS), remain the standard medical procedure\nto determine EGFR mutation status. However, the inherent\ndisadvantages of these approaches, such as the need for\ninvasive procedures, sampling bias, and high cost, limit their\nclinical application in some s cenarios. Furthermore, EGFR\nmutation status and the immune landscape may change during\ncancer progression and/or therapy [6]. In clinical, different\ntumor sizes face different clinical issues in EGFR mutation\ntesting. For example, tumors with sizes less than 1 cm might\nnot have residual tissues for EGFR testing after histopatho-\nlogic analysis. Another case is that patients with unresectable\ntumors may require repeated sampling to identify the EGFR\nmutation status and guide EGFR-TKI therapy. Therefore,\nhighly efﬁcient, noninvasive, longitudinal, high-throughput\nmethods for predicting EGFR mutation status, preferably\nsize-based subgroup analysis, are urgently needed in the\nclinic.\nRecently, an increasing number of investigators have sug-\ngested that CT images contain rich information that may\nintrinsically reﬂect the inherent characteristics of EGFR muta-\ntion status. Therefore, many investigators have attempted to\nachieve noninvasive EGFR identiﬁcation using imaging data\nmining methods (i.e., radiomics and deep learning) [7]–[9].\nHowever, radiomics involves time-consuming image segmen-\ntation and inevitable feature selection, making it difﬁcult to\napply in the clinical environment. In contrast, deep learning\ncan substantially overcome the abovementioned disadvantages\nand outperform radiomics methods in the same task [8], [10],\n[11]. Nonetheless, mining CT images to efﬁciently predict\nEGFR mutation status by deep learning remains a substantial\nchallenge, notably because previous studies only conducted\na single task and ignored the inherent correlations between\nmutation status and other biological behaviors that may inﬂu-\nence the mutation status of a gene, such as pathological\ncategories and the internal microenvironment.\nRecently, several scientiﬁc papers reported that EGFR muta-\ntions can occur in the early stage of lung adenocarcinoma\nand during tumor initiation from preneoplastic to neoplastic\nlung parenchyma conditions, including atypical adenomatous\nhyperplasia (AAH), adenocarcinomain situ (AIS), minimally\ninvasive adenocarcinoma (MIA), and invasive adenocarcinoma\n(IAC) [6], [12]. In addition, the probability of mutation can\npotentially increase as the invasive extent progresses [13].\nThus, it is reasonable to hypothesize that invasiveness infor-\nmation can have predictive value in evaluating EGFR mutation\nstatus. To the best of our knowledge, no previous studies\nhave investigated the hypotheses to date. Multitask learning\naims to improve such generalization by leveraging domain-\nspeciﬁc information contained in the training signals of related\ntasks and has shown promising results w.r.t. performance,\ncomputations, and/or memory footprint, by jointly tackling\nmultiple tasks through a learned shared representation [14].\nRecently, a new kind of encoder–decoder neural architecture,\ntransformer, is proposed, which can effectively extract and\nutilize the relational features between different input data\nor feature representations [15], [16]. Incorporating multi-\ntask learning and transformer may improve the predictive\nperformance by mining the relational patterns between inva-\nsiveness and EGFR mutation status. Notably, not all lesion\nsections reveal EGFR expression simultaneously due to the\ninherent heterogeneity of the tumor, which results in the lack\nof valuable features extracted from the lesion patches and\ncan easily lead to overﬁtting. Therefore, improving the uti-\nlization of valuable features and increasing the signal-to-noise\nratio (SNR) of feature space should be addressed. Multiple-\ninstance learning methods integrating the attention mechanism\ncan improve the feature expression ability of the model\nwhile reducing the data annota tion requirements [17]–[20].\nIn addition, the active learning method, which seeks to ﬁnd the\nmost informative samples inthe model development process,\ncan improve the model training efﬁciency and the model\ngeneralization ability [21].\nTherefore, to improve the predictive performance of EGFR\nmutation status and investigate whether adding the inva-\nsiveness information of lung adenocarcinoma to the model\ncould obtain better performance, we propose a novel gated\nmultiple instance learning transformer (GMILT) architecture\nby integrating multiple-instance learning and active learning\n(see Fig. 1), which could efﬁciently exploit and utilize rich\ndiscriminative patterns by bridging the representation gap in\nthe spatial and global information domains in the tumor. The\nmain contributions are given as follows. First, we proposed\nan innovative online sample selection method to improve the\ngeneralizability of this model in an active learning setting.\nTo further enhance the feature representation ability of visual\ntransformers, we ﬁrst applied the group ensemble method to\nincorporate cardinality constraints on visual words in each\nminibatch to leverage possible prior knowledge based on\nmultitask learning, i.e., EGFR classiﬁcation and pathological\ninvasiveness classiﬁcation. To the best of our knowledge,\nthis is the ﬁrst study to investigate the interaction effects\nbetween EGFR mutation status and invasiveness information\nwith a deep learning model, and it is also the ﬁrst study to\nintroduce the transformer method to medical tasks to improve\nthe efﬁciency of feature learning. Second, we validated our\nmodel on three different datasets and compared it to previous\nrelated deep learning studies to verify the advantages of the\nproposed framework. Finally, we designed an attention pooling\nthat can be used to visualize model decisions and to provide\nprecision guidance for biopsy procedures.\nWe collected 726 lung adenocarcinoma patients with\nEGFR mutation testing from three datasets. All nodules\nwere manually segmented, and labeled as EGFR mutant\n(EGFR+)/wild-type (EGFR-). An originally proposed GMILT\nmodel, incorporating multiple instance learning (MIL), trans-\nformer, active learning, and multitask learning algorithms, was\nconstructed to efﬁciently utilize invasiveness information as\nembeddings to improve the performance of EGFR mutation\nstatus prediction and potentially guide biopsy.\nII. M\nETHODS\nThis retrospective study was approved by The Second\nXiangya Hospital, Institutional Review Board (IRB), which\nwaived the requirement for informed consent.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 3\nFig. 1. Workﬂow of our study. We collected 726 lung adenocarcinoma patients with EGFR mutation testing from three datasets. All nodules were manually\nsegmented and labeled as EGFR mutant (EGFR+)/wild-type (EGFR-). An originally proposed GMILT model,incorporating MIL, transformer, active learning,\nand multitask learning algorithms, was constructed to efﬁciently utilize invasiveness information as embeddings to improve the performance of EGFR mutation\nstatus prediction and potentially guide biopsy.\nA. Patients and Inclusion Criteria\nIn this study, we collected three datasets for analysis.\nDataset 1, including 640 patients from January 2013 to\nDecember 2018, was collected from Huadong Hospital and\nused for model development and validation. Dataset 2, includ-\ning 50 patients from January 2020 to March 2021, was\ncollected from Second Xiangya Hospital and used for external\ntesting. Dataset 3, including 36 patients, was collected from\nThe Cancer Imaging Archive (TCIA) public database [22]\nand used to validate the stability and generalization of the\nGMILT network. For datasets 1 and 2, the inclusion criteria:\n1) patients who underwent thin-slice chest CT (0.75–1.5 mm)\nscans prior to biopsies or surgical treatment; 2) patients with\ndetailed pathological reports diagnosing lung adenocarcinoma;\nand 3) patients with detailed EGFR mutation testing reports.\nThe inclusion criteria for dataset 3 were given as follows:\n1) CT images with slice thickness ≤1.5 mm (to avoid\ndata inconsistency); 2) patients with EGFR mutations testing\nreports; 3) patients with pathology reports for the diagnosis\nof lung adenocarcinoma; and 4) those lesions that could be\ncertainly identiﬁed as the resected or biopsied lesions. The\nexclusion criteria were: 1) CT images with slice thickness\n>1.5 mm; 2) patients without EGFR mutation testing reports;\nand 3) patients with pathological reports for the diagnosis\nother than lung adenocarcinoma. Only malignant nodules with\nEGFR testing results were included. The CT scanning and data\npreprocessing information were presented in Section 1 in the\nSupplementary Material.\nB. Design of the Experiments and Model Construction\nConsidering the inherent relationship between the EGFR\nmutation status and pathological invasiveness information,\nwe designed ﬁve deep learning models to comprehensively\ninvestigate the inherent interactions in a multitask environ-\nment, especially focusing on the incremental value of inva-\nsiveness information for predicting EGFR mutation status from\ndifferent aspects (see Fig. 2).\nC. Single Task Analysis\nIn this part, we constructed two single tasks to investigate\nthe performance of deep learning in two separate tasks.\nModel 1 : Constructing a 2.5-D network to predict the\ninvasiveness of lung adenocarcinoma without using EGFR\nmutational information.\nModel 2: Proposing a novel MIL network (the baseline\nnetwork, also called the baseline of GMILT) to predict the\nEGFR mutation status of lung adenocarcinoma without using\npathological invasiveness information.\nD. Interaction Analysis (Multitask)\nIn this part, we add the invasiveness information to model 2\nin three different scenarios.\nModel 3: Consider the invasiveness information as aninput\ninto the network to investigate its inﬂuence in predicting the\nEGFR mutation status.\nModel 4: Consider the invasiveness information assuper-\nvised information to the network to simultaneously predict\nthe EGFR mutation status and the invasiveness.\nModel 5 : Considering the invasiveness information as\nsemantical information embedded into the proposed network\nto predict the EGFR mutation status of lung adenocarcinoma.\nIn model 5, we ﬁrst introduce two skills (embedding and active\nlearning) to facilitate embedded intermediate feature learning,\nthus constructing the ﬁnal proposed network—GMILT.\nAs the main purpose of the study was to noninvasively pre-\ndict the EGFR mutation status, the performance of identifying\nthe invasiveness of lung adenocarcinoma was not evaluated in\nour model 5.\nE. Proposed Approach\nIn this section, we formulate the problem of EGFR type\nclassiﬁcation and describe our proposed GMILT approach.\nAs illustrated in Fig. 3, compared to traditional MIL,\nGMILT ﬁrst transforms a raw unseparated bag into multiple\npseudo-3-D instances with aggregated semantic representation\nfusing the transaxial, coronal, and sagittal representations\nbased on the transformer architecture. It then combines the\ndeep pseudo-3-D instances into the bag representation using\nattention-based MIL pooling in the transformer encoder mod-\nule. It ﬁnally transforms the bag representation into the ﬁnal\nprediction by using a neural network to learn the Bernoulli\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nFig. 2. Overall design of the ﬁve models. The MLP head contains two fully connected layers that are applied for feature transformation and nonlinearity.\ndistribution of the bag. Meanwhile, in the transformer encoder\nmodule, we propose the group ensemble module (GEM) and\nthe sample activation module (SAM) in conjunction with\nactive smoothing (GAS) loss to improve the discriminative\nfeature learning and generalization ability of GMILT.\n1) Problem Setting: In this part, we will ﬁrst analyze\nthe main challenges in EGFR type identiﬁcation caused by\ntumor heterogeneity and multitask learning, and then provide\ncorresponding countermeasures.\nFor the annotation of CT images, lesionwise labels directly\ncome from the gene detection of EGFR, which is concretely\nbased on the part of sampling through biopsy or surgery. In this\nsense, the diagnostic results of EGFR type are restricted by\nthe sampled parts, and the results of the residual parts of\nlesions remain unknown (actually without the ground truth)\neven when this image has already been labeled. This poses\na great challenge for the utilization of traditional supervised\nlearning models. To address this challenge, we borrow an idea\nfrom an MIL framework, which is a typical weakly supervised\nlearning paradigm to address patient-level (bag-level) predic-\ntion without knowledge of any region-level annotation.\nIn the MIL setting, one lesionwise CT cube is divided into\nN 3-D subparts of equal size, which are seen as instances.\nWe consider a group ofN instances (called “Bag”) and assume\nthat each group from a positive class sample contains at least\na few 3-D instances with positive class-speciﬁc information,\nwhereas each group from a negative sample does not contain\nany 3-D instances having positive class-speciﬁc information.\nHowever, in conﬂict with MIL constraints, bag-level false-\nnegative results in gene detection still exist. To address this\nchallenge, we introduce the active learning method [22] to\ncontrol whether a sample is used to train the model online,\nwhich improves the training efﬁciency. Nevertheless, it is also\nworth noting that 3-D inputs with a low SNR are unfavorable\nfor feature learning, and a combination of MIL and 3-D\ninputs leads to an increase in computation and memory costs,\nlimiting the usage of minibatch sizes and the convergence\nability of the backpropagation (BP) neural network model.\nTo address this challenge, we implement an MIL-based visual\ntransformer using pseudo-3-D inputs, which can preserve\n3-D semantic representations and reduce the effects of noise\nand memory cost simultaneously. Another challenge is the\ninteractive effects among multiple supervised tasks in the\nmultitask learning setting. We noticed that most area under\nthe curves (AUC) of previous CT image-based EGFR clas-\nsiﬁcation models varies in the range of 65% ∼ 81% [8],\n[10], showing the difﬁculty of feature learning. Thus, it is\nessential to construct a multitask learning approach that makes\nrepresentations of EGFR mutation status more discriminative\nand promotes EGFR classiﬁcation. To address this issue,\nwe adopt the ensemble learning technique during training by\nstructured coding in the process of feature learning with EGFR\nand pathology information. In other words, to address the\naforementioned challenges, our proposed approach has four\nmajor components: 1) baseline GMILT; 2) SAM; 3) GEM; and\n4) GAS loss. In particular, we build a deep MIL-based visual\ntransformer with pseudo-3-D inputs to predict the bag level\n(i.e., EGFR+ or EGFR-). In the training stage, we incorporate\nthe three semantic embedding modules—SAM, GEM, and\nGAS losses—into the baseline of the GMILT model.\n2) Preprocessing: Deep Instance Generation: Considering\na set of patients, {X\ni }, i = 1,..., N, each patient has an\nannotated lesionwise cube based on CT images with its EGFR\nmutational labelGi ∈ {0,1} and pathological invasiveness label\nG′i ∈ {0,1}. We ﬁrst crop the lesionwise cubes from the\noriginal CT scans according to the 3-D annotations (bounding\nboxes). Then, each lesionwise cube is divided intom equal\nparts, and based on their centroid points, we obtain the transax-\nial, coronal, and sagittal patches. Speciﬁcally, the transaxial,\ncoronal, and sagittal patches, which pass through the same\ncentroid point, make up an MIL instance representing the 3-D\ninformation of the responding equal part (called a pseudo-3-D\ninstance). Finally, the m MIL instances constitute a patient-\nlevel MIL bag with a bag-level label (see Fig. 3).\n3) Baseline of Gated MIL Transformer:The graphical rep-\nresentation of the proposed baseline of the GMILT model\nis illustrated in Figs. 4 and 5. GMILT receives as input a\nsequence of pseudo-3-D instances. In GMILT, we view a bag\nof pseudo-3-D instances as a visual sentence that represents the\n3-D lesion, i.e., a visual sentence is composed of a sequence\nof visual words\nX\ni − >\n[\nxi,1, xi,2,..., xi,m ]\n(1)\nwhere xi, j is the jth word visual word of the ith visual\nsentence, j = 1,2,..., m.\nTo handle pseudo-3-D instances, we prepend a learnable\nembedding on the sequence of transaxial, coronal, and sagittal\npatches with a Siamese MobileNet-V2 network outputting the\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 5\nFig. 3. Overview of deep instance generation. Th e origin CT volumes were ﬁrst preprocessed usingcropping to extract the lesionwise cubes. Then, the\ncubes were partitioned equally to construct the MIL instances.\nFig. 4. Framework of the transformer encoder module. In this module, we ﬁrst fused the patch-based features withinan instance using a shared feature\naggregation module, which is based on an attention mechanism, to obtain theinstance-level latent vectors. Then, we used the attention pooling blockto fuse\nthe instance-level latent vectors, which belongs to the same bag, obtainingthe bag-level embedding feature vectors. In addition, with the help of the K-means\nalgorithm, the SAM clusters the instance-level latent vectors and obtains the gating values of the instance level and the bag level by comparing the clustering\nresults with the real labels. Meanwhile, using instance-level latent vectors and instance-level gating values as input, the GEM uses group convolutions to\nperform structural encoding on the instance-level latent vectors.\ncorresponding patch embeddings. In this way, we transform\nthe visual words into a sequence of word embeddings\nY i =\n[\nyi,1, yi,2,..., yi,m ]\n(2)\nwhere yi, j is the jth word embedding, which contains a\ngroup of patch embeddings—(v 0,i, j ,v 1,i, j ,v 2,i, j ),v 0,i, j is\nthe transaxial patch embedding, v 1,i, j is the coronal patch\nembedding, and v2,i, j is the sagittal patch embedding.\nClinical information embeddings (such as pathological inva-\nsiveness information) or position embeddings can be added to\npatch embeddings. The resulting sequence of latent vectors\nserves as input to the transformer encoder. In the baseline of\nGMILT, the transformer encoder consists of alternating layers\nof self-attention and FC layers, as shown in Figs. 4 and 5.\nWe ﬁrst use attention pooling and multilayer perception (MLP)\nto aggregate the patch features for each word, obtaining a\nsingle word representation/Phi1\ni, j in the shared word embedding\naggregation module. Subsequently, we implement a second\nattention pooling layer that performs attention-based permuta-\ntion invariant pooling to obtain a single sentence representation\nz\ni . Most notably, the two attention-based operations allow\nGMILT to learn both local information and 3-D global infor-\nmation in a visual word and across the visual sentence. Next,\nz\ni is passed to the MLP head module to obtain predictions for\nthe entire bag.\nAttention Pooling: The self-attention operator is an inter-\npretable symmetric function [23]. Formally, we denoteH =\n{h1,..., hN } as the inputs with N embeddings. Then, the\noperator is deﬁned as\nz =\nN∑\nk=1\nαk hk (3)\nαk = exp\n{\nwT tanh\n(\nVh T\nk\n)}\n∑ N\nj=1 exp\n{\nwT tanh\n(\nVh T\nj\n)} (4)\nwhere w ∈ RN×1 and V ∈ RN×D are trainable parameters.\nIn addition, αk is considered the attention score per input\nembedding, indicating its contribution to the drawn conclu-\nsion, which is helpful for interpreting the trained model,\ni.e., interpretable analysis for identifying the potential core\narea. Importantly, the processing ﬂow of the transformer\nencoder implements attention in attention architecture on patch\nembeddings and word embeddings using the basic operator\ndifferentiating to the existing attention mechanism.\n4) SAM: In the training phase, given the bags input by\neach minibatch, our goal is to assess the beneﬁcial effect\nof word embeddings (instance-level latent vectors) for the\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nFig. 5. Framework of the GMILT. First, based on the MIL setting, we used the generated “bag” of pseudo-3-D instances as input into the transformer-\nbased model. Then, we used the Siamese MobileNet-V2 network to encode the multisectional patches from the three different 2-D view directions (i.e.,\ntransaxial, coronal, and sagittal axes) for each pseudo-3-D instance, obtaining the three corresponding latent vectors. Second, these latent vectors were fed to\nthe transformer encoder module to obtain the bag-level embedding features. Third, the MLP head further integrated the bag-level feature vectors to output\nthe predictive values of EGFR+/-. During training, GAS loss performed discriminative feature learning for EGFR classiﬁcation using bag-level features and\ngating values.\ndiscriminative learning of EGFR mutation status and deﬁne\na sample weight to coordinate model training online with an\nadaptive gating technique. To achieve this, we infer that word\nembeddings with the same EGFR label should be matched to\na single centroid that represents the EGFR type. Speciﬁcally,\nwe set each batch containing the same number of positive\nand negative bags and perform K-means clustering on all\ninstances in the batch to obtain two clusters using visual\nword embeddings x. Then, each instance’s EGFR label is\nassigned by its bag’s EGFR label. Based on the majority\nvoting method, we mark the two clusters as the “EGFR−”a n d\n“EGFR+” clusters. Generally, in the MIL setting, the instances\nin a negative bag are largely negative. Therefore, the cluster,\nwhich contains most of negative instances, is ﬁrstly seen as\n“EGFR−” cluster; then the other cluster is seen as “EGFR+”\nnaturally. On this basis, for each instance, if its cluster tag\nis the same as its EGFR label, its gate value is recorded as\n1; otherwise, it is recorded as 0. Furthermore, for each bag,\nif the cluster labels of all instances in the bag are consistent\nwith the EGFR label of the bag, its gate value is set to 1;\notherwise, it is set to 0. Signiﬁcantly, for positive (EGFR+)\nbags, its bag-level gate values are set to 1 because there are\nlittle fake positive bags based on the clinical scenario. Taking\nthe gate values as the sample weights of the weighted loss\nfunction, we control whether the model performs supervised\nlearning on the input samples online, as shown in Fig. 4. The\nformula is given as follows:\nδ(x) =\n{\n1, label == label\nKmeans\n0, otherelse (5)\nwhere label denotes the input sample’s real label based on\nannotation and labelKmeans denotes the input sample’s cluster-\ning label by k-means.\n5) GEM: As mentioned before, existing popular approaches\nof multitask learning with neural networks learn multiple\nrelated tasks simultaneously based on the underlying shared\nrepresentation to improve their generalization ability. How-\never, this approach neglects the impact of the complex rela-\ntion between tasks in modeling, which may make multitask\nlearning less efﬁcient. GENet [24] is an efﬁcient way to\nimprove model capacity across a wide range of learning tasks.\nHere, we propose an instance-level semantic structured coding\napproach (i.e., GEM) to provide an extra regularization for\nthe shared representation, as shown in Fig. 4. First, we apply\ngroup convolutions to each word embedding and divide the\noutput vector into several groups. Then, we use a multihead\nstructure to implement multitask learning by calculating the\nresponding loss functions of the tasks simultaneously, i.e.,\nEGFR classiﬁcation and pathology classiﬁcation. In particular,\neach group is one constituent member of our ensemble and has\nits own independent head classiﬁer, which introduces diversity\namong the learning tasks. We conduct two tasks (EGFR and\npathology) to train the model end to end simultaneously with\nits own objective function\nLoss\nGEM =\nn∑\nm=1\nLosse,m + Lossp,m (6)\nwhere LossGEM is the total objective function in this module,m\nis the group index, andn is the number of groups, i.e., Lossm=1\nand Loss m=2 denote the instance-level loss functions of\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 7\nEGFR classiﬁcation and pathologyclassiﬁcation, respectively.\nIn particular, the loss function Lossm is implemented with\nGAS loss, which is deﬁned in the following section.\n6) GAS Loss: To reduce the effect of noise labels while\nenhancing the discrimination of features, we design a loss\nfunction called GAS loss. The loss function draws on the\nidea of active learning and uses the gating values given by\nthe SAM during forwarding propagation for online sample\nweighting to actively learn valuable samples. Meanwhile, the\ncenter constraint is applied to the feature space, which pushes\nthe encoded features to the center of the cluster and reduces\nthe differences within the class [25]. Finally, the weighted\ncross-entropy loss function and label smoothing algorithm are\nincorporated to further alleviate the impact of noise labels and\nbalance the positive and negative samples. The loss function\nis given as follows:\nL\nGAS = 1\nN\n∑\ni\n−[δ · wpos ˆyi log(pi ) + δ · wneg (1 − ˆyi )\n× log(1 − pi )\n]\n+ 1\n2\nz′\ni − cyi\n2\n2\nˆyi = ε(1 − yi ) + (1 − ε)yi (7)\nwhere N represents the number of training samples;yi rep-\nresents the label of sample i, the positive class is set to 1,\nand the negative class is set to 0; pi is the probability that\nsample i is predicted to be a positive class; wpos and wneg\nare the weights of positive and negative samples, respectively,\nthe value range is [0,1], and the values in this part are set to\n0.75 and 0.25, respectively;ε is a small modulation parameter,\nand the value is set to 0.1 in this part; andci denotes the yith\nclass center of latent feature manifoldz\n′i and is updated based\non the following feature distances [25].\nFinally, with the previously deﬁned loss functions, the\noverall objective to optimize our model (GMILT) can be\nformulated as\nLtotal = LGEM + LEGFR (8)\nwhere LEGFR denotes the bag-level loss functions with the ﬁnal\nprediction of GMILT based onLGAS, as shown in Fig. 4.\n7) Implementation Details: A total of 640 patients in\ndataset 1 were randomly divided into a development set\n(n = 513) and a test set ( n = 129) at a 4:1 ratio. For\ntraining the proposed deep learning model, we use the\nRanger [26] optimization with a batch size of 32 and a learning\nrate of 0.001. Ranger is a synergistic optimizer combining\nRAdam [27], LookAhead [28], and gradient centralization [29]\nin one optimizer, which accelerates convergence and reduces\ntraining difﬁculty. In addition, the Siamese MobileNet-V2\nnetwork is initialized using the pretrained parameter of Ima-\ngeNet [30], and the dropout technique [31] is applied during\ntraining. To test the deep learning model, we select middle\npatches in the transaxial, coronal, and sagittal axes for each\npseudo-3-D instance in the MIL setting to construct the input\nsample.\n8) Visualization and Interpretation Analysis: For a given\n3-D lesion, the two attention pooling layers assign impor-\ntance values to each visual word, and the patches highlight\nthe important spine regions. A new gradient-weighted class\nactivation mapping (Grad-CAM) network was used to produce\na coarse localization map highlighting the important regions\nin the image for predicting the concept [32].\n9) Ablation Study: To assess the effect of bag size and\ngroup size on the GEM model during training on the proposed\nmethod, we performed two ablation studies to select the best\nhyperparameters k (k = 5) in MIL and μ (μ = 0.125) in\nGEM (see Sections 2 and 3 in the Supplementary Material).\nOur deep learning model is implemented using the popular\nopen-source framework PyTorch (1.6.0) and runs on an Nvidia\nGTX 1080Ti GPU.\nF . V alidation Analysis and Comparison Analysis With\nRelated Publishers\nWe tested our proposed model on three datasets, including\none internal dataset one external dataset, and a public dataset.\nTo verify the efﬁciency of our newly proposed network,\nwe also trained and tested three publishers’ models [7], [8],\n[10] on our Dataset 1, and evaluated and compared the\nperformance outcomes.\nG. Validation Analysis, Comparison Analysis With Related\nPublishers, and Subgroup Analysis\nIn clinical scenarios, small lesions may have inadequate\nsamples for gene analysis, and larger lesions without the\nindication for surgery may require repeated biopsies during\nsystemic therapy. Given the different clinical demands and\nT-stage classiﬁcation criterion, we further investigated the\nperformance of our proposed model in four different subgroups\ncategorized by the maximum diameter (MD) of the nodule\n(0 < MD < 1c m ;1≤ MD < 2c m ;2≤ MD < 3c m ;a n d\n3c m ≤ MD). Several subgroups had zero samples (i.e., the\n0 < MD < 1 cm subgroup in dataset 3 and the 1≤ MD <\n2 cm subgroup in dataset 2), so we merged them with the\nadjacent subgroup and calculated the AUC.\nH. Data Availability\nThe model code is available at https://github.com/\nTXVision/GMILT or on request to the corresponding author.\nIII. R\nESULTS\nA. Patients and Datasets\nIn Dataset 1, 640 patients were ﬁnally included and ran-\ndomly divided into the model training dataset (383 patients),\nthe validation dataset (129 patients), and the internal testing\ndataset (129 patients). Fifty patients from Dataset 2 were\nﬁnally included and used for model external testing. Dataset 3\nfrom the TCIA, ﬁnally including 36 patients, was used to\nvalidate the stability and generalization of the GMILT network\nas a public testing dataset. The distributions of patients were\npresented in Table I.\nB. Model Construction\nWe constructed ﬁve models to comprehensively investigate\nthe inherent interactions in a multitask environment, especially\nfocusing on the incremental value of invasiveness information\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nTABLE I\nDISTRIBUTIONS OF PATI ENTS I NTHREE DATASETS\nfor predicting EGFR mutation status from different aspects\n(see methods for detail). The ﬁnally designed original deep\nlearning scheme, named GMILT, incorporated MIL, trans-\nformer, active learning, and multitask learning algorithms.\nMIL, which reduced the annotation requirements using coarse-\ngrained input information, was conducive to solving the\nproblem of uncertain positive areas in the lesion. In this\narticle, attention-based MIL was used to purify and merge the\neffective feature information of each instance and then improve\nthe model characterization ability. Moreover, the concept of\npseudo-3-D was adopted in MIL. Drawing on the idea of active\nlearning, we innovatively proposed the combined use of SAM\nand GAS loss by selecting valuable samples online for feature\nlearning to improve the convergence speed and generalization\nability of the deep learning model.\nConsidering that EGFR classiﬁcation is related to the\ninvasiveness characteristics of the lesion itself, we use the\nensemble learning method to carry out multitask learning on\nvisual words implemented by GEM to realize the structured\ncoding of instance-level representation and further improve the\nrepresentation ability.\nC. GMILT Can Improve the Performance in Predicting\nEGFR Mutation Status\nTo mine highly discriminative features and improve the per-\nformance of EGFR prediction using CT images, we designed a\nmultiple-instance learning transformer network (the baseline of\nGMILT). The baseline GMILT (model 2) achieved an AUC of\n0.759 by using CT images only for predicting EGFR mutation\nstatus. To explore the interaction between the invasiveness\ninformation and EGFR mutation status, we designed three\nadditional (models 3–5) by considering the invasiveness infor-\nmation as different supplemental information to the EGFR\npredictive network. Model 3, in which lung nodule mask\nand invasiveness information were both fed as the inputs,\ndemonstrated a similar predictive efﬁciency for EGFR muta-\ntion status (model 3: AUC= 0.721 versus model 2: AUC=\n0.759), indicating that the given information of invasiveness\nmay be a confounding factor rather than a contributing factor.\nIn model 4, inspired by the inherent link between invasiveness\nand EGFR mutation status, we considered both the invasive-\nness and EGFR mutation information as supervised factors\nand constructed a multitask model to simultaneously predict\nboth. However, model 4 showed no improvement over model 2\nin performance for predicting EGFR mutation status (AUC=\n0.700 versus model 2: 0.759). However, its performance for\npredicting invasiveness substantially improved relative to that\nof model 1 (AUC = 0.926 versus model 1: 0.879). Finally,\nmodel 5 (GMILT) considered the invasiveness information\nas supplemental information embedded into the intermediate\nfeatures, and the performancefor predicting EGFR mutation\nstatus was improved, with the highest AUC of 0.772 in all\nmodels [see Table II and Fig. 6(a)]. Furthermore, the AUC\nof model 5 was signiﬁcantly higher than that of model 4\n(P = 0.042). It indicated that the invasiveness information\nmay be related to the EGFR mutation status and can be\nappropriately used to improve the performance in predicting\nEGFR mutation status.\nD. GMILT Obtained a Robust Performance on External and\nPublic Testing Datasets and Excelled Over Other Methods\nOne of the limitations of data-dependent deep learning is\nthe relatively weakened performance on external datasets due\nto the inconsistent data distribution. To validate the stability\nand generalizability of GMILT, we further tested the model\non an independent external dataset and a public dataset. Our\nmodel obtained similar performance outcomes on the external\ndataset (AUC= 0.756) and even better performance outcomes\non the TCIA dataset (AUC= 0.856) (see Table III) than on the\ninternal testing dataset. Moreover, to investigate the efﬁciency\nof our proposed model, we selected three representative pub-\nlished papers and repeated their deep learning methods on our\nDataset 1, constructed as model 2.5-D [8], model 3-D [10], and\nmodel SE-CNN [7]. Our proposed GMILT network excelled\nover the previous three methods (AUC: 0.772 versus 0.720,\n0.741, and 0.649) [see Table III and Fig. 6(b)]. Together, these\nﬁndings support the advantage of multi-instance learning and\nactive learning in predicting EGFR mutation status. The pre-\ncision curves and confusion matrix information are described\nin Sections 4–7 in the Supplementary Material.\nE. GMILT Achieved Better Performance in Different\nDiameter-Based Subgroups\nInspired by the different tumor sizes observed in clinical\nscenarios, we further investigated the performance ofGMILT\non four size-based subgroups. As described in Table IV , most\nof the results were superior to those of model 5 (AUC= 0.772)\non the testing dataset of Dataset 1, obtaining state-of-the-art\nperformance. GMILT obtained good performance in each sub-\ngroup, indicating that the constructed model had discriminative\nfeatures for each sample data point (see Table IV).\nF . Ablation Study of GMILT\nThe ablation study was typically used when testing a net-\nwork by removing individual parts. To verify the effectiveness\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 9\nTABLE II\nPERFORMANCE OUR DESIGNED MODELS IN PREDICTING EGFR MUTATION STATUS\nFig. 6. Performance of 5 proposed models in (a) our study and (b) comparative results with 2.5-D, 3-D, and SE-CNN models (trained and validated on our\ndataset).\nof our newly proposed parts in GMILT, we performed the\nablation study on our two key modules (GEM and SAM+\nGAS). We investigated the effect of separately adding these\ntwo modules to GMILT on performance outcomes. Both mod-\nules outperformed the baseline of GMILT (AUC: 0.769 and\n0.763 versus 0.759) (see Table V and Fig. 7), indicating the\nefﬁciency of the two modules in representation learning.\nG. Visualization and Interpretation Analysis\nConsidering the heterogeneous nature of tumors, we tried to\nidentify the core area from which the pathological character-\nistics were most likely to originate. The two attention pooling\nlayers in our GMILT block, inner attention pooling and outer\nattention pooling, model the relationships among transaxial,\ncoronal, and sagittal patches and visual words, respectively.\nWe constructed attention maps of different queries in the\ntransformer encoder to reveal the potential areas that con-\ntributed most to the predicted results (see Fig. 8). After 3-D\ntransformation, the potential “core area” (red dot in Fig. 9)\ncould be visualized and used to guide biopsy and minimize\nfalse negatives.\nH. t-SNE Visualization of the Features Learned by GMILT\nTo intuitively explore the manifold structure of the fea-\ntures, we visualized the features by testing Dataset 1 using\nt-distributed stochastic neighbor embedding (t-SNE) [33],\nwhich is particularly suitable for the visualization of high-\ndimensional data. As illustrated in Fig. 10, the EGFR+ [yellow\ndots in Fig. 10(a)] and EGFR- [purple dots in Fig. 10(a)] sam-\nples formed two distinct sample clusters. Samples predicted to\nbe EGFR+ were more likely to be located closer to the upper\nleft corner, and those predicted as EGFR- were more likely to\nbe located closer to the lower right corner [see Fig. 10(a)].\nThe prediction scores are consistent with the ground truth\n[see Fig. 10(b)]. Furthermore, our model was veriﬁed to be\neffective in representation learning and could construct highly\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nTABLE III\nPERFORMANCE ON TESTING DATASETS AND COMPARISON RESULTS\nTABLE IV\nDISTRIBUTIONS AND PERFORMANCE OF SUBGROUP ANALYSIS\ndiscriminative features, allowing for improved performance in\npredicting EGFR expression status.\nIV . DISCUSSION\nNoninvasively predicting the EGFR mutation status is a\npersistent challenge but represents an urgent need in the\nclinic. While deep learning has its own advantages in this\narea, its performance is limite d by the need to learn efﬁ-\ncient and discriminative features related to EGFR mutation\nstatus. Inspired by the potential inherent links between EGFR\nmutation status and invasiveness information, we hypothesized\nthat the predictive performance of a deep learning network\ncan be improved through extra utilization of the invasiveness\ninformation. Thus, in this study, we proposed a new deep\nlearning model, GMILT, to predict EGFR mutation status.\nTo the best of our knowledge, this is the ﬁrst study to\ninvestigate the interaction effects between EGFR mutation\nstatus and invasiveness information, and it is also the ﬁrst\nstudy to introduce the transformer method to medical tasks.\nOur study found that utilizing i nvasiveness information as\nembedding features in the network can substantially improve\nits performance. Our proposed model achieved an AUC of\n0.772, with favorable generalizability to a public dataset\nand external validation dataset (AUC = 0.856 and 0.756,\nrespectively). In addition, the proposed model performed better\nfor size-speciﬁc subgroups with state-of-the-art (65%∼ 81%\nin previous studies) classiﬁcation AUCs and, thus, can be\napplied in different clinical scenarios. Finally, our model can\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 11\nTABLE V\nPERFORMANCE OF THE ABLATION STUDY\nFig. 7. ROC curves of three ablation studies.\nvisualize the potential “core area” that most correlates to\nEGFR expression to guide biopsy or pathological evaluation\nprocedures.\nIn comparisons of data mining approaches in medical sce-\nnarios, deep learning seems to be superior to other methods\n(e.g., radiomics). This superiority has also been seen in the\ntask of predicting EGFR mutation status [8], [10]. In this\nstudy, our ﬁndings demonstrated the same tendency (see\nSection 8 in the Supplementary Material). However, the cur-\nrent deep learning models in predicting EGFR mutation status\ncould be further improved by efﬁciently and deeper mining\nthe higher dimensional features or relationships Invasiveness\ninformation has been proven to have potential inherent links\nwith EGFR mutation status [12], [13]. However, no previous\nstudy had investigated the incremental value of this link to\ndeep learning models for predicting EGFR mutation status.\nIn this context, we proposed a new network, GMILT, which\ncan efﬁciently exploit and utilize all discriminative patterns\n(i.e., the relationship between EGFR mutation status and\ninvasiveness) bridging the representation gap in the spatial and\nglobal information of the tumor to predict the EGFR mutation\nstatus.\nIdeally, adding the label as an input into the network may\nimprove the performance. However, the opposite result was\nobtained (AUC: 0.721 < 0.759). This ﬁnding suggests that\ndirectly using a pathological label as an input cannot make the\ndeep learning model efﬁciently mine its intrinsic relationship\nwith EGFR typing. Inspired by the successful experience in\nour previous multitask study (i.e., applying the segmentation\ntask to improve the classiﬁcation task), we also investigated\nthe potential mutual promotion of these factors in a mul-\ntitask environment, considering the invasiveness information\nas supervised learning information instead of an input. The\nperformance of predicting EGFR mutation status also failed\nto be improved, indicating that the features or links related\nto EGFR mutation status are more complicated and high\nlevel (AUC: 0.700 < 0.759). In contrast, the performance\nof predicting invasives was improved. This may indicate\nthat features related to EGFR mutation status might provide\nvaluable supplemental information in predicting the invasive-\nness of lung adenocarcinoma. Meanwhile, features related\nto the invasiveness status were relatively more obvious and\nrelevant, and could easily be learned by the model. Since\npathological invasiveness information and the EGFR category\nbelong to two completely different dimensions of information,\nthe inherent correlation is unclear, which makes it difﬁcult to\neffectively drive the model to use pathological information to\ndirectly promote EGFR classiﬁcation. In this context, we used\nensemble learning to structure and quantitatively construct the\nrepresentation space of supervised learning in the study so that\nthe network can effectively improve the efﬁciency of EGFR\nclassiﬁcation by facilitating auxiliary tasks in control. This\nstrategy improved the performance of the model in predicting\nEGFR mutation status (AUC: 0.772> 0.759).\nAlthough the improvement was slight, it is difﬁcult to\nmake a breakthrough in predicting EGFR mutational status\nusing CT images. This may be partly attributed to the reason\nthat the features or correlates regarding gene status are more\ncomprehensive and difﬁcult to learn than those correlated with\nother tasks, such as the prediction of benign or malignant\ntumors [34], the prediction of multiple pathological types [35],\nand the risk stratiﬁcation of lung adenocarcinoma [36]. Several\nclinical factors, such as smoking and sex, are well-known\nfactors related to EGFR mutations [37]. Adding these clinical\nfactors can improve predictive performance. Note that our\nproposed model presented a more effective result than clinical\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\nFig. 8. Suspicious areas (heat map) of EGFR mutant generated by Grad-CAM network within 15 patches from transaxial, coronal, and sagittal levels.\nFig. 9. Visualization of the attention mechanism in our method. The attention weights of visual words and transaxial, coronal, and sagittal patches for a\nlesion are presented above as orange and green boxes. The orange box shows the highest weight value of 0.75, which indicates the most probable EGFR\nmutant area in the lesion. 0.2, 0.6, and 0.2 represent the probability of EGFRmutant in the transaxial, sagittal, and coronal patches, respectively.\nFig. 10. Visualization of features in our internal test dataset usingthe t-SNE technique in a 2-D space. (a) Yellow and purple represent EGFR+ and EGFR-\nsamples, respectively. (b) t-SNE visualization scatter plot colored by the EGFR probability score predicted byGMILT. The prediction scores are consistent\nwith the ground truth.\nmodels [38] or radiomics combined with clinical factor models\n[39], [40], only using CT images. Moreover, our proposed\nmodel also outperformed traditional machine learning methods\n(i.e., radiomics) and previous deep learning methods in com-\nparative analysis (AUC: 0.772 versus 0.720, 0.741, and 0.649).\nThis indicates that our model may facilitate high-level feature\nlearning by embedding invasive information into the network\nto fully utilize the potential correlations or patterns hidden\nbehind the tumor. Images from other modalities, e.g., PET-\nCT, have also recently been used to predict EGFR mutation\nstatus with deep learning [41]. The authors found that models\nfed fusion images (i.e., PET/CT images) outperformed the\nCT image-based model. However, the data sample size was\nrelatively small, and the performance of the CT image-based\nmodel (AUC= 0.72) was inferior to ours. Moreover, their net-\nwork SE-ResNets neglects the interactive information between\nthe two layers, resulting in an inefﬁcient and unnecessary way\nto channel attention learning [42].\nThe performance of our model was substantially improved\nin the subgroup analysis (most of the AUCs were over 0.772).\nThe current results also illustrated the effectiveness of our\nmodel in the process of feature learning and the improvement\nof the feature discrimination ability. Generally, the larger the\nlesion is, the more heterogeneous it will be, leading to a more\ncomplex feature distribution. In this case, using our model\nto construct the discriminant characterization of the sample\nand then subgroup prediction could achieve higher predictive\nefﬁcacy.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 13\nIn the clinic, false negatives may lead to an inappropriate\ntreatment regimen, thus compromising the prognosis. False\nnegatives are possibly attributed to intratumor heterogeneity\n[43], [44]. In this context, predicting the core area most\nlikely to be related to the true EGFR mutation status in\nadvance can substantially minimize false negatives. Motivated\nby this clinical need, we used multiple attention layers in our\nmodel to visualize the importance of lesion areas affecting\nEGFR typing and improved the interpretability of the model.\nIt is worth noting that our proposed model uses a nested\nhierarchical structure to realize the uncoupling of feature\nlearning and abstraction processes, which can not only obtain\nthe probability of lesionwise cubes but also further obtain the\nprobability of the three axial planes corresponding to each\ncube. This strategy is similar to the decision tree. Therefore,\nan alternative way of interpretability is innovatively proposed\nin this study, which can locate the signiﬁcant area of the lesion\nvolume. With this guidance provided by the model, clinicians\ncan better ﬁnd the area expressing EGFR and excise tissue\ncores.\nIn general, images contain a lot of background information\n(i.e., noise) and limited effective or valuable information.\nIn another word, images in the real world are generally low-\nquality data or low-SNR data. CNNs are generally prone to\nnoise interruptions, i.e., small image noise can cause drastic\nchanges in the output and lead to overﬁtting [45]. In this\nstudy, we adopted MIL and transformer to reduce the noise\neffect and improve the SNR of the input information. This\nstrategy has been proven to resolve low-quality data [46].\nBased on previous MIL, we innovatively built pseudo-3-D\ninstances to ensure that each instance had enough spatial and\nsemantic information. The transformer structure is designed\nto improve the discrimination of characterization information\nand efﬁciently use spatial and global information to improve\nthe SNR in the representation space. Considering that EGFR\nclassiﬁcation is related to the invasiveness characteristics of\nthe lesion itself, we used the ensemble learning method to\ncarry out multitask learning on visual words implemented\nby GEM to realize the structured coding of instance-level\nrepresentation. At the same time, the organic combination of\nSAM and GEM promotes the effectiveness of online sample\nselection, thus improving the performance (AUC: model 5>\nmodel 2). Our proposed method provides a new approach\nfor analyzing medical data that could be considered either a\nreference or method for investigators performing other tasks\n(i.e., predicting prognosis or treatment of cancers).\nThere were still several conceived limitations in our study.\nFirst, this was a retrospective study and only veriﬁed it in\nthe current task. A further prospectively designed study and\napplication in other tasks are warranted to verify the efﬁciency\nof the model. Second, the sample size was relatively small,\nand the data distribution may be unbalanced and biased due\nto the limited number of centers that participated in our study.\nLarger sample size and more participating centers can lead\nto better performance. However, despite such a small dataset,\nwe yielded comparable and promising results compared with\nother studies, especially in subgroup analysis. The newly\nproposed technique for analyzing medical data introduced\nin our study can provide a novel methodology for other\nmedical tasks. Finally, although attention pooling can present\nthe potential core area to help clinicians in the biopsy, it is\nat the “proof-of-concept” stage. Slice-level comparisons with\ngross tissues (for pathological analysis) are needed to conﬁrm\nthe efﬁciency of this technique. However, our model uses an\nattention mechanism to deﬁne the weight of the “core area,”\nwhich can substantially improve the accuracy of key area\nidentiﬁcation [17], [47].\nV. C\nONCLUSION\nIn this article, we proposed a novel network-GMILT to\nnoninvasively predict EGFR mutation status, which can be\napplied in different clinical scenarios regarding patients with\nlung adenocarcinoma. Moreover, the visualization analysis\nshows the ability of our model to reveal the potential “core\narea” that most correlates to EGFR expression and, thus,\nfacilitate the application of precision medicine.\nR\nEFERENCES\n[1] R. L. Siegelet al., “Cancer statistics, 2020,”CA Cancer J. Clin., vol. 70,\nno. 1, pp. 7–30, 2020.\n[2] D. S. Ettinger et al., “NCCN guidelines insights: Non–small cell lung\ncancer, version 1.2020,”J. Nat. Comprehensive Cancer Netw., vol. 17,\nno. 12, pp. 1464–1472, 2019.\n[3] V . A. Miller et al. , “Molecular characteristics of bronchioloalveo-\nlar carcinoma and adenocarcinoma,bronchioloalveolar carcinoma sub-\ntype, predict response to erlotinib,” J. Clin. Oncol. , vol. 26, no. 9,\npp. 1472–1478, Mar. 2008.\n[4] T. S. Mok et al., “Geﬁtinib or carboplatin–paclitaxel in pulmonary\nadenocarcinoma,” New England J. Med., vol. 361, no. 10, pp. 947–957,\n2009.\n[5] K. Hastings et al.,“ EGFR mutation subtypes and response to immune\ncheckpoint blockade treatment in non-small-cell lung cancer,” Ann.\nOncol., vol. 30, no. 8, pp. 1311–1320, 2019.\n[6] H. Bai et al., “Inﬂuence of chemotherapy on EGFR mutation status\namong patients with non–small-cell lung cancer,”J. Clin. Oncol., vol. 30,\nno. 25, pp. 3077–3083, 2012.\n[7] B. Zhang et al., “Deep CNN model using CT radiomics feature map-\nping recognizes EGFR gene mutation status of lung adenocarcinoma,”\nFrontiers Oncol., vol. 10, Feb. 2021, Art. no. 598721.\n[8] S. Wang et al., “Predicting EGFR mutation status in lung adenocar-\ncinoma on computed tomography image using deep learning,” Eur .\nRespiratory J., vol. 53, no. 3, Mar. 2019, Art. no. 1800986.\n[9] Y . Donget al., “Multi-channel multi-task deep learning for predicting\nEGFR and Kras mutations of non-small cell lung cancer on CT images,”\nQuant. Imag. Med. Surg., vol. 11, no. 6, pp. 2354–2375, Jun. 2021.\n[10] W. Zhao et al., “Toward automatic prediction of EGFR mutation status\nin pulmonary adenocarcinoma with 3D deep learning,”Cancer Med.,\nvol. 8, no. 7, pp. 3532–3543, Jul. 2019.\n[11] S. Moreno et al., “A radiogenomics ensemble to predict EGFR and\nKras mutations in NSCLC,” Tomography, vol. 7, no. 2, pp. 154–168,\nApr. 2021.\n[12] S. B. Yoo, J.-H. Chung, H. J. Lee, C.-T. Lee, S. Jheon, and S. W. Sung,\n“Epidermal growth factor receptor mutation and p53 overexpression\nduring the multistage progression of small adenocarcinoma of the lung,”\nJ. Thoracic Oncol., vol. 5, no. 7, pp. 964–969, Jul. 2010.\n[13] H. Chen et al., “Genomic and immune proﬁling of pre-invasive lung\nadenocarcinoma,” Nature Commun., vol. 10, no. 1, p. 5472, 2019.\n[14] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans,\nD. Dai, and L. Van Gool, “Multi-task learning for dense prediction\ntasks: A survey,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 7,\npp. 3614–3633, Jul. 2021.\n[15] K. Han et al., “Transformer in transformer,” 2021,arXiv:2103.00112.\n[16] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers\nfor image recognition at scale,” 2020,arXiv:2010.11929.\n[17] Z. Li et al. , “A novel multiple instance learning framework for\nCOVID-19 severity assessment via data augmentation and self-\nsupervised learning,” Med. Image Anal. , vol. 69, Apr. 2021,\nArt. no. 101978.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \n14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\n[18] P. Chikontwe, M. Kim, S. J. Nam, H. Go, and S. H. Park, “Multiple\ninstance learning with center embeddings for histopathology classiﬁca-\ntion,” inProc. MICCA, 2020.\n[19] J. Yao, X. Zhu, J. Jonnagaddala, N. Hawkins, and J. Huang, “Whole\nslide images based cancer survival prediction using attention guided\ndeep multiple instance learning networks,”Med. Image Anal., vol. 65,\nOct. 2020, Art. no. 101789.\n[20] T. Vu, P. Lai, R. Raich, A. Pham, X. Z. Fern, and U. A. Rao, “A novel\nattribute-based symmetric multiple instance learning for histopatho-\nlogical image analysis,” I E E ET r a n s .M e d .I m a g ., vol. 39, no. 10,\npp. 3125–3136, Oct. 2020.\n[21] S. Budd, E. C. Robinson, and B. Kainz, “A survey on active learning\nand human-in-the-loop deep learning for medical image analysis,”Med.\nImage Anal., vol. 71, Jul. 2021, Art. no. 102062.\n[22] K. Clark et al., “The cancer imaging archive (TCIA): Maintaining and\noperating a public information repository,”J. Digit. Imag., vol. 26, no. 6,\npp. 1045–1057, Dec. 2013.\n[23] M. Ilse, J. M. Tomczak, and M. Welling, “Attention-based deep\nmultiple instance learning,” in Proc. Int. Conf. Mach. Learn. , 2018,\npp. 2127–2136.\n[24] H. Chen and A. Shrivastava, “Group ensemble: Learning an ensemble\nof ConvNets in a single ConvNet,” 2020,arXiv:2007.00649.\n[25] Y . Wenet al., “A discriminative feature learning approach for deep face\nrecognition,” inProc. Eur. Conf. Comput. Vis., 2016, pp. 499–515.\n[26] W. L. Ranger. A synergistic optimizer. GitHub. [Online]. Available:\nhttps://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n[27] L. Liu et al., “On the variance of the adaptive learning rate and beyond,”\n2019, arXiv:1908.03265.\n[28] M. R. Zhang, J. Lucas, J. Ba, and G. E. Hinton, “Lookahead optimizer:\nk steps forward, 1 step back,” inProc. Adv. Neural Inf. Process. Syst.,\n2019, pp. 1–12.\n[29] H. Yong, J. Huang, X. Hua, and L. Zhang, “Gradient centraliza-\ntion: A new optimization technique for deep neural networks,” 2020,\narXiv:2004.01461.\n[30] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248–255.\n[31] G. E. Hinton et al., “Improving neural networks by preventing co-\nadaptation of feature detectors,”Comput. Sci., vol. 3, no. 4, pp. 212–223,\n2012.\n[32] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-CAM: Visual explanations from deep networks via\ngradient-based localization,” Int. J. Comput. Vis. , vol. 128, no. 2,\npp. 336–359, Feb. 2020.\n[33] L. Van der Maaten and G. Hinton, “Visualizing data using t-SNE,”\nJ. Mach. Learn. Res., vol. 9, no. 11, pp. 2579–2605, 2015.\n[34] D. Ardila et al. , “End-to-end lung cancer screening with three-\ndimensional deep learning on low-dose chest computed tomography,”\nNature Med., vol. 25, no. 6, pp. 954–961, Jun. 2019.\n[35] Y . Fuet al., “Fusion of 3D lung CT and serum biomarkers for diagnosis\nof multiple pathological types on pulmonary nodules,”Comput. Methods\nPrograms Biomed., vol. 210, Oct. 2021, Art. no. 106381.\n[36] J. Gong et al., “Deep learning-based stage-wise risk stratiﬁcation for\nearly lung adenocarcinoma in CT images: A multi-center study,”Can-\ncers, vol. 13, no. 13, p. 3300, Jun. 2021.\n[37] G. Singal et al. , “Association of patient characteristics and tumor\ngenomics with clinical outcomes among patients with non–small cell\nlung cancer using a clinicogenomic database,” J. Amer. Med. Assoc.,\nvol. 321, no. 14, pp. 1391–1399, 2019.\n[38] J. Zhao et al., “CT characteristics in pulmonary adenocarcinoma with\nepidermal growth factor receptor mutation,”PLoS ONE, vol. 12, no. 9,\nSep. 2017, Art. no. e0182741.\n[39] E. R. Velazquez et al. , “Somatic mutations drive distinct imaging\nphenotypes in lung cancer,”Cancer Res., vol. 77, no. 14, pp. 3922–3930,\nJul. 2017.\n[40] Y . Liuet al., “Radiomic features are associated with EGFR mutation\nstatus in lung adenocarcinomas,” Clin. Lung Cancer, vol. 17, no. 5,\npp. 441–448.e6, Sep. 2016.\n[41] G. Yin et al., “Prediction of EGFR mutation status based on 18F-FDG\nPET/CT imaging using deep learning-based model in lung adenocarci-\nnoma,” Frontiers Oncol., vol. 11, Jul. 2021, Art. no. 709137.\n[42] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “ECA-Net: Efﬁcient\nchannel attention for deep convolutional neural networks,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 13–19.\n[43] T. Zhang et al., “Genomic and evolutionary classiﬁcation of lung cancer\nin never smokers,”Nature Genet., vol. 53, no. 9, pp. 1348–1359, 2021.\n[44] H. Zhou et al., “Multi-region exome sequencing reveals the intratumoral\nheterogeneity of surgically resected small cell lung cancer,” Nature\nCommun., vol. 12, no. 1, Dec. 2021, Art. no. 5431.\n[45] Q. Li et al., “Wavelet integrated CNNs for noise-robust image clas-\nsiﬁcation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\nJun. 2020, pp. 7245–7254.\n[46] Z. Wang, J. Poon, S. Wang, S. Sun, and S. Poon, “A novel method\nfor clinical risk prediction with low-quality data,” Artif. Intell. Med.,\nvol. 114, Apr. 2021, Art. no. 102052.\n[47] Z. Zhang, H. Zhang, L. Zhao, T. Chen, S. Ö. Arik, and T. Pﬁster,\n“Nested hierarchical transformer: Towards accurate, data-efﬁcient and\ninterpretable visual understanding,” in Proc. AAAI Conf. Artif. Intell.,\n2021, pp. 3417–3425.\nWei Zhao received the Ph.D. degree in imaging and\nnuclear medicine from Fudan University, Shanghai,\nChina, in 2019.\nHe is currently an Associate Professor with Central\nSouth University, Changsha, China, where he is also\nthe Assistant Director of the Radiology Department,\nThe Second Xiangya Hospital. His research interests\ninclude imaging, radiomics, and deep learning.\nDr. Zhao is also the Young Talent in Hunan\nProvince and received the Hunan Provincial Natural\nScience Foundation for Excellent Young Scholars.\nWeidao Chen received the master’s degree in\nbiomedical engineering from Zhejiang University,\nHangzhou, China, in 2018.\nHe is currently a Machine Learning Researcher\nwith InferVision Medical Technology Company\nLtd., Beijing, China. His research interests include\nmedical image algorithms and medical artiﬁcial\nintelligence.\nGe Li received the M.S. degree in imaging and\nnuclear medicine from Central South University,\nChangsha, China, in 2016.\nShe is currently a Radiology Technologist with\nthe Xiangya Hospital, Central South University.\nHer research interests include medical imaging and\nbreast imaging.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. \nZHAO et al.: GMILT: A NOVEL TRANSFORMER NETWORK 15\nDu Lei received the Ph.D. degree in radio physics\nfrom East China Normal University, Shanghai,\nChina, in 2011.\nHe is currently a Research Scientist with the\nCollege of Medicine, University of Cincinnati,\nCincinnati, OH, USA. His research interests include\nmagnetic resonance imaging (MRI) and medical\nartiﬁcial intelligence.\nJiancheng Yang (Member, IEEE) r eceived the\nB.Eng. and M.Eng. degrees in automation from\nShanghai Jiao Tong University, Shanghai, China,\nin 2015 and 2018, respectively, and the Engineer\nDegree (double master’s degree) from the Insti-\ntut Mines-Télécom, Évry-Courcouronnes, France,\nin 2016. He is currently pursuing the Ph.D. degree\nwith Shanghai Jiao Tong University.\nHe was a Visiting Researcher with Harvard Uni-\nversity, Cambridge, MA, USA, and the École poly-\ntechnique fédérale de Laus anne (EPFL), Lausanne,\nSwitzerland. His research interests center around the interdisciplinary ﬁeld of\nmedical image analysis and 3-D computer vision.\nYanjing Chen received the bachelor’s degree in\nmedical imaging from Fujian Medical Univer-\nsity, Fuzhou, China, in 2019. She is currently\npursuing the master’s degree in radiology with\nthe Xiangya Hospital, Central South University,\nChangsha, China.\nShe is currently a Research Assistant with The\nSecond Xiangya Hospital, Central South University.\nYingjia Jiang received the bachelor’s degree in\nmedical imaging from Shanxi Medical University,\nJinzhong, China, in 2019.\nShe is currently a Research Assistant with The\nSecond Xiangya Hospital, Central South University,\nChangsha, China. Her research interests include neu-\nroimaging and diagnosis.\nJiangfen Wu received the Biomedical Engineering\ndegree from the Nanjing University of Aeronautics\nand Astronautics, Nanjing, China, in 2014.\nShe is currently in charge of the Institute of Trans-\nlational Medicine, InferVision Medical Technology\nCompany Ltd., Beijing, China. Her research inter-\nests include medical image algorithms and medical\nartiﬁcial intelligence.\nBingbing Ni received the B.Eng. degree in electrical\nengineering from Shanghai Jiao Tong University,\nShanghai, China, in 2005, and the Ph.D. degree from\nthe National University of Singapore, Singapore,\nin 2011.\nHe was a Research Scientist with the Advanced\nDigital Sciences Center, Singapore. He was a\nResearch Intern with Microsoft Research Asia,\nBeijing, China, in 2009. He was a Software Engi-\nneer Intern with Google Inc., Mountain View, CA,\nUSA, in 2010. He is currently a Professor with the\nDepartment of Electrical Engineering, Shanghai Jiao Tong University.\nDr. Ni was a recipient of the First Prize in the International Contest\non Human Activity Recognition and Localization in conjunction with the\nInternational Conference on Pattern Recognition in 2012.\nYeqi Sun received the Ph.D. degree in clinical\npathology from the School of Medicine, Shanghai\nJiao Tong University, Shanghai, China, in 2019.\nShe is currently an Attending Pathologist with The\nSecond Xiangya Hospital, Central South University,\nChangsha, China. Her research interests include\nmolecular pathology and diagnosis.\nShaokang Wang received the master’s degree in\nstatistics from The University of Chicago, Chicago,\nIL, USA, in 2012.\nHe is currently in charge of the application of\nartiﬁcial intelligence technology in medical imag-\ning at InferVision Medical Technology Company\nLtd., Beijing, China. His research interests include\nmedical image algorithms and medical artiﬁcial\nintelligence.\nYingli Sun received the M.D. degree in imaging and\nnuclear medicine from Fudan University, Shanghai,\nChina, in 2018.\nShe is currently an Attending Radiologist with the\nHuadong Hospital Afﬁliated to Fudan University.\nHer research interests include lung disease diagnosis\nand machine learning.\nMing Li received the Ph.D. degree in imaging and\nnuclear medicine from Fudan University, Shanghai,\nChina, in 2012.\nHe is currently a Chief Physician with the\nHuadong Hospital Afﬁliated to Fudan University.\nHis research interests include medical imaging and\nmedical artiﬁcial intelligence.\nJun Liu is currently a Professor with Central South\nUniversity, Changsha, China, where he is also the\nDirector of the RadiologyDepartment, The Second\nXiangya Hospital. His research interests include\nbrain functional imaging, radiomics, and deep\nlearning.\nDr. Liu is also a National Member of the Neu-\nrology Group of the Chinese Society of Radiology,\nNational Committee of the Neurology Group of\nthe Radiological Branch of the Chinese Medical\nAssociation. He is also the Technological Leading\nTalent and the Leader of 225 subjects in Hunan Province.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5449886918067932
    },
    {
      "name": "Computer science",
      "score": 0.40178728103637695
    },
    {
      "name": "Engineering",
      "score": 0.21601155400276184
    },
    {
      "name": "Electrical engineering",
      "score": 0.14076682925224304
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210153856",
      "name": "Second Xiangya Hospital of Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210111607",
      "name": "InferVision (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159865",
      "name": "Xiangya Hospital Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I92156820",
      "name": "University of Cincinnati Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210099446",
      "name": "Huadong Hospital",
      "country": "CN"
    }
  ]
}