{
    "title": "Local and global convolutional transformer-based motor imagery EEG classification",
    "url": "https://openalex.org/W4385984038",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2105392805",
            "name": "Jiayang Zhang",
            "affiliations": [
                "University of Leeds"
            ]
        },
        {
            "id": "https://openalex.org/A2047264473",
            "name": "Kang Li",
            "affiliations": [
                "University of Leeds"
            ]
        },
        {
            "id": "https://openalex.org/A2114151449",
            "name": "Banghua Yang",
            "affiliations": [
                "Shanghai University"
            ]
        },
        {
            "id": "https://openalex.org/A2171991091",
            "name": "Xiaofei Han",
            "affiliations": [
                "University of Leeds"
            ]
        },
        {
            "id": "https://openalex.org/A2105392805",
            "name": "Jiayang Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2047264473",
            "name": "Kang Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114151449",
            "name": "Banghua Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2171991091",
            "name": "Xiaofei Han",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2954214015",
        "https://openalex.org/W6679555981",
        "https://openalex.org/W4229068725",
        "https://openalex.org/W2096597330",
        "https://openalex.org/W2005685654",
        "https://openalex.org/W2915893085",
        "https://openalex.org/W2971518519",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W1991149669",
        "https://openalex.org/W4292731283",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2517500902",
        "https://openalex.org/W2050116264",
        "https://openalex.org/W2167693375",
        "https://openalex.org/W4221164106",
        "https://openalex.org/W2986984881",
        "https://openalex.org/W2559463885",
        "https://openalex.org/W2912885887",
        "https://openalex.org/W2131321253",
        "https://openalex.org/W4225725965",
        "https://openalex.org/W2156192068",
        "https://openalex.org/W2075647286",
        "https://openalex.org/W6847689685",
        "https://openalex.org/W3040552008",
        "https://openalex.org/W6782816849",
        "https://openalex.org/W2307720145",
        "https://openalex.org/W1972708427",
        "https://openalex.org/W2152863712",
        "https://openalex.org/W2140413964",
        "https://openalex.org/W4307235828",
        "https://openalex.org/W2741907166",
        "https://openalex.org/W2151035455",
        "https://openalex.org/W4312597583",
        "https://openalex.org/W2119163516",
        "https://openalex.org/W4205466227",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2990721662",
        "https://openalex.org/W4289538860",
        "https://openalex.org/W6851547765",
        "https://openalex.org/W4295500742",
        "https://openalex.org/W2963283402",
        "https://openalex.org/W3082414409",
        "https://openalex.org/W3102455230",
        "https://openalex.org/W4385486090",
        "https://openalex.org/W4247993926"
    ],
    "abstract": "Transformer, a deep learning model with the self-attention mechanism, combined with the convolution neural network (CNN) has been successfully applied for decoding electroencephalogram (EEG) signals in Motor Imagery (MI) Brain-Computer Interface (BCI). However, the extremely non-linear, nonstationary characteristics of the EEG signals limits the effectiveness and efficiency of the deep learning methods. In addition, the variety of subjects and the experimental sessions impact the model adaptability. In this study, we propose a local and global convolutional transformer-based approach for MI-EEG classification. The local transformer encoder is combined to dynamically extract temporal features and make up for the shortcomings of the CNN model. The spatial features from all channels and the difference in hemispheres are obtained to improve the robustness of the model. To acquire adequate temporal-spatial feature representations, we combine the global transformer encoder and Densely Connected Network to improve the information flow and reuse. To validate the performance of the proposed model, three scenarios including within-session, cross-session and two-session are designed. In the experiments, the proposed method achieves up to 1.46%, 7.49% and 7.46% accuracy improvement respectively in the three scenarios for the public Korean dataset compared with current state-of-the-art models. For the BCI competition IV 2a dataset, the proposed model also achieves a 2.12% and 2.21% improvement for the cross-session and two-session scenarios respectively. The results confirm that the proposed approach can effectively extract much richer set of MI features from the EEG signals and improve the performance in the BCI applications.",
    "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/seven.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nOPEN ACCESS\nEDITED BY\nShugeng Chen,\nFudan University, China\nREVIEWED BY\nGiulia Cisotto,\nUniversity of Milano-Bicocca, Italy\nFangzhou Xu,\nQilu University of Technology, China\n*CORRESPONDENCE\nKang Li\nk.li/one.tnum@leeds.ac.uk\nBanghua Yang\nyangbanghua@shu.edu.cn\nRECEIVED /zero.tnum/nine.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /zero.tnum/seven.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/seven.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nZhang J, Li K, Yang B and Han X (/two.tnum/zero.tnum/two.tnum/three.tnum) Local\nand global convolutional transformer-based\nmotor imagery EEG classiﬁcation.\nFront. Neurosci./one.tnum/seven.tnum:/one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Zhang, Li, Yang and Han. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nLocal and global convolutional\ntransformer-based motor\nimagery EEG classiﬁcation\nJiayang Zhang /one.tnum, Kang Li /one.tnum*, Banghua Yang /two.tnum* and Xiaofei Han /one.tnum\n/one.tnumSchool of Electrical Engineering, University of Leeds, Leeds, Uni ted Kingdom, /two.tnumSchool of Mechatronic\nEngineering and Automation, Shanghai University, Shanghai, Chin a\nTransformer, a deep learning model with the self-attention m echanism, combined\nwith the convolution neural network (CNN) has been successfully app lied\nfor decoding electroencephalogram (EEG) signals in Motor Imager y (MI)\nBrain-Computer Interface (BCI). However, the extremely non-li near, nonstationary\ncharacteristics of the EEG signals limits the eﬀectiveness and eﬃ ciency of the\ndeep learning methods. In addition, the variety of subjects and the experimental\nsessions impact the model adaptability. In this study, we propo se a local\nand global convolutional transformer-based approach for MI-E EG classiﬁcation.\nThe local transformer encoder is combined to dynamically extract tem poral\nfeatures and make up for the shortcomings of the CNN model. The spatial\nfeatures from all channels and the diﬀerence in hemispheres are obtained\nto improve the robustness of the model. To acquire adequate temp oral-\nspatial feature representations, we combine the global transf ormer encoder\nand Densely Connected Network to improve the information ﬂow an d reuse.\nTo validate the performance of the proposed model, three scena rios including\nwithin-session, cross-session and two-session are designed. In the experiments,\nthe proposed method achieves up to /one.tnum./four.tnum/six.tnum%, /seven.tnum./four.tnum/nine.tnum% and /seven.tnum./four.tnum/six.tnum% accuracy\nimprovement respectively in the three scenarios for the publi c Korean dataset\ncompared with current state-of-the-art models. For the BCI comp etition IV /two.tnuma\ndataset, the proposed model also achieves a /two.tnum./one.tnum/two.tnum% and /two.tnum./two.tnum/one.tnum% improvement for\nthe cross-session and two-session scenarios respectively. The results conﬁrm that\nthe proposed approach can eﬀectively extract much richer set of MI f eatures from\nthe EEG signals and improve the performance in the BCI applicati ons.\nKEYWORDS\nbrain-computer interface, motor imagery, transformer, attent ion mechanism,\nConvolutional Neural Network\n/one.tnum. Introduction\nThe brain-computer interface (BCI), as a promising tool for stroke rehabilitation and\nother biomedical applications, enables people to interact with external devices by decoding\nthe electroencephalogram (EEG) signals generated by various brain activities (\nMane et al.,\n2020a). EEG is widely used in the clinical and neuroscience domain, especially in BCI\nsystems because of its excellent properties of noninvasiveness and portability ( Zhang and\nLi, 2022 ). Motor imagery (MI), the mental rehearsal of physical movement task ( Decety\nand Ingvar, 1990 ), is commonly used to allow disabled people to self-regulate EEG signals\nthrough active modulation rather than external stimulus and assistance. When a person is\nimagining a certain motor behavior, the related motor cortex generates the corresponding\nneuron responses which help improve motor functional recovery by reducing the gap\ninduced by brain disorders between motor intention and sensory feedback of motor\nmovements (\nCraik et al., 2019 ).\nFrontiers in Neuroscience /zero.tnum/one.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nHowever, the non-stationary, low signal-to-noise ratio and\nnon-linear characteristics make it diﬃcult to decode MI-EEG\nsignals. Traditional machine-learning approaches mainly focus\non analyzing the spatial information from MI-EEG signals. The\nrepresentative method called the Common Spatial Pattern (CSP)\nis valid in building optimal spatial ﬁlters to distinguish binary\nMI tasks (\nPfurtscheller and Neuper, 2001 ). The improved variants\nof the CSP method such as Common Spatio-Spectral Pattern\n(CSSP) (\nLemm et al., 2005 ), Sub-band Common Spatial Pattern\n(SBCSP) ( Novi et al., 2007 ) and Filter Bank Common Spatial\nPattern (FBCSP) ( Ang et al., 2008 ) have also been implemented\nsuccessfully in MI classiﬁcation. Further, Alexandre et al. proposed\nthe minimum distance to Riemannian mean (MDM) and tangent\nspace mapping (TSM) (\nBarachant et al., 2011 ) for the EEG\nclassiﬁcation by using the topology of the manifold of symmetric\nand positive deﬁnite (SPD) matrices. Conventional classiﬁers such\nas Linear Discriminant Analysis (LDA) (\nChen et al., 2014 ) and\nSupport Vector Machines (SVM) ( Li et al., 2013 ) are common\nmethods used in MI-BCI. Although many methods have achieved\nimpressive results, the unﬁtting and ineﬃcient combination of\nfeature extraction methods and classiﬁers limits the model’s\naccuracy, robustness, and adaptability performance.\nAs an end-to-end signal processing method, Deep Learning\n(DL) has been successfully applied in extracting and analyzing\nabstract information from MI-EEG signals in recent years (\nLotte\net al., 2007 ; Zancanaro et al., 2023 ). For example, Schirrmeister\net al. proposed several models based on the Convolutional\nNeural Networks (CNNs) according to the principle of FBCSP\n(\nSchirrmeister et al., 2017 ). The two ﬁrst layers used in the model\ncapture temporal and spatial features which greatly inﬂuence\nsubsequent studies.\nLawhern et al. (2018) replaced the common\nCNN layer with the separable one to reduce the calculated\ndimensions. The hyper-parameters were also adopted according\nto the sampling rate and brain rhythms. To further improve the\ncapabilities of the DL model based on CNNs,\nDai et al. (2020)\nand Wu et al. (2019) used multiscale CNN ﬁlters to capture the\nfeatures in diﬀerent ﬁelds of view. Huang et al. adopted the SPD\nmatrices as inputs to capture the spatial patterns of EEG signals\n(\nHuang and Van Gool, 2017 ). Syed et al. fused the results from\nfour parallel CNN structures with multi-depth for fusing shallow\nand deep layers to learn the relevant MI information at diﬀerent\nlevels (\nAmin et al., 2019 ). Mane et al. (2020b) proposed a multi-\nview CNN namely Filter-Bank Convolutional Network(FBCNet)\nto encode spectro-spatial discriminative information from MI-\nEEG with various spectral ﬁltering.\nZhang et al. (2019) proposed\na hybrid architecture based on the CNN and Long Short-term\nMemory (LSTM) for processing time series signals. However, the\ncharacteristics of hard inductive bias in the CNNs and LSTM are\noverly restrictive, limiting the potential performance (\nChen et al.,\n2018). The locality of CNNs impairs the ability to extract features\nfrom long-range signals like MI-EEG. The huge serial computing\nconsumption and restricted sequence length in LSTM bring the\nchallenge in decoding MI-EEG signals.\nIn recent years, the DL model based on the transformer\n(\nVaswani et al., 2017 ) with a multi-head attention mechanism\nhas been successfully applied in the Computer Vision (CV) and\nNatural Language Processing (NLP) domain. Compared with the\nCNN or LSTM structure, the transformer has soft inductive bias\n(\nd’Ascoli et al., 2021 ) increasing the upper limit of model\nperformance. The Self-Attention (SA) structure and parallel\ncomputing mode allow the transfer extract global information\nwithout multiple convolutions and pooling calculations. In the\nBCI ﬁeld, the transformer is adopted to handle signals in the\napplications such as person identiﬁcation (\nDu et al., 2022 ), emotion\nrecognition ( Li et al., 2022 ), visual stimulus classiﬁcation ( Bagchi\nand Bathula, 2022 ) and signal denoising ( Pu et al., 2022 ). For MI-\nEEG decoding, Ma et al. ( Ma et al., 2022 ) proposed a hybrid CNN-\nTransformer model to weigh spatial features and frequency signals\nby employing the attention mechanism. However, the model uses\nthe CSP features as inputs which loses the advantage of the end-\nto-end process in the DL model.\nSong et al. (2023) also proposed\na hybrid model with six transformer encoders after extracting\nfeatures from MI-EEG by CNN layers. The model performs well\nin the hold-out tests, but the huge computational costs caused by\nencoders limit the actual use.\nTao et al. (2021) employed the gating\nmechanism on the transformer to improve the model performance,\nbut missed the extraction of EEG spatial information.\nXie et al.\n(2022) designed ﬁve hybrid models with diﬀerent layers in the CNN\nand transformer. This study adapted the model in the cross-subject\nscenario with much more training data than small data scenarios\nlike within-subject and within-session applications, limiting the\nmodel robustness. Besides that, the shortfall of these studies is that\nthey only extract the spatial features from the fusion of all channels,\nneglecting the possible information learned from the diﬀerences\nbetween the hemispheres.\nTo address the above issues, a novel approach with the local and\nglobal transformer combing with CNNs for MI-EEG classiﬁcation\nis proposed in this study. First, we adopt the local transformer\nand 1-dimension CNN ﬁlter with the same kernel size to extract\ntemporal features from each channel. Although the respective\nﬁelds from the local transformer and CNN are the same in the\nbeginning, the diﬀerent mechanisms allow the model to learn\na comprehensive set of useful and subtle features from multi-\nviews. The local transformer also avoids the overﬁtting problem\ncompared with the global transformer which extracts more subtle\nfeatures from raw EEG signals in the ﬁrst layer. Then, two parallel\nbranches use diﬀerent depthwise CNN to extract and fuse diﬀerent\nspatial information. One branch focuses on all channels in the\nmotor cortex and the other one extracts the features of channels\nfrom the left and right motor regions respectively. Next, for\nbetter mining the temporal-spatial features, we use the Densely\nConnected CNN (DenseNet) (\nHuang et al., 2017 ) on both CNN and\nglobal transformer layers by connecting each layer to every other\nin a feed-forward way. The short path helps the information reuse\nand ﬂow which improve the model adaptability and robustness.\nFinally, the proposed model is validated and compared with other\nbaseline models in diﬀerent scenarios including within-session and\ncross-session to verify its performance.\n/two.tnum. Materials and methods\nIn this section, we ﬁrst introduce the dataset and preprocessing\napproach used in the experiment brieﬂy. The diﬀerent scenarios are\ngiven in detail. Then, we present the proposed model including the\nmechanism, structure and hyper-parameters.\nFrontiers in Neuroscience /zero.tnum/two.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\n/two.tnum./one.tnum. Dataset and preprocessing\nWe used the Korea University dataset ( Lee et al., 2019 ) and\nthe BCI Competition IV 2a ( Tangermann et al., 2012 ) dataset to\nevaluate the proposed model performance on the two-class and\nfour-class MI tasks classiﬁcation.\n(1) Korea University (KU) Dataset: We used the Korea\nUniversity Dataset containing 54 subjects with binary MI tasks of\nthe left hand and right hand. Two sessions were conducted on\ndiﬀerent days in the dataset, each with 200 trials for every subject.\nThe MI-EEG signals were collected by 62 Ag/AgCl electrodes\nwith impedances of less than 10 k)Omega1 . To better decode MI\ninformation, 20 electrodes in the motor cortex region were selected\n(C-z/1/2/3/4/5/6, CP-z/1/2/3/4/5/6, FC-1/2/3/4/5/6) according to\nprevious studies (\nKwon et al., 2019 ; Mane et al., 2020b ; Ju and\nGuan, 2022). The sampling rate was 1,000 Hz and we downsampled\nto 250 Hz.\n(2) BCI Competition IV 2a (BCIC-IV-2a) Dataset: The BCIC-\nIV-2a consists of recordings from nine healthy subjects performing\nfour diﬀerent motor imagery tasks: left-hand, right-hand, both-\nfoot, and tongue. The signals were acquired using 22 EEG\nelectrodes with a sampling frequency of 250 Hz and were bandpass\nﬁltered between 0.5 Hz and 100 Hz, as well as notch ﬁltered at 50\nHz. Two sessions were recorded on diﬀerent days for each subject,\nwith each session comprising 288 trials. The dataset only has 22\nchannels so that we feed all channel signals into the proposed\nmodel.\nThe most common frequency band used in the MI-EEG ﬁeld\nis α rhythm (\nJasper and Andrews, 1938 ) which is about 10 Hz and\nβ rhythm which is around 20 Hz ( Jasper and Penﬁeld, 1949 ). The\nﬁlter bands that include useful spectral MI information vary from\nperson to person (\nNovi et al., 2007 ). Therefore, some studies ( Kwon\net al., 2019 ; Mane et al., 2020b ; Ma et al., 2022 ) divided the raw\nMI signals into several bands with a 4 Hz length ranging from\n4 to 40 Hz by spectral ﬁlters. Considering the extra calculations\ncaused by multi-inputs, we only feed three inputs including the\nraw signals and two ﬁltered bands based on α (7–12 Hz) and β\n(13–32 Hz) rhythms. Each trial has 4 s with 1,000 samples in total.\nWe employed the Z-score normalization to handle the signals, as\nshown:\nZ = x − µ\nσ (1)\nwhere x was the raw data of each channel. µ was the mean value\nof x and σ represents the standard deviation.\n/two.tnum./two.tnum. Scenarios description\nWe design three scenarios of the within-subject analysis using\ndata from the same subject for training, validation, and testing\n(\nFigure 1). Diﬀerent scenarios help verify the models’ adaptability\nand robustness for actual applications. The details of diﬀerent\nscenarios are described as follows:\n1) Within-Session Scenario: This scenario only uses one session\nwith 200 trials for 10-fold cross-validation (CV). Although the\ntraining data is limited, within-session ensures the stability of\nthe data distribution as far as possible.\n2) Cross-Session Scenario: The ﬁrst session is used for training\nand the second one for testing. Two cases are presented in this\nscenario considering the diﬀerent applications in reality. The\nﬁrst namely the hold-out scenario uses the part of the data in\nsession two for validation and the rest for the test. The other case\nonly uses the whole data in session two for the test, ensuring no\ndata participates in validation at the modeling stage. In either\ncase, the data from session two will not be used in training.\nDue to the circumstance that two sessions were conducted on\ndiﬀerent days, the drift of statistical distributions brings the\nchallenge for classiﬁcation.\n3) Two-Session Scenario: Two sessions of one subject are grouped\nfor a 10-fold CV to show the performance of the models in\nbig data.\nIn the BCIC-IV-2a dataset, each phase contains 144 trials\nbecause there are 288 trials for each session while one phase in the\nKU dataset has only 100 trials.\n/two.tnum./three.tnum. The proposed model\n/two.tnum./three.tnum./one.tnum. Architecture\nThe proposed model has three branches which were fed from\nﬁltered data and concentrated by a fully connected layer for fusing\nfeatures from multi-bands. Each branch has the same structure\nconsisting of the temporal block, spatial block and transformer-\nbased densenet block (T-Densenet Block) (\nFigure 2).\n/two.tnum./three.tnum./two.tnum. Temporal block\nConsidering that the MI-EEG signals are time series, the\nprevious studies ( Schirrmeister et al., 2017 ; Lawhern et al., 2018 ;\nMane et al., 2020b ) preferred using a 1-D CNN ﬁlter to extract\nthe temporal feature which is one of the most distinguished MI\ninformation. CNN ﬁlter has a strong inductive bias of weight\nsharing (\nSimoncelli and Olshausen, 2001 ). Such a characteristic\nreduces a huge amount of computation and makes a model more\nparameter-eﬃcient, but it ignores the dynamic relationship among\nthe input data in a kernel with the ﬁlter sliding because the weights\nlearned by the CNN are ﬁxed after training.\n/two.tnum./three.tnum./two.tnum./one.tnum. Self-attention\nThe self-attention mechanism focuses more on the correlation\nbetween each value in the kernel and all other values. First, the\ntransformer encoder divides the input into three representations\nnamely Queries (Q), Keys (K) and Values (V) by the linear dense\nlayers. Then the speciﬁc attention “Scaled Dot-Product Attention”\n(shown in\nFigure 3d) computed the dot products of the queries with\nall keys. The results were divided by\n√\ndk and ended with a softmax\nfunction to obtain the weights on the values. The formula is:\nAttention(Q, K, V) = softmax\n\n QKT\n√\ndk\n\n V (2)\nFrontiers in Neuroscience /zero.tnum/three.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /one.tnum\nDescriptions of diﬀerent scenarios (KU dataset). (A) Within-session scenario. (B) Cross-session scenario case /one.tnum.(C) Cross-session scenario case /two.tnum.(D)\nTwo-session scenario.\nwhere dk was the dimension of keys. To better jointly learn\nthe information from diﬀerent representation subspaces at diﬀerent\npositions (\nVaswani et al., 2017 ), the scaled dot-product attention\nwas embedded in the structure of the “Multi-head Self Attention”\n(\nFigure 3d):\nMultiHead (Q, K, V) = Concat\n(\nhead1, ..., headh\n)\nWO\nheadi = Attention(Q, K, V) (3)\nwhere WO ∈ Rhdv× dmodel , hdv resprents the dimension of values\nand dmodel is the dimension of the outputs. We employ h = 2\nparallel attention layers in the proposed model. From Equations (2)\nand (3), the calculation of the output is determined by a weighted\ntotal of the values, and the weight for each value is determined by\na function that assesses the compatibility between the query and its\ncorresponding key. Therefore, the weights are dynamic rather than\nﬁxed like CNN ﬁlters.\n/two.tnum./three.tnum./two.tnum./two.tnum. Local transformer encoder\nIn this work, to take full advantage of the characteristics of the\nmodes of CNN and transformer, we add the outputs from the local\ntransformer encoder and those from the CNN ﬁlter together as\nthe ﬁnal temporal features (\nFigure 3a). Compared with the global\ntransformer that obtains the attention score of a query based on all\nkeys (\nFigure 4B), the local transformer encoder reduces the number\nof keys to ensure that the queries are multiplied by the limited\nkeys every time (\nFigure 4A). Such a mode improves the temporal\nfeature decoding by increasing the locality. Although the local\nmode cannot learn the global features, it selects local subtle features\nwhich otherwise are largely ignored in the global mode. And it can\nfurther overcome the overﬁtting and underﬁtting problems for long\nraw EEG signals.\n/two.tnum./three.tnum./two.tnum./three.tnum. Positional encoding\nConsidering that the MI-EEG signals are the sequence that has\nthe order, the position information is injected by the sum of the\nPositional Encoding (PE) value and the raw signals. According\nto the successful PE application in the MI-EEG ﬁeld (\nXie et al.,\n2022), we used sine and cosine functions to represent the position\nas follows:\nPE(pos,2i) = sin\n( pos\n100002i/d\n)\n(4)\nPE(pos,2i+ 1) = cos\n( pos\n100002i/d\n)\n(5)\nwhere pos means the position and i is the dimension. d\nrepresents the dimension of the inputs.\n/two.tnum./three.tnum./three.tnum. Spatial block\nPrevious research has already demonstrated the feasibility of\nusing the brain hemisphere to control both the left and right hands,\nbut the degree of control for each hand diﬀers due to lateralization\n(\nMüller et al., 1998 ; Martin et al., 2016 ). Therefore, the spatial\nfeature diﬀerences between the two hemispheres may potentially\nFrontiers in Neuroscience /zero.tnum/four.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /two.tnum\nThe proposed model structure.\nbe useful for motor imagery classiﬁcation. After concatenating\nthe temporal features learned from the CNN and the local\ntransformer encoder, the depthwise CNNs are used to extract\nspatial information from the EEG channels. The proposed model\nsends the input into three parallel paths (\nFigure 3b). The ﬁrst CNN\nﬁlters extract the spatial features from all C channels in the motor\nregion. The rest two CNN ﬁlters extract features from C− 1\n2 channels\nin the left hemisphere and the right hemisphere respectively. The\nextra channel Cz was deleted because it was set in the central\nposition. Then the diﬀerence was obtained by subtracting the\nfeatures of the two hemispheres. Finally, the spatial features based\non the channels from the motor region and the diﬀerence caused\nby hemispheres are fed into the next block.\n/two.tnum./three.tnum./four.tnum. T-Dense block\nThis block comprises one T-Dense Unit and a 1-D CNN\nﬁlter, as shown in\nFigure 3c. The T-Dense Unit ( Figure 3e) has\ntwo branches with the CNN ﬁlters and the global transformers\n(\nFigure 4B) respectively. Both branches in the T-dense unit has\nthe similar structure and processing steps as shown in Figure 3e.\nFor instance, in the CNN ﬁlter branch, the features of the ﬁrst\nCNN ﬁlter are concatenated with the ones from the second\nﬁlter to feed into the third CNN ﬁlter. Each subsequent layer of\nCNN is not only connected to the immediate preceding layer,\nbut also to all other preceding layers, enabling the establishment\nof shorter paths to help the ﬂow and reuse of the information\n(\nHuang et al., 2017 ). Meanwhile, batch normalization and dropout\ntechniques are applied to address the overﬁtting issue. In the T-\ndense unit, the global mode is applied to the transformer to retain\nits original advantages of extracting the global information using\nall neurons based on the SA mechanism while CNN ﬁlters works\ndiﬀerently, as they learn the global information by sliding and\npooling layers steps given the limited size of a ﬁlter. The branches\nin the T-Dense Unit are combined to learn a comprehensive set\nof features which otherwise can not be achieved using a single\nfeature extraction mechanism. After the T-Dense Unit, the 1-D\nCNN layer is used to reduce the dimension thus to reduce the\ncalculation burden for the subsequent output neurons after three\nparallel branches.\nFrontiers in Neuroscience /zero.tnum/five.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /three.tnum\nThe proposed model structure.\n/two.tnum./three.tnum./five.tnum. Training setup\nThe cross-entropy function is employed as a loss function\nwhich evaluates the distance between the probability distribution\nof the model prediction values yp and the true labels yt:\nL\n(\nyp, yt\n)\n= −\n∑\nm\nyp,m log yt,m. (6)\nwhere m is the index of y. The optimizer is Adam (\nKingma\nand Ba, 2014 ) and the learning rate is set to 0.0001. The training\ntakes 800 epochs with 32 batches per epoch. The early stopping\ntechnology was used to save the best weights. The training step\nended after checking if the validation loss value decreased for the\nlast 100 epochs. After reaching the threshold, the model with the\nbest weights produces the classiﬁcation results of the test fold.\nThe computer used in this experiment had 15 Intel processors\nand 80 GB RAM. GTX 3090 GPU with 24 GB memory was used for\ntraining and testing MI-EEG signals. Keras based on TensorFlow\nwas used for constructing the proposed model.\n/three.tnum. Results\n/three.tnum./one.tnum. Performance comparison\nWe evaluate the proposed model and other models in the\ndiﬀerent scenarios. The average classiﬁcation accuracies of all\nsubjects of the KU dataset and BCIC-IV-2a with standard deviation\n(SD) are shown in\nTables 1, 2 respectively.\nIn the KU dataset, our proposed model achieved the best\nperformance in all scenarios, especially on the cross-session and\ntwo-session ones. Constrained by the limited data size of each\nsubject, which only comprises 200 trials per session, achieving\neven slight improvements can be a challenge. In the within-session\nscenario, the proposed model achieved accuracy of 75.94% and\n77.38% in session 1 and session 2 respectively. which are 0.99%\nand 1.46% higher than the best public model namely tensor-\nCSPNet. When utilizing twice the amount of data in the two-\nsession scenario, the proposed model achieved a classiﬁcation rate\nexceeding 80%, 7.46% higher than the Shallow ConvNet. In two\ncases of cross-session scenarios, as\nFigures 1B, C presented, both\nof them used the data from session 2 as the test and did not\nallow them to present in the training step. The diﬀerence was\nthat case 1 used half of the data from session 2 to validate while\ncase 2 did not use it. Considering the drift of data distributions\ncaused by the diﬀerent sessions conducted on diﬀerent days,\nthe results of cross-session are lower than the ones of within-\nsession. The performances of most compared methods decrease\nincluding the proposed model. The existing high-performing\nmodels such as FBCNet and Tensor-CSPNet exhibited reduced\nperformance to <70% while the proposed model only lost an\naverage of 0.24% accuracy and still produced the best accuracy\nof 77.14% in case1. Given the BCI application that people often\nonly used the data collected in one day to build the model\nwithout training or updating in the following day for saving\npatients’ time, case 2 is more suitable in practical applications. The\nproposed model achieved 74.51%, a much higher accuracy than\nthe benchmarks, which conﬁrms the superiority of our proposed\nmodel on adaptability and robustness. The statistical test was also\nconducted to compare the performances of diﬀerent models. We\nobserved that the proposed model outperformed most baseline\nmodels ( p < 0.001), FBCNet( p < 0.05) and Tensor-CSPNet\n(p < 0.05) in diﬀerent scenarios.\nFrontiers in Neuroscience /zero.tnum/six.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /four.tnum\nAttention patterns in the transformer. The blue squares repre sent\ncorresponding attention scores are calculated and the blank ones\nmean the attention score is discarded. (A) Local pattern. (B) Global\npattern.\nIn the BCIC-IV-2a dataset, FBCNet performed best in two\nwithin-session scenarios while the proposed model showed an\naccuracy decrease of 3.07% and 0.33% respectively in session 1\nand session 2. However, in the two cross-session scenarios, our\nproposed model improved signiﬁcantly which reached 75.84% in\ncase1, 1.24% higher than the Shallow ConvNet and 75.08% in\ncase2, 2.12% higher than the Tensor-CSPNet. The result in the two-\nsession scenario also reached 81.04% which improved the accuracy\nby 2% compared to the Shallow ConvNet. The statistical test\nshowed that the proposed model outperformed all baseline models\n(p < 0.05) in both of the cross-session and two-session scenarios.\nWe also checked the statistical signiﬁcance between each\nscenario. The t-tests result of within-session 1 with within-session\n2 was [ correlation = 0.781, p < 0.001; t(53) = − 1.062, p = 0.293]\nand [ correlation = 0.88, p < 0.01; t(53) = − 1.024, p = 0.336]\nin KU and BCIC-IV-2a dataset separately, which shows that there\nis consistency between diﬀerent sessions for each subject, but the\ndiﬀerence between two sessions is not statistically signiﬁcant. In\nthe KU dataset, the t-tests of within-session1 with case 1 and\ncase 2 in the cross-session scenario were [ t(53) = − 0.838, p =\n0.406] and [ t(53) = 1.182, p = 0.242]. The t-tests in BCIC-IV-\n2a were [ t(53) = − 0.627, p = 0.548] and [ t(53) = − 0.475, p =\n0.648]. Both results of the t-test did not have statistically signiﬁcant\ndiﬀerences. Hence, the data quality of an individual varies on\ndiﬀerent days. Building a model for each day is time-consuming\nand impractical, but employing a cross-session model may result\nin a decrease in classiﬁcation accuracy, making it a challenging\ntask. The t-tests results of within-session 1 with two-session and\nwithin-session 2 with two-session are [ correlation = 0.882, p <\n0.001; t(53) = − 4.514, p < 0.001] and [ correlation = 0.901, p <\n0.001; t(53) = − 3.102, p < 0.01] separately. Evidently, an\nincrease in the volume of data contributes to the enhancement of\nmodel performance even though the sessions were collected on\ndiﬀerent days.\nIn summary, for the KU dataset, the proposed model\noutperforms other models, achieving up to 0.99% and 1.46% for\nthe session 1 and 2 respectively in the within-session scenario, up\nto 7.49% and 8.19% for the case 1 and 2 respectively in the cross-\nsession scenario and up to 7.46% for the two-session scenario.\nWhen testing on the BCIC-IV-2a dataset, the model can also\nimprove the classiﬁcation accuracy by 1.24% and 2.12% for the\ncase 1 and 2 in the cross-session scenario and 2.21% for the two-\nsession scenario, conﬁrming the superiority of the proposed model\nin decoding MI-EEG information.\n/three.tnum./two.tnum. Ablation study\nThe purpose of an ablation study is to assess the impact of\nspeciﬁc components on the overall performance of a model by\nremoving them and analyzing their contribution. We conducted\nthe ablation tests to evaluate the eﬀectiveness of the transformer\nencoders, the hemisphere diﬀerence in the spatial block, and the\nT-Dense units in diﬀerent scenarios. (1) The proposed model\nwithout transform encoders (w/o_Trans) removes both the local\nand global transformer encoders; (2) The proposed model without\nFrontiers in Neuroscience /zero.tnum/seven.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nTABLE /one.tnum Comparison of average classiﬁcation accuracy (%) and standard deviation (SD) for diﬀerent methods (KU dataset).\nWithin-session Cross-session Two-session\nSession/one.tnum (SD)Session/two.tnum (SD)Case/one.tnum (SD)Case/two.tnum (SD)Session /one.tnum&/two.tnum (SD)\nCSP 56.53 (13.10) 58.38 (14.63) 61.70 (16.14) 60.43 (13.98) 55.80 (11.07)\nFBCSP 64.41 (16.28) 66.47 (16.53) 59.67 (14.32) 61.57 (14.73) 65.62 (14.75)\nMDM 50.47 (8.63) 51.93 (9.79) 52.33 (6.74) - -\nTSM 54.59 (8.94) 54.97 (9.93) 51.65 (6.11) - -\nSPDNet 57.88 (8.68) 58.88 (8.68) 60.41 (12.13) - -\nShallow ConvNet 67.73 (17.58) 68.47 (17.65) 67.79 (19.16) 66.32 (16.18) 72.74 (15.82)\nDeep ConvNet 56.19 (13.71) 57.38 (15.27) 56.59 (15.29) 56.75 (13.03) 62.91 (17.64)\nEEGNet 63.37 (17.06) 64.73 (17.97) 65.26 (19.31) 63.28 (15.69) 69.73 (17.05)\nFBCNet 74.16 (12.60) 73.81 (13.99) 67.83 (14.34) - -\nTensor-CSPNet 74.95 (15.27) 75.92 (13.99) 69.65 (14.97) - -\nProposed model 75.94 (14.71) 77.38 (15.29) 77.14 (14.76) 74.51 (13.93) 80.20 (13.01)\nThe bold values indicate the highest value within each column of data in the table.\nTABLE /two.tnum Comparison of average classiﬁcation accuracy (%) and standard deviation (SD) for diﬀerent methods (BCIC-IV-/two.tnuma dataset).\nWithin-session Cross-session Two-session\nSession/one.tnum (SD)Session/two.tnum (SD)Case/one.tnum (SD)Case/two.tnum (SD)Session /one.tnum&/two.tnum (SD)\nCSP 57.75 (13.71) 60.60 (14.29) 54.01 (12.77) 54.07 (12.13) 57.15 (12.26)\nFBCSP 73.57 (16.28) 72.46 (16.53) 65.59 (17.51) 65.79 (14.21) 75.01 (12.97)\nMDM 62.96 (14.01) 59.49 (16.63) - 50.74 (13.80) -\nTSM 68.71 (14.32) 63.32 (12.68) - 49.72 (12.39) -\nSPDNet 65.91 (10.31) 61.16 (10.50) - 55.67 (9.54) -\nShallow ConvNet 71.83 (15.63) 72.64 (19.62) 74.61 (12.36) 68.96 (14.28) 78.83 (12.32)\nEEGNet 69.26 (11.59) 66.93 (11.31) 61.65 (14.20) 60.31 (10.52) 70.67 (17.27)\nFBCNet 77.26 (14.82) 76.58 (13.09) - 72.71 (14.67) -\nTensor-CSPNet 75.98 (14.26) 74.92 (14.63) - 72.96 (14.98) -\nProposed model 74.19 (10.60) 76.25 (12.67) 75.85 (14.11) 75.08 (12.66) 81.04 (8.54)\nThe bold values indicate the highest value within each column of data in the table.\nthe hemisphere diﬀerence in the spatial block (w/o_Diﬀ-hemi)\nremoves the structures in\nFigure 3b which extract the spatial\nfeatures from each hemisphere and calculate the diﬀerence. The\nprevious models proposed in the literature which were shown to\nachieve good classiﬁcation results in the KU EEG dataset such as\nEEGNet (\nLawhern et al., 2018 ), Shallow ConvNet ( Schirrmeister\net al., 2017 ) and FBCNet ( Mane et al., 2020b ) focus on the spatial\nfeatures from all channels in the motor region and ignore the\navailable information that might be learned from the hemispheric\ndiﬀerences. (3) The proposed model without T-dense units (w/o_T-\ndense) replaces the two T-dense units with common CNN layers.\nThe results of the ablation study in diﬀerent scenarios are shown in\nTable 3.\nIn the KU dataset, the T-test results indicated that there is no\nstatistical signiﬁcance ( p > 0.05) to show the w/o_trans brings\na negative impact on the classiﬁcation accuracy in session 1 of\nthe within-session scenario. Apart from this, the absence of any\nspeciﬁc components will make the accuracy drop( p < 0.05).\nEspecially for session 2 in the within-session scenario, without\nthe transformer encoders, the classiﬁcation result decreased by\n9.37%. In the BCIC-IV-2a dataset, although in within-session1 and\ntwo-session scenarios, the model without transformer encoders\nperformed better, other cases still show the importance of the\ndiﬀerent modules. The statistical analysis showed that w/o_trans\ndecreased the classiﬁcation accuracy in the cross-session scenario\n(p < 0.05). The w/o_Diﬀ-hemi had signiﬁcance in within-session\nand cross-session scenarios ( p < 0.05) while the T-test result\nin the two-session scenario was p = 0.127. The w/o_T-dense\nhad statistical signiﬁcance in all scenarios ( p < 0.01). It is clear\nthat the T-dense has signiﬁcant contributions to the classiﬁcation\naccuracy improvement, as it produces a comprehensive set of\ntemporal-spatial features. Without this module, using a simple\ntemporal block and spatial block is unable to capture suﬃcient\nand subtle useful features embedded in the highly corrupted\nand diﬀused EEG raw data. We also tested the selection of\nthe activation functions on cross-session case 1 from the KU\ndataset (\nFigure 5). The ELU function performed best with the\nhighest accuracy.\nFrontiers in Neuroscience /zero.tnum/eight.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nTABLE /three.tnum Ablation study of the proposed method on the diﬀerent modules.\nWithin-session Cross-session Two-session\nSession/one.tnum (SD)Session/two.tnum (SD)Case/one.tnum (SD)Case/two.tnum (SD)Session /one.tnum&/two.tnum (SD)\nKU dataset\nw/o_trans 75.81 (14.22) 68.01 (13..31) 75.02 (14.89) 66.42 (11.01) 72.88 (12.63)\nw/o_diﬀ-hemi 75.20 (14.92) 75.98 (15.59) 73.89 (16.01) 73.20 (13.98) 78.89 (13.60)\nw/o_T-dense 67.88 (12.36) 68.53 (13.12) 68.54 (12.88) 66.74 (11.39) 73.61 (12.81)\nProposed model 75.94 (14.71) 77.38 (15.29) 77.14 (14.76) 74.51 (13.93) 80.20 (13.01)\nBCIC-IV-/two.tnuma dataset\nw/o_trans 74.64 (11.21) 76.07 (12.58) 73.14 (13.64) 73.64 (11.54) 84.01(8.60)\nw/o_diﬀ-hemi 72.56 (11.09) 73.95 (14.59) 70.67 (15.15) 73.72 (11.82) 78.91(10.08)\nw/o_T-dense 63.31 (8.43) 65.74 (10.11) 68.82 (13.73) 63.12 (6.47) 77.21 (8.66)\nProposed model 74.19 (10.60) 76.25 (12.67) 75.85 (14.11) 75.08 (12.66) 81.04 (8.54)\nThe bold values indicate the highest value within each column of data in the table.\n/three.tnum./three.tnum. Complexity\nTable 4 shows the model complexity based on the number\nof trainable parameters. The results show that there is no\ndecisive relationship between the complexity of a model and its\nperformance. Deep ConvNet has the most parameters because\nof more CNN layers used in the structure. However, regardless\nof the scenarios, the Deep ConvNet performs badly even worse\nthan the traditional approach FBCSP. Among these compared\nmodels, the EEGNet only has no more than 2k trainable parameters\nbecause the depthwise separable convolution layer is employed\nto reduce the dimensions. However, EEGNet performs much\nbetter than the Deep ConNet in each scenario. The Tensor-\nCSPNet divides the raw signals into several frequency bands\nto learn subtle features within diﬀerent frequency bands, thus\nencompassing the spectral diﬀerences among diﬀerent subjects.\nThis approach adds additional computational parameters but\nthe model performance is the best as demonstrated in the\nprevious studies. The proposed model includes 12K of trainable\nparameters that are only half of the Tensor-CSPNet but has\nbetter classiﬁcation results, which demonstrated its eﬃcacy and\neﬀectiveness.\n/three.tnum./four.tnum. Feature visualization\nThe t-distributed Stochastic Neighbor Embedding (t-SNE)\napproach was employed to visualize the feature distribution\nafter the last fully connected layer of the proposed model.\nFigure 6 shows the comparison of the visualization based on\nthe diﬀerent scenarios. We used the data from subject 3 in the\ntwo datasets respectively.\nFigures 6A–E belongs to the BCIC-IV-\n2a while Figures 6F–J belongs to the KU dataset. Each color\nrepresents one label of MI-EEG tasks. According to the t-SNE\nresult, the proposed model showed a great ability to classify EEG\nsignals. In comparison to within-session, the feature distribution\nin cross-session and two-session scenarios appears to be more\ndispersed. However, there are still clear distinctions that can\nbe observed, further showing the superior performance of the\nproposed model.\n/four.tnum. Discussion\nIn this work, we proposed a local and global convolutional\ntransformer-based model for MI-EEG classiﬁcation. The\ntransformer encoder with the self-attention mechanism is\nwidely applied to the computer version and natural language\nprocessing. Compared with the CNN limited by the size of its ﬁlter,\nthe transformer can capture all samples simultaneously, which is\nsuitable to extract global features. Meanwhile, the calculation step\nof the self-attention mechanism focuses on ﬁnding the relationship\nof diﬀerent features while CNN extracts common mode from\nfeatures. Once the CNN-based model is trained, the weights\nin the ﬁlters are ﬁxed. However, in a transformer encoder, the\nweights depend on the inputs, so they are dynamically changed\naccording to the data. Previous studies have shown that the EEG, as\nintricate time series, varies from subject to subject which makes the\ntransformer a suitable approach for processing EEG signals. Due\nto the distinct characteristics of CNN and transformer, combining\nand complementing each other makes for exploring more useful\nfeatures of EEG signals and ensuring the robustness of the model.\nIn the proposed model, we employed two strategies for the\ntransformer, speciﬁcally the local and the global modes. When\nextracting temporal features from raw EEG signals, such a long\ntime series will signiﬁcantly increase the cost of model computation\nand lead to severe overﬁtting problems. Using the local transformer\nencoder can limit the size of the ﬁlter like the learning mode\nof a CNN layer. Although this will cause the transformer to\nlose the chance of obtaining global features of long sequences at\nonce, it can still leverage the advantage of dynamically extracting\nlearning feature relationships, complementing the CNN. When\nthe features are sent into the T-Dense block, the transformer\nencoder employs the global mode because the time series has been\nprocessed with the pooling layers. Meanwhile, the feed-forward\nfashion connecting each layer to every other layer in the CNN\nand transformer branch encourages feature reuse and information\nFrontiers in Neuroscience /zero.tnum/nine.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /five.tnum\nThe eﬀect of activation functions of all subjects in KU dataset.\nﬂow which improve the model performance. In the spatial block,\ncompared with previous models the proposed model used the\ndepthwise CNN layer to extract spatial features not only from\nall channels like ConvNet (\nSchirrmeister et al., 2017 ), EEGNet\n(Lawhern et al., 2018 ) and FBCNet ( Mane et al., 2020b ) which\nperformed well in the KU and BCIC-IV-2a dataset but also from\nthe diﬀerence of two hemispheres. The result of the ablation study\nhas shown the eﬃciency of this module. After extracting features\nfrom the hemisphere diﬀerences, the proposed model got higher\nclassiﬁcation results in all scenarios, especially in the cross-session\ncases.\nTo better validate the superiority of the proposed model, we\ndesigned three scenarios including within-session, cross-session,\nand two-session in two famous public datasets. From\nTables 1, 2,\nthe results show that our proposed model achieved the highest\nclassiﬁcation result in the diﬀerent scenarios. Compared with\nthe other two scenarios, the cross-session scenario is closer to\nthe real application which limits the model performance because\nof the number of data and the drift of statistical distributions.\nHowever, our proposed model still performed well and was less\nthan only 3% than within-session results which further shows the\ngood robustness and adaptability. Previous models based on the\ntransformer for MI classiﬁcation use the CNN layers (\nMa et al.,\n2022; Xie et al., 2022 ; Song et al., 2023 ) to extract temporal features\nwhile the transformer is used to reﬁne features. While the proposed\nmodel adopted the local mode of the transformer to complement\nthe functionality of CNN in time-series data analysis, rather than\nsimply placing the transformer behind the CNN layer. Meanwhile,\nduring the feature reﬁnement stage, the proposed model not only\nemployed the attention mechanism in the transformer but also\ncombined with the DenseNet to improve the ﬂow and reuse\nof information. Further, the spatial features learned from the\ndiﬀerence of the hemispheres were also taken into consideration.\nAlthough our proposed model has shown superior\nperformance than previous methods, there is still room for\nimprovement. First, the proposed model only adopts the\ntransformer encoder on the time series but ignores the possible\nTABLE /four.tnum Model complexity based on the number of trainable parameters.\nModels Parameters\nShallow ConvNet 42,884\nDeep ConvNet 282,004\nEEGNet 1,876\nTensor-CSPNet 232,360\nProposed Model 118,337\nspatial features extracted based on the self-attention mechanism.\nThe main reason we did not add this module is that the overall\nlength of the sequence is quite long after obtaining temporal\nfeatures. If we use a transformer to learn the correlation between\neach channel’s features and replace the deepwise convolutions,\nit will cause severe overﬁtting problems. Meanwhile, although\nthe complexity based on the trainable parameters of our model\nis not high, the computation time is still large because the\nlocal transformer slid to process inputs like CNNs which is\ntime-consuming. Also compared with other transformer-based\nmodels, the selection of the position encoding methods is\nnot considered in the proposed model. Thus, a future work\nwill investigate a more eﬃcient model structure. Secondly,\nself-attention can help reallocate the weights that present the\nimportance of each feature. In future works, investigations and\nvisualization of these weights to expand the interpretability and\nmechanism corresponding with the nerve disease are desirable\nto study.\n/five.tnum. Conclusion\nIn this article, we have presented a novel and eﬀective\napproach for MI-EEG classiﬁcation using a local and global\nconvolutional transformer-based model. The proposed model has\nbeen validated on the three scenarios and two public datasets.\nFrontiers in Neuroscience /one.tnum/zero.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nFIGURE /six.tnum\nThe feature map obtained by the proposed model in /two.tnum-D embedding based on t-SNE. (A–E) is the distribution of the extracted features of the third\nsubject from the BCIC-IV-/two.tnuma dataset.(F–J) show the distribution of extracted features of the third subje ct from the KU dataset.The combination of CNN ﬁlters and transformer encoders with\nlocal and global structures has the advantage of extracting a\ncomprehensive set of useful features from EEG signals. In the\nspatial module, we also consider the possible information from\nthe diﬀerences between the hemispheres which helps improve the\nrobustness of the model. Our results showed that the proposed\nmodel outperformed the state-of-the-art methods for MI-EEG\nclassiﬁcation on the KU dataset, achieving up to 0.99% and 1.46%\nfor the session 1 and 2 respectively in the within-session scenario,\nup to 7.49% and 8.19% for the case 1 and 2 respectively in\nthe cross-session scenario and up to 7.46% for the two-session\nscenario. For the BCIC-IV-2a dataset, the model can also improve\nthe classiﬁcation accuracy by 1.24% and 2.12% for the case 1\nand 2 in the cross-session scenario and 2.21% for the two-\nsession scenario.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: https://academic.oup.com/gigascience/article/8/\n5/giz002/5304369.\nEthics statement\nThe studies involving humans were approved by Korea\nUniversity Institutional Review Board (1040548-KUIRB-16-159-\nA-2). The studies were conducted in accordance with the\nlocal legislation and institutional requirements. Written informed\nconsent for participation was not required from the participants or\nthe participants’ legal guardians/next of kin in accordance with the\nnational legislation and institutional requirements.\nAuthor contributions\nJZ: conceptualization, investigation, methodology, software,\nand writing—original draft. KL: conceptualization, supervision,\nand modiﬁcation. BY: conceptualization and modiﬁcation. XH:\ninvestigation and methodology. All authors contributed to the\narticle and approved the submitted version.\nFunding\nThis work was supported by the National Key Research\nand Development Program of China (2022YFC3602700 and\n2022YFC3602703), Shanghai Major Science and Technology\nProject (No. 2021SHZDZX), and National Natural Science\nFoundation of China (No. 61976133).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers in Neuroscience /one.tnum/one.tnum frontiersin.org\nZhang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/one.tnum/nine.tnum/nine.tnum/eight.tnum/eight.tnum\nReferences\nAmin, S. U., Alsulaiman, M., Muhammad, G., Mekhtiche, M. A., and\nHossain, M. S. (2019). Deep learning for eeg motor imagery clas siﬁcation based\non multi-layer cnns feature fusion. Future Generat. Comp. Syst . 101, 542–554.\ndoi: 10.1016/j.future.2019.06.027\nAng, K. K., Chin, Z. Y., Zhang, H., and Guan, C. (2008). “Filter b ank common\nspatial pattern (fbcsp) in brain-computer interface, ” in 2008 IEEE International Joint\nConference on Neural Networks (IEEE World Congress on Computational In telligence).\nHong Kong: IEEE, 2390-2397.\nBagchi, S., and Bathula, D. R. (2022). Eeg-convtransformer f or single-\ntrial eeg-based visual stimulus classiﬁcation. Pattern Recognit . 129, 108757.\ndoi: 10.1016/j.patcog.2022.108757\nBarachant, A., Bonnet, S., Congedo, M., and Jutten, C. (2011 ). Multiclass brain-\ncomputer interface classiﬁcation by riemannian geometry. IEEE. Trans. Biomed. Eng .\n59, 920–928. doi: 10.1109/TBME.2011.2172210\nChen, C.-Y., Wu, C.-W., Lin, C.-T., and Chen, S.-A. (2014). “ A novel classiﬁcation\nmethod for motor imagery based on brain-computer interface, ” in 2014 International\nJoint Conference on Neural Networks (IJCNN) . Beijing: IEEE, 4099–4102.\nChen, Y., Kalantidis, Y., Li, J., Yan, S., and Feng, J. (2018). “Aˆ2-nets: Double\nattention networks, ” in 32nd Conference on Neural Information Processing Systems\n(Montreal, QC).\nCraik, A., He, Y., and Contreras-Vidal, J. L. (2019). Deep lear ning for\nelectroencephalogram (eeg) classiﬁcation tasks: a review. J. Neural Eng . 16, 031001.\ndoi: 10.1088/1741-2552/ab0ab5\nDai, G., Zhou, J., Huang, J., and Wang, N. (2020). Hs-cnn: a cnn with hybrid\nconvolution scale for eeg motor imagery classiﬁcation. J. Neural Eng . 17, 016025.\ndoi: 10.1088/1741-2552/ab405f\nd’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biro li, G., and Sagun,\nL. (2021). “Convit: Improving vision transformers with soft convolutional inductive\nbiases, ” in International Conference on Machine Learning . New York: PMLR, 2286-\n2296.\nDecety, J., and Ingvar, D. H. (1990). Brain structures partic ipating in mental\nsimulation of motor behavior: a neuropsychological interpreta tion. Acta Psychol . 73,\n13–34. doi: 10.1016/0001-6918(90)90056-L\nDu, Y., Xu, Y., Wang, X., Liu, L., and Ma, P. (2022). Eeg tempora l-\nspatial transformer for person identiﬁcation. Sci. Rep . 12, 14378.\ndoi: 10.1038/s41598-022-18502-3\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2 017). “Densely\nconnected convolutional networks, ” in 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (Honolulu, HI: IEEE), 2261–2269.\nHuang, Z., and Van Gool, L. (2017). “A riemannian network for spd matrix\nlearning, ” inThirty-First AAAI Conference on Artiﬁcial Intelligence , Vol. 31 (AAI Press).\ndoi: 10.1609/aaai.v31i1.10866\nJasper, H., and Penﬁeld, W. (1949). Electrocorticograms in man : eﬀect of voluntary\nmovement upon the electrical activity of the precentral gyrus. Archiv für Psychiatrie\nund Nervenkrankheiten 183, 163–174. doi: 10.1007/BF01062488\nJasper, H. H., and Andrews, H. L. (1938). Electro-encephalograph y: III normal\ndiﬀerentiation of occipital and precentral regions in man. AMA Arch. Neurol.\nPsychiatry 39, 96–115. doi: 10.1001/archneurpsyc.1938.022700101060 10\nJu, C., and Guan, C. (2022). “Tensor-cspnet: A novel geometric deep learning\nframework for motor imagery classiﬁcation, ” in IEEE Transactions on Neural Networks\nand Learning Systems (IEEE) . doi: 10.1109/TNNLS.2022.3172108\nKingma, D. P., and Ba, J. (2014). Adam: A method for stochasti c optimization. arXiv\n[Preprint]. arXiv: 1412.6980. Available online at: https://arxiv.org/pdf/1412.6980.pdf\nKwon, O.-Y., Lee, M.-H., Guan, C., and Lee, S.-W. (2019). Subj ect-independent\nbrain-computer interfaces based on deep convolutional neura l networks. IEEE Trans.\nNeural Netw. Learn. Syst . 31, 3839–3852. doi: 10.1109/TNNLS.2019.2946869\nLawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M., Hu ng, C. P., and\nLance, B. J. (2018). Eegnet: a compact convolutional neural ne twork for eeg-based\nbrain-computer interfaces. J. Neural Eng . 15, 056013. doi: 10.1088/1741-2552/aace8c\nLee, M.-H., Kwon, O.-Y., Kim, Y.-J., Kim, H.-K., Lee, Y.-E., Williamson, J., et al.\n(2019). Eeg dataset and openbmi toolbox for three bci paradigms : an investigation into\nBCI illiteracy. GigaScience 8, giz002. doi: 10.1093/gigascience/giz002\nLemm, S., Blankertz, B., Curio, G., and Muller, K.-R. (2005). Spat io-spectral ﬁlters\nfor improving the classiﬁcation of single trial eeg. IEEE. Trans. Biomed. Eng . 52,\n1541–1548. doi: 10.1109/TBME.2005.851521\nLi, C., Zhang, Z., Zhang, X., Huang, G., Liu, Y., and Chen, X.\n(2022). Eeg-based emotion recognition via transformer neu ral architecture\nsearch. IEEE Trans. Ind. Inform . 19, 6016–6025. doi: 10.1109/TII.2022.\n3170422\nLi, S., Zhou, W., Yuan, Q., Geng, S., and Cai, D. (2013). Featur e extraction\nand recognition of ictal eeg using emd and svm. Comput. Biol. Med . 43, 807–816.\ndoi: 10.1016/j.compbiomed.2013.04.002\nLotte, F., Congedo, M., Lécuyer, A., Lamarche, F., and Arnald i, B. (2007). A review\nof classiﬁcation algorithms for eeg-based brain-computer int erfaces. J. Neural Eng . 4,\nR1. doi: 10.1088/1741-2560/4/2/R01\nMa, Y., Song, Y., and Gao, F. (2022). “A novel hybrid cnn-tran sformer model for\neeg motor imagery classiﬁcation, ” in 2022 International Joint Conference on Neural\nNetworks (IJCNN). Padua: IEEE, 1–8.\nMane, R., Chouhan, T., and Guan, C. (2020a). Bci for stroke reh abilitation: motor\nand beyond. J. Neural Eng . 17, 041001. doi: 10.1088/1741-2552/aba162\nMane, R., Robinson, N., Vinod, A. P., Lee, S.-W., and Guan, C. ( 2020b). “A multi-\nview cnn with novel variance layer for motor imagery brain com puter interface, ” in\n2020 42nd Annual International Conference of the IEEE Engineering in Med icine &\nBiology Society (EMBC) . Montreal, QC: IEEE, 2950–2953.\nMartin, M., Nitschke, K., Beume, L., Dressing, A., Bühler, L. E., Ludwig, V. M.,\net al. (2016). Brain activity underlying tool-related and imita tive skills after major left\nhemisphere stroke. Brain 139, 1497–1516. doi: 10.1093/brain/aww035\nMüller, R.-A., Rothermel, R. D., Behen, M. E., Muzik, O., Mangne r, T. J., and\nChugani, H. T. (1998). Diﬀerential patterns of language and mo tor reorganization\nfollowing early left hemisphere lesion: a pet study. Arch. Neurol . 55, 1113–1119.\ndoi: 10.1001/archneur.55.8.1113\nNovi, Q., Guan, C., Dat, T. H., and Xue, P. (2007). “Sub-band co mmon spatial\npattern (sbcsp) for brain-computer interface, ” in 2007 3rd International IEEE/EMBS\nConference on Neural Engineering . Kohala Coast, HI: IEEE, 204–207.\nPfurtscheller, G., and Neuper, C. (2001). Motor imagery and dire ct brain-computer\ncommunication. Proc. IEEE 89, 1123–1134. doi: 10.1109/5.939829\nPu, X., Yi, P., Chen, K., Ma, Z., Zhao, D., and Ren, Y. (2022). E egdnet: Fusing non-\nlocal and local self-similarity for eeg signal denoising with tr ansformer. Comput. Biol.\nMed. 151, 106248. doi: 10.1016/j.compbiomed.2022.106248\nSchirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J. , Glasstetter, M.,\nEggensperger, K., Tangermann, M., et al. (2017). Deep learning with convolutional\nneural networks for eeg decoding and visualization. Hum. Brain Mapp . 38, 5391–5420.\ndoi: 10.1002/hbm.23730\nSimoncelli, E. P., and Olshausen, B. A. (2001). Natural image\nstatistics and neural representation. Annu. Rev. Neurosci . 24:1193–1216.\ndoi: 10.1146/annurev.neuro.24.1.1193\nSong, Y., Zheng, Q., Liu, B., and Gao, X. (2023). “EEG conform er: Convolutional\ntransformer for EEG decoding and visualization, ” in IEEE Transactions on\nNeural Systems and Rehabilitation Engineering, Vol. 31 (IEEE) , 710–719.\ndoi: 10.1109/TNSRE.2022.3230250\nTangermann, M., Müller, K.-R., Aertsen, A., Birbaumer, N., Br aun, C., Brunner,\nC., et al. (2012). Review of the bci competition iv. Front. Neurosci . 6, 55.\ndoi: 10.3389/fnins.2012.00055\nTao, Y., Sun, T., Muhamed, A., Genc, S., Jackson, D., Arsanjan i, A., et al. (2021).\n“Gated transformer for decoding human brain eeg signals, ” in 2021 43rd Annual\nInternational Conference of the IEEE Engineering in Medicine & Biology Society\n(EMBC). Mexico: IEEE, 125–130.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in 31st Conference on Neural Information Processing\nSystems (Long Beach, CA).\nWu, H., Niu, Y., Li, F., Li, Y., Fu, B., Shi, G., et al. (2019). A par allel multiscale\nﬁlter bank convolutional neural networks for motor imagery ee g classiﬁcation. Front.\nNeurosci. 13, 1275. doi: 10.3389/fnins.2019.01275\nXie, J., Zhang, J., Sun, J., Ma, Z., Qin, L., Li, G., et al. (2022) . A transformer-\nbased approach combining deep learning network and spatial-tempo ral information\nfor raw eeg classiﬁcation. IEEE Trans. Neural Syst. Rehabilitation Eng . 30, 2126–2136.\ndoi: 10.1109/TNSRE.2022.3194600\nZancanaro, A., Zoppis, I., Manzoni, S., and Cisotto, G. (2023). “veegnet: A new deep\nlearning model to classify and generate eeg, ” in Proceedings of the 9th International\nConference on Information and Communication Technologies for Ageing Wel l and e-\nHealth, ICT4AWE 2023, Prague, Czech Republic, April 22-24, 2023 . Setúbal: Science\nand Technology Publications, 245–252.\nZhang, J., and Li, K. (2022). “A pruned deep learning approach for classiﬁcation\nof motor imagery electroencephalography signals, ” in 2022 44th Annual International\nConference of the IEEE Engineering in Medicine & Biology Society (EMBC) . Glasgow:\nIEEE, 4072–4075.\nZhang, R., Zong, Q., Dou, L., and Zhao, X. (2019). A novel hybr id deep\nlearning scheme for four-class motor imagery classiﬁcation. J. Neural Eng . 16, 066004.\ndoi: 10.1088/1741-2552/ab3471\nFrontiers in Neuroscience /one.tnum/two.tnum frontiersin.org"
}