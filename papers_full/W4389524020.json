{
  "title": "Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting",
  "url": "https://openalex.org/W4389524020",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2725974942",
      "name": "Nikolay Bogoychev",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2158079484",
      "name": "PinZhen Chen",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951770285",
    "https://openalex.org/W4306834317",
    "https://openalex.org/W3120124891",
    "https://openalex.org/W2949830548",
    "https://openalex.org/W4286982966",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W2962714778",
    "https://openalex.org/W4318903120",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W4286970574",
    "https://openalex.org/W4379933104",
    "https://openalex.org/W2970758702",
    "https://openalex.org/W2978264350",
    "https://openalex.org/W4321177597",
    "https://openalex.org/W4287373797",
    "https://openalex.org/W3153567180",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W3035308166"
  ],
  "abstract": "Terminology correctness is important in the downstream application of machine translation, and a prevalent way to ensure this is to inject terminology constraints into a translation system. In our submission to the WMT 2023 terminology translation task, we adopt a translate-then-refine approach which can be domain-independent and requires minimal manual efforts. We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model. Further, we explore two post-processing methods. First, we use an alignment process to discover whether a terminology constraint has been violated, and if so, we re-decode with the violating word negatively constrained. Alternatively, we leverage a large language model to refine a hypothesis by providing it with terminology constraints. Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 890–896\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n890\nTerminology-Aware Translation with Constrained Decoding\nand Large Language Model Prompting\nNikolay Bogoychev* Pinzhen Chen*\nSchool of Informatics, University of Edinburgh\nn.bogoych@ed.ac.uk, pinzhen.chen@ed.ac.uk\nAbstract\nTerminology correctness is important in the\ndownstream application of machine translation,\nand a prevalent way to ensure this is to inject\nterminology constraints into a translation sys-\ntem. In our submission to the WMT 2023 ter-\nminology translation task, we adopt a translate-\nthen-refine approach which can be domain-\nindependent and requires minimal manual ef-\nforts. We annotate random source words with\npseudo-terminology translations obtained from\nword alignment to first train a terminology-\naware model. Further, we explore two post-\nprocessing methods. First, we use an align-\nment process to discover whether a terminol-\nogy constraint has been violated, and if so, we\nre-decode with the violating word negatively\nconstrained. Alternatively, we leverage a large\nlanguage model to refine a hypothesis by pro-\nviding it with terminology constraints. Results\nshow that our terminology-aware model learns\nto incorporate terminologies effectively, and\nthe large language model refinement process\ncan further improve terminology recall.\n1 Introduction\nOne of the major obstacles encountered by neural\nmachine translation (NMT) systems pertains to the\nutilization of suitable domain-related words when\ntranslating specialized content not present in the\ntraining data. An illustrative instance of this chal-\nlenge arises when translating “transformer” from\nEnglish into another language, where the accurate\ntranslation depends on the context or the preference\nof the audience (Figure 1). A straightforward lit-\neral translation approach often leads to suboptimal\noutcomes, prompting human translators unfamiliar\nwith domain-specific knowledge to resort to ref-\nerence materials for terminology precision. This\nissue is prevalent in the translation industry, with\nmany commercial translation service providers of-\nfering paid solutions to address it. Furthermore, it\n*Equal contribution.\nTranslate \"transformer\" to Chinese?\n变压器  (electric transformer)\n变形金刚  (the Transformer character)\n变换器  (something that changes)\nFigure 1: Terminology hints can help disambiguate pol-\nysemantic words when translating with limited context.\nis a popular area in machine translation research,\nindicated by efforts such as WMT shared tasks or-\nganization and participation focusing on terminol-\nogy and domain-specific translations (Alam et al.,\n2021; Bawden et al., 2019, 2020, inter alia).\nThis year’s WMT terminology translation task\nfeatures three language directions: German-to-\nEnglish, Chinese-to-English, and English-to-Czech.\nIn addition to reading in a source sentence, partic-\nipating systems need to employ a provided dic-\ntionary, which contains source-target terminology\nword mappings, to incorporate into the target trans-\nlation. For each source sentence in the test set,\nthere are three modes of applying terminology con-\nstraints:\n1. Terminology constraint: Dictionaries of real\nterminology words are provided, to be incor-\nporated in the translations.\n2. Random constraint: Random (but presumably\ncorrect) word mappings are obtained using a\nword alignment tool and provided as a pseudo-\nterminology dictionary.\n3. No constraint: Source sentences can be freely\ntranslated without external information.\nWe interpret that the no-constraint setting al-\nlows us to measure the competing systems’ quality\nand understand to what degree the systems effec-\ntively utilize the provided random and terminol-\nogy dictionaries. Our baseline approach is to train\n891\na terminology-aware translation (TAT) system in-\nspired by Dinu et al. (2019), where, in the training\ndata, source words are tagged with desired transla-\ntions inline on the source side. Then we propose\ntwo separate refinement strategies on top of it to\naggressively encourage the appearance of termi-\nnologies:\n1. We use a neural word aligner to identify ter-\nminology constraints missed by the baseline\nsystem, and use the same system to re-decode\nthe source by negatively constraining (disal-\nlowing) previously incorrectly translated to-\nkens.\n2. We also investigate the capability of a large\nlanguage model to simultaneously paraphrase\nan existing translation to include the desired\nterminology constraints via curated prompts.\nOur proposed techniques can incorporate target\nterminology words with around 80% recall, using\nautomatic and soft constraints in a two-step refine-\nment process. We observe that for German-English,\nour terminology-aware training and negatively con-\nstrained decoding perform better, whereas, for\nChinese-English and English-Czech, LLM-based\nrefinement achieves higher scores. In terms of over-\nall translation accuracy, we find that negatively\nconstrained decoding could lead to a tiny drop and\nLLMs are able to maintain or improve quality ac-\ncording to a reference-free neural metric.\n2 Related Work\nPrevious research on terminology translation could\nbe divided into two categories: soft constraint and\nhard constraint, depending on whether the resulting\ntranslation system will enforce the appearance of\ndesired target translations. In the soft constraint set-\nting, the convention is to train a model that is able to\ningest the target terminology words inline, directly\nplacing them after the corresponding source words\nin the source input (Dinu et al., 2019). Many later\nimplementations stem from this to include new ele-\nments such as additional lemmatization (Bergmanis\nand Pinnis, 2021) or grammatical error correction\n(Pham et al., 2021) as a post-processing step in\norder to achieve a more fluent output. Instead of\nplacing the target constraint words inline, some\nother works train a system that takes the terminol-\nogy constraint as either a prefix or a suffix (Jon\net al., 2021; Turcan et al., 2022).\nMost hard constraint work involves post-\nprocessing a translation with desired terminologies.\nPost et al. (2019) inserted untranslatable tokens\n(also known as placeholders) into the source, which\nwill remain unchanged through the translation pro-\ncess. Then the placeholders are replaced with ter-\nminology words in the target language. This is\nentirely performed as a post-processing step. Such\nterminology replacement could also be done by\nkeeping and replacing the source word at inference\ntime, and it is also feasible to run target word re-\nplacement as post-processing (Molchanov et al.,\n2021). A hard constraint method guarantees that\nthe chosen terminology token will appear, but often\nresults in less fluent output, especially for morpho-\nlogically rich languages because the context is not\ntaken into consideration during replacement. It also\nmandates more complicated post-processing than\nthe soft constraint approaches.\nOur first post-processing proposal relies on con-\nstrained decoding, which refers to either allowing\ncertain tokens or blocking specific tokens during\ninference time (Hokamp and Liu, 2017). It has\nbeen applied to terminology injection, paraphras-\ning, parallel sentence mining, etc (Hasler et al.,\n2018; Kajiwara, 2019; Chen et al., 2020). We opt\nfor negatively constraining the tokens that violated\nthe given terminology alignments by preventing\nthem from entering the hypothesis beam in the re-\nfinement stage. These alignments are computed\nusing word alignment tools (Dyer et al., 2013; Dou\nand Neubig, 2021).\nAnother post-processing method in our study\nprompts an LLM to refine a translation and incorpo-\nrate terminology terms simultaneously. Whilst pre-\nvious studies have explored the translation capabil-\nity of LLMs (Vilar et al., 2023; Zhang et al., 2023),\nthe works closely relevant to us are from Moslem\net al. (2023) and Ghazvininejad et al. (2023). We\nadopt the paradigm from the latter, which re-words\na constraint dictionary as a natural text and affixes\nit into a translation prompt. While they focused on\nrare words without directly benchmarking on ter-\nminology translation, our post-processing step can\nbe seen as an extension of word-level controlled\nprompting to terminology translation with large lan-\nguage models. Both of our post-processing meth-\nods should be categorized as soft constraint ap-\nproaches since there is no guarantee that negatively\nconstrained decoding or an LLM will necessarily\nincorporate the constraints in a re-generation.\n892\n3 Terminology-Aware Training\nThe goal of our system implementation is to create\na general-purpose terminology-aware translation\nsystem that is unsupervised and domain-agnostic,\nand requires the minimum effort of pre- and post-\nprocessing.\n3.1 Terminology creation\nInspired by Dinu et al. (2019), we applied terminol-\nogy constraints during training, but a key difference\nis that, unlike their approach, we assume that we\nhave no access to downstream domain or terminol-\nogy constraints during training, in order to build\na general-purpose domain-agnostic system. Con-\nsequently, we have no curated terminology data to\nuse. Therefore, we generate (pseudo-)terminology\ninformation using word alignments. Our workflow\ncan be detailed as:\n1. We compute the word alignment information\nfor the entire training set using fast_align\n(Dyer et al., 2013).\n2. For each sentence, we select all bijective\nsource-target mappings as our terminology\ncandidates. We also filter out trivial mappings\nwhere the source and target tokens are the\nsame (e.g. numbers, names), because those\nmappings are simple and hence likely to be\ncorrectly translated by a translation system\neven without any terminology awareness.\n3. In the training data, we replace srcwordi in\nthe source sentence with:\nsrcwordi __target__ trgwordj __done__\nwhere the srcwordi is the i-th source word in-\nside the sentence, and trgwordj is the word\ninside the target sentence, corresponding to\nsrcwordi according to word alignment infor-\nmation. This replacement occurs with around\n10% probability for each candidate source-\ntarget pair. For a sentence that does not have\nan associated terminology constraint, the data\nis the same as normal NMT.\n4. At inference time, we process the test data sim-\nilarly to above, except that the source-target\nword mapping comes from a supplied termi-\nnology dictionary.\nIn practice, our translation system is trained with\na mix of normal translation data and terminology-\ninjected data. The advantage of this strategy is that\nthe trained models are general-purpose, so they can\ntranslate normal texts without terminology injec-\ntion. Further, they have been exposed to a wide\nvariety of constraints during training, making them\nrobust to potentially unseen domain constraints.\nOverall, our method is very similar to Bergmanis\nand Pinnis (2021)’s work, except that we use whole\nwords but not lemmas to ease pre-processing. We\npresume that the language model will be able to\nadjust the terminologies accordingly, especially for\nmorphologically rich languages on the target side.\nThis enables our method to be trivially transferable\nacross languages.\nFinally, our systems could easily be turned into\nhard-constrained by replacing the source word with\nthe desired target terminology word. This could\nbe feasible because our terminology-aware training\ninstalls the copying behaviour in the neural transla-\ntion model, although in this mode the model would\nproduce markedly less fluent output.\n3.2 Model architecture\nWe trained Transformer-style machine translation\nmodels (Vaswani et al., 2017) using the Marian\nNMT toolkit (Junczys-Dowmunt et al., 2018). We\nused the Transformer-Bigpreset which is a 6 en-\ncoder, 6 decoder architecture with 1024 hidden size,\nand 4096 feedforward size.1\n3.3 Data\nThe terminology task uses the same data as the\nconstrained condition in the WMT23 general trans-\nlation task. We carefully cleaned, filtered, and de-\nduplicated the available WMT training sets pro-\nvided by the organisers, as well as the available\nback-translation data. After preprocessing we were\nleft with the following:\n• German-to-English ( de-en): 199M lines\nof parallel data and 29.5M lines of back-\ntranslated data.\n• Chinese-to-English ( zh-en): 21.8M lines\nof parallel data and 15.6M lines of back-\ntranslated data.\n• Czech-to-English (cs-en): 61.8M lines of par-\nallel data and 57M lines of back-translated\ndata.\n1https://github.com/marian-nmt/marian/blob/\nmaster/src/common/aliases.cpp#L114\n893\nQuery Prompt template\nTranslation Source: ${source}\nPlease give me a translation in ${lang} without any explanation.\nRefinement\nSource: ${source}\nTranslation: ${translation}\nPlease give me a better ${lang} translation without any explanation.\n“${srcword0}” should be translated as “${trgword0}”;\n“${srcword1}” should be translated as “${trgword1}”;\n...\n“${srcwordk}” should be translated as “${trgwordk}”. (with k >= 0)\nTable 1: Large language model prompt templates for unconstrained and constrained translation.\n3.4 General quality\nThe quality of our models without terminology\ntranslation is shown in Table 2, where we report\nBLEU (Papineni et al., 2002) and COMETDA2 (Rei\net al., 2020) scores on test sets from the WMT22\ngeneral translation task. We note that terminology\naugmentation during training could result in a slight\nquality drop.\nBLEU COMET DA\nde-en 31.3 0.8334\nen-cs 39.5 0.8715\nzh-en 20.3 0.7559\nTable 2: Performance of our terminology-aware transla-\ntion systems in the WMT22 general translation task.\n4 Post-Translation Terminology Injection\nDespite training our model with terminology aware-\nness, there is no mechanism to ensure that the de-\nsired terminology constraint will appear on the tar-\nget side. The neural network decoding behaviour\nis not entirely predictable, especially given the as-\nsumption of no additional domain adaptation. Be-\nlow, we present two distinct strategies to tryharder\nto promote the terminology constraints, via auto-\nmatic post-editing through constrained beam search\nand large language models.\n4.1 Negatively constrained decoding\nWhile it is easy enough to notice when a target\nterminology term is not generated as per a given\nconstraint, it is not trivial to understand which word\n2wmt22-comet-da. This is a reference-based metric which\nrequires the source input, hypothesis, and reference.\nhas been produced in place of the desired term. In\norder to do this, we make use of awesome-align, a\nneural multilingual word aligner (Dou and Neubig,\n2021), with the following procedure:\n1. For each source-translation pair, we check if\nall required terminology terms appear on the\ntarget side. If they do, then we stop processing\nmore rules.\n2. Then, we use awesome-align to compute word\nalignments and detect the word(s) that have\nbeen generated in place of the desired terms\naccording to the provided terminology con-\nstraints.\n3. We decode the source sentence again, penal-\nising the words that violated the terminology\nconstraint, by forbidding the decoder from\ngenerating them at each generation step, un-\nless they carry more than 95% of the probabil-\nity mass at a certain step.\nIn practice, this procedure can be repeated in-\nfinitely, until all terminology constraints are ful-\nfilled, but we decided to limit it to only one itera-\ntion, to keep this a realistic production scenario in\nterms of computational budget.\n4.2 Large language models\nRecent years saw the rise of large language models\n(LLMs), which have a strong capability in various\nNLP tasks. In this paper, we investigate the effec-\ntiveness of using a large language model to gener-\nate terminology terms during translation by adding\nconstraints to Chen et al. (2023)’s translation re-\nfinement prompts. We use two distinct prompts:\nfree translation and translation refinement queries.\nThe translation query sends a source sentence and\n894\nMode Model Refine de→en zh →en en →cs\nRecall COMET QE Recall COMET QE Recall COMET QE\nterminology\nconstraints\nTAT - 82.30 .0797 49.98 -.0896 73.75 .0601\nTAT NCD 82.01 .0775 50.42 -.0903 73.26 .0588\nTAT LLM 64.35 .1197 83.06 .0185 76.00 .0866\nLLM - 41.86 .1244 46.63 .0191 48.14 .0913\nLLM LLM 70.48 .1180 81.01 .0201 78.94 .0882\nno\nconstraint†\nTAT - 39.82 .1085 13.64 -.1163 48.11 .0712\nTAT LLM 39.59 .1251 42.76 .0203 47.31 .0955\nLLM - 41.86 .1244 46.63 .0191 48.14 .0913\nLLM LLM 39.65 .1258 46.72 .0228 46.22 .0943\nrandom\nconstraints\nTAT - 76.17 .0716 81.55 -.1105 57.10 .0502\nTAT NCD 75.79 .0698 82.03 -.1123 56.42 .0465\nTAT LLM 61.46 .1206 63.17 .0175 70.97 .0875\nLLM - 38.70 .1244 52.49 .0191 39.34 .0913\nLLM LLM 66.74 .1188 67.10 .0196 73.37 .0867\nno\nconstraint‡\nTAT - 35.60 .1085 36.18 -.1163 37.35 .0712\nTAT LLM 37.58 .1251 49.48 .0203 39.03 .0955\nLLM - 38.70 .1244 52.49 .0191 39.34 .0913\nLLM LLM 37.62 .1258 49.00 .0228 38.42 .0943\n†Recall computed against terminology constraints.\n‡Recall computed against random constraints.\nTable 3: Terminology recall and translation quality measured by COMETQE of our systems on the blind testset.\nTAT: terminology-aware translation; NCD: negatively constrained decoding; LLM: large language model.\nrequests a translation in the target language with-\nout any other information. On the other hand, the\nrefinement query feeds back an unconstrained trans-\nlation together with terminology constraints to re-\nquest a new translation. This essentially forms\nan LLM version of the constrained beam search\ndiscussed in Section 4.1. The constraints are en-\nforced through natural language instructions in the\nprompts, under the situation where the softmax dis-\ntribution from an LLM is not accessible by users.\nThe LLM we use is OpenAI’s GPT-3.5.3 It is a\nclosed-source commercial system, where the model\nweights and the inference states are not available\nto users. The model has a context window of 4096\nwhich is sufficient to cover an instruction, a source\nsentence, several terminology constraints, as well\nas the target translation. It is public to all users\nat a relatively cheap cost. In our settings, each\ntranslation is carried out in a new query session.\nIn Table 1 we outline the two prompt templates\nwe used. During querying, the placeholder vari-\nables are substituted with corresponding string val-\n3gpt-3.5-turbo-0613, a snapshot of the GPT-3.5 model\non 13 June 2023\nues. For the refinement query, when a terminology\ndictionary is supplied, the source and target words\nare fed to the LLM via the prompt (Ghazvininejad\net al., 2023); if there is no terminology dictionary,\nthe query simply asks for a refined translation. The\ntwo-step experiment with LLMs can be summa-\nrized as follows:\n1. We obtain an initial unconstrained translation,\nwhich may or may not fulfil all the terminol-\nogy constraints. It can come from either the\nLLM itself or the terminology-aware transla-\ntion model built in Section 3.1.\n2. We query the LLM with the constrained trans-\nlation prompt to obtain a refined translation\nwith terminology incorporated in the prompt.\n5 Results and Discussions\nWe present our blind testresults in Table 3, which\ninclude both terminology recall and COMET QE\nscores computed by us. 4 We used COMETQE in\nparticular because it does not require references\n4wmt21-comet-da-qe\n895\nwhich are not accessible to us. We assess the ef-\nfectiveness of our methods by comparing the ter-\nminology recall of our systems with and without\napplying terminology constraints, in both random\nand real terminologyscenarios.\n5.1 Translation quality\nIn terms of translation quality reflected in\nCOMETQE, we observe that the LLM rows attain\nsuperior results, which is not surprising considering\nthat we use an unconstrained commercial model\nGPT-3.5. By comparing TAT with TAT+NCD, or\ncomparing LLM with LLM+LLM under a con-\nstrained scenario, we conclude that applying ter-\nminology constraints usually lead to a sacrifice in\ntranslation quality regardless of the language direc-\ntion or the systems involved. Nonetheless, as a con-\ntrasting experiment with no constraint, LLM+LLM\nachieves a slightly better COMET QE score than\nusing an LLM to translate without refinement.\nOur model performed poorly on the zh-en task\nin terms of COMET QE scores. We suspect that\nthis is because of the domain mismatch between\nthe translation data from the general domain and\nthe Chinese terminology test set. Upon manual\ninspection, we found that the latter includes web\nnovels and literal writing which are likely to be\nunder-represented in the generic training data.\n5.2 Terminology recall\nFocusing on terminology generation, compared\nwith TAT or LLM in unconstrained settings, TAT\nmarks 30-40 higher recall of terminology terms in\nthe constrained terminology and random settings.\nThis indicates that our terminology-aware training\nis effective in teaching translation models to follow\ncustomized source-target word alignments.\nNext, as a post-processing step, negatively con-\nstrained decoding seems to be disappointing in\npractice. TAT+NCD often produces worse results\nthan TAT alone in terms of both quality and ter-\nminology recall, except for zh-en with random\nconstraints. We hypothesize that this could be due\nto two problems: (1) word alignment errors could\npropagate into this process, and (2) by applying\nNCD, we might capture a missed terminology term\nbut at the cost of mis-translating other words. Our\nconstraining procedure might be improved by per-\nforming shortlisting, namely positively constrained\ndecoding, as opposed to negatively limiting the\nbeam search in an iterative approach.\nWe find the results promising when using LLMs\nfor terminology injection. Looking at LLM+LLM\nversus LLM alone in various constrained condi-\ntions, terminology recall improves significantly\nwith very little drop in overall quality. Also by\ncomparing TAT+LLM with TAT alone, we observe\nthat TAT and LLMs each have their own merits de-\npending on the language direction. In terms of re-\ncall, TAT wins inde-en, TAT+LLM wins inzh-en,\nand they are close in en-cs. However, TAT+LLM\nis way ahead if measured by COMET QE. How-\never, we must note that an LLM costs significantly\nmore resources than a dedicated translation model\nat both training and inference time.\n6 Conclusion and Future Work\nWe participated in all tracks of the WMT 2023 ter-\nminology shared task with a terminology-aware\ntranslation baseline, and two distinct refinement\nprocedures using negatively constrained beam\nsearch and large language models separately. The\nresults we produced gave us insights into the pros\nand cons of our systems. In future work, we could\nexplicitly enforce the generation of the terminol-\nogy token by identifying the appropriate time step\nand manipulating the probability distribution after\nsoftmax computation, even in an open-source large\nlanguage model. This is not entirely trivial due to\nthe presence of subwords but could be achievable.\nAcknowledgement\nThis project has received funding from UK Re-\nsearch and Innovation (UKRI) under the UK\ngovernment’s Horizon Europe funding guarantee\n[grant numbers 10052546 and 10039436].\nSome computations described in this re-\nsearch were performed using the Baskerville Tier\n2 HPC service (https://www.baskerville.ac.uk/).\nBaskerville was funded by the EPSRC and\nUKRI through the World Class Labs scheme\n(EP/T022221/1) and the Digital Research Infras-\ntructure programme (EP/W032244/1) and is op-\nerated by Advanced Research Computing at the\nUniversity of Birmingham.\nReferences\nMd Mahfuz Ibn Alam, Ivana Kvapilíková, Antonios\nAnastasopoulos, Laurent Besacier, Georgiana Dinu,\nMarcello Federico, Matthias Gallé, Kweonwoo Jung,\nPhilipp Koehn, and Vassilina Nikoulina. 2021. Find-\n896\nings of the WMT shared task on machine translation\nusing terminologies. In Proceedings of WMT.\nRachel Bawden, Kevin Bretonnel Cohen, Cristian\nGrozea, Antonio Jimeno Yepes, Madeleine Kittner,\nMartin Krallinger, Nancy Mah, Aurelie Neveol, Mar-\niana Neves, Felipe Soares, Amy Siu, Karin Verspoor,\nand Maika Vicente Navarro. 2019. Findings of the\nWMT 2019 biomedical translation shared task: Eval-\nuation for MEDLINE abstracts and biomedical ter-\nminologies. In Proceedings of WMT.\nRachel Bawden, Giorgio Maria Di Nunzio, Cris-\ntian Grozea, Inigo Jauregi Unanue, Antonio Ji-\nmeno Yepes, Nancy Mah, David Martinez, Aurélie\nNévéol, Mariana Neves, Maite Oronoz, Olatz Perez-\nde Viñaspre, Massimo Piccardi, Roland Roller, Amy\nSiu, Philippe Thomas, Federica Vezzani, Maika Vi-\ncente Navarro, Dina Wiemann, and Lana Yeganova.\n2020. Findings of the WMT 2020 biomedical transla-\ntion shared task: Basque, Italian and Russian as new\nadditional languages. In Proceedings of WMT.\nToms Bergmanis and M¯arcis Pinnis. 2021. Facilitating\nterminology translation with target lemma annota-\ntions. In Proceedings of EACL.\nPinzhen Chen, Nikolay Bogoychev, Kenneth Heafield,\nand Faheem Kirefu. 2020. Parallel sentence mining\nby constrained decoding. In Proceedings of ACL.\nPinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-\nneth Heafield. 2023. Iterative translation refinement\nwith large language models. arXiv preprint.\nGeorgiana Dinu, Prashant Mathur, Marcello Federico,\nand Yaser Al-Onaizan. 2019. Training neural ma-\nchine translation to apply terminology constraints. In\nProceedings of ACL.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment\nby fine-tuning embeddings on parallel corpora. In\nProceedings of EACL.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of IBM model 2. In Proceedings of NAACL-\nHLT.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based phrase-level prompt-\ning of large language models for machine translation.\narXiv preprint.\nEva Hasler, Adrià de Gispert, Gonzalo Iglesias, and\nBill Byrne. 2018. Neural machine translation decod-\ning with terminology constraints. In Proceedings of\nNAACL-HLT.\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of ACL.\nJosef Jon, Michal Novák, João Paulo Aires, Dusan Varis,\nand Ondˇrej Bojar. 2021. CUNI systems for WMT21:\nTerminology translation shared task. In Proceedings\nof WMT.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heafield,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings\nof ACL.\nTomoyuki Kajiwara. 2019. Negative lexically con-\nstrained decoding for paraphrase generation. In Pro-\nceedings of ACL.\nAlexander Molchanov, Vladislav Kovalenko, and Fedor\nBykov. 2021. PROMT systems for WMT21 termi-\nnology translation task. In Proceedings of WMT.\nYasmin Moslem, Rejwanul Haque, John D. Kelleher,\nand Andy Way. 2023. Adaptive machine transla-\ntion with large language models. In Proceedings of\nEAMT.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of ACL.\nMinh Quang Pham, Josep Crego, Antoine Senellart,\nDan Berrebbi, and Jean Senellart. 2021. SYSTRAN\n@ WMT 2021: Terminology task. In Proceedings of\nWMT.\nMatt Post, Shuoyang Ding, Marianna Martindale, and\nWinston Wu. 2019. An exploration of placeholding\nin neural machine translation. In Proceedings of MT\nSummit.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of EMNLP.\nElsbeth Turcan, David Wan, Faisal Ladhak, Petra Galus-\ncakova, Sukanta Sen, Svetlana Tchistiakova, Wei-\njia Xu, Marine Carpuat, Kenneth Heafield, Douglas\nOard, and Kathleen McKeown. 2022. Constrained re-\ngeneration for cross-lingual query-focused extractive\nsummarization. In Proceedings of COLING.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2023. Prompt-\ning PaLM for translation: Assessing strategies and\nperformance. In Proceedings of ACL.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study. In Proceedings of ICML.",
  "topic": "Terminology",
  "concepts": [
    {
      "name": "Terminology",
      "score": 0.9547818303108215
    },
    {
      "name": "Computer science",
      "score": 0.8673772215843201
    },
    {
      "name": "Natural language processing",
      "score": 0.6943795680999756
    },
    {
      "name": "Correctness",
      "score": 0.688994288444519
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5770503878593445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5711178183555603
    },
    {
      "name": "Machine translation",
      "score": 0.5384483337402344
    },
    {
      "name": "Process (computing)",
      "score": 0.500654935836792
    },
    {
      "name": "Word (group theory)",
      "score": 0.43371278047561646
    },
    {
      "name": "Programming language",
      "score": 0.22725340723991394
    },
    {
      "name": "Linguistics",
      "score": 0.17597824335098267
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ]
}