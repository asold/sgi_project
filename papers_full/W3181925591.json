{
  "title": "Local-to-Global Self-Attention in Vision Transformers",
  "url": "https://openalex.org/W3181925591",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1612430329",
      "name": "Li Jinpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355341808",
      "name": "Yan, Yichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2492744646",
      "name": "Liao Shengcai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1798755108",
      "name": "Yang, Xiaokang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101740539",
      "name": "Shao, Ling",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2991062542",
    "https://openalex.org/W2964444661",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2936599103",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W2125556102",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2895126795",
    "https://openalex.org/W2490270993",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2964081403",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2963741402",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Transformers have demonstrated great potential in computer vision tasks. To avoid dense computations of self-attentions in high-resolution visual data, some recent Transformer models adopt a hierarchical design, where self-attentions are only computed within local windows. This design significantly improves the efficiency but lacks global feature reasoning in early stages. In this work, we design a multi-path structure of the Transformer, which enables local-to-global reasoning at multiple granularities in each stage. The proposed framework is computationally efficient and highly effective. With a marginal increasement in computational overhead, our model achieves notable improvements in both image classification and semantic segmentation. Code is available at https://github.com/ljpadam/LG-Transformer",
  "full_text": "Local-to-Global Self-Attention in Vision\nTransformers\nJinpeng Li∗\nInception Institute of\nArtiﬁcial Intelligence (IIAI),\nAbu Dhabi, UAE\nljpadam@gmail.com\nYichao Yan∗†\nMoE Key Lab of Artiﬁcial Intelligence,\nAI Institute,\nShanghai Jiao Tong University, China\nyanyichao91@gmail.com\nShengcai Liao†\nInception Institute of\nArtiﬁcial Intelligence (IIAI),\nAbu Dhabi, UAE\nscliao@ieee.org\nXiaokang Yang\nMoE Key Lab of Artiﬁcial Intelligence,\nAI Institute,\nShanghai Jiao Tong University, China\nxkyang@sjtu.edu.cn\nLing Shao\nInception Institute of Artiﬁcial Intelligence (IIAI), Abu Dhabi, UAE\nling.shao@ieee.org\nAbstract\nTransformers have demonstrated great potential in computer vision tasks. To\navoid dense computations of self-attentions in high-resolution visual data, some\nrecent Transformer models adopt a hierarchical design, where self-attentions are\nonly computed within local windows. This design signiﬁcantly improves the\nefﬁciency but lacks global feature reasoning in early stages. In this work, we\ndesign a multi-path structure of the Transformer, which enables local-to-global\nreasoning at multiple granularities in each stage. The proposed framework is\ncomputationally efﬁcient and highly effective. With a marginal increasement in\ncomputational overhead, our model achieves notable improvements in both image\nclassiﬁcation and semantic segmentation. Code is available at https://github.com/\nljpadam/LG-Transformer.\n1 Introduction\nOver the past decade, we have witnessed the continuous success of convolutional neural networks\n(CNNs) in computer vision. Recently, Visual Transformers have also demonstrated strong potential,\nachieving state-of-the-art performance in several visual tasks, including image classiﬁcation, object\ndetection, and semantic segmentation. Transformers originally emerged from natural language\nprocessing (NLP) models and are considered as more ﬂexible alternatives to CNNs for processing\nmulti-modal inputs.\nOne of the major differences between a CNN and Transformer is the feature interaction mechanism.\nIn a CNN, convolutional kernels are locally connected to the input feature maps, where features only\n∗*Equal contribution\n†Corresponding author\narXiv:2107.04735v1  [cs.CV]  10 Jul 2021\n1×1conv3×3conv5×5conv\n(a) Multi-granular conv in the Inception Network [5]\ncopy 2×2downsample4×4downsample\nW-MSAW-MSAW-MSA (b) Local-to-Global self-Attention in our framework\nFigure 1: Our design is inspired by the multi-granular convolutional structure in CNN models, where\n1 ×1, 3 ×3 and 5 ×5 convolution kernels captures different levels of local information. In our\nframework, feature maps are down-sampled into different scales, and window-based multi-head self\nattention (W-MSA) is applied to each scale to model local-to-global feature interaction.\ninteract with their local neighbors. In contrast, Transformer models, such as ViT [1], require global\nfeature reasoning by computing self-attentions among all the tokens. As a result, Transformers are\ncomputationally inefﬁcient when processing images with a large number of visual tokens. To address\nthis issue, several recent works, such as Swin Transformer [2], CvT [3], and PVT [4], take inspiration\nfrom CNN models, and only compute self-attentions within local windows. This strategy brings\nsigniﬁcant improvements in efﬁciency, but abandons global feature reasoning in early stages, which\nweakens the potential of Transformer models.\nIn this work, we study the necessity of global and local feature interactions in self-attention modules\nof Vision Transformers and propose a local-to-global multi-path mechanism in self-attention, which\nwe refer to as the LG-Transformer. Fig. 2 provide an outline of our architecture, which follows the\nmacro-structure of locally connected Transformers [2, 3, 4]. In contrast to these models, in the early\nstages (stage 1-3), we expand the self-attention module into a multi-path structure, where feature\nmaps (tokens) are downsampled so as to cover different local scales. To enable global reasoning,\nwe set the window size equal to the smallest feature scale (i.e., H\n32 ×W\n32 ). After the multi-path\nself-attention, the local-to-global features are aggregated as a more discriminative representation\nfor the tokens. As the scales of global features are signiﬁcantly downsampled at each stage, the\ncomputational overhead is marginally increased compared with the single-path locally connected\nbaseline Transformer. In the meantime, the multi-path self-attention structure is easy to implement,\nand it does not bring in novel operations, which yields an efﬁcient and strong Transformer model.\nOur design is inspired by the multi-granular convolutional structure in CNN models, such as Inception\nNetworks [5, 6, 7] and High Resolution Networks [8], where feature interactions are computed within\nlocal regions of different scales. Different from these CNN frameworks, our model not only computes\nthe local feature interactions, but also enables global feature reasoning, taking advantage of both\nCNN and Transformer models. A comparison is illustrated in Fig. 1.\nLG-Transformer achieves competitive results on both image classiﬁcation and semantic segmentation.\nCompared with the state-of-the-art model Swin-Transformer [2], LG-Transformer improves the Top-1\naccuracy by 0.9% on ImageNet [ 9] classiﬁcation. Meanwhile, it improves the mIoU by 0.8% on\nADE20K [10] for semantic segmentation.\n2 Related Work\nTransformers and Vision Transformers. The pioneering deep neural network architecture, Trans-\nformer [11], was ﬁrstly proposed to solve sequence transduction tasks in NLP. The original Trans-\nformer is built on self-attention modules which excel at modeling long-range information under\narbitrary input lengths. Based on this simple and powerful architecture, several Transformer vari-\nants [12, 13, 14, 15] achieve dominant positions in many NLP tasks, such as machine translation and\nsummarization.\n2\nMotivated by their great success in NLP, self-attention mechanisms and Transformer have also been\nintroduced to computer vision [16, 17, 18, 19, 20]. The early Transformer-based vision models mainly\nleverage the advantage of global dependencies. For example, the Vision Transformer (ViT) [1] is a\npure Transformer-based image classiﬁcation model which takes image patches as tokens and directly\nmodels the global attention on all the tokens. It achieves comparable performance as hand-designed\nand automatically searched CNN architectures [21, 20]. However, due to the large resource cost of\nglobal attention, ViT is more suitable to generate low-resolution outputs, which limits its applications.\nTo address this issue, a Pyramid Vision Transformer (PVT) [4] is proposed to reduce the resolutions\nof keys and values with a spatial-reduction attention, which improves the efﬁciency while still\nmaintaining the capability of aggregating global information. Although global connectivity in\nTransformer models shows promising results in computer vision, it is undeniable that existing CNN\nmodels also demonstrate the importance of local information. Some works try to combine the\nadvantages of CNN and self-attention by appended blocks [ 22], interleave architecture [ 23, 24]\nand parallel paths [25]. LocalViT [24] employs depth-wise convolution layers into transformers to\nexplicitly model the local dependencies. CvT [3] introduces convolution into token embeddings and\nfeature projection to combine local information and global image contexts. Swin-Transformer [2]\nexplores local information by a pure transformer based architecture which splits feature maps into non-\noverlapping windows, and applies intra-window and cross-window interactions to build hierarchical\nattentions.\nMulti-granular connection in CNN . Multi-granular information has been widely explored in\ncomputer vision since the era of hand-crafted features [ 26, 27, 28, 29]. Although CNN mod-\nels [ 21, 20, 30, 31] demonstrate strong feature representation capability due to the end-to-end\nlearning and deep hierarchical architectures, they still beneﬁt from multi-granular connections in\nmany computer vision applications, such as image classiﬁcation [32, 5], object detection [33] and se-\nmantic segmentation [34, 35]. Inception Networks [5, 6, 7] are a serious of classiﬁcation models built\nupon multi-granular convolution kernels to improve the classiﬁcation performance and computation\nefﬁciency. HRNet [8, 36] constructs multi-granular feature maps in parallel paths to simultaneously\nmaintain more semantic and spatial information, and it achieves promising performance on several\ndense prediction tasks. To detect objects with various sizes, pyramid feature maps are generated\nby connecting bottom-up and top-down pathways in many CNN based detectors [37, 38, 39]. PSP-\nNet [40] introduces a pyramid pooling module to fuse multi-granular context information for semantic\nsegmentation methods. Hourglass [ 41] stacks multiple encoder and decoder networks and builds\nmulti-granular connections among them for human pose estimation.\nIn this work, we take advantage of both the local connectivity in CNNs and global connectivity in\nTransformers, building a Transformer model with local-to-global attention, yielding a framework\nwith improved performance and meanwhile maintains high efﬁciency.\n3 Method\nIn this section, we describe the detailed architecture of the proposed LG-Transformer. As illustrated\nin Fig. 2, our model follows the hierarchical design of several recent visual Transformers [2, 3, 4]\nfor efﬁcient learning. Our framework contains four stages. In the ﬁrst stage, patch embeddings\nare applied to the input image, resulting in H\n4 ×W\n4 tokens. In each of the subsequent stages, the\nresolution of feature maps (tokens) is reduced by a factor of 4 (2 ×2) with a patch merging layer [2],\nwhile the feature dimensions are increased by a factor of 2. Finally, the network outputs H\n32 ×W\n32\nvisual tokens.\n3.1 Local-to-Global Attention Block\nThe core design of our framework is the Local-to-Global (LG) Attention block, applied to stages 1-3.\nAs illustrated in the bottom part of Fig. 2, the LG-Attention block contains several parallel attention\npaths. Take stage 2 for example. After layer normalization (LN), the input feature z1 with resolution\nH\n8 ×W\n8 is bilinearly down-sampled into feature maps with lower dimensions ( i.e., H\n16 ×W\n16 and\nH\n32 ×W\n32 ). Each feature map is processed with a window based multi-head self-attention with shifted\nwindow partitioning (SW-MSA) [2]. Speciﬁcally, feature maps are partitioned into non-overlapped\nwindows, and self-attentions are only calculated within each window, thus the computation complexity\nis signiﬁcantly reduced compared performing self-attention on the entire feature map. By displacing\n3\ninputPatch EmbeddingLocal-Global AttentionStage 1H4×W4×%\nx2 Patch MergingLocal-Global AttentionStage 2H8×W8×2%\nx2 Patch MergingLocal-Global AttentionStage 3H16×W16×4%\nx6 Patch MergingSelf-AttentionStage 4H32×W32×8%\nx2\nLN BilinearDown-samplingBilinearDown-sampling\nSW-MSA\nSW-MSA\nSW-MSA\nLNBilinearUp-samplingBilinearUp-sampling\nMLPH8×W8\nH8×W8H16×W16\nH32×W32\nH8×W8H8×W8\nH8×W8\nH8×W8\nLocal-Global Attention\n!!\"#\n!\"!\"\n!\"#,%\"\n!\"#,&\" \"\"!\"\"\nFigure 2: Our Local-to-Global (LG) Transformer contains four stages, each of which includes a\npatch embedding/merging layer, followed by several LG-Attention blocks. Each LG-Attention block\ncontains several parallel SW-MSA modules to calculate local attentions, gathering local-to-global\ninformation with feature interactions.\nthe window between consecutive attention blocks, SW-MSA enables local patches to communicate\nwith its different neighbors. However, the feature interaction is still limited to a local region. In this\nwork, we set the window size to [ H\n32 ,W\n32 ], such that the feature map with the lowest resolution can be\nprocessed with global self-attention. After the SW-MSA modules, the downsampled features maps\nare upsampled to the same resolution and combined, before being fed into the LN and MLP layers.\nThe LG-Attention block can be calculated as:\nˆzl\no = SW-MSA(LN(zl−1)), (1)\nˆzl\nd,1 = SW-MSA(BD1(LN(zl−1))), (2)\nˆzl\nd,2 = SW-MSA(BD2(LN(zl−1))), (3)\nˆzl = ˆzl\no + BU1(ˆzl\nd,1) + BU2(ˆzl\nd,2) + zl−1, (4)\nzl = MLP(LN(ˆzl)) + ˆzl, (5)\nwhere BD and BU denote bilinear downsampling and bilinear upsampling, respectively. zl−1 is the\ninput feature, ˆzl\no, ˆzl\nd,1 and ˆzl\nd,2 are the intermediate features containing local-to-global information,\nˆzl is the aggregation of these features, and zl is the output feature.\n3.2 Computational Complexity\nDue to the downsampling operations in the LG-Attention blocks, the computational complexity is\nmarginally increased compared with the Swin-Transformer baseline. Suppose each local window\ncontains M ×M tokens. The complexities of a SW-MSA module and our LG-Attention block (with\nparallel down-sampled ratios of 2 and 4) based on a feature map containing h×wtokens are3:\nΩ(SW-MSA) = 4hwC2 + 2M2hwC, (6)\nΩ(LG-Att) = 5.25hwC2 + 2.625M2hwC. (7)\nAlthough two novel branches are added to the network, LG-Attention block only increases the\ncomputations about 0.3 times compared with SW-MSA. Further, the complexity is a linear function\nof hw, making it signiﬁcantly lower than the complexity of the global self-attention, which is\nproportional to (hw)2.\n3We omit the computation of Softmax and window shift.\n4\nDownsp. Rate\n(output size) Layer Name LG-T LG-S\nstage 1 4×\n(56×56)\nPatch Embeding C=4×4, D=96, LN C=4×4, D=96, LN\nLG-Att\n[ S=[4×, 16×, 32×]\nW=7×7, D=96, H=3\n]\n×2\n[ S=[4×, 16×, 32×]\nW=7×7, D=96, H=3\n]\n×2\nstage 2 8×\n(28×28)\nPatch Merging C=2×2, D=192, LN C=2×2, D=192, LN\nLG-Att\n[ S=[8×, 16×, 32×]\nW=7×7, D=192, H=6\n]\n×2\n[ S=[8×, 16×, 32×]\nW=7×7, D=192, H=6\n]\n×2\nstage 3 16×\n(14×14)\nPatch Merging C=2×2, D=384, LN C=2×2, D=384, LN\nLG-Att\n[ S=[16×, 32×]\nW=7×7, D=384, H=12\n]\n×6\n[ S=[16×, 32×]\nW=7×7, D=384, H=12\n]\n×18\nstage 4 32×\n(7×7)\nPatch Merging C=2×2, D=768, LN C=2×2, D=768, LN\nSW-MSA\n[ S=[32×, ]\nW=7×7, D=768, H=24\n]\n×2\n[ S=[32×, ]\nW=7×7, D=768, H=24\n]\n×2\nTable 1: Detailed architecture of LG-T and LG-S. C denotes the concatenation rate. D is the number\nof dimensions of the embedding. S are the scale rates of parallel attention paths in the LG-Attention\nblock. W denotes the window size of SW-MSA. H is the number of heads in the multi-head attention.\nNetwork Image Size Params (M) FLOPs (G) Top-1 Acc. (%) Top-5 Acc. (%)\nCNN Based Models\nMobileNetV1 [42] 224 × 224 4.2 0.6 70.6 –\nMobileNetV2 (1.4) [43] 224 × 224 6.9 0.6 74.7 –\nResNet-18 [21] 224 × 224 11.7 1.8 69.8 89.1\nResNet-50 [21] 224 × 224 25.6 4.1 76.1 92.9\nDenseNet-169 [31] 224 × 224 14.2 3.4 75.6 92.8\nRegNet-4GF [44] 224 × 224 20.7 4.0 80.0 –\nRegNet-16GF [44] 224 × 224 84 16 82.9 –\nEfﬁcientNet-B4 [45] 380 × 380 19.3 4.5 82.9 96.4\nEfﬁcientNet-B6 [45] 528 × 528 43 19 84.0 96.8\nTransformers Based Models\nDeiT-T [46] 224 × 224 5.7 1.3 72.2 91.1\nDeiT-S [46] 224 × 224 22.1 4.6 79.8 95.1\nCrossViT-S [47] 224 × 224 26.7 5.6 81.0 –\nT2T-ViT-14 [48] 224 × 224 22 5.2 81.5 –\nTNT-S [49] 224 × 224 23.8 5.2 81.3 –\nCoaT Mini [50] 224 × 224 10 6.8 80.8 –\nCoaT-Lite Small [50] 224 × 224 20 4.0 81.9 –\nPVT-Small [4] 224 × 224 24.5 3.8 79.8 –\nCPVT-Small-GAP [51] 224 × 224 23 4.6 81.5 –\nSwin-T [2] 224 × 224 29 4.5 81.3 –\nLG-T (ours) 224 × 224 32.6 4.8 82.1 95.8\nT2T-ViT-19 [48] 224 × 224 39.2 8.9 81.9 –\nPVT-Medium [4] 224 × 224 44.2 6.7 81.2 –\nSwin-S [2] 224 × 224 50 8.7 83.0 –\nSwin-B [2] 224 × 224 88 15.4 83.3 –\nLG-S (ours) 224 × 224 61.0 9.4 83.3 96.2\nTable 2: Image classiﬁcation results on ImageNet datasets.\n3.3 Architecture Variants\nWe build two variants of our framework, denoted as LG-T and LG-S, where the only difference is the\nnumber of LG-Attention blocks in each stage. The detailed architecture of our framework is reported\nin Table 1. Except for the LG-Attention blocks, LG-Transformer basically employs the same settings\nas Swin-Transformer [2].\n5\nStages with LG-Att Params (M) FLOPs (G) Top-1 Acc. (%)\n– 4.5 28.3 78.44\n1 4.5 28.4 78.68\n1∼2 4.6 29.0 78.89\n1∼3 4.8 32.6 79.72\nTable 3: Ablation study by inserting LG Attention blocks into different stages.\nFigure 3: Visualization of feature maps in stage 2 of LG-T. The LG-Attention block contains three\npaths, which generate feature maps of the input image with downsampling rates of 8 (local attention),\n16 (mid-level attention), and 32 (global attention). Zoom in for better visualization.\n4 Experimental Results\nIn this section, we report the experimental results of our LG-Transformer on two computer vision\ntasks, i.e., image classiﬁcation, and semantic segmentation. Comparisons with state-of-the-art models\nare given in each subsection. We also provide a thorough ablation study on model components and\nstructure variations.\n4.1 Image Classiﬁcation\nExperiment Settings. To achieve fair comparison with previous works, most of our experiment\nsettings follows Swin-Transformer [2]. Experiments on image classiﬁcation are conducted on the\nImageNet-1K dataset [9] which contains 1,000 classes, and 1.28M and 50K images for the training\nset and validation set, respectively. Resolutions of input images are set to 224 ×224 for both training\nand evaluation. AdamW with β1 = 0.9 and β2 = 0.999 is use as optimizer for all of our models. We\ntrain LG-T and LG-S for 300 epochs in the comparison with state-of-the-art methods, and 100 epochs\nin the ablation studies. The initial learning rate is 0.001 with a cosine scheduler for learning rate\ndecay, and a linear warm-up with 20 epochs is employ to stabilize training. All models are trained on\na cluster with 8 V100 GPUs. We set Batch size to 128 on each GPU, and it takes near 4 days and 6\ndays to train a LG-T and a LG-S with 300 epochs, respectively. Data augmentation is used in the\ntraining phase, and we employ the same parameters as in [2]. We evaluate the test images based on a\ncenter cropped image without any test-time augmentation.\nComparative Results. Table 2 presents the comparative results on ImageNet classiﬁcation. When\ncomparing LG-Transformer with state-of-the-art Vision Transformers, we observe that our framework\nachieves competitive or better results under similar computational overheads. For example, LG-T\n6\n(a) Training loss of LG-T and Swin-T.\n (b) Validation accuracy of LG-T and Swin-T.\n(c) Training loss of LG-T with different numbers of\nLG-Att.\n(d) Validation accuracy of LG-T with different num-\nbers of LG-Att.\nFigure 4: Training losses and validation accuracies on ImageNet.\nachieves 82.1% top-1 accuracy, outperforming Swin-T by 0.8%, PVT-Small by 2.3%, T2T-ViT-14 by\n0.6%, and DeiT-S by 2.3%. Meanwhile, LG-S also outperforms its Transformer counterparts, e.g.,\nT2T-ViT-19, PVT-Medium, Swin-S. Notably, LG-S achieves comparable performance to Swin-B,\nwith signiﬁcantly fewer parameters and lower FLOPs.\nFurther, the performance of LG-Transformer is signiﬁcantly better than the widely utilized CNN\nmodels, i.e., MobileNet, ResNet, and DenseNet. Notably, the LG-Transformers achieve comparable\nperformance to the state-of-the-art CNN models, i.e., RegNet and EfﬁcientNet.\nScale 1\n16 Scale 1\n32 Params (M) FLOPs (G) Top-1 Acc. (%)\n4.5 28.3 78.44\n✓ 4.6 28.7 78.74\n✓ 4.7 32.2 79.60\n✓ ✓ 4.8 32.6 79.72\nTable 4: Ablation study for LG-Attention paths.\nAblation Studies. To evaluate the effectiveness of the proposed LG-Attention block, we carry out\na series of ablation studies.4 First, as shown in Table 3, by inserting the LG-Attention block into\ndifferent stages, we can observe that the performance of LG-T improves continuously (from 78.4%\nto 79.7%) when LG-Attention is employed in more stages, while the parameters and FLOPs are only\nmarginally increased (from 28.3 GFLOPs to 32.6 GFLOPs).\n4We report results by training 100 epochs.\n7\nADE20K val #param. FLOPsMethod Backbone mIoU\nDANet [52] ResNet-101 45.2 69M 1119G\nACNet [53] ResNet-101 45.9 - -\nDNL [54] ResNet-101 46.0 69M 1249G\nOCRNet [36] ResNet-101 45.3 56M 923G\nOCRNet [36] HRNet-w48 45.7 71M 664G\nDeepLab.v3+ [55] ResNet-101 44.1 63M 1021G\nDeepLab.v3+ [55] ResNeSt-101 46.9 66M 1051G\nDeepLab.v3+ [55] ResNeSt-200 48.4 88M 1381G\nSETR [56] T-Large 50.3 308M -\nUperNet [57] ResNet-101 44.9 86M 1029G\nUperNet DeiT-S 44.0 52M 1099G\nUperNet Swin-T 46.1 60M 945G\nUperNet Swin-T ∗ 44.5 60M 945G\nUperNet LG-T 45.3 64M 957G\nTable 5: Results on the ADE20K val set. ∗indicates the results from the public codes of Swin-\nTransformer [2]. Our models are implemented based on this code base.\nSecond, we compare different combinations of the attention paths with downsampling rates 16 and\n32. As illustrated in Table 4, LG-Attention achieves the best performance when combining both types\nof attention.\nFurthermore, Fig. 4 shows the training losses and validation accuracies of different models. From\nFig. 4(a)(b), we ﬁnd that GL-T converges faster, while consistently achieving a lower training loss\nand higher validation accuracy. When comparing the loss curves and validation accuracies of the\nmodel variants in Fig. 4(c)(d), we ﬁnd that adding LG-Attention blocks to more stages constantly\nimproves the validation performance and training convergence.\nIn summary, these results validate the importance of the proposed multi-path attention mechanism\nand demonstrate the necessity of individual attention paths.\nQualitative Results. To better illustrate how the local-to-global attention blocks work, we provide\nsome qualitative results containing the feature maps from stage 2 of LG-T in Fig. 3. We show three\nlevels of feature maps, with downsampling rates of 8 (local attention), 16 (mid-level attention), and\n32 (global attention). The basic building block of the feature maps are 7 ×7 feature blocks. For\nexample, the feature maps with a downsampling rate of 8 contains 4 ×4 feature blocks. Within each\nblock, only the 7 ×7 local features interact with each other. Feature maps with a downsampling\nrate of 16 contain 4 blocks of 7 windows. Although the features in each block can represent more\nlocal information, each block is still processed individually. Attentions in the global feature maps a\ndownsampling rate of 32 can cover the whole image, thus containing global semantic information.\nNotably, there is clear discontinuity across the borders of the local attention windows in the feature\nmaps of downsampling rates of 8 and 16, showing the limitation of local attentions.\n4.2 Semantic Segmentation\nExperimental Settings . We conduct experiments of semantic segmentation on the ADE20K\ndataset [58], which includes 150 classes, and 20K, 2K images for training, and validation, re-\nspectively. We follow most of the settings as [2]. We resize and crop images to 512 ×512 resolutions\nfor training, and resize images to2048×512 for evaluation. Our model is pretrained on ImageNet-1K.\nAdamW with β1 = 0.9 and β2 = 0.999 is used as optimizer, and weight decay is set to 0.01. Our\nmodel is trained for 160K iterations. The initial learning rate is 6 ×10−5 using a poly scheduler for\nlearning rate decay. Batch size is set to 2 on each GPU. It takes near 1 day to train UperNet with\nLG-T, on a cluster with 8 V100 GPUs. We use the same training augmentation methods as [2], and\nno test-time augmentation is employed.\nResults Analysis. We apply our model to semantic segmentation by combining LG-T with Uper-\nNet [57]. As shown in Table 5, our model achieves improvements compared with ResNet-101\n(↑0.4%), DeiT-S (↑1.3%) and Swin-T∗(↑0.8%), with a similar number of parameters and computa-\n8\ntional complexity. Although our model achieves inferior performance compared with state-of-the-art\nsegmentation models, i.e., DeepLab V3 + ResNeSt-200 and SETR, these models require a larger\nmodel size and more computation. From this perspective, our model achieves a good trade-off\nbetween performance and efﬁciency.\n5 Conclusion\nIn this paper, we propose to incorporate local and global attention into a Vision Transformer. By\nbuilding a multi-path structure in the hierarchical Vision Transformer framework, our model takes\nadvantage of both local the feature learning mechanisms in CNNs and global feature learning\nmechanisms in Transformers. We conduct thorough studies on two computer vision tasks, and the\nresults demonstrate that our framework yields improved performance with limited sacriﬁce in model\nparameters and computational overhead. As a multi-path framework, one of the limitations of our\nmodel is that the inference speed is lower than its single-path Transformer counterparts. We will try\nto improve the efﬁciency in our future work.\nReferences\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. CoRR, abs/2010.11929, 2020. 2, 3\n[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. CoRR,\nabs/2103.14030, 2021. 2, 3, 5, 6, 8\n[3] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvt: Introducing convolutions to vision transformers. CoRR, abs/2103.15808, 2021. 2, 3\n[4] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. CoRR, abs/2102.12122, 2021. 2, 3, 5\n[5] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.\nIn IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015. 2, 3\n[6] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 2818–2826, 2016. 2, 3\n[7] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4,\ninception-resnet and the impact of residual connections on learning. In Satinder P. Singh and\nShaul Markovitch, editors, Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages\n4278–4284, 2017. 2, 3\n[8] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning\nfor human pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition,\npages 5693–5703, 2019. 2, 3\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale\nhierarchical image database. In IEEE Computer Society Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009. 2, 6\n[10] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In\nEuropean Conference on Computer Vision, pages 740–755, 2014. 2\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\nGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npages 5998–6008, 2017. 2\n9\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages\n4171–4186. Association for Computational Linguistics, 2019. 2\n[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 2\n[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\npretraining approach. CoRR, abs/1907.11692, 2019. 2\n[15] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Kentaro Inui,\nJing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,\n2019, pages 3728–3738. Association for Computational Linguistics, 2019. 2\n[16] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst\nBischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346\nof Lecture Notes in Computer Science, pages 213–229. Springer, 2020. 3\n[17] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR:\ndeformable transformers for end-to-end object detection. CoRR, abs/2010.04159, 2020. 3\n[18] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture trans-\nformer network for image super-resolution. In 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 5790–5799.\nIEEE, 2020. 3\n[19] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In 2019 IEEE/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,\n2019, pages 7463–7472. IEEE, 2019. 3\n[20] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. CoRR,\nabs/2012.00364, 2020. 3\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016. 3,\n5\n[22] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka,\nKurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and\nprocessing for computer vision. CoRR, abs/2006.03677, 2020. 3\n[23] Irwan Bello, Barret Zoph, Quoc Le, Ashish Vaswani, and Jonathon Shlens. Attention augmented\nconvolutional networks. In 2019 IEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 3285–3294. IEEE,\n2019. 3\n[24] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing\nlocality to vision transformers. CoRR, abs/2104.05707, 2021. 3\n10\n[25] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Max-deeplab:\nEnd-to-end panoptic segmentation with mask transformers. CoRR, abs/2012.00759, 2020. 3\n[26] David G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis.,\n60(2):91–110, 2004. 3\n[27] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005),\n20-26 June 2005, San Diego, CA, USA, pages 886–893. IEEE Computer Society, 2005. 3\n[28] Pedro F. Felzenszwalb, Ross B. Girshick, David A. McAllester, and Deva Ramanan. Object\ndetection with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach.\nIntell., 32(9):1627–1645, 2010. 3\n[29] Piotr Dollár, Ron Appel, Serge J. Belongie, and Pietro Perona. Fast feature pyramids for object\ndetection. IEEE Trans. Pattern Anal. Mach. Intell., 36(8):1532–1545, 2014. 3\n[30] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference\non Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings, 2015. 3\n[31] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proc. CVPR, pages 2261–2269, 2017. 3, 5\n[32] Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. In Daniel D. Lee, Masashi\nSugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors,Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural Information Processing\nSystems 2016, December 5-10, 2016, Barcelona, Spain, pages 4053–4061, 2016. 3\n[33] Zhaowei Cai, Quanfu Fan, Rogério Schmidt Feris, and Nuno Vasconcelos. A uniﬁed multi-scale\ndeep convolutional neural network for fast object detection. In Bastian Leibe, Jiri Matas, Nicu\nSebe, and Max Welling, editors, Computer Vision - ECCV 2016 - 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV , volume 9908 of\nLecture Notes in Computer Science, pages 354–370. Springer, 2016. 3\n[34] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834–848, 2018. 3\n[35] Damien Fourure, Rémi Emonet, Élisa Fromont, Damien Muselet, Alain Trémeau, and Christian\nWolf. Residual conv-deconv grid network for semantic segmentation. In British Machine Vision\nConference 2017, BMVC 2017, London, UK, September 4-7, 2017. BMV A Press, 2017. 3\n[36] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic\nsegmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors,\nComputer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part VI, volume 12351 of Lecture Notes in Computer Science, pages 173–190.\nSpringer, 2020. 3, 8\n[37] Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J.\nBelongie. Feature pyramid networks for object detection. In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017,\npages 936–944. IEEE Computer Society, 2017. 3\n[38] Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin, and Pheng-Ann Heng.\nBidirectional feature pyramid network with recurrent attention residual modules for shadow\ndetection. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors,\nComputer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part VI, volume 11210 of Lecture Notes in Computer Science, pages\n122–137. Springer, 2018. 3\n[39] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V . Le. NAS-FPN: learning scalable feature pyramid\narchitecture for object detection. In IEEE Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 7036–7045. Computer\nVision Foundation / IEEE, 2019. 3\n[40] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n11\n2017, Honolulu, HI, USA, July 21-26, 2017, pages 6230–6239. IEEE Computer Society, 2017.\n3\n[41] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose\nestimation. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision\n- ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part VIII, volume 9912 of Lecture Notes in Computer Science, pages 483–499.\nSpringer, 2016. 3\n[42] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural\nnetworks for mobile vision applications. CoRR, abs/1704.04861, 2017. 5\n[43] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages\n4510–4520. IEEE Computer Society, 2018. 5\n[44] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Dollár.\nDesigning network design spaces. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10425–10433.\nIEEE, 2020. 5\n[45] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Machine Learning Research, pages 6105–6114.\nPMLR, 2019. 5\n[46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. CoRR,\nabs/2012.12877, 2020. 5\n[47] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. CoRR, abs/2103.14899, 2021. 5\n[48] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.\nCoRR, abs/2101.11986, 2021. 5\n[49] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer, 2021. 5\n[50] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers. CoRR, abs/2104.06399, 2021. 5\n[51] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua\nShen. Conditional positional encodings for vision transformers, 2021. 5\n[52] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual\nattention network for scene segmentation. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3146–3154. Computer\nVision Foundation / IEEE, 2019. 8\n[53] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive\ncontext network for scene parsing. In 2019 IEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6747–6756.\nIEEE, 2019. 8\n[54] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu.\nDisentangled non-local neural networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox,\nand Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part XV, volume 12360 of Lecture Notes in\nComputer Science, pages 191–207. Springer, 2020. 8\n[55] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.\nEncoder-decoder with atrous separable convolution for semantic image segmentation. In\nVittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer\nVision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018,\n12\nProceedings, Part VII, volume 11211 of Lecture Notes in Computer Science, pages 833–851.\nSpringer, 2018. 8\n[56] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yan-\nwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with transformers, 2021. 8\n[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing\nfor scene understanding. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair\nWeiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany,\nSeptember 8-14, 2018, Proceedings, Part V , volume 11209 of Lecture Notes in Computer\nScience, pages 432–448. Springer, 2018. 8\n[58] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ADE20K dataset. Int. J. Comput. Vis.,\n127(3):302–321, 2019. 8\n13",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4180598855018616
    },
    {
      "name": "Computer science",
      "score": 0.33642834424972534
    },
    {
      "name": "Engineering",
      "score": 0.15173554420471191
    },
    {
      "name": "Electrical engineering",
      "score": 0.12154600024223328
    },
    {
      "name": "Voltage",
      "score": 0.0555344820022583
    }
  ]
}