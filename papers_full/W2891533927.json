{
  "title": "A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding",
  "url": "https://openalex.org/W2891533927",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5101857422",
      "name": "Changliang Li",
      "affiliations": [
        "Kingsoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100420654",
      "name": "Liang Li",
      "affiliations": [
        "Kingsoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5003943585",
      "name": "Ji Qi",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2803211626",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2963974889",
    "https://openalex.org/W2397579082",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2076440176",
    "https://openalex.org/W2150859660",
    "https://openalex.org/W2400801499",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2473329891",
    "https://openalex.org/W2155524666",
    "https://openalex.org/W1971034924",
    "https://openalex.org/W2024632416",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2091671846",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2053463056",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1602011256",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W1705588077",
    "https://openalex.org/W2094472029",
    "https://openalex.org/W1550863320",
    "https://openalex.org/W2166293310",
    "https://openalex.org/W2227316993",
    "https://openalex.org/W2399456070",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2575101493",
    "https://openalex.org/W2395389931",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2963050422",
    "https://openalex.org/W2108366050",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2405622356",
    "https://openalex.org/W2137871902",
    "https://openalex.org/W1587506928",
    "https://openalex.org/W2964117975"
  ],
  "abstract": "Spoken Language Understanding (SLU), which typically involves intent determination and slot filling, is a core component of spoken dialogue systems. Joint learning has shown to be effective in SLU given that slot tags and intents are supposed to share knowledge with each other. However, most existing joint learning methods only consider joint learning by sharing parameters on surface level rather than semantic level. In this work, we propose a novel self-attentive model with gate mechanism to fully utilize the semantic correlation between slot and intent. Our model first obtains intent-augmented embeddings based on neural network with self-attention mechanism. And then the intent semantic representation is utilized as the gate for labelling slot tags. The objectives of both tasks are optimized simultaneously via joint learning in an end-to-end way. We conduct experiment on popular benchmark ATIS. The results show that our model achieves state-of-the-art and outperforms other popular methods by a large margin in terms of both intent detection error rate and slot filling F1-score. This paper gives a new perspective for research on SLU.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3824–3833\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n3824\nA Self-Attentive Model with Gate Mechanism\nfor Spoken Language Understanding\nChangliang Li1, Liang Li2, Ji Qi1\n1Kingsoft AI Lab\n2Tsinghua University\n1{lichangliang, qiji1}@kingsoft.com\n2liliang17@mails.tsinghua.edu.cn\nAbstract\nSpoken Language Understanding (SLU),\nwhich typically involves intent determination\nand slot ﬁlling, is a core component of spoken\ndialogue systems. Joint learning has shown\nto be effective in SLU given that slot tags\nand intents are supposed to share knowledge\nwith each other. However, most existing\njoint learning methods only consider joint\nlearning by sharing parameters on surface\nlevel rather than semantic level. In this work,\nwe propose a novel self-attentive model with\ngate mechanism to fully utilize the semantic\ncorrelation between slot and intent. Our model\nﬁrst obtains intent-augmented embeddings\nbased on neural network with self-attention\nmechanism. And then the intent semantic rep-\nresentation is utilized as the gate for labelling\nslot tags. The objectives of both tasks are\noptimized simultaneously via joint learning in\nan end-to-end way. We conduct experiment\non popular benchmark ATIS. The results show\nthat our model achieves state-of-the-art and\noutperforms other popular methods by a large\nmargin in terms of both intent detection error\nrate and slot ﬁlling F1-score. This paper gives\na new perspective for research on SLU.\n1 Introduction\nOne long-term goal in artiﬁcial intelligence ﬁeld\nis to build an intelligent human-machine dialogue\nsystem, which is capable of understanding hu-\nman’s language and giving smooth and correct re-\nsponses. A typical dialogue system is designed to\nexecute the following components: (i) automatic\nspeech recognition converts a spoken query into\ntranscription, (ii) spoken language understanding\ncomponent analyzes the transcription to extract se-\nmantic representations, (iii) dialogue manager in-\nterprets the semantic information and decides the\nbest system action, according to which the system\nresponse is further generated either as a natural\nlanguage output(Jurafsky, 2000).\nIn this paper, we focus on spoken language un-\nderstanding which is a core component of a spo-\nken dialogue system. It typically involves two ma-\njor tasks, intent determination and slot ﬁlling. In-\ntent determination aims to automatically identify\nthe intent of the user as expressed in natural lan-\nguage. Slot ﬁlling aims to extract relevant seman-\ntic constituents from the natural language sentence\ntowards achieving a goal.\nUsually, intent detection and slot ﬁlling are car-\nried out separately. However, separate modeling\nof these two tasks is constrained to take full ad-\nvantage of all supervised signals. Joint learning of\nintent detection and slot ﬁlling is worthwhile for\nthree reasons. Firstly, the two tasks usually appear\nsimultaneously in SLU systems. Secondly, the in-\nformation of one task can be utilized in the other\ntask to promote each other and a joint prediction\ncan be made (Zhang and Wang, 2016). For exam-\nple, if the intent of a utterance is to ﬁnd a ﬂight, it\nis likely to contain the departure and arrival cities,\nand vice versa. Lastly, slot tags and intents, as\nsemantics representations of user behaviours, are\nsupposed to share knowledge with each other.\nRecently, joint model for intent detection and\nslot ﬁlling has achieved much progress. (Xu and\nSarikaya, 2013) proposed using CNN based trian-\ngular CRF for joint intent detection and slot ﬁll-\ning. (Guo et al., 2014) proposed using a recursive\nneural network that learns hierarchical represen-\ntations of the input text for the joint task. (Liu\nand Lane, 2016b) describes a recurrent neural net-\nwork (RNN) model that jointly performs intent de-\ntection, slot ﬁlling and language modeling. The\nneural network models keep updating the intent\nprediction as word in the transcribed utterance ar-\nrives and uses it as contextual features in the joint\nmodel.\nIn this work, we propose a novel model for\n3825\njoint intent determination and slot ﬁlling by intro-\nducing self-attention and gating mechanism. Our\nmodel can fully utilize the semantic correlation be-\ntween slot and intent. To the best of our knowl-\nedge, this is the ﬁrst attempt to utilize intent-\naugmented embedding as a gate to guide the learn-\ning of slot ﬁlling task. To fully evaluate the ef-\nﬁciency of our model, we conduct experiment on\nAirline Travel Information Systems (ATIS) dataset\n(Hemphill et al., 1990), which is popularly used\nas benchmark in related work. And empirical\nresults show that our independent model outper-\nforms the previous best result by 0.54% in terms\nof F1-score on slot ﬁlling task, and gives excel-\nlent performance on intent detection task. Our\njoint model further promotes the performance and\nachieves state-of-the-art results on both tasks.\nThe rest of our paper is structured as follows:\nSection 2 discusses related work, Section 3 gives\na detailed description of our model, Section 4\npresents experiments results and analysis, and\nSection 5 summarizes this work and the future di-\nrection.\n2 Related Work\nThere is a long research history for spoken dia-\nlogue understanding, which emerged in the 1990s\nfrom some call classiﬁcation systems (Gorin et al.,\n1997) and the ATIS project. In this section, we de-\nscribe some typical works on intent classiﬁcation\nand slot-ﬁlling, which are both core tasks of SLU\n(De Mori, 2007).\nFor intent detection task, the early traditional\nmethod is to employ n-grams as features with\ngeneric entities, such as locations and dates\n(Zhang and Wang, 2016). This type of method\nis restricted to the dimensionality of the input\nspace. Another line of popular approaches is to\ntrain machine learning models on labeled training\ndata (Young, 2002; Hahn et al., 2011). For ex-\nample, SVM (Haffner et al., 2003) and Adaboost\n(Schapire and Singer, 2000) have been explored\nto improve intent detection. Approaches based on\nneural network architecture have shown good per-\nformance on intent detection task. Deep belief net-\nworks (DBNs) have been ﬁrst used in call routing\nclassiﬁcation (Deoras and Sarikaya, 2013). More\nrecently, RNNs have shown excellent performance\non the intent classiﬁcation task (Ravuri and Stol-\ncke, 2015).\nFor slot-ﬁlling task, traditional approaches are\nbased on conditional random ﬁelds (CRF) archi-\ntecture, which has strong ability on sequence la-\nbelling (Raymond and Riccardi, 2007). Recently,\nmodels based on neural network and its extensions\nhave shown excellent performance on the slot ﬁll-\ning task and outperform traditional CRF models.\nFor example, (Yao et al., 2013) proposed to take\nwords as input in a standard recurrent neural net-\nwork language model, and then to predict slot la-\nbels rather than words on the output side. (Yao\net al., 2014b) improved RNNs by using transition\nfeatures and the sequence-level optimization cri-\nterion of CRF to explicitly model dependencies\nof output labels. (Mesnil et al., 2013) tried bi-\ndirectional and hybrid RNN to investigate using\nRNN for slot ﬁlling. (Yao et al., 2014a) introduced\nLSTM architecture for this task and obtained a\nmarginal improvement over RNN. Besides, fol-\nlowing the success of attention based models in\nthe NLP ﬁeld, (Simonnet et al., 2015) applied the\nattention-based encoder-decoder to the slot ﬁlling\ntask, but without LSTM cells.\nRecently, there has been some work on learn-\ning intent detection and slot ﬁlling jointly ex-\nploited by neural networks. Slot labels and in-\ntents, as semantics of user behaviors, are supposed\nto share knowledge with each other. (Guo et al.,\n2014) adapted recursive neural networks (RNNs)\nfor joint training of intent detection and slot ﬁll-\ning. (Xu and Sarikaya, 2013) described a joint\nmodel for intent detection and slot ﬁlling based\non convolutional neural networks (CNN). The pro-\nposed architecture can be perceived as a neural\nnetwork version of the triangular CRF model (Tri-\nCRF). (Hakkani-T ¨ur et al., 2016) proposed a sin-\ngle recurrent neural network architecture that in-\ntegrates the three tasks (domain detection, intent\ndetection and slot ﬁlling for multiple domains)\nin a model. (Liu and Lane, 2016a) proposed an\nattention-based neural network model for joint in-\ntent detection and slot ﬁlling. Their joint model\ngot the best performance of 95.98% slot ﬁlling\nF1-score and 1.57% intent error rate in the ATIS\ndataset.\nDespite the great progress those methods have\nachieved, it is still a challenging and open task for\nintent detection and slot ﬁlling. Therefore, we are\nmotivated to design a powerful model, which can\nimprove the performance of SLU systems.\n3826\nfrom\nEmbedding\nSentence\nSelf-Attention\nBi-LSTM Layer\nIntent\nSelf-Attention\nIntent Gating\nBi-LSTM\nGate\nSoftmax\nSelf-Attention\nconcat\n concat\n concat\nSelf-Attention\nGate\n Gate\nbaltimore to\nIntent \nClassifier\nO Fromloc.city_name O\n Flight\nFigure 1: Illustration of our proposed model for joint intent detection and slot ﬁlling. Red arrows represent the\nintent classiﬁcation task based on the weighted average of BiLSTM outputs. The embeddings coloured with\ndifferent intensity values denote word-level and char-level embeddings (three kinds of convolution kernels).\n3 Model\nIn this section, we present our model for the joint\nlearning of intent detection and slot ﬁlling. Figure\n1 gives an overview of our model.\nThe ﬁrst layer maps input sequence into vec-\ntors by concatenating its word-level embeddings\nand character-level embeddings (obtained by con-\nvolution). And we use these vectors as merged\nembeddings in downstream layers. In many situa-\ntions, contextual information is useful in sequence\nlabelling. In this paper, we introduce an approach\nthat leverages context-aware features at each time\nstep. In particular, we make use of self-attention\nto produce context-aware representations of the\nembeddings. Then a bidirectional recurrent layer\ntakes as input the embeddings and context-aware\nvectors to produce hidden states. In the last step,\nwe propose to exploit the intent-augmented gat-\ning mechanism to match the slot label. The gate\nfor a speciﬁc word is obtained by taking a linear\ntransformation of the intent embedding and an-\nother contextual representation of this word com-\nputed by self-attention. We apply element-wise\ndot-product between the gate and each BiLSTM\noutput.\nFinally, a softmax layer is added to classify the\nslot labels on top of the gate layer. For simplic-\nity, we only take the weighted average of BiLSTM\noutputs to predict the intent label.\nThe design of this structure is motivated by the\neffectiveness of multiplicative interaction among\nvectors and by self-attention mechanism which\nhas been used successfully in a variety of tasks\n(Cheng et al., 2016; Vaswani et al., 2017; Lin et al.,\n2017). It also typically corresponds to our ﬁnding\nthat the intent is highly correlated with slot label\nin some cases, so the semantics of intent should\nbe useful for detecting the slot labels.\n3.1 Embedding Layer\nWe ﬁrst convert the indexed words w =\n(w1,w2,...,w T ) to word-level embeddings Ew =\n[ew\n1 ,ew\n2 ,...,e w\nT ], and character-level embeddings\nEc = [ ec\n1,ec\n2,...,e c\nT ]. Although word embed-\ndings are sufﬁcient for many NLP task, pro-\nvided by a well-pretrained glove 1 or word2vec2,\ncharacter-level information provides some more\nprior knowledge (e.g. morphemes) to the embed-\nding learning procedure. Some morphemic corre-\nlated words are more close in vector space, which\nis useful for identifying the slot labels. Character\nembeddings also alleviate the out-of-vocabulary\n(OOV) problem in the testing phase. In this pa-\nper we focus on a character-aware convolution\nlayer used in (Kim et al., 2016) for words. The\n1http://nlp.stanford.edu/projects/glove/\n2https://code.google.com/p/word2vec/\n3827\ncharacter-level embeddings are generated by con-\nvolution over characters in the word with multiple\nwindow size to extract n-gram features.\nLet C be the vocabulary of characters, V be\nthe vocabulary of words. The dimensions of\ncharacter-level embedding and word-level embed-\nding are denoted as dc and dw, respectively. For\neach word wt ∈ V, characters in wt constitute\nthe matrix Ct ∈Rdc×l, where the columns cor-\nresponds to lcharacter embeddings.\nA narrow convolution is applied between Ct\nand a ﬁlter (or kernel) H ∈Rdc×w. Here we sup-\npose the ﬁlter width is w. After that, we obtain a\nfeature map ft ∈Rl−w+1 by adding a nonlinearity\nactivation. The ﬁnal n-gram features is generated\nby taking the max-over-time:\nft[i] =relu(H·Ct[:,i : i+ w−1] +b) (1)\nct = max\ni\nft[i] (2)\nwhere Ct[:,i : i+ w −1] is the i-to-(i+w-1)-th\ncolumn of Ct, and the character-level embedding\nec\nt is made up of multiple ct generated by different\nconvolution kernels.\n3.2 Self-Attention\nAttention mechanism is usually used to guide the\nforming of sentence embedding, extra knowledge\nis also used to weigh the CNN or LSTM hidden\nstates (i.e. document words sometimes attend to\nquestion information). However in slot ﬁlling task,\nthe input to our model is just one sequence. So\nthe attention mechanism used here is called self-\nattention, that is to say, the word at each time step\nattends to the whole words in this sentence. And\nit helps to determine which region is likely to be a\nslot. Since the embedding at each time step con-\nsists of multiple parts (i.e. word embedding and\ncharacter embeddings of different kernel width),\neach part has its own semantic meaning. As shown\nin Figure 2, we divide the embedding into mul-\ntiple parts and the attention of each part is pro-\ncessed within its corresponding dimension. In this\napproach, we restrict the interaction among differ-\nent aspects of the embedding. We hypothesize that\ndifferent semantic parts are relatively independent\nand play different roles in our network.\nSuppose M ∈Rdm×T to be the matrix contain-\ning sentence hidden vectors [m1,...,m T ], where\ndm is the dimension of these T vectors. Consider-\ning the characteristics of slot ﬁling task, our aim\nfrom baltimore to\nWa\n Wb\n Wc\n Wa\n Wb\n Wc\nWa\n Wb\n Wc\nFigure 2: The structure of self-attention layer. Red\ncoloured rectangles stand for matrices which map the\ninput to different subspaces. These transformed vec-\ntors are divided into multiple parts for computing self-\nattention.\nis to encode each hidden vector into a context-\naware representation. We achieve that by using\nattention over all the sentence hidden vectors M.\nFirstly, We linearly map all the vectors in M to\nthree feature spaces by different projection param-\neters Wa, Wb and Wc, so the resulting vectors\nare expressed as Ma, Mb and Mc with the same\nshape as M. These matrices are shared across all\ntime steps. Considering the structure of embed-\nding which consists of K different parts (we use\n4 kinds of embeddings with the same dimension),\nthese transformed matrices are equally split intoK\nparts. Furthermore, the attention weight is com-\nputed by dot product between Ma and Mb. Lastly,\nthe attention output is a weighted sum of Mc.\nSpeciﬁcally, we consider different K parts in\ndetail for k= 1,..,K :\n\n\nMa\nMb\nMc\n\n =\n\n\nWaM\nWbM\nWcM\n\n (3)\nαk,t = softmax(mT\nk,a,tMk,b) (4)\nS-Att(mt,M) = [M1,cαT\n1,t,...,M K,cαT\nK,t] (5)\nwhere Mk,a ∈R(dm/K)×T is the k-th part of Ma\nwhich is transformed from M by Wa. Index t\nis word position ranging over T time steps and\nmk,a,t ∈Rdm/K is the t-th column of Mk,a. αk,t\n3828\nis the attention weights over Mk,c. The output of\nself-attention module generated at time step t is\nthe concatenation of Kparts by using Equation 5.\n3.3 BiLSTM\nCharacter embeddings and word embeddings are\nboth important features in our task. To further uti-\nlize these features, we associate each embedding\nwith a context-aware representation which is typ-\nically implemented by self-attention mechanism.\nFor current word wt, the input of the recurrent\nlayer at time step tis represented as xt:\nea\nt = S-Att([ec\nt,ew\nt ],E) (6)\nxt = [ec\nt,ew\nt ,ea\nt ] (7)\nea\nt is the context-aware vector of wt which is\nobtained by applying self-attention mechanism\non the concatenated embeddings E = [ ec\n1 ∥\new\n1 ,...,e c\nT ∥ew\nT ] .\nIt was difﬁcult to train RNNs to capture long-\nterm dependencies because the gradients tend to\neither vanish or explode. Therefore, some more\nsophisticated activation functions with gating units\nwere designed. We use LSTM (Hochreiter and\nSchmidhuber, 1997) in this work:\nit = σ(Wixt + Uiht−1 + bi) (8)\nft = σ(Wf xt + Uf ht−1 + bf ) (9)\not = σ(Woxt + Uoht−1 + bo) (10)\n˜ct = tanh(Wcxt + Ucht−1 + bc) (11)\nct = it ⊙˜ct + ft ⊙ct−1 (12)\nht = ot ⊙tanh(ct) (13)\nWhere ⊙denotes element-wise product of two\nvectors. To consider both the previous history and\nthe future history, we use BiLSTM as encoder in\nadvance. The bi-directional LSTM (BiLSTM), a\nmodiﬁcation of the LSTM, consists of a forward\nand a backward LSTM. The encoder reads the in-\nput vectors x = (x1,x2,...,x T ) and generates T\nhidden states by concatenating the forward and\nbackward hidden states of BiLSTM:\n− →ht = −−−−→LSTM(xt,− →ht−1) (14)\n← −ht = ←−−−−LSTM(xt,← −ht+1) (15)\nht = [− →ht,← −ht] (16)\nwhere ← −ht is the hidden state of backward pass in\nBLSTM and − →ht is the hidden state of forward pass\nin BLSTM at time t.\n3.4 Intent-Augmented Gating Mechanism\nAs described above, intent information is useful\nfor slot ﬁlling task. To measure the probability\nof words in target slots and attend to the ones\nrelevant to the intent, we add a gate to the out-\nput of BiLSTM layer. Let H ∈ R2d×T be a\nmatrix consisting of hidden vectors [h1,...,h T ]\nproduced by BiLSTM. For each word, we use\nself-attention mechanism to form another context-\naware representation, the gate vector h∗\nt is calcu-\nlated by linearly transforming the concatenation\nof the context-aware representation and the intent\nembedding vector vint with a multi-layer percep-\ntron (MLP) network. The intent label is provided\nby correct label during training phase, and by the\noutput from intent classiﬁcation layer in the test\nphase. Speciﬁcally, for t= 1,...T:\nst = Self-Attention(ht,H) (17)\nh∗\nt = MLP([st,vint]) (18)\not = ht ⊙h∗\nt (19)\nWe use element-wise multiplication to model the\ninteraction between BiLSTM outputs and the gate\nvector.\n3.5 Task Learning\nThe bidirectional recurrent layer converts a se-\nquence of words w = (w1,w2,...,w T ) into hid-\nden states H = [h1,...,h T ] which are shared by\ntwo tasks. We use simple attention pooling func-\ntion denoted asfatt over Hto get an attention-sum\nvector for intent label classiﬁcation. The classiﬁed\nlabel yint is transformed to an embedding vint by\nmatrix Eint for gate computing.\nhint = fatt(H) (20)\nyint = softmax(Winthint + bint) (21)\nDuring the training phase, model parameters are\nupdated w.r.t. a cross-entropy loss between the\npredicted probabilities and the true label. The la-\nbel with maximum probability will be selected as\nthe predicted intent during the testing phase.\nFor another task, the hidden states processed by\nour gating layer are used for predicting slot labels.\nyslot\nt = softmax(Wslotot + bslot) (22)\nSlot ﬁlling can be deﬁned as a sequence labelling\nproblem which is to map a utterance sequence\nw = (w1,...,w T ) to its corresponding slot label\n3829\nsequence y = (y1,...,y T ). The objective is to\nmaximize the likelihood of a sequence:\nP(yslot|w) =\nT∏\nt=1\nP(yslot\nt |w) (23)\nIt is equal to minimize Negative Log-likelihood\n(NLL) of the correct labels for the predicted se-\nquence yslot.\n4 Experiments\n4.1 Dataset\nIn order to evaluate the efﬁciency of our proposed\nmodel, we conduct experiments on ATIS (Air-\nline Travel Information Systems) dataset, which is\nwidely used as benchmark in SLU research (Price,\n1990).\nFigure 3 gives one example of sentence in ATIS\ndataset. The words are labelled with their value ac-\ncording to certain semantic frames. The slot labels\nof the words are represented in an In-Out-Begin\n(IOB) format and the intent is highlighted with a\nbox surrounding it.\nSentence   Show  flights    from      Boston        to       New         York      today\n Slots          O  O   O      B-FromCity    O    B-ToCity   I-ToCity   B-D ate\nFigure 3: Example of sentence annotated by slots sam-\npled from ATIS corpus, the black boxed word indicates\nthe intent.\nIn this paper, we use the ATIS corpus set-\nting following previous related works (Liu and\nLane, 2016a; Mesnil et al., 2015; Liu and Lane,\n2015; Xu and Sarikaya, 2013; Tur et al., 2010).\nThe training set contains 4978 utterances from\nATIS-2 and ATIS-3 datasets, and test set contains\n893 utterances from ATIS-3 NOV93 and DEC94\ndatasets. The number of slot labels is 127 and the\nintent has 18 different types.\n4.2 Metrics\nThe performance of slot ﬁlling task is measured by\nthe F1-score, while intent detection task is evalu-\nated with prediction error rate that is the ratio of\nthe incorrect intent of the test data.\n4.3 Training Details\nWe preprocess the ATIS following (Yao et al.,\n2013; Liu and Lane, 2016a). To deal with unseen\nwords in the test set, we mark those words that ap-\npear only once in the training set as ⟨UNK⟩, and\nuse this label to represent those unseen words in\nthe test set. Besides, each number is converted to\nthe string DIGIT.\nThe model is implemented in the Tensorﬂow\nframework (Abadi et al., 2016). At training stage,\nwe use LSTM cell as suggested in (Sutskever\net al., 2014) and the cell dimension dis set to be\n128 for both the forward and backward LSTM.\nWe set the dimension of word embedding dw\nto be 64 and the dimension of character embed-\nding dc to be 128. We generate three character-\nlevel embeddings using multiple widths and ﬁlters\n(the convolution kernel width w ∈{2,3,4}with\n64 ﬁlters each) followed by a max pooling layer\nover time. Then, the dimension of concatenated\nembeddings is 256. We make the dimensions of\neach parts equal for the convenience of dimension\nsplitting during the self-attention in later stage. All\nthe parameters in the network are randomly initial-\nized with uniform distribution (Sussillo and Ab-\nbott, 2014) which are ﬁne-tuned during training.\nWe use the stochastic gradient descent algorithm\n(SGD) for updating parameters. And the learning\nrate is controlled by Adam algorithm (Kingma and\nBa, 2014). The model is trained on all the train-\ning data with mini-batch size of 16. In order to\nenhance our model to generalize well, the maxi-\nmum norm for gradient clipping is set to 5. We\nalso apply layer normalization (Ba et al., 2016) on\nthe self-attention layer after we add a residul con-\nnection between the output and input. Meanwhile,\ndropout rate 0.5 is applied on recurrent cell pro-\njection layer (Zaremba et al., 2014) and on each\nattention activation.\n4.4 Independent Learning\nThe results of separate training for slot ﬁlling and\nintent detection are reported in Table 1 and Table 2\nrespectively. On the independent slot ﬁlling task,\nwe ﬁxed the intent information as the ground truth\nlabels in the dataset. But on the independent in-\ntent detection task, there is no interaction with slot\nlabels.\nTable 1 compares F1-score of slot ﬁlling be-\ntween our proposed architecture and some previ-\nous works. Our model achieves state-of-the-art\nresults and outperforms previous best model by\n0.56% in terms of F1-score. We attribute the im-\nprovement of our model to the following reasons:\n1) The attention used in (Liu and Lane, 2016a) is\nvanilla attention, which is used to compute the de-\n3830\nMethods F1-score\nCRF (Mesnil et al., 2013) 92.94\nsimple RNN (Yao et al., 2013) 94.11\nCNN-CRF (Xu and Sarikaya, 2013) 94.35\nLSTM (Yao et al., 2013) 94.85\nRNN-SOP (Liu and Lane, 2015) 94.89\nDeep LSTM (Yao et al., 2013) 95.08\nRNN-EM (Peng et al., 2015) 95.25\nBi-RNN with Ranking Loss (Vu\net al., 2016)\n95.47\nEncoder-labeler Deep LSTM (Ku-\nrata et al., 2016)\n95.66\nAttention BiRNN (Liu and Lane,\n2016a)\n95.75\nBLSTM-LSTM (focus) (Zhu and\nYu, 2017)\n95.79\nOur Model 96.35\nTable 1: Results of independent training for slot ﬁlling\nin terms of F1-score.\ncoding states. It is not suitable for our model since\nthe embeddings are composed of several parts.\nSelf-attention allows the model to attend to infor-\nmation jointly from different representation parts,\nso as to better understand the utterance. 2) intent-\naugmented gating layer connects the semantics of\nsequence slot labels, which captures complex in-\nteractions between the two tasks.\nTable 2 compares the performance of our pro-\nposed model to previously reported results on in-\ntent detection task. Our model gives good per-\nformance in terms of classiﬁcation error rate, but\nnot as good as Attention Encoder-Decoder (with\naligned inputs) method (Liu and Lane, 2016a).\nAs their published state-of-the-art result described\nin (Liu and Lane, 2016a), their attention-based\nmodel is based on word-level embeddings. While\nin our model, we introduce character-level embed-\ndings to improve the performance of joint learn-\ning. But independent learning for intent classiﬁ-\ncation aims at capturing the global information of\nan utterance, not caring much about the details of\nspeciﬁc word. The character-level embeddings in-\ntroduced in our model bring very little hurt to inde-\npendent learning of intent detection, as a trade-off\nin performance between both criterion.\n4.5 Joint Learning\nWe compare our model against the following base-\nline models based on joint learning:\nMethods Error(%)\nRecursive NN (Guo et al., 2014) 4.60\nBoosting (Tur et al., 2010) 4.38\nBoosting + Simpliﬁed sentences\n(Tur et al., 2011)\n3.02\nAttention Enc-Dec (Liu and Lane,\n2016a)\n2.02\nOur Model 2.69\nTable 2: Results of independent training for intent de-\ntection in terms of error rate.\nMethods F1 Error(%)\nRecursive NN (Guo et al.,\n2014)\n93.22 4.60\nRecursive NN+Viterbi\n(Guo et al., 2014)\n93.96 4.60\nAttention Enc-Dec (Liu\nand Lane, 2016a)\n95.87 1.57\nAttention BiRNN (Liu\nand Lane, 2016a)\n95.98 1.79\nOur Model 96.52 1.23\nTable 3: Results of joint training for slot ﬁlling and\nintent detection.\n•Recursive NN:(Guo et al., 2014) employed\nrecursive neural networks for joint training of\ntwo tasks.\n•Recursive NN + Viterbi:(Guo et al., 2014)\napplied the Viterbi algorithm on Recursive\nNN to improve the result on slot ﬁlling.\n•Attention Enc-Dec: (Liu and Lane, 2016a)\nproposed Attention Encoder-Decoder (with\naligned inputs) which introduced context\nvector as the explicit aligned inputs at each\ndecoding step.\n•Attention BiRNN: (Liu and Lane, 2016a)\nintroduced attention to the alignment-based\nRNN sequence labeling model. Such atten-\ntion provides additional information to the in-\ntent classiﬁcation and slot label prediction.\nTable 3 compares our joint model with reported\nresults from previous works. We can see that our\nmodel achieves state-of-the-art results and outper-\nforms previous best result by 0.54% in terms of\nF1-score on slot ﬁlling, and by 0.34% in terms of\nerror rate on intent detection. This improvement is\nstatistically signiﬁcant. Besides, the joint learning\n3831\nMethods F1-Score Error(%)\nW/O char-embedding 96.30 1.23\nW/O self-attention 96.26 1.34\nW/O attention-gating 96.25 1.46\nFull Model 96.52 1.23\nTable 4: Feature ablation comparison of our proposed\nmodel on ATIS. slot ﬁlling and intent detection result\nare shown each row after after we exclude each feature\nfrom the full architecture\nachieves better results than separate learning. It\ncan be interpreted that the two tasks are highly cor-\nrelated and boost the performance each other. The\nslot ﬁlling task enables the model to learn more\nmeaningful representations which give more su-\npervisory signals for the learning of shared param-\neters. Similarly, intent is also useful to determine\nthe slot label.\n4.6 Ablation Study\nThe ablation study is performed to evaluate\nwhether and how each part of our model con-\ntributes to our full model. To further evaluate the\nadvances of our gating architecture for joint learn-\ning, we ablate some techniques used in our model.\nWe ablate three important components and con-\nduct different approaches in this experiment. Note\nthat all the variants are based on joint learning with\nintent-augmented gate:\n•W/O char-embedding, where no character\nembeddings are added to the embedding\nlayer. The embedding layer is composed of\nword embeddings only.\n•W/O self-attention, where no self-attention is\nmodelled after the embedding layer and in\nthe intent-augmented gating layer. The intent\ngate is computed by the output of BiLSTM\nand intent embedding.\n•W/O attention-gating, where no self-\nattention mechanism is performed in the\nintent-augmented gating layer. The gate is\ncomputed by the output of BiLSTM and\nintent embedding. But we still use the\nself-attention on top of embedding layer to\naugment the context information.\nTable 4 shows the joint learning performance of\nour model on ATIS data set by removing one mod-\nule at a time. We ﬁnd that all variants of our model\nperform well based on our gate mechanism. As\nlisted in the table, all features contribute to both\nslot ﬁlling and intent classiﬁcation task.\nIf we remove the self-attention from the holistic\nmodel or just in the intent-augmented gating layer,\nthe performance drops dramatically. The result\ncan be interpreted that self-attention mechanism\ncomputes context representation separately and\nenhances the interaction of features in the same as-\npect. We can see that self-attention does improve\nperformance a lot in a large scale, which is consis-\ntent with ﬁndings of previous work (Vaswani et al.,\n2017; Lin et al., 2017).\nIf we remove character-level embeddings and\nonly use word-level embeddings, we see 0.22%\ndrop in terms of F1-score. Though word-level em-\nbeddings represent the semantics of each word,\ncharacter-level embeddings can better handle the\nout-of-vocabulary (OOV) problem which is essen-\ntial to determine the slot labels.\n5 Conclusion\nIn this paper, we propose a novel self-attentive\nmodel gated with intent for spoken language un-\nderstanding. We apply joint learning on both\nintent detection and slot ﬁlling tasks. In our\nmodel, self-attention mechanism is introduced to\nbetter represent the semantic of utterance, and gate\nmechanism is introduced to make full use of the\nsemantic correlation between slot and intent. Ex-\nperiment results on ATIS dataset have shown efﬁ-\nciency of our model and outperforms the state-of-\nthe-art approach on both tasks. Besides, our model\nalso shows consistent performance gain over the\nindependent training models. In future works, we\nplan to improve our model by introducing extra\nknowledge.\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, et al.\n2016. Tensorﬂow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint\narXiv:1603.04467.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733.\n3832\nRenato De Mori. 2007. Spoken language understand-\ning: A survey. In Automatic Speech Recognition\n& Understanding, 2007. ASRU. IEEE Workshop on,\npages 365–376. IEEE.\nAnoop Deoras and Ruhi Sarikaya. 2013. Deep belief\nnetwork based semantic taggers for spoken language\nunderstanding. In Interspeech, pages 2713–2717.\nAllen L Gorin, Giuseppe Riccardi, and Jeremy H\nWright. 1997. How may i help you? Speech com-\nmunication, 23(1):113–127.\nDaniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey\nZweig. 2014. Joint semantic utterance classiﬁca-\ntion and slot ﬁlling with recursive neural networks.\nIn Spoken Language Technology Workshop (SLT),\n2014 IEEE, pages 554–559. IEEE.\nPatrick Haffner, Gokhan Tur, and Jerry H Wright.\n2003. Optimizing svms for complex call classiﬁca-\ntion. In Acoustics, Speech, and Signal Processing,\n2003. Proceedings.(ICASSP’03). 2003 IEEE Inter-\nnational Conference on, volume 1, pages I–I. IEEE.\nStefan Hahn, Marco Dinarelli, Christian Raymond,\nFabrice Lefevre, Patrick Lehnen, Renato De Mori,\nAlessandro Moschitti, Hermann Ney, and Giuseppe\nRiccardi. 2011. Comparing stochastic approaches\nto spoken language understanding in multiple lan-\nguages. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 19(6):1569–1583.\nDilek Hakkani-T ¨ur, G ¨okhan T ¨ur, Asli Celikyilmaz,\nYun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-\nYi Wang. 2016. Multi-domain joint semantic frame\nparsing using bi-directional rnn-lstm. In INTER-\nSPEECH, pages 715–719.\nCharles T Hemphill, John J Godfrey, George R Dod-\ndington, et al. 1990. The atis spoken language sys-\ntems pilot corpus. In Proceedings of the DARPA\nspeech and natural language workshop, pages 96–\n101.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nDan Jurafsky. 2000. Speech & language processing.\nPearson Education India.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu.\n2016. Leveraging sentencelevel information with\nencoder lstm for natural language understanding.\narXiv preprint.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130.\nBing Liu and Ian Lane. 2015. Recurrent neural net-\nwork structured output prediction for spoken lan-\nguage understanding. In Proc. NIPS Workshop\non Machine Learning for Spoken Language Under-\nstanding and Interactions.\nBing Liu and Ian Lane. 2016a. Attention-based recur-\nrent neural network models for joint intent detection\nand slot ﬁlling. arXiv preprint arXiv:1609.01454.\nBing Liu and Ian Lane. 2016b. Joint online spo-\nken language understanding and language model-\ning with recurrent neural networks. arXiv preprint\narXiv:1609.01462.\nGr´egoire Mesnil, Yann Dauphin, Kaisheng Yao,\nYoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-\naodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.\n2015. Using recurrent neural networks for slot ﬁll-\ning in spoken language understanding. IEEE/ACM\nTransactions on Audio, Speech and Language Pro-\ncessing (TASLP), 23(3):530–539.\nGr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua\nBengio. 2013. Investigation of recurrent-neural-\nnetwork architectures and learning methods for spo-\nken language understanding. In Interspeech, pages\n3771–3775.\nBaolin Peng, Kaisheng Yao, Li Jing, and Kam-Fai\nWong. 2015. Recurrent neural networks with exter-\nnal memory for spoken language understanding. In\nNatural Language Processing and Chinese Comput-\ning, pages 25–35. Springer.\nPatti J Price. 1990. Evaluation of spoken language sys-\ntems: The atis domain. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Hidden\nValley, Pennsylvania, June 24-27, 1990.\nSuman V Ravuri and Andreas Stolcke. 2015. Re-\ncurrent neural network and lstm models for lexical\nutterance classiﬁcation. In INTERSPEECH, pages\n135–139.\nChristian Raymond and Giuseppe Riccardi. 2007.\nGenerative and discriminative algorithms for spoken\nlanguage understanding. In Eighth Annual Confer-\nence of the International Speech Communication As-\nsociation.\nRobert E Schapire and Yoram Singer. 2000. Boostex-\nter: A boosting-based system for text categorization.\nMachine learning, 39(2-3):135–168.\nEdwin Simonnet, Nathalie Camelin, Paul Del ´eglise,\nand Yannick Est `eve. 2015. Exploring the use of\nattention-based recurrent neural networks for spo-\nken language understanding. In Machine Learning\nfor Spoken Language Understanding and Interac-\ntion NIPS 2015 workshop (SLUNIPS 2015).\n3833\nDavid Sussillo and LF Abbott. 2014. Random walk\ninitialization for training very deep feedforward net-\nworks. arXiv preprint arXiv:1412.6558.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nGokhan Tur, Dilek Hakkani-T ¨ur, and Larry Heck.\n2010. What is left to be understood in atis? In Spo-\nken Language Technology Workshop (SLT), 2010\nIEEE, pages 19–24. IEEE.\nGokhan Tur, Dilek Hakkani-T ¨ur, Larry Heck, and\nSarangarajan Parthasarathy. 2011. Sentence sim-\npliﬁcation for spoken language understanding. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2011 IEEE International Conference on , pages\n5628–5631. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nNgoc Thang Vu, Pankaj Gupta, Heike Adel, and Hin-\nrich Sch ¨utze. 2016. Bi-directional recurrent neural\nnetwork with ranking loss for spoken language un-\nderstanding. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2016 IEEE International Confer-\nence on, pages 6060–6064. IEEE.\nPuyang Xu and Ruhi Sarikaya. 2013. Convolutional\nneural network based triangular crf for joint in-\ntent detection and slot ﬁlling. In Automatic Speech\nRecognition and Understanding (ASRU), 2013 IEEE\nWorkshop on, pages 78–83. IEEE.\nKaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-\noffrey Zweig, and Yangyang Shi. 2014a. Spoken\nlanguage understanding using long short-term mem-\nory neural networks. In Spoken Language Technol-\nogy Workshop (SLT), 2014 IEEE, pages 189–194.\nIEEE.\nKaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong\nYu, Xiaolong Li, and Feng Gao. 2014b. Recurrent\nconditional random ﬁeld for language understand-\ning. In Acoustics, Speech and Signal Processing\n(ICASSP), 2014 IEEE International Conference on,\npages 4077–4081. IEEE.\nKaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,\nYangyang Shi, and Dong Yu. 2013. Recurrent neu-\nral networks for language understanding. In Inter-\nspeech, pages 2524–2528.\nSteve J Young. 2002. Talking to machines (statistically\nspeaking). In INTERSPEECH.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nXiaodong Zhang and Houfeng Wang. 2016. A joint\nmodel of intent determination and slot ﬁlling for\nspoken language understanding. In IJCAI, pages\n2993–2999.\nSu Zhu and Kai Yu. 2017. Encoder-decoder with\nfocus-mechanism for sequence labelling based spo-\nken language understanding. In Acoustics, Speech\nand Signal Processing (ICASSP), 2017 IEEE Inter-\nnational Conference on, pages 5675–5679. IEEE.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8657839298248291
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.7290186882019043
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7054561972618103
    },
    {
      "name": "Spoken language",
      "score": 0.6914454698562622
    },
    {
      "name": "Joint (building)",
      "score": 0.6552603840827942
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6083139181137085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.551966667175293
    },
    {
      "name": "Natural language processing",
      "score": 0.47385337948799133
    },
    {
      "name": "Representation (politics)",
      "score": 0.4321008324623108
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.4308796525001526
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4267652928829193
    },
    {
      "name": "Language model",
      "score": 0.4146493077278137
    },
    {
      "name": "Core (optical fiber)",
      "score": 0.41004669666290283
    },
    {
      "name": "Machine learning",
      "score": 0.299416184425354
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}