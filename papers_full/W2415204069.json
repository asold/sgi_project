{
  "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention",
  "url": "https://openalex.org/W2415204069",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2634307789",
      "name": "Sun Chengjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140434272",
      "name": "Lin Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1806612777",
      "name": "Wang Xiao-long",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2172888184",
    "https://openalex.org/W2173361515",
    "https://openalex.org/W2469057590",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1581407678",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2045254372",
    "https://openalex.org/W2951264799"
  ],
  "abstract": "In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.",
  "full_text": "arXiv:1605.09090v1  [cs.CL]  30 May 2016\nLearning Natural Language Inference using Bidirectional LSTM model and\nInner-Attention\nYang Liu, Chengjie Sun, Lei Linand Xiaolong Wang\nHarbin Institute of Technology, Harbin, P.R.China\n{yliu,cjsun,linl,wangxl}@insun.hit.edu.cn\nAbstract\nIn this paper, we proposed a sentence\nencoding-based model for recognizing text en-\ntailment. In our approach, the encoding of\nsentence is a two-stage process. Firstly, av-\nerage pooling was used over word-level bidi-\nrectional LSTM (biLSTM) to generate a ﬁrst-\nstage sentence representation. Secondly, at-\ntention mechanism was employed to replace\naverage pooling on the same sentence for bet-\nter representations. Instead of using target\nsentence to attend words in source sentence,\nwe utilized the sentence’s ﬁrst-stage represen-\ntation to attend words appeared in itself, which\nis called ”Inner-Attention” in our paper . Ex-\nperiments conducted on Stanford Natural Lan-\nguage Inference (SNLI) Corpus has proved\nthe effectiveness of ”Inner-Attention” mech-\nanism. With less number of parameters, our\nmodel outperformed the existing best sentence\nencoding-based approach by a large margin.\n1 Introduction\nGiven a pair of sentences, the goal of recogniz-\ning text entailment (RTE) is to determine whether\nthe hypothesis can reasonably be inferred from the\npremises. There were three types of relation in RTE,\nEntailment (inferred to be true), Contradiction (in-\nferred to be false) and Neutral (truth unknown).A\nfew examples were given in Table 1.\nTraditional methods to RTE has been the domin-\nion of classiﬁers employing hand engineered fea-\ntures, which heavily relied on natural language pro-\ncessing pipelines and external resources. Formal\nreasoning methods (Bos and Markert, 2005) were\nP The boy is running through a grassy area.\nThe boy is in his room. C\nH A boy is running outside. E\nThe boy is in a park. N\nTable 1: Examples of three types of label in RTE, where P\nstands for Premises and H stands for Hypothesis\nalso explored by many researchers, but not been\nwidely used because of its complexity and domain\nlimitations.\nRecently published Stanford Natural Language\nInference (SNLI1) corpus makes it possible to use\ndeep learning methods to solve RTE problems.\nSo far proposed deep learning approaches can be\nroughly categorized into two groups: sentence\nencoding-based models and matching encoding-\nbased models. As the name implies, the encoding of\nsentence is the core of former methods, while the lat-\nter methods directly model the relation between two\nsentences and didn’t generate sentence representa-\ntions at all.\nIn view of universality, we focused our efforts on\nsentence encoding-based model. Existing methods\nof this kind including: LSTMs-based model, GRUs-\nbased model, TBCNN-based model and SPINN-\nbased model. Single directional LSTMs and GRUs\nsuffer a weakness of not utilizing the contextual\ninformation from the future tokens and Convolu-\ntional Neural Networks didn’t make full use of in-\nformation contained in word order. Bidirectional\nLSTM utilizes both the previous and future context\nby processing the sequence on two directions which\nhelps to address the drawbacks mentioned above.\n1http://nlp.stanford.edu/projects/snli/\nP Multiplication H\nSoftMax \nY\nDifference \nBLSTM \nMean Pooling \nBLSTM BLSTM BLSTM \nSentence Vec \nBLSTM \nMean Pooling \nBLSTM BLSTM BLSTM \nSentence Vec \nInter-Attention \nimmersed pleasant photograph conversation involved headted discussion Canon \nPremise Hypothesis \nδAε\nSentence \nInput \nδBε\nSentence \nEncoding \nδCε\nSentence \nMatching \n② \n① \nFigure 1:Architecture of Bidirectional LSTM model with Inner-Attention\n(Tan et al., 2015)\nA recent work by (Rockt¨ aschel et al., 2015) im-\nproved the performance by applying a neural atten-\ntion model that didn’t yield sentence embeddings.\nIn this paper, we proposed a uniﬁed deep learning\nframework for recognizing textual entailment which\ndose not require any feature engineering, or external\nresources. The basic model is based on building biL-\nSTM models on both premises and hypothesis. The\nbasic mean pooling encoder can roughly form a intu-\nition about what this sentence is talking about. Ob-\ntained this representation, we extended this model\nby utilize an Inner-Attention mechanism on both\nsides. This mechanism helps generate more accurate\nand focused sentence representations for classiﬁca-\ntion. In addition, we introduced a simple effective\ninput strategy that get ride of same words in hypoth-\nesis and premise, which further boosts our perfor-\nmance. Without parameter tuning, we improved the\nart-of-the-state performance of sentence encoding-\nbased model by nearly 2%.\n2 Our approach\nIn our work, we treated RTE task as a supervised\nthree-way classiﬁcation problem. The overall archi-\ntecture of our model is shown in Figure 1. The de-\nsign of this model we follow the idea of Siamese\nNetwork, that the two identical sentence encoders\nshare the same set of weights during training, and\nthe two sentence representations then combined to-\ngether to generated a ”relation vector” for classiﬁ-\ncation. As we can see from the ﬁgure, the model\nmainly consists of three parts. From top to bottom\nwere: (A). The sentence input module; (B). The sen-\ntence encoding module; (C). The sentence matching\nmodule. We will explain the last two parts in detail\nin the following subsection. And the sentence input\nmodule will be introduced in Section 3.3.\n2.1 Sentence Encoding Module\nSentence encoding module is the fundamental part\nof this model. To generate better sentence repre-\nsentations, we employed a two-step strategy to en-\ncode sentences. Firstly, average pooling layer was\nbuilt on top of word-level biLSTMs to produce sen-\ntence vector. This simple encoder combined with\nthe sentence matching module formed the basic ar-\nchitecture of our model. With much less parame-\nters, this basic model alone can outperformed art-of-\nstate method by a small margin. (refer to Table 3).\nSecondly, attention mechanism was employed on\nthe same sentence, instead of using target sentence\nrepresentation to attend words in source sentence,\nwe used the representation generated in previous\nstage to attend words appeared in the sentence it-\nself, which results in a similar distribution with other\nattention mechanism weights. More attention was\ngiven to important words.2\nThe idea of ”Inner-attention” was inspired by the\nobservation that when human read one sentence,\npeople usually can roughly form an intuition about\nwhich part of the sentence is more important accord-\ning past experience. And we implemented this idea\nusing attention mechanism in our model. The atten-\ntion mechanism is formalized as follows:\nM = tanh(W yY + W hRave ⊗ eL)\nα = softmax (wT M)\nRatt = Y α T\nwhere Y is a matrix consisting of output vectors\nof biLSTM, Rave is the output of mean pooling\nlayer,α denoted the attention vector andRatt is the\nattention-weightedsentence representation.\n2.2 Sentence Matching Module\nOnce the sentence vectors are generated. Three\nmatching methods were applied to extract relations\nbetween premise and hypothesis.\n• Concatenation of the two representations\n• Element-wise product\n• Element-wise difference\nThis matching architecture was ﬁrst used by\n(Mou et al., 2015). Finally, we used a SoftMax layer\nover the output of a non-linear projection of the gen-\nerated matching vector for classiﬁcation.\n3 Experiments\n3.1 DataSet\nTo evaluate the performance of our model,\nwe conducted our experiments on Stanford\nNatural Language Inference (SNLI) corpus\n(Bos and Markert, 2005). At 570K pairs, SNLI\nis two orders of magnitude larger than all other\nresources of its type. The dataset is constructed\nby crowdsourced efforts, each sentence written\nby humans. The target labels comprise three\nclasses: Entailment, Contradiction, and Neutral\n2Recently, (Yang et al., 2016) proposed a Hierarchical At-\ntention model on the task of document classiﬁcation also used\nfor but the target representation in attention their mechanism is\nrandomly initialized.\n(two irrelevant sentences). We applied the standard\ntrain/validation/test split, containing 550k, 10k, and\n10k samples, respectively.\n3.2 Parameter Setting\nThe training objective of our model is cross-entropy\nloss, and we use minibatch SGD with the Rmsprop\n(Tieleman and Hinton, 2012) for optimization. The\nbatch size is 128. A dropout layer was applied in the\noutput of the network with the dropout rate set to\n0.25. In our model, we used pretrained 300D Glove\n840B vectors (Pennington et al., 2014) to initialize\nthe word embedding. Out-of-vocabulary words in\nthe training set are randomly initialized by sampling\nvalues uniformly from (0.05, 0.05). All of these em-\nbedding are not updated during training . We didn’t\ntune representations of words for two reasons: 1. To\nreduced the number of parameters needed to train.\n2. Keep their representation stays close to unseen\nsimilar words in inference time, which improved\nthe model’s generation ability. The model is imple-\nmented using open-source framework Keras.3\n3.3 The Input Strategy\nIn this part, we investigated four strategies to modify\nthe input on our basic model which helps us increase\nperformance, the four strategies are:\n• Inverting Premises (Sutskever et al., 2014)\n• Doubling Premises (Zaremba and Sutskever, 2014)\n• Doubling Hypothesis\n• Differentiating Inputs (Removing same words\nappeared in premises and hypothesis)\nExperimental results were illustrated in Table 2.\nAs we can see from it, doubling hypothesis and\ndifferentiating inputs both improved our model’s\nperformance.While the hypothesises usually much\nshorter than premises, doubling hypothesis may ab-\nsorb this difference and emphasize the meaning\ntwice via this strategy. Differentiating input strat-\negy forces the model to focus on different part of\nthe two sentences which may help the classiﬁcation\nfor Neutral and Contradiction examples as we ob-\nserved that our model tended to assign unconﬁdent\ninstances to Entailment. And the original input sen-\ntences appeared in Figure 1 are:\nPremise: Two man in polo shirts and tan pants im-\nmersed in a pleasant conversation about photograph.\n3http://keras.io/\nInput Strategy Test Acc.\nOriginal Sequences 83.24%\nInverting Premises 82.60%\nDoubling Premises 83.66%\nDoubling Hypothesis 82.83%\nDifferentiating Inputs83.72%\nTable 2:Comparison of different input strategies\nHypothesis:Two man in polo shirts and tan pants in-\nvolved in a heated discussion about Canon.\nLabel:Contradiction\nWhile most of the words in this pair of sentences\nare same or close in semantic, It is hard for model\nto distinguish the difference between them, which\nresulted in labeling it with Neutral or Entailment.\nThrough differentiating inputs strategy, this kind of\nproblems can be solved.\n3.4 Comparison Methods\nIn this part, we compared our model against the fol-\nlowing art-of-the-state baseline approaches:\n• LSTM enc: 100D LSTM encoders + MLP.\n(Bowman et al., 2015)\n• GRU enc: 1024D GRU encoders + skip-thoughts +\ncat, -. (Vendrov et al., 2015)\n• TBCNN enc: 300D Tree-based CNN encoders +\ncat,◦ , -. (Mou et al., 2015)\n• SPINN enc: 300D SPINN-NP encoders + cat,◦ , -.\n(Bowman et al., 2016)\n• Static-Attention:100D LSTM + static attention.\n(Rockt¨ aschel et al., 2015)\n• WbW-Attention: 100D LSTM + word-by-word at-\ntention. (Rockt¨ aschel et al., 2015)\nThe cat refers to concatenation,- and ◦ denote\nelement-wise difference and product, respectively.\nMuch simpler and easy to understand.\n3.5 Results and Qualitative Analysis\nAlthough the classiﬁcation of RTE example is not\nsolely relying on representations obtained from at-\ntention, it is still instructive to analysis Inner-\nAttention mechanism as we witnessed a large per-\nformance increase after employing it. We hand-\npicked several examples from the dataset to visual-\nize. In order to make the weights more discrimi-\nnated, we didn’t use a uniform colour atla cross sen-\ntences. That is, each sentence have its own color\natla, the lightest color and the darkest color de-\nnoted the smallest attention weight the biggest value\nModel Params Test Acc.\nSentence encoding-based models\nLSTM enc 3.0M 80.6%\nGRU enc 15M 81.4%\nTBCNN enc 3.5M 82.1%\nSPINN enc 3.7M 83.2%\nBasic model 2.0M 83.3%\n+ Inner-Attention 2.8M 84.2%\n+ Diversing Input 2.8M 85.0%\nOther neural network models\nStatic-Attention 242K 82.4%\nWbW-Attention 252K 83.5%\nTable 3:Performance comparison of different models on SNLI.\nwithin the sentence, respectively. Visualizations of\nInner-Attention on these examples are depicted in\nFigure 2.\nA Two Three \nmen women firefighter \nis climbing come \nriding on out \na a of \nmoto wooden subway \nscaffold station \nFigure 2:Inner-Attention Visualizations.\nWe observed that more attention was given to\nNones, Verbs and Adjectives. This conform to\nour experience that these words are more semantic\nricher than function words. While mean pooling re-\ngarded each word of equal importance, the attention\nmechanism helps re-weight words according to their\nimportance. And more focused and accurate sen-\ntence representations were generated based on pro-\nduced attention vectors.\n4 Conclusion and Future work\nIn this paper, we proposed a bidirectional LSTM-\nbased model with Inner-Attention to solve the RTE\nproblem. We come up with an idea to utilize\nattention mechanism within sentence which can\nteach itself to attend words without the information\nfrom another one. The Inner-Attention mechanism\nhelps produce more accurate sentence representa-\ntions through attention vectors. In addition, the sim-\nple effective diversing input strategy introduced by\nus further boosts our results. And this model can be\neasily adapted to other sentence-matching models.\nOur future work including:\n1. Employ this architecture on other sentence-\nmatching tasks such as Question Answer, Para-\nphrase and Sentence Text Similarity etc.\n2. Try more heuristics matching methods to make\nfull use of the sentence vectors.\nAcknowledgments\nWe thank all anonymous reviewers for their hard\nwork!\nReferences\n[Bos and Markert2005] Johan Bos and Katja Markert.\n2005. Recognising textual entailment with logical in-\nference. InProceedings of the conference on Human\nLanguage Technology and Empirical Methods in Natu-\nral Language Processing, pages 628–635. Association\nfor Computational Linguistics.\n[Bowman et al.2015] Samuel R Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D Manning. 2015.\nA large annotated corpus for learning natural language\ninference.arXiv preprint arXiv:1508.05326.\n[Bowman et al.2016] Samuel R Bowman, Jon Gauthier,\nAbhinav Rastogi, Raghav Gupta, Christopher D Man-\nning, and Christopher Potts. 2016. A fast uniﬁed\nmodel for parsing and sentence understanding.arXiv\npreprint arXiv:1603.06021.\n[Mou et al.2015] Lili Mou, Men Rui, Ge Li, Yan Xu,\nLu Zhang, Rui Yan, and Zhi Jin. 2015. Recogniz-\ning entailment and contradiction by tree-based convo-\nlution.arXiv preprint arXiv:1512.08422.\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. InEMNLP ,\nvolume 14, pages 1532–1543.\n[Rockt¨ aschel et al.2015] Tim Rockt¨ aschel, Edward\nGrefenstette, Karl Moritz Hermann, Tom´ aˇ s Koˇ cisk` y,\nand Phil Blunsom. 2015. Reasoning about en-\ntailment with neural attention.arXiv preprint\narXiv:1509.06664.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. 2014. Sequence to sequence learning with\nneural networks. InAdvances in neural information\nprocessing systems, pages 3104–3112.\n[Tan et al.2015] Ming Tan, Bing Xiang, and Bowen\nZhou. 2015. Lstm-based deep learning models\nfor non-factoid answer selection.arXiv preprint\narXiv:1511.04108.\n[Tieleman and Hinton2012] Tijmen Tieleman and Geof-\nfrey Hinton. 2012. Lecture 6.5-rmsprop.COURS-\nERA: Neural networks for machine learning.\n[Vendrov et al.2015] Ivan Vendrov, Ryan Kiros, Sanja\nFidler, and Raquel Urtasun. 2015. Order-\nembeddings of images and language.arXiv preprint\narXiv:1511.06361.\n[Yang et al.2016] Zichao Yang, Diyi Yang, Chris Dyer,\nXiaodong He, Alex Smola, and Eduard Hovy. 2016.\nHierarchical attention networks for document classiﬁ-\ncation. InProceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies.\n[Zaremba and Sutskever2014] Wojciech Zaremba and\nIlya Sutskever. 2014. Learning to execute.arXiv\npreprint arXiv:1410.4615.",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.8977504372596741
    },
    {
      "name": "Computer science",
      "score": 0.7641364932060242
    },
    {
      "name": "Pooling",
      "score": 0.6997487545013428
    },
    {
      "name": "Natural language processing",
      "score": 0.685738205909729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6646865606307983
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6367123126983643
    },
    {
      "name": "Inference",
      "score": 0.6253281831741333
    },
    {
      "name": "Encoding (memory)",
      "score": 0.6018989086151123
    },
    {
      "name": "Representation (politics)",
      "score": 0.5786600112915039
    },
    {
      "name": "Word (group theory)",
      "score": 0.5221648216247559
    },
    {
      "name": "Logical consequence",
      "score": 0.41219577193260193
    },
    {
      "name": "Linguistics",
      "score": 0.20963695645332336
    },
    {
      "name": "Machine learning",
      "score": 0.18649840354919434
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}