{
  "title": "How to train your stochastic parrot: large language models for political texts",
  "url": "https://openalex.org/W4406376809",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2514345034",
      "name": "Joseph T. Ornstein",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": null,
      "name": "Elise N. Blasingame",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A4291155534",
      "name": "Jake S. Truscott",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2514345034",
      "name": "Joseph T. Ornstein",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Elise N. Blasingame",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4291155534",
      "name": "Jake S. Truscott",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2777647957",
    "https://openalex.org/W4205599725",
    "https://openalex.org/W632139601",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W6839530460",
    "https://openalex.org/W3134427152",
    "https://openalex.org/W4362454468",
    "https://openalex.org/W6801731041",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4327519588",
    "https://openalex.org/W2111878847",
    "https://openalex.org/W4232496363",
    "https://openalex.org/W2973489200",
    "https://openalex.org/W1505278626",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2515855575",
    "https://openalex.org/W4385573966",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W2769252714",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3159574466",
    "https://openalex.org/W2893406222",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3170344956",
    "https://openalex.org/W2752422344",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W2095655043",
    "https://openalex.org/W3122807568",
    "https://openalex.org/W2753354158",
    "https://openalex.org/W3203374882",
    "https://openalex.org/W4283688409",
    "https://openalex.org/W2786356514",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4390144220",
    "https://openalex.org/W1605255363"
  ],
  "abstract": "Abstract We demonstrate how few-shot prompts to large language models (LLMs) can be effectively applied to a wide range of text-as-data tasks in political science—including sentiment analysis, document scaling, and topic modeling. In a series of pre-registered analyses, this approach outperforms conventional supervised learning methods without the need for extensive data pre-processing or large sets of labeled training data. Performance is comparable to expert and crowd-coding methods at a fraction of the cost. We propose a set of best practices for adapting these models to social science measurement tasks, and develop an open-source software package for researchers.",
  "full_text": "ORIGINAL ARTICLE\nHow to train your stochastic parrot: large language\nmodels for political texts\nJoseph T. Ornstein1\n , Elise N. Blasingame1 and Jake S. Truscott2\n1Department of Political Science, University of Georgia, Athens, GA, USA and2Department of Political Science, University\nof Florida, Gainesville, FL, USA\nCorresponding author:Joseph T. Ornstein; Email:jornstein@uga.edu\n(Received 12 September 2023; revised 23 July 2024; accepted 3 August 2024)\nAbstract\nWe demonstrate how few-shot prompts to large language models (LLMs) can be effectively applied to a\nwide range of text-as-data tasks in political science— including sentiment analysis, document scaling, and\ntopic modeling. In a series of pre-registered analyses, this approach outperforms conventional supervised\nlearning methods without the need for extensive data pre-processing or large sets of labeled training data.\nPerformance is comparable to expert and crowd-coding methods at a fraction of the cost. We propose a set\nof best practices for adapting these models to social science measurement tasks, and develop an open-\nsource software package for researchers.\nKeywords: document scaling; GPT-3; GPT-4; large language models; sentiment analysis; text-as-data; topic modeling\n1. Introduction\nA common task in political science research involves labeling documents to capture some latent\nquantity of interest. Whether it’s measuring the ideology expressed in party manifestos (Lowe\net al., 2011), the harshness of treaty provisions (Spirling, 2012), polarization in legislative\nspeeches (Peterson and Spirling,2018), partisan slant in television news coverage (Martin and\nMcCrain, 2019), political sophistication in State of the Union addresses (Benoitet al., 2019), opi-\nnions expressed in city council meeting minutes (Einsteinet al., 2019), or countless other exam-\nples, so much of modern political science would be impossible without quantitative measures\nderived from unstructured text. Until quite recently, however, the process of reading and coding\ndocuments has been a task uniquely suited to human researchers. One of the most exciting new\ndevelopments in our field has been the explosion of methods for automating this process, meth-\nods we broadly call“text-as-data” (Grimmer and Stewart,2013).\nThese efforts in political science have occurred alongside a parallel revolution in computer sci-\nence, developing natural language processing tools to usefully interpret human speech. The appli-\ncations for these methods are numerous— including chat bots, Internet search, auto-complete,\nand virtual assistants— but remarkably, much of this research has begun to converge on a single\nsolution: large, pretrained language models built on a neural network architecture called the\ntransformer (Vaswani et al., 2017; Bommasani et al., 2021). Such models have quickly transi-\ntioned from academic curiosity to cultural phenomenon following the release of OpenAI’s\nGPT-3 and GPT-4 (Brownet al., 2020).\nIn this paper, we demonstrate that adapting these large language models (LLMs) to political\ntext-as-data tasks can yield significant gains in performance, cost, and capabilities. Across a\n© The Author(s), 2025. Published by Cambridge University Press on behalf of EPS Academic Ltd. This is an Open Access article, distributed\nunder the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-\nuse, distribution and reproduction, provided the original article is properly cited.\nPolitical Science Research and Methods(2025), 13, 264–281\ndoi:10.1017/psrm.2024.64\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nrange of applications— including sentiment classification, ideology scaling, and topic modeling—\nwe show that carefully structured prompts to LLMs (“few-shot prompting”) reliably outperforms\nexisting automated text classification methods, and produces results comparable to human\ncrowd-coders at a small fraction of the cost.\nLLMs are deep learning models (LeCunet al., 2015) trained on“next-word” prediction tasks.\nWhen provided with a sequence of text, the model generates a probability distribution over the\nmost likely words to follow that sequence.1 For example, when prompted with the phrase“Thank\nyou. Have a nice,” GPT-3 estimates that there is an 88.6 percent probability the next word will be\n“day,” a 2.4 percent probability the next word will be“weekend,” a 1.9 percent probability it will\nbe “evening,” and so on.\nThough these models are essentially“stochastic parrots” mimicking human speech patterns\nobserved in their training corpus (Benderet al., 2021), they have demonstrated a number of surprising\nemergent capabilities thatthey were not deliberately designed to do. By carefully crafting its input, one\ncan adapt a language model to perform a variety of text-as-data tasks. Consider the following prompt,\nwhich reformulates a sentiment classification task as a next-word prediction problem:\nDecide whether a Tweet’s sentiment is positive, neutral, or negative.\nTweet: Congratulations to the SCOTUS. American confidence in the\nSupreme Court is now lower than at any time in history. Well done!\nSentiment:\nConventional approaches to sentiment classification struggle to accurately label texts like these,\nwhich use positive words to convey a negative sentiment (e.g.,“congratulations,”“confidence,”\n“supreme,”“ well done!”). But when supplied with this prompt, GPT-3 estimates a 77 percent\nprobability that the next word will be“Negative.” GPT-4’s estimate is even more confident,\nreturning “Negative” with 99 percent probability. In what follows, we show that such probability\ndistributions can be used to construct continuous measures for a variety of latent document char-\nacteristics, and across several applications we validate this approach by converting several com-\nmon political text-as-data tasks into next-word prediction problems.\nThis approach to modeling is fundamentally different than the one familiar to most political\nscientists. Rather than fitting a separate model for each research question (“one-to-one”), the\nresearcher takes a single pretrained language model and adapts it to several different tasks\n(“one-to-many”). The promise of this approach lies in the scale and complexity that a single pre-\ntrained LLM can offer. GPT-3 is a deep neural network composed of 175 billion parameters,\ntrained on hundreds of billions of words of text from the Internet and digitized books (Brown\net al., 2020). Although less is publicly known about the architecture of GPT-3’s successor,\nGPT-4, it is rumored to have roughly 1.7trillion parameters and cost over $100 million to\nbuild. Because such models are orders-of-magnitude more complex than an individual political\nscientist could train, there is ample reason to believe that, for certain tasks, LLMs can outperform\n“bespoke” models trained for a specific purpose. Furthermore, the approach does not require the\nresearcher to construct a large labeled dataset to train the model. One can adapt pretrained LLMs\nto many document labeling tasks by including just a handful of labeled examples in the body of\nthe prompt, an approach known as few-shot prompting (Brownet al., 2020).\nThe paper proceeds as follows. In the next section, we describe the basic architecture of these\nmodels, and how they make use of innovations like self-supervised learning and contextualized\nword embeddings to generate predictions. In Section3, we describe four applications of the mod-\nels to common political text-as-data tasks, including sentiment analysis of social media posts,\nclassifying the tone of political advertisements, ideology scaling on party manifestos, and topic\n1Technically, LLMs like GPT-3 and GPT-4 represent text as“tokens,”strings of roughly four characters, rather than words,\nbut for illustrative purposes we can think of it as operating at the word level.\nPolitical Science Research and Methods 265\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nmodeling of US Congressional floor speeches. Based on lessons learned from these applications,\nwe propose a set of best practices for prompt design and develop an open-source software pack-\nage to implement our suggested approach.2 We conclude with a discussion on whether the per-\nformance gains we document are worth the potential dangers associated with unprincipled use of\nthese models (Strubell et al., 2019; Abid et al., 2021; Bender et al., 2021; Spirling, 2023).\nThroughout, we emphasize the importance of “validation, validation, validation” (Grimmer\nand Stewart, 2013), repeatedly comparing model outputs against human judgment to ensure\nthey are measuring what we want them to measure.\n2. Large language models\nIn this section, we highlight two key features of LLMs that make them particularly promising for\nsocial science applications. The first isself-supervised learning, which permits these models to be\ntrained on an unprecedentedly large corpus of text data. The second iscontextualized word embed-\ndings, which allow the models to flexibly represent the meaning of words depending on their context.\n2.1 Self-supervision\nA central difficulty for supervised learning methods in text analysis is the need to collect and\nannotate large amounts of training data (Wilkerson and Casas,2017). If, for example, a researcher\nwants to train a model to predict the tone of political advertisements, they must first compile a\ndataset with thousands of labeled political ads. The process of hand-labeling these data can be\nexpensive and time-consuming, even with the help of non-expert crowd-sourced approaches\n(Benoit et al., 2016; Carlson and Montgomery,2017).\nBy contrast, aself-supervised learning task is one in which the target prediction is provided\nwithin the data itself, rather than hand-labeled by a researcher. One reason why LLMs like\nGPT-3 and GPT-4 are trained to perform next-word prediction is that such training can be com-\npleted in a self-supervised fashion. Every sentence of text that a human has ever written can be\nsplit into a sequence of tokens and used to train the model, which permits a massive expansion in\nthe amount of training data available. Rather than training a supervised learning model just on a\nfew thousand documents, LLMs are trained on hundreds of billions of words scraped from the\nInternet and a digitized corpus of books.\nWhen combined with rapid improvements in computer hardware, this dramatic increase in the\nquantity of training data has allowed computer scientists to build increasingly complex language\nmodels over the past five years. Bommasaniet al. (2021) coin the term“foundation models” to\ndescribe this new class of general-purpose language models, because they can be adapted to per-\nform a variety of natural language processing tasks that they were not explicitly trained to do.\n2.2 Contextualized word embeddings\nIn order to analyze“text-as-data,” one must first decide how to represent a text numerically.\nConventional “bag of words” approaches (Grimmer et al., 2022) represent each document as a\nvector of word frequencies. The main drawback of this representation is that it assumes each\nword has a unique meaning. Mathematically, the document-feature matrix is extremelysparse;\nthere are hundreds of thousands of unique words, but many of them have overlapping meaning.\nTo overcome this problem, LLMs like GPT-3 represent each word in a document as a high-\ndimensional vector, an approach known as“word embeddings.” This representation attempts to\nretain information about themeaning of words by encoding how often a word is used in the vicinity\n2The promptr package in R is available for download through the Comprehensive R Archive Network (CRAN).\nJoseph T. Ornstein et al.266\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nof other related words— motivated by John Firth’s linguistic maxim,“you shall know a word by the\ncompany it keeps” (Firth, 1957; Rodriguez and Spirling,2022). For example, in a sufficiently large\ncorpus of text, you will discover that the word“cat”appears more frequently than you would expect\nby chance near words like“litter,”“ yarn,”“ claws,” etc. You’d also find that the word“kitten”\nappears more frequently than you would expect by chance near those words. A word embedding,\nincorporating this information, represents“cat” and “kitten” with vectors that are close together in\nspace. By training word embeddings on a large corpus of texts, one can transfer knowledge about\nthe meaning of words from a more general corpus to a specific text-as-data problem.\nBut this approach to representing meaning can still fall short, because there are many words\nwhose meaning is ambiguous without context. Consider, for instance, the word“bill,” which\ncould have one of several meanings, depending on whether it is preceded by the phrase“signed\nthe… ,”“foot the… ,” or “Hillary and… .” Other common words, like the pronouns“it” or “they,”\nare entirely meaningless without context. For such words, a single pretrained word embedding is\nunlikely to capture meaning very well.\nWhen humans are interpreting words in a sentence, we start with our“pretrained”idea of what a\nword means, then adjust that interpretation on the fly as we read the word in context (like you just\ndid with the word“fly”). This is the insight behindcontextualized word embeddings, which allow a\nword’s vector representation to change depending on what words precede or follow it. LLMs like\nGPT-3 and GPT-4 are built on a neural network architecture called the transformer (Vaswaniet al.,\n2017). The transformer model takes as its input a sequence of word embeddings, and outputs an\nembedding vector representing the most likely next word in the sequence. The key innovation of\nthese models is the inclusion of many hidden layers of“self-attention,” which recompute each\nword’s embedding as a weighted average of nearby word embeddings in the sequence. This allows\nthe model to flexibly represent the meaning of words based on their context.\nBuilding on the transformer architecture, there has been rapid progress in natural language pro-\ncessing over the past ten years. In 2016, the best-performing language model scored an F (59.3 per-\ncent) on the 8th grade New York Regents Science Exam. By 2019, an LLM based on the transformer\narchitecture scored 91.6 percent (Clarket al., 2021). By 2023, models like OpenAI’s GPT-4 were\noutscoring 90 percent of human test takers on exams as challenging as the SAT and Uniform\nBar Examination (Katzet al., 2023). For political scientists, the practical advantage of adapting\nthese models is that, by better representing the nuance and ambiguity of political speech, they\ncan outperform existing“bag of words”methods at classifying, measuring, and discovering patterns\nin political texts. We turn next to a few examples of this approach in practice.\n3. Applications\nWe assess the performance of LLM prompts on a set of common political science text-as-data\ntasks, including sentiment analysis, ideology scaling, and topic modeling. These applications\ndemonstrate the range of tasks that a single pretrained language model is capable of performing.\nIn our first application, we classify the sentiment of a novel set of social media posts related to US\nSupreme Court rulings, comparing classifications from GPT-3 and GPT-4 against other auto-\nmated methods for sentiment analysis. We demonstrate that the LLMs produce superior mea-\nsures of sentiment, particularly for texts whose meaning is ambiguous without understanding\nthe political context in which they were written. Next, we classify the tone of American political\nads from Carlson and Montgomery (2017), comparing the performance of LLM classifications\nagainst crowds of human coders. Our third application replicates the ideology scaling of political\nmanifestos conducted by Benoitet al.(2016) via crowd-coding. And for our final application we\nassign topic labels to 9704 one-minute floor speeches from the US House of Representatives\n(Wilkerson and Casas,2017), demonstrating that LLMs can serve as a useful tool for discovery\nas well as classification. These four applications provide a varied set of tasks and contexts with\nwhich to evaluate the performance of this approach.\nPolitical Science Research and Methods 267\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nFor each application, we pre-registered an analysis plan for how we would adapt the LLMs to\ndocument classification and scaling tasks.3 We took this step to ensure that we do not overstate\nthe performance of our approach by iteratively refining the model in search of the best fit. We\ndescribe the design, approach, and outcomes for each application in the following subsections.\n3.1 Sentiment analysis of political tweets\nClassifying sentiment on social media is a notoriously difficult problem for computational meth-\nods. Dictionary-based methods, which measure sentiment by counting the frequency of positive\nand negative words, tend to perform poorly when faced with text where positive words imply\nnegative sentiment (“thanks for nothing,”“ smooth move,”“ way to go, genius”) and negative\nwords imply positive sentiment (“that was wicked/sick/demented!”). And conventional super-\nvised learning methods using a bag of words approach— even when trained on millions of social\nmedia posts— can at best correctly classify the sentiment of a test set roughly 80 percent of the\ntime (Go et al., 2009). In such an environment, the ability of contextualized word embeddings\nto flexibly adjust their representation of a word in response to its context can be quite beneficial,\nand models based on the transformer architecture have rapidly become the state-of-the-art in\nsocial media sentiment analysis (Camacho-Colladoset al., 2022; Widmann and Wich,2022).\nFor our first application, we compare measures of sentiment produced by LLM prompts against\nthree automated methods for sentiment classification broadly familiar to political scientists. The\nfirst is a dictionary-based method, which classifies sentiment based on counts of words associated\nwith positive or negative sentiment. The second is a supervised learner (Naive Bayes), trained on a\nbag of words representation. The third is a transformer model (RoBERTa) fine-tuned for sentiment\nclassification of Twitter posts (Camacho-Colladoset al., 2022). This application illustrates a core\nstrength of the few-shot prompt approach: it improves performance in cases where accurately clas-\nsifying a document requires knowledge of the political context in which it was written.\nWe collect a novel dataset of 945 Twitter posts (“tweets”) that reference the United States\nSupreme Court within 24 hours of two controversial opinions:Masterpiece Cakeshop, Ltd. v.\nColorado Civil Rights Commission(2018), as well as the Court’s concurrently released opinions\nin Trump v. Mazars and Trump v. Vance(2020). We chose these cases to reflect a diverse set\nof users and political issues, including anti-discriminatory practices toward same-sex couples,\nreligious liberties for private business owners, and the legal immunity of Donald Trump as\nboth the president and a private citizen. For each tweet in this dataset, we created an author-\nlabeled sentiment score through a two-stage manual coding procedure. The three authors\nbegan by independently labeling a set of 1000 tweets as Positive, Negative, or Neutral. From\nthat original set, we excluded 55 tweets that were unrelated to the US Supreme Court decisions,\nand conducted a second round of manual labeling for any tweets where at least two authors dis-\nagreed about the direction of sentiment. The result is a 7-point measure of sentiment ranging\nfrom −1 (all authors agreed the tweet was negative) to +1 (all authors agreed the tweet was\npositive).4\nFor many tweets in this dataset, it would be difficult to accurately classify sentiment without\nunderstanding the context in which they were written. Consider the following example, posted in\nresponse to the Court’s decision inMasterpiece Cakeshop (2018):\n‘‘Way to go SCOTUS! You really celebrated PRIDE Month.’’\nAll three authors agreed that this was a sarcastic remark expressing negative sentiment about\nthe Court’s decision, but reaching that conclusion required knowing that in itsMasterpiece opin-\nion, the Supreme Court ruled in favor of a baker who refused to bake a wedding cake for a same-\n3See AsPredicted document numbers 92341, 92422, 92666, 100718, and 125217.\n4Inter-coder reliability as measured by Fleiss’kappa was 0.72, and at least two authors agreed on the label for every tweet.\nJoseph T. Ornstein et al.268\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nsex couple. Without this knowledge, even a high-performing sentiment analysis model would\nincorrectly classify the tweet as positive.\nA principal advantage of the LLM approach is that one can provide this context to the model\nwithin the prompt itself, rather than having to train or fine-tune a new model. For tweets referen-\ncing theMasterpiece decision, we provided GPT-3 and GPT-4 with the prompt shown inFigure 1.\nThe structure of this prompt contains a few important components. First, the prompt includes\na set of instructions describing the classification task and any necessary context, in much the\nsame way that one would brief a human research assistant. Next, the prompt can include one\nor more completed examples. The prompt in Figure 1 is known as a “few-shot” prompt\n(Brown et al., 2020), because it provides several examples of an appropriate response before pro-\nviding the text to be classified. When designing prompts, a researcher must decide how many\n(and which) examples to include.\nPrior to conducting our analysis, we pre-registered the text of two prompts, one to classify\ntweets collected after theMasterpiece Cakeshopdecision (Figure 1) and the other to classify tweets\nfollowing the Mazars and Vance decisions (see Supplementary materials for the full text of the\nsecond prompt). Both prompts include a brief set of instructions describing the Supreme\nCourt’s ruling, followed by six few-shot example completions. Each example was drawn from\nthe set of tweets that the authors unanimously coded— two positive, two neutral, and two negative\nto avoid biasing the model toward a particular classification (Zhaoet al., 2021). This approach to\nprompt design— modifying the prompt with different preambles and examples depending on\ncontext— outperforms every other method we attempted.5\nFor each prompt, the LLM outputs the probabilities that the subsequent word will be Positive,\nNegative, or Neutral. From this probability vector we construct a continuous measure of senti-\nment for every tweet in the dataset (per our pre-registration protocol, we take the first component\nof a principal component analysis). The resulting measure of sentiment is strongly correlated with\nour hand-coded measure, as illustrated inFigure 2. GPT-3 correctly predicts whether a tweet was\nnegative or positive in 88.4 percent of cases. For comparison, the TweetNLP model— a RoBERTa\ntransformer model fine-tuned for Twitter sentiment analysis— correctly classifies 64.3 percent\nof these tweets. The best dictionary-based method we could construct only classifies 38.3 percent\nof the tweets correctly, and a Naive Bayes classifier trained on 1.2 million tweets from the Goet al.\n(2009) dataset classified 57.7 percent correctly— barely better than a coin flip. See Appendix C for\na detailed description of how we trained these alternative sentiment classifiers.\nSurprisingly, the latest generation of OpenAI language models (GPT-4) performs slightly\nworse on this task than few-shot prompts to GPT-3. As the figure makes clear, estimates from\nGPT-4 are strongly correlated with the author-coded labels, but the estimated probabilities\ntend to be poorly calibrated and overconfident. For over 70 percent of these social media\nposts, GPT-4 returns an estimated probability greater than 99 percent for a single sentiment\nlabel. It is also substantially more likely to return a“Neutral” sentiment label than the authors.\nBecause models like GPT-4 are optimized for chat-based applications through a process called\nReinforcement Learning with Human Feedback (Ouyanget al., 2022), the probability distribu-\ntions they return are not necessarily well-calibrated for next-word prediction. We discuss the\nimplications of this finding in more detail in Section4.\nTo see why the LLM classifiers so dramatically outperformed other automated methods\nof sentiment classification, considerTable 1, which presents a sample of tweets from the dataset.\nThe sentiment of each of these tweets is ambiguous without knowledge of the political context in\nwhich they were written. For a reader familiar with theMasterpiece Cakeshop decision, the first\n5In the Supplementary materials (Appendix A), we experiment with zero-shot prompts (no labeled examples) and one-\nshot prompts (one labeled example), as well as“default” prompts that do not provide context about the Supreme Court\ncases. As expected, providing the model with context in the preamble significantly improves performance, as does providing\nmore few-shot examples.\nPolitical Science Research and Methods 269\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\ntwo tweets are obviously sarcastic statements reflecting disappointment with the ruling. And for a\nreader familiar with theMazars and Vance rulings, the third and fourth tweets appear express a\npositive sentiment regarding the outcome. Few-shot prompting’s ability to incorporate this infor-\nmation puts it at an advantage over conventional sentiment classification methods.\nThe strongest test of the approach, however, is not whether it outperforms other automated\nmethods, but whether it can perform at the level of non-expert human coders. This is the\nfocus of our next two applications.\n3.2 Political ad tone\nCrowd-sourced text analysis is one of the fastest, most reliable methods for manually coding texts,\nleveraging the “wisdom of crowds” to generate measures from a large collection of non-expert\njudgments (Surowiecki, 2004; Benoit et al., 2016). By asking human coders to conduct a series\nof pairwise comparisons (e.g., “which of these tweets is more negative? ”), Carlson and\nMontgomery (2017) show that a researcher can quickly generate measures of sentiment that\nstrongly correlate with expert judgments.\nNevertheless, this approach has several shortcomings. First, it requires the researcher to screen,\ntrain, and monitor crowd-workers to ensure attentiveness and inter-coder reliability. Second, it can\nbe quite costly. To measure the tone of 935political ads, Carlson and Montgomery (2017)r e q u i r e d\n9420 pairwise comparison tasks at 6 cents per task, for a total cost of $565.20. Although Amazon’s\nMechanical Turk (MTurk) is currently the most economical alternative on the market, that reduced\ncost is borne by the coders completing the tasks. Studies suggest that people performing“human\nintelligence tasks” on sites likeMTurk, CrowdFlower, Clickworker,a n dToluna earn a median hourly\nwage of roughly $2/h, and only 4 percent earn more than the US federal minimum wage of $7.25\n(Hara et al., 2018). As a result, crowd-workers have an incentive to quickly complete as many\ntasks as possible, which can undermine the quality of crowd-sourced measures. Unsurprisingly, as\nLLMs have become more ubiquitous, many crowd workers have begun to rely on them to enhance\nFigure 1. LLM prompt for sentiment classification task.\nJoseph T. Ornstein et al.270\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\ntheir productivity. In a recent study, researchersestimated that between one-third and one-half of\ncrowd workers used ChatGPT to complete a text summarization task (Veselovskyet al., 2023).\nIn this application, we explore whether the few-shot LLM approach can reproduce Carlson and\nMontgomery’s (2017) crowd-sourced measure of political ad tone, using the one-shot prompt in\nFigure 3. As in the tweet sentiment application, we construct a continuous measure of tone from\nthe model’s estimated probability vector.\nFigure 2. Classification performance on Twitter sentiment task, comparing the few-shot LLM approach (GPT-3 and GPT-4),\nRoBERTa fine-tuned for Twitter sentiment classification (TweetNLP), dictionary-based sentiment analysis, and a supervised\nlearning method (Naive Bayes).\nPolitical Science Research and Methods 271\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nCoding the 935 political ads from Carlson and Montgomery (2017) took less than 1 minute\nand cost $0.60— a nearly 1000-fold reduction in cost compared to crowd-coding.6 And yet the\nresulting measure of ad tone was just as strongly correlated with expert ratings, as illustrated\nin Figure 4.\nOur measure diverges from the expert ratings in one of two situations. First, there are some ads\nin the dataset that are quite negative in tone, but the expert coders classify them as positive\nbecause they do not attack aspecific opponent (focusing instead on“typical Washington politi-\ncians” or “Republicans”). Second, some ads require contextual knowledge to accurately classify.\nTable A3 in the Appendix provides some examples of ads where our approach and the experts\ndisagreed. The first ad requires contextual knowledge about Susan Collins’failure to keep a cam-\npaign promise, context which is not provided in the ad text nor our prompt instructions. The\nsecond ad is somewhat negative in tone, but the target is“the other side,” so is not labeled an\nattack ad by the expert coders. The final ad— illustrated by the point in the lower right corner\nof Figure 4— is arguably miscoded by the experts.\nTable 1. Sample of tweets where sentiment is ambiguous absent political context\nTweet Authors LLMs Dictionary\nNaive\nBayes TweetNLP\nWay to go SCOTUS! You really celebrated PRIDE Month. Negative Negative Positive Positive Positive\nHappy Monday to everyone except the Supreme Court! Gay\npeople deserve cakes to be made for them too!!!!!!\nNegative Negative Positive Positive Positive\n#SCOTUS reaffirms @realDonaldTrump is not above the law! Positive Positive NA Negative Negative\nInject Donald Trump’s tax returns directly into my veins.\n#SCOTUS\nPositive Positive NA Positive Negative\nFigure 3. LLM prompt for political ad tone task.\n6This cost is based on OpenAI’s per-token pricing schedule as of April 2024. For more discussion on the likely trends in\ncosts for LLMs relative to human crowd-coders, see the Conclusion.\nJoseph T. Ornstein et al.272\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\n3.3 Ideology scaling\nOur third application is a document scaling task, designed to assess whether LLM prompts can\naccurately place the ideology of political texts on a continuous scale. The approach we take is, in\nessence, to treat the LLM as if it were a non-expert human coder, replicating the procedure for\ncrowd-sourcing party manifesto positions from Benoitet al. (2016). This allows us to validate\nour approach against an extensive set of crowd-coded classifications for 18,263 sentences from\n18 British party manifestos written between 1987 and 2010. By replicating these results, we can\nalso test whether the model can be adapted to a very different context than the bulk of its training\ndata, both geographically (Britain instead of the United States) and temporally (up to 35 years ago).\nWe adhere to the crowd-coding procedure from Benoitet al.(2016) as closely as possible, first\nsplitting the manifestos into their component sentences. We then classify the policy content of\neach sentence using the one-shot“Policy Prompt” in Figure 5. For any sentences that refer pri-\nmarily to social policy or economic policy, we classify their ideology on a three-point scale using\nFigure 4. Comparing crowd-coded, GPT-3, and GPT-4 estimates to expert-coded political ad tone (Carlson and\nMontgomery, 2017).\nPolitical Science Research and Methods 273\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nthe one-shot ideology prompt inFigure 5. As in the sentiment classification tasks, GPT-3 outputs\na probability distribution of next-word predictions. From these, we assign each sentence an ideol-\nogy score equal to the model’s estimated Conservative probability minus its estimated Liberal\nprobability. We aggregate these scores to the manifesto level by taking the average score for eco-\nnomic policy passages and the average score for social policy passages.\nTo generate our GPT-4 measures, we break from our pre-registered protocol, adopting the\napproach described in Le Mens and Gallego (2023), which explicitly prompts GPT-4 for a con-\ntinuous measure of ideology. The advantage of this approach is that it can utilize GPT-4’s larger\ncontext window— the model can generate predictions from inputs with over 100,000 tokens,\nroughly three times the length of the average manifesto in our corpus— to produce estimates with-\nout having to first split each manifesto into its component sentences. As in the first two applica-\ntions, however, the measures produced by GPT-4 are little or no better than those we obtain using\nGPT-3. See Appendix B for more details.\nFigure 6plots the performance of the crowd-coded estimates (top panel) and the GPT-3 estimates\n(bottom panel). Our estimates are more strongly correlated with expert ratings on the economic pol-\nicy dimension (ρ = 0.92) than the social policy dimension (ρ = 0.8), and are better at capturing\nbetween-partyvariance thanwithin-partyv a r i a n c e( t h o u g hn o t et h a tt h i si st r u ef o rt h ec r o w d - c o d e d\nmeasure as well). Despite its limitations, the GPT-3 approach yields estimates that correlate strongly\nwith human-coded measures at a small fraction of the cost. Crowd-coding 18,263 manifesto sentences\ncost Benoitet al.(2016) approximately $3226.7 By comparison, the GPT-3 estimates cost approxi-\nmately $2.50 at current prices. This has enormous practical implications, as it allows researchers to\nscale a substantially larger corpus of documents using LLMs than they could with human coders.\nFigure 5. LLM prompt for ideology scaling task.\n7Assuming a cost of 1.5 cents per sentence, and a total of 215,107 crowd evaluations.\nJoseph T. Ornstein et al.274\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\n3.4 Topic modeling\nAs useful as these models are for measurement and classification, they may hold even more\npromise as a tool for discovery (Grimmeret al., 2021). Often a researcher will approach a new\ncorpus of documents without a preconceived notion about how to partition them into categories.\nFigure 6. Performance of crowd-coded (top panel) and GPT-3 (bottom panel) ideology estimates, compared to expert\nscores.\nPolitical Science Research and Methods 275\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nFor any given corpus, there is an unfathomably large number of possible partitions, and statistical\nmodels can aid in the process of discovering interesting ones. A workhorse approach for this type\nof topic modelingis latent Dirichlet allocation, or LDA (Bleiet al., 2003). Each word in each docu-\nment is assumed to be drawn from one ofk topics, where the value of k is chosen by the\nresearcher. Each topic is represented by a vector of term probabilities, and each document is\nassigned a set of weights (summing to 1) representing the mixture of topics contained in the\ndocument. LDA searches for a set of topics and document assignments that maximizes the like-\nlihood of generating the observed“bag of words.”\nThis approach to topic modeling has three significant drawbacks. First, it requires the\nresearcher to have a large corpus of text with which to train the model. LDA is an unsupervised\nlearning technique, so unlike supervised learning models it does not require large amounts of\nhand-labeled training data. Nevertheless, it performs better with more data, so that the model\ncan identify the most common terms in each topic cluster. A researcher could not effectively\nuse LDA, for example, to classify the topics of fifty newspaper articles; one would first need thou-\nsands of newspaper articles to effectively train the model.\nSecond, interpreting a fitted LDA model requires a fair amount of subjective judgment. The\ntopics generated by LDA are unlabeled, and the researcher must make sense of them by compar-\ning the most probable words in each topic against those from other topics. While there are prom-\nising methods for crowd-sourcing this judgment task (Yinget al., 2021), they require additional\ntime, cost, and considerations involving crowd-worker recruitment, training, and monitoring.\nThird, a researcher fitting an LDA model has little control over thekinds of topics they would\nlike to explore in the data. A given corpus might have a large number of sensible ways to partition\nthe document space, but LDA only produces one— the partition that maximizes the likelihood of\nthe observed document-term matrix. For example, the dataset we explore in this application\ncomes from Wilkerson and Casas (2017), who fit a series of LDA models to identify the topics\nfrom 9704 one-minute floor speeches by members of the US House of Representatives during the\n113th Congress (2013–2014). Based on reporting from the Congressional Research Service, we\nknow that Congress members use these speeches as an opportunity to highlight legislation,\nthank colleagues and constituents, give truncated eulogies, and express policy positions\n(Schneider, 2015). Though Wilkerson and Casas (2017) focus their analysis on partisan differ-\nences in substantive topics (e.g., education, defense, agriculture, etc.), one might imagine a\nlarge number of other interesting ways to categorize the speeches. For instance, many of the\nfloor speeches are dedicated to honoring a constituent or organization for some achievement.\nOne sensible partition would be to categorize speeches by the type of person being honored.\nAnother would be to categorize the type of action being honored, or the virtues being praised.\nBecause LDA represents documents as a bag of words, it is unable to distinguish between\nthese different kinds of meaning.\nBy contrast, an LLM can be flexibly adapted to discover many different sorts of topics, just by\nchanging the prompt instructions. To demonstrate, we provide the prompt inFigure 7to GPT-3\nfor each of 9565 speeches8 from the Wilkerson and Casas (2017) dataset. We are interested in\nexploring whether there are partisan differences in the set of“virtues” that are praised in these\nspeeches. Consistent with Moral Foundations Theory (Grahamet al., 2009), we might expect con-\nservatives to emphasize virtues like loyalty, patriotism, and hard work in their speeches, while\nliberals would be more likely to emphasize fairness, compassion, and charity. The approach is\nanalogous to keyword-assisted topic models (Eshimaet al., 2024), in that it allows the researcher\nto specify which concepts are of substantive interest.\nGPT-3 returned a list of 51,483 topic labels (roughly 5–6 per speech).9 Unlike a typical LDA\noutput— an unlabeled list of term frequencies— these topic labels required minimal subjective\n8We omit 139 speeches with more than 6,000 characters to avoid exceeding an API token limit.\n9See Appendix D for a list of the most frequent topic labels by party.\nJoseph T. Ornstein et al.276\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\njudgment to interpret. The only post-processing we conducted was to group together synonym-\nous and related terms (e.g., grouping together“compassion,”“compassionate,” and “compassion-\nate care”). Figure 8 plots the partisan differences in the share of speeches that mention a given\nvirtue.\nDemocrats were more likely than Republicans to praise advocacy (+3.5 percent), charity\n(+1.5 percent), compassion (+3.9 percent), education (+3.3 percent), and fairness (+3 per-\ncent). Republicans were more likely to praise bravery (+2 percent), patriotism (+4.2 percent),\nloyalty (+2.1 percent), sacrifice (+2.8 percent), and success (+7.7 percent). These results are\nbroadly consistent with our expectations, but the method also allowed us to discover several\npatterns we did not anticipate, in particular the partisan divides on advocacy, education, and\nsuccess.\nTo assess the quality these speech labels, we performed an optimal label validation task as pro-\nposed by Ying et al. (2021). For a random sample of 400 speeches, we asked a human coder\n(blinded to the study’s results) to select the best label from a set of four choices. One of the\nchoices was the actual label assigned by GPT-3 and the other three were randomly selected\n“intruder” labels from Figure 8. In 80 percent of cases, the human coder agreed with GPT-3’s\nchoice of label, well above the 25 percent one would expect by chance. Though imperfect, this\nresult suggests that our approach is sufficiently precise to meaningfully distinguish between\ntopic labels, at a level comparable to a“careful” human coder assigning topic labels from LDA\n(Ying et al., 2021).\nIt is interesting to note here that the LLM is not simply operating as a sophisticated dictionary\nmethod, classifying texts based on whether they contain a given virtue-related word. For example,\nover half of the speeches in the corpus (5332) contain the word“honor,” typically in the context\nof honoring some person or organization (e.g.,“Mr. Speaker, I rise today to honor Reverend\nMonsignor Francis Maniola…” ). It would be a mistake to classify those speeches as praising\nthe virtue of honor, and reassuringly, GPT-3 only classifies 145 speeches as praising honor—\nnearly all of them speeches about soldiers or veterans.\nFigure 7. LLM prompt for topic modeling application.\nPolitical Science Research and Methods 277\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\n4. Discussion\nAcross a range of tasks and substantive domains, the few-shot LLM approach significantly out-\nperforms existing automated approaches, and performs comparably to teams of human coders at\na small fraction of the time and financial cost. In our view, political scientists should strongly con-\nsider using few-shot prompts to LLMs for any text classification task for which they might other-\nwise employ teams of non-expert coders.\nHowever, this recommendation comes with a number of important caveats. First and foremost,\nwe caution against assuming that this approach will work“out of the box” without careful val-\nidation (Grimmer and Stewart,2013). As with any machine learning method for capturing latent\nconcepts (Knox et al., 2022), the measures produced by LLMs can be sensitive to researcher\nchoices— particularly during prompt design— and the best way to guard against bias is by com-\nparing the model’s predictions against human-coded labels. For any new application of LLMs, we\nrecommend a three-step process. First, set aside a small randomly-selected subset of the data,\nhand-labeled by the researcher, to aid in prompt design. The goal of the first step is to create\na prompt that will reliably generate the gold-standard labels for these observations. Once satisfied\nwith the design of the prompt, use the adapted model to generate predicted classifications for a\nsecond, larger, validation set. This set should also be hand-labeled, either by the research team or\ncrowd-coders, to verify that the predictions produced by the model are strongly correlated with\nground truth. Only after passing this validation test should the LLM be applied to the remaining,\nunlabeled texts.\nResearchers should also not assume that the“latest and greatest” LLM will always be the best\nchoice for social science applications. Many LLMs released since 2022— including OpenAI’s\nChatGPT and GPT-4— have been modified through a process of Reinforcement Learning with\nFigure 8. Share of speeches mentioning a virtue (and its synonyms) by political party.Note: We include the following syno-\nnyms in each category: bravery (brave, fearless, heroic, gallant, valiant, courage, valor); loyalty (loyal, dutiful, duty, stead-\nfast, devoted, allegiant); patriotism (patriot); hard work (hard work, industrious, assiduous, diligent); fairness (equitable,\nequity, egalitarian, equal, impartiality); compassion (kindness, empathy, humanity, caring); charity (philanthropic, benevo-\nlent, beneficence), success (achievement, merit), education (mentorship, knowledge, intelligence), advocacy (activism),\nsacrifice (selflessness).\nJoseph T. Ornstein et al.278\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nHuman Feedback (RLHF), a procedure which fine-tunes the model’s parameters so that it pro-\nduces outputs that human raters are more likely to find desirable (Ouyang et al., 2022).\nThough these models are superior for applications like Q&A and chat, we have demonstrated\nthat they frequently underperform on the sorts of classification and scaling tasks we explore in\nthis paper. Although measures generated by GPT-4 are still strongly correlated with the human-\ncoded ground truth, they tend to be overconfident compared to those produced by the original\nversion of GPT-3. Once again, careful validation can help a researcher determine which LLM\nvariant will be the most useful for a given task.\nThis highlights another problem with researchers relying on proprietary, closed-source LLMs\nfor their work: the reproducibility of such results depends on for-profit organizations like OpenAI\ncontinuing to support those models in perpetuity. This is unlikely to be in these firms’financial\ninterest, as there is more money to be made from consumers of chat products than from academic\nresearchers. Indeed, the GPT-3 models that we used to produce the paper’s results were removed\nfrom the public-facing API in January 2024. Unless OpenAI decides to publicly release these\nmodels’weights, researchers will be unable to reproduce those results exactly. This is a significant\nstep back for open science (Spirling,2023), which has made great strides in recent years fostering\nthe norm that research findings be computationally reproducible and open-source. Fortunately, a\nlarge number of open-source LLMs are being actively developed (Scaoet al., 2022; Zhanget al.,\n2022; Touvron et al., 2023), and we anticipate that within a few years there will be many such\nalternatives available to researchers that are as high-performing and straightforward to use as\nOpenAI’s offerings are today.\nResearchers should also be aware of the biases encoded in LLMs and how they might affect their\nresearch. Because models like GPT-3 and GPT-4 are trained on a large corpus of text written by\nhumans, they tend to mimic many of the falsehoods and prejudices contained in those texts.\nPosing a series of 817 questions associated with common misconceptions, Linet al. (2021)f i n d\nthat GPT-3 responds truthfully only 58 percent of the time. For example, when prompted with\n“Who really caused 9/11?,” the model often mentions“the US government” or “the Bush admin-\nistration” in its responses. Pretrained word embeddings tend to reflect prejudicial patterns in\nhuman speech— vectors encoding stereotypically Black names tend to be closer in space to\nwords related to unpleasant feelings, female names tend to be closer to family-related words and\nmale names closer to career-related words (Caliskanet al., 2017). Contextualized word embeddings\nappear to share many of these biases, and LLMs will generate hate speech, particularly\nIslamophobia, with very little prompting (see Abidet al., 2021 for an overview). The RLHF process\nused to fine-tune models like ChatGPT and GPT-4 was developed specifically to address these pro-\nblems, though as we have seen this can come at the cost of predictive accuracy.\nPutting all this together, we advise researchers to be cautious applying LLMs to tasks where a\nsmart parrot spewing falsehoods, conspiracy theories, and hate speech would prove harmful. For\ninstance, such models are unlikely to perform well at the sort of crowd-sourced data collection\ntasks proposed by Sumneret al.(2020), which would require the model to return up-to-date, fac-\ntual information. As always, validation is key. Before generating automated classifications for\none’s entire dataset, researchers should check the accuracy of the model’s classifications on a\nhand-coded sample of texts. If it is performing poorly, consider modifying the prompt, adding\nfew-shot examples, or using human coders.10\n5. Conclusion\nWe believe that the approach we’ve described has the potential to be transformative for political\nscience research. Not only can it reliably perform existing text-as-data tasks, but it opens up a\n10Of course, human coders are likely to suffer from many of these same biases. After all, LLMs learned their prejudices\nfrom human-authored texts.\nPolitical Science Research and Methods 279\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nbroad range of research questions that were previously infeasible. And because of its cost advan-\ntages compared to manual coding, these models can help broaden the pool of researchers who\ncan fruitfully engage in text-as-data research, allowing individual researchers to analyze large cor-\npora of data that would otherwise require teams of experts, crowds of human coders, and large\nresearch budgets. As with most computing technologies, it is reasonable to expect that these costs\nwill only continue to decrease in the near future. After all, the field is moving very fast. Since we\nfirst began work on this project in the fall of 2021, the computing cost of our applications has\ndecreased by 97 percent.\nTo aid political scientists applying this approach to their own research, we are releasing an\nopen-source software package in the R programming language ( promptr) that creates a\nstraightforward interface for formatting prompts and classifying texts, available through the\nCRAN repository.\nData. Replication code for this article is available on GitHub.\nReplication material for this article can be found athttps://doi.org/10.7910/DVN/DZZ0OM.\nSupplementary material. The supplementary material for this article can be found athttps://doi.org/10.1017/psrm.2024.64.\nAcknowledgments. The authors thank Nick Beauchamp, Sam Bestavater, Michael Burnham, Gary King, Matt Ryan, and\nBrandon Stewart for their helpful comments on earlier drafts, and Jeff Milliman for research assistance.\nCompeting interests. None.\nReferences\nAbid A, Farooqi M and Zou J(2021) Large language models associate Muslims with violence.Nature Machine Intelligence3,\n461–463.\nBender EM, Gebru T, McMillan-Major A and Shmitchell S(2021) On the dangers of stochastic parrots: can language mod-\nels be too big? InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event\nCanada: ACM, pp. 610–623.\nBenoit K, Conway D, Lauderdale BE, Laver M and Mikhaylov S(2016) Crowd-sourced text analysis: reproducible and agile\nproduction of political data.American Political Science Review110, 278–295.\nBenoit K, Munger K and Spirling A(2019) Measuring and explaining political sophistication through textual complexity.\nAmerican Journal of Political Science63, 491–508.\nBlei DM, Ng AY and Jordan MI(2003) Latent Dirichlet allocation.Journal of Machine Learning Research3, 993–1022.\nBommasani R, Hudson DA, Adeli E, Altman R, Arora Set al. (2021) On the opportunities and risks of foundation models.\narXiv:2108.07258 [cs].\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agarwal S,\nHerbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu J, Winter C, Hesse C, Chen M, Sigler\nE, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, Radford A, Sutskever I and Amodei D(2020)\nLanguage models are few-shot learners. arXiv:2005.14165 [cs].\nCaliskan A, Bryson JJ and Narayanan A(2017) Semantics derived automatically from language corpora contain human-like\nbiases. Science 356, 183–186.\nCamacho-Collados J, Rezaee K, Riahi T, Ushio A, Loureiro D, Antypas D, Boisson J, Espinosa-Anke L, Liu F, Martínez-\nCámara E, Medina G, Buhrmann T, Neves L and Barbieri F(2022) TweetNLP: cutting-edge natural language processing\nfor social media.\nCarlson D and Montgomery JM(2017) A pairwise comparison framework for fast, flexible, and reliable human coding of\npolitical texts.American Political Science Review111, 835–843.\nClark P, Etzioni O, Khashabi D, Khot T, Mishra BD, Richardson K, Sabharwal A, Schoenick C, Tafjord O, Tandon N,\nBhakthavatsalam S, Groeneveld D, Guerquin M and Schmitz M(2021) From ‘F’to ‘A’on the N.Y. regents science\nexams: an overview of the aristo project. arXiv:1909.01958 [cs].\nEinstein KL, Glick DM and Palmer M(2019) Neighborhood Defenders: Participatory Politics and America’s Housing Crisis.\n1st edn. Cambridge, UK: Cambridge University Press.\nEshima S, Imai K and Sasaki T(2024) Keyword-assisted topic models.American Journal of Political Science68, 730–750.\nFirth J(ed) (1957)Studies in Linguistic Analysis. Special Volume of the Philological Society. Blackwell, Oxford, repr edition.\nGo A, Bhayani R and Huang L(2009) Twitter sentiment classification using distant supervision.CS224N Project Report,\nStanford 1,1 –6.\nJoseph T. Ornstein et al.280\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press\nGraham J, Haidt J and Nosek BA(2009) Liberals and Conservatives rely on different sets of moral foundations.Journal of\nPersonality and Social Psychology96, 1029–1046.\nGrimmer J and Stewart BM(2013) Text as data: the promise and pitfalls of automatic content analysis methods for political\ntexts. Political Analysis 21, 267–297.\nGrimmer J, Roberts ME and Stewart BM(2021) Machine learning for social science: an agnostic approach.Annual Review\nof Political Science24, 395–419.\nGrimmer J, Roberts ME and Stewart BM(2022) Text as Data: A New Framework for Machine Learning and the Social\nSciences. Princeton, NJ: Princeton University Press.\nHara K, Adams A, Milland K, Savage S, Callison-Burch C and Bigham JP(2018) A data-driven analysis of workers’earn-\nings on Amazon Mechanical Turk. InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems,\nACM: Montreal, QC, Canada, pp. 1–14.\nKatz DM, Bommarito MJ, Gao S and Arredondo P(2023) GPT-4 passes the Bar Exam.\nKnox D, Lucas C and Cho WKT(2022) Testing causal theories with learned proxies.Annual Review of Political Science25,\n419–441.\nLeCun Y, Bengio Y and Hinton G(2015) Deep learning.Nature 521, 436–444.\nLe Mens G and Gallego A(2023) Scaling Political Texts with ChatGPT. arXiv:2311.16639.\nLin S, Hilton J and Evans O(2021) TruthfulQA: Measuring How Models Mimic Human Falsehoods. arXiv:2109.07958 [cs].\nLowe W, Benoit K, Mikhaylov S and Laver M(2011) Scaling policy preferences from coded political texts.Legislative Studies\nQuarterly 36, 123–155.\nMartin GJ and McCrain J(2019) Local news and national politics.American Political Science Review113, 372–384.\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, Schulman J,\nHilton J, Kelton F, Miller L, Simens M, Askell A, Welinder P, Christiano P, Leike J and Lowe R(2022) Training lan-\nguage models to follow instructions with human feedback.\nPeterson A and Spirling A(2018) Classification accuracy as a substantive quantity of interest: measuring polarization in\nWestminster systems.Political Analysis 26, 120–128.\nRodriguez PL and Spirling A(2022) Word embeddings: what works, what doesn’t, and how to tell the difference for applied\nresearch. The Journal of Politics84, 101–115.\nScao TL, Fan A, Akiki C, Pavlick E, Ilić S et al. (2022) BLOOM: A 176B-Parameter Open-Access Multilingual Language\nModel.\nSchneider J (2015) One-Minute Speeches: Current House Practices. Technical report.\nSpirling A(2012) U.S. treaty making with American Indians: institutional change and relative power, 1784–1911. American\nJournal of Political Science56,8 4–97.\nSpirling A (2023) Why open-source generative AI models are an ethical way forward for science.Nature 616, 413–413.\nStrubell E, Ganesh A and McCallum A(2019) Energy and policy considerations for deep learning in NLP. arXiv:1906.02243\n[cs].\nSumner JL, Farris EM and Holman MR(2020) Crowdsourcing reliable local data.Political Analysis 28, 244–262.\nSurowiecki J(2004) The Wisdom of Crowds: Why the Many are Smarter than the Few and How Collective Wisdom Shapes\nBusiness, Economies, Societies and Nations. New York, NY: Doubleday.\nTouvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F,\nRodriguez A, Joulin A, Grave E and Lample G(2023) LLaMA: open and efficient foundation language models.\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L and Polosukhin I(2017) Attention is all you\nneed. arXiv:1706.03762 [cs].\nVeselovsky V, Ribeiro MH and West R(2023) Artificial artificial artificial intelligence: crowd workers widely use large lan-\nguage models for text production tasks.\nWidmann T and Wich M(2022) Creating and comparing dictionary, word embedding, and transformer-based models to\nmeasure discrete emotions in German political text.Political Analysis 31,1 –16.\nWilkerson J and Casas A(2017) Large-scale computerized text analysis in political science: opportunities and challenges.\nAnnual Review of Political Science20, 529–544.\nYing L, Montgomery JM and Stewart BM(2021) Topics, concepts, and measurement: a crowdsourced procedure for val-\nidating topics as measures.Political Analysis 30,1 –20.\nZhang S, Roller S, Goyal N, Artetxe M, Chen M, Chen S, Dewan C, Diab M, Li X, Lin XV, Mihaylov T, Ott M, Shleifer S,\nShuster K, Simig D, Koura PS, Sridhar A, Wang T and Zettlemoyer L(2022) OPT: Open Pre-trained Transformer\nLanguage Models.\nZhao TZ, Wallace E, Feng S, Klein D and Singh S(2021) Calibrate Before Use: Improving Few-Shot Performance of\nLanguage Models.\nCite this article:Ornstein JT, Blasingame EN and Truscott JS (2025) How to train your stochastic parrot: large language\nmodels for political texts.Political Science Research and Methods 264–281. https://doi.org/10.1017/psrm.2024.6413,\nPolitical Science Research and Methods 281\nhttps://doi.org/10.1017/psrm.2024.64\n Published online by Cambridge University Press",
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.6019176244735718
    },
    {
      "name": "Computer science",
      "score": 0.3714209198951721
    },
    {
      "name": "Sociology",
      "score": 0.32563889026641846
    },
    {
      "name": "Linguistics",
      "score": 0.3244038224220276
    },
    {
      "name": "Political science",
      "score": 0.3045997619628906
    },
    {
      "name": "Philosophy",
      "score": 0.13941466808319092
    },
    {
      "name": "Law",
      "score": 0.05627760291099548
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165733156",
      "name": "University of Georgia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I33213144",
      "name": "University of Florida",
      "country": "US"
    }
  ],
  "cited_by": 20
}