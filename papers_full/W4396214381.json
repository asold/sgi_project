{
  "title": "FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification",
  "url": "https://openalex.org/W4396214381",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101789396",
      "name": "Dayeon Yoo",
      "affiliations": [
        "Kookmin University"
      ]
    },
    {
      "id": "https://openalex.org/A5102027209",
      "name": "Jeesu Kim",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A5019044787",
      "name": "Jinwoo Yoo",
      "affiliations": [
        "Kookmin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6810563863",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2168809519",
    "https://openalex.org/W4283653444",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W4315705623",
    "https://openalex.org/W3154396742",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W4292829111",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4226424839",
    "https://openalex.org/W4312257978",
    "https://openalex.org/W6803680838",
    "https://openalex.org/W4386075553",
    "https://openalex.org/W4386075796",
    "https://openalex.org/W6794345597",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W4394566113",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W6797235774",
    "https://openalex.org/W3096654432",
    "https://openalex.org/W6753038380",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W4312638656",
    "https://openalex.org/W6730179637",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6796237581",
    "https://openalex.org/W4312599212",
    "https://openalex.org/W4221142474",
    "https://openalex.org/W2561238782",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W4297810817"
  ],
  "abstract": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6423467993736267
    },
    {
      "name": "Transformer",
      "score": 0.6142545938491821
    },
    {
      "name": "Computer vision",
      "score": 0.595879077911377
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5913571119308472
    },
    {
      "name": "Window (computing)",
      "score": 0.41832977533340454
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3819773197174072
    },
    {
      "name": "Electrical engineering",
      "score": 0.12499639391899109
    },
    {
      "name": "Voltage",
      "score": 0.12362396717071533
    },
    {
      "name": "Engineering",
      "score": 0.11907222867012024
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I110273157",
      "name": "Kookmin University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4921948",
      "name": "Pusan National University",
      "country": "KR"
    }
  ]
}