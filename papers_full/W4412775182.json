{
  "title": "Impact of large language models and vision deep learning models in predicting neoadjuvant rectal score for rectal cancer treated with neoadjuvant chemoradiation",
  "url": "https://openalex.org/W4412775182",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2297119973",
      "name": "Hyun Bin Kim",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2478905364",
      "name": "Hong Qi Tan",
      "affiliations": [
        "Nanyang Technological University",
        "National Cancer Centre Singapore",
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A2034386599",
      "name": "Wen Long Nei",
      "affiliations": [
        "National Cancer Centre Singapore",
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": null,
      "name": "Ying Cong Ryan Shea Tan",
      "affiliations": [
        "Duke-NUS Medical School",
        "National Cancer Centre Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2116505113",
      "name": "Yi-Yu Cai",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2135707117",
      "name": "Fu-qiang Wang",
      "affiliations": [
        "National Cancer Centre Singapore",
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A2297119973",
      "name": "Hyun Bin Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2478905364",
      "name": "Hong Qi Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2034386599",
      "name": "Wen Long Nei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ying Cong Ryan Shea Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116505113",
      "name": "Yi-Yu Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135707117",
      "name": "Fu-qiang Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4322719806",
    "https://openalex.org/W3044579612",
    "https://openalex.org/W2051224942",
    "https://openalex.org/W2118924162",
    "https://openalex.org/W2137777824",
    "https://openalex.org/W2060706863",
    "https://openalex.org/W2050365174",
    "https://openalex.org/W1894792744",
    "https://openalex.org/W3092740001",
    "https://openalex.org/W3004038413",
    "https://openalex.org/W2998110103",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2794803511",
    "https://openalex.org/W3080854745",
    "https://openalex.org/W2990500665",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4401753041",
    "https://openalex.org/W4392372617",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W3127785515",
    "https://openalex.org/W4226225547",
    "https://openalex.org/W3033965569",
    "https://openalex.org/W4384282814",
    "https://openalex.org/W3119668554",
    "https://openalex.org/W4288068240",
    "https://openalex.org/W3133631487",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W3028169869",
    "https://openalex.org/W3182999054",
    "https://openalex.org/W4313830579",
    "https://openalex.org/W4386394139",
    "https://openalex.org/W4380995257",
    "https://openalex.org/W4378783467",
    "https://openalex.org/W4392938433",
    "https://openalex.org/W2009843696",
    "https://openalex.org/W2515914977",
    "https://openalex.org/W3004088204",
    "https://openalex.org/W3134968295",
    "https://openalex.org/W4405081677",
    "https://openalex.org/W3102564565"
  ],
  "abstract": null,
  "full_text": "RESEARCH Open Access\nIntroduction\nColorectal Cancer (CRC) is the third most frequently \noccurring cancer and the second deadliest cancer world -\nwide [ 1]. In locally advanced rectal cancer (LARC), \nneoadjuvant chemoradiation (NACRT) is a standard of \ncare [2], where it is used for downstaging patients prior \nto surgery [ 3, 4]. Post NACRT, pathological complete \nresponse (pCR) rate ranges between 24 and 42% [ 2, 5]. \nOn hindsight, for patients with pCR, perhaps a wait-and-\nsee approach could have been adopted to avoid unneces -\nsary surgical resection through total mesorectal excision \n(TME) [ 6]. Beyond pCR, tumor regression grade (TRG) \n[7] and neoadjuvant rectal (NAR) score [ 8] have been \ncorrelated with survival outcomes in LARC patients \n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n  c - n  d / 4 .  0 / .\nKim et al. BMC Medical Imaging          (2025) 25:306 \nhttps://doi.org/10.1186/s12880-025-01844-5\nBMC Medical Imaging\nFirst author: Hyun Bin Kim\n*Correspondence:\nYiyu Cai\nmyycai@ntu.edu.sg\nFuqiang Wang\nwang.fu.qiang@singhealth.com.sg\n1Division of Radiation Oncology, National Cancer Centre Singapore, \nSingapore, Singapore\n2Oncology Academic Clinical Programme, Duke-NUS Medical School, \nSingapore, Singapore\n3School of Physical and Mathematical Science, Nanyang Technological \nUniversity, Singapore, Singapore\n4College of Computing and Data Science, Nanyang Technological \nUniversity, Singapore, Singapore\n5Division of Medical Oncology, National Cancer Centre Singapore, \nSingapore, Singapore\n6School of Mechanical & Aerospace Engineering, Nanyang Technological \nUniversity, Singapore, Singapore\nAbstract\nThis study aims to explore Deep Learning methods, namely Large Language Models (LLMs) and Computer Vision \nmodels to accurately predict neoadjuvant rectal (NAR) score for locally advanced rectal cancer (LARC) treated with \nneoadjuvant chemoradiation (NACRT). The NAR score is a validated surrogate endpoint for LARC. 160 CT scans of \npatients were used in this study, along with 4 different types of radiology reports, 2 generated from CT scans and \nother 2 from MRI scans, both before and after NACRT. For CT scans, two different approaches with convolutional \nneural network were utilized to tackle the 3D scan entirely or tackle it slice by slice. For radiology reports, an \nencoder architecture LLM was used. The performance of the approaches was quantified by the Area under the \nReceiver Operating Characteristic curve (AUC). The two different approaches for CT scans yielded 0.537 (±0.095) \nand 0.541 (±0.072) while the LLM trained on post NACRT MRI reports showed the most predictive potential at \n0.708 (±0.041) and a statistical improvement, p = 0.03, over the baseline clinical approach (from 0.687 (±0.136) \nto 0.726 (±0.106)). This study showcases the potential of Large Language Models and the inadequacies of CT \nscans in predicting NAR values.\nClinical trial number  Not applicable.\nKeywords Large language models, Computer vision, Rectal cancer, Deep learning\nImpact of large language models \nand vision deep learning models in predicting \nneoadjuvant rectal score for rectal cancer \ntreated with neoadjuvant chemoradiation\nHyun Bin Kim4, Hong Qi Tan1,2,3, Wen Long Nei1,2, Ying Cong Ryan Shea Tan2,5, Yiyu Cai6* and Fuqiang Wang1,2*\nPage 2 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nundergoing NACRT. The NAR score has been shown to \nbe statistically associated with Overall Survival (OS) rate \nof patients [9] and serve not only as a surrogate endpoint \nbut also as a prognostic marker [ 10, 11]. However, to cal-\nculate the NAR score, post-operative pathology input is \nrequired. Hence, the question is can we predict for path -\nological and survival outcomes without having to put the \npatient under the knife?\nIn modern computer science, Deep Learning (DL) has \nbeen producing significant results. DL has come a long \nway ever since Rumelhart D et al. [ 12] opened the eyes of \nthe scientific community to the potential of backpropaga-\ntion in neural networks, a foundational yet crucial aspect \nof modern DL. Computer Vision (CV) models showed \npromising results with the introduction of AlexNet [ 13] \nwhere it surpassed previous CV models by a huge margin \nin performance in the ImageNet classification challenge \n[14]. Since then, models with superior performance have \nemerged [15, 16] that displayed superior performance in \nvarious tasks such as image classification and segmenta -\ntion. Modern Language Models (LM) also improved sub -\nstantially from Recurrent Neural Networks (RNN), Long \nShort-Term Memory (LSTM) models, and Gated Recur -\nrent Units (GRU). Modern LM’s architectures are heav -\nily inspired by the transformer architecture [ 17, 18]. This \nallowed for substantially more complex and better per -\nforming models such as GPT-3 [19] and BERT [20] which \nexcel at text generation, text classification, feature extrac-\ntion amongst other capabilities. Even in the medical field, \nDL has been making immense progress. Much of the use \ncases for Visual Deep Learning for RC has been focused \non detection or outcome prediction of patients based on \ntissue images [ 21–23]. Additionally, LLMs have solidi -\nfied its place in the medical field as potent assistive tool \n[24]. It could potentially assist in classification of cancer \nstages [25] or act as chatbots for interaction with cancer \npatients [ 26]. This was enabled by the advent of LLMs \nspecifically tailored to usages in biomedical field such \nas BioBERT, GatorTron, PubMedBERT, and Clinical -\nBERT [27–30] which are all an encoder-only architecture \nmodel based on the transformer architecture and useful \nfor Named Entity Recognition and classification tasks, \nwhile models such as MedPaLM 2 and ClinicalGPT [ 31, \n32] are useful for question answering and chatbot style \ninteraction. These DL algorithms possess the ability to \nunderstand intricate relations and extract out crucial \ninformation from various high dimensional modalities, \nsuch as text and images, and predict target variables, \nwhich in our case would be the NAR value.\nApart from DL, machine learning methods have been \napplied to radiomics features of MRI and CT scans to \npredict for NAR values [ 33–35]. These attempts have \nproduced a decent result for prediction of NAR values \nbut have only been limited to CT/MRI images of patients \nand have not explored any other modalities such as radi -\nology texts. Most importantly, there has not been any \nsubstantial research of predictive power of DL methods \ntowards NAR prediction for CRC yet.\nThe primary aim of this study is to investigate the pre -\ndictive performance of DL methods for the NAR score, \nbased on pre-operative data including CT scans and \nradiology reports from patients with LARC undergoing \nNACRT.\nMethods and materials\nPatient and data characteristics\nThis study was approved with waiver of informed consent \nfrom Singhealth centralized institution review board and \nall methods were performed in accordance with relevant \nguidelines and regulations. This is a single center, retro -\nspective study involving 192 patients who had LARC and \nreceived NACRT with subsequent TME between 2006 \nand 2017. Retrospective chart review of these patients \nwas conducted for basic demographic, disease staging \n(based on the AJCC, 7th edition) chemoradiation and \nsurgical details as well as the pathology where available.\nOut of the 192 patients, 160 of them had valid CT \nscans. CT Images with contrast were captured with two \ndifferent CT scanners located in the center’s radiother -\napy department. The first CT scanner was the GE Light -\nSpeed RT16 and the second was Siemens SOMATOM \ndefinition AS. All images were acquired with 120 kVp \nX-ray with slice thickness 2.0 mm (Siemens scanner) and \n2.5  mm (GE Scanner). The default standard and B31f \nconvolution kernels were used for the GE and Siemens \nscanner respectively. The in-slice resolution was 512 \nby 512 for all images. The number of total layers differ \nthroughout the images, but the mean is 152 layers with a \nstandard deviation of 23. The minimum number of layers \nis 63 while the maximum is 333.\nThe segmentations were performed manually by the \nradiation oncologist (F. Q. Wang) without knowledge of \nthe pathologic outcome of the patient. Two segmenta -\ntions consisting of the GTV and CTV were contoured \nusing the CT image and there were no overlaps in these \nsegmentations. These segmentations were used for shape \ncalculation and was known as the morphological mask in \nImage Biomarker Standardization Initiative (IBSI). The \nmanually contoured segmentations were subsequently \nre-segmented to remove any voxel with HU below − 50 \nHU. This was to remove part of contours which encom -\npassed the air in the rectum and was known as the inten-\nsity mask in IBSI.\nRadiology reports data\nIn this study, 4 types of radiology reports was used. Two \nof them were generated from CT and MRI scans taken \nbefore NACRT (CT_Pre, MRI_Pre) while the other two \nPage 3 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nwere generated from CT and MRI scans taken after \nNACRT (CT_Post, MRI_Post). There was a total of 165 \nand 164 data for CT_Pre and MRI_Pre respectively and \n137 and 140 data for CT_Post and MRI_Post respectively.\nRadiology reports are written in a descriptive style \nand lengthy in nature, containing various information \nsuch as description, history, observation, findings and \nconclusion. To ensure that our LLM model was able to \nanalyze all the information within its context length of \n512 tokens, we decided to train our model only on the \nconclusion section of the report. This was done also to \nmitigate the effects of different styles of report writing \nfrom different radiologist and changing reporting con -\nventions throughout the years. The conclusion section \ncontains succinct descriptions of the patients’ condition \nsuch as the cancer stage, the seriousness of the observ -\nable tumor or the suspected numerical values of certain \nclinical variables. Additionally, any unnecessary numbers \nwere removed such as ‘1. ’ and ‘2. ’ to prevent confusion \nbetween important clinical numerical figures and unim -\nportant simple numbering practices. It was also observed \nthat the sentences in the radiology reports were rela -\ntively independent. Therefore, reordering of the sentence \norder would not affect the entire meaning of the report. \nHence, to increase the number of our training data, sen -\ntence permutation was carried out [ 36]. For each conclu-\nsion section, we produced P distinct variations of it while \nkeeping the output predictions uniform, successfully \nincreasing the total number of training data. P in our \nstudy was set at 5.\nThe endpoint of interest was the NAR score. It was cal -\nculated and a binary outcome with the threshold set at 16 \n(NAR ≥  16 and NAR < 16) was used for this work [8].\nVision model\nPreprocessing of the 3D CT scans of patients were car -\nried out prior to any Deep Learning operation. Each of \nthe 3D CT scans had varying pixel spacing. Thus, cubic \ninterpolation using the interpolate function in the scipy \nv1.14.0 library was carried out to ensure isotropic voxel \nspacing (0.7  mm) [ 37]. To ensure convergence during \ntraining, the HU units were normalized into a range of 0 \nand 1. After normalization, only the layers containing the \ntumor segmentation was selected to increase efficiency \nduring training. With the 3D CT scans, two different \napproaches were carried out, namely the 2D approach \nand the 3D approach. The pipeline schematics for vision \nmodel is illustrated on the right side of Fig. 1.\nVision model − 2D approach\nFor a 2D approach, the 3D CT scans for a single patient \nwas divided into axial slices. Each of these slices were \nthen used as input for a pretrained model RadIma -\ngeNet [ 38], based on ResNet 50 architecture. It has \nbeen pretrained on 1.35  million CT images and has \nbeen shown to perform better than conventional Ima -\ngeNet models. After being trained on individual slices, \nthe averaged logit output of the model would be calcu -\nlated for the final prediction of a complete 3D CT scan \nof individual patients. This approach of obtaining pre -\ndictions by analyzing single layers for a 3D CT scan has \ndisplayed unexpected potential despite concerns of loss \nof information when data is transitioned from 3D to 2D \n[39]. Parameters were initialized from the RadImageNet \nmodel and all weights, including the final linear layer, \nwas trained. Training was carried out for 20 epochs with \nbatch size of 8 and learning rate set at 0.00001 using the \ncosine annealing scheduler, with the Adam optimizer. \nThe model architecture is shown in Fig. 2.\nVision model − 3D approach\nFor a 3D approach, the whole 3D CT scan was used to \ntrain a 3D model. For the model, a 3D rendition of the \nResNet architecture was utilized [ 40]. Specifically, the \nResNet 50 architecture with the Bottleneck Residual \nBlock implementation was used. This model had no pre -\ntraining involved and all parameters were trained from \nrandomized weights. As the 3D CT scans had varying \nnumber of layers, an equal number of layers in the middle \nwere used for training [ 39]. This minimum total number \nof layers throughout all the 3D CT scans were chosen as \nthe number of layers to be selected out from the middle \nof the scans. Training was carried out for 10 epochs with \nbatch size of 2, and learning rate set at 0.00001 using the \ncosine annealing scheduler, with the Adam optimizer. \nThe model architecture is shown in Fig. 2.\nTextual model\nTo train a binary classification model, we used an \nencoder-only architecture model to analyze and encode \nthe text for final binary prediction. In this study, we found \nthat BioBERT-Large [ 30] showed the most predictive \npotential amongst other similar models such as Gator -\nTron and BioClinicalBERT. Being pretrained specifically \non biomedical knowledge, only finetuning was needed \nfor our case of NAR score classification. For finetuning, \nwe added a new set of linear layers after the encoding to \noutput 2 values as the probability of belonging to a cer -\ntain class. This was chosen over an alternative option of \ntraining all the pre-trained weights [ 41]. We wished to \navoid excessive number of trainable parameters relative \nto the limited dataset, which could potentially lead to a \nvolatile training process with extreme overfitting. How -\never, for our study, we found that training the final lay -\ners with the addition of the final transformer encoder \nlayer produced the best outcome. Training was carried \nout for 50 epochs with a batch size of 16, learning rate \nset at 0.00008 using the cosine annealing scheduler, with \nPage 4 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nFig. 1 Schematics of our training and testing process for our 3 single modality models and 2 combined models\n \nPage 5 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nFig. 2 Model Architectures of the 2D Vision model (top), 3D Vision model (middle), and the Textual model (bottom)\n \nPage 6 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nthe Adam optimizer. Pretrained weights of the BioBERT-\nLarge model was used, and all weights were frozen except \nfor the final transformer encoder layer’s weights and the \nnewly appended linear layers. The pipeline schematics for \nthe textual model is illustrated on the left side of Fig.  1. \nThe model architecture is shown in Fig. 2.\nClinical model\nA clinical model was developed for comparison with \ndeep learning model to show improvement over conven -\ntional means of predicting for NAR values. The clinical \nmodel was trained using purely clinical variables, cT, cN, \nand cM, pertaining to the size of the tumor, presence of \ncancer cells in the lymph nodes and metastases of cancer, \nall obtained during pre-operation. A Logistic Regression \nmodel (scikit-learn v1.5.0) was used to train and predict \nNAR scores with these variables. The pipeline schematics \nfor the clinical model is illustrated in the middle of Fig. 1.\nAssessing model performance\nA nested 5-folds stratified cross validation was used to \nassess performance of the 3 different models, visual, tex -\ntual and clinical. Stratified cross validation was employed \nin the training fold to tune our hyperparameters of our \ndeep learning model and Logistic Regression. This \nassessment pipeline can be observed in the bottom part \nof Fig.  1. Performance of the models are evaluated using \nAUC and reported with a 95% Confidence Interval. The \ncombined model uses Logistic Regression, and the three \nclinical variables and the probability logit output from \neither the vision or textual model as its inputs. Further -\nmore, to be able to interpret the combined model, fea -\nture importances are calculated as the absolute value of \neach features’ coefficients used in the linear combination \nof features in the Logistic Regression model. It is trained \non the same set of folds as the vision or textual model to \nensure an accurate and fair comparison. For comparison \nbetween the combined and clinical models, the Wilcoxon \nsigned rank test was used to show statistical improve -\nment, if any.\nResults\nPatient characteristics and CT data\nThe distribution of the clinical parameters of the patients \nis shown in Table  1. In total, there are 192 patients, with \n39.5% of them having NAR value greater or equal to 16 \nwhile the rest of them has a NAR value smaller than \n16. Distribution of clinical and pathological variables \namongst patients are not equal throughout.\nVision model\nFor our 2D and 3D approach of our vision model, \nboth yielded AUC results of 0.537 (±0.095) and \n0.541 (±0.072) respectively. Utilizing Grad-CAM [ 42] \non our 2D approach model, the model’s inefficiencies \nare visible as there is not one single area of focus. The \n3D approach showed no sign of improvement with both \ntraining and validation loss wandering around its initial \nvalues. Comparison of the performances of the combined \nmodels and their respective ROC curves are shown in \nFig. 3. We can see from the comparison that the 2D and \n3D approach shows high p values, 0.50 and 0.31 respec -\ntively. This makes it clear that vision models add no sta -\ntistical value to the clinical model.\nTextual model\nComparisons of the 4 different sets of radiology reports \nare made and can be seen in Table  2. Post-NACRT MRI \nradiology reports (MRI_Post) showed the most predic -\ntive potential over the other 3 reports with an AUC value \nof 0.708 (±0.042). Comparisons of the clinical and \ncombined models for all 4 different radiology reports \nare shown in Fig.  4. Out of these four, MRI_Post was the \nonly radiology report that was able to show a statistical \nimprovement ( p = 0.03), achieving 0.726 (±0.106) as a \ncombined model over its clinical counterpart achieving \n0.687 (±0.136). The ROCs of all 5 different folds of the \ncombined model with MRI_Post is shown in Fig.  4. The \nTable 1 Distribution table of clinical parameters of the patients\nNo. of patients NAR ≥  16 NAR < 16\nTotal number of patients 192 76 116\nClinical Diagnosis\ncT stage\n T1 0 0 0\n T2 10 3 7\n T3 143 60 83\n T4 39 13 26\ncN stage\n N0 43 8 35\n N1 113 49 64\n N2 36 19 17\ncM stage\n M0 183 70 113\n M1 9 6 3\nPathology Diagnosis\npT stage\n P0 24 1 23\n P1 11 0 11\n P2 35 12 23\n P3 101 48 53\n P4 21 15 6\npN stage\n N0 115 2 113\n N1 62 59 3\n N2 15 15 0\npM stage\n M0 182 70 112\n M1 10 6 4\nPage 7 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \ncombined model’s feature importance for MRI_Post can \nbe seen in Fig.  5, where it clearly shows meaningful con -\ntribution of the textual model’s probability logit output to \nthe combined model.\nClinical model\nThe clinical model was trained on the full 192 patients’ \ninformation, without being limited to the availability \nof the respective radiology reports. With 3 variables, \na baseline Logistic Regression model with default l2 \npenalty produced the best outcome with an AUC of \n0.679 (±0.128). The feature importances of the Logistic \nRegression model across all 5 folds is shown in Fig. 5. Out \nof the three variables, the cM variables appear to be the \nmost important of them all.\nDiscussion\nIn this study, we explored the potential of Deep Learning \napproaches for predicting NAR score of LARC patients \nusing 3D CT scans and accompanying radiology reports. \nThe results show that while vision models used on CT \nscans do not add value to clinical models, textual mod -\nels used on radiology reports show better potential with \nspecifically MRI radiology report generated after NACRT \nshowing statistical improvement compared to the clini -\ncal model. As mentioned earlier, the ability to predict the \nresponse to NACRT during pre-operative phase could \nplay an important role in the clinical decision of recom -\nmending the patient for surgery upfront or to stick to \nNACRT.\nRelevant Deep Learning approaches applied in the field \nof LARC and CRC focus on detection and classification \nTable 2 Comparison of the 4 different types of radiology reports’ performance for predicting NAR values\nArea under the Receiver Operating Characteristic curve (AUC) metric\nPre-NACRT CT radiology reports (CT_Pre) Post-NACRT CT radiology reports \n(CT_Post)\nPre-NACRT MRI radiology reports \n(MRI_Pre)\nPost-NACRT \nMRI radiol-\nogy reports \n(MRI_Post)\n0.622 ± 0.1067 0.510 ± 0.2563 0.559 ± 0.0662 0.708 ± 0.0417\nFig. 3 ROC curves of 5 folds for 2D (a) and 3D (b) approach and a comparison of combined models using vision model (c)\n \nPage 8 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nof CT scans or tissue images [43, 44]. Much of these stud-\nies obtain relatively accurate results as the models are \nmainly focused on observing for the presence of certain \npatterns in the data, which is a task that Deep Learn -\ning Vision models are exceptional in. However, different \ntasks like predicting for patient’s condition outcome is a \ntougher task as it requires the model not only to observe \nthe pattern but also expect it to correctly arrive at a pre -\ndiction which may or may not be deducible from the \ninput image data alone [ 21]. With a more complicated \ntask, such as predicting for the NAR value of patients \nby looking at their CT scan, a more detailed and infor -\nmation-rich input data might be needed such as tumor \ntissue samples and cell images, which have successfully \nshown better predictive power for several clinical factors \nsuch as five-year disease specific survival and relapse-free \nsurvival [ 21, 45]. We have tried to combat the generality \nof CT scans by selecting only those layers with tumors to \nfeed into our model, in the hopes that this would probe \nour model towards the right direction and assist in its \nanalysis. However, even this attempt has failed to show \npromising result. The failure of CT scans to provide \nample information to machine learning models have also \nbeen demonstrated by Wang F et al. [ 34] where radiomic \nfeatures extracted out from CT images have shown lim -\nited value to a baseline clinical model for predicting NAR \nvalues of patients with LARC. Therefore, we believe that \nthe lack of necessary information in CT scans to be the \nreason behind the poor results of our vision model. How-\never, other forms of input visual data, more detailed and \nspecific in nature, such as MRI scans [ 46] or even cell \nor tumor tissue images could show predictive potential \ntowards NAR score.\nThe second approach this study took, utilizing LLMs, \nshowed a much more promising predictive potential than \nits first approach. The role of LLMs in the medical field \nhas been gaining more attention recently [ 36, 47–49] in \nvarious forms, such as showing promising results in clas -\nsification tasks or serving as a potent generative assis -\ntance to medical professionals. However, when it comes \nto predicting for classification task such as for the NAR \nFig. 5 Feature Importance of the combined model for MRI_Post ( a) and \npure clinical Logistic Regression model ( b) and visualization of SHAP val -\nues for report correctly classified as having low NAR value\n \nFig. 4 Comparisons of all 4 different radiology reports with the combined \nmodel (a) and ROCs of all 5 folds of combined model with MRI_Post radiol-\nogy report (b). Blue portion represents the results when only using clinical \nvariables while the accompanying portion represents results of combin -\ning clinical variables and respective radiology reports\n \nPage 9 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \nscore, research is limited, possibly because LLMs cur -\nrently perform the best when carrying out structured \ntasks like information retrieval [ 50]. However, our BioB -\nERT model showed some predictive potential when \ntrained on the MRI radiology report generated after \nNACRT. We believe this to be the result of a greater \ndescription of soft tissue details [51, 52] which could pro-\nvide more information for the radiologists. Additionally, \nthe fact that the MRI scans were obtained after NACRT \ncould also play a part in its greater predictive potential as \nit can contain information about the changes or improve-\nments of patients’ condition after the NACRT. This could \nhave provided the decisive information about patients’ \nconditions for our LLMs to work with. We also provide \ninterpretability of the LLMs’ predictions, which is a cru -\ncial aspect in the medical field. We employed the SHapley \nAdditive exPlanations (SHAP) framework [ 53] to be able \nto interpret the inner workings of our model. An inter -\npretation of a radiology report correctly classified as hav -\ning a low NAR score can be seen in Fig.  5. Our textual \nmodel was also shown to add value to the clinical model \nas shown in Fig.  5. With its reasonable and trustworthy \ninterpretation of the radiology reports and strong perfor -\nmance, it can serve as a reliable predictive tool for NAR \nscore with further research and experiments.\nAdditionally, the permutation of the sentence ordering \nfor our radiology reports showed substantial improve -\nment, from 0.637 (±0.032) to 0.708 (±0.041) when \ntrained using MRI_Post radiology report. The permuta -\ntion was only applied on the training dataset, increasing \nour training dataset from 112 to 560. This was shown to \nbe a promising option when faced with a limited dataset.\nWe have adopted several thorough measures to ensure \nthe quality and reliability of the study. Throughout our \ntraining process for all models, we employed a five-folds \nnested cross validation to tune the models’ hyperparam -\neters on the validation dataset and report results on an \nunseen test dataset. Throughout the nested cross valida -\ntions, effort was taken to ensure that there was no data \nleakage in the study. Additionally, we showcased several \nof our textual model’s interpretation of radiology reports \nto ensure that the model makes the correct prediction in \na logical manner. We also showed conclusively through \nthe combined textual model’s feature importances that \nthe strong performance was due to the inclusion of the \nprobability logit outputs from the textual model.\nThere are several strengths of our study. To our knowl -\nedge, this is the first deep learning study analyzing 3D \nCT scans and accompanying radiology reports to predict \nfor NAR scores. So far, predictions of NAR scores were \nperformed mostly with radiomics features [ 34, 35, 46] \nextracted out from either CT or MRI scans and fed into \na machine learning model. The studies utilizing CT scans \nwere only able to produce mediocre results and were not \nable to add much value to clinical models [34]. This study \nadds weight behind this finding, that even with Deep \nLearning vision models analyzing CT scans, no improve -\nment can be made to baseline clinical models. This seems \nto solidify the claim that pre-operative CT scans have \nlimited correlation to the NAR values.\nAdditionally, we demonstrated a novel approach for \nNAR prediction, by using LLMs. LLM research in the \nfield of medicine is mostly focused on generative LLMs \nused for chatbots or daily assistant roles [ 26, 48, 49] \nwith the advent of decoder-only architecture LLMs such \nas ChatGPT. There are many publications on the use of \ndeep learning with radiology reports, but they are mainly \nfocused on clinical term extractions, identification of \nkey writings or classification of patient’s condition [ 36, \n54, 55]. To our knowledge, this is the first study to apply \nLLMs to predict the NAR score of patients based on their \nradiology reports. In addition to this novel approach, we \ncarried out a thorough comparison of 4 different radiol -\nogy reports from the patients. Through this comparison, \nwe were able to show that MRI radiology reports gener -\nated after NACRT can contribute meaningfully to a base-\nline clinical model. With this knowledge, future research \nto incorporate MRI radiology reports in the prediction \nof NAR score should be considered in a larger patient \ncohort.\nThere are several limitations to this study. Firstly, as this \nwas an exploratory study, the sample size was small, and \nthe study was conducted in a single center with no exter -\nnal validation cohort. Although we used nested cross-\nvalidation which is more rigorous than a single hold-out \ninternal test set, this was not as rigorous as external vali -\ndation. Secondly, with the data being collected over an \neleven year period, the reporting practices of radiolo -\ngists have changed throughout the years and also differ \nbetween radiologist. This introduces inconsistencies in \nthe data that may affect the performance our model. To \novercome this problem, we only took the conclusion sec -\ntion and if absent, the section that contained the sum -\nmary of the radiology report is used to train our LLMs.\nFuture work will involve external validation and coop -\neration among large institutions to create a large data -\nset to overcome the problem of small sample size found \nin other similar studies [ 34, 35]. Further experiments of \nusing deep learning vision models on MRI scans instead \nof CT scans can be carried out as MRI scans shows bet -\nter predictive potential [ 47] for NAR scores. In addition, \nmulti-modal approaches, using both scans and accompa -\nnying radiology reports, for prediction of NAR scores can \nalso be explored which has been made possible with the \nadvent of models pretrained specifically on biomedical \ndata [ 56, 57]. The immense improvement of LLMs over \nthe past few years and development of smaller, quantized \nversions of these is a future direction of work, where we \nPage 10 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \ncan explore LLMs such as Google’s MedGemma that has \nbeen trained specifically in the medical domain or the \nQwen family of models.\nConclusions\nThis study explored the potential impact of deep learn -\ning approaches for predicting NAR scores. It was found \nthat deep learning vision model trained on 3D CT scans \nis not able to contribute meaningfully to baseline clinical \napproaches. However, LLMs, trained on MRI radiology \nreports generated after NACRT, when combined with \nclinical variables showed statistical improvement from a \nbaseline clinical approach. The resulting information can \naid clinicians with their decisions for treatment options \nbased on the predicted NAR score.\nAcknowledgements\nThis work was supported by the funding support from Ai3 Lab Fund \nand supported by Nanyang Technological University under the URECA \nUndergraduate Research Programme.\nAuthor contributions\nH.B.K. along with T.H.Q., Y.C. and W.L.N. came up with the study conception \nand design, and performed data interpretation and statistical analysis. T.H.Q., \nW.L.N., Y.C.R.S.T. and F.W. performed data acquisition. H.B.K. and T.H.Q. drafted \nup the manuscript and all authors reviewed the manuscript.\nFunding\nSupport for this project from the NCCS Ai3 Lab.\nData availability\nThe data underlying this article is not publicly available. However, the code for \nthis article is available upon reasonable request to H.B.K.\nDeclarations\nEthics approval and consent to participate\nThis study was approved with waiver of informed consent from SingHealth \ncentralized institution review board and all methods were performed in \naccordance with relevant guidelines and regulations. This study adhered to \nthe Declaration of Helsinki.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 7 March 2025 / Accepted: 24 July 2025\nReferences\n1. Siegel RL, Wagle NS, Cercek A, Smith RA, Jemal A. Colorectal cancer statistics, \n2023. CA Cancer J Clin. May 2023;73(3):233–54.  h t t p  s : /  / d o i  . o  r g /  1 0 .  3 3 2 2  / c  a a c . \n2 1 7 7 2.\n2. Bhudia J, Glynne-Jones R, Smith T, Hall M. Neoadjuvant chemotherapy with-\nout radiation in colorectal cancer. Clin Colon Rectal Surg. 2020;33(05):287–\n297.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 5 5  / s  - 0 0 4 0 - 1 7 1 3 7 4 6\n3. Bismuth H et al. Resection of nonresectable liver metastases from colorectal \ncancer after neoadjuvant chemotherapy. Ann Surg. 1996;224(4):509–20. \ndiscussion 520-2.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 7  / 0  0 0 0  0 6 5  8 - 1 9  9 6  1 0 0 0 0 - 0 0 0 0 9\n4. Shao Y-C, et al. Neoadjuvant chemotherapy can improve outcome of \ncolorectal cancer patients with unresectable metastasis. Int J Colorectal Dis. \nOct. 2013;28(10):1359–65.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  0 0 3 8 4 - 0 1 3 - 1 7 1 3 - x.\n5. Habr-Gama A, São Julião GP , Perez RO. Nonoperative management of rectal \ncancer. Hematol Oncol Clin North Am. Feb. 2015;29(1):135–51.  h t t p  s : /  / d o i  . o  r g \n/  1 0 .  1 0 1 6  / j  . h o c . 2 0 1 4 . 0 9 . 0 0 4.\n6. Habr-Gama A et al. Watch and wait approach following extended \nneoadjuvant chemoradiation for distal rectal cancer. Dis Colon Rectum. \n2013;56(10):1109–1117.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 7  / D  C R .  0 b 0  1 3 e 3  1 8  2 a 2 5 c 4 e\n7. Trakarnsanga A, et al. Comparison of tumor regression grade systems for \nlocally advanced rectal cancer after multimodality treatment. JNCI: J Natl \nCancer Inst. Oct. 2014;106(10).  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 3  / j  n c i / d j u 2 4 8.\n8. George TJ, Allegra CJ, Yothers G. Neoadjuvant rectal (NAR) score: a new \nsurrogate endpoint in rectal cancer clinical trials. Curr Colorectal Cancer Rep. \nOct. 2015;11(5):275–80.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 1 8 8 8 - 0 1 5 - 0 2 8 5 - 2.\n9. Ucar G et al. Prognostic and predictive value of NAR score in gastric cancer. J \nGastrointest Cancer. 2021;52(3):1054–1060.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 2 0 2 9 - 0 \n2 0 - 0 0 5 3 7 - 2\n10. Mukai T et al. Importance of the neoadjuvant rectal (NAR) score to the out-\ncome of neoadjuvant chemotherapy alone for locally advanced rectal cancer. \nSurg Today. 2020;50(8):912–919.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  0 0 5 9 5 - 0 2 0 - 0 1 9 6 4 - 1\n11. MAEDA K, et al. Prognostic significance of neoadjuvant rectal score and \nindication for postoperative adjuvant therapy in rectal cancer patients after \nneoadjuvant chemoradiotherapy. In Vivo (Brooklyn). 2020;34(1):283–289.  h t t p  \ns : /  / d o i  . o  r g /  1 0 .  2 1 8 7  3 /  i n v i v o . 1 1 7 7 2\n12. Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-\npropagating errors. Nature. 1986;323(6088):533–536.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / \n3  2 3 5 3 3 a 0\n13. Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep \nconvolutional neural networks. Commun ACM. May 2017;60:84–90.  h t t p  s : /  / d \no i  . o  r g /  1 0 .  1 1 4 5  / 3  0 6 5 3 8 6.\n14. Russakovsky O, et al. ImageNet large scale visual recognition challenge. Int J \nComput Vis. Dec. 2015;115(3):211–52.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 1 2 6 3 - 0 1 5 - 0 8 \n1 6 - y.\n15. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale \nimage recognition. 2014.\n16. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. \n2015.\n17. Vaswani A, et al. Attention is all you need. 2017.\n18. Dosovitskiy A, et al. An image is worth 16x16 words: Transformers for image \nrecognition at scale. 2020.\n19. Brown TB, et al. Language models are few-shot learners. 2020.\n20. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. 2018.\n21. Bychkov D, et al. Deep learning based tissue analysis predicts outcome in \ncolorectal cancer. Sci Rep. 2018;8(1):3395.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 5 9 8 - 0 1 \n8 - 2 1 7 5 8 - 3.\n22. Xu L, et al. Colorectal cancer detection based on deep learning. J Pathol Inf. \nJan. 2020;11(1):28.  h t t p  s : /  / d o i  . o  r g /  1 0 .  4 1 0 3  / j  p i . j p i _ 6 8 _ 1 9.\n23. Khumsi AF, Almezhghwi K, Adweb K. Deep learning based analysis in onco-\nlogical studies. Colorectal Cancer Staging’ . 2020;573–9.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 \n7  / 9  7 8 -  3 - 0  3 0 - 3  5 2  4 9 - 3 _ 7 3.\n24. Clusmann J, et al. The future landscape of large language models in medi-\ncine. Commun Med. 2023;3(1):141.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 3 8 5 6 - 0 2 3 - 0 0 3 7 \n0 - 1\n25. Chang C-H, Lucas MM, Lu-Yao G, Yang CC. Classifying cancer stage with \nopen-source clinical large language models. 2024.\n26. Huo B, et al. Dr. GPT will see you now: the ability of large Language model-\nlinked chatbots to provide colorectal cancer screening recommendations. \nHealth Technol (Berl). May 2024;14(3):463–9.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 2 5 5 \n3 - 0 2 4 - 0 0 8 3 6 - 9.\n27. Alsentzer E, et al. Publicly available clinical BERT embeddings. 2019.\n28. Yang X, et al. A large language model for electronic health records. NPJ Digit \nMed. 2022;5(1):194.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 7 4 6 - 0 2 2 - 0 0 7 4 2 - 2\n29. Gu Y, et al. Domain-Specific Language model pretraining for biomedical \nnatural Language processing. ACM Trans Comput Healthc. Jan. 2022;3(1):1–\n23.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 4 5  / 3  4 5 8 7 5 4.\n30. Lee J, et al. BioBERT: a pre-trained biomedical language representation model \nfor biomedical text mining. 2019.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 3  / b  i o i  n f o  r m a t  i c  s / b t z 6 \n8 2\n31. Wang G, Yang G, Du Z, Fan L, Li X. ClinicalGPT: Large language models fine-\ntuned with diverse medical data and comprehensive evaluation. 2023.\n32. Singhal K, et al. Towards expert-level medical question answering with large \nlanguage models. 2023.\nPage 11 of 11\nKim et al. BMC Medical Imaging           (2025) 25:306 \n33. Yuan Z et al. Pretreatment CT and PET radiomics predicting rectal cancer \npatients in response to neoadjuvant chemoradiotherapy. Rep Pract Oncol \nRadioth. 2021;26(1):29–34.  h t t p  s : /  / d o i  . o  r g /  1 0 .  5 6 0 3  / R  P O R . a 2 0 2 1 . 0 0 0 4\n34. Wang F, et al. Predicting outcomes for locally advanced rectal cancer treated \nwith neoadjuvant chemoradiation with CT-based radiomics. Sci Rep. Apr. \n2022;12(1):6167.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 5 9 8 - 0 2 2 - 1 0 1 7 5 - 2.\n35. Shaish H, et al. Radiomics of MRI for pretreatment prediction of patho-\nlogic complete response, tumor regression grade, and neoadjuvant rectal \nscore in patients with locally advanced rectal cancer undergoing neo-\nadjuvant chemoradiation: an international multicenter study. Eur Radiol. \n2020;30(11):6263–6273.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  0 0 3 3 0 - 0 2 0 - 0 6 9 6 8 - 6\n36. Tan RSYC, et al. Inferring cancer disease response from radiology reports \nusing large language models with data augmentation and prompting. J \nAmerican Med Inform Associa. 2023;30(10):1657–1664.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 \n3  / j  a m i a / o c a d 1 3 3\n37. Masoudi S, et al. Quick guide on radiology image pre-processing for deep \nlearning applications in prostate cancer research. J Med Imaging. Jan. \n2021;8(01).  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 1 7  / 1  . J M I . 8 . 1 . 0 1 0 9 0 1.\n38. Mei X, et al. RadImageNet: an open radiologic deep learning research dataset \nfor effective transfer learning. Radiol Artif Intell. Sep. 2022;4(5).  h t t p  s : /  / d o i  . o  r g \n/  1 0 .  1 1 4 8  / r  y a i . 2 1 0 3 1 5.\n39. Serte S, Demirel H. Deep learning for diagnosis of COVID-19 using 3D CT \nscans. Comput Biol Med. May 2021;132:104306.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . c o  m \np b  i o m e  d .  2 0 2 1 . 1 0 4 3 0 6.\n40. Kataoka H, Wakamiya T, Hara K, Satoh Y. Would mega-scale datasets further \nenhance spatiotemporal 3D CNNs? 2020.\n41. Kim Y, Oh J, Kim S, Yun S-Y. How to fine-tune models with few samples: \nUpdate, data augmentation, and test-time augmentation. 2022.\n42. Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Oct., ‘Grad-\nCAM: visual explanations from deep networks via Gradient-based localiza-\ntion’ , 2016,  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 1 2 6 3 - 0 1 9 - 0 1 2 2 8 - 7\n43. Dif N, Elberrichi Z. A new deep learning model selection method for colorec-\ntal cancer classification. Internat J Swarm Intelli Res. 2020;11(3):72–88.  h t t p  s : /  / \nd o i  . o  r g /  1 0 .  4 0 1 8  / I  J S I R . 2 0 2 0 0 7 0 1 0 5\n44. Tsai M-J, Tao Y-H. Deep learning techniques for the classification of colorectal \ncancer tissue. Electronics (Basel). 2021;10(14):1662.  h t t p  s : /  / d o i  . o  r g /  1 0 .  3 3 9 0  / e  l \ne c  t r o  n i c s  1 0  1 4 1 6 6 2\n45. Foersch S, et al. Multistain deep learning for prediction of prognosis and \ntherapy response in colorectal cancer. Nat Med. Feb. 2023;29(2):430–9.  h t t p  s : /  \n/ d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 5 9 1 - 0 2 2 - 0 2 1 3 4 - 1.\n46. Chong WC, et al. A comprehensive evaluation of MR-radiomics role in NAR \nscore prediction in locally advanced rectal cancer. Royal Coll Radiol Open. \n2023;1:100004.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . r c r o . 2 0 2 3 . 1 0 0 0 0 4.\n47. Liu J, Wang C, Liu S. Utility of ChatGPT in clinical practice. J Med Internet Res. \nJun. 2023;25:e48568.  h t t p  s : /  / d o i  . o  r g /  1 0 .  2 1 9 6  / 4  8 5 6 8.\n48. Li M, Blaes A, Johnson S, Liu H, Xu H, Zhang R. CancerLLM: a large language \nmodel in cancer domain. 2024.\n49. Sorin V, et al. Large language model (ChatGPT) as a support tool for breast \ntumor board. NPJ Breast Cancer. 2023;9(1):44.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 5 2 \n3 - 0 2 3 - 0 0 5 5 7 - 8\n50. Sorin V, et al. Utilizing large Language models in breast cancer management: \nsystematic review. J Cancer Res Clin Oncol. Mar. 2024;150(3):140.  h t t p  s : /  / d o i  . o  \nr g /  1 0 .  1 0 0 7  / s  0 0 4 3 2 - 0 2 4 - 0 5 6 7 8 - 6.\n51. Beiderwellen K, et al. Whole-body [18F]FDG PET/MRI vs. PET/CT in the \nassessment of bone lesions in oncological patients: initial results. Eur Radiol. \n2014;24(8):2023–2030.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  0 0 3 3 0 - 0 1 4 - 3 2 2 9 - 3.\n52. Riola-Parada C, García-Cañamaque L, Pérez-Dueñas V, Garcerant-Tafur M, \nCarreras-Delgado JL. PET/RM simultánea vs. PET/TC En oncología. Una \nrevisión Sistemática. Rev Esp Med Nucl Imagen Mol. Sep. 2016;35(5):306–12.  \nh t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . r e m n . 2 0 1 6 . 0 6 . 0 0 1.\n53. Lundberg S, Lee S-I. A unified approach to interpreting model predictions. \n2017.\n54. Sorin V, Barash Y, Konen E, Klang E. Deep learning for natural Language \nprocessing in Radiology—Fundamentals and a systematic review. J Am Coll \nRadiol. May 2020;17(5):639–48.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . j a c r . 2 0 1 9 . 1 2 . 0 2 6.\n55. Sugimoto K, et al. Extracting clinical terms from radiology reports with deep \nlearning. J Biomed Inf. Apr. 2021;116:103729.  h t t p s :   /  / d o  i .  o r  g  /  1 0  . 1 0   1   6 / j .  j b i .  2 0 2 \n1 . 1 0 3 7 2 9.\n56. Li C, et al. LLaVA-Med: Training a large language-and-vision assistant for \nbiomedicine in one day. 2023.\n57. Eslami S, de Melo G, Meinel C. Does CLIP benefit visual question answering in \nthe medical domain as much as it does in the general domain? 2021.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "Neoadjuvant therapy",
  "concepts": [
    {
      "name": "Neoadjuvant therapy",
      "score": 0.6552776098251343
    },
    {
      "name": "Colorectal cancer",
      "score": 0.6446616649627686
    },
    {
      "name": "Computer science",
      "score": 0.5862994194030762
    },
    {
      "name": "Medicine",
      "score": 0.4101518988609314
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3490108251571655
    },
    {
      "name": "Cancer",
      "score": 0.29283106327056885
    },
    {
      "name": "Internal medicine",
      "score": 0.16474521160125732
    },
    {
      "name": "Breast cancer",
      "score": 0.0
    }
  ]
}