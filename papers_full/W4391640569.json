{
    "title": "An Integrated Smart Contract Vulnerability Detection Tool Using Multi-Layer Perceptron on Real-Time Solidity Smart Contracts",
    "url": "https://openalex.org/W4391640569",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093888531",
            "name": "Lee Song Haw Colin",
            "affiliations": [
                "Singapore Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2225572683",
            "name": "Purnima Murali Mohan",
            "affiliations": [
                "Singapore Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2181331760",
            "name": "Jonathan Pan",
            "affiliations": [
                "Defence Science and Technology Agency"
            ]
        },
        {
            "id": "https://openalex.org/A2799732888",
            "name": "Peter Loh Kok Keong",
            "affiliations": [
                "Singapore Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6767115310",
        "https://openalex.org/W4387870711",
        "https://openalex.org/W4387609289",
        "https://openalex.org/W2970809537",
        "https://openalex.org/W2999378142",
        "https://openalex.org/W2539190473",
        "https://openalex.org/W3176695921",
        "https://openalex.org/W3000032641",
        "https://openalex.org/W3003036212",
        "https://openalex.org/W4296442576",
        "https://openalex.org/W3026203297",
        "https://openalex.org/W6793285265",
        "https://openalex.org/W6769278921",
        "https://openalex.org/W4206573979",
        "https://openalex.org/W4377235325",
        "https://openalex.org/W2107024044",
        "https://openalex.org/W2166766372",
        "https://openalex.org/W2291637985",
        "https://openalex.org/W3083471387",
        "https://openalex.org/W6632118081",
        "https://openalex.org/W6637572315",
        "https://openalex.org/W3174560274",
        "https://openalex.org/W3136521802",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W6843013978",
        "https://openalex.org/W3106007553",
        "https://openalex.org/W4288257480",
        "https://openalex.org/W4295886294",
        "https://openalex.org/W3151748982",
        "https://openalex.org/W3105187050"
    ],
    "abstract": "perceptron (MLP). We use feature vectors from the Opcodes and CFG for the machine learning (ML) model training. The existing ML-based approaches for analyzing the smart contract code are constrained by the vulnerability detection space, significantly varying Solidity versions, and no unified approach to verify against the ground truth. The primary contributions in this paper are 1) a standardized pre-processing method for smart contract training data, 2) introducing bugs to create a balanced dataset of flawed files across Solidity versions using AST, and 3) standardizing vulnerability identification using the Smart Contract Weakness Classification (SWC) registry. The ML models employed for benchmarking the proposed MLP, and a multi-input model combining MLP and Long short-term memory (LSTM) in our study are Random forest (RF), XGBoost (XGB), Support vector machine (SVM). The performance evaluation on real-time smart contracts deployed on the Ethereum Blockchain show an accuracy of up to 91&#x0025; using MLP with the lowest average False Positive Rate (FPR) among all tools and models, measuring at 0.0125.",
    "full_text": "Digital Object Identifier\nAn Integrated Smart Contract Vulnerability\nDetection Tool Using Multi-layer Perceptron on\nReal-time Solidity Smart Contracts\nLEE SONG HAW COLIN1, PURNIMA MURALI MOHAN1, JONATHAN PAN2, and PETER LOH KOK\nKEONG1\n1Infocomm Technology Cluster, Singapore Institute of Technology, Singapore (e-mail: colin.lee, purnima.mohan, peter.loh@singaporetech.edu.sg)\n2Disruptive Technologies Office, Home Team Science and Technology Agency, Singapore (e-mail: Jonathan.PAN@htx.gov.sg)\nCorresponding author: Purnima Murali Mohan (e-mail: purnima@ieee.org).\nThis work was supported by the Singapore MoE grant, Singapore Institute of Technology (SIT) WBS R-MOE-A405-I002.\nABSTRACT Smart contract vulnerabilities have led to substantial disruptions, ranging from the DAO attack\nto the recent Poolz Finance. While initially, the smart contract vulnerability definition lacked standardization,\neven with the advancements in Solidity, the potential for deploying malicious contracts to exploit legitimate\nones persists. The Abstract syntax tree (AST), opcodes, and control flow graph (CFG) are the intermediate\nrepresentations for Solidity contracts. In this paper, we propose an integrated and efficient smart contract\nvulnerability detection algorithm based on Multi-layer perceptron (MLP). We use feature vectors from the\nOpcodes and CFG for the machine learning (ML) model training. The existing ML-based approaches for\nanalyzing the smart contract code are constrained by the vulnerability detection space, significantly varying\nSolidity versions, and no unified approach to verify against the ground truth. The primary contributions\nin this paper are (i) a standardized pre-processing method for smart contract training data, (ii) introducing\nbugs to create a balanced dataset of flawed files across Solidity versions using AST, and (iii) standardizing\nvulnerability identification using the Smart Contract Weakness Classification (SWC) registry. The ML\nmodels employed for benchmarking the proposed MLP, and a multi-input model combining MLP and Long\nshort-term memory (LSTM) in our study are Random forest (RF), XGBoost (XGB), Support vector machine\n(SVM). The performance evaluation on real-time smart contracts deployed on the Ethereum Blockchain\nshow an accuracy of up to 91% using MLP with the lowest average False Positive Rate (FPR) among all\ntools and models, measuring at 0.0125.\nINDEX TERMS Blockchain, Ethereum, Machine learning, Multi-layer Perceptron, Real-time Smart con-\ntracts, Solidity smart contracts, Vulnerability analysis and detection, Code analysis, Software testing.\nI. INTRODUCTION\nI\nN 2022 the amount of funds controlled by smart contracts\nwas worth around USD 1750 million and it is set to reach\nUSD 9850 million by 2030 [1]. The gaining popularity of\nsmart contracts is due to its autonomy, trust, and secure\nenvironment and this has given birth to what we know as\nDecentralised applications ( Dapps) today. Dapps are appli-\ncations that have their logic written in smart contracts and\ndeployed to the blockchain. Dapps covers a wide range of\nuse cases from logistics to finance which enables day to day\nand monitoring of usage. The transparency and traceability\ncharacteristics of a blockchain attract not only private com-\npanies but also keen government agencies. For example, a\npermissioned or private blockchain can be used to manage\nthe flow of currency [2]. Blockchain also finds application\nin sharing data between heterogeneous devices using smart\ncontracts. A framework by [3] suggests using smart con-\ntracts for recording data in the cloud, for data security, and\naccountability of Internet of Things (IoT) devices. Another\nuse case for IoT smart contracts is unmanned aerial vehicles\n(UA Vs) [4] that transform a centralized trusted authority into\na secure decentralized network. However, these works use\nsmart contracts as authentication mechanisms for IoT and\nassume the inherent security of smart contracts.\nSmart contracts have not been immune to hacks, since early\n2016 when major hacks started to surface and gain media\nattention. These hacks are not only harmful to the industry\nbut also cast a large shadow over the longevity of blockchain\napplications. In 2022, approximately 1.9 billion USD have\nbeen lost to various hacks by exploiting vulnerabilities in\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nsmart contract logic and manipulation of human errors in\nsmart contracts[5]. To add on, specifically, Poolz Finance\nsuffered a major arithmetic overflow hack, where one of its\nmethods for pool creation contains a manual summing of\ntoken count which resulted in losing USD 6, 650, 000 [6], [7].\nThere have been attempts to prevent such hacks by creat-\ning contract standards and defining vulnerabilities. OpenZep-\npelin [8], Consensys [9] and a trail of bits [10] are some of the\nfront runners on the quest to support developers and overcome\npotential security vulnerabilities.\nIn terms of standardization, smart contract vulnerabili-\nties have been loosely defined since its emergence. There\nhave been different flavors namely — the Smart Contract\nWeakness Classification (SWC) registry [11], DASP [12]\nand Crytic’s static analyzer, i.e., the Slither’s detector doc-\numentation [13]. These definitions cover most of the notable\nvulnerabilities, however, there is an existing gap in terms of\ncompliance with any security standards and coverage of the\nentire smart contract vulnerability space constrained by the\nvarying solidity versions and ever-emerging smart contract\nlogic bugs. These vulnerabilities, often resulting from human\nerrors and version changes, underscore the need for more ro-\nbust and reliable security measures in smart contracts. While\nsuch vulnerabilities can be detected by software verification\nand validation tools using static [13], [14], dynamic [15], and\nformal verification [14], [16], each method has its own limi-\ntations. In the work done by [17], the authors have conducted\nan extensive test on software verification and validation tools\nand concluded that there is no single analysis tool that can\ndetect all smart contract vulnerabilities. Not only that new\nvulnerabilities cannot be detected if it was not predefined\nwithin the tool, but also existing vulnerability detection had\nsignificant false positive rates. These findings therefore advo-\ncate the use of a Machine Learning (ML) approach that can be\ndynamically trained to newer smart contract bugs and solidity\nversions while reducing the false positive rate.\nFor any ML model the key factors that decide its reliability\nof that model are, (i) the origin of the dataset used for training\nthat model, (ii) the quality and correctness of the training\ndataset, (iii) scalability and run-time of the model, and (iv)\nstandardized verification and validation of the model. Recent\nworks that have claimed to have sourced data from reliable\nsources [18], [19], [20], are still based on the validation of\nsoftware verification and validation tools such as Mythril\n[14], Slither [13], and Oyente [16] to determine the ground\ntruth information about the smart contracts used for training.\nFirstly, the main concern with such reliance on third-party\nsoftware verification tools for the training dataset is that it\nis prone to inaccuracies that are inherent to these software\nverification tools. Secondly, to introduce vulnerabilities into\nthe training dataset, a synthetic data generation method using\nthe Synthetic Minority Oversampling Technique (SMOTE) is\nbeing used [19], [20]. However, synthetic data can never be a\ntrue representation of a truly vulnerable dataset. Not only does\nit not keep updated with the significantly varying Solidity\nversions, but also leads to an imbalance in the training dataset\ndue to the way it is implemented. Thirdly, some existing\nworks simply skip the pre-processing steps (that ensures the\nquality and correctness of the training dataset) [18] which\nmakes the solution practically not useful — especially when\nthe training data set that comes across different solidity ver-\nsions, contains commented codes with code-like syntax. This\nbecomes a prime concern since the solc compiler cannot\ndifferentiate code-like commented syntax — which is crucial\nfor bug-insertion algorithm. Also, the existing bug-injection\ntools [21] suffer from practical issues such as solidity version,\nsyntax based on solidity version, no exception, and no control\nover bug injection logic (i.e., the existing tools dump all\nbugs in a single smart contract). Motivated by these practical\nconcerns, our proposed solution injects known vulnerability\npatterns into clean contracts by ensuring a clear indication of\na vulnerability bug type in each contract while developing a\nstandardized pre-processing method to generate a balanced\nand good quality training dataset and validate against well-\ndefined vulnerability standards.\nIn this paper, we will be introducing an ML approach\nthat uses a runtime opcode extraction algorithm for feature\nextraction and a trigram-based method for vectorization. We\nreference the SWC [11] registry for smart contract vulnerabil-\nity categorization as it has been one of the most well-defined\nmappings while loosely coupled with the Common Weakness\nEnumeration (CWE) [22]. To test the efficacy of our solutions\nand perform benchmarking, we will employ Mythril, Slither,\nand an integrated tool known as MythSlith. We have devel-\noped MythSlith — which is a tool that integrates Mythril and\nSlither to increase the coverage of smart contract vulnerability\ndetection space. These tools will be used to compare against\nthe ML model trained with the bug-injected dataset. Our\ncontributions in this paper are summarized as follows:\n• We developed a standardized pre-processing algorithm\nfor cleaning smart contract training data to address the\nlimitation of the solc compiler not being able to differ-\nentiate code from commented code-like syntax.\n• We developed a practical bug injection algorithm to cre-\nate a balanced dataset across Solidity versions using the\nAbstract syntax tree on verified smart contracts that were\ncleaned using the proposed pre-processing algorithm.\n• We model an MLP framework and a multi-input model\nbased on MLP and LSTM for smart contract vulnera-\nbility detection. The proposed framework scales up the\nsmart contract vulnerability space by utilizing opcodes\nand Control Flow Graph (CFG) extracted during model\ntraining. Before vectorization, a simplification method\nis applied to the opcodes to decrease dimensionality (re-\nduce the running time) and eliminate contract-specific\nhexadecimal values.\n• We thoroughly analyze the time complexity of the pro-\nposed algorithms and the running time for bug detection.\n• For MLP framework performance benchmarking, we\nintegrate the well-known software verification tools,\nMythril and Slither, to develop an experimental tool\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nknown as MythSlith which has an increased smart con-\ntract vulnerability detection space than the individual\ntools. The machine learning models show a superior per-\nformance in vulnerability detection than existing Soft-\nware verification tools with the lowest false positive rate\nof up to 0.0015.\n• We verify the results against the standardized vulnerabil-\nity identification — Smart Contract Weakness Classifi-\ncation (SWC) registry as a common analysis platform.\nThe rest of the paper is organized as follows: Section II\nprovides a background of smart contract intermediate repre-\nsentation, smart contract vulnerabilities, and types of software\nvalidation techniques used. Section III presents a literature\nreview of the recent works on smart contract vulnerability\ndetection space. In Section IV, we illustrate the proposed\nmethodology and framework for feature extraction to model\ntraining algorithms. Section V reports the experimental find-\nings and inferences made on the efficiency of the proposed\nmethods. Section VI will discuss the findings, some possible\nalternatives, and future development before we conclude our\nwork in Section VII.\nII. BACKGROUND\nIn this section, we provide an overview of the vulnerabilities\nand tools that will be used in the proposed algorithm for smart\ncontract vulnerability detection.\nA. SMART CONTRACTS\nSmart contracts are programs written to be executed on the\nblockchain. Designed to be autonomous, self-sufficient and\nexpected to do their written or agreed-upon task in code\nwithout interference. They are considered to be the key to a\ndecentralized system. However, being in the eye of the public\nblockchain any vulnerabilities could lead to a huge amount of\nfinancial loss.\nSmart contracts can also be represented in different forms\nwith the help of the Solidity compiler. Some forms are op-\ncodes, abstract syntax tree (AST), and Control Flow Graph\n(CFG).\nOpcodes also known as operation code, are instructions for\nthe Ethereum virtual machine (EVM) to execute any sequen-\ntial and conditional actions. The complete list of opcodes with\ndescriptions can be found in Ethereum’s yellow paper [23].\nAbstract Syntax Tree is a hierarchical tree representation\nof the synthetic structure of the source code. Each section\nis represented as nodes, which construct the details of the\nreal syntax. Such representation is commonly used for syn-\ntax checking, semantic analysis, code generation, and code\noptimization.\nControl Flow Graph is the representation of program\nflow from the context of the stack. This is derived from the\nopcodes, where a bunch of opcodes is broken down into basic\nblocks by flow conditions such as JUMP, JUMPI, REVERT,\netc.\nB. TYPES OF BUGS\nSmart contract vulnerabilities are often caused by oversights\nduring the programming stage, these bugs may seem harmless\nwhen written but can potentially cause huge financial loss.\nThe 7 vulnerabilities below are chosen as they are commonly\nfound in other studies and they have become the founda-\ntion for vulnerabilities in both empirical and ML-based ap-\nproaches. [21], [24], [19], [25]. The availability of predefined\nbug snippets in [21] also helps cut down the amount of\ndevelopment time.\nReentrancy was first uncovered in 2016 when a large sum\nof money was stolen in the DAO contract (Fig 1). This was\nprimarily caused by the action of sending cryptocurrency to\nan external account and updating it only after it was sent.\nThis sequence of events might seem normal for traditional\nsoftware code, however, in the case of Solidity, a fallback\nfunction of the external account can re-trigger the same send-\ning function again before an update can happen within the\noriginal contract. This could result in an indirect recursive\nfunction call.\nmapping(address => uint) userBalance_re_ent5;\nfunction withdrawBalance_re_ent5() public {\nif(!(msg.sender.send(userBalance_re_ent5[msg.sender])))\n{\nrevert();\n}\nuserBalance_re_ent5[msg.sender] = 0;\n}\nFIGURE 1: Reentrancy snippet\nArithmetic vulnerabilities, more commonly known as\nOverflow-Underflow (Fig 2). Overflow occurs when an oper-\nation tries to add to a variable that is already at its maximum\npossible value. Without any sort of guard, this variable will\noverflow and back to 0 or the minimum possible value. As\nopposed to Overflow, Underflow happens when an operation\ntries to subtract a variable when it is at the minimum possible\nvalue, resulting in the value jumping to the maximum possible\nvalue. This vulnerability can lead to severe security issues\nas hackers can use this behavior to alter account balance or\nchange ownership of contract.\nmapping(address => uint) balances_intou2;\nfunction transfer_undrflow2(address _to, uint _value)\npublic returns (bool)\n{\nrequire(balances_intou2[msg.sender] - _value >= 0);\nbalances_intou2[msg.sender] -= _value;\nbalances_intou2[_to] += _value;\nreturn true;\n}\nFIGURE 2: Arithmetic snippet\nUnauthorized send arises when there is no access control\nfor a function that requires an access check (Fig 3). Such\na function may contain withdrawal or reward disbursement\nfunctionality.\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfunction bug_unauth_send5() payable public\n{\nmsg.sender.transfer(1 ether);\n}\nFIGURE 3: Unauthorized send snippet\nTransaction origin also known as tx.origin, arises\nfrom the misuse of the global variable ’tx.origin’ of Solidity\n(Fig 4). In all transactions there is an origin and a sender.\nOrigin is the address that started the chain of calls while\nthe sender is the address that initiated the current call. Tx\norigin vulnerability occurs when a contract uses ’tx.origin’\nto authenticate a user rather than ’msg.sender’, which refers\nto the current immediate caller. This is particularly dangerous\nwhen a tranferOwnership function uses ’tx.origin’ for\nauthentication.\nfunction transferTo_txorigin3(address to, uint amount,\naddress owner_txorigin3) public\n{\nrequire(tx.origin == owner_txorigin3);\nto.call.value(amount);\n}\nFIGURE 4: Tx origin snippet\nTimestamp dependency arises when a contract uses block\nvariables such as block hash, timestamp, number, difficulty,\ngaslimit and coinbase to perform critical operations (Fig 5).\nThese operations are generation of random numbers and time\ncritical applications such as auction. This is in particular dan-\ngerous because miners gets to choose the block’s timestamp.\nfunction bug_tmstmp5() view public returns (bool) {\nreturn block.timestamp >= 1546300800;\n}\nFIGURE 5: Timestamp dependency snippet\nTransaction Order Dependency is a type of vulnerability\nwhere the sequence of calling transactions can impact the\nfinal outcome of the application (Fig 6). This arises when a\nstate is altered based on the order of incoming transactions,\nwhich can lead to unintended exploitation.\nbool claimed_TOD10 = false;\naddress owner_TOD10;\nuint256 reward_TOD10;\nfunction setReward_TOD10() public payable {\nrequire (!claimed_TOD10);\nrequire(msg.sender == owner_TOD10);\nowner_TOD10.transfer(reward_TOD10);\nreward_TOD10 = msg.value;\n}\nfunction claimReward_TOD10(uint256 submission) public {\nrequire (!claimed_TOD10);\nrequire(submission < 10);\nmsg.sender.transfer(reward_TOD10);\nclaimed_TOD10 = true;\n}\nFIGURE 6: Transaction Order Dependency\nUnhandled Exceptions occurs when checks on send,\ntransfer, or call are not done (Fig 7). This is important because\ncalls can fail and intended changes will be reverted. Some\nof the reasons for failure can be, out-of-gas exceptions and\nwrong arithmetic operations such as zero division errors.\nbool public payedOut_unchk9 = false;\nfunction withdrawLeftOver_unchk9() public {\nrequire(payedOut_unchk9);\nmsg.sender.send(address(this).balance);\n}\nFIGURE 7: Unhandled Exceptions\nIII. LITERATURE REVIEW\nIn this section, we will discuss currently available software\nverification and validation tools for smart contract detection,\nas well as works that use a machine learning approach. Our\nreview will cover the data sampling techniques, feature ex-\ntraction methods, and smart contract vulnerability standards.\nAn overview of the comparison can be found in Table 1.\nA. SOFTWARE VERIFICATION AND VALIDATION TOOLS\nIn recent years, a number of analysis tools have been in-\ntroduced and they can be categorized into 3 different types,\nStatic, Dynamic, and Formal verification. Static tools rely\non the static information of the code to derive a prediction\nwithout executing the program [27]. Some of those features\nextracted are the abstract-syntax tree (AST), compiled byte-\ncode, and opcodes. Dynamic tools analyze a running pro-\ngram, one such example is the Fuzzer [28]. Formal Verifica-\ntion tools rely on the mathematical definition and use a solver\nsuch as Z3 to resolve the derived formula [29].\nAmong the software verification and validation tools for\nsmart contracts are Slither [13], Mythril [14], and De-\nfectChecker [30] for static verification; Manticore [15] for dy-\nnamic analysis; and Oyente [16], Mythril, and DefectChecker\nfor formal verification. Although these tools have been in-\nstrumental in numerous audits, they are constrained by the\npredefined patterns of each bug. Should new bugs emerge, an\nexpert update would be necessary.\nB. MACHINE LEARNING BASED VULNERABILITY\nDETECTION\nAs the name suggests, ML method is used to construct a\nset of decisions from the features of the data to make a\nlogical conclusion of a trend or classification. Such a set\nof decisions is known as a model and it can be done with\nsupervised or unsupervised learning. Supervised learning is\nan algorithm that learns with labeled data while unsupervised\nwill suggest clusters of possible outcomes and then deriving\nof the outcome will depend largely on the feature set.\n1) Data sampling\nAs discussed in Section I, data integrity is an integral part\nof ML model training but existing works from [18], [19],\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1: Comparison with existing works\nReference Data Source Labeling Pre-processing Mapping to SWC Bug Injection Validation\nSolidity Version Comments removal\n[18] Mythril, Slither ✓ ✗ ✗ ✗\n[19] Oyente ✗ ✗ ✗ ✗\n[20] Mythril, Slither ✗* ✗ ✗ ✗\n[26] Oyente, Slither, DefectChecker ✗ ✓ ✗ ✗\n[21] Bug Injection ✗ ✗ ✗ ✗\nOur work Bug Injection ✓ ✓ ✓ ✓\n∗ Author stated as pre-processing done but with no steps/details\n[20] did not have a cleaning or pre-processing step to en-\nsure contracts are clean. While [26] has done checks using\nthree software validation tools namely, Slither, Oyente, and\nDefectChecker to ensure contracts are properly labelled and\nalso removed if any error were present in the tool’s output,\nin addition, smart contracts without version numbers were\nalso removed. However, the labeling of data relies on software\nvalidation tools, which face the same issue as previous works,\ni.e., the training dataset is prone to inaccuracies inherent to\nthese tools.\nWhereas in our approach, bug injection ensures identified\nbugs to be injected. However, bug injection from Solidifi [21]\nlacks post-injection error checking, leading to the creation of\nan erroneous dataset. Moreover, the code from [21] did not\nanticipate a situation where the bug count is less than the\navailable injection location. To accommodate such scenarios,\na recursive function was incorporated to check on the bug\ncount and the number of available locations. In addition,\nwe have also incorporated a validation and error-handling\nmechanism into the proposed bug injection algorithm.\n2) Feature Extraction\nUsing opcodes for feature extraction is one of the more\npopular methods as it is agnostic to the expert pattern and\npresents a clear path to identify any malicious act. Abstract\nSyntax Tree (AST) is also one of the common methods as it\ncontains useful semantic information. [19] uses a simplified\nopcode followed by Bigram for vectorization, while [18] uses\nfeatures from AST, [20] and [26] uses a mixture of AST and\nsimplified Opcode with Bigram.\nWhile traditional models typically process data in a single\ncommon format, this does not preclude the combination of\nvectorized data from various features. Works of [20] and [26]\nhave both mixed their feature embeddings and can produce\na good amount of variance for classification. However, in\nour proposed work aside from the classical models, we will\nimplement a multi-model approach that allows features of\ndifferent shapes to collaborate effectively.\nC. VULNERABILITY STANDARDS\nVulnerability standards in the smart contract field are not\nyet prevalent, resulting in a spew of different definitions by\ndifferent organizations [12], [11]. This is not healthy for\nthe industry and will hinder further development. It is also\nclear from existing works [18], [19], [20] and [26], that no\nvulnerability standards were taken up. Consequently, in our\nproposed work, we have selected the SWC Registry [11] as\nthe vulnerability standard to provide a clear definition.\nIn the proposed solution, opcodes will be used alongside\nCFG features. Trigram is used instead as it captures more\ncontext than Bigram or unigram. Rather than using AST, CFG\nis chosen because it contains flow information which AST\ndoes not. Simplification of opcodes is done but different from\nprevious works, rather than replacing the entire set of PUSH,\nDUP, and SWAP opcodes with a constant, we will leave the\nfirst five numbers untouched, allowing more variance to be\ncaptured. In addition, we have also removed all hexadecimal\nvalues to prevent any contract-specific values which could be\nlearned by the models.\nThis review has highlighted the need for expert knowl-\nedge to define new bugs for current software verification and\nvalidation tools, significant variations in data sampling and\nfeature extraction methods, and the absence of vulnerability\nstandards. While each work does give clear definitions of\ntheir selected vulnerabilities, there is no compliance. Data\nsampling from [26] have implemented a robust method using\nvalidation tools for labeling, others have not, raising concerns\nabout data integrity. Opcode is a popular representation for\nfeature extraction and mixing of features is a popular method\nas seen in works of [20] and [26] but a multi-model ap-\nproach has yet to be attempted. The disarray in vulnerability\nstandards indicates the need for a single, universally adopted\nstandard to streamline the development process. The insights\ngained from this review provide a good foundation for de-\nveloping a robust and reliable smart contract vulnerability\ndetection method.\nThe research gap is summarized in Table 1 highlighting the\nnovelty of the paper. In this paper, we perform standardized\npre-processing steps, by introducing both erroneous solidity\nversion exclusion and code-like comments removal which\ncould generate an incorrect AST. For data source labeling\nwe use bug injection in our work as it provides a reliable\nground truth. This was not done in the referenced existing\nworks in Table 1. While the work in [21] uses bug injection,\nit does not validate the bug injection nor include any pre-\nprocessing steps. The most recent work that performs pre-\nprocessing by removing code-like comments is found in [26].\nHowever, it does not verify incorrect solidity versions (during\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 8: Pre-processing Flow Diagram for the ML model Training\nTABLE 2: Variables used in the Bug Injection Framework.\nVariable Description\nfc The file contents. This is the data that will be manipulated\nby the algorithm.\nf ∗\nc new file contents after bug injection\nbselected selected bug\nnb The number of bugs. This is one of the factors that deter-\nmines how many times the file will be modified.\nLfDef List of functions obtained from AST.\nfselected Selected function from list of functions obtained from AST\nB Set that represents all the bugs\nBused Set that represents all used bugs\nBunused Set that represents all unused bugs\nAST Abstract Syntax Tree of the current file content\nSOLC Solidity compiler\npre-processing) while relying on third-party tools (such as\nMythril, Slither, Oyente, DefectChecker, etc.) to label their\ndatasets.\nIV. METHODOLOGY\nThe approach to this research will be detailed in this sec-\ntion in the following sequence: IV-A Preparation of dataset,\nIV-B Feature extraction, IV-D Machine Learning Models for\nClassification, IV-E Multi-Model Approach, IV-F Design of\nMythSlith, IV-G Challenges. A flow diagram of the end-to-\nend pre-processing steps that lead to the ML model training\nis illustrated in Figure 8.\nA. PREPARE DATASET FOR ML TRAINING\n1) Dataset\nFor any ML model to be efficient and usable, generating an\nerror-free and practical dataset is important to achieve good\naccuracy and false positive rate. As a first step, the clean\ndataset was initially sourced from the smart contract sanctu-\nary [31] — an open-sourced repository and we validated the\nground truth by compiling each Solidity file using the Solidity\ncompiler (solc) to ensure no errors were present before any\nbug injection was performed. The version of the compiler\nused for each file is determined with a JSON file provided\nby [31], which consists of the specific version used when the\nsmart contract was actually deployed to the mainnet.\nThe second step is to use the validated clean dataset and\ninject with bugs defined by Solidifi [21] — which is an au-\ntomated bug injection tool that checks for potential locations\nusing AST tree to inject a set of predefined bugs. However,\nSolidifi cannot be directly used to inject bugs since it suffers\ndrawbacks such as there is (i) no solidity version check,\n(ii) no syntax check, (ii) no exception for bugs, and more\nimportantly, (iv) it injects all the predefined bugs to a single\nsolidity smart contract which is not a practical scenario to\ntrain the ML model. We hence propose two pre-processing\nalgorithms and a bug injection algorithm below to mitigate\nthe drawbacks of Solidifi.\n2) Bug injection\nThe goal of employing the bug injection technique is to imi-\ntate the introduction of bugs by developers [21] in the smart\ncontract logic. The number of bug snippets is determined by\na predefined bug density . The bug density is defined as the\nnumber of vulnerable lines of code per clean smart contract.\nRefer to Table 2 for the variables used in the Bug injection\nalgorithm. For example, for every 100 line of code, when\nwe insert 1 line of bug, then the bug density for that smart\ncontract will be 1%. By setting the bug density, we are able\nto have an even spread of bugs within the entire dataset used\nfor training the ML model. To be uniform across the smart\ncontract vulnerability space, we represent each bug snippet\nas a function.\nThe process of injecting bugs has the following steps:\nStep1: Pre-process the source file using Algorithm (1)\nStep2: Obtain source attribute information by generating\nthe abstract syntax tree (AST)\nStep3: Filter the function definition within the smart con-\ntract definition that is generated by the AST using\nAlgorithm 2\nStep4: Generate the number of bugs to be injected per\nSolidity file based on a pre-defined bug density\nStep5: Randomly select a function location ( LfDef ) from\nthe AST node and randomly choose a bug ( bselected )\nfrom the list of smart contract bugs to insert the\nbug to the identified random location using the\nAlgorithm 3\nPre-processing Solidity files: Cleaning Solidity files is\na crucial pre-processing step, as the solc compiler cannot\ndistinguish between commented code and code-like syntax.\nThis distinction is especially vital in our implementation, as\nit impacts the injection process and, consequently, influences\nthe false positive rates in the bug classification process. The\ndetails of the pre-processing algorithm are outlined in Algo-\nrithm (1).\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAlgorithm 1 Pre-processing Procedure\n1: procedure PreProcess(file_path, output_directory)\n2: lines ← ReadLines(file_path)\n3: new_lines ← []\n4: inside_comment_block ← False\n5: is_tgt_with_start_end_block ← False\n6: is_inside_unique_comment_block ← False\n7: for line ∈ lines do\n8: if \"/*/\" in line then\n9: Toggle is_inside_unique_comment_block\n10: Continue\n11: end if\n12: if is_inside_unique_comment_block then\n13: Continue\n14: end if\n15: if \"/*\" and \"*/\" in line and \"/*/\" not in line then\n16: Remove inline comment in line\n17: end if\n18: if \"/*\" in line and \"//***\" not in line then\n19: inside_comment_block ← True\n20: Remove text after \"/*\" in line\n21: end if\n22: if \"*/\" in line then\n23: inside_comment_block ← False\n24: Remove text before \"*/\" in line\n25: end if\n26: if inside_comment_block then\n27: if is_tgt_with_start_end_block then\n28: is_tgt_with_start_end_block ← False\n29: Append line to new_lines\n30: end if\n31: Continue\n32: end if\n33: if \"http://\" or \"https://\" in line then\n34: Append line to new_lines\n35: Continue\n36: end if\n37: if \"//\" in line then\n38: Remove text after \"//\" in line\n39: end if\n40: Append line to new_lines\n41: end for\n42: file ← Join(new_lines)\n43: Write file to output_directory\n44: return file\n45: end procedure\nSimilar to the approach used in [21] bug locations are\nderived from AST nodes of the given Solidity files. The\ninjection process is shown in detail in Algorithm (2) wherein\nthe fuction is filtered from the AST tree and in Algorithm (3)\nwhere it takes in the file contents, number of bugs, the list of\nall bugs to uniquely insert as inputs to recursively insert the\nbugs in randomly selected function location until one of the\nAlgorithm 2 Filtering Function Definition from AST\n1: procedure F(AST)\n2: Initialize LfDef ← ∅\n3: for n ∈ N do\n4: t ← τ(n)\n5: if t = ‘‘FunctionDefinition\" then\n6: LfDef ← LfDef ∪ {n}\n7: end if\n8: end for\n9: return LfDef\n10: end procedure\nbelow conditions are met.\n• the function locations are exhausted\n• the bug density condition is satisfied\n• all available bug snippets are exhausted\nThe above condition checks will only be done after a\ncompilation check using solc after every bug injection. This\nensures that the new file is free of errors, i.e., an error-free\ndataset for training the ML model.\nAST Instance: When defining the AST instance in this\nsection, we will only take the attribute of interest into consid-\neration. Let T be the set of node types, including ‘Function-\nDefinition’, ‘PragmaDirective’, ‘ContractDefinition’, ’Vari-\nableDeclaration’, etc. Let N be the set of nodes, where node\nn ∈ Nand n has a type from T . Let S be the set of all possible\n‘src’ attributes, representing the location of a particular node\nin the code. We define the below functions to define the AST\ntree instance per solidity file.\n• τ : N →T : Maps each node to its type (corresponding\nto the ‘nodeType’ attribute).\n• C : N → 2N : Maps each node to its child nodes,\ncapturing the hierarchical structure of the code.\n• σ : N →S: Maps each node to its ‘src’ attribute.\nThe AST is then represented as a 4-tuple containing the\nnodes, their types, child nodes, and source locations.\nAST = (N, τ,C, σ)\nFunction Extraction: Next, we define the function that\nextracts only the‘FunctionDefinition’ from the AST nodes.\nDefine a function F : AST → Lto extract nodes with\nthe ‘nodeType’ of ‘FunctionDefinition’ from the AST. The\nfunction extraction step operates as follows:\n• It takes the AST as input, where the AST is represented\nas AST = (N, τ,C, σ).\n• It iterates through the nodes in N, checking the ‘node-\nType’ attribute for each node using the function τ : N →\nT .\n• If the ‘nodeType’ of a node is ‘FunctionDefinition’, it\nadds the node to the resulting list.\n• It returns the list of nodes with the ‘nodeType’ of ‘Func-\ntionDefinition’ as the output.\nLfDef ← F(AST )\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAlgorithm 3 Bug Injection Algorithm\n1: procedure InjectBugRecursively(fc, nb, B, Bused)\n2: AST ← SOLC(fc)\n3: LfDef ← F(AST )\n4: Bunused = B − Bused\n5: fselected ← R(LfDef)\n6: if (fselected = None) or\n7: (|Bused| = nb and |Bunused| = 0)then\n8: Return fc\n9: end if\n10: bselected = R(Bunused)\n11: f ∗\nc = SliceIn(fc, bselected, fselected)\n12: B∗\nused = Bused ∪ {bselected}\n13: Comment: Recursive call to continue bug injection\n14: InjectBugRecursively(f ∗\nc , nb, B, B∗\nused)\n15: end procedure\nBug density: Selection of the number of bugs is done with\na bug density value, which is a predefined ratio of 0.01, and\neach clean contract will have at least 1 bug inserted:\nLet S represent the number of lines of code in the file,\nand let δ be the predetermined constant ratio representing bug\ndensity. The variable nb, which denotes the number of bugs\nto be injected, is given by:\nnb = max(1, ⌊S ×δ⌋)\nHere, max(1, . . .) ensures that the minimum number of bugs\nis at least 1, and ⌊. . .⌋ represents the floor function, rounding\ndown to the nearest integer.\nInjection of smart contract bugs: With each iteration of\nbug injection, used bugs will be tracked, hence no bugs will be\nrepeated which will result in compilation failure. To represent\nthis, we will let B be the set of all available bug files, and let\nBused be the set of bug files that have already been used. The\nset of remaining bug files Bunused can be represented as:\nBunused = B − Bused\nHere, B − Bused denotes the set difference, which results in a\nnew set Bunused containing all the elements that are in B but\nnot in Bused.\nA random function definition node, which has the source\nlocation, will be selected for each bug. This is represented by\nfselected and it is selected from LfDef:\nfselected =\n(\nR(LfDef) if LfDef ̸= ∅\nNone otherwise\nHere, R(LfDef) represents the random selection of a function\ndefinition node from LfDef. If LfDef is empty, fselected will be\nset to None.\nSimilar to how a function definition node is selected, bug\nis also selected randomly and represented as bselected which is\nselected from the set R of remaining unused bugs:\nbselected = R(Bunused)\nWe will be slicing the selected bug bselected with the given\nfselected into the fc. The slicing location is determined by the\nvalues within the source attribute, where the start and length\nvalues are represented in sequence while delimited by colons.\nGiven that, slicing is done to place in the bug snippet and it is\nrepresented by the following function: Let f ∗\nc be the new file\ncontent that has the bug injected:\nf ∗\nc = SliceIn(fc, bselected, fselected)\nFinally, after each successful injection bselected will be ap-\npended into B∗\nused. The updated set B∗\nused of used bugs can be\nobtained as:\nB∗\nused = Bused ∪ {bselected}\nAfter each injection, B∗\nused will be saved and kept for feature\nextraction use later on.\nWith the bug injection process explained, we now transition\nto discussing our feature extraction methodologies.\nB. FEATURE EXTRACTION\nModels will be built with features extracted from smart con-\ntracts that have been injected with the predefined bugs as\nmentioned in the previous section. However, there is one\nkey thing to be noted, as each injection location is randomly\nselected we would not have the same set of code for the 7 cate-\ngories of smart contract bugs. This will be further explained in\neach feature extraction algorithm section. The vulnerabilities\nto be injected are Reentrancy, Arithmetic, Unauthorized send,\nTransaction Origin, Timestamp dependency, Transaction Or-\nder Dependency, and Unhandled Exceptions.\n1) Opcodes\nOpcodes are obtained by using SOLC by using the input\nand output JSON method — which is also the recommended\nway that has a consistent interface throughout all compiler\nversions. As not all contracts within each Solidity file will\nhave a bug injected, a cross-check will be needed to only take\nin the opcode from injected contracts. Hence, given the file\nname and B∗\nused, we will sift out contracts by comparing the\nfunction name with the Abstract Binary Interface (ABI). The\nfull algorithm of the Opcode extraction process can be found\nin Algorithm (4).\nAs a next step, to describe the sifting process, let’s consider\nC to be the set of m contracts returned by the solc. Each\ncontract in C contains both an Application Binary Interface\n(ABI) and an ordered list of opcodes. Formally, C is defined\nas:\nC = {(ABI1, O1), (ABI2, O2), . . . ,(ABIm, Om)}\nwhere ABIi represents the Application Binary Interface for\nthe ith contract, and Oi represents the ordered list of opcodes\nfor the ith contract. Also, ABIi = {G1, G2, . . . ,Gr }, where\neach Gi is a function descriptor with a maximum number of\nr function descriptors per ABI.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAlgorithm 4 Extract opcodes from bugged files\n1: procedure SimplifyOpcode(fc, B∗\nused)\n2: Initialize Lopcodes ← ∅\n3: C ← SOLC(fc)\n4: for (ABI, O) ∈ C do\n5: found_match ← False\n6: for bug ∈ B∗\nused do\n7: for Gi ∈ ABI do\n8: if namei = bug.name then\n9: found_match ← True\n10: O∗ ← Simplify(O)\n11: Lopcodes ← Lopcodes ∪ {O∗}\n12: end if\n13: end for\n14: if found_match then\n15: Break\n16: end if\n17: end for\n18: end for\n19: return Lopcodes\n20: end procedure\nEach function descriptor Gi could be represented as a tuple:\nGi =\n\u0000\nconstanti, inputsi, namei, outputsi,\npayablei, stateMutabilityi, typei\n\u0001\nEach element in Gi would map to the corresponding prop-\nerty in the ABI JSON.\nFollowing this, opcodes will be simplified. Simplification\nis done because opcodes such as PUSH have 32 variations\nwhile SWAP, and DUP have 16. Where each variation rep-\nresents the number of bytes to be pushed on the stack. i.e.,\nPUSH1 - 1 byte, PUSH4 - 4 bytes. The simplification rules\nenforced are similar to [19] and can be found in Table 3. With\nthis simplification, the number of opcodes remaining will\nonly be 77, thus reducing the dimension of the feature vector.\nWe then employ the n-gram algorithm [32] for feature\nextraction. In natural language processing and computational\nlinguistics n-gram is widely used as a calculation of the fre-\nquency distribution of a selected n-number of tokens, where\nn refers to the number of adjacent elements to consider from\na string of tokens. Unigrams, bigrams, and trigrams are some\nexamples of n-grams where n is 1,2 or 3, respectively [19]. For\nthis paper, we have chosen the trigram approach for feature\nextraction. This choice is motivated by the need to capture\nmore information while maintaining scalability, particularly\nas additional bugs are incorporated, allowing for more syntax\nto be captured for precise classification. Additionally, all hex-\nadecimal values are removed, as they are unique to each smart\ncontract and do not provide any flow information relevant to\nbug identification.\nThe vectorizing of text features is done by Term\nFrequency-Inverse Document Frequency ( Tfidf ). Tfidf can\nbe broken down into two sections, 1) Term Frequency, 2)\nInverse Document Frequency. 1) Term frequency will re-\nflect frequently occurring sections of a document by using a\nweighting factor, in which the weight increases proportionally\nto the number of times a word appears in the document.\nHowever, this is offset by 2) Inverse Document Frequency\nwhere it buries commonly appearing words and highlights\nrare occurring words. This allows unique features to surface.\nPost extraction, we are left with a sparse matrix. To further\nreduce the dimension for model training, SelectKBest based\non Chi-square and TruncatedSVD from scikit learn API [33]\nare both employed. SelectKBest will select the best 2000 fea-\ntures from the sparse matrix followed by the TruncatedSVD.\nRather than the more commonly known dimension reduction\nmethod — Principle Component Analysis (PCA), which does\nnot work on sparse matrix. TruncatedSVD is able to work\non it efficiently because it does not center the data before\ncomputing the single value decomposition. The number of\ncomponents are based on the cumulative variance of 95%\nfrom the selected 2000 features. With this method, we will\nobtain the features that have a cumulative variance of at least\n95% and thereby reducing the number of features.\nUpon completion of these steps, the Opcode trigram feature\nvector is primed and ready for the machine learning model\ntraining.\nC. CONTROL FLOW GRAPH\nOther than extracting static features from opcodes using\nn-gram, constructing a CFG has more benefits as it contains\nsequence data from the runtime opcodes. Allowing much\nmore intrinsic patterns to surface. In this work, we employ\na CFG builder from Ethersolve [34]. It is a tool that uses\nsymbolic stack execution to resolve jump destination, result-\ning in accurate edges. As Ethersolve is built using Java, we\nhave utilised its’ core module by creating a wrapper in python\ncontained within a docker instance.\nSimilar to the process done in opcode for contract selec-\ntion, we have done the same by obtaining deployed bytecode\nfrom solc and only process contracts that has bug function\ninside the ML model training. Full algorithm can be found in\nAlgorithm 5. Below we will describe each representation.\nLet C be the set of contracts returned by the solc. Each\ncontract in C contains both an Application Binary Interface\n(ABI) and an ordered list of opcodes. Formally, C is defined\nas:\nC = {(ABI1, BC1), (ABI2, BC2), . . . ,(ABIm, BCm)}\nwhere, ABIi represents the Application Binary Interface for\nthe ith contract, and BCi represents the ordered list of deployed\nbytecode for the ith contract.\nGraph information was subsequently extracted using\nPecanPy [35], a fast, efficient, and parallelized Python im-\nplementation of Node2Vec [36]. Although Node2Vec is adept\nat learning low-dimensional representations of nodes within a\ngraph, it lacks parallelization in its random walks. Addressing\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAlgorithm 5 Extract CFG from bugged smart contracts\n1: procedure RuntimeBytecode(fc, B∗\nused)\n2: Initialize Lcfg ← ∅\n3: C ← SOLC(fc)\n4: for (ABI, BC) ∈ C do\n5: found_match ← False\n6: for bug ∈ B∗\nused do\n7: for Gi ∈ ABI do\n8: if namei = bug.name then\n9: found_match ← True\n10: CFG ← Ethersolve(BC)\n11: Lcfg ← Lcfg ∪ {CFG}\n12: end if\n13: end for\n14: if found_match then\n15: Break\n16: end if\n17: end for\n18: end for\n19: return Lcfg\n20: end procedure\nthis limitation significantly enhances the efficiency of learn-\ning in dense networks [35].\nDuring each walk, the gathered information is input into\na Word2Vec model, resulting in node embeddings. For this\npurpose, the Precomp model is employed. In Precomp, the\nreturn parameter p is set to 2, and the in-out parameter q is 0.5.\nThis model represents an optimized version of Node2Vec,\nprecomputing and storing all transition probabilities for ran-\ndom walks. The values of p and q significantly influence\nthe structure of the resultant graph. The return parameter\np influences the random walk’s likelihood of selecting an\nimmediately preceding node, while the in-out parameter q\nencourages the walk to remain within its local neighborhood.\nWith a lower value of q, the walk is more likely to visit nodes\nthat are also connected to the previous node.\nThe final embedded information is constrained by available\nresources, therefore the number of walks from each node will\nbe limited to 1, and walk length will be set to the average\ngraph length across all categories, including Clean contracts.\nWalks shorter than this average are padded when the depth of\nthe selected node is less than the average walk length.\nGiven the parameters and constraints, we can now do a\nsmall example on how Node2vec walks are done using a tree\nsnippet generated by Ethersolve CFG at Figure 9. Probabili-\nties of paths are grouped into 3 types, direct return node, node\nthat does not have a path to the direct return node and node\nthat has a return edge to the direct return node. Assuming that\nwe will start at node 202, it has neighbours 13, 210, 233, 214\nand the return node is 13. The transition probabilities for each\npaths will be 1\np to 13 as it is the return node, 1\nq for node 210\nand 233 as they do not have a return edge to node 13 and 1\nfor node 214 as it has a return path to node 13. It will then be\nnormalized by the sum of each possible paths, which will give\nus probabilities of 0.111 for a walk going back to 13, 0.444 to\nnode 210 and 233, finally 0.222 to node 214. Given as such,\nthe next walk will most probably be either 210 or 233.\nAfter obtaining the embedded data, a Long Short Term\nMemory (LSTM) model will be used for the evaluation.\nModel training was implemented based on dataset size sup-\nportable by available computational resources.\nOn algorithmic complexity for the Pre-processing algo-\nrithm (Algorithm 1) and Bug insertion algorithm (Algo-\nrithm 3), the worst-case complexity depends on the size of the\nsmart contract, O(S), where S represents the number of lines\nin the smart contract. Whereas the complexity of the Function\nfiltering algorithm (Algorithm 2) is determined based on the\nnumber of nodes in the AST tree, O(N). In order to extract\nthe opcodes from a given smart contract, the complexity\ndepends on the total number of function descriptors in an\nABI (r) and the total number of ordered lists of ABI, opcodes\nwithin a contract ( m). Hence the algorithmic complexity for\nextracting opcodes from a smart contract (Algorithm 4) is in\nthe order of polynomial complexity, O(rm2). For a smaller\nnumber of opcodes within a contract (which is usually true\nin practical smart contract development), this time is much\nless. This can be observed from Figure 13. The algorith-\nmic complexity for the feature selection algorithm using the\nn-gram algorithm is given by O(n2 log n). Thus the worst-\ncase complexity for Algorithm 5 that extracts the run-time\nbytecode is given by O(rm2n2 log n). For a small number\nof opcodes within a smart contract and n = 3 (for the tri-\ngram feature selection algorithm), the polynomial complexity\nscales down to a practical running time for detecting smart\ncontract vulnerabilities.\n0\n13\n65 202\n210 233 214\n849\nFIGURE 9: A snippet of digraph from Ethersolve CFG with\nparameters p = 2, q = 0.5\nD. MACHINE LEARNING MODELS FOR CLASSIFICATION\nOur dataset encompasses eight distinct categories, necessitat-\ning the use of a multiclass classifier algorithm for the training\nof classifiers. These algorithms are capable of handling multi-\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3: Opcode simplification\nSubstituted Opcodes Original Opcodes\nARITHMETIC_OP ADD MUL SUB DIV SDIV\nSMOD\nCONSTANT1\nBLOCKHASH TIMESTAMP\nNUMBER DIFFICULTY\nGASLIMIT COINBASE\nCONSTANT2 ADDRESS ORIGIN CALLER\nCOMPARISON LT GT SLT SGT\nLOGIC_OP AND OR XOR NOT\nDUP DUP6-DUP16\nSWAP SWAP6 - SWAP16\nPUSH PUSH6 - PUSH32\nLOG LOG1-LOG4\nple classes without the need to be adjusted or modified. In this\nresearch, we will be using the following models from scikit-\nlearn api [33] for comparison: Random Forest (RF), XGBoost\n(XGB), Support Vector Machine (SVM) and Multi-layer Per-\nceptron (MLP) which is a Neutral-Network implementation\nof scikit-learn.\nWe will also utilize sequential neutral network models such\nas Long Short-Term Memory (LSTM). LSTM is a variant\nof Recurrent Neutral Network (RNN) and it is designed to\naddress the problem of vanishing gradients that arises during\nthe back propagation process. The vanishing gradients is a\nproblem because weights of edges are adjusted according to\nthe calculated loss. This inhibit long sequences from updating\nearlier layers as the update value is according to the difference\nfrom the last layer. The main difference between LSTM and\nits predecessor is the ability to retain more information within\neach cell. This is done so by having ’gates’ to control the\namount of information flow in the network. However, after\na preliminary study, CFG data with LSTM did not yield good\nresults and we therefore decided to combine the MLP model\ninto the process making it into a multi-model. MLP model\nis chosen because the training epoch can be shared between\nboth models, allowing for simpler implementation. Inputs\nare the Tfidf and CFG data. This allows increased depth\nand complexity, resulting in better performance in terms of\ngeneralizing the features.\nTo ensure a well-trained model, we have implemented\nseveral measures such as k −fold validation , learning−curve\nand roc curve . These checks are done to all models before\npassing off for the unseen data test. K-fold validation is a data\npartitioning method for assessing model performances. This\nmethod evaluate how well a model generalise an independent\ndataset. This is done so by having k folds, where k refers to a\nnumber of divided parts for the data. After dividing, 1 part of\nthe k fold will be used as the independent test data while the\nrest would be the training set. This method provides variance\nand bias reduction ensuring a more accurate estimation. The\npurpose of learning curves is to identify 2 undesirable states\nduring the training phase for the models, the 2 states are\nTABLE 4: Variables used in the MythSlith Algorithm.\nVariable Description\nmythrilDepth The depth of mythril symbolic execution.\nVulnerability The current detection vulnerability\nsc Smart contract\nMr Mythril scanning result\nSr Slither scanning result\nDeepMr Deeper depth Mythril scanning result\nUnderfitting and Overfitting. These states are identified by\nlooking at it’s training scores, since this is a classification task\nwe will be using accuracy as the score. Underfitting occurs\nwhen both training and validation scores are low and continue\nto persist when more training data is added. Which infer that\nthe model is too simple and is incapable of capturing the\nunderlying pattern. Overfitting is when training score is high\nwhile the validation score is significantly lower. This infer\nthat model may be too complex and will most likely fail at\ngeneralizing the different classes. Learning curves of each\nmodel can be found in Figure 11a. Epochs training informa-\ntion is much suited for neural network representation, for our\nMLP and MLP + LSTM model learning curve comparison\ncan be found in Figure 11b.\nThe ROC curve analysis is utilized to evaluate each model’s\nability to distinguish specific vulnerabilities from the rest.\nThese curves, shown in Figure 12, reveal that the RF and MLP\n+ LSTM models struggle to effectively differentiate between\nthe classes.\nE. MULTI-MODEL APPROACH\nBuilding upon traditional models, we have further developed\na multi-model approach utilizing a combination of Tfidf\nvectorized opcodes and PecanPy node embeddings, employ-\ning a Keras multiple input model. This technique enables the\nuse of multiple sub-networks, which can be concatenated or\nmerged into a unified network at a certain point. This method\nproves particularly advantageous when dealing with diverse\ndata sources, allowing for each to be processed by distinct\nmodels.\nIn the framework of this multi-model architecture, we em-\nployed an MLP for opcode data and an LSTM for PecanPy\ndata processing. However, due to the extensive size of the\ndataset, we limited our test to only 50 randomly selected,\nbugged Solidity contracts from each category.\nF. DESIGN OF MYTHSLITH\nAnalysis tools are considered the go-to when checking for any\nsmart contracts vulnerabilities. However, contemporary tools\nare not yet capable of a full detection [37]. Hence in order\nto have a guideline for the ML model, Mythril and Slither is\nused.\nBoth Mythril and Slither are considered as static tools,\nhowever, Mythril uses symbolic execution and a SMT solver,\nZ3, to determine the satisfiability of the generated symbolic\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nformula, which is a mix of dynamic and formal verification.\nIn [24] an empirical study on available open-sourced tools\nwas done, the result for their curated dataset, which was\ncollected from real vulnerable contracts or injected with bugs,\nshowed that Mythril yields an accuracy of 27% while Slither\n17%, though low but they are the highest among all the 9\ntools presented. In conclusion, [24] suggested to use of a\ncombination of Mythril and Slither which could yield a 37%\naccuracy.\nCertainly, there are also other open-sourced tools available\nbut, Mythril and Slither are both actively updated and there-\nfore more suited for a direct comparison. We have designed\na new integrated algorithm, MythSlith, a simple and elegant\ncombination of both tools, which takes reference from a nor-\nmal depth Mythril analysis to decide on the process. Mythril\nis used as the baseline for MythSlith because it has a higher\ndetection accuracy as shown in [24]. Furthermore, specific\ntools have been designated for addressing vulnerabilities like\nReentrancy, Unauthorized send, Tx origin, and Arithmetic.\nThis allocation stems from the outcomes detailed in Table 5.\nAdditionally, although the severity level was initially absent\nin Slither, it has been incorporated, referencing the SWC\nregistry and aligning with the identified vulnerabilities. The\nalgorithm design can be found in the Algorithm 6. The vari-\nables used in MythSlith Algorithms is listed in Table 4.\nFIGURE 10: Mapping of Slither vulnerabilities to SWC\nG. CHALLENGES\nAlgorithm 6 MythSlith Algorithm\n1: mythrilDepth ← 22\n2: if Vulnerability = Reentrancy then\n3: Sr ← Slither(sc)\n4: output Sr\n5: else if (Vulnerability = Unauthorizedsend) or\n6: (Vulnerability = Txorigin) or\n7: (Vulnerability = Arithmetic) then\n8: Mr ← Mythril(sc, mythrilDepth)\n9: output Mr\n10: else\n11: Mr ← Mythril(sc, mythrilDepth)\n12: if Mr .Severity = High then\n13: output Mr\n14: else\n15: mythrilDepth ← 100\n16: Sr ← Slither(sc)\n17: DeepMr ← Mythril(sc, mythrilDepth)\n18: if DeepMr .Severity = High then\n19: output DeepMr\n20: else if Sr .Severity = High then\n21: output Sr\n22: else if (DeepMr .Severity = Other) or\n23: (Sr .Severity = Other) then\n24: if DeepMr .Severity = Other then\n25: output DeepMr\n26: end if\n27: if Sr .Severity = Other and\n28: |DeepMr .MediumSeverity| < |Sr .MediumSeverity| and\n29: |DeepMr .LowSeverity| < |Sr .LowSeverity| then\n30: output Sr\n31: end if\n32: end if\n33: end if\n34: end if\n1) Multiprocessing\nMultiprocessing is implemented to increase the time taken for\ntest, however, this has spun off a new problem while changing\nsolc version. Therefore, in order to prevent compilation error,\nall processes are dockerized and solc-select is included inside\nboth Mythril and Slither dockerfile. Any action that has got\nto deal with solc will be spinning up a docker container to\nprevent any compiler mismatch issue.\n2) Unified Vulnerability Classification\nCombining Mythril and Slither poses another challenge\nwhich is the definition of vulnerabilities. Though both tools\nare capable of detecting some common bugs, they do not use\nthe same standard. While Mythril uses the more recognized\nSWC, Slither uses its own definition. To resolve this, we have\nadded in SWC definition in the Slither code and incorporated\nthe output with SWC ID. This will allow both tools to com-\nmunicate with reference to the same vulnerabilities definition\nstandard. In this paper, we have mapped the seven different\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nvulnerabilities that can be found in Slither to SWC ID. The\nmapping can be found in Figure 10.\nV. RESULTS AND PERFORMANCE ANALYSIS\nIn this section, experiment results from both analysis tools\nand our ML based approach will be put against each other for\ncomparison. Effectiveness of each tool will be based on four\nparameters namely, (i) accuracy, (ii) precision, (iii) recall,\nand (iv) F1 score. These parameters will then form the con-\nfusion matrix for better visualization. All experiments were\nconducted on a machine with the following specifications:\nR162-ZA1-00 with 16 CPUs x AMD EPYC 7282 16-Core\nProcessor 64GB of RAM and 1.9TB of SSD.\nIn this paper, a total of 4335 manually verified Solidity files\nwere sourced from the smart contract sanctuary [31] reposi-\ntory to track verified smart contracts from the Ethereum main-\nnet and testnets, such as rinkeby, ropston, kovan, etc. And\nalso from Binance Chain, Polygon/Matic, and Tron. In the\nmainnet repository, contract versions have a wide spread from\n0.4.1 to 0.8.7 given the latest update. For this experiment, we\nwill initially consider contracts from the mainnet and Solidity\nversions from 0.4.11 to 0.4.26 to compare against the known\nground truths. Any contracts below 0.4.11 will not be consid-\nered due to a known compiler bug where source indexes could\nbe inconsistent between different Solidity compiler version.\nInconsistency between the source indexes will affect the bug\ninjection process where it is reliant on it to insert bugs.\nThe unseen test set is prepared by a random selection of\n100 solidity files from the Smart Contract Sanctuary. These\nfiles are in addition to the existing 4335 contracts. These\ncontracts will then go through the same bug injection and fea-\nture extraction procedure as in Algorithm 3 and Algorithm 4,\nAlgorithm 5.\nComplete results can be found in both Table 6 and Table\n5. Table 6 illustrates the overall comparison between analysis\ntools and ML models.\nA. EVALUATION METRICS\nTo enable the comparison, SWC standard is utilized as the\nbenchmark and it can be found in Fig 10. The results of\nthe analysis tools and ML models are obtained by executing\nthe same set of data with seven different classifications of\nvulnerabilities. The clean category is also added for clarity. To\nensure a balanced and unbiased result from the analysis tool,\neach tool will run the clean dataset and then the result will be\nused as the baseline, ensuring a reference ground truth. This\nis because we can never assume each smart contract is free of\nbugs.\nGiven the result from the analysis, a confusion matrix\nwill be constructed for the evaluation. The confusion matrix\ncomprises the following values in accordance with the result\nprediction. These values with description can be found in\nTable 7.\nWith these values, we will be able to construct metrics:\n• Accuracy: represents ratio of correctly predicted values\nto the total dataset. It is a measure of the overall correct-\nness.\nAccuracy = TP + TN\nTP + TN + FP + FN (1)\n• Precision: can also be referred as positive predicted\nvalue, represents the number of correctly classified pos-\nitive values to the total predicted positives.\nPrecision = TP\nTP + FP (2)\n• Recall: also known as true positive rate, hit rate or\nsensitivity, represents the ratio of correctly classified\ninstances to all datasets.\nRecall = TP\nTP + FN (3)\n• F1 Score: more useful for uneven dataset or class distri-\nbution. This metric is the weighted average for Precision\nand Recall.\nF1 Score = 2 × (Precision × Recall)\nPrecision + Recall (4)\n• False Positive Rate : is a proportion for the measure of\nincorrectly classified instances as positive.\nFPR = FP\nFP + TN (5)\nIn this study, we will be looking at recall followed by\nprecision and f1 score then accuracy. This is because missing\nFN has much higher consequences than an increase in FP,\nas FN can lead to potential security breaches and financial\nlosses. However, it is advised to take both Precision and F1\nscores into consideration to ensure a balanced model.\nB. PERFORMANCE OF MACHINE LEARNING MODELS\nIn this section, we illustrate the performance comparison\nbetween our ML models, Mythril, Slither, and MythSlith\non the 100 bugged solidity files. The overall performance\nanalysis of the tools, as summarized in Table 6, shows that\nthe MLP and SVM models exhibited the highest levels of\naccuracy, precision, recall, and F1 score, with MLP achieving\nan accuracy of 0.9129 and an F1 score of 0.9127, and SVM\nshowing a slightly lower accuracy of 0.8954 and an F1 score\nof 0.8979. FPR of the models has shed some light on the\nperformance, that the MLP among the all other models has\nthe lowest FPR at 0.0125 — which indicates its likeliness to\nflag out False positive is unlikely.\nThe detailed results of the individual vulnerability analysis\nare presented in Table 5. This analysis revealed that MLP has\nconsistently demonstrated superior performance across most\nof the vulnerability detection, particularly for the detection\nof Clean and Unauthorized send with f1 scores of 0.8723\nand 0.9153 respectively. SVM did well across most of the\ncategories, however, it has a little trouble with Reentrancy,\nwhile having a high recall of 0.9518 its’ precision took a toll at\n0.7940. The XGB model displayed a consistent performance\nacross various categories, with a notable F1 score of 0.7716\nin the Clean category, though it did not outperform all other\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5: Comparison of different tools\nTool Precision Recall F1 Score FPR\nClean\nMythril 0.1474 0.0955 0.1159 0.2590\nSlither 0.3750 0.2247 0.2810 0.0715\nMythSlith 0.1185 0.0323 0.0508 0.1045\nSVM 0.8289 0.8417 0.8352 0.0365\nRF 0.5699 0.6627 0.6128 0.0268\nXGB 0.8732 0.6911 0.7716 0.0211\nMLP 0.8880 0.8571 0.8723 0.0227\nMLP + LSTM 0.7692 0.1931 0.3086 0.0121\nReentrancy\nMythril 0.1889 0.7321 0.3004 0.1004\nSlither 0.4354 0.5120 0.4706 0.0557\nMythSlith 0.2656 0.5120 0.3497 0.1108\nSVM 0.7940 0.9518 0.8658 0.0309\nRF 0.5699 0.6627 0.6128 0.0626\nXGB 0.5738 0.8193 0.6749 0.0762\nMLP 0.8514 0.8976 0.8739 0.0196\nMLP + LSTM 0.3443 0.1265 0.1850 0.0301\nArithmetic\nMythril 0.2902 1.0000 0.4498 0.1043\nSlither 0.0000 0.0000 0.0000 0.0000\nMythSlith 0.2931 1.0000 0.4533 0.0992\nSVM 0.8710 0.8804 0.8757 0.0183\nRF 0.6606 0.5924 0.6246 0.0428\nXGB 0.8047 0.7391 0.7705 0.0252\nMLP 0.8967 0.8967 0.8967 0.0145\nMLP + LSTM 0.4815 0.0707 0.1232 0.0107\nTx origin\nMythril 0.9505 1.0000 0.9746 0.0062\nSlither 0.9800 0.4414 0.6087 0.0007\nMythSlith 0.9897 1.0000 0.9948 0.0013\nSVM 0.9639 0.9397 0.9517 0.0054\nRF 0.6260 0.7990 0.7020 0.0735\nXGB 0.7269 0.8693 0.7918 0.0502\nMLP 0.9552 0.9648 0.9600 0.0070\nMLP + LSTM 0.5759 0.7437 0.6491 0.0843\nUnauthorized Send\nMythril 0.8571 1.0000 0.9231 0.0106\nSlither 0.3225 1.0000 0.4877 0.1373\nMythSlith 0.5192 1.0000 0.6835 0.0620\nSVM 0.8988 0.8483 0.8728 0.0129\nRF 0.6400 0.7191 0.6772 0.0548\nXGB 0.8075 0.7303 0.7670 0.0236\nMLP 0.9205 0.9101 0.9153 0.0106\nMLP + LSTM 0.9167 0.1236 0.2178 0.0015\nTimestamp Dependency\nMythril 0.3171 0.2671 0.2900 0.0505\nSlither 0.7273 0.6667 0.6957 0.0179\nMythSlith 0.4174 0.3019 0.3504 0.0428\nSVM 0.9752 0.9691 0.9721 0.0030\nRF 0.6754 0.7963 0.7309 0.0466\nXGB 0.8344 0.8086 0.8213 0.0195\nMLP 0.9695 0.9815 0.9755 0.0037\nMLP + LSTM 0.6854 0.7531 0.7176 0.0421\nTransaction Order Dependency\nMythril 0.0000 0.0000 0.0000 0.0000\nSlither 0.0000 0.0000 0.0000 0.0000\nMythSlith 0.0000 0.0000 0.0000 0.0000\nSVM 0.9620 0.9672 0.9646 0.0053\nRF 0.7857 0.8415 0.8127 0.0321\nXGB 0.8844 0.8361 0.8596 0.0152\nMLP 0.9674 0.9727 0.9700 0.0045\nMLP + LSTM 0.8182 0.6393 0.7178 0.0199\nUnhandled Exceptions\nMythril 1.0000 0.7683 0.8690 0.0000\nSlither 0.7377 0.4737 0.5769 0.0105\nMythSlith 0.8056 0.5859 0.6784 0.0086\nSVM 0.9197 0.7826 0.8456 0.0082\nRF 0.5691 0.4348 0.4930 0.0398\nXGB 0.7105 0.6708 0.6901 0.0330\nMLP 0.8544 0.8385 0.8464 0.0173\nMLP + LSTM 0.1791 0.8199 0.2940 0.4545\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 6: Overall comparison different tools\nAccuracy Precision Recall F1 score FPR\nMythril 0.7516 0.7092 0.3108 0.4023 0.0664\nSlither 0.6035 0.6133 0.1753 0.2430 0.0367\nMythSlith 0.7388 0.6745 0.3065 0.3883 0.0536\nRF 0.6676 0.6757 0.6718 0.6627 0.0473\nMLP 0.9129 0.9128 0.9129 0.9127 0.0125\nXGB 0.7681 0.7837 0.7681 0.7702 0.0330\nSVM 0.8954 0.9017 0.8976 0.8979 0.0151\nMLP + LSTM 0.4189 0.5963 0.4337 0.4017 0.0819\n(a) Learning rate for XGBoost, SVM, RF, MLP.\n (b) Learning rate for MLP Vs MLP+LSTM.\nFIGURE 11: Learning rate with increasing training size (a) and increasing epochs (b).\nTABLE 7: Variables used in evaluation metrics calculation.\nMetrics Description\nTP True Positive. This is when an instance has been correctly\nclassified as the intended class.\nTN True Negative. This represents instances where it has been\ncorrectly classified as negative.\nFP False Positive. This is also known as Type 1 error, is when\nincorrect positive classification when in fact it is a negative\nclass.\nFN False Negative. This is also known as Type 2 error, is when\na positive case has been classified as negative\ntools in this category. Surprisingly, bagging ensemble learn-\ning technique like RF did not do very well, measures across\nthe board is not more than 0.67. The multi-model of MLP\nand LSTM weak results, while doing well in Transaction\norder with F1 score of 0.7176, it is less effective in other\ncategories. One notable FPR measure is for the multi-model,\nwhere it is 0.4545 for Unhandled Exceptions, this clearly\nindicates the low effectiveness of the features used for the\nmodels. While MLP excels in Timestamp Dependency with\nthe lowest measure at 0.0037, indicating high confidence in\nthe detection.\nC. PERFORMANCE COMPARISON WITH SOFTWARE\nVERIFICATION AND VALIDATION TOOLS\nResults of the tools can be found at Table 5. Right off the bat,\nit is clear that current analysis tool is unable to identify some\nvulnerabilities. Transaction order seem to be a challenge for\nthe analysis tools, while Arithmetic proof to be too hard for\nSlither to handle. In addition, while it seem great from the\nprecision point of view for the 3 tools, recall and F1 score of\nthe does not do very well. This has yet again emphasised on\nthe findings from [17]. And current tools has difficulty iden-\ntifying Clean contracts, with Slither leading the pack at only\n0.375 in precision while the rest is well below 0.2. Another\nvulnerability to highlight is the TimestampDependency, sur-\nprisingly all 3 tools did not fare well with recall being lesser\nthan 0.2. FPR of clean contracts for Mythril is particularly\nhigh at 0.2590, and this behaviour continues in Reentrancy\nand Arithmetic. However, it did really well in Unhandled\nexceptions where no FP were raised. Slither did really well\nin Tx origin with just 0.0007 for its FPR. However, such\nperformance is not backed by its recall and f1 score which has\nclearly indicate poor TP. MythSlith results were not spectac-\nular, it is just hovering between Mythil and Slither measures.\nOne example is Timestamp Dependency where MythSlith has\na FPR of 0.0428, Mythril and Slither have 0.0505 and 0.0179\nrespectively. Due to its design, it can never be better than the\nhighest measure by either tools.\nThese findings highlight the varying strengths and limita-\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 12: ROC Curves for various models. From left to right, top to bottom: Random Forest, XGBoost, SVM, MLP,\nMLP+LSTM.\ntions of each model or tool, underscoring the influence of\nvulnerability type on the effectiveness of detection methods.\nVI. ANALYSIS AND INFERENCES\nA. PRE-PROCESSING AND FEATURE EXTRACTION\nIn our methodology, opcode sequences are utilized and sim-\nplified technique aimed at enhancing variance. We then em-\nploy Tfidf technique with trigram-based feature extraction.\nWhile this approach is efficient and straightforward, it suffers\nfrom a lack of context awareness. This limitation stems from\nits focus on only three consecutive opcodes at a time, leading\nto a sparsity issue. The sparsity results from the model’s\nlimited exposure to examples, potentially compromising its\naccuracy and robustness.\nFurthermore, the sparse nature of the Tfidf representa-\ntion presents computational challenges, particularly as data\nvolumes escalate. In the context of our opcode dataset, this\nmethod generates 4, 824 feature columns via the Tfidf\nvectorizer. Additionally, the process of opcode simplifica-\ntion, while conserving computational resources, introduces\nsignificant drawbacks. A primary concern is the discarding\nof hexadecimal values, resulting in the loss of vital address\ninformation crucial for source location tracing.\nLooking ahead, the adoption of an alternative vectorizer to\nTfidf could potentially yield improved contextual under-\nstanding, enhancing the models’ learning capabilities.\nB. COMPARISON OF MYTHSLITH AND MLP MODEL\nFrom the test conducted, it is clear that relying on one tool\nfor smart contract analysis is not ideal. Current Software and\nverification tools such as Mythril [14] and Slither [13] tend to\nbe on the safe side because the result has high precision with\nlow recall and f1 score. In our effort to combine them and\nweave through the cracks by constructing MythSlith, such\nbehavior still exists. No significant detection progress was\nmade, which then again is expected as no improvement was\ndone to each tool. However, MythSlith did cover Slither’s\ninability to detect Arithmetic vulnerabilities. This can be\nobserved in Table 5.\nIn our proposed method, we show that detection via ex-\ntracted features from smart contracts presents viable patterns\nfor machine learning models to learn and classify them. How-\never, the suitability of different models varies for this specific\napplication, this can be clearly seen from the results of RF.\nUnlike the gradient correction method employed by XGB,\nthe ensemble bagging approach of RF did not yield equally\neffective results. In terms of neural network method, MLP did\nwell for all categories. In contrast, the multi-model of MLP\n+ LSTM did not. This is primarily due to the CFG features\nderived from Ethersolve [34]. These CFG features did not\npresent a strong pattern for our model to learn effectively.\nFurthermore, the generation of features using PecanPy [35]\ntakes up a substantial amount of time due to random walks\ngeneration. However, the limited effectiveness of Ethersolve\nfeatures in this context does not inherently diminish their\nvalue; the challenge may lie more with PecanPy’s processing\ndemands.\n16 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 13: Pre-processing time (in ms) for opcode extraction with respect to (from left to right): Line Count, AST Node\nCount, and Opcode count, respectively.\nTABLE 8: Average time taken for Smart contract vulnerabil-\nity detection tool to analyze each smart contract\nSoftware Validation Tool Average Time per Smart\nContract (seconds)\nDepth\nSlither 5.36 -\nMythril 1270.33 22\nMythSlith 1102.14 0/22/100\nRF 4.45 -\nMLP 4.43 -\nXGB 4.20 -\nSVM 6.46 -\nMLP+LSTM 51.52 -\nC. RUNNING TIME\nIn Table 8, we analyze and compare the running times for\nthe various software validation tools. The running time for\nMythSlith is comparable to that of Mythril (or sometimes\nlesser). The average running time for MythSlith is 1102.14\nseconds whereas that of Mythril is 1270.33 seconds. Note\nthat unlike Mythril, the average running time of Slither is\nonly 5.36 seconds since it does not involve the depth of the\nsymbolic execution. The average running time for MythSlith\nis slightly less than that of Mythril since the depth of the\nsymbolic execution is only increased for certain types of\nvulnerabilities, whereas for all other cases where Slither has\nbetter detection accuracy, MythSlith chooses Slither.\nAs observed in the Table 8, the running time to analyze a\nsmart contract using ML models is much lesser (in the order\nof < 7 seconds) compared to the software verification tools\nexcept for the multi-model MLP+LSTM since it involves two\nML models. It is evident that the time taken to analyze the\nsmart contract features and predict using the ML-model is\nreal-time for the smart contract analysis. Whereas the pre-\nprocessing and training time for the ML-model is an offline\nprocess. The pre-processing time for opcode extraction with\nrespect to the number of lines in the smart contract, number\nof AST nodes and number of CFG opcodes is depicted in\nFigure 13 from left to right, respectively. The maximum pre-\nprocessing time observed for a smart contract with 3500 lines\nis 26 seconds. While most practical smart contracts with less\nthan 1000 lines of code take < 10 seconds for pre-processing\nand opcode extraction.\nVII. CONCLUSION AND FUTURE WORKS\nSecuring smart contracts is no easy feat, as they are not pro-\ntected and visible to everyone. Current software verification\ntools tend to be on a defensive stance, only flagging the bugs\nif they are fully sure about it. This however would let the False\nnegatives slip through. Therefore, we proposed a machine\nlearning approach to effectively and efficiently detect seven\ntypes of vulnerabilities while also identifying clean contracts.\nThis was done by employing models such as Random\nForest, XGBoost, Support Vector Machine, Multi-Layer Per-\nception, and Long-Short Term Memory. To insert practical\nbugs and increase the vulnerability space, we proposed a\npractical bug injection technique that injects bugs into ver-\nified smart contracts that were cleaned using our proposed\npre-processing algorithm. This helps to scale up the smart\ncontracts’ vulnerability space using features such as opcodes\nand CFG that were extracted for the model training. Prior\nto vectorization, simplification was done to the opcodes in\norder to reduce the dimension and remove contract-specific\nhexadecimal values. TDIDF was utilized with trigram for the\nvectorization and the CFG data was processed by PencanPy\nwhere random walks are generated and vectorized with the\nWord2Vec model.\nThe results of the models were then benchmarked with\nsoftware verification tools such as Mythril, Slither, and an\nexperimental tool proposed in our work — MythSlith. From\nthe results, machine learning models have shown superior\nperformance in vulnerability detection than existing Soft-\nware verification tools. While testing with real-time smart\ncontracts, the MLP model performs the best at having 91%\naccuracy along with higher recall and f1-scores. The FPR\nmeasures show that MLP achieved the best performance with\nthe lowest average among all other tools at 0.0125, while\nMLP+LSTM achieved the lowest FPR of 0.0015 for unau-\nthorized send.\nWhile the current model improves the accuracy with sig-\nnificantly less FPR while detecting contract-wide bugs, it can\nfurther be improved by pinpointing the exact source location.\nVOLUME 11, 2023 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nOne possible solution is to use a combination of fuzzing\nwith formal verification to extend our current model to add\nthis feature. This solution however will face a constraint that\nmodel patterns cannot be traced back to the source location.\nWe can explore some viable solutions for this constraint by\ntagging the source index (that may not be used for model\ntraining) to a specific feature, thereby allowing traceback to\nthe source. Bug injection method ensures that newer forms\nof features and inherent patterns are added. The current bug\ninjection method leverages on function descriptors. However,\nnot all bugs are in the form of functions. In future works, we\nwill explore more generic bug injection methods.\nACKNOWLEDGMENT\nThis work was supported by the Singapore MoE grant, Singa-\npore Institute of Technology (SIT) WBS R-MOE-A405-I002.\nREFERENCES\n[1] PR Newswire. Global smart contracts market to reach usd 9850 million by\n2030 with 24\n[2] P. Praitheeshan, L. Pan, J. Yu, J. Liu, and R. Doss, ‘‘Security analysis meth-\nods on ethereum smart contract vulnerabilities: a survey,’’ arXiv preprint\narXiv:1908.08605, 2019.\n[3] K. Ramana, R. M. Mohana, C. K. K. Reddy, G. Srivastava, and T. R.\nGadekallu, ‘‘A blockchain-based data-sharing framework for cloud based\ninternet of things systems with efficient smart contracts,’’ in 2023 IEEE In-\nternational Conference on Communications Workshops (ICC Workshops) .\nIEEE, 2023, pp. 452–457.\n[4] W. Wang, Z. Han, T. R. Gadekallu, S. Raza, J. Tanveer, and C. Su,\n‘‘Lightweight blockchain-enhanced mutual authentication protocol for\nuavs,’’IEEE Internet of Things Journal , 2023.\n[5] Jennifer Korn. Cnn business. https://edition.cnn.com/2022/08/16/tech/\ncrypto-hack-rise-2022/index.html. Accessed: 16.08.2022.\n[6] Binance. (2023) Poolz finance hacked, token price drops 93\n[7] SolidityScan. (2023) Poolz finance hack analysis: Still experi-\nencing overflow. [Online]. Available: https://blog.solidityscan.com/\npoolz-finance-hack-analysis-still-experiencing-overflow-fcf35ab8a6c5\n[8] openzeppelin. https://www.openzeppelin.com/.\n[9] consensys. https://consensys.net/.\n[10] trailofbits. https://www.trailofbits.com/.\n[11] consensys. https://swcregistry.io/.\n[12] NCC Group. https://dasp.co/.\n[13] J. Feist, G. Grieco, and A. Groce, ‘‘Slither: a static analysis framework\nfor smart contracts,’’ in 2019 IEEE/ACM 2nd International Workshop\non Emerging Trends in Software Engineering for Blockchain (WETSEB) .\nIEEE, 2019, pp. 8–15.\n[14] B. Mueller, ‘‘Smashing ethereum smart contracts for fun and real profit,’’\nHITB SECCONF Amsterdam , vol. 9, p. 54, 2018.\n[15] M. Mossberg, F. Manzano, E. Hennenfent, A. Groce, G. Grieco, J. Feist,\nT. Brunson, and A. Dinaburg, ‘‘Manticore: A user-friendly symbolic\nexecution framework for binaries and smart contracts,’’ in 2019 34th\nIEEE/ACM International Conference on Automated Software Engineering\n(ASE). IEEE, 2019, pp. 1186–1189.\n[16] L. Luu, D.-H. Chu, H. Olickel, P. Saxena, and A. Hobor, ‘‘Making smart\ncontracts smarter,’’ in Proceedings of the 2016 ACM SIGSAC conference\non computer and communications security , 2016, pp. 254–269.\n[17] X. Tang, K. Zhou, J. Cheng, H. Li, and Y. Yuan, ‘‘The vulnerabilities\nin smart contracts: A survey,’’ in Advances in Artificial Intelligence and\nSecurity: 7th International Conference, ICAIS 2021, Dublin, Ireland, July\n19-23, 2021, Proceedings, Part III 7 . Springer, 2021, pp. 177–190.\n[18] P. Momeni, Y. Wang, and R. Samavi, ‘‘Machine learning model for smart\ncontracts security analysis,’’ in 2019 17th International Conference on\nPrivacy, Security and Trust (PST) . IEEE, 2019, pp. 1–6.\n[19] W. Wang, J. Song, G. Xu, Y. Li, H. Wang, and C. Su, ‘‘Contractward:\nAutomated vulnerability detection models for ethereum smart contracts,’’\nIEEE Transactions on Network Science and Engineering , vol. 8, no. 2, pp.\n1133–1144, 2020.\n[20] S. Shakya, A. Mukherjee, R. Halder, A. Maiti, and A. Chaturvedi, ‘‘Smart-\nmixmodel: Machine learning-based vulnerability detection of solidity\nsmart contracts,’’ in 2022 IEEE International Conference on Blockchain\n(Blockchain). IEEE, 2022, pp. 37–44.\n[21] A. Ghaleb and K. Pattabiraman, ‘‘How effective are smart contract analysis\ntools? evaluating smart contract static analysis tools using bug injection,’’\nin Proceedings of the 29th ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis , 2020, pp. 415–427.\n[22] The MITRE Corporation. Common weakness enumeration. https://cwe.\nmitre.org/. Accessed: 28.12.2022.\n[23] G. Wood et al., ‘‘Ethereum: A secure decentralised generalised transaction\nledger,’’Ethereum project yellow paper, vol. 151, no. 2014, pp. 1–32, 2014.\n[24] T. Durieux, J. F. Ferreira, R. Abreu, and P. Cruz, ‘‘Empirical review of auto-\nmated analysis tools on 47,587 ethereum smart contracts,’’ in Proceedings\nof the ACM/IEEE 42nd International conference on software engineering ,\n2020, pp. 530–541.\n[25] S. S. Kushwaha, S. Joshi, D. Singh, M. Kaur, and H.-N. Lee, ‘‘Systematic\nreview of security vulnerabilities in ethereum blockchain smart contract,’’\nIEEE Access, vol. 10, pp. 6605–6621, 2022.\n[26] L. Duan, L. Yang, C. Liu, W. Ni, and W. Wang, ‘‘A new smart contract\nanomaly detection method by fusing opcode and source code features for\nblockchain services,’’IEEE Transactions on Network and Service Manage-\nment, 2023.\n[27] J. Zheng, L. Williams, N. Nagappan, W. Snipes, J. P. Hudepohl, and M. A.\nVouk, ‘‘On the value of static analysis for fault detection in software,’’ IEEE\ntransactions on software engineering , vol. 32, no. 4, pp. 240–253, 2006.\n[28] T. Ball, ‘‘The concept of dynamic analysis,’’ ACM SIGSOFT Software\nEngineering Notes, vol. 24, no. 6, pp. 216–234, 1999.\n[29] R. Calinescu, C. Ghezzi, K. Johnson, M. Pezzé, Y. Rafiq, and G. Tambur-\nrelli, ‘‘Formal verification with confidence intervals to establish quality of\nservice properties of software systems,’’ IEEE transactions on reliability ,\nvol. 65, no. 1, pp. 107–125, 2015.\n[30] J. Chen, X. Xia, D. Lo, J. Grundy, X. Luo, and T. Chen, ‘‘Defectchecker:\nAutomated smart contract defect detection by analyzing evm bytecode,’’\nIEEE Transactions on Software Engineering, vol. 48, no. 7, pp. 2189–2207,\n2021.\n[31] M. Ortner and S. Eskandari, ‘‘Smart contract sanctuary.’’ [Online].\nAvailable: https://github.com/tintinweb/smart-contract-sanctuary\n[32] W. B. Cavnar, J. M. Trenkle et al. , ‘‘N-gram-based text categorization,’’\nin Proceedings of SDAIR-94, 3rd annual symposium on document analysis\nand information retrieval , vol. 161175. Las Vegas, NV, 1994.\n[33] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel,\nV. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler, R. Layton, J. Van-\nderPlas, A. Joly, B. Holt, and G. Varoquaux, ‘‘API design for machine\nlearning software: experiences from the scikit-learn project,’’ in ECML\nPKDD Workshop: Languages for Data Mining and Machine Learning ,\n2013, pp. 108–122.\n[34] F. Contro, M. Crosara, M. Ceccato, and M. Dalla Preda, ‘‘Ethersolve:\nComputing an accurate control-flow graph from ethereum bytecode,’’ in\n2021 IEEE/ACM 29th International Conference on Program Comprehen-\nsion (ICPC). IEEE, 2021, pp. 127–137.\n[35] R. Liu and A. Krishnan, ‘‘Pecanpy: a fast, efficient and parallelized python\nimplementation of node2vec,’’ Bioinformatics, vol. 37, no. 19, pp. 3377–\n3379, 2021.\n[36] A. Grover and J. Leskovec, ‘‘node2vec: Scalable feature learning for\nnetworks,’’ in Proceedings of the 22nd ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining , 2016.\n[37] P. Qian, Z. Liu, Q. He, B. Huang, D. Tian, and X. Wang, ‘‘Smart\ncontract vulnerability detection technique: A survey,’’ arXiv preprint\narXiv:2209.05872, 2022.\n18 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLee Song Haw Colinet al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLEE SONG HAW COLIN (M’23) received the\nBachelor’s Degree in Mechatronics Engineer-\ning from the Singapore Institute of Technology-\nUniversity of Glasgow in 2016. He is currently\na Research Engineer at the Singapore Institute of\nTechnology, where he is also pursuing his Mas-\nter of Engineering in Future Communications and\nBlockchain, projected to be completed in 2025. His\nprofessional journey includes a significant role in\nthe development of a COVID-19 CMS application\nwith the GPConnect team. His current research interests lie in the intersection\nof AI, Blockchain, and their applications in 5G networks.\nPURNIMA MURALI MOHAN (M’14) received\nthe Ph.D. and M.S. degrees in Electrical and Com-\nputer Engineering from the National University\nof Singapore in 2018 and 2014, respectively. She\nheld the Post-doctoral researcher position at the\nNational University of Singapore until 2018. She is\ncurrently an Assistant Professor at the Information\nand Communications Technology Cluster, Singa-\npore Institute of Technology. She has expertise\nin Layer 2 and Layer 3 network protocols while\nworking with the Industry. Her current research interests include Blockchain\nand AI, Security in Next-generation networks, Optimization and Heuristics\nalgorithm design.\nJONATHAN PANJonathan Pan (M’13) received\nthe Doctoral degree in Information Technology,\nCyber Security from the Murdoch University, Aus-\ntralia. He is currently the Chief of the Disruptive\nTechnologies Office and Director of xCybersecu-\nrity at the Home Team Science and Technology\nAgency, which is a statutory board formed under\nSingapore’s Ministry of Home Affairs to develop\nscience and technology capabilities for the Home\nTeam. He is also an Adjunct Associate Professor\nwith Nanyang Technological University Singapore. His research interests\ninclude Cybersecurity, AI and Blockchain.\nPETER LOH(SM’xx) received the Ph.D. degree in\nComputer Engineering from the Nanyang Techno-\nlogical University, Singapore and MSc, Computer\nScience from the University of Manchester, United\nKingdom. He is currently an Associate Professor at\nthe Singapore Institute of Technology, Information\nand Communications Technology Cluster. He has\nmore than 35 years of professional engineering,\nresearch, academic and consultative experience.\nTo date, he has authored/co-authored more than\n100 publications, with several in high-impact, international, peer-reviewed\njournals. His research interests include Information and Cyber Security,\nData analytics and MAchine learning for Digital crime, Blockchain and IoT,\nMalware analysis and classification. He is a registered Professional Engineer\n(Singapore), Chartered Engineer (UK), and a senior member of IEEE.\nVOLUME 11, 2023 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364351\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}