{
  "title": "Pivot Based Language Modeling for Improved Neural Domain Adaptation",
  "url": "https://openalex.org/W2803777992",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2527733719",
      "name": "Yftah Ziser",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1972118651",
      "name": "Roi Reichart",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2104094955",
    "https://openalex.org/W2962887999",
    "https://openalex.org/W2168897002",
    "https://openalex.org/W1932968309",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2153353890",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2251732921",
    "https://openalex.org/W774326453",
    "https://openalex.org/W1956343362",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2507341862",
    "https://openalex.org/W2962897886",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2144321756",
    "https://openalex.org/W2244609359",
    "https://openalex.org/W1571969069",
    "https://openalex.org/W4294238563",
    "https://openalex.org/W2096873754",
    "https://openalex.org/W2120708938",
    "https://openalex.org/W2963463240",
    "https://openalex.org/W181016146",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2301095666",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W22861983",
    "https://openalex.org/W2105523772",
    "https://openalex.org/W2014703436",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2593887162",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W2134036914",
    "https://openalex.org/W2756484070",
    "https://openalex.org/W98423455",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2032235985",
    "https://openalex.org/W2111362445",
    "https://openalex.org/W2567698949",
    "https://openalex.org/W3146885639",
    "https://openalex.org/W2141443306",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2215421138",
    "https://openalex.org/W2950886545",
    "https://openalex.org/W2112483442",
    "https://openalex.org/W2158108973"
  ],
  "abstract": "Representation learning with pivot-based methods and with Neural Networks (NNs) have lead to significant progress in domain adaptation for Natural Language Processing. However, most previous work that follows these approaches does not explicitly exploit the structure of the input text, and its output is most often a single representation vector for the entire text. In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner. Particularly, our model processes the information in the text with a sequential NN (LSTM) and its output consists of a representation vector for every input word. Unlike most previous representation learning models in domain adaptation, PBLM can naturally feed structure aware text classifiers such as LSTM and CNN. We experiment with the task of cross-domain sentiment classification on 20 domain pairs and show substantial improvements over strong baselines.",
  "full_text": "Proceedings of NAACL-HLT 2018, pages 1241–1251\nNew Orleans, Louisiana, June 1 - 6, 2018.c⃝2018 Association for Computational Linguistics\nPivot Based Language Modeling for Improved Neural Domain Adaptation\nYftah Ziser and Roi Reichart\nFaculty of Industrial Engineering and Management, Technion, IIT\nsyftah@campus.technion.ac.il, roiri@ie.technion.ac.il\nAbstract\nRepresentation learning with pivot-based\nmethods and with Neural Networks (NNs)\nhave lead to signiﬁcant progress in domain\nadaptation for Natural Language Processing.\nHowever, most previous work that follows\nthese approaches does not explicitly exploit\nthe structure of the input text, and its output\nis most often a single representation vector\nfor the entire text. In this paper we present\nthe Pivot Based Language Model (PBLM) ,\na representation learning model that marries\ntogether pivot-based and NN modeling in\na structure aware manner. Particularly, our\nmodel processes the information in the text\nwith a sequential NN (LSTM) and its output\nconsists of a context-dependent representation\nvector for every input word. Unlike most\nprevious representation learning models in\ndomain adaptation, PBLM can naturally\nfeed structure aware text classiﬁers such as\nLSTM and CNN. We experiment with the\ntask of cross-domain sentiment classiﬁcation\non 20 domain pairs and show substantial\nimprovements over strong baselines.1\n1 Introduction\nDomain adaptation ( DA, (Daum ´e III, 2007; Ben-\nDavid et al., 2010)) is a fundamental challenge in\nNLP, due to the reliance of many algorithms on\ncostly labeled data which is scarce in many do-\nmains. To save annotation efforts, DA aims to im-\nport algorithms trained with labeled data from one\nor several domains to new ones. While DA algo-\nrithms have long been developed for many tasks\nand domains (e.g. (Jiang and Zhai, 2007; Mc-\nClosky et al., 2010; Titov, 2011; Bollegala et al.,\n2011; Rush et al., 2012; Schnabel and Sch ¨utze,\n1Our code is publicly available at: https://github.\ncom/yftah89/PBLM-Domain-Adaptation.\n2014)), the unprecedented growth of heteroge-\nneous online content calls for more progress.\nDA through Representation Learning (DReL),\nwhere the DA method induces shared representa-\ntions for the examples in the source and the tar-\nget domains, has become prominent in the Neural\nNetwork (NN) era. A seminal (non-NN) DReL\nwork is structural correspondence learning (SCL)\n(Blitzer et al., 2006, 2007) which models the con-\nnections between pivot features – features that are\nfrequent in the source and the target domains and\nare highly correlated with the task label in the\nsource domain – and the other, non-pivot, fea-\ntures. While this approach explicitly models the\ncorrespondence between the source and the target\ndomains, it has been outperformed by NN-based\nmodels, particularly those based on autoencoders\n(AEs, (Glorot et al., 2011; Chen et al., 2012))\nwhich employ compress-based noise reduction to\nextract features that empirically support domain\nadaptation. Recently, Ziser and Reichart (2017)\n(ZR17) proposed to marry these approaches. They\nhave presented the autoencoder-SCL models and\ndemonstrated their superiority over a large num-\nber of previous approaches, particularly those that\nemploy pivot-based ideas only or NNs only.\nCurrent DReL methods, however, suffer from\na fundamental limitation: they ignore the struc-\nture of their input text (usually sentence or docu-\nment). This is reﬂected both in the way they repre-\nsent their input text, typically with a single vector\nwhose coordinates correspond to word counts or\nindicators across the text, and in their output which\ntypically consists of a single vector representa-\ntion. This structure-indifferent approach stands\nin a sharp contrast to numerous NLP algorithms\nwhere text structure plays a key role.\nMoreover, learning a single feature vector per\n1241\ninput example, these methods can feed only task\nclassiﬁers such as SVM and feed-forward NNs\nthat take a single vector as input, but cannot feed\nsequential (e.g. RNNs and LSTMs (Hochreiter\nand Schmidhuber, 1997)) or convolution (CNNs\n(LeCun et al., 1998)) networks that require an in-\nput vector per word or sentence in their input. This\nmay be a serious limitation given the excellent per-\nformance of structure aware models in a large va-\nriety of NLP tasks, including sentiment analysis\nand text classiﬁcation (e.g.(Kim, 2014; Yogatama\net al., 2017)) - prominent DA evaluation tasks.\nFig. 1 demonstrates the limitation of structure-\nindifferent modeling in DA for sentiment analysis.\nWhile the example review contains more positive\npivot features (see deﬁnition in Sec. 2), the senti-\nment expressed in the review is negative. A rep-\nresentation learning method should encode the re-\nview structure (e.g. the role of the terms at ﬁrst\nand However) in order to uncover the sentiment.2\nIn this paper we overcome these limitations.\nWe present (Section 3) the Pivot Based Language\nModel (PBLM) - a domain adaptation model that\n(a) is aware of the structure of its input text; and\n(b) outputs a representation vector for every in-\nput word. Particularly, the model is a sequential\nNN (LSTM) that operates very similarly to LSTM\nlanguage models (LSTM-LMs). The fundamen-\ntal difference is that while for every input word\nLSTM-LMs output a hidden vector and a predic-\ntion of the next word, the output of PBLM is a hid-\nden vector and a prediction of the next word if that\nword is a pivot feature or else, a generic NONE\ntag. Hence, PBLM not only exploits the sequential\nnature of its input text, but its output states can nat-\nurally feed LSTM and CNN task classiﬁers. No-\ntice that PBLM is very ﬂexible: instead of pivot\nbased unigram prediction it can be deﬁned to pre-\ndict pivots of arbitrary length (e.g. the next bigram\nor trigram), or, alternatively, it can be deﬁned over\nsentences or other textual units instead of words.\nFollowing a large body of DA work, we ex-\nperiment (Section 5) with the task of binary sen-\ntiment classiﬁcation. We consider adaptation be-\ntween each domain pair in the four product review\ndomains of Blitzer et al. (2007) (12 domain pairs)\nas well as between these domains and an airline\nreview domain (Nguyen, 2015) and vice versa (8\ndomain pairs). The latter 8 setups are particularly\n2Pivots are deﬁned with respect to a (source, target) do-\nmain pair. The pivots highlighted in the ﬁgure are the pivots\nfor this review in all the setups we explored.\nI was at ﬁrst ::::very:::::::excited with my new Zyliss\nsalad spinner - it is easy to spin and looks:::::great\n... . However, ... it doesn’t get your greens very\ndry. I’ve been surprised and disappointed by\nthe amount of water left on lettuce after spin-\nning, and spinning, and spinning.\nFigure 1: Example review from the kitchen appliances\ndomain of Blitzer et al. (2007). Positive pivot features\nare underlined with a wavy line. Negative pivot fea-\ntures are underlined with a straight line. Although there\nare more positive pivots than negative ones, the review\nis negative.\nchallenging as the airline reviews tend to be more\nnegative than the product reviews (see Section 4).\nWe implement PBLM with two task classi-\nﬁers, LSTM and CNN, and compare them to\nstrong previous models, among which are: SCL\n(pivot based, no NN), the marginalized stacked de-\nnoising autoencoder model (MSDA, (Chen et al.,\n2012) - AE based, no pivots), the MSDA-DAN\nmodel ((Ganin et al., 2016) - AE with a Do-\nmain Adversarial Network (DAN) enhancement)\nand AE-SCL-SR (the best performing model of\nZR17, combining AEs, pivot information and pre-\ntrained word vectors). PBLM-LSTM and PBLM-\nCNN perform very similarly to each other and\nstrongly outperform previous models. For exam-\nple, PBLM-CNN achieves averaged accuracies of\n80.4%, 84% and 76.2% in the 12 product domain\nsetups, 4 product to airline setups and 4 airline to\nproduct setups, respectively, while AE-SCL-SR,\nthe best baseline, achieves averaged accuracies of\n78.1%, 78.7% and 68.1%, respectively.\n2 Background and Previous Work\nDA is an established challenge in machine learn-\ning in general and in NLP in particular (e.g.\n(Roark and Bacchiani, 2003; Chelba and Acero,\n2004; Daum ´e III and Marcu, 2006)). While DA\nhas several setups, the focus of this work is on un-\nsupervised DA. In this setup we have access to un-\nlabeled data from the the source and the target do-\nmains, but labeled data is available in the source\ndomain only. We believe that in the current web\nera with the abundance of text from numerous do-\nmains, this is the most realistic setup.\nSeveral approaches to DA have been proposed,\nfor example: instance reweighting (Huang et al.,\n2007; Mansour et al., 2009), sub-sampling from\n1242\nboth domains (Chen et al., 2011) and learning joint\ntarget and source feature representations (DReL),\nthe approach we take here. The rest of this section\nhence discusses DReL work that is relevant to our\nideas, but ﬁrst we describe our problem setup.\nUnsupervised Domain Adaptation with DReL\nThe pipeline of this setup typically consists of two\nsteps: representation learning and classiﬁcation.\nIn the ﬁrst step, a representation model is trained\non the unlabeled data from the source and target\ndomains. In the second step, a classiﬁer for the\nsupervised task is trained on the source domain la-\nbeled data. To facilitate domain adaptation, ev-\nery example that is fed to the task classiﬁer (sec-\nond step) is ﬁrst represented by the representation\nmodel of the ﬁrst step. This is true both when the\ntask classiﬁer is trained and at test time when it is\napplied to the target domain.\nAn exception of this pipeline are end-to-end\nmodels that jointly learn to represent the data and\nto perform the classiﬁcation task, exploiting the\nunlabeled and labeled data together. A representa-\ntive member of this class of models (MSDA-DAN,\n(Ganin et al., 2016)) is one of our baselines.\nPivot Based Domain Adaptation This ap-\nproach was proposed by Blitzer et al. (2006,\n2007), through their SCL method. Its main idea\nis to divide the shared feature space of the source\nand the target domains to a set of pivot features\nthat are frequent in both domains and have a strong\nimpact on the source domain task classiﬁer, and a\ncomplementary set of non-pivot features.\nIn SCL, after the original feature set is divided\ninto the pivot and non-pivot subsets, this divi-\nsion is utilized in order to map the original fea-\nture space of both domains into a shared, low-\ndimensional, real-valued feature space. To do so,\na binary classiﬁer is deﬁned for each of the pivot\nfeatures. This classiﬁer takes the non-pivot fea-\ntures of an input example as its representation,\nand is trained on the unlabeled data from both the\nsource and the target domains, to predict whether\nits associated pivot feature appears in the example\nor not. Note that no human annotation is required\nfor the training of these classiﬁers, the supervision\nsignal is in the unlabeled data. The matrix whose\ncolumns are the weight vectors of the classiﬁers is\npost-processed with singular value decomposition\n(SVD) and the derived matrix maps feature vectors\nfrom the original space to the new.\nSince the presentation of SCL, pivot-based DA\nhas been researched extensively (e.g. (Pan et al.,\n2010; Gouws et al., 2012; Bollegala et al., 2015;\nYu and Jiang, 2016; Ziser and Reichart, 2017)).\nPBLM is a pivot-based method but, in contrast to\nprevious models, it relies on sequential NNs to ex-\nploit the structure of the input text. Even models\nsuch as (Bollegala et al., 2015), that embed pivots\nand non-pivots so that the former can predict if the\nlatter appear in their neighborhood, learn a single\nrepresentation for all the occurrences of a word in\nthe input corpus. That is, Bollegala et al. (2015),\nas well as other methods that learn cross-domain\nword embeddings (Yang et al., 2017), learn word-\ntype representations, rather than context speciﬁc\nrepresentations. In Sec. 3 we show how PBLM’s\ncontext speciﬁc outputs naturally feed structure\naware task classiﬁers such as LSTM and CNN.\nAE Based Domain Adaptation The basic ele-\nments of an autoencoder are an encoder function\neand a decoder function d, and its output is a re-\nconstruction of its input x: r(x) = d(e(x)). The\nparameters of the model are trained to minimize a\nloss between xand r(x), such as their Kullback-\nLeibler (KL) divergence or their cross entropy.\nVariants of AEs are prominent in recent DA\nliterature. Examples include Stacked Denoising\nAutoencoders (SDA, (Vincent et al., 2008; Glo-\nrot et al., 2011) and marginalized SDA (MSDA,\n(Chen et al., 2012)) that is more computationally\nefﬁcient and scalable to high-dimensional feature\nspaces than SDA, and has been extended in var-\nious manners (e.g. (Yang and Eisenstein, 2014;\nClinchant et al., 2016)). Finally, models based\non variational autoencoders (Kingma and Welling,\n2014; Rezende et al., 2014) have recently been\napplied in DA (e.g. variational fair autoencoder\n(Louizos et al., 2016)), but in our experiments they\nwere still not competitive with MSDA.\nWhile AE based models have set a new state-\nof-the-art for DA in NLP, they are mostly based\non noise reduction in the representation and do\nnot exploit task speciﬁc and linguistic information.\nThis paved the way for ZR17 that integrated pivot-\nbased ideas into domain adaptation with AEs.\nCombining Pivots and AEs in Domain Adapta-\ntion ZR17 combined AEs and pivot-based mod-\neling for DA. Their basic model (AE-SCL) is a\nthree layer feed-forward network where the non-\npivot features are fed to the input layer, encoded\n1243\ninto a hidden representation and this hidden rep-\nresentation is then decoded into the pivot features\nof the input example. Their advanced model (AE-\nSCL-SR) has the same architecture but the decod-\ning matrix consists of pre-trained embeddings of\nthe pivot features, which encourages input docu-\nments with similar pivots to have similar hidden\nrepresentations. These embeddings are induced by\nword2vec (Mikolov et al., 2013) trained with unla-\nbeled data from the source and the target domains.\nZR17 have demonstrated the superiority of\ntheir models (especially, AE-SCL-SR) over SCL\n(pivot-based, no AE), MSDA (AE-based, no piv-\nots) and MSDA-DAN (AE-based with adversar-\nial enhancement, no pivots) in 16 cross-domain\nsentiment classiﬁcation setups, including the 12\nlegacy setups of Blitzer et al. (2007). However,\nas in previous pivot based methods, AE-SCL and\nAE-SCL-SR learn a single, structure-indifferent,\nfeature representation of the input text. Our core\nidea is to implement a pivot-based sequential neu-\nral model that exploits the structure of its input text\nand that its output representations can be smoothly\nintegrated with structure aware classiﬁers such as\nLSTM and CNN. Our second goal is motivated by\nthe strong performance of LSTM and CNN in text\nclassiﬁcation tasks (Yogatama et al., 2017).\n3 Domain Adaptation with PBLMs\nWe now introduce our PBLM model that learns\nrepresentations for DA. As PBLM is inspired by\nlanguage modeling, we assume the original fea-\nture set of the NLP task classiﬁer consists of word\nunigrams and bigrams. This choice of features\nalso allows us to directly compare our work to the\nrich literature on DA for sentiment classiﬁcation\nwhere this is the standard feature set. PBLM, how-\never, is not limited to word n-gram features.\nWe start with a brief description of LSTM based\nlanguage modeling (LSTM-LM, (Mikolov et al.,\n2010)) and then describe how PBLM modiﬁes that\nmodel in order to learn pivot-based representations\nthat are aware of the structure of the input text. We\nthen show how to employ these representations in\nstructure aware text classiﬁcation (with LSTM or\nCNN) and how to train such PBLM-LSTM and\nPBLM-CNN classiﬁcation pipelines.\nLSTM Language Modeling LSTMs address\nthe vanishing gradient problem commonly found\nin RNNs (Elman, 1990) by incorporating gating\nfunctions into their state dynamics (Hochreiter and\nSchmidhuber, 1997). At each time step, an LSTM\nmaintains a hidden vector, ht, computed in a se-\nquence of non-linear transformations of the input\nxt and the previous hidden states h1,...,h t−1.\nGiven an input word, an LSTM-LM should pre-\ndict the next word in the sequence. For a lexicon\nV, the probability of the j-th word is:\np(yt = j) = eht·Wj\n∑|V |\nk=1 eht·Wk\nHere, Wi is a parameter vector learned by the net-\nwork for each of the words in the vocabulary. The\nloss function we consider in this paper is the cross-\nentropy loss over these probabilities.\nvery witty great story not bad overall\nNONE\nnot \nbad NONENONENONEgreat NONE\n(a)\nvery witty great story not bad overall\nSentiment \nclass\n(b)\nvery witty great story not bad overall\nText matrix \nFilters \nMax-Pooling  \nSentiment \nclass\nFCClassification\n(c)\nFigure 2: (a) Second order PBLM for representa-\ntion learning. (b+c) PBLM based models for DA:\nPBLM-LSTM (b) and PBLM-CNN (c).\nRepresentation Learning with PBLM Fig-\nure 2a provides an illustration of the PBLM model.\nThe ﬁrst (bottom) layer is an embedding layer,\n1244\nwhere a 1-hot word vector input is multiplied by\na (randomly initialized) parameter matrix before\nbeing passed to the next layer. The second layer is\nan LSTM that predicts the next bigram or unigram\nif one of these is a pivot (if both are, it predicts the\nbigram). Otherwise its prediction is NONE.\nPBLM operates similarly to LSTM-LM. The\nbasic difference between the models is the predic-\ntion they make for a given input word (xt). While\nan LSTM-LM aims to predict the next input word,\nPBLM predicts the next word unigram or bigram\nif one of these is a pivot, and NONE otherwise.\nPBLM is very ﬂexible. It can be of any order:\na k-order PBLM predicts the longest preﬁx of the\nsequence consisting of the next k words, as long\nas that preﬁx forms a pivot. If none of the pre-\nﬁxes forms a pivot then PBLM predicts NONE. 3\nMoreover, while PBLM is deﬁned here over word\nsequences, it can be deﬁned over other sequences,\ne.g., the sentence sequence of a document.\nIntuitively, in the example of ﬁg. 2a a second or-\nder model is more informative for sentiment clas-\nsiﬁcation than a ﬁrst-order model (that predicts\nonly the next word unigram in case that word is\na pivot) would be. Indeed, ”not bad” conveys\nthe relevant sentiment-related information, while\n”bad” is misleading with respect to that same sen-\ntiment. Notice that after the preﬁx ”very witty”\nthe model predicts ”great” and not ”great story”\nbecause in this example ”great” is a pivot while\n”great story” is not, as ”great story” is unlikely to\nbe frequent outside the book review domain.\nFigures 2a and 1 also demonstrate a major ad-\nvantage of PBLM over models that learn a single\ntext representation. From the book review exam-\nple in ﬁg. 2a, PBLM learns the connection be-\ntween witty - an adjective that is often used to\ndescribe books, but not kitchen appliances - and\ngreat - a common positive adjective in both do-\nmains, and hence a pivot feature. Likewise, from\nthe example of ﬁg. 1 PBLM learns the connection\nbetween easy - an adjective that is often used to\ndescribe kitchen appliances, but not books - and\ngreat. That is, PBLM is able to learn the connec-\ntion between witty and easy which will facilitate\nadaptation between the books and kitchen appli-\nances domains. Previous work that learns a single\ntext representation, in contrast, would learn from\nﬁg. 1 a connection between easy and the three piv-\nots: very excited, great and disappointed. From\n3A word sequence is one of its own preﬁxes.\nﬁg. 2a such a method would learn the connection\nbetween witty and great and not bad. The connec-\ntion between witty and easy will be much weaker.\nStructure Aware Classiﬁcation with PBLM\nRepresentations PBLM not only exploits the\nsequential nature of its input text, but its output\nvectors can feed LSTM (PBLM-LSTM, ﬁg. 2b)\nand CNN (PBLM-CNN, ﬁg. 2c) classiﬁers.\nPBLM-LSTM is a three-layer model. The bot-\ntom two layers are the PBLM model of ﬁg. 2a.\nWhen PBLM is combined with a classiﬁer, its\nsoftmax layer (top layer of ﬁg. 2a) is cut and only\nits output vectors (ht) are passed to the next LSTM\nlayer (third layer of ﬁg. 2b). The ﬁnal hidden vec-\ntor of that layer feeds the task classiﬁer.\nNote that since we cut the PBLM softmax layer\nwhen it is combined with the task classiﬁer, PBLM\nshould be trained before this combination is per-\nformed. Below we describe how we exploit this\nmodularity to facilitate domain adaptation.\nIn PBLM-CNN, the combination between the\nPBLM and the CNN is similar to ﬁg. 2b: the\nPBLM’s softmax layer is cut and a matrix whose\ncolumns are the ht vectors of the PBLM is passed\nto the CNN. We employ K different ﬁlters of size\n|ht|×d, each going over the input matrix in a slid-\ning window of dconsecutive hidden vectors, and\ngenerating a 1×(n−d+1) size vector, wherenis\nthe input text length. A max pooling is performed\nfor each of thekvectors to generate a single1×K\nvector that is fed into the task classiﬁer.\nPBLM can feed structure aware classiﬁers other\nthan LSTM and CNN. Moreover, PBLM can also\ngenerate a single text representation as in most\nprevious work. This can be done, e.g., by aver-\naging the PBLM’s hidden vectors and feeding the\naveraged vector into a linear non-structured clas-\nsiﬁer (e.g. logistic regression) or a feed-forward\nNN. In Sec. 5 we demonstrate that PBLM’s ability\nto feed structure aware classiﬁers such as LSTM\nand CNN provides substantial accuracy gains. To\nthe best of our knowledge, PBLM is unique in\nits structure aware representation: previous work\ngenerated one representation per input example.\nDomain Adaptation with PBLM Representa-\ntions We focus on unsupervised DA where the\ninput consists of a source domain labeled set and\na plentiful of unlabeled examples form the source\nand the target domains. Our goal is to use the un-\nlabeled data as a bridge between the domains.\n1245\nOur fundamental idea is to decouple the PBLM\ntraining which requires only unlabeled text, from\nthe NLP classiﬁcation task which is supervised\nand for which the required labeled example set is\navailable only for the source domain. We hence\nemploy a two step training procedure. First PBLM\n(ﬁgure 2a) is trained with unlabeled data from\nboth the source and the target domains. Then the\ntrained PBLM is combined with the classiﬁer lay-\ners (top layer of ﬁg. 2b, CNN layers of ﬁg. 2c) and\nthe ﬁnal model is trained with the source domain\nlabeled data to perform the classiﬁcation task. As\nnoted above, in the second step we cut the PBLM’s\nsoftmax layer, only its ht vectors are passed to the\nclassiﬁer. Moreover, during this step the parame-\nters of the pre-trained PBLM are held ﬁxed, only\nthe parameters of the classiﬁer layers are trained.\n4 Experimental Setup\n4Task and Domains Following a large body of\nDA work, we experiment with the task of cross-\ndomain sentiment classiﬁcation. To facilitate com-\nparison with previous work we experiment with\nthe product review domains of (Blitzer et al.,\n2007) – Books (B), DVDs (D), Electronic items\n(E) and Kitchen appliances (K) (12 ordered do-\nmain pairs) – replicating the experimental setup\nof ZR17 (including baselines, design, and hyper-\nparameter details). For each domain there are\n2000 labeled reviews, 1000 positive and 1000 neg-\native, and unlabeled reviews: 6000 (B), 34741 (D),\n13153 (E) and 16785 (K).\nTo consider a more challenging setup we ex-\nperiment with a domain consisting of user reviews\non services rather than products. We downloaded\nan airline review dataset, consisting of reviews la-\nbeled by their authors (Nguyen, 2015). We ran-\ndomly sampled 1000 positive and 1000 negative\nreviews for our labeled set, the remaining 39396\nreviews form our unlabeled set. We hence have 4\nproduct to airline and 4 airline to product setups.\nInterestingly, in the product domains unlabeled\nreviews tend to be much more positive than in the\nairline domain. Particularly, in the B domain there\nare 6.43 positive reviews on every negative review;\nin D the ratio is 7.39 to 1; in E it is 3.65 to 1; and\nin K it is 4.61 to 1. In the airline domain there are\nonly 1.15 positive reviews for every negative re-\nview. We hence expect DA from product to airline\n4The URLs of the datasets and the code (previous models\nand standard packages) we used, are in Appendix A.\nreviews and vice versa to be more challenging than\nDA from one product review domain to another.5\nBaselines We consider the following baselines:\n(a) AE-SCL-SR (ZR17). We also experimented\nwith the more basic AE-SCL but, like in ZR17,\nwe got lower results in most cases; (b) SCL with\npivot features selected using the mutual informa-\ntion criterion (SCL-MI, (Blitzer et al., 2007)). For\nthis method we used the implementation of ZR17;\n(c) MSDA (Chen et al., 2012), with code taken\nfrom the authors’ web page; (d) The MSDA-DAN\nmodel (Ganin et al., 2016) which employs a do-\nmain adversarial network (DAN) with the MSDA\nvectors as input. The DAN code is taken from\nthe authors’ repository; (e) The no domain adapta-\ntion case where the sentiment classiﬁer is trained\nin the source domain and applied to the target do-\nmain without adaptation. For this case we consider\nthree classiﬁers: logistic regression (denoted NoSt\nas it is not aware of its input’s structure), as well as\nLSTM and CNN which provide a control for the\nimportance of the structure aware task classiﬁers\nin PBLM models. To further control for this effect\nwe compare to the PBLM-NoSt model where the\nPBLM output vectors ( ht vectors generated after\neach input word) are averaged and the averaged\nvector feeds the logistic regression classiﬁer.6\nIn all the participating methods, the input fea-\ntures consist of word unigrams and bigrams. The\ndivision of the feature set into pivots and non-\npivots is based on the the method of ZR17 that\nfollowed the work of Blitzer et al. (2007) (de-\ntails are in Appendix C). The sentiment classi-\nﬁer employed with the SCL-MI, MSDA and AE-\nSCL-SR representations is the same logistic re-\ngression classiﬁer as in the NoSt condition men-\ntioned above. For these methods we concatenate\nthe representation learned by the model with the\noriginal representation and this representation is\nfed to the classiﬁer. MSDA-DAN jointly learns\nthe feature representation and performs the senti-\nment classiﬁcation task. It is hence fed by a con-\ncatenation of the original and the MSDA-induced\nrepresentations.\n5While we have the labels for our unlabeled data, we did\nnot use them in our research except in this analysis.\n6We considered several additional baselines: (1) Vari-\national fair autoencoder (Louizos et al., 2016) which per-\nformed substantially worse than the DA baselines ((a)-(d));\n(2) We tried to compare to (Bollegala et al., 2015) but, sim-\nilarly to ZR17, failed to replicate their results; and (3) We\nreplaced PBLM with an LSTM-LM, but the results substan-\ntially degraded. We do not report results for these models.\n1246\nFive Fold CV We employ a 5-fold cross-\nvalidation protocol as in (Blitzer et al., 2007; Ziser\nand Reichart, 2017). In all ﬁve folds 1600 source\ndomain examples are randomly selected for train-\ning data and 400 for development, such that both\nthe training and the development sets are balanced\nand have the same number of positive and negative\nreviews. The results we report are the averaged\nperformance of each model across these 5 folds.\nHyperparameter Tuning For all previous\nmodels, we follow the tuning process described\nin ZR17 (paper and appendices). Hyperparameter\ntuning for the PBLM models and the non-adapted\nCNN and LSTM is described in Appendix B.\n5 Results\nOverall Results Table 1 presents our results.\nPBLM models with structure aware classiﬁers\n(PBLM-LSTM and PBLM-CNN, henceforth de-\nnoted together as S-PBLM) outperform all other\nalternatives in all 20 setups and three averaged\nevaluations (All columns in the tables). The gaps\nare quite substantial – the average accuracy of\nPBLM-LSTM and PBLM-CNN compared to the\nbest baseline, AE-SCL-SR, are: 79.6% and 80.4%\nvs. 78.1% for the product review setups, 85% and\n84% vs. 78.7% for the product to airline (service)\nreview setups, and 76.1% and 76.2% vs. 68.1%\nfor the airline to product review setups.\nS-PBLM performance in the more challenging\nproduct to airline and airline to product setups are\nparticularly impressive. The challenging nature\nof these setups stems from the presumably larger\ndifferences between product and service reviews\nand from the different distribution of positive and\nnegative reviews in the unlabeled data of both do-\nmains (Sec. 4). These differences are reﬂected\nby the lower performance of the non-adapted clas-\nsiﬁers: an averaged accuracy of 70.6%-73.1%\nacross product domain pairs (three lower lines of\nthe All column of the top table), compared to an\naverage of 67.3%-69.9% across product to airline\nsetups and an average of 61.3%-62.4% across air-\nline to product setups. Moreover, while the best\nprevious method (AE-SCL-SR) achieves an av-\neraged accuracy of 78.1% for product domains\nand an averaged accuracy of 78.7% when adapt-\ning from product to airline reviews, when adapt-\ning from airline to product reviews its averaged\naccuracy drops to 68.1%. The S-PBLM models\ndo consistently better in all three setups, with an\n0.96\n0.98\n1\n1.02\n1.04\n1.06\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n1 2 3 4 5 6 7 8 9 10\nLoss\nAccuracy\nEpoch\nB->K PBLM-CNN\nsentiment accuracy PBLM loss\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n1 2 3 4 5 6 7 8 9 10\nLoss\nAccuracy\nEpoch\nD->E PBLM-LSTM\nsentiment accuracy PBLM loss\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77\n0.78\n0.7\n0.75\n0.8\n0.85\n0.9\n1 2 3 4 5 6 7 8 9 10\nLoss\nAccuracy\nEpoch\nE->A PBLM-LSTM\nsentiment accuracy PBLM loss\n1.85\n1.9\n1.95\n2\n2.05\n2.1\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n1 2 3 4 5 6 7 8 9 10\nLoss\nAccuracy\nEpoch\nA->E PBLM-CNN\nsentiment accuracy PBLM loss\nFigure 3: PBLM loss (solid, red line) vs. sentiment\naccuracy (dashed, blue line) of PBLM-CNN (top) and\nPBLM-LSTM (bottom) in four representative setups.\nPatterns in other setups are very similar.\naveraged accuracy of 80.4%, 85% and 76.2% of\nthe best S-PBLM model, respectively.\nAnalysis of S-PBLM Strength The results shed\nlight on the sources of the S-PBLM models suc-\ncess. The accuracy of these models, PBLM-\nLSTM and PBLM-CNN, is quite similar across\nsetups: their accuracy gap is up to 3.1% in all\n20 setups and up to 1% in the three averages ( All\ncolumns). However, the S-PBLM models sub-\nstantially outperform PBLM-NoSt that employs a\nstructure-indifferent classiﬁer. The averaged gaps\nare 5.6% (80.4% vs. 74.8%) in the product to\nproduct setups, 11.1% in the product to airline se-\ntups (85% vs. 73.9%) and 10.9% in the airline\nto product setups (76.2% vs. 65.3%). Hence, we\ncan safely conclude that while the integration of\nPBLM with a structured task classiﬁer has a dra-\nmatic impact on cross-domain accuracy, it is less\nimportant if that classiﬁer is an LSTM or a CNN.\nComparison with non-adapted models reveals\nthat structure aware modeling, as provided by\nLSTM and CNN, is not sufﬁcient for high perfor-\nmance. Indeed, non-adapted LSTM and CNN do\nsubstantially worse than S-PBLM in all setups. Fi-\nnally, comparison with AE-SCL-SR demonstrates\nthat while the integration of pivot based learning\nwith NNs leads to stronger results than in any\nother previous work, the structure awareness of the\nS-PBLM models substantially improves accuracy.\n1247\nProduct Review Domains (Blitzer et al., 2007)\nSource-Target D-B E-B K-B B-D E-D K-D B-E D-E K-E B-K D-K E-K All\nPBLM Models\nPBLM-LSTM 80.5 70.8 73.5 82.6 77.6 78.6 74.5 80.4 85.4 80.9 83.3 87.1 79.6\nPBLM-CNN 82.5 71.4 74.2 84.2 75 79.8 77.6 79.6 87.1 82.5 83.2 87.8 80.4\nPBLM-NoSt 74 68.6 67.4 78.3 73.2 73.3 71.3 74.2 82.1 75.5 76.9 83.2 74.8\nPrevious Work Models\nAE-SCL-SR 77.3 71.1 73 81.1 74.5 76.3 76.8 78.1 84 80.1 80.3 84.6 78.1\nMSDA 76.1 71.9 70 78.3 71 71.4 74.6 75 82.4 78.8 77.4 84.5 75.9\nMSDA-DAN 75 71 71.2 79.7 73.1 73.8 74.7 74.5 82.1 75.4 77.6 85 76.1\nSCL-MI 73.2 68.5 69.3 78.8 70.4 72.2 71.9 71.5 82.2 77.2 74 82.9 74.3\nNo Domain Adaptation\nNoSt 73.6 67.9 67.6 76 69.1 70.2 70 70.9 81.6 74 73.2 82.4 73.1\nLSTM 69.2 67.9 67.5 72.8 68.1 66.2 65.9 68.3 78.2 72.1 70.5 80.6 70.6\nCNN 71.2 65.6 66.5 73.6 67.1 70.8 69.6 69.7 79.9 72.7 72.6 80.6 71.6\nProduct and Airline Review Domains (Blitzer et al., 2007; Nguyen, 2015)\nSource-Target B-A D-A E-A K-A All (P-Air) A-B A-D A-E A-K All (Air-P)\nPBLM Models\nPBLM-LSTM 83.7 81 87.7 87.4 85 70.3 71.1 80.5 82.6 76.1\nPBLM-CNN 83.8 78.3 86.5 86.1 84 70.6 71.3 81.1 81.8 76.2\nPBLM-NoSt 74.2 74.9 72.4 73.9 73.9 62.5 62 69.6 67.3 65.3\nPrevious Work Models\nAE-SCL-SR 79.1 76.1 82.6 76.9 78.7 60.5 66 74.4 71.7 68.1\nMSDA 72.2 73.3 75.1 76.8 74.3 58.5 61 70.6 69 64.8\nMSDA-DAN 73.5 73.9 76.3 76.6 75 59.5 60.7 71 71.7 65.7\nSCL 70.9 69 80.2 72.3 73 61.7 62.1 72.3 69.7 66.4\nNo Domain Adaptation\nNoSt 68.5 67.6 74 69.6 69.9 57.5 59.7 67.2 65.2 62.4\nLSTM 68.3 65 72.1 68.6 67.3 56.7 57.3 66.2 65 61.3\nCNN 67.6 66.7 72 70 69.1 56.3 59 66 66.6 62\nTable 1: Accuracy of adaption between product review domains (top table). and between product review\ndomains and the airline (A) review domain (bottom table). All the differences between PBLM-CNN and\nAE-SCL-SR and between PBLM-LSTM and AE-SCL-SR are statistically signiﬁcant (except from E-B\nin the former comparison and E-B and K-B in the latter). Statistical signiﬁcance is computed with the\nMcNemar paired test for labeling disagreements ((Gillick and Cox, 1989; Blitzer et al., 2006),p< 0.05).\nFigure 3 further demonstrates the adequacy of\nthe PBLM architecture for domain adaptation.\nThe graphs demonstrate, for both S-PBLM mod-\nels, a strong correlation between the PBLM cross-\nentropy loss values and the sentiment accuracy\nof the resulting PBLM-LSTM and PBLM-CNN\nmodels. We show these patterns for two product\ndomain setups and two setups that involve a prod-\nuct domain and the airline domain – the patterns\nfor the other setups of table 1 are very similar.\nThis analysis highlights our major contribution.\nWe have demonstrated that it is the combination\nof four components that makes DA for sentiment\nclassiﬁcation very effective: (a) Neural network\nmodeling; (b) Pivot based modeling; (c) Structure\nawareness of the pivot-based model; and (d) Struc-\nture awareness of the task classiﬁer.\n6 Conclusions\nWe addressed the task of DA in NLP and presented\nPBLM: a representation learning model that com-\nbines pivot-based ideas and NN modeling, in a\nstructure aware manner. Unlike previous work,\nPBLM exploits the structure of its input, and its\noutput consists of a vector per input word. PBLM-\nLSTM and PBLM-CNN substantially outperform\nstrong previous models in traditional and newly\npresented sentiment classiﬁcation DA setups.\nIn future we intend to extend PBLM so that it\ncould deal with NLP tasks that require the predic-\ntion of a linguistic structure. For example, we be-\nlieve that PBLM can be smoothly integrated with\nrecent LSTM-based parsers (e.g. (Dyer et al.,\n2015; Kiperwasser and Goldberg, 2016; Dozat and\nManning, 2017)). We also intend to extend the\nreach of our approach to cross-lingual setups.\n1248\nReferences\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from differ-\nent domains. Machine learning 79(1-2):151–175.\nhttps://doi.org/10.1007/s10994-009-5152-4.\nJohn Blitzer, Mark Dredze, Fernando Pereira,\net al. 2007. Biographies, bollywood, boom-\nboxes and blenders: Domain adaptation for\nsentiment classiﬁcation. In Proc. of ACL .\nhttp://aclweb.org/anthology/P07-1056.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural cor-\nrespondence learning. In Proc. of EMNLP .\nhttp://aclweb.org/anthology/W06-1615.\nDanushka Bollegala, Takanori Maehara, and Ken-ichi\nKawarabayashi. 2015. Unsupervised cross-domain\nword representation learning. In Proc. of ACL .\nhttps://doi.org/10.3115/v1/P15-1071.\nDanushka Bollegala, Yutaka Matsuo, and Mit-\nsuru Ishizuka. 2011. Relation adaptation:\nlearning to extract novel relations with min-\nimum supervision. In Proc. of IJCAI .\nhttps://doi.org/10.1109/TKDE.2011.250.\nCiprian Chelba and Alex Acero. 2004. Adap-\ntation of maximum entropy capitalizer: Little\ndata can help a lot. In Proc. of EMNLP .\nhttp://aclweb.org/anthology/W04-3237.\nMinmin Chen, Yixin Chen, and Kilian Q Weinberger.\n2011. Automatic feature decomposition for single\nview co-training. In Proc. of ICML. http://dblp.uni-\ntrier.de/rec/bib/conf/icml/ChenWC11.\nMinmin Chen, Zhixiang Xu, Kilian Weinberger, and\nFei Sha. 2012. Marginalized denoising autoen-\ncoders for domain adaptation. In Proc. of ICML .\nhttp://icml.cc/2012/papers/416.pdf.\nSt´ephane Clinchant, Gabriela Csurka, and Boris\nChidlovskii. 2016. A domain adaptation regulariza-\ntion for denoising autoencoders. In Proc. of ACL\n(short papers) . https://doi.org/10.18653/v1/P16-\n2005.\nHal Daum ´e III. 2007. Frustratingly easy\ndomain adaptation. In Proc. of ACL .\nhttp://aclweb.org/anthology/P07-1009.\nHal Daum ´e III and Daniel Marcu. 2006. Domain\nadaptation for statistical classiﬁers. Journal\nof Artiﬁcial Intelligence Research 26:101–126.\nhttp://dl.acm.org/citation.cfm?id=1622559.1622562.\nTimothy Dozat and Christopher D Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In Proc. of ICLR.\nChris Dyer, Miguel Ballesteros, Wang Ling,\nAustin Matthews, and Noah A Smith. 2015.\nTransition-based dependency parsing with stack\nlong short-term memory. In Proc. of ACL .\nhttp://www.aclweb.org/anthology/P15-1033.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science 14(2):179–211.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc ¸ois Lavio-\nlette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks.\nJournal of Machine Learning Research 17(59):1–\n35. http://jmlr.org/papers/v17/15-239.html.\nLaurence Gillick and Stephen J Cox. 1989. Some sta-\ntistical issues in the comparison of speech recog-\nnition algorithms. In Proc. of ICASSP . IEEE.\nhttps://doi.org/10.1109/ICASSP.1989.266481.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Domain adaptation for large-scale sentiment\nclassiﬁcation: A deep learning approach. In In\nproc. of ICML . pages 513–520. http://dblp.uni-\ntrier.de/rec/bib/conf/icml/GlorotBB11.\nStephan Gouws, GJ Van Rooyen, MIH Medialab, and\nYoshua Bengio. 2012. Learning structural corre-\nspondences across different linguistic domains with\nsynchronous neural language models. In Proc. of\nthe xLite Workshop on Cross-Lingual Technologies,\nNIPS.\nSepp Hochreiter and J ¨urgen Schmidhu-\nber. 1997. Long short-term memory.\nNeural computation 9(8):1735–1780.\nhttps://doi.org/10.1162/neco.1997.9.8.1735.\nJiayuan Huang, Arthur Gretton, Karsten M Borgwardt,\nBernhard Sch¨olkopf, and Alex J Smola. 2007. Cor-\nrecting sample selection bias by unlabeled data. In\nProc. of NIPS.\nJing Jiang and ChengXiang Zhai. 2007. Instance\nweighting for domain adaptation in nlp. In Proc.\nof ACL. http://aclweb.org/anthology/P07-1034.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In In Proc. of EMNLP .\nhttp://www.aclweb.org/anthology/D14-1181.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proc. of\nICLR.\nDiederik P Kingma and Max Welling.\n2014. Auto-encoding variational bayes.\nIn Proc. of ICLR . http://dblp.uni-\ntrier.de/rec/bib/journals/corr/KingmaW13.\nEliyahu Kiperwasser and Yoav Goldberg. 2016.\nSimple and accurate dependency parsing us-\ning bidirectional lstm feature representations.\nTransactions of the ACL (TACL) 4:313–327.\nhttps://transacl.org/ojs/index.php/tacl/article/view/885.\n1249\nYann LeCun, L ´eon Bottou, Yoshua Bengio, and\nPatrick Haffner. 1998. Gradient-based learn-\ning applied to document recognition. Pro-\nceedings of the IEEE 86(11):2278–2324.\nhttps://doi.org/10.1109/5.726791.\nChristos Louizos, Kevin Swersky, Yujia Li, Max\nWelling, and Richard Zemel. 2016. The\nvariational fair autoencoder http://dblp.uni-\ntrier.de/rec/bib/journals/corr/LouizosSLWZ15.\nYishay Mansour, Mehryar Mohri, and Afshin Ros-\ntamizadeh. 2009. Domain adaptation with multiple\nsources. In Proc. of NIPS.\nDavid McClosky, Eugene Charniak, and Mark\nJohnson. 2010. Automatic domain adap-\ntation for parsing. In Proc. of NAACL .\nhttp://aclweb.org/anthology/N10-1004.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nInterspeech. https://doi.org/10.1109/AINL-ISMW-\nFRUCT.2015.7382966.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proc. of NIPS.\nQuang Nguyen. 2015. The airline review dataset.\nhttps://github.com/quankiquanki/\nskytrax-reviews-dataset. Scraped from\nwww.airlinequality.com.\nSinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang\nYang, and Zheng Chen. 2010. Cross-domain sen-\ntiment classiﬁcation via spectral feature alignment.\nIn Proceedings of the 19th international confer-\nence on World wide web . ACM, pages 751–760.\nhttps://doi.org/10.1145/1772690.1772767.\nDanilo Jimenez Rezende, Shakir Mohamed, and\nDaan Wierstra. 2014. Stochastic backpropaga-\ntion and approximate inference in deep genera-\ntive models. In Proc. of ICML . http://dblp.uni-\ntrier.de/rec/bib/conf/icml/RezendeMW14.\nBrian Roark and Michiel Bacchiani. 2003. Su-\npervised and unsupervised pcfg adaptation to\nnovel domains. In Proc. of HLT-NAACL .\nhttp://aclweb.org/anthology/N03-1027.\nAlexander M Rush, Roi Reichart, Michael Collins,\nand Amir Globerson. 2012. Improved pars-\ning and pos tagging using inter-sentence consis-\ntency constraints. In Proc. of EMNLP-CoNLL .\nhttp://aclweb.org/anthology/D12-1131.\nTobias Schnabel and Hinrich Sch ¨utze. 2014. Flors:\nFast and simple domain adaptation for part-\nof-speech tagging. Transactions of the Asso-\nciation for Computational Linguistics 2:15–26.\nhttp://aclweb.org/anthology/Q/Q14/Q14-1002.pdf.\nIvan Titov. 2011. Domain adaptation by con-\nstraining inter-domain variability of latent\nfeature representation. In Proc. of ACL .\nhttp://www.aclweb.org/anthology/P11-1007.\nPascal Vincent, Hugo Larochelle, Yoshua Ben-\ngio, and Pierre-Antoine Manzagol. 2008. Ex-\ntracting and composing robust features with de-\nnoising autoencoders. In Proc. of ICML .\nhttps://doi.org/10.1145/1390156.1390294.\nWei Yang, Wei Lu, and Vincent Zheng. 2017. A simple\nregularization-based algorithm for learning cross-\ndomain word embeddings. In Proc. of EMNLP .\nhttps://www.aclweb.org/anthology/D17-1312.\nYi Yang and Jacob Eisenstein. 2014. Fast easy unsu-\npervised domain adaptation with marginalized struc-\ntured dropout. In Proc. of ACL (short papers) .\nhttps://doi.org/10.3115/v1/P14-2088.\nDani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-\nsom. 2017. Generative and discriminative text clas-\nsiﬁcation with recurrent neural networks. arXiv\npreprint arXiv:1703.01898 .\nJianfei Yu and Jing Jiang. 2016. Learning sentence\nembeddings with auxiliary tasks for cross-domain\nsentiment classiﬁcation. In Proc. of EMNLP .\nhttp://aclweb.org/anthology/D16-1023.\nYftah Ziser and Roi Reichart. 2017. Neural structural\ncorrespondence learning for domain adaptation. In\nProc. of CoNLL . http://aclweb.org/anthology/K17-\n1040.\nA URLs of Code and Data\nAs mentioned in section 4 of the paper, we provide\nhere a list of URLs for the code and data we use\nin the paper. We do that in order to avoid a large\nnumber of footnotes in the main paper:\n•Blitzer et al. (2007) product review\ndata: http://www.cs.jhu.edu/\n˜mdredze/datasets/sentiment/\nindex2.html.\n•The airline review data is (Nguyen, 2015).\n•Code for the AE-SCL and AE-SCL-SR\nmodels of ZR17 (Ziser and Reichart, 2017):\nhttps://github.com/yftah89/\nNeural-SCLDomain-Adaptation.\n•Code for the SCL-MI method of Blitzer et al.\n(2007): see footnote 7 (the URL does not ﬁt\ninto the line width).\n7https://github.com/yftah89/\nstructural-correspondence-learning-SCL\n1250\n•Code for MSDA (Chen et al., 2012): http:\n//www.cse.wustl.edu/˜mchen.\n•Code for the domain adversarial network\nused as part of the MSDA-DAN baseline\n(Ganin et al., 2016): https://github.\ncom/GRAAL-Research/domain_\nadversarial_neural_network.\n•Logistic regression code: http:\n//scikit-learn.org/stable/.\nB Hyperparameter Tuning and\nExperimental Details\nHyperparameter Tuning As discussed in sec-\ntion 4 of the paper, for all previous work models,\nwe follow the experimental setup of ZR17 (pa-\nper and appendices) including their hyperparam-\neter estimation protocol. The hyperparameters of\nthe PBLM models and the non-adapted CNN and\nLSTM are provided here. For PBLM we consid-\nered the following hyperparameteres:\n•Input word embedding size:\n(32,64,128,256).\n•Number of pivot features:\n(100,200,300,400,500).\n•|ht|: (128,256,512).\n•PBLM model order: second order.\nFor the LSTM in PBLM-LSTM as well as the\nbaseline non-adapted LSTM we considered the\nsame |ht|and input word embedding size values\nas for PBLM. For PBLM-CNN and for the base-\nline, non-adapted, CNN we only experimented\nwith K = 250 ﬁlters and with a kernel of size\nd= 3.\nAll the algorithms in the paper that involve a\nCNN or a LSTM (including the PBLM itself) are\ntrained with the ADAM algorithm (Kingma and\nBa, 2015). For this algorithm we used the param-\neters described in the original ADAM article:\n•Learning rate: lr= 0.001.\n•Exponential decay rate for the 1st moment es-\ntimates: β1 = 0.9.\n•Exponential decay rate for the 2nd moment\nestimates: β2 = 0.999.\n•Fuzz factor: ϵ= 1e−08.\n•Learning rate decay over each update:\ndecay= 0.0.\nExperimental Details All sequential models\nconsidered in our experiments are fed with one re-\nview example at a time. For all models in the pa-\nper, punctuation is ﬁrst removed from the text be-\nfore it is processed by the model (sentence bound-\naries are still encoded). This is the only pre-\nprecessing step we employ in the paper.\nWe considered several alternative implementa-\ntions of the PBLM-NoSt baseline. In the vari-\nant we selected the PBLM output vectors (ht vec-\ntors generated after each word of the input review)\nare averaged and the averaged vector feeds a non-\nstructured logistic regression classiﬁer. We also\ntried to take only the ﬁnal ht vector of PBLM as\nan input to the classiﬁer or to sum the ht vectors\ninstead of taking their average. These alternatives\ngave worse results.\nC Pivot Feature Selection\nAs mentioned in the main paper, the division of\nthe feature set into pivots and non-pivots is based\non the unlabeled data from both the source and the\ntarget domains, using the method of ZR17 (which\nis in turn based on (Blitzer et al., 2007)). Here we\nprovide the details of the pivot selection criterion.\nPivot features are frequent in the unlabeled data\nof both the source and the target domains, appear-\ning at least 10 times in each, and among those fea-\ntures are the ones with the highest mutual informa-\ntion with the task (sentiment) label in the source\ndomain labeled data. For non-pivot features we\nconsider unigrams and bigrams that appear at least\n10 times in their domain.\n1251",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8223330974578857
    },
    {
      "name": "Representation (politics)",
      "score": 0.7431632280349731
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6751559972763062
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6441400647163391
    },
    {
      "name": "Word (group theory)",
      "score": 0.6060279011726379
    },
    {
      "name": "Domain adaptation",
      "score": 0.5666275024414062
    },
    {
      "name": "Artificial neural network",
      "score": 0.49909377098083496
    },
    {
      "name": "Natural language processing",
      "score": 0.4985365867614746
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4949890971183777
    },
    {
      "name": "Exploit",
      "score": 0.47774022817611694
    },
    {
      "name": "Language model",
      "score": 0.447574645280838
    },
    {
      "name": "Feature learning",
      "score": 0.4145324230194092
    },
    {
      "name": "Machine learning",
      "score": 0.3818458616733551
    },
    {
      "name": "Mathematics",
      "score": 0.07016706466674805
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}