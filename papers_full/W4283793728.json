{
    "title": "Differential Assessment of Black-Box AI Agents",
    "url": "https://openalex.org/W4283793728",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2798029543",
            "name": "Rashmeet Kaur Nayyar",
            "affiliations": [
                "Arizona State University"
            ]
        },
        {
            "id": "https://openalex.org/A2150699366",
            "name": "Pulkit Verma",
            "affiliations": [
                "Arizona State University"
            ]
        },
        {
            "id": "https://openalex.org/A2123069162",
            "name": "Siddharth Srivastava",
            "affiliations": [
                "Arizona State University"
            ]
        },
        {
            "id": "https://openalex.org/A2798029543",
            "name": "Rashmeet Kaur Nayyar",
            "affiliations": [
                "Fraunhofer Institute for Intelligent Analysis and Information Systems"
            ]
        },
        {
            "id": "https://openalex.org/A2150699366",
            "name": "Pulkit Verma",
            "affiliations": [
                "Fraunhofer Institute for Intelligent Analysis and Information Systems"
            ]
        },
        {
            "id": "https://openalex.org/A2123069162",
            "name": "Siddharth Srivastava",
            "affiliations": [
                "Fraunhofer Institute for Intelligent Analysis and Information Systems"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6762888764",
        "https://openalex.org/W6756414120",
        "https://openalex.org/W31576638",
        "https://openalex.org/W6732448551",
        "https://openalex.org/W2615896489",
        "https://openalex.org/W1530488533",
        "https://openalex.org/W6653841608",
        "https://openalex.org/W2337392266",
        "https://openalex.org/W1552549180",
        "https://openalex.org/W6670193606",
        "https://openalex.org/W6786039469",
        "https://openalex.org/W1773969059",
        "https://openalex.org/W3176655765",
        "https://openalex.org/W6799701560",
        "https://openalex.org/W2111384628",
        "https://openalex.org/W6628263010",
        "https://openalex.org/W6645670200",
        "https://openalex.org/W3183713542",
        "https://openalex.org/W6760883923",
        "https://openalex.org/W6731497439",
        "https://openalex.org/W2617295680",
        "https://openalex.org/W3134043334",
        "https://openalex.org/W2024113144",
        "https://openalex.org/W6713741781",
        "https://openalex.org/W3189222392",
        "https://openalex.org/W2945938295",
        "https://openalex.org/W2152033849",
        "https://openalex.org/W3101355526",
        "https://openalex.org/W2013831071",
        "https://openalex.org/W2951570986",
        "https://openalex.org/W4300880349",
        "https://openalex.org/W1443446730",
        "https://openalex.org/W2966842825",
        "https://openalex.org/W50425026",
        "https://openalex.org/W2575562425",
        "https://openalex.org/W2900860440",
        "https://openalex.org/W2407058104",
        "https://openalex.org/W2119709400",
        "https://openalex.org/W4283793728",
        "https://openalex.org/W2566920815",
        "https://openalex.org/W4256120538"
    ],
    "abstract": "Much of the research on learning symbolic models of AI agents focuses on agents with stationary models. This assumption fails to hold in settings where the agent's capabilities may change as a result of learning, adaptation, or other post-deployment modifications. Efficient assessment of agents in such settings is critical for learning the true capabilities of an AI system and for ensuring its safe usage. In this work, we propose a novel approach to differentially assess black-box AI agents that have drifted from their previously known models. As a starting point, we consider the fully observable and deterministic setting. We leverage sparse observations of the drifted agent's current behavior and knowledge of its initial model to generate an active querying policy that selectively queries the agent and computes an updated model of its functionality. Empirical evaluation shows that our approach is much more efficient than re-learning the agent model from scratch. We also show that the cost of differential assessment using our method is proportional to the amount of drift in the agent's functionality.",
    "full_text": "Differential Assessment of Black-Box AI Agents\nRashmeet Kaur Nayyar*, Pulkit Verma*, and Siddharth Srivastava\nAutonomous Agents and Intelligent Robots Lab,\nSchool of Computing and Augmented Intelligence, Arizona State University, AZ, USA\n{rmnayyar, verma.pulkit, siddharths}@asu.edu\nAbstract\nMuch of the research on learning symbolic models of AI\nagents focuses on agents with stationary models. This as-\nsumption fails to hold in settings where the agent’s capa-\nbilities may change as a result of learning, adaptation, or\nother post-deployment modifications. Efficient assessment of\nagents in such settings is critical for learning the true capabil-\nities of an AI system and for ensuring its safe usage. In this\nwork, we propose a novel approach to differentially assess\nblack-box AI agents that have drifted from their previously\nknown models. As a starting point, we consider the fully ob-\nservable and deterministic setting. We leverage sparse obser-\nvations of the drifted agent’s current behavior and knowl-\nedge of its initial model to generate an active querying pol-\nicy that selectively queries the agent and computes an up-\ndated model of its functionality. Empirical evaluation shows\nthat our approach is much more efficient than re-learning the\nagent model from scratch. We also show that the cost of dif-\nferential assessment using our method is proportional to the\namount of drift in the agent’s functionality.\n1 Introduction\nWith increasingly greater autonomy in AI systems in recent\nyears, a major problem still persists and has largely been\noverlooked: how do we accurately predict the behavior of a\nblack-box AI agent that is evolving and adapting to changes\nin the environment it is operating in? And how do we ensure\nits reliable and safe usage? Numerous factors could cause\nunpredictable changes in agent behaviors: sensors and actu-\nators may fail due to physical damage, the agent may adapt\nto a dynamic environment, users may change deployment\nand use-case scenarios, etc. Most prior work on the topic\npresumes that the functionalities and the capabilities of AI\nagents are static, while some works start with a tabula-rasa\nand learn the entire model from scratch. However, in many\nreal-world scenarios, the agent model is transient and only\nparts of its functionality change at a time.\nBryce, Benton, and Boldt (2016) address a related prob-\nlem where the system learns the updated mental model of a\nuser using particle filtering given prior knowledge about the\nuser’s mental model. However, they assume that the entity\n*Equal contribution. Alphabetical order.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAgent update \nModel learning from scratch \n(expensive)\nInitially known model of   \n \nSparse observations   \n(collected once) \nQuery Response \nLearned model of  \n \nModel learning from scratch \nnot needed\nDifferential Assessment of \nAI System (DAAISy)\nevents \nX \nFigure 1: The Differential Assessment of AI System\n(DAAISy) takes as input the initially known model of the\nagent prior to model drift, available observations of the up-\ndated agent’s behavior, and performs a selective dialog with\nthe black-box AI agent to output its updated model through\nefficient model learning.\nbeing modeled can tell the learning system about flaws in\nthe learned model if needed. This assumption does not hold\nin settings where the entity being modeled is a black-box\nAI system: most such systems are either implemented using\ninscrutable representations or otherwise lack the ability to\nautomatically generate a model of their functionality (what\nthey can do and when) in terms the user can understand.\nThe problem of efficiently assessing, in human-interpretable\nterms, the functionality of such a non-stationary AI system\nhas received little research attention.\nThe primary contribution of this paper is an algorithm for\ndifferential assessment of black-box AI systems (Fig. 1).\nThis algorithm utilizes an initially known interpretable\nmodel of the agent as it was in the past, and a small set of\nobservations of agent execution. It uses these observations to\ndevelop an incremental querying strategy that avoids the full\ncost of assessment from scratch and outputs a revised model\nof the agent’s new functionality. One of the challenges in\nlearning agent models from observational data is that reduc-\ntions in agent functionality often do not correspond to spe-\ncific “evidence” in behavioral observations, as the agent may\nnot visit states where certain useful actions are no longer ap-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n9868\nplicable. Our analysis shows that if the agent can be placed\nin an “optimal” planning mode, differential assessment can\nindeed be used to query the agent and recover information\nabout reduction in functionality. This “optimal” planning\nmode is not necessarily needed for learning about increase in\nfunctionality. Empirical evaluations on a range of problems\nclearly demonstrate that our method is much more efficient\nthan re-learning the agent’s model from scratch. They also\nexhibit the desirable property that the computational cost of\ndifferential assessment is proportional to the amount of drift\nin the agent’s functionality.\nRunning Example Consider a battery-powered rover with\nlimited storage capacity that collects soil samples and takes\npictures. Assume that its planning model is similar to IPC\ndomain Rovers (Long and Fox 2003). It has an action that\ncollects a rock sample at a waypoint and stores it in a stor-\nage iff it has at least half of the battery capacity remaining.\nSuppose there was an update to the rover’s system and as\na result of this update, the rover can now collect the rock\nsample only when its battery is full, as opposed to at least\nhalf-charged battery that it needed before. Mission planners\nfamiliar with the earlier system and unaware about the ex-\nact updates in the functionality of the rover would struggle\nto collect sufficient samples. This could jeopardise multiple\nmissions if it is not detected in time.\nThis example illustrates how our system could be of value\nby differentially detecting such a drift in the functionality of\na black-box AI system and deriving its true functionality.\nThe rest of this paper is organized as follows: The next\nsection presents background terminology. This is followed\nby a formalization of the differential model assessment prob-\nlem in Section 3. Section 4 presents our approach for differ-\nential assessment by first identifying aspects of the agent’s\nfunctionality that may be affected (Section 4.1) followed by\nthe process for selectively querying the agent using a primi-\ntive set of queries. We present empirical evaluation of the ef-\nficiency of our approach on randomly generated benchmark\nplanning domains in Section 5. Finally, we discuss relevant\nrelated work in Section 6 and conclude in Section 7.\n2 Preliminaries\nWe consider models that express an agent’s functionalities in\nthe form of STRIPS-like planning models (Fikes and Nils-\nson 1971; McDermott et al. 1998; Fox and Long 2003) as\ndefined below.\nDefinition 1. A planning domain model is a tuple M =\n⟨P, A⟩, where P = {pr1\n1 , . . . , prn\nn }is a finite set of predi-\ncates with arities ri, i ∈[1, n]; and A = {a1, . . . , ak}is a fi-\nnite set of parameterized relational actions. Each actionai ∈\nA is represented as a tuple ⟨header(ai), pre(ai), eff(ai)⟩,\nwhere header(ai) represents the action header consisting\nof the name and parameters for the action ai, pre(ai) rep-\nresents the conjunction of positive or negative literals that\nmust be true in a state where the action ai is applicable, and\neff(ai) is the conjunction of positive or negative literals that\nbecome true as a result of execution of the action ai.\nIn the rest of the paper, we use the term “model” to refer\nto planning domain models and use closed-world assump-\ntion as used in the Planning Domain Definition Language\n(PDDL) (McDermott et al. 1998). Given a model M and a\nset of objects O, let SM,O be the space of all states defined\nas maximally consistent sets of literals over the predicate\nvocabulary of M with O as the set of objects. We omit the\nsubscript when it is clear from context. An action a ∈A is\napplicable in a state s ∈S if s |= pre(a). The result of exe-\ncuting a is a state a(s) = s′∈S such that s′|= eff(a), and\nall atoms not in eff(a) have literal forms as in s.\nA literal corresponding to a predicate p ∈ P can ap-\npear in pre(a) or eff(a) of an action a ∈A if and only if\nit can be instantiated using a subset of parameters of a. E.g.,\nconsider an action navigate (?rover ?src ?dest) and a pred-\nicate (can\ntraverse ?rover ?x ?y) in the Rovers domain dis-\ncussed earlier. Suppose a literal corresponding to predicate\n(can\ntraverse ?rover ?x ?y) can appear in the precondition\nand/or the effect of navigate (?rover ?src ?dest) action. As-\nsuming we know ?x and ?y in can traverse, and ?src and\n?dest in navigate are of the same type waypoint, the possi-\nble lifted instantiations of predicatecan traverse compatible\nwith action navigate are (can traverse ?rover ?src ?dest),\n(can traverse ?rover ?dest ?src), (can traverse ?rover ?src\n?src), and (can traverse ?rover ?dest ?dest). The number of\nparameters in a predicate p ∈P that is relevant to an action\na ∈A, i.e., instantiated using a subset of parameters of the\naction a, is bounded by the maximum arity of the action a.\nWe formalize this notion of lifted instantiations of a predi-\ncate with an action as follows:\nDefinition 2. Given a finite set of predicates P =\n{pr1\n1 , . . . , prn\nn }with arities ri, i ∈ [1, n]; and a finite set\nof parameterized relational actions A = {aψ1\n1 , . . . , aψk\nk }\nwith arities ψj and parameters par(aψj\nj ) = ⟨α1, . . . , αψj ⟩,\nj ∈[1, k], the set of lifted instantiations of predicates P∗\nis defined as the collection {pi(σ(x1), . . . , σ(xri)) |pi ∈\nP, a∈A, σ: {x1, . . . , xri}→ par(a)}.\n2.1 Representing Models\nWe represent a model M using the set of all possible pal-\ntuples ΓM of the form γ = ⟨p, a, ℓ⟩, where a is a parameter-\nized action header for an action in A, p ∈P∗is a possible\nlifted instantiation of a predicate inP, and ℓ ∈{pre, eff }de-\nnotes a location in a, precondition or effect, where p can ap-\npear. A model M is thus a function µM : ΓM →{+ , −, ∅}\nthat maps each element inΓM to a mode in the set{+, −, ∅}.\nThe assigned mode for a pal-tuple γ ∈ΓM denotes whether\np is present as a positive literal (+), as a negative literal (−),\nor absent (∅) in the precondition (ℓ =pre) or effect (ℓ = eff )\nof the action header a.\nThis formulation of models aspal-tuples allows us to view\nthe modes for any predicate in an action’s precondition and\neffect independently. However, at times it is useful to con-\nsider a model at a granularity of relationship between a pred-\nicate and an action. We address this by representing a model\nM as a set of pa-tuples ΛM of the form ⟨p, a⟩where a is a\nparameterized action header for an action in A, and p ∈P∗\nis a possible lifted instantiation of a predicate in P. Each\npa-tuple can take a value of the form ⟨mpre, meff⟩, where\nmpre and meff represents the mode in which p appears in the\n9869\nprecondition and effect of a, respectively. Since a predicate\ncannot appear as a positive (or negative) literal in both the\nprecondition and effect of an action, ⟨+, +⟩and ⟨−, −⟩are\nnot in the range of values that pa-tuples can take. Hence-\nforth, in the context of a pal-tuple or a pa-tuple, we refer to\na as an action instead of an action header.\nMeasure of model difference Given two models M1 =\n⟨P, A1⟩and M2 = ⟨P, A2⟩, defined over the same sets\nof predicates P and action headers A, the difference be-\ntween the two models ∆(M1, M2) is defined as the num-\nber of pal-tuples that differ in their modes in M1 and M2,\ni.e., ∆(M1, M2) = |{γ ∈P ×A ×{+ , −, ∅}|µM1 (γ) ̸=\nµM2 (γ)}|.\n2.2 Abstracting Models\nSeveral authors have explored the use of abstraction in\nplanning (Sacerdoti 1974; Giunchiglia and Walsh 1992;\nHelmert, Haslum, and Hoffmann 2007; B¨ackstr¨om and Jon-\nsson 2013; Srivastava, Russell, and Pinto 2016). We define\nan abstract model as a model that does not have a mode as-\nsigned for at least one of the pal-tuples. Let ΓM be the set\nof all possible pal-tuples, and ?⃝be an additional possible\nvalue that a pal-tuple can take. Assigning ?⃝mode to a pal-\ntuple denotes that its mode is unknown. An abstract model\nM is thus a function µM : ΓM →{+ , −, ∅, ?⃝}that maps\neach element in ΓM to a mode in the set {+, −, ∅, ?⃝}. Let\nUbe the set of all abstract and concrete models that can pos-\nsibly be expressed by assigning modes in {+, −, ∅, ?⃝}to\neach pal-tuple γ ∈ΓM . We now formally define model ab-\nstraction as follows:\nDefinition 3. Given models M1 and M2, M2 is an ab-\nstraction of M1 over the set of all possible pal-tuples Γ iff\n∃Γ2 ⊆Γ s.t. ∀γ ∈Γ2, µM2 (γ) = ?⃝and ∀γ ∈Γ \\Γ2,\nµM2 (γ) = µM1 (γ).\n2.3 Agent Observation Traces\nWe assume limited access to a set of observation traces O,\ncollected from the agent, as defined below.\nDefinition 4. An observation trace o is a sequence of states\nand actions of the form ⟨s0, a1, s1, a2, . . . , sn−1, an, sn⟩\nsuch that ∀i ∈[1, n] ai(si−1) = si.\nThese observation traces can be split into multiple action\ntriplets as defined below.\nDefinition 5. Given an observation trace o =\n⟨s0, a1, s1, a2, . . . , sn−1, an, sn⟩, an action triplet is a\n3-tuple sub-sequence of o of the form ⟨si−1, ai, si⟩, where\ni ∈[1, n] and applying an action ai in state si−1 results in\nstate si, i.e., ai(si−1) = si. The states si−1 and si are called\npre- and post-states of action ai, respectively.\nAn action triplet⟨si−1, ai, si⟩is said to beoptimal if there\ndoes not exist an action sequence (of length ≥1) that takes\nthe agent from statesi−1 to si with total action cost less than\nthat of action ai, where each action ai has unit cost.\n2.4 Queries\nWe use queries to actively gain information about the func-\ntionality of an agent to learn its updated model. We assume\nthat the agent can respond to a query using a simulator. The\navailability of such agents with simulators is a common as-\nsumption as most AI systems already use simulators for de-\nsign, testing, and verification.\nWe use a notion of queries similar to Verma, Marpally,\nand Srivastava (2021), to perform a dialog with an au-\ntonomous agent. These queries use an agent to deter-\nmine what happens if it executes a sequence of actions\nin a given initial state. E.g., in the rovers domain, the\nrover could be asked: what happens when the action sam-\nple\nrock(rover1 storage1 waypoint1) is executed in an ini-\ntial state {(equipped rock analysis rover1), (battery half\nrover1), (at rover1 waypoint1)}?\nFormally, a query is a function that maps an agent to a\nresponse, which we define as:\nDefinition 6. Given a set of predicates P, a set of actions\nA, and a set of objects O, a query Q⟨s, π⟩: A→ N ×S\nis parameterized by a start state sI ∈ S and a plan π =\n⟨a1, . . . , aN ⟩, where S is the state space over P and O, and\n{a1, . . . , aN }is a subset of action space over A and O. It\nmaps agents to responses θ = ⟨nF , sF ⟩such that nF is the\nlength of the longest prefix of π that Acan successfully ex-\necute and sF ∈S is the result of that execution.\nResponses to such queries can be used to gain use-\nful information about the model drift. E.g., consider an\nagent with an internal model MA\ndrift as shown in Tab. 1.\nIf a query is posed asking what happens when the ac-\ntion sample\nrock(rover1 storage1 waypoint1) is executed\nin an initial state {(equipped rock analysis rover1), (bat-\ntery half rover1), (at rover1 waypoint1)}, the agent would\nrespond ⟨0, {(equipped rock analysis rover1), (battery half\nrover1), (at rover1 waypoint1)}⟩, representing that it was\nnot able to execute the plan, and the resulting state was\n{(equipped\nrock analysis rover1), (battery half rover1), (at\nrover1 waypoint1)}(same as the initial state in this case).\nNote that this response is inconsistent with the model MA\ninit,\nand it can help in identifying that the precondition of action\nsample rock(?r ?s ?w) has changed.\n3 Formal Framework\nOur objective is to address the problem of differential assess-\nment of black-box AI agents whose functionality may have\nchanged from the last known model. Without loss of gen-\nerality, we consider situations where the set of action head-\ners is same because the problem of differential assessment\nwith changing action headers can be reduced to that with\nuniform action headers. This is because if the set of actions\nhas increased, new actions can be added with empty precon-\nditions and effects to MA\ninit, and if it has decreased, MA\ninit can\nbe reduced similarly. We assume that the predicate vocabu-\nlary used in the two models is the same; extension to situ-\nations where the vocabulary changes can be used to model\nopen-world scenarios. However, that extension is beyond the\nscope of this paper.\nSuppose an agentA’s functionality was known as a model\nMA\ninit = ⟨P, Ainit⟩, and we wish to assess its current func-\ntionality as the model MA\ndrift = ⟨P, Adrift⟩. The drift in the\nfunctionality of the agent can be measured by changes in the\n9870\nModel Precondition Effect\nMA\ninit (equipped rock analysis?r)\n(battery half ?r)\n(at ?r ?w)\n→(rock sample taken?r)\n(store full ?r ?s)\n¬(battery half ?r)\n(battery reserve ?r)\nMA\ndrift (equipped rock analysis?r)\n(battery full ?r)\n(at ?r ?w)\n→(rock sample taken ?r)\n(store full ?r ?s)\n¬(battery full ?r)\n(battery half ?r)\nTable 1: sample rock (?r ?s ?w) action of the agent Ain\nMA\ninit and a possible drifted model MA\ndrift.\npreconditions and/or effects of all the actions in Ainit. The\nextent of the drift between MA\ninit and MA\ndrift is represented as\nthe model difference ∆(MA\ninit, MA\ndrift).\nWe formally define the problem of differential assessment\nof an AI agent below.\nDefinition 7. Given an agent Awith a functionality model\nMA\ninit, and a set of observations O collected using its current\nversion of Adrift with unknown functionality MA\ndrift, the dif-\nferential model assessment problem ⟨MA\ninit, MA\ndrift, O, A⟩is\ndefined as the problem of inferring Ain form of MA\ndrift using\nthe inputs MA\ninit, O, and A.\nWe wish to develop solutions to the problem of differ-\nential assessment of AI agents that are more efficient than\nre-assessment from scratch.\n3.1 Correctness of Assessed Model\nWe now discuss the properties that a model, which is a so-\nlution to the differential model assessment problem, should\nsatisfy. A critical property of such models is that they should\nbe consistent with the observation traces. We formally define\nconsistency of a model w.r.t. an observation trace as follows:\nDefinition 8. Let o be an observation trace\n⟨s0, a1, s1, a2, . . . , sn−1, an, sn⟩. A model M = ⟨P, A⟩\nis consistent with the observation trace o iff\n∀i ∈ {1, .., n} ∃a ∈ A and ai is a grounding of ac-\ntion a s.t. s i−1 |= pre(ai) ∧ ∀l ∈eff(ai) si |= l.\nIn addition to being consistent with observation traces, a\nmodel should also be consistent with the queries that are\nasked and the responses that are received while actively in-\nferring the model of the agent’s new functionality. We for-\nmally define consistency of a model with respect to a query\nand a response as:\nDefinition 9. Let M = ⟨P, A⟩be a model; O be a set of ob-\njects; Q = ⟨sI, π= ⟨a1, . . . an⟩⟩be a query defined using\nP, A,and O, and let θ = ⟨nF , sF ⟩, (nF ≤n) be a response\nto Q. M is consistent with the query-response ⟨Q, θ⟩iff\nthere exists an observation trace ⟨sI, a1, s1, . . . , anF , snF ⟩\nthat M is consistent with and snF ̸|= pre(anF +1) where\npre(anF +1) is the precondition of anF +1 in M.\nWe now discuss our methodology for solving the problem\nof differential assessment of AI systems.\n4 Differential Assessment of AI Systems\nDifferential Assessment of AI Systems (Alg. 1) -- DAAISy\n-- takes as input an agent Awhose functionality has drifted,\nthe model MA\ninit = ⟨P, A⟩representing the previously known\nfunctionality of A, a set of arbitrary observation traces O,\nand a set of random states S ⊆S. Alg. 1 returns a set of\nupdated models MA\ndrift, where each model MA\ndrift ∈MA\ndrift\nrepresents A’s updated functionality and is consistent with\nall observation traces o ∈O.\nA major contribution of this work is to introduce an ap-\nproach to make inferences about not just the expanded func-\ntionality of an agent but also its reduced functionality using a\nlimited set of observation traces. Situations where the scope\nof applicability of an action reduces, i.e., the agent can no\nlonger use an action a to reach state s′from state s while\nit could before (e.g., due to addition of a precondition lit-\neral), are particularly difficult to identify because observing\nits behavior does not readily reveal what it cannot do in a\ngiven state. Most observation based action-model learners,\neven when given access to an incomplete model to start with,\nfail to make inferences about reduced functionality. DAAISy\nuses two principles to identify such a functionality reduc-\ntion. First, it uses active querying so that the agent can be\nmade to reveal failure of reachability, and second, we show\nthat if the agent can be placed in optimal planning mode,\nplan length differences can be used to infer a reduction in\nfunctionality.\nDAAISy performs two major functions; it first identifies a\nsalient set of pal-tuples whose modes were likely affected\n(line 1 of Alg. 1), and then infers the mode of such af-\nfected pal-tuples accurately through focused dialog with the\nagent (line 2 onwards of Alg. 1). In Sec. 4.1, we present\nour method for identifying a salient set of potentially af-\nfected pal-tuples that contribute towards expansion in the\nfunctionality of the agent through inference from available\narbitrary observations. We then discuss the problem of iden-\ntification of pal-tuples that contribute towards reduction in\nthe functionality of the agent and argue that it cannot be per-\nformed using successful executions in observations of sat-\nisficing behavior. We show that pal-tuples corresponding to\nreduced functionality can be identified if observations of op-\ntimal behavior of the agent are available (Sec. 4.1). Finally,\nwe present how we infer the nature of changes in all affected\npal-tuples through a query-based interaction with the agent\n(Sec. 4.2) by building upon the Agent Interrogation Algo-\nrithm (AIA) (Verma, Marpally, and Srivastava 2021). Iden-\ntifying affected pal-tuples helps reduce the computational\ncost of querying as opposed to the exhaustive querying strat-\negy used by AIA. We now discuss the two major functions\nof Alg. 1 in detail.\n4.1 Identifying Potentially Affected pal-tuples\nWe identify a reduced set of pal-tuples whose modes were\npotentially affected during the model drift, denoted by Γδ,\nusing a small set of available observation tracesO. We draw\ntwo kinds of inferences from these observation traces: in-\nferences about expanded functionality, and inferences about\nreduced functionality. We discuss our method for inferring\n9871\nAlgorithm 1: Differential Assessment of AI Systems\nInput:MA\ninit, O, A, S\nOutput: MA\ndrift\n1: Γδ ←identify affected pals()\n2: Mabs ←set pal-tuples in MA\ninit corresponding to Γδ to ?⃝\n3: MA\ndrift ←{M abs}\n4: for each γ in Γδ do\n5: for each Mabs in MA\ndrift do\n6: Mabs ←Mabs ×{γ+, γ−, γ∅}\n7: Msieved ←{}\n8: if action corresponding to γ: γa in O then\n9: spre ←states where γa applicable(O, γa)\n10: Q ←⟨spre \\{γ p ∪¬γp}, γa ⟩\n11: θ ←ask query(A, Q)\n12: Msieved ←sieve models(Mabs, Q, θ)\n13: else\n14: for each pair ⟨Mi, Mj⟩in Mabs do\n15: Q ←generate query(Mi, Mj, γ, S)\n16: θ ←ask query(A, Q)\n17: Msieved ←sieve models({Mi, Mj}, Q, θ)\n18: end for\n19: end if\n20: Mabs ←Mabs\\Msieved\n21: end for\n22: MA\ndrift ←Mabs\n23: end for\nΓδ for both types of changes in the functionality below.\nExpanded functionality To infer expanded functionality\nof the agent, we use the previously known model of the\nagent’s functionality and identify its differences with the\npossible behaviors of the agent that are consistent with O.\nTo identify the pal-tuples that directly contribute to an ex-\npansion in the agent’s functionality, we perform an analysis\nsimilar to Stern and Juba (2017), but instead of bounding\nthe predicates that can appear in each action’s precondition\nand effect, we bound the range of possible values that each\npa-tuple in MA\ndrift can take using Tab. 2. For any pa-tuple,\na direct comparison between its value in MA\ninit and possible\ninferred values in MA\ndrift provides an indication of whether it\nwas affected.\nTo identify possible values for a pa-tuple ⟨p, a⟩, we first\ncollect a set of all the action-triplets from O that contain the\naction a. For a given predicate p and state s, if s |= p then\nthe presence of predicate p is represented as pos, similarly,\nif s |= ¬p then the presence of predicate p is represented as\nneg. Using this representation, a tuple of predicate presence\n∈{(pos,pos), (pos,neg), (neg,pos), (neg,neg)}is determined\nfor the pa-tuple ⟨p, a⟩for each action triplet ⟨s, a, s′⟩∈ O\nby analyzing the presence of predicate p in the pre- and\npost-states of the action triplets. Possible values of the pa-\ntuple that are consistent with O are directly inferred from\nthe Tab. 2 using the inferred tuples of predicate presence.\nE.g., for a pa-tuple, the values ⟨+, −⟩and ⟨∅, −⟩are consis-\ntent with (pos, neg), whereas, only ⟨∅, +⟩is consistent with\n⟨mpre, meff⟩(pos,pos) (pos,neg) (neg,pos) (neg,neg)\n⟨+, −⟩ ✗ ✓ ✗ ✗\n⟨+, ∅⟩ ✓ ✗ ✗ ✗\n⟨−, +⟩ ✗ ✗ ✓ ✗\n⟨−, ∅⟩ ✗ ✗ ✗ ✓\n⟨∅, +⟩ ✓ ✗ ✓ ✗\n⟨∅, −⟩ ✗ ✓ ✗ ✓\n⟨∅, ∅⟩ ✓ ✗ ✗ ✓\nTable 2: Each row represents a possible value ⟨mpre, meff⟩\nfor a pa-tuple ⟨p, a⟩. Each column represents a possible tuple\nrepresenting presence of predicate p in the pre- and post-\nstates of an action triplet ⟨si, a, si+1⟩(discussed in Sec.4.1).\nThe cells represent whether a value forpa-tuple is consistent\nwith an action triplet in observation traces.\n(pos, pos) and (neg, pos) tuples of predicate presence that\nare inferred from O.\nOnce all the possible values for each pa-tuple in MA\ndrift\nare inferred, we identify pa-tuples whose previously known\nvalue in MA\ninit is no longer possible due to inconsistency\nwith O. The pal-tuples corresponding to such pa-tuples are\nadded to the set of potentially affected pal-tuples Γδ. Our\nmethod also infers the correct modes of a subset of pal-\ntuples. E.g., consider a predicatep and two actions triplets in\nO of the form ⟨s1, a, s′\n1⟩and ⟨s2, a, s′\n2⟩that satisfy s1 |= p\nand s2 |= ¬p. Such an observation clearly indicates that p\nis not in the precondition of action a, i.e., mode for ⟨p, a⟩\nin the precondition is ∅. Such inferences of modes are used\nto update the known functionality of the agent. We remove\nsuch pal-tuples, whose modes are already inferred, fromΓδ.\nA shortcoming of direct inference from successful execu-\ntions in available observation traces is that it cannot learn\nany reduction in the functionality of the agent, as discussed\nin the beginning of Sec. 4. We now discuss our method to\naddress this limitation and identify a larger set of potentially\naffected pal-tuples.\nReduced functionality We conceptualize reduction in\nfunctionality as an increase in the optimal cost of going from\none state to another. More precisely, reduction in functional-\nity represents situations where there exist states si, sj such\nthat the minimum cost of going from si to sj is higher in\nMA\ndrift than in MA\ninit. In this paper, this cost refers to the num-\nber of steps between the pair of states as we consider unit\naction costs. This notion encompasses situations with reduc-\ntions in reachability as a special case. In practice, a reduction\nin functionality may occur if the precondition of at least one\naction in MA\ndrift has new pal-tuples, or the effect of at least\none of its actions has new pal-tuples that conflict with other\nactions required for reaching certain states.\nOur notion of reduced functionality captures all the vari-\nants of reduction in functionality. However, for clarity, we\nillustrate an example that focuses on situations where pre-\ncondition of an action has increased. Consider the case from\nTab. 1 where A’s model gets updated from MA\ninit to MA\ndrift.\n9872\nThe action sample rock’s applicability in MA\ndrift has reduced\nfrom that in MA\ninit as Acan no longer sample rocks in sit-\nuations where the battery is half charged but needs a fully\ncharged battery to be able to execute the action. In such sce-\nnarios, instead of relying on observation traces, our method\nidentifies traces containing indications of actions that were\naffected either in their precondition or effect, discovers ad-\nditional salient pal-tuples that were potentially affected, and\nadds them to the set of potentially affected pal-tuples Γδ.\nTo find pal-tuples corresponding to reduced functional-\nity of the agent, we place the agent in an optimal plan-\nning mode and assume limited availability of observation\ntraces O in the form of optimal unit-cost state-action tra-\njectories ⟨s0, a1, s1, a2, . . . , sn−1, an, sn⟩. We generate op-\ntimal plans using MA\ninit for all pairs of states in O. We hy-\npothesize that, if for a pair of states, the plan generated us-\ning MA\ninit is shorter than the plan observed in O, then some\nfunctionality of the agent has reduced.\nOur method performs comparative analysis of optimality\nof the observation traces against the optimal solutions gen-\nerated using MA\ninit for same pairs of initial and final states. To\nbegin with, we extract all the continuous state sub-sequences\nfrom O of the form ⟨s0, s1, . . . , sn⟩denoted by Odrift as they\nare all optimal. We then generate a set of planning problems\nPusing the initial and final states of trajectories in Odrift.\nThen, we provide the problems in Pto MA\ninit to get a set of\noptimal trajectories Oinit. We select all the pairs of optimal\ntrajectories of the form ⟨oinit, odrift⟩for further analysis such\nthat the length of oinit ∈Oinit for a problem is shorter than\nthe length of odrift ∈ Odrift for the same problem. For all\nsuch pairs of optimal trajectories, a subset of actions in each\noinit ∈Oinit were likely affected due to the model drift. We\nfocus on identifying the first action in each oinit ∈Oinit that\nwas definitely affected.\nTo identify the affected actions, we traverse each pair of\noptimal trajectories ⟨oinit, odrift⟩simultaneously starting from\nthe initial states. We add all the pal-tuples corresponding to\nthe first differing action in oinit to Γδ. We do this because\nthere are only two possible explanations for why the action\ndiffers: (i) either the action in oinit was applicable in a state\nusing MA\ninit but has become inapplicable in the same state\nin MA\ndrift, or (ii) it can no longer achieve the same effects in\nMA\ndrift as MA\ninit. We also discover the first actions that are ap-\nplicable in the same states in both the trajectories but result\nin different states. The effect of such actions has certainly\nchanged in MA\ndrift. We add all the pal-tuples corresponding\nto such actions to Γδ. In the next section, we describe our\napproach to infer the correct modes of pal-tuples in Γδ.\n4.2 Investigating Affected pal-tuples\nThis section explains how the correct modes ofpal-tuples in\nΓδ are inferred (line 2 onwards of Alg.1). Alg. 1 creates an\nabstract model in which all the pal-tuples that are predicted\nto have been affected are set to ?⃝(line 2). It then iterates\nover all pal-tuples with mode ?⃝(line 4).\nRemoving inconsistent models Our method generates\ncandidate abstract models and then removes the abstract\nmodels that are not consistent with the agent (lines 7-18 of\nAlg. 1). For each pal-tuple γ ∈Γ, the algorithm computes a\nset of possible abstract models Mabs by assigning the three\nmode variants+, −, and ∅to the currentpal-tuple γ in model\nMabs (line 6). Only one model in Mabs corresponds to the\nagent’s updated functionality.\nIf the action γa in the pal-tuple γ is present in the set\nof action triplets generated using O, then the pre-state of\nthat action spre is used to create a state sI (lines 9-10). sI is\ncreated by removing the literals corresponding to predicate\nγp from spre. We then create a queryQ=⟨sI, ⟨γa⟩⟩(line 10),\nand pose it to the agent A(line 11). The three models are\nthen sieved based on the comparison of their responses to the\nquery Q with that of A’s response θ to Q (line 12). We use\nthe same mechanism as AIA for sieving the abstract models.\nIf the action corresponding to the current pal-tuple γ be-\ning considered is not present in any of the observed action\ntriplets, then for every pair of abstract models in Mabs (line\n14), we generate a query Q using a planning problem (line\n15). We then pose the query Q to the agent (line 16) and\nreceive its response θ. We then sieve the abstract models\nby asking them the same query and discarding the models\nwhose responses are not consistent with that of the agent\n(line 17). The planning problem that is used to generate the\nquery and the method that checks for consistency of abstract\nmodels’ responses with that of the agent are used from AIA.\nFinally, all the models that are not consistent with the\nagent’s updated functionality are removed from the possi-\nble set of models Mabs. The remaining models are returned\nby the algorithm. Empirically, we find that only one model\nis always returned by the algorithm.\n4.3 Correctness\nWe now show that the learned drifted model representing\nthe agent’s updated functionality is consistent as defined in\nDef. 8 and Def. 9. The proof of the theorem is available in\nthe extended version of the paper (Nayyar, Verma, and Sri-\nvastava 2022).\nTheorem 1. Given a set of observation traces O generated\nby the drifted agent Adrift, a set of queries Q posed to Adrift\nby Alg. 1, and the modelMA\ninit representing the agent’s func-\ntionality prior to the drift, each of the models M = ⟨P, A⟩\nin MA\ndrift learned by Alg. 1 are consistent with respect to all\nthe observation traces o ∈O and query-responses ⟨q, θ⟩for\nall the queries q ∈Q.\nThere exists a finite set of observations that if collected\nwill allow Alg. 1 to achieve 100% correctness with any\namount of drift: this set corresponds to observations that al-\nlow line 1 of Alg. 1 to detect a change in the functionality.\nThis includes an action triplet in an observation trace hinting\nat increased functionality, or a shorter plan using the previ-\nously known model hinting at reduced functionality. Thus,\nmodels learned by DAAISy are guaranteed to be completely\ncorrect irrespective of the amount of the drift if such a finite\nset of observations is available. While using queries signifi-\ncantly reduces the number of observations required, asymp-\ntotic guarantees subsume those of passive model learners\nwhile ensuring convergence to the true model.\n9873\n5 Empirical Evaluation\nIn this section, we evaluate our approach for assessing a\nblack-box agent to learn its model using information about\nits previous model and available observations. We imple-\nmented the algorithm for DAAISy in Python 1 and tested it\non six planning benchmark domains from the International\nPlanning Competition (IPC) 2. We used the IPC domains as\nthe unknown drifted models and generated six initial do-\nmains at random for each domain in our experiments.\nTo assess the performance of our approach with increas-\ning drift, we employed two methods for generating the initial\ndomains: (a) dropping thepal-tuples already present, and (b)\nadding new pal-tuples. For each experiment, we used both\ntypes of domain generation. We generated different initial\nmodels by randomly changing modes of random pal-tuples\nin the IPC domains. Thus, in all our experiments an IPC do-\nmain plays the role of ground truth M∗\ndrift and a randomized\nmodel is used as MA\ninit.\nWe use a very small set of observation traces O (single\nobservation trace containing 10 action triplets) in all the ex-\nperiments for each domain. To generate this set, we gave\nthe agent a random problem instance from the IPC corre-\nsponding to the domain used by the agent. The agent then\nused Fast Downward (Helmert 2006) with LM-Cut heuris-\ntic (Helmert and Domshlak 2009) to produce an optimal\nsolution for the given problem. The generated observation\ntrace is provided to DAAISy as input in addition to a ran-\ndom MA\ninit as discussed in Alg. 1. The exact same observation\ntrace is used in all experiments of the same domain, without\nthe knowledge of the drifted model of the agent, and irre-\nspective of the amount of drift.\nWe measure the final accuracy of the learned model\nMA\ndrift against the ground truth model M∗\ndrift using the mea-\nsure of model difference ∆(MA\ndrift, M∗\ndrift). We also mea-\nsure the number of queries required to learn a model with\nsignificantly high accuracy. We compare the efficiency of\nDAAISy (our approach) with the Agent Interrogation Algo-\nrithm (AIA) (Verma, Marpally, and Srivastava 2021) as it is\nthe most closely related querying-based system.\nAll of our experiments were executed on 5.0 GHz Intel\ni9 CPUs with 64 GB RAM running Ubuntu 18.04. We now\ndiscuss our results in detail below.\n5.1 Results\nWe evaluated the performance of DAAISy along 2 direc-\ntions; the number of queries it takes to learn the updated\nmodel MA\ndrift with increasing amount of drift, and the cor-\nrectness of the model MA\ndrift it learns compared to M∗\ndrift.\nEfficiency in number of queries As seen in Fig. 2, the\ncomputational cost of assessing each agent, measured in\nterms of the number of queries used by DAAISy, increases\nas the amount of drift in the model M∗\ndrift increases. This is\nexpected as the amount of drift is directly proportional to the\n1Code available at https://github.com/AAIR-lab/DAAISy\n2https://www.icaps-conference.org/competitions\nAccuracy gained by DAAISy Number of queries by DAAISy\nAccuracy of initial model\n Accuracy of model computed by DAAISy\nModel Accuracy\nNumber of Queries\n% drift\nFigure 2: The number of queries used by DAAISy (our ap-\nproach) and AIA (marked\non y-axis), as well as accuracy\nof model computed by DAAISy with increasing amount of\ndrift. Amount of drift equals the ratio of drifted pal-tuples\nand the total number of pal-tuples in the domains (nPals).\nThe number of action triplets in the observation trace used\nfor each domain is 10.\nnumber of pal-tuples affected in the domain. This increases\nthe number of pal-tuples that DAAISy identifies as affected\nas well as the number of queries as a result. As demonstrated\nin the plots, the standard deviation for number of queries re-\nmains low even when we increase the amount of drift, show-\ning the stability of DAAISy.\nComparison with AIA Tab. 3 shows the average number\nof queries that AIA took to achieve the same level of accu-\nracy as our approach for 50% drifted models, and DAAISy\nrequires significantly fewer queries to reach the same levels\nof accuracy compared to AIA. Fig. 2 also demonstrates that\nDAAISy always takes fewer queries as compared to AIA to\nreach reasonably high levels of accuracy.\nThis is because AIA does not use information about the\npreviously known model of the agent and thus ends up\nquerying for all possible pal-tuples. DAAISy, on the other\nhand, predicts the set of pal-tuples that might have changed\nbased on the observations collected from the agent and thus\nrequires significantly fewer queries.\nCorrectness of learned model DAAISy computes mod-\nels with at least 50% accuracy in all six domains even when\nthey have completely drifted from their initial model, i.e.,\n∆(MA\ndrift, M∗\ndrift) = nPals. It attains nearly accurate models\nfor Gripper and Blocksworld for upto 40% drift. Even in\nscenarios where the agent’s model drift is more than 50%,\nDAAISy achieves at least 70% accuracy in five domains.\nNote that DAAISy is guaranteed to find the correct mode\nfor an identified affected pal-tuple. The reason for less than\n100% accuracy when using DAAISy is that it does not pre-\ndict a pal-tuple to be affected unless it encounters an obser-\n9874\nDomain #Pals AIA DAAISy\nGripper 20 15.0 6.5\nMiconic 36 32.0 7.7\nSatellite 50 34.0 9.0\nBlocksworld 52 40.0 11.4\nTermes 134 115.0 27.0\nRovers 402 316.0 61.0\nTable 3: The average number of queries taken by AIA to\nachieve the same level of accuracy as DAAISy (our ap-\nproach) for 50% drifted models.\nvation trace conflicting with MA\ninit. Thus, the learned model\nMA\ndrift, even though consistent with all the observation traces,\nmay end up being inaccurate when compared to M∗\ndrift.\nDiscussion AIA always ends up learning completely accu-\nrate models, but as noted above, this is because AIA queries\nexhaustively for all the pal-tuples in the model. There is a\nclear trade-off between the number of queries that DAAISy\ntakes to learn the model as compared to AIA and the correct-\nness of the learned model. As evident from the results, if the\nmodel has not drifted much, DAAISy can serve as a better\napproach to efficiently learn the updated functionality of the\nagent with less overhead as compared to AIA. Deciding the\namount of drift after which it would make sense to switch\nto querying the model from scratch is a useful analysis not\naddressed in this paper.\n6 Related Work\nWhite-box model drift Bryce, Benton, and Boldt (2016)\naddress the problem of learning the updated mental model\nof a user using particle filtering given prior knowledge about\nthe user’s mental model. However, they assume that the en-\ntity being modeled can tell the learning system about flaws\nin the learned model if needed. Eiter et al. (2005, 2010) pro-\npose a framework for updating action laws depicted in the\nform of graphs representing the state space. They assume\nthat changes can only happen in effects, and that knowledge\nabout the state space and what effects might change is avail-\nable beforehand. Our work does not make such assumptions\nto learn the correct model of the agent’s functionalities.\nAction model learning The problem of learning agent\nmodels from observations of its behavior is an active area\nof research (Gil 1994; Yang, Wu, and Jiang 2007; Cress-\nwell, McCluskey, and West 2009; Zhuo and Kambhampati\n2013; Arora et al. 2018; Aineto, Celorrio, and Onaindia\n2019). Recent work addresses active querying to learn the\naction model of an agent (Rodrigues et al. 2011; Verma,\nMarpally, and Srivastava 2021). However, these methods do\nnot address the problem of reducing the computational cost\nof differential model assessment, which is crucial in non-\nstationary settings.\nOnline action model learning approaches learn the model\nof an agent while incorporating new observations of the\nagent behavior ( ˇCertick´y 2014; Lamanna et al. 2021a,b).\nUnlike our approach, they do not handle cases where (i) the\nnew observations are not consistent with the older ones due\nto changes in the agent’s behavior; and/or (ii) there is reduc-\ntion in functionality of the agent. Lindsay (2021) solve the\nproblem of learning all static predicates in a domain. They\nstart with a correct partial model that captures the dynamic\npart of the model accurately and generate negative examples\nby assuming access to all possible positive examples. Our\nmethod is different in that it does not make such assump-\ntions and leverages a small set of available observations to\ninfer about increased and reduced functionality of an agent’s\nmodel.\nModel reconciliation Model reconciliation litera-\nture (Chakraborti et al. 2017; Sreedharan et al. 2019;\nSreedharan, Chakraborti, and Kambhampati 2021) deals\nwith inferring the differences between the user and the\nagent models and removing them using explanations. These\nmethods consider white-box known models whereas our\napproach works with black-box models of the agent.\n7 Conclusions and Future Work\nWe presented a novel method for differential assessment of\nblack-box AI systems to learn models of true functional-\nity of agents that have drifted from their previously known\nfunctionality. Our approach provides guarantees of correct-\nness w.r.t. observations. Our evaluation demonstrates that\nour system, DAAISy, efficiently learns a highly accurate\nmodel of agent’s functionality issuing a significantly lower\nnumber of queries as opposed to relearning from scratch. In\nthe future, we plan to extend the framework to more general\nclasses, stochastic settings, and models. Analyzing and pre-\ndicting switching points from selective querying in DAAISy\nto relearning from scratch without compromising the cor-\nrectness of the learned models is also a promising direction\nfor future work.\nAcknowledgements\nWe thank anonymous reviewers for their helpful feedback\non the paper. This work was supported in part by the NSF\nunder grants IIS 1942856, IIS 1909370, and the ONR grant\nN00014-21-1-2045.\nReferences\nAineto, D.; Celorrio, S. J.; and Onaindia, E. 2019. Learn-\ning Action Models With Minimal Observability. Artificial\nIntelligence, 275: 104–137.\nArora, A.; Fiorino, H.; Pellier, D.; M ´etivier, M.; and Pesty,\nS. 2018. A Review of Learning Planning Action Models.\nThe Knowledge Engineering Review, 33: E20.\nB¨ackstr¨om, C.; and Jonsson, P. 2013. Bridging the Gap Be-\ntween Refinement and Heuristics in Abstraction. In Proc.\nIJCAI.\nBryce, D.; Benton, J.; and Boldt, M. W. 2016. Maintaining\nEvolving Domain Models. In Proc. IJCAI.\nˇCertick´y, M. 2014. Real-Time Action Model Learning\nwith Online Algorithm 3SG. Applied Artificial Intelligence,\n28(7): 690–711.\n9875\nChakraborti, T.; Sreedharan, S.; Zhang, Y .; and Kambham-\npati, S. 2017. Plan Explanations as Model Reconciliation:\nMoving Beyond Explanation as Soliloquy. In Proc. IJCAI.\nCresswell, S.; McCluskey, T.; and West, M. 2009. Acquisi-\ntion of Object-Centred Domain Models from Planning Ex-\namples. In Proc. ICAPS.\nEiter, T.; Erdem, E.; Fink, M.; and Senko, J. 2005. Updating\nAction Domain Descriptions. In Proc. IJCAI.\nEiter, T.; Erdem, E.; Fink, M.; and Senko, J. 2010. Up-\ndating Action Domain Descriptions. Artificial Intelligence,\n174(15): 1172–1221.\nFikes, R. E.; and Nilsson, N. J. 1971. STRIPS: A New Ap-\nproach to the Application of Theorem Proving to Problem\nSolving. Artificial Intelligence, 2(3-4): 189–208.\nFox, M.; and Long, D. 2003. PDDL2.1: An Extension to\nPDDL for Expressing Temporal Planning Domains.Journal\nof Artificial Intelligence Research, 20(1): 61–124.\nGil, Y . 1994. Learning by Experimentation: Incremental Re-\nfinement of Incomplete Planning Domains. In Proc. ICML.\nGiunchiglia, F.; and Walsh, T. 1992. A Theory of Abstrac-\ntion. Artificial Intelligence, 57(2-3): 323–389.\nHelmert, M. 2006. The Fast Downward Planning System.\nJournal of Artificial Intelligence Research, 26: 191–246.\nHelmert, M.; and Domshlak, C. 2009. Landmarks, Critical\nPaths and Abstractions: What’s the Difference Anyway? In\nProc. ICAPS.\nHelmert, M.; Haslum, P.; and Hoffmann, J. 2007. Flexible\nAbstraction Heuristics for Optimal Sequential Planning. In\nProc. ICAPS.\nLamanna, L.; Gerevini, A. E.; Saetti, A.; Serafini, L.; and\nTraverso, P. 2021a. On-line Learning of Planning Domains\nfrom Sensor Data in PAL: Scaling up to Large State Spaces.\nIn Proc. AAAI.\nLamanna, L.; Saetti, A.; Serafini, L.; Gerevini, A.; and\nTraverso, P. 2021b. Online Learning of Action Models for\nPDDL Planning. In Proc. IJCAI.\nLindsay, A. 2021. Reuniting the LOCM Family: An Alterna-\ntive Method for Identifying Static Relationships. In ICAPS\n2021 KEPS Workshop.\nLong, D.; and Fox, M. 2003. The 3rd International Planning\nCompetition: Results and Analysis. Journal of Artificial In-\ntelligence Research, 20: 1–59.\nMcDermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram,\nA.; Veloso, M.; Weld, D. S.; and Wilkins, D. 1998. PDDL –\nThe Planning Domain Definition Language. Technical Re-\nport CVC TR-98-003/DCS TR-1165, Yale Center for Com-\nputational Vision and Control.\nNayyar, R. K.; Verma, P.; and Srivastava, S. 2022. Differ-\nential Assessment of Black-Box AI Agents. arXiv preprint\narXiv: 2203.13236.\nRodrigues, C.; G ´erard, P.; Rouveirol, C.; and Soldano, H.\n2011. Active Learning of Relational Action Models. In\nProc. ILP.\nSacerdoti, E. D. 1974. Planning in a Hierarchy of Abstrac-\ntion Spaces. Artificial Intelligence, 5(2): 115–135.\nSreedharan, S.; Chakraborti, T.; and Kambhampati, S. 2021.\nFoundations of Explanations as Model Reconciliation. Arti-\nficial Intelligence, 103558.\nSreedharan, S.; Hernandez, A. O.; Mishra, A. P.; and Kamb-\nhampati, S. 2019. Model-Free Model Reconciliation. In\nProc. IJCAI.\nSrivastava, S.; Russell, S.; and Pinto, A. 2016. Metaphysics\nof Planning Domain Descriptions. In Proc. AAAI.\nStern, R.; and Juba, B. 2017. Efficient, Safe, and Probably\nApproximately Complete Learning of Action Models. In\nProc. IJCAI.\nVerma, P.; Marpally, S. R.; and Srivastava, S. 2021. Asking\nthe Right Questions: Learning Interpretable Action Models\nThrough Query Answering. In Proc. AAAI.\nYang, Q.; Wu, K.; and Jiang, Y . 2007. Learning Action Mod-\nels from Plan Examples Using Weighted MAX-SAT. Artifi-\ncial Intelligence, 171(2-3): 107–143.\nZhuo, H. H.; and Kambhampati, S. 2013. Action-Model Ac-\nquisition from Noisy Plan Traces. In Proc. IJCAI.\n9876"
}