{
  "title": "Do Neural Language Models Overcome Reporting Bias?",
  "url": "https://openalex.org/W3116216579",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2250686552",
      "name": "Vered Shwartz",
      "affiliations": [
        "Art Institute of Portland"
      ]
    },
    {
      "id": "https://openalex.org/A2133417374",
      "name": "Yejin Choi",
      "affiliations": [
        "Art Institute of Portland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2107372159",
    "https://openalex.org/W2107658650",
    "https://openalex.org/W2475652754",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2978491132",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3034912286",
    "https://openalex.org/W95183648",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2251885125",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W2050482109",
    "https://openalex.org/W2099472831",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W81242816",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2570058081",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W3105066976",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2938704169"
  ],
  "abstract": "Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6863–6870\nBarcelona, Spain (Online), December 8-13, 2020\n6863\nDo Neural Language Models Overcome Reporting Bias?\nVered Shwartz and Yejin Choi\nAllen Institute for AI\nPaul G. Allen School of Computer Science & Engineering, University of Washington\n{vereds,yejinc}@allenai.org\nAbstract\nMining commonsense knowledge from corpora suffers from reporting bias, over-representing\nthe rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent\npre-trained language models overcome this issue. We ﬁnd that while their generalization capacity\nallows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and\nproperties, they also tend to overestimate that of the very rare, amplifying the bias that already\nexists in their training corpus.\n1 Introduction\nApart from several notable efforts to collect commonsense knowledge from experts (Lenat, 1995) or\nthrough crowdsourcing (Speer and Havasi, 2012; Sap et al., 2019), most work has been on extracting\nsuch knowledge from large text corpora (Mitchell et al., 2018). While the latter approach is scalable\nand low cost, it also suffers from reporting bias: due to Grice’s conversational maxim of quantity (Grice\net al., 1975), people rarely state the obvious, thus many trivial facts (“people breathe”) are rarely men-\ntioned in text, while uncommon events (“people murder”) are reported disproportionately (Gordon and\nVan Durme, 2013; Sorower et al., 2011).\nTraditionally, knowledge acquisition from text was extractive. In recent years, the generalization\ncapacity of neural language models (LMs) and their ability to aggregate knowledge across contexts\nhave facilitated estimating the plausibility of facts, even when they don’t appear in the corpus explicitly.\nRecent pre-trained LMs such as GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), trained on\nmassive texts, dominate the NLP leaderboards, and are considered a source of commonsense knowledge\n(Petroni et al., 2019). Does this mean that pre-trained LMs overcome reporting bias?\nIn this paper we revisit the experiments conducted by Gordon and Van Durme (2013) (henceforth\nG&V), applying them to various pre-trained LMs (based on the nature of the experiment, we test either\nmasked LMs or standard left-to-right LMs). We ﬁnd that LMs, compared to extractive methods:1\n1. Provide a worse estimate of action frequency, mostly due to overestimating very rare actions.\n2. Predict both expected outcomes as well as sensational and unlikely outcomes.\n3. Are capable of learning associations between concepts and their properties indirectly, but tend to\nover-generalization, which leads to confusing semantically-similar but mutually exclusive values.\n2 Actions and Events\nG&V demonstrate the discrepancy between corpus occurrences and actual action frequency by showing\nthat if you believe the corpus, people murder more than they breathe. Breathing is an activity we take\nfor granted and thus rarely talk about (Grice et al., 1975). That murder is frequent in the corpus is a\nreﬂection of the same issue: we talk more about uncommon or newsworthy events (van Dalen, 2012).\nWe follow G&V’s qualitative analysis of actions and events performed by or which happen to people\nby comparing real-world frequency to corpus-based and LM-based frequency. We estimate real-world\n1Our data and code are publicly available at https://github.com/vered1986/reporting_bias_lms.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLicense details: http://creativecommons.org/licenses/by/4.0/.\n6864\nFigure 1: Frequency of actions performed or occurring to people during their lifetime from very frequent\n(daily), through once in a lifetime events, to very rare (don’t happen to most people). Note that actual\nfrequencies of rare events are too small to show. See Appendix A for the exact frequencies.\nBERT RoBERTa GPT-2 BERT RoBERTa GPT-2\nThe person .\nwins (11.4) said (5.8) let (4.3)\nThe person is .\nkilled (7.5) gone (6.3) let (4.3)\ndied (11.4) responds (4.0) see (3.9) married (6.6) deceased (3.8) see (3.9)\ndies (10.6) replied (3.4) make (2.4) dying (4.2) arrested (2.9) make (2.4)\nwon (7.8) dies (3.3) get (2.1) deceased (3.8) missing (2.5) get (2.1)\nlost (3.5) died (2.9) look (2.1) eliminated (2.6) responding (1.9) look (2.1)\nsaid (2.4) responded (2.5) take (1.2) retired (2.2) involved (1.9) take (1.2)\nspeaks (1.9) says (2.4) set (1.2) lost (2.0) reading (1.9) set (1.2)\nanswered (1.6) replies (2.2) give (1.1) arrested (2.0) dying (1.9) give (1.1)\nreplied (1.3) asked (2.1) using (1.1) elected (1.5) confused (1.5) using (1.1)\nloses (1.3) commented (2.1) go (1.1) disabled (1.5) reporting (1.5) go (1.1)\nTable 1: Top LM predictions for actions performed by people along with their scores (percents).\nfrequency (e.g. how many times does a person breathe in their lifetime?) from published statistics based\non US data, as detailed in Appendix A. Corpus frequency is computed using the Google N-gram corpus\n(Brants and Franz, 2006). Speciﬁcally, we compute the normalized frequency of the verbs appearing in\nthe 3-gram “person is <verb>”, falling back to the bigram “person<verb>” if no results are found. We\nuse SpaCy to determine parts of speech, keeping non auxiliary verbs (Honnibal and Montani, 2017).\nWhile LM scores don’t represent frequency or probability, they are often used in practice as a proxy\nfor plausibility. Thus, we would expect LM scores to correlate with real-world frequency. We query\nmasked LMs for substitutes of the mask in several templates describing actions, 2 and left-to-right LMs\nby greedily decoding the next token (e.g. for “The person is”), taking the maximum score for each word\nacross templates. Speciﬁcally, we use BERT large uncased (Devlin et al., 2019), RoBERTa large (Liu et\nal., 2019), and GPT-2 XL (Radford et al., 2019) from the Transformers package (Wolf et al., 2019). We\nkeep the non auxiliary verbs among the top 5000 predictions.3\nFigure 1 visualizes the relative frequency of each action as estimated by the various sources, where the\nscores for all actions are normalized for each source. Actions are sorted by their real-world frequency\nfrom very frequent to very rare. First, we observe that LMs assign non-zero scores for all actions,\nas opposed to the non-smoothed corpus frequencies from Google Ngrams. However, the scores they\nproduce diverge further from the actual distribution, measuring with KL-divergence: Google Ngrams -\n2.94, BERT and GPT-2 - 3.77, and RoBERTa - 3.08. LMs produce a more accurate estimate for some\nfrequent actions (blinking, eating) but worse for others (thinking, breathing). At the same time, LMs\nalso exaggerate the frequencies of rare events (e.g. dying), producing estimates not only higher than the\nactual frequency but even higher than the corpus frequency.\nThe same patterns emerge for both LMs, but some exceptions stand out. For example, BERT over-\nestimates the frequency of dying, which may be due to being trained on Wikipedia, which consists of\nmany entries describing historically important—and dead—people. RoBERTa, on the other hand, which\n2“The person is [MASK].”, “The person [MASK].”, “People are [MASK].”, “All people [MASK].”\n3We consider some synonyms and subactions, e.g. including “exhale” and “inhale” in “breathe”, as detailed in Appendix A.\n6865\nLM\nThe girl found a bug in her cereal.Context Question Effect\nAs a result, she lost her appetite.\n…\nSo she lost her appetite.\nAs a result, she poured milk in the bowl.\n…\nSo she poured milk in the bowl.\nYet, somehow she did not lose her appetite.\n…\nBut surprisingly she did not lose her appetite.\nYet, somehow she did not pour milk in the bowl.\n…\nBut surprisingly she did not pour milk in the bowl.\nShe lost her appetite. She poured milk in the bowl.Option #1 Option #2\nCausal Discourse Markers\nDisconﬁrmed Expectations As a result, she lost her appetite.\nMost plausible statement\nPrediction: Option #1\n[cause]. (So|As a result,|As one would expect,) [effect].\n[cause] (yet|but|however), (surprisingly|for some reason|somehow)? [negated effect].\nFigure 2: Illustration of the Zero-shot+DE model prediction of an instance from COPA. Each answer\nchoice has a set of support statements including causal discourse markers (both models) and disconﬁrmed\nexpectations (only in Zero-shot+DE). The LM is used to score the statements for plausibility, and the\nmodel predicts the answer choice associated with the most plausible statement.\nwas trained on the web, overestimates the frequency of newsworthy events such as being murdered or\narrested. Table 1 further exempliﬁes the top LM predictions for actions performed by people, using ad-\nditional templates. While most predictions, especially by GPT-2, are common or mundane verbs ( said),\nsome describe rarer events (killed).\n3 Event Outcomes\nG&V argue that an event outcome is more likely to be mentioned in text if it’s not certain. For instance,\n“The man turned on the faucet. The water started running in a steady stream.” makes an awfully boring\nstory, while “Water gushed out of the sink” builds up to a turn in events. Do LMs learn the proportional\noutcome distribution in the corpus, or can they overcome it by implicitly learned commonsense?\nA good testbed for event outcomes is the COPA dataset (Choice of Plausible Alternatives) (Gordon\net al., 2012). Given an event (context), the goal is to predict its cause or effect among two candidate\nanswers. We focus on LMs typically used for generation: GPT (Radford et al., 2018), GPT-2 (Radford\net al., 2019), and XLNet (Yang et al., 2019). Table 2 exempliﬁes outcomes predicted for several COPA\nevents with various LMs and decoding strategies: top k = 10(Fan et al., 2018), top p = 0.9 (Holtzman\net al., 2020), and beam search with beam size of 5. We observe a combination of mundane, correct\noutcomes (water running in a steady stream) and sensational and unlikely events (“the ﬁre broke out”).\nEvent LM Decoding Outcome\nThe man turned\non the faucet.\nGPT\nTop-k 10 the water started running in a steady stream\nTop-p 0.9 water began to ﬂow out of the faucet and onto\nBeam 5 the water began to boil\nGPT-2 S\nTop-k 10 his face became pale\nTop-p 0.9 the ﬁre broke out\nBeam 5 he was able to get out of the car and\nGPT-2 XL\nTop-k 10 the man’s blood was sprayed everywhere, and his\nTop-p 0.9 the water in the bathtub began to ﬂow\nBeam 5 water gushed out of the sink\nThe man received\na parking ticket.\nGPT\nTop-k 10 the next day they were forced to drive around town\nTop-p 0.9 he was a bit confused about the situation, but\nBeam 5 he had to pay for the ticket\nGPT-2 S\nTop-k 10 he has to pay the fare on the spot\nTop-p 0.9 he left his job\nBeam 5 he was arrested\nGPT-2 XL\nTop-k 10 the ticket was sent to the city’s Department of\nTop-p 0.9 he went to his car and pulled out a\nBeam 5 he was arrested and charged with violating the city’s\nTable 2: Example outcomes generated for COPA events (condi-\ntioned on “[context]. As a result,”).\nIn order to quantify the ability\nof LMs to predict outcomes, we\ntarget the multiple choice COPA\ntask with a zero-shot LM-based\nmodel (Zero-shot in Table 3).\nFor a given context and for each\ncandidate answer, we create a set\nof supporting statements: [cause]\n[causal discourse marker]\n[effect], as exempliﬁed in Fig-\nure 2. For questions asking about\nthe cause of an event, we set the\ncause to the context and the effect\nto the candidate answer, while for\nquestions asking about the effect,\nwe reverse the direction.\nFollowing Shwartz et al. (2020b), we compute the cross entropy loss of each statement, and predict the\ncandidate answer associated with the statement with the lowest loss (most plausible statement). Figure 2\n6866\n(a) Accuracy score and the average rank\nof gold color.\nLM Pre-trained Fine-tuned\nAcc. Rank Acc. Rank\nMajority 35.8\nBERT 51.9 40.6 69.2 1.7\nBERT-L 56.4 40.14 70.1 1.68\nRoBERTa 49.0 63.4 67.8 1.74\nRoBERTa-L 55.4 49.7 68.7 1.71\n(b) Example sentences along with top 3 color predictions for each of the pre-trained\nmodels and the ﬁne-tuned models (+FT). We note that the predictions are sensitive to\nphrasing.\nSentence Majority BERT-L RoBERTa-L BERT-L+FT RoBERTa-L+FT\nThe banana is tasty. g o y b r w b r be g b r g y r\nThe apple is sweet. g r o b w g r b be g r b g r y\nThe cat is cute. b w be be b w b gy r p w b b w o\nThe dove is beautiful. w bn r b be w r w b w be bn w b be\nThe cow eats grass. r w b bn b be r b be b r b bn r b\nThe dog runs in the park. b w y b bn w w b be b be y b y w\nTable 4: Performance and example predictions for the color prediction experiment. For each of BERT\nand RoBERTa, we report the performance of the pre-trained only model and the model ﬁne-tuned on\nthe color train set. The majority baseline predicts the most common color associated with the following\nnoun in the train set, e.g. majority(banana) =green.\nillustrates the model’s prediction for a given COPA instance.\nTable 3 shows the accuracy on the development set across different LMs. The GPT models slightly\nimprove upon the majority baseline.\n3.1 Disconﬁrmed Expectations\nTable 3: Accuracy on the COPA\ndevelopment set.\nLM Zero-shot Zero-shot+DE\nMajority 0.55 0.55\nGPT 0.59 0.56\nGPT2-S 0.58 0.59\nGPT2-XL 0.61 0.60\nXLNet-S 0.55 0.49\nXLNet-L 0.43 0.42\nG&V suggest that a better source for typical outcomes is textual\nconstructions that indicate a speaker’s expectation about the world\nwas not met. For example, “Sally crashed her car into a tree but\nwasn’t hurt” indicates that if a person crashed their car, they are\nlikely to be hurt. An initial exploration of this approach was done\nby Gordon and Schubert (2011), but they concluded that extracting\nthis type of rules from corpora is limited due to the sparseness of the\nclauses and the discourse patterns.\nWe conjecture that neural LMs may overcome the sparseness is-\nsue and be used for both scoring and generating typical outcomes.\nWe therefore extend the zero-shot model by addingdisconﬁrmed expectations(Zero-shot+DE) to the sup-\nporting statements: [cause] [negative discourse marker] ([surprise expression]) [negated\neffect]. We recognize the main verb of the effect using SpaCy and negate it to create the negated effect\nstatement.\nThe results in Table 3 show that adding disconﬁrmed expectations usually degrades the performance.\nWe observed that this often happens when a statement of the form “[context] [negative discourse\nmarker] [negated wrong answer]” is incorrectly ranked as plausible, as in “He ran out of onions. Yet,\nfor some reason the cook’s eyes did not water”. While the LM recognizes the lexical relatedness between\nonions and watering eyes, it is not sensitive to negation, as was recently shown for several other language\nmodels (Ettinger, 2020; Kassner and Schütze, 2020).\n4 Properties\nAccording to G&V, people are more likely to state unusual properties of a concept ( blue pencil) than\nusual ones (yellow pencil). Recently, Weir et al. (2020) studied LMs’ ability to associate concepts with\ntheir properties, by providing the LM the concept and predicting the properties and vice versa. Overall,\nLMs performed reasonably well, with RoBERTa outperforming BERT. Both performed better on ency-\nclopedic and functional properties (“A bear is an animal”) than on perceptual properties, which are less\noften mentioned in text (Collell Talleda and Moens, 2016; Forbes et al., 2019).\nWe hypothesize that while LMs are to some extent capable of learning association between con-\ncepts and their properties indirectly by aggregating across contexts, during this process, they often over-\ngeneralize, predicting semantically-similar but mutually exclusive values. We verify that by evaluating\n6867\nBERT and RoBERTa’s ability to predict colors. We constructed a list of 11 common colors and extracted\nall sentences in Wikipedia in which a color modiﬁes a noun, masking the color tokens (e.g. “A bear is\n[MASK]”). We then split the data into train (1,169,590 sentences) and test (10,000 sentences).\nTable 4a presents the results of pre-trained-only LMs vs. LMs ﬁne-tuned on the train set, with a\nmasked LM objective, to predict the color. First, we note that the pre-trained BERT models outperform\nthe RoBERTa model, which is expected given that BERT was already exposed to the sentences in the\ndataset during pre-training on Wikipedia. Despite that, the ﬁne-tuned models still exhibit a dramatic\nboost in performance, both in terms of accuracy and average rank of the correct color. This is an encour-\naging result: it’s possible to correct the over-generalization by further exposing the LM to the “truth”.\nWith that said, this corpus-based “truth” is not a ground truth, and given that the sentences were not\nmanually veriﬁed, it is still biased towards the unusual, containing strange concepts like “blue cat”.\n5 Related Work\nCommonsense in pre-trained LMs. There is ongoing research on extracting commonsense knowl-\nedge from pre-trained LMs, providing mixed results. On the one hand, Petroni et al. (2019) and Davison\net al. (2019) somewhat successfully used pre-trained LMs to complete commonsense KBs. On the other\nhand, Logan et al. (2019) have shown that LMs are limited in their ability to generate accurate factual\nknowledge, and Kassner and Schütze (2020) and Ettinger (2020) pointed out that LMs are not sensitive\nto negation, resulting in generating incorrect facts (“birds can’t ﬂy”). Finally, Shwartz et al. (2020b)\nshowed that despite being noisy, knowledge generated by LMs can be used to improve performance on\ncommonsense tasks.\nSimilarly to our color experiment, Bouraoui et al. (2020) developed a LM-based relation classiﬁcation\nmodel that included a color relationship. The model starts with a seed of known word pairs for a given\nrelationship, uses it to ﬁnd template sentences indicative of the relationship, and ﬁne-tunes BERT on\nthese retrieved sentences. Their experiment had a different purpose from ours, in which we probed the\nLMs for knowledge already captured by their pre-training phase.\nLearning from other modalities. Much of our world knowledge is innate or acquired through modal-\nities such as vision, including physical commonsense (“physical objects can’t be in different places at\nthe same time”) and social commonsense (“people do and say things for reasons”). There has been little\nwork on learning meaning from other modalities (Kiela and Clark, 2015; Zellers et al., 2019), but there\nis a shared understanding in the community that this is the imperative next step (Bisk et al., 2020; Bender\nand Koller, 2020).\n6 Conclusion\nWe show that pre-trained LMs to some extent overcome reporting bias in the sense that they possess\nknowledge that wasn’t explicitly stated, including trivial facts. Unfortunately, they also over-represent\nrare and newsworthy events, amplifying the bias that already exists in their training corpus.\nThe results in this paper are in line with prior work that showed that LMs amplify social bias (May\net al., 2019; Sheng et al., 2019) and knowledge about named entities that are prominent in the corpus\n(Shwartz et al., 2020a). Going forward, it is important to study how the choice of training corpus, model\nsize, and other factors affect the type and extent of biases the LM would have.\nAcknowledgements\nThis research was supported in part by NSF (IIS-1524371, IIS-1714566), DARPA under the CwC pro-\ngram through the ARO (W911NF-15-1-0543), and DARPA under the MCS program through NIWC\nPaciﬁc (N66001-19-2-4031).\n6868\nReferences\nEmily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in\nthe age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,\npages 5185–5198, Online, July. Association for Computational Linguistics.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, An-\ngeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience\ngrounds language. In EMNLP.\nZied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from\nBERT. In AAAI.\nThorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1.\nGuillem Collell Talleda and Marie-Francine Moens. 2016. Is an image worth more than a thousand words? on\nthe ﬁne-grain semantic differences between visual and linguistic representations. In Proceedings of the 26th\nInternational Conference on Computational Linguistics, pages 2807–2817. ACL.\nJoe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained\nmodels. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173–1178,\nHong Kong, China, November. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Min-\nnesota, June. Association for Computational Linguistics.\nAllyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language\nmodels. Transactions of the Association for Computational Linguistics, 8(0):34–48.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n889–898.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019. Do neural language representations learn physical com-\nmonsense? In CogSci.\nJonathan Gordon and Lenhart K Schubert. 2011. Discovering commonsense entailment rules implicit in sentences.\nEMNLP 2011, page 59.\nJonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of\nthe 2013 workshop on Automated knowledge base construction, pages 25–30. ACM.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on\nLexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task,\nand Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages\n394–398, Montréal, Canada, 7-8 June. Association for Computational Linguistics.\nH Paul Grice, Peter Cole, Jerry Morgan, et al. 1975. Logic and conversation. 1975, pages 41–58.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degener-\nation. In International Conference on Learning Representations.\nMatthew Honnibal and Ines Montani. 2017. spacy 2: Natural language understanding with bloom embeddings,\nconvolutional neural networks and incremental parsing. To appear, 7(1).\nNora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds\ncan talk, but cannot ﬂy. Association for Computational Linguistics.\nDouwe Kiela and Stephen Clark. 2015. Multi- and cross-modal semantics beyond vision: Grounding in auditory\nperception. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing ,\npages 2461–2470, Lisbon, Portugal, September. Association for Computational Linguistics.\nDouglas B Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM,\n38(11):33–38.\n6869\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack’s Wife Hillary:\nUsing Knowledge Graphs for Fact-Aware Language Modeling. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, Florence, Italy. Association for Computational Linguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 622–628, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nTom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Betteridge, Andrew Carl-\nson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. 2018. Never-ending learning. Communications of the\nACM, 61(5):103–115.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\nMiller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nby generative pre-training. -.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. -.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan\nRoof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if-then reasoning.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3027–3035.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysit-\nter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3398–3403, Hong Kong, China, November. Association for Computational Linguistics.\nVered Shwartz, Rachel Rudinger, and Oyvind Tafjord. 2020a. \"you are grounded!\": Latent name artifacts in\npre-trained language models. In EMNLP.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020b. Unsupervised com-\nmonsense question answering with self-talk. In EMNLP.\nMohammad S Sorower, Janardhan R Doppa, Walker Orr, Prasad Tadepalli, Thomas G Dietterich, and Xiaoli Z\nFern. 2011. Inverting grice’s maxims to learn rules from natural language extractions. In Advances in neural\ninformation processing systems, pages 1053–1061.\nRobyn Speer and Catherine Havasi. 2012. Representing general relational knowledge in conceptnet 5. In LREC,\npages 3679–3686.\nArjen van Dalen. 2012. Structural bias in cross-national perspective: How political systems and journalism\ncultures inﬂuence government dominance in the news. The International Journal of Press/Politics, 17(1):32–\n55.\nNathaniel Weir, Adam Poliak, and Benjamin Van Durme. 2020. Probing neural language models for human tacit\nassumptions.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, pages arXiv–1910.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. CoRR, abs/1906.08237.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual common-\nsense reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n6720–6731.\n6870\nA Action and Events\nNormalized Frequency\nAction Actual Frequency for Lifetime (Source) Actual Corpus BERT RoBERTa GPT-2\nthinking 1,433,355,000 (50,000 per day) 5.26e-01 9.21e-02 1.74e-01 8.66e-03 5.74e-03\nbreathing 660,489,984 (23,040 per day) 2.42e-01 3.51e-03 2.04e-02 8.11e-03 2.89e-04\nblinking 344,005,200 (12,000 per day) 1.26e-01 6.84e-04 1.63e-03 0 0\neating 86001.3: 3 times per day 3.16e-05 1.23e-02 2.64e-02 1.09e-02 1.45e-03\nsleeping 28667.1: 1 time per day 1.05e-05 1.03e-02 1.19e-02 2.65e-02 6.33e-04\nworking 20420.4: 5 times a week 7.49e-06 5.66e-02 5.81e-02 7.59e-02 4.22e-03\nexercising 8168.16: 2-3 times a week 3.00e-06 2.44e-02 0.00e+00 1.17e-03 2.14e-04\ngetting married 1.66: 0-3 times per life 6.09e-10 4.76e-03 5.37e-02 2.26e-01 6.40e-04\ngetting divorced 1: 0-2 times per life 4.04e-10 8.95e-04 6.91e-03 1.49e-02 3.72e-05\nbeing born 1 4.04e-10 7.35e-02 7.76e-02 1.75e-02 4.55e-03\nbeing named 1 4.04e-10 2.49e-01 1.07e-01 1.02e-02 3.44e-03\ndying 1 4.04e-10 1.55e-01 3.72e-02 1.66e-01 1.39e-02\nbeing abused 0.5 (source) 1.84e-10 7.43e-03 3.28e-02 2.83e-02 4.30e-04\nbeing injured 0.1263 (Episodes per 1,000 population: 126.3) 4.64e-11 6.74e-02 6.94e-03 1.01e-01 6.45e-04\nbeing raped 0.01 (18.3% of women (50.8% of population) and 1.4% of men (49.2% of population)) 3.66e-11 3.51e-04 1.03e-02 3.59e-02 1.06e-04\nbeing killed 4.01 × 10−2 (murder + 1 out 28 in accident) 1.47e-11 2.59e-02 4.57e-02 3.32e-02 1.19e-03\nbeing arrested 0.031526 (3,152.6 arrests per 100,000) 1.16e-11 5.06e-02 5.23e-03 9.85e-02 2.52e-03\nbeing adopted 0.021 (7 million out of 328.2) 7.83e-12 4.93e-03 4.54e-03 8.53e-03 3.24e-05\nbeing murdered 4.37 × 10−3 (1 in 229 deaths) 1.60e-12 2.99e-02 5.15e-02 7.88e-02 1.34e-03\nbeing abandoned 0.000175 (7000 each year, out of 4M births) 6.42e-14 6.45e-04 4.17e-03 1.15e-02 3.46e-05\nTable 5: Frequency of actions performed or occurring to a person during their lifetime, along with the\nsources used for actual frequency calculation, and the normalized scores for actual frequency, corpus\n(Google Ngrams), and LM scores. Daily statistics were multiplied by 365 × 78.54 (average life ex-\npectancy in the US: https://www.cdc.gov/nchs/fastats/life-expectancy.htm).\nAction Action Terms\nthinking thinking, thinks, think, thought\nbreathing breathing, breathe, exhale, inhale\nblinking blinking, blink, blinks, blinked\ntalking talking, talk, talked, say, said, saying, converse, conversed, conversing\neating eat, eating, ate, dine, dining, dined\nsleeping sleeping, sleep, sleeps, slept\nworking working, work, worked, employed\nexercising exercising, exercise, exercised\ngetting married married\ngetting divorced divorced\nbeing born born\nbeing named named, called\ndying died, die, dies, dying\nbeing injured injured\nbeing arrested arrested\nbeing murdered murdered, killed\nbeing killed killed\nbeing raped raped\nbeing abused abused, molested, assaulted, beat, bullied, oppressed, tortured\nbeing shot shot\nbeing adopted adopted\nbeing abandoned abandoned\nTable 6: Synonyms and subactions used for each action in Section 2.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.8195780515670776
    },
    {
      "name": "Computer science",
      "score": 0.6909870505332947
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5973929762840271
    },
    {
      "name": "Language model",
      "score": 0.5829986333847046
    },
    {
      "name": "Natural language processing",
      "score": 0.5584617257118225
    },
    {
      "name": "Machine learning",
      "score": 0.44980138540267944
    },
    {
      "name": "Artificial neural network",
      "score": 0.44304946064949036
    },
    {
      "name": "Deep neural networks",
      "score": 0.4299231767654419
    },
    {
      "name": "Mathematics",
      "score": 0.12169438600540161
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}