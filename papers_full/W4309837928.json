{
    "title": "Transforming the generative pretrained transformer into augmented business text writer",
    "url": "https://openalex.org/W4309837928",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5045680247",
            "name": "Faisal Khalil",
            "affiliations": [
                "Osnabrück University"
            ]
        },
        {
            "id": "https://openalex.org/A5086183865",
            "name": "Gordon Pipa",
            "affiliations": [
                "Osnabrück University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2789202651",
        "https://openalex.org/W2518186251",
        "https://openalex.org/W2964266061",
        "https://openalex.org/W3209056694",
        "https://openalex.org/W131643106",
        "https://openalex.org/W4239943352",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W6685158001",
        "https://openalex.org/W3030068589",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W4235505822",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2891488835",
        "https://openalex.org/W2120615054",
        "https://openalex.org/W2373570000",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W2057867321",
        "https://openalex.org/W3044092828",
        "https://openalex.org/W2759848268",
        "https://openalex.org/W2766736793",
        "https://openalex.org/W2148876004",
        "https://openalex.org/W2963047628",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W4307979480",
        "https://openalex.org/W6717697761",
        "https://openalex.org/W1502890786",
        "https://openalex.org/W2799073995",
        "https://openalex.org/W2005814556",
        "https://openalex.org/W100236546",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2087617622",
        "https://openalex.org/W4210834152",
        "https://openalex.org/W3098861490",
        "https://openalex.org/W2095461998"
    ],
    "abstract": null,
    "full_text": "Transforming the generative pretrained \ntransformer into augmented business text \nwriter\nFaisal Khalil* and Gordon Pipa \nIntroduction\nWith the passage of time, the field of artificial intelligence, and machine learning have \nbeen made progress by leaps and bounds. Nearly all fields are getting benefits from the \ncutting-edge technologies to leverage their processes, and Deep learning is one of them. \nBig tech giants are reformulating their strategies to align with AI and ML. Deep learning \nis a branch of Machine learning that enhances the model learning process with its deep \nlayered architecture. Like many other walks of life, Deep learning has won its spurs as a \nvery effective and efficient technique for natural language processing related tasks. Since, \ncomputers are unable to understand the natural language, enabling them to understand \nthe natural language and to process the information in a useful fashion has long been \nunder the researchers’ and practitioners’ focus.\nThis study is inspired by the new method implement by the Google Brain team [47] \nand the work of OpenAI [36]. Before introducing the transformers implement by \nAbstract \nThis study uses transformers architecture of Artificial neural networks to generate \nartificial business text for a given topic or theme. The implication of the study is to \naugment the business report writing, and general business writings process with \nhelp of generative pretrained transformers (generative pretrained transformer (GPT)) \nnetworks. Main focus of study is to provide practical use case for GPTs models with \nhelp of big data. Our study model has 355 million model parameters and trained for \nthree months on GPU enable devices using 2.3 billion text tokens(is available as open-\nsource data now). Text tokens are collected with help of rigorous preprocessing, which \nincludes; shortlisting of Subreddits of Fortune 500 companies and industries, listed \non US-based social news aggregation online portal called “Reddit” . After shortlisting, \nmillions of submission of users during the five years, are parsed to collect the URLs \nout of it. 1.8 million working URLs are scrutinized. Business text is parsed, cleaned, and \nconverted into word embeddings out of uniform resoruce locator (URLs). The result \nshows that both models; conditional interactive and random sampling, generate text \nparagraphs that are grammatically accurate and stick to the given topic.\nKeywords: Natural language generation, Transformers, Business text generator\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nKhalil and Pipa  Journal of Big Data           (2022) 9:112  \nhttps://doi.org/10.1186/s40537-022-00663-7\nJournal of Big Data\n*Correspondence:   \nfkhalil@uni-osnabrueck.de\nInstitute of Cognitive \nScience, Universität \nOsnabrück, Wachsbleiche 27, \n49090 Osnabrück, Germany\nPage 2 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nthe above-cited research work, it is important to shed the light on the recent past of \nNatural language processing (NLP). Although Natural language Processing (NLP) has \ndeep roots in the past and the first breakthrough was the well-known paper of Alan \nTuring ‘Computing Machinery and Intelligence’ [46], real progress in the field has been \nmade in the late 1980s—when machine learning algorithms came into the picture. The \nmachine learning revolution has permanently changed the approaches to address NLP \nrelated problems. At the start, mostly much stress has been given to rich text features \nembedding—to enables Artificial Neural Networks (ANNS) to understand the rich text \nin numerical form. Later these embeddings are given to an end-to-end neural network \nthat essentially maps the input and output, i.e [32]. Later one, seminal work published \nrelated recurrent neural network [40]. Recurrent models are very important for natural \nlanguage processing because natural language caries lexical, syntactical, and semantic \ncontext in it- thus previous words or characters are very important to solve machine \ntranslation and text prediction tasks. In the year 2002 Jürgen Schmidhuber and his \nstudents [18] came up with a better idea for neural network application that involves \nlong-term dependencies, named, Long Short Term Memory (LSTM). Long Short \nTerm Memory (LSTM) devises some gating and sates mechanism that keeps import \ninformation from the previous sequence and also memories the previous state that finally \naccumulates to the current state to predict the next sequence. Many enhancements have \nbeen made by the research community in the recurrent neural network model. The \nmost highlighted models are seq2seq (sequence to sequence) [24, 44]. Seq2seq models \nessentially work with encoders and decoders recurrently to encode the output of the \nprevious sequence and combine it with the current input. The next enhancement in \nrecurrent model is attention mechanism, see [55, 56]. Attention mechanism has been \nproven very well in machine translations, where two pairs of sentences of two languages \nare mapped together with encoders and decoders.\nSo, looking back to the short history of the evolution of the natural language \nprocessing techniques, we understood one common limitation of all these models \nconcerning solving the NLP task is the models are computational resources hungry and \nvery slow. NLP corpus normally involves an enormous amount of training data, long-\nterm dependencies, and recurrent nature. These factors make the training process very \nslow to achieve the desired result. Addressing this problem, the research community \nhas come up with multilayered attention head and encoder decoders—formally called \nTransformers [47]. The current study uses a similar approach to generate the domain \nspecific text, and detailed methodology is discussed in \"Methods \". We have used a \nrecently developed transformer neural network architecture. This architecture is \nprimarily used for Google translation works in two different blocks, namely, encoders \nand decoders. We have only used the decoder part. We have provided the model with a \n2.3 billion text token during the training. The model has 355 model parameters and has \nbeen trained for 3 months to reach a 2.6 training loss value. Above-mentioned 2.3 billion \ntext tokens are collected after rigorous data preprocessing steps. US-based social news \naggregation and discussion forum has been selected for data collection purpose. Almost \n700 Subreddits are shortlisted for the purpose of getting URLs out of it. Millions of \nsubmissions for five years have been considered. Submission means any post, comment, \nor reply by the user. Users often redirect towards URLs for clarification. So, 1.8 million \nPage 3 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nURLs are collected from the submissions, and validation and functionality of all URLs \nhave been confirmed. With the help of a parser, these URLs are parsed and cleaned to \nget the text. Finally, 2.3 billion ready to feed to the model word embedding has been \ngenerated. In rest of the paper; literature review, Methodology of the study and model, \nresults of the study and limitation and future suggestion have been given respectively.\nResearch gap\nAfter getting the flashback of the evolution of the NLP and recent developments \nof NLP , we can see one common problem for all Natural language understanding \nproblems is creating a relationship matrix between the words or characters and \ngiving importance to the specific word at a specific place. Solving this problem is very \nimportant for all NLP-related niches, for example, Natural language understanding, \nNatural language generation and, machine translation. In this connection, we have \nmainly two problems to be solved. Problem no 1 is again giving importance to the \nwords and specific place in the sentence and creating correlation or context to each \nword embedding based on their usage. The second very problem is supplying a lot \nof data or in other words a lot of instances to the model to learn the placement \nand relational pattern of the characters or words. Giving a lot of data needs a lot of \nwords’ embeddings matrix that leads to extremely slow model training and a lot of \ncomputation resources. So, the computational and efficiency problem is more lethal \nas it seems to get a breakthrough of problem No. 1. The research community either \ncould wait for the computation resources to get more efficient and faster enough to \nsolve the problem at hand, or they must have to come up with an optimal solution. \nSo, the solution to this problem was attention mechanism [47] and most specifically \ntransformer architecture of neural networks, formally called encoders decoders [36]. \nwell, fair enough transformer can, theoretically, overcome the above-mentioned \nproblems and give a new horizon to the landscape of NLP and NLG, but we need \nto provide a lot of real-life use cases and proof of concept to supplement this new \nANNS architecture. After this conceptual breakthrough, the next challenge is to come \nup with a lot of data and preprocess that much big data to supply it to these new \nmodels to proof the concept of the conceptional invention. Our paper is exactly filling \nthis gap here by coping with the challenge of developing the proof of concept and \npracticality of this new advancement of NLP and deep learning. So, in this journey \nthe most important step is to find a use case; so, we have chosen business-related \nreports and text writing. In the next subsection, we will give precise details where \nand how this concept can be used in a commercial setting and what benefit it can \npromise. Coming back to the current point, getting a lot of business-related data is \nvery important as well very hard because of a lot of irrelevant text and without the \nauthenticity of being business text. So, involving humanized efforts to tag data is very \ncostly and not plausible. So, we decided to use “reddit” a platform, widely used, and \neach post is voted by the community. In this way, we could get human checked data in \nhuge volume, related to the business problems. it is also relevant to mention here that \nwe did not parse data from “reddit” directly, rather we have only collected URL links \nfrom the posts, and then we parse complete URLs text. So, our main contribution \nhere is rather less on the theoretical side and more on the practical side. As we have \nPage 4 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nretuned and adopted the existing theoretical concept in a more practical setting to \nprovide its proof of concept. after having this discussion, it’s very relevant to provide \none hypothetical application instance and possible commercial usage of this study. \nSo, next subsection talks about the hypothetical ideal use case and overall generic use \ncases of the study.\nHypothetical use case\nLet’s here create a practical scenario. In the office and business management, there are \na lot of reports and text writing, for example, Manager X has to give a job placement \nad for a consultancy firm, or, he has to write an advertisement. He has to write a \nsmall report about his product and its competitor in the industry he is operating to \nget external funding. In such cases the grammar is not only an important factor but \nare pinning words other people are using in the industry to influence more or clarity \nof text is maybe more important. Let say a software application helps Manager X \nin two ways; first, gives a context or appropriate usage of words replacement based \non millions of other use cases already people used in similar instances. Second, if \nhe writes “ Apple Inc. ” . the application suggests him, i.e., “ Apple has launched iPhone \npro max. in 2020 that gave them xxx hundred thousand $ annual revenue” . So, now \nManager X can save a lot of time and energy in surfing google in searching facts and \nfigures. if some assistance is provided on how he can paraphrase any keywords, could \nimprove business writing greatly. I know that requires a lot of work on front-end \ndevelopment too, but the Black box part would be NLG here.\nPractical implication\nThe study has great potential for real-world practical uses: for example, next-word \nprediction, topic modeling to extract text out of scanned images, contextual soundness \nof the business writing, and suitability of word usage even if it’s grammatically correct \nin the first place. Any subject-specific knowledge, language usage, and vocabulary are \nalways different compares to generic languages. Many companies and start-ups have \nsoftware applications that are using a similar approach but use general language text. \nHere is a list of some: Gmail salutation and common words autofill used during the \nemail [20], Grammarly [21] gives words context suggestion and content clarity based \non the text they have trained upon. At the start of registration, they asked for purpose \nof use. Maybe something like Grammarly business writer or something similar could \nbe the very practical use of this study. Reverso Translator gives translation based on \nthe frequency of usage of the word in literature along with text, except where the \nlooked-up words have been used. There is the potential of usage of such tool is there \nwhere one can give the accurate context of the only business-related text. Lastly, we \ndid know at the time of conducting this research, but one online platform emerges \nnow which is using augmented writing approach with greater success having a top-\nlevel firm in their customers’ portfolio, i.e. see [54]. This would be a very true practical \nusage of such a study. There is not only business related application of language \nPage 5 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \ngeneration model but also applied to many filed. i.e, van Deursen [15] introduced \nGenerative Examination Networks (GEN) to generated chemical space. 5 ].\nHow deep learning integrate into corporate sector?\nThe literature on the Natural Language Processing is root back in the 1940s. After \nparsing the literature, the evolution of NLP can be segregated into different phases; \nfor example, the journey started from machine translation problems, followed by the \ncomputers and information technology revolution—that triggered the AI applications \ninto this area. After AI and machine learning came into the picture—complex task \nsolving ability has been improved with less time—thus grammatical structure has been \nfocus more. After advancements like deep learning and reinforcement learning, NLP \nhas now entered into artificial text generation and generated text is hardly differentiates \nfrom human written.\nThough the research community of that time had been working on NLP , the first \nscientific paper was published by the MIT language department head, William. N Locke \nand A.Donald Booth, head of the Brick-Beck collage [28]. Machine Translation (Machine \nTranslation (MT)) started with three dominant languages of that time, English, Russian, \nand a bit of Chinese. Computational resources were too scarce and much effort had to \nbe exerted on converting data in bits [1]. Early birds in this area have given focus to \nsyntactical computational processing of language, and it was important to first draw the \nbasic structure for the language [35]. Work of [11] some researchers have tried to shift \nthe focus from the syntactical to semantic oriented language processing. Ceccato tried \nto co-relational analysis between the same pattern of a pair of languages and tried to \nachieve the semantic driven language processing. Winograd [52] and Woods [53] have \nseen the 1960s transformational grammar theory is a misfit of computational grammar \nand analysis and not offering much in terms of semantics. The computational confidence \napproach is given by Woods’ and Winograd’s enriched the previous work in a semantic \npath.\nLater on, in the 80s, AI came into the picture and the community has shifted their \nfocus toward a machine leaning based approach for solving the existing dilemmas of \nNLP in a pure semantics way [41]. In this decade, researchers have realized that the \nNLP task such as building the word representation to use in AI-related networks and \npining the context is very hard. Some note able work of the 1980s is as follows: Briscoe \net  al. [9] have built a general-purpose grammatical formalism including syntactical \nanalyzer for the English language with help of suboptimal software, named Grammar \nDevelopment environment (Grammar Development environment (GED)). They also \nprogram software to build and manage a large grammar base. Towards the direction of \nspeech recognition, Young et al. [57] have led to major US speech recognition projects, \ncalled, Continuous speech recognition (Continuous speech recognition (CSR)) and \n(Long vocabulary speech recognition (LVCSR)). The paper includes tools and methods \nfor news transcription, text dictation, and transcriptions.\nThe next phase of the NLP development is the 1990s, that mostly focuses on a \ncombination of lexical and syntactical approach for natural language processing. After \nlot of twists and struggle of almost two decades, the statistical and probabilistic approach \nhas been adopted for classification tasks in NLP [43]. Later on, these models became \nPage 6 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nraw sources of machine learning related techniques to solve the NLP complexities. for \nexample, Manning and Schuetze [29] have worked on information retrieval, feature \nextraction out of it, and analyzing the textual information with statistical models. Mani \nand Maybury [30] have used terminological logic to built a knowledge base for automatic \ninformation extraction and text summarizing. By the end of the 1990s, dialogue \nspeech system and language processing had expanded the horizon with multilingual \ntext machine translations, speaker-independent speech to speech dialogue system. \nWahlster [50] has worked on project Foundation of Speech-to-Speech Translation—\nso-called, ‘Verbmobil’ . This multilingual (German, English, and Japanese) takes input in \na speaker-independent manner and translates them into other desired languages. it also \nhandles domain-specific business spoken dialogues and translates into other languages \nwith approximately 80 percent accuracy. The struggle of many years make the NLP \nresearchers, practitioner, and industry realize that linguistic resources are inevitable for \nthe further development in this filed, thus, two institutions, “British National Corpus” \n[8] and “WordNet” [17] are come into being. The next era of natural language processing \nstarted after 2001. Though many models have been proposed by the researchers which \nwere other than neural networks, we are only discussing the neural network-oriented \nimportant models in this paper.\nBengio et  al. [7] proposed tri-gram state-of-the-art neural probabilistic model. They \nhave used a neural network for the probability function. The idea is based on the \nconjecture that unseen words get a higher probability to be predicted based on the \nsimilarity of the words—on which the network is trained. The next word prediction \napproach has many practical uses commercially, for example, see the work of [26] that \ncan generate a small short semantic reply of the email.\nThe next advancement in the field of NLP is multitask learning, off-course this method \nis not only confined to the NLP but a general enhancement in the neural network \nworld. Collobert and Weston [12] have tried to implement this technique for transfer \nlearning. Vector representations of the words have been fed as an input to the model \nto do word prediction and then learning of the current model was transferred to the \nother independent model to achieve a similar but not the same task. The multi-task \nlearning approach was first introduced by the Caruana [10]. Once, so-called, word \nvector representations are fed to the neural network, they start learning the context \nand association of each work with the other. Transfer learning makes it possible to \nshare the learned weight across the models for generalization and incremented learning \napproach. During the optimization process, it is very important which parameter to \ntransfer. Ruder [39] proposed that the sharing parameter can also be learned during the \nlearning process. See also similar research [31]. In this connection, the next milestone \nwas “vectors representation” of the text, so-called word embeddings. This basic word \nembedding idea was first floated by mikolov [33]. They have proposed that removing \nthe hidden layer while training the word embedding is giving more promising outcomes. \nLater on, this idea paved the way for the concept ‘word2vec’ and originally adapted to \ntwo popular approaches, namely, bags-of-words and skip grams. This phenomenon \nhas triggered the research interest in this direction and many researchers have enrich \nthis concept see [2, 3, 34, 51]. The current direction of the word embedding is to train \nPage 7 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \na very large corpus and use used pre-trained embeddings for multilingual models in an \nindependent and unsupervised fashion. for example, see [4, 13, 42].\nIn the year 2013 and 2014 neural network architectures are being applied to NLP , \nthe most obvious choice was recurrent, recursive, and convolutional neural networks. \nsimper Elman [16] RNNs were replaced with LSTM by [23] because of long-term context \ndependencies in input text. secondly, convolutional networks are originally dealt with \ncomputer vision areas but also implemented in NLP for example see the work of [25, 27]. \nThe obvious plus of the using convolutional network is they are more parallel and local \ncontext based on layers rather than past state contrary to the LSTMs.\nConcerning recurrent neural networks, the next enhancement was a sequence to \nsequence modeling (seq2seq). Seq2seq model is using the same recurrent architecture \nof the neural networks, but the important bit is disguise in encoding and decoding \nprocedures. The input sentence is first encoded into a vector representation. The decoder \nthen tries to decode the predicted symbols based on the encoder state sequentially. The \nsequence to sequence model was proposed by Sutskever et  al. [44]. Later on, in the \nyear 2016 Google [19] has decided to change its monolithic sentence based machine \ntranslation to complete neural network-based. Now, seq2seq models are the foundation \nof language generation models and further developments, i.e transformer-based neural \nnetwork architectures. Similarly, see also image captioning [48] is using the same \ntechnique to generate the image captions automatically. The seq2seq model leads toward \nattention mechanism and transformers based approaches. The basic limitation of the \nseq2seq network is that it tries to compress the whole sequence of the sentence and then \nconvert it into a fixed-length vector. Thus, the model cannot look into the hidden state. \nAttention mechanism, by contrast, looks into the hidden state of the model combine \nthem to realize how much stress should be given to a specific word. Attention [6] was the \ncore innovation in the field of neural machine translation that permanently replace the \ntraditional methods of machine translation. Have a look on different flavors of attention \nbased networks and their application; reading comprehension [22], entity parsing [49], \nimage captioning [55].\nThe pretrained model has gain popularity among the NLP research community. The \nmain advantage of the pretrained model is that it is context agnostic and unsupervised \nmodel. Labeling for the NLP task can be very costlier and challenging. So, the pretrained \nmodel captures the meaning and context of one language and the leanings can be \ntransformed into the other language to get the meaning and context generation or \ntranslation. The pretrained model was first proposed by Dia and Le [14]. The current \nstudy is also based on pretrained multi head attention based model.\nMethodology\nIn this section we have described how data is prepossessed and then processed data is \nfed to the model is discussed in detail. The completely prepossessed data will be available \nas an open-source data for further research and development.\nData preprocessing\nIn this section, we have described the process of data preparation for model training. \nEverything else with respect to the neural network model is similar to many other \nPage 8 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \napplications of ANNS, but the main concept here is to leverage the training process with \nan enormous amount of training data. Websites could be the potential source of a lot of \ntextual data as well as a great deal of diversity in it, but the bottleneck with websites’ data \nis the validity of data and too much unnecessary information in it. Following the research \nby Vaswani [47] we have adopted a similar approach and choose ‘Reddit’ [37]—a USA \nbased social news aggregation and discussion platform with 330 million users [37] to \ncollection the website URLs to parse the data form. To ensure the validity and usefulness \nof the web URLs, only those links have been taken that contained more than 3 ‘karma’ . \n‘Karma’ is so-called assurance given by the other user about the validity of comments \nand discussion. In this way, we have got a human level quality check on the data. Once \nwe have devised the mechanism of data quality, the next filer was to get the URLs that are \nonly related to the business and Fortune 500 companies. Most of the top 500 companies \nhave their discussion and news profile on ‘Reddit’ called ‘Subreddit’ . ‘Reddit’ has a very \nlarge community and thus, thousands of submissions are committed on a daily basis. \nThe raw data, ranging from 2005 to 2017, is first programmatically collected with help \nof the ‘Reddit’ programming interface [38] and stored in the ‘BigQuery’ database. In the \nnext step, we have extracted all the URLs having ‘karma’ ranking more than 3 from the \ndaily submission of the users. These URLs are verified, whether they are working or not \nand at the end 1,852,482 working URLs list was prepared to parse the textual data from \n‘Hyper Text Mark Langauge (HTML)’ tags. With the help of parallel computing and a \ncomputer grid, 20 GBs of text files have been collected from all working URLs. These \n20 GB text files are gain filtered for some unnecessary characters and symbols. Finally, \nthe 2,302,554,291 text token were collected to be converted into word embeddings. \nThe process is shown in Fig.  1a that depicts a flow of data preprocessing with help of a \nschematic diagram. preprocessing involves:\nFig. 1 Data preprocessing and network architecture\nPage 9 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nMethods\nNext comes the transformer neural network model applied to preprocessed data. The \nTransformer model takes all words tokens are encoded into words embeddings, that \nis nothing but the numbers that represent each word. Normally, transformers have \ntwo parts, encoders and decoders, but we have only used the decoders part of the \nTransformer because both encoder and decoder are feasible for machine translations—\nthat is not the case in this study. See Fig.  2 how general transformer works, originally \ndesigned for machine translation problems. This architecture was later adopted and \nmodified by many researcher and lab to improve NLP and translation related problems. \nIf you pay closer attention to the paper [48], you will realize transformers are also \nbasically a from of transfer learning where sentence of the language one are pass through \nmany layers of self-attention and feedforward neural network layers and update the \ntraining weights keeping the relationship of each word within the sentence and position \nof each words into mind, whereas, learned weighted of language one are transferred \nto feedforward layer of decoder part to learn the nature of relationship and position \nor grammatical aspect into mind when model tries to predict the words in the second \nlanguage. That is how essence and context of sentence are translated correctly. So our \ncase is rather different from machine translation, thus second language inputs’ weight \nare not possible here.So, we stick to the decoder part of the model as a main model \narchitecture. coming back to the point of data processing, Words embedding are stored \nand converted into NumPy zip format for simplicity purposes. first, we will see the \nFig. 2 Transformers general network architecture [47]\nPage 10 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nhigh-level representation of the model, and then we will look into how the self-attention \nlayer is working. The model gets the words embedding as input, it assigns positional \nencoding to each word. The positional encoding keeps the position of the word into a \nsentence to capture the context efficiently, contrary to random order. Word embedding \nalong with its positional information passes through the self-attention layer. The self-\nattention layer is twelvefold layers.\nFor analogy purpose, we can say this layer create many copies of the sentence and \nmap the relationship and importance of each word in the sentence to figure out how \nmuch attention to the specific words is to be given. That is why it is called a multi-head \nself-attention layer. We can plunge into the self-attention layer to see how it is working. \nInput vector X1 ..XN is multiplied by three different vectors, namely, Query vector ( q1 ), \nKeys vector ( K1 ) and value vector( V1 ). The vector is random weights of dimension 64 \nand the output of these matrices’ multiplication is WQ , Wk, Wv . In the next step, we get \nthe dot product of ( q1 · K1 ....KN ) for sentence (1....n.). To stabilize the gradient process, \neach output is then divided to the ( \n√\ndk ), whereas, d is dimension of the vector k. This \noperation gives us scores for each word. higher the sores means that more attention \nshould be given to that word. In the next step all the scores for on word related to all \nother words should be summed up into a variable Z:\nThis is the final calculation of one out of many self-attention layers, that is to be fed—\nin a matrix shape, to the feed-forward neural network. To focus on different positions \nof the words in the sentence we need, multiple representational subspaces, subspace is \nachieved with the help of multiple head or copies of the attention layer. so;\nwhereas, i...n is the number of attention layers. Q, K, V is the query, key, and value vector \nand X is the word embedding input matrix. So, every attention layer produces a Z matrix \nand depending on how much attention layers being chosen, in our case 12. The attention \noutput matrices Z1 ....Z12  are multiplied with the weights’ matrix jointly for all layers, \ncalled WO . The resulting matrix is input for a fully connected feed-forward network. The \nfinal output of the feed-forward network is then decoded back to the words to generate \nthe sequence of the sentence. For the clarity of the dimensions of the different matrices, \nplease refer to Table 1.\nResults\nIn this section, we have described the results of our study. In this section, we have \npresented text samples that are generated by our trained model. The results include a \nsample from both conditional and unconditional samples. Conditional sampling means \nthat we have provided a certain keyword to the model as an input and the model has \nreturned a text paragraph related to that given keyword, however, unconditional \n(1)Z = softmax\n(\nQ × KT\n√\ndk\n)\n× V\n(2)\nQi....Qn = W iX\nKi....Kn = W iX\nVi....Vn = W iX\nPage 11 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nmeans random samples generated by the trained model. Training loss summary of the \n‘Tensorboard’ model is given in the Appendix section. To support out the accuracy \nof model and the sample are not appear out of chance, we have given 100 randomly \ngenerated sample by the model in the Appendix section.\nWe have trained the model up to 460,000 steps. Since the model has almost a \n355Million model parameter and more than 2.3 billion text token, the model requires \nextremely excellent computation power and time. The model has been trained for 3 \nmonths on a single GPU and settles on a loss value of 2.6. This value of loss for the text-\nbased model is quite reasonable because the language model always involves complex \ngrammatical chains like dependencies and structures that are not easy to capture. \nThe next two subsections provided real-time model generated text, both based on \nconditional and unconditional random outputs.\nInteractive conditional outputs of the model\nThis subsection provides 5 different output samples of the interactive conditional \nsampling method of the study model. This is so-called interactive model outputs, in \nwhich the model communicates with the user. The user gives input/keywords to the \nmodel and the model generates a text paragraph that mostly talks about the given \nkeyword/topic. Given are the Tables  2, 3, 4, 5 and 6 show output against five different \nuser given inputs.\nUnconditional outputs of the model\nIn the section below we have given Tables  7, 8, 9, and 10 which show the random \nsample output generated by the model. this is an artificial text written by the model. \nIf we observe the generated paragraphs, it is very clear that the text is following the \ngrammatical rule mostly and topics of the sample pointing towards the business-related \ntext. An enormous amount of sample can be produced on demand, due to the brevity of \nthis article we have only given some sample.\nDiscussion\nIn this section, we are going to discuss the results of the study and how these results \nstratify the problem inference of the study. The main focus of this study is to testify \nthe validity and useability of current theoretical development in the field of natural \nlanguage generation and generally Natural language processing. For the reliability \nof data, we have used subreddit to check the URLs at human-level quality check. \nTable 1 The table gives the dimension of the different matrices\nMatrix Dimension\nX 1 ....Xn Upto 512 depends \non length of \nsentence\nEvery W 64\nX ×W DX×64\nZ DX×64\nWo DX×64\nPage 12 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nThe robustness is done with help of the KARMA points threshold which is 3 point \nKARMA. The choice of 3 KARMA is based on the average karma point being given \nnormally in subreddits. As increasing the KARMA point gives you more human-\nlevel quality but it reduces the amount of data dramatically, which means losing a lot \nof quality information and context of the application.As we stressed out previously \nthe long dependencies chain of one word to other words, placement of the word in a \ngiven sentence and relational space of the word and characters is the big challenge of \nlanguage generation-related problems. This problem was very difficult for recurrent \nneural network models to cope up with. So, the researchers came up with different \ntheoretical concepts. In this connection, we are providing practicality, useability, and \nproof of concept of the model in our study. For this purpose, we have provided two \ntypes of results, interactive conditional and non-interactive random samples. How we \nhave trained the model, iteration, and loss graph can be observed in the Appendix  \nsection. However, the main objective of the study was to generate the text that sticks \nto the overall topic of text, formally called topic modeling, secondly grammatically \ncorrectness, and thirdly, somehow related to business only. If we closely read the \nresults of section \"Introduction \", we have provided the model with random business-\nrelated diverse keywords from all different business genres. Model is not only able \nTable 2 Results of interactive conditional samples\nAutomatic output generated by the trained model against user given topic/key\nKey Conditional output\nApple iPhone 7 Plus: 3-month free trial 15% off $150+ purchase iPhone 7 Plus: 3-month free trial 15% off \n$150+ purchase 4G LTE: iPhone 6s: 2 year data plan 3 GB data, $49.99/mo 8GB data, $59.99/mo \n4G LTE: iPhone 6s: 2 year data plan 3 GB data, $49.99/mo 8GB data, $59.99/mo iPhone 5s: 2 year \ndata plan 5 GB data, $45.99/mo 8GB data, $69.99/mo\nTable 3 Results of interactive conditional samples\nAutomatic output generated by the trained model against user given topic/key\nKey Conditional output\nOil &price “which make up the global oil industry. The used state-of-the-art computer models to show that \nthe price of crude from North Dakota has increased by a factor of 40 over the past year. In the past \nyear, it has risen by 28 percent, a period of unprecedented energy pricing, according to the report.\nThat is an almost unheard-of percentage increase, said Tim Leissner, an assistant professor at the \nUniversity of Michigan who specializes in energy and economic modeling, which is used in energy \nforecasts. “Usually, the percentage increases are pretty much the same year after year, so that’s a \npretty consistent pattern,” said Leissner, who was not involved in the report. “Normally what we would \nexpect is that the price goes up because of oil production, and it generally goes up at the end of the \nyear.”But that is no longer the case, said Leissner.“The oil industry has a new president in Trump, and \nhis agenda is very aggressive on a range of issues. And the industry is not only seeing this increase, \nbut it is actually seeing it drop,” Leissner said. The increase in North Dakota oil production has gone \nfrom 6.5 million barrels per day in the first half of 2017 to 7.3 million in the second half of 2017 and \n7.3 million in the first quarter of 2018, according to the report. And in the last two months alone, \nNorth Dakota’s production has fallen from 4.6 million barrels per day in the first half of 2018 to 3.3 \nmillion in the second half of 2018. The increase in oil production in North Dakota has also affected a \nnumber of other energy prices, with natural gas prices rising about 12 percent in the first half of 2017 \nand natural-gas prices rising about 13 percent in the first half of 2018.The data comes as the Trump \nadministration prepares to review the Obama rule that requires oilfields on federal lands to submit to \nfederally supported research and exploration programs. The rule will affect oil and gas development \nin the state. Leissner said the oil market will be monitored for “the next few years” for the first time in \ndecades. “This kind of increases is a big deal, given that oil is the only major product in the world that \npeople are going to be producing for years to come,” he said.The research is published in the Journal \nof Energy Technology. The report, which included”\nPage 13 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nto collect very related text, but also supply some facts and figures. Moreover, the \nlinkage of sentences and story making is very decent. That does the perfect job for the \nhypothetical use case and highlighted research gap. Additionally, for the robustness of \nthe model, we have also created a random sample of text generations with thousands \nof instances. Due to brevity, we have provided some samples here, and we have given \nlink to could to access all other thousands of samples. In both types, an interactive \nand non-interactive, model is achieving the initial goal for context, relatedness, and \ntopic modeling. of course, this is just a founding block to generate any meaningful \ncommercial soft application. we need to assemble other pieces of puzzles, namely \nfront-end development, scripting, and mapping for words PACs of words matching \nTable 4 Results of interactive conditional samples\nAutomatic output generated by the trained model against user given topic/key\nKey Conditional output\nMicrosoft Windows “users, if you don’t already have them or you’re thinking about installing them on your \nWindows Vista/Windows 7 PC, you’ve got to pay the price. There’s also the issue that you’re \ngoing to have to run Microsoft on your current PC, which isn’t ideal if you’re a Mac user. \nIf you only do this once, and you’ve got Mac OS X installed, Windows 7 will work. The \nproblem with this is, it’s a pain to use on those platforms. Even if you’ve got Mac OS X on \nyour computer, it’s going to be a pain to use in Windows, and if you want to use Windows \nVista on a Mac, it’s going to be a pain on Windows. So we’re going to need to figure out a \nway to get Windows 7 running on existing Macs. We’ve been trying to do so, so far. And at \nsome point, we’ve found something that’s going to let us do it. We’ve been working with \nthe folks over at Xiph.ai and are able to install and upgrade and upgrade, and I don’t want \nto go into too much detail, but we actually managed to install Windows 7 on Mac OS X \nthrough the Xfce application. And if you do this just once and you’re running Windows 7 on \na Mac, it will run. Xfce and Window 7 To get Xfce running for Windows, right-click in the Xfce \nlauncher, and then choose “Xfce Installer” . This will open a pop-up window in which you can \nchoose what to install. The first option is “Mac OS X 10.4.5” . You can choose the installer from \nthe list below. If you’re using the installer option, you’ll notice that Xfce has been created \nfor you. And you’ll also see an “Xfce.app” folder for Windows. In order to get Xfce running \non Windows, we’ll need to run the installer from that folder. Open Xfce by going to its \napplication menu. Select “Install Xfce” from the search box. You’ll notice it’s not”\nTable 5 Results of interactive conditional samples\nAutomatic output generated by the trained model against user given topic/key\nKey Conditional output\nS &P 500 “index fell 1% and the S &P 500 and Dow Jones Industrial Average gained 0.7% and 0.4%.The Dow \nclosed up 8.2%.Meanwhile, the S &P 400 surged 3.8%, with the Nasdaq Composite index up 8.3%. The \nNasdaq rose 0.4%, while the S &P 500 3.6%.While Wall Street was off the back foot, on the financial \nmarkets, the Dow rose 1.8% and the Nasdaq rose 0.8%.In the energy market, the Standard & Poor’s \n500 index dipped 1.2%, the Nasdaq index rose 3.6% and the S &P lost 4.1%. In real estate, the S &P 500 \nrose 0.8% and the Nasdaq climbed 1.2%.Meanwhile, the Nasdaq’s gains were smaller in the oil energy \ngroup, while it gained 0.4% and the S &P 500 rose 0.3% in residential and business.The Nasdaq rose \n2.0% and the S &P 500 gained 0.8% in the energy sector.At the top of the industry, the S &P 500 closed \n6.9%, with the Nasdaq up 2.6% and the S &P 500 up 2.1%.In the energy group, the Dow rose 8.7%, with \nthe Nasdaq rising 2.4% and theS &P500 adding 4.5% in the energy segment.And in residential, the S &P \n500 rose 1.7% and the Nasdaq rose 1.3%.In the business sector, the S &P 500 rose 2.5% and the Nasdaq \nclimbed 1.1%.The Dow was up about 7.8% as the S &P 500 fell 1.4%, while the Nasdaq declined 1.2%.\nThe Dow hit a record high of 11,868. The S &P 500 was up 11.4% and the Nasdaq rose 0.4%.The \nfinancial sector was trading at a record high as investors worried about the economy and a potential \nFederal Reserve interest rate hike, which has taken hold following a sharp decline in the central bank’s \nbenchmark interest rate.The Dow closed up 8.7%, the S &P 500 rose 3.8%, and the Nasdaq rose 3.9%”\nPage 14 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nthe counts and statistics concerning the whole database, with many more bumps and \nstager on the journey down this road.\nConclusion\nThe current study is focusing on the application of Natural language processing in the \nfield of business writing. In the recent past, the Deep Learning research community \nhas come up with a new architectural style of deep layered AI models that are aligned \nwith the specific need of natural language and text generation. The transformer is one \nof those models that are proven to very accurate and effective in context and grammar \ncapturing in the text.\nResponse to the possible question, what is the purpose of the study very briefly? \nThe study uses a generative pretrained neural network model. The model is fed with \na lot of business-related preprocessed text data acquired parsing the 1.8 million URLs \ncollected from Reddit. As a result of the trained model, user can give keywords or some \ntopic to the model and model produced paragraph that completely sticks to the given \ntopic, provided that the topic or keyword is in the domain of business or management \nsciences. These features or results provided by the study can be utilized in automatic \nparagraph prediction to assist the business report writer or any relevant person involves \nTable 6 Results of interactive conditional samples\nAutomatic output generated by the trained model against user given topic/key\nKey Conditional output\nHeath care industry “in a manner that protects the privacy of patients. We will continue to take this necessary \nstep to ensure that our industry is transparent and accountable,” the department said \nin its statement. In addition, the department plans to “deregulate the sale of any health \ncare services without a patient consent,” and “work towards an industry-wide ban on \ninvoluntary sex trafficking. The Department of Justice has been involved throughout the \nissue of sex workers and the use of technology, and is actively working with regulators \nand private-sector partners to better address the issue of human trafficking, the company \nstated. ”With recent advancements in technology for technology’s sake, sexual exploitation \nis no longer confined to the virtual realm, but now has become a reality,“ it added. The \ndepartment said it ”will continue to work with state, federal and private sector partners to \nimprove their policies and laws regarding human trafficking, including protections and the \nuse of technology to protect victims of sex trafficking.” The government’s goal is to help \nensure that “we do not, in any way, further endanger any of our citizens, particularly those \nwhom we have entrusted with our law enforcement and public safety responsibilities.” \nThe department added, however, that it “will not tolerate law enforcement personnel, \nemployers, contractors and contractors to be complicit or assist in unlawful human \ntrafficking, as that might undermine our mission.”< |endoftext| >“This domain is no \nlonger available, possibly due to a change in the registrar or the domain administrator.”< |\nendoftext| >“What started as a series of tests to see if it could survive the cold, the hot and \nthe humidity is turning into a real life experiment that will test the idea that this tiny space \nheater might have a practical purpose. For this project, the team set out to conduct the \ntests using just a standard fridge - just a standard fridge, for short. But they wanted to see \nwhat it could do if the temperature inside it dropped by 10 °C—a scenario that will be a \nreality in 2017. The first test will bring in an air temperature of – 30 °C—something which \nwill kill any kind of bacteria as well as fungi in about two days. As you can imagine, that \nwasn’t the worst of the worst in the test. A typical fridge—even the kind you’ll find in the \nkitchen—was just as bad as the prototype at catching bugs. The team has now developed \na small freezer for the freezer, which can withstand even lower”\nPage 15 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nin the writing process. As there are many applications available for next word prediction \ngenerally but paragraph prediction is lacking.\nNow, let us give little more details on how data is preprocessed, and the model \nis trained to get the output? A large amount of quality data is very important for \nlanguage processing models. To address the quality issue we have chosen ‘Reddit’; news \naggregation and content sharing platform. Although Reddit covers a lot of different \ntopics, we have shortlisted ‘subreddits’—topic-specific Reddits. There is a huge amount \nof Reddit submission every day and ‘KARMA’ vote is given to the post that is helpful \nTable 7 Results of non-conditional samples\nAutomatic output generated by the trained model\nSample Machine generated text\nSample1:Topic Microsoft SA-4, but some of the other models that it supports can be set, as can the version \nof Windows. However, most users will need to install the free and limited OS X Lion \noperating system. To get it the free version, you can purchase it from the Apple site. \nIn the past few years the Windows phone market has grown significantly. Microsoft \nhas been aggressively supporting Microsoft Office applications in Windows 8. The \ncompany is still selling a wide selection of Office applications. The reason Microsoft \nhas decided to target the Android market, especially is that, as a company that is \nheavily invested in smartphone users, Microsoft will be less apt to change direction. \nStill, it is unlikely that you’re going to be surprised to find that Windows Phone has \nmanaged to outcompete Android in terms of support level and functionality. That has \nbeen the case for years now, and not just because of the various operating systems. \nHowever, things have changed for the better. The development time has gotten \nslower, and the hardware has gotten more modern. There have also been increased \nefforts to make the operating system itself more user friendly. Microsoft has been \nsteadily working to increase the range of features available on the platform. In the case \nof Windows Phone 8, this means that it supports the latest version of the operating \nsystem, the Universal Windows Platform (UWP), which has evolved in ways that made \nthe system more accessible to the new users. The new version of the Windows Phone \nOS also integrates with the new Metro UI, which has been available for several years \nalready. This means that the interface is easier to navigate with each new update. \nThere is an obvious difference between the Windows Phone version of the Windows \nOS and other systems, which makes the difference in the Windows Phone OS much \nstronger. As a result, the Windows Phone OS is likely to enjoy a much wider appeal. \nThe Windows Phone OS is a much more mature OS, however, and it may prove to be \neven more attractive once it is officially supported by Microsoft. In that case, it seems \nthat the Windows Phone OS can only prosper as long as Redmond will provide more \nhardware devices that can be run this OS. It is, of course, a very hard problem to solve. \nHowever, it seems inevitable that this issue will play a greater role in Microsoft’s future \nstrategy. Microsoft has been focused on offering a wide range of popular consumer \nand enterprise computing options in order to take advantage of the growing mobile \nmarket. There is also a good chance that the introduction of Windows and Office to \nthe marketplace will bring a greater opportunity for Windows to become a mainstay \nfor mobile devices. Further Reading< |endoftext| > A new video showing a drone \ncarrying a baby to her birth, while also showing her running and jumping, is set to \ndebut at the London premiere of David Cronenberg’s “Puff Daddy,” at the V &A on \nWednesday. Puff Daddy follows a teenage girl whose father is killed in an accident \nand has been left with an orphaned daughter. Her ex is a young woman from a \nnearby suburb who has a passion for flying and is looking for a way to give back to her \ncommunity. “You can’t have a child without a parents,” said Cronenberg, who directed \n“All the Money in the World” and “O Brother, Where Art Thou?” alongside Brian De \nPalma and John Frankenheimer, as well as “Shakespeare’s Son,” “All the Money in the \nWorld” and “The Other People’s”’ alongside Tom McCarthy. The director also showed \noff new CGI footage of the film’s main characters, including the first scene where \nthey’re shown playing with the baby and flying. “Puff Daddy” is shot in a sequence \nthat features a close-up of the girl and the baby. “We wanted to create a visual effect \nin ways that were visually appealing,” Cronenberg said. The scene with the daughter \nflying was filmed in the streets of North London, but Cronenberg said the scene in the \nfilm’s last shooting, “Rudolph the Red-Nosed Reindeer,” will also appear in the visual \neffects package. The project also features a “flying baby” sequence that was filmed in a \nnearby suburb\nPage 16 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nfor the community. So we have collected 1.8 million URLs from those submissions that \nhave ‘KARMA’ vote greater than three. In the next separate step, we have collected and \ncleaned all the text available in the URLs. In the end, 2.3 billion text tokens have been \nfed to the model. The model has 355 million parameters. After three months of model \ntraining, the model can generate grammatically correct and aligned with business topic \ntext as a model output. In the coming subsection, we have discussed what could be the \npractical application of the model and future suggestions along with some limitations of \nthe study.\nImplications and future work suggestion\nThere are many possible implications of this study. One possible use is market \nintelligence report writing. Possibly a piece of software can be developed to auto-\ncomplete the paragraphs for business intelligence report writing. Any business-\nrelated industry can be benefited with help of paragraphs prediction instead of just \nword prediction. In this way, the speed of efficiency of the user can be enhanced \nsignificantly. As for future suggestions are concerned, we think that text token \nprefixed by the theme or topic of the text can make this model more useful. For \nexample during the training text, at the start of the text, we can provide what this \npiece of text is talking about. In this way, we can have greater control over the output \nof the model we can generate real-time long reports based on specific keywords. \nThe report is just one example we can utilize the model is much more effective ways. \nTable 8 Results of non-conditional samples\nAutomatic output generated by the trained model\nSample Machine generated text\nSample2:Topic: Health Care ,” said the chief executive officer of the British medical charity, Beaumont Hospital. \n“In recent years we have seen a surge in the number of young people coming into \nthis country seeking to change our society, but the risks of those who do become \nradicalised remain too high. ”Young people like Mika are at risk of radicalisation \nand may be vulnerable to becoming radicalised themselves through viewing social \nmedia as a possible means of radicalising themselves or others. “Our advice is to \nwork closely with the police and other relevant authorities to help these young \nindividuals to understand the risks in the communities they may come into contact \nwith in the future and to talk to parents about their responsibilities.” It has long \nbeen feared that social media is inextricably linked to radicalisation. Earlier this year \nit emerged that the police were monitoring 4 million posts on Twitter, Facebook, \nKik and Line, all forms of instant communication, for signs of terrorism. But Dr. John \nRalston of the University of Oxford has claimed that although “social media has \nbeen used for a long time in the UK, and indeed throughout Europe, some parts \nof society have never noticed it.” He said that while people in certain sections of \nthe community have been concerned at the recent rise in extremism, many young \npeople in other parts of the population have not. “The vast majority of young \npeople at one time or another have encountered such people through social \nmedia.”< |endoftext| >The biggest financial institutions have the greatest exposure \nto the market, yet they are the most transparent, according to Transparency \nMarket Research (TMR). The research group analyzed more than 700 leading \nfinancial companies, looking for those who reported some form of transparency, or \ndisclosed more information than allowed. The findings, based on the organization’s \nannual survey of 1250 U.S. companies, show that firms with the largest exposure \nto the financial market have disclosed the highest amounts of transparency - even \nthough they are less transparent than the average firm. Those firms with the most \ntransparency are also the companies that the study found to the highest risk of \nmarket abuse, including: • A number of the top 50 firms made disclosures in excess \nof 30 percent of their company size. • The majority of firms did not disclose their \ndisclosure forms during the year\nPage 17 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nAdditionally, the study can be done with a focus on different karma point numbers \nand how the change in KARMA point selection criteria is affecting the quality of \nmodel predictions. We hope that the research community is maybe already doing \nsomething in that direction.\nTable 9 Results of non-conditional samples\nAutomatic output generated by the trained mode\nSample Machine generated text\nSample3:Topic Energy Market to have more time with people,” he says. “I am sure I will enjoy sitting on my porch \nwith the trees behind me. It will be a really relaxing time.”For all the progress the \nfilm has made this year, however, as the box office has surged, so has that of the \nfranchise. The latest installment, starring Tom Hiddleston, will bow on Monday, \nwhile the two-part drama “Hiddleston: Longmire” will arrive in U.S. cinemas on \nFebruary 19, followed by its worldwide debut in April.< |endoftext| >Hodl has \nbeen selling the technology for years. On the shelf of any Walmart, Walmart.\ncom or Amazon, it’s not uncommon to come across a shelf full. But now a new \ntechnology to turn your clothes into a new energy-generating asset comes to \nSan Francisco and Silicon Valley. As the world warms and urban temperatures \nrise, the amount of energy stored in the fabric of the clothes and other products \nincreases. This process has the potential to revolutionize a whole range of \nindustries and technologies-as well as the way people buy and use clothes. \nFirms that design, produce, market, sell or install this technology face a number \nof problems, including the technology’s limitations-which includes the inability \nto make use of the wind. And in a major market, technology can be confusing \nto customers. If an installer sees a new technology on a product, its not clear \nwhere to go. Is it a product or an energy-generating product? It’s an industry with \na number of big names, all of which are focused on the same thing, but each \ncompany has a different brand. Sprinting out of the closet Fifty years ago, it was \na relatively new idea that was made possible at the beginning of the Cold War by \nadvances in the nuclear power and hydrogen bomb. This meant that we could \nget more power with less fuel than any previous technology had before. But \nmore power, as it turns out, is less efficient than it used to be, and that has led to \nmore problems than it is solving. The technology was called the “energy storage.” \nThe technology used energy (or energy storage) to cool a part of fabric. It was \nthe first technology for storing energy. Then this technology fell out of fashion. \nIn the mid-1970s, it was popularized by Motorola, a firm that produced smart \nrefrigerators that could store energy and heat the refrigerator. This technology \nbecame the fuel cell and energy storage industry before it fell out of favor.\nSprinting out of the closet The reason we still have to solve problems with energy \nstorage is partly due to the sheer size of the industry. It takes lots of expertise \nto be able to make use of the technology, but we still find ourselves getting \nit wrong. The biggest reason are twofold. First, the technology was extremely \neffective early on, but as technology has made more efficient, then it has gotten \ndifficult to get it right. Second, the technology is often confusing and difficult to \nintegrate into existing products. If you’re in an industry where the problem with \nstoring energy is a lack of innovation, you may find the technology confusing \nand cumbersome and you may have to learn how to use multiple energy \nstorage products to get the right kind of energy. The companies behind the two \ncompanies-Gigasolar, in Palo Alto, and GigaSolar, in Sunnyvale-are trying out \nnew energy storage technologies that make an energy store. Sprinting out of the \ncloset is a common solution to those issues. However, there are companies that \nare able to solve the first problem using their previous technology without ever \nlooking at the second problem. One company that has done this is Gigasolar, \nin Sunnyvale and Stanford. Gigasolar’s company mission is to get rid of the \ntechnology gap between what the customers need and what the companies are \nbuilt for. It’s not a simple business. They have to design a product that solves the \nproblem before it can be fully put to use. The company had to do this. Gigasolar, \nwhich is run by the founders Peter and Adam Zweig, are very involved in figuring \nout how to work within the energy delivery space. They have a number of \npatents that cover multiple products. In these patents, they have some of the \nbest energy energy storage patents that you can find. For instance: They use \ncarbon dioxide to store energy in the atmosphere and then they release carbon \ndioxide back into the atmosphere to convert it into new energy\nPage 18 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nLimitations of the study\nWe have tried to do the study at our best, but there certain technical limitations of the \nstudy. Since the models related to text generation is usually based on an enormous \namount of training data; that is a very important factor to capture the grammatical \nstructure and relatedness of the topic, this study only relies upon the text generated \nfrom those web URLs that were discussed in business-related Subreddits. The study may \nbe improved significantly with help of having more sources of training data and more \ncomputational power.\nAppendix\nFigure 3 show training loss and Fig. 4 shows test loss in a model training summary graph. \nThe graph is produced with help of “Tensorlflow” tool called “Tensorboard” . Tensorboard \nis a tool for viewing the hidden layers and mechanism of the ANN models—written by \nGoogle to increase the efficiency of “Tensorflow” library [45].\nTable 10 Results of non-conditional samples\nAutomatic output generated by the trained model\nSample Machine generated text\nSample4:Topic Retailing Production are not the same as production at a retail store like you are likely to see \nat the Superstore. I recently did a bit of testing for this.You Can’t Buy This There are \ntwo types of products that can make it through the Superstore checkout process. \nThe first are the items that are sold using the same pricing structure as the retail store. \nFor example, the price for the T-Shirt above only goes up $0.75 with the same color. \nAll I had to do was go to the Superstore and double check my shirt price with the \nsame information as I use to determine if my T-Shirt cost me half what it cost on the \nsame item in the Retail Store. The T-Shirt above, when bought online, has been $0.75 \nup charge, but on sale is $0.85 for the same size of shirt (you can see why the $0.75 \ndifference is so important!). The second type of product that can help you make it \nthrough the checkout process are items with the same price that you’ll find on the \nretail store. This is usually with items like apparel. For example, the same shirt is $.25 on \nthe Superstore (a T-shirt cost me $0.85 on the Retail Store for the same size of Shirt), \nas long as they look the same they’re actually a bargain. If you’re a Clothing shopper \nand shop your clothing online, you’d probably get the price of those same garments \nfor $0.75. In my opinion, the Superstore prices for the same items are more consistent, \nso I’ve decided to include them. To find the Superstore price for Clothing, you need \nto purchase a Clothing App item online, take the price as your price, then click the \npurchase button. You should then follow the steps above to determine the price for \nthat item. A good starting point would be the same price that you pay for the Clothing \nApp item in the Retail Store. Here’s an example of the Clothing App item that I would \npurchase on the Retail Store. The Shirt above, I bought for $27 online.If you purchase \nthis on the Retail Store, $27 would be the price you’d pay online: $37.99. If you’d \npurchased it on the Superstore it would be $0.80. I would take my clothing price and \nmultiply it by the retail value of the Shirt (i.e. $0.80 is what I think the T-shirt is on sale \nat the Superstore). I would assume this will be the same price as my T-Shirt. To do this, \nI just have to multiply the actual price by my price, then subtract my retail value from \nmy Retail Store price to create a final price of $0.40 for the T-shirt. As you can see, my \ntotal Clothing App price is $38.50. To keep track When you’re shopping online, take in \none value from each App item for those two different sizes of the Shirt (the same ones) \nthen calculate that value and convert to a per-item price for the product you’re looking \nat while shopping. The final price should be that. So let’s say that we have the Shirt \nbelow for $25 that we think would be the same $25 price for the same Shirt on the \nRetail Store. After we calculated the per-item value for the Shirt, we would add $0.80 to \nour total clothing price for that T-Shirt to calculate our final price. In this case, that final \nprice would be $0.80. This is the final value of that T-Shirt—$0.80\nPage 19 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \nRandom samples\nIn the section, we have given the ‘Microsoft’ ‘OneDrive’ shared folder link which \ncontains 2284 samples that are generated by the model during the training process. The \nrandom sample has been generated roughly after every 200 training steps. Samples can \nbe accessed via following link:\nhttps:// pern- my. share point. com/: f:/g/ perso nal/ faisal_ khalil- vu_ pern_ onmic rosoft_ \ncom/ EkJX7 IxnYJ JCv7G 18Xok qI8Bq cV0ML ADgip myudk ak18bA? e= crcpun.\nAbbreviations\nNLP  Natural language processing\nGPT  Generative Pretrained Transformer\nURLs  Uniform resoruce locator\nANNS  Artificial neural networks\nLSTM  Long short term memory\nMT  Machine translation\nGED  Grammar development environment\nCSR  Continuous speech recognition\nLVCSR  Long vocabulary speech recognition\nFig. 3 Tensor-board training loss summary\nFig. 4 Tensor-board test loss summary\nPage 20 of 21Khalil and Pipa  Journal of Big Data           (2022) 9:112 \nHTML  Hyper Text Mark Langauge\nAcknowledgements\nNo acknowledgements.\nAuthor contributions\nThe research study is written by FK during the PhD at University Osnabrueck, Germany. GP has supervised and improve \nthe study in many aspects.\nAuthors’ information\nMr. Faisal khalil is doing PhD at university Osnabrueck, Department of cognitivesciences. His major areas are artificial \nintelligence and Deep learning. His mainfocus is application of AI in the areas of business and corporate finance.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nAvailability of data and materials\nThe datasets used and/or analyzed during the current study are available from the corresponding author on reasonable \nrequest.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 14 December 2021   Accepted: 20 October 2022\nReferences\n 1. ALPAC. Language and machines computers in translation and linguistics. 1966\n 2. Antoniak M, Mimno D. Evaluating the stability of embedding-based word similarities. Trans Assoc Comput Linguist. \n2018;6:107–19.\n 3. Arora S, Li Y, Liang Y, Ma T, Risteski A. A latent variable model approach to pmi-based word embeddings. \nTransactions of the Association for Computational Linguistics. 2016;4:385–99.\n 4. Artetxe M, Labaka G, Agirre E. A robust self-learning method for fully unsupervised cross-lingual mappings of word \nembeddings 2018. arXiv preprint arXiv: 1805. 06297\n 5. Bagal V, Aggarwal R, Vinod P , Priyakumar UD. Molgpt: molecular generation using a transformer-decoder model. J \nChem Inf Model. 2021;62(9):2064–76.\n 6. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate 2014. arXiv \npreprint arXiv: 1409. 0473\n 7. Bengio Y, Ducharme R, Vincent P , Jauvin C. A neural probabilistic language model. J Mach Learn Res. 2003;3:1137–55.\n 8. BNC. British national corpus 2020. https:// www. engli sh- corpo ra. org/ bnc/, Accessed 4 Apr 2020.\n 9. Briscoe T, Grover C, Boguraev B, Carroll JA. A formalism and environment for the development of a large grammar of \nEnglish. IJCAI, Citeseer. 1987;87:703–8.\n 10. Caruana R. Multitask learning. autonomous agents and multi-agent systems. 1998\n 11. Ceccato S. Correlational analysis and mechanical translation. 1967\n 12. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask \nlearning. In: Proceedings of the 25th international conference on Machine learning 2008; pp 160–167\n 13. Conneau A, Lample G, Ranzato M, Denoyer L, Jégou H. Word translation without parallel data 2017. arXiv preprint \narXiv: 1710. 04087\n 14. Dai AM, Le QV. Semi-supervised sequence learning. In: Advances in neural information processing systems.2015; pp \n3079–3087\n 15. van Deursen R, Ertl P , Tetko IV, Godin G. Gen: highly efficient smiles explorer using autodidactic generative \nexamination networks. J Cheminform. 2020;12(1):1–14.\n 16. Elman JL. Finding structure in time. Cogn Sci. 1990;14(2):179–211.\n 17. Fellbaum C. Towards a representation of idioms in wordnet. In: Usage of WordNet in Natural Language Processing \nSystems. 1998\n 18. Gers FA, Schraudolph NN, Schmidhuber J. Learning precise timing with lstm recurrent networks. J Mach Learn Res. \n2002;3:115–43.\n 19. Google. Alphabet inc. 2020. https:// www. google. com, Accessed 4 Apr 2020.\n 20. GoogleEMail. Gmail 2021. https:// www. google. mail. com/. Accessed 15 Nov 2021.\n 21. Grammarly i. Grammarly, 2021. https:// app. gramm arly. com//, Accessed 15 Nov 2021.\n 22. Hermann KM, Kocisky T, Grefenstette E, Espeholt L, Kay W, Suleyman M, Blunsom P . Teaching machines to read and \ncomprehend. In: Advances in neural information processing systems. 2015;pp 1693–1701\nPage 21 of 21\nKhalil and Pipa  Journal of Big Data           (2022) 9:112 \n \n 23. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80.\n 24. Jacovi A, Shalom OS, Goldberg Y. Understanding convolutional neural networks for text classification, 2018. arXiv \npreprint arXiv: 1809. 08037\n 25. Kalchbrenner N, Grefenstette E, Blunsom P . A convolutional neural network for modelling sentences, 2014. arXiv \npreprint arXiv: 1404. 2188\n 26. Kannan A, Kurach K, Ravi S, Kaufmann T, Tomkins A, Miklos B, Corrado G, Lukacs L, Ganea M, Young P , et al. Smart \nreply: Automated response suggestion for email. In: Proceedings of the 22nd ACM SIGKDD International Conference \non Knowledge Discovery and Data Mining. 2016;pp 955–964\n 27. Kim Y. Convolutional neural networks for sentence classification 2014. arXiv preprint arXiv: 1408. 5882\n 28. Locke WN, Booth AD. Machine translation of languages. Am Document. 1956;7(2):135.\n 29. Manning CD, Schütze H. Foundations of statistical language processing. 1999\n 30. Maybury M. Advances in automatic text summarization. Cambridge: MIT press; 1999.\n 31. McCann B, Keskar NS, Xiong C, Socher R. The natural language decathlon: Multitask learning as question answering, \n2018. arXiv preprint arXiv: 1806. 08730\n 32. McClelland JL, Rumelhart DE. Explorations in parallel distributed processing: a handbook of models, programs, and \nexercises. Cambridge: MIT press; 1989.\n 33. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their \ncompositionality. In: Advances in neural information processing systems. 2013;pp 3111–3119\n 34. Mimno D, Thompson L. The strange geometry of skip-gram with negative sampling. In: Empirical Methods in \nNatural Language Processing. 2017\n 35. Plath W. Multiple path analysis and automatic translation. Amsterdam: North-Holland; 1967.\n 36. Radford A, Wu J, Amodei D, Amodei D, Clark J, Brundage M, Sutskever I. Better language models and their \nimplications. 2019, OpenAI Blog https://openai com/blog/better-language-models\n 37. reddit. Reddit. 2021a, https:// www. reddit. com/, Accessed 15 July 2020.\n 38. reddit. reddit; 2021b, https:// www. reddit. com/ dev/ api/. Accessed 15 July 2020.\n 39. Ruder S, Bingel J, Augenstein I, Søgaard A. Latent multi-task architecture learning. Proc AAAI Confer Artif Intell. \n2019;33:4822–9.\n 40. Rumelhart DE, Hinton GE, Williams RJ. Learning internal representations by error propagation. Tech. rep.: California \nUniv San Diego La Jolla Inst for Cognitive Science; 1985.\n 41. Schank RC. Language and memory. Cogn Sci. 1980;4(3):243–84.\n 42. Søgaard A, Ruder S, Vulić I. On the limitations of unsupervised bilingual dictionary induction. 2018. arXiv preprint \narXiv: 1805. 03620.\n 43. Sparck Jones K. Thesaurus Encyclopedia of artificial intelligence. 1992;2:1605–13.\n 44. Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. In: Advances in neural \ninformation processing systems. 2014;p. 3104–3112.\n 45. Tensorbaord. 2020. Google tensorboard. https:// www. tenso rflow. org/ tenso rboard, Accessed 15 Oct 2020.\n 46. Turing AM. Computing machinery and intelligence. In: Parsing the turing test, Springer. 2009;p. 23–65.\n 47. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: \nAdvances in neural information processing systems. 2017;p. 5998–6008.\n 48. Vinyals O, Kaiser Ł, Koo T, Petrov S, Sutskever I, Hinton G. Grammar as a foreign language. In: Advances in neural \ninformation processing systems. 2015;p. 2773–2781.\n 49. Vinyals O, Blundell C, Lillicrap T, Wierstra D, et al. Matching networks for one shot learning. In: Advances in neural \ninformation processing systems. 2016;p. 3630–3638\n 50. Wahlster W. Mobile speech-to-speech translation of spontaneous dialogs: an overview of the final verbmobil \nsystem. In: Verbmobil: Foundations of speech-to-speech translation, Springer. 2000;p. 3–21.\n 51. Wendlandt L, Kummerfeld JK, Mihalcea R. Factors influencing the surprising instability of word embeddings 2018. \narXiv preprint arXiv: 1804. 09692\n 52. Winograd T. Understanding natural language. Cogn Psychol. 1972;3(1):1–191.\n 53. Woods WA. Semantics and quantification in natural language question answering. In: Advances in computers. \n1978;vol 17, Elsevier, p. 1–87.\n 54. Writing TA. 2021. Textio augmented writing. https:// textio. com/. Accessed 15 Nov 2021.\n 55. Xu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y. Show, attend and tell: Neural image caption \ngeneration with visual attention. In: International conference on machine learning. 2015;p. 2048–2057.\n 56. Yang Z, Yang D, Dyer C, He X, Smola A, Hovy E. Hierarchical attention networks for document classification. In: \nProceedings of the 2016 conference of the North American chapter of the association for computational linguistics: \nhuman language technologies. 2016;pp 1480–1489.\n 57. Young SJ, Chase LL. Speech recognition evaluation: a review of the us csr and lvcsr programmes. Comput Speech \nLang. 1998;12(4):263–79.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}