{
  "title": "Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network",
  "url": "https://openalex.org/W4285247467",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102511683",
      "name": "Zheng Gong",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2102854352",
      "name": "Kun Zhou",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098250721",
      "name": "Zhao Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1934761110",
      "name": "Jing Sha",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A2128038036",
      "name": "Shi-jin Wang",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134642945",
    "https://openalex.org/W3123123873",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W3172141633",
    "https://openalex.org/W3183303265",
    "https://openalex.org/W2951107864",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W4214784181",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3102315106",
    "https://openalex.org/W2884517879",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3083978629",
    "https://openalex.org/W2905122540",
    "https://openalex.org/W3103939752",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3035157853",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963705839",
    "https://openalex.org/W3035147733",
    "https://openalex.org/W3185289994",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2031071334",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4299547686",
    "https://openalex.org/W3158196282",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3035373548",
    "https://openalex.org/W4287136589"
  ],
  "abstract": "In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach called COMUS to continually pre-train language models for math problem understanding with syntax-aware memory network. In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text. With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas. Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/COMUS.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5923 - 5933\nMay 22-27, 2022câƒ2022 Association for Computational Linguistics\nContinual Pre-training of Language Models for Math Problem\nUnderstanding with Syntax-Aware Memory Network\nZheng Gong1,4â€ , Kun Zhou2,4â€ , Wayne Xin Zhao 1,4âˆ—, Jing Sha3,5,\nShijin Wang5,6, Ji-Rong Wen1,4\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China 3iFLYTEK Research, Hefei, Anhui, China\n4Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\n5State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China\n6AI Research(Central China), iFLYTEK, Wuhan, Hubei, China\nAbstract\nIn this paper, we study how to continually pre-\ntrain language models for improving the under-\nstanding of math problems. Specifically, we\nfocus on solving a fundamental challenge in\nmodeling math problems, i.e., how to fuse the\nsemantics of textual description and formulas,\nwhich are highly different in essence. To ad-\ndress this issue, we propose a new approach\ncalled COMUS to continually pre-train lan-\nguage models for math problem understanding\nwith syntax-aware memory network. In this\napproach, we first construct the math syntax\ngraph to model the structural semantic informa-\ntion, by combining the parsing trees of the text\nand formulas, and then design the syntax-aware\nmemory networks to deeply fuse the features\nfrom the graph and text. With the help of syntax\nrelations, we can model the interaction between\nthe token from the text and its semantic-related\nnodes within the formulas, which is helpful\nto capture fine-grained semantic correlations\nbetween texts and formulas. Besides, we de-\nvise three continual pre-training tasks to further\nalign and fuse the representations of the text\nand math syntax graph. Experimental results\non four tasks in the math domain demonstrate\nthe effectiveness of our approach. Our code and\ndata are publicly available at the link: https:\n//github.com/RUCAIBox/COMUS.\n1 Introduction\nUnderstanding math problems via automated meth-\nods is a desired machine capacity for artificial in-\ntelligence assisted learning. Such a capacity is the\nkey to the success of a variety of educational appli-\ncations, including math problem retrieval (Reusch\net al., 2021), problem recommendation (Liu et al.,\n2018), and problem solving (Huang et al., 2020).\nTo automatically understand math problems, it\nis feasible to learn computational representations\nâ€  â€ Equal contribution. This work was done when the two\nauthor were interns at iFLYTEK Research.\nâˆ— âˆ—Corresponding author, email: batmanfly@gmail.com\nMath Problem:Given that sin xis equal to 0.6and xis anacute angle, find the value ofsin2ð‘¥+tan!\".\nGiven\n[MATH]0.6of\nthe\nvalueRootfind sin\nsqrt\ndividetan\nMath Syntax Graph\nequal\nsinxâ‹¯\nplus\ntimes\nrootadvcl\nnsubjcomp\nobjccompobl nmodcase\ndet\n22x x\nTextual Description:  Given that sin x is â€¦ find the value of $\\sqrt { \\sin{ 2x }+\\tan { \\frac { x }{ 2 } } }$ .Operator Tree\nâ‹¯\nFigure 1: Illustration of a math problem with its textual\ndescription and math syntax graph.\nfrom problem statement texts with pre-trained lan-\nguage models (PLMs) (Shen et al., 2021; Peng\net al., 2021). Pre-trained on the large-scale gen-\neral corpus, PLMs (Devlin et al., 2019) can be\neffectively transferred into new domains or tasks\nby continual pre-training on task-specific datasets.\nDifferent from traditional text comprehension tasks,\nas shown in Figure 1, math problems usually in-\nvolve a complex mixture of mathematical symbols,\nlogic and formulas, which becomes a barrier to the\naccurate understanding of math problems.\nHowever, previous works (Reusch et al., 2021;\nShen et al., 2021) mostly oversimplify the issues\nof math problem understanding. They directly con-\ncatenate the formulas with the textual description\nas an entire sentence, and then perform continual\npre-training and encoding without special consid-\nerations. Therefore, two major shortcomings are\nlikely to affect the understanding of math problems.\nFirst, formulas (the most important elements of the\nproblem) contain complex mathematical logic, and\nmodeling them as plain text may incur the loss of\nimportant information. Second, the textual descrip-\ntion contains essential explanations or hints about\nthe symbols and logic within the formulas. Hence,\nit is necessary to accurately capture fine-grained\n5923\ncorrelations between words from the description\ntext and symbols from math formulas.\nTo better model the computational logic of for-\nmulas, operator trees are introduced to represent the\nmath formulas (Zanibbi and Blostein, 2012), which\nare subsequently encoded by graph neural network\n(GNN). Although these methods can improve the\ncomprehension capacity of math problems to some\nextent, there still exists a semantic gap between\ngraph encoding and text encoding due to the hetero-\ngeneity of formulas and texts. With simple concate-\nnation or self-attention mechanisms (Peng et al.,\n2021), it is still hard to capture the fine-grained\nassociations among tokens and symbols, e.g., the\ndependency relation between math symbols and\ncorresponding explanation tokens.\nIn order to better fuse the information from for-\nmulas and texts, our solution is twofold. First, we\nconstruct a syntax-aware memory network based\non a structure called math syntax graph (Figure 1),\nwhich integrates operator trees from formulas and\nsyntax trees from texts. The key point lies in that\nwe store the node embeddings from the GNN and\ndependency relation embeddings as entries of mem-\nory networks, and then design the corresponding\nread and write mechanism, using token embed-\ndings from the PLM as queries. Such a way can\neffectively associate the representation spaces of\nthe text and formulas. Second, we devise specific\ncontinual pre-training tasks to further enhance and\nfuse the text and graph representations, including\nthe masked language model and dependency triplet\ncompletion tasks to improve the understanding of\nmath symbols in the text and formulas logic in\nthe syntax graph, respectively, and the text-graph\ncontrastive learning task to align and unify the rep-\nresentations of the text and graph.\nTo this end, we proposeCOMUS, to continually\npre-train language models for math problem\nunderstanding with syntax-aware memory network.\nIn our approach, we first encode the textual de-\nscription and math syntax graph via PLM and GAT,\nrespectively. Then, we add syntax-aware memory\nnetworks between the last k layers of PLM and\nGAT. In each of the lastk layers, we first conduct\nthe multi-view read and write operation to fuse\nthe token and node representations, respectively,\nand then adopt the next layer of PLM and GAT\nto encode the fused representations. All parame-\nters of our model are initialized from PLMs and\nwill be continually pre-trained by our devised three\ntasks, namely masked language model, dependency\ntriplet completion and text-graph contrastive learn-\ning. Experimental results on four tasks in the math\ndomain have demonstrated the effectiveness of our\napproach, especially with limited training data.\nOur contributions can be summarized as follows:\n(1) We construct a novel syntax-aware memory\nnetwork to capture the fine-grained interactions\nbetween the text and formulas.\n(2) We design three continual pre-training tasks\nto further align and fuse the representations of the\ntext and graph data.\n(3) Experiments on four tasks in the math do-\nmain demonstrate the effectiveness of our model.\n2 Preliminaries\nIn this section, we formulate the problem statement\nand then introduce the math syntax graph.\nProblem Statement. Generally, a math problem\nconsists of a textual description d and several for-\nmulas {f1, f2, Â·Â·Â· , fm}. The textual description\nprovides necessary background information for the\nmath problem. It is formally denoted as a sequence\nof tokens d = {t1, t2, Â·Â·Â· , tl}, where ti is either\na word token or a mathematical symbol ( e.g., a\nnumber or an operator). The formulas describe the\nrelationship among mathematical symbols, which\nis the key to understand and solve the math problem.\nEach formula consists of a sequence of mathemati-\ncal symbols, denoted as fi = {s1, Â·Â·Â· , sn}.\nBased on the above notations, this work focuses\non continually pre-training a PLM on unsupervised\nmath problem corpus for domain adaptation. After\nthat, the PLM can be fine-tuned on various tasks in\nthe math domain (e.g., knowledge point classifica-\ntion), and improve the task performance.\nMath Syntax Graph. In order to understand the\nmathematical text and formulas, it needs to capture\nthe complex correlations within words, symbols\nand operators. Inspired by previous works (Man-\nsouri et al., 2019; Peng et al., 2021), we construct\na syntax graph, where the textual description is\nrepresented as a syntax dependency tree and the\nformulas are represented as operator trees (OPT).\nSpecifically, given a math problem consisting\nof a textual description d and several formulas\n{f1, f2, Â·Â·Â· , fm}, we first utilize the open-source\ntoolkit TangentS1 to convert each formula into an\n1https://github.com/BehroozMansouri/TangentCFT\n5924\nOPT, and Stanza2 to convert the textual description\ninto a syntax dependency tree. Then, we com-\nbine the syntax dependency tree and the OPTs to\ncompose an entire graph, where a special token\nâ€œ[MATH] â€ is applied to link them. We call such\na composite graph as the math syntax graph G of\nthe math problem. Let N and R denote the set\nof nodes and relations on G, respectively. We can\nextract dependency triplets from G, where a depen-\ndency triplet (h, r, t) denotes that there exists an\nedge with the relation r âˆˆ Rto link the head node\nh âˆˆ Nto the tail node t âˆˆ N.\n3 Methodology\nAs shown in Figure 2, our approach aims to effec-\ntively encode the textual description and formulas,\nand fuse these two kinds of information for under-\nstanding math problems. In what follows, we first\npresent the base models for encoding math prob-\nlems, and then introduce the devised syntax-aware\nmemory network and continual pre-training tasks.\n3.1 Base Models\nEncoding Math Text. We use BERT (Devlin et al.,\n2019) as the PLM to encode the math text, i.e., the\ntextual description d. Given d = {t1, t2, Â·Â·Â· , tL}\nof a math problem, the PLM first projects these to-\nkens into corresponding embeddings. Then, a stack\nof Transformer layers will gradually encode the em-\nbeddings to generate the l-th layer representations\n{h(l)\n1 , h(l)\n2 , Â·Â·Â· , h(l)\nL }. Since the textual description\nd may contain specific math symbols that were not\nseen during pre-training, we add them into the vo-\ncabulary of the PLM and randomly initialize their\ntoken embeddings. These new embeddings will be\nlearned during continual pre-training.\nEncoding Math Syntax Graph. We incorporate a\ngraph attention network (GAT) (VeliË‡ckoviÂ´c et al.,\n2018) to encode the math syntax graph, which is\ncomposed of an embedding layer and a stack of\ngraph attention layers. Given a math syntax graph\nG with N nodes, the GAT first maps the nodes\ninto a set of embeddings {n1, n2, Â·Â·Â· , nN }. Then\neach graph attention layer aggregates the neighborsâ€™\nhidden states using multi-head attentions to update\nthe node representations as:\nn(l+1)\ni =\nK\nâˆ¥\nk=1\nÏƒ(\nX\njâˆˆNi\nÎ±k\nijW(l)\nk n(l)\nj ). (1)\n2https://stanfordnlp.github.io/stanza/\nwhere n(l+1)\ni is the representation of the i-th node\nin the l + 1layer, âˆ¥ denotes the concatenation op-\neration, Ïƒ denotes the sigmoid function, K is the\nnumber of attention heads, Ni is the set of neigh-\nbors of node i in the graph, W(l)\nk is a learnable\nmatrix, and Î±k\nij is the attention value of the node i\nto its neighbor j in attention head k.\n3.2 Syntax-Aware Memory Network\nTo improve the semantic interaction and fusion\nof the representations of math text and the syntax\ngraph, we add k syntax-aware memory networks\nbetween the last k layers of PLM and GAT. In the\nmemory network, node embeddings (from the math\nsyntax graph) with dependency relations are con-\nsidered as slot entries, and we design multi-view\nread/write operations to allow token embeddings\n(e.g., explanation tokens or hints) to attend to highly\nrelated node embeddings (e.g., math symbols).\nMemory Initialization. We construct the mem-\nory network based on the dependency triplets and\nnode representations of the math syntax graph.\nGiven the dependency triplets {(h, r, t)}, we treat\nthe head and relation (h, r) as the key and the\ntail t as the value, to construct a syntax-aware\nkey-value memory. The representations of the\nheads and tails are the corresponding node rep-\nresentations from GAT, while the relation repre-\nsentations are randomly initialized and will be\noptimized by continual pre-training. Finally, we\nconcatenate the representations of heads and rela-\ntions to compose the representation matrix of Keys\nas K(l) = {[n(l)\nh1 ; r1], [n(l)\nh2 ; r2], Â·Â·Â· , [n(l)\nhN\n; rN ]},\nand obtain the representation matrix of Values as\nV(l) = {n(l)\nt1 , n(l)\nt2 , Â·Â·Â· , n(l)\ntN }.\nMulti-view Read Operation. We read important\nsemantics within the syntax-aware memory to up-\ndate the token representations from PLM. Since\na token can be related to several nodes within\nthe math syntax graph, we design a multi-view\nread operation to capture these complex seman-\ntic associations. Concretely, via different bilinear\ntransformation matrices {WS\n1 , WS\n2 , Â·Â·Â· , WS\nn},\nwe first generate multiple similarity matrices\n{S1, S2, Â·Â·Â· , Sn} between tokens and keys (head\nand relation) within the memory, and then aggre-\ngate the values (tail) to update the token represen-\ntations. Given the token representations from the\nl-th layer of PLM H(l) = {h(l)\n1 , h(l)\n2 , Â·Â·Â· , h(l)\nL },\n5925\nGiven that sin xis equal to 0.6and xis an acute angle, find â€¦\nGATLayer1\nGATLayerð‘€âˆ’1\nGATLayerð‘€\nâ‹¯ Keys\nsin\nValues\ncompxequalobl0.6x amodacuteâ‹¯â‹¯\nGate\n0.6\nequal\nsin\nx\nPLMLayer1\nPLMLayerð‘âˆ’1\nPLMLayerð‘CLSmeanTGCLMLM DTC\nAggregateAggregateSyntax-Aware Memory Network\nToken RepresentationsNode Representations\nâ‹¯\nx\nacute\nequalnsubjsin\namod\noblnsubj\ncomp\n0.70.3\n0.10.20.20.40.5(sin, comp)(equal, obl)\nsinx0.6\n0.3\n0.3(x, amod)\nsoftmaxsoftmax\nFigure 2: Illustration of our COMUS. We encode the textual description and the math syntax graph using PLM and\nGAT, respectively, and insert the syntax-aware memory networks in the lastk layers to fuse their representations. In\nthe syntax-aware memory network, we utilize the token representations and the node representations as the queries\nand values, respectively, and implement theread and write operations to update them.\nthe similarity matrix Si is computed as\nSi = H(l)WS\ni K(l)âŠ¤\n(2)\nwhere WS\ni is a learnable matrix, and an entry\nSi[j, k] denotes the similarity between the j-th to-\nken and the k-th key in the i-th view. Based on\nthese similarity matrices, we update the token repre-\nsentations by aggregating the value representations\nas\nË†H(l) = H(l) + [Î±1V; Î±2V; Â·Â·Â· ; Î±hV]WO (3)\nÎ±i = softmax(Si) (4)\nwhere WO is a learnable matrix and Î±i is the at-\ntention score distribution along the key dimension.\nIn this way, we can capture the multi-view corre-\nlations between tokens and nodes, and the token\nrepresentations can be enriched by the represen-\ntations of multiple semantic-related nodes. After\nthat, the updated token representations Ë†H(l) are fed\ninto the next layer of PLM, where the Transformer\nlayer can capture the interaction among token rep-\nresentations to fully utilize the fused knowledge\nfrom the syntax graph.\nMulti-View Write Operation. After updating\nthe token representations, we update the represen-\ntations of nodes from GAT via memory writing.\nWe still utilize the multi-view similarity matrices\n{S1, S2, Â·Â·Â· , Sh}. Concretely, we compute the at-\ntention score distribution Î² using softmax function\nalong the token dimension of the similarity matri-\nces, and then aggregate the token representations\nas\nV(l)\nnew = [Î²1H(l); Î²2H(l); Â·Â·Â· ; Î²hH(l)]WR (5)\nÎ²i = softmax(SâŠ¤\ni ) (6)\nwhere WR is a learnable matrix. Based on the\naggregated token representations, we incorporate a\ngate to update the representations of the values as\nz = Ïƒ(V(l)\nnewWA + V(l)WB) (7)\nË†V(l) = z Â· V(l)\nnew + (1âˆ’ z) Â· V(l) (8)\nwhere WA and WB are learnable matrices. The\nupdated node representations Ë†V(l) are also fed into\nthe next layer of GAT, where the graph attention\nmechanism can further utilize the fused knowledge\nfrom the text to aggregate more effective node rep-\nresentations.\n3.3 Continual Pre-training\nContinual pre-training aims to further enhance and\nfuse the math text and math syntax graph. To\nachieve it, we utilize the masked language model\nand dependency triplet completion tasks to improve\nthe understanding of math text and math syntax\ngraph, respectively, and the text-graph contrastive\nlearning task to align and fuse their representations.\n5926\nMasked Language Model (MLM).Since the math\ntext contains a number of special math symbols,\nwe utilize the MLM task to learn it for better under-\nstanding the math text. Concretely, we randomly se-\nlect 15% tokens of the input sequence to be masked.\nOf the selected tokens, 80% are replaced with a spe-\ncial token [MASK], 10% remain unchanged, and\n10% are replaced by a token randomly selected\nfrom the vocabulary. The objective is to predict the\noriginal tokens of the masked ones as:\nLMLM =\nX\ntiâˆˆVmask\nâˆ’log p(ti) (9)\nwhere Vmask is the set of masked tokens, and p(ti)\ndenotes the probability of predicting the original\ntoken in the position of ti.\nDependency Triplet Completion (DTC). In the\nmath syntax graph, the correlation within the de-\npendency triplet (h, r, t) is essential to understand\nthe complex math logic of the math problem. Thus,\ninspired by TransE (Bordes et al., 2013), we design\nthe dependency triplet completion task to capture\nthe semantic correlation within a triplet. Specifi-\ncally, for each triplet (h, r, t) within the math syn-\ntax graph, we minimize the DTC loss by\nLDTC = max\n\u0000\nÎ³+d(nh+r, nt)âˆ’d(nh+r\nâ€²\n, nt), 0\n\u0001\n(10)\nwhere Î³ >0 is a margin hyper-parameter, d(Â·) is\nthe euclidean distance, and r\nâ€²\nis the randomly sam-\npled negative relation embedding. In this way, the\nhead and relation embeddings can learn to match\nthe semantics of the tail embeddings, which en-\nhances the node and relation representations by\ncapturing the graph structural information.\nText-Graph Contrastive Learning (TGCL).Af-\nter enhancing the representations of the math text\nand math syntax graph via MLM and DTC tasks re-\nspectively, we further align and unify the two types\nof representations. The basic idea is to adopt con-\ntrastive learning to pull the representations of the\ntext and graph of the same math problem together,\nand push apart the negative examples. Concretely,\ngiven a text-graph pair of a math problem (di, Gi),\nwe utilize the representation of the [CLS] token\nhd\ni as the sentence representation of di, and the\nmean pooling of the node representations nG\ni as\nthe graph representation of Gi. Then, we adopt the\ncross-entropy contrastive learning objective with\nin-batch negatives to align the two representations\nLTGCL = âˆ’log exp(f(hd\ni , nG\ni )/Ï„)P\niÌ¸=j exp(f(hd\ni , nG\nj )/Ï„)\n(11)\nwhere f(Â·) is a dot product function and Ï„ denotes\na temperature parameter. In this way, the represen-\ntations of the text and graph can be aligned, and the\ndata representations from one side will be further\nenhanced by another side.\n3.4 Overview and Discussion\nOverview. Our approach focuses on continually\npre-training PLMs to improve the understanding\nof math problems. Given the math text and math\nsyntax graph of the math problem, we adopt PLM\nand GAT to encode them, respectively, and utilize\nsyntax-aware memory networks in the last k layers\nto fuse the representations of the text and graph.\nIn each of the last k layers, we first initialize the\nqueries and values of the memory network using\nthe representations of tokens and nodes, respec-\ntively, then perform the read and write operations\nto update them using Eq. 3 and Eq. 8. After that,\nwe feed the updated representations into the next\nlayers of PLM and GAT to consolidate the fused\nknowledge from each other. Based on such an ar-\nchitecture, we adopt MLM, DTC and TGCL tasks\nto continually pre-train the model parameters using\nEq. 9, Eq. 10 and Eq. 11. Finally, for downstream\ntasks, we fine-tune our model with specific data\nand objectives, and concatenate the representations\nof text hd and graph nG from the last layer for\nprediction.\nDiscussion. The key of our approach is to deeply\nfuse the math text and formula information of the\nmath problem via syntax-aware memory networks\nand continual pre-training tasks. Recently, Math-\nBERT (Peng et al., 2021) is proposed to continually\npre-train BERT in math domain corpus, which ap-\nplies the self-attention mechanism for the feature\ninteraction of formulas and texts, and learns simi-\nlar tasks as BERT. As a comparison, we construct\nthe math syntax graph to enrich the formula in-\nformation and design the syntax-aware memory\nnetwork to fuse the text and graph information. Via\nthe syntax-aware memory network, the token from\nmath text can trace its related nodes along the rela-\ntions in the math syntax graph, which can capture\nthe fine-grained correlations between tokens and\nnodes. Besides, we model the math syntax graph\n5927\nTask Train Dev Test\nKPC 8,721 991 1,985\nQRC 10,000 2,000 4,000\nQAM 14,000 2,000 4,000\nSQR 250,000 11,463 56,349\nTable 1: Statistics of the datasets.\nvia GAT, and devise the DTC task to improve the\nassociations within triplets from the graph, and the\nTGCL task to align the representations of the graph\nand text. In this way, we can better capture graph\nstructural information and fuse it with textual infor-\nmation. It is beneficial for understanding logical\nsemantics from formulas of math problems .\n4 Experiment\n4.1 Experimental Setup\nWe conduct experiments on four tasks in the math\ndomain to verify the effectiveness of our approach.\nPre-training Corpus . Our pre-training corpus\nis collected from a Chinese educational website\nZhixue 3, which consists of 1,030,429 problems of\nhigh school math exams and tests. Each math prob-\nlem contains the information of problem statement,\nanswer and solution analysis. For data preprocess-\ning, we first transform these collected problems\nfrom the HTML format into plain text format, then\nextract and convert the formulas and mathematical\nsymbols into a unified LaTex mathematical format.\nEvaluation Tasks. We construct four tasks based\non the collected math problems for high school\nstudents, which cover math problem classification\nand recommendation. The statistics of these tasks\nare summarized in Table 1.\nâ€¢ Knowledge Point Classification (KPC) is a\nmulti-class classification task. Given a math ques-\ntion, the goal is to classify what knowledge point\n(KP) this question is associated with. The knowl-\nedge points are defined and annotated by profes-\nsionals, and we finally have 387 KPs in this task.\nâ€¢ Question-Answer Matching (QAM) is a bi-\nnary classification task to predict whether an an-\nswer is matched with a question. For each question,\nwe randomly sample an answer from other prob-\nlems as the negative example.\nâ€¢ Question Relation Classification (QRC) is\na 6-class classification task. Given a pair of math\nquestions, this task aims to predict their relation\n3http://www.zhixue.com\n(e.g., equivalent, similar, problem variant, condi-\ntional variant, situation variant, irrelevant).\nâ€¢ Similar Question Recommendation (SQR) is\na ranking task. Given a question, this task aims to\nrank retrieved candidate questions by the similarity.\nEvaluation Metrics. For classification tasks (KPC,\nQRC, QAM), we adopt Accuracy and F1-macro\nas the evaluation metrics. For the recommen-\ndation task (SQR), we employ top- k Hit Ratio\n(HR@k) and top- k Normalized Discounted Cu-\nmulative Gain (NDCG@k) for evaluation. Since\nthe length of candidate list is usually between 6 and\n15, we report results on HR@3 and NDCG@3.\nBaseline Methods. We compare our proposed ap-\nproach with the following nine baseline methods:\nâ€¢ TextCNN (Kim, 2014) is a classic text classifi-\ncation model using CNN on top of word vectors.\nâ€¢ TextRCNN (Lai et al., 2015) combines both\nRNN and CNN for text classification tasks.\nâ€¢ GAT (VeliË‡ckoviÂ´c et al., 2018) utilizes the at-\ntention mechanism to aggregate neighborsâ€™ repre-\nsentations to produce representation for each node.\nâ€¢ R-GCN (Schlichtkrull et al., 2018) extended\nGraph Convolutional Network with multi-edge en-\ncoding to aggregate neighborsâ€™ representations.\nâ€¢ BERT-Base(Devlin et al., 2019) is a popular\npre-trained model. We use the bert-base-chinese,\nand add some new tokens into the original vocab to\nrepresent specific symbols in math problem dataset.\nâ€¢ DAPT-BERT(Gururangan et al., 2020) con-\ntinually pre-trains BERT on the domain-related\ncorpus. We use our collected math problem dataset\nwith the masked language model task for imple-\nmentation.\nâ€¢ BERT+GATconcatenates the [CLS] embed-\nding from BERT and mean node embedding from\nGAT as the representation of a math question.\nâ€¢ DAPT-BERT+GAT replaces BERT in\nBERT+GAT with the DAPT-BERT.\nâ€¢ MathBert (Peng et al., 2021) continually pre-\ntrain BERT on the math corpus with similar pre-\ntraining tasks, and revises the self-attention layers\nfor encoding the OPT of formulas.\nImplementation Details. For baseline models, all\nhyper-parameters are set following the suggestions\nfrom the original papers. For all PLM-related mod-\nels, we implement them based on HuggingFace\nTransformers 4 (Wolf et al., 2020). For the models\n4https://huggingface.co/transformers/\n5928\nTasks KPC QAM QRC SQR\nMetrics Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro HR@3 NDCG@3\nTextCNN 51.2 31.7 91.6 91.6 75.1 55.8 0.321 0.301\nTextRCNN 56.8 40.3 89.3 89.2 80.3 62.9 0.334 0.317\nGAT 42.5 28.5 90.0 89.9 66.6 45.4 0.315 0.300\nR-GCN 40.7 26.0 91.6 91.5 70.4 50.0 0.316 0.298\nBERT-Base 59.4 36.0 96.8 96.8 82.3 63.1 0.578 0.576\nBERT+GAT 61.1 38.0 97.0 96.9 83.0 64.3 0.568 0.566\nDAPT-BERT 67.1 45.2 98.8 98.7 85.9 67.7 0.641 0.643\nDAPT-BERT+GAT 67.8 47.3 98.9 98.9 85.8 67.2 0.646 0.649\nMathBert 66.4 43.2 98.9 98.9 86.4 68.3 0.640 0.641\nCOMUS 72.6 57.9 99.5 99.5 88.9 81.4 0.658 0.660\nTable 2: Main results on four downstream tasks. The best and the second best methods are marked in bold and\nunderlined fonts respectively.\nTasks KPC\nRatio 40% 20% 10% 5%\nMethod Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro\nDAPT-BERT 53.1 27.9 38.6 15.2 26.4 7.7 16.8 4.2\nDAPT-BERT+GAT 53.3 27.5 38.3 15.5 26.2 6.8 11.8 2.5\nMathBERT 49.6 32.1 31.2 11.1 19.5 5.7 8.4 1.9\nCOMUS 62.7 41.5 52.2 27.8 36.9 15.0 22.1 7.1\nTasks QRC\nRatio 40% 20% 10% 5%\nMethod Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro\nDAPT-BERT 78.8 59.7 73.5 52.7 65.5 46.1 61.4 40.3\nDAPT-BERT+GAT 81.4 62.3 73.3 53.1 69.1 48.5 61.8 38.4\nMathBERT 80.5 60.9 73.3 47.9 65.6 38.3 58.0 22.6\nCOMUS 82.6 67.4 77.7 57.1 69.8 49.6 64.6 40.7\nTable 3: Performance comparison w.r.t. different amount of training data on KPC and QRC tasks.\ncombining PLM and GAT, we set GATâ€™s number\nof layer, attention head and hidden states as 6, 12\nand 64, respectively. And we set the number of\nsyntax-aware memory network layers k as 2 for\nour proposed COMUS.\nIn the continual pre-training stage, we initialize\nthe weights of all models with bert-base-chinese 5\nand pre-train them on our pre-training corpus with\nthe same hyper-parameter setting as follows. We\ncontinually pre-train the parameters with a total\nof 128 batch size for 100,000 steps. And the max\nlength of input sequences is set as 512. We use\nAdamW (Loshchilov and Hutter, 2019) optimiza-\ntion with Î²1 = 0.9, Î²2 = 0.999, and apply learning\nrate warmup over the first 5% steps, and linear de-\ncay of the learning rate. The learning rate is set\nas 1eâˆ’4. We set Ï„ as 0.07 for our TGCL tasks.\nIt costs about 40 hours to perform the continual\npre-training on 4 Tesla-V100-PCIE-32G GPUs.\nDuring fine-tuning on downstream tasks, we use\nAdamW with the same setting as pre-training. And\nbatch size for all experiments is set as 32. The\nlearning rate is set to 3eâˆ’5 for pre-training based\nmethods, and 1eâˆ’3 for other methods.\n5https://huggingface.co/bert-base-chinese\n4.2 Main Results\nThe results of all the comparison methods on four\ntasks are shown in Table 2. Based on these results,\nwe can find:\nAs for non-pre-training methods, text-based\nmethods (i.e., TextCNN and TextRCNN) outper-\nform GNN-based methods (i.e., GAT and R-GCN).\nIt indicates that text representations are more ca-\npable of understanding math problems than graph\nrepresentations in our dataset. Overall, non-pre-\ntraining methods perform worse than pre-training\nbased methods, since pre-training based models\nhave learned sufficient general knowledge during\nthe pre-training on large-scale corpus.\nAmong the five pre-training methods, we can\nhave two major findings. First, combining PLMs\nwith GNN yields performance improvement in\nmost cases. The reason is that GNN can capture\nthe structural semantics from formulas as the aux-\niliary information to help PLMs understand the\nmath problem, but the improvement is unstable,\nsince these methods simply concatenate the rep-\nresentations of the text and graph without deeply\nfusing them. Second, continual pre-training brings\na significant improvement on all the evaluation\n5929\nKPC QRC\nMethod Acc F1 Acc F1\nCOMUS 72.6 57.9 88.9 81.4\n- w/o GAT 69.4 49.2 87.9 78.3\n- w/o BERT 41.7 27.2 64.1 39.6\n- w/o Memory 69.4 49.2 88.1 73.7\n- w/o MLM 36.5 21.9 70.2 51.2\n- w/o DTC 70.8 55.3 87.8 73.5\n- w/o TGCL 71.9 56.5 87.9 69.8\nTable 4: Ablation study of our approach on the KPC\nand QRC tasks.\ntasks. General-purpose PLMs canâ€™t effectively un-\nderstand mathematical semantics, and it is the key\nto adapt them to the math domain via continual\npre-training.\nFinally, by comparing our approach with all the\nbaselines, it is clear to see that our model performs\nconsistently better than them on four tasks. We\nutilize the syntax-aware memory network to fuse\nand interact the representations of textual descrip-\ntions and formulas, and adopt three continual pre-\ntraining tasks to further align and enhance these\nrepresentations. Among these results, we can see\nthat our model achieves a large improvement on\nthe KPC task. A possible reason is that it requires\na deeper semantic fusion of formulas and text for\nidentifying the correct knowledge points.\n4.3 Few-shot Learning\nTo validate the reliability of our method under the\ndata scarcity scenarios, we conduct few-shot exper-\niments on KPC and QRC tasks by using different\nproportions of the training data, i.e., 5%, 10%, 20%\nand 40%. We compare our model with DAPT-\nBERT, DAPT-BERT+GAT and MathBERT.\nTable 3 shows the evaluation results with dif-\nferent ratios of training data. We can see that the\nperformance substantially drops when the size of\ntraining set is reduced. However, our model per-\nforms consistently better than the others across\ndifferent tasks and metrics. It demonstrates that our\nmodel is capable of leveraging the data more effec-\ntively with the help of the syntax-aware memory\nnetworks and continual pre-training tasks. With\n5% training data, our model exceeds the best base-\nline by a large margin. It further indicates that our\nmodel is more robust to the data scarcity problem.\n4.4 Ablation Study\nOur proposed approach contains several comple-\nmentary modules and pre-training tasks. Thus, we\nconduct experiments on KPC and QRC tasks to\n20k 40k 60k 80k 100k65.00\n71.25\n77.50\n83.75\n90.00\nKPC\nQRC\n(a) Pre-training Steps\n0 3 6 9 1265.00\n71.25\n77.50\n83.75\n90.00\nKPC\nQRC (b) GAT Layers\nFigure 3: Performance comparison w.r.t. the number of\npre-training steps and GAT layers\nverify the contribution of these modules and tasks.\nConcretely, we remove the module GAT, BERT,\nSyntax-Aware Memory Network, or the task MLM,\nDTC and TGCL, respectively.\nIn Table 4, we can see that the performance\ndrops by removing any modules or pre-training\ntasks. It shows the effectiveness of these mod-\nules or pre-training tasks in our proposed model.\nEspecially, the model performance significantly\ndecreases when we removing the textual encoder\nBERT, which implies that the text representations\nare more important for math problem understand-\ning. Besides, we can see that removing MLM also\nresults in a large performance drop, since it is the\nkey pre-training task for our text encoder.\n4.5 Hyper-Parameters Analysis\nOur proposed model contains a few parameters to\ntune. In this part, we tune two parameters and\nexamine their robustness on model performance,\ni.e., the number of GAT Layer and the continual\npre-training steps. We conduct experiments on\nKPC and QRC tasks and show the change curves\nof Accuracy in Figure 3.\nWe can observe that our model achieves the best\nperformance in 80k steps. It indicates that our\nmodel can be improved by continual pre-training\ngradually and may overfit after 80k steps. Besides,\nour model achieves the best performance with 6\nGAT layers, which shows that 6 GAT layers are suf-\nficient to capture the information in syntax graph.\n5 Related Work\nIn this section, we review the related work from the\nfollowing two aspects, namely math problem un-\nderstanding and continual pre-training of language\nmodels.\nMath Problem Understanding . Math problem\nunderstanding tasks focus on understanding the\ntexts, formulas and symbols in math domain. A\n5930\nsurge of works aim to understand the math for-\nmulas for problem solving or mathematical infor-\nmation retrieval. In a typical way, the formula is\nusually transformed as a tree or graph (e.g., Oper-\nator Tree (Zanibbi and Blostein, 2012)), then net-\nwork embedding method (Mansouri et al., 2019)\nand graph neural network (Song and Chen, 2021)\nare utilized to encode it. Besides, a number of\nworks focus on understanding math problem based\non the textual information. Among them, Math\nWord Problem (MWP) Solving is a popular task\nthat generates executable mathematical expression\nfor the math word problem to produce the final an-\nswer. Numerous deep learning based methods have\nbeen proposed to tackle the MWP task, including\nSeq2Seq (Chiang and Chen, 2019; Li et al., 2019),\nSeq2Tree (Wang et al., 2019; Qin et al., 2020), and\nPre-trained Language Models (Kim et al., 2020;\nLiang et al., 2021). More recently, several stud-\nies attempt to model more complex math prob-\nlems (Huang et al., 2020; Hendrycks et al., 2021)\nthat require a deep understanding of both textual\nand formula semantics.\nContinual Pre-training of Language Models .\nContinually pre-training can effectively improve\npre-trained modelâ€™s performance on new domains\nor downstream tasks (Gururangan et al., 2020). To\nachieve it, most of previous works either continu-\nally optimize the model parameters with BERT-like\ntasks on domain or task related corpus (e.g., scien-\ntific (Beltagy et al., 2019) and bio-media (Lee et al.,\n2020)), or design new pre-training objectives for\ntask adaption (e.g., commonsense reasoning (Zhou\net al., 2021) and dialogue adaption (Li et al., 2020)).\nBesides, several works (Wang et al., 2020; Xiang\net al., 2020) utilize both domain-related corpus\nand new pre-training objectives for continual pre-\ntraining, or revise the Transformer structure of\nPLMs for better adaption (Ghosal et al., 2020). For\nmath problem understanding, the recently proposed\nMathBERT (Peng et al., 2021) adopts math domain\ncorpus and formula-related pre-training tasks for\ncontinual pre-training.\n6 Conclusion and Future Work\nIn this paper, we proposed COMUS, a continual\npre-training approach for math problem understand-\ning. By integrating the formulas with the syntax\ntree of mathematical text, we constructed the math\nsyntax graph and designed the syntax-aware mem-\nory network to fuse the semantic information from\nthe text and formulas. In the memory network,\nwe treated tokens from the text and triplets from\nthe graph as the queries and slot entries, respec-\ntively, and modeled the semantic interaction be-\ntween tokens and their semantic-related nodes via\nmulti-view read and write operations. Besides, we\ndevised three continual pre-training tasks to fur-\nther enhance and align the representations of the\ntextual description and math syntax graph of the\nmath problem. Experimental results have shown\nthat our approach outperforms several competitive\nbaselines on four tasks in the math domain.\nIn future work, we will consider applying our\nmethod to solve more difficult math-related tasks,\ne.g., automatic math problem solving and analysis\ngeneration. Besides, we will also consider incor-\nporating external math domain knowledge into our\nmodel to improve the understanding of mathemati-\ncal logic and numerical reasoning.\nEthical Consideration\nIn this part, we discuss the main ethical considera-\ntion of this work: (1) Privacy. The data adopted in\nthis work (i.e., pre-training corpus and fine-tuning\ndata) is created by human annotation for research\npurposes, and should not cause privacy issues. (2)\nPotential Problems. PLMs have been shown to cap-\nture certain biases from their pre-trained data (Ben-\nder et al., 2021). There are increasing efforts to\naddress this problem in the community (Ross et al.,\n2021).\nAcknowledgement\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nand National Natural Science Foundation of China\nunder Grant No. 61872369, Beijing Outstand-\ning Young Scientist Program under Grant No.\nBJJWZYJH012019100020098, the Outstanding In-\nnovative Talents Cultivation Funded Programs 2021\nand Public Computing Cloud, Renmin University\nof China. This work is also supported by Beijing\nAcademy of Artificial Intelligence (BAAI). Xin\nZhao is the corresponding author.\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n5931\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615â€“\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610â€“623.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in neural information pro-\ncessing systems, 26.\nTing-Rui Chiang and Yun-Nung Chen. 2019.\nSemantically-aligned equation generation for\nsolving and reasoning math word problems.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2656â€“2668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171â€“4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDeepanway Ghosal, Devamanyu Hazarika, Abhinaba\nRoy, Navonil Majumder, Rada Mihalcea, and Sou-\njanya Poria. 2020. KinGDOM: Knowledge-Guided\nDOMain Adaptation for Sentiment Analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3198â€“\n3210, Online. Association for Computational Lin-\nguistics.\nSuchin Gururangan, Ana Marasovi Â´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Donâ€™t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342â€“8360, Online. Association for Computational\nLinguistics.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the MATH dataset.\nZhenya Huang, Qi Liu, Weibo Gao, Jinze Wu, Yu Yin,\nHao Wang, and Enhong Chen. 2020. Neural mathe-\nmatical solver with enhanced formula structure. In\nProceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1729â€“1732.\nBugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gah-\ngene Gweon. 2020. Point to the expression: Solving\nalgebraic word problems using the expression-pointer\ntransformer model. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 3768â€“3779.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746â€“1751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text clas-\nsification. In Twenty-ninth AAAI conference on artifi-\ncial intelligence.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234â€“1240.\nJierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian\nDai, and Dongxiang Zhang. 2019. Modeling intra-\nrelation in math word problems with different func-\ntional multi-head attentions. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6162â€“6167.\nJunlong Li, Zhuosheng Zhang, Hai Zhao, Xi Zhou,\nand Xiang Zhou. 2020. Task-specific objectives of\npre-trained language models for dialogue adaptation.\narXiv preprint arXiv:2009.04984.\nZhenwen Liang, Jipeng Zhang, Jie Shao, and Xian-\ngliang Zhang. 2021. Mwp-bert: A strong base-\nline for math word problems. arXiv preprint\narXiv:2107.13435.\nQi Liu, Zai Huang, Zhenya Huang, Chuanren Liu, En-\nhong Chen, Yu Su, and Guoping Hu. 2018. Finding\nsimilar exercises in online education systems. In\nProceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining,\npages 1821â€“1830.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nBehrooz Mansouri, Shaurya Rohatgi, Douglas W Oard,\nJian Wu, C Lee Giles, and Richard Zanibbi. 2019.\nTangent-cft: An embedding model for mathematical\nformulas. In Proceedings of the 2019 ACM SIGIR\ninternational conference on theory of information\nretrieval, pages 11â€“18.\nShuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang.\n2021. Mathbert: A pre-trained model for math-\nematical formula understanding. arXiv preprint\narXiv:2105.00377.\n5932\nJinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang,\nand Liang Lin. 2020. Semantically-aligned universal\ntree-structured solver for math word problems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3780â€“3789.\nAnja Reusch, Maik Thiele, and Wolfgang Lehner. 2021.\nTu_dbs in the arqmath lab 2021, clef.\nCandace Ross, Boris Katz, and Andrei Barbu. 2021.\nMeasuring social biases in grounded vision and lan-\nguage embeddings. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 998â€“1008, Online. Asso-\nciation for Computational Linguistics.\nMichael Sejr Schlichtkrull, Thomas N Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In ESWC.\nJia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil\nHeffernan, Xintao Wu, Ben Graff, and Dongwon Lee.\n2021. Mathbert: A pre-trained language model for\ngeneral nlp tasks in mathematics education. arXiv\npreprint arXiv:2106.07340.\nYujin Song and Xiaoyu Chen. 2021. Searching for\nmathematical formulas based on graph representation\nlearning. In International Conference on Intelligent\nComputer Mathematics, pages 137â€“152. Springer.\nPetar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro LiÃ², and Yoshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations.\nLei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu,\nLianli Gao, Bing Tian Dai, and Heng Tao Shen.\n2019. Template-based math word problem solvers\nwith recursive neural networks. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 33, pages 7144â€“7151.\nWeiran Wang, Qingming Tang, and Karen Livescu.\n2020. Unsupervised pre-training of bidirectional\nspeech encoders via masked reconstruction. In\nICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6889â€“6893. IEEE.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38â€“45, Online. Association\nfor Computational Linguistics.\nSuncheng Xiang, Yuzhuo Fu, Guanjie You, and Ting\nLiu. 2020. Unsupervised domain adaptation through\nsynthesis for person re-identification. In 2020 IEEE\nInternational Conference on Multimedia and Expo\n(ICME), pages 1â€“6. IEEE.\nRichard Zanibbi and Dorothea Blostein. 2012. Recogni-\ntion and retrieval of mathematical expressions. Inter-\nnational Journal on Document Analysis and Recog-\nnition (IJDAR), 15(4):331â€“357.\nWangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Sel-\nvam, Seyeon Lee, and Xiang Ren. 2021. Pre-training\ntext-to-text transformers for concept-centric common\nsense. In International Conference on Learning Rep-\nresentations.\n5933",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7800548076629639
    },
    {
      "name": "Syntax",
      "score": 0.7406187057495117
    },
    {
      "name": "Parsing",
      "score": 0.656749427318573
    },
    {
      "name": "Graph",
      "score": 0.5489044189453125
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5346452593803406
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5321259498596191
    },
    {
      "name": "Natural language processing",
      "score": 0.5191968679428101
    },
    {
      "name": "Abstract syntax",
      "score": 0.5126046538352966
    },
    {
      "name": "Focus (optics)",
      "score": 0.4937313497066498
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.4414014220237732
    },
    {
      "name": "Programming language",
      "score": 0.43425464630126953
    },
    {
      "name": "Construct (python library)",
      "score": 0.4244711697101593
    },
    {
      "name": "Security token",
      "score": 0.4197466969490051
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3553754389286041
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}