{
  "title": "nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?",
  "url": "https://openalex.org/W3175079695",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5002931312",
      "name": "Mihir Kale",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021329708",
      "name": "Aditya Siddhant",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5025234902",
      "name": "Rami Al‐Rfou",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5037677672",
      "name": "Linting Xue",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5090701958",
      "name": "Noah Constant",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5076150774",
      "name": "Melvin Johnson",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W2988395732",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3097879195",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W3035464238",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2998653650",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3118106810",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3169425228",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3085479580"
  ],
  "abstract": "Mihir Kale, Aditya Siddhant, Rami Al-Rfou, Linting Xue, Noah Constant, Melvin Johnson. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 683–691\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n683\nnmT5 - Is parallel data still relevant for pre-training massively\nmultilingual language models?\nMihir Kale∗ Aditya Siddhant∗ Rami Al-Rfou\nLinting Xue Noah Constant Melvin Johnson\nGoogle Research\nAbstract\nRecently, mT5 - a massively multilingual ver-\nsion of T5 - leveraged a uniﬁed text-to-text for-\nmat to attain state-of-the-art results on a wide\nvariety of multilingual NLP tasks. In this pa-\nper, we investigate the impact of incorporating\nparallel data into mT5 pre-training. We ﬁnd\nthat multi-tasking language modeling with ob-\njectives such as machine translation during pre-\ntraining is a straightforward way to improve\nperformance on downstream multilingual and\ncross-lingual tasks. However, the gains start to\ndiminish as the model capacity increases, sug-\ngesting that parallel data might not be as essen-\ntial for larger models. At the same time, even\nat larger model sizes, we ﬁnd that pre-training\nwith parallel data still provides beneﬁts in the\nlimited labelled data regime.\n1 Introduction\nRecent works have shown that cross-lingual trans-\nfer learning in pre-trained multilingual models such\nas mBERT (Devlin et al., 2019) and XLM-R (Con-\nneau et al., 2020) could be improved further by us-\ning parallel data (Conneau and Lample, 2019; Hu\net al., 2020a; Ouyang et al., 2020; Luo et al., 2020).\nIn this paper, we continue this line of work by im-\nproving the recent mT5 model (Xue et al., 2020) by\nleveraging parallel corpora. We experiment with\nseveral text-to-text objectives that incorporate par-\nallel data (spanning 198 language pairs) into mT5\npre-training. Our key ﬁndings are summarized be-\nlow:\n• In the regime of very small ﬁne-tuning\ndatasets, objectives with parallel data improve\nresults signiﬁcantly.\n• The gain from using parallel data decreases as\nwe scale up the size of the pre-trained model.\n∗Equal Contribution. Please direct correspondence to {\nmihirkale, adisid } @google.com\n• Simple objectives based on neural machine\ntranslation (NMT) perform better than the\ntraditionally employed “translation language\nmodeling” (TLM) objective.\n2 Method\nWe focus on the mT5-Large model, which is a 24\nlayer encoder-decoder transformer model and has\nshown strong performance on a variety of cross-\nlingual benchmarks (Xue et al., 2020). Instead\nof training a new model from scratch, we start\nfrom the publicly available mT5-Large checkpoint\n- which has been trained for over 1 trillion tokens\n- and do a second stage pre-training with a mix of\nmonolingual and parallel data.\n2.1 Objectives\nThe mT5 - multilingual version of T5 (Raffel et al.,\n2020) - series of models were pre-trained on a mul-\ntilingual version of the C4 corpus with a masked\nlanguage modeling “span-corruption” objective\n(Raffel et al., 2020), where the encoder is fed a\nchunk of text with random spans replaced with a\nmask token, and the decoder must reconstruct the\nmasked-out tokens. One of their primary distinc-\ntions is the use of a uniﬁed “text-to-text” format for\nall text-based NLP problems.\nIn keeping with the text-to-text format, we exper-\niment with the following objectives to incorporate\nparallel data into pre-training:\n• TLM - A text-to-text version of translation\nlanguage modeling, proposed by Conneau and\nLample (2019) and subsequently used in sev-\neral prior works for encoder only pre-training.\nWe trivially extend it to the encoder-decoder\nsetting.\n• NMT - Standard machine translation. The\ninput is the source text and the target is its\n684\nFigure 1: Example source and targets for different text-to-text style pre-training objectives incorporating parallel\ndata. All objectives except TLM specify target language in the source sentence.\ntranslation. A language code is preﬁxed to\nthe input to inform the model of the target\nlanguage (Johnson et al., 2017).\n• Denoised-NMT - Similar to NMT, but we\nadditionally mask spans in the source sen-\ntence. The model must now learn to implic-\nitly perform language modeling of the source\nlanguage while translating into the target lan-\nguage.\n• Denoised-NMT+LM - Similar to Denoised-\nNMT, but instead of implicit language mod-\neling, the model must explicitly predict the\nsource text in addition to the translation. The\ntarget is a concatenation of the translation and\nsource sentence, while the input is the masked\nsource sentence.\nWe refer to the model trained with the standard\nNMT objective as nmT5.\n3 Experiment Setup\nPre-training datasets For pre-training we use\nmonolingual data from mC4 (Xue et al., 2020) and\nparallel data from OPUS-100 (Zhang et al., 2020).\nOPUS-100 is a dataset of 55M translations cover-\ning 100 languages (198 language pairs, either into\nor from English). The mC4 corpus consists of un-\nlabeled web text covering 101 languages, of which\n81 overlap with the OPUS-100 languages.\nFine-tuning datasets For downstream evalua-\ntion, we use the following four tasks:\n• TyDi QA(Clark et al., 2020) - The GoldP\nsubtask, which corresponds to extractive ques-\ntion answering. The input is a passage and a\nquestion, with the answer being a span from\nthe passage.\n• MTOP (Li et al., 2020) - Multilingual Task-\nOriented Parsing. The task is one of structured\nDataset Langs Train size Setting\nTyDi QA 9 3.7K zero-shot\nMTOP 6 22K zero-shot\nWikiAnn NER 40 20K zero-shot\nWikiLingua 18 660K multilingual\nTable 1: Statistics of datasets used in the paper.\nprediction, where user queries must be parsed\ninto a tree, capturing the domain, intent and\nslots.\n• WikiAnn NER (Pan et al., 2019) - Named\nentity recognition task covering 40 languages\nfeatured in the XTREME benchmark (Hu\net al., 2020b). There are 4 categories of enti-\nties - location, person, organization and mis-\ncellaneous.\n• WikiLingua (Ladhak et al., 2020) - A re-\ncently introduced cross-lingual summariza-\ntion dataset, where a document from an arbi-\ntrary language must be summarized in English.\nSince the dataset does not come with training\nand evaluation splits, we randomly create val-\nidation and test sets of 1000 examples each,\nand the rest of the data is used for training.\nTable 1 lists further details of each dataset. Fol-\nlowing Xue et al. (2020), all tasks are cast into\nthe text-to-text format. The evaluation for TyDi\nQA, MTOP and NER is done in the zero-shot set-\nting, where the model is trained on the English data\nand evaluated on all languages. Since zero-shot\ncross-lingual language generation is much harder,\nfor WikiLingua we train the model in a multilin-\ngual setting, using available training data for all\nlanguages.\nHyperparameters Pre-training is done with a\nbatch size of 1M tokens and ﬁne-tuning with\n131,072 tokens, with a constant learning rate of\n685\nModel TyDi QA MTOP NER WikiLingua Avg.\n(Metric) (F1/EM) (EM) (F1) (ROUGE-L)\nmT5 66.3 / 49.8 43.7 58.4 25.2 46.3\n+MLM (additional 100K steps)71.3 / 55.6 48.6 59.9 26.1 49.5\n+MLM+TLM 71.1 / 54.6 48.6 61.4 26.1 49.7\n+MLM+NMT 75.1 / 60.1 57.7 61.4 27.4 53.5\n+MLM+denoised NMT 75.3 / 60.2 56.5 61.5 27.4 53.3\n+MLM+denoised NMT-LM 75.0 / 59.4 56.0 62.4 26.9 53.1\nTable 2: Results are averaged across all the languages in each dataset. We report F1/EM for QA, exact match\naccuracy (EM) for structured prediction, ROUGE-L (Lin, 2004) for summarization and F1 for NER. Each score is\nthe median over ﬁve runs. The ﬁnal columns lists the average of all the scores. Refer to Appendix A for scores on\nindividual languages.\n0.001. Starting from publicly available mT5-Large\ncheckpoints, we further pre-train for 100K steps\nwith a mix of monolingual and parallel objectives.\nThe parallel data is mixed into monolingual data\nat a 10% ratio, which amounts to roughly 4 passes\nover the OPUS-100 corpus. Examples from each\nlanguage pair are sampled using the same language\nsampling distribution as Xue et al. (2020), with\nalpha=0.3. For downstream tasks, we ﬁne-tune\nfor 10K steps for TyDiQA, MTOP, NER and 25K\nfor WikiLingua, since it is a much larger dataset.\nCheckpoint selection is done based on the valida-\ntion set.\nBaselines Our ﬁrst baseline is the publicly avail-\nable mT5-Large model (1.3 billion parameters).\nFor a fair comparison, we also experiment with\nan mT5 model further pre-trained for 100k steps\nwith only monolingual data from mC4 (see row 2:\nmT5+MLM in Table 2). This lets us assess whether\nimprovements stem from using parallel data or just\npre-training for longer.\n4 Results\nWe report results in table 2. Overall, adding parallel\ndata through neural machine translation objectives\nimproves scores for all 4 tasks, with the NMT ob-\njective performing the best.\nSimply pre-training mT5 for longer with just\nmonolingual data (MLM) leads to improved scores\nfor all tasks. The TLM objective is not be able to ef-\nfectively leverage the parallel data and performs on\npar with MLM. On the other hand, our three NMT-\nbased objectives show gains over MLM across all\ntasks. Among these, NMT and Denoised-NMT\nare the best and perform similarly, while Denoised-\nNMT+LM fares slightly worse. Averaged across\nall tasks, NMT and Denoised-NMT outperform\nMLM by 4 points.\n4.1 Model size\nXue et al. (2020) ﬁnd that cross-lingual perfor-\nmance of language models increases monotonically\nwith model size. To study the impact of model\ncapacity, we also experiment with larger model\nsizes. Even at the XL size (3.7B params, 3× larger\nthan mT5-Large), we observe gains for all tasks\nwith nmT5 (Table 3). However, the magnitude of\nthe gains is largely diminished, hinting that the\nneed for parallel data reduces as model capacity\nincreases. This ﬁnding is particularly promising\nfor low-resource languages, where it is difﬁcult to\nobtain high-quality parallel data.\nAt the same time, nmT5-Large substantially re-\nduces the performance gap between mT5-Large\nand mT5-XL, covering 70% of the headroom.\nSince bigger models are expensive to train and\neven more expensive to deploy, this opens up av-\nenues for effectively using parallel data to improve\nperformance of smaller language models. Turc\net al. (2019) found that pre-training student models\nbefore model distillation is helpful, and using par-\nallel data to improve student pre-training is another\ninteresting avenue of future work.\nModel TyDi QA MTOP NER WikiLingua Avg.\nmT5-Large 66.3 / 49.8 43.7 58.4 25.2 46.3\nnmT5-Large75.1 / 60.1 57.7 61.4 27.4 53.5\n∆ 8.8 / 10.3 14.0 3.0 2.2 7.2\nmT5-XL 77.8 / 61.8 63.4 65.5 27.9 56.7\nnmT5-XL 78.4 / 63.3 64.9 66.2 28.4 57.6\n∆ 0.6 / 1.5 1.5 0.7 0.5 0.9\nTable 3: Impact of model size on nmT5’s performance.\n686\nModel Few-Shot (100) Low (3.7K) High (80K)\nmT5-Large 33.1 / 23.6 66.3 / 49.8 78.1 / 64.8\nnmT5-Large48.8 / 37.1 75.1 / 60.1 78.2 / 65.5\n∆ 15.7 / 13.5 8.8 / 10.3 0.1 / 0.7\nmT5-XL 45.0 / 31.7 77.8 / 61.8 78.7 / 65.8\nnmT5-XL 57.2 / 44.4 78.4 / 63.3 79.7 / 67.0\n∆ 12.2 / 12.7 0.6 / 1.5 1.0 / 1.2\nTable 4: Performance on the TyDi QA eval set when\nﬁne-tuned in the few-shot (100 examples from TyDi\nQA English), low (full TyDi QA English with 3.7K ex-\namples) and high data regime (SQuAD English with\n80K examples).\n4.2 Limited labeled data\nThe TyDi QA dataset has only 3.7K English train-\ning examples. To study the impact of the size of\nﬁne-tuning data, we run experiments in two addi-\ntional settings: a few-shot regime and a high data\nregime. Few-shot uses just 100 randomly sam-\npled training examples, while for the latter we use\nthe much larger SQuAD corpus (Rajpurkar et al.,\n2016), which consists of 80k examples.\nWhen ﬁne-tuned with SQuAD, nmT5 performs\nslightly better than mT5 for both Large and XL\nmodel sizes. However, in the few-shot setting,\nnmT5-Large improves over mT5-Large by 15\npoints. Even at the XL size, nmT5 is over 10 points\nhigher than mT5. nmT5-Large even outperforms\nthe much larger mT5-XL. Our experiments suggest\nthat pre-training with parallel data is particularly\nuseful in the limited labelled data setting.\n4.3 Mixing ratio\nSo far, we have mixed parallel data into mono-\nlingual data at a 10% ratio. To assess how the\nmixing ratio impacts performance, we compare re-\nsults with a 50% mix. With the 50% mix, average\nperformance is slightly lower, validating our initial\nchoice.\nMix TyDi QA MTOP NER WikiLingua Avg.\n10% 75.1 / 60.1 57.7 61.4 27.4 53.5\n50% 76.5 / 60.1 53.9 62.0 26.5 52.7\nTable 5: Impact of mixing ratio on nmT5.\n4.4 Performance on unseen languages\nWe also test downstream performance on languages\npreviously unseen by the models. We randomly\npick 30 languages from the WikiAnn NER dataset\nthat are not covered in either mC4 1 or OPUS, and\nhence none of our models have seen them during\npre-training. Table 6 shows nmT5 outperforms\nmT5 on this subset of languages as well, indicating\nthat the representations of the nmT5 model are\nbetter suited for cross-lingual transfer.\nModel ckb hsb xmf “Avg.”\nmT5-Large 66.5 64.8 58.4 54.9\nnmT5-Large 72.2 69.8 62.2 57.4\n∆ 5.7 5.0 3.8 2.5\nTable 6: Performance on three randomly picked unseen\nlanguages. “Avg.” is calculated by averaging perfor-\nmance across 30 unseen languages.\n5 Related Work\nPre-trained multilingual models such as mBERT\nand XLM-R have shown to be effective at cross-\nlingual transfer learning (Devlin et al., 2019; Con-\nneau et al., 2020). Subsequently, many attempts\nhave leveraged parallel data to improve cross-\nlingual capability of these models. Conneau and\nLample (2019) proposed translation language mod-\neling (TLM), to encourage the model to align repre-\nsentations across languages. Alternating language\nmodeling (Yang et al., 2020) and back-translation\nmasked language modeling (Ouyang et al., 2020)\nused code-switched sentences and back-translation\nrespectively to utilize parallel data. Other works\nusing parallel data in this line of work include FIL-\nTER (Fang et al., 2020), AMBER (Hu et al., 2020a)\nand, MMTE (Siddhant et al., 2020). A key factor\nthat differentiates this paper from these works is\nthat our pre-trained models use a text-to-text ar-\nchitecture, having both an encoder and a decoder,\nwhile the aforementioned models only have the\nencoder. Other pretrained multilingual encoder-\ndecoder models such as mT5 (Xue et al., 2020),\nmBART (Liu et al., 2020) and MASS (Song et al.,\n2019) do not make use of parallel data during pre-\ntraining.\n6 Conclusion\nIn this work we attempted to improve mT5 pre-\ntraining by incorporating parallel data. We exper-\nimented with various text-to-text objectives and\nfound that multi-tasking with the standard neural\nmachine translation objective during pre-training\n1Subject to precision of language ID models used for mC4.\n687\nleads to improved cross-lingual transfer. The im-\nprovements from parallel data are most pronounced\nin the limited labeled data scenario. Our experi-\nments also indicate that smaller models, with the\nhelp of parallel data, can approach the performance\nof larger ones, while also suggesting that the need\nfor parallel data is lesser as the model capacity\nincreases.\nReferences\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in ty po-\nlogically di verse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, ´Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun,\nand Jingjing Liu. 2020. Filter: An enhanced fu-\nsion method for cross-lingual language understand-\ning. arXiv preprint arXiv:2009.05166.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2020a. Explicit align-\nment objectives for multilingual bidirectional en-\ncoders. arXiv preprint arXiv:2010.07972.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020b.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalisation. In\nInternational Conference on Machine Learning.\nM. Johnson, Mike Schuster, Quoc V . Le, M. Krikun,\nY . Wu, Z. Chen, Nikhil Thorat, F. Vi´egas, M. Watten-\nberg, G. S. Corrado, Macduff Hughes, and J. Dean.\n2017. Google’s multilingual neural machine transla-\ntion system: Enabling zero-shot translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 5:339–351.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and\nK. McKeown. 2020. Wikilingua: A new benchmark\ndataset for cross-lingual abstractive summarization.\nArXiv, abs/2010.03093.\nHaoran Li, A. Arora, Shuohui Chen, Anchit Gupta,\nSonal Gupta, and Yashar Mehdad. 2020. Mtop: A\ncomprehensive multilingual task-oriented semantic\nparsing benchmark. ArXiv, abs/2008.09335.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVeco: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-\nm: Enhanced multilingual representation by align-\ning cross-lingual semantics with monolingual cor-\npora. arXiv preprint arXiv:2012.15674.\nXiaoman Pan, Thamme Gowda, Heng Ji, Jonathan\nMay, and Scott Miller. 2019. Cross-lingual joint\nentity and word embedding to improve entity link-\ning and parallel sentence mining. In Proceedings\nof the 2nd Workshop on Deep Learning Approaches\nfor Low-Resource NLP (DeepLo 2019) , pages 56–\n66, Hong Kong, China. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP.\nAditya Siddhant, Melvin Johnson, Henry Tsai, Naveen\nAri, Jason Riesa, Ankur Bapna, Orhan Firat, and\nKarthik Raman. 2020. Evaluating the cross-lingual\neffectiveness of massively multilingual neural ma-\nchine translation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence.\n688\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. In Interna-\ntional Conference on Machine Learning.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv: Computation and Language.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nJian Yang, Shuming Ma, Dongdong Zhang, ShuangZhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\n689\nA Per-Language Results on All Tasks\nen ar bn ﬁ id\nmt5 75.0 / 63.0 68.9 / 51.4 54.5 / 37.2 70.4 / 54.6 74.3 / 57.0\n+MLM 78.5 / 68.2 76.1 / 59.9 59.0 / 40.7 73.5 / 61.0 76.7 / 60.0\n+MLM+TLM 77.3 / 67.0 75.7 / 57.2 61.7 / 39.8 73.3 / 59.0 77.0 / 60.0\n+MLM+NMT 78.4 / 69.3 78.9 / 63.1 74.0 / 54.9 77.0 / 64.8 79.9 / 64.8\n+MLM+denoised NMT 78.7 / 68.6 79.8 / 64.7 72.6 / 53.1 77.2 / 64.2 79.8 / 67.6\n+MLM+denoised NMT-LM 78.2 / 68.2 78.8 / 62.3 69.1 / 49.6 78.2 / 65.7 79.6 / 64.8\nko ru sw te avg\nmt5 57.4 / 47.5 61.5 / 37.1 69.7 / 52.5 65.5 / 48.0 66.3 / 49.8\n+MLM 64.4 / 55.4 68.6 / 48.9 74.2 / 57.7 71.1 / 48.6 71.3 / 55.6\n+MLM+TLM 66.5 / 55.8 67.8 / 48.0 73.9 / 57.1 66.5 / 47.5 71.1 / 54.6\n+MLM+NMT 64.9 / 56.2 72.1 / 51.8 77.2 / 63.1 73.3 / 53.1 75.1 / 60.1\n+MLM+denoised NMT 67.9 / 58.7 71.9 / 51.5 75.7 / 59.7 74.3 / 53.5 75.3 / 60.2\n+MLM+denoised NMT-LM 67.8 / 59.4 72.7 / 51.1 76.0 / 59.9 74.4 / 54.0 75.0 / 59.4\nTable 7: TyDi QA GoldP results (F1/EM) for each language.\nen de es fr hi th avg\nmt5 83.5 41.2 45.4 43.3 21.3 27.5 43.7\n+MLM 83.3 44.5 46.3 51.8 31.9 34.0 48.6\n+MLM+TLM 85.0 42.4 47.5 49.6 31.8 35.2 48.6\n+MLM+NMT 86.1 55.1 59.0 61.7 42.2 42.1 57.7\n+MLM+denoised NMT 85.8 51.6 55.2 59.5 42.7 43.9 56.5\n+MLM+denoised NMT-LM 85.9 51.9 55.0 57.0 44.1 41.9 56.0\nTable 8: MTOP results (EM) for each language.\n690\nen af ar bg bn de el es et eu fa ﬁ fr he\nmt5 80.5 64.5 47.7 57.2 66.5 67.0 63.9 62.0 59.0 45.5 41.4 56.9 76.7 45.1\n+MLM 81.4 65.1 50.2 55.2 69.3 68.6 66.9 70.5 62.8 46.6 44.9 58.9 76.6 46.4\n+MLM+TLM 82.4 65.6 48.8 67.2 72.2 70.1 70.8 72.6 61.2 47.5 47.1 61.4 78.7 48.0\n+MLM+NMT 82.2 64.2 56.7 61.0 69.1 70.5 64.6 66.3 66.2 49.3 48.9 60.6 78.4 46.2\n+MLM+denoised NMT 82.5 65.7 50.3 63.6 69.6 70.7 68.6 73.7 64.9 48.6 44.3 63.3 77.7 45.5\n+MLM+denoised NMT-LM82.9 66.1 49.5 67.7 74.5 71.1 71.3 74.2 67.1 49.9 44.8 63.2 80.2 49.6\nhi hu id it ja jv ka kk ko ml mr ms my nl\nmt5 66.8 57.7 44.9 75.4 36.0 46.0 53.0 22.5 29.5 44.8 38.6 65.5 27.0 77.3\n+MLM 66.5 61.4 46.2 76.4 35.8 49.0 53.6 23.7 31.4 46.0 39.3 67.4 33.0 78.5\n+MLM+TLM 69.6 61.9 47.2 76.7 37.3 51.0 59.4 29.3 30.7 48.2 42.1 70.2 29.0 80.4\n+MLM+NMT 69.8 61.7 46.1 77.3 34.5 53.0 55.2 27.0 31.4 43.0 46.7 69.0 27.0 78.9\n+MLM+denoised NMT 65.8 63.0 46.6 77.6 37.0 54.0 58.3 26.4 29.8 44.8 42.1 64.3 30.0 80.2\n+MLM+denoised NMT-LM67.7 64.4 48.1 77.9 39.2 49.0 59.4 30.0 31.4 47.4 36.4 71.0 34.0 80.2\npt ru sw ta te th tl tr ur vi yo zh avg\nmt5 73.1 48.4 66.8 39.9 37.9 8.5 77.8 57.6 45.1 76.4 58.0 41.8 58.4\n+MLM 75.5 47.3 64.5 40.5 38.0 9.2 76.9 56.5 51.7 76.9 59.0 41.8 59.9\n+MLM+TLM 76.3 58.8 66.3 40.2 41.2 8.8 76.9 62.0 43.0 79.6 56.0 43.5 61.4\n+MLM+NMT 75.5 56.0 65.8 40.3 41.6 8.0 78.7 60.3 57.0 79.8 63.0 41.0 61.4\n+MLM+denoised NMT 75.5 58.9 66.2 40.4 40.4 7.9 78.7 60.5 50.0 80.3 64.0 41.4 61.5\n+MLM+denoised NMT-LM78.6 60.9 65.6 40.6 40.9 9.1 77.0 63.1 53.5 79.8 60.0 45.5 62.4\nTable 9: WikiAnn NER results (F1) for each language.\nen ar cs de es fr hi id it ja\nmt5 29.2 23.2 22.4 25.0 25.3 24.6 25.2 25.3 24.1 26.2\n+MLM 30.0 24.0 22.9 26.0 26.6 25.5 26.1 25.8 24.9 27.8\n+MLM+TLM 30.0 24.4 23.1 25.6 26.3 25.6 26.4 25.8 25.1 27.6\n+MLM+NMT 31.5 25.7 24.0 27.0 27.5 26.4 27.7 27.0 25.8 29.5\n+MLM+denoised NMT 31.3 25.7 24.7 27.3 27.5 26.8 27.8 27.2 25.8 29.2\n+MLM+denoised NMT-LM 30.8 25.0 23.7 26.5 27.1 26.3 27.3 26.7 25.6 28.7\nko nl pt ru th tr vi zh avg\nmt5 23.8 25.7 24.6 23.9 25.3 30.9 22.9 25.8 25.2\n+MLM 25.2 26.5 25.3 24.6 27.1 31.1 23.2 27.1 26.1\n+MLM+TLM 24.7 26.6 25.2 24.4 26.5 31.3 23.3 27.0 26.1\n+MLM+NMT 26.7 27.7 26.3 25.9 28.6 34.1 23.9 28.1 27.4\n+MLM+denoised NMT 26.6 28.0 25.9 25.8 28.3 33.4 24.3 28.4 27.4\n+MLM+denoised NMT-LM 25.9 27.4 25.6 24.9 27.3 33.1 23.8 27.8 26.9\nTable 10: Wikilingua results (Rouge-L) for each language.\n691\nace arz ast ba ce ckb csb eml fur gan gn\nmt5-Large 44.8 50.8 83.3 38.1 21.7 66.5 56.7 39.8 64.2 42.1 48.2\nnmt5-Large 46.7 53.6 84.8 43.7 28.3 72.2 58.1 41.9 65.6 41.2 51.0\nhsb ia jbo lij lmo min nap nov pdc pms pnb\nmt5-Large 64.8 63.2 42.1 46.3 69.8 39.1 62.2 62.1 48.1 81.5 61.1\nnmt5-Large 69.8 62.4 43.6 43.0 72.0 45.5 61.7 66.7 51.2 83.5 55.4\nrm sa tl qu vec vep vls xmf avg\nmt5-Large 64.1 17.4 78.6 27.5 66.9 63.6 74.4 58.4 54.9\nnmt5-Large 67.6 23.0 79.4 35.6 66.7 68.0 77.5 62.2 57.4\nTable 11: WikiAnn NER results on unseen languages. Refer to section 4.4",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7572641372680664
    },
    {
      "name": "Massively parallel",
      "score": 0.6085840463638306
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5902730226516724
    },
    {
      "name": "Computational linguistics",
      "score": 0.5465975403785706
    },
    {
      "name": "Constant (computer programming)",
      "score": 0.5414353609085083
    },
    {
      "name": "Natural language processing",
      "score": 0.5048080682754517
    },
    {
      "name": "Joint (building)",
      "score": 0.4630465805530548
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4354020953178406
    },
    {
      "name": "Programming language",
      "score": 0.42807114124298096
    },
    {
      "name": "Association (psychology)",
      "score": 0.4131571054458618
    },
    {
      "name": "Parallel computing",
      "score": 0.29449090361595154
    },
    {
      "name": "Engineering",
      "score": 0.09977340698242188
    },
    {
      "name": "Philosophy",
      "score": 0.0803709626197815
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 10
}