{
  "title": "Tools used to assess the quality of peer review reports: a methodological systematic review",
  "url": "https://openalex.org/W2938357171",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2617603756",
      "name": "Cecilia Superchi",
      "affiliations": [
        "Inserm",
        "Centre de Recherche Épidémiologie et Statistique",
        "Université Paris Cité",
        "Sorbonne Paris Cité",
        "Universitat Politècnica de Catalunya",
        "Délégation Paris 5"
      ]
    },
    {
      "id": "https://openalex.org/A2098511200",
      "name": "José Antonio González",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A2100905204",
      "name": "Ivan Solà",
      "affiliations": [
        "Iberoamerican Cochrane Centre",
        "Hospital de Sant Pau",
        "Centro de Investigación Biomédica en Red de Epidemiología y Salud Pública"
      ]
    },
    {
      "id": "https://openalex.org/A2045421052",
      "name": "Erik Cobo",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A2738476340",
      "name": "Darko Hren",
      "affiliations": [
        "University of Split"
      ]
    },
    {
      "id": "https://openalex.org/A108308545",
      "name": "Isabelle Boutron",
      "affiliations": [
        "Hôtel-Dieu de Paris"
      ]
    },
    {
      "id": "https://openalex.org/A2617603756",
      "name": "Cecilia Superchi",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "Centre de Recherche Épidémiologie et Statistique",
        "FC Barcelona",
        "Université Paris Cité",
        "Sorbonne Paris Cité",
        "Inserm",
        "Délégation Paris 5"
      ]
    },
    {
      "id": "https://openalex.org/A2098511200",
      "name": "José Antonio González",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100905204",
      "name": "Ivan Solà",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2045421052",
      "name": "Erik Cobo",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "FC Barcelona"
      ]
    },
    {
      "id": "https://openalex.org/A2738476340",
      "name": "Darko Hren",
      "affiliations": [
        "University of Split"
      ]
    },
    {
      "id": "https://openalex.org/A108308545",
      "name": "Isabelle Boutron",
      "affiliations": [
        "Hôtel-Dieu de Paris"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2038579646",
    "https://openalex.org/W2095455968",
    "https://openalex.org/W4241559800",
    "https://openalex.org/W1970502313",
    "https://openalex.org/W2065873852",
    "https://openalex.org/W2084698925",
    "https://openalex.org/W2026506934",
    "https://openalex.org/W2461645408",
    "https://openalex.org/W2096644655",
    "https://openalex.org/W2022419394",
    "https://openalex.org/W2043169285",
    "https://openalex.org/W1953064436",
    "https://openalex.org/W1578561495",
    "https://openalex.org/W2413426462",
    "https://openalex.org/W4294215472",
    "https://openalex.org/W2171251365",
    "https://openalex.org/W2531708643",
    "https://openalex.org/W2007701081",
    "https://openalex.org/W2130453155",
    "https://openalex.org/W2118856847",
    "https://openalex.org/W2021882344",
    "https://openalex.org/W2032478179",
    "https://openalex.org/W2041212401",
    "https://openalex.org/W4301115652",
    "https://openalex.org/W2143702204",
    "https://openalex.org/W1997515562",
    "https://openalex.org/W2551250719",
    "https://openalex.org/W2104924738",
    "https://openalex.org/W2124539370",
    "https://openalex.org/W2149244042",
    "https://openalex.org/W1971896716",
    "https://openalex.org/W2073066685",
    "https://openalex.org/W2072291958",
    "https://openalex.org/W2054100472",
    "https://openalex.org/W2095301612",
    "https://openalex.org/W1965416708",
    "https://openalex.org/W2023111124",
    "https://openalex.org/W2122027830",
    "https://openalex.org/W2051556928",
    "https://openalex.org/W2182443631",
    "https://openalex.org/W2098923148",
    "https://openalex.org/W1983639327",
    "https://openalex.org/W2892053593",
    "https://openalex.org/W2901749906",
    "https://openalex.org/W2044591801",
    "https://openalex.org/W2142900202",
    "https://openalex.org/W2759665011",
    "https://openalex.org/W1976461435",
    "https://openalex.org/W6600340491",
    "https://openalex.org/W1966519075",
    "https://openalex.org/W2156098321",
    "https://openalex.org/W2792585846",
    "https://openalex.org/W1515587369",
    "https://openalex.org/W246286872",
    "https://openalex.org/W2015225687",
    "https://openalex.org/W2588681363",
    "https://openalex.org/W4299588845",
    "https://openalex.org/W2058527576",
    "https://openalex.org/W2991792334",
    "https://openalex.org/W4255828633",
    "https://openalex.org/W2012950673",
    "https://openalex.org/W4285719527"
  ],
  "abstract": null,
  "full_text": "RESEARCH ARTICLE Open Access\nTools used to assess the quality of peer\nreview reports: a methodological\nsystematic review\nCecilia Superchi 1,2,3* , José Antonio González 1, Ivan Solà 4,5, Erik Cobo 1, Darko Hren 6 and Isabelle Boutron 7\nAbstract\nBackground: A strong need exists for a validated tool that clearly defines peer review report quality in biomedical\nresearch, as it will allow evaluating interventions aimed at improving the peer review process in well-performed\ntrials. We aim to identify and describe existing tools for assessing the quality of peer review reports in biomedical\nresearch.\nMethods: We conducted a methodological systematic review by searching PubMed, EMBASE (via Ovid) and The\nCochrane Methodology Register (via The Cochrane Library) as well as Google® for all reports in English describing a\ntool for assessing the quality of a peer review report in biomedical research. Data extraction was performed in\nduplicate using a standardized data extraction form. We extracted information on the structure, development and\nvalidation of each tool. We also identified quality components across tools using a systematic multi-step approach\nand we investigated quality domain similarities among tools by performing hierarchical, complete-linkage clustering\nanalysis.\nResults: We identified a total number of 24 tools: 23 scales and 1 checklist. Six tools consisted of a single item and\n18 had several items ranging from 4 to 26. None of the tools reported a definition of ‘quality’. Only 1 tool described\nthe scale development and 10 provided measures of validity and reliability. Five tools were used as an outcome in\na randomized controlled trial (RCT). Moreover, we classified the quality components of the 18 tools with more than\none item into 9 main quality domains and 11 subdomains. The tools contained from two to seven quality domains.\nSome domains and subdomains were considered in most tools such as the detailed/thorough (11/18) nature of\nreviewer’s comments. Others were rarely considered, such as whether or not the reviewer made comments on the\nstatistical methods (1/18).\nConclusion: Several tools are available to assess the quality of peer review reports; however, the development and\nvalidation process is questionable and the concepts evaluated by these tools vary widely. The results from this\nstudy and from further investigations will inform the development of a new tool for assessing the quality of peer\nreview reports in biomedical research.\nKeywords: Peer review, Quality control, Methods, Report, Systematic review\n* Correspondence: cecilia.superchi@upc.edu\n1Department of Statistics and Operations Research, Barcelona-Tech, UPC, c/\nJordi Girona 1-3, 08034 Barcelona, Spain\n2INSERM, U1153 Epidemiology and Biostatistics Sorbonne Paris Cité Research\nCenter (CRESS), Methods of therapeutic evaluation of chronic diseases Team\n(METHODS), F-75014 Paris, France\nFull list of author information is available at the end of the article\n© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 \nhttps://doi.org/10.1186/s12874-019-0688-x\nBackground\nThe use of editorial peer review originates in the eighteenth\ncentury [1]. It is a longstanding and established process that\ngenerally aims to provide a fair decision-making mecha-\nnism and improve the quality of a submitted manuscript\n[2]. Despite the long history and application of the peer\nreview system, its efficacy is still a matter of controversy\n[3– 7]. About 30 years after the first international Peer Re-\nview Congress, there are still ‘scarcely any bars to eventual\npublication. There seems to be no study too fragmented,\nno hypothesis too trivial [... ] for a paper to end up in print ’\n(Drummond Rennie, chair of the advisory board) [ 8].\nRecent evidence suggests that many current editors\nand peer reviewers in biomedical journals still lack the\nappropriate competencies [ 9]. In particular, it has been\nshown that peer reviewers rarely receive formal training\n[3]. Moreover, their capacity to detect errors [ 10, 11],\nidentify deficiencies in reporting [ 12] and spin [ 13]h a s\nbeen found lacking.\nSome systematic reviews have been performed to esti-\nmate the effect of interventions aimed at improving the\npeer review process [ 2, 14, 15]. These studies showed\nthat there is still a lack of evidence supporting the use of\ninterventions to improve the quality of the peer review\nprocess. Furthermore, Bruce and colleagues highlighted\nthe urgent need to clarify outcomes, such as peer review\nreport quality, that should be used in randomized con-\ntrolled trials evaluating these interventions [ 15].\nA validated tool that clearly defines peer review report\nquality in biomedical research is greatly needed. This\nwill allow researchers to have a structured instrument to\nevaluate the impact of interventions aimed at improving\nthe peer review process in well-performed trials. Such a\ntool could also be regularly used by editors to evaluate\nthe work of reviewers.\nHerein, as starting point for the development of a new\ntool, we identify and describe existing tools that assess the\nquality of peer review reports in biomedical research.\nMethods\nStudy design\nWe conducted a methodological systematic review and\nfollowed the standard Preferred Reporting Items for Sys-\ntematic Review and Meta-Analysis (PRISMA) guidelines\n[16]. The quality of peer review reports is an outcome\nthat in the long term is related to clinical relevance and\npatient care. However, the protocol was not registered in\nPROSPERO, as this review does not contain direct\nhealth-related outcomes [ 17].\nInformation sources and search strategy\nWe searched PubMed, EMBASE (via Ovid) and The\nCochrane Methodology Register (via The Cochrane\nLibrary) from their inception to October 27, 2017 as well\nas Google® (search date: October 20, 2017) for all reports\ndescribing a tool to assess the quality of a peer review\nreport in biomedical research. Search strategies were\nrefined in collaboration with an expert methodologist\n(IS) and are presented in the Additional file 1.W e\nhand-searched the citation lists of included papers and\nconsulted a senior editor with expertise in editorial\npolicies and peer review processes to further identify\nrelevant reports.\nEligibility criteria\nWe included all reports describing a tool to assess the\nquality of a peer review report. Sanderson and colleagues\ndefined a tool as ‘any structured instrument aimed at\naiding the user to assess the quality [...] ’ [18]. Building\non this definition, we defined a quality tool as any struc-\ntured or unstructured instrument assisting the user to\nassess the quality of peer review report (for definitions\nsee Table 1). We restricted inclusion to the English\nlanguage.\nStudy selection\nWe exported the references retrieved from the search\ninto the reference manager Endnote X7 (Clarivate Ana-\nlytics, Philadelphia, United States), which was subse-\nquently used to remove duplicates. We reviewed all\nrecords manually to verify and remove duplicates that\nhad not been previously detected. A reviewer (CS)\nscreened all titles and abstracts of the retrieved citations.\nA second reviewer (JAG) carried out quality control on\na 25% random sample obtained using the statistical soft-\nware R 3.3.3 [ 19]. We obtained and independently exam-\nined the full-text copies of potentially eligible reports for\nfurther assessment. In the case of disagreement, consen-\nsus was determined by a discussion or by involving a\nthird reviewer (DH). We reported the result of this\nTable 1 Definition of terms used in the present study\nStructured quality tool: scale or checklist including more than one item\naimed at guiding the user to assess the overall quality of a peer review\nreport.\nUnstructured quality tool: scale or checklist including only one item\ninquiring the overall quality of a peer review report.\nItems: elements of a scale or checklist representing a component of\npeer review report quality. Items in a scale could or could not have an\nattached numerical score. If there is no attached score, these items\nprovide the evaluator with a guidance to assess the overall quality of a\npeer review report.\nOverall quality score in a scale is measured as:\n Sum of scores: score obtained by summing all scores for each item\npresent in a scale.\n Mean of scores: score obtained by dividing the sum of scores for each\nitem with the total number of items included in the tool.\n Single score: score obtained in those scales based on a single item.\n Summary score: score obtained in those scales with more than one\nitem deriving from a question inquiring the overall quality of peer\nreview report.\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 2 of 14\nprocess through a PRISMA flowchart [ 16]. When several\ntools were reported in the same article, they were included\nas separate tools. When a tool was reported in more than\none article, we extracted data from all related reports.\nData extraction\nGeneral characteristics of tools\nWe designed a data extraction form using Google® Docs\nand extracted the general characteristics of the tools. We\ndetermined whether the tool was scale or checklist. We\ndefined a tool as a scale when it included a numeric or\nnominal overall quality score while we considered it as a\nchecklist when an overall quality score was not present.\nWe recorded the total number of items (for definitions\nsee Table 1). For scales with more than 1 item we\nextracted how items were weighted, how the overall\nscore was calculated, and the scoring range. Moreover,\nwe checked whether the scoring instructions were\nadequately defined, partially defined, or not defined\naccording to the subjective judgement of two reviewers\n(CS and JAG) (an example of the definition for scoring\ninstructions is shown in Table 2). Finally, we extracted\nall information related to the development, validation,\nand assessment of the tool ’s reliability and if the concept\nof quality was defined.\nTwo reviewers (CS and JAG) piloted and refined the\ndata extraction form on a random 5% sample of\nextracted articles. Full data extraction was conducted by\ntwo reviewers (CS and JAG) working independently for\nall included articles. In the case of disagreement, consen-\nsus was obtained by discussion or by involving a third\nreviewer (DH). Authors of the reports were contacted in\ncases where we needed further clarification of the tool.\nQuality components of the peer review report considered in\nthe tools\nWe followed the systematic multi-step approach recently\ndescribed by Gentles [ 20], which is based on a constant\ncomparative method of analysis developed within the\nGrounded Theory approach [ 21]. Initially, a researcher\n(CS) extracted all items included in the tools and for\neach item identified a ‘key concept ’ representing a qua-\nlity component of peer review reports. Next, two\nresearchers (CS and DH) organized the key concepts\ninto a domain-specific matrix (analogous to the\ntopic-specific matrices described by Gentles). Initially,\nthe matrix consisted of domains for peer review report\nquality, followed by items representative of each domain\nand references to literature sources that items were ex-\ntracted from. As the analysis progressed, subdomains\nwere created and the final version of the matrix included\ndomains, subdomains, items and references.\nFurthermore, we calculated the proportions of\ndomains based on the number of items included in each\ndomain for each tool. According to the proportions\nobtained, we created a domain profile for each tool.\nThen, we calculated the matrix of Euclidean distances\nbetween the domain profiles. These distances were used\nto perform the hierarchical, complete-linkage clustering\nanalysis, which provided us with a tree structure that we\nrepresent in a chart. Through this graphical summary,\nwe were able to identify domain similarities among\nthe different tools, which helped us draw our\nTable 2 Examples of definition of scoring system instructions\nScoring system instructions\nDefined Partially defined Not defined\n5 (Exceptional) = The rare outstanding critique that is\ncomprehensive, objective, and insightful. Evaluates purpose of\nthe study, study design, scientific validity, and conclusions by\nnumbering questions and constructive suggestions to be\naddressed by the author. Includes comments to the editor about\nwhether this is something new and important and useful to our\nreaders.\n4 (Very good) = Excellent review indicating that the paper was\ncarefully evaluated. Helpful comments to the author and editor\nwith well-documented reasons for decision.\n3 (Good) = Useful type of very satisfactory review. Analysis not as\nwell organized, documented, or as complete as above but is\nreasonable, with adequate comments for the authors.\n2 (Below average) = Very brief, superficial evaluation. Reasons for\nthe decision not explained and comments to authors not helpful.\n1 (Unacceptable) = Such a poor review that consideration should\nbe given to not sending further papers to this reviewer. Reasons\ncould include evidence of bias, unfair, faulty reasoning, or\nevaluation (totally disagrees with the opinion of other reviewers\nand editor) and comments to author either absent, inappropriate,\nor inadequate to explain how the paper was rated.\n(Landkroon 2006) [ 42]\n1 (Poor) = Does not follow reviewer guideline structure or\npreferred formatting in providing comments; unfavourable\ntimeliness.\n2 (Acceptable) = Comments are somewhat helpful; review meets\ntimeline.\n3 (Reliable) = Thorough and helpful comments; timely\nsubmission.\n4 (Excellent) = Very strong and detailed comments; review was\nsubmitted early or on time; comments enhance the manuscript ’s\nmerit and relevance in the field.\n(Rajesh 2013) [ 32]\n1 = poor;\n2 = fair;\n3 = good;\n4 = excellent\n(Friedam1995)\n[22]\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 3 of 14\nanalytical conclusions. The calculations and graphical\nrepresentations were obtained using the statistical\nsoftware R 3.3.3 [ 19].\nResults\nStudy selection and general characteristics of reports\nThe screening process is summarized in a flow diagram\n(Fig.1). Of the 4312 records retrieved, we finally included\n46 reports: 39 research articles; 3 editorials; 2 infor-\nmation guides; 1 was a letter to the editor and 1 study\nwas available only as an abstract (excluded studies are\nlisted in Additional file 2; included studies are listed in\nAdditional file 3).\nGeneral characteristics of the tools\nIn the 46 reports, we identified 24 tools, including 23\nscales and 1 checklist. The tools were developed from\n1985 to 2017. Four tools had from 2 to 4 versions [ 22– 25].\nFive tools were used as an outcome in a randomized con-\ntrolled trial [ 23, 25– 28]. Table 3 lists the general character-\nistics of the identified tools. Table 4 presents a more\ncomplete descriptive summary of the tools ’characteristics,\nincluding types and measures of validity and reliability.\nSix scales consisted of a single item enquiring into the\noverall quality of the peer review report, all of them\nbased on directly asking users to score the overall quality\n[22, 25, 29– 32]. These tools assessed the quality of a\npeer review report by using: 1) a 4 or 5 Likert point scale\n(n = 4); 2) as ‘good’, ‘fair’ and ‘poor’ (n = 1); and 3) a re-\nstricted scale from 80 to 100 (n = 1). Seventeen scales\nand one checklist had several items ranging in number\nfrom 4 to 26. Of these, 10 used the same weight for each\nitem [ 23, 24, 27, 28, 33– 38]. The overall quality score\nwas the sum of the score for each item ( n = 3); the mean\nof the score of the items ( n = 6); or the summary score\n(n = 11) (for definitions see Table 1). Three scales re-\nported more than one way to assess the overall quality\n[23, 24, 36]. The scoring system instructions were not\ndefined in 67% of the tools.\nNone of the tools reported the definition of peer\nreview report quality, and only one described the tool\ndevelopment [ 39]. The first version of this tool was\ndesigned by a development group composed of four\nresearchers and three editors. It was based on a tool\nused in an earlier study and that had been developed by\nreviewing the literature and interviewing editors. Succes-\nsively, the tool was modified by rewording some\nFig. 1 Study selection flow diagram\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 4 of 14\nquestions after some group discussions and a guideline\nfor using the tool was drawn up.\nOnly 3 tools assessed and reported a validation\nprocess [ 39– 41]. The assessed types of validity included\nface validity, content validity, construct validity, and\npreliminary criterion validit y. Face and content validity\ncould involve either a sole editor and author or a group\nof researchers and editors. Construct validity was\nassessed with multiple regression analysis using dis-\ncriminant criteria (reviewer characteristics such as age,\nsex, and country of residence) and convergent criteria\n(training in epidemiology and/or statistics); or the over-\nall assessment of the peer review report by authors and\nan assessment of ( n =4 – 8) specific components of the\npeer review report by editors or authors. Preliminary\ncriterion was assessed by comparing grades obtained by\nan editor to those obtained by an editor-in-chief using\nan earlier version of the tool. Reliability was assessed in\n9t o o l s[ 24– 27, 31, 36, 39, 41, 42]; all reported\ninter-rater reliability and 2 also reported test-retest reli-\nability. One tool reported the internal consistency mea-\nsured with the Cronbach ’sa l p h a[ 39].\nQuality components of the peer review reports\nconsidered in the tools with more than one item\nWe extracted 132 items included in the 18 tools. One\nitem asking for the percentage of co-reviews the re-\nviewer had graded was not included in the classification\nbecause it represented a method of measuring\nreviewer’s performance and not a component of peer\nreview report quality.\nWe organized the key concepts from each item into\n‘topic-specific matrices ’ (Additional file 4), identifying\nnine main domains and 11 subdomains: 1) relevance of\nstudy ( n = 9); 2) originality of the study ( n = 5); 3) inter-\npretation of study results ( n = 6); 4) strengths and weak-\nnesses of the study ( n = 12) (general, methods and\nstatistical methods); 5) presentation and organization of\nthe manuscript ( n = 8); 6) structure of the reviewer ’s\ncomments ( n = 4); 7) characteristics of reviewer ’s com-\nments ( n = 14) (clarity, constructiveness, detail/tho-\nroughness, fairness, knowledgeability, tone); 8) timeliness\nof the review report ( n =7 ) ; a n d 9 ) u s e f u l n e s s o f t h e\nreview report ( n = 10) (decision making and manuscript\nimprovement). The total number of tools corresponding\nto each domain and subdomain is shown in Fig. 2.A n\nexplanation and example of all domains and subdomains\nis provided in Table 5. Some domains and subdomains\nwere considered in most tools, such as whether the\nreviewers’ comments were detailed/thorough (n = 11) and\nconstructive (n = 9), whether the reviewers ’ comments\nwere on the relevance of the study (n = 9) and if the peer\nreview report was useful for manuscript improvement\n(n = 9). However, other items were rarely considered,\nsuch as whether the reviewer made comments on the\nstatistical methods (n =1 ) .\nClustering analysis among tools\nWe created a domain profile for each tool. For example,\nthe tool developed by Justice et al. consisted of 5 items\n[35]. We classified three items under the domain ‘Cha-\nracteristics of the reviewer ’s comments ’, one under ‘Time-\nliness of the review report ’ and one under ‘Usefulness of\nthe review report ’. According to the aforementioned clas-\nsification, the domain profile (represented by propor-\ntions of domains) for this tool was 0.6:0.2:0.2 for the\nincorporating domains and 0 for the remaining ones.\nThe hierarchical clustering used the matrix of Euclidean\ndistances among domain profiles, which led to five main\nclusters (Fig. 3).\nThe first cluster consisted of 5 tools developed from\n1990 to 2016. All tools included at least one item in the\nTable 3 Main characteristics of the included tools\nCharacteristics of tools N (%)\nType of tool:\nScale 23 (96%)\nChecklist 1 (4%)\nNumber of items:\n1 6 (25%)\n> 1 18 (75%)\nWeight of items a:\nSame weight 10 (42%)\nDifferent weight 2 (8%)\nUser defined weight 1 (4%)\nNot applicable 11 (46%) a\nScore System Instruction:\nDefined 5 (21%)\nPartially defined 3 (12%)\nNot defined 16 (67%)\nTool development:\nReported 1 (4%)\nNot reported 23 (96%)\nOverall quality assessment b\nSingle score 6 (22%)\nSummary score 11 (41%)\nMean score 6 (22%)\nSum score 3 (11%)\nNot reported 1 (4%)\naItem weight is not applicable for scale with a single item ( n = 6), checklist ( n\n= 1) and for scale including more than one item without a numerical score\nattached but presenting only a summary score ( n =4 )\nbThe total number is different because three tools presented more than one\nway to assess the overall quality and the checklist did not provide an\noverall score\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 5 of 14\nTable 4 Descriptive characteristics of tools used to assess the quality of a peer review report\nJournal or Company Name a First Author,\nYear\nFormat Quality\ndefined b\nOverall\nquality\nassessment\nItems\n(n)\nItems\nweights c\nScoring\nrange d\nScoring\nsystem\ninstruction e\nScale/\nChecklist\nDevelopment f\nValidity g Reliability h Internal c\nonsistency\nRCTs\ni\nAdvances in Nursing Science; Issues in\nMental Health Nursing; The Journal of\nHolistic Nursing\nShattell 2010\n[33]\nScale N Summary\nScore\n6S 1 – 10 N NR NR NR NR 0\nAmerican Journal of Roentgenology Friedman 1995\n[22]\nScale N Single\nScore\n1N A 1 – 4 N NR NR NR NR 0\nAmerican Journal of Roentgenology Kliewer 2005\n[49]\nScale N Summary\nScore\n4N A 1 – 4 N NR NR NR NR 0\nAmerican Journal of Roentgenology Rajesh 2013\n[32]\nScale N Single\nScore\n1N A 1 – 4 P NR NR NR NR 0\nAmerican Journal of Roentgenology Berquist 2017\n[50]\nScale N Summary\nScore\n4N A 0 – 4 Y NR NR NR NR 0\nAnnals of Emergency Medicine Callaham 1998\n[25]\nScale N Single\nScore\n1N A 1 – 5 N NR NR Inter-Rater\n(ICC = 0.44,\n0.24, 0.12) l\nNR 2 m\nAnnals of Emergency Medicine Callaham 2002\n[26, 51]\nScale N Summary\nScore\n6N A 1 – 5 N NR NR Inter-Rater\n(ICC = 0.44,\n0.24, 0.12) l\nNR 1\nAnnals of Emergency Medicine; Annals\nof Internal Medicine; JAMA; Obstetrics\n& Gynecology and Ophthalmology\nJustice 1998\n[35]\nScale N Summary\nScore\n4S 1 – 5 N NR NR NR NR 0\nBritish Journal of General Practice Moore 2014\n[29]\nScale N Single\nScore\n1 NA A-E Y NR NR NR 0\nBritish Medical Journal Black 1998 (RQI\n3.2) [ 23, 39]\nScale N Summary\nScore\n7S 1 – 5 N Y Face ( N = 20) Test-Retest\n(Kw = 1.00)\nInternal\nConsistency\n(Cronbach’s\nalpha = 0.84)\n5\nMean Content ( N =\n20)\nConstruct\nInter-Rater\n(Kw = 0.83)\nBritish Medical Journal Van Rooyen\n1999 (RQI 4)\n[27]\nScale N Mean n 8S 1 – 5 N NR NR Inter-Rater\n(Kw =\n0.38– 0.67) o\n2\nChinese Journal of Tuberculosis\nand Respiratory Diseases\nYang 2009 [ 52] Checklist N NA 5 NA NA N NR NR NR 0\nJournal of Clinical Investigation Stossel\n1985 [ 30]\nScale N Single\nScore\n1 NA Good-\nFair-\nPoor\nYN R N R N R 0\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 6 of 14\nTable 4 Descriptive characteristics of tools used to assess the quality of a peer review report (Continued)\nJournal or Company Name a First Author,\nYear\nFormat Quality\ndefined b\nOverall\nquality\nassessment\nItems\n(n)\nItems\nweights c\nScoring\nrange d\nScoring\nsystem\ninstruction e\nScale/\nChecklist\nDevelopment f\nValidity g Reliability h Internal c\nonsistency\nRCTs\ni\nJournal of General Internal Medicine McNutt 1990\n[28, 40]\nScale N Summary\nScore\n9S 1 – 5 N NR Construct NR 1\nJournal of Vascular\nInterventional Radiology\nFeurer 1994\n[41]\nScale N Sum 7 D 0 – 14 N NR Content ( N =2 )\nPreliminary\nCriterion\n(N =2 )\n(Kendall =\n0.94)\nInter-Rater\n(ICC = 0.84)\n0\nNA Review quality\ncollector (RQC)\n2012 [ 53]\nScale N Mean 4 User-\ndefined\nweights\n0– 100 N NR NR NR 0\nNursing Research Henly 2009 [ 24] Scale N Mean (CAS,\nGAS scale)\n15 S 1 – 5 P NR NR Inter-Rater\n(ICC = 0.79)p\n0\nSummary\nScore (OAS scale)\n1– 5\nSummary\nScore (GRQ\nscale)\n0– 100\nNursing Research Henly 2010 [ 36] Scale N Mean (CAS,\nGAR,\nSARNR scale)\n26 S 1 – 5 P NR NR Inter-Rater\n(ICC =\n0.75)p\n0\nSummary\nScore (GRQ\nscale)\n0– 100\nObstetrics & Gynecology,\nDutch Journal of Medicine\nLandkroon\n2006 [ 42]\nScale N Summary\nScore\n5N A 1 – 5 Y NR NR Test-Retest\n(ICC =0.66 –\n0.88)\nInter-Rater\n(ICC = 0.62)\n0\nPakistan Journal of\nMedical Sciences\nJawaid 2006 [ 34] Scale N NR q 5S 1 – 5 N NR NR NR 0\nPeerage of science Peerage Essay\nQuality (PEQ)\n2011 [ 37]\nScale N Mean 3 S 1 – 5 N NR NR NR 0\nPublons Academy Review Rating\nand Feedback\nForm 2016 [ 38]\nScale N Sum 4 S 0 – 3\n(Full\nscore:\n0– 12)\nNN R N R N R 0\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 7 of 14\nTable 4 Descriptive characteristics of tools used to assess the quality of a peer review report (Continued)\nJournal or Company Name a First Author,\nYear\nFormat Quality\ndefined b\nOverall\nquality\nassessment\nItems\n(n)\nItems\nweights c\nScoring\nrange d\nScoring\nsystem\ninstruction e\nScale/\nChecklist\nDevelopment f\nValidity g Reliability h Internal c\nonsistency\nRCTs\ni\nThe Journal of Bone and\nJoint Surgery\nThompson\n2016 [ 31]\nScale N Single Score 1 NA 80 – 100 Y NR NR Inter-Rater\n(ICC = -4.5\nto 0.99) r\n0\nThe National Medical\nJournal of India\nDas Sinha\n1999 [ 54]\nScale N Sum 5 D 0 – 100 N NR NR NR 0\naName of journal or company/organization where the tool was used to assess the quality of their peer review reports\nbThe quality of a peer review report is not clearly defined in any reports\ncNA Not applicable, S Same weight for each item, D Different weight for each item\ndNA Not applicable\neY Yes defined, P Partially defined, N Not defined\nf, g, h NR Not reported\niNumber of randomized controlled trials where the tool was used as outcome criteria\nlThe ICC was 0.44 for reviewers, 0.24 for editors, and 0.12 for manuscripts\nmOne article consists of two studies. First study is not a RCT while the second one is a RCT [ 55]\nnThe overall quality is based on the mean of the first seven items (the item about the tone of the review was not included)\noThe inter-rater reliability was measured with weighted K for item from 1 to 7 for two editors ’ independent assessments\npThe tool includes more than one scale. We reported inter-rater reliability only for General Review Quality (GRQ) scale\nqNot reported. Although the authors reported that the reviewers were rated as excellent, good and average based on the quality of the reviews, it is not r eported how they assessed the overall quality\nof peer review reports\nrICC range for 11 manuscripts. There was one outlier manuscript that if removed brought the range to 0.87 – 0.99\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 8 of 14\ncharacteristics of the reviewer ’s comments domain, repre-\nsenting at least 50% of each domain profile. In the\nsecond cluster, there were 3 tools developed from 1994\nto 2006. These tools were characterized to incorporate\nat least one item in the usefulness and timeliness do-\nmains. The third cluster included 6 tools that had been\ndeveloped from 1998 to 2010 and exhibited the most\nheterogeneous mix of domains. These tools were distinct\nfrom the rest because they encompassed items related to\ninterpretation of the study results and originality of the\nstudy. Moreover, the third cluster included two tools\nwith different versions and variations. The first, second,\nand third cluster were linked together in the hierarchical\ntree that presented tools with at least one quality com-\nponent grouped in the domain characteristics of the\nreviewer’s comments. In the fourth cluster, there are 2\ntools developed from 2011 to 2017 that consist of at\nleast one component in the strengths and weaknesses\ndomain. Finally, the fifth cluster included 2 tools deve-\nloped from 2009 to 2012 and which consisted of the\nsame 2 domains. The fourth and fifth clusters were sepa-\nrated from the rest in the hierarchical tree that\npresented tools with only a few domains.\nDiscussion\nTo the best of our knowledge, this is the first compre-\nhensive review that has systematically identified tools\nused in biomedical research for assessing the quality of\npeer review reports. We have identified 24 tools from\nboth the medical literature and an internet search: 23\nscales and 1 checklist. One out of four tools consisted of\na single item that simply asked the evaluator for a direct\nassessment of the peer review report ’s ‘overall quality ’.\nThe remaining tools had between 4 to 26 items in which\nthe overall quality was assessed as the sum of all items,\ntheir mean, or as a summary score.\nSince a definition of overall quality was not provided,\nthese tools consisted exclusively of a subjective quality\nassessment by the evaluators. Moreover, we found that\nonly one study reported a rigorous development process\nof the tool, although it included a very limited number\nof people. This is of concern because it means that the\nidentified tools were, in fact, not suitable to assess the\nquality of a peer review report, particularly because they\nlack a focused theoretical basis. We found 10 tools that\nwere evaluated for validity and reliability; in particular,\ncriterion validity was not assessed for any tool.\nFig. 2 Frequency of quality domains and subdomains\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 9 of 14\nTable 5 Explanations and Examples of quality domains and subdomains\nN Domains Subdomains Explanations and Examples\n1 Relevance of the study Explanation: Items inquiring if the reviewer has discussed in the peer review report the\nimportance of the research question and usefulness of the study.\nExample: ‘Did the reviewer give appropriate attention to the importance of the question? ’ [28]\n2 Originality of the study Explanation: Items inquiring if the reviewer has commented in the peer review report on the\noriginality of the manuscript.\nExample: ‘Did the reviewer discuss the originality of the paper? ’ [23, 27]\n3 Interpretation of the\nstudy results\nExplanation: Items inquiring if the reviewer has commented in the peer review report on how\nauthors interpreted and discussed the results of the study.\nExample: ‘The reviewer commented accurately and productively on the quality of the author ’s\ninterpretation of the data, including acknowledgment of the data ’s limitations. ’ [26]\n4 Strengths and weaknesses\nof the study\nGeneral Explanation: Items inquiring if the reviewer has identified and commented in the peer review\nreport on the general strong and weak points of the study.\nExample: ‘How well it identified the study ’s strengths and weaknesses? ’ [35]\nMethods Explanation: Items inquiring if the reviewer has identified and commented in the peer review\nreport on the strong and weak points specifically related to study ’s methods\nExample: ‘Did the reviewer clearly identify strengths and weaknesses in the study ’s\nmethods?’ [28]\nStatistical\nmethods\nExplanation: Items inquiring if the reviewer has identified and commented in the peer review\nreport on the strong and weak points specifically related to study ’s statistical methods\nExample: ‘Confidence intervals/ p-values/overall fit ’ [36]\n5 Presentation and organization\nof the manuscript\nExplanation: Items inquiring if the reviewer has made comments in the peer review report on\nthe data presentation such as tables and figures and on the organization of the manuscript\nsuch as writing communication.\nExample: ‘Are there any constructive suggestions on improvement of a. writing; b. data\npresentation and c. interpretation ’ [54]\n6 Structure of reviewer ’s\ncomments\nExplanation: Items inquiring if the reviewer has made in the peer review report organized and\nstructured comments.\nExample: ‘Concise well-organized comments to the editor ’ [50]\n7 Characteristics of reviewer ’s\ncomments\nClarity Explanation: Items inquiring if the reviewer has provided in the peer review report clear and\neasily to read comments.\nExample: ‘How clear was this review? The review was easily read and interpreted by the editor\nand authors. ’ [38]\nConstructiveness Explanation: Items inquiring if the reviewer has provided in the peer review report helpful,\nrelevant and realistic comments.\nExample: ‘Were the reviewer ’s comments constructive? ’ [23, 27]\nDetail/\nThoroughness\nExplanation: Items inquiring if the reviewer has provided in the peer review report detailed and\nthorough comments supplying appropriate evidence.\nExample: ‘Detail of commentary ’ [33]\nFairness Explanation: Items inquiring if the reviewer has provided in the peer review report balanced\nand objective comments.\nExample: ‘Balanced/fair’ [24, 36]\nKnowledgeability Explanation: Items inquiring if the reviewer has showed in the peer review report to know and\nunderstand correctly the content of the manuscript.\nExample: ‘Knowledge of the manuscript ’s content area. ’ [28]\nTone Explanation: Items inquiring if the reviewer has used a courteous tone in the peer review\nreport.\nExample: ‘Overall tone of the reviewers was also assessed as harsh or courteous. ’ [34]\n8 Timeliness of the r\neview report\nExplanation: Items inquiring if the reviewer has completed the peer review report on time.\nExample: ‘Punctuality of the review ’ [49]\n9 Usefulness of the\nreview report\nDecision making Explanation: Items inquiring if the reviewer has provided a peer review report useful to make a\ndecision about the acceptance, revision or rejection of a manuscript\nExample: ‘The reviewer provided the editor with the proper context and perspective to make\na decision about acceptance or revision of the manuscript. ’ [26]\nManuscript\nimprovement\nExplanation: Items inquiring if the reviewer has provided useful suggestions in the peer review\nreport to improve the manuscript.\nExample: ‘This aspect is solely interested in how well the review aids the authors for improving\ntheir work and/or writing. Whether the review makes a good judgment regarding acceptance\nof the submission plays no role here whatsoever. ’ [53]\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 10 of 14\nMost of the scales with more than one item resulted\nin a summary score. These scales did not consider how\nitems could be weighted differently. Although commonly\nused, scales are controversial tools in assessing quality\nprimarily because using a score ‘in summarization\nweights’ would cause a biased estimation of the mea-\nsured object [ 43]. It is not clear how weights should be\nassigned to each item of the scale [ 18]. Thus different\nweightings would produce different scales, which could\nprovide varying quality assessments of an individual\nstudy [ 44].\nn our methodological systematic review, we found only\none checklist. However, it was neither rigorously devel-\noped nor validated and therefore we could not consider it\nadequate for assessing peer review report quality. We be-\nlieve that checklists may be a more appropriate means for\nassessing quality because they do not present an overall\nscore, meaning they do not require a weight for the items.\nIt is necessary to clearly define what the tool mea-\nsures. For example, the Risk of Bias (RoB) tool [ 45]\nhas a clear aim (to assess trial conduct and not\nreporting), and it provides a detailed definition of\neach domain in the tool, including support for judg-\nment. Furthermore, it was developed with transparent\nprocedures, including wide consultation and review of\nthe empirical evidence. Bias and uncertainty can arise\nwhen using tools that are not evidence-based, rigor-\nously developed, validated and reliable; and this is\nparticularly true for tools that are used for evaluating\ninterventions aimed at improving the peer review\nprocess in RCTs, thus affecting how trial results are\ninterpreted.\nWe found that most of the items included in the\ndifferent tools did not cover the scientific aspects of a\npeer review report nor were constrained to biomedical\nresearch. Surprisingly, few tools included an item related\nFig. 3 Hierarchical clustering of tools based on the nine quality domains. The figure shows which quality domains are present in each tool. A\nslice of the chart represents a tool, and each slice is divided into sectors, indicating quality domains (in different colours). The area of each secto r\ncorresponds to the proportion of each domain within the tool. For instance, the “Review Rating ” tool consists of two domains: Timeliness,\nmeaning that 25% of all its items are encompassed in this domain, and Characteristics of reviewer ’s comments occupying the remaining 75%. The\nblue lines starting from the centre of the chart define how the tools are divided into the five clusters. Clusters #1, #2 and #3 are sub-nodes of a\nmajor node grouping all three, meaning that the tools in these clusters have a similar domain profile compared to the tools in clusters #4 and #5\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 11 of 14\nto the methods used in the study, and only one inquired\nabout the statistical methods.\nIn line with a previous study published in 1990 [ 28],\nwe believe that the quality components found across all\ntools could be further organized according to the\nperspective of either an editor or author, specifically by\ntaking into account the different yet complementary uses\nof a peer review report. For instance, reviewer ’s com-\nments on the relevance of the study and interpretation of\nthe study ’s results could assist editors in making an\neditorial decision, clarity and detail/thoroughness of\nreviewer’s comments are important attributes which help\nauthors improve manuscript quality. We plan to further\ninvestigate the perspectives of biomedical editors and\nauthors towards the quality of peer review reports by\nconducting an international online survey. We will also\ninclude patient editors as survey ’s participants as their\ninvolvement in the peer review process can further\nensure that research manuscripts are relevant and\nappropriate to end-users [ 46].\nThe present study has strengths but also some limita-\ntions. Although we implemented a comprehensive\nsearch strategy for reports by following the guidance for\nconducting methodological reviews [ 20], we cannot ex-\nclude a possibility that some tools were not identified.\nMoreover, we limited the eligibility criteria to reports\npublished only in English. Finally, although the number\nof eligible records we identified through Google® was\nvery limited, it is possible that we introduced selection\nbias due to a (re)search bubble effect [ 47].\nDue to the lack of a standard definition of quality, a\nvariety of tools exist for assessing the quality of a peer\nreview report. Overall, we were able to establish 9 qua-\nlity domains. Between two to seven domains were used\namong each of the 18 tools. The variety of items and\nitem combinations amongst tools raises concern about\nvariations in the quality of publications across bio-\nmedical journals. Low-quality biomedical research im-\nplies a tremendous waste of resources [ 48] and explicitly\naffects patients ’ lives. We strongly believe that a vali-\ndated tool is necessary for providing a clear definition of\npeer review report quality in order to evaluate interven-\ntions aimed at improving the peer review process in\nwell-performed trials.\nConclusions\nThe findings from this methodological systematic review\nshow that the tools for assessing the quality of a peer re-\nview report have various components, which have been\ngrouped into 9 domains. We plan to survey a sample of\neditors and authors in order to refine our preliminary\nclassifications. The results from further investigations\nwill allow us to develop a new tool for assessing the\nquality of peer review reports. This in turn could be\nused to evaluate interventions aimed at improving the\npeer review process in RCTs. Furthermore, it would help\neditors: 1) evaluate the work of reviewers; 2) provide\nspecific feedback to reviewers; and 3) identify reviewers\nwho provide outstanding review reports. Finally, it might\nbe further used to score the quality of peer review\nreports in developing programs to train new reviewers.\nAdditional files\nAdditional file 1: Search strategies. (PDF 182 kb)\nAdditional file 2: Excluded studies. (PDF 332 kb)\nAdditional file 3: Included studies. (PDF 244 kb)\nAdditional file 4: Classification of peer review report quality\ncomponents. (PDF 2660 kb)\nAbbreviations\nPRISMA: Preferred Reporting Items for Systematic Reviews; RCT: Randomized\ncontrolled trials; RoB: Risk of Bias\nAcknowledgments\nThe authors would like to thank the MiRoR consortium for their support,\nElizabeth Moylan for helping to identify further relevant reports and Melissa\nSharp for providing advice during the writing of this article.\nFunding\nThis project was supported by the European Union ’s Horizon 2020 research\nand innovation programme under the Marie Sklodowska-Curie grant agree-\nment no 676207. The funders had no role in the study design, data collec-\ntion and analysis, decision to publish, or preparation of the manuscript.\nAvailability of data and materials\nThe datasets supporting the conclusions of the present study will be\navailable in the Zenodo repository in the Methods in Research on Research\n(MiRoR) community [ https://zenodo.org/communities/miror/\n?page=1&size=20].\nAuthors’ contributions\nAll authors provided intellectual contributions to the development of this\nstudy. CS, EC and IB had the initial idea and with JAG and DH, designed the\nstudy. CS designed the search in collaboration with IS. CS conducted the\nscreening and JAG carried out a quality control of a 25% random sample. CS\nand JAG conducted the data extraction. CS conducted the analysis and with\nJAG designed the figures. CS led the writing of the manuscript. IB led the\nsupervision of the manuscript preparation. All authors provided detailed\ncomments on earlier drafts and approved the final manuscript.\nEthics approval and consent to participate\nNot required.\nConsent for publication\nNot applicable.\nCompeting interests\nAll authors have completed the ICMJE uniform disclosure form at http://\nwww.icmje.org/coi_disclosure.pdf (available on request from the\ncorresponding author) and declare that (1) no authors have support from\nany company for the submitted work; (2) IB is the deputy director of French\nEQUATOR that might have an interest in the work submitted; (3) no author ’s\nspouse, partner, or children have any financial relationships that could be\nrelevant to the submitted work; and (4) none of the authors has any non-\nfinancial interests that could be relevant to the submitted work.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 12 of 14\nAuthor details\n1Department of Statistics and Operations Research, Barcelona-Tech, UPC, c/\nJordi Girona 1-3, 08034 Barcelona, Spain. 2INSERM, U1153 Epidemiology and\nBiostatistics Sorbonne Paris Cité Research Center (CRESS), Methods of\ntherapeutic evaluation of chronic diseases Team (METHODS), F-75014 Paris,\nFrance. 3Paris Descartes University, Sorbonne Paris Cité, Paris, France.\n4Iberoamerican Cochrane Centre, Hospital de la Santa Creu i Sant Pau, C/\nSant Antoni Maria Claret 167, Pavelló 18 - planta 0, 08025 Barcelona, Spain.\n5CIBER de Epidemiología y Salud Pública (CIBERESP), Madrid, Spain.\n6Department of Psychology, Faculty of Humanities and Social Sciences,\nUniversity of Split, Split, Croatia. 7Centre d ’épidémiologie Clinique, Hôpital\nHôtel-Dieu, 1 place du Paris Notre-Dame, 75004 Paris, France.\nReceived: 11 July 2018 Accepted: 20 February 2019\nReferences\n1. Kronick DA. Peer review in 18th-century scientific journalism. JAMA. 1990;\n263(10):1321– 2.\n2. Jefferson T, Alderson P, Wager E, Davidoff F. Effects of editorial peer review.\nJAMA. 2002;287(21):2784 – 6.\n3. Smith R. Peer review: a flawed process at the heart of science and journals.\nJ R Soc Med. 2006;99:178 – 82.\n4. Baxt WG, Waeckerle JF, Berlin JA, Callaham ML. Who reviews the reviewers?\nFeasibility of using a fictitious manuscript to evaluate peer reviewer\nperformance. Ann Emerg Med. 1998;32(3):310 – 7.\n5. Kravitz RL, Franks P, Feldman MD, Gerrity M, Byrne C, William M. Editorial\npeer reviewers ’ recommendations at a general medical journal : are they\nreliable and do editors care? PLoS One. 2010;5(4):2 – 6.\n6. Yaffe MB. Re-reviewing peer review. Sci Signal. 2009;2(85):1 – 3.\n7. Stahel PF, Moore EE. Peer review for biomedical publications : we can\nimprove the system. BMC Med. 2014;12(179):1 – 4.\n8. Rennie D. Make peer review scientific. Nature. 2016;535:31 – 3.\n9. Moher D. Custodians of high-quality science: are editors and peer reviewers\ngood enough? https://www.youtube.com/watch?v=RV2tknDtyDs&t=454s.\nAccessed 16 Oct 2017.\n10. Ghimire S, Kyung E, Kang W, Kim E. Assessment of adherence to the\nCONSORT statement for quality of reports on randomized controlled trial\nabstracts from four high-impact general medical journals. Trials. 2012;13:77.\n11. Boutron I, Dutton S, Ravaud P, Altman DG. Reporting and interpretation of\nrandomized controlled trials with statistically nonsignificant results. JAMA.\n2010;303(20):2058– 64.\n12. Hopewell S, Collins GS, Boutron I, Yu L-M, Cook J, Shanyinde M, et al. Impact of\npeer review on reports of randomised trials published in open peer review\njournals: retrospective before and after study. BMJ. 2014;349:g4145.\n13. Lazarus C, Haneef R, Ravaud P, Boutron I. Classification and prevalence of\nspin in abstracts of non-randomized studies evaluating an intervention.\nBMC Med Res Methodol. 2015;15:85.\n14. Jefferson T, Rudin M, Brodney Folse S, et al. Editorial peer review for\nimproving the quality of reports of biomedical studies. Cochrane Database\nSyst Rev. 2007;2:MR000016.\n15. Bruce R, Chauvin A, Trinquart L, Ravaud P, Boutron I. Impact of interventions\nto improve the quality of peer review of biomedical journals: a systematic\nreview and meta-analysis. BMC Med. 2016;14:85.\n16. Moher D, Liberati A, Tetzlaff J, Altman DG, Group TP. Preferred reporting\nitems for systematic reviews and meta-analyses : the PRISMA statement.\nPLoS Med. 2009;6(7):e1000097.\n17. NHS. PROSPERO International prospective register of systematic reviews.\nhttps://www.crd.york.ac.uk/prospero/. Accessed 6 Nov 2017.\n18. Sanderson S, Tatt ID, Higgins JPT. Tools for assessing quality and\nsusceptibility to bias in observational studies in epidemiology: a systematic\nreview and annotated bibliography. Intern J Epidemiol. 2007;36:666 – 76.\n19. R Core Team. R: a language and environment for statistical computing.\nhttp://www.r-project.org/. Accessed 4 Dec 2017.\n20. Gentles SJ, Charles C, Nicholas DB, Ploeg J, McKibbon KA. Reviewing the\nresearch methods literature: principles and strategies illustrated by a systematic\noverview of sampling in qualitative research. Syst Rev. 2016;5:172.\n21. Glaser B, Strauss A. The discovery of grounded theory. Chicago: Aldine; 1967.\n22. Friedman DP. Manuscript peer review at the AJR: facts, figures, and quality\nassessment. Am J Roentgenol. 1995;164(4):1007 – 9.\n23. Black N, Van Rooyen S, Godlee F, Smith R, Evans S. What makes a good\nreviewer and a good review for a general medical journal? JAMA. 1998;\n280(3):231– 3.\n24. Henly SJ, Dougherty MC. Quality of manuscript reviews in nursing research.\nNurs Outlook. 2009;57(1):18 – 26.\n25. Callaham ML, Baxt WG, Waeckerle JF, Wears RL. Reliability of editors ’\nsubjective quality ratings of peer reviews of manuscripts. JAMA.\n1998;280(3):229 – 31.\n26. Callaham ML, Knopp RK, Gallagher EJ. Effect of written feedback by\neditors on quality of reviews: two randomized trials. JAMA. 2002;\n287(21):2781 – 3.\n27. Van Rooyen S, Godlee F, Evans S, Black N, Smith R. Effect of open peer\nreview on quality of reviews and on reviewers ’ recommendations : a\nrandomised trial. BMJ. 1999;318(7175):23 – 7.\n28. Mcnutt RA, Evans AT, Fletcher RH, Fletcher SW. The effects of blinding on\nthe quality of peer review. JAMA. 1990;263(10):1371 – 6.\n29. Moore A, Jones R. Supporting and enhancing peer review in the BJGP. Br J\nGen Pract. 2014;64(624):e459 – 61.\n30. Stossel TP. Reviewer status and review quality. N Engl J Med. 1985;\n312(10):658 – 9.\n31. Thompson SR, Agel J, Losina E. The JBJS peer-review scoring scale: a valid,\nreliable instrument for measuring the quality of peer review reports. Learn\nPubl. 2016;29:23 – 5.\n32. Rajesh A, Cloud G, Harisinghani MG. Improving the quality of manuscript\nreviews : impact of introducing a structured electronic template to submit\nreviews. AJR. 2013;200:20 – 3.\n33. Shattell MM, Chinn P, Thomas SP, Cowling WR. Authors ’ and editors ’\nperspectives on peer review quality in three scholarly nursing journals. J\nNurs Scholarsh. 2010;42(1):58 – 65.\n34. Jawaid SA, Jawaid M, Jafary MH. Characteristics of reviewers and quality of\nreviews: a retrospective study of reviewers at Pakistan journal of medical\nsciences. Pakistan J Med Sci. 2006;22(2):101 – 6.\n35. Justice AC, Cho MK, Winker MA, Berlin JA. Does masking author identity\nimprove peer review quality ? A randomized controlled trial. JAMA.\n1998;280(3):240 – 3.\n36. Henly SJ, Bennett JA, Dougherty MC. Scientific and statistical reviews of\nmanuscripts submitted to nursing research: comparison of completeness,\nquality, and usefulness. Nurs Outlook. 2010;58(4):188 – 99.\n37. Hettyey A, Griggio M, Mann M, Raveh S, Schaedelin FC, Thonhauser\nKE, et al. Peerage of science: will it work? Trends Ecol Evol. 2012;\n27(4):189 – 90.\n38. Publons. Publons for editors: overview. https://static1.squarespace.com/\nstatic/576fcda2e4fcb5ab5152b4d8/t/58e21609d482e9ebf98163be/\n1491211787054/Publons_for_Editors_Overview.pdf. Accessed 20 Oct 2017.\n39. Van Rooyen S, Black N, Godlee F. Development of the review quality\ninstrument (RQI) for assessing peer reviews of manuscripts. J Clin Epidemiol.\n1999;52(7):625– 9.\n40. Evans AT, McNutt RA, Fletcher SW, Fletcher RH. The characteristics of\npeer reviewers who produce good-quality reviews. J Gen Intern Med.\n1993;8(8):422 – 8.\n41. Feurer I, Becker G, Picus D, Ramirez E, Darcy M, Hicks M. Evaluating peer\nreviews: pilot testing of a grading instrument. JAMA. 1994;272(2):98 – 100.\n42. Landkroon AP, Euser AM, Veeken H. Quality assessment of reviewers ’ reports\nusing a simple instrument. Obstet Gynecol. 2006;108(4):979 – 85.\n43. Greenland S, O ’Rourke K. On the bias produced by quality scores in\nmeta-analysis, and a hierarchical view of proposed solutions.\nBiostatistics. 2001;2(4):463 – 71.\n44. Jüni P, Witschi A, Bloch R. The hazards of scoring the quality of clinical trials\nfor meta-analysis. JAMA. 1999;282(11):1054 – 60.\n45. Higgins JPT, Altman DG, Gøtzsche PC, Jüni P, Moher D, Oxman AD, et al.\nThe Cochrane Collaboration ’s tool for assessing risk of bias in randomised\ntrials. BMJ. 2011;343:d5928.\n46. Schroter S, Price A, Flemyng E, et al. Perspectives on involvement in the\npeer-review process: surveys of patient and public reviewers at two journals.\nBMJ Open. 2018;8:e023357.\n47. Ćurković M, Ko šec A. Bubble effect: including internet search engines in\nsystematic reviews introduces selection bias and impedes scientific\nreproducibility. BMC Med Res Methodol. 2018;18(1):130.\n48. Chalmers I, Bracken MB, Djulbegovic B, Garattini S, Grant J, Gülmezoglu AM,\net al. How to increase value and reduce waste when research priorities are\nset. Lancet. 2014;383(9912):156 – 65.\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 13 of 14\n49. Kliewer MA, Freed KS, DeLong DM, Pickhardt PJ, Provenzale JM. Reviewing\nthe reviewers: comparison of review quality and reviewer characteristics at\nthe American journal of roentgenology. AJR. 2005;184(6):1731 – 5.\n50. Berquist T. Improving your reviewer score: it ’s not that difficult. AJR.\n2017;209:711 – 2.\n51. Callaham ML, Mcculloch C. Longitudinal trends in the performance of\nscientific peer reviewers. Ann Emerg Med. 2011;57(2):141 – 8.\n52. Yang Y. Effects of training reviewers on quality of peer review: a before-and-\nafter study (Abstract). https://peerreviewcongress.org/abstracts_2009.html.\nAccessed 7 Nov 2017.\n53. Prechelt L. Review quality collector. https://reviewqualitycollector.org/static/\npdf/rqdef-example.pdf. Accessed 20 Oct 2017.\n54. Das Sinha S, Sahni P, Nundy S. Does exchanging comments of Indian and\nnon-Indian reviewers improve the quality of manuscript reviews? Natl Med\nJ India. 1999;12(5):210 – 3.\n55. Callaham ML, Schriger DL. Effect of structured workshop training on\nsubsequent performance of journal peer reviewers. Ann Emerg Med.\n2002;40(3):323 – 8.\nSuperchi et al. BMC Medical Research Methodology           (2019) 19:48 Page 14 of 14",
  "topic": "Systematic review",
  "concepts": [
    {
      "name": "Systematic review",
      "score": 0.5187276005744934
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4954949617385864
    },
    {
      "name": "MEDLINE",
      "score": 0.4675026834011078
    },
    {
      "name": "Computer science",
      "score": 0.4080878496170044
    },
    {
      "name": "Data science",
      "score": 0.3988915681838989
    },
    {
      "name": "Medicine",
      "score": 0.3762251138687134
    },
    {
      "name": "Psychology",
      "score": 0.3577750325202942
    },
    {
      "name": "Management science",
      "score": 0.34432393312454224
    },
    {
      "name": "Political science",
      "score": 0.1082068681716919
    },
    {
      "name": "Engineering",
      "score": 0.10307377576828003
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}