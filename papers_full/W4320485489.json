{
  "title": "Automated occupation coding with hierarchical features: a data-centric approach to classification with pre-trained language models",
  "url": "https://openalex.org/W4320485489",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4320605274",
      "name": "Parisa Safikhani",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4320605275",
      "name": "Hayastan Avetisyan",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4289762700",
      "name": "Dennis Föste-Eggers",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2134819571",
      "name": "David Broneske",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4320605274",
      "name": "Parisa Safikhani",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4320605275",
      "name": "Hayastan Avetisyan",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4289762700",
      "name": "Dennis Föste-Eggers",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134819571",
      "name": "David Broneske",
      "affiliations": [
        "German Centre for Higher Education Research and Science Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2012256356",
    "https://openalex.org/W1599774589",
    "https://openalex.org/W2071841567",
    "https://openalex.org/W2073617979",
    "https://openalex.org/W3048450859",
    "https://openalex.org/W3037636001",
    "https://openalex.org/W4200399640",
    "https://openalex.org/W2103718426",
    "https://openalex.org/W3095502049",
    "https://openalex.org/W2591159053",
    "https://openalex.org/W3035066987",
    "https://openalex.org/W3209399758",
    "https://openalex.org/W2913139235",
    "https://openalex.org/W2804950989",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6602368977",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W3156333129",
    "https://openalex.org/W3095319910"
  ],
  "abstract": "Abstract Occupation coding is the classification of information on occupation that is collected in the context of demographic variables. Occupation coding is an important, but a tedious task for researchers in social science and official statistics that calls for automation. Due to the complexity of the task, currently, researchers carry out hand-coding or computer-assisted coding. However, we argue that, with the rise of transformer-based language models, hand-coding can be displaced by models, such as BERT or GPT3. Hence, we compare these models with state-of-the-art encoding approaches, showing that language models have a clear advantage in Cohen’s kappa compared to related approaches, but also allow for flexible fine-grained coding of single digits. Taking into consideration the hierarchical structure of the occupational group, we also develop an approach that achieves better performance for the classification of different single digit combinations.",
  "full_text": "Vol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\nDiscover Artificial Intelligence\nResearch\nAutomated occupation coding with hierarchical features: \na data‑centric approach to classification with pre‑trained language \nmodels\nParisa Safikhani1 · Hayastan Avetisyan1 · Dennis Föste‑Eggers1 · David Broneske1\nReceived: 22 November 2022 / Accepted: 28 January 2023\n© The Author(s) 2023  OPEN\nAbstract\nOccupation coding is the classification of information on occupation that is collected in the context of demographic \nvariables. Occupation coding is an important, but a tedious task for researchers in social science and official statistics that \ncalls for automation. Due to the complexity of the task, currently, researchers carry out hand-coding or computer-assisted \ncoding. However, we argue that, with the rise of transformer-based language models, hand-coding can be displaced by \nmodels, such as BERT or GPT3. Hence, we compare these models with state-of-the-art encoding approaches, showing that \nlanguage models have a clear advantage in Cohen’s kappa compared to related approaches, but also allow for flexible \nfine-grained coding of single digits. Taking into consideration the hierarchical structure of the occupational group, we \nalso develop an approach that achieves better performance for the classification of different single digit combinations.\nKeywords BERT · GPT3 · Occupation coding\n1 Introduction\nIn empirical educational research or official statistics, occupations are central to many studies on social status or inequality \n(e.g., [1–5]). For example, information on parental occupations is often used to determine the socioeconomic status of \nparents. However, information on occupational activity is not only suitable for mapping aspects of social origin; a variety \nof occupation-related measures are now available that can be used to quantify occupation-specific health risks, gender \nsegregation, or occupational closure [6].\nTypically, the collection of occupational data in both written and online surveys is (still) carried out by means of open-\nended questions [7, 8]. While job titles are usually quite short (mostly only a few keywords), some surveys collect more \ndetailed job descriptions and activities, which could help in the post-processing of the job titles. The post-processing \nis a classification task of the textual descriptions using a chosen classification system. There are several classification \nsystems, but all of them have hundreds of occupational codes, and the codes are always nested in hierarchies. The two \ncentral standard German categorization schemes are the German Classification of Occupations 2010 (KldB 2010) [9] and \nParisa Safikhani and Hayastan Avetisyan contributed equally to this work\n * Parisa Safikhani, safikhani@dzhw.eu; Hayastan Avetisyan, avetisyan@dzhw.eu; Dennis Föste -Eggers, foeste@dzhw.eu; David Broneske, \nbroneske@dzhw.eu | 1German Centre for Higher Education Research and Science Studies (DZHW), Lange Laube 12, 30159 Hannover, \nLower Saxony, Germany.\nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\nthe International Standard Classification of Occupations 2008 (ISCO-08) [10]. While the KldB 2010 classifies specific job \ntitles, ISCO classifies occupations. Our study leverages the former one because we want to classify German occupations.\nOccupation coding is a complex activity, mostly manual or semi-automated, where tools assist human labelers in their \ndecision-making of the thousands or hundred thousands of free-text answers of participants. Hence, an automation \nwould reduce human effort enormously. The task of automating occupation coding can be seen as a subset of automated \ntext classification (ATC), which is a well-described task. Within ATC, occupation coding is mostly related to automated \nsurvey coding (ASC), which also operates on short texts for social-science research and where accuracy is principal [11]. \nCurrent approaches in occupation coding, e.g., by Schierholz and Schonlau [12], use word similarity to train a classi-\nfier. However, they do not take into account the semantic connection between the job descriptions and job activities. \nFurthermore, they lack the ability to encode only a subset of the hierarchical digits of the KldB. As a solution, we argue \nthat recently introduced pre-trained language models have the potential to solve this problem, leading to a boost in \naccuracy of automated occupation coding.\nHence, in this work, we fine-tune BERT and GPT3 for the task of automated occupation coding given an extensive, pre-\nlabeled set of job occupations and activities from the DZHW Graduate Survey Series1 and DZHW Survey Series of School \nLeavers.2 It is a complex and challenging dataset since participants (adolescents) have only a vague idea of their future \njobs and the dataset has not been extensively curated. Furthermore, due to the short texts, it is a challenge for language \nmodels due to missing linguistic features. Still, our approaches show a performance increase of 15.72 percentage points \ncompared to the state-of-the-art methods. In summary, we contribute the following:\n• We analyze the use case of occupation coding based on the current research by extracting important properties and \nrequirements from the classification system and the scientists.\n• We propose the use of transformer-based language models due to their superiority in extracting context, which is an \nimportant property for occupation coding.\n• In our extensive evaluation, we show that the chosen language models outperform the state-of-the-art in occupation \ncoding and even have a significantly better performance for a fine-grained encoding of single digits of the KldB.\nTo the best of our knowledge, this is the first attempt of automating (German) occupation coding using pre-trained \nlanguage models, such as BERT and GPT3.\nThe remainder is structured as follows. In Sect.  2, we present the most important research in automated occupation \ncoding and, in Sect. 3, we formally define our classification task and the pre-trained models, which we use for classifying \nthe occupations. We also introduce our data and the classification system KldB 2010. Section 4 describes the experiments \nand discusses evaluations of the adopted approaches to our classification task. Section 5 discusses open questions, while \nSect. 6 concludes the work and presents opportunities for further research in applications to occupational coding and \nshort text classification with hierarchical labels.\n2  Related work\nCoded occupation data usually represent the premise for any further analysis and studies [13] and the significance of \nautomating the coding process has been emphasized by numerous researchers. More precisely, even if 35–50 percent \nof the occupations can be coded automatically, the time saved, in comparison to applying hand-coding, is enormous. \nFurthermore, if especially easy-to-code professions are automatically coded, this would significantly reduce the workload \nof coders [13]. Numerous attempts have been made to accomplish this goal.\nAn extensive body of research has been conducted by Schierholz [14], who introduced a method based on a \nsupervised learning approach for automating the coding of occupational data using KldB 2010. The study concludes \nthat the best results can be achieved by combining rule-based coding with supervised learning. Furthermore, Schi-\nerholz and Schonlau [12] compared seven occupation coding algorithms found in the literature and compared their \nperformance on five datasets from Germany. The best results are obtained by leveraging Tree boosting (cod index), \ni.e., the list of job titles is merged with coded responses from previous surveys before using this combined training \n1 https:// www. dzhw. eu/ en/ forsc hung/ proje kt? pr_ id= 467.\n2 https:// www. dzhw. eu/ en/ forsc hung/ proje kt? pr_ id= 465.\nVol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y \n Research\n1 3\ndata for statistical learning. The performance rates of the four algorithms that rely on training data only (memory-\nbased reasoning, adapted nearest neighbor, multinomial regression, tree boosting (XGBoost)) are closely comparable \nwithin each data set.\nThree other approaches for automated occupation coding are proposed by Gweon et al. [15]: (1) a combination of \ntwo statistical learning models for different levels of aggregation, (2) a combination of a duplicate-based approach \nwith a statistical learning one, and (3) a modified nearest neighbor approach. Using data from the German General \nSocial Survey (ALLBUS), the two best-performing methods turned out to be the modified nearest neighbor method \n(NN-3) and a hybrid method (hybrid-3/4digit), which substantially improved the accuracy compared with both sta-\ntistical learning by itself and the duplicate method at any production rate in the ALLBUS data. As the percentage of \nduplicates decreases, NN-3 gains a relative advantage over the hybrid method.\nLim et al. [16] proposed a system without index database and achieved higher performance using KoBERT. Results \nof 95.65 percent, 91.51 percent, and 97.66 percent were obtained for occupation/industry and industry code clas-\nsification on standardized texts.\nFurthermore, Decorte et al. [17] proposed a neural representation model for job titles, called JobBERT, by sup -\nplementing the pre-trained language model with co-occurrence information from skill labels extracted from job \npostings. The model leads to significant improvements over the use of generic sentence encoders for the title nor -\nmalization work task, for which we publish a new evaluation benchmark. The method is based on the premise that \nskills are the essential components that define a job.\nThe study of Bao et al. [18] introduced a strict algorithm that will be able to identify the NOC (2016) codes using \njob titles and industry information as input exclusively. The ACA-NOC was applied to over 500 manually-coded job \nand industry titles. The accuracy rate at the four-digit NOC code level was 58.7 percent and improved when broader \njob categories were considered (65.0 percent at the three-digit NOC code level, 72.3 percent at the two-digit NOC \ncode level, and 81.6 percent at the one-digit NOC code level). Several different search strategies were employed in \nthe ACA-NOC algorithm to find the best match, including exact search, minor exact search, like search, near (same \norder) search, near (different order) search, any search, and weak match search. In addition, a filtering step based on \nthe hierarchical structure of the NOC data was applied to the algorithm to select the best matching codes. Garcia \net al. [19] designed and tested an automated coding prototype classification system ENENOC (the ENsemble Encoder \nfor the National Occupational Classification), encompassing several steps: data cleaning, exact match search, multi \nclassifier ensembling, hierarchical classification, and multiple output selection. The prototype was benchmarked on \na manually annotated data set comprising 64,000 records. It produced a top-1 Per-Digit Macro F1-Score of 0.65 and a \ntop-5 Per-Digit Macro F1-Score of 0.76. In the absence of exact matching between job title input and NOC category \ndescriptions, the input data is embedded using the TF-IDF algorithm and Doc2Vec. The embeddings are fed into a \nhierarchical ensemble classifier that uses classical machine learning techniques: Random Forests, Support Vector \nMachine, and K-Nearest Neighbor.\nSavic et al. [20] developed a web tool named Procode to code free texts against classifications and recoding between \ndifferent categories. To this end, three text classifiers—Complement Naïve Bayes (CNB), Support Vector Machine (SVM), \nand Random Forest Classifier (RFC)—were investigated using k-fold cross-validation. 30,000 free texts with manually \nassigned classification codes of the French classification of occupations (PCS) and French classification of activities (NAF) \nwere available. For recoding, Procode integrated a workflow that converts codes of one classification to another accord-\ning to existing crosswalks. CNB resulted in the best performance among the three investigated text classifiers, where the \nclassifier accurately predicted 57–81 percent and 63–83 percent classification codes for PCS and NAF, respectively. SVM \nled to lower results (by 1–2 percent), while RFC coded accurately up to 30 percent of the data. There are already some \napproaches to occupation coding, mostly simple ML approaches based on statistical similarity measures. However, the \nresults while applying those methods are not satisfactory enough. Although language models have been used in this \narea, they were not intended for classification based on categorization, but rather for skill to job offer coding. Therefore, \nfor the task of occupation coding, we want to use robust language models such as BERT and GPT3 to achieve better \nresults.\nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\n3  Methods\nIn our study, we utilize occupation coding to meet the specific needs of the researchers. As we rely on the KldB classification \nsystem, we will present it in detail. Additionally, we have a requirement to code a subset of the digits within the KldB, and will \ndiscuss the models we have chosen to accomplish this. To provide clarity, we also provide an outline of our study regarding \nthe methods and the experiments (see Fig. 1).\n3.1  KldB 2010\nThe German classification of occupations (KldB 2010) [9] is developed by the German Federal Employment Agency and is \nvalid since January 1, 2011. A transformation from KldB-2010 to the International Standard Classification of Occupations \n2008 (ISCO-08) is possible through conversion keys, which is used by several researchers [21].\nThe KldB is a five-digit, hierarchically structured code, as shown in Table  1. The first digit of the code denotes the \noccupational area (Berufsbereich) like ”Agriculture, forestry, animal husbandry and horticulture” or ”Health, social affairs, \nteaching and education” , which is more and more specified by the following three digits (the second digit denotes \noccupational main group (Berufshauptgruppe), the third digit denotes the occupational group (Berufsgruppe), and the \nfourth digit denotes the occupational sub-group (Berufsuntergruppe)). The fifth digit, occupational type (Berufsgattung), \ndenotes the requirement level (Anforderungsniveau) or degree of complexity of the occupational activity like ”Helper/\napprentice occupations” or ”professionally oriented activities” .\nA challenge of the KldB 2010 is that it contains an enormous number of classes, i.e., 1286 occupational categories \nwhen using all five digits. However, the KldB 2010 encodes two dimensions, occupational group and requirement level, \nwhich we can exploit when optimizing the training of the language models.\nFig. 1  Study process\nVol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y \n Research\n1 3\n3.1.1  Occupational group\nThe horizontal differentiation of an occupation group is coded in positions 1 to 4, and the level of differentiation increases \nwith each position, which results in 700 classes. The group names and descriptions are an essential source of information \nfor any trained model. Besides numerical codes for the groups, there are hierarchical descriptions of combinations of \ndigits from left to right. The hierarchical description of code 1 is shown in Table 2. The description of the first digit shows \nus that the occupations that have the number one as the first digit belong to the occupations in the field of agriculture, \nforestry, farming, and gardening. If the second digit is also one, the description shows that these occupations can no \nlonger be categorized under the group gardening. If the third digit is also one, it shows that the occupations with the \nfirst three one digits are categorized only under farming. The description of the combination of the whole digits shows \nthe complexity level of the occupations categorized under farming. These eight hierarchical descriptions can be used as \nfeatures to categorize the occupations for this example under the 4 existing KldB numbers, 11101, 11102, 11103 or 11104.\n3.1.2  Requirement level\nThe requirement level is coded on the 5th digit with overall four classes, shown in the right part of Table 1. The number \nof differentiation options indicates how many entries would result if the entries of the outline level were each broken \ndown by requirement level. For example, since there are no assistants or experts for some occupational subgroups, these \nfigures cannot simply be derived from the number of entries per outline level.\nDue to the two dimensions and as a result of the hierarchical structure of the first dimension, exploiting the relation \nbetween the first four digits and splitting the coding into separate models is a reasonable optimization approach for us.\nTable 1  German classification \nof occupations: structure and \ndifferentiation possibilities. \nAdapted from the German \nClassification of Occupations \n2010 [9]\nBasic structure Differentiation options\nLevel Outline level Number of \nclasses\nAssistant Skilled worker Specialist Expert\n1st level Occupational area\n(Berufsbereich)\n10 9 10 10 10\n2nd level Occupational main group\n(Berufshauptgruppe)\n37 26 37 37 35\n3th level Occupational group\n(Berufsgruppe)\n144 54 125 129 123\n4th level Occupational sub-group\n(Berufsuntergruppe)\n700 60 414 442 370\n5th level Occupational types\n(Berufsgattung)\n1286 60 414 442 370\nTable 2  Example of \nhierarchical description of \nKldB directory, starting with \nfirst digit of code one\nCode Hierarchy description\n1 Occupations in agriculture, forestry, farming, and gardening\n11 Occupations in agriculture, forestry, and farming\n111 Occupations in farming\n1110 Occupations in farming (without specialization)\n11101 Occupations in farming (without specialization)-unskilled/semi-skilled tasks\n11102 Occupations in farming (without specialization)-skilled tasks\n11103 Occupations in farming (without specialization)-complex tasks\n11104 Occupations in farming (without specialization)-highly complex tasks\nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\n3.2  The case for single‑digit coding\nThe classification of occupations is imperative in official statistics and in social science research. However, not all studies \nuse the KldB codes to their full extent. Numerous studies in different disciplines leverage separate digits of the KldB codes \nin their analyses. For instance, some studies use the whole KldB number as in [22] and others leverage separate digits of \nthe KldB codes in their analyses [23–27]. This ranges from use cases where only the first two or three digits are used [23] \nto a combination of the first two or three digits with the fifth digit [24, 25]. Currently, in those studies, the whole 5-digit \nKldB is encoded and, afterward, necessary digits are extracted for further analysis. However, due to the big number of \nclasses for a 5-digit KldB encoding, an approach that only encodes the necessary number of digits could be worthwhile.\nHence, we pose the questions, (1) whether we could exploit the relation between single digits for occupation coding \nand (2) whether we can fine-tune the task of occupation coding for use cases where only a subset of the KldB digits is \nneeded.\n3.3  Used models\nAs mentioned in related work, the only approach to automated German occupation coding using artificial intelligence \nwas by Schierholz and Schonlau [12], who used supervised learning algorithms that nowadays do not have the best \nperformance for multi-class text classification. Training deep language models on our data is time-consuming and \ncomputationally expensive. Pre-trained language models are, therefore, attractive because they reduce the burden on \npractitioners to provide the appropriate resources (time, hardware, and data) to train the models [28, 29]. Furthermore, \noccupations are mostly short texts, and short text classification is difficult to model statistically due to fewer features and \ntraining data. As shown in Luo and Wang [30] using a pre-trained language model can be useful for this task.\nIn search of a better and more time-efficient performance for automatic coding of occupations, we propose to use two \npre-trained language models BERT [31] and GPT3 [32] for classification of occupations with the whole KldB number as \nwell as for single digits of the KldB. Due to the fact that occupation texts are not sentences, there are not many semantic \nand syntactic relations. Thus, we think that GPT3, which has a capacity of 175 billion parameters and has the syntactic \nability to assign words, could be useful for our task. From the other side, we have additional data, that can represent \nsemantic relationships between digits. In using this information as features, a bidirectional model like BERT with its \nmasking capability could play a better role.\n3.3.1  BERT\nBERT is a transformer-based language model that stands for Bidirectional Encoder Representation. It is the first deeply \nbidirectional, unsupervised language representation. With just one additional output layer, this pre-trained model can \nbe fine-tuned to create state-of-the-art models for a large variety of tasks such as text classification, question answering, \nand language inference. BERT is trained with the masked language modeling (MLM) task, in which some tokens in a text \nsequence are randomly masked. Then, the masked tokens are independently recovered by conditional encoding vec -\ntors obtained by a bidirectional transformer [31]. For German-speaking applications, such as occupation coding, using \na German-language BERT model is useful, e.g., the one built by deepset. 3 Google Multilingual BERT also supports the \nGerman language, but the German BERT model significantly outperforms Google’s multilingual BERT model [33], which \nis why we chose deepset’s model for our experiment.\n3.3.2  GPT3\nGPT3 is the third generation of auto-regressive transformer-based language models. GPT1 was built with unsupervised \npre-training and supervised fine-tuning for a single task. GPT1 is followed by GPT2, which did not need supervised fine-\ntuning for a specific task and was well suited for multiple tasks. The GPT3 model, having a similar architecture basis as \nprevious models, is able to make accurate predictions without gradient updating or fine-tuning [32, 34]. The depth of \nGPT3 has increased considerably. It has by far more parameters than BERT [31, 35].\n3 https:// www. deeps et. ai/.\nVol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y \n Research\n1 3\nGPT3 is unidirectional, which is its biggest limitation compared to BERT. The choice of architectures during fine-tuning \nis limited by being unidirectional. In GPT3, a left-to-right architecture was used, where each token can only respond to \nprevious tokens in the self-attention layers of the transformer [32, 34]. It means, the word can be predicted by GPT3 based \non previous predictions [31, 35]. Another disadvantage of GPT3 is that it has no ability to understand the semantics and \ncontext of the query, but only a statistical ability to match words [36].\n4  Experiments\nEven the best machine learning algorithms cannot perform well without carefully collected and reprocessed data. \nRecently, data-centric AI [37] has gained prominence. Its main goal is to improve not the training algorithm for the \nmodel, but the data preprocessing for model accuracy. Hence, we first present how we cleaned and prepared our data. \nAfterward, we describe our experiments. The experiments are: \n1. Our first experiment analyzes the performance of the pre-trained language models for the whole five-digit KldB. As \na result, we can draw conclusions about whether our approaches can outperform state-of-the-art coding schemes \nfor German occupations.\n2. The second experiment tests the performance for use cases that require only a subset of the available KldB digits \n(see Sect. 3.2). Thus, we expect our models to improve due to the smaller number of classes.\n3. In the third experiment, we investigate whether the second experiment can be improved by propagating decisions \nfrom previous predictions. This way, we target to exploit the hierarchical nature of the KldB system for both training \nand testing the models to improve our predictions.\n4. The last experiment investigates the influence of integrating entire hierarchical features on prediction performance \nfor whole five-digits KldB.\n4.1  Data cleaning\nOur data originates from the DZHW Graduate Survey Series and the DZHW Survey Series of School Leavers conducted \nthroughout the years 2005–2015 as paper surveys that were digitalized afterward. Occupations are gathered using a \nfree-text field for the participants to fill in. To facilitate the subsequent coding, there is another free-text field, where \nparticipants should enter typical activities in their jobs. Hence, both data—job titles and typical tasks—can be used in our \nstudy as input for classifying the exact occupation. Therefore, the usually short texts of job titles, which are challenging for \nlanguage models due to their limited context, are extended with more verbose task descriptions, if they were provided.\nTo have a ground-truth, the data set consisting of 52,807 entries was encoded by several trained coders using a \nstandardized set of rules. 10,000 entries of the dataset have additional information about professional tasks. Hence, we \nbase our study on a quite reliable, but not perfect dataset. In fact, we found 935 entries that were not assigned a correct \nKldB number and removed those from our dataset. This improved our overall prediction performance by 2–3 percentage \npoints in the following experiments. Due to the amount of our data, the old heuristic of a 70%/30% split of training and \ntesting is not the optimal option in this case [38]. Therefore, in order to as efficiently as possible leverage the existing \ndata for training, we allocated 90% for the training and 10% for the validation. As a test set, we used a separate test set \nwith 4329 entries, which is being classified after saving the respective trained model.\n4.2  Experiment 1: classifying occupations on full five‑digit KldB\nIn our first experiment, we classify the occupations (job titles and, if available, their tasks) as a multi-class text classifica-\ntion task on the whole 5-digit KldB. This experiment should evaluate whether the baseline performance of BERT and \nGPT3 can keep up with the algorithms of [12] for a challenging dataset with limited semantics and short texts. To this \nend, we fine-tuned the models as follows: \nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\n1. Fine-tuning BERT: For our experiments, we used the pre-trained language model German BERT.4 The model is imple-\nmented with Pytorch 5 in Google Colab with GPU acceleration. We achieved the best results with seven epochs, a \nlearning rate of 1e–5, and a batch size of 16, which covered the size of all occupations plus their tasks. Moreover, we \nused the Adam optimizer.\n2. Fine-tuning GPT3: For fine-tuning the encoder of GPT3, we followed the approach proposed by Cansen Çağlayan. 6 \nOur implementation is written in TensorFlow.7 We fine-tuned GPT3 with 92,316,156 trainable parameters. The Adam \noptimizer was used here.\nThe calculated Cohen’s kappa on our test set, which is coded by both fine-tuned models, can be seen in Table 2. We have \nalso coded our test set with Schierholz’ model as the state-of-the-art approach. 8\nTable 3 shows that the results of BERT are superior to the other models. It outperforms Schierholz’ algorithm by 15.72 \npercent points and GPT3 by 34.29 percent points. Surprisingly, although our data consists of short sequences and do not \ncarry a lot of semantic and syntactic information, GPT3 did not achieve higher performance. Possibly, the reason might \nbe the fact that our data has a high number of labels, which might be too much for GPT3. On the other hand, the results \nshow that despite short text and not enough semantic information between words in our data, BERT could still capture \nthe semantic relations between classes of KldB.\n4.3  Experiment 2: predicting single digits of KldB\nConsidering the fact that the number of labels has a significant influence on the performance of the pre-trained model, \nwe split the KldB numbers into five digits. Instead of fine-tuning a model with the entire KldB numbers with 1286 differ-\nent classes, we fine-tuned five models, each with a single digit of KldB numbers, each comprising at most ten different \nlabels. The results can be seen in Table 4. The Cohen’s kappa values refer to the performance of our model on the test set.\nAs expected, the number of labels has a large impact on the behavior of BERT and GPT3. Compared to Experiment 1, \nthe accuracy values for both models have increased substantially. Interestingly, the performance of GPT3 is in the same \nrange as BERT’s, which suggests that GPT3’s poor performance in Experiment 1 is due to the number of classes.\nA second insight is that the first dimension (occupational group) has a direct impact on the performance of the models. \nWhile the first three digits only have an accuracy difference between 2–3 percentage points, the gap is slightly bigger for \npredicting the fourth digit. The decreasing accuracy manifests that especially the fourth digit is harder to predict, since textual \ndifferences inside the class members with the same prefix are only minor and, thus, harder to learn.\nSince most studies use several concatenated digits, our approach allows combining the predicted digits without restriction. \nHence, we assembled the digits step by step from left to right and calculated the resulting Cohen’s kappa, which is shown in \nTable 3  Performance of BERT, \nGPT3 and Schierholz on the \nsame test set\nModel BERT GPT3 Schierholz\nCohens’ kappa [%] 64.22 29.93 48.50\nTable 4  Performance of BERT \nand GPT3 fine-tuned with \nseparate single digits of KldB, \ncompared to split digits from \nwhole KldB numbers from \nSchierholz and Schonlau [12]\nLabel BERT [%] GPT3 [%] Schierholz [%]\n1st digit 84.49 75.97 72.69\n2nd digit 80.52 70.16 68.83\n3rd digit 76.93 64.43 60.14\n4th digit 70.76 63.70 52.49\n5th digit 69.80 57.78 53.32\n4 https:// huggi ngface. co/ bert- base- german- cased.\n5 https:// github. com/ pytor ch/ pytor ch.\n6 https:// github. com/ kmkar akaya/ Deep- Learn ing- Tutor ials/ blob/ master/ Multi_ Class_ Text_ Class ifica tion_ End_ to_ End_ Examp le_. ipynb.\n7 https:// www. tenso rflow. org/ api_ docs/ python/ tf.\n8 https:// github. com/ malsch/ occup ation Coding.\nVol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y \n Research\n1 3\nTable 5. Overall, BERT is still the best approach due to its better performance for single-digit classification. It is striking that \nthe combination of 5 digits in GPT3 achieves a better Cohen’s kappa value than when trained with the whole KldB number.\n4.4  Experiment 3: hierarchical feature integration in training and testing\nBased on the bias-variance analysis in conjunction with the training loss and validation loss values, we found that our \nmodel is under-fitted. From a data-centric AI perspective, we further investigated the problem and came up with the idea \nthat we can use the available KldB 2010 information as features, which are shown in Table 2. In the KldB 2010, each digit \nof labels has a literal description, which is a subcategory of the previous digit and a concise description for the next digit.\nTo this end, we improved the fine-tuning procedure for the models at the second to fifth levels by incorporating addi-\ntional input features. Specifically, before fine-tuning the second model for the second digit of the label, we added the \nliteral description of the first digit as a feature to the input. We then repeated this step for the remaining digits, using literal \ndescriptions of the combination of the first, second, third, and fourth digits as additional features so that if X i be the input \ndata (job descriptions) of ith entry from our dataset D and L i,j are the literal descriptions when j ∈{ 1, 2, 3, 4} , as follows:\nL i,1 = the literal description of the first digit\nL i,2 = the literal description of the combination of the first and second digit\nL i,3 = the literal description of the combination of the first, second, and third digit\nL i,4 = the literal description of the combination of the first, second, third, and fourth digit\nand k ∈{ 1, 2, 3, 4, 5} be the digit number of KldB. For instance, the fifth model was fine-tuned with F i,5 input which \nrepresents the job description including the feature set for the fifth digit, as follows:\nFor a generalizable fine-tuned model, the validation and test sets should look as similar as possible [38]. Therefore, we \nbuilt our test set for this experiment step by step, similar to the validation and training set. After we coded the test set \nwith the fine-tuned model, which is trained with the first digit as the label, we added the literal information of the first \ndigit as strings, which was predicted, to the occupations and made a new test set. To code the test set for the second \ndigit, we used the new test set. We repeated this step again for the further digits so that the entire process can be pre -\nsented as follows:\nLet Y i,k be the output label of ith entry and the kth digit (digits in KldB 2010), L i,j as described be the literal descriptions \nof ith entry, M k be the fine-tuned model for digit k , V be the validation set and Tk be the test set for digit k . The process \nfor fine-tuning the models for the second to fifth level can be represented as:\nThis process aims to improve the model’s generalizability by gradually incorporating the literal descriptions of the \ndigits as features and building the test set in a similar way to the validation and training set.\nWe report the performance of our fine-tuned model on the stepwise hierarchically built test set in Table  6. Although \nthe results of BERT for single digits did not improve compared to the last experiment (cf. Table 4 vs. Table 6), considering \nthe relationship between digits led to an increase of 4.46 percentage points in mean Cohen’s kappa when combining \nthe digits (see Table 7).\nFi,5= Xi + Li,1+ Li,2+ Li,3+ Li,4\nTable 5  Cohen’s kappa of \nassembled digits, predicted \nby GPT3 and BERT with \nseparate digits, compared \nwith Schierholz\nModel First two digits First three digits First four digits Whole digits\nBERT [%] 78.90 73.16 63.82 56.14\nGPT3 [%] 70.04 63.60 55.37 49.82\nSchierholz [%] 70.28 63.23 54.45 48.05\nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\n4.5  Experiment 4: fine‑tuning BERT and GPT3 using additional data for encoding the whole KldB\nOur goal was to reduce bias, but at the same time not contribute to over-fitting. Hence, we use the existing information \nfrom KldB while making sure that our test and training set do not look too different. To this end, we created a new data \nset. Half of our data had as input occupations plus the literal term for the first digit plus literal terms for the combination \nof first two, first three, first four, and total KldB digits. The other half had just the occupation as input. Both half parts had \nthe whole KldB number as label. With this experiment, we could increase the Cohen’s kappa of BERT to 66.01% and the \nCohen’s kappa of GPT3 to 42.60%, which is a slight but significant increase compared to the results in Table 3.\n5  Discussion\nThe number of occupations to be encoded and the quality of the encoded occupations depend heavily on the quality of \nthe answers collected. The more complete and accurate the answers are, the better they can be coded (either manually \nor automatically). Spelling mistakes and abbreviations, prevalent in web surveys, also lead to significantly poorer auto-\ncoding results. Therefore, it is worth doing a rough cleaning of the data [13]. When coding occupations, it is helpful to \ndraw on additional information that enables the occupational activity to be classified as precisely as possible. Without \nthis supplementary information, some details cannot be coded entirely [13]. Furthermore, our results can be applied not \nonly in the context of German occupational coding, but also on a larger scale, i.e., internationally.\nIn the future, we would like to develop a recommendation for job encoding by suggesting the 3 codes with the high-\nest softmax value to the user and letting them decide.\n6  Conclusion\nOccupation coding is an important task, which transforms textual job descriptions and task to a common numeric \nscheme. For our use case of complex German questionnaire data, we choose the KldB 2010 classification system. It gives \nus the possibility to not only encode the full five-digit KldB number, but being able to also return a prefix/subset of the \nfive digits. To this end, we have presented two approaches for occupation coding using the two pre-trained language \nmodels, BERT and GPT3. Overall, BERT was able to outperform the state-of-the-art approach by 15.72 percent points on \nthe whole KldB number. Furthermore, we were able to increase Cohen’s kappa beyond the value of predicting the whole \nKldB number due to our tuning for single KldB digits with enhanced hierarchical information.\nTable 6  Performance of the \nfine-tuned BERT models, \nwhich are fine-tuned with \nliteral information of KldB \nnumbers as features\nTraining set Cohen’s kappa [%]\nText = Occupation +. Label BERT GPT3\n– First digit 84.59 75.97\nLiteral description of the first digit Second digit 80.34 70.79\nLiteral description of the first and second digit Third digit 77 66.35\nLiteral description of the first, second, and third digit Fourth digit 69.10 59.36\nLiteral description of the first, second, third, and fourth digit Fifth digit 69.64 59.23\nTable 7  Cohen’s kappa of assembled digits, after they are being coded with the fine-tuned models with separate digits as labels and with \nstep-wise literal description of KldB 2010 as features\nModel First two digits First three digits First four digits Whole digits\nBERT [%] 81.08 77.18 68.72 62.17\nGPT3 [%] 69.81 64.18 56.57 51.64\nVol.:(0123456789)\nDiscover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y \n Research\n1 3\nAuthor contributions The contributions of the authors were distributed in the following way: Conceptualization, PS, HA, DF, DB; methodology, \nPS, HA, DB; model implementation, PS, HA; validation, PS, HA; writing—original draft preparation, PS, HA; writing—review and editing, PS, \nHA, DB; supervision, DB. All authors read and approved the final manuscript.\nFunding Open Access funding enabled and organized by Projekt DEAL.\nDeclarations \nCompeting interests The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Fujishiro K, Xu J, Gong F. What does “occupation’’ represent as an indicator of socioeconomic status?: Exploring occupational prestige and \nhealth. Soc Sci Med. 2010;71(12):2100–7.\n 2. Connelly R, Gayle V, Lambert PS. A review of occupation-based social classifications for social survey research. Methodol Innov. \n2016;9:2059799116638003.\n 3. Schooler C, Schoenbach C. Social class, occupational status, occupational self-direction, and job income: A cross-national examination. In: \nSociological Forum, vol. 9, pp. 431–458 (1994). Springer.\n 4. Hatt PK. Occupation and social stratification. Am J Sociol. 1950;55(6):533–43.\n 5. Qi Y, Liang T, Ye H. Occupational status, working conditions, and health: evidence from the 2012 china labor force dynamics survey. J Chin \nSociol. 2020;7(1):1–23.\n 6. Christoph B, Matthes B, Ebner C. Occupation-based measures-an overview and discussion. KZfSS Kölner Zeitschrift für Soziologie und Sozi-\nalpsychologie. 2020;72(1):41–78.\n 7. Peycheva DN, Sakshaug JW, Calderwood L. Occupation coding during the interview in a web-first sequential mixed-mode survey. J Off Stat. \n2021;37(4):981–1007.\n 8. Rapley TJ. The art (fulness) of open-ended interviewing: some considerations on analysing interviews. Qual Res. 2001;1(3):303–23.\n 9. Klassifikation der Berufe K. Band 1: Systematischer und alphabetischer Teil mit Erläuterungen. Bundesagentur für Arbeit (2010)\n 10. Office IL. International Standard Classification of Occupations 2008 (ISCO-08): Structure, Group Definitions and Correspondence Tables. Geneva: \nInternational Labour Office; 2012.\n 11. Gendlin A, Viechnicki P . Computer-assisted historical occupation coding. URL: https:// schol ar. google. com/ schol ar? hl= de& as_ sdt=0% 2C5& \nq=+ Gendl in+A% 2C+ Viech nicki+ P .+ Compu ter- assis ted+ histo rical+ occup ation+ coding. & btnG=\n 12. Schierholz M, Schonlau M. Machine learning for occupation coding-a comparison study. J Surv Stat Methodol. 2021;9(5):1013–34.\n 13. Züll C. The coding of occupations. GESIS Survey Guidelines. 2016.\n 14. Schierholz M. Automating survey coding for occupation. PhD thesis; 2014.\n 15. Gweon H, Schonlau M, Kaczmirek L, Blohm M, Steiner S. Three methods for occupation coding based on statistical learning. J Off Stat. \n2017;33(1):101–22.\n 16. Lim J, Moon H, Lee C, Woo C, Lim H. An automated industry and occupation coding system using deep learning. J Korea Converg Soc. \n2021;12(4):23–30.\n 17. Decorte J-J, Van Hautte J, Demeester T, Develder C. Jobbert: Understanding job titles through skills. arXiv preprint arXiv: 2109. 09605. 2021.\n 18. Bao H, Baker CJ, Adisesh A, et al. Occupation coding of job titles: iterative development of an automated coding algorithm for the Canadian \nnational occupation classification (aca-noc). JMIR Form Res. 2020;4(8):16422.\n 19. Garcia CAS, Adisesh A, Baker CJ. S-464 Automated Occupational Encoding to the Canadian National Occupation Classification using an \nEnsemble Classifier from TF-IDF and Doc2Vec Embeddings. London: BMJ Publishing Group Ltd; 2021.\n 20. Savic N, Bovio N, Gilbert F, Canu IG. Procode: the swiss multilingual solution for automatic coding and recoding of occupations and economic \nactivities. arXiv preprint arXiv: 2012. 07521. 2020.\n 21. Tiemann M, Kaiser F. Klassifikationen der Berufe-Begriffliche Grundlagen, Vorgehensweise, Anwendungsfelder. na, Bonn; 2013.\n 22. Kraft MHG. How important are linguistic competencies on the german labour market? a qualitative content analysis of job advertise -\nments. EJEBS. 2021;v5i3:35–41. https:// doi. org/ 10. 26417/ ejes.\n 23. Geis-Thöne W. Zuwanderung hat den gesundheitsbereich gestärkt. Technical report, IW-Kurzbericht; 2020.\n 24. Koebe J, Samtleben C, Schrenker A, Zucco A. Systemically relevant but little recognized: Compensation of indispensable occupations under-\nperformed in the Corona crisis. 2020. https:// www. diw. de/ de/ diw_ 01.c. 792754. de/ publi katio nen/ diw_ aktue ll/ 2020_ 0048/ syste mrele vant__ \naber_ denno ch_ kaum_ anerk annt__ entlo hnung_ unver zicht barer_ berufe_ in_ der_ coron akrise_ unter durch schni ttlich. html.\n 25. Guggemos J. Analyse beruflicher tätigkeitsfelder von wirtschaftspädagogen/-innen anhand von daten des karriereportals xing. Zeitschrift \nfür Berufs-und Wirtschaftspädagogik. 2018;114(4):551–77.\n 26. Diel A. Ein viertel der pharmabeschäftigten arbeitet in der produktion. Technical report, IW-Kurzbericht; 2019.\n 27. Frank F, Jablotschkin M, Arthen T, Riedel A, Fangmeier T, Hölzel LP , Tebartz van Elst L. Education and employment status of adults with autism \nspectrum disorders in Germany–a cross-sectional-survey. BMC Psychiatry. 2018;18(1):1–10\nVol:.(1234567890)\nResearch Discover Artificial Intelligence             (2023) 3:6  | https://doi.org/10.1007/s44163-023-00050-y\n1 3\n 28. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al. Language models are unsupervised multitask learners. OpenAI blog. 2019;1(8):9.\n 29. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ, et al. Exploring the limits of transfer learning with a unified \ntext-to-text transformer. J Mach Learn Res. 2020;21(140):1–67.\n 30. Luo L, Wang Y. Emotionx-hsu: Adopting pre-trained bert for emotion classification. arXiv preprint arXiv: 1907. 09669. 2019.\n 31. Devlin J, Chang M-W, Lee K, Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint \narXiv: 1810. 04805. 2018.\n 32. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell A, et al. Language models are few-shot \nlearners. Adv Neural Inf Process Syst. 2020;33:1877–901.\n 33. Chan B, Schweter S, Möller T. German’s next language model. arXiv preprint arXiv: 2010. 10906. 2020.\n 34. Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training (2018)\n 35. Minaee S, Kalchbrenner N, Cambria E, Nikzad N, Chenaghlu M, Gao J. Deep learning-based text classification: a comprehensive review. ACM \nComputing Surveys (CSUR). 2021;54(3):1–40.\n 36. Floridi L, Chiriatti M. Gpt-3: its nature, scope, limits, and consequences. Minds Mach. 2020;30(4):681–94.\n 37. Ng A, Laird D, He L. Data-centric ai competition. DeepLearning AI. 2021. https:// deepl earni ng- ai. github. io/ data- centr ic- comp/. Accessed 9 \nDec 2021.\n 38. Andrew Ng. Yearning for machine learning. 2018. https:// info. deepl earni ng. ai/ machi ne- learn ing- yearn ing- book.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Coding (social sciences)",
  "concepts": [
    {
      "name": "Coding (social sciences)",
      "score": 0.751531183719635
    },
    {
      "name": "Computer science",
      "score": 0.7030923366546631
    },
    {
      "name": "Language model",
      "score": 0.5933154225349426
    },
    {
      "name": "Natural language processing",
      "score": 0.5496205687522888
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45664024353027344
    },
    {
      "name": "Machine learning",
      "score": 0.34888556599617004
    },
    {
      "name": "Statistics",
      "score": 0.11340969800949097
    },
    {
      "name": "Mathematics",
      "score": 0.09403404593467712
    }
  ]
}