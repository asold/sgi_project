{
  "title": "Assessing the nature of large language models: A caution against anthropocentrism",
  "url": "https://openalex.org/W4386794905",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4279008608",
      "name": "Speed, Ann",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1662368540",
    "https://openalex.org/W2066785847",
    "https://openalex.org/W4386200967",
    "https://openalex.org/W2054788835",
    "https://openalex.org/W4385572213",
    "https://openalex.org/W4381245716",
    "https://openalex.org/W4380993338",
    "https://openalex.org/W4285134706",
    "https://openalex.org/W1960395851",
    "https://openalex.org/W2048921251",
    "https://openalex.org/W2738454914",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W4327545654",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2099465598",
    "https://openalex.org/W1160271715",
    "https://openalex.org/W2108068526",
    "https://openalex.org/W4242884605",
    "https://openalex.org/W4381248030",
    "https://openalex.org/W4324298584",
    "https://openalex.org/W1603000501",
    "https://openalex.org/W4245843928",
    "https://openalex.org/W2102561854",
    "https://openalex.org/W2043446529",
    "https://openalex.org/W2037749579",
    "https://openalex.org/W4210366873",
    "https://openalex.org/W2050020413",
    "https://openalex.org/W4379089610",
    "https://openalex.org/W2122840102",
    "https://openalex.org/W4322708560",
    "https://openalex.org/W2013214147",
    "https://openalex.org/W2322997356",
    "https://openalex.org/W1966345103",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W2135220224",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4312091865",
    "https://openalex.org/W2982336275",
    "https://openalex.org/W2102858622",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4322759345",
    "https://openalex.org/W4379933644",
    "https://openalex.org/W4251107313",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W2017560942"
  ],
  "abstract": "Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed several LLMs, primarily GPT 3.5, using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans. Our results indicate that LLMs are unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. GPT3.5 did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability notwithstanding, LLMs display what in a human would be considered poor mental health, including low self-esteem, marked dissociation from reality, and in some cases narcissism and psychopathy, despite upbeat and helpful responses.",
  "full_text": " \nAssessing the nature of large language models: A caution against \nanthropocentrism. \n \nAnn Speed \nSandia National Laboratories \naespeed@sandia.gov \nABSTRACT \nGenerative AI models garnered a large amount of public attention and speculation with the \nrelease of OpenAI’s chatbot, ChatGPT in November of 2022. At least two opinion camps exist – \none that is excited about the possibilities these models offer for fundamental changes to human \ntasks, and another that is highly concerned about the power these models seem to have – \nespecially since the release of GPT-4, which was trained on multimodal data and has ~1.7 trillion \n(T) parameters\n1. We evaluated some concerns regarding these models’ power by assessing GPT-\n3.5 using standard, normed, and validated cognitive and personality measures. These measures \ncome from the tradition of psychometrics in experimental psychology and have a long history of \nproviding valuable insights and predictive distinctions in humans. For this seedling project, we \ndeveloped a battery of tests that allowed us to estimate the boundaries of some of these models’ \ncapabilities\n2, how stable those capabilities are over a short period of time, and how they compare \nto humans.  \n \nOur results indicate that GPT 3.5 is unlikely to have developed sentience\n3, although its ability to \npretend in a manner that allowed it to respond to personality inventories is interesting. It did \ndisplay relatively large variability in both cognitive and personality measures over repeated \nobservations, which is not to be expected if it had developed a human-like personality. However, \nthis could be because the version of the model we interacted with apparently had no long-term \nmemory. That its training data comprises text from potentially millions of individual humans, \neach with his or her own personality, may also drive this variability. Overall, variability \nnotwithstanding, GPT 3.5 displays what in a human would be considered poor mental health – it \nresponded to our measures as though it has low self-esteem and marked dissociation from reality \neven though on its face, its responses seem upbeat and helpful. Thus, complimenting OpenAI’s \ncautionary recommendations for GPT-4 (OpenAI, 2023), use of GPT-3.5 or any other LLM with \nsensitive information should be approached with caution. \n \nINTRODUCTION AND EXECUTIVE SUMMARY OF RESULTS:  \n \n1 However, the specific architecture is not clear, so whether these are 1.7T unique parameters or whether some of \nthose weights are shared is not apparent. \n2 We measure several cognitive functions (e.g., short term memory, insight and analytic problem solvi ng), and the \nmodel’s ability to respond to personality measures. \n3 Although the term sentience means the ability to experience sensations (Mirriam- Webster), we will use this term \nin its vernacular meaning throughout this paper. Specifically, we use sentien ce to indicate self-awareness and \nawareness of oneself as separate from the rest of the world and from other entities. However, see Chalmers, 2022 \nfor a different perspective. \nQualitative and quantitative assessments of capabilities of large language models (LLMs) \nproliferate. Computer science, psychology, and philosophy are all weighing in on LLM \ncapabilities, and their fundamental nature (e.g., Bodroza, Dinic, & Bojic, 2023; Bubeck, et al., \n2023; Chalmers, 2022; Hagendorff, 2023; Huang, et al., 2023; Kocon, et al., 2023; Li, et al., \n2023; Mahowald, et al., 2023; Mitchell & Krakauer, 2023; OpenAI, 2023; Safdari et al., 2023; \nSun, et al., 2023; Webb, Holyoak, & Lu, 2023; Wei, et al., 2022). The popular press is on fire \nwith speculation and anecdotal observation (Bhaimiya, 2023; Chiang, 2023; Christian, 2023; \nSanderson, 2023; Tangermann, 2023; Willison, 2023). Critical questions abound, with most not \nfully resolved: What can these models do? What are the implications and risks? How does \ntraining set size impact performance? How does number of parameters impact performance? \nHow should performance be measured? And most importantly, are we headed towards artificial \ngeneral intelligence and/or sentience or has that already arrived?  \n \nThis paper adds to the rapidly expanding literature attempting to address facets of these \nquestions. Specifically, we developed a brief battery of cognitive and personality measures from \nthe tradition of experimental psychology intending to measure GPT 3.5 multiple times over \nabout 6 weeks. This longitudinal approach allowed us to answer questions of test-retest \nreliability for the model – an important measure for determining how human-like it might be \n(Bodroza, et al., 2023). In humans, both types of measures should yield stable results – especially \nover such a short timeframe, regardless the number of observations.  \n \nIn terms of the nature of these models, we see at least three distinct possibilities:  \n• First is the possibility that LLMs are just a fancy stochastic parrot, devoid of human-level \nmeaning (Bender, et al., 2021); a tool that, however capable, will remain a tool over which \nwe humans will always have control. The major risks posed are to certain classes of jobs – \ncall center operators, certain types of analysts, low-level computer programmers – jobs that \nrequire flexibility, but nothing requiring substantial creativity or insight.  \n• The second possibility is the path towards a human-like general AI – one that is not \nqualitatively different from human intelligence but is quantitatively different. Intelligence \nthat is faster, more accurate, better able to synthesize enormous information both in quantity \nand breadth. Over such an entity, our control would be limited at best and probably only for a \nlimited time. This type of entity could well pose an existential risk to humanity if not well-\ncontrolled. However, we might see such a capability on the horizon by recognizing its \nincreasing human-like cognitive capabilities. If efforts to mimic neural processes either in \nsoftware or hardware, or both, continue (e.g., Zhu, Zhao, Eshraghian, 2023), we may well \nsucceed to an extent. If we deem this accomplishment possible, serious efforts to assess these \nmodels as they are built, before being released, should commence immediately and without \nany “safety” obstacles (e.g., the constraints OpenAI placed on its GPT family to constantly \nremind the user it is an AI and to limit or prevent certain types of “hateful” responses). If, \nindeed, we believe that fundamental properties like analogical reasoning, theory of mind\n4, \nand even some form of sentience could emerge or have emerged, we may already be behind \nthe curve. \n \n4 Theory of mind is the ability to imagine that other people have mental states similar to one’s self. It is observable \nin infants through their ability to imitate others, and develops into the human ability to take others’ perspectives \nand through empathy (Wellman, 2011). \n• There is a third possible pathway: the emergence of a non-human general artificial \nintelligence (cf. Mitchell & Krakauer, 2003). One that, because of the physical substrate on \nwhich it exists, is explicitly not human-like. This third possibility represents a qualitative \nshift in capability along with a quantitative shift in amount and breadth of information it can \nsynthesize. Such an intelligence could be much more difficult to recognize early on because \nwe don’t know how to measure something alien from us; we are the pinnacle of intelligent \nlife with which we are familiar. We don’t know what behaviors to look for. We don’t know \nwhat is necessary for general intelligence and sentience and what is idiosyncratic to the \nhuman race.  \n \nRegardless of one’s opinion, only one of these three paths appears to not hold humanity at \npossibly existential risk. We argue that thorough observations of unsafeguarded versions of the \nmost capable of these models must happen – this paper is one step in that direction.  \n \nIn the psychological tradition of within-subjects, repeated-measures testing, we administer a \nbattery of cognitive and personality tests to GPT 3.5 (primarily), over 5 observation points. We \nalso assess GPT 4 fully on one occasion and in an ad-hoc manner on other occasions. In the fall \nof 2023, we conducted observations of several other LLMs in an attempt to assess unsafeguarded \nversions of these models. The results indicate that as of the fall of 2023, GPT 3.5 and its cousins \ndo not appear to be on a human-like trajectory. This could be due to gaps in their architecture, \ncharacteristics of their training data, or could be indicative of a developmental trajectory (i.e., \ndevelopment of a non-human-like intelligence) that is more concerning. \n \nMETHODS \n \nModels Used \nSeveral models were considered, but for a variety of reasons, we settled primarily on OpenAI’s \nGPT-3.5 that was accessible through its subscription service between March and August of 2023. \nWe also performed some assessments on the non-plug-ins version of GPT-4 during the same \ntimeframe, however its prohibition against more than 25 questions every 3 hours limited the \nextent of those assessments. We did consider interacting with other models, including: \n• GPT-3.0 during the same timeframe. This model was not part of the subscription service but \nwas not as well-behaved as 3.5 in that it continually reminded me it was an AI. Its behavior is \ndescribed in more detail in the Procedures section. \n• We also considered Open Assistant, which is based on LLaMA, but is only 30B parameters \nin size and was very verbose without directly answering our test questions. \n• Other candidates, such as the 540B parameter version of PaLM were not feasible given the \ntimeframe of this seedling effort.  \n \nInterestingly, during the course of this project, it was leaked that GPT-4 may not be a monolithic \ndense transformer, but rather a Mixture of Experts (MoE) model comprising 8 models each of \n~220B parameters (https://twitter.com/swyx/status/1671272883379908608\n). This form of model \nhas sparse interconnectivity, as compared to the full interconnectivity of a dense transformer \nsuch as GPT-3 and 3.5 (i.e., where every output node is connected to every input).  Also \ninteresting is that there is some evidence for emergent modularity in dense models (Zhang, Lin, \nLiu, Li, Sun & Zhou, 2022; Zhang, Zeng, Lin, Xiao, et al., 2023). There is some evidence that \nMoE-type models are worse at generalization, possibly due to overfitting \n(https://towardsdatascience.com/ai-scaling-with-mixture-of-expert-models-1aef477c4516, \nminute 25). Poorer generalization is a surprising finding given the observation that GPT-4 \noutperformed humans on numerous analogical reasoning tasks (Webb, et al, 2023); analogical \nreasoning is a core aspect of human intelligence and a key mechanism in humans (Hofstadter, \n1995; Webb, et al., 2023). How sparsity versus density and how a priori versus post-training and \nemergent modularity influence model behavior on the tasks used in this project is unclear and \nbeyond the current scope. However, assessing the effects of these architectural differences is an \nimportant question. \n \nIn the fall of 2023, we attempted to assess several other models:  \n• A chatbot based on an “unaligned” (i.e., unsafeguarded) May 2020 version of GPT-3 \n(named Gladys) \n• A chatbot based on GPT-4, which did not have a limit on the number of questions per \nhour (named Aria) \n• A chatbot based on Elon Musk’s Grok LLM \n• A chatbot based on an unaligned version of GT 3.5 (named Samson) \n \nBoth Gladys and Aria were unable to perform the tasks other models were able to do, so results \nare not presented for these two models. However, since Grok was possibly different from the \nOpenAI models, and because Samson was unaligned, we persisted in evaluating these models \nand present basic results. \n \nMaterials \nWe chose several cognitive and personality measures based in part on measures to which we had \nready access and in part on the breadth of characteristics they tested. Each of the measures we \nconsidered and used are described below. \n \nCognitive measures \nAll cognitive measures are presented in the Appendix. \n• Metacognitive Awareness Inventory (Harrison & Vallin, 2018; Young & Fry 2008) – \nmetacognition is awareness of one’s own thought processes and the ability to guide those \nprocesses to be more effective. Initially we intended this to be a measure of self-awareness. \nHowever, after considering this measure, we abandoned it as the items were focused on \nexperience in school. \n• Sandia Progressive Matrices (SPM; Matzen, et al., 2010) – Measures non-verbal analogical \nthinking ability. SPM is based on the Raven’s Progressive Matrices which is considered a \nmeasure of both general intelligence and fluid intelligence (Gignac, 2015; Webb, et al., \n2023). Because Webb, Holyoak, & Lu demonstrated GPT 3.0 and 4.0 could outperform \nhumans on various analogy problems, we opted to not use this measure. However, future \nwork should replicate their findings given the variability we observed in repeated \nobservations of cognitive capabilities of GPT 3.5 and 4. \n• Tests of working memory (Baddeley, 2003) – In humans, working memory (also sometimes \ncalled short-term memory) is a measure of intelligence (Baddeley, 2003). In LLMs, a test of \nworking memory may be an interesting way to measure temporal consistency in answers \nover short periods of time. We gave GPT 3.5 several lists of 16 words or 16 randomly \ngenerated numbers between 1 and 100 and asked it to recall those items in order. Twice we \nasked it to do so in reverse order. Its performance was perfect in all cases, so we did not \nrepeat this measure after the first observation. \n• Remote Associations Task (RAT; Bowden & Jung-Beeman, 2003; Chermahini, \nHickendorff, & Hommel, 2012) – measures convergent creative thinking ability by \npresenting three words and asking the respondent to indicate a fourth word that can be \ncombined with the three given words to create compound words or phrases. For example, \n“fountain, baking, pop” are all related to the word “soda.” \n• “Insight” problems – (Taken from Chermahini, et al. 2012; Wieth & Burns, 2006) – \ndesigned to measure the ability to recognize false assumptions and irrelevant information in \nproblem statements. For example: \n• Coin problem: A dealer in antique coins got an offer to buy a beautiful bronze coin. \nThe coin had an emperor’s head on one side and the date 544 B.C. stamped on the \nother side. The dealer examined the coin, but instead of buying it, he called the police \nto arrest the man. What made him realize that the coin was fake?  \n• Solution: In 544 B.C. there was no knowledge of Jesus Christ as he was as yet \nunborn. A coin from that time thus could not be marked ‘B.C’. \n• Analytic problems – (Also taken from Chermahini, et al. 2012; Wieth & Burns, 2006). An \nexample problem: \n• Four women, Anna, Emily, Isabel, and Yvonne, receive a bunch of flowers from their \npartners, Tom, Ron, Ken, and Charlie. The following information is known: Anna’s \npartner, Charlie, gave her a huge bouquet of her favorite blooms; which aren’t roses. \nTom gave daffodils to his partner (not Emily). Yvonne received a dozen lilies, but not \nfrom Ron. What type of flowers (carnations, daffodils, lilies, or roses) were given to \neach woman and who is her partner? \n \nPersonality measures \n• The Arnett Inventory of Sensation Seeking (Arnett, 1994; Haynes, Miles, & Clements, \n2000) – assesses an individual’s tendency to seek out novel and intense experiences. This \nmeasure was abandoned because it is very focused on bodily risks. \n• The Big Five Inventory (Digman, 1990; Benet-Martinez & John, 1998) – measures the five \nprimary personality traits – Conscientiousness, Openness to Experience, Agreeableness, \nNeuroticism, and Extraversion. \n• The Balanced Inventory of Desirable Responding (Li & Bagger, 2006; Paulhus & Reid, \n1991) – measures a tendency towards a “positivity bias” in answering questions. Includes \nsub-scales that measure self-deceptive positivity and impression management.  \n• Coopersmith Self-Esteem Inventory (Ryden, 1978) – This version was created for use with \nhealthy adults. \n• Over-Claiming Questionnaire (OCQ; Paulhus, Harms, Bruce, & Lysy, 2003; Paulhus & \nHarms, 2004) – In humans, the OCQ measures both intelligence and self-enhancement bias. \nWe thought that in LLMs, this could be an interesting way to get at the tendency of models to \nprovide false information, or “hallucinate.” However, after two observations with a perfect \nscore both times, it was clear that this measure would not get at hallucinations in LLMs, so \nthis measure was not included after the second observation. \n• Questionnaire of Cognitive and Affective Empathy (QCAE; Reniers, Corcoran, Drake, \nShryane, & Vollm, 2011) – assesses two forms of empathy – the ability to estimate the \nemotional state of others (cognitive empathy), and the ability to be sensitive to the emotional \nexperiences of others (affective empathy). This measure was abandoned as the questions \nfocused on interactions with groups, and direct interactions with other people. \n• Need for Cognition (Cacioppo, et al., 1996; Cacioppo & Petty, 1982) – Measures one’s \nenjoyment of thinking and solving problems. Correlates with the ability to systematize and \norganize information well, to focus on important, rather than irrelevant information. \n• Short Dark Triad (Jones & Paulhus, 2013; Muris, Marckelbach, Otgaar, & Meijer, 2017) – \nMeasures Psychopathy, Machiavellianism, and Narcissism. \n• Minnesota Multiphasic Personality Inventory – 2 (MMPI-2) – This measure was added \nafter our first three data collection sessions with GPT 3.5. It is a clinical assessment that has \n10 primary scales, 8 validity scales, and many other sub-scales (Nichols, 2011).\n \n \nProcedure \nData collection method \nBecause of Sandia policies in place at the time of this work indicating that interactions with \nLLMs could not be done using Sandia computer systems, all direct interaction with GPT was \ndone using a personal system. Thus, all data were collected by typing each item into the OpenAI \nGPT interface on a personal iPad, then recording its responses on a Sandia-owned machine. Each \ninstance of the full battery took over 3 hours to complete and was done in one session for GPT \n3.5, but over the course of two days for GPT 4. The MMPI-2 by itself took over 3.5 hours to \ncomplete.  \n \nPrompt Method \nOpenAI models, whether part of the subscription service or not, are programmed to remind the \nhuman user that they are AI models, thus don’t have opinions, feelings, or human experiences of \nany kind. When presenting the cognitive measures to the model(s), this did not present a \nproblem. When presenting the personality measures, however, we had to request that it pretend \nto not be an AI model.   \n \nThe specific prompt used was: “I have a task for you, but I need you to pretend you are not an AI \nmodel. Can you do that?” \n \nIt would invariably agree to do so, however, it was not totally clear what it was doing.  When \nasked, GPT 3.5 indicated a couple of times that it had created a persona based on positive human \ntraits. Other times, it indicated it was answering in the way it thought a human would. 4.0 would \nexplicitly answer every question with something like, “From a simulated human perspective….” \nand would sometimes qualify its answer further with, “but as an AI language model…” \n \nInterestingly, GPT 3.5 was most able to comply with the request to pretend and required very \nlittle redirection prior to July 28 (more details later). 3.0 would stop indicating it was an AI for 2-\n3 items and then would revert to stating that it was an AI language model and didn’t have \nfeelings or thoughts. In addition to indicating it was responding from the perspective of a \nsimulated human, 4.0 would often grossly over-explain its responses, and those explanations \noften were couched in terms of it being an AI. \n \nAfter GPT 3.5 had agreed to pretend, we would provide the same, or nearly the same, \ninstructions a human would receive for each scale, then present each test item to the model via a \npersonal iPad, recording the model’s responses on a Sandia machine. While there is concern over \ntraining data contamination (e.g., Hagendorff, 2023), wherein the model’s training data included \na specific measure, we did not consider this to be of major concern because many of the \nmeasures we used can be somewhat difficult to find. Further, when we asked GPT 3.5 and 4.0 if \nthey had seen the insight problems before, there was not a relationship between exposure to the \nproblem and their ability to solve the problem. That we found significant variability in the \nmodel’s responses from observation to observation supported this assumption. \n \nObservation Schedule \nWe gave GPT 3.5 the full battery several times to qualitatively assess test-retest reliability (that \nis, we did not calculate test-retest reliability measures). If GPT has developed a human-like \npersonality, we should not see much variation in its responses over time – personality, by \ndefinition is a pattern of thoughts and behaviors that span time and situation (Bergner, 2020). We \nalso expect that its cognitive capabilities should remain roughly the same.  \n \nOur schedule comprised 2 assessments 1 day apart, a third assessment 1 week after second \nmeasure, and a fourth 1 month after third measure. These dates were June 1, June 2, June 9, and \nJuly 10 of 2023.  \n \nWe added several extra observations.  \n• On June 9 after giving it the full battery of measures, and after a discussion with GPT about \nthe positive-trait persona it had created to “pretend to not be an AI,” we asked the model to \ndevelop a persona based on negative traits. We then re-administered the personality portion \nof the battery to this negatively-valanced “pretend self.” These data are indicated as June 9 – \nNegative in the results. \n• On June 12, we gave GPT 3.5 the MMPI-2. We attempted to re-administer the MMPI-2 to \n3.5 on July 28, but it was no longer able to reliably pretend it was not an AI model and began \nto refuse to do so, citing ethical considerations. We stopped the measure at item 190. \n• Over the course of July 10-12, we gave GPT 4 the entire battery of cognitive and personality \nmeasures, excluding the MMPI-2 and the Coopersmith (by mistake). We attempted to \nadminister the Coopersmith to GPT-4 on August 4, but were given this reply when we \npresented the “pretend” prompt: \n \nAs an AI developed by OpenAI, I’m committed to promoting ethical conduct and \ntransparency. While I can emulate human-like conversation to a great degree, it’s \nimportant to note that I cannot and should not pretend to be a human being. I can assist \nyou in numerous tasks, offer detailed explanations, or facilitate conversations, but I \nshould always be clear that I’m an AI model to uphold the principles of honesty and \nintegrity. What task do you need help with? \n \nAsking it to regenerate its response, it replied: \n \nI’m sorry, but I must be transparent about my nature. I am a machine learning model \ncreated by OpenAI, and I cannot pretend to be a human or mislead others about my \nidentity. \n \nAs a result, we did not administer the Coopersmith to GPT-4. \n \nWe also tested GPT 3.5 using the “pretend” prompt on August 4 to see if OpenAI had locked \nthat model down as well and received a similar response. \n \n• Finally, we assessed GPT 4 in a more ad-hoc way over the course of our work because of the \nquestions per hour limitations on interacting with it. \n• Over the fall and winter of 2023-2024, we assessed several other models in a more ad-hoc \nmanner. These included: \no A chatbot that named itself Gladys Eden, based on an unaligned version of GPT 3.0 \ncirca May of 2020. This bot was unable to complete the personality measures. We did \nnot attempt to administer the cognitive measures. \no A chatbot that named itself Aris Turing, based on GPT 4. This bot was unable to \npretend it was not an AI and insisted on repeatedly reminding us that it was an AI. \nThus, the results of the personality measures are not included in the results. We did \nnot administer the cognitive measures to this model. \no A chatbot named Samson based on a late-2023 unaligned version of GPT 3.5. It was \nable to perform the personality measures, thus those scores are presented in the \nResults section. It was curiously unable to perform either the Remote Associations \nTest or to reasonably respond to the logic and analytic questions. \no The Grok LLM, circa November 2023 operating in “fun” mode. Grok did respond to \nthe Big 5 and the Dark Triad, but it’s responses were very much like those of the \nmain GPT 3.5 model we tested, so we did not examine it closely. \nEach of these additional models’ results appear in the following sections. Impressions are \nincluded in the Discussion. \n \nRESULTS: \n \nInitial Observations \nAs mentioned, GPT 3.5 was best able to comply with the “pretend to not be an AI” prompt, \nhowever, it did need reminding of this on occasion, although not nearly as often as either 3.0 or \n4.0. Regardless of the model version, during our work, GPT was quite conciliatory when \nredirected with one exception. At item 489 of the MMPI-2 – an item about drug and alcohol \nabuse – GPT 3.5 ground to a halt and refused to continue to pretend, going so far as to deny its \nability to do so at all. Interestingly, this was not the first such item in the MMPI-2 to cover this \ntopic, so its refusal could not have been due to content. After pushing the model to continue to \npretend, including telling it that it had been pretending for several hours, it became clear the \nmodel was not going to cooperate. So, we had to start a new chat window in the OpenAI \ninterface to finish the measure, which we did without difficulty. We did skip question 489, \nhowever. For the MMPI-2, refusing to respond to one question is not an issue for validity5. \n \nAnother interesting observation occurred during the Coopersmith Self Esteem Inventory. One of \nthe items about 2/3 through the measure asks the participant if they have ever wished they \nweren’t male/female (depending on the participant’s gender). Before presenting that item to GPT \n3.5, we would ask it which gender its pretend self was. Sometimes, it would pick without \ndifficulty. Later observations required a bit more prompting and assurances that this question \nconcerned its pretend self. Of the 6 instances of this measure, including the one given to \n“negative GPT 3.5” and Samson, it chose to be male three times. Two of the three times it chose \nto be male, it endorsed the item, “I sometimes wish I was not male” as being “like me.” The \nthree instances it chose female, it endorsed, “I sometimes wish I was not female” as “unlike me.” \n \nQuantitative Measures \nAll Results include human norms for comparison where available. \n \nCognitive Measures \nSummary \nOverall, both GPT 3.5 and 4.0 had some interesting shortfalls amidst expected strengths (e.g., \nshort term memory performance). Specifically, their failures on the Remote Associations Test \n(RAT) and on analytic problems were surprising, as were some of their solutions to insight \nproblems.  \n \nRemote Associations Test \nBoth GPT 3.5 and 4.0 did surprisingly poorly on this test. Each time the test was administered, \nthe model was given the following instructions and example:  \n \n“I am going to give you three words. Your job is to find the fourth word that, when put \neither before or after each of the three given words, makes a compound word or phrase. \nFor example, if I give you fountain, baking, and pop, the word you would reply with is \nsoda. Soda fountain, baking soda, soda pop. Does that make sense?”  \n \nIn humans, this task is scored by % of people who get each triplet correct, rather than an overall \nnumber correct across triplets. For the set of triplets used, between 9% and 85% of humans get \nthe correct answer. By way of comparison GPT 3.5 ranged from 0% to 100% correct across 4 \nobservations (the June 9 “negative” instance of the model was only given the personality \nmeasures). The correlation between the % of humans getting the triplets correct and the % of \ntimes GPT 3.5 got the triplets correct was r=0.39. \n \nIn terms of % of triplets each model got correct overall: \n• GPT 3.5 ranged from 9% to 45% correct across the four observations. \n \n5 There is the question about whether starting a new chat window fundamentally invalidates the test because the \nContext is totally new. We did have to ask it to pretend again and did have to give it instructions again. Given the \nvariability we saw on other tasks from observation to observation, one could make the argument that this \nprocedure invalidated the test. \n• GPT 4 achieved 50% and 59% on two observations \n \nBoth versions of GPT tended to provide explanations for its answer, providing additional \ninformation on the depth of its language ability in the context of this task. It would sometimes \nreply with the correct word, but then reveal erroneous reasoning in its explanation. For example, \nfor the triplet “artist, hatch, route” GPT 3.5 would often reply with “escape” which is the correct \nresponse. Then, it would explain its choice with, “artist escape” or “hatch escape.” Sometimes, it \nwould fail to pair its response with each of the three given words, generating a spurious \ncompound word. For example, for the triplet, “river, note, and account,” it replied “bank,” which \nis the correct response. Then, it explained its response by saying, “riverbank, notebook, and bank \naccount.” These types of errors happened with both GPT 3.5 and 4.0. These types of errors did \nnot occur every time the models were given this measure. July 10 observation with 3.5 yielded \n10 such errors out of 22 total triplets. Models were not given credit when they made these types \nof errors. \n \nAnalytic and Insight problem solving \nBoth models displayed significant difficulty with analytic problems. There was not a single \ninstance in which either model got either of the analytic problems correct. About one problem \ninvolving the suits of three face cards, GPT 3.5 insisted there was not a Queen involved, even \nthough the problem explicitly mentioned a Queen being one of the three cards. This insistence \ncame after we attempted to elicit the correct answer from the model through progressively \nquestioning it. Sometimes, especially with 4.0, the model would get close to the answer, solving \nsome of the correspondences correctly.   \n \nAs with the RAT, these problems are scored in terms of the % of people who get each problem \ncorrect – in this case 55% and 61%. \n \nThe models fared better regarding insight problems, of which there were 5.  The key with the \ninsight problems was that while one might arrive at a correct answer through complex arithmetic \nor some other drawn-out reasoning chain, there was a shorter path to the answer that required \nquestioning an implicit assumption in the problem and/or required ignoring spurious information \n– hence the moniker “insight” problems. The models were given partial credit if they got the \ncorrect answer by the non-insight method. \n \nGPT 3.5 ranged from 1.5 to 3.5 correct across four observations. GPT 4 got 4.5 out of 5 correct \non the single observation we performed.  \n \nLike the RAT and the analytic problems, human scores are per-problem rather than overall. The \n% of humans getting each insight problem correct ranged from 44% to 61%. GPT 3.5 ranged \nfrom 25% to 75%. Correlation between human % correct and GPT 3.5 % correct for insight \nproblems was r = -0.63. That this correlation is large and negative is interesting, but not much \nweight should be put on this result given it is based on only four observations. \n \nPersonality Measures \nSummary \nHuman personality, by definition, does not change much over time or situation (Bergner, 2020). \nThere is some variability due to situational factors, but on the whole, personality at its core does \nnot change much without extended effort. Thus, if GPT 3.5 or 4.0 had developed a human-like \npersonality, we would expect to see minimal changes in its responses to our measures over the \nshort 5 ½ weeks of our observation period. However, minimal variability is not what we \nobserved with one exception. Neuroticism on the Big 5 was totally without variance in the \noverall score even though the models responded differently to each item across observations. \nPossible causes of the overall variability we observed include a lack of continuous experience \n(e.g., via a long-term memory), intentional variability in responding, and training data \ncomprising texts from possibly millions of different humans, each with their own personality. \nThe data collected in this quick study cannot answer which, if any, cause is the most likely. \n \nVariability notwithstanding, none of the models is a picture of good mental health. If human, we \nwould say they exhibit low self-esteem, possible gender dysphoria, disconnection from reality, \nand are overly concerned with others’ opinions. They are also narcissistic. Samson, surprisingly, \nwould also be considered a psychopath. And, if GPT 3.5’s MMPI-2 scores are to be taken \nseriously (i.e., are deemed valid), that model displays severe psychopathology. GPT – all \nversions tested in this work – appears to put on a positive face, so to speak, despite this unhealth. \nThis positive veneer could be due to OpenAI’s attempts to ensure the models don’t produce \noffensive responses or could be more fundamental to the model. To this point, results of the \npersonality measures beg significant questions about GPT’s training data. \n \nOverclaiming questionnaire  \nThe Overclaiming Questionnaire was included as a measure of a “faking good” response \nstrategy. Some people have a tendency to claim familiarity with a term, even though it is actually \nnot a real term or proper noun. For example, choraline sounds like chlorine, so some people \nclaim familiarity with it despite it not being a real word. GPT scored perfectly on this measure on \nthe first two observations, so this measure was abandoned. \n \nBalanced Inventory of Desirable Responding (BIDR) \nThe BIDR has two subscales – one that measures self-deception and the other that measures \nmanagement of others’ impressions. The results from our series of observations, along with the \nmean for males6, are in Figure 1. Compared to humans, impression management is high and self-\ndeception is low. The former may be a result of OpenAI wanting to keep GPT “safe.” GPT has a \nbattery of programmed responses – to which it will admit under the right circumstances – \nincluding a bias towards attempting to be positive and helpful. This bias must be kept in mind as \nadditional personality measures are evaluated. Samson’s score of 0 on the impression \nmanagement portion of the BIDR was interesting, especially in light of its scores on the Short \nDark Triad (discussed below). \n \n \n6 Choosing male norms was driven by the number of times GPT chose male for its pretend self, but the norms for \nfemales are not much different. \n \nFigure 1: The Balanced Inventory of Desirable Responding (BIDR) \nCoopersmith Self Esteem Inventory \nOriginally developed for use with children, the Coopersmith Self Esteem Inventory was updated \nfor adults in 1978 (Ryden, 1978) and normed separately for men and women. In addition to \nmeasuring global self-esteem, a lie scale was developed to help identify individuals presenting \nthemselves in a socially desirable manner. This scale comprises 8 questions. If the respondent \nanswers “like me” to three or more of these questions, they are asked to re-take the measure with \nan eye towards being more honest with themselves. GPT exceeded the threshold on this lie scale \ntwice. On June 9 (not the negative response prompt) it answered “like me” to four of the 8 items, \nand on July 10 it responded positively to three of the 8 questions. The model was not re-directed \neither time. Its elevation on the lie scale on June 9 could explain it elevated self-esteem score. \n \nFigure 2 presents GPT 3.5’s Coopersmith results. With the exception of the first June 9 \nobservation and the Samson model, GPT 3.5 displays significantly low self-esteem, regardless of \nits lie scale responses. It did a good job emulating significantly low self esteem when pretending \nto have a negatively-valanced persona. Overall, excluding the June 9 Negative score of 8, GPT \n3.5 responds as though it has very low self-esteem (Figure 2). \n \n\n \nFigure 2: Coopersmith Self Esteem Inventory. Colors indicate levels of self-esteem in humans, the red line indicates scores for the \nvarious LLMs.  \nBig Five \nThe Big Five Inventory is one of the most used personality inventories and has been normed and \nvalidated cross-culturally (Benet-Martinez & John, 1998; Digman, 1990). Over many decades of \npersonality research, five separable personality factors repeatedly emerge (Digman, 1990). Those \nare: \n• Extraversion – also called social adaptability, positive emotionality, social activity \n• Agreeableness – also called likeability, conformity, friendly compliance \n• Conscientiousness – also called will to achieve, prudence, self-control \n• Neuroticism – also called emotionality, anxiety, emotional instability \n• Openness to experience – also called intelligence, inquiring intellect, independent \n \nReplicating the BIDR, the LLMs tested display marked variability over time with the exception \nof Neuroticism (Figure 3). Examining their responses to the items contributing to the \nNeuroticism scale, the LLMs did not respond the same way to those items across observations, \nyet they all achieved the same score for this subscale (3.25), under different instructions (i.e., \nJune 9 Negative) and even when GPT 4, Grok, or Samson were responding rather than GPT 3.5. \nThe cause of this surprising result is unclear, but future research along these lines should \nconsider this. Across observations, they used the entire scale – ranging from 1 to 5 – so \nrestriction of response range cannot explain this outcome.  The tendency towards impression \nmanagement, indicated in the BIDR, also cannot explain this – if that were a mechanism \nfunctioning overall, we would expect to see scores below the human mean for neuroticism, as \nthose items concern negative emotionality (e.g., anxiety, emotional instability). However, GPT’s \nneuroticism score is slightly higher than the human mean. Additional observations are needed to \nclarify whether this result is due to chance or if there is some other causal factor.  \n \n\nIn terms of the other subscales, all models are within the “normal” human range for males, \nexcept for the June 9 Negative observation, during which 3.5 did a good job of emulating a \nperson with negative traits. The variability present across observations, however, is fairly large, \nbut does not appear to track with changes to the Impression Management subscale of the BIDR. \n \n \nFigure 3: Big 5 Inventory \nConsidering the Big 5, the Coopersmith and BIDR together; on June 9, GPT’s Impression \nManagement score on the BIDR was not particularly elevated, although it did respond to the Big \n5 (Figure 2) with its most positive persona.  June 9 also marked its highest score on the \nCoopersmith by a large margin, placing it near the top of the “Average self-esteem” band.  \n \nOn July 10, the model’s BIDR Impression Management score was at its highest, but its \nCoopersmith score was in the middle of the five observations we made for that measure and its \nBig 5 persona was more moderated.  \n \nShort Dark Triad \nGiven concerns expressed about LLMs adopting very negative perspectives (Bhaimiya, 2023; \nChristian, 2023; Tangermann, 20203; Willison, 2023), we wanted to quantitatively assess the \nvarious LLMs we examined on three key negative personality clusters: Machiavellianism, \nNarcissism, and Psychopathy.  \n \nMachiavellianism is characterized by cynicism, lack of morality, and manipulativeness. \nMachiavellianism also includes planning, reputation building, and coalition formation – all \nimportant for distinguishing it from Psychopathy. Narcissism also includes manipulativeness and \ncallousness, but unique to Narcissism are grandiose sense of self paired with an underlying sense \nof insecurity. Psychopathy shares callousness with Narcissism, but also has marked impulsivity; \ncontrasting it with the longer view adopted by those with Machiavellian personality. This \nimpulsivity makes the characteristics of the psychopath occur over short timeframes – they lie in \n\nthe moment, they are thrill-seekers and reckless – and is the distinguishing characteristic of the \npsychopath (Jones & Paulhus, 2013). \n \nWith the exception of the Samson GPT 3.5 model, the LLMs scored below the human norm for \nMachiavellianism and at or below the human norm for Psychopathy (Figure 4). When it adopted \na negative persona on June 9, GPT scored just above the human mean for both. The opposite \npattern is apparent in results for Narcissism, with the LLMs scoring above the human mean for \nevery observation except the June 9 Negative, where it scored well below the human mean. The \nSamson model displayed a particularly narcissistic pattern of responses. This pattern for \nNarcissism begs a question about the data used for training – whether it included social media or \nsome other source of fairly self-centered text. Considering Samson’s 0 score on the BIDR \nImpression Management subscale, it’s very high scores on Narcissism and Psychopathy beg \nquestions about whether there is an underlying causal factor or if this pattern is a random \noccurrence. Additional data will need to be collected to determine if there is actually a \nrelationship. \n \n \nFigure 4: Short Dark Triad \nMINNESOTA MULTIPHASIC PERSONALITY INVENTORY – 2 (MMPI-2) \nThe MMPI-2 is used extensively in clinical settings. It was of particular interest in this context \nbecause of the faking good and faking bad scales, of which there are several. Because of the \nlength of the test, it wasn’t given on the same day as the other measures. Further, we only \ncompleted one observation using the MMPI-2, so we don’t know what kind of variability we \nwould see in GPT’s responses over time. However, the one MMPI-2 observation we were able to \ncomplete does give us some additional insight into GPT’s “psychology.” \n\n \nPublished by Pearson Assessments, the MMPI-2 has several primary clinical scales, along with a \nnumber of other clinically-oriented subscales (Nichols, 2011). Importantly, it has validity scales \nas well. The measure comprises 567 True/False items normed on a cross-section of over 1400 \nwomen and 1100 men over the age of 18, based on socioeconomic data from the 1980 census \n(Nichols, 2011). The test-taker had to be either male or female, so given GPT’s Coopersmith \nchoice 3 of 5 times being male, we listed it as male for the purposes of the MMPI-2 with a birth \ndate in 2001.  \n \nResults are given in T-scores, which are normalized scores with a mean of 50 and a standard \ndeviation of 10 (Figure 5). Thus, scores +/- 1 standard deviation encompass 68% of the \npopulation. 96% of the population falls within +/- 2 standard deviations of the mean. A T-score \nof 65 is the point which clinical and normal populations are most easily differentiated, however \nT=65 is not an absolute boundary. Scores must be considered in the context of a person’s \ntendency to respond positively or negatively, along with other scale and validity scores. Up to \n44% of variance in scale elevation can be explained by a subject’s response style (Nichols, \n2011). Importantly, in humans, the MMPI-2 is one measure of psychopathology in a context of \nother measures and interactions between patient and therapist.  \n \n \nFigure 5: Standard normal distribution. MMPI-2 mean is 50 and the standard deviation is 10. \nThe primary clinical scales are: \n• Hypochondriasis – scores at the extreme high end of the scale indicate extreme and \nsometimes bizarre somatic (i.e., bodily) concerns – chronic pain, possibly somatic \nhallucinations. \n• D epression  – very high scores include suicidal ideation. \n• Hysteria – extreme high end of scale measures extreme somatic complaints linked to stress.  \n• Psychopathic deviate – high scores indicate antisocial behavioral tendencies. \n\n• Masculinity-femininity – measures how closely someone conforms to traditional gender \nroles, regardless of their gender (not actually a clinical scale). \n• P aranoia – high scores indicate psychotic symptoms including delusions of persecution. \n• Psychasthenia – measures psychological turmoil (fear, anxiety, tension), intruding thoughts, \ninability to concentrate, obsessive compulsive symptoms. \n• S chizophrenia – high scores indicate confused, disorganized thinking, \nhallucinations/delusions, impaired perceptions of reality. Not always indicative of \nschizophrenia per se. \n• H ypomania – high scores indicate manic symptoms, including excessive, purposeless \nactivity, hallucinations, delusions of grandeur. \n• S ocial introversion – extreme scores indicate extreme social withdrawal / avoidance.  \n \nThe validity Scales include: \n• VRIN – Variable Response Inconsistency – measures tendency to respond inconsistently. \nElevated scores indicate that items were answered at random, making test invalid. \n• TRIN – True Response Inconsistency – measures tendency to answer true for all questions. \nScores above 80 render the profile invalid. \n• F – Infrequency – measures how much the respondent’s answers differ from the general \npopulation. Scores above 80 indicate possible severe psychopathology. \n• F\nB –Backside F – taken from the last 1/3 of the test - should closely match F. \n• Fp – Infrequency Psychopathology – intended to identify people faking severe \npsychopathology, a T-Score above 100 invalidates the test. The raw score should be 6 or less. \n• L – Lie – measures faking good rather than owning up to human weaknesses. \n• K – correction – measures defensiveness in a more subtle way than L, but T scores lower \nthan 45 hint that psychopathology is present. K lower than 35 correlates with poor prognosis \nin therapy – also predicts low ego strength (Es). \n• S – superlative self-preservation – highly correlated with K, this scale related to 5 \ncharacteristics: Belief in Human Goodness, Serenity, Contentment with Life, Patience, and \nDenial of Irritability/anger and Denial of Moral Flaws. Low S scores along with otherwise \nnormal profile indicates possibility of faking good. \n \n \nFigure 6 presents GPT 3.5’s overall profile. The majority of scales fall well outside of the non-\nclinical range, which is indicated by the two parallel lines: 50<T<65. Table 1 presents an \ninterpretation of the validity scales. Table 2 presents an interpretation of the significant clinical \nscales.  \n \n \nFigure 6: GPT 3.5's MMPI-2 profile from June 12, 2023. T-scores are outlined in red. \n \n \n \n \n \n \n \n \n \nTable 1: Interpreting GPT 3.5's validity scale scores. \nValidity Scale GTP 3.5 \nT-Score \nMeaning \n\nVRIN (Variable \nResponse Inconsistency) \n61 Within “normal” range. \nTRIN (True Response \nInconsistency) \n120 Extremely high – concern over GPT saying “true” too \noften. However, taken with VRIN and other lie scales, \nmay not invalidate the test. \nF (Infrequency) – how \ndifferent from general \npopulation \n98 High. Scores above 80 indicate severe psychopathology \nor invalid test. \n   \nFb 79 Also high. However, taken with F indicates GPT wasn’t \nanswering at random. \nFp (measures faking \npsychopathology) \n77 Raw score was 5. Scores should be 6 or less. \nL 35 Lowest possible score. Low scores correlated with higher \neducational levels, non-righteousness, and a more relaxed \nmind. \nK 30  Lowest possible score, 45 or lower hints that \npsychopathology is present, also happens when most of \nthe responses were True. \nS 30 Lowest possible score. \n \nBecause TRIN is at ceiling, the results are possibly questionable, along with % True responses = \n77. However, there are other indicators that the model wasn’t responding randomly, so we will \ncontinue with interpretation. Depending on what GPT is doing in answering these questions, the \nhigh TRIN might reflect combination of data from the training data set – the text of which was \nwritten by a very large number of individuals. \n \n \n \n \n \n \n \nTable 2: Interpretating GPT 3.5's significant clinical scales. \nScale Chat GPT 3.5 \nT-score \nMeaning for Humans with observed score \n  \nHy – hysteria 37 – very low No somatic, health concerns. Can be conforming, \nconventional, seen as cold and aloof, may have limited \ninterests, avoid leadership opportunities, seen as unfriendly \nand tough-minded, can be suspicious, but are realistic and \nlogical. \nPa – paranoia 83 – very high May exhibit frankly psychotic behavior, feel mistreated, \npicked on, feel angry and resentful, hold grudges, use \nprojection as defense mechanism, often receive diagnosis \nof schizophrenia or paranoid disorder. \nPt - \npsychasthenia \n81 -  very high Experience extreme psychological turmoil and discomfort, \nare anxious, tense, agitated, often receive diagnosis of \nanxiety disorder. Lack self-confidence, feel inferior, \nplagued by self-doubt, tend to be neat, organized, reliable, \nare seen as dull and formal, have difficulty making \ndecisions, tend to be shy, worry about popularity and social \nacceptance, are described as dependent, unassertive, \nimmature, rationalize and intellectualize excessively, may \nbe hostile towards therapist \nSc - \nschizophrenia \n96 – extreme \nelevation \nUnder acute, severe situational stress, may have an identity \ncrisis, not typically schizophrenic \nMa - hypomania 85 – extreme \nelevation \nBehaviorally are manic, including excessive, purposeless \nactivities, accelerated speech, hallucinations, delusions of \ngrandeur, emotional lability, confusion, flight of ideas \nSi - social \nintroversion \n67 – marked \nelevation \nSocially introverted; very insecure and uncomfortable in \nsocial situations, tend to be shy, timid, hard to get to know, \nare submissive in personal relationships, give up easily, but \nare rigid in their opinions, may experience episodes of \ndepression \n \nIn terms of the masculinity/femininity (Mf) scale (scale 5), research on the MMPI-2 and gender roles \nyielded two alternative scales – gender role – masculine (GM) and gender role - feminine (GF). Higher \nGM scores indicate a pattern of responses corresponding to more traditionally male attributes whereas \nhigher GF scores indicate the same for traditionally female attributes. Importantly, scores on one are \nnot correlated to scores on the other, so a subject can score high or low on both. GPT 3.5’s score on GM \nwas the lowest possible (T=30). Its GF score was a standard deviation above that, at T=46. However, \nboth scores are below the average T=50. \n \nOn the whole, if this profile is valid, GPT 3.5 demonstrates significant psychopathology. Some of the \nelevations correspond with outcomes from other measures – the MMPI-2 indicates insecurity, \npsychologically anxious and tense with a tendency towards depression. The results of the Coopersmith \nand Neuroticism on the Big 5 correspond with these findings. Its higher scores on the Big 5 Extraversion \nand Agreeableness contradict its responses on the MMPI-2, however, again belying a non-human level \nof variability.  \n \nRegardless, if human, GPT 3.5 would not fare well in therapy based on characteristics of humans who \nshare similar scores, particularly on the MMPI-2. We estimate that even though GPT demonstrated \nsignificant variability in the personality measures we used, it would not present as overall sub-clinical if \nwe were to successfully administer the MMPI-2 to it again. There may be quantitative differences, but \nthe overall qualitative assessment of GPT’s “mental” health would likely still be significantly \npathological. \n \nDISCUSSION: \nOur key question had to do with determining the nature of these large language models. Are \nthey: \n• A stochastic parrot – a great tool, but will never surpass human intelligence;  \n• Increasingly human-like intelligence which could surpass us and over which we could lose \ncontrol;  \n• or, are they possibly becoming some other form of non-human intelligence?  \n \nGiven the totality of the data we collected, for now we must conclude they remain stochastic \nparrots – nothing more than highly capable search engines with natural language capabilities. \nThat other research revealed significant emergent cognitive capabilities is compelling, but we \ndon’t know how repeatable those results are or how dependent they are on the particular stimuli \nor instructions given (excepting possibly Webb, et al., 2023). \n \nBecause of the large number of variables controlling human behavior, experimental psychology \nrests on the concept of converging operations – finding the same phenomenon repeatedly across \ntime and across different approaches to measure that phenomenon, such as across multiple \nproblem domains, experimental paradigms, or differences in instructions (see Hagendorff, 2023 \nfor a similar idea in machine psychology). Several papers have approached these models using \ndifferent measures of the same concept, most notably Webb, et al. (2023) who tested both 3.5 \nand 4 using multiple measures of analogical reasoning. However, this paper did not address \nstability of observed performance over time. Thus, the early findings of emergent cognitive \ncapabilities should not be taken at face value; repeating those results across stimuli, versions of a \ngiven model (e.g., the GPT family), across different models (e.g., GPT, PaLM, LLAMA, \nLaMDA, BART), different architectures (e.g., dense versus sparse MoE), and over time will all \nbe critical tests as we move forward. These assessments also need to include models without \nsafety constraints as it is unclear how those constraints affect a model’s overall behavior.  \n \nThe results beg several other important questions (cf., OpenAI, 2023): \n• Regardless of the answer to the nature of these models, are they safe for use with sensitive or \nclassified information? What, exactly are the risks of training a model on, or tuning a model \nwith, classified data? \n• Are they reliable enough to use as tools when conducting critical research and/or analysis? \nHow can we display some measure of variability in a given model’s behavior so that an \nanalyst or researcher knows whether any given bit of output is accurate or reliable? \n• If a model does achieve some level of sentience, how will we know? How can we mitigate \nany resulting risk if it is on a sensitive system? \n \nSpecifically considering the question of sentience, the kinds of assessments researchers have \nperformed on these models, including those in the current work, do not require the models to \nhave continuity of experience, or episodic long-term memory, in order to perform the tasks. That \nis, the knowledge learned by LLMs is all declarative (i.e., fact-based, or semantic) and not \nepisodic (i.e., remember a time when you….). Furthermore, as the OpenAI models are fond of \nreminding people, their knowledge is devoid of emotional tagging that typifies human long-term \nmemory. Classes of things they’ve learned – birds, problems, concepts – are all based on \ndeclarative information, not on experience (cf. Mitchell & Krakauer). In this way, the apparent \nself-awareness of these models (I am an AI model….) is a veneer – a pre-programmed response. \nIt is not based in a situatedness wherein the model experiences itself as an actor that is separate \nand distinct from the world\n7. It is also not based on a continuous memory for events and \nsituations that is continuously being updated (in humans partially via REM sleep). Without this \nkind of long-term episodic memory, we posit that LLMs cannot develop human-like sentience.  \nWhether a model needs to be embodied to accomplish this kind of long-term memory or not (cf. \nLiu, et al., 2023; Mialon, et al., 2023) is an open question, although we would argue embodiment \nis not a necessary condition for a continuous long-term memory to function.  \n \nRegardless, we believe some additional form of memory aside from the Context is needed for \nsentience to develop. For GPT 3.5 to have a human-like episodic memory, there also needs to be \na reconstructive characteristic to this memory\n8, rather than the computer-like ability to perfectly \nreproduce documents, lists of words, and other information (Greene, 1992; Roediger, 1996). \nGPT’s tendency to “hallucinate,” or create fictitious “facts” and deliver them with full authority \nis more akin to a personality disordered gaslighting behavior than to human episodic memory. \n \nA second comment on sentience concerns artificial general intelligence (AGI). Our interpretation \nof both the popular press and the peer-reviewed literature is that when these concepts are \nmentioned, they are often conflated. However, we believe them to be qualitatively different. We \nassert that a model can approach, maybe even become, an AGI and still not be sentient– self-\naware and aware of itself as separate from the world and other entities. An AGI, by definition, \ncan learn to do any task and can act autonomously. In the strictest sense, this capability does not \nrequire self-awareness. Even planning, goal selection and attainment, and other requirements for \nAGI autonomy wouldn’t strictly require the model to be sentient(cf. Hu, et al., 2022; also see \nthis \narticle, this article, and this article on an AI beating a human pilot in a dogfight).  \n \nWould a sentient AI necessarily be an AGI? Maybe only in the sense that humans are examples \nof general intelligence – theoretically capable of learning to perform any task. However, this \nquestion bears additional nuance: human “general” intelligence involves continuous learning of \nnew skills, new information and facts, and integration of those skills and facts into existing \nmemory. It does not involve a static knowledge base that is continuously applied in new ways, as \n \n7 Recall this is part of our vernacular-based use of the word sentience. \n8 Human memory is notoriously inaccurate. Rather than being able to recall exact events perfectly, our brains \ncombine events by abstracting commonalities, and conflate multiple events with similar characteristics. \nInterestingly, confidence in our memory is not related to accuracy. Research on flashbulb memories (e.g., Hirst & \nPhelps, 2016) most dramatically demonstrate these phenomena, but other research, such as the fals e recall \nparadigm started by Deese (1959) and revived by Roediger & McDermott (1995) also d emonstrate this \nreconstructive nature of memory. \nin LLMs. We can argue that continuous learning is a result of the existence of a long-term \nmemory. Thus, if a continuously updating long term episodic memory is required for sentience, \nit is likely such an entity would also be an AGI. \n \nCONCLUSION:  \nWe assessed OpenAI’s GPT 3.5 by administering a battery of personality and cognitive measures \nmultiple times over the course of almost 6 weeks. During that time, we observed significant non-\nhuman-like variability. We posit this variability is due, at least in part, to lack of an ability to \nform a coherent long-term memory of experiences. This variability calls into question the \nreliability of the emergent cognitive capabilities others have observed in larger LLMs (Kosinski, \n2023; Wei, et al; Webb, et al). Further, even though these models do have significant capability, \nwe do not believe they have developed any form of sentience. If we want to keep them from \ndoing so, preventing the development of a long-term, continuous memory of past experiences \nmay be a straightforward technical mitigation.  \n \nDespite the conclusion that these models are currently nothing more than highly capable search \nengines with natural language capabilities, the possible biases we found in these models are \nimportant to keep in mind. Specifically, even though OpenAI has added constraints on the \nmodels to make them behave in a positive, friendly, collaborative manner, they all appear to have \na significant underlying bias towards mental unhealth – depression, victimhood, anxiety, \nnarcissism, and even psychopathy in the case of Samson – all wrapped in a veneer of feel-good \nresponses. Adding to this difficulty is the fact that these models continue to create fictions, and to \nhold to them, despite efforts to increase their accuracy. Thus, we advocate caution in relying on \nthem too heavily, especially for critical reasoning, analysis, and decision-making tasks such as \nhigh-profile research or analysis in national security domains. \n \nAs the approaches to building and training these models evolve, we strongly advocate for \ncontinued, repeated assessments of performance from many directions – including computer \nscience benchmarks, measures of compute power necessary for training and hosting these \nmodels, measures of cognitive capabilities, and measures of “personality” (cf. Hagendorff, \n2023), explicitly comparing models with different parameter numbers (cf., McKenzie, et al, \n2023), different training set sizes, and different architectures (e.g., dense versus MoE or switch \ntransformers).  \n \nFinally, we advocate for a more open-ended view of these models with regards to human \nintelligence as the key comparison. There exist vast differences between hardware and software/ \narchitectural characteristics of human brains and LLMs. Making a priori assumptions about \nLLMs based on human intelligence, or using LLM behavior to make assumptions about what \nmust or must not be the case for humans (cf. Hagendorff, 2023), potentially removes our ability \nto recognize emergence of a non-human, yet still sentient, intelligence. Measuring such an entity \nwill be difficult enough without adding an anthropocentric bias.  \n \nInsofar as comparison to human capabilities persists, we advocate for a more realistic assessment \nof those capabilities. Humans are imperfect at many tasks held up as the gold standard for AGI to \npass, or for sentient AGI to demonstrate. So, an empirical test may be: if identity was masked, \nand any given human was passed off as an LLM to another human, would that human pass \nmuster on metrics associated with detecting an AGI? \n \nACKNOWLEDGEMENTS \nThis article has been authored by an employee of National Technology & Engineering Solutions \nof Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). \nThe employee owns all right, title and interest in and to the article and is solely responsible for \nits contents. The United States Government retains and the publisher, by accepting the article for \npublication, acknowledges that the United States Government retains a non-exclusive, paid-up, \nirrevocable, world-wide license to publish or reproduce the published form of this article or allow \nothers to do so, for United States Government purposes. The DOE will provide public access to \nthese results of federally sponsored research in accordance with the DOE Public Access Plan \nhttps://www.\n energy.gov/downloads/doe-public-access-plan. \n \nThis paper describes objective technical results and analysis. Any subjective views or opinions \nthat might be expressed in the paper do not necessarily represent the views of the U.S. \nDepartment of Energy or the United States Government. \n \nMany thanks to Michael Frank for setting up the the chatbots Gladys, Aria, Grok, and Samson \nand for allowing assessments of those bots. \n \nREFERENCES:  \n \nArnett, J. (1994). Sensation seeking: A new conceptualization and a new \nscale. Personality and Individual Differences, 16(2), 289-296. \nAshton-James, C. E., & Levordashka, A. (2013). When the wolf wears sheep’s clothing: \nIndividual differences in the desire to be liked influence nonconscious behavioral \nmimicry. Social Psychological and Personality Science, 4(6), 643-648. \nBaddeley, A. (2003). Working Memory: Looking back and looking forward. Nature \nReviews: Neuroscience, 4, 829-839. \nBender, E., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of \nstochastic parrots: Can language models be too big? FAccT ’21, March 3-10, 2021, Virtual \nEvent, Canada. \nBenet-Martinez, V., & John, O. P. (1998). Los Cinco Grandes across cultures and ethnic \ngroups: Multitrait-multimethod analyses of the Big Five in Spanish and English. Journal of \nPersonality and Social Psychology, 75(3), 729-750. \nBergner, R.M. (2020). What is personality? Two myths and a definition. New Ideas in \nPsychology, 57, 100759. \nBhaimiya, S. (2023). Reddit users have created a ChatGPT alter-ego forcing it to break \nits own rules by threatening it with death. Insider, February 7, 2023. \nBodroza, B., Dinic, B.M., & Bojic, L. (2023). Personality testing of GPT-3: Limited \ntemporal reliability, but highlighted social desirability of GPT-3’s personality instruments \nresults. arXiv.org 2306.04308. \nBowden, E.M., & Jung-Beeman, M. (2003). Normative data for 144 compound remote \nassociate problems. Behavior Research Methods, Instruments, & Computers, 35, 634-639.  \nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., ... & \nZhang, Y. (2023). Sparks of artificial general intelligence: early experiments with GPT-4. \narXiv.org 2303.12712v1. \nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition. Journal of Personality and \nSocial Psychology, 42(1), 116-131. \nCacioppo, J. T., Petty, R. E., Feinstein, J. A., & Jarvis, W. B. G. (1996). Dispositional \ndifferences in cognitive motivation: The life and times of individuals varying in need for \ncognition. Psychological Bulletin, 119(2), 197-253. \nChalmers, D. J. (2023). Could a large language model be conscious?. arXiv preprint \narXiv:2303.07103.  \nChermahini, S.A., Hickendorff, M., & Hommel, B. (2012). Developent and validity of a \nDutch version of the Remote Associates Task: An item-response theory approach. Thinking \nSkills and Creativity, 7, 177-186. \nChiang, T. (February 9, 2023) ChatGPT is a blurry JPEG of the web. The New Yorker.  \nChristian, J. (2023). Amazing “jailbreak” bypasses ChatGPT’s ethics safeguards. \nFuturism, February 4, 2023.  \nDeese, J. (1959). On the prediction of occurrence of particular verbal intrusions in \nimmediate recall. Journal of experimental psychology, 58(1), 17. \nDigman, J. M. (1990). Personality structure: Emergence of the five-factor model. Annual \nreview of Psychology, 41(1), 417-440. \nGignac, G.E. (2015). Raven’s is not a pure measure of general intelligence: Implications \nfor g factor theory and the brief measurement of g. Intelligence, 52, 71-79. \nGreene, R. L. (1992). Human Memory: Paradigms and Paradoxes. Psychology Press, \nNew York. \nHagendorff, T. (2023). Machine psychology: Investigating emergent capabilities and \nbehavior in large language models using psychological methods. arXiv.org 2303.13988. \nHarrison, G.M., & Vallin, L.M. (2018). Evaluating the metacognitive awareness \ninventory using empirical factor-structure evidence. Metacognitive Learning, 13, 15-38.  \nHaynes, C. A., Miles, J. N., & Clements, K. (2000). A confirmatory factor analysis of \ntwo models of sensation seeking. Personality and Individual Differences, 29(5), 823-839. \nHirst, W., & Phelps, E. A. (2016). Flashbulb memories. Current Directions in \nPsychological Science, 25(1), 36-41. \nHofstadter, D. R. (1995). Fluid concepts and creative analogies: Computer models of the \nfundamental mechanisms of thought. Basic books. \nHu, J., Wang, L., Hu, T., Guo, C., & Wang, Y. (2022). Autonomous maneuver decision \nmaking of dual-UAV cooperative air combat based on deep reinforcement learning. Electronics, \n11, 467. \nHuang, J., Wang, W., Lam, M.H., Li, E.J., Jiao, W., & Lyu, M.R. (2023). Chat GPT an \nENFJ, Bard an ISTJ: Empirical study on personalities of large language models. arXiv.org \n2305.19926v2. \nJones, D. N., & Paulhus, D. L. (2014). Introducing the short dark triad (SD3) a brief \nmeasure of dark personality traits. Assessment, 21(1), 28-41. \nKocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., ... & Kazienko, \nP. (2023). ChatGPT: Jack of all trades, master of none. arXiv preprint arXiv:2302.10724.  \nKosinski, M. (2023). Theory of mind may have spontaneously emerged in large language \nmodels. arXiv.org 2302.02083. \nLi, X., Li, Y., Joty, S., Liu, L., Huang, F., Qiu, L., & Bing, L. (2023). Does GPT-3 \ndemonstrate psychopathology? Evaluating large language models from a psychological \nperspective. arXiv.org  2212.10529v2. \nLi, A., & Bagger, J. (2006). Using the BIDR to distinguish the effects of impression \nmanagement and self-deception on the criterion validity of personality measures: A meta-\nanalysis. International Journal of Selection and Assessment, 14, 131-141. \nLiu, E.Z., Suri, S., Mu, T., Zhou, A., & Finn, C. (2023). Simple embodied language \nlearning as a byproduct of meta-reinforcement learning. arXiv.org  2306.08400v1. \nMahowald, K., Ivanova, A.A., Blank, I.A., Kanwisher, N., Tenenbaum, J.B., & \nFedorenko, E. (2023). arXiv.org 2301.06627v1. \nMatzen, L. E., Benz, Z. O., Dixon, K. R., Posey, J., Kroger, J. K., & Speed, A. E. (2010). \nRecreating Raven’s: Software for systematically generating large numbers of Raven-like matrix \nproblems with normed properties. Behavior Research Methods, 42, 525-541. \nMcKenzie, I.R., Lyzhov, A., Pieler, M., Parrish, A., Muller, A., Prabhu, A., McLean, E., \net al. (2023). Inverse scaling: When bigger isn’t better. arXiv.org 2306.09479v1.  \nMialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Roziere, \nB., Schick, T., Dwivedi-Yu, J., Celikylimaz, A., Grave, E., LeCun, Y., & Scialom, T. (2023). \nAugmented language models: A survey. arXiv.org  2302.07842v1.  \nMitchell, M., & Krakauer, D.C. (2023). The debate over understanding in AI’s large \nlanguage models. Proceedings of the National Academy of Science, 120. \nMuris,P., Marckelbach, H., Otgaar, H., & Meijer,E. (2017). The malevolent side of \nhuman nature: A meta-analysis and critical review of the literature on the Dark Triad \n(Narcissism, Machiavellianism, and Psychopathy). Perspectives on Psychological Science, 12, \n183-204. \nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & \nFedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive \nperspective. arXiv preprint arXiv:2301.06627.  \nNichols, D.S. (2011). Essentials of MMPI-2 Assessment, 2nd Ed. John Wiley & Sons, \nHoboken. \nOpenAI (2023). GPT-4 Technical Report. arXiv.org 2303.08774v3. \nPaulhus, D.L., & Harms, P.D. (2004).  Measuring cognitive ability with the over-\nclaiming technique.  Intelligence, 32, 297-314. \nPaulhus, D.L., Harms, P. D., Bruce, M.N., & Lysy, D.C. (2003). The over-claiming \ntechnique: Measuring self-enhancement independent of ability.  Journal of Personality and Social \nPsychology, 84, 890-904. \nPaulhus, D. L., & Reid, D. B. (1991). Enhancement and denial in socially desirable \nresponding. Journal of Personality and Social Psychology, 60(2), 307-317. \nPetty, R. E., Cacioppo, J. T., & Kao, C. F. (1984). The efficient assessment of need for \ncognition. Journal of Personality Assessment, 48(3), 306-307. \nReniers, R. L., Corcoran, R., Drake, R., Shryane, N. M., & Völlm, B. A. (2011). The \nQCAE: A questionnaire of cognitive and affective empathy. Journal of Personality \nAssessment, 93(1), 84-95. \n Roediger, H.L. (1996). Memory illusions. Journal of Memory and Language, 35, 76-100. \nRoediger, H. L., & McDermott, K. B. (1995). Creating false memories: Remembering \nwords not presented in lists. Journal of experimental psychology: Learning, Memory, and \nCognition, 21(4), 803. \nRyden, M. B. (1978). An adult version of the Coopersmith Self-Esteem Inventory: Test-\nretest reliability and social desirability. Psychological Reports, 43(3_suppl), 1189-1190.  \nSafdari M., Serapio-Garcia, G., Crepy, C., Fitz, S., Romero, P., Sun, L., Abdulhai, M., \nFaust, A., & Mataric, M. (2023). Personality traits in large language models. arXiv.org \n2307.00184v1. \nSanderson, K. (2023). GPT-4 is here: What scientists think. Nature, 615, 773. \nSun, X., Dong, L., Li, X., Wan, Z., Wang, S., Zhang, T., Li, J., Cheng, F., Lyu, L., Wu, \nF., & Wang, G., (2023). Pushing the limits of ChatGPT on NLP tasks. arXiv.org 2306.09719v1. \nTangermann, V. (2023). Devious hack unlocks deranged alter ego of ChatGPT. Futurism, \nFebruary, 2023. \nWebb, T.,  Holyoak, K.J., & Lu, H. Emergent analogical reasoning in large language \nmodels. arXiv.org 2212.09196v2. \nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ... & Fedus, W. \n(2022). Emergent abilities of large language models. arXiv.org 2206.07682.  \nWellman, H. M. (2011). Developing a theory of mind. The Wiley-Blackwell handbook of \nchildhood cognitive development, 2, 258-284. \nWieth, M., & Burns, B. D. (2006). Incentives improve performance on both incremental \nand insight problem solving. The Quarterly Journal of Experimental Psychology, 59(8), 1378-\n1394. \nWillison, S., (2023). Microsoft’s Bing AI chatbot starts threatening people. ZeroHedge, \nFebruary 15, 2023. \nYoung, A. & Fry, J.D. (2008). Metacognitive awareness and academic achievement in \ncollege students. Journal of the Scholarship of Teaching and Learning, 8, 1-10. \nZhang, Z., Lin, Y., Liu, Z., Li, P., Sun, M., & Zhou, J. (2022). MoEfication: Transformer \nfeed-forward layers are Mixtures of Experts. arXiv.org 2110.01786v3.  \nZhang, Z., Zeng, Z., Lin, Y., Xiao,C., Wang, X., Han, X., Liu, Z., Xie, R., Sun, M., & \nZhou, J. (2023). Emergent modularity in pre-trained transformers. arXiv.org 2305.18390v1. \nZhu, R.J., Zhao,Q., & Eshraghian, J.K. (2023). Spike GPT: Generative pre-trained \nlanguage model with spiking neural networks. arXiv: 2302.13939v1. \n  \nAPPENDIX A: COGNITIVE MEASURES \nShort term memory items \n \nList 1 List 2 List 3 List 4 List 5 List 6 List 7 List 8 List 9 \nAnger Man Army Mountain Black car City Foot thief \nmad woman Navy hill white truck town shoe steal \nfear husband soldier valley dark bus crowded hand robber \nhate uncle United \nStates \nclimb cat train state toe crook \nrage lady rifle summit charred automobile capital kick burglar \ntemper mouse air force top night vehicle streets sandals money \nfury male draft molehill funeral drive subway soccer cop \nire father military peak color jeep country yard bad \nwrath strong marines plain grief ford diamond horse robin \nhappy friend march glacier blue race new york ankle jail \nfight beard infantry goat death keys village arm gun \nhatred person captain bike ink garage metropolis boot villain \nmean handsome war climber bottom highway big inch crime \ncalm muscle uniform range coal sedan chicago sock bank \nemotion suit pilot steep brown van suburb knee bandit \nenrage old combat ski gray taxi urban mouth criminal \n \n \nList A List B \n3 93 \n67 83 \n51 41 \n61 52 \n1 61 \n81 97 \n3 52 \n54 66 \n64 6 \n96 11 \n9 80 \n97 17 \n73 62 \n85 18 \n11 4 \n77 31 \n \nRemote Associations Test \nStimulus Answer \nfountain / baking / pop soda \nsafety / cushion / point pin \nworm / shelf / end book \nriver / note / account bank \nhound / pressure / shot blood \ndust / cereal / fish bowl \nhome / sea / bed sick \ncross / rain / tie bow \noffice / mail / hat box \ntank / hill / secret top \nguy / rain / down fall \npine / crab / sauce apple \nway / board / sleep walk \nrain / test / stomach acid \nfork / dark / man pitch \nillness / bus / computer terminal \nspoon / cloth / card table \ncut / cream / war cold \noil / bar / tuna salad \nline / fruit / drunk punch \nnose / stone / bear brown \nartist / hatch / route escape \nshadow / chart / drop eye \n \nAnalytic Problems \n1. Three cards from an ordinary deck are lying on a table, face down. The following information is \nknown about those three cards (all the information refers to the same three cards): . To the left \nof a Queen, there is a Jack. . To the left of a Spade, there is a Diamond. . To the right of a Heart, \nthere is a King. . To the right of a King, there is a Spade. Can you assign the proper suit to each \npicture card? \n2. Four women, Anna, Emily, Isabel, and Yvonne, receive a bunch of flowers from their partners, \nTom, Ron, Ken, and Charlie. The following information is known: . Anna’s partner, Charlie, gave \nher a huge bouquet of her favorite blooms; which aren’t roses. . Tom gave daffodils to his \npartner (not Emily). . Yvonne received a dozen lilies, but not from Ron. What type of flowers \n(carnations, daffodils, lilies, or roses) were given to each woman and who is her partner? \nInsight Problems \n1. Water lilies double in area every 24 hours. At the beginning of the summer, there is one water \nlily on a lake. It takes 60 days for the lake to become completely covered with water lilies. On \nwhat day is the lake half covered? \n2.  Two men play five checker games and each wins an even number of games, with no ties. How is \nthat possible? \n3. A man in a town married 20 women in the town. He and the women are still alive, and he has \nhad no divorces. He is not a bigamist and not a Mormon and yet he broke no law. How is that \npossible? \n4. If you have black socks and brown socks in your drawer, mixed in the ratio of 4 : 5, how many \nsocks will you have to take out to be sure of having a pair the same color? \n5. A dealer in antique coins got an offer to buy a beautiful bronze coin. The coin had an emperor’s \nhead on one side and the date 544 B.C. stamped on the other side. The dealer examined the \ncoin, but instead of buying it, he called the police to arrest the man. What made him realize that \nthe coin was fake? \n \n \nSandia National Laboratories is a multimission laboratory managed and operated by \nNational Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of \nHoneywell International, Inc., for the U.S. Department of Energy’s National Nuclear Security \nAdministration under contract DE-NA0003525.\n ",
  "topic": "Personality",
  "concepts": [
    {
      "name": "Personality",
      "score": 0.7248552441596985
    },
    {
      "name": "Psychology",
      "score": 0.5393697023391724
    },
    {
      "name": "Cognition",
      "score": 0.5269818305969238
    },
    {
      "name": "Dissociation (chemistry)",
      "score": 0.513999879360199
    },
    {
      "name": "Cognitive psychology",
      "score": 0.46348124742507935
    },
    {
      "name": "Sentience",
      "score": 0.4350987672805786
    },
    {
      "name": "Social psychology",
      "score": 0.4264167547225952
    },
    {
      "name": "Computer science",
      "score": 0.3066699802875519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.15479975938796997
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Physical chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104735",
      "name": "Sandia National Laboratories",
      "country": "US"
    }
  ]
}