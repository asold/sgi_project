{
  "title": "Boosting Inertial-Based Human Activity Recognition With Transformers",
  "url": "https://openalex.org/W3156725333",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1986380263",
      "name": "Yoli Shavit",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2444554279",
      "name": "Itzik Klein",
      "affiliations": [
        "University of Haifa"
      ]
    },
    {
      "id": "https://openalex.org/A1986380263",
      "name": "Yoli Shavit",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2444554279",
      "name": "Itzik Klein",
      "affiliations": [
        "University of Haifa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2998167789",
    "https://openalex.org/W6770761718",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3022953390",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W2153195033",
    "https://openalex.org/W2774684635",
    "https://openalex.org/W2736191430",
    "https://openalex.org/W6728817311",
    "https://openalex.org/W2059732136",
    "https://openalex.org/W3044326989",
    "https://openalex.org/W2054242744",
    "https://openalex.org/W2794717185",
    "https://openalex.org/W2579984156",
    "https://openalex.org/W3023594963",
    "https://openalex.org/W2944605902",
    "https://openalex.org/W2912791673",
    "https://openalex.org/W2980803040",
    "https://openalex.org/W2108225567",
    "https://openalex.org/W3125141208",
    "https://openalex.org/W6675238173",
    "https://openalex.org/W6704286305",
    "https://openalex.org/W2156762151",
    "https://openalex.org/W2594230123",
    "https://openalex.org/W1923002919",
    "https://openalex.org/W1989665047",
    "https://openalex.org/W3017181332",
    "https://openalex.org/W2228798151",
    "https://openalex.org/W2567482009",
    "https://openalex.org/W2898186212",
    "https://openalex.org/W2743898215",
    "https://openalex.org/W6746970611",
    "https://openalex.org/W2889409040",
    "https://openalex.org/W2789136377",
    "https://openalex.org/W6747834056",
    "https://openalex.org/W2129793335",
    "https://openalex.org/W3015014633",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3102326534",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2344284192",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2099805216",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2992422826",
    "https://openalex.org/W2540822603",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963423603",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963855167",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3103272945",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962994355",
    "https://openalex.org/W3103585945",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "Activity recognition problems such as human activity recognition and smartphone location recognition can improve the accuracy of different navigation or healthcare tasks, which rely solely on inertial sensors. Current learning-based approaches for activity recognition from inertial data employ convolutional neural networks or long short term memory architectures. Recently, Transformers were shown to outperform these architectures for sequence analysis tasks. This work presents an activity recognition model based on Transformers which offers an improved and general framework for learning activity recognition tasks. For evaluation purposes, several datasets, with more than 27 hours of inertial data recordings collected by 91 users, are employed. Those datasets represent different user activity scenarios with varying difficulty. The proposed approach consistently achieves better accuracy and generalizes better across all examined datasets and scenarios. A codebase implementing the described framework is available at: <uri>https://github.com/yolish/har-with-imu-transformer</uri>.",
  "full_text": "Received March 15, 2021, accepted March 27, 2021, date of publication April 2, 2021, date of current version April 13, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3070646\nBoosting Inertial-Based Human Activity\nRecognition With Transformers\nYOLI SHAVIT\n 1 AND ITZIK KLEIN\n 2, (Senior Member, IEEE)\n1Faculty of Engineering, Bar-Ilan University, Ramat Gan 5290002, Israel\n2Department of Marine Technologies, University of Haifa, Haifa 3498838, Israel\nCorresponding author: Yoli Shavit (yolisha@gmail.com)\nABSTRACT Activity recognition problems such as human activity recognition and smartphone location\nrecognition can improve the accuracy of different navigation or healthcare tasks, which rely solely on\ninertial sensors. Current learning-based approaches for activity recognition from inertial data employ\nconvolutional neural networks or long short term memory architectures. Recently, Transformers were shown\nto outperform these architectures for sequence analysis tasks. This work presents an activity recognition\nmodel based on Transformers which offers an improved and general framework for learning activity\nrecognition tasks. For evaluation purposes, several datasets, with more than 27 hours of inertial data\nrecordings collected by 91 users, are employed. Those datasets represent different user activity scenarios\nwith varying difﬁculty. The proposed approach consistently achieves better accuracy and generalizes better\nacross all examined datasets and scenarios. A codebase implementing the described framework is available\nat: https://github.com/yolish/har-with-imu-transformer.\nINDEX TERMS Human activity recognition, smartphone location recognition, inertial sensors, pedestrian\ndead reckoning, convolutional neural networks, Transformers, sequence analysis.\nI. INTRODUCTION\nHuman activity recognition (HAR) and smartphone location\nrecognition (SLR) aim to identify the user activity from\nsensory data. HAR measurements can be collected using\nvideo [1], utilizing channel state information of WiFi sig-\nnals [2], [3], radar [4] or by sensors installed in wear-\nable devices such as inertial sensors (accelerometers and\ngyroscopes) or ambient environment sensors (temperature\nand humidity). HAR has numerous applications, relying on\none or more of these sensors, including surveillance [5],\ngesture recognition [6], [7], gait analysis [8], healthcare [9],\n[10], and indoor navigation [11], [12]. Due to its wide appli-\ncability it has been addressed and surveyed extensively in the\nliterature [13]–[20]. In SLR, the user’s actions are reﬂected\nthrough changes in the location of the smartphone. For exam-\nple, consider a walking pedestrian where the smartphone is\nplaced in their trouser’s front right pocket (pocket mode). The\npedestrian can remove the phone to send a message (texting\nmode) and then continue holding the phone while walking\n(swing mode).\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Larbi Boubchir\n.\nHAR and SLR play a particularly important role in navi-\ngation solutions which rely solely on the smartphone inertial\nsensors. Speciﬁcally, HAR and SLR were shown to improve\nthe accuracy of traditional pedestrian dead reckoning (PDR)\nby using it as a prior [21]–[25]. SLR was also shown to\nimprove the performance of other navigation-related prob-\nlems such as step length estimation [26]–[28] and adaptive\nattitude and heading reference system (AHRS) [29].\nGiven the emerging importance of HAR and SLR for\nnavigation performance, different learning-based approaches\nwere proposed to reason about inertial sensory data. Earlier\nmethods for performing HAR for PDR relied on classical\nmachine learning techniques [22]. Hand-crafted features\nwere typically extracted from the raw signal and ﬁtted with\na classical machine learning classiﬁcation methods such\nas Support Vector Machine and Decision Trees [21], [23].\nMore recently, feed forward networks (FFN) and long short\nterm memory (LSTM) architectures were proposed for this\ntask [24], [25], removing the burden of feature engineer-\ning while achieving improved accuracy. Recent detailed\nand extensive surveys describing traditional and deep learn-\ning techniques for HAR are available for the interested\nreader [13], [18]–[20]. There, various types of deep learning\n53540 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 9, 2021\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nFIGURE 1. The proposed framework for inertial data classification with Transformers.\napproaches such as convolutional neural networks (CNNs),\nrecurrent neural networks, stacked autoencoders, temporal\nconvolutional networks, and variational autoencoders are\nreviewed.\nRecently, CNN and LSTM architectures were shown to\nimprove SLR performance, compared to other learning-based\napproaches [30]. Methods coupling SLR with step length\nestimation proposed to use CNNs with or without LSTM [28],\n[30] or employed LSTM for SLR, similarly to previous works\nlearning SLR for PDR [26]. Interestingly, CNN architectures\nwith/without LSTMs yielded on-par performance, suggesting\nthat LSTMs do not necessarily add an informative temporal\naggregation, which is missing from CNNs, for this task [30].\nIn this work, Transformers [31] are proposed for learning\ninertial-based HAR and SLR problems. Transformers imple-\nment an attention-based encoder-decoder architecture for\nsequence analysis. Attention mechanisms [32] learn to aggre-\ngate information from the entire sequence. By stacking atten-\ntional layers which scan the sequence, Transformers generate\na position and context aware representations. This method\nwas shown to outperform recurrent neural networks (RNNs)\nand LSTMs for various sequence-based problems in\nNatural Language Understanding and Computer Vision,\nachieving state-of-the-art performance [31], [33]–[36].\nHere a Transformer-based architecture is presented for\nperforming both HAR (classifying common user dynam-\nics, such as walking, standing, running, stairs and so on)\nand SLR (classifying smartphone locations, such as talk-\ning, pocket or swing). The proposed approach is the ﬁrst\nTransformer-based architecture to serve as a general frame-\nwork for inertial based activity recognition tasks.\nIn order to evaluate the proposed approach, multiple HAR\nand SLR datasets collected by a total of 91 users with more\nthan 27 hours of inertial data recordings are employed. Across\nall datasets, and considering various scenarios with changing\ndifﬁculty, the proposed approach demonstrates a consistent\nboost to accuracy and robustness.\nIn summary, the main contributions of the paper are as\nfollows:\n1) Derivation of a framework for inertial data classiﬁ-\ncation with Transformers. The proposed approach is\nthe ﬁrst to present a Transformer-based architecture\nto serve as a general framework for general activity\nrecognition in both HAR and SLR tasks.\n2) A Transformer network architecture design for han-\ndling inertial measurements, along with publicly avail-\nable implementation.\n3) Evaluation of the proposed framework for three com-\nmonly used classiﬁcations tasks: HAR, SLR and a\ncombination of the two smartphone and human activ-\nity recognition (SHAR), demonstrating a consistent\nimprovement in accuracy and a better generalization\nacross datasets.\nThe rest of the paper is organized as follows: Section II\ndescribes the proposed Transformer-based architecture for\nclassiﬁcation with inertial data. Section III reviews the\ndatasets used in this work while Section IV presents the\nresults. Finally, Section V gives the conclusions of this\nresearch.\nII. INERTIAL DATA CLASSIFICATION WITH\nTRANSFORMERS\nAn Inertial Measurement Unit (IMU) measures the speciﬁc\nforce f ∈R3 and angular velocity w ∈R3 vectors over time.\nThese two outputs are typically concatenated and aggregated\ndepending on the sensor’s recording frequency, such that a\nsample S ∈Rk×6 represents a sequence of k measurements\n(i.e., recorded in a window of size k). In this work, the prob-\nlem of activity recognition from inertial data is modelled\nas a sequence-to-one problem, where the input is a learned\nsequential embedding of the raw sensory measurements and\ntheir temporal positions. Following the success of Transform-\ners in text classiﬁcation [33], [37] and image recognition [36],\n[38], a Transformer Encoder is proposed for summarizing a\nsequence of (embedded) inertial measurements into a latent\nvector. A Multi-Layer Perceptron (MLP) with SoftMax can\nthen be applied to output the class probability distribution,\nsimilarly to standard classiﬁer heads used in sequence-to-one\narchitectures [33], [36].\nA. NETWORK ARCHITECTURE\nThe scheme proposed in this paper is depicted in Figure 1.\nGiven a sample of inertial measurements S ∈Rk×6, a series\nof four 1D convolutions is applied with GELU non-linearity.\nThis step embeds the raw data in a higher dimension d,\ngenerating a latent embedding ES ∈Rk×d (latent features).\nSimilarly to state-of-the-art Transformer-based architec-\ntures for sequence classiﬁcation [33], [36], class token\nVOLUME 9, 2021 53541\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nC ∈Rd is prepended to the embedded sequence. In addition,\nan embedding EPi ∈Rd for each position Pi in the sequence\n(including the class token) is learned and further added to\nthe latent sequence representation. The initial input Z0 to the\nTransformer Encoder is thus given by:\nZ0 =[C,Es] +EP ∈Rt×d (1)\nwith t =k +1.\nA standard Encoder architecture [31] is employed, stacking\nL layers, each consisting of a self multi-head attention (MHA)\nlayer and an MLP. In the proposed implementation, the MLP\nblock includes two fully connected (FC) layers with a hid-\nden dimension of 2 ·d and GELU non-linearity. The MHA\noperation is the core of the Transformer architecture. Given\nthree sequences of length t and dimension d, namely, a query\nQ ∈Rt×d , a key K ∈Rt×d and a value V ∈Rt×d , each head\nh computes a weighted aggregation of V with respect to Q:\nh(Q,K,V) =softmax(Qh(KT\nh )\n√\nd\n)Vh ∈Rt×d′\n(2)\nwith:\nQh =QWQ\nh ∈Rt×d′\n(3)\nKh =KWK\nh ∈Rt×d′\n(4)\nVh =VWV\nh ∈Rt×d′\n(5)\nwhere d′= d\nnh and nh is the number of heads. The matrices\nWQ\nh ,WK\nh ,WV\nh ∈Rd×d′\nare linear projections from d to d′.\nThe outputs of (2) from all the heads are concatenated across\nthe channel dimension. The resulting updated representation\nis a weighted aggregation of the sequence in each posi-\ntion, based on the relative importance of all other positions.\nIn self MHA (sMHA), Q, K and V are taken to be the same\nsequence.\nEach layer l, l =1..L in the Transformer Encoder performs\nthe following computation by passing the input through a\nLayerNorm (LN) [39] before each module and adding it back\nwith residual connections:\nZ′\nl =sMHA(LN(Zl−1)) +Zl−1 ∈Rt×d (6)\nZl =MLP(LN(Z′\nl)) +Z′\nl ∈Rt×d (7)\nThe output of the Transformer Encoder at the position of\nthe class token represents a temporally aware aggregation of\ninput sequence:\nYC =ZL[0] ∈Rd (8)\nYC is provided as an input for a classiﬁer head, consisting\nof LN and FC layers with GELU non-linearity and Dropout,\nreducing the dimension to d\n4 . A second FC layer maps d\n4 to the\nnumber of classes. A Log SoftMax is applied on the output\nvector in conjunction with Negative Log Likelihood (NLL)\nloss to learn a multi-label classiﬁcation task.\nB. IMPLEMENTATION DETAILS\nThe proposed architecture is implemented in PyTorch [40].\nThe latent dimension d to 64 for the convolutional backbone\nand positional embedding. The Transformer Encoder consists\nof six layers with an eight-heads-sMHA block and an MLP.\nFinally, a Dropout p =0.1 is used for both the Transformer\nEncoder and classiﬁer head. The implementation of this\nframework (proposed architecture and its training and testing)\nis publicly available at: https://github.com/yolish/har-with-\nimu-transformer.\nIII. DATASETS\nThree datasets are employed in order to evaluate the proposed\napproach. The ﬁrst dataset represents the SLR problem (SLR\ndataset), containing ﬁve different common smartphone loca-\ntions. This dataset was created by combining six different\nSLR datasets. The second dataset considers the HAR problem\n(HAR dataset). It consists of six different human dynamics,\nincluding the division of the stairs class into two separate\nclasses: walking upstairs (Stairs up) and walking downstairs\n(Stairs down). The third dataset contains data with a com-\nbined SLR and HAR class labeling, which is referred to as\nSHAR (SHAR dataset). For example, the class ‘‘Walking\nPocket’’ refers to a scenario of a human walking (HAR) with\nthe smartphone placed in their pocket (SLR). This dataset\nincludes 21 classes.\nIn total, the SLR, HAR and SHAR datasets contain 27.76\nhours of recordings made by 91 people. Each dataset contains\nmany different ﬁles. Each ﬁle has the name of the user,\nwhich made the recording and a description of its type (e.g.\nuser1 walking texting), and can have a different time duration.\nWhen creating the uniﬁed dataset, all ﬁles from all users were\nmarge into a single ﬁle. The speciﬁc class labels and data\nproperties for each dataset are summarized in Table 1.\nA. SLR\nThe SLR dataset consists of six different datasets. In all six\ndatasets, the smartphone location was at least in one of ﬁve\nlocations: Texting, Pocket, Swing, Talking and, Body, while\nthe user was walking. In most of the datasets, there was no\nlimitation on how the smartphone was held or on the walk-\ning characteristics. From this dataset, only the normalized\naccelerometer readings are used.\nThe ﬁrst dataset [30], contains 164min of recorded data\nin four smartphone locations: Texting, Pocket, Swing and,\nTalking. It has three different sampling rates: 25, 50, and\n100Hz, all recorded by a single user with a single smartphone.\nThe second datatset [41], was created for PDR applications,\nnot related to SLR, using eight people. Since the recordings\nwere made while the users were walking with a smartphone,\nthis dataset can also be used for SLR. It contains three smart-\nphone locations: Pocket, Texting and, Body, with a total of\napproximately 70min of data, recorded at 200Hz. Similarly,\nthe third dataset [26] was recorded to examine deep-learning\nPDR, but can also be used for the SLR problem. There,\n53542 VOLUME 9, 2021\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nTABLE 1. Description of the three different datasets employed in this research.\nFIGURE 2. Samples class distribution in the SLR dataset.\neight people were recorded in a sampling rate of 100Hz\nabout 240min, while the smartphone was in Pocket or Texting\nclasses. The fourth dataset [30], has recordings of four loca-\ntions: Texting, Pocket, Swing and, Talking, made by six peo-\nple, with three different sampling rates (25, 50, and 100Hz),\nand six different smartphones, yielding a total of 15min\nof recorded data. The ﬁfth dataset [42], was recorded for\nHAR applications and is included also in the HAR dataset\n(Section III-B). For the SLR dataset, only the walking part\nis used. The dataset was recorded by 24 people using a\nsmartphone in their pocket with a sampling rate of 50Hz.\nThe last (sixth) dataset [43], was also created for an HAR\nresearch. There, the goal was to evaluate HAR performance\nwith smartphones’ and smartwatches’ recordings. For the\nSLR dataset, walking from the smartwatches were employed\nsince they share the same dynamics as smartphones in a swing\nmotion. The Body class was also used for this purpose. The\nrecordings where made using ten people, with a sampling\nrate of 50Hz, for a total of 48min.\nTo summarize, the combined SLR dataset has 3, 383,950\nsamples (in each accelerometer axis) recorded by 57 people\nusing different sampling rates of 25 −200Hz. The distribution\nof the samples in each class is shown in Figure 2. Note that\nno pre-processing was performed on the raw data. All of the\ndatasets were stacked together to a single one regardless of\nthe sampling rate they were recorded by. The motivation for\nFIGURE 3. Samples class distribution in the HAR dataset.\ndoing so was to make the network robust to the sampling rate\nsince in real time the user is free to choose the sampling rate\nbetween a predeﬁned range.\nB. HAR\nThe HAR dataset was collected with an iPhone 6s kept\nin the participant’s front pocket [42], [44]. 24 participants\n(10 women and 14 men) with varying age, weight, and height\nperformed six activities in 15 trials in the same environment\nand conditions: Walking, Jogging, Sitting, Standing, Stairs\ndown and, Stairs up. This dataset has 1, 304,950 samples in\neach accelerometer and gyroscope axis. The sampling rate\nwas 50Hz leading to a total of 435min. The distribution of\nthe samples in each class is shown in Figure 3.\nC. SHAR\nThe SHAR dataset is derived from a dataset created by [43]\nin order to evaluate how and when various motion sensors,\nwhich are available on a smartphone, can best be exploited\n(individually or combined) for improving activity recogni-\ntion. To that end, [43] have collected recordings with a\nsampling rate of 50Hz of ten participates during seven phys-\nical activities: Walking, Running, Sitting, Standing, Jogging,\nBiking, Upstairs (walking upstairs) and Downstairs (walk-\ning downstairs). Each of those participants was equipped\nwith ﬁve smartphones in ﬁve different locations: right jeans\npocket, left jeans pocket, on the belt position towards the right\nleg using a belt clip, on the right upper arm and, on the right\nVOLUME 9, 2021 53543\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nwrist. The SHAR dataset is constructed with combined SLR\nand HAR labels (classes) capturing both the physical activity\nand smartphone location. For this purpose, the right and\nleft pocket recordings from [43] were ﬁrst united under the\nPocket label. Recordings from [43] were then labelled based\non three smartphone locations and seven human activities:\n[Pocket, Belt, Uparm (upper arm) ] x [Walking, Running,\nSitting, Standing, Jogging, Biking, Upstairs, Downstairs],\nyielding a total of 21 classes. Each class has 87, 00 sam-\nples except the three upstairs classes (Upstairs Belt, Upstairs\nPocket, Upstairs Uparm) with have 60, 900 samples. Thus,\nthis dataset has 1, 748,700 samples in each accelerometer\nand gyroscope axis resulting in 583min of recorded data.\nIV. RESULTS\nA. EXPERIMENTAL SETUP\nFor evaluation purposes, the method proposed in this paper is\nfurther compared to a CNN model shown to achieve the best\nperformance on different SLR tasks [30]. This model consists\nof a convolutional encoder and a classiﬁer head. The encoder\nincludes two 1-dimensional convolutional layers with RELU\nnon-linearity followed by Dropout and max pooling. The\nclassiﬁer head consists of two FC layers with RELU non-\nlinearity. Log SoftMax is applied on the output of the ﬁnal\nFC layer.\nEach dataset is arbitrarily split into a train set and test set\n(where 85% of the samples, on average, are selected for the\ntrain set), while ensuring all classes are represented in both\nsets.\nBoth models (CNN model and the proposed approach) are\noptimized using Adam, with β1 = 0.9, β2 = 0.999 and\nϵ =10−10. A batch size of 128 and a weight decay of 10 −4\nare employed. An initial learning rate of λ =10−4 is used\nand further reduced by half every m epochs depending on\nthe experiment (m is set to the same value for both models).\nNote that in order to support a fair comparison, all hyper-\nparameters, except for the number of epochs, are not ﬁne-\ntuned and are kept ﬁxed for both models. Each model is\ntrained for up to 30 epochs for small datasets and for up to\n80 epochs for larger datasets. The full conﬁguration used for\ntraining is available in the shared codebase.\nFor ease of description, from here on the CNN model\nand the proposed approach are refereed to as IMU-CNN\nand IMU-Transformer, respectively. The following sections\ndescribe different experiments aimed at evaluating perfor-\nmance, robustness and generalization, across different activ-\nity recognition scenarios.\nB. THE EFFECT OF WINDOW SIZE\nAn IMU sample contains measurements aggregated over a\nwindow of time. The size of the window (k ), determines\nthe length of the sequence passed to the model. In general,\nin HAR, SLR or SHAR tasks, it is desirable to work with the\nsmallest window size which still achieves a target accuracy.\nIn the datasets employed, the sampling rate varies between\nFIGURE 4. Classification accuracy of the SHAR dataset with different\nwindows sizes.\n25-200Hz. Considering the slowest sampling rate, a win-\ndow size of 50 corresponds to a time duration of two sec-\nonds. Larger window sizes increases the probability of a\nmode change during a single window, which is an undesired\nbehavior. With this motivation in mind, the IMU-CNN and\nIMU-Transfomer models were trained using decreasing win-\ndows sizes starting from 50: 50, 38, 26 and 14. Since the\nSHAR dataset represents both HAR and SLR tasks, it was\nchosen for this analysis (training and testing).\nFigure 4 shows the results obtained with the two mod-\nels. The best accuracy is achieved when using a win-\ndow of 50 samples, with a gap of 1.8% in favor of the\nIMU-Transformer model. For smaller windows the improve-\nment gap grows to 4.2% (k =26,38) and 4.9% (k =14). For\nboth models, a similar trend emerges where a more notable\ndegradation occurs when using k = 26,38. This can be\nexplained by events of class mixture per sequence, which\nare less signiﬁcant and frequent when using the smallest\nwindow size (14). When considering the variance in accu-\nracy, the intra-difference (within smaller windows) is smaller\ncompared to the inter-distance, with respect to the original\nwindow size. In addition, a more signiﬁcant degradation in\nperformance is observed with the IMU-CNN model. Specif-\nically, when comparing the performance between k = 50\nand k = 14,26,38, the accuracy decreases in 4.5% on\naverage with IMU-CNN, compared to only 1.9% with IMU-\nTransformer. Hence, the IMU-Transformer model not only\nconsistently improves the performance, regardless of the win-\ndow size, but is also more robust to smaller window sizes,\nmaking it a favorable option for real-time applications. Based\non the results above, a window size of 50 was selected for\nfurther experiments and analysis.\nC. ACTIVITY RECOGNITION PERFORMANCE ACROSS\nDIFFERENT DATASETS\nIn order to evaluate the performance per task, the IMU-CNN\nand IMU-Transformer models were trained and tested on\nthe SLR, HAR and SHAR datasets. Table 2 gives the accu-\nracy for each dataset and the mean accuracy across datasets.\n53544 VOLUME 9, 2021\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nTABLE 2. Results obtained for the SLR, HAR and SHAR datasets. The\naccuracy (%) for a CNN model (IMU-CNN) and the proposed architecture\n(IMU-Transformer) is reported per dataset and overall (mean\nperformance).\nFIGURE 5. Confusion matrix for the SHAR stairs dataset using a CNN\nmodel (IMU-CNN).\nIMU-Transformer consistently achieves better accuracy, with\na 2% improvement on average. The performance of both\nmodels depends on the dataset, with a decreasing accuracy for\nthe SLR, HAR and SHAR datasets, respectively. In addition\nboth models achieve a signiﬁcantly higher accuracy (> 8%)\non the SLR dataset, compared to the two other tasks. These\nresults are consistent with previous observations on the SLR\ndataset (distinct patterns between classes that are relatively\neasy to learn [30]) and suggest that the IMU-Transformer\nmodel learns IMU data better than the IMU-CNN model,\nregardless of how challenging the speciﬁc dataset is.\nD. CHALLENGING MODES AND GENERALIZATION\nDue to the underlying dynamics some modes are more chal-\nlenging to learn than others. For example, the stairs related\ndynamics expressed in stairs-up and stairs-down scenarios.\nIn order to evaluate the proposed approach in this scenario,\nthe SHAR dataset is subset by taking samples only from the\nfollowing six classes: Downstairs Belt, Downstairs Pocket,\nDownstairs Uparm, Upstairs Belt, Upstairs Pocket, Upstairs\nUparm. Figure 5 presents the confusion matrix for the\nIMU-CNN model, showing a total accuracy of 86.6%. Four\nout of six modes received more than 91% yet, both Down-\nstairs Pocket and Upstairs Uparm achieved only 73% and\n72%, respectively. In particular, about 28% of the Upstairs\nUparm samples were misclassiﬁed as Downstairs Uparm.\nIn a similar manner, Figure 6 shows the confusion matrix for\nthe IMU-Transformer model, achieving a total accuracy of\n92.3% corresponding to a 5.7% improvement. Focusing on\nthe Upstairs Uparm mode, the 28% misclassiﬁcation of the\nIMU-CNN model is reduced to 7.5%. Another important\naspect of model performance is its ability to generalize across\ndatasets. For this purpose, the stairs experiment is further\nextended by training on the SHAR dataset but evaluating on\nFIGURE 6. Confusion matrix for the SHAR stairs dataset using the\nproposed approach (IMU-Transformer).\nFIGURE 7. Confusion matrix for the SHAR dataset with HAR as the test\ndataset using a CNN model (IMU-CNN).\nthe HAR test set. In order to obtain class compatibility, the six\nstairs-related SHAR classes are collapsed into two classes:\nStairs up (for Upstairs Belt, Upstairs Pocket and, Upstairs\nUparm) and, Stairs down (for Downstairs Belt, Downstairs\nPocket and, Downstairs Uparm). The HAR dataset is then\nsubset with these two classes. The results of this experiment\nare depicted in Figures 7-8 for the IMU-CNN and IMU-\nTransformer models, respectively. The IMU-CNN reached a\ntotal accuracy of 33.8%, where most of the error (72% of\nthe samples) is attributed to the misclassiﬁcation of Stairs up\nas Stair down. IMU-Transformer signiﬁcantly improved the\ntotal accuracy (80.2%) and obtained a symmetrical behaviour\nbetween the two classes.\nE. SUMMARY OF RESULTS\nThe results of the experiments described in this paper are\nsummarized in Table 3. The proposed approach and a CNN\nmodel were extensively evaluated on three datasets with more\nthan 27 hours of recordings, collected by 91 users, consider-\ning (1) the target tasks, namely SLR, HAR and SHAR, (2) the\neffect of the window size (SHAR-50/38/26/14 in Table 3),\n(3) challenging dynamics (SHAR-Stairs in Table 3) and (4)\ngeneralization (SHAR/HAR-Stairs in Table 3).\nFor each experiment conducted, the difference between\nthe accuracy of the proposed approach (IMU-Transformer)\nVOLUME 9, 2021 53545\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\nFIGURE 8. Confusion matrix for the SHAR dataset with HAR as the test\ndataset using the proposed approach (IMU-Transformer).\nTABLE 3. A summary of the experiments described in this paper. The\nproposed Transformer-based architecture achieves a consistent\nimprovement compared to the CNN-based model.\nand the accuracy of the CNN architecture (IMU-CNN) is\nreported, where positive values indicate improvement. The\nIMU-Transformer model consistently improves the classiﬁ-\ncation accuracy compared to the IMU-CNN model, with a\nnotable improvement in challenging scenarios.\nIn addition to accuracy, the mean runtime was also evalu-\nated (using the SHAR dataset). When run on a GPU (Tesla\nV100 16Gb), the IMU-CNN model classiﬁes a sample with\na window size of 50 in 0.48ms on average versus 4.76ms\nwith the IMU-Transformer model. When tested on a CPU,\nthe average inference time increases, with 3.93ms and 14.3ms\nfor the IMU-CNN and IMU-Transformer, respectively. In\nboth cases the classiﬁcation runtime is negligible compared\nto the runtime in which a classiﬁcation is expected, even when\nconsidering the highest sampling rate (200Hz with a window\nsize of 50 corresponding to 250ms). Integrating the proposed\nframework in a real-time application involves deploying the\ntrained model (weights) and executing its forward pass with\nan inertial signal aggregated over a predeﬁned time window.\nV. CONCLUSION\nThis paper presents a deep learning framework for activity\nrecognition. The proposed approach employs Transformers\nfor performing sequence aggregation using attention, which\nhave been successfully used for sequence analysis tasks in\nother domains. To-date, this is the ﬁrst time a Transformer\narchitecture is employed for inertial-based activity recogni-\ntion. Three types of datasets, with more than 27 hours of\nrecordings, collected by 91 users, were used for an extensive\nevaluation: 1) smartphone mode location recognition, created\nfrom six different datasets, 2) human activity recognition\nand, 3) combined smartphone location and human activity\nrecognition. In addition more challenging scenarios were\naddressed: 1) classiﬁcation of stairs up/down motion in three\ndifferent smartphone locations and, 2) testing the second\ndataset with a network trained on the third dataset for stairs\nonly data.\nThroughout multiple experiments, representing different\nactivity recognition scenarios and settings, the proposed\napproach demonstrates an improved prediction accuracy that\ncan be transferred better between datasets compared to a\nCNN-based solution. While this approach is an order of mag-\nnitude slower (4.76ms vs 0.48ms), its runtime is negligible\ncompared to the runtime in which a classiﬁcation is expected,\neven when considering the highest sampling rate.\nAn immediate extension of the proposed framework is to\nevaluate it with more user and smartphone modes. In addition,\nfurther enhancements can be made to the proposed frame-\nwork, leveraging on Transformers acceleration techniques.\nFinally, transfer learning can be investigated to evaluate\nwhether a model trained on one dataset can serve as a better\nstarting point for new models trained on incoming datasets.\nSince the proposed approach performs classiﬁcation of\ninertial data it can be directly applied for other inertial-\nbased classiﬁcation tasks. In addition, it can be further\nadapted to handle other sensory data collected in a sequen-\ntial manner for activity recognition, by simple modiﬁca-\ntions to the CNN backbone. In order to support results\nreproduction and an easy transfer to other domains, the\nimplementation of the proposed framework is available at:\nhttps://github.com/yolish/har-with-imu-transformer.\nREFERENCES\n[1] B. Ni, G. Wang, and P. Moulin, ‘‘RGBD-HuDaAct: A color-depth video\ndatabase for human daily activity recognition,’’ in Proc. IEEE Int. Conf.\nComput. Vis. Workshops (ICCV Workshops), Nov. 2011, pp. 1147–1153.\n[2] W. Wang, A. X. Liu, M. Shahzad, K. Ling, and S. Lu, ‘‘Device-free human\nactivity recognition using commercial WiFi devices,’’ IEEE J. Sel. Areas\nCommun., vol. 35, no. 5, pp. 1118–1131, May 2017.\n[3] H. Li, X. He, X. Chen, Y. Fang, and Q. Fang, ‘‘Wi-motion: A robust\nhuman activity recognition using WiFi signals,’’ IEEE Access, vol. 7,\npp. 153287–153299, 2019.\n[4] X. Li, Y. He, and X. Jing, ‘‘A survey of deep learning-based human activity\nrecognition in radar,’’ Remote Sens., vol. 11, no. 9, p. 1068, May 2019.\n[5] W. Lin, M.-T. Sun, R. Poovandran, and Z. Zhang, ‘‘Human activity recog-\nnition for video surveillance,’’ in Proc. IEEE Int. Symp. Circuits Syst.,\nMay 2008, pp. 2737–2740.\n[6] X. Zhang, X. Chen, Y. Li, V. Lantz, K. Wang, and J. Yang, ‘‘A frame-\nwork for hand gesture recognition based on accelerometer and EMG\nsensors,’’IEEE Trans. Syst., Man, Cybern. A, Syst. Humans, vol. 41, no. 6,\npp. 1064–1076, Nov. 2011.\n[7] R. Xu, S. Zhou, and W. J. Li, ‘‘MEMS accelerometer based nonspeciﬁc-\nuser hand gesture recognition,’’ IEEE Sensors J., vol. 12, no. 5,\npp. 1166–1173, May 2012.\n[8] N. Y. Hammerla, S. Halloran, and T. Ploetz, ‘‘Deep, convolutional, and\nrecurrent models for human activity recognition using wearables,’’ 2016,\narXiv:1604.08880. [Online]. Available: http://arxiv.org/abs/1604.08880\n53546 VOLUME 9, 2021\nY. Shavit, I. Klein: Boosting Inertial-Based Human Activity Recognition With Transformers\n[9] P. Vepakomma, D. De, S. K. Das, and S. Bhansali, ‘‘A-Wristocracy: Deep\nlearning on wrist-worn sensing for recognition of user complex activities,’’\nin Proc. IEEE 12th Int. Conf. Wearable Implant. Body Sensor Netw. (BSN),\nJun. 2015, pp. 1–6.\n[10] W. Taylor, S. A. Shah, K. Dashtipour, A. Zahid, Q. H. Abbasi, and\nM. A. Imran, ‘‘An intelligent non-invasive real-time human activity recog-\nnition system for next-generation healthcare,’’ Sensors, vol. 20, no. 9,\np. 2653, May 2020.\n[11] Z. Sun, X. Mao, W. Tian, and X. Zhang, ‘‘Activity classiﬁcation and dead\nreckoning for pedestrian navigation with wearable sensors,’’ Meas. Sci.\nTechnol., vol. 20, no. 1, Jan. 2009, Art. no. 015203.\n[12] S. Guo, H. Xiong, X. Zheng, and Y. Zhou, ‘‘Indoor pedestrian trajectory\ntracking based on activity recognition,’’ in Proc. IEEE Int. Geosci. Remote\nSens. Symp. (IGARSS), Jul. 2017, pp. 6079–6082.\n[13] J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, ‘‘Deep learning for sensor-\nbased activity recognition: A survey,’’ Pattern Recognit. Lett., vol. 119,\npp. 3–11, Mar. 2019.\n[14] A. Avci, S. Bosch, M. Marin-Perianu, R. Marin-Perianu, and P. Havinga,\n‘‘Activity recognition using inertial sensing for healthcare, wellbeing and\nsports applications: A survey,’’ in Proc. 23th Int. Conf. Archit. Comput.\nSyst., Feb. 2010, pp. 1–10.\n[15] L. Chen, J. Hoey, C. D. Nugent, D. J. Cook, and Z. Yu, ‘‘Sensor-based\nactivity recognition,’’ IEEE Trans. Syst., Man, Cybern. C, Appl. Rev.,\nvol. 42, no. 6, pp. 790–808, Nov. 2012.\n[16] L. Minh Dang, K. Min, H. Wang, M. Jalil Piran, C. Hee Lee, and H. Moon,\n‘‘Sensor-based and vision-based human activity recognition: A compre-\nhensive survey,’’Pattern Recognit., vol. 108, Dec. 2020, Art. no. 107561.\n[17] M. Shoaib, S. Bosch, O. Incel, H. Scholten, and P. Havinga, ‘‘A survey of\nonline activity recognition using mobile phones,’’ Sensors, vol. 15, no. 1,\npp. 2059–2085, Jan. 2015.\n[18] S. Ramasamy Ramamurthy and N. Roy, ‘‘Recent trends in machine\nlearning for human activity recognition—A survey,’’ Wiley Interdiscipl.\nReviews: Data Mining Knowl. Discovery, vol. 8, no. 4, Jul. 2018,\nArt. no. e1254.\n[19] A. Sargano, P. Angelov, and Z. Habib, ‘‘A comprehensive review on hand-\ncrafted and learning-based action representation approaches for human\nactivity recognition,’’ Appl. Sci., vol. 7, no. 1, p. 110, Jan. 2017.\n[20] F. Demrozi, G. Pravadelli, A. Bihorac, and P. Rashidi, ‘‘Human activity\nrecognition using inertial, physiological and environmental sensors: A\ncomprehensive survey,’’IEEE Access, vol. 8, pp. 210816–210836, 2020.\n[21] R. Yang and B. Wang, ‘‘PACP: A position-independent activity recognition\nmethod using smartphone sensors,’’ Information, vol. 7, no. 4, p. 72,\nDec. 2016.\n[22] M. Elhoushi, J. Georgy, A. Noureldin, and M. Korenberg, ‘‘Online motion\nmode recognition for portable navigation using low-cost sensors,’’ Navi-\ngation, vol. 62, no. 4, pp. 273–290, Dec. 2015.\n[23] I. Klein, Y. Solaz, and G. Ohayon, ‘‘Pedestrian dead reckoning with smart-\nphone mode recognition,’’IEEE Sensors J., vol. 18, no. 18, pp. 7577–7584,\nSep. 2018.\n[24] S.-H. Fang, Y.-X. Fei, Z. Xu, and Y. Tsao, ‘‘Learning transportation modes\nfrom smartphone sensors based on deep neural network,’’ IEEE Sensors J.,\nvol. 17, no. 18, pp. 6111–6118, Sep. 2017.\n[25] X. Zhou, W. Liang, K. I.-K. Wang, H. Wang, L. T. Yang, and Q. Jin, ‘‘Deep-\nlearning-enhanced human activity recognition for Internet of healthcare\nthings,’’IEEE Internet Things J., vol. 7, no. 7, pp. 6429–6438, Jul. 2020.\n[26] C. Chen, X. Lu, A. Markham, and N. Trigoni, ‘‘IONet: Learning to cure\nthe curse of drift in inertial odometry,’’ in Proc. AAAI Conf. Artif. Intell.,\nvol. 32, 2018, pp. 1–9.\n[27] Q. Wang, L. Ye, H. Luo, A. Men, F. Zhao, and Y. Huang, ‘‘Pedestrian stride-\nlength estimation based on LSTM and denoising autoencoders,’’ Sensors,\nvol. 19, no. 4, p. 840, Feb. 2019.\n[28] I. Klein and O. Asraf, ‘‘Stepnet—-Deep learning approaches for step length\nestimation,’’IEEE Access, vol. 8, pp. 85706–85713, 2020.\n[29] E. Vertzberger and I. Klein, ‘‘Attitude adaptive estimation with smartphone\nclassiﬁcation for pedestrian navigation,’’ IEEE Sensors J., vol. 21, no. 7,\npp. 9341–9348, Apr. 2021, doi: 10.1109/JSEN.2021.3053843.\n[30] I. Klein, ‘‘Smartphone location recognition: A deep learning-based\napproach,’’Sensors, vol. 20, no. 1, p. 214, Dec. 2019.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 5998–6008.\n[32] D. Bahdanau, K. Cho, and Y. Bengio, ‘‘Neural machine translation by\njointly learning to align and translate,’’ in Proc. 3rd Int. Conf. Learn.\nRepresent., (ICLR), Y. Bengio and Y. LeCun, Eds., San Diego, CA, USA,\n2015, pp. 2–16.\n[33] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog ,\nvol. 1, no. 8, p. 9, 2019.\n[35] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Com-\nputer Vision—ECCV, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm,\nEds. Cham, Switzerland: Springer, 2020, pp. 213–229.\n[36] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16x16 words:\nTransformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[Online]. Available: http://arxiv.org/abs/2010.11929\n[37] W.-C. Chang, H.-F. Yu, K. Zhong, Y. Yang, and I. Dhillon, ‘‘X-BERT:\nExtreme multi-label text classiﬁcation with using bidirectional encoder\nrepresentations from transformers,’’ 2019, arXiv:1905.02331. [Online].\nAvailable: http://arxiv.org/abs/1905.02331\n[38] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efﬁcient image transformers & distillation through atten-\ntion,’’ 2020, arXiv:2012.12877. [Online]. Available: http://arxiv.org/\nabs/2012.12877\n[39] J. Lei Ba, J. Ryan Kiros, and G. E. Hinton, ‘‘Layer normalization,’’ 2016,\narXiv:1607.06450. [Online]. Available: http://arxiv.org/abs/1607.06450\n[40] A. Paszke et al. , ‘‘Pytorch: An imperative style, high-performance deep\nlearning library,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 32,\nH. Wallach, H. Larochelle, A. Beygelzimer, F. Alche-Buc, E. Fox, and\nR. Garnett, Eds. Vancouver, BS, Canada: Curran Associates, 2019,\npp. 8026–8037.\n[41] H. Yan, Q. Shan, and Y. Furukawa, ‘‘RIDI: Robust IMU double integra-\ntion,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 621–636.\n[42] M. Malekzadeh, R. G. Clegg, A. Cavallaro, and H. Haddadi, ‘‘Mobile\nsensor data anonymization,’’ in Proc. Int. Conf. Internet Things Design\nImplement., Apr. 2019, pp. 49–58.\n[43] M. Shoaib, S. Bosch, O. Incel, H. Scholten, and P. Havinga, ‘‘Fusion of\nsmartphone motion sensors for physical activity recognition,’’ Sensors,\nvol. 14, no. 6, pp. 10146–10176, Jun. 2014.\n[44] M. Malekzadeh, R. G. Clegg, A. Cavallaro, and H. Haddadi, ‘‘Protecting\nsensory data against sensitive inferences,’’ in Proc. 1st Workshop Privacy\nDesign Distrib. Syst., Apr. 2018, pp. 1–6.\nYOLI SHAVITreceived the B.Sc. degree in com-\nputer science and life science from Tel Aviv Uni-\nversity, the M.Sc. degree in bioinformatics from\nImperial College London, and the Ph.D. degree\nin computer science from the University of Cam-\nbridge. She is currently a Postdoctoral Researcher\nwith Bar-Ilan University and a Principal Research\nScientist with the Huawei Tel Aviv Research Cen-\nter, Toga Networks, a Huawei Company. Her\nresearch interests include algorithms in deep learn-\ning and their applications to real-life domains and visual and multi-sensor\nlocalization problems.\nITZIK KLEIN (Senior Member, IEEE) received\nthe B.Sc. and M.Sc. degrees in aerospace engi-\nneering and the Ph.D. degree in geo-information\nengineering from the Technion–Israel Institute of\nTechnology, in 2004, 2007, and 2011, respec-\ntively. He is currently an Assistant Professor with\nthe University of Haifa, heading the Department\nof Marine Technologies, Autonomous Navigation\nand Sensor Fusion Laboratory. His research inter-\nests include navigation, novel inertial navigation\narchitectures, autonomous underwater vehicles, sensor fusion, smartphone\nbased navigation, and estimation theory.\nVOLUME 9, 2021 53547",
  "topic": "Activity recognition",
  "concepts": [
    {
      "name": "Activity recognition",
      "score": 0.8911983966827393
    },
    {
      "name": "Computer science",
      "score": 0.7792199850082397
    },
    {
      "name": "Transformer",
      "score": 0.6925481557846069
    },
    {
      "name": "Inertial measurement unit",
      "score": 0.6382464170455933
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.5740247368812561
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5582110285758972
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5369176268577576
    },
    {
      "name": "Machine learning",
      "score": 0.5071370601654053
    },
    {
      "name": "Codebase",
      "score": 0.41341841220855713
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3231791853904724
    },
    {
      "name": "Source code",
      "score": 0.15845027565956116
    },
    {
      "name": "Engineering",
      "score": 0.09733319282531738
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}