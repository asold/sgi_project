{
  "title": "A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model",
  "url": "https://openalex.org/W4285240123",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320548623",
      "name": "Imad Lakim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226134900",
      "name": "Ebtesam Almazrouei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2046639894",
      "name": "Ibrahim Abualhaol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1224291158",
      "name": "Merouane Debbah",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2403313545",
      "name": "Julien Launay",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2065088268",
    "https://openalex.org/W3208860256",
    "https://openalex.org/W3082020764",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2955300494",
    "https://openalex.org/W4226153346",
    "https://openalex.org/W4288796004",
    "https://openalex.org/W4206336135",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W3176062169",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4229506649",
    "https://openalex.org/W3153642904",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3156160505",
    "https://openalex.org/W2740572477",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3204998121",
    "https://openalex.org/W2991588540",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W3025490408",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226318502",
    "https://openalex.org/W4312968147",
    "https://openalex.org/W3096543861",
    "https://openalex.org/W4200517036",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3213101878"
  ],
  "abstract": "Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, Julien Launay. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.",
  "full_text": "Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 84 - 94\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nA Holistic Assessment of the Carbon Footprint of Noor,\na Very Large Arabic Language Model\nImad Lakim\nTII, Abu Dhabi\nimad.lakim@tii.ae\nEbtesam Almazrouei\nTII, Abu Dhabi\nebtesam.almazrouei@tii.ae\nIbrahim Abu Alhaol\nTII, Abu Dhabi\nibrahim.abualhaol@tii.ae\nMerouane Debbah\nTII, Abu Dhabi\nmerouane.debbah@tii.ae\nJulien Launay\nLightOn, Paris\njulien@lighton.io\nAbstract\nAs ever larger language models grow more\nubiquitous, it is crucial to consider their envi-\nronmental impact. Characterised by extreme\nsize and resource use, recent generations of\nmodels have been criticised for their voracious\nappetite for compute, and thus signiﬁcant car-\nbon footprint. Although reporting of carbon\nimpact has grown more common in machine\nlearning papers, this reporting is usually lim-\nited to compute resources used strictly for\ntraining. In this work, we propose a holis-\ntic assessment of the footprint of an extreme-\nscale language model, Noor. Noor is an on-\ngoing project aiming to develop the largest\nmulti-task Arabic language models–with up to\n13B parameters–leveraging zero-shot general-\nisation to enable a wide range of downstream\ntasks via natural language instructions. We as-\nsess the total carbon bill of the entire project:\nstarting with data collection and storage costs,\nincluding research and development budgets,\npretraining costs, future serving estimates, and\nother exogenous costs necessary for this inter-\nnational cooperation. Notably, we ﬁnd that in-\nference costs and exogenous factors can have\na signiﬁcant impact on total budget. Finally,\nwe discuss pathways to reduce the carbon foot-\nprint of extreme-scale models.\n1 Introduction\nRecent progress in natural language processing\n(NLP) has been driven by the emergence of so-\ncalled foundation models (Bommasani et al., 2021).\nThis paradigm shift is characterised by a homogeni-\nsation of modelling methods– crystallising around\nthe Transformer architecture (Vaswani et al., 2017)–\nand by emergent capabilities (e.g. zero-shot gener-\nalisation) predominantly arising from sheer scale\nalone (Brown et al., 2020). NLP models are now\nexperiencing a 3-4 months doubling time in size, as\noutlined by Figure 1. Most recent large language\nmodels such as MT-NLG 530B (Smith et al., 2022),\nGopher 280B (Rae et al., 2021), or Jurassic-1 178B\n(Lieber et al., 2021), all report training budgets\nin the thousands of PF-days 1 range. Because AI\naccelerators performance per watt has plateaued\ncompared to deep learning budgets (Reuther et al.,\n2021; Sevilla et al., 2022), practitioners have had to\nscale-out training over an increasingly large num-\nber of accelerators (Narayanan et al., 2021). Ac-\ncordingly, the energy cost of training state-of-the-\nart models has grown signiﬁcantly: increase in\ncompute is no longer fuelled by improvements in\nhardware efﬁciency, but in hardware scale.\nAlthough this increase in size and compute bud-\nget is backed by empirical scaling laws drawing a\nclear link between compute spent and model perfor-\nmance (Kaplan et al., 2020), the societal beneﬁts\nof larger models have been questioned (Tomaˇsev\net al., 2020; Bender et al., 2021). Speciﬁcally to\nenvironmental concerns, in a time of climate cri-\nsis when carbon emissions must be drastically cut\n(Masson-Delmotte et al., 2018), one may question\nwhether these large compute budgets are justiﬁed.\nA crucial step towards answering this question is an\nin-depth evaluation of the footprint of large models.\nExisting assessments of the environmental im-\npacts of large models are usually focused on hyper-\nparameter tuning and pretraining costs (Strubell\net al., 2019; Patterson et al., 2021). This trend is re-\nﬂected by the growing number of tools available to\nhelp practitioners quantify the impact of machine\nlearning computations (Bannour et al., 2021). If\nsome studies have also endeavoured to quantify\nselect aspects of the machine learning pipeline (e.g.\nconference attendance (Skiles et al., 2021), hard-\nware lifecycle (Gupta et al., 2021), etc.), end-to-end\nevaluations of machine learning projects life cycle\nemissions remain rare (Wu et al., 2022).\n1A PF-day is 1 PFLOPs (10 A100) sustained for a day.\n84\nFigure 1: Over the last four years, the size of state-\nof-the-art language models has doubled every 3-4\nmonths. Note that this trend has been slowing down,\ndue to scale-out limitations.\nTo ﬁll this gap, we produce an end-to-end as-\nsessment of the carbon footprint of Noor, a project\nseeking to train a very large Arabic language model.\nOur contributions are the following:\nHolistic assessment. We evaluate the total car-\nbon bill of the entire project: starting with data col-\nlection, curation, and storage, including research\nand development and hyper-parameters tuning bud-\ngets, pretraining costs, future serving estimates,\nand other exogenous impacts sparked by this inter-\nnational cooperation (e.g. ﬂights, personnel, etc.)\nBeyond pretraining. We identify pretraining\ncompute as driving more than half of the emissions\nof the project. However, all combined, other R&D,\nstorage, and personnel counts still amount for 35%\nof the carbon footprint. We also identify down-\nstream use in the wild as potentially signiﬁcant.\nThis leads us to recommend for the end-to-end foot-\nprint to be systematically assessed on a per-project\nbasis. Notably, in scenarios with a low-impact\ntraining electric mix, costs beyond pretraining may\nbecome the main sources of emissions.\nPathways to lower footprints. Finally, we dis-\ncuss ways to reduce the environmental footprints\nof projects involving large models, and put in per-\nspective the footprint of similar projects.\n2 Related work\nIn light of ever increasing computational budgets\n(Sevilla et al., 2022) and of the need to cut on emis-\nsions to abate global warming (Masson-Delmotte\net al., 2018), the environmental impact of deep\nlearning has drawn signiﬁcant interest.\nStrubell et al., 2019 notably highlighted the po-\ntential high environmental costs of deep learning.\nHowever, its headline ﬁgures were produced in\nthe speciﬁc context of neural architecture search,\na relatively rare practice for extreme-scale mod-\nels nowadays. Lacoste et al., 2019; Lottick et al.,\n2019; Schwartz et al., 2020 subsequently called for\nAI research to be more aware of its environmen-\ntal cost. An increasing number of tools, such as\ncodecarbon (Schmidt et al., 2021), have been\ndeveloped to help with tracking the impact of deep\nlearning experiments (Bannour et al., 2021). All\nof these lines of research share similar recommen-\ndations: the carbon footprint of deep learning is a\ndirect consequence of the electricity mix and efﬁ-\nciency of the data center, suggesting that picking\nan appropriate provider is the most straightforward\nway to reduce environmental impact.\nSpeciﬁcally to extreme-scale models, Patterson\net al., 2021 estimated the energy consumption of\nﬁve large NLP models, including GPT-3. They\nidentiﬁed that a judicious choice of neural architec-\nture, datacenter and accelerator can help reduce\nconsiderably carbon budgets. Thompson et al.,\n2020 identiﬁed a clear relationship between large\nmodels performance and their carbon impact, build-\ning upon work on neural scaling laws (Kaplan et al.,\n2020). Taddeo et al., 2021 estimated the cost of\ntraining GPT-3 in different data centers across the\nworldwide, highlighting again the high dependency\non the local energy mix and speciﬁc infrastructure.\nTwo recent studies have provided insights into\nthe end-to-end carbon footprint of deployed mod-\nels in the industry. Wu et al., 2022 studied the\nimpact of the increasingly large recommender sys-\ntems leveraged at Meta, while Patterson et al., 2022\nprovided an assessment of the costs (including in-\nference) of large models at Google. They expect\nthe carbon footprint of training to plateau in coming\nyears, and then to shrink–owing to more efﬁcient\nhigh performance computing platforms. They also\nassert that current studies are overestimating the\nreal environmental costs of large models, in light of\nthe wide availability of ”clean” compute platforms.\nIn the ﬁeld of astrophysics, Aujoux et al., 2021\ndid an extensive study to estimate the carbon foot-\nprint of the Giant Array for Neutrino Detection\n(GRAND) project, a multi-decade worldwide ex-\nperiment. Inspired by their holistic methodology,\nwe seek to establish the ﬁrst end-to-end assessment\nof an extreme-scale NLP project.\n85\n3 The Noor Project\nThe current state-of-the-art generative language\nmodel in Modern Standard Arabic is AraGPT (An-\ntoun et al., 2021), a 1.5B parameters model. The\nNoor project seeks to expand upon this model, in-\ntroducing a 1.5B, 2.7B, 6.7B, and 13B Arabic mod-\nels, trained a custom curated dataset of 150B to-\nkens, inspired by The Pile (Gao et al., 2020). These\nlarger scales are expected to make the model able\nto tackle novel tasks through zero-shot generaliza-\ntion, as exhibited by GPT-3 (Brown et al., 2020) or\nGPT-J (Wang and Komatsuzaki, 2021).\nNoor is an on-going international cooperation\nbetween the Technology Innovation Institute in the\nUnited Arab Emirates and LightOn in France. The\nNoor project can be split in four parts:\n• Data curation. A custom curated dataset of\n150B tokens has been assembled for Noor.\nThis dataset has been scrapped from diver-\nsiﬁed sources, and also includes data from\nCommon Crawl. We ﬁlter this data with an\nLM-based quality-scoring system inspired by\nCCNet (Wenzek et al., 2019).\n• R&D experiments. To validate tokenization,\ndataset, architecture, and establish scaling\nlaws, we trained a number of R&D models\n(100M-1.5B parameters on 10-30B tokens).\n• Main training. We train a suite of four mod-\nels of 1.5B, 2.7B, 6.7B, and 13B parameters.\n• Model use. Prospectively, we include some\nestimations of the future inference cost of\nthese models as they are put in use.\n4 Factors inﬂuencing the carbon\nfootprint of large models\nBefore beginning our assessment, we propose to\nidentify some of the key inﬂuencing factors on the\npotential carbon footprint of large models, focus-\ning ﬁrst on factors directly related to the models\nthemselves and not to the project producing them.\nModel size. The number of ﬂoating operations\nper forward pass is directly proportional to the\nsize of the network. A common approximation\nfor the total compute budget C required for train-\ning a Transformer model with N parameters on D\ntokens is C = 6ND (Kaplan et al., 2020). As the\noptimal dataset size only grows sublinearly with\nmodel size for autoregressive modelling (Henighan\net al., 2020), compute budget will scale more or\nless linearly with model size. The larger the num-\nber of operations, the more energy is needed to\ntrain the model. For inference, the cost for each to-\nken is reduced to a third compared to training, and\nenvironmental impact will be driven by the total\nnumber of words/tokens processed.\nHardware characteristics. The throughput (in\nFLOPs) that can be tackled by the hardware will\ndrive the total time required to perform the task.\nMore efﬁcient hardware will have more through-\nput per Watt. We note however that most avail-\nable chips suitable for large model training (e.g.,\nNVIDIA GPUs, Google TPUs, etc.) exhibit similar\nefﬁciency characteristics (Reuther et al., 2021).\nModelling decisions. We identiﬁed above two\nkey factors: number of tokens processed (for train-\ning or inference), and hardware throughput. We\nnote that both of these are also strongly impacted by\nmodelling decisions. A more fertile tokenizer will\nuse less tokens for the same text, leading to faster\nprocessing. Similarly, small changes in model ar-\nchitecture (e.g., choosing hidden sizes in accor-\ndance with wave/tile quantization) and in imple-\nmentation (e.g., 3D parallelism) can drastically in-\ncrease throughput, and reduce total training time.\nData center efﬁciency. The energy consumed\ndoes not serve only to power up the servers, but\nalso to cool down the data center itself and to re-\nspond to other electrical needs. The Power Usage\nEffectiveness (PUE) is used to assess the overall\nefﬁciency of a data center. It measures the quotient\nof the total energy requirement and the ﬁnal energy\nused by the servers. The PUE will be inﬂuenced by\nthe data center architecture. Worldwide average is\naround 1.8, but Google for instance reports an av-\nerage PUE of 1.11. Waste heat in data centers can\nalso be reused for collective water heating, driving\ndown the PUE, as in the Jean Zay HPC.\nElectricity mix. The breakdown of the energy\nsources powering a data center is a crucial factor,\nand depends primarily on the region. The elec-\ntricity mix determines the carbon emissions per\nkWh of electricity. Today, the world average of\ncarbon emission by kwh of electricity generated is\n475 gCO2e/kWh, and an increasing number of data\ncenters from cloud providers are using 100% re-\nnewable or nuclear energy to power their hardware.\nTaking Google Cloud as an example again, their\n86\nMontreal facility reports 27gCO2e/kWH, twenty\ntimes lower than the world average.\nBeyond factors related to the models themselves,\nwe seek in this study to take into account a number\nof other costs: storage, preprocessing, and trans-\nfer costs for the dataset, personnel costs such as\ntravel and individual laptops, etc. We note however\none limitation from our study: we do not take into\naccount the lifecycle of the hardware used. Unfor-\ntunately, numbers are scarcely available, and not\nmade public by the main manufacturers.\n5 Carbon footprint of the Noor project\n5.1 Electricity consumption\nWe begin by accounting for the electricity consump-\ntion of all aspects of the project. The impact of\nthis consumption will be highly dependent on the\ncarbon intensity of the electricity mix used. Non-\nelectric sources (e.g., international ﬂights) will be\nadded to the carbon budget in a second phase.\n5.1.1 Data storage and transfers\nThe energy consumption of data depends on both\nthe energy required for powering the disks to store\nthe data, and the energy consumed when moving\nthe data from one server to another. We average\nstorage costs over the 6 months of the project.\nStorage. Although disk wattage is generally re-\nported on per-disk level, Posani et al., 2019 esti-\nmates the power per TB of data using aggregated\ntechnical speciﬁcations. The paper reports that\nthe average peak consumption of cloud storage is\naround 11.3W/TB. It means an energy consump-\ntion of 99 kWh/TB a year. This estimation consid-\ners a PUE of 1.6 and a redundancy factor of 2 since\nmanaged services will also have a back-up.\nThe breakdown of our data storage is as follows:\n• Curated data. Including both raw and pro-\ncessed data, we have accumulated around 2TB\nof curated data. This is stored for the 6 months\nof the project, resulting in 99kWh used.\n• Bulk data. We use Common Crawl (CC) for\nacquiring large amounts of web data. Each\nCC dump is on average around 10TB, and we\ndiscard it immediately after processing it. On\naverage, it takes 24 hours to fully process a\ndump: we used 21 dumps from CC, meaning\nwe stored 210TB of data for 24hours, equiv-\nalent to 57 kWh of energy consumption. Af-\nter processing the dumps, we got on average\n1.2TB of data per dump, thus 25TB in total.\nConsidering that this data will be stored for 6\nmonths, we end up with 1.3 MWh of energy\nconsumption for the bulk data. Note that we\nkeep the processed data in all languages (not\njust Modern Standard Arabic).\n• Models. The weights of the Noor models\n(1.3B, 2.7B, 6.7B and 13B) are respectively\n2.6GB, 5.4G, 13.4GB, and 26GB in half-\nprecision. This corresponds to training check-\npoints (including the full-precision optimizer)\nof 20.8GB, 43.2GB, 107.2GB, and 208GB.\nWe save such checkpoints every 10B tokens.\nIn total, we end-up with 5.7TB of model\nweights and intermediary checkpoints for fu-\nture analysis and interpretability work, con-\nsuming 0.3MWh in total.\nTransfers. Posani et al., 2019 provided an esti-\nmate of 23.9 kJ per GB (6.38 kWh per TB) trans-\nferred, using the formula of Baliga et al., 2011 and\nthe same hypothesis as Aslan et al., 2017 (800km\naverage distance between core nodes). The 210TB\nof CC data are downloaded on the preprocessing\nservers once; the 25TB of processed data are moved\nonce to our archival machines, and another time\nto the HPC used for training; the curated data is\ndownloaded once, moved to the archival machines,\nand then moved to the HPC; the 5.7TB of models\nare moved once from our HPC, and then to our in-\nference servers for ﬁnal models or to workstations\nfor intermediary checkpoints. Consequently, we\nestimate the transfer energy bill at 1.8 MWh.\nTotal. Thus, the total energy consumption of data\nis estimated to be about 3.5 MWh, dominated by\nthe multilingual Common Crawl data. We note\nthat as ideal dataset size increases sublinearly with\nmodel size (Kaplan et al., 2020), we expect check-\npoints and model transfers to eventually dominate\nthe costs of storage and transfer for larger models.\nNote that we neglect costs linked to a poten-\ntial public release of the models, as it is difﬁcult to\npredict trafﬁc. As a rough estimation, 10,000 down-\nloads of the 13B model would represent 260TB of\ntrafﬁc, and 1.66MWh consumed.\n5.1.2 Data processing\nWe take all text data through a pipeline inspired\nby CCNet (Wenzek et al., 2019) for preprocessing.\nThis pipeline takes care of deduplication, language\nidentiﬁcation, and ﬁnally quality ﬁltering with a\n87\nTable 1: Training compute budget and energy used for training the Noor models. Assuming a pretraining\ndataset of 150B tokens and a throughput of 100 TFLOPs per A100.\nModel Budget [PF-days] Budget [A100-hours] HPC Consumption [MWh]\n1.3B 13.5 3300 MeluXina 2.1\n2.7B 28.1 6800 Noor-HPC 4.8\n6.7B 69.8 17000 Noor-HPC 11.8\n13B 135 33000 Noor-HPC 22.9\nreference language model trained on Wikipedia.\nProcessing with our pipeline occurs on a CPU clus-\nter with 768 cores, split over 16 nodes.\nUsing average high-performance CPUs TDP ﬁg-\nures, we estimate the average power consumption\nof each node at 350W; hence, the power of the\ncluster is 5.6kW. We processed 21 dumps of Com-\nmonCrawl, plus our curated data, for a total of 381\nwall-clock hours. Accordingly, assuming a PUE\nof 1.1 as reported by Google, the total energy con-\nsumed by data preprocessing is 2.35MWh.\nNote that for CommonCrawl data, this results\nin data processed for every language supported\n(176 for identiﬁcation, 48 for quality ﬁltering). Ac-\ncordingly, this cost could be amortised over future\nprojects. For high-resource languages, this also\nresults in very large amounts of data: processing\nmore dumps would not be necessary, even to train\na 1 trillion parameters model.\n5.1.3 Research and development\nWe carried experiments to validate tokenization\nmethods, dataset composition, tune hyperparame-\nters, and establish scaling laws. This early research\nand development work was performed on MeluX-\nina, a high-performance super-computer located\nin Luxembourg. We used a total of 16,800 A100-\nhours in this phase. Each node used in MeluXina\nhas 4 A100 SXM 40GB with a TDP of 400W, and\ntwo AMD EPYC 7763 CPUs with a TDP of 280W.\nThey report a PUE of 1.35. Thus, we estimate the\nconsumption of this R&D phase to be of10.7MWh.\nWe expect the budget of this phase to roughly\nscale with model size. Indeed, debugging poten-\ntial issues (e.g., numerical instabilities (Kim et al.,\n2021), etc.) for the ﬁnal larger model will cost\nsigniﬁcantly more.\n5.1.4 Main training\nUsing the C = 6ND approximation, it is pos-\nsible to calculate in advance the training budget\nrequired for a speciﬁc model. We observe an ef-\nfective throughput with our Megatron+DeepSpeed\ncodebase of around 100 TFLOPs2 across models,\nin line with the state-of-the-art. We train four main\nmodels (1.5B, 2.7B, 6.7B, 13B) on 150B tokens.\nWe train the smaller model on MeluXina, but\nthe other three on our own HPC cluster. Each node\ncontains 8 A100 80GB and 2 AMD EPYC 7763\nCPUs. The PUE of our data center is 1.5, 20%\nmore efﬁcient than the world average.\nTable 1 outlines the costs of the main training.\nThe total electric energy consumed to train the\nNoor suite of models is thus 41.6 MWh, 55% of it\nspent on the largest 13B model.\n5.1.5 Inference\nAs the models of Noor have yet to be deployed,\nthis is only a prospective estimate. Inference costs\nin general are difﬁcult to estimate in advance, even\nmore so for open source models which will be\ndeployed to platforms with varying characteristics.\nWe provide an estimate of the energy consumption\nduring inference per generated token.\nWe thereafter denote as processed tokensthe to-\nkens in the original prompt sent to the model, and\nas generated tokensthe tokens generated by the\nmodel using the prompt. To simplify calculations,\nwe make the following assumptions from our expe-\nrience with another large-scale API: (1) an A100 is\nused, which is sufﬁcient for Noor-13B, but could be\nreduced to a more efﬁcient T4 for Noor-1.5B/2.7B;\n(2) inference time per generated token is constant,\nwhichever the number of processed tokens (per our\nbenchmarks, thanks to caching, this is true up to\n512 processed tokens roughly); (3) batch size is\nassumed to be 1, as batching is more challenging\nand less consistent for inference workloads.\nUnder these hypothesises, an A100\ncan generate up to 72,000 tokens per\nhour. Accordingly, we estimate that\n26 Joules are required per token generated (400W\n2These are effective FLOPs for training the model, not\nhardware FLOPs. Hardware FLOPs are closer to 150 TFLOPs.\n88\nFigure 2: Breakdown of the electricity consumption\n(total 59.14 MWh) of the Noor project. Data prepro-\ncessing is included in R&D, amounting for 20% of it.\nWe also note that R&D and dataset costs could be amor-\ntised through other projects or larger models.\nfor the GPU, 70W for the CPU, and 1.1 PUE\non Google Cloud imply 517Wh of energy con-\nsumption for 72,000 tokens. Converted to Joule,\nit results in 26 Joules per token.) Accordingly,\n3 billion tokens would have to be generated for\ninference costs to catch up with training costs. At\nsome point during its beta, GPT-3 was reported\nto generate 4.5 billion words per day (Pilipiszyn,\n2021).\n5.1.6 Additional costs\nBeyond costs related to data, R&D, training, and\ninference, one may wonder if direct electricity use\nfrom scientists involved in the project is signiﬁ-\ncant. Assuming that the average laptop consumes\n70W, plus 30W for an external screen, six research\nscientists dedicating 100% of their time during 6\nmonths for this project, 8 hours per day, will use up\n0.604MWh. We could also include costs of e-mail\nexchanges and video-conferences speciﬁcally, but\nthese were found to be negligible in Aujoux et al.,\n2021. We round up the marginal costs to 1MWh,\nand note that this is but a rough estimate.\n5.1.7 Summary\nWe showed that the total electricity consumption of\nthe Noor project is not only about training the ﬁnal\nmodels, as outlined in Figure 2. Nearly a third of\nthe energy consumed (30%) went to tasks outside\nof main models pretraining.\nBecause of larger uncertainties, we keep the serv-\ning/inference assessment out of the previous bud-\nget. However, especially in the context of openly\navailable models, the inference budget can rapidly\ncatch up with the total budget outlined in 2.\n5.2 Carbon footprint\nNow, from the electricity consumption, and using\ninformation on the local carbon intensity, we will\nderive the full footprint of the Noor project. We\nwill also add energy use coming from non-electric\nsources (e.g., ﬂights). As the carbon intensity of the\nelectricity mix varies signiﬁcantly across regions,\nwe outlined below the locations of interest:\n• Storage. We used Amazon S3 in Bahrain;\n• R&D. We used a GCP CPU cluster located in\nNetherlands, and MeluXina in Luxembourg;\n• Main training. The smaller 1.3B model was\ntrained on MeluXina, and the remaining mod-\nels were trained on our dedicated HPC plat-\nform in the United Arab Emirates (UAE);\n• Other. Six full-time scientists were involved,\nhalf in France and half in the UAE.\nTable 2 shows the resulting carbon footprint for\neach of the development stages of Noor project.\nThis highlights the importance of location for car-\nbon footprint: notably, all calculations on per-\nformed on the relatively low-carbon MeluXina\nHPC end-up having very limited costs, even com-\npared to small items like storage in Bahrain.\nIn addition to these development costs, we con-\nsider the carbon footprint of three round-trip ﬂights\nof four scientists between Paris and Abu Dhabi.\nThese trips were taken to run training workshops,\nbrainstorming sessions, and discussions related to\nthe project. We use the carbon emissions simula-\ntor of the International Civil Aviation Organization.\nOne round-trip emits 527 kgCO2e per person, to-\ntalling 6.4 tons of emissions over all trips.\nFinally, Figure 3 displays the total distribution\nof the carbon footprint of the project. As shown\nin the ﬁgure, factors like ﬂights may be usually\nneglected, but have a signiﬁcant contribution in the\ntotal carbon footprint. Speciﬁcally, as conference\nreturns in-person, this is a systematic impact that\nexists on most papers. In the case of Noor, the few\nﬂights operated account for 18% of the total carbon\nemission of the whole project.\n89\nTable 2: Carbon footprint of each phase of the Noor project.\nPhase Provider Location Mix [gCO2e/kWh] Use MWh Footprint [tCO2e]\nStorage Amazon S3 Bahrain 1188 3.5 4.2\nR&D GCP Netherlands 410 2.35 0.96\nMeluXina Luxembourg 60 10.7 0.65\nTraining MeluXina Luxembourg 60 2.1 0.13\nNoor-HPC UAE 600 39.5 23.7\nOthers France 56 0.33 0.02\nUAE 600 0.66 0.4\nInterestingly, we note that with increasingly\nclean electricity and efﬁcient data centers, the ex-\nogenous costs linked to ﬂights and personnel are\nbound to increase in proportional impact.\nInference. Forecasting the carbon footprint of in-\nference is harder for open models: as they may be\ndownloaded and deployed by anyone, it is impossi-\nble to predict the carbon intensity of the electricity\nthey will use. We study two scenarios: an interme-\ndiate one, based on the world average emission per\nkWh (475 gCO2e/kWh) and a best-case one, based\non the low-impact French mix (56 gCO2e/kWh).\nThese two scenarios correspond to around 300,000\ntokens generated per kgCO2e, or to 2,500,000 to-\nkens generated per kgCO2e in the best-case. Going\nback to the 4.5 billion words per day of GPT-3, this\namounts to 30 tons of CO2e per day and 3.5 tons.\nFigure 3: Breakdown of the carbon footprint (total\n36.5t tC02e) of the Noor project. This breakdown is\nhighly dependent on the localisation of the workloads\nand the local carbon intensity of the electricity mix.\n6 Best practices and recommendations\nFrom our experience with Noor, we highlight some\nrecommendations for future projects to minimise\ntheir carbon footprint.\n6.1 Modelling & engineering\nA ﬁrst angle of attack is to make the machine learn-\ning techniques used more efﬁcient.\n• Efﬁcient architectures. Mixture-of-experts\n(MoE) models split the large fully-connected\nlayers of a Transformer into distinct experts\n(Fedus et al., 2021). Although larger, MoE\nTransformers can bring signiﬁcant energy sav-\nings during training and inference (Du et al.,\n2021), as the experts are only sparsely acti-\nvated. Recent work demonstrate that they may\neven scale favorably compared to dense mod-\nels (Clark et al., 2022). More broadly, even\nsmall changes (e.g. better embeddings, acti-\nvation functions) may have a non-negligible\nimpact on the overall carbon footprint.\n• Efﬁcient inference. As we have shown, infer-\nence costs can rapidly catch up with training\ncosts: it is also interesting to make the model\nleaner for inference. Quantization (Yang et al.,\n2019) reduces numerical precision at infer-\nence time and accelerates inference, but it has\nseen limited adoption with large models. Dis-\ntillation (i.e., training a smaller model from\nthe outputs of a larger one) is a promising di-\nrection, already demonstrated for Transform-\ners applied to vision (Touvron et al., 2021).\n• Efﬁcient implementations. Crucially, dis-\ntributed training implementations must be as\nefﬁcient as possible, to amortise the large idle\nconsumption of the hardware – MeluXina re-\nports for instance idle power of around 150W\n90\nper GPU when accounting for CPU cores, in-\nfrastructure, etc. This includes taking into\naccount ﬁne-grained effects depending on ar-\nchitectures, such as wave and tile quantization,\nto achieve the best throughput possible.\n6.2 Hardware\nA second angle of attack is to focus on the hardware\nused to train these models.\n• Data center choice. A data center with a\nPUE of 1.1 will decrease energy consump-\ntion by 39% compared to the world average of\n1.8. Low PUE platforms should be preferred.\n• Local carbon intensity. As highlighted by\nTable 2, the carbon intensity of the electricity\nmix signiﬁcantly impacts the ﬁnal footprint.\nLocating training in an area with a clean mix\nis an easy step to take that can drastically cut\nthe footprint of a project. This is especially\neasy to do on online cloud platforms, which\nhave many areas of availability.\n• Efﬁcient inference. Carefully selecting a\nproper AI accelerator for managed inference\nworkloads can limit the footprint of model\nuse. For instance, for smaller models (<3B),\nit may be possible to use T4s rather than\nA100s, which are 20% more energy efﬁcient\nper FLOP than A100s. Finally, specialised\naccelerators are also starting to become avail-\nable (Reuther et al., 2020). We note that this\nmay however require speciﬁc developments.\n6.3 Other practices\nFinally, it is important to not underestimate costs\nbeyond machine learning workloads.\n• Minimising exogenous impact. Although\nwe found the ﬁnal footprint to be dominated\nby the main training runs, we still note the\nsigniﬁcant impact of the international ﬂights\ntaken during this cooperation (20% of the ﬁ-\nnal footprint). Minimising such high-intensity\ncost center is important.\n• Costs reporting and offset. The full cost of\nmodel development is rarely, if ever, reported\nin the literature. We highly recommend the AI\ncommunity to start reporting the full energy\nconsumption and the CO2e of their projects.\nThis reporting can also be used as the basis\nfor offsetting carbon emissions.\n7 Discussion and conclusion\nWe undertook an end-to-end assessment of the car-\nbon footprint associated with the development of\nan extreme-scale language model. We took into\naccount data collection and storage, research and\ndevelopment, pretraining, and included estimates\nfor future serving and inference. We also added\npersonnel costs, such as international ﬂights to run\ntraining workshops and brainstorming sessions.\nIn total, we estimate the development of the\nsuite of the four Noor models to have emitted\n36.5 tons of CO2, 65% of which for training the\nmodels, 18% for the international ﬂights, 12% for\ndata storage, and 4% for small-scale research and\ndevelopment experiments. To put this in perspec-\ntive, the average carbon footprint per individual in\nthe US is around 20 tons, so our project generated\na little over two years of individual US emissions.\nWe ﬁnd that the main driver of this carbon foot-\nprint is the carbon intensity of the mix used for\nmodel training. Appropriately selecting the loca-\ntion of calculations can signiﬁcantly reduce the\nenvironmental impact of a project. For instance, in\nthis project, running all computations in France\nwould have reduced the total footprint to 14.9\ntCO2e, 42% of which from the international ﬂights.\nAs the impact of the computations themselves be-\ncome smaller, it is important for practitionners to\nmore carefully weigh in exogenous contributions.\nAll-in-all, with careful considerations around\ndata center choice, it is possible to run extreme-\nscale NLP projects with a low carbon impact.\nFinally, we also identiﬁed that large-scale infer-\nence could also rapidly outtake pretraining costs in\nterms of carbon impact. Inference, if not centrally\nmanaged, is harder to control: with a publicly avail-\nable model, it will happen on hardware decided by\nthe end user. We thus think its equally important for\npractitioners to alert users regarding best efﬁcient\ninference practices, and regarding best practices to\nlimit the environmental cost of computations (e.g.\nchoosing an efﬁcient data center, running inference\nin a country with a low-impact mix, etc.)\n91\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021.\nAragpt2: Pre-trained transformer for arabic lan-\nguage generation.\nJoshua Aslan, Kieren Mayers, Jonathan Koomey, and\nChris France. 2017. Electricity intensity of internet\ndata transmission: Untangling the estimates: Elec-\ntricity intensity of data transmission. Journal of In-\ndustrial Ecology, 22.\nClarisse Aujoux, Kumiko Kotera, and Odile Blanchard.\n2021. Estimating the carbon footprint of the grand\nproject, a multi-decade astrophysics experiment. As-\ntroparticle Physics, 131:102587.\nJayant Baliga, Robert Ayre, Kerry Hinton, and Rodney\nTucker. 2011. Green cloud computing: Balancing\nenergy in processing, storage, and transport. Pro-\nceedings of the IEEE, 99:149 – 167.\nNesrine Bannour, Sahar Ghannay, Aur´elie N´ev´eol, and\nAnne-Laure Ligozat. 2021. Evaluating the carbon\nfootprint of nlp methods: a survey and analysis of\nexisting tools. In EMNLP , Workshop SustaiNLP.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities\nand risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nAidan Clark, Diego de las Casas, Aurelia Guy, Arthur\nMensch, Michela Paganini, Jordan Hoffmann, Bog-\ndan Damoc, Blake Hechtman, Trevor Cai, Se-\nbastian Borgeaud, et al. 2022. Uniﬁed scaling\nlaws for routed language models. arXiv preprint\narXiv:2202.01169.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2021. Glam: Efﬁcient scaling of language mod-\nels with mixture-of-experts.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nUdit Gupta, Young Geun Kim, Sylvia Lee, Jor-\ndan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David\nBrooks, and Carole-Jean Wu. 2021. Chasing car-\nbon: The elusive environmental footprint of com-\nputing. In 2021 IEEE International Symposium on\nHigh-Performance Computer Architecture (HPCA),\npages 854–867. IEEE.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Dong Hyeon Jeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, et al. 2021. What changes can large-scale\nlanguage models bring? intensive study on hyper-\nclova: Billions-scale korean generative pretrained\ntransformers. arXiv preprint arXiv:2109.04650.\nAlexandre Lacoste, Alexandra Luccioni, Victor\nSchmidt, and Thomas Dandres. 2019. Quantifying\nthe carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021. Jurassic-1: Technical details and\nevaluation. White Paper. AI21 Labs.\nKadan Lottick, Silvia Susai, Sorelle A. Friedler, and\nJonathan P. Wilson. 2019. Energy usage reports:\nEnvironmental awareness as part of algorithmic ac-\ncountability. Workshop on Tackling Climate Change\nwith Machine Learning at NeurIPS 2019.\nVal´erie Masson-Delmotte, Panmao Zhai, Hans-Otto\nP¨ortner, Debra Roberts, Jim Skea, Priyadarshi R\nShukla, Anna Pirani, W Moufouma-Okia, C P ´ean,\n92\nR Pidcock, et al. 2018. Global warming of 1.5 c. An\nIPCC Special Report on the impacts of global warm-\ning of, 1(5).\nDeepak Narayanan, Mohammad Shoeybi, Jared\nCasper, Patrick LeGresley, Mostofa Patwary, Vijay\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021.\nEfﬁcient large-scale language model training on\ngpu clusters using megatron-lm. In Proceedings\nof the International Conference for High Perfor-\nmance Computing, Networking, Storage and Anal-\nysis, pages 1–15.\nDavid Patterson, Joseph Gonzalez, Urs H ¨olzle,\nQuoc Hung Le, Chen Liang, Lluis-Miquel Munguia,\nDaniel Rothchild, David So, Maud Texier, and Jef-\nfrey Dean. 2022. The Carbon Footprint of Machine\nLearning Training Will Plateau, Then Shrink.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\nAshley Pilipiszyn. 2021. Gpt-3 powers the next gener-\nation of apps.\nLorenzo Posani, Alessio Paccoia, and Marco Moschet-\ntini. 2019. The carbon footprint of distributed cloud\nstorage.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nAlbert Reuther, Peter Michaleas, Michael Jones, Vi-\njay Gadepally, Siddharth Samsi, and Jeremy Kep-\nner. 2020. Survey of machine learning accelerators.\nIn 2020 IEEE high performance extreme computing\nconference (HPEC), pages 1–12. IEEE.\nAlbert Reuther, Peter Michaleas, Michael Jones, Vi-\njay Gadepally, Siddharth Samsi, and Jeremy Kepner.\n2021. Ai accelerator survey and trends. In 2021\nIEEE High Performance Extreme Computing Con-\nference (HPEC), pages 1–9. IEEE.\nVictor Schmidt, Kamal Goyal, Aditya Joshi, Boris\nFeld, Liam Conell, Nikolas Laskaris, Doug Blank,\nJonathan Wilson, Sorelle Friedler, and Sasha Luc-\ncioni. 2021. CodeCarbon: Estimate and Track Car-\nbon Emissions from Machine Learning Computing.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54–63.\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Be-\nsiroglu, Marius Hobbhahn, and Pablo Villalobos.\n2022. Compute trends across three eras of machine\nlearning. arXiv preprint arXiv:2202.05924.\nMatthew Skiles, Euijin Yang, Orad Reshef,\nDiego Robalino Mu˜noz, Diana Cintron, Mary Laura\nLind, Alexander Rush, Patricia Perez Calleja,\nRobert Nerenberg, Andrea Armani, et al. 2021.\nConference demographics and footprint changed by\nvirtual platforms. Nature Sustainability, pages 1–8.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using\ndeepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in nlp.\nMariarosaria Taddeo, Andreas Tsamados, Josh Cowls,\nand Luciano Floridi. 2021. Artiﬁcial intelligence\nand the climate emergency: Opportunities, chal-\nlenges, and recommendations. One Earth, 4:776–\n779.\nNeil C. Thompson, Kristjan Greenewald, Keeheon Lee,\nand Gabriel F. Manso. 2020. The computational lim-\nits of deep learning.\nNenad Toma ˇsev, Julien Cornebise, Frank Hutter,\nShakir Mohamed, Angela Picciariello, Bec Con-\nnelly, Danielle Belgrave, Daphne Ezer, Fanny\nCachat van der Haert, Frank Mugisha, et al. 2020.\nAi for social good: unlocking the opportunity for\npositive impact. Nature Communications, 11(1):1–\n6.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In Inter-\nnational Conference on Machine Learning, pages\n10347–10357. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/\nmesh-transformer-jax.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, Michael Gschwind, Anurag Gupta,\nMyle Ott, Anastasia Melnikov, Salvatore Candido,\n93\nDavid Brooks, Geeta Chauhan, Benjamin Lee,\nHsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Ba-\nlandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim\nHazelwood. 2022. Sustainable ai: Environmental\nimplications, challenges and opportunities.\nJiwei Yang, Xu Shen, Jun Xing, Xinmei Tian,\nHouqiang Li, Bing Deng, Jianqiang Huang, and Xi-\nansheng Hua. 2019. Quantization networks.\n94",
  "topic": "Arabic",
  "concepts": [
    {
      "name": "Arabic",
      "score": 0.7472068667411804
    },
    {
      "name": "Carbon footprint",
      "score": 0.6772593259811401
    },
    {
      "name": "Computer science",
      "score": 0.544640064239502
    },
    {
      "name": "Footprint",
      "score": 0.4493027329444885
    },
    {
      "name": "Natural language processing",
      "score": 0.4187008738517761
    },
    {
      "name": "Linguistics",
      "score": 0.3466363549232483
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3460419774055481
    },
    {
      "name": "Cognitive science",
      "score": 0.34132540225982666
    },
    {
      "name": "History",
      "score": 0.25248247385025024
    },
    {
      "name": "Philosophy",
      "score": 0.21912020444869995
    },
    {
      "name": "Psychology",
      "score": 0.18903875350952148
    },
    {
      "name": "Archaeology",
      "score": 0.14516660571098328
    },
    {
      "name": "Geology",
      "score": 0.117220938205719
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Greenhouse gas",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087059",
      "name": "Technology Innovation Institute",
      "country": "AE"
    }
  ],
  "cited_by": 19
}