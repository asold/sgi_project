{
  "title": "Discrete Representations Strengthen Vision Transformer Robustness",
  "url": "https://openalex.org/W3217023900",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2670307061",
      "name": "Mao Cheng-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097595792",
      "name": "Jiang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2273124145",
      "name": "Dehghani, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746513368",
      "name": "Vondrick, Carl",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742363879",
      "name": "Sukthankar, Rahul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2937913430",
      "name": "Essa, Irfan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963812505",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3165098671",
    "https://openalex.org/W3175800953",
    "https://openalex.org/W3033102790",
    "https://openalex.org/W2131846894",
    "https://openalex.org/W2970692043",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2905544595",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2804047946",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1625255723",
    "https://openalex.org/W2989929945",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3128475431",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3168489096",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3142085127",
    "https://openalex.org/W2970018230",
    "https://openalex.org/W3094975698",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3104962541",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2902617128",
    "https://openalex.org/W3202202633",
    "https://openalex.org/W3106845355",
    "https://openalex.org/W3175937066",
    "https://openalex.org/W3035422918"
  ],
  "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",
  "full_text": "Published as a conference paper at ICLR 2022\nDISCRETE REPRESENTATIONS STRENGTHEN VISION\nTRANSFORMER ROBUSTNESS\nChengzhi Mao2∗, Lu Jiang1, Mostafa Dehghani1, Carl Vondrick2, Rahul Sukthankar1, Irfan Essa1,3\n1 Google Research\n2 Computer Science, Columbia University\n3 School of Interactive Computing, Georgia Insitute of Technology\n{mcz, vondrick}@cs.columbia.edu\n{lujiang, dehghani, sukthankar, irfanessa}@google.com\nABSTRACT\nVision Transformer (ViT) is emerging as the state-of-the-art architecture for im-\nage recognition. While recent studies suggest that ViTs are more robust than their\nconvolutional counterparts, our experiments ﬁnd that ViTs trained on ImageNet\nare overly reliant on local textures and fail to make adequate use of shape infor-\nmation. ViTs thus have difﬁculties generalizing to out-of-distribution, real-world\ndata. To address this deﬁciency, we present a simple and effective architecture\nmodiﬁcation to ViT’s input layer by adding discrete tokens produced by a vector-\nquantized encoder. Different from the standard continuous pixel tokens, discrete\ntokens are invariant under small perturbations and contain less information indi-\nvidually, which promote ViTs to learn global information that is invariant. Experi-\nmental results demonstrate that adding discrete representation on four architecture\nvariants strengthens ViT robustness by up to 12% across seven ImageNet robust-\nness benchmarks while maintaining the performance on ImageNet.\n1 I NTRODUCTION\nDespite their high performance on in-distribution test sets, deep neural networks fail to generalize\nunder real-world distribution shifts (Barbu et al., 2019). This gap between training and inference\nposes many challenges for deploying deep learning models in real-world applications where closed-\nworld assumptions are violated. This lack of robustness can be ascribed to learned representations\nthat are overly sensitive to minor variations in local texture and insufﬁciently adept at representing\nmore robust scene and object characteristics, such as the shape.\nVision Transformer (ViT) (Dosovitskiy et al., 2020) has started to rival Convolutional Neural Net-\nworks (CNNs) in many computer vision tasks. Recent works found that ViTs are more robust than\nCNNs (Paul & Chen, 2021; Mao et al., 2021b; Bhojanapalli et al., 2021) and generalize favorably on\na variety of visual robustness benchmarks (Hendrycks et al., 2021b; Hendrycks & Dietterich, 2019).\nThese work suggested that ViTs’ robustness comes from the self-attention architecture that captures\na globally-contextualized inductive bias than CNNs.\nHowever, though self-attention can model the shape information, we found that ImageNet-trained\nViT is still biased to textures than shape (Geirhos et al., 2019). We hypothesize that this deﬁciency\nin robustness comes from the high-dimensional, individually informative, linear tokenization which\nbiases ViT to minimize empirical risk via local signals without learning much shape information.\nIn this paper, we propose a simple yet novel input layer for vision transformers, where image patches\nare represented by discrete tokens. To be speciﬁc, we discretize an image and represent an image\npatch as a discrete token or “visual word” in a codebook. Our key insight is that discrete tokens\ncapture important features in a low-dimensional space (Oord et al., 2017) preserving shape and\nstructure of the object (see Figure 2). Our approach capitalizes on this discrete representation to\npromote the robustness of ViT. Using discrete tokens drives ViT towards better modeling of spatial\ninteractions between tokens, given that individual tokens no longer carry enough information to\n∗Work done while interning at Google Research.\n1\narXiv:2111.10493v2  [cs.CV]  2 Apr 2022\nPublished as a conference paper at ICLR 2022\n3 1 2 5\n4 8 0 1\n0 3 5 5\n3 2 9 5\nLinear Projection\nFine-tuned \nCodeBook\n0 [0.01, 0.2, 0.53, …, 0.04]\n1 [0.6, 0.22, 0.83, …, 0.01]\n1024 [0.26, 0.75, 0.27, …, 0.98]\nPosition \nEmbedding\nClassiﬁcation\nPrediction\nTransformer\nEncoder\nPixel Token Pixel Embedding\nVector\nQuantized Model\nDiscrete Token Discrete\n Embedding\nInput Image\nReconstructed\nImage\n*Class Token\nDecode\nFigure 1: Overview of\nthe proposed ViT us-\ning discrete representa-\ntions. In addition to the\npixel embeddings (or-\nange), we introduce dis-\ncrete tokens and em-\nbeddings (pink) as the\ninput to the standard\nTransformer Encoder of\nthe ViT model (Doso-\nvitskiy et al., 2020).\ndepend on. We also concatenate a low dimensional pixel token to the discrete token to compensate\nfor the potentially missed local details encoded by discrete tokens, especially for small objects.\nOur approach only changes the image patch tokenizer to improve generalization and robustness,\nwhich is orthogonal to all existing approaches for robustness, and can be integrated into architectures\nthat extend vision transformer. We call the ViT model using ourDiscrete representation or Dr. ViT.\nOur experiments and visualizations show that incorporating discrete tokens in ViT signiﬁcantly im-\nproves generalization accuracy for all seven out-of-distribution ImageNet benchmarks: Stylized-\nImageNet by up to 12%, ImageNet-Sketch by up to 10%, and ImageNet-C by up to 10%. Our\nmethod establishes the new state-of-the-art on four benchmarks without any dataset-speciﬁc data\naugmentation. The success of discrete representation has been limiting to image generation (Ramesh\net al., 2021; Esser et al., 2021). Our work is the ﬁrst to connect discrete representations to robustness\nand demonstrate consistent robustness improvements. We hope our work helps pave the road to a\njoint vision transformer for both image classiﬁcation and generation.\n2 R ELATED WORK\nVision Transformer (ViT)(Dosovitskiy et al., 2020), inspired by the Transformer (Vaswani et al.,\n2017) in NLP, is the ﬁrst CNN-free architecture that achieves state-of-the-art image classiﬁcation\naccuracy. Since its inception, numerous works have proposed improvements to the ViT architec-\nture (Wang et al., 2021; Chu et al., 2021; Liu et al., 2021; d’Ascoli et al., 2021), objective (Chen\net al., 2021a), training strategy (Touvron et al., 2021), etc.. Given the difﬁculty to study all existing\nViT models, this paper focuses on the classical ViT model (Dosovitskiy et al., 2020) and its recent\npublished versions. Speciﬁcally, Steiner et al. (2021) proposed ViT-AugReg that applies stronger\ndata augmentation and regularization to the ViT model. Tolstikhin et al. (2021) introduced MLP-\nMixer to replace self-attention in ViT with multi-layer perceptions (MLP).\nWe select the above ViT model family (Dosovitskiy et al., 2020; Steiner et al., 2021; Tolstikhin\net al., 2021) in our robustness study for three reasons. First, they represent both the very ﬁrst and\none-of-the-best vision transformers in the literature. Second, these models demonstrated competitive\nperformance when pre-trained on sufﬁciently large datasets such as ImageNet-21K and JFT-300M.\nFinally, unlike other Transformer models, they provide architectures consisting of solely Trans-\nformer layers as well as a hybrid of CNN and Transformer layers. These properties improve our\nunderstanding of robustness for different types of network layers and datasets.\nRobustness. Recent works established multiple content robustness datasets to evaluate the out-of-\ndistribution generalization of deep models (Barbu et al., 2019; Hendrycks et al., 2021b;a; Wang\net al., 2019; Geirhos et al., 2019; Hendrycks & Dietterich, 2019; Recht et al., 2019). In this paper,\nwe consider 7 ImageNet robustness benchmarks of real-world test images (or proxies) where deep\nmodels trained on ImageNet are shown to suffer from notable performance drop. Existing works\non robustness are targeted at closing the gap in a subset of these ImageNet robustness benchmarks\nand were extensively veriﬁed with the CNNs. Among them, carefully-designed data augmenta-\ntions (Hendrycks et al., 2021a; Cubuk et al., 2018; Steiner et al., 2021; Mao et al., 2021b;a), model\nregularization (Wang et al., 2019; Huang et al., 2020b; Hendrycks et al., 2019), and multitask learn-\ning (Zamir et al., 2020) are effective to address the issue.\nMore recently, a few studies (Paul & Chen, 2021; Bhojanapalli et al., 2021; Naseer et al., 2021;\nShao et al., 2021) suggest that ViTs are more robust than CNNs. Existing works mainly focused\n2\nPublished as a conference paper at ICLR 2022\non analyzing the cause of superior generalizability in the ViT model. As our work focuses on\ndiscrete token input, we train the models using the same data augmentation as the ViT-AugReg\nbaseline (Steiner et al., 2021). While tailoring data augmentation (Mao et al., 2021b) may further\nimprove our results, we leave it out of the scope of this paper.\nDiscrete Representationwas used as a visual representation prior to the deep learning revolution,\nsuch as in bag-of-visual-words model (Sivic & Zisserman, 2003; Csurka et al., 2004) and VLAD\nmodel (Arandjelovic & Zisserman, 2013). Recently, (Oord et al., 2017; Vahdat et al., 2018) proposed\nneural discrete representation to encode an image as integer tokens. Recent works used discrete\nrepresentation mainly for image synthesis (Ramesh et al., 2021; Esser et al., 2021). To the best of our\nknowledge, our work is the ﬁrst to demonstrate discrete representations strengthening robustness.\nThe closest work to ours is BEiT (Bao et al., 2021) that pretrains the ViTs to predict the masked\ntokens. However, the tokens are discarded after pretraining, where the ViT model can still overﬁt\nthe non-robust nuisances in the pixel tokens at later ﬁnetuning stage, undermining its robustness.\n3 M ETHOD\n3.1 P RELIMINARY ON VISION TRANSFORMER\nVision Transformer (Dosovitskiy et al., 2020) is a pure transformer architecture that operates on a\nsequence of image patches. The 2D image x ∈RH×W×C is ﬂattened into a sequence of image\npatches, following the raster scan, denoted by xp ∈RL×(P2·C), where L = H×W\nP2 is the effective\nsequence length and P2 ×Cis the dimension of image patch. A learnable classiﬁcation token xclass\nis prepended to the patch sequence, then the position embeddingEpos is added to formulate the ﬁnal\ninput embedding h0.\nh0 = [xclass; x1\npE; x2\npE; ··· ; xL\npE] +Epos, E ∈R(P2·C)×D, Epos ∈R(L+1)×D (1)\nh′\nℓ = MSA(LN(hℓ−1)) +hℓ−1, ℓ = 1,...,L f (2)\nhℓ = MLP(LN(h′\nℓ)) +h′\nℓ, ℓ = 1,...,L f (3)\ny = LN(h0\nL), (4)\nThe architecture of ViT follows that of the Transformer (Vaswani et al., 2017), which alternates\nlayers of multi-headed self-attention (MSA) and multi-layer perceptron (MLP) with LayerNorm\n(LN) and residual connections being applied to every block. We denote the number of blocks asLf.\nThis paper considers the ViT model family consisting of 4 ViT backbones: the vanilla ViT dis-\ncussed above, ViT-AugReg (Steiner et al., 2021) which shares the same ViT architecture but applies\nstronger data augmentation and regularization, MLP-Mixer (Tolstikhin et al., 2021) which replaces\nself-attention in ViT with MLP, a variant called Hybrid-ViT which replaces the raw image patches\nin Equation 1 with the CNN features extracted by a ResNet-50 (He et al., 2016).\n3.2 A RCHITECTURE\nExisting ViTs represent an image patch as a sequence ofpixel tokens, which are linear projections of\nﬂattened image pixels. We propose a novel architecture modiﬁcation to the input layer of the vision\ntransformer, where an image patch xp is represented by a combination of two embeddings. As\nillustrated in Fig. 1, in addition to the original pixel-wise linear projection, we discretize an image\npatch into an discrete token in a codebook V ∈RK×dc, where Kis the codebook size and dc is the\ndimension of the embedding. The discretization is achieved by a vector quantized (VQ) encoder pθ\nthat produces an integer zfor an image patch xas:\npθ(z= k|x) =1(k= arg min\nj=1:K\n∥ze(x) −Vj∥2), (5)\nwhere ze(x) denotes the output of the encoder network and 1(·) is the indicator function.\nThe encoder is applied to the patch sequence xp ∈RL×(P2·C) to obtain an integer sequence zd ∈\n{1,2,...,K }L. Afterward, we use the embeddings of both discrete and pixel tokens to construct the\ninput embedding to the ViT model. Speciﬁcally, the input embedding in Equation 1 is replaced by:\nh0 = [xclass; f(Vz1\nd\n,x1\npE); f(Vz2\nd\n,x2\npE); ··· ; f(VzL\nd\n,xL\npE)] +Epos, (6)\nwhere f is the function, embodied as a neural network layer, to combine the two embeddings. We\nempirically compared four network designs for f and found that the simplest concatenation works\n3\nPublished as a conference paper at ICLR 2022\n4\n1\n2\n8\n9\n7\n5\n9\n2\ndecode\ntabby \ncat\ntabby \ncat\nvulture\nflamingo\nflamingo\ncoutinous \ntokens\n(image \npatch)\ndiscrete \ntokens\nFigure 2: Comparison\nof pixel tokens (top)\nand the reconstructed\nimage decoded from\nthe discrete tokens\n(bottom). Discrete to-\nkens capture important\nshapes and structures\nbut may lose local\ntexture.\nbest. Note that our model only modiﬁes the input layer of ViT (Equation 1) and leaves intact the\nremaining layers depicted in Equation 2-4.\nComparison of pixel and discrete embeddings:Pixel and discrete embeddings represent differ-\nent aspects of the input image. Discrete embeddings capture important features in a low-dimension\nspace (Oord et al., 2017) that preserves the global structure of an object but lose local details. Fig. 2\ncompares the original image (top) and the reconstructed images decoded from the discrete embed-\ndings of our model (bottom). As shown, the decoded images from discrete embeddings reasonably\ndepict the object shape and global context. Due to the quantization, the decoder hallucinates the\nlocal textures, e.g., in the cat’s eye, or the text in the “vulture” and “ﬂamingo” images. It is worth\nnoting that the VQ encoder/decoder is only trained on ImageNet 2012 but they can generalize to\nout-of-distribution images. Please see more examples in Appendix A.\nOn the ﬂip side, pixel embeddings capture rich details through the linear projection from raw pixels.\nHowever, given the expressive power of transformers, ViTs can spend capacity on local textures\nor nuance patterns that are often circumferential to robust recognition. Since humans recognize\nimages primarily relying on the shape and semantic structure, this discrepancy to human perception\nundermines ViT’s generalization on out-of-distribution data. Our proposed model leverages the\npower of both embeddings to promote the interaction between modeling global and local features.\n3.3 T RAINING PROCEDURE\nThe training comprises two stages: pretraining and ﬁnetuning. First, we pretrain the VQ-V AE\nencoder and decoder (Oord et al., 2017) on the given training set. We do not use labels in this\nstep. In the ﬁnetuning stage, as shown in Fig. 3a, we train the proposed ViT model from scratch and\nﬁnetune the discrete embeddings. Due to the straight-through gradient estimation in VQ-V AE (Oord\net al., 2017), we stop the back-propagation after the discrete embedding.\n1 # x: input image mini-batch; pixel_embed: pixel\n2 # embeddings of x. vqgan.encoder and codebook are\n3 # initialized form the pretraining.\n4 import jax.numpy as np\n5 discrete_token = jax.lax.stop_gradient(vqgan.encoder(x))\n6 discrete_embed = np.dot(discrete_token, codebook)\n7 tokens = np.concatenate(\n8 [discrete_embed, pixel_embed], dim=2)\n9 predictions = TransformerEncoder(tokens)\n(a) Pseudo JAX code\nViT\nOurs\n (b) Pixel/RGB embedding ﬁlters\nFigure 3: (a) Pseudo code for training the proposed ViT model. (b) Comparing visualized pixel\nembeddings of the ViT and our model. Top row shows the randomly selected ﬁlters and Bottom\nshows the ﬁrst 28 principal components. Our ﬁlters capture more structural and shape patterns.\nNow we discuss the objective that the proposed ViT model optimizes. Let qφ(z|x) denote the VQ\nencoder, parameterized by network weights φ, to represent an image xinto a sequence of integer\ntokens z. The decoder models the distribution pθ(x|z) over the RGB image generated from discrete\ntokens. pψ(y|x,z) stands for the proposed vision transformer shown in Fig. 1.\nWe factorize the joint distribution of image x, label y, and the discrete token z by p(x,y,z ) =\npφ(x|z)pψ(y|x,z)p(z). Our overall training procedure is to maximize the evidence lower bound\n4\nPublished as a conference paper at ICLR 2022\n(ELBO) on the joint likelihood:\nlog p(x,y) ≥Eqφ(z|x)[log pθ(x|z)] −DKL[qφ(z|x)∥pψ(y|x,z)p(z)] (7)\nIn the ﬁrst stage, we maximize the ELBO with respect to φand θ, which corresponds to learning\nthe VQ-V AE encoder and decoder. Following (Oord et al., 2017), we assume a uniform prior for\nboth pψ(y|x,z) and p(z). Given that qφ(z|x) predicts a one-hot output, the regularization term (KL\ndivergence in Equation 7) will be a constant. Note that the DALL-E model (Ramesh et al., 2021)\nuses a similar assumption to stabilize the training. Our implementation uses VQ-GAN (Esser et al.,\n2021) which adds a GAN loss and a perceptual loss. We also include results with VQ-V AE (Oord\net al., 2017) in Appendix A.8 for reference.\nIn the ﬁnetuning stage, we optimize ψwhile holding θand φﬁxed, which corresponds to learning a\nViT and ﬁnetuning the discrete embeddings. This can be seen by rearranging the ELBO:\nlog p(x,y) ≥Eqφ(z|x)[log pψ(y|x,z)] +Eqφ(z|x)[log pθ(x|z)p(z)\nqφ(z|x) ], (8)\nwhere pψ(y|x,z) is the proposed ViT model. The ﬁrst term denotes the likelihood for classiﬁca-\ntion that is learned by minimizing the multi-class cross-entropy loss. The second term becomes a\nconstant given the ﬁxed θ and φ. It is important to note that the discrete embeddings are learned\nend-to-end in both pretraining and ﬁnetuning. Details are discussed in Appendix A.2.\n4 E XPERIMENTS\n4.1 E XPERIMENTAL SETUP\nDatasets. In the experiments, we train all the models, including ours, on ImageNet 2012 or\nImageNet-21K under the same training settings, where we use identical training data, batch size,\nand learning rate schedule, etc. Afterward, the trained models are tested on the ImageNet robustness\nbenchmarks to assess their robustness and generalization capability.\nIn total, we evaluate the models on nine benchmarks. ImageNet and ImageNet-Real are two in-\ndistribution datasets. ImageNet (Deng et al., 2009) is the standard validation set of ILSVRC2012.\nImageNet-Real (Beyer et al., 2020) corrects the label errors in the ImageNet validation set (Northcutt\net al., 2021), which measures model’s generalization on different labeling procedures.\nSeven out-of-distribution (OOD) datasets are considered. Fig. 4 shows their example images.\nImageNet-Rendition (Hendrycks et al., 2021a) is an OOD dataset that contains renditions, such\nas art, cartoons, of 200 ImageNet classes. Stylized-ImageNet (Geirhos et al., 2019) is used to induce\nthe texture vs. the shape bias by stylizing the ImageNet validation set with 79,434 paintings. The\n“shape ImageNet labels” are used as the label. ImageNet-Sketch (Wang et al., 2019) is a “black\nand white” dataset constructed by querying the sketches of the 1,000 ImageNet categories. Object-\nNet (Barbu et al., 2019) is an OOD test set that controls the background, context, and viewpoints of\nthe data. Following the standard practice, we evaluate the performance on the 113 overlapping Ima-\ngeNet categories. ImageNet-V2 (Recht et al., 2019) is a new and more challenging test set collected\nfor ImageNet. The split “matched-frequency” is used. ImageNet-A (Hendrycks et al., 2021b) is a\nnatural adversarial dataset that contains real-world, unmodiﬁed, natural examples that cause image\nclassiﬁers to fail signiﬁcantly. ImageNet-C (Hendrycks & Dietterich, 2019) evaluates the model’s\nrobustness under common corruptions. It contains 5 serveries of 15 synthetic corruptions including\n‘Defocus,’ ‘Fog,’ and ‘JPEG compression’.\nModels. We verify our method on three ViT backbones: the classical ViT (Dosovitskiy et al., 2020)\ntermed as ViT Vanilla, ViT-AugReg (Steiner et al., 2021), and MLPMixer (Tolstikhin et al., 2021).\nBy default, we refer ViT to ViT-AugReg considering its superior performance. For ViT, we study the\nTiny (Ti), the Small (S), and the Base (B) variants, all using the patch size of 16x16. We also com-\npare with a CNN-based ResNet (He et al., 2016) baseline, and the Hybrid-ViT model (Dosovitskiy\net al., 2020) that replaces image patch embedding with a ResNet-50 encoder.\nImplementation details On ImageNet, the models are trained with three ViT model variants, i.e.\nTi, S, and B, from small to large. The codebook size is K = 1024, and the codebook embeds the\ndiscrete token into dc = 256dimensions. On ImageNet-21K, the quantizer model is a VQGAN and\nis trained on ImageNet-21K only with codebook size K = 8,192. All models including ours use\nthe same augmentation (RandAug and Mixup) as in the ViT baseline (Steiner et al., 2021). More\ndetails can be found in the Appendix A.12.\n5\nPublished as a conference paper at ICLR 2022\nTable 1: Model performance trained on ImageNet. All columns indicate the top-1 classiﬁcation accuracy\nexcept the last column ImageNet-C which indicates the mean Corruption Error (the lower the better). The bold\nnumber indicates higher accuracy than the corresponding baseline. The box highlights the best accuracy.\nOut of Distribution Robustness Test\nModel ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nResNet-50 76.61 83.01 36.35 6.56 23.06 26.52 64.70 4.83 75.07\nViT-B Vanilla 72.47 78.69 24.56 5.94 14.34 13.44 59.32 5.23 88.72\n+ Ours (discrete only) 73.73 79.63 34.61 8.94 23.66 20.84 60.30 6.45 74.82\nViT-Ti 58.75 66.30 21.37 6.17 10.76 12.63 46.89 3.73 86.62\n+Ours (discrete only) 61.74 69.94 32.35 13.44 19.94 15.35 50.60 3.81 83.62\nMLPMixer 68.33 74.74 30.65 7.03 21.54 13.47 53.52 5.20 81.11\n+ Ours (discrete only) 68.00 74.36 33.85 8.98 25.36 15.44 54.12 4.88 80.75\n+ Ours 69.23 76.04 36.27 13.05 26.34 16.45 55.68 4.84 72.06\nHybrid ViT-S 75.10 81.45 34.01 7.42 26.69 24.50 62.53 6.17 69.26\nViT-S 75.21 82.36 34.39 10.39 23.27 24.50 63.00 10.39 61.99\n+ Ours (discrete only) 72.42 80.14 42.58 18.44 31.16 23.95 60.89 7.52 66.82\n+ Ours 77.03 83.59 39.02 14.22 28.78 26.49 64.49 11.85 56.89\nHybrid ViT-B 74.94 80.54 33.03 7.50 25.33 23.08 61.30 7.44 69.61\nViT-B 78.73 84.85 38.15 10.39 28.60 28.71 67.34 16.92 53.51\n+ Ours (discrete only) 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\n+ Ours 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nViT-B (384x384) 81.63 85.06 38.23 7.58 28.07 32.36 68.57 24.01 59.01\n+ Ours 81.83 86.48 44.70 14.06 35.72 36.01 70.33 27.19 46.32\n4.2 M AIN RESULTS\nTable 1 shows the results of the models trained on ImageNet. All models are trained under the same\nsetting with the same data augmentation from (Steiner et al., 2021) except for the ViT-B Vanilla\nrow, which uses the data augmentation from (Dosovitskiy et al., 2020). Our improvement is en-\ntirely attributed to the proposed discrete representations. By adding discrete representations, all ViT\nvariants, including Ti, S, and B, improve robustness across all eight benchmarks. When only using\ndiscrete representations as denoted by “(discrete only)”, we observe a larger margin of improvement\n(10%-12%) on the datasets depicting object shape: Rendition, Sketch, and Stylized-ImageNet. It\nis worth noting that it is a challenging for a single model to obtain robustness gains across all the\nbenchmarks as different datasets may capture distinct types of data distributions.\nTable 2: Model performance when pretrained on ImageNet21K and ﬁnetuned on ImageNet with 384x384\nresolution. See the caption of Table 1 for more description.\nOut of Distribution Robustness Test\nModel ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nViT-B 84.20 88.61 51.23 13.59 37.83 44.30 74.54 46.59 49.62\n+ Ours (discrete only) 83.40 88.44 55.26 19.69 44.72 43.92 72.68 36.64 45.86\n+ Ours 84.43 88.88 54.64 18.05 41.91 44.61 75.17 44.97 38.74\n+ Ours (512x512) 85.07 89.04 54.27 16.02 41.92 46.62 75.55 52.64 38.97\nOur approach scales as more training data is available. Table 2 shows the performance for the models\npretrained on ImageNet-21K and ﬁnetuned on ImageNet. Training on sufﬁciently large datasets\ninherently improves robustness (Bhojanapalli et al., 2021) but also renders further improvement\neven more challenging. Nevertheless, our model consistently improves the baseline ViT-B model\nacross all robustness benchmarks (by up to 10% on ImageNet-C). The results in Table 1 and Table 2\nvalidate our model as a generic approach that is highly effective for the models pretrained on the\nsufﬁciently large ImageNet-21k dataset.\nComparison to the State-of-the-Art We compare our model with the state-of-the-art results, in-\ncluding ViT with CutOut (DeVries & Taylor, 2017) and CutMix (Yun et al., 2019), on four datasets\nin Table 3 and Table 4. It is noteworthy that different from the compared approaches that are tai-\nlored to speciﬁc datasets, our method is generic and uses the same data augmentation as our ViT\nbaseline (Steiner et al., 2021), i.e. RandAug and Mixup, for all datasets.\nOn ImageNet-Rendition, we compare with the leaderboard numbers from (Hendrycks et al., 2021a).\nTrained on ImageNet, our approach beats the state-of-the-art DeepAugment+AugMix approach by\n6\nPublished as a conference paper at ICLR 2022\nImageNet\n \nRendition\nStylized\ncar\noriginal\nSketch\nObjectNet\nImageNet-A\nImageNet-C\nImageNet-V2\ndecoded\noriginal\ndecoded\noriginal\ndecoded\noriginal\ndecoded\nFigure 4: Visualization of the\neight evaluation benchmarks.\nEach image consists of the\noriginal test image (Left) and the\ndecoded image from the ﬁnetuned\ndiscrete embeddings (Right).\nNote that the encoder and decoder\nare trained only on ImageNet\n2012 data but generalize on out-\nof-distribution datasets. See more\nexamples in Appendix A.1.1.\nTable 3: Comparison to the state-of-the-art classiﬁcation accuracy on three ImageNet robustness datasets.\nModel Rendition\nResNet 50 36.1\nResNet 50 *21K 37.2\nDeepAugment 42.2\nDeepAugment + AugMix 46.8\nRandAug + Mixup 29.6\nViT-B 38.2\nViT-B + CutOut 38.1\nViT-B + CutMix 38.4\nOur ViT-B (discrete only) 48.8\nOur ViT-B *21K 55.3\n(a) ImageNet-Rendition\nModel Sketch\nHuang et al. (2020a) 16.1\nRandAug + Mixup 17.7\nXu et al. (2020) 18.1\nMishra et al. (2020) 24.5\nHermann et al. (2020) 30.9\nViT-B 23.3\nViT-B + CutOut 26.9\nViT-B + CutMix 27.5\nOur ViT-B (discrete only) 39.1\nOur ViT-B (discrete only) *21K 44.7\n(b) ImageNet-Sketch\nModel Stylized\nTop5\nBagNet-9 1.4\nBagNet-17 2.5\nBagNet-33 4.2\nResNet-50 16.4\nViT-B 22.2\nViT-B + CutOut 24.7\nViT-B + CutMix 22.7\nViT-B *21K 31.3\nOur ViT-B (discrete only) 40.3\n(c) Stylized-ImageNet\n2%. Notice that the augmentation we used—RandAug+MixUp—is 17% worse than the DeepAug-\nment+AugMix on ResNets. Our performance can be further improved by another 6% when pre-\ntrained on ImageNet-21K. On ImageNet-Sketch, we surpass the state-of-the-art (Hermann et al.,\n2020) by 8%. On Stylized-ImageNet, our approach improves 18% top-5 accuracy by switching to\ndiscrete representation. On ImageNet-C, our approach slashes the prior mCE by up to 10%. Note\nthat most baselines are trained with CNN architectures that have a smaller capacity than ViT-B. The\ncomparison is fair for the bottom entries that are all built on the same ViT-B backbone.\n4.3 I N-DEPTH ANALYSIS\nIn this subsection, we demonstrate, both quantitatively and qualitatively, that discrete representations\nfacilitate ViT to better capture object shape and global contexts.\nQuantitative results. Result in Table 3c studies the OOD generalization setting “IN-SIN” used\nin (Geirhos et al., 2019), where the model is trained only on the ImageNet (IN) and tested on the\nStylized ImageNet (SIN) images with conﬂicting shape and texture information, e.g. the images of a\ncat with elephant texture. The Stylized-ImageNet dataset is designed to measure the model’s ability\nto recognize shapes rather than textures, and higher performance in Table 3c is a direct proof of\nour model’s efﬁcacy in recognizing objects by shape. While ViT outperforms CNN on the task, our\ndiscrete representations yield another 10+% gain. However, ifthe discrete token is replaced with the\nglobal, continuous CNN features, such robustness gain is gone (cf. Table 8). This substantiates the\nbeneﬁt of discrete representations in recognizing object shapes.\nAdditionally, we conduct the “shape vs. texture biases” analysis following (Geirhos et al., 2019)\nunder the OOD setting “IN-SIN” (cf. Appendix A.5). Figure. 5a compares shape bias between\nTable 4: State-of-the-art mean Corruption Error (mCE) ↓ rate on ImageNet-C (the smaller the better).\nModel mCE\n↓ Gauss Shot Impul Defoc Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG\nResnet152 (He et al., 2016) 69.27 72.5 73.4 76.3 66.9 81.4 65.7 74.5 70.7 67.8 62.1 51.0 67.1 75.6 68.9 65.1\n+Stylized (Geirhos et al., 2019) 64.19 63.3 63.1 64.6 66.1 77.0 63.5 71.6 62.4 65.4 59.4 52.0 62.0 73.2 55.3 62.9\n+GenInt (Mao et al., 2021a) 61.70 59.2 60.2 62.4 60.7 70.8 59.5 69.9 64.4 63.8 58.3 48.7 61.5 70.9 55.2 60.0\nDA (Hendrycks et al., 2021a) 53.60 - - - - - - - - - - - - - - -\nViT-B 52.71 43.0 47.2 44.4 63.2 73.4 55.1 70.8 51.6 45.6 35.2 44.2 41.3 61.6 54.0 59.4\nViT-B + Ours 46.22 36.9 38.6 36.0 54.6 57.4 53.4 63.2 45.4 38.7 34.1 40.9 39.6 56.6 45.0 53.0\nViT-B *21K 49.62 47.0 48.3 46.0 55.7 65.6 46.7 54.0 40.0 40.6 32.2 42.3 42.0 57.1 63.1 63.6\nViT-B + Ours *21K 38.74 30.5 30.6 29.2 47.3 54.3 44.4 49.4 34.5 31.7 25.7 34.7 33.1 52.9 39.5 43.2\n7\nPublished as a conference paper at ICLR 2022\nhuman \nmean=0.96\nDr. \nViT-B/16 \n(Ours) \nmean \n= \n0.62\nViT-B/16 \nmean \n= \n0.42\nResNet-50 \nmean \n= \n0.20\n(a) Shape vs. Texture Biases.\n(i) \nattention \non \ntest \nimages\n(ii) \naverage \nattention\nViT Ours ViT Ours Image Image ViT\nOurs\n (b) Attention Visualization.\nFigure 5: We show the fraction of shape decisions on Stylized-ImageNet in Figure (a), and attention\non OOD images in Figure (b), where (i) is the attention map, and (ii) is the heat map of averaged\nattention from images in a mini-batch. See Appendix A.1.4 for details.\nhumans and three models: ResNet-50, ViT-B, and Ours. It shows the scores on 16 categories along\ntheir average denoted by the colored horizontal line. Humans are highly biased towards shape with\nan average fraction of 0.96 to correctly recognize an image by shape. ViT (0.42) is more shape-\nbiased than CNN ResNet-50 (0.20), which is consistent with the prior studies (Naseer et al., 2021).\nAdding discrete representation (0.62) greatly shrinks the gap between the ViT (0.42) and human\nbaseline (0.96). Such behavior is not observed when adding global CNN features whose average\nfraction of shape decisions is 0.3, lower than the baseline ViT, hence is not displayed in the ﬁgure.\nFinally, we validate discrete representation’s ability in modeling shape information via position em-\nbedding. Following (Chen et al., 2021b), we compare training ViT with and without using position\nembedding. As position embedding is the only vehicle to equip ViT with shape information, its\ncontribution suggests to what degree the model makes use of shape for recognition. As shown in\nTable 5, removing position embedding from the ViT model only leads to a marginal performance\ndrop (2.8%) on ImageNet, which is consistent with (Chen et al., 2021b). However, without po-\nsition embedding, our model accuracy drops by 29%, and degrades by a signiﬁcant 36%-94% on\nthe robustness benchmarks. This result shows that spatial information becomes crucial only when\ndiscrete representation is used, which also suggests our model relies on more shape information for\nrecognition.\nTable 5: Contribution of position embedding for robust recognition as measured by the relative performance\ndrop when the position embedding is removed. Position embedding is much more crucial in our model.\nOut of Distribution Robustness Test\nModel ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nViT-B 78.73 84.85 38.15 10.39 28.6 28.71 67.34 16.92 53.51\n-w/o. PosEmb 76.51 77.01 28.25 5.86 15.17 24.02 63.22 13.13 69.99\nRelative drop (%) 2.8% 9.2% 26.0% 43.6% 47.0% 16.3% 6.1% 22.4% 30.8%\nOurs ViT-B 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\n- w/o. PosEmb 56.27 59.06 17.24 4.14 6.57 11.36 42.98 3.51 89.76\nRelative drop (%) 29.2% 30.4% 61.5% 78.6% 81.0% 62.8% 36.8% 79.6% 94.2%\nQualitative results. First, following (Dosovitskiy et al., 2020), we visualize the learned pixel em-\nbeddings, called ﬁlters, of the ViT-B model and compare them with ours in Fig. 3b with respect to(1)\nrandomly selected ﬁlters (the top row in Fig. 3b) and(2) the ﬁrst principle components (the bottom).\nWe visualize the ﬁlters of our default model here and include more visualization in Appendix A.1.3.\nThe evident visual differences suggest that our learned ﬁlters capture structural patterns.\nSecond, we compare the attention from the classiﬁcation tokens in Fig. 5b, where (i) visualizes the\nindividual images and (ii) averages attention values of all the image in the same mini-batch. Our\nmodel attends to image regions that are semantically relevant for classiﬁcation and captures more\nglobal contexts relative to the central object.\nFinally, we investigate what information is preserved in discrete representations. Speciﬁcally, we\nreconstruct images by the VQ-GAN decoder from the discrete tokens and ﬁnetuned discrete embed-\ndings. Fig. 4 visualizes representative examples from ImageNet and the seven robustness bench-\nmarks. By comparing the original and decoded images in Fig. 4, we ﬁnd the discrete representation\n8\nPublished as a conference paper at ICLR 2022\npreserves object shape but can perturb unimportant local signals like textures. For example, the\ndecoded image changes the background but keeps the dog shape in the last image of the ﬁrst row.\nSimilarly, it hallucinates the text but keeps the bag shape in the ﬁrst image of the third row. In\nthe ﬁrst image of ImageNet-C, the decoding deblurs the bird image by making the shape sharper.\nNote that the discrete representation is learned only on ImageNet, but it can generalize to the other\nout-of-distribution test datasets. We present more results with failure cases in Appendix A.1.1.\n4.4 A BLATION STUDIES\nModel capacity vs. robustness.Does our robustness come from using larger models? Fig. 6 shows\nthe robustness vs. the number of parameters for the ViT baselines (blue) and our models (orange).\nWe use ViT variants Ti, S, B, and two hybrid variants Hybrid-S and Hybrid-B, as baselines. We use\n“+Our” to denote our method and “+Small” to indicate that we use a smaller version of quantization\nencoder (cf. Appendix A.11.1). With a similar amount of model parameters, our model outperforms\nthe ViT and Hybrid-ViT in robustness.\nCodebook size vs. robustness.In Table 6, we conduct an ablation study on the size of the codebook,\nfrom 1024 to 8192, using the small variant of our quantized encoder trained on ImageNet. A larger\ncodebook size gives the model more capacity, making the model closer to a continuous one. Overall,\nStylized-ImageNet beneﬁts the most when the feature is highly quantized. With a medium codebook\nsize, the model strikes a good balance between quantization and capacity, achieving the best overall\nperformance. A large codebook size learned on ImageNet can hurt performance.\n0 50 100 150 200 250 300\nparams (Million)\n20\n25\n30\n35\n40\n45\n50\nRedition Acc(%)\n Ti\n S  Hybrid-S\n B\n     Hybrid-B  LTi+Our\nS+Our\nB+Our+Small\nB+Our\n0 50 100 150 200 250 300\nparams (Million)\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nStylized Acc(%)\n Ti\n S\n Hybrid-S\n B\n     Hybrid-B  L\nTi+Our\nS+Our\nB+Our+Small\nB+Our\n0 50 100 150 200 250 300\nparams (Million)\n10\n15\n20\n25\n30\n35\n40\nSketch Acc(%)\n Ti\n S\n Hybrid-S\n B\n     Hybrid-B  L\nTi+Our\nS+Our\nB+Our+Small\nB+Our\n0 50 100 150 200 250 300\nparams (Million)\n45\n50\n55\n60\n65\n70\n75\n80\n85\nImageNet-C MC-Error \n Ti\n S\n Hybrid-S\n B\n     Hybrid-B\n L\nTi+Our\nS+Our\nB+Our+Small\nB+Our\nFigure 6: The robustness vs. #model-parameters on 4 robust test set. Our models (orange) achieve\nbetter robustness with a similar model capacity.\nTable 6: The impact of codebook size for robustness.\nCodeBook Out of Distribution Robustness Test\nSize ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\n1024 76.63 77.69 45.92 21.95 35.02 26.03 64.18 10.68 58.64\n4096 77.31 78.17 47.04 21.33 35.71 27.72 64.79 11.37 57.26\n8192 77.04 77.92 46.58 20.94 35.72 27.54 65.23 11.00 57.89\nDiscrete vs. Continuous global representation.In Appendix A.4, instead of using our discrete\ntoken, we study whether concatenate continuous CNN representations with global information can\nimprove robustness. Results show that concatenating global CNN representation performs no better\nthan the baseline, which demonstrates the necessity of discrete representations.\nNetwork designs for combining the embeddings.In Appendix A.9, we experiment with 4 different\ndesigns, including addition, concatenation, residual gating (V o et al., 2019), and cross-attention, to\ncombine the discrete and pixel embeddings. Among them, the simple concatenation yields the best\noverall result.\n5 C ONCLUSION\nThis paper introduces a simple yet highly effective input representations for vision transformers, in\nwhich an image patch is represented as the combined embeddings of pixel and discrete tokens. The\nresults show the proposed method is generic and works with several vision transformer architectures,\nimproving robustness across seven out-of-distribution ImageNet benchmarks. Our new ﬁndings\nconnect the robustness of vision transformer to discrete representation, which hints towards a new\ndirection for understanding and improving robustness as well as a joint vision transformer for both\nimage classiﬁcation and generation.\n9\nPublished as a conference paper at ICLR 2022\n6 A CKNOWLEDGEMENTS\nWe thank the discussions and feedback from Han Zhang, Hao Wang, and Ben Poole.\nEthics statement: The authors attest that they have reviewed the ICLR Code of Ethics for the\n2022 conference and acknowledge that this code applies to our submission. Our explicit intent with\nthis research paper is to improve the state-of-the-art in terms of accuracy and robustness for Vision\nTransformers. Our work uses well-established datasets and benchmarks and undertakes detailed\nevaluation of these with our novel and improved approaches to showcase state-of-the-art improve-\nments. We did not undertake any subject studies or conduct any data-collection for this project. We\nare committed in our work to abide by the eight General Ethical Principles listed at ICLR Code of\nEthics (https://iclr.cc/public/CodeOfEthics).\nReproducibility statement Our approach is simple in terms of implementation, as we only change\nthe input embedding layer for the standard ViT while keeping everything else, such as the data\naugmentation, the same as the established ViT work. We use the standard, public available training\nand testing dataset for all our experiments. We include ﬂow graph for our architecture in Fig. 1,\npseudo JAX code in Fig. 3a, and implementation details in Section 4.1 and Appendix A.11. We will\nrelease our model and code to assist in comparisons and to support other researchers in reproducing\nour experiments and results.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. arXiv preprint\narXiv:2005.00928, 2020.\nRelja Arandjelovic and Andrew Zisserman. All about VLAD. In CVPR, 2013.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh\nTenenbaum, and Boris Katz. ObjectNet: A large-scale bias-controlled dataset for pushing the\nlimits of object recognition models. In NeurIPS, 2019.\nLucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\nSrinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and An-\ndreas Veit. Understanding robustness of transformers for image classiﬁcation. arXiv preprint\narXiv:2103.14586, 2021.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets\nwithout pretraining or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021a.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. arXiv preprint arXiv:2104.02057, 2021b.\nXiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv\npreprint arXiv:2104.13840, 1(2):3, 2021.\nGabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, and C ´edric Bray. Visual\ncategorization with bags of keypoints. InIn Workshop on Statistical Learning in Computer Vision,\nECCV, pp. 1–22, 2004.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pp. 702–703, 2020.\n10\nPublished as a conference paper at ICLR 2022\nSt´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.\nConvit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint\narXiv:2103.10697, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\nwith cutout. arXiv preprint arXiv:1708.04552, 2017.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, 2021.\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and\nWieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias\nimproves accuracy and robustness. In ICLR, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In ICLR, 2019.\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning\ncan improve model robustness and uncertainty. arXiv preprint arXiv:1906.12340, 2019.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV,\n2021a.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\nexamples. CVPR, 2021b.\nKatherine Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias in\nconvolutional neural networks. In NeurIPS, 2020.\nZeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain\ngeneralization. In ECCV, 2020a.\nZeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain\ngeneralization. In ECCV, 2020b.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution. In ECCV, 2016.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nChengzhi Mao, Augustine Cha, Amogh Gupta, Hao Wang, Junfeng Yang, and Carl V ondrick. Gen-\nerative interventions for causal learning. In CVPR, 2021a.\nXiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui\nXue. Towards robust vision transformer. arXiv preprint arXiv:2105.07926, 2021b.\n11\nPublished as a conference paper at ICLR 2022\nShlok Mishra, Anshul Shah, Ankan Bansal, Jonghyun Choi, Abhinav Shrivastava, Abhishek\nSharma, and David Jacobs. Learning visual representations for transfer learning by suppress-\ning texture. arXiv preprint arXiv:2011.01901, 2020.\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz\nKhan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv preprint\narXiv:2105.10497, 2021.\nCurtis Northcutt, Lu Jiang, and Isaac Chuang. Conﬁdent learning: Estimating uncertainty in dataset\nlabels. Journal of Artiﬁcial Intelligence Research, 70:1373–1411, 2021.\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-\ning. arXiv preprint arXiv:1711.00937, 2017.\nSayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint\narXiv:2105.07581, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? In ICML, 2019.\nRulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robust-\nness of visual transformers. arXiv preprint arXiv:2103.15670, 2021.\nJ. Sivic and A. Zisserman. Video Google: a text retrieval approach to object matching in videos. In\nCVPR, 2003.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers.arXiv\npreprint arXiv:2106.10270, 2021.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nICML, 2021.\nArash Vahdat, Evgeny Andriyash, and William G Macready. Dvae#: Discrete variational autoen-\ncoders with relaxed boltzmann priors. arXiv preprint arXiv:1805.07445, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nNam V o, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing\ntext and image for image retrieval-an empirical odyssey. In CVPR, 2019.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representa-\ntions by penalizing local predictive power. In NeurIPS, 2019.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\nZhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable\nvisual representation learning via random convolutions. arXiv preprint arXiv:2007.13003, 2020.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019.\n12\nPublished as a conference paper at ICLR 2022\nAmir R. Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, and\nLeonidas J. Guibas. Robust learning through cross-task consistency. In CVPR, 2020.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-\ncal risk minimization. In ICLR, 2018.\n13\nPublished as a conference paper at ICLR 2022\nA A PPENDIX\nThe Appendix is organized as follows. Appendix A.2 extends the discussion on learning objective.\nAppendix A.1 presents more ﬁgures to visualize the attention, the decoded images, and the decoding\nfailure cases. Appendix A.9 compares designs for combing the pixel and discrete embeddings. In\nthe end, Appendix A.11 discusses the implementation details.\nImageNet\nImageNet\nRendition\nImageNet\nSketch\nStylized\nImageNet\ncat\nchair\ncar\njellyfish\nsnail\nsea \nlion\nbasset \nhound\nblack \nswan\nplatypus\nImageNet-C\njack-o'-lantern\njellyfish\njellyfish\nblack \nswan\nplatypus\ntusker\nbird\nbrambling \n(motion \nblur)\nbrambling \n(contrast)\nbrambling \n(JPEG \ncompression)\nbrambling \n(Gaussian \nnoise)\nbackpack\nband \naid\nbeer \nbottle\nbarber \nchair\nBorder \nterrier\nblack \nswan\njellyfish\ntusker\nagama\nfox \nsquirrel\njellyfish\nwalkingstick\nImageNet \nV2\nObjectNet\nImageNet-A\nFigure 7: Visualization of eight evaluation benchmarks. Each image consists of the original test\nimage (Left) and the decoded image (Right) from the ﬁnetuned discrete embeddings. The encoder\nand decoder are trained only on ImageNet 2012 data but generalize on out-of-distribution datasets.\nA.1 V ISUALIZATION\nA.1.1 R ECONSTRUCTED IMAGES FROM DISCRETE REPRESENTATION\nFig. 7 shows more examples of reconstructed images decoded from the ﬁnetuned discrete embed-\nding. Generally, the discrete representation reasonably preserves object shape but can perturb local\nsignals. Besides, Fig 7 also shows the distribution diversity in the experimented robustness bench-\nmarks, e.g., the objects in ImageNet-A are much smaller. Note that while the discrete representation\nis learned only on the ImageNet 2012 training dataset (the ﬁrst row in Fig. 7), it can generalize to\nthe other out-of-distribution test datasets.\n14\nPublished as a conference paper at ICLR 2022\ntext \n(letters \nor \ndigits)\nhuman \nfaces\nparallel \nlines \nor \ncomplex \ntextures \nsmall \nobjects \nor \nparts\nFigure 8: Visualization of failure cases for the decoded images. Each image consists of the original\ntest image (Left) and the decoded image (Right) from the ﬁnetuned discrete embeddings.\nA.1.2 L IMITATIONS OF DISCRETE REPRESENTATION\nWe ﬁnd four failure cases that the discrete representation are unable to capture: text, human faces,\nparallel lines, and small objects or parts. The example images are illustrated in Fig. 8. Without prior\nknowledge, the discrete representation has difﬁculty to reconstruct text in image. On the other hand,\nit is interesting to ﬁnd the decoder can model animal faces but not human faces. This may be a result\nof lacking facial images in the training data of ImageNet and ImageNet-21K.\nThe serious problem we found for recognition is its failure to capture small objects or parts. Though\nthis can be beneﬁcial sometimes, e.g. force the model to recognize the tire without using the “BMW”\nlogo (see the ﬁrst image in the last row of Fig. 8), it can cause problems in many cases, e.g. recog-\nnizing the small whales in the last image. As a result, our proposed model leverages the power of\nboth embeddings to promote the interaction between modeling global and local features.\nA.1.3 F ILTERS\nIn Fig. 3b of the main paper, we illustrate the visual comparison of the learned pixel embeddings\nbetween the standard ViT and our best-performing model. In this subsection, we extend the visual-\nization to our models with a varying number of pixel dimensions when concatenating the discrete\nand pixel embedding, where their classiﬁcation performances are compared in Table 13. As the\ndimension of pixel embedding grows, the ﬁlters started to pick up more high-frequency signals.\nHowever, the default setting in the paper using only 32 pixel dimensions (pixel dim=32), which\nseems to induce an inductive bias by limiting the budget of pixel representation, turns out to be the\nbest performing model.\nA.1.4 A TTENTIONS\nWe visualize the attention following the steps in (Dosovitskiy et al., 2020). To be speciﬁc, to com-\npute maps of the attention from the output token to the input space, we use Attention Rollout (Abnar\n& Zuidema, 2020). Brieﬂy, we average attention weights of ViT-B and ViT-B+Ours across all heads\nand then recursively multiply the weight matrices of all layers. Both ViT and ViT+Ours are trained\non ImageNet.\nIn Fig. 11, Fig. 12 and Fig. 13, we visualize the attention of individual images in the ﬁrst mini-batch\nof each test set. As shown, our model attends to image regions that are semantically relevant for\n15\nPublished as a conference paper at ICLR 2022\nOurs\n(pixel_dim=128)\nOurs\n(pixel_dim=64)\nOurs\n(pixel_dim=32)\nViT\n(all \npixels)\nFigure 9: Comparing visualized pixel embeddings of the ViT and our model. The top row shows\nthe randomly selected ﬁlters and the Bottom shows the ﬁrst 28 principal components. Our models\nwith varying pixel dimensions are shown, where their classiﬁcation performances are compared in\nTable 13. Ours (pixel dim=32) works the best and is used as the default model in the main paper.\nclassiﬁcation and captures more global contexts relative to the central object. This can be better\nseen from Fig. 10, where the heat map averages the attention values of all images in the ﬁrst mini-\nbatch of each test set. As found in prior works (Dosovitskiy et al., 2020; Naseer et al., 2021), ViTs\nput much attentions on the corners of the image, our attentions are more global and do not exhibit\nthe same defect.\n(a) ViT (ImageNet)\n (b) ViT (ImageNet-R)\n (c) ViT (Stylized)\n (d) ViT (ObjectNet)\n(e) Ours (ImageNet)\n (f) Ours (ImageNet-R)\n (g) Ours (Stylized)\n (h) Ours (ObjectNet)\nFigure 10: Comparison of average attention of the ViT (top row) and the proposed model (bottom\nrow) on four validation datasets: ImageNet 2012, ImageNet-R, Stylized-ImageNet, and ObjectNet.\nThe heat map averages the attention values of all images in the ﬁrst mini-batch of each test set. The\nresults show our attention capture more global context relative to the central object.\nA.2 D ETAILS ON LEARNING OBJECTIVE\nLet x denote an image with the class label y, and z represent the discrete tokens z ∈ Z. For\nnotational convenience, we use a single random variable to represent the discrete latent variables\nwhereas our implementation actually represents an image as a ﬂattened 1-D sequence following the\nraster scan.\n• qφ(z|x) denotes the VQ-V AE encoder, parameterized by φ, to represent an image xinto\nthe discrete token z.\n16\nPublished as a conference paper at ICLR 2022\n• pθ(x|z) is the decoder that models the distribution over the RGB image generated from\ndiscrete tokens.\n• pψ(y|x,z) stands for the vision transformer model shown in Fig. 1 of the main paper.\nWe model the joint distribution of image x, label y, and the discrete token z using the factoriza-\ntion p(x,y,z ) = pθ(x|z)pψ(y|x,z)p(z). Our overall procedure can be viewed as maximizing the\nevidence lower bound (ELBO) on the joint likelihood, which yields:\nDKL[qφ(z|x),p(z|x,y))] =−\n∑\nz\nqφ(z|x) logp(z|x,y)\nqφ(z|x) (9)\n= −\n∑\nz\nqφ(z|x) log p(x,y,z )\np(x,y)qφ(z|x) (10)\nSince the f-divergence is non-negative, we have:\n−\n∑\nz\nqφ(z|x) logp(x,y,z )\nqφ(z|x) + logp(x,y)[\n∑\nz\nqφ(z|x)] ≥0 (11)\nlog p(x,y) ≥\n∑\nz\nqφ(z|x) logp(x,y,z )\nqφ(z|x) (12)\nUsing the factorization, we have:\nlog p(x,y) ≥Eqφ(z|x)[log pψ(y|x,z)pθ(x|z)p(z)\nqφ(z|x) ] (13)\nEquation 13 yields the ELBO:\nlog p(x,y) ≥Eqφ(z|x)[log pθ(x|z)] +Eqφ(z|x)[log pψ(y|x,z)p(z)\nqφ(z|x) ] (14)\n≥Eqφ(z|x)[log pθ(x|z)] −DKL[qφ(z|x)∥pψ(y|x,z)p(z)] (15)\nIn the ﬁrst stage, we maximize the ELBO with respect to φand θ, which corresponds to learning\nthe VQ-V AE encoder and decoder. We assume a uniform prior for bothpψ(y|x,z) and p(z). Given\nthat qφ(z|x) predicts a one-hot output, the regularization term (KL divergence) will be a constant\nlog K, where K is the size of the codebook V. As in the VQ-V AE model, the distribution pθ is\ndeterministic. We note a similar assumption as in the DALL-E model (Ramesh et al., 2021) is used\nto stabilize the training, which means the transformer is not learned at this stage. We have the same\nobservation as in DALL-E that this strategy works better than jointly training with pψ(y|x,z).\nIn addition to the reconstruction loss, the VQ-V AE’s training objective adds a dictionary learning\nloss and a commitment loss, calculated from:\nLVQ-V AE= logp(x|zq(x)) +∥sg[ze(x)] −v∥2 + β∥ze(x) −sg[v]∥, (16)\nwhere ze(x) and zq(x) represent the encoder output and the decoder input, respectively. v de-\nnotes the discrete embedding for the image x. sg (·) is the stop gradient function. The training\nused straight-through estimation which just copies gradients from zq(x) to ze(x). Notice that our\nimplementation uses VQ-GAN (Esser et al., 2021) which adds an additional GAN loss:\nLVQ-GAN = logp(x|zq(x)) +∥sg[ze(x)] −v∥2 + β∥ze(x) −sg[v]∥+ λLGAN, (17)\nwhere β = 0.25 and λ= 0.1 are used to balance different loss terms. LVQ-GAN also has a perceptual\nloss (Johnson et al., 2016).\nIn the second stage of ﬁnetuning, we optimize ψ while holding φand θ ﬁxed, which corresponds\nto learning a vision transformer for classiﬁcation and ﬁnetuning the discrete embedding V. By\nrearranging Equation 14, we have:\nlog p(x,y) ≥Eqφ(z|x)[log pψ(y|x,z)] +Eqφ(z|x)[log pθ(x|z)p(z)\nqφ(z|x) ], (18)\n17\nPublished as a conference paper at ICLR 2022\nwhere pψ(y|x,z) is the proposed ViT model as illustrated in Fig. 1 of the main paper. The ﬁrst term\ndenotes the likelihood for classiﬁcation that is learned by minimizing the multi-class cross-entropy\nloss. The second term becomes a constant given the ﬁxed θand φ.\nIn the ﬁnetuning stage, two sets of parameters are updated: the vision transformer ψis learned from\nscratch and the discrete embedding V is ﬁnetuned. We ﬁx the encoder and stop the gradient prop-\nagating back to the encoder considering the instability of straight-through estimation and the stop\ngradient operation in the dictionary learning loss, i.e. Equation 16. Empirically, we also compared\nthe proposed pretraining-ﬁnetuning strategy with the joint learning strategy in which all parameters\nare optimized simultaneously. We ﬁnd the joint learning signiﬁcantly underperforms the proposed\nlearning strategy.\nA.3 C OMPARING TO DATA AUGMENTATIONS\nSince our model only changes the token representation in the ViT architecture, it is conceptually\ndifferent and complementary to data augmentation. Nevertheless, this subsection compares our\nmethod with seven combinations of recent data augmentation strategies, including CutOut (DeVries\n& Taylor, 2017), CutMix (Yun et al., 2019), RandAug (Cubuk et al., 2020), and Mixup (Zhang et al.,\n2018). As discussed following (Steiner et al., 2021), the ViT baseline and our model both use the\ndefault augmentation (RandAug + Mixup).\nAs shown in Table 7, our method achieves the best performance on the robustness benchmarks\nwhile is marginally worse than the best baseline with state-of-the-art data augmentation strategy\n(CutMix + RandAug) on the ImageNet in-distribution validation set. Incorporating state-of-the-art\ndata augmentation in our model is worthy of future research.\nTable 7: Model performance trained on ImageNet. We compared with several recent data augmentations,\nincluding CutOut (DeVries & Taylor, 2017) and CutMix Yun et al. (2019), on the ViT-B model. All columns\nindicate the top-1 classiﬁcation accuracy except the last column ImageNet-C which indicates the mean Cor-\nruption Error (lower the better). The bold number indicates higher accuracy than the corresponding baseline.\nThe box highlights the best accuracy. While the state-of-the-art data augmentation (CutMix+RandAug) can\nimprove Imagenet in-distribution accuracy, they are worse than ours on the OOD test accuracy.\nOut of Distribution Robustness Test\nModel ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nViT-B No Augmentation 72.47 78.69 24.56 5.94 14.34 13.44 59.32 5.23 88.72\n+ Ours (discrete only) 73.73 79.63 34.61 8.94 23.66 20.84 60.30 6.45 74.82\nViT-B + CutOut 72.27 77.91 25.74 7.89 16.50 17.82 58.05 10.23 69.01\nViT-B + CutMix 75.51 80.53 28.45 7.89 17.15 21.62 62.36 14.72 64.07\nViT-B + CutOut + Mixup 71.31 77.07 25.07 6.17 15.55 16.28 56.84 8.33 76.44\nViT-B + CutMix + Mixup 71.66 76.82 23.46 4.92 13.91 16.99 56.98 10.23 80.54\nViT-B + CutOut + RandAug 79.07 84.64 38.10 12.34 26.92 28.32 66.88 15.77 54.52\nViT-B + CutMix + RandAug 79.63 85.24 38.34 10.94 27.46 29.80 68.05 18.80 51.63\nViT-B + RandAug + Mixup 78.73 84.85 38.15 10.39 28.60 28.71 67.34 16.92 53.51\nOur ViT-B + RandAug + Mixup 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nA.4 T HE IMPORTANCE OF DISCRETE INFORMATION\nThis subsection compares to a new baseline that directly concatenates the global feature extracted\nby CNN to the input token of ViT. The only difference between this baseline and our model is that\nours concatenates discrete embeddings rather than global CNN features. In Table 8, we extensively\nsearch the concatenation dimension of the CNN features in [64, 128, 256, 384, 512, 640]. How-\never, none of the models improve robustness. Our results show that concatenating the global CNN\nfeature performs no better than the ViT baseline and signiﬁcantly worse than concatenating discrete\nrepresentations, demonstrating the necessity of discrete representations for robustness.\nA.5 T EXTURE VS . S HAPE STUDY\nIn Fig. 5a of the main paper, we performs the “shape vs. texture biases” analysis following (Geirhos\net al., 2019) (see Figure 4 in their paper). It uses the score “fraction of shape decision” to quantify a\n18\nPublished as a conference paper at ICLR 2022\nTable 8: We replace our discrete token representation with global continuous (GC) features from the CNN\nmodel that has the same CNN architecture as our VQGAN encoder. We denote the dimension for the global\nfeatures as GF-Dim. We vary the dimension of the pixel token to concatenate with the global continuous CNN\nfeatures. Simply concatenating the global feature extracted from CNN does not improve robustness.\nGF- Out of Distribution Robustness Test\nModel Dim ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nGC CNN + ViT 640 78.48 83.27 34.75 9.38 24.85 28.37 65.65 16.13 58.71\nGC CNN + ViT 512 78.59 83.24 34.31 10.23 25.50 27.50 65.35 16.48 57.66\nGC CNN + ViT 384 78.51 83.44 34.66 10.47 25.14 28.35 65.86 15.97 57.64\nGC CNN + ViT 256 78.44 83.09 34.96 9.61 25.20 27.66 65.95 15.96 58.35\nGC CNN + ViT 128 78.29 82.96 34.32 9.22 24.90 26.91 65.28 15.73 58.71\nGC CNN + ViT 64 78.34 82.98 34.35 9.19 25.06 27.07 65.48 15.40 58.58\nViT 0 78.73 84.85 38.15 10.39 28.60 28.71 67.34 16.92 53.51\nOurs (discrete only) - 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\nOurs - 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nmodel’s shape-bias between 0 and 1. To compute the score, we follow the steps on their Github 1as\nfollows: 1) evaluate the model on all 1280 images in Stylized ImageNet; 2) map the model decision\nto 16 classes; 3) exclude images without a cue conﬂict; 4) take the subset of “correctly” classi-\nﬁed images (either shape or texture category correctly predicted); (5) compute “shape bias” as the\nfraction between correct shape decisions and (correct shape decisions + correct texture decisions).\nFig. 5a in the main paper presents the scores on 16 categories along their average denoted by the\ncolored horizontal line. We compute the scores for three models trained on ImageNet, i.e. ResNet-\n50, ViT-B/16, and ours, and quote human scores from (Geirhos et al., 2019). We also compute the\nscore for the concatenating global CNN features baseline which is about 0.3. Note that the OOD\ngeneralization setting “IN-SIN” (Geirhos et al., 2019) is used, where the model is trained only on\nthe ImageNet (IN) and tested on the Stylized ImageNet (SIN) images with conﬂicting shape and\ntexture cue.\nA.6 T HE EFFECT OF CODEBOOK ENCODER CAPACITY ON ROBUSTNESS\nTable 9: The impact of codebook encoder capacity for robustness.\nCodeBook Out of Distribution Robustness Test\nSize ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nSmall VQ Encoder 76.63 77.69 45.92 21.95 35.02 26.03 64.18 10.68 58.64\nVQ Encoder 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\nWe analyze how the capacity of the discrete encoder affects the model’s robustness. We train a small\nencoder and a standard encoder and compare their performance. We describe the VQ encoder’s\narchitecture conﬁguration in Table 17, where the small encoder has only 13% of the #parameters of\nthe standard VQ encoder. We run experiments in Table 9. Our results show that using the standard\nVQ-encoder is better but the small VQ-encoder comparatively yields reasonable results considering\nthe reduced model capacity.\nA.7 T HE EFFECT OF PRETRAINING CODEBOOK WITH MORE DATA ON ROBUSNTESS\nTable 10: The impact of pretraining codebook on sufﬁciently large data for robustness. The code-\nbook is pretrained on the standard ImageNet ILSVRC 2012 or the ImageNet21K dataset (about 11x\nlarger). Using the codebook, we train ViT models on ImageNet21K.\nCodebook Out of Distribution Robustness Test\nPretrained Data ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nImageNet 83.32 88.36 51.56 14.77 41.10 42.50 72.57 33.51 55.31\nImageNet21K 83.40 88.44 55.26 19.69 44.72 43.92 72.68 36.64 45.86\nWe analyze the effect of pre-training codebook on sufﬁciently large data. We use the same model\narchitecture, but pretrain two codebooks (and encoders) on the ImageNet ILSVRC 2012 and the\n1https://github.com/rgeirhos/texture-vs-shape\n19\nPublished as a conference paper at ICLR 2022\nImageNet21K (about 11x larger), respectively. Using the pre-trained codebook, we train ViTs on\nImageNet21K and compare the results in Table 10, where it shows pretraining on large data improves\nthe robustness.\nA.8 A DDITIONAL MODELS FOR LEARNING DISCRETE REPRESENTATION\nThe results show that the ability of discrete representations to improve ViT’s robustness is general\nwhich is not limited to speciﬁc VQ-GAN models.\nTable 11: Model architecture for discrete representation. We bold the numbers if they improve\nrobustness over the baseline ViT model. The numbers are boxed where VQ-V AE further improves\nrobustness than VQ-GAN under apple-to-apple comparison.\nCodebook Out of Distribution Robustness Test\nPretrained Model ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nBaseline (No discrete) 78.73 84.85 38.15 10.39 28.60 28.71 67.34 16.92 53.51\nVQ-V AE (Discrete Only) 78.36 84.35 46.22 23.36 35.17 29.34 66.24 13.61 52.20\nVQ-GAN (Discrete Only) 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\nVQ-V AE 78.51 83.68 41.54 17.50 30.91 27.43 65.74 15.61 50.47\nVQ-GAN 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nA.9 C OMPARING DESIGNS FOR COMBINING EMBEDDINGS\nIn this subsection, we compare different designs to combine the pixel and discrete embeddings. See\nthe combination operation in Fig. 1 and in Equation 6 of the main paper. The results are used verify\nour design of using a simple concatenation presented in the paper. Four designs are considered and\ndiscussed below. Their performances are compared in Table 12.\nTable 12: The accuracy using different combination methods. While directly adding pixel token\nto discrete token improves the most ImageNet and ImageNet-A accuracy, concatenation method\nimproves the overall robustness.\nCombining Out of Distribution Robustness Test\nMethod ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nAddition 79.67 84.89 40.68 13.59 30.89 29.18 67.19 17.68 50.85\nConcatenation 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nResidual Gating 74.59 79.66 41.06 17.03 29.56 25.34 62.37 9.33 59.88\nCross-Attention 79.18 84.53 43.75 18.67 34.19 29.71 66.73 15.81 50.06\nAddition. We ﬁrst use linear projection matrixE to obtain the pixel embedding that shares the same\ndimension as the discrete embedding which is 256. We then add the two embedding, and project the\nresulting embedding to the dimension required by the ViT if needed. Formally, we compute\nf(Vzd,xpE) = (Vzd + xpE)E2, (19)\nwhere Vzd and xpE represent the discrete and pixel embeddings, respectively, andE2 is a new MLP\nprojection layer that is needed when the resulting dimension is different to the ViT input.\nConcatenation. We concatenate the discrete embedding with the pixel embedding, and then feed\nthe resulting embedding into the transformer encoder. By default, we use a dimension of 32 for the\npixel embedding, a dimension of 256 for the discrete embedding, thus we pad 0 to the vector if the\ninput dimension of the ViT is higher than 288.\nf(Vzd,xpE) = [Vzd; xpE; 0], (20)\nwhere “;” indicates the concatenation operation.\nResidual Gating. As pixel embeddings may contain nuisances, inspired by (V o et al., 2019), we\nlearn a gate from both embeddings, and then multiply it with the pixel embeddings to ﬁlter out\ndetails that are unimportant to classiﬁcation. Speciﬁcally, we calculate the gate by a 2-layer MLP,\nand apply a softmax to the output from the last layer.\n20\nPublished as a conference paper at ICLR 2022\nG = Softmax(MLP(Vzd; xpE)) (21)\nThen the pixel embeddings are gated by:\nP = G ⊙xpE (22)\nThen we concatenate the gated embedding with the discrete embedding:\nf(Vzd,xpE) = [Vzd; P; 0] (23)\nCross-Attention. We use the discrete embedding as the query, and the pixel token as the key and\nvalue in a standard multi-head cross-attention module (MCA).\nA = MCA(Vzd,xpE,xpE) (24)\nWe then concatenate the attended feature output with the discrete embedding. We also pad 0 if the\nViT requires larger input dimensionality.\nf(Vzd,xpE) = [Vzd; A; 0] (25)\nA.9.1 T HE ROBUSTNESS EFFECT OF USING CONTINUOUS REPRESENTATION WITH\nDISCRETE REPRESENTTION .\nThe above study shows that concatenation is the most effective way to combine the discrete and\ncontinuous representations. As we ﬁxed the dimensionality for the discrete token to be 256, we can\nstudy the optimal ratio between continuous and discrete representations by changing the dimension-\nality on the pixel token.\nTable 13: The robust performance under different pixel token dimension in concatenation combin-\ning method. By increasing the dimension of the pixel embeddings, the model uses more continuous\nrepresentations than discrete representations. There is an inherent trade off between the out of dis-\ntribution robustness performance. The more continuous representations the model use, the lower\nrobustness on the out-of-distribution set that depicting object shape, but also aciheves higher ac-\ncuracy on the in-distribution ImageNet and out of distribution variants ImageNet-A that requires\ndetailed information. However, using only continuous embeddings also hurt robustness. Thus we\nchoice to use dimension 32, which balance the trade-off and achieves the best overall robustness.\nPixel Token Out of Distribution Robustness Test\nDimension ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\n0 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\n8 79.05 84.50 46.08 21.17 34.67 30.20 66.71 15.15 48.42\n16 79.45 84.84 46.00 20.47 34.88 29.42 67.09 15.81 46.91\n32 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\n64 79.71 84.98 43.97 18.75 33.89 29.91 67.98 17.47 46.10\n128 80.04 85.07 41.96 15.47 31.65 30.05 68.03 17.80 49.58\nAll Pixel 78.73 84.85 38.15 10.39 28.60 28.71 67.34 16.92 53.51\nA.10 T HE IMPORTANCE OF SPATIAL INFORMATION FOR DIFFERNT ARCHITECTURES\nIn addition to the analysis in the main paper, we also investigate whether it is the convolutional\noperation in the vector quantized encoder that makes the model use the spatial information better.\nWe remove the position embedding from the Hybrid-B model, whose input is also produced by a\nconvolution encoder. Our results show that removing the positional information from the Hybrid\nmodel with a convolutional encoder does not decrease the performance. However, for both our\nmodels, the discrete only and the combined one, removing positional embedding causes a large drop\nin performance. This shows that our robustness gain via the spatial structure is from our discrete\ndesign, not convolution.\n21\nPublished as a conference paper at ICLR 2022\nTable 14: Contribution of position embedding for robust recognition as measured by the relative performance\ndrop when the position embedding is removed. We experiment on ViT, Ours, Hybrid-ViT, and Ours with\ndiscrete token only. Note that Hybrid uses the continuous, global feature from a ResNet-50 CNN as the input\ntoken. A larger relative drop indicates the model relies on spatial information more. Adding discrete token\ninput representation drives position information and spatial structure more crucial in our model.\nOut of Distribution Robustness Test\nModel ImageNet Real Rendition Stylized Sketch ObjectNet V2 A C ↓\nViT-B 78.73 84.85 38.15 10.39 28.6 28.71 67.34 16.92 53.51\n-w/o. PosEmb 76.51 77.01 28.25 5.86 15.17 24.02 63.22 13.13 69.99\nRelative drop (%) 2.8% 9.2% 26.0% 43.6% 47.0% 16.3% 6.1% 22.4% 30.8%\nHybrid-ViT-B 74.94 80.54 33.03 7.5 25.33 23.08 61.30 7.44 69.61\nHybrid-ViT-B- w/o. PosEmb 75.13 80.66 33.11 7.5 25.15 22.78 61.75 7.60 68.51\nrelative drop (%) -0.3 -0.1 -0.2 0 0.7 3.0 0.7 2.1 1.6\nOurs (discrete only) 78.67 84.28 48.82 22.19 39.10 30.27 66.52 14.77 55.21\nOurs (discrete only) w/o. PosEmb 16.34 5.09 1.95 2.09 1.20 1.75 11.75 1.29 124\nRelative Drop (%) 79.2 94.1 96.0 90.5 96.9 94.2 82.3 91.3 124.6\nOurs 79.48 84.86 44.77 19.38 34.59 30.55 68.05 17.20 46.22\nOurs- w/o. PosEmb 56.27 59.06 17.24 4.14 6.57 11.36 42.98 3.51 89.76\nRelative drop (%) 29.2% 30.4% 61.5% 78.6% 81.0% 62.8% 36.8% 79.6% 94.2%\nTable 15: Model conﬁguration for the Transformer Encoder.\nModel Layers Hidden size MLP size Heads #Params\nTiny (Ti) 12 192 768 3 5.8M\nSmall (S) 12 384 1546 6 22.2M\nBase (B) 12 768 3072 12 86M\nLarge (L) 24 1024 4096 16 307M\nA.11 I MPLEMENTATION DETAILS\nA.11.1 A RCHITECTURE\nWe use three variants of ViT transformer encoder backbone in our experiments. We show the conﬁg-\nuration details and the number of parameters in Table 15. We also use hybrid model with ResNet50\nas the tokenization backbone. We show the conﬁguration in Table 16.\nWe use B-16 for the MLPMixer Tolstikhin et al. (2021).\nWe use two kinds of VQ Encoder in our experiment: Small VQ Encoder and VQ Encoder. The VQ\nencoder uses the same encoder architecture as the VQGAN encoder (Esser et al., 2021). For Small\nVQ Encoder, we decrease both the number of the resisual blocks and the number for ﬁlter channel\nby half, which is a lightweight model that decrease the model size by around 8 times. We show the\nconﬁguration detail in Table 17.\nA.12 T RAINING\nImplementation details We implement our model in Jax and optimize all models with\nAdam (Kingma & Ba, 2014). Unless speciﬁed otherwise, the input images are resized to 224x224,\ntrained with a batch size of 4,096, with a weight decay of 0.1. We use a linear learning rate warm-\nup and cosine decay. On ImageNet, the models are trained for 300 epochs using three ViT model\nvariants, i.e. Ti, S, and B, from small to large. The VQ-GAN model is also trained on ImageNet for\n100K steps using a batch size of 256. The codebook size is K = 1024, and the codebook embeds\nthe discrete token intodc = 256dimensions. For discrete ViT-B, we use average pooling on the ﬁnal\nfeatures. On ImageNet-21K, we train 90 epochs. We ﬁnetune on a higher resolution on ImageNet,\nwith Adam and 1e-4 learning rate, batch size 512, for 20K iterations. The quantizer model is a VQ-\nGAN and is trained on ImageNet-21K only with codebook size K = 8,192. All models including\nours use the same augmentation (RandAug and Mixup) as in the ViT baseline (Steiner et al., 2021).\nMore details can be found in the Appendix.\nImageNet Training We train our model with the Adam (Kingma & Ba, 2014) optimizer. Our batch-\nsize is 4096 and trained on 64 TPU cores. We start from a learning rate of 0.001 and train 300 epoch.\n22\nPublished as a conference paper at ICLR 2022\nTable 16: Model conﬁguration for ResNet+ViT hybrid models.\nModel Resblocks Patch-size #Params\nHybrid-S [3,4,6,3] 1 46.1\nHybrid-B [3,4,6,3] 1 111\nTable 17: Model conﬁguration for the VQ encoders.\nModel Resblocks Filter Channel Embedding dimension #Params\nSmall VQ Encoder [1,1,1,1,1] 64 256 3.0M\nVQ Encoder [2,2,2,2,2] 128 256 23.7M\nWe use linear warm up for 10k iterations and then a cosine annealing for the learning rate schedule.\nWe use 224 ×224 resolution for the input image. We conduct experiment on three variants of the\nViT model, Ti, S, and B, from small to large. In addition, we also experiment on the newly released\nMLPMixer, which also uses image patch as the token embeddings. We denote our approach as us-\ning both representations, and we also run a discrete only variant where we only uses the discrete\nembeddings without pixel embeddings. Due to the small input dimension of Ti, we only use the\ncode representation without concatenate the pixel representation, and we project the 256 dimension\nto 192 via a linear projection layer. For the other variants, we concatenate both representations, and\npad zeros if additional dimension required for the transformer. We use the same augmentation and\nmodel regularization as (Steiner et al., 2021), where we use RandAug with hyper-parameter (2,15)\nand mixup with α= 0.5, and we apply a dropout rate of 0.1 and stochastic block dropout of 0.1. We\ndownload pretrained VQGAN model, which is trained on only ImageNet. We use VQ-GAN’s en-\ncoder only with a codebook size of 1,024. The codebook integer is embedded into a 256 dimension\nvector. For the ViT-B discrete, we ﬁnd training 800 epoch instead of 300 epoch can give another\n0.1-1% gain over the test set, while training others for longer results in decreased performance.\nImageNet-21K Training Our models use learning rate of 0.001, weight decay of 0.03, and train 90\nepoch. We use linear warm up for 10k iterations and then a cosine annealing for the learning rate\nschedule. We use224×224 resolution for the input image. We use the same augmentation as (Steiner\net al., 2021). For the quantizer model, we use VQGAN that is trained on unlabeled ImageNet-21K\nonly. We use a codebook size of 8,192, and the codebook integer is embedded into a 256 dimension\nvector. To evaluate on the standard test set, we further ﬁnetune the pretrained ImageNet-21K model\non ImageNet. We use Adam and optimize for 20k with 500 steps of warm up. We use a learning\nrate of 0.0001. We use a larger resolution 384 ×384 and 512 ×512. We only experiment the ViT-B\nvariant.\nFor training VQ Encoder and Decoder, we train with batchsize 256 until it converges. We use\nperceptual loss with weight of 0.1, adversarial loss with weight 0.1. We use L1 gradient penalty of\n10 during the optimization. The model is trained with resolution of 256 ×256. The model can scale\nto different image resolution without ﬁnetuning.\n23\nPublished as a conference paper at ICLR 2022\n(a) ViT-B/16\n(b) Ours ViT-B/16\nFigure 11: Attention comparison of the ViT and the proposed model on ImageNet 2012.\n24\nPublished as a conference paper at ICLR 2022\n(a) ViT-B/16\n(b) Ours ViT-B/16\nFigure 12: Attention comparison of the ViT and the proposed model on ImageNet-R.\n25\nPublished as a conference paper at ICLR 2022\n(a) ViT-B/16 (Sketch)\n (b) Ours ViT-B/16 (Sketch)\n(c) ViT-B/16 (ObjectNet)\n (d) Ours ViT-B/16 (ObjectNet)\nFigure 13: Attention comparison of the VIT and the proposed model on ImageNet Sketch and\nObjectNet.\n26",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.7269952893257141
    },
    {
      "name": "Encoder",
      "score": 0.6758325099945068
    },
    {
      "name": "Computer science",
      "score": 0.6047347784042358
    },
    {
      "name": "Architecture",
      "score": 0.5734847784042358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5498157739639282
    },
    {
      "name": "Transformer",
      "score": 0.5329537987709045
    },
    {
      "name": "Convolutional neural network",
      "score": 0.45414894819259644
    },
    {
      "name": "Invariant (physics)",
      "score": 0.4229482114315033
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3947378993034363
    },
    {
      "name": "Machine learning",
      "score": 0.32406991720199585
    },
    {
      "name": "Mathematics",
      "score": 0.19418489933013916
    },
    {
      "name": "Engineering",
      "score": 0.1281413733959198
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 10
}