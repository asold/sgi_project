{
    "title": "QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation",
    "url": "https://openalex.org/W4385573586",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2111157491",
            "name": "Krishna A Srinivasan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2127877947",
            "name": "Karthik Raman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2145923783",
            "name": "Anupam Samanta",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121252590",
            "name": "Lingrui Liao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1981082629",
            "name": "Luca Bertelli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112702096",
            "name": "Michael Bendersky",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W1979182316",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2010463775",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2099871636",
        "https://openalex.org/W2033112710",
        "https://openalex.org/W2123198781",
        "https://openalex.org/W2997006708",
        "https://openalex.org/W1997542518",
        "https://openalex.org/W4225369941",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W4384652670",
        "https://openalex.org/W3107925315",
        "https://openalex.org/W4284967471",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2951292886",
        "https://openalex.org/W2937036051",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W3093655911",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W3017018726",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W2098876286",
        "https://openalex.org/W3094554104",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W1602389404"
    ],
    "abstract": "Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli, Michael Bendersky. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2022.",
    "full_text": "Proceedings of EMNLP 2022 Industry Track, pages 502–511\nDecember 9–11, 2020. ©2022 Association for Computational Linguistics\n502\nQUILL: Query Intent with Large Language Models using Retrieval\nAugmentation and Multi-stage Distillation\nKrishna Srinivasan∗\nGoogle Research\nkrishnaps@google.com\nKarthik Raman∗\nGoogle Research\nkarthikraman@google.com\nAnupam Samanta\nGoogle\nanupamsamanta@google.com\nLingrui Liao\nGoogle\nlingrui@google.com\nLuca Bertelli\nGoogle\nlbertelli@google.com\nMike Bendersky\nGoogle Research\nbemike@google.com\nAbstract\nLarge Language Models (LLMs) have shown\nimpressive results on a variety of text under-\nstanding tasks. Search queries though pose\na unique challenge, given their short-length\nand lack of nuance or context. Complicated\nfeature engineering efforts do not always lead\nto downstream improvements as their perfor-\nmance beneﬁts may be offset by increased\ncomplexity of knowledge distillation. Thus,\nin this paper we make the following contribu-\ntions: (1) We demonstrate that Retrieval Aug-\nmentation of queries provides LLMs with valu-\nable additional context enabling improved un-\nderstanding. While Retrieval Augmentation\ntypically increases latency of LMs (thus hurt-\ning distillation efﬁcacy), (2) we provide a prac-\ntical and effective way of distilling Retrieval\nAugmentation LLMs. Speciﬁcally, we use a\nnovel two-stage distillation approach that al-\nlows us to carry over the gains of retrieval\naugmentation, without suffering the increased\ncompute typically associated with it. (3) We\ndemonstrate the beneﬁts of the proposed ap-\nproach (QUILL) on a billion-scale, real-world\nquery understanding system resulting in huge\ngains. Via extensive experiments, including\non public benchmarks, we believe this work\noffers a recipe for practical use of retrieval-\naugmented query understanding.\n1 Introduction\nThe recent advent of billion+ parameter Large Lan-\nguage Models (LLMs) – such as T5 (Raffel et al.,\n2019), mT5 (Xue et al., 2021), GPT-3 (Brown et al.,\n2020) and most recently PaLM (Chowdhery et al.,\n2022) – has disrupted many language understand-\ning tasks – with new benchmarks set or eclipsed\nroutinely by these Transformer models and their\nvariants.\nQueries – especially keyword search ones –\npresent a unique challenge though. Their short\n∗ Corresponding Authors\nlength, inherent ambiguity and lack of grammar\nmean query understanding tasks typically require\nmore memorization and world knowledge than\nother NLP tasks (Broder et al., 2007). Conse-\nquently, despite LLMs leading performance on lan-\nguage and query understanding tasks – like intent\nclassiﬁcation, query parsing and relevance predic-\ntion – there is signiﬁcant room for further improve-\nment.\nIn this paper we leverage Retrieval-\nAugmentation to provide LLMs more context\nand grounding for search queries. We show that\nthe titles and URLs of documents retrieved for\nthe query, greatly help improve LLMs query\nunderstanding capabilities. While different\nretrieval augmentation models exist, we show that\neven simple concatenation of these titles / urls with\nthe query can help improve LLM performance\nconsiderably.\nHowever, the use of retrieval augmentation leads\nto a new challenge: Increased complexity of LLM\ninference. More speciﬁcally, the quadratic com-\nplexity of self-attention in Transformer models\nmeans that the latency of LLMs blows up given\nthese (often 10x+) longer input sequences. This\npresents a signiﬁcant problem as LLMs are imprac-\ntical for online use and thus need to be distilled\ninto smaller, more efﬁcient models to be served\nonline. However knowledge distillation (Gou et al.,\n2021) into these student models requires a lot of\ndistillation data annotated by these LLMs – which\nmay not be feasible for these retrieval augmented\nmodels.\nThus as a remedy we introduce a new two-stage\ndistillation approach. In the ﬁrst stage of this ap-\nproach we distill the retrieval-augmented (long in-\nput) LLM (the Professor) into a non-retrieval aug-\nmented (short input) LLM (the Teacher) using a\nsmall distillation set. This second LLM Teacher is\nin turn distilled into the ﬁnal Student using a large\nset.\n503\nVia extensive experiments on a large-scale, real-\nworld problem and data we demonstrate that the re-\nsulting QUILL system provides for an efﬁcient and\neffective way of retaining the performance gains of\nretrieval augmented LLMs on query understanding\ntasks.\n2 Related Work\nLarge language models (LLMs) such as mT5 (Xue\net al., 2021) demonstrated signiﬁcant performance\nimprovements on a variety of natural language un-\nderstanding (NLU) tasks. Speciﬁcally in the con-\ntext of query understanding, researchers found that\n(a) model size signiﬁcantly effects the quality of\nthe resulting models (Nogueira et al., 2019; Han\net al., 2020), and (b) using additional context in the\nform of query-associated documents is crucial to\nthe model performance due to the paucity of con-\ntext available in the query itself (Nogueira and Lin,\n2019; Zhang et al., 2020). Retrieval augmentation\nof the query with the search results retrieved by\nit is a proven way to incorporate such context in\nLLM training for NLU tasks, as has been shown re-\ncently by models such as RAG (Lewis et al., 2020),\nREALM (Guu et al., 2020), and RETRO (Borgeaud\net al., 2022).\nIn this paper, we leverage this insight to improve\nperformance of query intent prediction (Broder,\n2002) — a crucial query understanding task that\nis at the heart of modern search engines – using\nLLMs. Prior work by Broder et al. (2007) found\nimportance of retrieval augmentation using statisti-\ncal methods for this task. Statistical retrieval aug-\nmentation has also been found critical for other\nquery understanding tasks including query expan-\nsion (Broder et al., 2008; Diaz and Metzler, 2006)\nand query tagging (Wang, 2020). We demonstrate\nsimilar beneﬁts when using retrieval-augmented\nLLMs as well.\nWe also leverage Knowledge Distillation (Hin-\nton et al., 2015; Mirzadeh et al., 2020; Gou et al.,\n2021) techniques to create a Student model that\nretains the LLMs gains.\n3 Query Intent Understanding\nWhile the techniques described in this paper could\nbe applied to any query understanding task, for\nthe sake of brevity we focus on the task of query\nintent classiﬁcation. Query intent (QI) classiﬁca-\ntion is a classical IR task studied for over two\ndecades (Kang and Kim, 2003; Baeza-Yates et al.,\nData Train Val Test Unlabeled\nOrcas-I 1.28M 1K 1K 10.3M\nEComm 36K 4K 4K 128M\nTable 1: Statistics of datasets used.\n2006; Jansen et al., 2008; Kathuria et al., 2010;\nLewandowski et al., 2012; Figueroa, 2015; Mohas-\nseb et al., 2019). This task is particularly important\nin practice, as it is at the top of the search funnel,\nand the entire search engine behavior may vary\nbased on the predicted query intent. Given the cen-\ntrality of this task on overall retrieval, models for\nthis task need to be both fast (i.e., low latency) and\nhigh efﬁcacy. Thus even a single percentage point\nquality gain on the QI task can be considered a\nmajor accomplishment.\nIn this paper we tackle the QI task using LLMs.\nIn particular we use two datasets in our study whose\ndetails are provided in Table 1:\n• EComm: Our main dataset will be a real-\nworld dataset. Cast as a binary classiﬁcation\nproblem, this task involves identifying queries\nwith a speciﬁc intent – where the required\nintent is similar to the transactional intent of\nthe Broder taxonomy (Broder, 2002) in the\ncontext of e-commerce. As common in real-\nworld applications, the human labeled data is\naccompanied by a large unlabeled set – that is\nused for knowledge distillation.\n• Orcas-I: The largest publicly available query\nintent dataset is ORCAS-I (Alexander et al.,\n2022). This comprises queries of the ORCAS\ndataset (Craswell et al., 2020) labeled with\none of 5 intent classes. Note that while the\ntest set is human-labeled, the training set la-\nbels are weak labels as detailed in ORCAS-I\n(Alexander et al., 2022) paper’s Methodology\nsection.\n4 QUILL Methodology\nThe keyword nature of queries and lack of context\nmake the QI task (like other query understanding\ntasks) challenging for LLMs. Thus we propose\nQUILL as a solution. As seen in Figure 1, QUILL\nconsists of two stages: (a) Retrieval Augmented\nLLM training, (b) Multi-Stage Distillation into efﬁ-\ncient student.\nRetrieval Augmented (RA) LLM:The key in-\nsight here is that titles / urls of related documents\n504\nFigure 1: QUILL Architecture : Retrieval Augmentation and Multi-stage Distillation.\nFeature EComm Orcas-I\nMedian 99% Median 99%\nQuery 5 18 5 10\nExpandTerms 17 28 N/A N/A\n(Up to) 10 Titles 157 245 13 104\n(Up to) 10 URLs 159 304 39 238\nTable 2: mT5 sequence lengths by features.\ncould provide valuable context to help understand\nthe intent of the query. For example, it may not be\nimmediately apparent what a query like ua 1234\nmay mean. However, via the retrieved documents\nwe can understand that the query is seeking infor-\nmation about a United Airlines ﬂight.\nWhile there are multiple ways of augmenting\nthe input via retrieved documents (example: the\nFusion-in-Decoder architecture (Izacard and Grave,\n2020)), we chose to study the most straightforward\nand popular approach of concatenating the titles\n/ urls of the retrieved top-k documents with the\noriginal query as the input to our LLM. As shown\nempirically (Sec 5), this model outperforms all\nbaselines – demonstrating the value of additional\ncontext.\nMulti-stage distillation:The drawback of RA\nis the additional sequence length of the input. As\nseen from Table 2, augmenting a query with (upto)\n10 titles and urls increases the sequence length\nby an order of magnitude. Consequently, this\nmakes distillation far more challenging given the\nquadratic complexity of sequence length (due to\nself-attention) in transformer models. This leaves\nus in a dichotomy between a more effective model\nwith a much smaller distillation set, vs. a lower per-\nforming model with a larger distillation set. Given\na large distillation set is required for training an\neffective student, this leaves us at risk of not being\nable to beneﬁt from RA, given that a very large\ndataset with RA will incur very long and impracti-\ncal inference times.\nTo get the best of both worlds we propose a two-\nstage distillation approach. In the ﬁrst stage we\ndistill the Professor RA LLM into a Teacher LLM\nwithout RA. The Teacher model uses ExpandTerms\nwhich provide additional context to the queries.\nWhile this may not be as expressive as retrieval\naugmentation, this provides a good compromise of\ngreatly reducing sequence length while giving up\nonly a little in performance. We do so by using\na small subset of the unlabeled data. As shown\nempirically, a LLM teacher trained in this manner\nperforms signiﬁcantly better than a non-RA LLM\ntrained directly on the human data, while at the\nsame time allowing us to efﬁcient distillation.\nIn the second stage we use the Teacher LLM to\nannotate the entire unlabeled dataset. This is in\nturn used to train the ﬁnal Student model that will\nbe used in practice.\n5 Experiments and Results\nExperimental Setup: Our experiments were all\nconducted using the mT5 (Xue et al., 2021) check-\npoints. We validate performance across three learn-\ning rates (1e-3, 1e-4, 5e-5) – selecting the best\ncheckpoint using the validation set loss. For mod-\n505\nels trained from the provided training sets, we used\na batch size of 64 in our experiments and trained\nfor 4K steps (EComm) / 20K steps (Orcas-I).\nFor distilled models, we used a batch size of 128\nfor Teacher models and 1024 for Student models.\nWe use different batch sizes because of the model\narchitectures, mT5 for the Teacher vs a BERT-\nbased model for the Student. In both cases, we\ntrained for 1 epoch, unless mentioned otherwise.\nWe only use the encoder of the mT5 model with\nan additional layer added on top to predict the clas-\nsiﬁcation scores. The Professor, the Teacher and\nthe Student ﬁne-tuning experiments are all set up\nas a query intent classiﬁcation task. Given that the\nTeacher and Student models are trained on millions\nof examples and this itself is a time and resource\nintensive step, we restrict our experiments to only\none epoch. We demonstrate performance gains\neven with one epoch via the techniques elaborated\nin this paper.\nWe studied the effect of distillation data size, for\nboth stages of distillation. For the EComm dataset,\nwe used an in-house retriever to ﬁnd related doc-\numents. For Orcas-I, we use the provided docids\n(aggregated at per-query level) for retrieval aug-\nmentation. Unless speciﬁed, we use (upto) the top-\n10 results for retrieval augmentation 1. Sequence\nlength for models are based on the training set\nand features (set to 99%-percentile of sequence\nlengths).\nStudents and Features: Our experiments\ndemonstrate results for a fast, efﬁcient 4-layer trans-\nformer student architecture, with hidden dimension-\nality of 256. We default to using the query as the\nonly feature in the student for simplicity. To com-\npare against query expansion techniques, we used a\nsophisticated in-house memorization-based query\nexpansion model in our Professor / Teacher experi-\nments on EComm. This expansion model – which\nwe refer to as ExpandTerms – provides a list of\nrelated terms for a given query, which are concate-\nnated with the query (and identiﬁers for start / end\nof each feature).\nMetrics: To compare performance of differ-\nent models we use two metrics: MicroF1 and\nMacroF1 for Orcas-I, and AUC-PR and AUC-\nROC for EComm. For EComm, we only report\n1For Orcas-I, nearly 2/3rd of the queries only have a single\nprovided result, while some have upwards of 2000 results,\nwhich is why the lengths for RA features on Orcas-I in Table 2\nare smaller. The 10 results augmented are randomly chosen if\nmore exist.\nModel Size ROC PR\nquery Base 0.0% 0.0%\n+ RA (titles, urls) Base +4.3% +4.6%\nquery XL +2.7% +3.1%\n+ RA (titles, urls) XL +6.3% +6.7%\nquery XXL +3.0% +3.3%\n+ RA (titles, urls) XXL +6.4% +6.9%\nTable 3: Results demonstrating the beneﬁt of Retrieval\nAugmentation (RA) across all model sizes.\nEComm ROC PR\nquery 0.0% 0.0%\n+ Terms +2.6% +1.9%\n+ RA (titles) +4.8% +4.8%\n+ RA (titles) + Terms +5.1% +5.2%\n+ RA (urls) +5.3% +5.7%\nTable 4: Analysis of the impact of different features\n(using Base-sized models) for the EComm dataset. Ex-\npandTerms abbreviated as Terms.\nperformance of models relative to the mT5 query-\nonly Base-sized model2.\n5.1 Effect of Retrieval Augmentation\nWhile the use of retrieval augmentation (RA) has\nbeen known to improve query classiﬁcation per-\nformance (Broder et al., 2007), the beneﬁt of RA\nis unclear in the age of LLMs. Thus, we start by\nevaluating the ﬁrst stage of QUILL i.e., the RA\nmodel. As seen in Table 3, RA improves perfor-\nmance signiﬁcantly across all model sizes includ-\ning the billion-parameter+ XL and XXL models.\nIn fact the gains from RA on the Base-sized model\nexceed the gains obtained by increasing model size\nof a query-only model to XXL. Given the gains ob-\nserved across all models sizes, we use Base-sized\nmodels in the rest of the paper to simplify experi-\nmentation.\n2For a sense of scale, each 0.5% point increase in metrics\non EComm is considered a signiﬁcant gain.\nEComm ROC PR\nOrcas-I MicF1 MacF1\nquery 69.8 69.75\n+ RA (titles) +6.3% +5.1%\n+ RA (urls) +8.2% +6.2%\n+ RA (titles+urls) +9.0% +7.2%\nTable 5: Analysis of the impact of different features\n(using Base-sized models) for the Orcas-I dataset.\n506\nEComm ROC PR\nBaseline Teacher +2.6% +1.9%(Finetuned on Training Set)\nQUILL Teacher +3.3% +2.8%(2M Prof Distilled Set)\nQUILL Teacher (4M) +3.4% +2.9%\nQUILL Teacher (8M) +3.5% +2.9%\nQUILL Professor +5.3% +5.7%\nTable 6: Comparison of different Teacher models\ntrained directly or via Professor-distillation for the\nEComm dataset.\nA natural question that may arise though is how\ndo these gains from RA compare to those obtained\nby powerful query expansion techniques. Thus, we\nperformed an in-depth ablation of features for the\nEComm dataset (on a Base-sized model for ease\nof experimentation) as seen in Table 4 and for the\nOrcas-I dataset as seen in Table 5. These results\nclearly demonstrate the potency of powerful query\nexpansion models ( i.e., ExpandTerms) – as evi-\ndenced by the large ∼2% gains over query-only\nmodels. However, we ﬁnd that RA adds even more\nvalue over these highly sophisticated expansion\nmodels with an a nearly 5+% increase in perfor-\nmance. Furthermore, we ﬁnd that RA techniques\ncan still be combined with query expansion for\nfurther gains.\nThe improvements on RA for Orcas-I (seen in\nTable 5) are even more substantial, with a nearly\n9% improvement over the query-only baseline, via\nthe use of the titles and urls of related documents.\nInterestingly, among RA features we ﬁnd that urls\ntend to perform slightly better than titles on both\ndatasets. We believe this to be because titles can\nhave a higher variance of informativeness – with\nboth highly verbose and very short titles commonly\nseen. Hence, given the simplicity and consistency\nof urls, we chose to use RA(urls) for subsequent\nexperiments as the Professor model.\n5.2 Distilling gains from RA\nWe next focus on the second stage of QUILL: Dis-\ntilling the RA model. Typically larger amounts of\ndistillation data lead to better performance. How-\never, given the increased sequence length of RA\nmodels and the cost of retrieval augmentation itself,\nannotating large distillation sets is highly challeng-\ning. Thus to capture such practical trade-offs, we\nOrcas-I MicF1 MacF1\nBaseline Teacher 69.8 69.75(Finetuned on Training Set)\nQUILL Teacher +1.1% +0.8%(2M Prof Distilled Set)\nTable 7: Comparison of different Teacher models\ntrained directly or via Professor-distillation for the\nOrcas-I dataset.\nonly used a small subset of the unlabeled data for\nthe QUILL Professor to Teacher distillation. In\nparticular, we used 4M examples for EComm (i.e.,\n3.1% of unlabeled data) and 2M for Orcas-I (19%)\nfor this ﬁrst stage of distillation – to represent a set\nthat is small enough set to be practical, but large\nenough to learn from. However, we do share results\nfor varying this size to understand its importance.\nQUILL Teacher models were thus trained by dis-\ntilling the RA(urls) Professor models. Our Teacher\nmodels had the same capacity and architecture (i.e.,\nmT5-Base as the Professor3 – except it does not use\nRA (features).\nAs a realistic and competitive baseline, we chose\na Baseline Teacher that resembles the QUILL\nTeacher in all aspects bar one – the data they are\ntrained on. Speciﬁcally, the Baseline Teacher is\ndirectly trained from the gold-labeled training data,\nunlike the QUILL teacher. We believe this is rep-\nresentative of practical applications today, where\nLLMs are trained directly on gold-labeled sets\n(before being distilled into the ﬁnal student mod-\nels). To further challenge QUILL, we leverage the\npowerful ExpandTerms features (for the EComm\ndataset) in our Teacher models – both Baseline and\nQUILL. We believe this provides a more challeng-\ning but realistic evaluation setup, since many base-\nline models in use today avail of powerful features\n(along with the query).\nAs seen from the results in Table 6 and Table 7,\nwe ﬁnd the QUILL Teachers provide a signiﬁ-\ncant performance improvement over the Baseline\nTeacher, despite having never directly seen the gold\nlabel data. On EComm, despite using an enhanced\n(realistic) baseline, QUILL teachers are ∼1% bet-\nter on all metrics. We ﬁnd a similar gap on Orcas-I\ndespite the Teacher there being trained on only 2M\nexamples (just 1.5x the training set size). Put dif-\nferently, we now have trained our non-retrieval aug-\n3We observed similar trends even if the Teacher had less\ncapacity than the Professor.\n507\nModel (# Distillation) ROC PR\nNo Distillation Student -6.3% -7.3%\nBaseline Student -0.9% -1.6%\nQUILL Student +2.0% +1.5%\nQUILL 1-Stage Student(4M) +0.4% -0.2%\nQUILL 1-Stage Student(32M) +1.1% +0.6%\nTable 8: Performance of the different student mod-\nels trained from different teachers and using differing\namounts of distillation data (on EComm).\nmented language model to beneﬁt from the gains\nof retrieval augmentation. Even though the stu-\ndent model does not have Retrieval Augmentation,\nbecause of the Teacher model’s performance im-\nprovement, it is possible to annotate a considerably\nlarge number of training examples. We observe the\nStudent models to close the gap (compared to the\nTeacher) given larger training datasets.\nTo test the robustness of QUILL teachers we also\nvaried the amount of distillation data used – halv-\ning or doubling it. While there still exist distillation\ngaps to the professor (which can be narrowed via\nmore distillation data) on both datasets, our pro-\nposed approach works well even when using small\namounts of distillation data – which in turn allows\nus to save signiﬁcant compute.\nQuery Example URL W/L\nbengals sports.yahoo.com/\nnﬂ/teams/cin/\n\u0013\npah com-\npounds\nen.wikipedia.org/\nwiki/Polycyclic_aro\nmatic_hydrocarbon\n\u0013\nlaunch tech\nusa\nlaunchtechusa.com/ \u0013\nnoun univer-\nsity\nen.wikipedia.org/\nwiki/Noun\n\u0017\nairbed uk www.airbnb.co.uk/ \u0017\nTable 9: Wins/losses examples on Orcas-I.\n5.3 Final student training\nSo far, we have shown that QUILL can learn a bet-\nter (non-RA) teacher. However, an important ques-\ntion remains unanswered: Can these Teacher gains\nbe translated to the ﬁnal student model? In particu-\nlar, we postulate that the predictions of the QUILL\nTeacher may be more robust and easier to learn\n(for student models) than those of the Baseline. To\nverify this hypothesis we compared 4 fast student\nmodels (4-layer encoder-only models), with the\nonly difference being the data they were trained on:\n• No Distillation Student : This is the simple\nsolution of directly training the Student using\nthe labeled data.\n• Baseline Student : This is the current stan-\ndard involving distilling the Baseline Teacher\nmodel using the full unlabeled set.\n• QUILL Student: This is the proposed solution\ninvolving distilling the QUILL Teacher model\nusing the full unlabeled set.\n• QUILL 1-Stage Student: Rather than the two\nstage distillation approach, this student is di-\nrectly distilled from the Professor using a sub-\nset of the unlabeled data.\nAs seen from Table 8, all QUILL-based students\nsigniﬁcantly outperform the Baseline Student. In\nparticular our proposed 2-stage approach leads to\na ∼3 point gain on both metrics. This is notable\nin that the gap between QUILL and Baseline stu-\ndents is even higher than the Teachers – which we\nattribute to the QUILL Teacher labels being more\nrobust.\nComparing different QUILL students, we ﬁnd\nthat there is a notable performance gain by ﬁrst dis-\ntilling into a non-RA teacher, before distilling into\nthe ﬁnal student. While 1-stage distillation perfor-\nmance improves as more data is used, even when\n1/4th of all unlabeled data is retrieval-augmented\nand annotated by the Professor for direct distil-\nlation, it still falls short of the 2-stage approach.\nTogether, these results show: (1) QUILL students\noutperform the current state-of-the-art signiﬁcantly,\nand (2) QUILL beneﬁts from the 2 stage distillation\nof Professor to Teacher to ﬁnal student.\n5.4 Examples of Wins/Losses from RA\nWhile the previous sections focused on demon-\nstrating the efﬁcacy (and efﬁciency) of QUILL,\nwe wanted to also understand why and where are\nsome of these gains from RA stem from. To do so\nwe used the test-set of Orcas-I and sampled illus-\ntrative examples of wins / losses (Table 9) between\nthe baseline and the retrieval-augmented professor\nmodels. One common win pattern we found for\nRA models is when the query is unclear, or uses\ntechnical terms / abbreviations. In these cases, the\naugmented urls / titles help provide additional con-\ntext for the language model to understand what the\n508\nquery is about. On the ﬂip side, we also found\nthe biggest loss pattern to be when retrieval was\ninaccurate, which in turn misled the model regard-\ning the query intent. For example, we found our\nretriever returned wikipedia results more often than\nit should, which mislead the model to believe the\nquery had Factual intent.\n6 Future Work\nWhile we studied the problem of query intent clas-\nsiﬁcation in this paper, the approach proposed in\nour paper is general and could be applied to any\nquery understanding task. Following our approach,\ncould enable myriad query understanding tasks use\nretrieval augmentation in a practically realistic and\nefﬁcient manner. We leave this to future work\nthough. We should also note that our experiments\nreveal non-trivial distillation gaps in both stages\nof distillation, which we believe is another open\nopportunity for future improvements.\n7 Conclusion\nThis paper provides a practical recipe for combin-\ning Retrieval Augmentation and Large Language\nModels. In particular, we proposed QUILL as an\napproach to tackle the problem of query intent clas-\nsiﬁcation. Our empirical study demonstrates con-\nclusively that Retrieval Augmentation can provide\nsigniﬁcant value over existing approaches. Fur-\nthermore we show that via our two-stage distilla-\ntion approach, that QUILL not only learns better\nperforming, more robust teachers, but also leads\nto even bigger gains when distilled into fast, real-\nworld capable production student models.\n8 Acknowledgements\nWe sincerely thank Jiecao Chen, William Den-\nnis Kunz, Austin Tarango, Lee Gardner, Yang\nZhang, Constance Wang, Derya Ozkan, Nitin\nNalin, Raphael Hoffmann, Iftekhar Naim, Sid-\ndhartha Brahma, Siamak Shakeri, Hongkun Yu,\nJohn Nham, Ming-Wei Chang, Marc Najork,\nCorinna Cortes and many others for their insight-\nful feedback and help. We also thank the EMNLP\nReviewers for their thorough review, feedback and\nsuggestions.\n509\nLimitations\nThis paper focuses on efﬁcient and effective way\nof improving query intent classiﬁcation using Re-\ntrieval Augmentation (RA) and Multi-stage distil-\nlation. While we have made the best attempts to\nensure a robust and efﬁcient method, we would be\nremiss to not point out some key limitations of our\nwork:\n• Quality of Retrieval:A key reason for the\ngains seen in this paper is the use of Retrieval\nAugmentation. This additional context pro-\nvided in the form of result titles / URLs are\nhelpful, but are dependent on the quality of\nthe retrieval system. While we did not get a\nchance to explore the dependence of perfor-\nmance gains on retrieval quality, we plan to\nexplore this in future work.\n• Dependency of Retrieval: While our ap-\nproach provides for a practical and low-\ncompute way of incorporating retrieval aug-\nmentation, it still does add some compute (to\naugment the datasets) and system complexity.\nWhile we considered this trade-off well worth\nit in our use case, this may depend on speciﬁc\nsettings.\n• Retrieval-Augmentation techniques: As\ndiscussed in Section 4, we used a simple\nconcatenation based retrieval augmentation.\nHowever, there do exist more sophisticated\ntechniques for retrieval augmentation. For ex-\nample, models built on a Fusion-in-Decoder\n(Izacard and Grave, 2020) backbone have\ndemonstrated great performance (Hofstätter\net al., 2022b; Izacard et al., 2022) and im-\nproved efﬁciency (Hofstätter et al., 2022a).\nWe believe that these more sophisticated\nretrieval-augmentation technique may bring\nfurther improvements in our system and leave\nthis for future work to follow up on.\n• Datasets: The lack of large public query sets\nmeans that we were very limited in terms of\nwhat public benchmarks we could study this\nproblem on. While ORCAS-I is the largest\nsuch available set, they lack many alternatives\nthat are large enough to study the effects of dis-\ntillation. In the future though, we hope to use\nthe (somewhat related) problem of question-\nanswering where larger datasets (with large\nenough unlabeled data) exist for a more thor-\nough study.\n• Distillation gaps: Our results also clearly\ndemonstrate large distillation gaps in both\nstages. While there have been innovative tech-\nniques proposed to improve distillation perfor-\nmance, we intentionally chose to keep things\nsimple as those approaches are largely com-\nplementary to the problem we study in this\nwork.\n• Limited \"Large\" Model Experiments:\nWhile our work is intended for and positioned\nin the context of \"Large\" Language Mod-\nels, we realize that our most common model\nchoice (mT5-Base), may not be the most rep-\nresentative model in that category. This was\nan intentional choice on our end as we hoped\ndoing so would make the work more relevant\nto use cases and applications with more lim-\nited compute. For practitioners interested in\nmodels with tens of billions of parameters,\nwe refer them to our analysis of mT5-XXL\nsized models in Table 3, that demonstrates the\nviability of our approach on models of that\nscale.\nEthics Statement\nIn this paper, we used only publicly available Lan-\nguage Model and Checkpoints that have been pre-\nviously published – namely mT5.\nAn important consideration when working with\nquery datasets is data privacy. This is perhaps\nthe biggest reason why there do not exist many\nlarge public query datasets. We intentionally chose\nORCAS-I for this reason, as it is constructed\nfrom the ORCAS query set – which is widely re-\ngarded as a well-constructed, non-PII, sufﬁciently\nanonymized query dataset. While the EComm\ndataset used in this paper is proprietary, we should\nnote that it too has been scrubbed of PII and aims\nto follow the same (if not higher) data privacy prin-\nciples. Our data (and methodology) do not contain\nany information for or target any demographic or\nidentity characteristics.\nThe task we focus on – query classiﬁcation –\nis a general problem that beneﬁts everyone. In\nfact, it can enable better IR systems thereby bene-\nﬁting users who otherwise might not get answers.\nThus, we do not anticipate any biases or misuse is-\nsues stemming from this. We believe that by using\n510\npublicly available and vetted retrieval models, the\nresulting retrieval augmented models should not\ncreate any new or further any existing biases.\nIn many ways a goal of our work is making\nretrieval augmentation more practical and reducing\ncompute needs for any such applications. While\nwe did present results with XXL sized models, we\nfocused most of our experiments on the smaller,\nmore efﬁcient Base-sized models so as to beneﬁt a\nwider section of our community and to reduce the\ncomputational needs of our experiments.\nReferences\nDaria Alexander, Wojciech Kusa, and Arjen P. de Vries.\n2022. ORCAS-i. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. ACM.\nRicardo Baeza-Yates, Liliana Calderón-Benavides, and\nCristina González-Caro. 2006. The intention behind\nweb queries. In International symposium on string\nprocessing and information retrieval, pages 98–109.\nSpringer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.\nAndrei Broder. 2002. A taxonomy of web search. In\nACM Sigir forum , volume 36, pages 3–10. ACM\nNew York, NY , USA.\nAndrei Z. Broder, Peter Ciccolo, Marcus Fontoura,\nEvgeniy Gabrilovich, Vanja Josifovski, and Lance\nRiedel. 2008. Search advertising using web rele-\nvance feedback. In Proceedings of the 17th ACM\nConference on Information and Knowledge Manage-\nment, CIKM ’08, page 1013–1022, New York, NY ,\nUSA. Association for Computing Machinery.\nAndrei Z Broder, Marcus Fontoura, Evgeniy\nGabrilovich, Amruta Joshi, Vanja Josifovski,\nand Tong Zhang. 2007. Robust classiﬁcation of rare\nqueries using web knowledge. In Proceedings of\nthe 30th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 231–238.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nNick Craswell, Daniel Campos, Bhaskar Mitra, Em-\nine Yilmaz, and Bodo Billerbeck. 2020. Orcas: 20\nmillion clicked query-document pairs for analyzing\nsearch. In Proceedings of the 29th ACM Interna-\ntional Conference on Information Knowledge Man-\nagement, CIKM ’20, page 2983–2989, New York,\nNY , USA. Association for Computing Machinery.\nFernando Diaz and Donald Metzler. 2006. Improving\nthe estimation of relevance models using large exter-\nnal corpora. In Proceedings of the 29th annual inter-\nnational ACM SIGIR conference on Research and\ndevelopment in information retrieval , pages 154–\n161.\nAlejandro Figueroa. 2015. Exploring effective features\nfor recognizing the user intent behind web queries.\nComputers in Industry, 68:162–169.\nJianping Gou, Baosheng Yu, Stephen J Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A\nsurvey. International Journal of Computer Vision ,\n129(6):1789–1819.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Inter-\nnational Conference on Machine Learning , pages\n3929–3938. PMLR.\nShuguang Han, Xuanhui Wang, Mike Bendersky, and\nMarc Najork. 2020. Learning-to-rank with bert in\ntf-ranking. arXiv preprint arXiv:2004.08476.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nSebastian Hofstätter, Jiecao Chen, Karthik Raman, and\nHamed Zamani. 2022a. Fid-light: Efﬁcient and ef-\nfective retrieval-augmented text generation. arXiv\npreprint arXiv:2209.14290.\nSebastian Hofstätter, Jiecao Chen, Karthik Raman,\nand Hamed Zamani. 2022b. Multi-task retrieval-\naugmented text generation with relevance sampling.\narXiv preprint arXiv:2207.03030.\nGautier Izacard and Edouard Grave. 2020. Lever-\naging passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\n511\nBernard J Jansen, Danielle L Booth, and Amanda\nSpink. 2008. Determining the informational, navi-\ngational, and transactional intent of web queries. In-\nformation Processing & Management , 44(3):1251–\n1266.\nIn-Ho Kang and GilChang Kim. 2003. Query type clas-\nsiﬁcation for web document retrieval. In Proceed-\nings of the 26th annual international ACM SIGIR\nconference on Research and development in infor-\nmaion retrieval, pages 64–71.\nAshish Kathuria, Bernard J Jansen, Carolyn Hafernik,\nand Amanda Spink. 2010. Classifying the user in-\ntent of web queries using k-means clustering. Inter-\nnet Research.\nDirk Lewandowski, Jessica Drechsler, and Sonja\nV on Mach. 2012. Deriving query intents from\nweb search engine queries. Journal of the Ameri-\ncan Society for Information Science and Technology,\n63(9):1773–1788.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distil-\nlation via teacher assistant. In Proceedings of\nthe AAAI conference on artiﬁcial intelligence , vol-\nume 34, pages 5191–5198.\nAlaa Mohasseb, Mohamed Bader-El-Den, and Mihaela\nCocea. 2019. A customised grammar framework for\nquery classiﬁcation. Expert Systems with Applica-\ntions, 135:164–180.\nRodrigo Nogueira and Jimmy Lin. 2019. From\ndoc2query to doctttttquery. Online preprint, 6.\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and\nKyunghyun Cho. 2019. Document expansion by\nquery prediction. arXiv preprint arXiv:1904.08375.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nXuanhui Wang. 2020. Query Segmentation and Tag-\nging, pages 43–67. Springer International Publish-\ning, Cham.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nRuqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan,\nand Xueqi Cheng. 2020. Query understanding via\nintent description generation. In Proceedings of the\n29th ACM International Conference on Information\namp; Knowledge Management , CIKM ’20, page\n1823–1832, New York, NY , USA. Association for\nComputing Machinery."
}