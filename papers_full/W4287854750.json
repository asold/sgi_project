{
    "title": "AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
    "url": "https://openalex.org/W4287854750",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096181728",
            "name": "Yue Yu",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2112272916",
            "name": "Lingkai Kong",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100936175",
            "name": "Jieyu Zhang",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2233944054",
            "name": "Rongzhi Zhang",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2096826587",
            "name": "Chao Zhang",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2959716049",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3172399575",
        "https://openalex.org/W3035256610",
        "https://openalex.org/W4235216760",
        "https://openalex.org/W3091002423",
        "https://openalex.org/W2774918944",
        "https://openalex.org/W3101889167",
        "https://openalex.org/W3034199299",
        "https://openalex.org/W3101345273",
        "https://openalex.org/W601603264",
        "https://openalex.org/W3174481471",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4294554825",
        "https://openalex.org/W3035146082",
        "https://openalex.org/W4226250826",
        "https://openalex.org/W3105522431",
        "https://openalex.org/W3089635645",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2951786554",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W4306385136",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2073459066",
        "https://openalex.org/W3156501750",
        "https://openalex.org/W3105241646",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4226113762",
        "https://openalex.org/W4287388854",
        "https://openalex.org/W2471138382",
        "https://openalex.org/W3159676894",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W4206648492",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3103981637",
        "https://openalex.org/W3168921237",
        "https://openalex.org/W3123356983",
        "https://openalex.org/W2970457158",
        "https://openalex.org/W4287554606",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3110608319",
        "https://openalex.org/W3099403624",
        "https://openalex.org/W2963442201",
        "https://openalex.org/W2096507791",
        "https://openalex.org/W2951970475",
        "https://openalex.org/W4287075708",
        "https://openalex.org/W2120887445",
        "https://openalex.org/W2070246124",
        "https://openalex.org/W3203375606",
        "https://openalex.org/W2789225886",
        "https://openalex.org/W2948367246",
        "https://openalex.org/W3200496214",
        "https://openalex.org/W2098742124",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, Chao Zhang. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1422 - 1436\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nACTUNE : Uncertainty-Based Active Self-Training for\nActive Fine-Tuning of Pretrained Language Models\nYue Yu1, Lingkai Kong1, Jieyu Zhang2, Rongzhi Zhang1, Chao Zhang1\n1Georgia Institute of Technology, Atlanta, GA 2University of Washington, Seattle, W A\n{yueyu, lkkong, rongzhi.zhang, chaozhang}@gatech.edu, jieyuz2@cs.washington.edu\nAbstract\nWhile pre-trained language model (PLM) ﬁne-\ntuning has achieved strong performance in\nmany NLP tasks, the ﬁne-tuning stage can be\nstill demanding in labeled data. Recent works\nhave resorted to active ﬁne-tuning to improve\nthe label efﬁciency of PLM ﬁne-tuning, but\nnone of them investigate the potential of un-\nlabeled data. We propose A CTUNE , a new\nframework that leverages unlabeled data to\nimprove the label efﬁciency of active PLM\nﬁne-tuning. A CTUNE switches between data\nannotation and model self-training based on\nuncertainty: it selects high-uncertainty unla-\nbeled samples for active annotation and low-\nuncertainty ones for model self-training. Un-\nder this framework, we design (1) a region-\naware sampling strategy that reduces redun-\ndancy when actively querying for annotations\nand (2) a momentum-based memory bank that\ndynamically aggregates the model’s pseudo la-\nbels to suppress label noise in self-training.\nExperiments on 6 text classiﬁcation datasets\nshow that ACTUNE outperforms the strongest\nactive learning and self-training baselines and\nimproves the label efﬁciency of PLM ﬁne-\ntuning by 56.2% on average. Our imple-\nmentation is available at https://github.\ncom/yueyu1030/actune.\n1 Introduction\nFine-tuning pre-trained language models (PLMs)\nhas achieved much success in natural language\nprocessing (NLP) (Devlin et al., 2019; Liu et al.,\n2019; Brown et al., 2020). One beneﬁt of PLM\nﬁne-tuning is the promising performance it offers\nwhen consuming only a few labeled data (Bansal\net al., 2020; Gao et al., 2021). However, there\nare still signiﬁcant gaps between few-shot and\nfully-supervised PLM ﬁne-tuning in many tasks.\nBesides, the performance of few-shot PLM ﬁne-\ntuning can be sensitive to different sets of training\ndata (Bragg et al., 2021). Therefore, there is a\ncrucial need for approaches that make PLM ﬁne-\ntuning more label-efﬁcient and robust to selection\nof training data, especially for applications where\nlabeled data are scarce and expensive to obtain.\nTowards this goal, researchers have recently re-\nsorted to active ﬁne-tuning of PLMs and achieved\ncomparable performance to fully-supervised meth-\nods with much less annotated samples (Ein-Dor\net al., 2020; Margatina et al., 2021a,b; Yuan et al.,\n2020). Nevertheless, they usually neglect unlabeled\ndata, which can be useful for improving label efﬁ-\nciency for PLM ﬁne-tuning (Du et al., 2021). To\nincorporate unlabeled data into active learning, ef-\nforts have been made in the semi-supervised active\nlearning literature (Wang et al., 2016; Rottmann\net al., 2018; Siméoni et al., 2020). However, the\nquery strategies proposed in these works can return\nhighly redundant samples due to limited representa-\ntion power, resulting in suboptimal label efﬁciency.\nMoreover, they usually rely on pseudo-labeling\nto utilize unlabeled data, which requires greater\n(yet often absent) care to denoise the pseudo la-\nbels, otherwise the errors could accumulate and\nhurt model performance. This issue can be even\nmore severe for PLMs, as the ﬁne-tuning process is\noften sensitive to different weight initialization and\ndata orderings (Dodge et al., 2020). Thus, it still\nremains open and challenging to design robust and\nlabel efﬁcient method for active PLM ﬁne-tuning.\nTo tackle the above challenges, we propose AC-\nTUNE , a new method that improves the label efﬁ-\nciency and robustness of active PLM ﬁne-tuning.\nBased on the estimated model uncertainty, AC-\nTUNE tightly couples active learning with self-\ntraining in each learning round: (1) when the\naverage uncertainty of a region is low, we trust\nthe model’s predictions and select its most certain\npredictions within the region for self-training; (2)\nwhen the average uncertainty of a region is high,\nindicating inadequate observations for parameter\nlearning, we actively annotate its most uncertain\n1422\nsamples within the region to improve model per-\nformance. Different from existing AL methods\nthat only leverage uncertainty for querying labels,\nour uncertainty-driven self-training paradigm grad-\nually leverages unlabeled data with low uncertainty\nvia self-training, while reducing the chance of er-\nror propagation triggered by highly-uncertain mis-\nlabeled data.\nTo further boost model performance for AC-\nTUNE , we design two techniques to improve the\nquery strategy and suppress label noise, namely\nregion-aware sampling (RS) and momentum-based\nmemory bank (MMB). Inspired by the fact that\nexisting uncertainty-based AL methods often end\nup with choosing uncertain yet repetitive data (Ein-\nDor et al., 2020; Margatina et al., 2021b), we de-\nsign the region-aware sampling technique to pro-\nmote both diversity and representativeness by lever-\naging the representation power of PLMs. Speciﬁ-\ncally, we ﬁrst estimate the uncertainties of the unla-\nbeled data with PLMs, then cluster the data using\ntheir PLM representations and weigh the data by\nthe corresponding uncertainty. Such a clustering\nscheme partitions the embedding space into small\nsub-regions with an emphasis on highly-uncertain\nsamples. Finally, by sampling over multiple high-\nuncertainty regions, our strategy selects data with\nhigh uncertainty and low redundancy.\nTo rectify the erroneous pseudo labels derived\nby self-training, we design a simple but effec-\ntive way to select low-uncertainty data for self-\ntraining. Our method is motivated by the fact that\nﬁne-tuning PLMs suffer from instability issues —\ndifferent initializations and data orders can lead\nto large variance in model performance (Dodge\net al., 2020; Zhang et al., 2020b; Mosbach et al.,\n2021). However, previous approaches only select\npseudo-labeled data based on the prediction of the\ncurrent round and are thus less reliable. In con-\ntrast, we maintain a dynamic memory bank to save\nthe predictions of unlabeled samples for later use.\nWe propose a momentum updating method to dy-\nnamically aggregate the predictions from preced-\ning rounds (Laine and Aila, 2016) and select low-\nuncertainty samples based on aggregated predic-\ntion. As a result, only the samples with high predic-\ntion conﬁdence over multiple rounds will be used\nfor self-training, which mitigates the issue of label\nnoise. We highlight that our active self-training\napproach is an efﬁcient substitution to existing AL\nmethods, requiring little extra computational cost.\nOur key contributions are: (1) an active self-\ntraining paradigm ACTUNE that integrates self-\ntraining and active learning to minimize the label-\ning cost for ﬁne-tuning PLMs; (2) a region-aware\nquerying strategy to enforce both the informative-\nness and the diversity of queried samples during\nAL; (3) a simple and effective momentum-based\nmethod to leverage the predictions in preceding\nrounds to alleviate the label noise in self-training\nand (4) experiments on 6 benchmarks demonstrat-\ning ACTUNE improves the label efﬁciency over\nexisting self-training and active learning baselines\nby 56.2%.\n2 Uncertainty-aware Active Self-training\n2.1 Problem Formulation\nWe study active ﬁne-tuning of pre-trained lan-\nguage models for text classiﬁcation, formulated\nas follows: Given a small number of labeled sam-\nples Xl = {(xi,yi)}L\ni=1 and unlabeled samples\nXu = {xj}U\nj=1 (|Xl|≪|X u|), we aim to ﬁne-tune\na pre-trained language model f(x; θ) :X→Y in\nan interactive way: we perform active self-training\nfor T rounds with the total labeling budget b. In\neach round, we aim to query B = b/T samples\ndenoted as Bfrom Xu to ﬁne-tune a pre-trained\nlanguage model f(x; θ) with both Xl,Band Xu\nto maximize the performance on downstream text\nclassiﬁcation tasks. Here X = Xl ∪Xu denotes\nall samples, and Y= {1,2,··· ,C}is the label set\nwhere Cis the number of classes.\n2.2 Overview of A CTUNE Framework\nWe now present our active self-training paradigm\nACTUNE underpinned by estimated uncertainty.\nWe begin the active self-training loop by ﬁne-\ntuning a BERT f(θ(0)) on the initial labeled data\nXL. Formally, we solve the following optimization\nproblem\nmin\nθ\n1\n|XL|\n∑\n(xi,yi)∈XL\nℓCE\n(\nf(xi; θ(0)),yi\n)\n. (1)\nIn round t(1 ≤t≤T) of active self-training, we\nﬁrst calculate the uncertainty score based on a given\nfunction a(t)\ni = a(xi,θ(t)) 1 for all xi ∈Xu. Then,\nwe query labeled samples and generate pseudo-\nlabels for unlabeled data Xu simultaneously to\nfacilitate self-training. For each sample xi, the\npseudo-label ˜yis calculated based on the current\n1Note that ACTUNE is agnostic to the way uncertainty\nscore a(t)\ni is computed.\n1423\nAlgorithm 1: Training Procedures of ACTUNE .\nInput: Initial labeled samples Xl; Unlabeled samples\nXu; Pre-trained LM f(·; θ), number of active\nself-training rounds T.\n// Fine-tune the LM with initial labeled data.\n1. Calculate θ(0) based on Eq. (1).\n2. Initialize the memory bank g(x; θt) based on the\ncurrent prediction.\n// Conduct active self-training with all data.\nfor t= 1,2,··· ,T do\n1. Run weighted K-Means (Eq. (3), (4)) until\nconvergence.\n2. Select sample set Q(t) for AL and S(t) for\nself-training from Xu based on Eq. (11) or (13).\n3. Augment the labeled set XL = XL ∪Q(t).\n4. Obtain θ(t) by ﬁnetuning f(·; θt) with LST (\nEq. (14)) using AdamW.\n5. Update memory bank g(x; θt) with Eq. (10)\nor (12).\nOutput: The ﬁnal ﬁne-tuned model f(·; θT ).\nmodel’s output:\n˜y= argmax\nj∈Y\n[\nf(x; θ(t))\n]\nj\n, (2)\nwhere f(x; θ(t)) ∈RC is a probability simplex\nand [f(x; θ(t))]j is the j-th entry. The procedure\nof ACTUNE is summarized in Algorithm 1.\n2.3 Region-aware Sampling for Active\nLearning on High-uncertainty Data\nAfter obtaining the uncertainty for unlabeled data,\nwe aim to query annotation for high-uncertainty\nsamples. However, directly sampling the most un-\ncertain samples gives suboptimal results as it tends\nto query repetitive data (Ein-Dor et al., 2020) that\nrepresent the overall data distribution poorly.\nTo tackle this issue, we propose region-aware\nsampling to capture both uncertainty and diversity\nduring active self-training. Speciﬁcally, in the t-\nth round, we ﬁrst conduct the weighted K-means\nclustering (Huang et al., 2005), which weights sam-\nples based on their uncertainty. Denote by K the\nnumber of clusters and v(t)\ni = BERT(xi) the repre-\nsentation of xi from the penultimate layer of BERT.\nThe weighted K-means process ﬁrst initializes the\ncenter of each each cluster µi(1 ≤i ≤K) via\nK-Means++ (Arthur and Vassilvitskii, 2007). Then,\nit jointly updates the centroid of each cluster and\nassigns each sample to cluster ci as\nc(t)\ni = argmin\nk=1,...,K\n∥vi −µk∥2 , (3)\nµ(t)\nk =\n∑\nxi∈C(t)\nk\na(xi,θ(t)) ·v(t)\ni\n∑\nx∈C(t)\nk\na(xi,θ(t)) (4)\nwhere C(t)\nk = {x(t)\ni |c(t)\ni = k}(k = 1,...,K )\nstands for the k-th cluster. The above two steps in\nEq. (3), (4) are repeated until convergence. Com-\npared with vanilla K-Means method, the weighting\nscheme increases the density of the samples with\nhigh uncertainty, thus enabling the K-Means meth-\nods to discover clusters with high uncertainty. After\nobtaining K regions with the corresponding data\nC(t)\nk , we calculate the uncertainty of each region as\nu(t)\nk = U(C(t)\nk ) +βI(C(t)\nk ) (5)\nwhere\nU(C(t)\nk ) = 1\n|C(t)\nk |\n∑\nxi∈C(t)\nk\na(xi,θ(t)), (6)\nis the average uncertainty of samples and\nI(C(t)\nk ) =−\n∑\nj∈C\nf(t)\nj log f(t)\nj (7)\nis the inter-class diversity within cluster k and\nf(t)\nj =\n∑\ni 1 {˜yi=j}\n|C(t)\nk |\nis the frequency of class j on\ncluster k. Notably, the term U(C(t)\nk ) assigns higher\nscore for clusters with more uncertain samples, and\nI(C(t)\nk ) grants higher scores for clusters containing\nsamples with more diverse predicted classes from\npseudo labels since such clusters would be closer\nto the decision boundary.\nThen, we rank the clusters in an ascending order\nin u(t)\nk . A high score indicates high uncertainty of\nthe model in these regions, and we need to actively\nannotate the member instances to reduce uncer-\ntainty and improve the model’s performance. We\nadopt a hierarchical sampling strategy: we ﬁrst se-\nlect the M clusters with the highest uncertainty,\nand then sample b′ = ⌊B\nM⌋data with the highest\nuncertainty to form the batch Q(t).2\nK(t)\na = top-M\nk∈{1,...,K}\nu(t)\nk ,\nQ(t) =\n⋃\nk∈K(t)\na\nC(t)\na,k where C(t)\na,k = Top-b′\nxi∈C(t)\nk\na(xi,θ(t)).\n(8)\nWe remark that such a hierarchical sampling strat-\negy queries most uncertain samples from differ-\nent regions, thus the uncertainty and diversity of\nqueried samples can be both achieved.\n2If the number of samples in the i-th cluster Ci is smaller\nthan b′, then we sample all the data within Ci and increase the\nbudget for the (i+ 1)-th cluster by b′−|Ci|.\n1424\n2.4 Self-training over Conﬁdent Samples\nfrom Low-uncertainty Regions\nFor self-training, we aim to select unlabeled sam-\nples which are most likely to have been correctly\nclassiﬁed by the current model. This requires the\nsample to have low uncertainty. Therefore, we\nselect the top ksamples from the M lowest uncer-\ntainty regions to form the acquired batch S(t):\nC(t)\ns =\n⋃\nk∈K(t)\ns\nC(t)\nk where K(t)\ns = bottom-M\nk∈{1,...,K}\nu(t)\nk ,\nS(t) = bottom-k\nxi∈C(t)\ns\na(xi,θ(t)).\n(9)\nMomentum-based Memory Bank for Self-\ntraining. As PLMs are sensitive to the stochas-\nticity involved in ﬁne-tuning, the model suffers\nfrom the instability issue — different weight ini-\ntialization and data orders may result in different\npredictions on the same dataset (Dodge et al., 2020).\nAdditionally, if the model gives inconsistent pre-\ndictions in different rounds for a speciﬁc sample,\nthen it is potentially uncertain about the sample,\nand adding it to the training set may harm the ac-\ntive self-training process. For example, for a two-\nclass classiﬁcation problem, suppose we obtain\nf(x; θ(t−1)) = [0.65,0.35] for sample xthe round\n(t−1) and f(x; θ(t)) = [0.05,0.95] for the round t.\nAlthough the model is quite ‘conﬁdent’ on the class\nof xwhen we only consider the result of the round\nt, it gives contradictory predictions over these two\nconsecutive rounds, which indicates that the model\nis actually uncertain to which class xbelongs.\nTo effectively mitigate the noise and stabilize the\nactive self-training process, we maintain a dynamic\nmemory bank to save the results from previous\nrounds, and use momentum update (He et al., 2020;\nLaine and Aila, 2016) to aggregate the results from\nboth the previous and current rounds. Then, during\nactive self-training, we will select samples with the\nhighest aggregated score. In this way, only those\nsamples that the model is certain about over all pre-\nvious rounds will be selected for self-training. We\ndesign two variants for the memory bank, namely\nprediction-based and value-based aggregation.\nPrediction based Momentum Update. We adopt\nan exponential moving average approach to aggre-\ngate the prediction g(x; θ(t)) on round tas\ng(x; θ(t)) =mtf(x; θ(t)) + (1−mt)g(x; θ(t−1)),\n(10)\nwhere mt = (1 − t\nT)mL + t\nTmH (0 < mL ≤\nmH ≤1) is a momentum coefﬁcient. We gradu-\nally increase the weight for models on later rounds,\nsince they are trained with more labeled data\nthus being able to provide more reliable predic-\ntions. Then, we calculate the uncertainty based on\ng(x; θ(t)) and rewrite Eq. (9) and (2) as\nS(t) = bottom-k\nxi∈C(t)\ns\na\n(\nxi,g(x; θ(t)),θ(t)\n)\n˜y= argmax\nj∈Y\n[\ng(x; θ(t))\n]\nj\n,\n(11)\nValue-based Momentum Update. For methods\nthat do not directly use prediction for uncertainty\nestimation, we aggregate the uncertainty value as\ng(x; θ(t)) =mta(x; θ(t)) + (1−mt)g(x; θ(t−1)). (12)\nThen, we use Eq. (12) to sample low-uncertainty\ndata for self-training as3\nS(t) = bottom-k\nxi∈C(t)\ns\ng(xi,θ(t)),\n˜y= argmax\nj∈Y\n[\nf(x; θ(t))\n]\nj\n.\n(13)\nBy aggregating the prediction results over previ-\nous rounds, we ﬁlter the sample with inconsistent\npredictions to suppress noisy labels.\n2.5 Model Learning and Update\nAfter obtaining both the labeled data and pseudo-\nlabeled data, we ﬁne-tune a new pre-trained BERT\nmodel θ(t+1) on them. Although we only include\nlow-uncertainty samples during self-training, it is\ndifﬁcult to eliminate all the wrong pseudo-labels,\nand such mislabeled samples can still hurt model\nperformance. To suppress such label noise, we\nuse a threshold-based strategy to further remove\nnoisy labels by selecting samples that agree with\nthe corresponding pseudo labels. The loss objective\nof optimizing θ(t+1) is\nLST = 1\n|L(t)|\n∑\nxi∈L(t)\nℓCE\n(\nf(xi; θ(t+1)),yi\n)\n+ λ\n|S(t)|\n∑\n˜xi∈S(t)\nωiℓCE\n(\nf(˜xi; θ(t+1)),˜yi\n)\n,\n(14)\nwhere L(t) = XL ∪Q(t) is the labeled set,\nλ is a hyper-parameter balancing the weight\nbetween clean and pseudo labels, and ωi =\n1 {\n[\nf(xi; θ(t+1))\n]\n˜yi\n> γ}stands for the thresh-\nolding function.\nComplexity Analysis. The running time of AC-\nTUNE is mainly consisted of two parts: the in-\nference time O(|Xu|) and the time for K-Means\nclustering O(dK|Xu|), where dis the dimension\nof the BERT feature v. For self-training, the size\n3Other choices such as soft pseudo label (Xie et al., 2020;\nLiang et al., 2020) is also applicable.\n1425\nDataset Label Type# Class# Train# Dev#Test\nSST-2 Sentiment 2 60.6k 0.8k 1.8k\nAG News News Topic 4 119k 1k 7.6k\nPubmedMedical Abstract5 180k 1k 30.1k\nDBPediaWikipedia Topic14 280k 1k 70k\nTREC Question 6 5.0k 0.5k 0.5k\nChemprotMedical Abstract10 12.8k 0.5k 1.6k\nTable 1: Dataset Statistics. For DBPedia, we randomly\nsample 20k sample from each class due to the limited\ncomputational resource.\nof the memory bank g(x; θ) is proportional to |Xu|,\nwhile the extra computation of maintaining this dic-\ntionary is ignorable since we do not inference over\nthe unlabeled data for multiple times in each round\nas BALD (Gal et al., 2017) does. The running time\nof ACTUNE will be shown in section C.\n3 Experiments\n3.1 Experiment Setup\nTasks and Datasets. In our main experiments,\nwe use 4 datasets, including SST-2 (Socher et al.,\n2013) for sentiment analysis, AGNews (Zhang\net al., 2015) for news topic classiﬁcation, Pubmed-\nRCT (Dernoncourt and Lee, 2017) for medical ab-\nstract classiﬁcation, and DBPedia (Zhang et al.,\n2015) for wikipedia topic classiﬁcation. For\nweakly-supervised text classiﬁcation, we choose\n2 datasets, namely TREC (Li and Roth, 2002)\nand Chemprot (Krallinger et al., 2017) from the\nWRENCH benchmark (Zhang et al., 2021) for eval-\nuation. The statistics are shown in Table 1.\nActive Learning Setups. Following (Yuan et al.,\n2020), we set the number of rounds T = 10, the\noverall budget for all datasets b= 1000and the ini-\ntial size of the labeled |Xl|is set to 100. In each AL\nround, we sample a batch of 100 samples from the\nunlabeled set Xu and query their labels. Since large\ndevelopment sets are impractical in low-resource\nsettings (Kann et al., 2019), we keep the size of\ndevelopment set as 1000, which is the same as the\nlabeling budget4. For weakly-supervised text clas-\nsiﬁcation, since the datasets are much smaller, we\nkeep the labeling budget and the size of develop-\nment set to b= 500.\nImplementation Details. We choose RoBERTa-\nbase (Liu et al., 2019) from the HuggingFace code-\nbase (Wolf et al., 2020) as the backbone for AC-\nTUNE and all baselines except for Pubmed and\nChemprot, where we use SciBERT (Beltagy et al.,\n2019), a BERT model pre-trained on scientiﬁc cor-\n4This is often neglected in previous low-resource AL stud-\nies, and we highlight it to ensure the true low-resource setting.\npora. In each round, we train from scratch to avoid\noverﬁtting the data collected in earlier rounds as\nobserved by Hu et al. (2019). More details are in\nAppendix B.\nHyperparameters. The hyperparameters setting\nis in Appendix B.5. In the t-th round of active\nself-training, we increase the number of pseudo-\nlabeled samples by k, where kis 500 for TREC and\nChemprot, 3000 for SST-2 and Pubmed-RCT, and\n5000 for others. For the momentum factor, we tune\nmLfrom [0.6,0.7,0.8] and mH from [0.8,0.9,1.0]\nand report the best {mL,mH}based on the perfor-\nmance of the development set.\nBaselines.\nSelf-training Methods: (1) Self-training (ST,\nLee (2013)): It is the vanilla self-training method\nthat generates pseudo labels for unlabeled data.\n(2) UST (Mukherjee and Awadallah, 2020; Rizve\net al., 2021): It is an uncertainty-based self-training\nmethod that only uses low-uncertainty data for self-\ntraining. (3) COSINE (Yu et al., 2021): It uses\nself-training to ﬁne-tune LM with weakly-labeled\ndata, which achieves SOTA performance on vari-\nous text datasets in WRENCH benchmark (Zhang\net al., 2021). Note that for these two baselines, we\nrandomly sample blabeled data as the initialization.\nActive Learning Methods: (1) Random: It ac-\nquires annotation randomly, which serves as a base-\nline for all methods. (2) Entropy (Holub et al.,\n2008): It is an uncertainty-based method that ac-\nquires annotations on samples with the highest pre-\ndictive entropy. (3) BALD (Gal et al., 2017): It\nis also an uncertainty-based method, which calcu-\nlates model uncertainty using MC Dropout (Gal\nand Ghahramani, 2015). (4) BADGE (Ash et al.,\n2020): It ﬁrst selects high uncertainty samples then\nuses KMeans++ over the gradient embedding to\nsample data. (5) ALPS (Yuan et al., 2020): It uses\nthe masked language model (MLM) loss of BERT\nto query labels for samples. (6) CAL (Margatina\net al., 2021b) is the most recent AL method for pre-\ntrained LMs. It calculates the uncertainty of each\nsample based on the KL divergence between the\nprediction of itself and its neighbors’ prediction.\nSemi-supervised Active Learning (SSAL) Meth-\nods: (1) ASST (Tomanek and Hahn, 2009;\nSiméoni et al., 2020) is an active semi-supervised\nlearning method that jointly queries labels for AL\nand samples pseudo labels for self-training. (2)\nCEAL (Wang et al., 2016) acquires annotations\non informative samples, and uses high-conﬁdence\n1426\nsamples with predicted pseudo labels for weights\nupdating. (3) BASS (Rottmann et al., 2018) is sim-\nilar to CEAL, but use MC dropout for querying\nlabeled sample. (4) REVIV AL(Guo et al., 2021)\nis the most recent SSAL method, which uses an\nadversarial loss to query samples and leverage label\npropagation to exploit adversarial examples.\nOur Method: We experiment with both Entropy\nand CAL as uncertainty measures for ACTUNE .\nNote that when compared with active learning base-\nlines, we do not augment the train set with pseudo-\nlabeled data (Eq. (9)) to ensure fair comparisons.\n3.2 Main Result\nFigure 1 reports the performance of ACTUNE and\nthe baselines on 4 benchmarks. From the results,\nwe have the following observations:\n•ACTUNE consistently outperforms baselines in\nmost of the cases. Different from studies in the\ncomputer vision (CV) domain (Siméoni et al.,\n2020) where the model does not perform well in\nthe low-data regime, pre-trained LM has achieved\ncompetitive performance with only a few labeled\ndata, which makes further improvements to the\nvanilla ﬁne-tuning challenging. Nevertheless, AC-\nTUNE surpasses baselines in more than 90% of the\nrounds and achieves 0.4%-0.7% and 0.3%-1.5%\nabsolute gain at the end of AL and SSAL respec-\ntively. Figure 3 quantitatively measures the num-\nber of labels needed for the most advanced active\nlearning model and self-training model (UST) to\noutperform ACTUNE with 1000 labels. These\nbaselines need >2000 clean labeled samples to\nreach the performance as ours. ACTUNE saves\non average 56.2% and 57.0% of the labeled sam-\nples than most advanced active learning and self-\ntraining baselines respectively, which justiﬁes its\npromising performance under low-resource scenar-\nios. Such improvements show the merits of two key\ndesigns under our active self-training framework:\nthe region-aware sampling for active learning and\nthe momentum-based memory bank for robust self-\ntraining, which will be discussed in the section 3.5.\n•Compared with the previous AL baselines, AC-\nTUNE can bring consistent performance gain, while\nprevious semi-supervised active learning methods\ncannot. For instance, BASS is based on BALD\nfor active learning, but sometimes it performs even\nworse than BALD with the same number of la-\nbeled data (see Fig. 1(b) and Fig. 1(f)). This is\nmainly because previous methods simply combine\nnoisy pseudo labels with clean labels for training\nwithout explicitly rectifying the wrongly-labeled\ndata, which will cause the LM to overﬁt these haz-\nardous labels. Moreover, previous methods do not\nexploit momentum updates to stabilize the learning\nprocess, as there are oscillations in the beginning\nrounds. In contrast, ACTUNE achieves a more\nstable learning process and enables an active self-\ntraining process to beneﬁt from more labeled data.\n•The self-training methods (ST & UST) achieve\nsuperior performance with limited labels. However,\nthey mainly focus on leveraging unlabeled data\nfor improving the performance, while our results\ndemonstrate that adaptive selecting the most useful\ndata for ﬁne-tuning is also important for improving\nthe performance. With a powerful querying policy,\nACTUNE can improve these self-training baselines\nby 1.05% in terms of accuracy on average.\n3.3 Weakly-supervised Learning\nACTUNE can be naturally used for weakly-\nsupervised classiﬁcation, where Xl is a set of noisy\nlabels derived from linguistic patterns or rules.\nSince the initial label set is noisy, the model trained\nwith Eq. (1) can overﬁt the label noise (Zhang et al.,\n2022a), and we can actively query labeled data to\nreﬁne the model. We conduct experiments on the\nTREC and Chemprot dataset5, where we ﬁrst use\nSnorkel (Ratner et al., 2017) to obtain weak label\nset Xl, then ﬁne-tune the pre-trained LM f(θ(0))\non Xl. After that, we adopt ACTUNE for active\nself-training.\nFig. 2 shows the results of these two datasets 6.\nWhen combining ACTUNE with CAL, the perfor-\nmance is unsatisfactory. We believe it is because\nCAL requires clean labels to calculate uncertain-\nties. When labels are inaccurate, it will prevent AC-\nTUNE from querying informative samples. In con-\ntrast, ACTUNE achieves the best performance over\nbaselines when using Entropy as the uncertainty\nmeasure. The performance gain is more notable\non the TREC dataset, where we achieve 96.68%\naccuracy, close to the fully supervised performance\n(96.80%) with only ∼6% of clean labels.\n3.4 Combination with Other AL Methods\nFig. 5(a) demonstrates the performance of AC-\nTUNE combined with other AL methods (e.g.\nBADGE, ALPS) on SST-2 dataset. It is clear that\neven if the AL methods are not uncertainty-based\n5Details for labeling functions are in Zhang et al. (2021).\n6We omit AL methods since they perform worse than\nSSAL methods on these datasets in general.\n1427\n200 300 400 500 600 700 800 900 1000\n86\n87\n88\n89\n90Performance\nRandom\nBALD\nALPS\nBADGE\nEntropy\nCAL\nAcTune+Entropy\nAcTune+CAL\n(a) SST-2, AL\n200 300 400 500 600 700 800 900 100087\n88\n89\n90\n91Performance\nRandom\nBALD\nALPS\nBADGE\nEntropy\nCAL\nAcTune+Entropy\nAcTune+CAL (b) AG News, AL\n200 300 400 500 600 700 800 900 1000\n80\n81\n82\n83\n84Performance\nRandom\nBALD\nALPS\nBADGE\nEntropy\nCAL\nAcTune+Entropy\nAcTune+CAL (c) Pubmed, AL\n200 300 400 500 600 700 800 900 1000\n96.5\n97.0\n97.5\n98.0\n98.5Performance\nRandom\nBALD\nALPS\nBADGE\nEntropy\nCAL\nAcTune+Entropy\nAcTune+CAL (d) DBPedia, AL\n200 300 400 500 600 700 800 900 100087\n88\n89\n90\n91\n92Performance\nASST\nBASS\nCEAL\nREVIVAL\nAcTune+Entropy\nAcTune+CAL\nST\nUST\n(e) SST-2, SSAL\n200 300 400 500 600 700 800 900 1000\n88\n89\n90\n91\n92Performance\nASST\nBASS\nCEAL\nREVIVAL\nAcTune+Entropy\nAcTune+CAL\nST\nUST (f) AG News, SSAL\n200 300 400 500 600 700 800 900 1000\n80\n81\n82\n83\n84\n85Performance\nASST\nBASS\nCEAL\nREVIVAL\nAcTune+Entropy\nAcTune+CAL\nST\nUST (g) Pubmed, SSAL\n200 300 400 500 600 700 800 900 1000\n96.5\n97.0\n97.5\n98.0\n98.5Performance\nASST\nBASS\nCEAL\nAcTune+Entropy\nAcTune+CAL\nST\nUST (h) DBPedia, SSAL †\nFigure 1: The comparision of A CTUNE with active learning, semi-supervised active learning and self-training\nbaselines. The ﬁrst row is the result under active learning setting (AL, i.e. no unlabeled data is used), the second\nrow is the result under semi-supervised active learning (SSAL) setting. The metric is accuracy. †: REVIV AL\ncauses OOM error for DBPedia dataset.\n50 100 150 200 250 300 350 400 450 500\n90\n92\n94\n96Performance\n ASST\nBASS\nCEAL\nREVIVAL\nAcTune+Entropy\nAcTune+CAL\nST\nCOSINE\n(a) TREC\n50 100 150 200 250 300 350 400 450 500\n56\n58\n60\n62\n64\n66Performance\n ASST\nBASS\nCEAL\nREVIVAL\nAcTune+Entropy\nAcTune+CAL\nST\nCOSINE (b) Chemprot\nFigure 2: The comparison of A CTUNE and baselines\non weakly-supervised classiﬁcation tasks.\nSST-2 AG News Pubmed DBPedia0\n1000\n2000\n3000\n4000Number of Labels\n1000 1000 1000 1000\n3000 2900\n2400\n1600\n3400\n2000\n2500\n1800\nAcTune\nSelf-training (ST)\nActive Learning (AL)\nFigure 3: The label-efﬁciency of A CTUNE compared\nwith AL and self-training baselines. According to\nFig. 1, the best AL method is Entropy for DBPedia and\nCAL for others.\n(e.g. BADGE), when using the entropy as an un-\ncertainty measure to select pseudo-labeled data for\nself-training, ACTUNE can further boost the perfor-\nmance. This indicates that ACTUNE is a general\nactive self-training approach, as it can serve as an\nefﬁcient plug-in module for existing AL methods.\n3.5 Ablation and Hyperparameter Study\nThe Effect of Different Components in A C-\nTUNE . We inspect different components of\nACTUNE , including the region-sampling (RS),\nmomentum-based memory bank (MMB), and\nweighted clustering (WClus) 7. Experimental re-\nsults (Fig. 5(b)) shows that all the three compo-\nnents contribute to the ﬁnal performance, as remov-\ning any of them hurts the classiﬁcation accuracy.\nAlso, we ﬁnd that when removing MMB, the perfor-\nmance hurts most in the beginning rounds, which\nindicates that MMB effectively suppresses label\nnoise when the model’s capacity is weak. Con-\nversely, removing WClus hurts the performance on\nlater rounds, as it enables the model to select most\ninformative samples.\nHyperparameter Study. We study two hyperpa-\nrameters, namely β and K used in querying la-\nbels. Figure 4(a) and 4(b) show the results. In\ngeneral, the model is insensitive to β as the per-\nformance difference is less than 0.6%. The model\ncannot perform well with smaller Ksince it cannot\npinpoint to high-uncertainty regions. For larger\nK, the performance also drops as some of the\nhigh-uncertainty regions can be outliers and sam-\npling from them would hurt the model perfor-\nmance (Karamcheti et al., 2021).\nA Closer Look at the Momentum-based Mem-\nory Bank. To examine the role of MMB, we show\nthe overall accuracy of pseudo-labels on AG News\ndataset in Fig. 4(c). From the result, it is clear that\nthe momentum-based memory bank can stabilize\nthe active self-training process, as the accuracy of\npseudo labels increases around 1%, especially in\n7For models w/o RS, we directly select samples with high-\nest uncertainty during AL. For models w/o MMB, we only\nuse the prediction from the current round for self-training. For\nmodels w/o WClus, we cluster data with vanilla K-Means.\n1428\n0.0 0.1 0.5 1 591.0\n91.2\n91.4\n91.6\n91.8\n92.0\n92.2Performance\nAG News w/ AcTune+CAL\nSST-2 w/ AcTune+Entropy\n(a) Effect of β\n15 20 25 30 35 40\n91.2\n91.4\n91.6\n91.8\n92.0\n92.2Performance\nAG News w/ AcTune+CAL\nSST-2 w/ AcTune+Entropy (b) Effect of K\n200 400 600 800 100095.5\n96.0\n96.5\n97.0\n97.5Performance\nmL=0.7, mH=0.9\nmL=0.8, mH=0.9\nmL=0.9, mH=0.9\nNo Momentum (c) Acc. of PL\n200 400 600 800 1000\n88\n89\n90\n91Performance\nValue, mL=0.7, mH=0.9\nValue, mL=0.9, mH=0.9\nProb, m=0.7, mH=0.9\nProb, m=0.9, mH=0.9\nNo Momentum (d) Entropy\n200 400 600 800 1000\n89\n90\n91\n92Performance\nValue, mL=0.7, mH=0.9\nValue, mL=0.9, mH=0.9\nProb, m=0.7, mH=0.9\nProb, m=0.9, mH=0.9\nNo Momentum (e) CAL\nFigure 4: Parameter study. Note the effect of different mL and mH is conducted on AG News dataset.\n200 300 400 500 600 700 800 900 1000\n86\n87\n88\n89\n90\n91Performance\nALPS\nBADGE\nAcTune+ALPS\nAcTune+BADGE\n(a) Combining w/ AL Methods\n200 300 400 500 600 700 800 900 100087\n88\n89\n90\n91\n92Performance\nAcTune-RS\nAcTune-MMB\nAcTune-RS-MMB\nAcTune-WClus\nAcTune (b) Ablation Study\nFigure 5: Results of A CTUNE with different AL\nmethods (SST-2), ablation study (SST-2 with A C-\nTUNE +Entropy).\nlater rounds. Fig 4(d) and 4(e) illustrates the model\nperformance with different mL and mH. Over-\nall, we ﬁnd that our model is robust to different\nchoices as ACTUNE outperform the baseline with-\nout momentum update consistently. Moreover, we\nﬁnd that the larger mH will generally lead to bet-\nter performance in later rounds. This is mainly\nbecause in later rounds, the model’s prediction is\nmore reliable. Conversely, at the beginning of the\ntraining, the model’s prediction might be oscillat-\ning on unlabeled data. In this case, using a smaller\nmL will favor samples with consistent predictions\nto improve the robustness of active self-training.\nAnother ﬁnding is that for different AL methods,\nthe optimal memory bank can be different. For\nEntropy, probability-based memory bank leads to\na better result, while for CAL, simple aggregating\nover uncertainty score achieves better performance.\nThis is mainly because the method used in CAL\nis more complicated, and using probability-based\nmemory bank may hurt the uncertainty calculation.\n3.6 Case Study\nWe give an example of our querying strategy on\nAG News dataset for the 1st round of active self-\ntraining process in ﬁgure 6. Note that we use t-SNE\nalgorithm (Van der Maaten and Hinton, 2008) for\ndimension reduction, and the black triangle stands\nfor the queried samples while other circles stands\nfor the unlabeled data. We can see that the existing\nuncertainty-based methods such as Entropy and\nCAL, are suffered from the issue of limited diver-\nsity. However, when combined with ACTUNE , the\ndiversity is much improved. Such results, com-\npared with the main results in ﬁgure 1, demonstrate\nthe efﬁcacy of ACTUNE empirically.\n4 Related Work\nActive Learning. Active learning has been widely\napplied to various NLP tasks (Yuan et al., 2020;\nShelmanov et al., 2021; Karamcheti et al., 2021).\nSo far, AL methods can be categorized into\nuncertainty-based methods (Gal et al., 2017; Mar-\ngatina et al., 2021a,b), diversity-based methods (Ru\net al., 2020; Sener and Savarese, 2018) and hy-\nbrid methods (Yuan et al., 2020; Ash et al., 2020).\nEin-Dor et al. (2020) offer an empirical study of\nactive learning with PLMs. Very recently, there\nare also several works attempted to query labeling\nfunctions for weakly-supervised learning (Boeck-\ning et al., 2020; Hsieh et al., 2022; Zhang et al.,\n2022b). In our study, we leverage the power of\nunlabeled instances via self-training to further pro-\nmote the performance of AL.\nSemi-supervised Active Learning (SSAL). Gao\net al. (2020); Guo et al. (2021) design query strate-\ngies for speciﬁc semi-supervised methods, Zhang\net al. (2020a); Jiang et al. (2020) combine active\nlearning with data augmentation and Tomanek and\nHahn (2009); Rottmann et al. (2018); Siméoni et al.\n(2020) exploit the most-certain samples from the\nunlabeled with pseudo-labeling to augment the\ntraining set. So far, most of the SSAL approaches\nare designed for CV domain and it remains un-\nknown how this paradigm performs with PLMs on\nNLP tasks. In contrast, we propose ACTUNE to\neffectively leverage unlabeled data during ﬁnetuing\nPLMs for NLP tasks.\nSelf-training. Self-training is one of the earliest\nand simplest approaches to semi-supervised learn-\ning (Lee, 2013). It ﬁrst generates pseudo labels\nfor high-conﬁdence samples, then ﬁts a new model\non pseudo labeled data to improve the generaliza-\ntion ability. However, it is known to be vulnera-\nble to error propagation (Arazo et al., 2020; Rizve\net al., 2021; Zuo et al., 2021). To alleviate this,\nwe adopt a simple momentum-based method to se-\nlect high conﬁdence samples, effectively reducing\n1429\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\n(a) AG News, Entropy\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40 (b) AG News, A CTUNE w/ Entropy\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40\n(c) AG News, CAL\n40\n 20\n 0 20 40\n40\n20\n0\n20\n40 (d) AG News, A CTUNE w/ CAL\nFigure 6: Visualization of the querying strategy of ACTUNE . Black dots stand for the queried data points. Different\ncolors indicates different categories.\nthe pseudo labels noise for active learning. Note\nthat although Mukherjee and Awadallah (2020);\nRizve et al. (2021) also leverage uncertainty infor-\nmation for self-training, their focus is on develop-\ning better self-training methods, while we aim to\njointly query high-uncertainty samples and gener-\nate pseudo-labels for low-uncertainty samples. The\nexperiments in Sec. 3 show that with appropriate\nquerying methods, ACTUNE can further improve\nthe performance of self-training.\n5 Conclusion and Discussion\nIn this paper, we developACTUNE , a general active\nself-training framework for enhancing both label\nefﬁciency and model performance in ﬁne-tuning\npre-trained language models (PLMs). We propose\na region-aware sampling approach to guarantee\nboth the uncertainty the diversity for querying la-\nbels. To combat the label noise propagation issue,\nwe design a momentum-based memory bank to\neffectively utilize the model predictions for pre-\nceding AL rounds. Empirical results on 6 public\ntext classiﬁcation benchmarks suggest the superi-\nority of ACTUNE to conventional active learning\nand semi-supervised active learning methods for\nﬁne-tuning PLMs with limited resources.\nThere are several directions to improveACTUNE .\nFirst, since our focus is on ﬁne-tuning pre-trained\nlanguage models, we use the representation of\n[CLS] token for classiﬁcation. In the future work,\nwe can consider using prompt tuning (Gao et al.,\n2021; Schick and Schütze, 2021), a more data-\nefﬁcient method for adopting pre-trained language\nmodels on classiﬁcation tasks to further promote\nthe efﬁciency. Also, due to the computational re-\nsource constraints, we do not use larger pre-trained\nlanguage models such as RoBERTa-large (Liu et al.,\n2019) which shown even better performance with\nonly a few labels (Du et al., 2021). Moreover, we\ncan explore more advanced uncertainty estimation\napproach (Kong et al., 2020) into ACTUNE to fur-\nther improve the performance. Last, apart from\nthe text classiﬁcation task, we can also extend our\nwork into other tasks such as sequence labeling and\nnatural language inference (NLI).\nAcknowledgements\nWe thank the anonymous reviewers for their feed-\nback. This work was supported in part by NSF\nIIS-2008334, IIS-2106961, CAREER IIS-2144338,\nand ONR MURI N00014-17-1-2656.\n1430\nReferences\nEric Arazo, Diego Ortego, Paul Albert, Noel E\nO’Connor, and Kevin McGuinness. 2020. Pseudo-\nlabeling and conﬁrmation bias in deep semi-\nsupervised learning. In 2020 International Joint\nConference on Neural Networks (IJCNN), pages 1–8.\nIEEE.\nDavid Arthur and Sergei Vassilvitskii. 2007. K-\nmeans++: The advantages of careful seeding.\nIn Proceedings of the Eighteenth Annual ACM-\nSIAM Symposium on Discrete Algorithms , page\n1027–1035, USA.\nJordan T. Ash, Chicheng Zhang, Akshay Krishna-\nmurthy, John Langford, and Alekh Agarwal. 2020.\nDeep batch active learning by diverse, uncertain gra-\ndient lower bounds. In International Conference on\nLearning Representations.\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020. Self-supervised\nmeta-learning for few-shot natural language classiﬁ-\ncation tasks. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 522–534, Online. Association\nfor Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–\n3620.\nBenedikt Boecking, Willie Neiswanger, Eric Xing, and\nArtur Dubrawski. 2020. Interactive weak supervi-\nsion: Learning useful heuristics for data labeling.\narXiv preprint arXiv:2012.06046.\nJonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-\nagy. 2021. Flex: Unifying evaluation for few-shot\nnlp. Advances in Neural Information Processing\nSystems, 34.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, et al. 2020. Language models are few-\nshot learners. In Advances in Neural Information\nProcessing Systems, volume 33, pages 1877–1901.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsiﬁcation in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. CoRR, abs/2002.06305.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-\nichart. 2018. The hitchhiker’s guide to testing statis-\ntical signiﬁcance in natural language processing. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1383–1392.\nJingfei Du, Edouard Grave, Beliz Gunel, Vishrav\nChaudhary, Onur Celebi, Michael Auli, Veselin\nStoyanov, and Alexis Conneau. 2021. Self-training\nimproves pre-training for natural language under-\nstanding. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5408–5418. Association for Compu-\ntational Linguistics.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim.\n2020. Active Learning for BERT: An Empirical\nStudy. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7949–7962. Association for Com-\nputational Linguistics.\nYarin Gal and Zoubin Ghahramani. 2015. Bayesian\nconvolutional neural networks with bernoulli\napproximate variational inference. CoRR,\nabs/1506.02158.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017. Deep bayesian active learning with image\ndata. In International Conference on Machine\nLearning, pages 1183–1192. PMLR.\nMingfei Gao, Zizhao Zhang, Guo Yu, Sercan Ö\nArık, Larry S Davis, and Tomas Pﬁster. 2020.\nConsistency-based semi-supervised active learning:\nTowards minimizing labeling cost. In European\nConference on Computer Vision , pages 510–526.\nSpringer.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nJiannan Guo, Haochen Shi, Yangyang Kang, Kun\nKuang, Siliang Tang, Zhuoren Jiang, Changlong\n1431\nSun, Fei Wu, and Yueting Zhuang. 2021. Semi-\nsupervised active learning for semi-supervised mod-\nels: Exploit adversarial examples with graph-based\nvirtual labels. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 2896–2905.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nAlex Holub, Pietro Perona, and Michael C Burl. 2008.\nEntropy-based active learning for object recogni-\ntion. In 2008 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition Work-\nshops, pages 1–8. IEEE.\nCheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner.\n2022. Nemo: Guiding and contextualizing weak su-\npervision for interactive data programming. arXiv\npreprint arXiv:2203.01382.\nPeiyun Hu, Zack Lipton, Anima Anandkumar, and\nDeva Ramanan. 2019. Active learning with partial\nfeedback. In International Conference on Learning\nRepresentations.\nJoshua Zhexue Huang, Michael K Ng, Hongqiang\nRong, and Zichen Li. 2005. Automated variable\nweighting in k-means type clustering. IEEE transac-\ntions on pattern analysis and machine intelligence ,\n27(5):657–668.\nZhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang,\nChanglong Sun, Qiong Zhang, and Xiaozhong Liu.\n2020. Camouﬂaged Chinese spam content detection\nwith semi-supervised generative active learning. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3080–\n3085, Online. Association for Computational Lin-\nguistics.\nKatharina Kann, Kyunghyun Cho, and Samuel R. Bow-\nman. 2019. Towards realistic practices in low-\nresource natural language processing: The develop-\nment set. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3342–3349, Hong Kong, China. Association for\nComputational Linguistics.\nSiddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and\nChristopher Manning. 2021. Mind your outliers! in-\nvestigating the negative impact of outliers on active\nlearning for visual question answering. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 7265–7281,\nOnline. Association for Computational Linguistics.\nLingkai Kong, Jimeng Sun, and Chao Zhang. 2020.\nSde-net: Equipping deep neural networks with un-\ncertainty estimates. In International Conference on\nMachine Learning, pages 5405–5415. PMLR.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\net al. 2017. Overview of the biocreative VI\nchemical-protein interaction track. In BioCreative\nevaluation Workshop, volume 1, pages 141–146.\nSamuli Laine and Timo Aila. 2016. Temporal ensem-\nbling for semi-supervised learning. arXiv preprint\narXiv:1610.02242.\nDong-Hyun Lee. 2013. Pseudo-label: The simple and\nefﬁcient semi-supervised learning method for deep\nneural networks. In ICML Workshop on challenges\nin representation learning, volume 3, page 896.\nXin Li and Dan Roth. 2002. Learning question classi-\nﬁers. In The 19th International Conference on Com-\nputational Linguistics.\nChen Liang, Yue Yu, Haoming Jiang, Siawpeng Er,\nRuijia Wang, Tuo Zhao, and Chao Zhang. 2020.\nBond: Bert-assisted open-domain named entity\nrecognition with distant supervision. In Proceed-\nings of the 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining ,\npages 1054–1064.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKaterina Margatina, Loic Barrault, and Nikolaos\nAletras. 2021a. Bayesian active learning with\npretrained language models. arXiv preprint\narXiv:2104.08320.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021b. Active learning by\nacquiring contrastive examples. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 650–663, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of ﬁne-tuning\n{bert}: Misconceptions, explanations, and strong\nbaselines. In International Conference on Learning\nRepresentations.\nSubhabrata Mukherjee and Ahmed Awadallah. 2020.\nUncertainty-aware self-training for few-shot text\nclassiﬁcation. Advances in Neural Information Pro-\ncessing Systems, 33.\nAlexander Ratner, Stephen H Bach, Henry Ehrenberg,\nJason Fries, Sen Wu, and Christopher Ré. 2017.\nSnorkel: Rapid training data creation with weak su-\npervision. In Proceedings of the VLDB Endowment.,\nvolume 11, page 269.\n1432\nMamshad Nayeem Rizve, Kevin Duarte, Yogesh S\nRawat, and Mubarak Shah. 2021. In defense\nof pseudo-labeling: An uncertainty-aware pseudo-\nlabel selection framework for semi-supervised learn-\ning. In International Conference on Learning Rep-\nresentations.\nMatthias Rottmann, Karsten Kahl, and Hanno\nGottschalk. 2018. Deep bayesian active semi-\nsupervised learning. In 2018 17th IEEE Interna-\ntional Conference on Machine Learning and Appli-\ncations (ICMLA), pages 158–164. IEEE.\nDongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingx-\nuan Wang, Weinan Zhang, Yong Yu, and Lei Li.\n2020. Active sentence learning by adversarial un-\ncertainty sampling in discrete space. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020 , pages 4908–4917, Online. Associa-\ntion for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In International Conference on Learning\nRepresentations.\nArtem Shelmanov, Dmitri Puzyrev, Lyubov\nKupriyanova, Denis Belyakov, Daniil Larionov,\nNikita Khromov, Olga Kozlova, Ekaterina Arte-\nmova, Dmitry V . Dylov, and Alexander Panchenko.\n2021. Active learning for sequence tagging with\ndeep pre-trained models and Bayesian uncertainty\nestimates. In Proceedings of the 16th Conference\nof the European Chapter of the Association for\nComputational Linguistics: Main Volume , pages\n1698–1712, Online. Association for Computational\nLinguistics.\nOriane Siméoni, Mateusz Budnik, Yannis Avrithis, and\nGuillaume Gravier. 2020. Rethinking deep active\nlearning: Using unlabeled data at model training. In\nthe 25th International Conference on Pattern Recog-\nnition (ICPR), pages 1220–1227. IEEE.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642. Association for Computational\nLinguistics.\nKatrin Tomanek and Udo Hahn. 2009. Semi-\nsupervised active learning for sequence labeling. In\nProceedings of the Joint Conference of the 47th An-\nnual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing\nof the AFNLP, pages 1039–1047.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nKeze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang,\nand Liang Lin. 2016. Cost-effective active learn-\ning for deep image classiﬁcation. IEEE Transac-\ntions on Circuits and Systems for Video Technology,\n27(12):2591–2600.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, and Others. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45, Online. Association for Computational\nLinguistics.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33.\nYue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo\nZhao, and Chao Zhang. 2021. Fine-tuning pre-\ntrained language model with weak supervision: A\ncontrastive-regularized self-training approach. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1063–1077. Association for Computational\nLinguistics.\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-\nGraber. 2020. Cold-start active learning through\nself-supervised language modeling. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7935–7948, Online. Association for Computational\nLinguistics.\nJieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang,\nand Alexander Ratner. 2022a. A survey on\nprogrammatic weak supervision. arXiv preprint\narXiv:2202.05433.\nJieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yam-\ning Yang, Mao Yang, and Alexander Ratner. 2021.\nWRENCH: A comprehensive benchmark for weak\nsupervision. In Thirty-ﬁfth Conference on Neural In-\nformation Processing Systems Datasets and Bench-\nmarks Track.\nRongzhi Zhang, Yue Yu, Pranav Shetty, Le Song,\nand Chao Zhang. 2022b. Prboost: Prompt-\nbased rule discovery and boosting for interac-\ntive weakly-supervised learning. arXiv preprint\narXiv:2203.09735.\n1433\nRongzhi Zhang, Yue Yu, and Chao Zhang. 2020a. Se-\nqMix: Augmenting active sequence labeling via se-\nquence mixup. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8566–8579, Online. As-\nsociation for Computational Linguistics.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2020b. Revisit-\ning few-sample bert ﬁne-tuning. arXiv preprint\narXiv:2006.05987.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. Advances in neural information process-\ning systems, 28:649–657.\nSimiao Zuo, Yue Yu, Chen Liang, Haoming Jiang,\nSiawpeng Er, Chao Zhang, Tuo Zhao, and\nHongyuan Zha. 2021. Self-training with differen-\ntiable teacher. arXiv preprint arXiv:2109.07049.\n1434\nA Datasets Details\nA.1 Data Source\nThe seven benchmarks in our experiments are all\npublicly available. Below are the links to down-\nloadable versions of these datasets.\n⋄SST-2: We use the datasets from https://\nhuggingface.co/datasets/glue.\n⋄AGNews: We use the datasets from https://\nhuggingface.co/datasets/ag_news.\n⋄Pubmed-RCT: Dataset is available at https:\n//github.com/Franck-Dernoncourt/\npubmed-rct.\n⋄ DBPedia: Dataset is available at\nhttps://huggingface.co/datasets/\ndbpedia_14.\nFor two weakly-supervised classiﬁcation tasks,\nwe use the data from WRENCH benchmark (Zhang\net al., 2021).\n⋄ TREC: Dataset is available at https:\n//drive.google.com/drive/u/1/\nfolders/1v55IKG2JN9fMtKJWU48B_5_\nDcPWGnpTq.\n⋄ ChemProt: The raw dataset is avail-\nable at http://www.cbs.dtu.dk/\nservices/ChemProt/ChemProt-2.0/.\nThe preprocessed dataset is available at\nhttps://drive.google.com/drive/u/\n1/folders/1v55IKG2JN9fMtKJWU48B_\n5_DcPWGnpTq.\nA.2 Train/Test Split\nFor all the datasets, we use the original\ntrain/dev/test split from the web. To keep the size\nof the development set small, we randomly sample\n1000 data for SST-2, AGNews, Pubmed-RCT, DB-\nPedia and randomly sample 500 samples for TREC,\nChemProt.\nB Details on Implementation and\nExperiment Setups\nB.1 Computing Infrastructure\nSystem: Ubuntu 18.04.3 LTS; Python 3.6; Pytorch\n1.6.\nCPU: Intel(R) Core(TM) i7-5930K CPU @\n3.50GHz.\nGPU: NVIDIA 2080Ti.\nB.2 Number of Parameters\nACTUNE and all baselines use Roberta-base (Liu\net al., 2019) with a task-speciﬁc classiﬁcation head\non the top as the backbone, which contains 125M\ntrainable parameters. We do not introduce any\nother parameters in our experiments.\nB.3 Experiment Setups\nFollowing (Ein-Dor et al., 2020; Yuan et al., 2020;\nMargatina et al., 2021b), all of our methods and\nbaselines are run with 3 different random seed and\nthe result is based on the average performance\non them. This indeed creates 4 (the number of\ndatasets) ×3 (the number of random seeds) ×\n11 (the number of methods) ×10 (the number of\nﬁne-tuning rounds in AL) = 1320 experiments for\nﬁne-tuning PLMs, which is almost the limit of our\ncomputational resources, not to mention additional\nexperiments on weakly-supervised text classiﬁca-\ntion as well as different hyper-parameter tuning.\nWe have show both the mean and the standard de-\nviation of the performance in our experiment sec-\ntions. All the results have passed a paired t-test\nwith p< 0.05 (Dror et al., 2018).\nB.4 Hyper-parameters for General\nExperiments\nWe use AdamW as the optimizer, and the learning\nrate is chosen from 1 ×10−5,2 ×10−5}. A lin-\near learning rate decay schedule with warm-up 0.1\nis used, and the number of training epochs is 15\nfor ﬁne-tuning. For active self-training & SSAL\nbaselines, we tune the model with 2000 steps, and\nevaluate the performance on the development set in\nevery 50 steps. Finally, we use the model with best\nperformance on the development set for testing.\nB.5 Hyper-parameters for A CTUNE\nAlthough ACTUNE introduces several hyper-\nparameters including K, M, mL, mH, β,γ, λ,\nmost of them are keep ﬁxed during our experiments,\nthus it does not require heavy hyper-parameter tun-\ning. All results are reported as the average over\nthree runs.\nIn our experiments, we keep β = 0.5, λ= 1for\nall datasets. For other parameters, we use a grid\nsearch to ﬁnd the optimal setting for each datasets.\nSpeciﬁcally, we search γ from [0.5,0.6,0.7], mL\nfrom [0.6,0.7,0.8], mH from [0.8,0.9,1]. For AC-\nTUNE with Entropy, we use probability based ag-\ngregation and forACTUNE with CAL, we use value\n1435\nHyper-parameter SST-2 AG News Pubmed DBPedia TREC Chemprot\nDropout Ratio 0.1\nMaximum Tokens 32 96 96 64 64 128\nBatch Size for Xl 8\nBatch Size for Xu in Self-training 32 48 48 32 16 24\nWeight Decay 10−8\nLearning Rate 2 ×10−5\nβ 0.5\nM 25 30 30 40 40 40\nK 5 10\nγ 0.7 0.6\nmL 0.8 0.9 0.7 0.8 0.8 0.8\nmH 0.9 0.9 0.8 0.9 0.9 1.0\nλ 1\nTable 2: Hyper-parameter conﬁgurations. Note that we only keep certain number of tokens.\nMethod Dataset\nPubmed DBPedia\nFinetune (Random) <0.1s <0.1s\nEntropy (Holub et al., 2008) 461s 646s\nBALD (Gal et al., 2017) 4595s 6451s\nALPS (Yuan et al., 2020) 488s 677s\nBADGE (Ash et al., 2020) 554s 1140s\nCAL (Margatina et al., 2021b)493s 688s\nREVIV AL (Guo et al., 2021)3240s OOM\nACTUNE+ Entropy 477s 733s\nw/ RS for Active Learning 15.8s 44.9s\nw/ MMB for Self-training 0.12s 0.18s\nACTUNE+ CAL 510s 735s\nw/ RS for Active Learning 16.6s 46.4s\nw/ MMB for Self-training 0.12s 0.18s\nTable 3: The running time of different baselines. Note\nthat for ASST, CEAL and BASS, they directly use ex-\nisting active learning methods so we do not list the run-\nning time here.\nbased aggregation by default.\nC Runtime Analysis\nTable 3 shows the time in one active learning round\nof ACTUNE and baselines. Here we highlight that\nthe additional time for region-aware sampling and\nmomentum-based memory bank is rather small\ncompared with the inference time. Also, we ﬁnd\nthat BALD and REVIV AL are not so efﬁcient. For\nBALD, it needs to infer the uncertainty of the\nmodel by passing the data to model with multit-\nple times. Such an operation will make the total\ninference time for PLMs very long. For REVIV AL,\nwe ﬁnd that calculating the adversarial gradient\nneeds extra forward passes and backward passes,\nwhich could be time-consuming for PLMs with\nmillions of parameters8.\n8The original model is proposed with CV tasks and they\nuse ResNet-18 as the backbone which only contains 11M\nparameters (around 10% of the parameters of Roberta-base).\n1436"
}