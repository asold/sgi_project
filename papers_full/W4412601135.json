{
  "title": "Cross-evaluation of Large Language Model Assessment Behaviours in Educational Tasks by Cognitive Level",
  "url": "https://openalex.org/W4412601135",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3216666910",
      "name": "Benjamin David Fedoruk",
      "affiliations": [
        "First Technical University",
        "University of Ontario Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3216666910",
      "name": "Benjamin David Fedoruk",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3086342064",
    "https://openalex.org/W4404186316",
    "https://openalex.org/W4308939463",
    "https://openalex.org/W3191550339",
    "https://openalex.org/W4282569893",
    "https://openalex.org/W2087484885",
    "https://openalex.org/W4402819599",
    "https://openalex.org/W3179330384",
    "https://openalex.org/W4393212576",
    "https://openalex.org/W4391713912",
    "https://openalex.org/W4281692409",
    "https://openalex.org/W4393320461",
    "https://openalex.org/W4405945887",
    "https://openalex.org/W4405667041",
    "https://openalex.org/W6922103734",
    "https://openalex.org/W4396833233",
    "https://openalex.org/W2576265988",
    "https://openalex.org/W4379919478",
    "https://openalex.org/W4391667354",
    "https://openalex.org/W2037732564",
    "https://openalex.org/W4387665600",
    "https://openalex.org/W4393299838",
    "https://openalex.org/W4403524471",
    "https://openalex.org/W4403746933"
  ],
  "abstract": "Large language models show promise for educational assessment, but their comparative capability across different cognitive domains remains understudied. This paper presents a systematic analysis of seven leading LLMs—ChatGPT, Claude, Gemini, Perplexity, Mistral, Command R+, and Grok—in their ability to both generate and evaluate educational responses across different levels of the revised Bloom’s taxonomy. Using a novel cross-evaluation methodology, 6,045 evaluations were analyzed using a standard rubric examining content accuracy, cognitive alignment, communication clarity, and response depth. The findings revealed three distinct clusters of grading behaviour: lenient evaluators (Mistral, Gemini, and ChatGPT), moderate evaluators (Claude and Grok), and strict evaluators (Command R+ and Perplexity). Significant variations in grading consistency emerged, with ChatGPT showing the greatest consistency and Perplexity the most variability. Notable systematic biases were observed, including Gemini’s positive bias toward Grok and Command R+’s negative bias toward Gemini. These patterns provide a framework for selecting appropriate LLMs for specific educational tasks while highlighting the importance of understanding their individual evaluation tendencies.",
  "full_text": " Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n2 \n \n  \nCross-Evaluation of Large Language Model Assessment \nBehaviours in Educational Tasks by Cognitive Level \nBenjamin D. Fedoruk, https://orcid.org/0000-0001-7703-6712 \nAbstract \nLarge language models show promise for educational assessment, but their \ncomparative capability across diﬀerent cognitive domains remains \nunderstudied. This paper presents a systematic analysis of seven leading LLMs—\nChatGPT, Claude, Gemini, Perplexity, Mistral, Command R+, and Grok—in their \nability to both generate and evaluate educational responses across diﬀerent \nlevels of the revised Bloom’s taxonomy. Using a novel cross-evaluation \nmethodology, 6,045 evaluations were analyzed using a standard rubric \nexamining content accuracy, cognitive alignment, communication clarity, and \nresponse depth. The ﬁndings revealed three distinct clusters of grading \nbehaviour: lenient evaluators (Mistral, Claude, Gemini, and ChatGPT), moderate \nevaluators (Command R+ and Grok), and strict evaluators (Perplexity). Signiﬁcant \nvariations in grading consistency emerged, with ChatGPT showing the greatest \nconsistency and Perplexity the most variability. Notable systematic biases were \nobserved, including Gemini’s positive bias toward Grok and Command R+’s \nnegative bias toward Gemini. These patterns provide a framework for selecting \nappropriate LLMs for speciﬁc educational tasks while highlighting the \nimportance of understanding their individual evaluation tendencies.  \nKeywords: large language models, educational assessment, cognitive level, \nartiﬁcial intelligence, automated evaluation, cross-model \n  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n3 \n \nAssessment of student learning has undergone signiﬁcant changes in the past century, \nfrom knowledge-based tests to evaluation of higher-order thinking (Brown, 2022; Schulz & \nFitzPatrick, 2016). With the emergence of large language models (LLMs), a new inﬂection \npoint is on the near horizon (Saputra et al., 2024). These artiﬁcial intelligence (AI) systems \ndemonstrate unprecedented capabilities in understanding and generating human-like text, \nraising the possibility of automated educational assessment (Wang et al., 2024b). Novel \nassessment techniques may allow instructors to employ automated assessment techniques \nbeyond traditional multiple-choice or keyword-based evaluation systems (Ercikan & \nMcCaﬀrey, 2022). \nThe development of automated assessment tools has historically faced two key \nchallenges. First, there is diﬃculty in the evaluation of complex cognitive processes. Second, \nit is often diﬃcult to maintain the consistency of assessment across diﬀerent contexts. Early \ncomputer-aided assessment systems of the 1960s and 1970s were limited to basic pattern \nmatching and predetermined answer keys (Oﬁesh, 1963; Shute & Rahimi, 2017). The advent of \nnatural language processing (NLP) in the 1990s and 2000s brought more sophisticated \nanalysis capabilities (Wang et al., 2008). However, these systems still struggled with \nunderstanding nuanced responses and were unable to provide students with meaningful \nfeedback. LLMs represent a notable leap in automated language understanding and \ngeneration. These models have the potential to engage with complex ideas, generate detailed \nexplanations and maintain consistency in evaluation. However, due to educator hesitancy \n(Ghimire et al., 2024), their potential as a tool to support evaluation remains largely \nunexplored (Luckin et al., 2022). With the rapid emergence of LLMs to public access, \neducators now have access to multiple generative AI systems that may be optimal for \ndiﬀerent educational functions. Little systematic research exists comparing the educational \ncapabilities of diﬀerent LLMs across various cognitive domains.  \nThis study presents a comprehensive comparative analysis of seven leading LLMs---\nChatGPT, Claude, Gemini, Perplexity, Mistral, Command R+, and Grok (Alphabet Inc., 2024; \nAnthropic, 2024; Cohere, 2024; Mistral AI, 2024; OpenAI, 2024; Perplexity AI, 2024; X.AI Corp., \n2024)---to determine their optimal applications across diﬀerent assessment tasks. Leveraging \na cross-evaluation system, the paper endeavours to identify which models excel at the \nevaluation at speciﬁc cognitive levels. This will allow educators to select the optimal LLM for \ntheir speciﬁc objective. \nThe methodology employs a three-stage evaluation process. First, each LLM responds \nto a curated question bank containing ﬁve questions for each of the six levels of the \ntaxonomy proposed by Anderson and Krathwohl (2001): remembering, understanding, \napplying, analyzing, evaluating, and creating. Second, each LLM's responses are evaluated by \nall other LLMs in the study, creating a cross-evaluation matrix. Thus, each model acts in two \ncapacities---ﬁrst as a reviewer of a student submission, and second as a cross-evaluator of \nother LLM reviewer responses to the same questions. Finally, responses and evaluations are \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n4 \n \naggregated to determine each LLM's strengths and limitations across diﬀerent cognitive \ndomains. We shall denote these three stages as response generation, cross-evaluation, and \naggregation, respectively.  \nThis novel approach addresses several limitations of previous LLM evaluations. It \nextends beyond simple accuracy metrics, permitting evaluation across diﬀerent cognitive \nlevels. Individual model bias is reduced due to the use of collective evaluations from multiple \nLLMs (Talebirad & Nadiri, 2023). Overall, the approach outlines a uniform framework for the \ncomparison of LLM capabilities in the educational context.  \nHerein, several key questions will be addressed. We shall discuss which LLMs excel at \naddressing questions at diﬀerent levels of the taxonomy of Anderson & Krathwohl (2001). We \nshall also examine the patterns emerging in the cross-evaluation process. The strengths and \nweaknesses of each LLM will be determined. Additionally, a perspective on the impact of \ndiﬀering LLM architectural approaches will be discussed. \nMethodology \nThis study implements a comprehensive evaluation framework to compare the \neducational capabilities of seven LLMs. Through a cross-evaluation process, we assess how \nthese models perform across diﬀerent cognitive domains while also analyzing their ability to \nevaluate educational responses. The methodology encompasses several aspects of data \ncollection and analysis. This allows for the provision of both quantitative metrics and \nqualitative insights into LLM performance.  \nHerein, seven leading LLMs that represent diﬀerent approaches to language model \ndevelopment are examined. These models were selected based on their widespread use, their \ndiverse architectural approaches, and their ability to both generate and evaluate responses. \nTo ensure consistency throughout the study, we use the most recent stable version of each \nmodel. Speciﬁcally, we shall use OpenAI's GPT-4o mini (OpenAI, 2024), Claude 3.5 Sonnet \n(Anthropic, 2024), Gemini 1.5 Flash (Alphabet Inc., 2024), Perplexity with LLAMA 3.1 Sonar \nSmall (Perplexity AI, 2024), Mistral Large 2 (Mistral AI, 2024), Command R+ (Cohere, 2024), and \nGrok 2 Beta (X.AI Corp., 2024). Of note, Microsoft's Copilot (formerly Bing Chat) was omitted \nas it uses the same underlying model as ChatGPT (Microsoft, 2024). \nThe foundation of this evaluation rests on the curation of the question bank, \nconsisting of 30 questions with ﬁve questions allocated to each of Anderson and Krathwohl’s \n(2001) revised taxonomy (remembering, understanding, applying, analyzing, evaluating, and \ncreating). This conﬁguration was determined through a priori power analysis using G*Power \n3.1.9.7 (Faul et al., 2007). For a repeated measures ANOVA design comparing six cognitive \nlevels with ﬁve measurements per level, assuming a medium eﬀect size (f=0.25), α=0.05, and \nmoderate correlation between repeated measures, the analysis indicated that 24 participants \nwould be required to achieve power of 0.87 (λ=15, F_critical=2.50, df=4, 72). With the cross-\nevaluation design generating 210 evaluations per cognitive level, the achieved power exceeds \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n5 \n \nthis threshold, thus allowing for the detection of meaningful diﬀerences between cognitive \nlevels and model evaluation patterns. With seven models conducting cross-evaluations, this \ngenerates 1,260 total evaluations with suﬃcient power for reliable statistical analysis while \nmaintaining computational feasibility.  \nEach question was designed to target speciﬁc cognitive processes within the \ntaxonomic framework. Remembering questions require factual recall without interpretation; \nunderstanding questions demand conceptual explanations; applying questions present novel \nscenarios requiring procedural transfer; analyzing questions require examination of complex \nrelationships; evaluating questions demand reasoned judgments based on criteria; and \ncreating level questions require original synthesis. Questions were selected for domain-\ngeneral applicability to minimize biases and emphasize cognitive processes over speciﬁc \ncontent. Topics were drawn from widely taught subjects, including basic science, \nmathematics, and literature. The question bank development relied on alignment with \nAnderson and Krathwohl’s (2001) taxonomy rather than formal panel validation, with a pilot \nphase in which the consistency of LLM responses was assessed. The questions were \ndeveloped in an iterative process, with each iteration reviewed for clarity, cognitive \nalignment, and evaluation potential. While this approach ensured theoretical grounding, \nfuture research would beneﬁt from expert classiﬁcation to strengthen content validity.  \nData Collection \nData were collected in two phases: response generation and cross-evaluation. In the \nresponse generation phase, each LLM is presented with all 30 questions using standard \nprompting templates. Questions are presented in new chat contexts. The templates used are \nintended to minimize prompt-related variations while ensuring each model receives clear \ninstructions on the expected depth and format of response. To maintain consistency, a \nprompt protocol is used to include speciﬁc instructions about response length, formatting \nand level of detail expected. \nAll interactions with the models are conducted through their respective application \nprogramming interfaces (APIs) or website interfaces. To avoid bias in the subsequent \nevaluation phase, any identifying terms within responses, including mentions of the model’s \nown name, were removed. However, each LLM response was internally tagged within the \ndataset to retain identifying information, while keeping it private from LLM cross-evaluators.   \nCross-Evaluation Process \nThe cross-evaluation phase represents a novel approach to LLM evaluation. Each \nmodel's responses are evaluated by all other models in the study, creating a comprehensive \ncross-evaluation matrix. The process results in ﬁve independent evaluations for each \nresponse. This allows for the assessment of the quality of the original responses and the \nconsistency of evaluation across diﬀerent models. The evaluation framework uses a rubric \nthat examines four key dimensions: content accuracy, cognitive level alignment, clarity of \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n6 \n \ncommunication, and depth of response. Each dimension is scored by the generative AI agents \non a 25-point scale, with detailed criteria established for each score level.  \nTo ensure evaluation consistency, the CRISPE framework (Wang et al., 2024a) was \nutilized. The CRISPE framework is divided into ﬁve components. First, the Capacity and Role \ncomponent speciﬁes the role which the LLM should play. Second, the Insight component \ndictates what background information and context the LLM should provide. Third, the \nStatement component provides the task to be completed. Fourth, the Personality component \noutlines the correct style and personality of the LLM's output. Finally, the Experiment \ncomponent allows for multiple trials and responses from the LLM for use in experimentation.  \nResults \nThe analysis of 6,045 cross-evaluations revealed patterns in grading behaviour, \nevaluator consistency, and inter-model agreement. This section presents the ﬁndings \norganized into four main areas: overall grading patterns, evaluator consistency analysis, cross-\nmodel agreement patterns, and an analysis of controversial cases.  \nAnalysis of 6,045 cross-evaluations among the seven LLMs revealed patterns in \nevaluation behaviour and inter-model reliability. All reported grades are on a 100-point scale. \nThe aggregate evaluation data yielded a mean grade of 86.99 with a standard deviation of \n5.16 and a median of 85, indicating generally positive assessment trends across all models. \nTable 1 presents the comprehensive statistics. \nTable 1 \nLLM Performance as Reviewers: Evaluation Statistics \n \nModel M SD Range n \nChatGPT 89.11 3.43 23 891 \nClaude 87.10 3.52 27 777 \nCommand R+ 82.40 4.74 39 806 \nGemini 89.17 4.01 33 899 \nGrok 86.91 5.27 23 891 \nMistral 89.44 3.81 23 890 \nPerplexity 84.38 6.21 90 891 \n \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n7 \n \nNote. All grade values are calculated on a 100-point scale. n represents the sample \nsize. Sample sizes diﬀer between models due to variations in dangerous content \ndetection.  \nWhen analyzing the models' performance as generators rather than evaluators, \nanother pattern emerged. Grok (M=89.13, SD=4.68), Perplexity (M=88.25, SD=4.86), and \nClaude (M=88.01, SD=4.68) received the highest mean grades for their responses. The \nremaining models performed moderately, with Gemini (M=83.98, SD=5.44) receiving the \nlowest mean grades. \nViolin plots of grade distributions, pictured in Figure A1, reveal distinct evaluation \npatterns for each LLM, both as reviewers and when being reviewed. As reviewers, ChatGPT, \nMistral, and Gemini showed concentrated distributions around the 85-90 range with minimal \nspread. Perplexity displayed the most dispersed distribution with a notable tail below 20. This \nis reﬂective of its tendency toward extreme evaluations. The lower panel shows grade \ndistributions received by each LLM. It demonstrates that Grok and Claude received the most \nconsistent evaluations across reviewers, while Gemini received grades with the highest \nvariability. \nThrough analysis of the grading patterns, three clusters of evaluation behaviour \nbecome apparent. The ﬁrst cluster, characterized by lenient grading, included Mistral, Claude, \nGemini, and ChatGPT, with mean scores approaching 89. A second cluster, characterized by \nmoderate grading, included Command R+ and Grok with an approximate mean of 87. Finally, \nthe third cluster, characterized by a strict approach to grading, included only Perplexity with \nmeans below 85. Formal cluster analysis validated the existence of the three types of \nevaluator (WCSS=9.91). The lenient cluster is characterized by consistent positive biases \n(cluster center: +2.19 points) and low variability (SD=3.52). The moderate cluster \ndemonstrates moderative negative biases (cluster center: -2.04 points) with medium \nvariability (SD=4.81). The strict cluster exhibits strong negative bias (-5.00 points) with \nextremely high variability (SD=20.90). This statistical validation conﬁrms that the observed \npatterns represent genuine diﬀerences in evaluation approach, rather than random \nvariations. As shown in Table 1, ChatGPT exhibited the greatest consistency in evaluation with \na standard deviation of 3.43 and a range of 23, closely followed by Claude and Mistral. In \ncontrast, Perplexity showed high variability with a standard deviation of 6.21 and a range of \n90. This suggests markedly diﬀerent approaches to the application of assessment criteria.  \nStatistical analysis revealed systematic biases in cross-evaluation patterns. Recall that \nthe mean across all model cross-evaluations was 86.99. The most pronounced positive bias \nwas observed in Gemini's evaluation of Grok's responses (4.60 points above the mean). The \nmost pronounced negative bias was observed in Command R+'s evaluation of Gemini's \nresponses (8.39 points below the mean). Figure 1 illustrates these relationships.  \n \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n8 \n \nFigure 1 \nAverage Grades: Reviewer vs Reviewed LLM \n \nAnalysis of pairwise disagreements between LLMs revealed patterns of evaluation \ndiscord. The heatmap demonstrates that Command R+ exhibited the highest average \ndisagreement with other models, particularly Mistral and ChatGPT. Conversely, Claude and \nChatGPT showed alignment in their evaluations, with an average disagreement of only 2.9 \npoints. Perplexity maintained moderate disagreement levels with most models except Claude, \nsuggesting similarities in their evaluation frameworks. These patterns of pairwise \ndisagreement are visualized in Figure 2. \n \n \n \n \n \n\n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n9 \n \nFigure 2 \nAverage Grade Disagreement Between LLM Pairs \n \n \nSeveral cases of controversial evaluations were noted, where inter-model \ndisagreement was elevated. The maximum standard deviation of such cases was 35.58. A \npattern emerged in these cases, with Perplexity frequently assigning extreme low grades (2-\n24) while other models awarded high grades (85-95) to the same responses. Table 2 presents \nthe ﬁve most controversial cases, illustrating the extent of evaluator disagreement. \n \n \n \n \n \n\n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n10 \n \nTable 2 \nMost Controversial Evaluation Cases \n \nModel M SD Range \nChatGPT 74.33 35.58 90 \nGrok 74.33 33.23 77 \nMistral 78.83 28.48 72 \nClaude 77.67 28.01 74 \nGrok 79.33 27.49 71 \nNote. All grade values are calculated on a 100-point scale. \nDetailed analysis of the controversial cases, pictured in Figure 3, reveals results of \ninterest. Notably, Perplexity often deviated from other evaluators, assigning scores below 25 \nin cases where other models gave scores above 80. This pattern was evident in evaluations of \nChatGPT (SD=35.61), Grok (SD=33.23), and Mistral (SD=28.48). The bottom panel of Figure 3 \nshows the contrast in evaluation consistency among models, with Perplexity showing \nsigniﬁcantly higher standard deviations, averaging at 13.2, compared to other evaluators, \naveraging at 4.8. \nFigure 3 \nGrading Patterns in Controversial Cases: Individual Grade Distribution and Reviewer \nStatistics \n\n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n11 \n \nThe cross-evaluation matrix revealed complex patterns of inter-model assessment, \nwith grade diﬀerentials reaching up to 90 points in extreme cases. The average standard \ndeviation between reviewers 4.09, with the average grade range across all evaluations being \n10.43. These ﬁndings indicate substantial variability in how diﬀerent models interpret and \napply educational assessment criteria, suggesting fundamental diﬀerences in their underlying \napproaches to educational content evaluation. The consistency analysis further revealed that \nmodels maintained diﬀerent levels of quality standards across educational domains. ChatGPT \ndemonstrated the most stable evaluation pattern while maintaining high means, while \nPerplexity showed higher variability with moderate means, suggesting a more discriminating \nbut less consistent evaluation approach. These patterns remained stable across question \ntypes and cognitive domains. This suggests inherent diﬀerences in how models approach \neducational assessment tasks. \nAnalysis across the taxonomy (Anderson & Krathwohl, 2001) revealed variations in \nevaluation patterns. Mean grades varied notably across cognitive levels, with understanding \n(M=88.04, SD=7.72) and analyzing (M=88.04, SD=9.77) showing the greatest scores and least \nvariability. Tasks at the creating level received the lowest mean scores (M=85.43, SD=9.77) \nsuggesting greater evaluator uncertainty at higher cognitive levels. Table 3 presents \ncomprehensive statistics across cognitive levels; Table 4 shows the most positive, negative, \nand consistent LLM evaluators across cognitive levels. \nTable 3 \nOverall Performance Statistics Across Cognitive Levels \n \nCognitive Level M SD \nUnderstanding 88.04 7.72 \nAnalyzing 88.04 7.94 \nApplying 86.43 10.88 \nEvaluating 86.04 8.80 \nRemembering 86.02 10.61 \nCreating 85.43 9.77 \n \nNote. All grade values are calculated on a 100-point scale.  \n \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n12 \n \nTable 4 \nEvaluator Biases by Cognitive Level \nLevel Most Positive Most Negative Most Consistent \nUnderstanding Gemini (+2.97) Perplexity (-4.08) ChatGPT (3.04) \nAnalyzing Gemini (+2.83) Comm. R+ (-3.85) ChatGPT (3.14) \nApplying Mistral (+3.23) Perplexity (-7.45) ChatGPT (3.54) \nEvaluating Mistral (+2.89) Comm. R+ (-3.57) Claude (3.00) \nRemembering ChatGPT (+3.21) Perplexity (-6.24) Claude (3.13) \nCreating Mistral (+3.10) Comm. R+ (-5.99) Claude (3.06) \n \nNote. Values in parentheses beside Most Positive and Most Negative represent bias from \nthe mean. Values in parentheses beside Most Consistent represent standard deviation.  \nEﬀect size analysis using Cohen’s d revealed small but meaningful diﬀerences \nbetween the higher-performing cognitive levels (analyzing and understanding) and the lower-\nperforming creating level (d=0.29). This indicates that, while statistically signiﬁcant, the \ndiﬀerences are modest. This ﬁnding implies that LLMs maintain relatively consistent \nperformance across diﬀerent levels of cognitive complexity.  \nThe previously identiﬁed LLM clusters maintained their behaviours across cognitive \nlevels. The intensities, however, varied. Lenient evaluators (Mistral, Claude, Gemini, ChatGPT) \nshowed consistent positive biases ranging from 1.74 to 3.23 points above the mean. This \neﬀect was most pronounced when applying tasks. The strict evaluator (Perplexity) showed \nnegative biases, increasing with higher cognitive levels. Grading consistency showed \nsystematic variation across cognitive levels. ChatGPT maintained stability across all levels, \nwith standard deviations between 3.04 and 3.54. In contrast, Perplexity was inconsistent, \nparticularly in applying tasks (SD=25.09) and creating tasks (SD=20.50). Tasks at the \nunderstanding level generated the strongest consensus among LLM evaluators, with the \nsmallest range of biases. Tasks at the creating level generated the largest disagreement, with \nbias ranges spanning over 9 points.  \nCharacteristic behaviours of the LLMs were ampliﬁed as cognitive complexity \nincreased. Gemini's positive bias increased from +1.28 points in remembering tasks to +3.07 \npoints in applying tasks, while Command R+’s negative bias increased from -3.32 points in \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n13 \n \nremembering tasks to -5.99 points in creating tasks. Tables 5 and 6 present a summary of the \nmean grades and grading biases. \nTable 5 \nMean Grades by Cognitive Level and LLM Reviewer \n \n \n \n \n \n \nNote. All values represent mean grades on a 100-point scale. Higher values indicate more lenient grading. ChatGPT Claude Command R+ Gemini Grok Mistral Perplexity Cognitive Level \n90.0 88.6 84.4 91.0 88.4 89.8 84.0 Understanding \n89.2 86.9 82.7 87.3 87.2 89.0 79.8 Remembering \n88.0 86.9 82.5 88.7 84.7 88.9 82.6 Evaluating \n88.2 87.0 79.4 88.2 85.6 88.5 82.2 Creating \n88.9 87.8 82.9 89.5 87.2 89.7 79.0 Analyzing \n90.0 88.9 84.2 90.9 88.0 90.1 84.3 Applying  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n14 \n \nTable 6 \nStandard Deviations in Grading by Cognitive Level and LLM Reviewer \n \nDiscussion \nThe cross-evaluation methods employed in this study revealed distinct patterns in the \napproach of diﬀerent LLMs when faced with educational tasks. Three clear clusters of \nevaluation behaviour emerged, each characterized by their approach to assessment. These \npatterns are illuminating in both the capabilities and limitations of current generative AI \ntechnologies in the classroom context. \nInterpretation of Evaluation Clusters \nThe emergence of three distinct evaluator clusters -- lenient (Mistral, Claude, Gemini, \nChatGPT), moderate (Command R+, Grok), and strict (Perplexity) -- suggests fundamental \ndiﬀerences in how models interpret and apply criteria. The lenient cluster consistently \nawarded grades two to three points above the mean, while maintaining relatively low \nstandard deviations (SD=3.75). This suggests that the lenient grading models prioritize \nidentifying correct elements over penalizing errors. In contrast, the strict cluster's lower mean \nNote. All values represent standard deviations of grades. Lower values indicate more consistent grading. ChatGPT Claude Command R+ Gemini Grok Mistral Perplexity Cognitive Level \n3.0 3.5 4.2 4.0 4.8 3.6 16.8 Understanding \n3.1 3.1 4.1 3.7 5.7 3.7 25.0 Remembering \n3.3 3.0 4.1 3.9 5.3 3.5 20.2 Evaluating \n3.3 3.1 5.7 4.2 5.8 4.0 20.5 Creating \n3.3 3.1 5.7 4.2 5.4 3.6 25.1 Analyzing \n3.1 3.4 3.3 3.3 4.9 3.2 17.9 Applying  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n15 \n \ngrades and higher standard deviation (SD=5.48) suggest a more critical evaluation approach, \nwith greater sensitivity to subtle errors or omissions.  \nThe moderate evaluators demonstrated a more balanced approach with means closer \nto the overall average standard deviations. This pattern aligns with human expert grading \nbehaviour observed in previous studies (Wang et al., 2024b), where experienced educators \noften avoid extreme evaluations while maintaining consistent standards. The statistical \nvalidation of these clusters through formal cluster analysis (WCSS=9.91) conﬁrms that these \ngrouping patterns are not arbitrary but represent genuine diﬀerences in evaluation \napproaches. The cluster centers (lenient: +2.19 points with SD=3.52; moderate: -2.04 points \nwith SD=4.811; strict: -5.00 points with SD=20.90) quantify the characteristic behaviours of \neach evaluator type.  \nCognitive Level Eﬀects \nThe variation in evaluation patterns across cognitive levels reveals important \nlimitations of current LLM technology. The ﬁnding that understanding and analyzing tasks \nfrom the taxonomy (Anderson & Krathwohl, 2001) showed the highest mean scores (M=88.04) \nwith relatively low variability suggests these cognitive levels may be more amenable to LLM \nevaluation. This aligns with previous research indicating that higher-order thinking in LLMs is \nstrong but still requires improvement (Shojaee-Mend et al., 2024; Yatani et al., 2024; Zhai et \nal., 2024). In particular, Long et al. (2024) noted that LLMs show strong potential in teaching \nevaluation and facilitation.  \nThe increased evaluator disagreement observed at higher cognitive levels, particularly \nfor creating tasks (SD=9.77), reﬂects inherent challenges in assessing complex cognitive \nprocesses. This pattern mirrors many challenges faced by human graders. However, the \nmodest Cohen’s d (d=0.29) indicates that while statistically signiﬁcant, these diﬀerences have \nlimited practical signiﬁcance. This statistical evidence suggests that LLMs do not process \ncognitive complexity in the traditional hierarchical manner assumed for human learners. \nFuture research should explore whether LLM graders outperform human graders at higher \nlevels of cognitive complexity. \nSystematic Biases and Inter-Model Relations \nThe observed systematic biases in cross-evaluation, particularly Gemini's positive bias \ntowards Grok (+4.60 points) and Command R+’s negative bias towards Gemini (-8.39 points), \nraise questions about model architecture and training eﬀects. These consistent patterns \nsuggest that architectural diﬀerences and similarities may inﬂuence one model's \ninterpretation of another model's output. The strong alignment between Claude and \nChatGPT's evaluations (mean diﬀerence of 2.9 points) despite their diﬀerent training \napproaches indicates that some evaluation standards may emerge independent of \narchitectural choices.  \n \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n16 \n \nPerplexity’s High Uncertainty \nOf note is Perplexity’s marked deviation from the trends of other LLMs. While most \nmodels demonstrated standard deviations between 3.0 and 6.0 across all cognitive levels, \nPerplexity broke this trend with extremely high variability. It had standard deviations ranging \nfrom 16.8 to 25.1. This extreme inconsistency was most pronounced in remembering tasks \n(SD=25.0) and applying tasks (SD=25.1). High variability in Perplexity is indicative of \nuncertainty in the evaluation process. Such high levels of uncertainty raise concerns about \nPerplexity’s reliability for educational assessment tasks. The instability, in tandem with \nPerplexity’s status as a strict grader, warrants extreme caution in the use of current iterations \nof Perplexity for automated grading for tasks at any cognitive level.  \nImplications on Assessment and Theory \nThese ﬁndings provide several key implications for both educators and for the \neducational research community. First, it may be advisable to use multiple LLMs in tandem to \nprovide a more balanced assessment, in contrast to over-reliance on a single model. Using the \naverage from multiple LLMs can address bias; however, Saidakhror (2024) outlines the \nconcerns of increased computational and ﬁnancial costs. LLMs are most reliable for evaluating \ntasks involving understanding and analysis. This suggests that these should be the ﬁrst \ncandidates for automation. For higher cognitive levels (especially tasks at the creating level), \nLLMs should serve as supplementary tools rather than primary evaluators. Further, at all \nlevels, it is recommended that LLMs support and not supplant educators. It is critical that \nhumans remain always in the assessment process in order to ensure fairness and quality \n(Fischer, 2023). The consistent behaviour within clusters could be leveraged to create robust \nevaluation systems.  \nTheoretically, the results presented herein suggest that architectures develop \ncharacteristic approaches to assessment. Questions are raised about the relationship \nbetween model architecture and evaluation behaviour. Architectural choices may predictably \ninﬂuence assessment style; however, the tension between the known architectural \ndiﬀerences between Claude and ChatGPT and the similar assessment style reﬂects the need \nfor further research. The increased variability as the level of tasks is increased in the \ntaxonomy (Anderson & Krathwohl, 2001) aligns with generative AI theories predicting greater \nuncertainty in abstract reasoning tasks (Barak & Loewenstein, 2024; Hubert et al., 2024; \nTestolin et al., 2024). Current LLM technologies may face fundamental limitations in assessing \ncomplex cognitive processes. However, due to the rapid rate of progress in the development \nof generative AI, future models may prove more capable of higher-order tasks.  \nA notable ﬁnding emerged in the analysis of performance across cognitive levels. \nContrary to the expected pattern of increasing diﬃculty from order increases in the \ntaxonomy (Anderson & Krathwohl, 2001), the results indicate remarkably consistent scores \nacross all cognitive levels. For instance, understanding tasks (M=88.04) received similar scores \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n17 \n \nto analyzing tasks (M=88.04), while creating tasks (M=85.43) showed only a marginal decrease \nin score. These ﬁndings may suggest that LLMs do not process cognitive complexity in the \nhierarchical manner traditionally assumed for human learners. Alternatively, their evaluation \nframeworks may not fully capture nuanced diﬀerences between cognitive levels. This raises \nimportant questions about whether current LLM architectures might be applying similar \npattern-matching strategies across all cognitive levels. These results warrant further \ninvestigation into the impact of cognitive complexity on LLM automated evaluation.  \nHuman-AI Collaboration in Education \nWhile the cross-evaluation framework provides valuable insights into LLM assessment \nbehaviours, the presence of human expert graders is necessary. Human educators bring \ncontextual understanding, pedagogical expertise, and nuance that advanced LLMs cannot yet \nfully replicate (Flodén, 2025). Human involvement in the assessment process is crucial for \nmaintaining ethical standards, particularly when evaluating marginalized or non-traditional \nlearners whose responses may not align with patterns represented in LLM training data \n(Cheuk, 2021; Sato et al., 2024). Ultimately, educational assessment systems should be \ndesigned as human-computer collaborations rather than replacements. \nThe ethical implementation of LLM-assisted grading requires several safeguards. \nTransparency in educator usage of LLMs is paramount. Students must understand when and \nhow automated tools contribute to their evaluation. Further, human oversight remains critical, \nparticularly for high-stakes assessments or when evaluating students from underrepresented \ncommunities (Sato et al., 2024). Appeals processes must include human review to address \npotential LLM biases or misinterpretations.  \n The clustering patterns observed herein (lenient, moderate, strict) highlight the \nimportance of understanding LLM evaluation tendencies prior to implementation. Educators \nmust be trained to recognize these biases and implement them accordingly. Rather than \nreplacing human judgment, LLMs should augment human grading.  \nLimitations and Future Areas of Research \nSeveral limiting factors should be considered when interpreting these results. The \nmost signiﬁcant limitation of this study is the circularity of using LLMs to evaluate other LLMs’ \noutputs. This methodological choice introduces potential systemic biases which may \npropagate across model uses. The systematic biases observed in this study, such as Gemini’s \nconsistent positive bias toward Grok (+4.60 points) and Command R+’s negative bias toward \nGemini (-8.39 points), may reﬂect these underlying similarities or diﬀerences rather than \nobjective quality assessments. The circularity manifests in two primary ways. First, models \ntrained on overlapping datasets may develop shared perspectives on what constitutes quality \nresponses. Second, architectural similarities may lead to the preferential evaluation of \nresponses between aligned models.  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n18 \n \nTo mitigate this circularity, several strategies were employed, and others are \nrecommended for future research. Response anonymization ensured that evaluating models \ncould not identify the source of responses being assessed, reducing direct model-preference \neﬀects. The seven models selected represent diverse architectural approaches, including \ntraditional transformer decoders (ChatGPT, Command R+), constitutional AI (Claude), \nmultimodal architectures (Gemini), search-augmented models (Perplexity, Grok), and mixture \nof experts (MoE) models (Mistral). This diversity in training methodologies provides some \nprotection against uniform bias while enabling analysis of the impact of model speciﬁcs. \nStatistical analysis of evaluation patterns enabled the identiﬁcation and quantiﬁcation of \nsystemic biases, allowing for the informed interpretation of results. Thus, despite the issue of \ncircularity, the cross-evaluation approach provides meaningful insights into model evaluation \ntendencies in education.  \nAdditionally, the focus of the study on domain-general questions may not capture the \nfull extent of LLM performance in speciﬁc subject areas. The use of standardized prompting \ntemplates, while necessary for consistency, may not reﬂect the full range of potential LLM \ncapabilities. Finally, the rapid pace of LLM developments implies that model behaviour will \ninherently evolve as new versions are released, thus requiring frequent updating and \nmonitoring.  \nThis study suggests several promising directions for future research. Investigation of \ndomain-speciﬁc assessment capabilities could provide insight into how LLMs handle \nspecialized knowledge. Longitudinal studies tracking changes in model behaviour across \nversions could illuminate the evolution of evaluation capabilities. Further, research into the \nrelation between model architecture and evaluation behaviour could illuminate potential \ncausation for clustering patterns.  \nConclusion \nThis study presents a comprehensive cross-evaluation of seven leading LLMs in \neducational assessment tasks across cognitive levels of the taxonomy proposed by Anderson \nand Krathwohl (2001). Through analysis of 6045 evaluations, distinct patterns of each model \nwere identiﬁed. Thereby, the potential and limitations of each model are illuminated.  \nThe emergence of three distinct evaluator clusters -- lenient (Mistral, Claude, Gemini, \nChatGPT), moderate (Command R+, Grok), and conservative (Perplexity) -- demonstrates that \ncurrent LLM technology produces characteristic assessment behaviours that persist across \ncognitive levels. The identiﬁcation of these clusters, combined with detailed analysis of their \nevaluation patterns, provides a framework to determine the optimal model for varying \neducational contexts.  \nThe ﬁndings presented herein suggest that LLMs show particular promise in \nevaluating tasks at the understanding and analyzing levels, where they demonstrated both \nhigh reliability (SD=7.72) and strong inter-model agreement. However, the increased \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n19 \n \nvariability in evaluations at higher cognitive levels, particularly for creating tasks (SD=9.77), \nindicates that careful consideration must be given to ensure these tools are implemented \nideally across educational settings. The systematic biases observed in cross-evaluation \npatterns highlight the criticality of understanding model-speciﬁc tendencies when \nimplementing automated assessment systems. These ﬁndings suggest that ideal \nimplementations may involve multiple models to balance out individual biases, thereby \nproviding a robust evaluation.  \nFor educators and educational technology developers, these results provide practical \nguidance for the integration of LLMs into assessment systems. The clear delineation of model \nstrengths across diﬀerent cognitive domains can inform the selection of appropriate tools for \nspeciﬁc assessment tasks. Further, the identiﬁcation of systematic biases and evaluation \npatterns guides the development of more balanced and reliable systems.  \nLooking forward, this research notes several promising areas of future research. The \nrelation between model architecture and evaluation behaviour, the potential for domain-\nspeciﬁcity, and the evolution of evaluation patterns across model versions all warrant further \nstudy. As LLM technology continues to advance, increased understanding of these patterns \nand relationships will become increasingly crucial.  \nWhile current LLM technology shows signiﬁcant promise for educational assessment \n(Long et al., 2024; Weng et al., 2024), its implementation requires careful consideration of \nmodel-speciﬁc behaviours and limitations, and other factors explored in the literature \n(Gardner et al., 2021). Through a better understanding of these patterns, educators and \ndevelopers can collaborate towards the creation of more eﬃcacious and reliable automated \nassessment systems that enhance, rather than replace, human evaluation.   \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n20 \n \nReferences \nAlphabet Inc. (2024). Google Gemini (Gemini 1.5 Flash) [Large language model]. \nhttps://gemini.google.com  \nAnderson, L. W., & Krathwohl, D. R. (2001). A Taxonomy for Learning, Teaching, and Assessing: \nA Revision of Bloom’s Taxonomy of Educational Objectives: Complete Edition. Addison \nWesley Longman.   \nAnthropic. (2024). Claude (Claude 3.5 Sonnet) [Large language model]. https://claude.ai   \nBarak, T., & Loewenstein, Y. (2024). Untrained neural networks can demonstrate \nmemorization-independent abstract reasoning. Scientiﬁc Reports, 14, Article 27249. \nhttps://doi.org/10.1038/s41598-024-78530-z  \nBrown, G. T. L. (2022). The past, present and future of educational assessment: A \ntransdisciplinary perspective. Frontiers in Education, 7. \nhttps://doi.org/10.3389/feduc.2022.1060633  \nCheuk, T. (2021). Can AI be racist? Color-evasiveness in the application of machine learning to \nscience assessments. Science Education, 105(5), 825–836. \nhttps://doi.org/10.1002/sce.21671  \nCohere. (2024). Command R+ (August 2024 version) [Large language model]. \nhttps://coral.cohere.com  \nErcikan, K., & McCaﬀrey, D. F. (2022). Optimizing implementation of artiﬁcial-intelligence-\nbased automated scoring: An evidence centred design approach for designing \nassessments for AI-based scoring. Journal of Educational Measurement, 59(3), 272–\n287. https://doi.org/10.1111/jedm.12332  \nFaul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A ﬂexible statistical power \nanalysis program for the social, behavioral, and biomedical sciences. Behavioral \nResearch Methods, 39, 175–191. https://doi.org/10.3758/BF03193146  \nFischer, I. (2023). Evaluating the ethics of machines assessing humans. Journal of Information \nTechnology Teaching Cases, 14(2), 273–281. \nhttps://doi.org/10.1177/20438869231178844  \nFlodén, J. (2025). Grading exams using large language models: A comparison between human \nand AI grading of exams in higher education using ChatGPT. British Educational \nResearch Journal, 51(1), 201–224. https://doi.org/10.1002/berj.4069  \nGardner, J., O’Leary, M., Yuan, L. (2021). Artiﬁcial intelligence in educational assessment: \n‘Breakthrough? Or buncombe and ballyhoo?’. Journal of Computer Assisted Learning, \n37(5), 1207–1216. https://doi.org/10.1111/jcal.12577 \nGhimire, A., Prather, J., & Edwards, J. (2024). Generative AI in education: A study of educators’ \nawareness, sentiments, and inﬂuencing factors. arXiv. \nhttps://doi.org/10.48550/arXiv.2403.15586  \nHubert, K. F., Awa, K. N., & Zabelina, D. L. (2024). The current state of artiﬁcial intelligence \ngenerative language models is more creative than humans on divergent thinking tasks. \nScientiﬁc Reports, 14, Article 3440. https://doi.org/10.1038/s41598-024-53303-w  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n21 \n \nLong, Y., Luo, H., & Zhang, Y. (2024) Evaluating large language models in analysing classroom \ndialogue. npj Science of Learning, 9, Article 60. https://doi.org/10.1038/s41539-024-\n00273-3  \nLuckin, R., Cukurova, M., Kent, C., & du Boulay, B. (2022). Empowering educators to be AI-\nready. Computers and Education: Artiﬁcial Intelligence, 3, Article 100076. \nhttps://doi.org/10.1016/j.caeai.2022.100076  \nMicrosoft. (2024). Copilot [Large language model]. https://copilot.microsoft.com  \nMistral AI. (2024). Mistral (Mistral Large 2) [Large language model]. https://chat.mistral.ai/chat  \nOﬁesh, G. D. (1963). Automated teaching: The coming revolution in education and training. \nTeaching Aids News, 3(4), 1–10. https://www.jstor.org/stable/44745270  \nOpenAI. (2024). ChatGPT (GPT-4o mini) [Large language model]. https://chatgpt.com  \nPerplexity AI. (2024). Perplexity AI (LLAMA 3.1 Sonar Small 128k Online) [Large language \nmodel]. https://www.perplexity.ai  \nSaidakhror, G. (2024). The impact of artiﬁcial intelligence on higher education and the \neconomics of information technology. International Journal of Law and Policy, 2(3), 1–\n6. https://doi.org/10.59022/ijlp.125  \nSaputra, I., Kurniawan, A., Yanita, M., Putri, E. Y., & Mahniza, M. (2024). The evolution of \neducational assessment: How artiﬁcial intelligence is shaping trends and future \nlearning evaluation. The Indonesian Journal of Computer Science, 13(6), 9056–9074. \nhttps://doi.org/10.33022/ijcs.v13i6.4465 \nSato, E., Shyyan, V., Chauhan, S., & Christensen, L. (2024). Putting AI in fair: A framework for \nequity in AI-driven learner models and inclusive assessments. Journal of Measurement \nand Evaluation in Education and Psychology, 15, 263–281. \nhttps://doi.org/10.21031/epod.1526527   \nSchulz, H., & FitzPatrick, B. (2016). Teachers’ understandings of critical and higher order \nthinking and what this means for their teaching and assessments. Alberta Journal of \nEducational Research, 62(1), 61–86. https://doi.org/10.11575/ajer.v62i1.56168  \nShojaee-Mend, H., Mohebbati, R., Amiri, M., & Atarodi, A. (2024). Evaluating the strengths and \nweaknesses of large language models in answering neurophysiology questions. \nScientiﬁc Reports, 14(1), Article 10785. https://doi.org/10.1038/s41598-024-60405-y  \nShute, V. J., & Rahimi, S. (2017). Review of computer-based assessment for learning in \nelementary and secondary education. Journal of Computer Assisted Learning, 33, 1–\n19. https://doi.org/10.1111/jcal.12172  \nTalebirad, Y., & Nadiri, A. (2023). Multi-agent collaboration: Harnessing the power of \nintelligent LLM agents. arXiv. https://doi.org/10.48550/arXiv.2306.03314  \nTestolin, A., Hou, K., & Zorzi, M. (2024). Visual enumeration is challenging for large-scale \ngenerative AI. arXiv. https://doi.org/10.48550/arXiv.2402.03328  \nWang, H.-C., Chang, C.-Y., & Li, T.-Y. (2008) Assessing creative problem-solving with automated \ntext grading. Computers and Education, 51(4), 1450–1466. \nhttps://doi.org/10.1016/j.compedu.2008.01.006  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n22 \n \nWang, M., Wang, M., Xu, X., Yang, L., Cai, D., & Yin, M. (2024a). Unleashing ChatGPT’s power: A \ncase study on optimizing information retrieval in ﬂipped classrooms via prompt \nengineering. IEEE Transactions on Learning Technologies, 17, 629–641. \nhttps://doi.org/10.1109/TLT.2023.3324714  \nWang, S., Xu, T., Li, H., Zhang, C., Liang, J., Tang, J., Yu, P. S., & Wen, Q. (2024b). Large language \nmodels for education: A survey and outlook. arXiv. \nhttps://doi.org/10.48550/arXiv.2403.18105 \nWeng, X., Xia, Q., Gu, M., Rajaram, K., & Chiu, T. K. F. (2024). Assessment and learning outcomes \nfor generative AI in higher education: A scoping review on current research status and \ntrends. Australasian Journal of Educational Technology, 40(6), 37–55. \nhttps://doi.org/10.14742/ajet.9540  \nX.AI Corp. (2024). Grok (Grok 2 Beta) [Large language model]. https://x.ai   \nYatani, K., Sramek, Z., & Yang, C.-L. (2024). AI as extraherics: Fostering higher-order thinking \nskills in human-AI interaction.  \nZhai, X., Nyaaba, M., & Ma, W. (2024). Can generative AI and ChatGPT outperform humans on \ncognitive-demanding problem-solving tasks in science? Science and Education, 1–22.  \n  \n Journal of Educational Informatics, 2025 \n \nVol. 6(1) 2-23 \n   \n  https://doi.org/10.51357/jei.v6i1.314 \n  \n  \n23 \n \nAppendix A \nGrade Distribution Violin Plots \nFigure A1 \nViolin Plots of Grade Distributions by Reviewer LLM and Reviewed LLM \n \n",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.5724769830703735
    },
    {
      "name": "Psychology",
      "score": 0.48485028743743896
    },
    {
      "name": "Cognitive psychology",
      "score": 0.41725409030914307
    },
    {
      "name": "Computer science",
      "score": 0.3878073990345001
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}