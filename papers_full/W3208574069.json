{
  "title": "Pre-training Co-evolutionary Protein Representation via A Pairwise Masked Language Model",
  "url": "https://openalex.org/W3208574069",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100745388",
      "name": "Liang He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010641905",
      "name": "Shizhuo Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102750692",
      "name": "Lijun Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065326308",
      "name": "Huanhuan Xia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020656041",
      "name": "Fusong Ju",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100676073",
      "name": "He Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100406645",
      "name": "Siyuan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5021772140",
      "name": "Yingce Xia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053196390",
      "name": "Jianwei Zhu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5052433375",
      "name": "Pan Deng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101518422",
      "name": "Bin Shao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020025718",
      "name": "Tao Qin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5115592065",
      "name": "Tie‐Yan Liu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2783644078",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W3108423942",
    "https://openalex.org/W2123745527",
    "https://openalex.org/W2172242904",
    "https://openalex.org/W2111326065",
    "https://openalex.org/W3105713608",
    "https://openalex.org/W3139654928",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W2137566700",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2114340287",
    "https://openalex.org/W2013425283",
    "https://openalex.org/W2169478909",
    "https://openalex.org/W2967606876",
    "https://openalex.org/W3037620288",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2166701319",
    "https://openalex.org/W3193589100",
    "https://openalex.org/W2801109052",
    "https://openalex.org/W3101509328",
    "https://openalex.org/W1861406683",
    "https://openalex.org/W2944959599",
    "https://openalex.org/W3015531900",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W2173591891",
    "https://openalex.org/W2780572554",
    "https://openalex.org/W3157271872",
    "https://openalex.org/W2008545402",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W2909727437",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2951599627",
    "https://openalex.org/W2331911623",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W3121000782",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3083386021",
    "https://openalex.org/W2092572492",
    "https://openalex.org/W2994967700",
    "https://openalex.org/W3169392527",
    "https://openalex.org/W2161162779",
    "https://openalex.org/W2085075228",
    "https://openalex.org/W2898402099",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3035486570",
    "https://openalex.org/W2245592118",
    "https://openalex.org/W3114990254",
    "https://openalex.org/W2949867299",
    "https://openalex.org/W2123850444"
  ],
  "abstract": "Understanding protein sequences is vital and urgent for biology, healthcare, and medicine. Labeling approaches are expensive yet time-consuming, while the amount of unlabeled data is increasing quite faster than that of the labeled data due to low-cost, high-throughput sequencing methods. In order to extract knowledge from these unlabeled data, representation learning is of significant value for protein-related tasks and has great potential for helping us learn more about protein functions and structures. The key problem in the protein sequence representation learning is to capture the co-evolutionary information reflected by the inter-residue co-variation in the sequences. Instead of leveraging multiple sequence alignment as is usually done, we propose a novel method to capture this information directly by pre-training via a dedicated language model, i.e., Pairwise Masked Language Model (PMLM). In a conventional masked language model, the masked tokens are modeled by conditioning on the unmasked tokens only, but processed independently to each other. However, our proposed PMLM takes the dependency among masked tokens into consideration, i.e., the probability of a token pair is not equal to the product of the probability of the two tokens. By applying this model, the pre-trained encoder is able to generate a better representation for protein sequences. Our result shows that the proposed method can effectively capture the inter-residue correlations and improves the performance of contact prediction by up to 9% compared to the MLM baseline under the same setting. The proposed model also significantly outperforms the MSA baseline by more than 7% on the TAPE contact prediction benchmark when pre-trained on a subset of the sequence database which the MSA is generated from, revealing the potential of the sequence pre-training method to surpass MSA based methods in general.",
  "full_text": "PRE-TRAINING CO-EVOLUTIONARY PROTEIN REPRE -\nSENTATION VIA A P AIRWISE MASKED LANGUAGE\nMODEL\nLiang He\nMicrosoft Research Asia\nBeijing, China\nlihe@microsoft.com\nShizhuo Zhang∗\nNanyang Technological University\nJurong West, Singapore\nM180175@e.ntu.edu.sg\nLijun Wu\nMicrosoft Research Asia\nBeijing, China\nlijuwu@microsoft.com\nHuanhuan Xia, Fusong Ju\nMicrosoft Research Asia\nBeijing, China\n{lexi,fusongju}@microsoft.com\nHe Zhang∗\nXi’an Jiaotong University\nShaanxi, China\nmao736488798@stu.xjtu.edu.cn\nSiyuan Liu∗\nSun Yat-sen University\nGuangdong, China\nliusy8@mail2.sysu.edu.cn\nYingce Xia, Jianwei Zhu, Pan Deng, Bin Shao, Tao Qin, Tie-Yan Liu\nMicrosoft Research Asia\nBeijing, China\n{yinxia,jianwzhu,paden,binshao,taoqin,tyliu}@microsoft.com\nABSTRACT\nUnderstanding protein sequences is vital and urgent for biology, healthcare, and\nmedicine. Labeling approaches are expensive yet time-consuming, while the\namount of unlabeled data is increasing quite faster than that of the labeled data\ndue to low-cost, high-throughput sequencing methods. In order to extract knowl-\nedge from these unlabeled data, representation learning is of signiﬁcant value for\nprotein-related tasks and has great potential for helping us learn more about pro-\ntein functions and structures. The key problem in the protein sequence represen-\ntation learning is to capture the co-evolutionary information reﬂected by the inter-\nresidue co-variation in the sequences. Instead of leveraging multiple sequence\nalignment as is usually done, we propose a novel method to capture this informa-\ntion directly by pre-training via a dedicated language model, i.e.,Pairwise Masked\nLanguage Model (PMLM). In a conventional masked language model, the masked\ntokens (i.e. amino acid residues) are modeled by conditioning on the unmasked\ntokens only, but processed independently to each other. However, our proposed\nPMLM takes the dependency among masked tokens into consideration, i.e., the\nprobability of a token pair is not equal to the product of the probability of the\ntwo tokens. By applying this model, the pre-trained encoder is able to generate\na better representation for protein sequences. Our result shows that the proposed\nmethod can effectively capture the inter-residue correlations and improves the per-\nformance of contact prediction by up to9% compared to the MLM baseline under\nthe same setting. The proposed model also signiﬁcantly outperforms the MSA\nbaseline by more than 7% on the TAPE contact prediction benchmark when pre-\ntrained on a subset of the sequence database which the MSA is generated from,\nrevealing the potential of the sequence pre-training method to surpass MSA based\nmethods in general.\n1 I NTRODUCTION\nLife is ruled by biological sequences and molecules, i.e. DNA, RNA, and protein sequences, fol-\nlowing the de facto ‘natural’ language of biology. For protein, the structure is determined by the\n∗This work was conducted at Microsoft Research Asia\n1\narXiv:2110.15527v1  [cs.CL]  29 Oct 2021\nsequence, and its function is realized by the structure, according to Anﬁnsen’s dogma. However,\nthe structure and function label is never easy to obtain (time-consuming and expensive) nor al-\nways effective due to the dark regions and structure dynamics. On the other hand, the number of\nunlabeled protein sequences increases quite faster than that of the labeled ones, due to the large\ngap between low-cost, high-throughput sequencing methods and expensive yet time-consuming la-\nbeling approaches by time- and labor-intensive manual curation process (Consortium, 2019). Un-\nderstanding the protein sequences is vital to advance in biology, healthcare, and medicine. The\nprotein sequence representation has been exploited in various real applications, including for dark\nproteome (Perdig˜ao et al., 2015), where the ‘dark’ regions of proteins are never observed by exper-\nimental structure determination and inaccessible to homology modeling; therapies, such as cancer\ndiagnosis (Vazquez et al., 2009) and antibody design (Whitehead et al., 2012); function/property\nprediction for protein engineering, such as ﬁtness prediction (Hsu et al., 2021); phylogenetic in-\nference (Hie et al., 2021b); sequence mutations, such as learning mutational semantics (Hie et al.,\n2020), virus evolution and escape prediction (Hie et al., 2021a), and cancer diagnosis by mutation\nprediction (Reva et al., 2011; Martelotto et al., 2014), to name a few.\nCo-evolutionary information represented by the inter-residue co-variation is essential for protein se-\nquences in terms of both structure and function due to the evolution pressure from natural selection\n– sequences with more stable structure and more adequate function are usually remained. A rep-\nresentation that can capture this information from the sequences is beneﬁcial for understanding the\nmolecular machinery of life. This information is previously quantiﬁed by analyzing homologous\nsequences, a.k.a multiple sequence alignment (MSA), for the target sequence, where the homologs\nare retrieved from public protein sequence databases by applying curated procedure and intensive\ncomputation with customized tools and hyper-parameters. For protein function prediction, unsuper-\nvised models can be learned from these homologous sequences, such as mutational effect prediction\nfrom sequence co-variation (Hopf et al., 2017) and mutational landscape inference (Figliuzzi et al.,\n2016). It is demonstrated that incorporating inter-residue dependencies using a pairwise model that\ncan power the predictions to agree more with the mutational experiment observations (Mann et al.,\n2014; Boucher et al., 2016; Riesselman et al., 2017). The reason is that, the function of the sequence\nis the combination effect of the residues other than the sum of the properties of each residue (Hopf\net al., 2017). As the cost to quantify the co-effect of multiple residues is combinatorial while model-\ning their interactions is critical, pairwise co-variation analysis becomes the best choice, meanwhile,\nFigliuzzi et al. (2018) demonstrates that pairwise models are able to capture collective residue vari-\nability. For protein structure prediction, the key step is to predict inter-residue contacts/distances,\nwhile the shared cornerstone of prediction is performing evolutionary coupling analysis, i.e. residue\nco-evolution analysis, on the constructed MSA for a target protein (Ju et al., 2021). The underlying\nrational is that two residues which are spatially close in the three-dimensional structure tend to co-\nevolve, which in turns can be exploited to estimate contacts/distances between residues (Seemayer\net al., 2014; Jones & Kandathil, 2018). Although the contacts are usually rare in the residue pairs\nof the sequence, they are critical to rebuilding the 3D structure of the protein due to their roles of\nconstraints on the sketch.\nA natural question arises that, as both derived from unlabeled sequences, can we directly capture\nthis co-evolutionary information via sequence pre-training instead of extracting from MSA? To an-\nswer this question, we ﬁrst discuss the key ingredient missed by conventional language models,\nthen we propose a pairwise masked language model for the protein sequence representation learn-\ning. The key information to extract from the MSA (denoted as X) is the statistics for Q(xi,xj|X),\ntypically the co-variation for each pair, where i,j are two indexes of the tokens/residues in the se-\nquences. The intuition is that, if the two residues are independent, their co-variance will be close to\nzero, otherwise, it indicates the co-evolutionary relation between these two positions. While for the\nTable 1: Example Residue Probabilities Conditioned on X/{A,B}: Co-evolutionary vs. Independent\nA ̸⊥ B a1 a2 a3 P(B)\nb1 20% 20% - 40%\nb2 20% 10% - 30%\nb3 - - 30% 30%\nP(A) 40% 30% 30% 100%\nA ⊥ B a1 a2 a3 P(B)\nb1 16% 12% 12% 40%\nb2 12% 9% 9% 30%\nb3 12% 9% 9% 30%\nP(A) 40% 30% 30% 100%\nconventional masked language model, although they scan through all the same sequences as MSA-\n2\nbased methods and the losses for all the residues in the masked tokens will be back-propagated, they\nare processed in an accumulative way within the batches, i.e. independently updating the weights.\nThis means that P(xi|X/M) is modeled by the LMs. Table 1 presents an example for this kind of\nmethod, originating from NLP but sub-optimal for protein sequences due to the critical and much\nfrequent co-evolution relationship among the residues. Here, we argue that for the protein sequences\nP(xi,xj|X/M) ̸= P(xi|X/M) ·P(xj|X/M). Auto-regressive methods, i.e. generative pre-training\n(GPT), where the latter tokens are predicted by conditioning on the previous ones from the same\ndirection, still having the similar issue. Inspired by this, we design a pairwise masked language\nmodel and pre-train the model with a pairwise loss calculated from the protein sequences directly.\nFollowing the masked language model where the tokens of each sequence are picked at a probability\nand then be masked, replaced, or kept, our proposed model takes the masked sequences as input and\npredicts these original tokens, especially, the label for each masked token pair.\nTo examine the capability of the model to capture the co-evolutionary information, we select protein\ncontact prediction as our main downstream task due to its well-established relationship to the co-\nevolutionary information and the plentiful data that are publicly available and well measured by\nempirical experiment. Here, protein contact prediction is a binary classiﬁcation task for amino acid\nresidue pairs, a residue pair is called a contact if their distance is less than or equal to a distance\nthreshold, typically 8 ˚A (i.e., 8 ×10−10 m). The result shows that this method can signiﬁcantly\nimprove the learned representation on the contact prediction task, it even surpasses the MSA baseline\nwhen pre-trained on another sequence subset UR50 with the sequence-only input on the TAPE\nbenchmark. Meanwhile, an additional experimental evaluation for secondary structure prediction\nillustrates that the proposed model does not hurt performance, while signiﬁcant improvements are\nobserved in some other settings for remote homology prediction.\n2 R ELATED WORK\nEvolutionary Coupling Analysis Co-evolution information is closely correlated to the contacts,\ndue to the rational that two residues are likely co-evolving when they are spatially close. To this end,\ntechniques such as mutual information (MI) (Gloor et al., 2005) are exploited to quantify this feature.\nLater than MI, direct coupling analysis (DCA) proves to be more accurate and be widely adopted,\nfor example, EVfold (Sheridan et al., 2015), PSICOV (Jones et al., 2012), GREMLIN (Kamisetty\net al., 2013), plmDCA (Ekeberg et al., 2013), CCMpred (Seemayer et al., 2014) are all built on\nDCA. For protein sequences, the common idea of these methods is to use statistical modeling to\nquantify the strength of the direct relationship between two positions of a protein sequence, exclud-\ning effects from other positions. A variety of DCA methods are further proposed to generate the\ndirect couplings of residues by ﬁtting Potts models (Ekeberg et al., 2013) or precision matrix (Jones\net al., 2012) to MSAs, e.g. mean-ﬁeld DCA (Morcos et al., 2011), sparse inverse covariance (Jones\net al., 2012) and pseudo-likelihood maximization (Ekeberg et al., 2013; Balakrishnan et al., 2011;\nSeemayer et al., 2014).\nBy taking DCA-derived scores as features, deep neural networks based supervised methods sig-\nniﬁcantly outperform the unsupervised methods (Senior et al., 2020; Jones & Kandathil, 2018;\nWang et al., 2017; Xu, 2019; Yang et al., 2020). Most neural network models, including Al-\nphaFold (AlQuraishi, 2019) and RaptorX (Xu, 2019), rely on this feature. However, due to the\nconsiderable information loss after transforming MSAs into hand-crafted features, supervised mod-\nels, such as CopulaNet (Ju et al., 2021) and AlphaFold2 (Jumper et al., 2021), are proposed to di-\nrectly build on the raw MSA. The superior performance over the baselines demonstrates that residue\nco-evolution information can be mined from the raw sequences by the model. A noticeable draw-\nback of the MSA based methods is that, when the MSA quality (i.e. the number of the homologous\nsequences) is low for a target sequence, the model performance drops a lot.\nPre-Training Methods Following the pre-train and ﬁne-tune paradigm (Peters et al., 2018; Ken-\nton & Toutanova, 2019; Liu et al., 2019; Radford et al., 2018), various pre-training methods are\nproposed recently to learn better representations for protein sequences. TAPE (Rao et al., 2019)\nis built as a benchmark to evaluate the protein sequence pre-training method. It demonstrates the\nperformance can be improved by pre-training compared to one-hot representation, however, also\nindicates that the performance of the pre-training based models on pure sequences still lags behind\nthe alignment-based method in the downstream tasks.\n3\nAmong pre-training methods, RNNs are exploited as the pre-training model. Bepler & Berger (2019)\nuse two layers of Bi-LSTM as an extraction part of the original protein sequences by applying next\ntoken prediction for pre-training language modeling. UniRep (Alley et al., 2019) uses a similar\ntraining scheme as Bepler & Berger (2019), then use evo-turning (evolutionary ﬁne-tuning) to ad-\ndress the importance of evolutionary information for the protein embedding. UDSMProt (Strodthoff\net al., 2020) relies on an AWD-LSTM language model, which is a three-layer LSTM regularized by\ndifferent kinds of dropouts.\nA variety of techniques based on Transformer (Vaswani et al., 2017) are applied to build dedicated\nmodels for proteins. ESM (Rives et al., 2021) demonstrates that Transformer models can outper-\nform RNN based models, it also illustrates that the protein sequence diversity and model size have\nsigniﬁcant impacts on the pre-trained encoder performance. PRoBERTa (Nambiar et al., 2020) fol-\nlows RoBERTa to pre-train the model with Byte Pair Encoding and other optimization techniques.\nLu et al. (2020) applies contrastive learning by mutual information maximization to the pre-training\nprocedure, MSA Transformer (Sturmfels et al., 2020) learns the representation on the MSAs for a\nprotein sequence, however, it relies on expensive database searches to generate the required MSAs\nfor each sequence.\nLarge-scale models are explored due to the fact that protein sequence datasets are large in size\nand complex in interaction. ProtTrans (Elnaggar et al., 2020) trains the protein LMs (pLMs) on\nthe Summit supercomputer using more than 4 thousand GPUs and TPU Pod up-to 1024 cores, the\nmost informative embeddings even outperform state-of-the-art models without multiple sequence\nalignments (MSAs) for the secondary structure prediction. Xiao et al. (2021) also demonstrate that\nsequence evolution information can be accurately captured by a large-scale model from pre-training,\nup to 3 billion parameters pre-trained on a 480 GPUs cluster.\nAdditional supervised labels are exploited for protein sequence pre-training. PLUS (Min et al.,\n2019) tries to model the protein sequence with masked language model together with an auxiliary\ntask, i.e. same family prediction, in their work. Sturmfels et al. (2020) add a pre-training task named\nproﬁle prediction for pre-training.\nGenerative pre-training is also exploited for protein engineering, for example, ProGen (Madani et al.,\n2020) is a generative model conditioned on taxonomic information as an unsupervised sequence\ngeneration problem in order to leverage the exponentially growing set of proteins that lack costly,\nstructural annotations.\nThe outputs of the pre-trained language models can be used in different ways. For example, Vig et al.\n(2020), Rao et al. (2020), and Bhattacharya et al. (2020) demonstrate that the attention weights from\nthe pre-trained models have a strong correlation with the residue contacts. Hie et al. (2021b) analyze\nthe correlation between the pseudo likelihood of the mutated sequences predicted by the language\nmodel and the evolution of the mutations, the results suggests that pre-trained language models on\nprotein sequences are able to predict evolutionary order at different timescales. Hie et al. (2021a;\n2020) apply pre-trained protein language models to predict mutational effects and virus escape.\n3 M ETHOD\nGiven the sequence data D= {X}, where X = (x1,x2,...,x N), language models can be built on\nthe sequences for pre-training.\n3.1 P AIRWISE MASKED LANGUAGE MODEL\nThe loss function of vanilla Masked Language Model (MLM) can be formulated as follows:\nLmlm = EX∼D\n(\nEM\n∑\ni∈M\n(\n−log Pθ(xi|X/M)\n)\n)\nwhere Dis the sequence set, X is a sequence in D, X/M represents the masked sequence of X\nwhere the masked token indices are in M, xi stands for the i-th token in the sequence X, and θ\ndenotes the encoder parameters.\n4\nIn this paper, we propose a novel Pairwise Masked Language Model (PMLM) whose loss function\ncan be written as:\nLpmlm = EX∼D\n\nEM\n∑\ni,j∈M,i̸=j\n(\n−log Pθ(xi,xj|X/M)\n)\n\n\nThe combined loss for both MLM and PMLM can be used for pre-training as:\nL= Lmlm + λ·Lpmlm\nwhere λis a weight coefﬁcient to balance two losses. In this paper, λis set to 1 during pre-training\nthe PMLM models. As we can see, all the MLM and PMLM labels for pre-training are from the\nsequences themselves, thus still self-supervised. For multiple rounds of updates, if i-th and j-th\npositions are independent, Pθ(xi,xj|X/M) degenerates to Pθ(xi|X/M) ·Pθ(xj|X/M). When they\nare co-evolutionary, the model learns a different distribution from the independent case.\n3.2 M ODEL ARCHITECTURE\nThe goal of pre-training is to build a good protein sequence encoder that generates better represen-\ntation. The pre-training model mainly consists of a protein sequence encoder and two prediction\nheads, i.e., a token prediction head and a pair prediction head. The sequence encoder is built on\nstacked Transformer encoder layers. The overview of our model is illustrated in Figure 1. Trans-\nformer is believed to be a powerful tool for modeling sequence data and has been applied to various\ntasks, including natural language understanding, question answering, computer vision, and so on.\nThus we exploit a Transformer encoder as the sequence encoder of our model. The sequence encoder\ntakes raw protein sequences as input and converts them into their vector representations. The model\nwas trained on protein sequences using both masked token prediction (MLM) and masked pair pre-\ndiction (PMLM) for protein language modeling. Each prediction head is a MLP, i.e. two-layer\nfully-connected (FC) neural network, where the output is mapped into the vocabulary via softmax,\ni.e. 20 amino acid residues (denoted as Vres) for the token prediction head and400 amino acid pairs\n(denoted as Vpair) for the pair prediction head.\n3.3 M ASKED PAIR PREDICTION\nThe masked language model lets the model to reconstruct the masked tokens conditional on the\nother tokens in the sequences. Predicting masked pairs follows the same idea of the masked token\nprediction, however, with losses for the pairs. A two-layer FC neural network is exploited for the\nprediction of masked pairs, whereas another one for that of masked tokens. During pre-training, the\ndot and difference of the vectors for the residue pair are concatenated before feeding into the pair\nprediction head. Then, the output is mapped into a vector of dimension |Vpair|by softmax. This\nprediction is ﬁnally compared with the pair label from the sequence itself.\nIt is not trivial to generate pair labels from multiple masked tokens. The pairwise label construction\nprocess is demonstrated in Figure 2. For each pair xi,xj for the masked token where i ̸= j, a\npairwise label is generated as wij = (xi,xj), where xi,xj ∈Vres and wij ∈Vpair. Obviously, we\nhave |Vpair|= |Vres|2 for Vres and Vpair. The pairs for xi,xj whose i = j are ignored since they\nare already a part of the MLM loss. Notably, i ̸= j does not necessarily mean xi ̸= xj since two\ndifferent positions may be the same residue. For each valid pair, the pair prediction head generates\nthe probability overVpair. For a masked sequenceX/M, the size of the pairwise labels is|M|2−|M|\nwhile each label is a one-hot encoding for Vpair.\nIntuitively, the masked pair loss is consistent with the masked token loss. For example, the loss\ncan be treated as a combined loss for the two masked tokens if the two positions within the pair\nare independent, otherwise this masked pair loss provides extra information of the sequence besides\neach position.\n5\nMet Ala Gly Ser Phe ... Gln[SOS] [EOS]Met Ala Gly Ser Phe ... Gln[SOS] [EOS]\nMet Ala [Mask] Ser [Mask] ... [Mask][SOS] [EOS]Met Ala [Mask] Ser [Mask] ... [Mask][SOS] [EOS]\nEMet EAla E[Mask] ESer E[Mask] ... E[Mask]E[SOS] E[EOS]EMet EAla E[Mask] ESer E[Mask] ... E[Mask]E[SOS] E[EOS]\nTransformer Encoder Layer × N\nLinear\nLinear\n Linear\nLinear\n× \n +\nResidue Pair\nResidue\nPMLM Head\nMLM Head\n× (G, F) (G, Q)\n(F, G)\n(Q, G)\n× (F, Q)\n(Q, F) ×\n× (G, F) (G, Q)\n(F, G)\n(Q, G)\n× (F, Q)\n(Q, F) ×\nG\n...\nQ\nG\n...\nQ\nFigure 1: Overview of the Pairwise Masked Language Model\n3.4 F INE -TUNING FOR CONTACT PREDICTION\nThe generated encoder is further ﬁne-tuned on a given supervised dataset for contact prediction. As\nthe prevalent approach does, different neural networks can be built on top of the pre-trained model\nfor further ﬁne-tuning, whereas, a simple FC layer followed by a softmax operator can be applied\nto evaluate the representation. For the residue pairs task, e.g. contact prediction, the ﬁne-tuning\nnetwork is built on top of the ﬁrst FC layer of the pair prediction head, other tasks are ﬁne-tuned on\nthe encoder outputs.\n4 E XPERIMENT EVALUATION\n4.1 E XPERIMENT SETUP\nTo evaluate the performance of the pairwise masked language model, several models with differ-\nent settings are pre-trained on two prevalent datasets, i.e. Pfam (El-Gebali et al., 2019) and UR50\n(release 2018 03). Following the RoBERTa-base setting, the hidden size, feed forward dimension,\nnumber of encoder layers, and attention heads of the base models are set as 768, 3072, 12, 12 re-\nspectively (denoted as MLM-base for MLM and PMLM-base for PMLM). A larger model named\nPMLM-large is pre-trained with the same setting except using 34 as the number of encoder layers.\nMoreover, the largest model with a hidden size of 1280 and a number of encoder layers of 36 is\nalso pre-trained on UR50, denoted as PMLM-xl. The implementation is optimized to speed up the\ntraining procedure, e.g., the pairwise loss is applied alone by enabling the diagonal elements and dis-\nabling the token prediction head. MLM-base and PMLM-base are pre-trained with maximum length\n512, while PMLM-large is pre-trained with a maximum length of 1024. The positional encoding\nof both models are non-learnable. MLM-base and PMLM-base are pre-trained using the Adam op-\ntimizer (0.9,0.98) with peak learning rate 0.0003 and clip norm 1.0, the learning rate scheduler\nis a polynomial decay scheduler where the learning rate is decreased linearly after warming up by\n20,000 steps to the peak. The hyper-parameters are almost the same for pre-training PMLM-large\n6\n× √ √\n√ × √\n√ √ ×\n× √ √\n√ × √\n√ √ ×\nMet Gly Gly Ala Ser Phe GlnMet Gly Gly Ala Ser Phe Gln\nMetGlyGlyAlaSerPheGln MetGlyGlyAlaSerPheGln\nSequence\n× (G, A) (G, S)\n(A, G)\n(S, G)\n× (A, S)\n(A, S) ×\n× (G, A) (G, S)\n(A, G)\n(S, G)\n× (A, S)\n(A, S) ×\n× (G, A) (G, S)\n(A, G)\n(S, G)\n× (A, S)\n(A, S) ×\nR Residue\nR [MASK]\n√ Pair Used\n× Pair Unused\nPairwise Label\nResidue \nPairs\nFigure 2: Pairwise Label Construction from the Masked Sequence\nexcept that the peak learning rate is set to 0.0001. MLM-base, PMLM-base, and PMLM-large are\npre-trained on 24 Tesla V100 GPU cards, about three weeks for MLM-base/PMLM-base and about\nseven weeks for PMLM-large. PMLM-xl is pre-trained on 16 Tesla V100 GPU cards for more than\ntwo weeks.\nTwo groups of experiments are conducted to examine the generated model on the TAPE contact\nprediction test set (denoted as TAPE-Contact) and the RaptorX contact prediction test set (denoted\nas RaptorX-Contact). The models are all evaluated on or ﬁne-tuned from the checkpoint of the six-\ntieth epoch, except PMLM-base on UR50, which is ﬁne-tuned from that of the ninetieth epoch. For\nTAPE-Contact, we use a shallow decoder as TAPE does, i.e. simply use an outer-dot and outer-\ndifference of each pair followed by a linear projection with softmax, for the sequence representation\nto evaluate the representation. To see the potential of the representations, RaptorX-Contact is eval-\nuated on a more expressive decoder, i.e., a ResNet with stacked convolution blocks, on top of the\npre-trained encoder as ESM does, however, the convolution blocks are not dilated. The ﬁne-tuning\nprocess for each model on each dataset is ﬁnished on one Tesla V100 GPU card.\n4.2 P RE-TRAINING VALIDATION\nThe two heads of the PMLM model output P(xi,xj) and P(xi),P(xj) separately, this raises an-\nother question that if they have already learned the difference between the independent and de-\npendent positions of the sequence. To answer this question, we aggregate the validation losses\nand accuracy scores of pre-training on the Pfam and UR50 sequence datasets, as shown in Ta-\nble 2. As we can see, even the ﬁne-tuned performance of PMLM-base is better than that of MLM-\nbase, the loss Lmlm of PMLM-base is slightly higher than that of MLM-base. We further use\n∆ACC = ACCpmlm −ACC2\nmlm to quantify the difference. For PMLM-base and PMLM-large on\nPfam and UR50, ∆ACC is not negligible as an expectation, which indicates the difference of the\ncorrect probability for the most likely residue on each position. The joint probability is quite dif-\n7\nferent to the multiplication of the individual probability, illustrating that PMLM is able to capture\nthe co-evolutionary information via pre-training on pure sequences. This observation can be further\nvalidated by the histograms of KL(P(xi) ·P(xj) ||P(xi,xj)) of the PMLM-large model on the\nPfam and UR50 validation sequences, which is shown in Figure 3, each(xi,xj) pair of the sequence\nis masked when predicting the probabilities.\nTable 2: Validation Losses and Accuracy Scores of Pre-training\nMODEL DATA Lmlm Lpmlm ACCmlm ACCpmlm ∆ACC ∆ACC / ACCpmlm\nMLM-base Pfam 1.6475 - 48.0% - - -\nPMLM-base Pfam 1.6728 3.3446 47.1% 22.4% 0.22% 1.0%\nMLM-base UR50 2.2725 - 32.0% - - -\nPMLM-base UR50 2.2784 4.5557 31.8% 10.9% 0.79% 7.2%\nPMLM-large UR50 2.1710 4.3419 35.1% 13.2% 0.88% 6.7%\nPMLM-xl UR50 - 3.9746 37.3% 16.9% 2.95% 17.5%\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nKL Divergence\n10 7\n10 5\n10 3\n10 1\nFrequency\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nKL Divergence\n10 7\n10 5\n10 3\n10 1\nFrequency\nFigure 3: KL Divergence of PMLM-large on Pfam Valid (Left) and UR50 Valid (Right)\n4.3 C OMPARISON ON TAPE-C ONTACT\nFor a fair comparison, the models are evaluated on the TAPE-Contact with the same contact predic-\ntion head, which is an FC layer with a0.5 dropout followed by a softmax operator. Medium and long\nrange precision at L/5 is used for evaluation, where medium and long range means the there are at\nleast 12 residues between the pair to test. With precision at L/5, the top L/5 contact predictions are\ncompared to the ground-truth, where Lis the length of the input sequence.\nTo better control the setting, a Transformer encoder for MLM (denoted as MLM-base) as well as a\nTransformer encoder (denoted as PMLM-base) are pre-trained on the Pfam and UR50 datasets. The\nhyper-parameters for these models resemble that of the RoBERTa-base for NLP, i.e., hidden size as\n768, feed forward dimension as 3072, and number of layers as 12. To further evaluate the effect of\nPMLM, we also pre-train a deeper Transformer encoder with the number of layers increased to 34,\ndenoted as PMLM-large.\nThe ﬁne-tuned performance of the models are compared in Table 3. The performance of TAPE\ntransformer and MSA baseline are reported in the paper. As we can see, the performance is increased\nfrom 36.0 for TAPE to 37.3 for MLM-base. Comparing the two models which is close in model\nsize pre-trained on Pfam, i.e. MLM-base and PMLM-base, a huge performance gain over 9% is\nwitnessed, demonstrating the positive effect of the PMLM for pre-training.\nNotably, PMLM-xl pre-trained on UR50 can even outperform the MSA baseline (about 8%), al-\nthough UR50 is a small subset of UniParc + Metagenome, which shields the light for the single\nsequence pre-training methods to exceed the MSA based methods.\n8\nTable 3: Medium-/Long- Range Precision@L/5 on TAPE Contact Prediction Test Set. The predic-\ntions are generated from a linear projection of the encoder outputs during ﬁne-tuning. UR50 is a\nsmall subset of UniParc + Metagenome.\nMODEL #PARAMS DATA PRE-TRAIN TASK RESULT\nTAPE Transformer 38M Pfam MLM 36.0\nMLM-base 85M Pfam MLM 37.3\nPMLM-base (ours) 87M Pfam MLM + PMLM 47.2\nPMLM-base (ours) 87M UR50 MLM + PMLM 57.7\nPMLM-large (ours) 250M UR50 MLM + PMLM 66.7\nPMLM-xl (ours) 713M UR50 MLM + PMLM 71.7\nMSA baseline - UniParc + Metagenome - 64.0\n4.4 C OMPARISON ON RAPTOR X-C ONTACT\nThe models are evaluated on the RaptorX-Contact with the same contact prediction module, how-\never, with different contact prediction modules, customized networks may be optimal for different\npre-trained encoder.\nAs shown in Table 4, PMLM-base outperforms ESM Transformer-12 by about 4% given that their\nmodel sizes are close, demonstrating the effect of PMLM for pre-training again. When compar-\ning the larger models of a close model size pre-trained, PMLM-xl signiﬁcantly outperforms ESM\nmodels (i.e., more than 9% for ESM Transformer-34 and 3% for ESM-1b) even that a systematic\nhyper-parameter searching on a 100M model is conducted to generate the hyper-parameters for\nESM-1b, which is not performed for PMLM-xl, showing the superior performance of the PMLM\nencoder.\nTable 4: Long-Range Precision@Lon RaptorX Contact Prediction Test Set. A ResNet with stacked\nconvolution blocks is built on top of each pre-trained encoder during ﬁne-tuning. The performance\nnumbers of the models except PMLM are reported by Rives et al. (2021).∗The PMLM-xl checkpoint\nﬁne-tuned from is pre-trained by a half of GPU time as that of the ESM-1b model does.\nMODEL #PARAMS DATA PRE-TRAIN TASK RESULT\nTAPE Transformer 38M Pfam MLM 23.2\nUniRep 18M UR50 Autoregressive 21.9\nSeqVec 93M UR50 Autoregressive 29.0\nESM Transformer-12 85M UR50 MLM 37.7\nPMLM-base (ours) 87M UR50 MLM + PMLM 41.6\nESM Transformer-34 669M UR50 MLM 50.2\nESM-1b 650M UR50 MLM 56.9\nPMLM-xl (ours) 715M UR50 MLM + PMLM 59.9∗\n4.5 A BLATION STUDY\nTo study the factors which inﬂuence the model performance, a systemic ablation study is conducted.\nPRE-TRAINING DATA Pfam is a dataset with protein sequences from a few thousand families,\nwhile UR50 consists of much more diverse sequences, i.e., sequences from UniRef with 50% se-\nquence identity. In other words, UR50 is more diverse than Pfam and representative for more se-\nquences. As shown in Table 3, when PMLM-base is pre-trained on UR50, which consists of se-\nquences with higher diversity, instead of Pfam, the performance gain for the task is more than 10%,\nincreased from 47.2 to 57.7. As shown in Table 5,\nMODEL SIZE Model under-ﬁtting is observed in both ESM model pre-training and ours. A\nstraightforward way to tackle this is by increasing the model size. To examine this factor, we com-\npare two models with different sizes pre-trained on the same data, i.e. PMLM-base and PMLM-\nlarge. As shown in Table 3, when PMLM-base and PMLM-large are both pre-trained on UR50, the\nperformance gap is about 14% between 57.7 (PMLM-base) and 71.7 (PMLM-xl), illustrating that\n9\nTable 5: Ablation Study on RaptorX Contact Prediction Test Set (Long-Range Precision@L)\nMODEL #PARAMS DATA PRE-TRAIN TASK RESULT\nMLM-base (+ Linear) 85M Pfam MLM 19.8\nPMLM-base (+ Linear) 85M Pfam MLM + PMLM 25.2\nMLM-base (+ Linear) 85M UR50 MLM 36.9\nPMLM-base (+ Linear) 87M UR50 MLM + PMLM 39.2\nPMLM-base (+ ResNet) 88M UR50 MLM + PMLM 41.6\nPMLM-large (+ ResNet) 252M UR50 MLM + PMLM 54.1\nPMLM-xl (+ ResNet) 715M UR50 MLM + PMLM 59.9\nincreasing model size indeed beneﬁts the performance for contact prediction. Performance increases\nby more than 18% as shown in Table 5 when comparing PMLM-xl (+ ResNet) with PMLM-base (+\nResNet). Both are pre-trained on UR50.\nDOWNSTREAM TASK MODULE On top of the pre-trained encoder, there are various methods to\nﬁne-tune the model for downstream tasks. To study the impact of this factor, we also compare the\npre-trained encoder PMLM-base with two different downstream modules, namely, a simple linear\nlayer (+ Linear) and an eight-layer ResNet with convolution blocks (+ ResNet). As shown in Table 4,\nby comparing the performance of PMLM-base (+ Linear) and PMLM-base (+ ResNet), we can see\nthat the precision score is increased from 39.2 to 41.6, indicating that a more expressive model can\nbe trained to improve the performance on the ever pre-trained encoder.\nTable 6: Secondary Structure Prediction on the TAPE Benchmark\nMODEL CB513 CASP12 TS115\nTAPE Transformer 0.730 0.710 0.770\nPMLM-base (pre-trained on Pfam, ours) 0.728 0.706 0.771\nTable 7: Remote Homology Prediction on the TAPE Benchmark\nMODEL FOLD SUPERFAMILY FAMILY\nTAPE Transformer 0.210 0.340 0.880\nPMLM-base (pre-trained on Pfam, ours) 0.199 0.446 0.946\nOTHER DOWNSTREAM TASKS To evaluate our pre-trained encoder on other tasks, we also com-\npared the performance of PMLM-base and the TAPE Transformer for Secondary Structure Pre-\ndiction and Remote Homology Prediction on the TAPE benchmark, the results in terms of accuracy\nscores are listed in Tables 6 and 7. The performance numbers of PMLM-base for secondary structure\nprediction are quite close to that of the TAPE baseline Transformer, illustrating that the proposed\nmodel does not hurt the performance with the additional loss. For Remote Homology Prediction, the\nperformance gap varies on the fold-level, superfamily-level, and family-level holdout test sets. The\nperformance of PMLM-base is slightly worse than that of the TAPE Transformer on the fold-level\ntest set, however, much better on the superfamily-level and family-level test sets up to 10%, showing\nthe potential improvement from the extracted co-evolutionary information on other tasks.\n5 D ISCUSSION\nIn this paper, we propose a novel model named pairwise masked language model for the protein\nsequence encoder to capture co-evolutionary information during pre-training. The pre-trained model\ngenerates a better representation for global structure by applying this model. Our result shows\nthat the proposed method signiﬁcantly improves the performance of contact prediction compared to\nthe baselines. Meanwhile, the proposed model surpasses the MSA baseline on the TAPE contact\nbenchmark. Although the performance is encouraging, the potential capability of the proposed\nmodel, i.e. PMLM, is not fully developed, neither it is the only way to extract the co-evolutionary\n10\ninformation from the sequences. For example, an observation is that increasing the mask probability\nfor the tokens might improve the representation for PMLM.\nAlthough it sheds light on the single sequence based methods for representation learning for pro-\nteins, more endeavors are needed to push forward the pre-training methods for protein sequences.\nFollowing the idea of PMLM, supervision from multiple residues, e.g. a Triple Masked Language\nModel, might also be helpful for pre-training the representation. The metaphor is that three points in\nEuclidean space follows the triangle in-equations inspired by AlphaFold2. However, the storage and\ncomputation cost will be cubic to the count of the masked tokens, thus how to efﬁciently pre-train\nthe model might be an important issue to tackle, which will be left to future work.\n6 A CKNOWLEDGMENTS\nWe would like to thank Dahai Cheng for his valuable work on initial analysis.\nREFERENCES\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.\nUniﬁed rational protein engineering with sequence-based deep representation learning. Nature\nMethods, 16(12):1315–1322, 2019.\nMohammed AlQuraishi. Alphafold at casp13. Bioinformatics, 35(22):4862–4865, 2019.\nSivaraman Balakrishnan, Hetunandan Kamisetty, Jaime G Carbonell, Su-In Lee, and Christo-\npher James Langmead. Learning generative models for protein fold families. Proteins: Structure,\nFunction, and Bioinformatics, 79(4):1061–1078, 2011.\nTristan Bepler and Bonnie Berger. Learning protein sequence embeddings using information from\nstructure. In ICLR 2019 : 7th International Conference on Learning Representations, 2019.\nNicholas Bhattacharya, Neil Thomas, Roshan Rao, Justas Daupras, Peter Koo, David Baker, Yun S\nSong, and Sergey Ovchinnikov. Single layers of attention sufﬁce to predict protein contacts.\nbioRxiv, 2020.\nJeffrey I Boucher, Daniel NA Bolon, and Dan S Tawﬁk. Quantifying and understanding the ﬁtness\neffects of protein mutations: Laboratory versus nature. Protein Science, 25(7):1219–1226, 2016.\nUniProt Consortium. Uniprot: a worldwide hub of protein knowledge. Nucleic acids research, 47\n(D1):D506–D515, 2019.\nMagnus Ekeberg, Cecilia L¨ovkvist, Yueheng Lan, Martin Weigt, and Erik Aurell. Improved contact\nprediction in proteins: using pseudolikelihoods to infer Potts models. Physical Review E, 87(1):\n012707, 2013.\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R Eddy, Aur´elien Luciani, Simon C Potter, Mat-\nloob Qureshi, Lorna J Richardson, Gustavo A Salazar, Alfredo Smart, et al. The pfam protein\nfamilies database in 2019. Nucleic acids research, 47(D1):D427–D432, 2019.\nA. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, W. Yu, L. Jones, T. Gibbs, T. Feher, C. Angerer,\nM. Steinegger, D. Bhowmik, and B. Rost. ProtTrans: Towards cracking the language of lifes code\nthrough self-supervised deep learning and high performance computing. IEEE Transactions on\nPattern Analysis and Machine Intelligence, (01):1–1, jul 2020. ISSN 1939-3539. doi: 10.1109/\nTPAMI.2021.3095381.\nMatteo Figliuzzi, Herv ´e Jacquier, Alexander Schug, Oliver Tenaillon, and Martin Weigt. Coevo-\nlutionary landscape inference and the context-dependence of mutations in beta-lactamase tem-1.\nMolecular biology and evolution, 33(1):268–280, 2016.\nMatteo Figliuzzi, Pierre Barrat-Charlaix, and Martin Weigt. How pairwise coevolutionary models\ncapture the collective residue variability in proteins? Molecular biology and evolution , 35(4):\n1018–1027, 2018.\n11\nGregory B Gloor, Louise C Martin, Lindi M Wahl, and Stanley D Dunn. Mutual information in\nprotein multiple sequence alignments reveals two classes of coevolving positions. Biochemistry,\n44(19):7156–7165, 2005.\nBrian Hie, Ellen Zhong, Bryan Bryson, and Bonnie Berger. Learning mutational seman-\ntics. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-\nvances in Neural Information Processing Systems , volume 33, pp. 9109–9121. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n6754e06e46dfa419d5afe3c9781cecad-Paper.pdf.\nBrian Hie, Ellen D. Zhong, Bonnie Berger, and Bryan Bryson. Learning the language of viral\nevolution and escape. Science, 371(6526):284–288, 2021a. ISSN 0036-8075. doi: 10.1126/\nscience.abd7331. URL https://science.sciencemag.org/content/371/6526/\n284.\nBrian L Hie, Kevin K Yang, and Peter S Kim. Evolutionary velocity with protein language models.\nbioRxiv, 2021b.\nThomas A Hopf, John B Ingraham, Frank J Poelwijk, Charlotta PI Sch¨arfe, Michael Springer, Chris\nSander, and Debora S Marks. Mutation effects predicted from sequence co-variation. Nature\nbiotechnology, 35(2):128–135, 2017.\nChloe Hsu, Hunter Nisonoff, Clara Fannjiang, and Jennifer Listgarten. Combining evolutionary and\nassay-labelled data for protein ﬁtness prediction. bioRxiv, 2021.\nDavid T Jones and Shaun M Kandathil. High precision in protein contact prediction using fully\nconvolutional neural networks and minimal sequence features. Bioinformatics, 34(19):3308–\n3315, 2018.\nDavid T Jones, Daniel W A Buchan, Domenico Cozzetto, and Massimiliano Pontil. PSICOV: precise\nstructural contact prediction using sparse inverse covariance estimation on large multiple sequence\nalignments. Bioinformatics, 28(2):184–190, 2012.\nFusong Ju, Jianwei Zhu, Bin Shao, Lupeng Kong, Tie-Yan Liu, Wei-Mou Zheng, and Dongbo Bu.\nCopulanet: Learning residue co-evolution directly from multiple sequence alignment for protein\nstructure prediction. Nature communications, 12(1):1–9, 2021.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with alphafold. Nature, pp. 1–11, 2021.\nHetunandan Kamisetty, Sergey Ovchinnikov, and David Baker. Assessing the utility of coevolution-\nbased residue–residue contact predictions in a sequence-and structure-rich era. Proceedings of\nthe National Academy of Sciences, 110(39):15674–15679, 2013.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. InProceedings of NAACL-HLT, pp. 4171–\n4186, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nAmy X Lu, Haoran Zhang, Marzyeh Ghassemi, and Alan M Moses. Self-supervised contrastive\nlearning of protein representations by mutual information maximization. BioRxiv, 2020.\nAli Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi,\nPo-Ssu Huang, and Richard Socher. ProGen: Language modeling for protein generation. arXiv\npreprint arXiv:2004.03497, 2020.\nJaclyn K Mann, John P Barton, Andrew L Ferguson, Saleha Omarjee, Bruce D Walker, Arup\nChakraborty, and Thumbi Ndung’u. The ﬁtness landscape of hiv-1 gag: advanced modeling\napproaches and validation of model predictions by in vitro testing. PLoS computational biology,\n10(8):e1003776, 2014.\n12\nLuciano G Martelotto, Charlotte KY Ng, Maria R De Filippo, Yan Zhang, Salvatore Piscuoglio, Ray-\nmond S Lim, Ronglai Shen, Larry Norton, Jorge S Reis-Filho, and Britta Weigelt. Benchmarking\nmutation effect prediction algorithms using functionally validated cancer-related missense muta-\ntions. Genome biology, 15(10):1–20, 2014.\nSeonwoo Min, Seunghyun Park, Siwon Kim, Hyun-Soo Choi, and Sungroh Yoon. Pre-training of\ndeep bidirectional protein sequence representations with structural information. arXiv preprint\narXiv:1912.05625, 2019.\nFaruck Morcos, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S Marks, Chris Sander,\nRiccardo Zecchina, Jos ´e N Onuchic, Terence Hwa, and Martin Weigt. Direct-coupling analysis\nof residue coevolution captures native contacts across many protein families. Proceedings of the\nNational Academy of Sciences, 108(49):E1293–E1301, 2011.\nAnanthan Nambiar, Maeve Heﬂin, Simon Liu, Sergei Maslov, Mark Hopkins, and Anna Ritz. Trans-\nforming the language of life: transformer neural networks for protein prediction tasks. In Pro-\nceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology\nand Health Informatics, pp. 1–8, 2020.\nNelson Perdig ˜ao, Julian Heinrich, Christian Stolte, Kenneth S. Sabir, Michael J. Buckley, Bruce\nTabor, Beth Signal, Brian S. Gloss, Christopher J. Hammang, Burkhard Rost, Andrea Schaf-\nferhans, and Se ´an I. O’Donoghue. Unexpected features of the dark proteome. Proceedings\nof the National Academy of Sciences , 112(52):15898–15903, 2015. ISSN 0027-8424. doi:\n10.1073/pnas.1508380112. URL https://www.pnas.org/content/112/52/15898.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT,\npp. 2227–2237, 2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter\nAbbeel, and Yun Song. Evaluating Protein Transfer Learning with TAPE. In NeurIPS 2019 :\nThirty-third Conference on Neural Information Processing Systems, pp. 9689–9701, 2019.\nRoshan Rao, Sergey Ovchinnikov, Joshua Meier, Alexander Rives, and Tom Sercu. Transformer\nprotein language models are unsupervised structure learners. bioRxiv, 2020.\nBoris Reva, Yevgeniy Antipin, and Chris Sander. Predicting the functional impact of protein muta-\ntions: application to cancer genomics. Nucleic Acids Research, 39(17):e118–e118, 07 2011. ISSN\n0305-1048. doi: 10.1093/nar/gkr407. URL https://doi.org/10.1093/nar/gkr407.\nAdam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of genetic\nvariation capture mutation effects. arXiv preprint arXiv:1712.06527, 2017.\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,\nMyle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences. Proceedings of the National\nAcademy of Sciences, 118(15), 2021.\nStefan Seemayer, Markus Gruber, and Johannes S ¨oding. CCMpred—fast and precise prediction of\nprotein residue–residue contacts from correlated mutations. Bioinformatics, 30(21):3128–3130,\n2014.\nAndrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,\nChongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein\nstructure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020.\nRobert Sheridan, Robert J Fieldhouse, Sikander Hayat, Yichao Sun, Yevgeniy Antipin, Li Yang,\nThomas Hopf, Debora S Marks, and Chris Sander. Evfold. org: Evolutionary couplings and\nprotein 3d structure prediction. biorxiv, pp. 021022, 2015.\n13\nNils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek. UDSMProt: universal deep\nsequence models for protein classiﬁcation. Bioinformatics, 36(8):2401–2409, 2020.\nPascal Sturmfels, Jesse Vig, Ali Madani, and Nazneen Fatema Rajani. Proﬁle prediction: An\nalignment-based pre-training task for protein sequence models.arXiv preprint arXiv:2012.00195,\n2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nEsther Vazquez, Neus Ferrer-Miralles, Ramon Mangues, Jose L Corchero, Simo Schwartz Jr, Anto-\nnio Villaverde, et al. Modular protein engineering in emerging cancer therapies. Current phar-\nmaceutical design, 15(8):893–916, 2009.\nJesse Vig, Ali Madani, Lav R Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema Ra-\njani. Bertology meets biology: Interpreting attention in protein language models. arXiv preprint\narXiv:2006.15222, 2020.\nSheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, and Jinbo Xu. Accurate de novo prediction of protein\ncontact map by ultra-deep learning model. PLoS computational biology, 13(1):e1005324, 2017.\nTimothy A Whitehead, Aaron Chevalier, Yifan Song, Cyrille Dreyfus, Sarel J Fleishman, Cecilia\nDe Mattos, Chris A Myers, Hetunandan Kamisetty, Patrick Blair, Ian A Wilson, et al. Optimiza-\ntion of afﬁnity, speciﬁcity and function of designed inﬂuenza inhibitors using deep sequencing.\nNature biotechnology, 30(6):543–548, 2012.\nYijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, and Jie Tang. Modeling protein using large-\nscale pretrain language model. arXiv preprint arXiv:2108.07435, 2021.\nJinbo Xu. Distance-based protein folding powered by deep learning. Proceedings of the National\nAcademy of Sciences, 116(34):16856–16865, 2019.\nJianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David\nBaker. Improved protein structure prediction using predicted interresidue orientations. Proceed-\nings of the National Academy of Sciences, 117(3):1496–1503, 2020.\n14",
  "topic": "Pairwise comparison",
  "concepts": [
    {
      "name": "Pairwise comparison",
      "score": 0.729576587677002
    },
    {
      "name": "Computer science",
      "score": 0.7113338112831116
    },
    {
      "name": "Security token",
      "score": 0.5890511870384216
    },
    {
      "name": "Language model",
      "score": 0.55832439661026
    },
    {
      "name": "Representation (politics)",
      "score": 0.5536459684371948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5524661540985107
    },
    {
      "name": "Sequence labeling",
      "score": 0.5198410153388977
    },
    {
      "name": "Machine learning",
      "score": 0.48780587315559387
    },
    {
      "name": "Natural language processing",
      "score": 0.37655821442604065
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}