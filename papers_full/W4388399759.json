{
  "title": "PEFT-SP: Parameter-Efficient Fine-Tuning on Large Protein Language Models Improves Signal Peptide Prediction",
  "url": "https://openalex.org/W4388399759",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105522723",
      "name": "Shuai Zeng",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2788354788",
      "name": "Duolin Wang",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2094446072",
      "name": "Dong Xu",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2105522723",
      "name": "Shuai Zeng",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2788354788",
      "name": "Duolin Wang",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2094446072",
      "name": "Dong Xu",
      "affiliations": [
        "University of Missouri"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2790742465",
    "https://openalex.org/W2004406780",
    "https://openalex.org/W2809247618",
    "https://openalex.org/W2171091522",
    "https://openalex.org/W2161746138",
    "https://openalex.org/W2094201044",
    "https://openalex.org/W2164186284",
    "https://openalex.org/W2126975650",
    "https://openalex.org/W2776525063",
    "https://openalex.org/W2912990441",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W2949676527"
  ],
  "abstract": "Abstract Signal peptides (SP) play a crucial role in protein translocation in cells. The development of large protein language models (PLMs) provides a new opportunity for SP prediction, especially for the categories with limited annotated data. We present a Parameter-Efficient Fine-Tuning (PEFT) framework for SP prediction, PEFT-SP, to effectively utilize pre-trained PLMs. We implanted low-rank adaptation (LoRA) into ESM-2 models to better leverage the protein sequence evolutionary knowledge of PLMs. Experiments show that PEFT-SP using LoRA enhances state-of-the-art results, leading to a maximum MCC2 gain of 0.372 for SPs with small training samples and an overall MCC2 gain of 0.048. Furthermore, we also employed two other PEFT methods, i.e., Prompt Tunning and Adapter Tuning, into ESM-2 for SP prediction. More elaborate experiments show that PEFT-SP using Adapter Tuning can also improve the state-of-the-art results with up to 0.202 MCC2 gain for SPs with small training samples and an overall MCC2 gain of 0.030. LoRA requires fewer computing resources and less memory compared to Adapter, making it possible to adapt larger and more powerful protein models for SP prediction.",
  "full_text": null,
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.8567723035812378
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8185849189758301
    },
    {
      "name": "Computer science",
      "score": 0.634872317314148
    },
    {
      "name": "Artificial intelligence",
      "score": 0.380199670791626
    },
    {
      "name": "Machine learning",
      "score": 0.34675341844558716
    },
    {
      "name": "Computer hardware",
      "score": 0.11368146538734436
    }
  ]
}