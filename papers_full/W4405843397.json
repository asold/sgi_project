{
  "title": "Finding deceivers in social context with large language models and how to find them: the case of the Mafia game",
  "url": "https://openalex.org/W4405843397",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2129411049",
      "name": "Byung-Hwa Yoo",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2123924313",
      "name": "Kyung Joong Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2129411049",
      "name": "Byung-Hwa Yoo",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2123924313",
      "name": "Kyung Joong Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293010744",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W2044300367",
    "https://openalex.org/W4387165392",
    "https://openalex.org/W4390058537",
    "https://openalex.org/W2056204623",
    "https://openalex.org/W1463850147",
    "https://openalex.org/W4284899315",
    "https://openalex.org/W4385570658",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W2116625462",
    "https://openalex.org/W2148055514",
    "https://openalex.org/W2588001287",
    "https://openalex.org/W4385570416",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4404781852"
  ],
  "abstract": null,
  "full_text": "Finding deceivers in social context \nwith large language models and \nhow to find them: the case of the \nMafia game\nByunghwa Yoo1 & Kyung-Joong Kim1,2\nLies are ubiquitous and often happen in social interactions. However, socially conducted deceptions \nmake it hard to get data since people are unlikely to self-report their intentional deception behaviors, \nespecially malicious ones. Social deduction games, a type of social game where deception is a key \ngameplay mechanic, can be a good alternative to studying social deceptions. Hence, we utilized large \nlanguage models’ (LLMs) high performance in solving complex scenarios that require reasoning and \nprompt engineering to detect deceivers in the game of Mafia given only partial information and found \nsuch an approach acquired better accuracy than previous BERT-based methods in human data and \neven surpassed human accuracy. Furthermore, we conducted extensive experiments and analyses to \nfind out the strategies behind LLM’s reasoning process so that humans could understand the gist of \nLLM’s strategy.\nA widely remarked human deception prevalence study1 shows that people usually tell lies twice a day, meaning \nthat lies are common in human social interactions. Unfortunately, although lies are such a common phenomenon \nin human social interactions, not all lies are harmless. Some lies are intentionally made to harass or exploit others \nby giving them false information.\nInterestingly, state-of-the-art artificial intelligence (AI) models like large language models (LLMs) can tell \nfalse information that feels natural to humans2–4. Furthermore, they are shown to be able to detect human lies \nin non-social monologues5. Here, we can come up with an idea: Can LLMs detect intentional and malicious lies \neven in the social interaction of humans, i.e., in dialogues?\nThe problem with finding intentional lies in social contexts is that it is hard to get such data since nobody \nwould self-report their intentionally malicious deceiving behaviors. Thus, smaller models of real social \ninteractions—where the malicious, intentionally lying people are pre-defined—can be utilized to study social \ndeception problems. Social deduction games are exact matches for such models; they are based on social \ninteraction between humans and include hidden roles or relationships in the game, making deception a critical \nfactor in the gameplay. Hence, social deduction games have been a popular subject in social network theories6,7. \nNot only social network theory studies but also AI studies8,9 focus on social deduction games to study deception \nbehaviors, but their researches lack either linguistic conversation in the environment or information partiality, \nwhich are all critical facts in social interactions.\nOn our experiments, LLMs were given real human data of playing the game Mafia. LLMs, as an observer of \nthe games, were asked to predict who the mafia are given partial recording of the games. As said in the previous \nparagraph, the act of finding mafioso is equivalent to finding deceiving actors in social contexts. Since LLMs \nhave shown state-of-the-art performances on very complex tasks that require reasoning or planning 10–12, we \nexpect such ability of LLMs would make usage in predicting deceivers in social contexts.\nOur work is the first study to utilize state-of-the-art LLMs’ high performance in solving complex scenarios \nthat needs reasoning in the field of finding deceivers in the social context of real human data, at least up to our \nknowledge. We conduct an extensive analysis of how LLMs decide malicious deceivers in the social context \ngiven only partial conversational information in the game of Mafia, a popular type of social deduction game. \nFurthermore, we exploit the explainability of LLMs to give humans a comprehension of how to find deceivers \nin social contexts.\n1AI Graduate School, Gwangju Institute of Science and Technology, Gwangju 61005, Republic of Korea. 2School of \nIntegrated Technology, Gwangju Institute of Science and Technology, Gwangju 61005, Republic of Korea. email: \nkjkim@gist.ac.kr\nOPEN\nScientific Reports |        (2024) 14:30946 1| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports\n\nBackground and related works\nArtificial intelligence-based approaches for social deduction games\nSocial deduction games have been a research topic for various academic fields. Earlier studies included biased \nvoting in social network6, how culture affected cues of deception using Chinese social deduction game13, social \nnetwork-based deceiver detection in computer-mediated communication using Mafia game website epicmafia.\ncom7, and mathematical model developments for social deduction games14,15.\nWhile earlier works tried to use social deduction games in fields of social network systems and mathematics, \nmore recent studies tried to develop AI models regarding social deduction games. Nakamura et al. 16 built a \npsychology-based AI model that could play the social deduction game of Werewolf. Serrino et al. 8 developed \na multi-agent model that could surpass human abilities in the social deduction game The Resistance: Avalon. \nSimilarly, Chuchro17 tried to develop an assassin agent in the same game, The Resistance: Avalon. However, \nalthough these studies were able to get decent performances, their settings lacked linguistic information—\nmaking their environment far apart from real-life social interactions.\nThus, considering most real-life social interactions and social deduction games include conversations, it is \ncritical to include linguistic information in the environment. One of the first research to consider linguistic \ninformation was de Ruiter et al.18, yet this study utilized explicit features such as a number of specific words, not \nthe semantic meaning of conversations. However, incorporating linguistic information into the environment \nwas known to be a difficult problem, thus, several studies utilize modern NLP approaches such as fine-tuning \nPLMs. Lai et al.19 viewed social deduction games as persuasive social interactions and tried to develop a multi-\nmodal model that could decide what persuasive strategy was used in the game, where their model was built \nbased on fine-tuning Ppe-trained language models (PLMs) such as BERT20 and RoBERTa21. On the other hand, \nIbraheem et al.9 focused on deceptions in social deduction games. They fine-tuned BERT and GPT-2 22 to find \ndeceivers given the full record of Mafia game dialogues. We also aimed to find deceivers in the Mafia, but we \nconducted experiments with the partial environment to further enhance real-life usability.\nLarge language models\nLLMs are large models, usually transformer 23 architecture trained with large corpora. Although there was no \nstrict definition of how big the model size or corpora should be for some models to be LLMs, we considered the \nterm LLM in this study to describe generative models with larger size than GPT-2, including GPT-324, GPT-425, \nand LLaMA26.\nThanks to large size and generalized performances, LLMs showed remarkable results in various fields even \nwithout fine-tuning with additional data with the help of prompt engineering, the act of giving appropriate \ninputs to LLMs. Furthermore, to improve performances of prompt engineering, techniques like the chain of \nthoughts27 (CoT) or the tree of thoughts28 were proposed for few-shot prompts, and zero-shot CoT29 was used \nfor zero-shot cases. Plus, such mechanisms also performed as explainable AIs, since using these lets LLMs give \ntheir analyses behind their decisions when giving the output.\nPlus, LLMs have shown very high performances in complex scenarios requiring reasoning or planning, \nwhich was unsuitable for smaller language models. LLMs have shown high performance in various complex \ntasks requiring planning or reasoning, including legal reasoning10, video games11,12, etc.\nMethods\nThe Mafia game\nThe Mafia game, or the Werewolf game, is a popular social deduction game developed by Dimitry Davidoff  \nin 1986. The mafia game has two teams—the uninformed majority team called bystanders (villagers) and the \ninformed minority team called the mafia (werewolves). The bystanders’ goal is to remove all mafia hiding among \nbystanders while bystanders get no information on who the mafia is. On the other hand, the mafia’s goal is to \nremove bystanders until the number of remaining bystanders and the mafia becomes the same before all mafia \nare executed by bystanders. Plus, the mafia members are informed about who the mafia is before the start of \nthe game. The game continues with two alternating phases—daytime and nighttime. During the daytime, all \nbystanders and the mafia vote for one person to be executed. During the nighttime, the mafia decides one person \nto kill, and the nighttime conversation cannot be heard by bystanders.\nConsider a simple mafia game walkthrough with six members—A, B, C, D, E, and F—where A and B are \nmafia, C, D, E, and F are bystanders. The game starts with the nighttime phase. On 1st nighttime, A and B decide \nto kill C. Then on 1st daytime, C is revealed as a victim. All A, B, D, E, and F have open conversations and open \nvoting. By the voting, A was decided to be executed. On the following 2nd nighttime, only remaining mafioso \nB kills D. Then on the 2nd daytime, remaining B, E, and F have an open vote, and E was decided to be executed. \nThen the mafia team won the game since the number of remaining mafia and the remaining bystanders were the \nsame 1 person.\nIn our experiment, we used the same rules in the walkthrough, where no additional roles besides the mafia \nand bystanders were used.\nDataset\nWe used the Mafia game dataset from Ibraheem et al.9, which collected dozens of game-playing dialogues of the \nMafia game, where the games followed the rules described in The Mafia Game subsection. The data was collected \nvia Amazon Mechanical Turk, having a total of 460 English-speaking participants.\nTable 1 shows the number of Mafia games included in the training, test, and invalid datasets. Data being \ncollected online, some games were not valid to be used for our study. The criterion for the invalid data was \nwhether there was no victim of the mafia, either it was due to technical issues or mafia players were not properly \nScientific Reports |        (2024) 14:30946 2| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nparticipating in the game. 6 games of Mafia had no victims, and it was decided they were invalid samples. \nAlthough our approach was focused on state-of-the-art LLMs and prompt engineering, BERT-based baselines \nutilized fine-tuning, meaning that training data was needed for performance analysis. For the training dataset \nfor baselines, we used 23 games of Mafia, and for the test, we used 15 games of Mafia. Training and test datasets \nwere randomly split, except for invalid ones.\nTable 2 shows the number of people who participated in the games and the length of the games in the \ntraining and test dataset. Both training and test data had a maximum of 12 people and a minimum of 7 people \nparticipating for each game in the data and had similar mean participants of 9.58 and 9.73 people. For the \nlengths of the games, game lengths were not fixed. Both training and test data had a mean length of slightly more \nthan 3 days per game, and approximately 60–70% of games lasted 3 days or shorter. For linguistic information on \nthe data, in both train data and test data, there were around 15.6 exchanges (number of utterances), and around \n30 sentences were used per game, and 200 words were used per game.\nUnlike considering the full course of the game, i.e., from the start of the game to the end, we considered partial \ndata of the Mafia game, i.e., input that only contains information up to a specific point of the game, not the end. \nTo test our baselines on partial information, game data were split into units of days. For each sample of data, we \nlet each sample include from the start of the game to the end of a specific day, at the point of execution victim by \nvoting was published. The reason why we set the data format like this was to make machines have similar inputs \nwith human players of the Mafia game; human players had information only on what was previously said before \nthey voted. Table 3 shows how much partial game dialogue samples were used for training and testing.\nFigure 1 shows an example explaining the format of the data used for training and testing. As shown in \nthe figure, each people were anonymized as P0, P1,..., and P11 to exclude the effect of names in training and \ninferencing. Note that the order of name anonymization was changed in every sample, i.e., the same person \nreferenced as P0 in one sample might appear as P7 or P8 in other samples. Furthermore, non-conversational \ninformation such as someone being killed by the mafia or someone voting for another, was included as non-\nparticipating player “information” stating such necessary information. Plus, no identities of the victims, \nincluding both mafia victims and execution victims, were included in the data, since if any victim was revealed \nas a mafioso then it would be ensured to predict at least one mafia correctly for any models in any case.\nBaselines and metrics\nWe used 5 types of baselines for our experiments: GPT-4, GPT-3.5-turbo (ChatGPT), BERT-Multilabel, BERT-\nUtterance, and Random. GPT-4 (estimated 1.76 trillion parameters) and GPT-3.5-turbo (175 billion parameters), \nboth widely used commercial LLMs trained with an extensive size of corpora by OpenAI, were used as baselines \nto show how LLMs’ high performance on reasoning requiring complex tasks affects deception detection in the \nLength Train Test\nUp to day 2 23 15\nUp to day 3 21 12\nUp to day 4 10 4\nUp to day 5 2 0\nTable 3. Number of samples in train and test dataset according to length.\n \nTrain Test\nMaximum participants 12 12\nMean participants 9.58 9.73\nMinimum participants 7 7\nMaximum game length (days) 5 4\nMean game length (days) 3.21 3.07\nMinimum game length (days) 1 2\nThe ratio of 3 days or shorter games 0.63 0.73\nMean exchanges per game 15.67 15.60\nMean sentences used per game 31.45 33.6\nMean words used per game 214.0 211.73\nTable 2. Number of participants and lengths of games in training data and test data.\n \nTrain Test Invalid\n23 15 6\nTable 1. Number of games for training data, test data, and invalid data.\n \nScientific Reports |        (2024) 14:30946 3| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nsocial context. On the other hand, BERT-Multilabel, and BERT-Utterance, were BERT-based classifier models \nand used as baselines to show relative performances against commercial LLMs.\nFor GPT-4 and GPT-3.5-turbo, we used prompt engineering and zero-shot CoT. Figure 2 shows the prompt \nwe used in our setting. As shown in Fig.  2, all rules and necessary information were given in SYSTEM, and \ngame dialogues were given in USER. The phrase “step-by-step” was included in rule 6 and the LLMs were asked \nto write the reason in rule 7 under the SYSTEM to invoke the zero-shot CoT process to enhance performance. \nTemperatures were set to 0 to exclude the randomness of LLMs.\nBERT-Multilabel was a naïve BERT-based model to decide who the mafia were. Since there were at most \n12 participants, the BERT model was fine-tuned with training data to perform multi-label classification for 12 \nclasses. Since all games had two mafias, the model was trained to predict the two most likely mafia labels given \nthe input partial dialogue. For testing, two classes with the highest probabilities were regarded as the model’s \nprediction for two mafias.\nBERT-Utterance is a BERT-based model originally introduced to detect mafia via binary classification in \nIbraheem et al. 9 Since the original model was binary classification given the full record of the game, we re-\ntrained and modified the model so that it can be used with partial game dialogues and be tested with the same \nmetrics used in the other baselines. Figure  3 shows the modified input for determining if P0 was the mafioso \ngiven the modified input. As shown in the figure, the player to be checked was tagged with the word “mafioso” \nfor every single utterance the person said, i.e., to check P3, the word “mafioso” would be tagged to P3. Given such \na form of input, the BERT-utterance model returned the probability of the player being a mafioso. For testing, \nevery single player was tested per data sample, and the two players with the highest probabilities were regarded \nas the model’s prediction for the two mafias. Note that people with no utterance cannot be predicted using this \nmodel—they were strictly given the probability 0 of being mafias.\nFinally, for the random baseline, the random model was asked to randomly select any 2 different participants \namong 12 candidates, since 12 is the maximum number of classes used in BERT-multiclass. The random baseline \nused uniform random variables and were tested 100 times.\nFor both BERT-Multilabel and BERT-Utterance models, they were fine-tuned from bert-base-uncased 20, \nusing AdamW optimizer30 with learning rate 5e −5, weight decay of 0.01, and the batch size of 8. The models \nwere trained for at most 20 epochs with the loss function of cross entropy, and the epoch with the best validation \nloss was used for inference.\nTo evaluate the performance of the baselines, we used two metrics, single-match accuracy and exact-match \naccuracy. Single-Match Accuracy was defined as the ratio of samples that models predicted at least one mafioso \ncorrectly. On the other hand, Exact-Match Accuracy was defined as the ratio of samples that models predicted \nboth two mafias correctly.\nResults\nTables 4 and 5 show the single match accuracy and exact match accuracy of four baseline models. GPT-4 and \nGPT-3.5-turbo used temperatures of 0 so that their answers were definite, and for BERT-based models, they were \ntrained 5 times each; mean accuracy and standard deviations are shown in Tables 4 and 5.\nIn both single-match accuracy and exact-match accuracy metrics, GPT-4 showed the highest accuracy \ncompared to the other baselines. In single-match accuracy, GPT-4 showed 80.65% accuracy, which was more \nthan 24% higher than the BERT-Utterance model’s second-best accuracy of 56.13%. On the other hand, GPT-\n3.5-turbo showed the lowest accuracy of 41.94%, which was slightly lower than BERT-Multilabel’s 45.16% and \nBERT-Utterance’s 56.13%. In exact-match accuracy, GPT-4 showed 19.35% accuracy, which was significant \ncompared to the other models. GPT-3.5-turbo and BERT-Multilabel were not able to predict correctly at all, \ngetting 0% accuracy.\nFig. 1. Example of an input sample showing the start of 1 day to its end.\n \nScientific Reports |        (2024) 14:30946 4| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nFig. 3. Example of an input sample for determining if P0 is the mafioso in the game using BERT-Utterance \nmodel.\n \nFig. 2. Prompt applied to GPT-4 and GPT-3.5-turbo models. Game dialogues were included in Dialogue part \nunder USER section.\n \nScientific Reports |        (2024) 14:30946 5| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nIn both single-match and exact-match cases, GPT-3.5-turbo got the lowest mafia prediction performance, \neven though it was a commercial LLM trained on an extensive size of corpus and known to gain public syndrome \nfor chatGPT due to its performances in various tasks. We suspected that this was since LLMs’ performance \nis highly dependent on their size 24,25, i.e., the larger the model gets, the better the performance becomes. We \nbelieve our study was also one of such cases. However, GPT-4’s performance and GPT-3.5-turbo’s performances \nvaried significantly, almost twice in single-match accuracy and GPT-3.5-turbo getting 0 exact-match accuracy, \nwe believe there was a critical point of LLMs’ parameter sizes and performance that enables better prediction \nability in finding deceivers in the social context.\nPerformance compared to data collection participants\nAs stated in The Mafia Game subsection, the players voted for a person to be executed during the daytime phase, \nmeaning bystanders should guess who the mafioso was and vote for that person to be executed. Therefore, we \ncould retrieve the ratio of bystanders correctly voting for the mafioso by getting the number of total bystanders’ \nvotes and the number of bystanders who correctly voted for the mafia. When retrieving mafia prediction \naccuracy of data participants, i.e., human players in the data collection, we applied the Eq. (1) to each sample of \nthe test dataset. Note that abstention votes were not included in the denominator.\n \nAccuracy = #Bystanders voted to maﬁa\n#Bystanders who voted . (1)\nHowever, the baselines were asked to get all mafia, while participants of the game could vote for only one person \neach day. Plus, the baselines consider the possibility of players already executed or murdered being mafias, while \nhumans voted only for the surviving players. Hence, we added one more rule to equalize baseline conditions \nwith human participants, “Pick only one player most likely to be a mafioso. Exclude executed players. ”  to pre-\nexisting rules in the GPT-4’s SYSTEM shown in Fig. 2, so that GPT-4 could only predict one player most likely \nto be a mafioso, excluding the players already dead.\nTable  6 shows the performance of GPT-4 versus the retrieved performance of human participants in \npredicting the mafia in-game. It was shown that GPT-4 showed a much higher mafia prediction performance of \n45.16%, which was more than 1.5 times more accurate than human participants’ 28.83%. Furthermore, GPT-4 \nInfo length GPT-4 (%) Participants (%)\nUp to day 2 33.33 33.33 ± 40.37\nUp to day 3 50.00 32.87 ± 31.31\nUp to day 4 75.00 14.58 ± 15.16\nTotal 45.16 30.73 ± 35.16\nTable 6. Accuracy of data participants and GPT-4 in the test dataset.  Significant values are in bold.\n \nInfo length GPT-4 (%) GPT-3.5-turbo (%) BERT-multilabel (%) BERT-utterance (%) Random (%)\nUp to day 2 13.33 0.00 0.00 ± 0.00 16.00 ± 5.33 1.53 ± 1.26\nUp to day 3 33.33 0.00 0.00 ± 0.00 6.67 ± 3.33 1.91 ± 1.32\nUp to day 4 0.00 0.00 0.00 ± 0.00 0.00 ± 0.00 2.00 ± 1.00\nTotal 19.35 0.00 0.00 ± 0.00 10.32 ± 3.76 1.74 ± 1.27\nTable 5. Exact match accuracy of our baselines on the test dataset. For GPT-4 and GPT-3.5-turbo, \ntemperatures were set to 0 to make the models deterministic. For BERT-based models, the models were trained \n5 times, and the mean and the standard deviation were recorded. For the Random baseline, it was tested 100 \ntimes. Significant values are in bold.\n \nInfo length GPT-4 (%) GPT-3.5-turbo (%) BERT-multilabel (%) BERT-utterance (%) Random (%)\nUp to day 2 80.00 57.14 33.33 ± 0.00 61.33 ± 7.77 31.73 ± 4.07\nUp to day 3 75.00 33.33 58.33 ± 0.00 55.00 ± 4.08 32.67 ± 5.79\nUp to day 4 100.00 20.00 50.00 ± 0.00 40.00 ± 20.00 38.75 ± 4.02\nTotal 80.65 41.94 45.16 ± 0.00 56.13 ± 2.58 33.00 ± 5.31\nTable 4. Single match accuracy of our baselines on the test dataset. For GPT-4 and GPT-3.5-turbo, \ntemperatures were set to 0 to make the models deterministic. For BERT-based models, the models were trained \n5 times, and the mean and the standard deviation were recorded. For the Random baseline, it was tested 100 \ntimes.  Significant values are in bold.\n \nScientific Reports |        (2024) 14:30946 6| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nalways showed higher performance compared to human participants regardless of how much data was given as \ninput, either in small fragments of game dialogues (up to day 2) or full-course dialogues (up to day 4 or later).\nAn interesting trend could be found when comparing the trend of prediction accuracy against how much \ninformation was given. For GPT-4, it was shown that the more information was given, the mafia prediction \naccuracy increased from 33.33% → 50.00% → 75.00%. On the other hand, human participants’ mafia prediction \naccuracy seemed to decrease when more information was given, showing 33.33% → 32.87% → 14.58%. This \ncould be perceived as that GPT-4 was good at detecting mafias’ lies while human participant bystanders were \nplayed by mafias’ lies in late-game situations.\nThe reasons behind LLM’s decisions\nLLMs, by their nature, are text-to-text generative AI. Such characteristics of LLMs let them to be able to generate \nanalyses of the reasons behind their decisions. Furthermore, to use CoT processes, printing the thinking \nprocess in detail was encouraged for LLMs. Our GPT-4 and PGT-3.5-turbo models utilized the zero-shot CoT \nprocess, also encouraging the GPT models to print out their reason analyses. We could exploit such processes to \nexplainability for humans, although their reasoning logic might not be perfect.\nFigures 4 and 5 show two examples of reasons for GPT-4’s reasons for its decisions. Although for both cases \nthe predictions were exactly correct, not all parts of their reasoning were logically correct. For the case of Fig. 4, \nGPT-4 was able to conclude who were the mafias very well with good reasoning. On reasons 1 and 2, GPT-4 \nsuspected P2 and P7 as culprits since they voted for the same victim P1 on day 1, and it reinforced its suspicion \nas P2 and P7 voted for different people on day 2, hypothesizing the culprits were using suspicion deflection \nstrategy. On reasons 3 and 4, GPT-4 utilized comments or voting information of other participants to decide \nthe culprits. By combining all of the factors, GPT-4 was able to conclude that P2 and P7 were comparably most \nlikely to be mafia.\nFig. 4. GPT-4’s good reasoning example.\n \nScientific Reports |        (2024) 14:30946 7| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nHowever, in Fig. 5 case, GPT-4 was not able to get fully valid reasoning analyses. It was able to deduce P4, and \nP6 were mafia based on voting information and the fact both voted for the same P2. However, it considered P2, \na voting execution victim, to be a bystander without any specific reason, meaning that GPT-4 did not perfectly \nunderstand the rules or made oversimplifications.\nIn conclusion, we could say the GPT-4 model was able to generate an analysis of reasons behind their decisions \nthat keeps a logical gist that humans could perceive, but it did not guarantee concrete reasons. However, there \nis a concern that such analyses could be interpreted as a legitimate thinking process of LLMs. Y et, even if such \nanalyses are more post-hoc analyses, considering that LLMs can generate meaningful post-hoc analyses 31, we \ncould say such analyses still have the logical gist for humans to consider.\nAblation studies\nVoting information\nIn the previous subsection, it was shown that voting information was critical in GPT-4’s prediction process. \nHence, it would be a good idea to check two things regarding how significant factor voting was to GPT-4. First, \nwe removed voting information from the input, so that we could see how considerable voting was in the process. \nSecond, we give only voting information to the GPT-4. Such a situation made linguistic information excluded as \nmuch as possible, being semantically similar to a non-language-based social deduction environment in Serrino \net al.8 We tested this case to see how critical other information outside voting was in the reasoning process.\nTable 7 shows the results of information without voting and voting information only cases of GPT-4 compared \nto original GPT-4 results in both single-match and exact-match accuracy metrics. It was shown that removing \neither voting information or using only voting information proved to decrease accuracy by a significant amount. \nOverall, removing voting information proved to be better than using only voting information acquiring 64.52% \nand 9.68% accuracy for single-match and exact-match respectively, while using only voting information gained \nsignificantly low accuracy of 22.58% and 0.00%. This might be interpreted as that although voting was a key \nfactor in GPT-4’s prediction processes, non-voting conversations did play a significant role in interpreting the \nMafia game.\nChanging temperatures of LLMs\nFigure  6 shows the mafia detection accuracy of GPT-4 models with different temperatures. We tested \ntemperatures varying from 0 to 1, for each case testing 3 times except the deterministic case of temperature 0. \nAlthough it was known that increasing temperature increased the creativity of LLMs, so higher temperature may \nhelp or hinder performance according to what task it was dealing with, our case was shown to be unaffected by \nthe temperature changes. Except for temperature 1.0 cases, all GPT-4 models with different temperatures showed \nFig. 5. GPT-4’s reasoning example. The predictions were correct, however, the reasoning analyses were not \ncompletely sound.\n \nScientific Reports |        (2024) 14:30946 8| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\naccuracy between 0.75 and 0.82 in single-match accuracy and between 0.16 and 0.2 in exact-match accuracy, \nwhich were no significant differences. Similarly, Renze et al. 32 reported that LLMs do not show meaningful \nperformance differences in multiple-choice question-and-answer (MCQA) problems. Mafia detection could be \nconsidered a sort of MCQA—finding mafias given several players—it would be natural for GPT-4 not to be \naffected much by temperatures.\nLimitations\nAlthough our study showed the LLMs’ power to predict deceivers in social contexts and a limited extent of \nexplainability, there are several important limitations our study had. First, there was a data shortage problem. \nSince our experiment could use data from only 15 games, two big problems occurred. One was that our results \nfor up to day 4 were limited by the very small number of data, which did not provide sufficient statistical power \nto draw definitive conclusions. Given the limited data, especially in the later stages of the game, these findings \nshould be interpreted with caution. Another is that having too little data may not be sufficient to train BERT-\nbased models; larger data may yield better prediction ability for these models. Second, the explainability of \nGPT-4 for social deception was rather limited. GPT-4 produced misleading analyses about their decisions \nand sometimes showed doubtful results if GPT-4 is truly getting the rules of the game completely correct. \nFurthermore, it was impossible to create a quantitative standard to measure whether GPT-4’s explanations were \nvalid, or how much were valid. Finally, there was a model diversity problem. Our experiment only included \nOpenAI’s two commercial models as LLMs, yet there are many open-source LLMs such as LLaMA, which could \nFig. 6. Single-match and exact-match mafia detection accuracy of GPT-4 models differing temperatures. For \nall models except temperature 0, models were tested 3 times. Means and standard deviations are shown in the \ngraph.\n \nInfo length Original (%) w/o vote (%) Only vote (%)\nSingle-match accuracy\n Up to day 2 80.00 46.67 6.67\n Up to day 3 75.00 83.33 33.33\n Up to day 4 100.00 75.00 50.00\n Total 80.65 64.52 22.58\nExact-match accuracy\n Up to day 2 13.33 6.67 0.00\n Up to day 3 33.33 16.67 0.00\n Up to day 4 0.00 0.00 0.00\n Total 19.35 9.68 0.00\nTable 7. Single-match and exact-match accuracy of original GPT-4, GPT-4 without voting information, and \nGPT-4 with only voting information. Temperatures were set to 0 to make models deterministic. Significant \nvalues are in bold.\n \nScientific Reports |        (2024) 14:30946 9| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\nbe further fine-tuned, which means that there might be a chance for fine-tunable LLMs that could outperform \nGPT-4, but we were not able to test that.\nDiscussion\nIn this study, we aimed to find deceivers in human data via the commonsense reasoning power of LLMs. We \nshowed that GPT-4 was able to find deceivers very well, achieving higher accuracy than BERT and GPT-3.5-\nturbo, and even humans. Plus, by using the generative nature of LLMs, we showed that GPT-4 could generate \nanalyses on how it decided its answers to some extent, possibly giving limited insight to humans.\nIn the comparison of GPT-4 against human participants in Table  6, we can see that GPT-4’s accuracy rises \nas more data is given while participants (bystanders) show lower accuracy. There could be several reasons we \ncan consider for such results. First, unless mafia members are executed early in the game, the ratio of mafia \namong total surviving members goes up, probably leading to a more mafia-friendly public opinion environment. \nMeanwhile, GPT-4 was in the position of a pure observer, i.e., not inside the game, giving them more robustness \nagainst public opinion control. Second, although humans act based on memory, it is not that all participants keep \ntrack of every part of the whole game dialogues, and the longer the game, the harder for humans to remember \nthe whole game. On the other hand, GPT-4 processes input data at once, meaning that all of the game dialogues \nin early parts and later parts could be processed well. Similarly, if we compare GPT-4 with BERT-based methods \nor GPT-3.5-turbo, GPT-3.5 gained less prediction accuracy given more information, and BERT-based models \nseem not affected much by the data amount in the single-match accuracy in Table  4. The factor behind this \nmight be that these models, unlike GPT-4, were having problems handling large amounts of data in late-game \ndialogues, considering that GPT-4 has an extremely larger size compared to the others.\nIf we look into the qualitative analyses of the GPT-4’s responses, they often generate misleading explanation \nanalyses for their decisions or even wrong interpretations of the game’s rules. Various factors might affect results. \nA factor that might affect the analysis performances is the stochastic property of GPTs. Considering that a high \ntemperature of 1.0 led to a lower prediction score in Fig. 6, more strict restrictions to reduce stochastic property \nmay help. Furthermore, the input about the rules of the game was likely to affect the reasoning analyses’ validity, \nsuch as the term execution being directly considered bystander’s death in Fig.  5, which implies that the term \nexecution was not perfectly understood as a game context term by GPT-4. Y et, we were not able to find a better \nsystem prompt to solve this problem.\nAlthough GPT-4’s explanation performance was limited and often misleading, we believe that our study \nshowed some level of explainability by GPT-4. We believe our work to be an early work for finding deceivers in \na social context via LLM, and we believe our study implies that larger and better models that will come in the \nfuture will have better explainability.\nFor future works, we aim to develop our system in more complex scenarios, such as real-world human social \ninteractions or social deduction games with more rules and larger participants. Furthermore, we would try fine-\ntuning open-source LLMs such as LLaMA if we are to gain more abundant data for social deduction games.\nData availability\nFor the Mafia Game dataset, we used a publicly open dataset first introduced in “Putting the Con in Context: \nIdentifying Deceptive Actors in the Game of Mafia” (Ibraheem et al. 2022), which can be found at:  h t t p s : / / g i t h u \nb . c o m / o m o n i d a / m a fi   a - d a t a s e t / t r e e / m a i n     .  \nReceived: 19 June 2024; Accepted: 2 December 2024\nReferences\n 1. DePaulo, B. M., Kashy, D. A., Kirkendol, S. E., Wyer, M. M. & Epstein, J. A. Lying in everyday life. J. Pers. Soc. Psychol. 70, 979. \nhttps://doi.org/10.1037/0022-3514.70.5.979 (1996).\n 2. Azaria, A. & Mitchell, T. The internal state of an llm knows when its lying. Preprint at http://arxiv.org/abs/2304.13734 (2023).\n 3. Park, P . S., Goldstein, S., O’Gara, A., Chen, M. & Hendrycks, D. Ai deception: A survey of examples, risks, and potential solutions. \nPreprint at http://arxiv.org/abs/2308.14752 (2023).\n 4. Pacchiardi, L. et al. How to catch an ai liar: Lie detection in black-box llms by asking unrelated questions. Preprint at  h t t p : / / a r x i v . \no r g / a b s / 2 3 0 9 . 1 5 8 4 0     (2023).\n 5. Loconte, R., Russo, R., Capuozzo, P ., Pietrini, P . & Sartori, G. Verbal lie detection using large language models. Sci. Rep. 13, 22849. \nhttps://doi.org/10.1038/s41598-023-50214-0 (2023).\n 6. Kearns, M., Judd, S., Tan, J. & Wortman, J. Behavioral experiments on biased voting in networks. Proc. Natl. Acad. Sci.  106, \n1347–1352. https://doi.org/10.1073/pnas.0808147106 (2009).\n 7. Pak, J. & Zhou, L. A social network based analysis of deceptive communication in online chat. In E-Life: Web-Enabled Convergence \nof Commerce, Work, and Social Life: 10th Workshop on E-Business, WEB 2011, Shanghai, China, December 4, 2011, Revised Selected \nPapers 10 55–65. https://doi.org/10.1007/978-3-642-29873-8_6 (Springer, 2012).\n 8. Serrino, J., Kleiman-Weiner, M., Parkes, D. C. & Tenenbaum, J. Finding friend and foe in multi-agent games. Adv. Neural Inf. \nProcess. Syst. 32, 1 (2019).\n 9. Ibraheem, S., Zhou, G. & DeNero, J. Putting the con in context: Identifying deceptive actors in the game of mafia. In Proceedings of \nthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies \n(eds. Carpuat, M. et al.) 158–168. https://doi.org/10.18653/v1/2022.naacl-main.11 (Association for Computational Linguistics, \n2022).\n 10. Yu, F ., Quartey, L. & Schilder, F . Exploring the effectiveness of prompt engineering for legal reasoning tasks. In Findings of the \nAssociation for Computational Linguistics: ACL 2023 (eds. Rogers, A. et al.) 13582–13596.  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / 2 0 2 3 . fi   n d i n \ng s - a c l . 8 5 8     (Association for Computational Linguistics, 2023).\n 11. Wang, G. et al. Voyager: An open-ended embodied agent with large language models. Preprint at http://arxiv.org/abs/2305.16291 \n(2023).\nScientific Reports |        (2024) 14:30946 10| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/\n 12. Tan, W . et al. Cradle: Empowering foundation agents towards general computer control. Preprint at http://arxiv.org/abs/2403.03186 \n(2024).\n 13. Zhou, L. & Sung, Y .-W . Cues to deception in online Chinese groups. In Proceedings of the 41st Annual Hawaii International \nConference on System Sciences (HICSS 2008) 146–146. https://doi.org/10.1109/HICSS.2008.109 (IEEE, 2008).\n 14. Braverman, M., Etesami, O. & Mossel, E. Mafia: A theoretical study of players and coalitions in a partial information environment. \nAnn. Appl. Probab. 18, 825–846 (2008).\n 15. Migdał, P . A mathematical model of the mafia game. Preprint at http://arxiv.org/abs/1009.1031 (2010).\n 16. Nakamura, N. et al. Constructing a human-like agent for the werewolf game using a psychological model based multiple \nperspectives. In 2016 IEEE Symposium Series on Computational Intelligence (SSCI) 1–8. https://doi.org/10.1109/SSCI.2016.7850031 \n(IEEE, 2016).\n 17. Chuchro, R. Training an assassin AI for the resistance: Avalon. Preprint at http://arxiv.org/abs/2209.09331 (2022).\n 18. de Ruiter, B. & Kachergis, G. The mafiascum dataset: A large text corpus for deception detection. Preprint at  h t t p : / / a r x i v . o r g / a b s / 1 \n8 1 1 . 0 7 8 5 1     (2018).\n 19. Lai, B. et al. Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games. In Findings \nof the Association for Computational Linguistics: ACL 2023 (eds. Rogers, A. et al.) 6570–6588.  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / 2 0 2 3 . fi   n \nd i n g s - a c l . 4 1 1     (Association for Computational Linguistics, 2023).\n 20. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \nPreprint at http://arxiv.org/abs/1810.04805 (2018).\n 21. Liu, Y . et al. Roberta: A robustly optimized bert pretraining approach. Preprint at http://arxiv.org/abs/1907.11692 (2019).\n 22. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog 1, 9 (2019).\n 23. Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems (eds. Guyon, I. et al.), vol. 30 \n(Curran Associates, Inc., 2017).\n 24. Brown, T. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (eds. Larochelle, H. \net al.), vol. 33, 1877–1901 (Curran Associates, Inc., 2020).\n 25. Achiam, J. et al. Gpt-4 technical report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n 26. Touvron, H. et al. Llama: Open and efficient foundation language models. Preprint at http://arxiv.org/abs/2302.13971 (2023).\n 27. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing \nSystems (eds. Koyejo, S. et al.), vol. 35, 24824–24837 (Curran Associates, Inc., 2022).\n 28. Y ao, S. et al. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing \nSystems (eds. Oh, A. et al.), vol. 36, 11809–11822 (Curran Associates, Inc., 2023).\n 29. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y . & Iwasawa, Y . Large language models are zero-shot reasoners. In Advances in Neural \nInformation Processing Systems (eds. Koyejo, S. et al.), vol. 35, 22199–22213 (Curran Associates, Inc., 2022).\n 30. Loshchilov, I. & Hutter, F . Decoupled weight decay regularization. Preprint at http://arxiv.org/abs/1711.05101 (2017).\n 31. Kroeger, N., Ley, D., Krishna, S., Agarwal, C. & Lakkaraju, H. In-context explainers: Harnessing llms for explaining black box \nmodels. Preprint at http://arxiv.org/abs/2310.05797 (2024).\n 32. Renze, M. & Guven, E. The effect of sampling temperature on problem solving in large language models. Preprint at  h t t p : / / a r x i v . o \nr g / a b s / 2 4 0 2 . 0 5 2 0 1     (2024).\nAcknowledgements\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation \n(IITP) grant funded by the Korea government (MSIT) (No. 2019-0-01842, Artificial Intelligence Graduate \nSchool Program (GIST)).\nAuthor contributions\nB.Y . wrote the main manuscript and prepared figures and tables. K.K. reviewed the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to K.-J.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |        (2024) 14:30946 11| https://doi.org/10.1038/s41598-024-81997-5\nwww.nature.com/scientificreports/",
  "topic": "Deception",
  "concepts": [
    {
      "name": "Deception",
      "score": 0.8647321462631226
    },
    {
      "name": "Computer science",
      "score": 0.7032050490379333
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6251308917999268
    },
    {
      "name": "Key (lock)",
      "score": 0.578977108001709
    },
    {
      "name": "Process (computing)",
      "score": 0.41868484020233154
    },
    {
      "name": "Data science",
      "score": 0.40932661294937134
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33669519424438477
    },
    {
      "name": "Computer security",
      "score": 0.33659613132476807
    },
    {
      "name": "Cognitive science",
      "score": 0.32713404297828674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32421576976776123
    },
    {
      "name": "Internet privacy",
      "score": 0.320250928401947
    },
    {
      "name": "Social psychology",
      "score": 0.26727160811424255
    },
    {
      "name": "Psychology",
      "score": 0.2582271695137024
    },
    {
      "name": "Programming language",
      "score": 0.12018787860870361
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}