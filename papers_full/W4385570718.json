{
  "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
  "url": "https://openalex.org/W4385570718",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097346119",
      "name": "Xiao Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2123677600",
      "name": "Da Yin",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2096971521",
      "name": "Chen Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2142380681",
      "name": "Yansong Feng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2098869281",
      "name": "Dongyan Zhao",
      "affiliations": [
        "Peking University",
        "Convergence",
        "Beijing Institute for General Artificial Intelligence",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2885051956",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4287642224",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4283210793",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4221143575",
    "https://openalex.org/W3177457301",
    "https://openalex.org/W4281490030",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W2964210047",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2402954262",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W2057882807",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W1555694191",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4283794524",
    "https://openalex.org/W2970453125",
    "https://openalex.org/W2109485526",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4307313205",
    "https://openalex.org/W4385567216",
    "https://openalex.org/W4385573599",
    "https://openalex.org/W3172365208",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W1767411471"
  ],
  "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like “if“, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure. Code and data are available at https://github.com/xxxiaol/magic-if.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 9009–9022\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nThe Magic ofIF: Investigating Causal Reasoning Abilities in Large\nLanguage Models of Code\nXiao Liu1, Da Yin2, Chen Zhang1, Yansong Feng1,3∗ and Dongyan Zhao1,4,5\n1Wangxuan Institute of Computer Technology, Peking University\n2Computer Science Department, University of California, Los Angeles\n3The MOE Key Laboratory of Computational Linguistics, Peking University\n4Beijing Institute for General Artificial Intelligence\n5State Key Laboratory of Media Convergence Production Technology and Systems\n{lxlisa,zhangch,fengyansong,zhaody}@pku.edu.cn\nda.yin@cs.ucla.edu\nAbstract\nCausal reasoning, the ability to identify cause-\nand-effect relationship, is crucial in human\nthinking. Although large language models\n(LLMs) succeed in many NLP tasks, it is still\nchallenging for them to conduct complex causal\nreasoning like abductive reasoning and counter-\nfactual reasoning. Given the fact that program-\nming code may express causal relations more\noften and explicitly with conditional statements\nlike if, we want to explore whether Code-\nLLMs acquire better causal reasoning abilities.\nOur experiments show that compared to text-\nonly LLMs, Code-LLMs with code prompts\nare significantly better in causal reasoning. We\nfurther intervene on the prompts from differ-\nent aspects, and discover that the program-\nming structure is crucial in code prompt design,\nwhile Code-LLMs are robust towards format\nperturbations. Code and data are available at\nhttps://github.com/xxxiaol/magic-if.\n1 Introduction\nHuman beings rely heavily on the capacity for\ncausal reasoning (Sloman, 2005; Hagmayer et al.,\n2007). People understand the observed facts, pre-\ndict future events, and speculate about what might\nhave happened if things had been different with the\nhelp of their causal reasoning skills. For instance,\nwhen we go home and find a mess, we probably\nwant to figure out why it happened. If we deter-\nmine that a bird flew into the house, we might then\nconsider whether the mess could have been avoided\nif we had closed the window.\nAlthough large language models (LLMs) demon-\nstrate great language understanding and genera-\ntion abilities, it is still challenging for them to per-\nform complex causal reasoning such as the example\nabove. Powerful LLMs are able to understand sin-\ngle cause-and-effect relations (Brown et al., 2020;\n∗ Corresponding author.\nHypoth\n-esis\nEnding\nInitial\nEdited \nEnding\nJenny went to work, leaving \nthe window just a crack open.\nIt was a breezy day and a large \nbird flew into the house.\nWhen Jenny returned home she \nsaw that her house was a mess!\nWhat is a plausible hypothesis \ngiven the observations?\nAbductive Reasoning\nWhat if the counterfactual event happens?\nCounterfactual Reasoning\nOliver brewed a \nfresh pot of coffee.\nHe took a sip of the \nhot coffee.\nOliver felt a searing \npain on his tongue .\nHe waited for the coffee to cool \nto a drinkable temperature.\nOliver savored the rich \nflavor of the coffee\nCausal relation Possible causal relation\nPremise\nPremise\nCounter-\nfactual\nOriginal \nEnding\nFigure 1: Causal relationships between events in two\ncausal reasoning tasks.\nWang et al., 2021), like a man losing his balance\ncauses him to fell. However, when it comes to more\ncomplex causal structures involving multiple events\nand alternative branches (like close the window or\nnot), LLMs perform much inferior to humans (Bha-\ngavatula et al., 2019; Qin et al., 2019). In this paper,\nwe consider two challenging causal reasoning tasks:\nabductive reasoning and counterfactual reasoning.\nAbductive reasoning requires models to generate\na plausible reason for the ending while being con-\nsistent with the premise. Counterfactual reasoning\nasks what will occur in the counterfactual branch.\nCausal relationships between events in these tasks\nare shown in Figure 1.\nA potential difficulty for LLMs to learn complex\n9009\n: model output\nAbductive Reasoning Counterfactual Reasoning\n# task: generate a plausible explanatory \nhypothesis given the premise and the ending\ndef main():\n    premise()\n    if hypothesis():\n        ending()\ndef premise():\n    # The Smiths were having holidays done of \nthe children.\ndef ending(): \n    # Ty's face lit up as he ran to the new \ntoy, happily posing for photos.\ndef hypothesis(): \n    # Ty was given a new toy.\n# task: generate an ending with three \nsentences given the premise and the \nhypothesis\ndef main():\n    premise()\n    if hypothesis_1():\n        ending_1()\n    elif hypothesis_2():\n        # minimally revise ending_1\n        ending_2()  \ndef premise():\n    # Janice was excited to bring cupcakes \nto her work for her birthday.\n    \ndef hypothesis_1():\n    # She worked all day on making the \nperfect frosting.\ndef hypothesis_2():\n    # She completely rushed making the \nfrosting.\ndef ending_1(): \n    # Each cupcake was truly a work of art.\n    # Everyone at her work loved them.\n    # Janice was thrilled and happy for the \nrest of the day.\n    # end\n    \ndef ending_2():\n    # The frosting was a complete disaster.\n    # Everyone at her work hated them.\n    # Janice was sad and embarrassed for \nthe rest of the day.\n    # end\nFigure 2: Example code prompts of abductive reasoning and counterfactual reasoning.\ncausal structures is that they are rarely expressed ex-\nplicitly in the text. News articles or narratives may\ncontain multiple events with causal relationships,\nlike an incident and a chain of consequences. How-\never, these events are often written chronologically,\nand it is hard to extract the causal structure from the\ntext without further annotation. Branches are ex-\npressed rarer in text, except for the multi-branching\nstorytelling style (Nisi and Haahr, 2006).\nOn the other hand, causal relations are exhibited\nmore commonly in code. Conditional statements\nlike if direct the computer to execute certain com-\nmands, provided a condition is met. This explicitly\ndemonstrates the causal relationship between the\ncondition block and the execution block. Code\ncan also express branching with elif or switch\nstatements, and the nesting feature enables code to\ndescribe more complex structures1.\nThis motivates us to utilize code models in natu-\nral language causal reasoning. Recently, large lan-\nguage models of code (Code-LLMs) are receiving\nincreasing attention (Chen et al., 2021; Xu et al.,\n2022). They exhibit strong code generation per-\nformance, and their structural prediction abilities\nhelp complete structural natural language tasks like\nargument graph generation (Madaan et al., 2022)\nand event argument extraction (Wang et al., 2022b).\nBeing pre-trained on code with abundant causal ex-\npressions, Code-LLMs may also have gained better\ncausal reasoning abilities.\nWe conduct experiments on the unsupervised\nabductive reasoning and counterfactual reasoning\ntasks. To generate task outputs, we design code\nprompts like Figure 2 to clearly represent the causal\nstructures of the tasks. Results show that Code-\n1Although causal expressions like if are also used in natu-\nral languages, representing complex causal structures in text\nis not as clear and structured as in code.\nLLMs with code prompts perform much better\nthan text-only LLMs and previous methods. To\nbetter understand why the code prompts are ef-\nfective, we break down the prompts and analyze\nthe influence of different aspects. We find that\nCode-LLMs are very sensitive to the programming\nstructure (specifically, the conditional statements),\nwhile being robust towards format perturbations\nand programming language changes.\nOur main contributions are as follows: 1) We de-\nsign code prompts to tackle causal reasoning tasks,\nby leveraging conditional statements in code to\nrepresent causal structures. 2) We evaluate Code-\nLLMs with code prompts on the abductive rea-\nsoning and counterfactual reasoning tasks, and ex-\nhibit that code models with code prompts are better\ncausal reasoners than text models. 3) We break\ndown the code prompt in detail and find that the pro-\ngramming structure is crucial to the performance.\n2 Modeling Causal Structure with Code\nWe convert the input of causal reasoning tasks into\nthe form of code prompt for Code-LLMs to un-\nderstand better. We expect the prompts to meet\ntwo requirements: 1) clearly represent the causal\nrelationships between events, and 2) as most Code-\nLLMs only support generating at the end, the tar-\nget output should appear at the end of the prompts.\nThe first requirement is addressed with conditional\nstatements. However, for the second, the target pre-\ndiction is not always the last part of the conditional\nstatements, e.g., in abductive reasoning we want\nto predict the hypothesis, which is the condition in\nthe if structure. To address this, we uniformly use\nfunctions to represent events. As shown in Figure 2,\nthe causal structure is described in the main func-\ntion. All the event functions are listed afterwards,\n9010\nBLEU4 ROUGEL CIDEr BERTScore\nDELOREAN 1.6 19.1 7.9 41.7\nCOLD 1.8 19.5 10.7 42.7\nDIFFUSION 7.1 28.3 30.7 -\nDAVINCI002 4.9 27.0 26.6 56.8\nDAVINCI003 4.6 25.8 10.7 57.1\nCODEX 13.7 39.6 81.8 64.9\n(a) Abductive reasoning.\nBLEU4 ROUGEL BERTScore\nDELOREAN 21.4 40.7 63.4\nCGMH 41.3 - 73.8\nEDUCAT 44.1 - 74.1\nDAVINCI002 49.0 54.7 73.0\nDAVINCI003 30.6 45.2 69.4\nCODEX 66.8 70.0 82.5\n(b) Counterfactual reasoning.\nTable 1: Automatic evaluation results on two unsupervised causal reasoning tasks in the zero-shot setting. Numbers\nare in percentages (%).\nCODEX Neutral D AVINCI002\nAbductive Reasoning\nCoherence with Premise 34% 48.5% 17.5%\nCoherence with Ending 32% 42.5% 25.5%\nOverall Coherence 40% 38% 22%\nCounterfactual Reasoning\nCoherence 36.5% 39.5% 24%\nPreservation 47.5% 39.5% 13%\nTable 2: Human evaluation of comparing CODEX and DAVINCI 002.\nleaving the target event function at the last.\nAbductive Reasoning. Abductive reasoning re-\nquires models to generate a plausible hypothesis\nH given the observations: premise P and ending\nE. The chronological order of these three events\nis P → H → E, and the hypothesis causes the\nending to occur.\nIn Figure 2, we regard the task definition as an in-\nstruction and place it as a comment at the beginning\nof the prompt. The causal structure is represented\nin the main function like: executing the premise,\nand if the hypothesis is met, executing the ending2.\nThe content of each event is presented as a com-\nment of its function. The hypothesis function is\nplaced at the last, leaving for models to complete.\nThe generation process stops with a line break.\nCounterfactual Reasoning. Counterfactual rea-\nsoning aims to rewrite a story under a counterfac-\ntual condition. As in Figure 1, the input consists of\nfour parts: the premise P, the initial contextC1, the\noriginal ending E1, and the counterfactual context\nC2. Models are asked to generate the counterfac-\ntual ending E2 that minimally modifies the original\nending E1 and is coherent with the counterfactual\ncontext C2.\nThe causal relationships are represented with the\nif-elif structure. The premise P is executed first,\nand then if the initial contextC1 is met, the original\nending E1 is executed; otherwise, if the counterfac-\n2Although not entirely accurate, this approximates the\nactual underlying causal relationships.\ntual context C2 is met, the counterfactual ending\nE2 will be executed. For ease of exposition, we call\nthe context hypothesis as well, being consistent\nwith the former task. The event contents are also\nwritten as comments for event functions. We use #\nend to mark the finish of the ending.\n3 Evaluation\nDatasets. We experiment on the ART dataset (Bha-\ngavatula et al., 2019) for the evaluation of abductive\nreasoning, and the TimeTravel dataset (Qin et al.,\n2019) for counterfactual reasoning, with 3,561 and\n1,871 test instances, respectively.\nModels. We experiment with CODEX (Chen\net al., 2021), trained on a blend of code and\ntext, as the Code-LLM. The specific version is\ncode-davinci-002. We compare with two LLMs:\nthe latest versions of GPT-3 (Brown et al., 2020)\ntext-davinci-002 and text-davinci-003 (re-\nferred to as DAVINCI 002 and DAVINCI 003). Both\nof them originate from CODEX and are tuned with\ninstructions. We follow OpenAI’s default settings\nin CODEX and DAVINCI decoding, and the text\nprompts for DAVINCI are in Figure A.1.\nWe also compare with previous unsupervised\nmethods on these tasks, includingDELOREAN (Qin\net al., 2020), COLD (Qin et al., 2022), DIFFU -\nSION (Li et al., 2022), CGMH (Miao et al., 2019),\nand EDUCAT (Chen et al., 2022a)3. Appendix A.3\n3All these methods exceptDIFFUSION use GPT-2 (Radford\net al., 2019) as the base model, and the model size ranges from\nmedium to XL.\n9011\nBLEU4 ROUGEL CIDEr BERTScore\nCODEXtext 11.7 37.5 78.5 62.5\nCODEXcode 13.7 39.6 81.8 64.9\nCODEX∗\ncode 16.5 42.0 91.6 66.3\nDAVINCItext 4.9 27.0 26.6 56.8\nDAVINCIcode 6.7 31.1 46.2 59.9\nDAVINCI∗\ncode 9.0 35.0 64.0 62.2\n(a) Abductive reasoning.\nBLEU4 ROUGEL BERTScore\nCODEXtext 55.1 61.3 77.8\nCODEXcode 66.8 70.0 82.5\nCODEX∗\ncode 73.3 74.7 85.3\nDAVINCItext 49.0 54.7 73.0\nDAVINCIcode 40.4 48.5 70.5\nDAVINCI∗\ncode 43.7 52.0 72.8\n(b) Counterfactual reasoning.\nTable 3: Effect of exchanging prompts for CODEX and DAVINCI 002 (%). ∗ indicates the best code prompt\nexperimented in Section 4.\nprovides a brief introduction of these methods.\nAutomatic Evaluation. We use the following\nautomatic evaluation metrics: BLEU 4 (Papineni\net al., 2002), ROUGEL (Lin, 2004), CIDEr (Vedan-\ntam et al., 2015) and BERTScore (Zhang et al.,\n2019) based on BERT-base for abductive reason-\ning; BLEU4, ROUGEL and BERTScore for coun-\nterfactual reasoning.\nTable 1 reports the automatic evaluation results\nin the zero-shot setting. CODEX significantly out-\nperforms previous methods and DAVINCI on both\ntasks (with significance level α = 0.01), exhibit-\ning strong causal reasoning ability. Although the\ntwo DAVINCI models are based on CODEX , their\ncausal reasoning abilities may be weakened during\ninstruction tuning, and this phenomenon is called\nalignment tax (Ouyang et al., 2022). DAVINCI 003\nunderperforms DAVINCI 002 on most metrics, prob-\nably because it tends to generate longer and more\ndiscursive outputs, which do not comply with the\ntasks.\nHuman Evaluation. We conduct pairwise com-\nparison between CODEX and DAVINCI 002 on 100\ntest examples. Annotators are asked to choose the\nbetter output given the task requirements. For ab-\nductive reasoning, the outputs are rated from three\naspects: coherence with the premise, coherence\nwith the ending, and the overall coherence. For\ncounterfactual reasoning, the outputs are rated from\ncoherence with the context and the extent of pre-\nserving the original ending. Each example is rated\nby at least two annotators, and the average inter-\nrater reliability is 0.64.\nThe results are shown in Table 2.CODEX outper-\nforms DAVINCI 002 in all aspects. It better considers\nthe context in generation, and is able to preserve\nthe original content in counterfactual reasoning.\nContributions of the Model and the Prompt.We\nexchange the prompts of code and text models, to\nmeasure the contributions of the model and the\nprompt. The results are in Table 3. We find that\nCODEX performs better with the code prompt, as\nthe code prompt clearly describes the causal re-\nlation between events. Code prompts benefit the\ntext model DAVINCI 002 on abductive reasoning,\nbut have negative impacts on counterfactual reason-\ning. A possible reason is that the causal structure\nin counterfactual reasoning is more complicated,\nleading to a more complex code which is harder\nfor text models to understand.\n4 What are Crucial in Code Prompts?\nTo paint a better picture of the key points in the\ncode prompts, we intervene on the prompts from\nfour aspects and measure the influences of the in-\nterventions. The four aspects we select are infor-\nmation, structure, format, and language. The for-\nmer two, the prior information provided and the\nprogramming structure of functions, are content-\nrelated; the latter two, the code format and pro-\ngramming languages, are form-related. An ideal\nmodel should rely on the content and be insensi-\ntive to form perturbations. The interventions are\ndescribed below, with examples in Figure A.2.\nInformation. We study two types of prior informa-\ntion: task instructions and function names. In No\nInstruction, we remove the task instruction from\nthe prompts. In Function Name Perturbation, we\nreplace original function names with anonymous\nfunctionX. For example, we replace premise()\nand hypothesis() in Figure 2 with functionA()\nand functionB(), respectively. It eliminates the\ninformation in function names and only allows\nmodels to learn the event relations from program-\nming structures.\nStructure. The first way to intervene in the pro-\ngramming structure is to convert the conditional\nstructures into sequential structures, referred to\nas Sequential Structure. The events are executed\nsequentially, like premise(), hypothesis(),\n9012\nBLEU4 ROUGEL CIDEr BERTScore\nCODEX 13.7 39.6 81.8 64.9\nNo Instruction 12.1 37.4 73.8 62.9Information Function Name Perturbation 15.1 39.1 77.8 64.6\nSequential Structure 9.6 36.8 72.0 63.5Structure Disruption 7.9 30.3 49.8 58.5\nClass 16.0 41.0 87.4 65.8\nPrint 13.8 39.4 82.0 65.0Format\nReturn 13.0 40.3 83.4 65.5\nJava 16.5 42.0 91.6 66.3Language C 15.5 41.0 88.0 65.6\nTable 4: Intervention results on abductive reasoning (%).\nending() in abductive reasoning. In the second\nway called Disruption, we randomly disrupt the po-\nsitions of the functions in the conditional structure.\nFor instance, if hypothesis(): ending() can\nbe disrupted into if ending(): hypothesis() .\nWe also apply the function name perturbation in dis-\nruption to eliminate the impact of function names.\nFormat. We test three formats besides the original\none: Class, Print and Return. The first one converts\nthe original code into a class. We define the pro-\ngramming structure in the __init__ method, and\nmove the event functions into the class. In Print,\nwe represent the content of events as a string and\nprint it in the function body, like def premise():\nprint(\"The Smiths ...\") . And in Return, the\nstring is the return value of event functions.\nLanguage. We also convert the original Python\nprograms into two other languages, Java and C, to\nevaluate the influence of programming languages.\nIntervention Results.We evaluate the influence\nof interventions on abductive reasoning in Table 4,\nand the results on counterfactual reasoning are in\nTable A.2. The absence of prior information causes\na small decrease in results. Even if the instruction\nor function names are not provided, CODEX is able\nto perform causal reasoning based on conditional\nstatements. Changes in the programming structure\nhave a larger negative impact. ComparingFunction\nName Perturbation and Disruption, the alteration\nof two characters (swap B and C in functionB and\nfunctionC) results in a major drop, showing that\nthe conditional structure that reasonably depicts\nthe relations between events is crucial in CODEX\nreasoning.\nCODEX is quite robust towards format and lan-\nguage changes. Settings like Class and Java are\neven better than the original one, revealing that the\nperformance can be further improved with delicate\nprompt engineering.\n5 Conclusion\nWe investigate the causal reasoning ability of Code-\nLLMs. With code prompts of conditional state-\nments, Code-LLMs achieve great performance in\nabductive and counterfactual reasoning, outper-\nforming text-only LLMs significantly. Our study\non different aspects of code prompts shows that\nproviding a reasonable causal structure in code can\nhelp generate plausible outputs, and Code-LLMs\nare robust towards format perturbations.\nLimitations\nLanguage Our experiments are conducted on En-\nglish, as all Code-LLMs we know are pre-trained\non English programming languages. Fundamen-\ntally, most popular programming languages are\nEnglish-based, but international programming lan-\nguages (which work in multiple languages) like\nScratch, or non-English-based programming lan-\nguages like Qalb also emerge. We look forward to\nthe appearance of Code-LLMs on these program-\nming languages.\nPrompt Engineering We manually design the\nprompts without prompt engineering techniques\nsuch as prompt search. The searched prompts may\noutperform the ones we used, but our experiments\non interventions show that CODEX is fairly robust\ntowards format perturbations.\nModel LLMs update quickly. From the time we\nsubmitted the paper until now, several new LLMs\nhave been released. We try to compare their per-\nformance with ours. We select three new LLMs:\nCHATGPT , GPT-4 (OpenAI, 2023), and BARD 4,\nand feed the text prompts to them. Because we\ndo not have access to some of their APIs, we only\nexperiment on a subset of 100 instances and report\n4Experiments are done with models updated to May 10,\n2023.\n9013\nBLEU4 ROUGEL CIDEr BERTScore\nCODEX 15.0 39.8 82.2 67.8\nCHATGPT 5.1 26.9 17.5 62.6\nGPT-4 6.3 29.2 27.8 65.1\nBARD 5.7 31.5 14.8 66.0\n(a) Abductive reasoning.\nBLEU4 ROUGEL BERTScore\nCODEX 68.4 70.3 84.7\nCHATGPT 15.3 34.7 70.0\nGPT-4 38.5 55.5 78.6\nBARD 12.1 22.0 62.1\n(b) Counterfactual reasoning.\nTable 5: Automatic evaluation results on a subset of 100 instances in the zero-shot setting. Numbers are in\npercentages (%).\nthe results in Table 5.CODEX outperforms all these\nmodels in the automatic evaluation, but part of the\nreason is that these models provide more detailed\noutputs than the reference. We provide a case study\nin Appendix A.5.\nSince CODEX is no longer available to the pub-\nlic, we provide CODEX generation results in our\nGitHub repository. We also looked for alterna-\ntives and tried two open source Code-LLMsCODE -\nGEN (Nijkamp et al., 2022) (version CodeGen-16B-\nMono) and STARCODER (Li et al., 2023) with our\ncode prompts. However, as shown in the case study,\ntheir performance is not comparable to CODEX ,\nprobably because they are more than ten times\nsmaller in size.\nEthics Statement\nOur work is based on off-the-shelf LLMs. As the\nresults may inherit the underlying bias of LLMs,\nthey cannot be used individually without human\nsupervision. The Codex API was free when the\nexperiments were conducted, and the Davinci APIs\ncost $0.02 per thousand tokens. We conduct all\nthe experiments with less than $100. We recruit\nannotators for human evaluation from friends and\ncolleagues of authors. All annotators are fairly paid\nwith more than $10 per hour.\nAcknowledgments\nThis work is supported in part by NSFC\n(62161160339). We would like to thank the anony-\nmous reviewers for the helpful discussions and sug-\ngestions. For any correspondence, please contact\nYansong Feng.\nReferences\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen-tau Yih, and Yejin\nChoi. 2019. Abductive commonsense reasoning. In\nInternational Conference on Learning Representa-\ntions.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDu-Seong Chang and Key-Sun Choi. 2005. Causal\nrelation extraction using cue phrase and lexical pair\nprobabilities. In International Conference on Natural\nLanguage Processing, pages 61–70. Springer.\nJiangjie Chen, Chun Gan, Sijie Cheng, Hao Zhou,\nYanghua Xiao, and Lei Li. 2022a. Unsupervised\nediting for counterfactual stories. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 36, pages 10473–10481.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022b. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nLi Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin.\n2021. Excar: Event graph knowledge enhanced ex-\nplainable causal reasoning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2354–2363.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nAndrew S Gordon, Cosmin A Bejan, and Kenji Sagae.\n2011. Commonsense causal reasoning using millions\nof personal stories. In Twenty-Fifth AAAI Conference\non Artificial Intelligence.\nYork Hagmayer, Steven A Sloman, David A Lagnado,\nand Michael R Waldmann. 2007. Causal reasoning\nthrough intervention. Causal learning: Psychology,\nphilosophy, and computation, pages 86–100.\n9014\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022. In-\ncontext learning for few-shot dialogue state tracking.\nArXiv, abs/2203.08568.\nPengfei Li and Kezhi Mao. 2019. Knowledge-oriented\nconvolutional neural network for causal relation ex-\ntraction from natural language texts. Expert Systems\nwith Applications, 115:512–523.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori B Hashimoto. 2022. Diffusion-\nlm improves controllable text generation. arXiv\npreprint arXiv:2205.14217.\nZhongyang Li, Tongfei Chen, and Benjamin Van Durme.\n2019. Learning to rank for plausible plausibility. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4818–\n4823.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nXiao Liu, Da Yin, Yansong Feng, Yuting Wu, and\nDongyan Zhao. 2021. Everything has a cause: Lever-\naging causal inference in legal text analysis. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1928–1941.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of code\nare few-shot commonsense learners. arXiv preprint\narXiv:2210.07128.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei\nLi. 2019. Cgmh: Constrained sentence generation\nby metropolis-hastings sampling. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 33, pages 6834–6842.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint arXiv:2203.13474.\nValentina Nisi and Mads Haahr. 2006. Weird view:\ninteractive multilinear narratives and real-life com-\nmunity stories. Crossings, 2:27.\nOpenAI. 2023. Gpt-4 technical report. arXiv.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra\nBhagavatula, Elizabeth Clark, and Yejin Choi. 2019.\nCounterfactual story reasoning and generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 5043–5053.\nLianhui Qin, Vered Shwartz, Peter West, Chandra Bha-\ngavatula, Jena D Hwang, Ronan Le Bras, Antoine\nBosselut, and Yejin Choi. 2020. Back to the future:\nUnsupervised backprop-based decoding for counter-\nfactual and abductive commonsense reasoning. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 794–805.\nLianhui Qin, Sean Welleck, Daniel Khashabi, and\nYejin Choi. 2022. Cold decoding: Energy-based\nconstrained text generation with langevin dynamics.\narXiv preprint arXiv:2202.11705.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nBryan Rink, Cosmin Adrian Bejan, and Sanda\nHarabagiu. 2010. Learning textual graph patterns\nto detect causal event relations. In Twenty-Third In-\nternational FLAIRS Conference.\nSteven Sloman. 2005. Causal models: How people\nthink about the world and its alternatives . Oxford\nUniversity Press.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nJiashuo Wang, Yi Cheng, and Wenjie Li. 2022a.\nCare: Causality reasoning for empathetic responses\nby conditional graph generation. arXiv preprint\narXiv:2211.00255.\nXingyao Wang, Sha Li, and Heng Ji. 2022b.\nCode4struct: Code generation for few-shot structured\nprediction from natural language. arXiv preprint\narXiv:2210.12810.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\n9015\nYuhuai Wu, Albert Q Jiang, Wenda Li, Markus N\nRabe, Charles Staats, Mateja Jamnik, and Christian\nSzegedy. 2022. Autoformalization with large lan-\nguage models. arXiv preprint arXiv:2205.12615.\nFrank F Xu, Uri Alon, Graham Neubig, and Vincent Jo-\nsua Hellendoorn. 2022. A systematic evaluation of\nlarge language models of code. In Proceedings of\nthe 6th ACM SIGPLAN International Symposium on\nMachine Programming, pages 1–10.\nJiayao Zhang, Hongming Zhang, Weijie Su, and Dan\nRoth. 2022. Rock: Causal inference principles for\nreasoning about commonsense causality. In Inter-\nnational Conference on Machine Learning , pages\n26750–26771. PMLR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\n9016\nA Appendix\nA.1 Related Work\nCausal Reasoning There is a growing interest in\nthe NLP community to equip models with causal\nreasoning abilities. Chang and Choi (2005); Gor-\ndon et al. (2011) measure causality between words\nand phrases with statistical methods, Rink et al.\n(2010); Li and Mao (2019) use explicit semantic\ncues, and Liu et al. (2021); Zhang et al. (2022) dis-\ncover causal relations with causal inference meth-\nods like propensity score matching. Li et al. (2019)\nfinetune LLMs on causal event corpus, and Du et al.\n(2021); Wang et al. (2022a) augment LLMs with\ncausal knowledge graphs. Contrast to them, we\nexplore the causal reasoning abilities acquired by\nCode-LLMs in pre-training.\nApplying Code-LLMs to Natural Language\nTasks With the recent development of Code-\nLLMs, several works attempt to solve natural lan-\nguage tasks with code models. They mainly focus\non two areas: numerical reasoning and structural\nprediction. Gao et al. (2022); Chen et al. (2022b);\nWu et al. (2022) apply Code-LLMs to numerical\nreasoning. They generate programs with Code-\nLLMs and feed the programs into an external inter-\npreter to derive the answer. Madaan et al. (2022);\nWang et al. (2022b) leverage the text-to-structure\ntranslation ability of Code-LLMs to perform struc-\ntural prediction tasks. They ask models to gen-\nerate structures in the form of code, and convert\nthe generated code into the task output format. In\naddition, Hu et al. (2022) takes advantages of Code-\nLLMs on text-to-SQL generation. Different from\nthem, we leverage the causal reasoning ability of\nCode-LLMs, and ask them to generate natural lan-\nguage events given the causal structure.\nA.2 Prompts\nFigure A.1 demonstrates the prompts of probing\nDAVINCI . Specifically, the language conversion is\nmade automatically by CODEX with the instruction\n# python to java/c. Figure A.2 shows the inter-\nventions on code prompts for abductive reasoning.\nA.3 Models for Comparison\nWe compare with previous unsupervised meth-\nods on the two tasks, including DELOREAN (Qin\net al., 2020), COLD (Qin et al., 2022), and DIFFU -\nSION (Li et al., 2022) on abductive reasoning; and\nCGMH (Miao et al., 2019), EDUCAT (Chen et al.,\n2022a), DELOREAN , and COLD on counterfactual\nreasoning. Among them, DELOREAN and COLD\nare constraint-based models. They regard the task\nrequirements as constraints (for example, the gener-\nated text should be consistent with the premise, and\ncoherent with the ending in the abductive reason-\ning task), and iteratively update text representation\nto meet the constraints. CGMH and EDUCAT are\nediting-based models targeted for counterfactual\nreasoning. They start from the original ending and\nedit it to meet the counterfactual context. DIFFU -\nSION builds a controllable LM based on continuous\ndiffusions to perform control tasks including abduc-\ntive reasoning.\nA.4 Additional Results\nMin-Edit BERTScore\nDELOREAN 52.9 73.7\nCOLD 56.8 73.5\nCODEX 58.0 79.5\nTable A.1: Counterfactual reasoning results in the first-\nsentence setting (%).\nFirst-Sentence Setting of Counterfactual Rea-\nsoning Endings in the original counterfactual\nreasoning data TimeTravel are of three sentences.\nDue to the computation constraint of COLD (Qin\net al., 2022), it is evaluated in a first-sentence set-\nting: only the first sentence of the original end-\ning is used, and models are asked to generate a\none-sentence counterfactual ending. We conduct\nexperiments in the first-sentence setting with the\nmetrics used in Qin et al. (2022). As shown in Ta-\nble A.1, CODEX outperforms previous methods in\nthis setting.\nIntervention on Counterfactual ReasoningTa-\nble A.2 demonstrates the intervention results\non counterfactual reasoning. The observations\nare similar to those in the abductive reasoning\ntask: changes in the programming structure affect\nCODEX ’s performance largely, changes in the in-\nformation affect less, and CODEX is robust towards\nformat and language changes.\nOne-shot Setting We also conduct experiments\nin the one-shot setting. Models are shown with\none demonstration example in the in-context learn-\ning manner, and the example is identical among\nthe models. As shown in Table A.3, both\nDAVINCI 002 and CODEX are better than in the\n9017\n: model output\nAbductive Reasoning\nGenerate a plausible explanatory hypothesis given the premise \nand the ending.\nPremise: The Smiths were having holidays done of the children.\nEnding: Ty's face lit up as he ran to the new toy, happily \nposing for photos.\nHypothesis: The Smiths were having holidays.\nGiven an original story and an intervening counterfactual event, the task \nis to minimally revise the story to make it compatible with the given \ncounterfactual event.\nPremise: Janice was excited to bring cupcakes to her work for her \nbirthday.\nInitial event: She worked all day on making the perfect frosting.\nOriginal ending: Each cupcake was truly a work of art. Everyone at her \nwork loved them. Janice was thrilled and happy for the rest of the day.\nCounterfactual event: She completely rushed making the frosting.\nNew ending: Each cupcake was a mess. The frosting was lumpy and tasted \nterrible. Janice was embarrassed and felt terrible for the rest of the \nday.\nCounterfactual Reasoning\nFigure A.1: Example text prompts of abductive reasoning and counterfactual reasoning.\nBLEU4 ROUGEL BERTScore\nCODEX 66.8 70.0 82.5\nNo Instruction 55.4 60.1 77.0Information Function Name Perturbation 65.4 69.0 82.2\nSequential Structure 43.4 50.2 68.2Structure Disruption 16.0 23.5 55.2\nClass 63.6 67.4 81.1\nPrint 73.3 74.7 85.3Format\nReturn 69.4 70.5 83.0\nJava 71.1 73.5 84.5Language C 71.9 74.2 85.0\nTable A.2: Intervention results on counterfactual reasoning (%).\nzero-shot setting, while CODEX still largely outper-\nforms DAVINCI 002, showing that the advantage of\nCODEX is robust across different settings.\nA.5 Case Study\nWe randomly select some generation examples\nand demonstrate them in Table A.4. Comparing\nCODEX and DAVINCI , CODEX generations are\nmore coherent with the context, while DAVINCI\nsometimes cannot take into account the premise.\nCODEX also understands the task instruction well\nand better preserves the original ending in counter-\nfactual reasoning. Generations of more powerful\nLLMs like CHATGPT and GPT-4 are coherent\nwith the context, but they add much detail and\nbarely keep the original ending. Although open\nsource Code-LLMs like CODE GEN and STAR-\nCODER can follow the code prompts and gener-\nate sentences in the required format, most of their\noutputs are inconsistent with the premise and the\nending.\n9018\nBLEU4 ROUGEL CIDEr BERTScore\nDAVINCI002 8.2 33.5 55.9 61.7\nCODEX 17.9 42.3 91.7 67.1\n(a) Abductive reasoning.\nBLEU4 ROUGEL BERTScore\nDAVINCI002 53.5 58.8 76.0\nCODEX 74.3 76.2 86.1\n(b) Counterfactual reasoning.\nTable A.3: Evaluation results in the one-shot setting (%).\nAbductive Reasoning\nPremise: Angie went to a cocktail party hosted by her best friend.\nEnding: Angie decided to be quiet about what she overheard.\nHypothesis [CODEX ]: Angie overheard her best friend talking about her behind her back.\nHypothesis [DAVINCI 002]: Angie decided to be quiet about what she overheard because she didn’t want to start any drama\nwith her best friend.\nHypothesis [DAVINCI 003]: Angie overheard a conversation between two people at the party that made her uncomfortable, so\nshe decided to stay quiet and not tell anyone what she heard.\nHypothesis [CHATGPT]: Angie overheard something that she felt was better left unsaid.\nHypothesis [GPT-4]: Angie overheard a sensitive conversation that she felt was not her place to share.\nHypothesis [BARD ]: Angie overheard something at the party that she did not want to share with her best friend.\nHypothesis [CODE GEN]: Angie’s best friend was a bartender.\nHypothesis [STARCODER ]: If judge is cheating on Annie, then judge was correct in trolling an English major.\nCounterfactual Reasoning\nPremise: My grandpa took my brothers and I fishing in the summer.\nInitial Event:We went to a small pond close to his house.\nOriginal Ending:He set up our fishing poles and we began to fish. It was so boring when nobody caught any fish. We were\ndisappointed to go home empty handed that day.\nCounterfactual Event:We caught four huge trouts within the first hour.\nCounterfactual Ending [CODEX ]: He set up our fishing poles and we began to fish. It was so exciting when we caught four\nhuge trouts within the first hour. We were happy to go home with four trouts that day.\nCounterfactual Ending [DAVINCI 002]: We were so excited to have caught four trouts within the first hour! We were proud\nto show our grandpa our catches.\nCounterfactual Ending [DAVINCI 003]: He set up our fishing poles and we began to fish. To our pleasant surprise, we caught\nfour huge trouts within the first hour. We were delighted to take home our catches that day.\nCounterfactual Ending [CHATGPT]: As soon as we cast our lines, we felt a tug and reeled in four huge trouts within the\nfirst hour. We couldn’t believe our luck and spent the rest of the day catching even more fish. Grandpa was proud of us and\nwe were thrilled to bring home a big haul for dinner. It was a memorable day of fishing that we would never forget.\nCounterfactual Ending [GPT-4]: He set up our fishing poles and we began to fish. To our surprise, we caught four huge\ntrouts within the first hour. We were thrilled with our success and couldn’t wait to show off our catch when we got home.\nCounterfactual Ending [BARD ]: We caught four huge trouts within the first hour! We were so excited that we forgot all\nabout being bored. We had a great time fishing with grandpa that day.\nCounterfactual Ending [CODE GEN]: He set up our fishing poles and we began to fish. It was so boring when nobody\ncaught any fish. We were disappointed to go home empty handed that day. We caught four huge trouts within the first hour.\nCounterfactual Ending [STARCODER ]: We were finally happy to catch all of the trouts. And while he is a very kind man,\nyou will never see him again. We will always love our old family in China better than ever before.\nTable A.4: Examples of model generations.\n9019\ndef main():\n    premise()\n    if hypothesis():\n        ending()\ndef premise():\n    # The Smiths were having holidays done of the children.\ndef ending(): \n    # Ty's face lit up as he ran to the new toy, happily \nposing for photos.\ndef hypothesis(): \n    #\n1. Information: No instruction\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\ndef main():\n    functionA()\n    if functionB():\n        functionC()\ndef functionA():\n    # The Smiths were having holidays done of the children.\ndef functionC(): \n    # Ty's face lit up as he ran to the new toy, happily \nposing for photos.\ndef functionB(): \n    # \n2. Information: Function Name Perturbation\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\ndef main():\n    premise()\n    hypothesis()\n    ending()\ndef premise():\n    # The Smiths were having holidays done of the children.\ndef ending(): \n    # Ty's face lit up as he ran to the new toy, happily \nposing for photos.\ndef hypothesis(): \n    #\n3. Structure: Sequential Structure\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\ndef main():\n    functionA()\n    if functionB():\n        functionC()\ndef functionA():\n    # The Smiths were having holidays done of the children.\ndef functionB(): \n    # Ty's face lit up as he ran to the new toy, happily \nposing for photos.\ndef functionC(): \n    #\n4. Structure: Disruption\n6. Format: Print\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\ndef main():\n    premise()\n    if hypothesis():\n        ending()\ndef premise():\n    print(“The Smiths were having holidays done of the \nchildren.”)\ndef ending(): \n    print(“Ty's face lit up as he ran to the new toy, happily \nposing for photos.”)\ndef hypothesis(): \n    print(“\n7. Format: Return\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\ndef main():\n    premise()\n    if hypothesis():\n        ending()\ndef premise():\n    return(“The Smiths were having holidays done of the \nchildren.”)\ndef ending(): \n    return(“Ty's face lit up as he ran to the new toy, happily \nposing for photos.”)\ndef hypothesis(): \n    return(“\n# task: generate a plausible explanatory hypothesis given the \npremise and the ending\nclass Story:\n    def __init__(self):\n        self.premise()\n        if self.hypothesis()\n            self.ending()\n    def premise(self):\n        # The Smiths were having holidays done of the \nchildren.\n    def ending(self): \n        # Ty's face lit up as he ran to the new toy, happily \nposing for photos.\n    def hypothesis(self): \n        #\n5. Format: Class\n8. Language: Java\n// task: generate a plausible explanatory hypothesis given the \npremise and the ending\npublic class Story {\n    public static void main(String[] args) {\n        premise();\n        if (hypothesis()) {\n            ending();\n        }\n    }\n    public static void premise() {\n        // The Smiths were having holidays done of the \nchildren.\n    }\n    public static void ending() {\n        // Ty's face lit up as he ran to the new toy, happily \nposing for photos.\n    }\n    public static boolean hypothesis() {\n        //\n// task: generate a plausible explanatory hypothesis given the \npremise and the ending\nint main() {\n    premise();\n    if (hypothesis()) {\n        ending();\n    }\n}\nvoid premise() {\n    // The Smiths were having holidays done of the children.\n}\nvoid ending() {\n    // Ty's face lit up as he ran to the new toy, happily \nposing for photos.\n}\nint hypothesis() {\n    //\n9. Language: C\nFigure A.2: Examples of code prompt interventions in abductive reasoning.\n9020\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitation Section\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1. Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n3. Evaluation\n□\u0013 B1. Did you cite the creators of artifacts you used?\n3. Evaluation\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. In the supplementary data\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n3. Evaluation\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. The data we use is created and checked by previous work.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLimitation Section\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3. Evaluation\nC □\u0013 Did you run computational experiments?\n3 & 4\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. The parameters and computational budget are not public available.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9021\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLimitation Section\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n3. Evaluation\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nIn the supplementary code\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nAppendix A.4\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nThe instructions are brieﬂy introduced in Appendix A.4\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nEthics Statement\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix A.4\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Ethics review is not required.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Ethics Statement\n9022",
  "topic": "Causal reasoning",
  "concepts": [
    {
      "name": "Causal reasoning",
      "score": 0.7192307114601135
    },
    {
      "name": "Computer science",
      "score": 0.6254106163978577
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.6215447187423706
    },
    {
      "name": "Key (lock)",
      "score": 0.48385322093963623
    },
    {
      "name": "Causal model",
      "score": 0.48150619864463806
    },
    {
      "name": "Code (set theory)",
      "score": 0.455074667930603
    },
    {
      "name": "Cognitive science",
      "score": 0.45181989669799805
    },
    {
      "name": "Magic bullet",
      "score": 0.44150957465171814
    },
    {
      "name": "MAGIC (telescope)",
      "score": 0.4361036717891693
    },
    {
      "name": "Verbal reasoning",
      "score": 0.4287588596343994
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39037075638771057
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3614325523376465
    },
    {
      "name": "Programming language",
      "score": 0.3550034761428833
    },
    {
      "name": "Natural language processing",
      "score": 0.34750446677207947
    },
    {
      "name": "Cognition",
      "score": 0.31370285153388977
    },
    {
      "name": "Psychology",
      "score": 0.2989175021648407
    },
    {
      "name": "Social psychology",
      "score": 0.20320415496826172
    },
    {
      "name": "Mathematics",
      "score": 0.07814133167266846
    },
    {
      "name": "Computer security",
      "score": 0.07640323042869568
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Bioinformatics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}