{
  "title": "DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding",
  "url": "https://openalex.org/W3217374296",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2124400109",
      "name": "Taolin Zhang",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2108884846",
      "name": "Chengyu Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2071140508",
      "name": "Nan Hu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2114674632",
      "name": "Minghui Qiu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3044201757",
      "name": "Chengguang Tang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2115656599",
      "name": "Xiaofeng He",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097175565",
      "name": "Jun Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2124400109",
      "name": "Taolin Zhang",
      "affiliations": [
        "East China Normal University",
        "Shanghai Key Laboratory of Trustworthy Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2071140508",
      "name": "Nan Hu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2115656599",
      "name": "Xiaofeng He",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3171331476",
    "https://openalex.org/W3170603656",
    "https://openalex.org/W6750611836",
    "https://openalex.org/W3126974869",
    "https://openalex.org/W6766886201",
    "https://openalex.org/W6797076564",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2949695381",
    "https://openalex.org/W3092171032",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W3172741267",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2973840669",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3162323273",
    "https://openalex.org/W3091617571",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W6787635146",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3188635098",
    "https://openalex.org/W3103673392",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W3176680950",
    "https://openalex.org/W3175234986",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970106668",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3176443840",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W2123142779",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4287366208",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W3175423875",
    "https://openalex.org/W2984147501",
    "https://openalex.org/W3182352988",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3176887068",
    "https://openalex.org/W3175277088"
  ],
  "abstract": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained models with relation triples injecting from knowledge graphs to improve language understanding abilities.Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. To guarantee effective knowledge injection, previous studies integrate models with knowledge encoders for representing knowledge retrieved from knowledge graphs. The operations for knowledge retrieval and encoding bring significant computational burdens, restricting the usage of such models in real-world applications that require high inference speed. In this paper, we propose a novel KEPLM named DKPLM that decomposes knowledge injection process of the pre-trained language models in pre-training, fine-tuning and inference stages, which facilitates the applications of KEPLMs in real-world scenarios. Specifically, we first detect knowledge-aware long-tail entities as the target for knowledge injection, enhancing the KEPLMs' semantic understanding abilities and avoiding injecting redundant information. The embeddings of long-tail entities are replaced by ``pseudo token representations'' formed by relevant knowledge triples. We further design the relational knowledge decoding task for pre-training to force the models to truly understand the injected knowledge by relation triple reconstruction. Experiments show that our model outperforms other KEPLMs significantly over zero-shot knowledge probing tasks and multiple knowledge-aware language understanding tasks. We further show that DKPLM has a higher inference speed than other competing models due to the decomposing mechanism.",
  "full_text": "DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for\nNatural Language Understanding\nTaolin Zhang1,3*, Chengyu Wang2*, Nan Hu5, Minghui Qiu2†, Chengguang Tang2\nXiaofeng He4,5†, Jun Huang2\n1 School of Software Engineering, East China Normal University 2 Alibaba Group\n3 Shanghai Key Laboratory of Trsustworthy Computing\n4 NPPA Key Laboratory of Publishing Integration Development, ECNUP\n5 School of Computer Science and Technology, East China Normal University\nzhangtl0519@gmail.com, hunan.vinny1997@gmail.com, hexf@cs.ecnu.edu.cn\n{chengyu.wcy, minghui.qmh, chengguang.tcg, huangjun.hj}@alibaba-inc.com\nAbstract\nKnowledge-Enhanced Pre-trained Language Models (KE-\nPLMs) are pre-trained models with relation triples injecting\nfrom knowledge graphs to improve language understanding\nabilities.Experiments show that our model outperforms other\nKEPLMs significantly over zero-shot knowledge probing tasks\nand multiple knowledge-aware language understanding tasks.\nTo guarantee effective knowledge injection, previous studies\nintegrate models with knowledge encoders for representing\nknowledge retrieved from knowledge graphs. The operations\nfor knowledge retrieval and encoding bring significant com-\nputational burdens, restricting the usage of such models in\nreal-world applications that require high inference speed. In\nthis paper, we propose a novel KEPLM named DKPLM that\ndecomposes knowledge injection process of the pre-trained\nlanguage models in pre-training, fine-tuning and inference\nstages, which facilitates the applications of KEPLMs in real-\nworld scenarios. Specifically, we first detect knowledge-aware\nlong-tail entities as the target for knowledge injection, en-\nhancing the KEPLMs’ semantic understanding abilities and\navoiding injecting redundant information. The embeddings\nof long-tail entities are replaced by “pseudo token represen-\ntations” formed by relevant knowledge triples. We further\ndesign the relational knowledge decoding task for pre-training\nto force the models to truly understand the injected knowledge\nby relation triple reconstruction. Experiments show that our\nmodel outperforms other KEPLMs significantly over zero-\nshot knowledge probing tasks and multiple knowledge-aware\nlanguage understanding tasks. We further show that DKPLM\nhas a higher inference speed than other competing models due\nto the decomposing mechanism.\nIntroduction\nRecently, Pre-trained Language Models (PLMs) improve var-\nious downstream NLP tasks significantly (He et al. 2020; Xu\net al. 2021; Chang et al. 2021). In PLMs, the two-stage strat-\negy (i.e., pre-training and fine-tuning) (Devlin et al. 2019)\ninherits the knowledge learned during pre-training and ap-\nplies it to downstream tasks. Although PLMs have stored a\n* T. Zhang and C. Wang contributed equally to this work.\n† Co-corresponding authors.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nlot of internal knowledge, it can hardly understand external\nbackground knowledge such as factual and commonsense\nknowledge (Colon-Hernandez et al. 2021; Cui et al. 2021).\nHence, the performance of PLMs can be improved by in-\njecting external knowledge triples, which are referred to as\nKnowledge-Enhanced PLMs (KEPLMs).\nIn the literature, the approaches of injecting knowledge\ncan be divided into two categories, including knowledge\nembedding and joint learning. (1) Knowledge embedding\nbased approaches inject triple representations in Knowledge\nGraphs (KGs) trained by knowledge embedding algorithms\n(e.g., TransE (Bordes et al. 2013)) into contextual representa-\ntions via well-designed feature fusion modules, which may\ncontain a large number of parameters (Zhang et al. 2019;\nPeters et al. 2019; Su et al. 2020). As reported in (Wang et al.\n2019b), different knowledge representation algorithms signif-\nicantly impact the performance of PLMs. (2) Joint learning\nbased approaches learn knowledge embeddings from KGs\njointly during pre-training (Wang et al. 2019b; Sun et al.\n2020; Liu et al. 2020), which are two significantly different\ntasks. We also observe that there are two potential drawbacks\nof previous methods. (1) These models inject knowledge\nindiscriminately into all entities in pre-training sentences,\nwhich introduces redundant and irrelevant information to\nPLMs (Zhang et al. 2021). (2) Large-scale KGs are required\nduring both fine-tuning and inference for obtaining outputs\nof knowledge encoders. This incurs additional computation\nburden that limits their usage for real-world applications that\nrequire high inference speed (Zhang et al. 2020).\nTo overcome the above problems, we present a novel KE-\nPLM named DKPLM that decomposes knowledge injection\nprocess of three stages for KEPLMs. A comparison between\nour model and other models is shown in Figure 1. Clearly,\nfor DKPLM, knowledge injection is only applied during\npre-training, without using additional knowledge encoders.\nHence, during the fine-tuning and inference stages, our model\ncan be utilized in the same way as that of BERT (Devlin et al.\n2019) and other plain PLMs, which facilitates the applica-\ntions of our KEPLM in real-world scenarios. Specifically, we\nintroduce three novel techniques for pre-training DKPLM:\n• Knowledge-aware Long-tail Entity Detection: our model\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11703\nPLMs\nKEPLMs\nDKPLM\nPre-trainingFine-tuning InferenceCorpus Dataset Sample\nCorpus+Know. Sources+Know. EncoderCorpus+Know. Sources+Know. EncoderCorpus+Know. Sources+Know. Encoder\nCorpus+Know. Sources Dataset Sample\nFigure 1: Comparison between DKPLM and other models. (1) Plain PLMs do not utilize external knowledge in all the three\nstages. (2) Existing KEPLMs utilize various knowledge sources (e.g., KGs and dictionaries) to enhance understanding abilities\nby using knowledge encoders in all the three stages. (3) During pre-training, DKPLM utilizes the same data sources as KEPLMs,\nwith no knowledge encoder (e.g., Neural Networks and Graph Neural Networks) required. During fine-tuning and inference, our\nmodel does not require KGs and is highly flexible and efficient. (Best viewed in color.)\ndetects long-tail entities for knowledge injection based\non the frequencies in the corpus, the number of adjacent\nentities in the KG and the semantic importance. In this\nway, we avoid learning too much redundant and irrelevant\ninformation (Zhang et al. 2021).\n• Pseudo Token Representation Injection: we replace the\nembeddings of detected long-tail entities with the repre-\nsentations of the corresponding knowledge triples gen-\nerated by the shared PLM encoder, referred to “pseudo\ntoken representations”. Hence, the knowledge is injected\nwithout introducing any extra parameters to the model.\n• Relational Knowledge Decoding: for a relation triple, we\nuse the representations of one entity and the relation pred-\nicate to decode each token of another entity. This pre-\ntraining task acts as a supervised signal for the KEPLM,\nforcing the model to understand what knowledge is in-\njected to the KEPLM.\nIn the experiments, we evaluate our model against strong\nbaseline KEPLMs pre-trained using the same data sources\nover various knowledge-related tasks, including knowledge\nprobing (LAMA) (Petroni et al. 2019), relation extraction and\nentity typing. For knowledge probing, the top-1 accuracy of\nfour datasets is increased by +1.57% on average, compared\nwith state-of-the-art. Meanwhile, in other tasks, our model\nalso achieves consistent improvement. In summary, we make\nthe following contributions in this paper:\n• We present a novel KEPLM named DKPLM to inject\nthe knowledge into PLMs, which specifically focuses on\nlong-tail entities, decomposing the knowledge injection\nprocess of three PLMs’ stages.\n• A dual knowledge injection process including encoding\nand decoding for long-tail entities is proposed to pre-train\nDKPLM, consisting of three modules: Knowledge-aware\nLong-tail Entity Detection, Pseudo Token Embedding\nInjection, and Relational Knowledge Decoding.\n• In the experiments, we evaluate DKPLM over multiple\npublic benchmark datasets, including knowledge prob-\ning (LAMA) and knowledge-aware tasks. Experimental\nresults show that DKPLM consistently outperforms state-\nof-the-art methods.\nRelated Work\nPLMs. Following BERT (Devlin et al. 2019), many PLMs\nhave been proposed to further improve performance in var-\nious NLP tasks. We summarize the recent studies, specifi-\ncally focusing on three techniques, including self-supervised\npre-training, model architectures and multi-task learning.\nTo improve the model’s semantic understanding, several\napproaches extend BERT by employing novel token-level\nand sentence-level pre-training tasks. Notable PLMs include\nBaidu-ERNIE (Sun et al. 2019), StructBERT (Wang et al.\n2020) and spanBERT (Joshi et al. 2020). Other models boost\nthe performance by changing the internal encoder archi-\ntectures. For example, XLNet (Yang et al. 2019) utilizes\nTransformer-XL (Dai et al. 2019) to encoder long sequences\nby permutation in language tokens. Sparse self-attention (Cui\net al. 2019) replaces the self-attention mechanism with more\ninterpretable attention units. Yet other PLMs such as MT-\nDNN (Liu et al. 2019) combine self-supervised pre-training\nwith supervised learning to improve the performance of vari-\nous GLUE tasks (Wang et al. 2019a).\nKEPLMs. As plain PLMs are only pre-trained on large-scale\nunstructured corpora, they lack the language understanding\nabilities of important entities. Hence, KEPLMs use structured\nknowledge to enhance the language understanding abilities\nof PLMs. We summary recent KEPLMs grouped into the\nfollowing three types: (1) Knowledge-enhancement by entity\nembeddings. For example, ERNIE-THU (Zhang et al. 2019)\ninjects entity embeddings into contextual representations via\nknowledge-encoders stacked by the information fusion mod-\nule. KnowBERT (Peters et al. 2019) introduces the knowl-\nedge attention and recontextualization (KAR) and entity link-\ning to inject knowledge embeddings to PLMs. (2) Knowledge-\nenhancement by entity descriptions. These studies learn entity\nembeddings by knowledge descriptions. For example, pre-\ntraining corpora and entity descriptions in KEPLER (Wang\net al. 2019b) are encoded into a unified semantic space within\n11704\n.QRZOHGJH\u0003*UDSK 3U H\u0010WUDLQLQJ\u0003&RUSXV\n/RQJ\u0010WDLO\u0003(QWLW\\\u0003'HWHFWLRQ\n΀>d\u001c΁ ΀>d\u001c΁ ͙ ͙ ΀>d\u001c΁ ΀>d\u001c΁\n΀>d\u001c΁ ΀>d\u001c΁ ͙ ͙ ΀>d\u001c΁ ΀>d\u001c΁\n5HSODFHG\u0003(PE\u0011\n2XWSXW\u0003WRNHQV\n7UDQVIRUPHU\u0003\n/D\\HUV 0XOWL\u0010+HDG\u00036HOI\u0010$WWHQWLRQ\u0003\t\u0003))1\u0003\u000b1\f\n<ĞǀŝŶ\u0003\u0018ƵƌĂŶƚ͕\u0003\u0011ƌŽŽŬůǇŶ\u0003EĞƚƐ\u0003ƉůĂǇĞƌ\u0003ŝŶ\u0003E\u0011\u0004͕\u0003\u0003ůĞĚ\u0003ƚŚĞ\u0003\nh͘^͘\u0003ŵĞŶΖƐ\u0003ďĂƐŬĞƚďĂůů\u0003ƚĞĂŵ\u0003ƚŽ\u0003ƚŚĞ\u0003ϮϬϮϭ\u0003KůǇŵƉŝĐ\u0003\n'ĂŵĞƐ\u0003ŝŶ\u0003:ĂƉĂŶ͘\u0003\n<ĞǀŝŶ\u0003 \u0018Ƶ ƌĂŶƚ E\u0011\u0004 ΀D\u0004^<΁ ƚŚĞ\n<ĞǀŝŶ\u0003 \u0018ƵƌĂŶ͘ E\u0011\u0004 ΀D\u0004^<΁ ƚŚĞ\n3UH\u0010WUDLQLQJ\u0003\nWDVNV\ndŽŬĞŶͲůĞǀĞů\u0003D>Dф^\u001cх EĞǁ zŽƌŬ ф\u001c\u001cх\nDŽĚĞů\u001cŶƚŝƚǇ\u0003WƌŽĐĞƐƐŝŶŐ\u0018ĂƚĂ\u0003^ŽƵƌĐĞƐ\n3VHXGR\u00037RNHQ\u0003(PEHGGLQJ\n\u0011ƌŽŽŬůǇŶ\nŚ ƚ\n\u000eU \u0010U\nORFDWHG\u0003LQ\nEĞƚƐ\n>ŽŶŐͲƚĂŝů\u0003\u001cŶƚŝƚǇ\nDƵůƚŝͲŚŽƉ\u0003&ŝůƚĞƌŝŶŐ\n\u001cŶƚŝƚǇ\u0003&ƌĞƋ͘\nc d\n<ĞǀŝŶ\u0003\u0018ƵƌĂŶƚ͕\u0003\u0011ƌŽŽŬůǇŶ\u0003EĞƚƐ\u0003ƉůĂǇĞƌ͙\n<ĞǀŝŶ\u0003\u0018ƵƌĂŶƚ͕\u0003΀h,E΁\u0003΀h,E΁\u0003ƉůĂǇĞƌ͙\n\u0017\u0003\u000f e\nZĞůĂƚŝŽŶĂů\u0003<ŶŽǁůĞĚŐĞ\u0003\u0018ĞĐŽĚĞƌ\n7RNHQ\u0003(PE\u0011 EĞƚƐ ͙ ͙ KůǇ͘\u0018ƵƌĂŶƚ E\u0011 \u0004 ΀D\u0004^<΁ ƚŚĞ\u0011ƌŽŽŬ͘<ĞǀŝŶ\u0003 'Ă ŵ͘\n\u001cŶƚŝƚǇ\u0003ƐĞŵĂŶƚŝĐƐ\nFigure 2: Overview of DKPLM. (1) Data Sources: large-scale pre-training corpora and relation triplets extracted from KGs. (2)\nInput Data: DKPLM detects long-tail entities and retrieves relation triples for learning “pseudo token embeddings”. (3) Model:\nPlain PLMs can be used as model backbones. We also propose the relational knowledge decoder for better knowledge injection.\n(Best viewed in color.)\nthe same PLM. (3) Knowledge-enhancement by converted\ntriplet’s texts. K-BERT (Liu et al. 2020) and CoLAKE (Sun\net al. 2020) convert relation triplets into texts and insert them\ninto training samples without using pre-trained embeddings.\nThese KEPLMs require knowledge encoder modules with\nadditional parameters to inject the knowledge into context-\naware hidden representations generated by PLMs. Previous\nstudies (Petroni et al. 2019; Cao et al. 2021) have also shown\nthat the semantics of high-frequency and general knowledge\ntriples are already captured by plain PLMs, and express re-\ndundant knowledge. In this paper, we argue that enhancing\nthe understanding ability of long-tail entities can further ben-\nefit the context-aware representations of PLMs, which is one\nof the major focus of this work.\nDKPLM: The Proposed Model\nWe first state some basic notations. Denote an input sequence\nof tokens as {w1, w2, . . . , wn}, where n is the length of\nthe input sequence. The hidden representation of input to-\nkens obtained by PLMs is denoted as {h1, h2, . . . , hn}and\nhi ∈Rd1 , where d1 is the dimension of the PLM’s output.\nFurthermore, we denote the knowledge graph as G= (E, R)\nwhere Eand Rare the collections of entities and relation\ntriples, respectively. In the KG, a relational knowledge triple\nis denoted as (eh, r, et), where eh and et refer to the head\nentity and the tail entity, respectively.r is the specific relation\npredicate between eh and et.\nOur pre-training process specifically focuses on the knowl-\nedge injection and decoding on certain tokens in the detected\nlong-tail entities. Specifically, we aim to solve three research\nquestions:\n• RQ1: What types of tokens in the pre-training corpus\nshould be detected for knowledge injection ?\n• RQ2: How can we inject knowledge to selected tokens\nwithout additional knowledge encoders ?\n• RQ3: How can we verify the effectiveness of injected\n11705\nFigure 3: The distribution of entity frequencies in the English\nWikipedia corpus.\nrelation triples during pre-training ?\nThe overall framework of DKPLM is presented in Figure\n2. In the following, we introduce the techniques of DKPLM\nand discuss how we can address the three research questions.\nKnowledge-aware Long-tail Entity Detection\nMotivation and Analysis. We first extract structured knowl-\nedge triples from large-scale KGs, and link entities in KGS\nto the target mentions in the pre-training samples by entity\nlinking tools (e.g., TAGME (Ferragina and Scaiella 2010)).\nFor a better understanding of how entities are distributed in\nthe corpus, we plot the distribution of the entity frequencies\nin the entire Wikipedia corpus, shown in Figure 3. As seen, it\nclosely follows the power-law distributionwith the formula\nas follows:\nF req(e) = C\nrankα , (1)\nwhere C and α are hyper-parameters, rank is the entity fre-\nquency rank and F req(e) is the frequency of the entity e. We\ncan see that, while a few entities frequently appear, most of\nthe entities seldom occur in the pre-training corpus, making it\ndifficult for PLMs to learn better contextual representations.\nAs reported by (Zhang et al. 2021), the high-frequency\nrelational triples are injected into PLMs is NOT always ben-\neficial for downstream tasks. This practice is more likely to\ntrigger negative knowledge infusion. Hence, knowledge in-\njection for long-tail entities instead of all entities that occur\nin the corpus may further improve the understanding abilities\nof PLMs. It should be further noted that the above analysis\nof entities only considers the frequencies in the pre-training\ncorpus, ignoring the information of each entity in KGs and\nthe importance of such entities in a sentence. In the following,\nwe present the Knowledge-aware Long-tail Entity Detection\nmechanism to select target entities for knowledge injection.\nOur Approach. In our work, we consider three neighbor-\ning information of entities to characterize the “long-tailness”\nproperty of entities, namely the entire pre-training corpus,\ncurrent input sentence and the KG. For a specific entity e, we\nconsider the three following factors:\n• Entity Frequency: the entity frequency w.r.t. the entire\npre-training corpus, denoted as F req(e);\n• Semantic Importance: the importance of the entity e in\nthe sentence, denoted as SI(e);\n• Knowledge Connectivity: the number of multi-hops\nneighboring nodes w.r.t. the entity\ne in the KG, denoted\nas KC(e).\nWhile the computation of F req(e) is quite straightfor-\nward, it is necessary to elaborate the computation of SI(e)\nand KC(e). SI(e) refers to the semantic similarity between\nthe representation of the sentence containing the entity e\nand the representation of the sentence with e being replaced.\nThe greater the similarity between sentences is, the smaller\nthe influence on the sentence semantics is when the entity\nis replaced. Denote\nho and hrep as the representations of\nthe original sentence and the sentence after entity replacing.\nFor simplicity, we use the reciprocal of cosine similarity to\nmeasure SI(e):\nSI(e) =\nhT\no\n ·∥hrep∥\nhTo ·hrep\n(2)\nIn the implementation, we use the special token “[UHN]” to\nreplace the entity e in the sentence.\nKC(e) represents the importance of entity e in triples’\nneighboring structure and we use the multi-hops neighboring\nentities’ number to calculate KC(e):\nKC(e) = [|N(e)|]Rmax\nRmin\n(3)\nN(e) ≜ {e′|Hop (e′, e) < Rhop ∧e′∈E} (4)\nwhere Rmin and Rmax are pre-defined thresholds. Specifi-\ncally, we constrain the number of hops for computingKC(e)\nis in the range of Rmin to Rmax. |·| means the neighbor-\ning entity number in the set. The Hop function denotes the\nnumber of multi-hops between entity e and entity e′in KGs’\nstructure. The degree of the “knowledge-aware long-tailness”\nKLT (e) of the entity e is then calculated as:\nKLT (e) =I{Freq (e)<Rfreq }·SI(e) ·KC(e) (5)\nwhere the I{x}is the indicator function withx to be a Boolean\nexpression. Rfreq is a pre-defined threshold. Given a sen-\ntence, we detect all the entities and regard the entities with the\nKLT (e) score lower than the average as knowledge-aware\nlong-tail entities.\nPseudo Token Embedding Injection\nIn order to enhance the PLMs’ understanding abilities of long-\ntail entities, we inject knowledge triples into the positions\nof such entities without introducing any other parameters.\nInspired by the KG embedding algorithms (Bordes et al.\n2013), if an entity in the pre-training sentence is a head entity\neh of knowledge triples, the representation of eh is modeled\nby the following function:\nheh = het −hr (6)\nwhere heh , het and hr are the representations of the head\nentity eh, the tail entity et and the relation predicate r, re-\nspectively. Similarly, if an entity is a tail entityet in the KG,\nwe have: het = heh + hr.\n11706\nSpecifically, we use the underlying PLM as the shared en-\ncoder to acquire the knowledge representations. Consider the\nsituation where the entity is a head entity eh in the KG. We\nconcatenate the tokens of the tail entity et and the relation\npredicate r, and feed them to the PLM. The token representa-\ntions of the last layer of the PLM are denoted as F(et) and\nF(r), respectively.\nThe pseudo token representations het and hr are then\ncomputed as follows:\nhet = LN(σ (fsp (F(et)) Wet )) (7)\nhr = LN(σ (fsp (F(r)) Wr)) (8)\nwhere LNis the LayerNorm function (Ba, Kiros, and Hinton\n2016) and fsp is the self-attentive pooling operator (Lin et al.\n2017) to generate the span representations. Wet and Wr are\ntrainable parameters.\nSince the lengths of the entity and the relation predicate are\nusually short, the generated representations by the PLM may\nbe not expressive. We further consider the description text of\nthe target entity, denoted as edes\nh . Let F(edes\nh )\nbe the token\nsequence representations of edes\nh\n, generated by the PLM. We\ndenote the pseudo token embedding heh of the head entity\neh as follows:\nheh = tanh\n(\n(het −hr) ⊕F(edes\nh )\n)\nWeh , (9)\nwhere ⊕refers to the concatenation of two representations,\nand Weh is the trainable parameter.\nFinally, we replace the representations of detected long-tail\nentities with the pseudo token representations in the PLM’s\nembedding layer (either heh or het , depending on whether\nthe target entity is the head or the tail entity in the KG). This\nfollows the successive multiple transformer encoder layers to\nincorporate the knowledge into the contextual representations\nwithout introducing any other new parameters for knowledge\nencoding.\nRelational Knowledge Decoding\nAfter the information of the relation triples has been injected\ninto the model, it is not clear whether the model has under-\nstood the injected knowledge. We design a relational knowl-\nedge decoder, forcing our model to understand the injected\nknowledge explicitly. Specifically, we employ a self-attention\npooling mechanism to obtain the masked entity span repre-\nsentations in the last layer:\nho\neh = LN\n(\nσ\n(\nfsp (F(heh )) Wo\neh\n))\n(10)\nwhere Wo\neh is the learnable parameter. Given the output repre-\nsentation of the head entity ho\neh and the relation predicate hr,\nwe aim to decode the tail entity.1 Let hi\nd be the representation\nof the i-th token of the predicted tail entity. We have:\nhi\nd = tanh(δdhri hi−1\nd ·Wd), (11)\nwhere δd is a scaling factor, and h0\nd equals to ho\neh as the\ninitialization heuristics.\n1If the target entity is the tail entity, we can also decode the head\nentity in a similar fashion.\nBecause the vocabulary size is relatively large, we use the\nSampled SoftMax function (Jean et al. 2015) to compare the\nprediction results against the ground truth. The token-level\nloss function Ldi is defined as follows:\nLdi = exp(fs(hi\nd, yi))\nexp(fs(hi\nd, yi)) +NEtn∼Q(yn|yi)[exp(fs(hi\nd, yn))]\n(12)\nfs(hi\nd, yi)) = (hi\nd)T ·yi −log\n(\nQ(t|ti)\n)\n(13)\nwhere yi is the ground-truth token and yn is the negative\ntoken sampled in Q(tn|ti). Q(·|·) is the negative sampling\nfunction (to be described in the experiments). N is the num-\nber of negative samples. In our DKPLM model, the train-\ning objectives include two pre-training tasks: (1) relational\nknowledge decoding and (2) token-level Masked Language\nModeling (MLM), as proposed in (Devlin et al. 2019). Hence,\nthe total loss function of DKPLM can be denoted as follows:\nLtotal = λ1LMLM + (1−λ1)LDe (14)\nwhere the λ1 is the hyper-parameter and LDe is the total\ndecoder loss of the target entity consisting of multiple tokens.\nExperiments\nPre-training Data and Model Settings\nIn this paper, we use English Wikipedia (2020/03/01) 2 as\nour pre-training data source, and WikiExtractor 3 to process\nthe downloaded Wikipedia dump, similar to CoLAKE (Sun\net al. 2020) and ERNIE-THU (Zhang et al. 2019). We use\nWikipedia anchors to align the entities in the pre-training texts\nrecognized by entity linking tools (e.g., TAGME (Ferragina\nand Scaiella 2010)) to WikiData5M (Wang et al. 2019b),\nwhich is a large-scale proposed KG data source including\nrelation triples and entity description texts. The additional pre-\nprocessing and filtration steps are kept the same as ERNIE-\nTHU (Zhang et al. 2019). In total, we have 3,085,345 entities\nand 822 relation types in the KG and 26 million training\nsamples in our pre-training corpus.\nBaselines\nWe consider the following models as strong baselines:\nERNIE-THU (Zhang et al. 2019): The model integrates a de-\nnoising entity auto-encoder pre-training task to inject knowl-\nedge embeddings into language representations.KnowBERT\n(Peters et al. 2019): It injects rich structured knowledge rep-\nresentations via knowledge attention and recontextualization.\nKEPLER (Wang et al. 2019b): The model encodes texts and\nentities into a unified semantic space with the same PLM as\nthe shared encoder. CoLAKE (Sun et al. 2020): It considers\na unified heterogeneous KG as the knowledge source and\nemploys adjacency matrices to control the information flow.\nK-Adapter (Wang et al. 2021): It uses different representa-\ntions for different types of knowledge via neural adapters.\n2https://dumps.wikimedia.org/enwiki/\n3https://github.com/attardi/wikiextractor\n11707\nDatasets PLMs KEPLMs\nELMo BERT RoBERTa CoLAKE K-Adapter ∗ KEPLER DKPLM △\nGoogle-RE 2.2% 11.4% 5.3% 9.5% 7.0% 7.3% 10.8% +1.3%\nUHN-Google-RE 2.3% 5.7% 2.2% 4.9% 3.7% 4.1% 5.4% +0.5%\nT-REx 0.2% 32.5% 24.7% 28.8% 29.1% 24.6% 32.0% +2.9%\nUHN-T-REx 0.2% 23.3% 17.0% 20.4% 23.0% 17.1% 22.9% -0.1%\nTable 1: The performance on knowledge probing datasets.△represents an improvement over the best results of existing KEPLMs\ncompared to our model. Besides, K-Adapter ∗is based on RoBERTa-large and uses a subset of T-REx as its training data, which\nmay contribute to its superiority over the other KEPLMs and is unfair for DKPLM to be compared against.\nModel Precision Recall F1\nUFET (Choi et al. 2018) 77.4 60.6 68.0\nBERT 76.4 71.0 73.6\nRoBERTa 77.4 73.6 75.4\nERNIEBERT 78.4 72.9 75.6\nERNIERoBERTa 80.3 70.2 74.9\nKnowBERTBERT 77.9 71.2 74.4\nKnowBERTRoBERTa 78.7 72.7 75.6\nKEPLERWiKi 77.8 74.6 76.2\nCoLAKE 77.0 75.7 76.4\nDKPLM 79.2 75.9 77.5\nTable 2: The performance of models on Open Entity (%).\nKnowledge Probing\nThe knowledge probing tasks, called LAMA (LAnguage\nModel Analysis) (Petroni et al. 2019), aim to measure\nwhether the factual knowledge is stored in PLMs via cloze-\nstyle tasks. The LAMA-UHN tasks (P¨orner, Waltinger, and\nSch¨utze 2019) are proposed to alleviate the problem of overly\nrelying on the surface form of entity names, and are con-\nstructed by filtering out the easy-to-answer samples. These\ntwo tasks are evaluated under the zero-shot setting without\nfine-tuning, which is a fair comparison of the knowledge\nunderstanding abilities of KEPLMs. We report the macro-\naveraged mean precision (P@1) of DKPLM.\nThe performance of LAMA and LAMA-UHN tasks is sum-\nmarized in Table 1. Compared to the results of other baselines,\nwe can draw the following conclusions. (1) BERT outper-\nforms RoBERTa by a large gap (+5.93% on average) because\nits vocabulary size is much smaller than RoBERTa. (2) Al-\nthough our model is trained on RoBERTa-base, it achieves\nstate-of-the-art results over three datasets (+1.57% on av-\nerage). The result of our model is only 0.1% lower than\nK-Adapter, without using any T-REx training data and large\nPLM backbone. From the overall results, we can see that our\nlearning process based on long-tail entities can effectively\nstore and understand factual knowledge from KGs.\nKnowledge-Aware Tasks\nWe evaluate our DKPLM model over the knowledge-aware\ntasks, including relation extraction and entity typing.\nEntity Typing: Unlike Named Entity Recognition (Jiang et al.\n2021; Shen et al. 2021), entity typing requires the model to\nModel Precision Recall F1\nCNN 70.30 54.20 61.20\nPA-LSTM (Zhang et al. 2017) 65.70 64.50 65.10\nC-GCN (Zhang and Qi 2018) 69.90 63.30 66.40\nBERT 67.23 64.81 66.00\nRoBERTa 70.80 69.60 70.20\nERNIEBERT 70.01 66.14 68.09\nKnowBERT 71.62 71.49 71.53\nDKPLM 72.61 73.53 73.07\nTable 3: The performance of models on TACRED (%).\npredict fine-grained entity types in given contexts. We fine-\ntune our DKPLM model over Open Entity (Choi et al. 2018).\nTable 2 shows the performance of various models including\nPLMs, KEPLMs and other taks-specific models. From the re-\nsults, we can observe: the KEPLMs outperform task-specific\nmodels and the plain PLMs. In addition, our DKPLM model\nwith injected long-tail entity knowledge achieves a large\nperformance gain compared to baselines (+2.2% Precision,\n+0.2% Recall and +1.1% F1).\nRelation Extraction: The Relation Extraction task (RE) aims\nto determine the fine-grained semantic relation between the\ntwo entities in a given sentence. We use a benchmark RE\ndataset TACRED (Zhang et al. 2017) to evaluate our model’s\nperformance. The relation types in TACRED is 42 and we\nadopt the micro averaged metrics and macro averaged met-\nrics for evaluation. As shown in Table 3, the performance\nof knowledge-injected models are much higher, and our\nmodel achieves new state-of-the-art performance (+1.46%\nF1), which implies injecting long-tail entities’ knowledge\ntriple into PLMs for RE is also very effective.\nAnalysis of Running Time\nIn this section, we compare DKPLM with other models on\npre-training, fine-tuning and inference time. Specifically, we\nchoose 1000 samples randomly from the pre-training corpus\nand two knowledge-aware tasks. The fine-tuning and infer-\nence time is the average time of the two datasets, respectively.\nAs shown in Table 4, we have the following observations.\n(1) The running time of the three stages of plain PLMs are\nconsistently shorter than existing KEPLMs due to the smaller\nsize of model parameters. Specifically, existing KEPLMs\ncontain the knowledge encoder module to project the knowl-\n11708\nFigure 4: The influence of different injected numbers of long-tail entities and high-frequency entities.\nModel Pre-training Fine-tuning Inference\nRoBETa base 9.60 7.09 1.55\nBERT base 8.46 6.76 0.97\nERNIE-THU 14.71 8.19 1.95\nKEPLER 18.12 7.53 1.86\nCoLAKE 12.46 8.02 1.91\nDKPLM 10.02 7.16 1.61\nTable 4: The running time (s) in the three stages of various\nPLMs and KEPLMs over 1000 random samples.\nedge embedding space to the contextual semantic space. (2)\nThe running time of our model (especially during model fine-\ntuning and inference) is very similar to that of plain PLMs.\nThe reason for the slightly longer time is that DKPLM adds a\nfew projection parameters to align the knowledge triple rep-\nresentations. This experiment shows that DKPLM is useful\nfor online applications due to its fast inference speed.\nInfluence of Long-tail and High-frequency Entities\nWe evaluate DKPLM using different injected entity numbers,\nand consider three types including long-tail entities only,\nhigh-frequency entities only and a mixture of these entities.\nWe choose TACRED and Open Entity datasets and report\nthe F1 metrics over testing sets to verify the effectiveness of\nknowledge injection. As shown in Figure 4, we can observe\nthat: (1) Injecting knowledge triples into long-tail entities is\nbetter than high-frequency entities. (2) The state-of-the-art\nperformance can be obtained by injecting knowledge to a\nfewer entities rather than all the entities. (3) Our results are\nconsistent with Zhang et al. (2021) in that injecting too much\nknowledge may hurt the performance.\nAblation Study\nWe report DKPLM’s performance in two knowledge-aware\ntesting sets to perform the ablation study on the F1 metric.\nAs shown in Table 5, we can conclude that (1) our proposed\nthree mechanisms are effective in contributing to the com-\nplete DKPLM model. (2) The model’s performance declines\nModel TACRED Open Entity\nDKPLM 77.5% 73.07%\n- Long-tail Entity Detection 77.3% 72.89%\n- Pseudo Token Embedding 76.7% 72.35%\n- Knowledge Decoding 77.1% 72.54%\nTable 5: Ablation study on two tasks (testing sets).\nsignificantly when removing the “Pseudo Token Embedding”\nmechanism. Here, the external knowledge of detected long-\ntail entities is not injected into the model. DKPLM degener-\nates to relying entirely on entity-level information to decode\nknowledge triples, leading to model confusion due to the\nsparsity of the knowledge of long-tail entities.\nConclusion and Future Work\nIn this paper, we propose a novel KEPLMs to decouple knowl-\nedge injection and fine-tuning for knowledge-enhanced lan-\nguage understanding named DKPLM. In DKPLM, we design\nthree entity-related mechanisms to inject the knowledge in-\nformation into the PLMs with minimum extra parameters\nfor the real-world scenarios, namely knowledge-aware long-\ntail entity detection, pseudo token embedding injection and\nrelational knowledge decoding. The experiments show that\nour model achieves the state-of-the-art performance over\nzero-shot knowledge probing tasks and knowledge-aware\ndownstream tasks. Future work includes (1) selecting more\neffective knowledge triples from large-scale KGs to inject\nexternal knowledge into the PLMs, and (2) utilizing noised\nknowledge triples to further enhance the language under-\nstanding abilities of PLMs.\nAcknowledgements\nThis work is supported by the Alibaba Group through Alibaba\nResearch Intern Program.\n11709\nReferences\nBa, L. J.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Nor-\nmalization. CoRR, abs/1607.06450.\nBordes, A.; Usunier, N.; Garc´ıa-Dur´an, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating Embeddings for Modeling\nMulti-relational Data. In NIPS, 2787–2795.\nCao, B.; Lin, H.; Han, X.; Sun, L.; Yan, L.; Liao, M.; Xue,\nT.; and Xu, J. 2021. Knowledgeable or Educated Guess?\nRevisiting Language Models as Knowledge Bases. In ACL,\n1860–1874.\nChang, T. A.; Xu, Y .; Xu, W.; and Tu, Z. 2021. Convolutions\nand Self-Attention: Re-interpreting Relative Positions in Pre-\ntrained Language Models. In ACL, 4322–4333.\nChoi, E.; Levy, O.; Choi, Y .; and Zettlemoyer, L. 2018. Ultra-\nFine Entity Typing. In ACL, 87–96.\nColon-Hernandez, P.; Havasi, C.; Alonso, J. B.; Huggins,\nM.; and Breazeal, C. 2021. Combining pre-trained language\nmodels and structured knowledge. CoRR, abs/2101.12294.\nCui, B.; Li, Y .; Chen, M.; and Zhang, Z. 2019. Fine-tune\nBERT with Sparse Self-Attention Mechanism. In EMNLP,\n3539–3544.\nCui, L.; Cheng, S.; Wu, Y .; and Zhang, Y . 2021. On Com-\nmonsense Cues in BERT for Solving Commonsense Tasks.\nIn ACL, 683–693.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive lan-\nguage models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL, 4171–4186.\nFerragina, P.; and Scaiella, U. 2010. TAGME: on-the-fly\nannotation of short text fragments (by wikipedia entities). In\nCIKM, 1625–1628.\nHe, Y .; Zhu, Z.; Zhang, Y .; Chen, Q.; and Caverlee, J. 2020.\nInfusing Disease Knowledge into BERT for Health Question\nAnswering, Medical Inference and Disease Name Recogni-\ntion. In EMNLP, 4604–4614.\nJean, S.; Cho, K.; Memisevic, R.; and Bengio, Y . 2015. On\nUsing Very Large Target V ocabulary for Neural Machine\nTranslation. In ACL, 1–10.\nJiang, H.; Zhang, D.; Cao, T.; Yin, B.; and Zhao, T. 2021.\nNamed Entity Recognition with Small Strongly Labeled and\nLarge Weakly Labeled Data. In ACL, 1775–1789.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. SpanBERT: Improving Pre-training by\nRepresenting and Predicting Spans. Trans. Assoc. Comput.\nLinguistics, 8: 64–77.\nLin, Z.; Feng, M.; dos Santos, C. N.; Yu, M.; Xiang, B.;\nZhou, B.; and Bengio, Y . 2017. A Structured Self-Attentive\nSentence Embedding. In ICLR.\nLiu, W.; Zhou, P.; Zhao, Z.; Wang, Z.; Ju, Q.; Deng, H.; and\nWang, P. 2020. K-BERT: Enabling Language Representation\nwith Knowledge Graph. In AAAI, 2901–2908.\nLiu, X.; He, P.; Chen, W.; and Gao, J. 2019. Multi-Task Deep\nNeural Networks for Natural Language Understanding. In\nACL, 4487–4496.\nPeters, M. E.; Neumann, M.; IV , R. L. L.; Schwartz, R.; Joshi,\nV .; Singh, S.; and Smith, N. A. 2019. Knowledge Enhanced\nContextual Word Representations. In EMNLP, 43–54.\nPetroni, F.; Rockt ¨aschel, T.; Riedel, S.; Lewis, P. S. H.;\nBakhtin, A.; Wu, Y .; and Miller, A. H. 2019. Language\nModels as Knowledge Bases? In EMNLP, 2463–2473.\nP¨orner, N.; Waltinger, U.; and Sch ¨utze, H. 2019. BERT\nis Not a Knowledge Base (Yet): Factual Knowledge vs.\nName-Based Reasoning in Unsupervised QA. CoRR,\nabs/1911.03681.\nShen, Y .; Ma, X.; Tan, Z.; Zhang, S.; Wang, W.; and Lu, W.\n2021. Locate and Label: A Two-stage Identifier for Nested\nNamed Entity Recognition. In ACL, 2782–2794.\nSu, Y .; Han, X.; Zhang, Z.; Li, P.; Liu, Z.; Lin, Y .; Zhou,\nJ.; and Sun, M. 2020. Contextual knowledge selection and\nembedding towards enhanced pre-trained language models.\narXiv e-prints, arXiv–2009.\nSun, T.; Shao, Y .; Qiu, X.; Guo, Q.; Hu, Y .; Huang, X.; and\nZhang, Z. 2020. CoLAKE: Contextualized Language and\nKnowledge Embedding. In COLING, 3660–3670.\nSun, Y .; Wang, S.; Li, Y .; Feng, S.; Chen, X.; Zhang, H.;\nTian, X.; Zhu, D.; Tian, H.; and Wu, H. 2019. ERNIE:\nEnhanced Representation through Knowledge Integration.\nCoRR, abs/1904.09223.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019a. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nICLR.\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Ji, J.;\nCao, G.; Jiang, D.; and Zhou, M. 2021. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters. In ACL,\n1405–1418.\nWang, W.; Bi, B.; Yan, M.; Wu, C.; Xia, J.; Bao, Z.; Peng, L.;\nand Si, L. 2020. StructBERT: Incorporating Language Struc-\ntures into Pre-training for Deep Language Understanding. In\nICLR.\nWang, X.; Gao, T.; Zhu, Z.; Liu, Z.; Li, J.; and Tang, J.\n2019b. KEPLER: A Unified Model for Knowledge Em-\nbedding and Pre-trained Language Representation. CoRR,\nabs/1911.06136.\nXu, Z.; Guo, D.; Tang, D.; Su, Q.; Shou, L.; Gong, M.;\nZhong, W.; Quan, X.; Jiang, D.; and Duan, N. 2021. Syntax-\nEnhanced Pre-trained Model. In ACL, 5412–5422.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J. G.; Salakhutdinov,\nR.; and Le, Q. V . 2019. XLNet: Generalized Autoregressive\nPretraining for Language Understanding. In NIPS, 5754–\n5764.\nZhang, N.; Deng, S.; Cheng, X.; Chen, X.; Zhang, Y .; Zhang,\nW.; Chen, H.; and Center, H. I. 2021. Drop Redundant, Shrink\nIrrelevant: Selective Knowledge Injection for Language Pre-\ntraining. In In IJCAI.\nZhang, N.; Deng, S.; Li, J.; Chen, X.; Zhang, W.; and Chen,\nH. 2020. Summarizing Chinese Medical Answer with Graph\n11710\nConvolution Networks and Question-focused Dual Attention.\nIn EMNLP, 15–24.\nZhang, Y .; and Qi, P. 2018. Graph Convolution over Pruned\nDependency Trees Improves Relation Extraction. In EMNLP,\n2205–2215.\nZhang, Y .; Zhong, V .; Chen, D.; Angeli, G.; and Manning,\nC. D. 2017. Position-aware Attention and Supervised Data\nImprove Slot Filling. In EMNLP, 35–45.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: Enhanced Language Representation with\nInformative Entities. In ACL, 1441–1451.\n11711",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.800370991230011
    },
    {
      "name": "Inference",
      "score": 0.7126889228820801
    },
    {
      "name": "Domain knowledge",
      "score": 0.6181622743606567
    },
    {
      "name": "Task (project management)",
      "score": 0.5340594053268433
    },
    {
      "name": "Language model",
      "score": 0.5298652052879333
    },
    {
      "name": "Question answering",
      "score": 0.519196629524231
    },
    {
      "name": "Natural language processing",
      "score": 0.5129591822624207
    },
    {
      "name": "Knowledge graph",
      "score": 0.509985089302063
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5028731226921082
    },
    {
      "name": "Relation (database)",
      "score": 0.47958341240882874
    },
    {
      "name": "Knowledge extraction",
      "score": 0.4668463170528412
    },
    {
      "name": "Process (computing)",
      "score": 0.4604988098144531
    },
    {
      "name": "Procedural knowledge",
      "score": 0.4425087571144104
    },
    {
      "name": "Knowledge integration",
      "score": 0.4369906187057495
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4268394708633423
    },
    {
      "name": "General knowledge",
      "score": 0.42106518149375916
    },
    {
      "name": "Natural language",
      "score": 0.41897454857826233
    },
    {
      "name": "Semantic memory",
      "score": 0.41348734498023987
    },
    {
      "name": "Machine learning",
      "score": 0.3759812116622925
    },
    {
      "name": "Data mining",
      "score": 0.14919763803482056
    },
    {
      "name": "Cognition",
      "score": 0.1203596293926239
    },
    {
      "name": "Programming language",
      "score": 0.09994444251060486
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 29
}