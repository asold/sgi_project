{
  "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers",
  "url": "https://openalex.org/W3186269967",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A809821746",
      "name": "Wang Wen",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097246861",
      "name": "Cao Yang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1925561233",
      "name": "Zhang Jing",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2681387168",
      "name": "He, Fengxiang",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4202086093",
      "name": "Zha, Zheng-Jun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2266430183",
      "name": "Wen, Yonggang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1960309444",
      "name": "Tao, Dacheng",
      "affiliations": [
        "Jingdong (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2104094955",
    "https://openalex.org/W2979548969",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2964115968",
    "https://openalex.org/W3054755450",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3034998639",
    "https://openalex.org/W2998115938",
    "https://openalex.org/W3004591935",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2990740643",
    "https://openalex.org/W2963730616",
    "https://openalex.org/W2990069979",
    "https://openalex.org/W2969583814",
    "https://openalex.org/W3027629341",
    "https://openalex.org/W2984121207",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2968634921",
    "https://openalex.org/W2748021867",
    "https://openalex.org/W3156977337",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W3034779842",
    "https://openalex.org/W3034937575",
    "https://openalex.org/W2938603906",
    "https://openalex.org/W3034203210",
    "https://openalex.org/W2984971137",
    "https://openalex.org/W2955731188",
    "https://openalex.org/W3109340983",
    "https://openalex.org/W3035365026",
    "https://openalex.org/W3189423081",
    "https://openalex.org/W3093025987",
    "https://openalex.org/W2955889502"
  ],
  "abstract": "Detection transformers have recently shown promising object detection results\\nand attracted increasing attention. However, how to develop effective domain\\nadaptation techniques to improve its cross-domain performance remains\\nunexplored and unclear. In this paper, we delve into this topic and empirically\\nfind that direct feature distribution alignment on the CNN backbone only brings\\nlimited improvements, as it does not guarantee domain-invariant sequence\\nfeatures in the transformer for prediction. To address this issue, we propose a\\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\\nthe adaptation of detection transformers. Technically, SFA consists of a domain\\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\\nglobal context from the token sequence of both domains. DQFA reduces the domain\\ndiscrepancy in global feature representations and object relations when\\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\\naligns token features in the sequence from both domains, which reduces the\\ndomain gaps in local and instance-level feature representations in the\\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\\nmatching consistency loss is proposed to enhance the feature discriminability\\nfor robust object detection. Experiments on three challenging benchmarks show\\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\\nCode has been made available at: https://github.com/encounter1997/SFA.\\n",
  "full_text": "Exploring Sequence Feature Alignment for Domain Adaptive\nDetection Transformers\nWen Wang1, Yang Cao1,2, Jing Zhang3, Fengxiang He4, Zheng-Jun Zha1\nYonggang Wen5, Dacheng Tao4\n1 University of Science and Technology of China\n2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3 The University of Sydney, 4 JD Explore Academy, China, 5 Nanyang Technological University\nwangen@mail.ustc.edu.cn,{forrest,zhazj}@ustc.edu.cn,jing.zhang1@sydney.edu.au,\n{hefengxiang,taodacheng}@jd.com,ygwen@ntu.edu.sg\nABSTRACT\nDetection transformers have recently shown promising object de-\ntection results and attracted increasing attention. However, how\nto develop effective domain adaptation techniques to improve its\ncross-domain performance remains unexplored and unclear. In this\npaper, we delve into this topic and empirically find that direct\nfeature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant\nsequence features in the transformer for prediction. To address\nthis issue, we propose a novel Sequence Feature Alignment (SFA)\nmethod that is specially designed for the adaptation of detection\ntransformers. Technically, SFA consists of a domain query-based\nfeature alignment (DQFA) module and a token-wise feature align-\nment (TDA) module. In DQFA, a novel domain query is used to\naggregate and align global context from the token sequence of both\ndomains. DQFA reduces the domain discrepancy in global feature\nrepresentations and object relations when deploying in the trans-\nformer encoder and decoder, respectively. Meanwhile, TDA aligns\ntoken features in the sequence from both domains, which reduces\nthe domain gaps in local and instance-level feature representations\nin the transformer encoder and decoder, respectively. Besides, a\nnovel bipartite matching consistency loss is proposed to enhance\nthe feature discriminability for robust object detection. Experiments\non three challenging benchmarks show that SFA outperforms state-\nof-the-art domain adaptive object detection methods. Code has\nbeen made available at: https://github.com/encounter1997/SFA.\nCCS CONCEPTS\nâ€¢ Theory of computation â†’Machine learning theory; â€¢ Com-\nputing methodologies â†’Computer vision problems .\nThis work was done during Wen Wangâ€™s internship at JD Explore Academy.\nCorresponding Author: Yang Cao, Jing Zhang.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475317\nKEYWORDS\nObject Detection, Detection Transformer, Domain Adaptation, Fea-\nture Alignment, Matching Consistency\nACM Reference Format:\nWen Wang1, Yang Cao1,2, Jing Zhang3, Fengxiang He4, Zheng-Jun Zha1\nand Yonggang Wen5, Dacheng Tao 4. 2021. Exploring Sequence Feature\nAlignment for Domain Adaptive Detection Transformers. In Proceedings\nof the 29th ACM International Conference on Multimedia (MM â€™21), October\n20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA, 13 pages.\nhttps://doi.org/10.1145/3474085.3475317\n1 INTRODUCTION\nObject Detection is one of the fundamental tasks in computer vi-\nsion and is an essential and prepositive step for multimedia ap-\nplications like captioning [39, 46], visual grounding [24], texture\nanalysis [40, 41], person re-identification / person search [47â€“49],\netc.. Tremendous progress has been made by CNN-based object de-\ntection methods in the last decade, e.g., Faster RCNN [28], SSD [25]\nand FCOS [33]. However, they rely on heuristics like non-maximum\nsuppression (NMS), and are not fully end-to-end. Recently, detec-\ntion transformers, e.g., DETR [4] and Deformable DETR [51], have\nattracted increasing attention due to their merits of fully end-to-\nend pipeline and promising performance. While effective, all these\nmethods rely heavily on the labeled training data, and suffer signif-\nicant performance drops when the test data and training data are\nsampled from different distributions, due to domain shifts.\nTo tackle this problem, unsupervised domain adaptive object\ndetection (DAOD) [5] attempts to train an object detector on the\nlabeled source domain that can be generalized to the unlabeled\ntarget domain. Existing DAOD methods [5, 17, 19] have achieved\nsignificant progress in improving the cross-domain performance\nfor specific object detection models, such as based on Faster RCNN,\nSSD, and FCOS. With the recent surge of detection transformers, it\nis natural to ask, can we empower them with such a capability to\nperform accurate object detection in cross-domain scenarios?\nA vanilla idea to solve this problem is to apply adversarial fea-\nture distribution alignment [11] on features extracted by the CNN\nbackbone [16, 29]. As shown in Figure 1 (a), direct feature align-\nment on the CNN backbone (denoted as DA-CNN) does improve\nthe detection transformerâ€™s cross-domain performance, yet the im-\nprovement is limited. We argue that this is because the feature\ndistribution alignment on the CNN backbone does not guarantee\ndomain-invariant sequence features in the subsequent transformer,\narXiv:2107.12636v4  [cs.CV]  15 Jul 2022\n(a) Convergence curves (b) T-SNE visualization\nCNN featuresEncoder featuresDecoder features\nSource Target\nFigure 1: (a) Performance comparison between feature distri-\nbution alignment applied on the CNN backbone and trans-\nformer (denoted as DA-CNN and DA-Seq, respectively). Both\nmethods are built on the Deformable DETR [51] with a\nResNet-50 backbone and evaluated on the Cityscapes [7] to\nFoggy Cityscapes [30] scenario. (b) T-SNE visualization of\nfeatures extracted by DA-CNN. While the CNN features are\nwell-align, the sequence feature extracted by transformer\nencoder and decoder can be easily separated by domain.\nwhich are directly utilized for prediction. In Figure 1 (b), we vi-\nsualize the distribution of features extracted by DA-CNN. While\nthe source and target features extracted by the CNN backbone are\nwell-aligned, the sequence features extracted by the transformer\nencoder can be separated by domain. Moreover, as the network\ngoes deeper, the distribution gaps in the decoder feature become\neven more significant. As a result, the detection transformer obtains\ninferior performance based on the shifted sequence features.\nTo tackle this problem, we propose a novel sequence feature\nalignment (SFA) method that is specially designed for the domain\nadaptation of detection transformers. SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise\nfeature alignment (TDA) module upon the transformer structure.\nSpecifically, the DQFA utilizes a novel domain query to aggregate\nglobal context from the sequence for feature alignment. When\napplied to the transformer encoder and decoder, DQFA alleviates\nthe domain gaps on global-level and object relations, respectively.\nMeanwhile, the TDA focus on feature alignment of each token in\nthe sequence, it effectively closes domain gaps at local- and instance-\nlevel when applied to the encoder and decoder, respectively. Besides,\na novel bipartite matching consistency loss is proposed to regular-\nize the detection transformer and improve its discriminability for\nrobust object detection. Experiments on three challenging bench-\nmarks show that SFA significantly improves detection transformersâ€™\ncross-domain performance and outperforms existing DAOD meth-\nods built on various object detectors. The main contribution of this\npaper can be summarized as follows:\nâ€¢We dedicate to improving detection transformersâ€™ cross-\ndomain performance, which is still unexplored and unclear.\nEmpirically, we observe that direct feature distribution align-\nment on the CNN backbone only brings limited improve-\nments, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction.\nâ€¢We propose Sequence Feature Alignment (SFA) that is spe-\ncially designed for domain adaptation of detection transform-\ners. It consists of a domain query-based feature alignment\n(DQFA) module and a token-wise feature alignment (TDA)\nmodule for aligning sequence features on a global- and local-\nlevel, respectively. Moreover, we provide technical insight\non the explicit meanings of these modules when applied to\nthe transformer encoder and decoder.\nâ€¢A novel bipartite matching consistency loss is proposed to\nfurther regularize the sequence features and improve the\ndiscriminability of the detection transformers.\nâ€¢Extensive experiments on three challenging domain adap-\ntation scenarios, including weather adaptation, synthetic to\nreal adaptation, and scene adaptation, verify the effective-\nness of our method, where SFA outperforms existing DAOD\nmethods and achieves state-of-the-art (SOTA) performance.\n2 RELATED WORK\n2.1 Object Detection\nObject detection is one of the fundamental tasks in computer vi-\nsion [6, 43]. Representative deep learning-based object detectors can\nbe roughly categorized as two-stage methods,e.g., Faster RCNN [28],\nand single-stage methods, e.g., YOLO [27] and SSD [25]. While the\nformer ones generally show better performance, the latter ones\nare faster during inference. Although significant progress has been\nmade, these object detectors are not fully end-to-end and heavily\nrely on hand-crafted components, such as anchor box generation\nand non-maximum suppression (NMS) post-processing. Recently,\nDETR [4] provides a simple and clean pipeline for object detection.\nIt views object detection as a direct set prediction problem and\nexplores transformer [ 10, 31, 32, 35] and bipartite matching for\neffective object detection. The success of DETR brought the recent\nsurge of detection transformers. Deformable DETR [51] proposes a\nnovel deformable attention, which speeds up model training with\nlearnable sparse sampling and improves model performance by\nintegrating multi-scale features. UP-DETR [8] introduces a novel\nself-supervised pre-training scheme to improve the performance of\nDETR with faster convergence.\n2.1.1 Formulation of Detection Transformers. The detection trans-\nformer models generally consist of three parts: the CNN backbone\nfor base feature extraction, the transformer for sequence feature\nmodeling, and feed-forward network (FFN) for prediction. The\nCNN backbone extracts hierarchical feature representation {ğ‘“ğ‘™}ğ¿\nğ‘™=1\nfrom the input images, where ğ¿is the number of feature levels and\nğ‘“ğ‘™ âˆˆRğ»ğ‘™ Ã—ğ‘Šğ‘™ Ã—ğ¶ğ‘™\nis the ğ‘™-th feature map. Afterwards, the hierarchi-\ncal features are flatten and embedded to form an one-dimensional\nsequence ğ‘“ğ‘’ âˆˆRğ‘Ã—ğ¶, whose length is ğ‘ = Ãğ¿\nğ‘™=1 ğ»ğ‘™ğ‘Šğ‘™ and the\ndimension of feature embeddings isğ¶. The input of the transformer\nis denoted as ğ‘§0, which is ğ‘“ğ‘’ augmented with explicit embeddings\nsuch as positional embedding and level embedding [51].\nThe transformer consists of an encoder and a decoder. The en-\ncoder is a stack ofğ¿ğ‘’ğ‘›ğ‘ encoder layers. Each encoder layerEncLayerâ„“\ntakes previous layerâ€™s outputğ‘§â„“âˆ’1 and the sample position reference\nğ‘â„“ as input and outputs the encoded sequence feature ğ‘§â„“ as follows,\nğ‘§â„“ = EncLayerâ„“ (ğ‘§â„“âˆ’1,ğ‘â„“), â„“ = 1 ...ğ¿ ğ‘’ğ‘›ğ‘. (1)\nSimilarly, the decoder is a stack of ğ¿ğ‘‘ğ‘’ğ‘ decoder layers. The input\ntoken sequence in the transformer decoder is initialized as ğ‘0, each\ndecoder layer DecLayerâ„“ takes the previous decoder layerâ€™s output\nğ‘â„“âˆ’1, the sample position referenceğ‘â„“, and the encoder outputğ‘§ğ¿ğ‘’ğ‘›ğ‘\nas input and outputs the decoded sequence features as follows,\nğ‘â„“ = DecLayerâ„“\n\u0000ğ‘â„“âˆ’1,ğ‘â„“âˆ’1,ğ‘§ğ¿ğ‘’ğ‘›ğ‘\n\u0001 , â„“ = 1 ...ğ¿ ğ‘‘ğ‘’ğ‘. (2)\nDetection transformers [4, 51] usually adopt deep supervision [23]\nto facilitate model training. Classification probability vectors and\nbounding boxes are predicted based on the output of each decoder\nlayer by the FFN, and are used to compute the auxiliary loss on\nthe source domain. We denote the supervised loss on the source\ndomain as Lğ‘‘ğ‘’ğ‘¡, which is defined in [4, 51].\n2.2 Domain Adaptive Object Detection\nDomain adaptive object detection (DAOD) has been raised very\nrecently for unconstrained scenes[5]. DAF [5] adopts adversarial\nfeature alignment [11, 45] at both image-level and instance-level.\nSWDA [29] adopts strong alignment on local features and weak\nalignment on global features from the CNN backbone. SCDA [50]\naligns region-level features across domains via grouping instances\ninto regions. MTOR [3] integrates object relations into the mea-\nsure of consistency cost between teacher and student modules for\nadaptation. Xu et al. [36] explore image-level categorical regulariza-\ntion and categorical consistency regularization for robust detection\nacross domains. GPA [37] explores graph-induced prototype and\nclass reweighted contrastive loss for effective feature alignment\nand adaptation. While these methods are built on the seminal Faster\nRCNN [28], Kim et al. [19] propose an SSD-based domain adaptive\nobject detector and explore effective self-training and adversarial\nbackground regularization for adaptation. Hsu et al. [17] build an\nadaptive object detector base on FCOS. They estimating pixel-wise\nobjectness and centerness and adopting center-aware feature align-\nment to close the domain gaps. Although significant progress has\nbeen made, most of these methods rely on specific model architec-\nture, e.g., Faster RCNN, SSD, and FCOS, therefore cannot be directly\napplied to detection transformers. In this paper, we dedicate to\nimprove detection transformersâ€™ cross-domain performance, which\nis still unexplored and unclear.\n3 METHODS\nThis section introduces our Sequence Feature Alignment (SFA) for\ndomain adaptive detection transformers. In unsupervised domain\nadaptation, the training data includes the labeled source images and\nthe unlabeled target images. Our goal is to train an object detector\non the training data that can generalize to the target domain.\nAs described in Section 1, feature distribution alignment on\nthe CNN backbone only brings limited improvements, as it does\nnot guarantee domain-invariant sequence features in transformer,\nwhich are directly utilized for final prediction. To solve the problem,\nwe explore effective distribution alignment on the sequence features\nin transformer. Specifically, domain query-based sequence feature\nalignment (Section 3.1) and token-wise sequence feature alignment\n(Section 3.2) equipped with a hierarchical domain adaptation strat-\negy (Section 3.3) are proposed. Moreover, in Section 3.4, we explore\nbipartite matching consistency to constrain the sequence features,\nwhich further improve detection transformerâ€™s discriminability on\nthe target domain. The overall framework of our SFA is shown\nin Figure 2. We use Deformable DETR [ 51] as the default detec-\ntion framework, while our sequence feature alignment can also be\napplied to other detection transformer methods, e.g., DETR [4].\n3.1 Domain Query-based Feature Alignment\nTo obtain domain-invariant sequence features, we first propose the\ndomain query-based feature alignment to align source and target\nfeatures from a global perspective. Specifically, on the encoder side,\na domain query embedding ğ‘ğ‘’ğ‘›ğ‘\nğ‘‘ is concatenated with the token\nsequence to form the input ğ‘§0 to transformer encoder, ğ‘–.ğ‘’.,\nğ‘§0 =\nh\nğ‘ğ‘’ğ‘›ğ‘\nğ‘‘ ; ğ‘“1\nğ‘’ ; ğ‘“2\nğ‘’ ; Â·Â·Â· ; ğ‘“ğ‘\nğ‘’\ni\n+ğ¸ğ‘ğ‘œğ‘  +ğ¸ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™, (3)\nwhere ğ¸ğ‘ğ‘œğ‘  âˆˆ R(ğ‘+1)Ã—ğ¶ is the positional embedding, ğ¸ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ âˆˆ\nR(ğ‘+1)Ã—ğ¶ is the feature level embedding [51].\nIn the encoding process, the domain query aggregates domain-\nspecific features from the whole sequence in an adaptive manner. It\nencodes the global context in input images and puts more concentra-\ntion on tokens with significant domain gaps. We input this domain\nquery into a domain discriminator ğ·ğ‘’ğ‘›ğ‘ğ‘ for feature alignment, ğ‘–.ğ‘’.,\nLâ„“\nğ‘’ğ‘›ğ‘ğ‘ = ğ‘‘log ğ·ğ‘’ğ‘›ğ‘ğ‘\n\u0010\nğ‘§0\nâ„“\n\u0011\n+(1 âˆ’ğ‘‘)log\n\u0010\n1 âˆ’ğ·ğ‘’ğ‘›ğ‘ğ‘\n\u0010\nğ‘§0\nâ„“\n\u0011\u0011\n, (4)\nwhere â„“ = 1 ...ğ¿ ğ‘’ğ‘›ğ‘ indexes layers in the encoder, ğ‘‘ is the domain\nlabel, which values 0 for source images and 1 for target images.\nSimilarly, we concatenate a domain query ğ‘ğ‘‘ğ‘’ğ‘\nğ‘‘ with the object\nqueries to obtain the input sequence to the transformer decoder:\nğ‘0 =\nh\nğ‘ğ‘‘ğ‘’ğ‘\nğ‘‘ ;ğ‘1;ğ‘2; Â·Â·Â· ;ğ‘ğ‘€\ni\n+ğ¸\nâ€²\nğ‘ğ‘œğ‘ , (5)\nwhere ğ¸\nâ€²\nğ‘ğ‘œğ‘  âˆˆR(ğ‘€+1)Ã—ğ¶ is the positional embedding and ğ‘ğ‘– is the\nğ‘–-th object query in the sequence. In the decoding process, the\ndomain query fuses context features from each object query in the\nsequence, which explicitly models the object relations. We feed the\ndomain query to domain discriminatorğ·ğ‘‘ğ‘’ğ‘ğ‘˜ for feature alignment:\nLâ„“\nğ‘‘ğ‘’ğ‘ğ‘\n= ğ‘‘log ğ·ğ‘‘ğ‘’ğ‘ğ‘˜\n\u0010\nğ‘0\nâ„“\n\u0011\n+(1 âˆ’ğ‘‘)log\n\u0010\n1 âˆ’ğ·ğ‘‘ğ‘’ğ‘ğ‘\n\u0010\nğ‘0\nâ„“\n\u0011\u0011\n, (6)\nwhere â„“ = 1 ...ğ¿ ğ‘‘ğ‘’ğ‘ indexes the layers in the transformer decoder.\nRemarks. Although both encoder and decoder adopt the domain\nquery to perform sequence feature alignment for reducing domain\ngaps, it should be noted that they have different implications. Specif-\nically, since the sequence features in the encoder are derived from\nfeature maps extracted by the CNN backbone, the domain query\naggregates global features that reflect scene layout in images for\nalignment. While the sequence features in the decoder represent ob-\nject instances, thus the domain query encodes object relationships\nfor adaptation. Moreover, with the help of the attention mechanism\nand adversarial learning, the domain query-based feature alignment\nCNN\nLevel + Position\nEmbedding\nmulti-scale image \nfeatures\nBackbone\ndeformable transformer \nencoder layer\ndeformable transformer \ndecoder layer\ndeformable transformer \ndecoder layer\nDomain Classification \ndeformable transformer \nencoder layer\nFFN\nDenck\ndomain \nquery\ndomain \nquery\nDomain Classification \nÂ£5\nÂ£5\nBipartite Matching \nConsistency\nEncoder Decoder\nforward path forward path with gradient reverse layer attached\nobject queries\n*\n*\n*\n *\n*\n*\nDdeck\nDdec\nDenc\nq\nq\nFigure 2: Diagram of our sequence feature alignment (SFA) for domain adaptive detection transformers. Domain query-based\nfeature alignment and token-wise feature alignment tackles global and local domain gaps, respectively. Moreover, a bipartite\nmatching consistency loss is proposed to improve the modelâ€™s discriminability. Here object queries with the same color are\nmatched to the same ground-truth object, while object queries with the same shape come from the same decoder layer.\nadaptively puts more effort on aligning features with significant\ndomain gaps, while less effort on features with smaller ones.\n3.2 Token-wise Feature Alignment\nThe domain query-based global feature alignment can effectively\nclose global domain gaps in scene layout and inter-object relation-\nships, but it struggles to address the domain shift caused by local\ntexture and style. To solve this problem, we propose token-wise se-\nquence feature alignment and apply it to both encoder and decoder.\nSpecifically, each token embedding in the encoder sequence are\nfed into a domain classifier ğ·ğ‘’ğ‘›ğ‘ğ‘˜ for adversarial feature alignment:\nLâ„“\nğ‘’ğ‘›ğ‘ğ‘˜ = âˆ’1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nh\nğ‘‘log ğ·ğ‘’ğ‘›ğ‘ğ‘˜\n\u0010\nğ‘§ğ‘–\nâ„“\n\u0011\n+(1 âˆ’ğ‘‘)log\n\u0010\n1 âˆ’ğ·ğ‘’ğ‘›ğ‘ğ‘˜\n\u0010\nğ‘§ğ‘–\nâ„“\n\u0011\u0011i\n.\n(7)\nSimilarly, a domain discriminator ğ·decğ‘˜ is attached on the decoder\nside to align each token embedding in the decoder sequence, ğ‘–.ğ‘’.,\nLâ„“\nğ‘‘ğ‘’ğ‘ğ‘˜\n= âˆ’1\nğ‘€\nğ‘€âˆ‘ï¸\nğ‘–=1\nh\nğ‘‘log ğ·ğ‘‘ğ‘’ğ‘ğ‘˜\n\u0010\nğ‘ğ‘–\nâ„“\n\u0011\n+(1 âˆ’ğ‘‘)log\n\u0010\n1 âˆ’ğ·ğ‘‘ğ‘’ğ‘ğ‘\n\u0010\nğ‘ğ‘–\nâ„“\n\u0011\u0011i\n.\n(8)\nRemarks. Although both encoder and decoder adopt the token-\nwise sequence feature alignment, it should be noted that they have\ndifferent implications. Specifically, since each token in the encoder\nsequence represents a local area of the image, the token-wise se-\nquence feature alignment here alleviates domain gaps caused by\nlocal texture, appearance, etc.. By contrast, each token at the de-\ncoder side represents an individual object, therefore, the token-wise\nsequence feature alignment closes domain gaps at the instance level.\nIt should be noted that the domain query-based feature alignment\ncannot be replaced by the token-wise feature alignment. Although\ntokens in transformer also aggregate global features to some extent,\nthey are generated from small image patch (tokens in encoder\nsequence) or target at one specific object instance (tokens in decoder\nsequence), thus they inherently have the tendency to focus more\non local content of the image (with more weight on themselves and\ntheir close neighbors during the attention process). By contrast,\ndomain queries do not need to focus on local features or instances,\nthus can better aggregate global context and close domain gaps\nrelated to scene layout and inter-object relationships without bias.\n3.3 Hierarchical Sequence Feature Alignment\nTo achieve a more comprehensive feature alignment, we adopt\nhierarchical feature alignment to progressively align the source\nand target sequence features in a shallow to deep manner. The\nhierarchical feature alignment on sequences in the transformer\nencoder is described as:\nLğ‘’ğ‘›ğ‘ =\nğ¿ğ‘’ğ‘›ğ‘âˆ‘ï¸\nğ‘™=1\n\u0010\nLâ„“\nğ‘’ğ‘›ğ‘ğ‘˜ +ğœ†ğ‘’ğ‘›ğ‘ğ‘ Lâ„“\nğ‘’ğ‘›ğ‘ğ‘\n\u0011\n, (9)\nwhere ğœ†ğ‘’ğ‘›ğ‘ğ‘ is a hyperparameter to balance the query-based align-\nment loss and token-based alignment loss. It is set as 0.1 in our\nexperiments. Similarly, the hierarchical feature alignment is applied\non the sequence features in the transformer decoder, ğ‘–.ğ‘’.,\nLğ‘‘ğ‘’ğ‘ =\nğ¿ğ‘‘ğ‘’ğ‘âˆ‘ï¸\nğ‘™=1\n\u0010\nLâ„“\nğ‘‘ğ‘’ğ‘ğ‘˜\n+ğœ†ğ‘‘ğ‘’ğ‘ğ‘ Lâ„“\nğ‘‘ğ‘’ğ‘ğ‘\n\u0011\n, (10)\nwhere ğœ†ğ‘‘ğ‘’ğ‘ğ‘ is a hyperparameter similar to ğœ†ğ‘’ğ‘›ğ‘ğ‘ , and is also set as\n0.1 in our experiments. For both encoder and decoder, 3-layer MLPs\nare adopted as the discriminators. Hierarchical feature alignment\nfacilitates better alignment on the sequence features.\n3.4 Bipartite Matching Consistency\nDetection transformers [4, 51] adopt deep supervision [23] for train-\ning. Auxiliary output are generated on each decoder layer. The\nauxiliary output on the â„“-th decoder layer is denoted as Ë†ğ‘¦â„“, which\ncontains predictions for ğ‘€ object instances. Prediction for each\ninstance includes a class probability vector Ë†ğ‘ğ‘–\nâ„“ and a bounding box\nprediction Ë†ğ‘ğ‘–\nâ„“. The auxiliary output can be written as:\nË†ğ‘¦â„“ =\nh\u0010\nË†ğ‘1\nâ„“,Ë†ğ‘1\nâ„“\n\u0011\n,\n\u0010\nË†ğ‘2\nâ„“,Ë†ğ‘2\nâ„“\n\u0011\n,...,\n\u0010\nË†ğ‘ğ‘€\nâ„“ ,Ë†ğ‘ğ‘€\nâ„“\n\u0011i\n. (11)\nDetection transformers view object detection as a set prediction\nproblem, and use bipartite matching to make one-to-one correspon-\ndences between the model output and the ground-truth objects or\nbackground class âˆ…in the image [ 4]. Since no semantic label is\navailable on the target domain, the object detector is prone to pro-\nduce inaccurate matches between object queries and ground-truth\nobjects on the target domain. To tackle this problem, we ensemble\nthe outputs of different decoder layers and constrain the outputs of\neach decoder layer to produce consistent bipartite matching during\ntraining. The loss is defined as:\nLğ‘ğ‘œğ‘›ğ‘  = 1\nğ¿ğ‘‘ğ‘’ğ‘\nğ¿ğ‘‘ğ‘’ğ‘âˆ‘ï¸\nâ„“=1\nLğ‘ğ‘œğ‘›ğ‘  (Ë†ğ‘¦, Ë†ğ‘¦â„“), (12)\nwhere Ë†ğ‘¦ is the reference output obtained by averaging the pre-\ndictions of all decoder layers, Lcons is the consistency loss that\nmeasures the bipartite matching consistency between two predic-\ntions. Specifically, it is the combination of JS-divergence between\nthe classification outputs and the L1 distance between bounding\nbox regression outputs, ğ‘–.ğ‘’.,\nLğ‘ğ‘œğ‘›ğ‘  (Ë†ğ‘¦â„“â€²,Ë†ğ‘¦â„“)= 1\nğ‘€\nğ‘€âˆ‘ï¸\nğ‘–=1\nh\nJSD\n\u0010\nË†ğ‘ğ‘–\nâ„“â€²âˆ¥Ë†ğ‘ğ‘–\nâ„“\n\u0011\n+ğœ†L1\n\r\r\rË†ğ‘ğ‘–\nâ„“â€² âˆ’Ë†ğ‘ğ‘–\nâ„“\n\r\r\r1\ni\n, (13)\nwhere JSD(Â·âˆ¥Â·)represents JS-divergence, ğœ†L1 is a hyper-parameter\nto balance the two losses. In this way, we constrain the output of\ndifferent decoder layers to be consistent and improve the detection\ntransformerâ€™s discriminability on the target domain.\n3.5 Total Loss\nTo summarize, the final training objective of SFA is defined as:\nmin\nğº\nmax\nğ·\nLğ‘‘ğ‘’ğ‘¡(ğº)âˆ’ğœ†ğ‘’ğ‘›ğ‘Lğ‘’ğ‘›ğ‘(ğº,ğ·)âˆ’ğœ†ğ‘‘ğ‘’ğ‘Lğ‘‘ğ‘’ğ‘(ğº,ğ·)\n+ğœ†ğ‘ğ‘œğ‘›Lğ‘ğ‘œğ‘›ğ‘ (ğº),\n(14)\nwhere ğº is the object detector and ğ· denotes the domain discrimi-\nnators. ğœ†ğ‘‘ğ‘’ğ‘, and ğœ†ğ‘ğ‘œğ‘›ğ‘  are hyper-parameters that balance the loss\nterms. The min-max loss function is implemented by gradient re-\nverse layers [11].Our method is not restricted to specific detection\ntransformer. Instead, it is widely applicable to the family of detec-\ntion transformers, such as DETR [4] and Deformable DETR [51].\n4 THEORETICAL ANALYSIS\nThis section theoretically analyses our method. The performance on\nthe target domain is decomposed into three factors: (1) the expected\nerror on the source domain; (2) the domain divergence between\nsource and target; and (3) the error of the ideal joint hypothesis\nshared by both domains. The domain divergence can further be\nestimated via a generalization bound.\n4.1 Domain Adaptation Analysis\nAccording to the theory of domain adaptation by Ben-David et\nal. [2], the expected error on the target samples, ğ‘…T(â„), can be\ndecomposed into three terms, as shown in the following theorem:\nTheorem 4.1. Let ğ» be the hypothesis class. Given two domains\nSand T, we have\nâˆ€â„ âˆˆğ»,ğ‘… T(â„)â‰¤ ğ‘…S(â„)+ 1\n2ğ‘‘HÎ”H(S,T)+ ğœ†. (15)\nHere, ğ‘…T(â„)is the error of hypothesis â„on the target domain, and\nğ‘…S(â„)is the corresponding error on the source domain. ğ‘‘HÎ”H(S,T)\nrepresents the domain divergence that is associated with the feature\ntransferability, and ğœ†= minâ„âˆˆğ» [ğ‘…S(â„)+ğ‘…T(â„)]is the error of joint\nideal hypothesis that is associate with the feature discriminability.\nIn Inequality 15, ğ‘…Sis easily minimized by the supervised loss\non source domain. Besides, our token-wise feature alignment mini-\nmizes the domain divergence ğ‘‘HÎ”H(S,T)and improves the fea-\nture transferability. In the meantime, domain-query feature align-\nment adaptively selects and aligns token features with significant\ndomain gaps while maintaining discriminability for features with\nsmaller domain shifts. Bipartite matching consistency loss ensem-\nbles predictions made by multiple decoder layers to obtain a more\naccurate result. These designs ensure the feature discriminability\non the target domain and minimize ğœ†.\n4.2 Generalization Analysis\nAdversarial training is employed to help learn the mapping from the\ntarget domain to the source domain. The generalizability determines\nthe performance of the mapping [14, 26].\nDenote the generating distributions of the existing data ğœ‡and\nthe generating distribution of the generated data as ğœˆ. Denote the\nempirical counterparts of ğœ‡and ğœˆ are Ë†ğœ‡ğ‘ and ğœˆğ‘, where ğ‘ is the\nsize of the training sample set. Suppose adversarial training is learn-\ning a generator ğ‘” âˆˆG and a discriminator ğ‘“ âˆˆF, where Gand F\nare both the hypothesis classes. As described in Section 3.3, the dis-\ncriminator is a three-layer MLP which is constituted by three fully\nconnected layers and three nonlinear operations (nonlinearities),\n(ğ´1,ğœ1,ğ´2,ğœ2,ğ´3,ğœ3), where ğ´ğ‘– is a fully connected layer, and ğœğ‘–\nis a nonlinearity (ReLU). Then we have the following theory.\nTheorem 4.2 (Covering bound for the discriminator).Sup-\npose the spectral norm of each weight matrix is bounded: âˆ¥ğ´ğ‘–âˆ¥ğœ â‰¤ğ‘ ğ‘–.\nAlso, suppose each weight matrix ğ´ğ‘– has a reference matrix ğ‘€ğ‘–, which\nis satisfied that âˆ¥ğ´ğ‘– âˆ’ğ‘€ğ‘–âˆ¥ğœ â‰¤ğ‘ğ‘–, ğ‘– = 1,..., 3. Then, the ğœ€-covering\nnumber satisfies that\nlog N(F|ğ‘†,ğœ€, âˆ¥Â·âˆ¥ 2)\nâ‰¤\nlog \u00002ğ‘Š2\u0001 âˆ¥ğ‘‹âˆ¥2\n2\nğœ€2\n 3Ã–\nğ‘–=1\nğ‘ ğ‘–\n!2 3âˆ‘ï¸\nğ‘–=1\nğ‘2\nğ‘–\nğ‘ 2\nğ‘–\n, (16)\nwhere ğ‘Š is the largest dimension of the feature maps throughout the\nalgorithm.\nTable 1: Results of different methods for weather adaptation, ğ‘–.ğ‘’., Cityscapes to Foggy Cityscapes. FRCNN and DefDETR are\nabbreviations for Faster RCNN and Deformable DETR, respectively.\nMethod Detector person rider car truck bus train mcycle bicycle mAP\nFaster RCNN (Source) FRCNN 26.9 38.2 35.6 18.3 32.4 9.6 25.8 28.6 26.9\nDAF [5] FRCNN 29.2 40.4 43.4 19.7 38.3 28.5 23.7 32.7 32.0\nDivMatch [20] FRCNN 31.8 40.5 51.0 20.9 41.8 34.3 26.6 32.4 34.9\nSWDA [29] FRCNN 31.8 44.3 48.9 21.0 43.8 28.0 28.9 35.8 35.3\nSCDA [50] FRCNN 33.8 42.1 52.1 26.8 42.5 26.5 29.2 34.5 35.9\nMTOR [3] FRCNN 30.6 41.4 44.0 21.9 38.6 40.6 28.3 35.6 35.1\nCR-DA [36] FRCNN 30.0 41.2 46.1 22.5 43.2 27.9 27.8 34.7 34.2\nCR-SW [36] FRCNN 34.1 44.3 53.5 24.4 44.8 38.1 26.8 34.9 37.6\nGPA [37] FRCNN 32.9 46.7 54.1 24.7 45.7 41.1 32.4 38.7 39.5\nFCOS (Source) FCOS 36.9 36.3 44.1 18.6 29.3 8.4 20.3 31.9 28.2\nEPM [17] FCOS 44.2 46.6 58.5 24.8 45.2 29.1 28.6 34.6 39.0\nDeformable DETR (Source) DefDETR 37.7 39.1 44.2 17.2 26.8 5.8 21.6 35.5 28.5\nSFA (Ours) DefDETR 46.5 48.6 62.6 25.1 46.2 29.4 28.3 44.0 41.3\nTable 2: Results of different methods for synthetic to real\nadaptation, ğ‘–.ğ‘’., Sim10k to Cityscapes.\nMethods Detector car AP\nFaster RCNN (Source) FRCNN 34.6\nDAF [5] FRCNN 41.9\nDivMatch [20] FRCNN 43.9\nSWDA [29] FRCNN 44.6\nSCDA [50] FRCNN 45.1\nMTOR [3] FRCNN 46.6\nCR-DA [36] FRCNN 43.1\nCR-SW [36] FRCNN 46.2\nGPA [37] FRCNN 47.6\nFCOS (Source) FCOS 42.5\nEPM [17] FCOS 47.3\nDeformable DETR (Source) DefDETR 47.4\nSFA (Ours) DefDETR 52.6\nThis theorem is based on [1, 13]. A detailed proof is given in the\nSupplementary Material. Also, Zhang et al. [44] suggests that the\ngeneralizability of a GAN [11, 12] is determined by the hypothesis\ncomplexity of the discriminator. Following this insight, we employ\nsimple discriminators to enhance the generalizability and further\nenhance the domain adaptation performance.\n5 EXPERIMENTAL RESULTS\nIn this section, we evaluate the proposed SFA on three challenging\ndomain adaptation scenarios. Ablation studies are performed to\ninvestigate the impact of each component in SFA. Finally, visualiza-\ntion and analysis are presented for better understanding.\n5.1 Experimental Setup\n5.1.1 Datasets. Four public datasets are utilized in our experiments,\nincluding Cityscapes [7], Foggy Cityscapes [30], Sim10k [18], and\nBDD100k [38], which are detailed as follows.\nâ€¢Cityscapes [7] is collected from urban scenes, which con-\ntains 3,475 images with pixel-level annotation. Among them,\n2,975 and 500 images are used for training and evaluation,\nrespectively. Bounding box annotation of 8 different object\ncategories can be obtained by taking the tightest rectangles\nof object masks.\nâ€¢Foggy Cityscapes [30] is obtained by applying the fog syn-\nthesis algorithm to Cityscapes, based on depth maps in the\nCityscapes dataset. It inherited the annotations in Cityscapes\nand is suitable for the evaluation of weather adaptation.\nâ€¢Sim10k [18] is generated by the Grand Theft Auto game\nengine, which contains 10,000 training images with 58,701\nbounding box annotations for cars. It is suitable for the eval-\nuation of synthetic to real adaptation.\nâ€¢BDD100k [38] contains 100k images, including 70k training\nimages and 10k validation images annotated with bound-\ning boxes. Following [36], we extract the daytime subset of\nBDD100k for the evaluation of scene adaptation. The subset\nincludes 36,728 training images and 5,258 validation images.\nBased on these datasets, we evaluate the proposed SFA on three\ndomain adaptation scenarios: (1) Weather adaptation,ğ‘–.ğ‘’., Cityscapes\nto Foggy Cityscapes, where the models are trained on cityscape\nand tested on foggy cityscape; (2) Synthetic to real adaptation, ğ‘–.ğ‘’.,\nSim10k to Cityscapes, where the models are trained on Sim10k and\ntested on Cityscapes; and (3) Scene Adaptation, ğ‘–.ğ‘’., Cityscapes to\nthe daytime subset of BDD100k, where the models are trained on\nCityscapes and tested on BDD100k daytime subset. Mean Average\nPrecision (mAP) with a threshold of 0.5 is adopted as the evaluation\nmetric, following [5].\n5.1.2 Implementation Details. By default, our method is built on\nDeformable DETR [51]. ImageNet [9] pre-trained ResNet-50 [15] is\nadopted as the backbone in all experiments. Following Deformable\nDETR, we train the network using the Adam optimizer [21] for 50\nepochs. The learning rate is initialized as 2 Ã—10âˆ’4 and decayed by\n0.1 after 40 epochs. The batch size is set as 4 in all experiments. Both\nğœ†ğ‘’ğ‘›ğ‘ and ğœ†ğ‘‘ğ‘’ğ‘ are set as 1 for weather adaptation, and 0.01 for other\ndomain adaptation scenarios. Similarly,ğœ†ğ‘ğ‘œğ‘›ğ‘  is set as 1 for weather\nadaptation, and 0.1 for other scenarios. All our experiments are\nperformed on NVIDIA Tesla V100 GPUs. To show the generality\nof our method, we also provide results of our method built on\nDETR [4], as shown in the Supplementary Material.\nTable 3: Results of different methods for scene adaptation, ğ‘–.ğ‘’., Cityscapes to BDD100k daytime subset.\nMethods Detector person rider car truck bus mcycle bicycle mAP\nFaster R-CNN (Source) FRCNN 28.8 25.4 44.1 17.9 16.1 13.9 22.4 24.1\nDAF [5] FRCNN 28.9 27.4 44.2 19.1 18.0 14.2 22.4 24.9\nSWDA [29] FRCNN 29.5 29.9 44.8 20.2 20.7 15.2 23.1 26.2\nSCDA [50] FRCNN 29.3 29.2 44.4 20.3 19.6 14.8 23.2 25.8\nCR-DA [36] FRCNN 30.8 29.0 44.8 20.5 19.8 14.1 22.8 26.0\nCR-SW [36] FRCNN 32.8 29.3 45.8 22.7 20.6 14.9 25.5 27.4\nFCOS [33] (Source) FCOS 38.6 24.8 54.5 17.2 16.3 15.0 18.3 26.4\nEPM [17] FCOS 39.6 26.8 55.8 18.8 19.1 14.5 20.1 27.8\nDeformable DETR (Source) DefDETR 38.9 26.7 55.2 15.7 19.7 10.8 16.2 26.2\nSFA (Ours) DefDETR 40.2 27.6 57.5 19.1 23.4 15.4 19.2 28.9\nTable 4: Ablation studies on the Cityscapes to Foggy Cityscapes scenario. CNN represents feature alignment on the CNN\nbackbone. DQ, TW, BMC, and HR represent domain query-based feature alignment, token-wise feature alignment, bipartite\nmatching consistency, and hierarchical feature alignment, respectively.\nMethods CNN DQ TW BMC HR person rider car truck bus train mcycle bicycle mAP\nDeformable DETR (Source) 37.7 39.1 44.2 17.2 26.8 5.8 21.6 35.5 28.5\nProposed\nâœ“ 43.8 45.6 55.5 18.2 38.7 8.4 28.1 43.3 35.2\nâœ“ 45.1 46.8 61.3 21.6 36.6 9.5 30.6 40.4 36.5\nâœ“ 45.6 47.6 60.7 23.6 41.3 15.5 26.2 41.1 37.7\nâœ“ 38.0 39.3 45.6 16.5 28.4 6.0 24.5 37.5 29.5\nâœ“ âœ“ 45.7 47.6 61.6 23.7 43.8 16.4 28.2 43.3 38.8\nâœ“ âœ“ âœ“ 46.0 46.7 62.4 24.1 45.6 22.1 27.6 43.9 39.8\nâœ“ âœ“ âœ“ âœ“ 46.3 48.2 62.2 22.1 43.4 24.3 29.9 43.1 39.9\nSFA (Ours) âœ“ âœ“ âœ“ âœ“ 46.5 48.6 62.6 25.1 46.2 29.4 28.3 44.0 41.3\n5.2 Comparisons with SOTA Methods\n5.2.1 Weather Adaptation. The object detectors are often required\nto be applicable under various weather conditions. We use the\nCityscape to Foggy Cityscape scenario to evaluate the modelâ€™s\nrobustness to weather variations [42]. As shown in Table 1, SFA sig-\nnificantly improves Deformable DETRâ€™s cross-domain performance,\nachieving a 12.8 absolute gain of mAP50. Moreover, it outperforms\nall previous domain adaptive object detection methods.\n5.2.2 Synthetic to Real Adaptation. Training an object detector\non synthetic images that can generalize to real-world images is\nfascinating, as it significantly reduces the labor of data collection\nand annotation. To this end, we evaluate our SFA on the synthetic\nto real adaptation scenario, as shown in Table 2. It can be seen\nthat SFA improves the source-only Deformable DETR with an over\n10% relative performance gain and outperforms all existing domain\nadaptive object detection methods.\n5.2.3 Scene Adaptation. In real-world applications like autonomous\ndriving, scene layouts frequently change [22]. The performance of\nour SFA for scene adaptation is shown in Table 3. Following [36],\nresults on 7 common categories are reported. We can see that SFA\noutperforms the previous state-of-the-art. Moreover, a systematic\nimprovement can be observed, where SFA improves Deformable\nDETR on all 7 categories over the source only model.\n5.3 Ablation Studies\nTo better understand our method, we conduct ablation studies by\nisolating each component in SFA, as shown in Table 4. We have the\nfollowing observations: (1) both domain query-based feature align-\nment and token-wise feature alignment can alleviate the domain\ngaps and improve detection transformerâ€™s cross-domain perfor-\nmance by 8.0 and 9.2 mAP, respectively; (2) domain query-based\nfeature alignment and token-wise feature alignment are comple-\nmentary to each other. Thereby, a combination of both brings fur-\nther improvement; (3) the hierarchical feature alignment (HFA)\nis effective and brings a 1.0 mAP gain. Moreover, with HFA, the\nshallower sequence features near the CNN backbone is aligned.\nAs a result, feature alignment on the CNN backbone can be safely\nremoved without loss of performance; and (4) using the bipartite\nmatching consistency loss alone can bring a 1.0 performance gain.\nMoreover, it is complementary with the alignment on sequence\nfeatures, and further improves the model performance by 1.5 mAP.\n5.4 Visualization and Analysis\n5.4.1 Detection Results. In Figure 3 (a), we show some visual re-\nsults by Deformable DETR and our SFA, accompanied with the\nground-truth. As can be seen, in all three scenarios, SFA improves\nthe detection performance. It successfully mitigates the false pos-\nitives generated by Deformanble DETR and detects challenging\nobjects overlooked by Deformanble DETR. Interestingly, from the\nfirst row, we can see that SFA successfully detects the distant car\nthat is not labeled in the ground-truth, which further indicates its\ngeneralizability to the target domain.\n5.4.2 Visualization of Feature Distribution. We present the distri-\nbution of features extracted by the CNN backbone, the transformer\nDeformable DETR (Source) SFA (Ours) Ground-truth\nmotorcycle train bicyclebus riderperson car truck\n(a) Qualitative results (b) Feature distribution visualization using T-SNE\nCNN featuresEncoder featuresDecoder features\nSFA (Ours)Deformable DETR (Source)\nFigure 3: Visualizations. (a) Qualitative results: From top to bottom are results on the scenarios of Cityscapes to Foggy\nCityscapes, Sim10k to Cityscapes, and Cityscapes to BDD100k, respectively. (b) Visualization of feature distributions using\nT-SNE [34]. The blue circles denote the source features, while red circles represent target features.\nhigh\nlow\n(a) domain query attention on the encoder side\n(b) domain query attention on the decoder side\nFigure 4: Visualization of domain queries from both trans-\nformer encoder and decoder sides, under the Cityscapes to\nFoggy Citysccapes scenario.\nencoder, and the transformer decoder. As shown in Figure 3 (b),\nboth CNN and sequence features extracted by the source-only De-\nformable DETR can be easily separated by domain. By contrast,\nour SFA learns domain-invariant sequence features in both encoder\nand decoder. Interestingly, the features in the CNN backbone are\nalso aligned in SFA, as the shallower sequence features near the\nCNN backbone can be aligned by hierarchical feature alignment.\n5.4.3 Visualization of Domain Query. The domain query-based\nfeature alignment utilizes a novel domain query to adaptively ag-\ngregate and align global context from the sequence feature in the\nencoder or decoder. It is natural to wonder where the domain query\nlooks at and how much it values each sampling location. To obtain a\nbetter understanding of the domain query-based feature alignment,\nwe visualize the sampling locations and attention weights for the\ndomain query. In Figure 4 (a), it can be seen that the domain query\nin the encoder learns to attend to sampling locations with obvious\ndomain gaps, i.e., the upper part of the target images with large\nscene depth dense fog. Moreover, it puts more weight on tokens\nwith more significant domain shifts. A similar phenomenon can be\nseen on the decoder side, as shown in Figure 4(b), the domain query\nin the decoder put more attention on locations with significant\ndomain gaps. The slight difference is that it focuses more on the\nforeground objects, e.g., the cars, riders, and bicycles, as the context\nsequence feature in the decoder models foreground objects.\n6 CONCLUSION\nIn this paper, we focus on making the promising detection trans-\nformers domain adaptive. Based on the observation that the fea-\nture distribution alignment on CNN backbone does not guarantee\ndomain-invariant features in transformer for prediction, we pro-\npose sequence feature alignment (SFA) for detection transformers.\nDomain query-based feature alignment (DQFA) and token-wise fea-\nture alignment (TDA) with explicit technical insights are adopted\nto close domain gaps at global and local scales, respectively. More-\nover, a novel bipartite matching consistency loss is proposed to\nenhance the feature discriminability for robust object detection.\nExperimental results validate the effectiveness of our SFA. We hope\nour approach will serve as a solid baseline and help ease future\nresearch on developing domain adaptive detection transformers.\nACKNOWLEDGEMENT\nThis work is supported by National Key R&D Program of China un-\nder Grant 2020AAA0105701, National Natural Science Foundation\nof China (NSFC) under Grants 61872327, 61806062, U19B2038, Major\nSpecial Science and Technology Project of Anhui (No. 012223665049),\nand the University Synergy Innovation Program of Anhui Province\nunder Grants GXXT-2019-025.\nA MORE IMPLEMENTATION DETAILS\nA.1 Implementation Details of DA-CNN\nIn Figure 1 and Table 4, we presented the results of applying adver-\nsarial feature alignment [11] on the CNN backbone of Deformable\nDETR (denoted as DA-CNN in Figure 1). Specifically, we apply hi-\nerarchical feature alignment on the CNN backbone, following [16].\nFeature maps of stages ğ¶3 through ğ¶5 in ResNet-50 [15], plus one\nmore feature map obtained via a 3 Ã—3 stride 2 convolution on the\nfinal ğ¶5 stage are utilized for feature alignment. The sizes of the fea-\nture maps are reduced by the information invariant scale reduction\nmodules (SRMs) in [16] before seed into the domain discriminators.\nWe also implemented feature alignment on the CNN backbone\nfollowing SWDA [29]. Specifically, strong local feature alignment\non the feature map of stageğ¶2 and weak global feature alignment on\nthe feature map ofğ¶5 in ResNet-50 are adopted. We obtain a similar\nresult of 35.5 mAP compared to hierarchical feature alignment\nin [16]. Thus, we only show DA-CNN implemented by hierarchical\nfeature alignment in Figure 1, which is conceptually cleaner.\nA.2 Structure of Domain Discriminators\nAs discussed in Section 4.2, simple discriminators can enhance the\ngeneralizability and further enhance the domain adaptation perfor-\nmance. Specifically, we simply adopt a 3-layer multilayer perceptron\n(MLP) structure, as shown in Table 5. All four discriminators, in-\ncluding ğ·ğ‘’ğ‘›ğ‘ğ‘ , ğ·ğ‘’ğ‘›ğ‘ğ‘˜ , ğ·ğ‘‘ğ‘’ğ‘ğ‘ , and ğ·ğ‘‘ğ‘’ğ‘ğ‘˜ , share the same structure.\nBesides, ğ·ğ‘’ğ‘›ğ‘ğ‘ and ğ·ğ‘’ğ‘›ğ‘ğ‘˜ share the same weights. Similarly, ğ·ğ‘‘ğ‘’ğ‘ğ‘\nand ğ·ğ‘‘ğ‘’ğ‘ğ‘˜ share the weights. Domain discriminators for encoder\nand decoder are shared across different feature levels.\nTable 5: The architecture of the domain discriminators.\nDomain Discriminators\nFully Connected 256 Ã—256\nReLU\nFully Connected 256 Ã—256\nReLU\nFully Connected 256 Ã—2\nSoftmax\nB DETAILED THEORETICAL ANALYSIS\nProof of Theorem 4.2.We denote the spaces of the output func-\ntions ğ¹(ğ´1,...,ğ´ğ‘–âˆ’1)induced by the weight matrices ğ´ğ‘–,ğ‘– = 1,..., 5 by\nHğ‘–,ğ‘– = 1,..., 5, respectively. Lemma A.7 in [1], suggests inequality,\nlog N(F|ğ‘†)\nâ‰¤log\n 5Ã–\nğ‘–=1\nsup\nAğ‘–âˆ’1 âˆˆBğ‘–âˆ’1\nNğ‘–\n!\nâ‰¤\n5âˆ‘ï¸\nğ‘–=1\nlog\nÂ©Â­Â­Â­\nÂ«\nsup\n(ğ´1,...,ğ´ğ‘–âˆ’1)\nâˆ€ğ‘—<ğ‘–,ğ´ğ‘— âˆˆğµğ‘—\nN\n\u0010\b\nğ´ğ‘–ğ¹(ğ´1,...,ğ´ğ‘–âˆ’1)\n\t\n,ğœ€ğ‘–,âˆ¥Â·âˆ¥ 2\n\u0011ÂªÂ®Â®Â®\nÂ¬\n. (17)\nWe thus get the following inequality,\nlog N(F|ğ‘†)â‰¤\n5âˆ‘ï¸\nğ‘–=1\nğ‘2\nğ‘– âˆ¥ğ¹(ğ´1,...,ğ´ğ‘–âˆ’1)(ğ‘‹)âˆ¥2ğœ\nğœ€2\nğ‘–\nlog\n\u0010\n2ğ‘Š2\n\u0011\n. (18)\nMeanwhile,\nâˆ¥ğ¹(ğ´1,...,ğ´ğ‘–âˆ’1)(ğ‘‹)âˆ¥2\nğœ =âˆ¥ğœğ‘–âˆ’1 (ğ´ğ‘–âˆ’1ğ¹(ğ´1,...,ğ´ğ‘–âˆ’2)(ğ‘‹))âˆ’ ğœğ‘–âˆ’1 (0)âˆ¥2\nâ‰¤âˆ¥ğœğ‘–âˆ’1 âˆ¥âˆ¥ğ´ğ‘–âˆ’1ğ¹(ğ´1,...,ğ´ğ‘–âˆ’2)(ğ‘‹)âˆ’0âˆ¥2\nâ‰¤ğœŒğ‘–âˆ’1 âˆ¥ğ´ğ‘–âˆ’1 âˆ¥ğœâˆ¥ğ¹(ğ´1,...,ğ´ğ‘–âˆ’2)(ğ‘‹)âˆ¥2\nâ‰¤ğœŒğ‘–âˆ’1ğ‘ ğ‘–âˆ’1 âˆ¥ğ¹(ğ´1,...,ğ´ğ‘–âˆ’2)(ğ‘‹)âˆ¥2. (19)\nTherefore,\nâˆ¥ğ¹(ğ´1,...,ğ´ğ‘–âˆ’1)(ğ‘‹)âˆ¥2\nğœ â‰¤âˆ¥ğ‘‹âˆ¥2\nğ‘–âˆ’1Ã–\nğ‘—=1\nğ‘ 2\nğ‘–ğœŒ2\nğ‘–. (20)\nMotivated by the proof given in [1], we suppose equations:\nğœ€ğ‘–+1 = ğœŒğ‘–ğ‘ ğ‘–+1ğœ€ğ‘– , (21)\nğœ€5 = ğœŒ1\n4Ã–\nğ‘–=2\nğ‘ ğ‘–ğœŒğ‘–ğ‘ 5ğœ–1 , (22)\nğœ€ = ğœŒ1\n5Ã–\nğ‘–=2\nğ‘ ğ‘–ğœŒğ‘–ğœ–1 . (23)\nTherefore,\nğœ€ğ‘– =\nğœŒğ‘–\nÃğ‘–âˆ’1\nğ‘—=1 ğ‘ ğ‘—ğœŒğ‘—\nÃ5\nğ‘—=1 ğ‘ ğ‘—ğœŒğ‘—\nğœ€ . (24)\nTherefore,\nlog N(F|ğ‘†,ğœ€, âˆ¥Â·âˆ¥ 2)â‰¤\nlog \u00002ğ‘Š2\u0001 âˆ¥ğ‘‹âˆ¥2\n2\nğœ€2\n 5Ã–\nğ‘–=1\nğ‘ ğ‘–ğœŒğ‘–\n!2 5âˆ‘ï¸\nğ‘–=1\nğ‘2\nğ‘–\nğ‘ 2\nğ‘–\n, (25)\nwhich is exactly Equation 16 of Theorem 4.2.\nThe proof is completed. â–¡\nC DETR-BASED SFA\nTo show the generalizability of our method, we also implement\nour Sequence Feature alignment (SFA) based on DETR [4]. We note\nthree key difference between the implementation of Deformable\nDETR-based SFA and DETR-based SFA: (1) hierarchical CNN fea-\nture representation of 4 different levels are token as input to the\ntransformer in Deformable DETR, while DETR only uses one fea-\nture map at the ğ¶5 stage in ResNet; (2) the deformable transformer\nadopts deformable attention mechanism for the self-attention in\nthe encoder and the cross-attention in the decoder, while DETR\nadopts the attention mechanism in [35]. Therefore, for Deformable\nDETR, the sample position reference ğ‘â„“ in Equation 1 and Equation\n2 are learnable sparse sample locations, while for DETR, ğ‘â„“ simply\nenumerates all possible locations.\nC.1 Implementation Details\nWe evaluate our DETR-based SFA on Cityscapes [ 7] to Foggy\nCityscapes [30] scenarios. ImageNet [9] pre-trained ResNet-50 [15]\nis adopted as the backbone. Following DETR, we train the network\nusing the Adam optimizer [21] for 300 epochs. The learning rate\nis initialized as 10âˆ’4 for the transformer and 10âˆ’5 for the CNN\nbackbone. Learning rates are decayed by a factor of 10 after 200\nepochs. The batch size is set as 8 in all experiments. Both ğœ†ğ‘’ğ‘›ğ‘ and\nğœ†ğ‘‘ğ‘’ğ‘ are set as 0.01, ğœ†ğ‘ğ‘œğ‘›ğ‘  is set as 0.1 for other scenarios. All our\nexperiments are performed on NVIDIA Tesla V100 GPUs.\nTable 6: Sequence feature alignment built on DETR [4]. DQ, TW, BMC, and HR represents domain query-based feature align-\nment, token-wise feature alignment, bipartite matching consistency, and hierarchical feature alignment, respectively.\nMethods DQ TW BMC HR person rider car truck bus train mcycle bicycle mAP\nDETR (Source) 19.4 16.0 35.2 5.1 7.8 3.0 10.5 15.9 14.1\nProposed\nâœ“ 19.9 23.5 38.8 15.7 17.0 3.8 12.9 16.4 18.5\nâœ“ 18.3 24.4 41.8 14.7 22.4 5.6 9.3 17.0 19.2\nâœ“ 20.7 16.5 35.5 7.6 9.0 5.0 10.3 16.0 15.0\nâœ“ âœ“ 20.8 24.5 42.0 13.0 20.3 12.9 12.7 17.7 20.5\nâœ“ âœ“ âœ“ 21.2 23.5 44.0 17.6 25.2 12.7 13.9 18.0 22.0\nC.2 Results and Analysis\nThe detailed results are shown in Table 6. We observe a much worse\nbaseline, we believe this is due to the deficit of transformer compo-\nnents in processing image feature maps. The attention modules are\ninitialized to cast nearly uniform attention weights to all the pixels\nin the feature maps at initialization. As a result, more training data\nand training epochs are required for the attention weights to be\nlearned to focus on sparse meaningful locations. Limited by the\nnumber of training images on the Cityscapes dataset (2,975) and a\nsmall batch size of 8, DETR shows inferior performance.\nHowever, as shown in Table 6, our SFA still brings a 9.1 improve-\nment on mAP50 (a relative improvement of 64.5%) compared to\nthe DETR baseline, which verifies the effectiveness of our method.\nMoreover, ablation studies are provided by isolating each compo-\nnent of SFA. From the results in Table 6, we have the following\nobservations: (1) Similar to the results based on Deformable DETR,\nboth domain query-based feature alignment and token-wise feature\nalignment can alleviate the domain gaps and improve the detec-\ntion transformerâ€™s cross-domain performance by 4.4 and 5.1 mAP,\nrespectively; (2) domain query-based feature alignment and token-\nwise feature alignment are complementary to each other. Thereby,\na combination of both brings further improvement; (3) the hierar-\nchical feature alignment is effective and brings a 1.5 mAP gain; and\n(4) using the bipartite matching consistency constraint alone can\nbring a 0.9 performance gain. Moreover, it is complementary with\nthe feature alignment on sequence features, and further improves\nthe model performance by 1.2 mAP.\nTo summarize, our SFA is widely applicable to the detection\ntransformer family, e.g., DETR, Deformable DETR, and significantly\nimproves their cross-domain performance.\nD MORE ABLATION STUDIES\nAs describe in Section 3, both domain-query based feature align-\nment and token-wise feature alignment can be applied on both\nencoder and decoder sides. To gain a more comprehensive un-\nderstanding, we conduct detailed ablation studies on the domain\nquery-based feature alignment and the token-wise feature align-\nment. Both feature alignment modules applied to the last encoder\nand decoder layers, without hierarchical feature alignment and\nbipartite matching consistency loss.\nThe detailed results are shown in Table 7, we have the following\nobservations: (1) both domain-query based feature alignment and\ntoken-wise feature alignment alleviate the domain gaps between\nsource and target domains, either when they are being applied to the\nencoder or decoder side. Moreover, they are complementary to each\nother; (2) the token-wise feature alignment generally shows better\nperformance compare to the domain-query based feature alignment.\nWe assume this is because the domain-query only samples sparse\nlocations for global feature alignment, while the token-wise feature\nalignment enumerates all possible locations, and enjoys a relatively\nmore comprehensive feature alignment; (3) the feature alignment\napplied on the encoder side generally performs better it applied\nto the decoder side. We assume this is because the sequence in\nencoder side contains both foreground and background features,\nwhile the sequence on the decoder side concentrates on modeling\nthe foreground objects. As a result, the feature alignment on the\ndecoder side does not align background feature well, will the feature\nalignment on the encoder side enjoys a more comprehensive feature\nalignment.\nE MORE QUALITATIVE RESULTS\nMore qualitative results sampled from the Cityscapes to Foggy\nCityscapes, Sim10k to Cityscapes, and Cityscapes to BDD100k adap-\ntation are shown in Figure 5, 6 and 7, respectively. Detection results\nproduced by the source-only Deformable DETR and our SFA, ac-\ncompanied with the corresponding ground-truth are presented. As\ncan be seen, in all three scenarios, SFA improves the detection per-\nformance. It successfully mitigates the false positives generated by\nDeformanble DETR and detects challenging objects overlooked by\nDeformanble DETR.\nTable 7: Ablation studies on domain query-based feature alignment (DQFA) and token-wise feature alignment (TWFA), without\nhierarchical feature alignment or bipartite matching consistency loss. Experiments are conducted on the Cityscapes to Foggy\nCityscapes Scenario. DQğ‘’ğ‘›ğ‘ and DQğ‘‘ğ‘’ğ‘ indicate the DQFA applied to the last encoder and decoder layers, respectively. Similarly,\nTWğ‘’ğ‘›ğ‘ and TWğ‘‘ğ‘’ğ‘ indicate the TWFA applied to the last encoder and decoder layers, respectively\nMethods DQğ‘’ğ‘›ğ‘ DQğ‘‘ğ‘’ğ‘ TWğ‘’ğ‘›ğ‘ TWğ‘‘ğ‘’ğ‘ person rider car truck bus train mcycle bicycle mAP\nDeformable DETR 37.7 39.1 44.2 17.2 26.8 5.8 21.6 35.5 28.5\nProposed\nâœ“ 44.8 45.9 56.9 18.7 37.8 8.5 23.1 40.3 34.5\nâœ“ 42.8 43.5 54.5 13.4 37.9 8.2 28.6 40.5 33.7\nâœ“ 44.6 48.0 58.7 24.4 39.6 9.1 29.2 43.1 37.1\nâœ“ 44.9 48.1 60.3 16.2 38.0 10.7 29.0 40.9 36.1\nâœ“ âœ“ 45.1 46.8 61.3 21.6 36.6 9.5 30.6 40.4 36.5\nâœ“ âœ“ 45.6 47.6 60.7 23.6 41.3 15.5 26.2 41.1 37.7\nâœ“ âœ“ 45.6 48.2 61.0 23.0 39.8 16.3 30.1 41.0 38.1\nâœ“ âœ“ 46.3 47.6 61.5 18.8 42.2 15.7 26.9 41.7 37.6\nmotorcycle train bicyclebus riderperson car truck\nDeformable DETR (Source) SFA (Ours) Ground-truth\nFigure 5: More qualitative detection results on Cityscapes to Foggy Cityscapes scenario.\nDeformable DETR (Source) SFA (Ours) Ground-truth\nFigure 6: More qualitative detection results on Sim10k to Cityscapes scenario.\nDeformable DETR (Source) SFA (Ours) Ground-truth\nmotorcycle train bicyclebus riderperson car truck\nFigure 7: More qualitative detection results on Cityscapes to BDD100k scenario.\nREFERENCES\n[1] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. 2017. Spectrally-\nnormalized margin bounds for neural networks. In Proc. NIPS . 6240â€“6249.\n[2] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and\nJennifer Wortman Vaughan. 2010. A theory of learning from different domains.\nMachine learning 79, 1 (2010), 151â€“175.\n[3] Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, and Ting Yao.\n2019. Exploring object relation in mean teacher for cross-domain detection. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n11457â€“11466.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer Vision . Springer, 213â€“229.\n[5] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2018.\nDomain adaptive faster r-cnn for object detection in the wild. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 3339â€“3348.\n[6] Zhe Chen, Jing Zhang, and Dacheng Tao. 2020. Recursive context routing for\nobject detection. International Journal of Computer Vision (2020), 1â€“19.\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En-\nzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016.\nThe cityscapes dataset for semantic urban scene understanding. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 3213â€“3223.\n[8] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. 2020. UP-DETR: Unsu-\npervised Pre-training for Object Detection with Transformers. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition .\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE conference on computer\nvision and pattern recognition . IEEE, 248â€“255.\n[10] Liang Ding, Longyue Wang, and Dacheng Tao. 2020. Self-Attention with Cross-\nLingual Position Representation. In ACL.\n[11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The journal of machine learning\nresearch 17, 1 (2016), 2096â€“2030.\n[12] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial\nnetworks. (2014).\n[13] Fengxiang He, Tongliang Liu, and Dacheng Tao. 2020. Why resnet works?\nresiduals generalize. IEEE Transactions on Neural Networks and Learning Systems\n31, 12 (2020), 5349â€“5362.\n[14] Fengxiang He and Dacheng Tao. 2020. Recent advances in deep learning theory.\narXiv preprint arXiv:2012.10931 (2020).\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[16] Zhenwei He and Lei Zhang. 2019. Multi-adversarial faster-rcnn for unrestricted\nobject detection. In Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision . 6668â€“6677.\n[17] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. 2020. Every\nPixel Matters: Center-aware Feature Alignment for Domain Adaptive Object\nDetector. In European Conference on Computer Vision . Springer, 733â€“748.\n[18] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Srid-\nhar, Karl Rosaen, and Ram Vasudevan. 2017. Driving in the Matrix: Can virtual\nworlds replace human-generated annotations for real world tasks?. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE, 746â€“753.\n[19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Changick Kim. 2019. Self-\ntraining and adversarial background regularization for unsupervised domain\nadaptive one-stage object detection. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 6092â€“6101.\n[20] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim.\n2019. Diversify and match: A domain adaptive representation learning paradigm\nfor object detection. InProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 12456â€“12465.\n[21] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. In International Conference on Learning and Representations .\n[22] Meng Lan, Yipeng Zhang, Lefei Zhang, and Bo Du. 2020. Global context based au-\ntomatic road segmentation via dilated convolutional neural network.Information\nSciences 535 (2020), 156â€“171.\n[23] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen\nTu. 2015. Deeply-supervised nets. In Artificial intelligence and statistics . PMLR,\n562â€“570.\n[24] Daqing Liu, Hanwang Zhang, Feng Wu, and Zheng-Jun Zha. 2019. Learning to\nassemble neural module tree networks for visual grounding. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision . 4673â€“4682.\n[25] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,\nCheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox detector.\nIn European conference on computer vision . Springer, 21â€“37.\n[26] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. Foundations\nof machine learning . MIT press.\n[27] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You\nonly look once: Unified, real-time object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition . 779â€“788.\n[28] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster R-CNN: to-\nwards real-time object detection with region proposal networks.IEEE transactions\non pattern analysis and machine intelligence 39, 6 (2016), 1137â€“1149.\n[29] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. 2019. Strong-\nweak distribution alignment for adaptive object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 6956â€“6965.\n[30] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2018. Semantic foggy scene\nunderstanding with synthetic data. International Journal of Computer Vision 126,\n9 (2018), 973â€“992.\n[31] Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang, Wen Wang, and Weiming\nLu. 2021. Locate and Label: A Two-stage Identifier for Nested Named Entity\nRecognition. In ACL.\n[32] Yongliang Shen, Xinyin Ma, Yechun Tang, and Weiming Lu. 2021. A Trigger-\nSense Memory Flow Framework for Joint Entity and Relation Extraction. In\nProceedings of the Web Conference 2021 . 1704â€“1715.\n[33] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. 2019. Fcos: Fully convolutional\none-stage object detection. InProceedings of the IEEE/CVF International Conference\non Computer Vision . 9627â€“9636.\n[34] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, 11 (2008).\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Conference on Neural Information Processing Systems .\n[36] Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, and Xiu-Shen Wei. 2020. Exploring\ncategorical regularization for domain adaptive object detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 11724â€“\n11733.\n[37] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wenjun Zhang. 2020. Cross-\ndomain detection via graph-induced prototype alignment. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 12355â€“12364.\n[38] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Mad-\nhavan, and Trevor Darrell. 2018. Bdd100k: A diverse driving video database with\nscalable annotation tooling. arXiv preprint arXiv:1805.04687 2, 5 (2018), 6.\n[39] Zheng-Jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, and Feng Wu.\n2019. Context-aware visual policy network for fine-grained image captioning.\nIEEE transactions on pattern analysis and machine intelligence (2019).\n[40] Wei Zhai, Yang Cao, Zheng-Jun Zha, HaiYong Xie, and Feng Wu. 2020. Deep\nstructure-revealed network for texture recognition. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 11010â€“11019.\n[41] Wei Zhai, Yang Cao, Jing Zhang, and Zheng-Jun Zha. 2019. Deep multiple-\nattribute-perceived network for real-world texture recognition. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision . 3613â€“3622.\n[42] Jing Zhang and Dacheng Tao. 2019. FAMED-Net: A fast and accurate multi-scale\nend-to-end dehazing network. IEEE Transactions on Image Processing 29 (2019),\n72â€“84.\n[43] Jing Zhang and Dacheng Tao. 2020. Empowering things with intelligence: a\nsurvey of the progress, challenges, and opportunities in artificial intelligence of\nthings. IEEE Internet of Things Journal 8, 10 (2020), 7789â€“7817.\n[44] Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. 2018.\nOn the Discrimination-Generalization Tradeoff in GANs. In International Confer-\nence on Learning Representations .\n[45] Qiming ZHANG, Jing Zhang, Wei Liu, and Dacheng Tao. 2019. Category Anchor-\nGuided Unsupervised Domain Adaptation for Semantic Segmentation. Advances\nin Neural Information Processing Systems 32 (2019), 435â€“445.\n[46] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and\nZheng-Jun Zha. 2020. Object relational graph with teacher-recommended learn-\ning for video captioning. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition . 13278â€“13288.\n[47] Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, and Zheng-Jun\nZha. 2020. Exploiting Sample Uncertainty for Domain Adaptive Person Re-\nIdentification. arXiv preprint arXiv:2012.08733 (2020).\n[48] Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo, and Zheng-Jun Zha.\n2021. Group-aware label transfer for domain adaptive person re-identification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n5310â€“5319.\n[49] Kecheng Zheng, Wu Liu, Jiawei Liu, Zheng-Jun Zha, and Tao Mei. 2020. Hierar-\nchical Gumbel Attention Network for Text-based Person Search. In Proceedings\nof the 28th ACM International Conference on Multimedia . 3441â€“3449.\n[50] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and Dahua Lin. 2019.\nAdapting object detectors via selective cross-domain alignment. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 687â€“696.\n[51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020.\nDeformable DETR: Deformable Transformers for End-to-End Object Detection.\nIn International Conference on Learning and Representations .",
  "topic": null,
  "concepts": []
}