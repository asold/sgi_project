{
    "title": "Large Language Models for Synthetic Tabular Health Data: A Benchmark Study",
    "url": "https://openalex.org/W4401834031",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2513929708",
            "name": "Marko Miletic",
            "affiliations": [
                "Bern University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2109766560",
            "name": "Murat Sariyar",
            "affiliations": [
                "Bern University of Applied Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4319793437",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W3012140936",
        "https://openalex.org/W4288296172",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3115265188",
        "https://openalex.org/W4300980427",
        "https://openalex.org/W4387839097",
        "https://openalex.org/W4387457231",
        "https://openalex.org/W4306178203"
    ],
    "abstract": "Synthetic tabular health data plays a crucial role in healthcare research, addressing privacy regulations and the scarcity of publicly available datasets. This is essential for diagnostic and treatment advancements. Among the most promising models are transformer-based Large Language Models (LLMs) and Generative Adversarial Networks (GANs). In this paper, we compare LLM models of the Pythia LLM Scaling Suite with varying model sizes ranging from 14M to 1B, against a reference GAN model (CTGAN). The generated synthetic data are used to train random forest estimators for classification tasks to make predictions on the real-world data. Our findings indicate that as the number of parameters increases, LLM models outperform the reference GAN model. Even the smallest 14M parameter models perform comparably to GANs. Moreover, we observe a positive correlation between the size of the training dataset and model performance. We discuss implications, challenges, and considerations for the real-world usage of LLM models for synthetic tabular data generation.",
    "full_text": null
}