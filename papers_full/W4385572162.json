{
  "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
  "url": "https://openalex.org/W4385572162",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2908416304",
      "name": "Shima Imani",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1909139773",
      "name": "Liang Du",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2223397902",
      "name": "Harsh Shrivastava",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2167366201",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4221161695"
  ],
  "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 37–42\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nMathPrompter: Mathematical Reasoning using Large Language Models\nShima Imani, Liang Du, Harsh Shrivastava\nMicrosoft Research, Redmond, USA\nContact: shimaimani@microsoft.com\nAbstract\nLarge Language Models (LLMs) have limited\nperformance when solving arithmetic reason-\ning tasks and often provide incorrect answers.\nUnlike natural language understanding, math\nproblems typically have a single correct an-\nswer, making the task of generating accurate\nsolutions more challenging for LLMs. To the\nbest of our knowledge, we are not aware of\nany LLMs that indicate their level of confi-\ndence in their responses which fuels a trust\ndeficit in these models impeding their adop-\ntion. To address this deficiency, we propose\n‘MathPrompter’, a technique that improves per-\nformance of LLMs on arithmetic problems\nalong with increased reliance in the predic-\ntions. MathPrompter uses the Zero-shot chain-\nof-thought prompting technique to generate\nmultiple Algebraic expressions or Python func-\ntions to solve the same math problem in dif-\nferent ways and thereby raise the confidence\nlevel in the output results. This is in contrast to\nother prompt based CoT methods, where there\nis no check on the validity of the intermedi-\nate steps followed. Our technique improves\nover state-of-the-art on the MultiArith dataset\n(78.7% →92.5%) evaluated using 175B pa-\nrameter GPT-based LLM.\n1 Introduction\nRecent advancements in natural language process-\ning (NLP) can be attributed to massive scaling of\nLarge Language Models (LLMs) (Vaswani et al.,\n2017; Devlin et al., 2018; Raffel et al., 2020; Brown\net al., 2020; Rae et al., 2021; Chowdhery et al.,\n2022; Thoppilan et al., 2022). A very interest-\ning recent discovery that the LLMs are naturally\ngood (in-context) Zero-shot or few-shot learners\nturned out to be very useful (Brown et al., 2020;\nLiu et al., 2021, 2023). This led to the development\nof ‘prompting’ technique, where the user provides\na small context for solving the task at-hand to the\nLLM. This conditioning of the models on a few\nexamples is termed as few-shot prompting, while\nproviding instructions to solve a task is known as\nZero-shot prompting. Extensive research efforts\nare being poured into designing these prompts, ei-\nther manually (Schick and Schütze, 2020; Reynolds\nand McDonell, 2021) or automatically (Shin et al.,\n2020; Gao et al., 2020). Although quite successful\nfor single-step system-I tasks (Stanovich and West,\n2000; Liu et al., 2023), the prompting techniques\nwere inadequate in their performance on system-II\ntasks where multi-step reasoning is required (Rae\net al., 2021). As humans, we tend to break down\na problem and attempt to solve them step-by-step.\nExtending this intuition to LLMs led to the de-\nvelopment of ‘chain-of-thought’ (CoT) prompting\ntechnique (Wei et al., 2022; Wang et al., 2022). The\nuse of CoT has led to improved performance on a\nrange of NLP tasks (Talmor et al., 2018; Gao et al.,\n2020; Patel et al., 2021; Cobbe et al., 2021; Geva\net al., 2021; Chowdhery et al., 2022; Srivastava\net al., 2022)\nIn this work, we investigate Zero-shot-CoT meth-\nods for solving mathematical reasoning tasks. To\nthe best of our knowledge, we found the recent\nwork by (Kojima et al., 2022) that proposed a Zero-\nshot-CoT technique to be the state-of-the-art where\nthey demonstrated a remarkable accuracy improve-\nment on the ‘MultiArith’ (Roy and Roth, 2016)\ndata (17.7% →78.7%). Now, we identify two\nkey aspects that lacks in the previous CoT prompt-\ning based SOTA, namely (1) Although, the chain-\nof-thought followed by the model improved the\nresults, but there is no check on the validity of\nthe steps followed by the chain-of-thought prompt-\ning and (2) The confidence in the predictions of\nLLMs are often not provided. In order to address\nthese gap to some extent, we derive inspiration\nfrom how we humans solve a math question by\nbreaking it down to a simpler multi-step procedure\nand make use of multiple ways to validate our ap-\nproach at each step. Specifically, given a question\nQ, (I) Generating Algebraic template: We first gen-\n37\nPython prompt \nWrite a python function that returns\nthe answer.\nAlgebraic prompt \nWrite a mathematical equation and\ngenerate the answer format starting\nwith `Answer =' \nLLM\nInput Query \n(I) Generating\nAlgebraic template\n(II) Math-Prompts\n(III) Compute\nVerification  \n\"Eval()\"\n(IV) Statistical\nsignificance\nFigure 1: MathPrompter flow. We outline the MathPrompter process with an example alongside.\nerate its corresponding Algebraic expression Qt\nthat replaces the numerical entries by variables. (II)\nMath-prompts: Then, we provide multiple prompts\nP to the LLM that can solve Qt analytically in\ndifferent ways. For eg. P can be ‘Derive an Al-\ngebraic expression’ or ‘Write a Python function’\netc. Following this procedure, we end up with P ex-\npressions that analytically solves Qt in terms of its\nvariables. (III) Compute verification: We then eval-\nuate the P analytical solutions by allotting multiple\nrandom values to the Qt variables. (IV) Statistical\nsignificance: If the solutions of the P analytical\nfunctions are in ‘consensus’ over N ∼5 different\nvariable choices, then we substitute the original val-\nues from Q to obtain the final solution. In the case\nwhere there is no definite consensus, we repeat the\nsteps (II), (III) & (IV). Our method, MathPrompter,\nuses 175B parameter LLM called GPT3 DaVinci\ncompletion engine (Brown et al., 2020). We were\nable to improve the accuracy on the MultiArith data\nfrom 78.7% →92.5%.\n2 Method\nSince the LLMs are generative models, it becomes\nvery tricky to ensure that the generated answers\nare accurate, especially for mathematical reasoning\ntasks. We take clues from the process followed\nby students to solve arithmetic problems. We nar-\nrowed down a few steps that students take in order\nto verify their solutions, namely\n• Compliance with known results: By comparing\nthe solution to a known result, one can assess its\naccuracy and make necessary adjustments. This is\nparticularly useful when the question is a standard\nproblem with a well-established solution.\n• Multi-verification: By approaching a problem\nfrom multiple perspectives and comparing the re-\nsults helps to confirm the validity of the solution\nand ensure that it is both sound and accurate.\n• Cross-checking: The process of solving a prob-\nlem is just as necessary as the final answer. Veri-\nfying the correctness of the intermediate steps of\nthe process provide a clear understanding of the\nthought process behind the solution.\n• Compute verification: Utilizing a calculator or\ncomputer to perform arithmetic calculations can\nassist in verifying the accuracy of the final answer.\n2.1 MathPrompter\nOur proposed method, MathPrompter, is an attempt\nto transfer some of this thought process to the\nLLM answer generation process. Fig. 1 provides\na high-level overview of steps followed by Math-\nPrompter to solve a mathematical reasoning prob-\nlem. We use the state-of-the-art GPT-3 DaVinci\n38\ncompletion engine (Brown et al., 2020) for the\nquestion-answering tasks.\nWe use the following question ‘Q’ from the Mul-\ntiArith dataset to demonstrate the problem solving\nprocess followed by MathPrompter.\nQ: At a restaurant, each adult meal costs\n$5 and kids eat free. If a group of 15\npeople came in and 8 were kids, how\nmuch would it cost for the group to eat?\n(I) Generating Algebraic template: We begin by\ntransforming the question into its Algebraic form\nby replacing the numeric entries with variables us-\ning a key-value mapping. In this particular instance,\nthe modified question ‘Qt’ becomes:\nQt: at a restaurant, each adult meal costs\nA and kids eat free. if a group of B peo-\nple came in and C were kids, how much\nwould it cost for the group to eat?\nMapping: {A:5, B:15, C:8}\n(II) Math-prompts: We build up on the intu-\nition provided by the multi-verification and cross-\nchecking thought processes mentioned above. We\ngenerate analytical solutions of Qt using two differ-\nent approaches, Algebraic way and Pythonic way.\nWe give the following prompts to the LLM to gen-\nerate additional context for Qt\nAlgebraic prompt: Write a mathematical\nequation and generate the answer format\nstarting with ‘Answer =’\nPython prompt: Write a Python function\nthat returns the answer.\nThe LLM model in response to the above\nprompts generated the following output expres-\nsions\n# Algebraic expression output\nAnswer = A*(B-C)\n# Python expression output\ndef total_price(A, B, C):\nreturn A * (B-C)\nThe above generated analytical solutions gives\nthe user a hint into the ‘intermediate thought pro-\ncess’ of the LLM. Incorporating additional prompts\nwill improve the accuracy and consistency of the\nresults. This will, in turn, enhance the Math-\nPrompter’s ability to generate more precise and\neffective solutions.\n(III) Compute verification: We evaluate the ex-\npressions generated in the previous step using mul-\ntiple randomized key-value mappings of the input\nvariables in Qt. To evaluate the expressions, we\nused the Python’s eval() method. We compare\nthe outputs to see if we can find a consensus among\nthe answers. This also provides us with a higher\nlevel of confidence that the answers are correct\nand reliable. Once the expressions agree on their\noutputs, we use the values of the variables in the\ninput Q to compute the final answer, as below\nAlgebraic-answer = 35\nPythonic-answer = 35\n(IV) Statistical significance: In order to ensure\nthat consensus is reached among various expres-\nsions’ output, in our experiments, we repeat the\nsteps (II) & (III) for N ∼5 times and report the\nmost frequent value observed for the answer.\n3 Experiment\n3.1 Dataset\nWe evaluate MathPrompter on Multi-\nArith dataset (Roy and Roth, 2016), which\nis a subset of the Math World Problem Repository\n(Koncel-Kedziorski et al., 2016). This dataset is\na collection of mathematical problems that are\nspecifically designed to test the ability of machine\nlearning models to perform complex arithmetic\noperations and reasoning. These problems demand\nthe application of multiple arithmetic operations\nand logical reasoning to be sucessfully solved.\n3.2 Baseline\nOne of the popular baselines is the standard Zero-\nshot model by (Brown et al., 2020). Their train their\nmodels in a way that it is able to recognize and clas-\nsify new objects or classes that it has never seen\nbefore during training. This was achieved by utiliz-\ning the semantic relationships between classes.\nWe also compared against the state-of-the-art\nZero-shot-CoT prompting model by (Kojima et al.,\n2022). This is a very recent approach that addresses\nthe limitations of the standard Zero-shot learning\nby incorporating a ‘context of the task’ using CoT\nto improve the performance. Briefly, their method\nfollows this procedure. Given a question Q, the\nauthors use the prompt ‘Lets think step-by-step’\nfollowed by Q to generate a response Z. Then, they\nuse the prompt ‘The answer (Arabic numericals) is’\nfollowed by Z to get their final result.\n39\nModel Accuracy\nZero-shot 17.7\nZero-shot (PaLM 540B) 25.5\nZero-shot-CoT 78.7\nZero-shot-CoT (PaLM 540B) 66.1\nZero-shot-CoT + self consistency (PaLM 540B) 89.0\nZero-shot-CoT (MathPrompter) 92.5\nFew-Shot (2 samples) 33.7\nFew-Shot (8 samples) 33.8\nFew-Shot-CoT (2 samples) 84.8\nFew-Shot-CoT (4 samples) 90.5\nFew-Shot-CoT (8 samples) 93.0\nZero-Plus-Few-Shot-CoT (8 samples) 92.8\nTable 1: Accuracy on MultiArith dataset. MathPrompter outperforms all the Zero-shot & Zero-shot-CoT baselines. We\nemphasize that our model’s performance is comparable to 540B parameter models as well as the SOTA Few-shot-CoT approaches.\n(If not mentioned explicitly, the models in each row consists of 175B parameters. Results are borrowed from (Kojima et al.,\n2022). They used Textdavinci-002 (175B) model along with the same 8 examples as described in (Wei et al., 2022) for Few-shot\nand Few-shot-CoT settings.)\n3.3 Results\n3.3.1 Accuracy comparisons\nTable 1 compares the performance of the Math-\nPrompter against the baseline models. The results\nof few-shot & zero-shot learning based approaches\nare shown. Furthermore, we add the results for\nmodels with different number of parameters to get\nbetter highlight the significance of our approach.\nSince, MathPrompter is a Zero-shot-CoT (175B\nparameters) method, we choose the state-of-the-art\nZero-shot-CoT (175B parameters) model by (Ko-\njima et al., 2022) and a Zero-shot(175B parameters)\nby (Brown et al., 2020) for fair comparison. We\nreport an accuracy of 92.5% which is a huge im-\nprovement to the other SOTA models with 78.7%\nand 17.7% accuracy, respectively.\n3.3.2 Example comparisons\nTable 2 presents a sample set of questions and\ntheir respective outputs, intermediate steps, and\nfinal answers generated by both MathPrompterand\nthe current state-of-the-art model (Kojima et al.,\n2022). For simplicity, only one output of Math-\nPrompter for each question is shown for both the\nAlgebraic and Pythonic outputs.\nThe table highlights areas where (Kojima et al.,\n2022) technique falls short, and where these can\nbe remedied with MathPrompter , which was de-\nsigned to address these issues. For example, the\ngenerated answers sometimes have one step of er-\nror, which can be avoided by running the model\nmultiple times and reporting the consensus results.\nAdditionally, the reasoning steps in (Kojima et al.,\n2022) can be excessively lengthy, but the Pythonic\nor Algebraic methods can address this by typically\nrequiring fewer tokens. Furthermore, the reasoning\nsteps may be correct, but the final computation is\nincorrect. MathPrompter address problem by using\nthe Python’s eval() method function.\nIn many cases, the MathPrompter generates cor-\nrect intermediate and final answers. However, there\nare a few cases, such as the last question in Table 2,\nwhere both the Algebraic and Pythonic outputs are\nin agreement, yet erroneous. We plan to address\nthese issues by incorporating additional methods to\nfurther enhance the performance of MathPrompter .\n4 Conclusions & Discussions\nWe introduced MathPrompter, a novel approach\nthat improves LLM performance on mathematical\nreasoning problems. It also addresses an important\nconcern of building the user trust to some extent in\nthe LLM predictions. We translated our intuition on\nhow students solve arithmetic problems to a LLM\nmodel by utilizing the Zero-shot chain-of-thought\nprompting technique. MathPrompter incorporates\nideas like cross-checking the intermediate steps\nand solving the same math problem using multiple\napproaches in its design. We empirically show that\nour model is comparable to SOTA Few-shot-CoT\nmodels as well as the larger Zero-shot-CoT models\nthat have 540B parameters. In future, we plan to\nfurther evaluate performance on additional datasets\nand explore incorporating additional prompts into\n40\nTable 2:\nExample-based comparison between current state of art Zero-shot-CoT and MathPrompter .\nQuestion SOTA Zero-shot-CoT MathPrompter\nQ: At the fair Adam bought 13\ntickets. After riding the ferris\nwheel he had 4 tickets left. If\neach ticket cost 9 dollars, how\nmuch money did Adam spend\nriding the ferris wheel?\nA (ONE UNNECESSARY STEP ERROR): Let’s\nthink step by step. Adam bought 13 tickets. That\nmeans he spent 13 * 9 = 117 dollars on tickets.\nAfter riding the ferris wheel, he had 4 tickets left.\nThat means he used 9 tickets to ride the ferris wheel.\nThat means he spent 9 * 9 = 81 dollars on riding the\nferris wheel. So the answer is 117 - 81 = 36 dollars.\nTherefore, the answer (arabic numerals) is 36. ✗\nMapping = {A:13, B:4, C:9}\n# Algabraic answer\nAnswer = A*(B-C)\n#Python code\ndef total_price(A, B, C):\nreturn A * (B-C)\nAnswer = 81 ✓\nQ: Each chocolate bar in a box\ncost $3. If a box had 9 bars\ntotal and Wendy sold all but 3\nbars, how much money would\nshe have made?\nA (OTHERS; MAXIMUM LIMIT OF TEXT\nLENGTH): Let’s think step by step. Each chocolate\nbar in a box costs $3. If a box had 9 bars total, then\nWendy sold all but 3 bars. How much money would\nshe have made? We can solve this problem using al-\ngebra. Let’s start by creating a variable to represent\nthe number of chocolate bars Wendy sold. We’ll\ncall this variable \"x\". If Wendy sold \"x\" chocolate\nbars, then she would have 9 - x chocolate bars left in\nthe box. We know that each chocolate bar costs $3,\nso the total cost of the chocolate bars Wendy sold\nTherefore, the answer (arabic numerals) is: Wendy\nwould have made $12. ✗\nMapping = {A:3, B:9, C:3}\n# Algabraic answer\nAnswer = A x (B - C)\n#Python code\ndef money_made(A, B, C):\nreturn (B-C)*A\nAnswer = 18 ✓\nQ: Jerry was helping the cafete-\nria workers pick up lunch trays,\nbut he could only carry 8 trays\nat a time. If he had to pick up 9\ntrays from one table and 7 trays\nfrom another, how many trips\nwill he make?\nA (COMMONSENSE MISTAKE): Let’s think step\nby step. Jerry can carry 8 trays at a time. There are\n9 trays on the first table. That means Jerry will need\nto make 2 trips to the first table. There are 7 trays\non the second table. That means Jerry will need\nto make 1 trip to the second table. In total, Jerry\nwill make 3 trips. Therefore, the answer (arabic\nnumerals) is 3. ✗\nMapping = {A:8, B:9, C:7}\n# Algabraic answer\nAnswer = (B+C)/A\n#Python code\ndef numTrips(A, B, C):\ntrips = (B + C) // A\nif (B + C) % A != 0:\ntrips += 1\nreturn trips\nAnswer = 2 ✓\nQ: Kaleb bought 14 boxes of\nchocolate candy and gave 5 to\nhis little brother. If each box\nhas 6 pieces inside it, how many\npieces did Kaleb still have?\nA: Let’s think step by step. Kaleb bought 14 boxes\nof chocolate candy. This means that he had 84\npieces of chocolate candy. He gave 5 boxes to his\nlittle brother, which means that he gave his little\nbrother 30 pieces of chocolate candy. Kaleb still\nhad 54 pieces of chocolate candy. Therefore, the\nanswer (arabic numerals) is 54 ✓.\nMapping = {A:14, B:5, C:6}\n# Algabraic answer\nAnswer = A*C - B\n#Python code\ndef candy_pieces(A, B, C):\nreturn A*C - B\nAnswer = 79 (COMMONSENSE MIS-\nTAKE)✗\nMathPrompter.\n5 Limitation\nOne of the limitations of our work is that while\nwe are running the MathPrompter multiple times\nin different ways to increase the accuracy of our\nresults, this does not always guarantee the correct-\nness of the output. Both Algebraic and Pythonic\nexpressions have the potential to produce the incor-\nrect results, even if the prompt outputs match each\nother. This is the fail case as shown in the last row\nof Table 2. Increasing the number of prompts will\nmitigate this issue. We are currently investigating\ntechniques that can address this issue in a more\nprincipled manner.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\n41\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In Proceedings of\nthe 2016 conference of the north american chapter of\nthe association for computational linguistics: human\nlanguage technologies, pages 1152–1157.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve\nsimple math word problems? arXiv preprint\narXiv:2103.07191.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1–7.\nSubhro Roy and Dan Roth. 2016. Solving gen-\neral arithmetic word problems. arXiv preprint\narXiv:1608.01413.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nKeith E Stanovich and Richard F West. 2000. 24. indi-\nvidual differences in reasoning: Implications for the\nrationality debate? Behavioural and Brain Science,\n23(5):665–726.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\n42",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5780381560325623
    },
    {
      "name": "Python (programming language)",
      "score": 0.5768276453018188
    },
    {
      "name": "Task (project management)",
      "score": 0.5706771612167358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3944852352142334
    },
    {
      "name": "Mathematics education",
      "score": 0.36449432373046875
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3442320227622986
    },
    {
      "name": "Programming language",
      "score": 0.27560073137283325
    },
    {
      "name": "Psychology",
      "score": 0.1832588016986847
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}