{
  "title": "Active Learning on Pre-trained Language Model with Task-Independent Triplet Loss",
  "url": "https://openalex.org/W4283791551",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2484761198",
      "name": "Seungmin Seo",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2098404811",
      "name": "Dong-Hyun Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A4284807676",
      "name": "Youbin Ahn",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2115347219",
      "name": "Kyong-Ho Lee",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2484761198",
      "name": "Seungmin Seo",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2098404811",
      "name": "Dong-Hyun Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A4284807676",
      "name": "Youbin Ahn",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2115347219",
      "name": "Kyong-Ho Lee",
      "affiliations": [
        "Yonsei University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949568611",
    "https://openalex.org/W2948367246",
    "https://openalex.org/W2135815793",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W2766801974",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W2187127363",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W6735374517",
    "https://openalex.org/W2966048335",
    "https://openalex.org/W2951061410",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2914802228",
    "https://openalex.org/W2951450498",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W6761381025",
    "https://openalex.org/W6761687610",
    "https://openalex.org/W3095594087",
    "https://openalex.org/W2740365665",
    "https://openalex.org/W6653211413",
    "https://openalex.org/W2911327180",
    "https://openalex.org/W2471138382",
    "https://openalex.org/W6762256473",
    "https://openalex.org/W3093169023",
    "https://openalex.org/W3019524780",
    "https://openalex.org/W6776173555",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2152748974",
    "https://openalex.org/W582134693",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W2956371155",
    "https://openalex.org/W2952402849",
    "https://openalex.org/W2976211247",
    "https://openalex.org/W2911333381",
    "https://openalex.org/W1872312298",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W3034607353",
    "https://openalex.org/W2011674654",
    "https://openalex.org/W3035499919",
    "https://openalex.org/W2296128027",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2986514296",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W4295521015",
    "https://openalex.org/W2963613359",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W2910982074",
    "https://openalex.org/W3179550234"
  ],
  "abstract": "Active learning attempts to maximize a task model’s performance gain by obtaining a set of informative samples from an unlabeled data pool. Previous active learning methods usually rely on specific network architectures or task-dependent sample acquisition algorithms. Moreover, when selecting a batch sample, previous works suffer from insufficient diversity of batch samples because they only consider the informativeness of each sample. This paper proposes a task-independent batch acquisition method using triplet loss to distinguish hard samples in an unlabeled data pool with similar features but difficult to identify labels. To assess the effectiveness of the proposed method, we compare the proposed method with state-of-the-art active learning methods on two tasks, relation extraction and sentence classification. Experimental results show that our method outperforms baselines on the benchmark datasets.",
  "full_text": "Active Learning on Pre-trained Language Model\nwith Task-Independent Triplet Loss\nSeungmin Seo, Donghyun Kim, Youbin Ahn, and Kyong-Ho Lee\nDepartment of Computer Science, Yonsei University, Seoul, Republic of Korea\n{smseo91, dhkim92, ybahn, khlee89}@yonsei.ac.kr\nAbstract\nActive learning attempts to maximize a task model’s perfor-\nmance gain by obtaining a set of informative samples from an\nunlabeled data pool. Previous active learning methods usually\nrely on speciﬁc network architectures or task-dependent sam-\nple acquisition algorithms. Moreover, when selecting a batch\nsample, previous works suffer from insufﬁcient diversity of\nbatch samples because they only consider the informative-\nness of each sample. This paper proposes a task-independent\nbatch acquisition method using triplet loss to distinguish hard\nsamples in an unlabeled data pool with similar features but\ndifﬁcult to identify labels. To assess the effectiveness of the\nproposed method, we compare the proposed method with\nstate-of-the-art active learning methods on two tasks, relation\nextraction and sentence classiﬁcation. Experimental results\nshow that our method outperforms baselines on the bench-\nmark datasets.\nIntroduction\nDeep neural networks have shown unprecedented break-\nthroughs in various research areas. Particularly in the ﬁeld\nof natural language processing (NLP), pre-trained language\nmodels such as GPT (Radford et al. 2018) and BERT (De-\nvlin et al. 2019) achieve high performance in many NLP\ntasks (Conneau and Lample 2019; Vu, Phung, and Haffari\n2020; Wang et al. 2019). Although a pre-trained language\nmodel is learned with massive corpora, it still needs to ob-\ntain sufﬁcient supervised data for a target task. To address\nthis low-resource problem, active learning has gradually at-\ntracted the attention of researchers.\nWhile unsupervised and semi-supervised learning fully\nutilize the unlabeled samples, active learning aims to select\na few unlabeled samples to be labeled for efﬁcient train-\ning. The key challenge of active learning is to ﬁnd the most\ninformative unlabeled samples that maximize the task per-\nformance when labeled and used for training. Some recent\nworks rely on feature representation derived from speciﬁc\nnetwork architectures such as Bayesian Neural Networks\n(Gal, Islam, and Ghahramani 2017; Tran et al. 2019; Kirsch,\nVan Amersfoort, and Gal 2019), or use task-dependent\nalgorithms to ﬁnd informative samples (Ostapuk, Yang,\nand Cudr´e-Mauroux 2019; Wang, Chiticariu, and Li 2017).\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nHowever, dependence on the ﬁxed feature representation\nof task classiﬁers may cause divergent issues (Wang et al.\n2016). Also, it is unreliable for choosing the most informa-\ntive sample based on the classiﬁer’s unreliable response. In\nsummary, it is challenging to apply existing approaches to\ndifferent tasks because of their task dependency.\nAnother challenge is that typical active learning strate-\ngies acquire and query the informative samples one by one,\nwhich is difﬁcult to be used in real-world applications (Zh-\ndanov 2019). To improve one-by-one sample acquisition,\nmany researchers present the batch acquisition strategies\n(Kirsch, Van Amersfoort, and Gal 2019; Zhdanov 2019; Ash\net al. 2019; Gal and Ghahramani 2016). A simple approach\nselects a batch sample based on the continuous one-by-one\nquery (Gal and Ghahramani 2016). However, this approach\ntends to take redundant informative samples. Some works\nconsider the mutual information inherent in batch samples\nto exclude redundant samples (Kirsch, Van Amersfoort, and\nGal 2019; Ash et al. 2019; Zhdanov 2019; Yuan, Lin, and\nBoyd-Graber 2020), but those methods still suffer from in-\nsufﬁcient diversity of batch samples because they do not\nfully capture the data distribution.\nThis paper proposes a task-independent batch acquisi-\ntion algorithm on a pre-trained language model with triplet\nloss (BATL). Previous approaches usually require a certain\namount of labeled data at the early stage of active learning to\nguarantee the ability to choose informative samples. To over-\ncome the limitation, our model utilizes the self-supervision\nof a pre-trained language model to ﬁnd informative samples\nin the early sampling iterations.\nOn the other hand, if unlabeled data is sampled only with\nthe self-supervision of a sentence, it can be overlooked that\ndifferent unlabeled samples have different importance for\nthe task model depending on a type of downstream task. In-\nstead of relying solely on the self-supervision of a language\nmodel, the proposed approach chooses informative samples\nusing both the pre-trained knowledge of the language model\nand the task-related feature extracted from task classiﬁers.\nMoreover, our method acquires diverse batch samples\nwith respect to data distribution. Speciﬁcally, the proposed\nmethod utilizes triplet loss to distinguish hard samples in\nthe unlabeled data pool that have similar features to each\nother (Schroff, Kalenichenko, and Philbin 2015; Hermans,\nBeyer, and Leibe 2017; Zeng et al. 2020). Triplet loss effec-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11276\nTask Classifier\nUnlabeled \nDa\ntaset Task-related Feature\nBatch Acquisition with Triplet Loss\n(cons\nidering both the amount and diversity of information)\nLabeled \nDa\ntaset\nPre-trained \nLanguage Model\nPre-trained Knowledge\nInitialization\nTraining ModelUp date dataset\n+\nConcatenation\nOracle\nFigure 1: The workﬂow of the proposed method. In the batch acquisition phase, the shape of points indicates the label of data\nsample, and the size of points indicates the amount of information in the data sample. The distance between points represents\nthe similarity between them.\ntively increases the distance between different samples and\ndecreases the distance between similar samples in high di-\nmensional space.\nFigure 1 illustrates the workﬂow of the proposed method.\nThe training model consists of a pre-trained language model\nand task classiﬁer, and the classiﬁer is initialized on the la-\nbeled dataset. The proposed model utilizes the pre-trained\nsentence representation of the language model and task-\nrelated features to measure the informativeness of sam-\nples. Our method selects informative samples (black colored\npoints in the ﬁgure) using target task loss and triplet loss by\nupdating the model and data pools, considering the quantity\nand diversity of information.\nWe evaluate our method on two target tasks, relation ex-\ntraction and sentence classiﬁcation with various datasets.\nExperimental results demonstrate that our method consis-\ntently outperforms the state-of-the-art active learning meth-\nods under different task settings. Furthermore, we conduct\nexperiments to analyze the effectiveness of the proposed\nmethod in capturing the uncertainty, diversity, and density\nof batch samples selected from the unlabeled data pool.\nRelated Work\nRecent active learning methods can be grouped into three\ncategories: uncertainty-based, distribution-based, and hybrid\nmethods combining uncertainty and distribution of query\nsamples. Uncertainty-based methods estimate the uncertain-\nties of samples and acquire the top-K most informative sam-\nples. BALD (Houlsby et al. 2011) and BatchBALD (Kirsch,\nVan Amersfoort, and Gal 2019) measure the mutual in-\nformation between model parameters and the model pre-\ndictions. Gal, Islam, and Ghahramani (2017) present the\nMonte Carlo dropout methods to combine Bayesian neural\nnetworks with BALD. In non-bayesian methods, Yoo and\nKweon (2019) uses a loss prediction module to predict the\nloss of the task module, and then top-K predicted losses\nare selected as uncertain samples. Although the uncertainty-\nbased methods perform well on various tasks, they cannot\nfully reﬂect data diversity. Also, the sampling performance\ntends to decrease as the number of target labels increases.\nDistribution-based methods choose uncertain samples\nbased on the distribution of extracted model features. Chitta\net al. (2019) use an ensemble active learning to build a train-\ning subset based on data distribution. He et al. (2019) em-\nploy multiple views from hidden layers of CNN and mea-\nsure the uncertainty. The Core-set approach constructs a core\nsubset representing the remaining unlabeled data samples\n(Sener and Savarese 2018). However, the pipeline of the tar-\nget task cannot be considered in distribution-based methods.\nTo combine uncertainty and diversity, BADGE exploits\ngradient embedding and K-means++ seeding algorithm (Ash\net al. 2019). However, BADGE is signiﬁcantly dependent on\nthe conﬁdence scores of task models, which do not imply the\ninformativeness of samples. V AAL learns the uncertainty\nand distribution of data using V AE and adversarial networks\n(Sinha, Ebrahimi, and Darrell 2019), but those V AAL-based\nmethods require V AE frameworks (Sinha, Ebrahimi, and\nDarrell 2019; Kim et al. 2020; Zhang et al. 2020). ALPS\nutilizes self-supervised loss derived from a pre-trained lan-\nguage model to classify uncertain samples (Yuan, Lin, and\nBoyd-Graber 2020). Since ALPS employs the masked lan-\nguage model loss generated by a small percentage of ran-\ndomly masked tokens, ALPS only captures atypical sen-\ntences which are challenging for a pre-trained language\nmodel to understand. Moreover, although ALPS applies k-\nmeans clustering to get the diversity of batch samples, ALPS\nsuffers from distinguishing outliers since it only extracts un-\ncertain samples close to the cluster’s center.\nPreliminaries\nPre-trained Language Model Encoder\nThe pre-trained language models are trained on millions\nor billions of unsupervised data to capture generic lin-\nguistic features. They use language modeling objectives\nand demonstrate strong performance in various downstream\ntasks. Their input is a sequence of word tokens, x =\n(x1;x2;···;xl) with sequence length l. The pre-trained\nlanguage model encoder computes sentence representation\n11277\ns(x;\u0012e) using the hidden state representations with weight\nparameters \u0012e.\nTask Classiﬁer\nA pre-trained language model is ﬁne-tuned on the down-\nstream task. The task classiﬁer predicts the label with a sen-\ntence representation sobtained by the pre-trained language\nmodel. The distribution over target labels is deﬁned as fol-\nlows:\nf(x;\u0012c) =softmax(hC ·WC + bC); (1)\nwhere given the sequence x and weight parameter \u0012c =\n(W;b), f(x;\u0012c) is a probability vector of scores assigned\nto candidate labels, and hC is the hidden representation of\nclassiﬁer’s ﬁnal layer. Then, the predicted label ^yis deﬁned\nas follows:\n^y= argmaxy∈Yf(x;\u0012c)y: (2)\nDuring training, we want to minimize the target task loss\nLtarget between target label y and predicted label ^y. When\ninitializing the classiﬁer with the labeled dataset, there is no\nproblem in obtaining the exact target loss because we know\nboth the target label yand predicted label ^y. However, since\nwe do not have the target labelyfor samples in the unlabeled\ndataset, we only use the loss of predicted label as the target\nloss during the active learning phase.\nMethod\nActive Learning Scenario\nWe ﬁrst describe the general active learning scenario. The\nkey to active learning is to ﬁnd the most informative un-\nlabeled samples that improve the model performance when\nlabeled and added to the labeled data pool for training. We\ndenote the unlabeled data pool by Du and the labeled pool\nby Dl. Initially, the pre-trained language model encoder and\ntask classiﬁer are trained on the initial labeled dataset Dl\n0. A\nbatch of informative samples is selected at each iteration us-\ning an acquisition function, and unlabeled and labeled data\npools are updated. The model is then updated with labeled\ndata until the labeling budget is exhausted or a more prin-\ncipled criterion is met, such as in (Bloodgood and Vijay-\nShanker 2009).\nBatch Acquisition with Triplet Loss (BATL)\nWe introduce a task-independent batch acquisition method\nwith triplet loss that (1) considers both pre-trained linguis-\ntic features and task-related features and (2) explores uncer-\ntainty and diversity in the unlabeled dataset.\nOur acquisition function takes a sentence representation\nand a task-related feature as inputs. A sentence representa-\ntion s(x;\u0012e) is learned from the pre-trained language model,\nwhile the task-related feature is extracted from the task clas-\nsiﬁer. Speciﬁcally, our method utilizes the hidden state rep-\nresentation hC of the classiﬁer’s ﬁnal layer as a task-related\nfeature. Our method concatenates sentence representation\nand task-related features and puts them in a fully connected\nAlgorithm 1: Active learning with BATL\nInput: unlabeled dataset Du, labeled dataset Dl, initial task\nclassiﬁer F with pre-trained language model encoder E,\nbatch size k, iteration number T\nInitialize: train an initial model F and Eon Dl\n1: while Ti in T do\n2: For all samples xin Du:\n3: compute sentence representation s(x;\u0012e\ni ) from E\n4: compute task-related feature f(x;\u0012c\ni ) from F\n5: concatenate s(x;\u0012e\ni ) and f(x;\u0012c\ni )\n6: compute target loss Ltarget and triplet loss Ltriplet\n7: select ksamples in order of high ﬁnal loss Lfinal and\nquery for labels\n8: receive newly labeled data Dnew\n9: ﬁne-tune parameters of Eand F using Dnew\n10: Dl ←D l ∪Dnew , Du ←D u \\Dnew\n11: end while\nlayer to obtain an integrated data sample feature. The task\nclassiﬁer is not trained on enough labeled datasets at the\nearly stage in active learning. Thus, a target loss Ltarget is\nyet an unreliable indicator to measure the amount of infor-\nmation of samples. Moreover, if we directly use the cross-\nentropy loss with the predicted label ^y inferred by the task\nclassiﬁer, it will mislead the optimization of the task model.\nThere are some previous studies to optimize the task\nmodel without ground-truth labels. Yoo and Kweon (2019)\nuse the pairwise loss between two losses predicted by the\nloss prediction module. The pairwise loss is usually helpful\nfor maximizing the distance between different samples in a\nhigh-dimensional space. However, the pairwise loss faces a\nsigniﬁcant shortcoming when minimizing the distance be-\ntween similar samples. In other words, the diversity of batch\nsamples is not considered well. A simple clustering algo-\nrithm may be applied to catch diversity of batch samples\n(Ash et al. 2019; Yuan, Lin, and Boyd-Graber 2020). Still,\nit is not easy to process outliers closed to several clusters si-\nmultaneously because those methods merely extract samples\nclose to the center of clusters.\nTo overcome the above challenges, the proposed method\nuses the triplet loss (Schroff, Kalenichenko, and Philbin\n2015) to ﬁnd informative batch samples elaborately, taking\nthe sample diversity into account. The triplet consists of an\nanchor, positive and negative samples. An anchor sample of\na speciﬁc label is closer to the positive sample than the neg-\native sample in the embedding space. A positive sample has\nthe same label as the anchor, while a negative sample has a\ndifferent label. The triplet constraint is deﬁned as follows:\nD(xi\na;xi\np) +m<D (xi\na;xi\nn) (3)\nwhere xi\na is the anchor sample, xj\np is the positive sample\nwhich has the same label as xi\na, and xj\nn is the negative sam-\nple which has different label from xi\na. Since we do not have\nthe label information for candidate samples in the unlabeled\ndataset, we use predicted label of the task classiﬁer. All sam-\n11278\nDataset T\narget Task Train Test # Labels\nNYT-10\nRelation Extraction 522,611 172,448 53\nWiki-KBP Relation Extraction 23,884 289 13\nAG News Sentence Classiﬁcation 110,000 7,600 4\nPubMed Sentence Classiﬁcation 180,040 30,135 5\nTable 1: Dataset used in the experiments.\nples here are represented in the embedding space.D(·) is the\nEuclidian distance, and mis the margin.\nSince generating all possible triplets would not improve\nthe quality of batch samples in active learning, it is crucial\nto select useful triplets to train the model. In other words,\nwe need to ﬁnd a triplet that violates the triplet constraint.\nSuch a hard sample consists of a hard positive sample satis-\nfying that argmaxxi\np D(xi\na;xi\np), and a hard negative sample\nsatisfying that argminxi\nn D(xi\na;xi\nn). The proposed method\ncomputes the hard samples within a mini-batch on the ﬂy to\nmine these useful triplets. The proposed method uses each\nsample in the batch as an anchor. It selects the hard pos-\nitive sample that is the most distant to the anchor and the\nsemi-hard negative sample since the hard negative sample\ncan result in local optima with the online triplet generation.\nThe semi-hard negative sample xi\nn is such that:\nD(xi\na;xi\np) <D(xi\na;xi\nn) (4)\nSemi-hard negative samples are far away from the anchor\nthan the positive samples, but lie inside the margin m\nThe triplet loss function is deﬁned as follows:\nLtriplet =\nkX\ni=1\nm+ maxD(xi\na;xi\np) −min D(xi\na;xi\nn);\n(5)\nwhere N is the triplet batch size.\nBy combining the target task loss and triplet loss for active\nlearning, our ﬁnal loss function is as follows:\nLfinal = Ltarget + \u0015·Ltriplet; (6)\nwhere \u0015is a scaling parameter. The overall active learning\nprocess is described in Algorithm 1.\nExperiments\nExperiment Setup\nTarget task: Our approach is not restricted to speciﬁc task\nmodels. We evaluated our approach on two tasks. The ﬁrst\ntask is relation extraction, which aims to ﬁnd a relational fact\nbetween an entity pair in the sentence. Current methods usu-\nally depend on the distantly supervised data containing noisy\nsentences that do not represent the relational fact between\nan entity pair, making relation extraction one of the most\nchallenging NLP tasks. We also evaluated different methods\non sentence classiﬁcation, which aims to ﬁnd the label of a\ngiven sentence.\nDatasets: For relation extraction, we used two publicly ac-\ncessible dataset, NYT-10 (Riedel, Yao, and McCallum 2010)\nand Wiki-KBP (Ellis et al. 2013). The NYT-10 dataset in-\ncludes the Freebase relations extracted from the New York\nTimes corpus. We used the preprocessed NYT-10 dataset\nintroduced in Lin et al. (2016). The Wiki-KBP consists of\n23,884 training sentences sampled from Wikipedia articles.\nWe use the preprocessed Wiki-KBP dataset introduced in\nRen et al. (2017). We used the Precision@N, which mea-\nsures precision scores for the top N extracted relation in-\nstances. Since the test data was generated via distant super-\nvision, we provide an approximate performance measure.\nFor sentence classiﬁcation, we used two benchmark\ndatasets, AG News (Zhang, Zhao, and LeCun 2015) and\nPubMed (Dernoncourt and Lee 2017). AG News contains\nnews sentences of 4 class labels. PubMed is constructed\nfrom the medical abstracts and has 5 class labels. We eval-\nuated the classiﬁcation accuracy with the micro-F1 score.\nTable 1 summarizes the datasets used in the experiments.\nTraining model: For relation extraction, we utilized the\nrelation extraction model DISTRE proposed in Alt, H¨ubner,\nand Hennig (2019). DISTRE utilizes GPT (Radford et al.\n2018) as a pre-trained language model encoder and the rela-\ntion classiﬁer. The input sequence is an ordered sequence to\navoid task-speciﬁc changes to the architecture. It starts with\nthe head and tail entity, separated by delimiters, followed by\nthe sentence containing the entity pair. For sentence classi-\nﬁcation, we followed the same setup as in Yuan, Lin, and\nBoyd-Graber (2020), in which BERT and SCIBERT were\nused as a pre-trained language model for the AG News and\nPubMed, respectively.\nBaselines: We compared the proposed method with the\nfollowing sample acquisition methods.\n• RAND (random sampling): selects random samples\n• CONF (least conﬁdence sampling): selects least conﬁ-\ndent samples (Wang and Shang 2014)\n• ENTROPY : selects samples with highest Shannon en-\ntropy (Wang and Shang 2014)\n• D-AL: selects samples making the labeled set indistin-\nguishable from the unlabeled pool (Gissin and Shalev-\nShwartz 2019)\n• BatchBALD: selects samples based on mutual informa-\ntion between model parameters and predictions (Kirsch,\nVan Amersfoort, and Gal 2019)\n• CORESET : selects samples using core subset selection\nwith a greedy furthest-ﬁrst traversal on labeled samples\n(Sener and Savarese 2018)\n• BADGE: selects samples based on the gradient loss of\nclassiﬁer and k-means++ clustering (Ash et al. 2019)\n• ALPS: selects samples based on masked language model\nloss of pre-trained language model and k-means cluster-\ning (Yuan, Lin, and Boyd-Graber 2020)\nImplementation details: We ﬁne-tuned the pre-trained\nlanguage model and task classiﬁer from scratch in a given\niteration. For each experiment, we repeated it ﬁve times\n11279\nFigure 2: Active learning results of relation extraction over the NYT-10 dataset for varying batch sizek= {500;2000}.\nP@1000 F1\nRAND 0.360\n0.248\nCONF 0.466 0.289\nENTROPY 0.480 0.320\nD-AL 0.504 0.322\nBatchBALD 0.518 0.337\nCORESET 0.499 0.324\nBADGE 0.520 0.345\nALPS 0.524 0.334\nBATL 0.535 0.352\nFULL 0.602 0.376\nTable 2: P@1000 and F1 score (NYT-10,k= 500).\nwith random initialization. We evaluated sampling strate-\ngies on the relation extraction with varying batch size K =\n{500;2000}for NYT-10, and K = {50;200}for Wiki-\nKBP. We set the batch size K = 100 for sentence classi-\nﬁcation. The learning rate is 2e−5, and scaling parameter\n\u0015 = 1. The experiments are performed on GeForce RTX\n2080 Ti and AMD Ryzen 7 3700X CPUs.\nResults\nRelation Extraction We ﬁrst investigate the effectiveness\nof the proposed batch sample acquisition method (BATL) on\nthe relation extraction task. Figure 2 reports the experimen-\ntal results of relation extraction over the NYT-10 dataset. As\nwe expected, traditional sampling methods are outperformed\nby state-of-the-art methods. Random sampling records the\nlowest precision at every experimental setting, and it clearly\nshows that the appropriate sampling method is required in\nactive learning.\nThe proposed method consistently shows the best per-\nformance at every iteration of the active learning process\namong state-of-the-art methods. At k = 500, the train-\ning model updated by the proposed method has about a\n3% higher precision score than recent methods after the\nfour iterations. We observe that hybrid approaches such as\nBADGE and ALPS slightly perform better than uncertainty-\nbased and distribution-based methods. Also, it is impres-\nsive that there is not much difference in performance be-\ntween uncertainty-based and distribution-based methods. It\nmeans that the informativeness of samples relying solely on\nuncertainty or diversity is lower than those selected by the\nhybrid sampling method, including the proposed method.\nCompared to the proposed method, ALPS chooses samples\nrelying on the self-supervision of the pre-trained language\nmodel, and it is likely to ignore the unlabeled samples con-\ntaining crucial features for the task model at the later learn-\ning iterations. Although BADGE tries to ﬁnd task-related\nuncertain samples using gradient loss, a simple clustering al-\ngorithm limited the diversity of batch samples. We veriﬁed\nthat the proposed method acquires more diverse batch sam-\nples with different relation labels than the other baselines.\nFrom the experimental results, it can be seen that the pa-\nrameters of the training model converge when a sufﬁcient\namount of batch samples is secured. The relation labels\nof the NYT-10 dataset are relatively large and imbalanced.\nSince most of the sentences are labeled with NA, it reduces\nlearning efﬁciency and makes convergence difﬁcult with a\nsmall amount of data. At the convergence, state-of-the-art\nmethods approach the precision score of the fully trained\nmodel, while traditional methods do not. Table 2 shows the\nP@1000 and F1-score on the NYT-10 dataset withk= 500.\nWe further compared the performance of batch acqui-\nsition methods on the Wiki-KBP dataset. The Wiki-KBP\ndataset has lower relation labels than the NYT-10 dataset\nbut has more diverse entity labels and sentences. We can\nobserve that all methods record lower precision scores on\nthe Wiki-KBP dataset than on the NYT-10 dataset. Mean-\nwhile, the proposed method still shows the best performance\n11280\nFigure 3: Active learning results of relation extraction over the Wiki-KBP dataset for varying batch sizek= {50;200}.\nFigure 4: Active learning results of sentence classiﬁcation\nover AG News and PubMed for batch size k= 100.\nat every iteration of the active learning process. Compared\nto the experimental results on the NYT-10 dataset, the train-\ning model updated by the proposed method on the Wiki-\nKBP dataset shows more performance gain than the other\nmodels. At convergence, the proposed method shows about\n4% higher precision scores than state-of-the-art methods.\nSince the Wiki-KBP dataset has fewer relation labels than\nthe NYT-10 dataset, the unlabeled data samples in the Wiki-\nKBP dataset are likely to have similar features. We con-\nﬁrmed that the proposed method is valid for distinguishing\nsuch hard samples in the unlabeled data from the experimen-\ntal results.\nSentence Classiﬁcation We now investigate the effective-\nness of the proposed batch sample acquisition method on the\nsentence classiﬁcation task. Figure 4 shows the performance\nFigure 5: Evaluation on diversity, uncertainty, and density.\nof different active learning methods over two-sentence clas-\nsiﬁcation datasets, AG NEWS and PubMed. Figure 4(a)\nshows that the proposed method has almost the same ac-\ncuracy as BADGE and ALPS in the experiment on the AG\nNews dataset. The results demonstrate no signiﬁcant differ-\nence in performance among state-of-the-art sampling strate-\ngies in the experiment on the AG News dataset. The AG\nNews dataset is relatively simple than the relation extrac-\ntion datasets, and the number of target labels is quite small.\nWe found that most sampling strategies show similar perfor-\nmance gains at convergence.\nOn the other hand, for the PubMed dataset, the proposed\nmethod shows the best test accuracy, which is about 1.5%\nhigher than BADGE, and 3% higher than CORESET and\nALPS, as shown in Figure 4(b). We reconﬁrmed that the pro-\nposed method ﬁnds informative samples better than state-\n11281\nFigure 6: Evaluation on the impact of scaling parameter \u0015.\nof-the-art baselines. Since the PubMed dataset is more im-\nbalanced than the AG News, traditional sampling methods\nshow poor performance than state-of-the-art methods. Af-\nter several iterations of the active learning process, random\nsampling shows the worst performance. The least conﬁdence\nsampling shows about 10% lower test accuracy than the pro-\nposed method at convergence.\nOverall, the experimental results of comparing sampling\nmethods on relation extraction and sentence classiﬁcation\ndemonstrate that as the task difﬁculty increases, the pro-\nposed method is more effective than other baselines.\nAnalysis\nUncertainty, diversity and density: We estimated active\nlearning strategies’ uncertainty, diversity, and density to ana-\nlyze their advantages and disadvantages. In this experiment,\neach method selects 5,000 samples at one iteration after ini-\ntializing with the same unlabeled dataset of PubMed.\nThe training model prefers a batch with high diversity to\navoid containing similar and redundant samples. To measure\ndiversity, we follow the deﬁnition of diversity introduced in\n(Zhdanov 2019):\nGdi =\n\u0012 1\n|Du|\nX\nxi∈Du\nmin\nxj∈S\nD(xi;xj)\n\u0013−1\n; (7)\nwhere xi is the feature representation of the data sample ob-\ntained from the encoder and classiﬁer, D() is the Euclidean\ndistance, and Sis the set of selected samples.\nFollowing (Yuan, Lin, and Boyd-Graber 2020), we com-\npute the uncertainty using the task classiﬁer f(x;\u0012c\n∗) trained\non the full training dataset. It assumes that the fully trained\ntask classiﬁer guarantees reliable inference performance. A\nselected sample is evaluated by entropy over labels inferred\nby f(x;\u0012c\n∗). Then, the average predictive entropy over se-\nlected batch samples is calculated as follo\nGun = 1\n|Dnew|\nX\nx∈Dnew\nNX\ni=1\nf(x;\u0012c\n∗)i ln(f(x;\u0012c\n∗)i)−1; (8)\nwhere N is the number of class labels.\nFor density, we use the KNN-density measure proposed\nin (Zhu et al. 2008). The sample density is evaluated by the\naverage distance between the query sample and kmost sim-\nilar samples. We use the Euclidean distance with k = 10,\nfollowing (Dor et al. 2020):\nGde =\nP\nzi∈Z cos(x;zi)\nK ; (9)\nwhere Z = {z1;z2;:::;z k}are most similar samples of the\nsample x.\nFigure 5 shows that BATL considerably outperforms other\nmethods in terms of diversity. It indicates that the triplet loss\nbetter reﬂects the data distribution than the naive clustering\nalgorithm adopted for BADGE and ALPS. We can see that\nCORESET, a distribution-based method, also shows a simi-\nlar diversity score to other state-of-the-art methods. For un-\ncertainty, BADGE shows slightly better performance than\nALPS. It implies that although the self-supervision of ALPS\nmay provide sufﬁcient information at early iterations of the\nactive learning process, we still need to consider the task-\nrelated context to catch the uncertainty of samples. The pro-\nposed method also shows the highest score in density. The\nresult shows that the optimization goal of our method is valid\nfor minimizing the distance between similar samples.\nImpact of scaling parameter: We further investigated the\nimpact of the scaling parameter of the proposed batch ac-\nquisition loss. We recorded the values of diversity, uncer-\ntainty, and density with batch size k = 2000 and varying\nscaling parameter \u0015 = 0:5;1;2 during ten active learning\niterations (Figure 6). In this experiment, we updated the\ntraining model from the previous iteration. As the training\nprogresses, the diversity and density slightly decrease while\nthe uncertainty signiﬁcantly decreases. It is notable that the\nproposed method consistently selects diverse batch samples\neven after several training iterations. However, the proposed\nmethod shows the difﬁculty of elaborately capturing the un-\ncertainty at later active learning iterations. As the value of \u0015\nincreases, batch samples tend to be more diverse and denser\nbut less uncertain.\nConclusion and Future Work\nIn this paper, we proposed a task-independent active learn-\ning method applied to various NLP tasks. The proposed\nmethod ﬁnds informative batch samples using a pre-trained\nlanguage model and task-related features extracted from a\ntask classiﬁer. We adopt triplet loss to distinguish hard sam-\nples in an unlabeled data pool that have similar features but\nare difﬁcult to be used to identify labels. We demonstrated\nthe effectiveness of our method on two downstream tasks,\nrelation extraction and sentence classiﬁcation. We conﬁrmed\nthe validity of the proposed approach through comparative\nexperiments and analysis. In the future, we plan to study a\nmethod for efﬁciently catching the diversity and density of\nimbalanced data with many labels, since current active learn-\ning approaches have difﬁculty understanding imbalanced\ndata,\n11282\nAcknowledgements\nThis research was supported by the National Research Foun-\ndation of Korea (NRF) grant funded by the Korea govern-\nment (MSIP; Ministry of Science, ICT & Future Planning)\n(No. NRF-2019R1A2B5B01070555). Kyong-Ho Lee is the\ncorresponding author.\nReferences\nAlt, C.; H¨ubner, M.; and Hennig, L. 2019. Fine-tuning Pre-\nTrained Transformer Language Models to Distantly Super-\nvised Relation Extraction. InProceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n1388–1398.\nAsh, J. T.; Zhang, C.; Krishnamurthy, A.; Langford, J.; and\nAgarwal, A. 2019. Deep Batch Active Learning by Diverse,\nUncertain Gradient Lower Bounds. In International Confer-\nence on Learning Representations.\nBloodgood, M.; and Vijay-Shanker, K. 2009. A Method for\nStopping Active Learning Based on Stabilizing Predictions\nand the Need for User-Adjustable Stopping. In Proceed-\nings of the Thirteenth Conference on Computational Natural\nLanguage Learning, 39–47.\nChitta, K.; Alvarez, J. M.; Haussmann, E.; and Farabet, C.\n2019. Training Data Distribution Search with Ensemble Ac-\ntive Learning. arXiv preprint arXiv:1905.12737.\nConneau, A.; and Lample, G. 2019. Cross-lingual language\nmodel pretraining. In Advances in Neural Information Pro-\ncessing Systems, 7059–7069.\nDernoncourt, F.; and Lee, J. Y . 2017. PubMed 200k RCT:\na Dataset for Sequential Sentence Classiﬁcation in Medi-\ncal Abstracts. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing (Volume\n2: Short Papers), 308–313.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nDor, L. E.; Halfon, A.; Gera, A.; Shnarch, E.; Dankin, L.;\nChoshen, L.; Danilevsky, M.; Aharonov, R.; Katz, Y .; and\nSlonim, N. 2020. Active Learning for BERT: An Empirical\nStudy. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, 7949–7962.\nEllis, J.; Li, X.; Grifﬁtt, K.; Strassel, S.; and Wright, J. 2013.\nLinguistic resources for 2013 knowledge base population\nevaluations. In Proceedings of the 2013 Text Analysis Con-\nferenc. NIST.\nGal, Y .; and Ghahramani, Z. 2016. Dropout as a bayesian ap-\nproximation: Representing model uncertainty in deep learn-\ning. In international conference on machine learning, 1050–\n1059. PMLR.\nGal, Y .; Islam, R.; and Ghahramani, Z. 2017. Deep Bayesian\nActive Learning with Image Data. In International Confer-\nence on Machine Learning, 1183–1192.\nGissin, D.; and Shalev-Shwartz, S. 2019. Discriminative ac-\ntive learning. arXiv preprint arXiv:1907.06347.\nHe, T.; Jin, X.; Ding, G.; Yi, L.; and Yan, C. 2019. Towards\nBetter Uncertainty Sampling: Active Learning with Multi-\nple Views for Deep Convolutional Neural Network. In2019\nIEEE International Conference on Multimedia and Expo ,\n1360–1365. IEEE.\nHermans, A.; Beyer, L.; and Leibe, B. 2017. In defense of\nthe triplet loss for person re-identiﬁcation. arXiv preprint\narXiv:1703.07737.\nHoulsby, N.; Husz ´ar, F.; Ghahramani, Z.; and Lengyel, M.\n2011. Bayesian active learning for classiﬁcation and prefer-\nence learning. arXiv preprint arXiv:1112.5745.\nKim, K.; Park, D.; Kim, K. I.; and Chun, S. Y . 2020.\nTask-Aware Variational Adversarial Active Learning.arXiv\npreprint arXiv:2002.04709.\nKirsch, A.; Van Amersfoort, J.; and Gal, Y . 2019. Batchbald:\nEfﬁcient and diverse batch acquisition for deep bayesian ac-\ntive learning. In Advances in neural information processing\nsystems, 7026–7037.\nLin, Y .; Shen, S.; Liu, Z.; Luan, H.; and Sun, M. 2016. Neu-\nral relation extraction with selective attention over instances.\nIn Proceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n2124–2133.\nOstapuk, N.; Yang, J.; and Cudr ´e-Mauroux, P. 2019. Ac-\ntivelink: deep active learning for link prediction in knowl-\nedge graphs. In The World Wide Web Conference, 1398–\n1408.\nRadford, A.; Narasimhan, K.; Salimans, T.; and\nSutskever, I. 2018. Improving language understand-\ning by generative pre-training. https://s3-us-west-\n2.amazonaws.com/openai-assets/research-covers/language-\nunsupervised/language\nunderstanding paper.pdf. Accessed:\n2022-03-17.\nRen, X.; Wu, Z.; He, W.; Qu, M.; V oss, C. R.; Ji, H.; Ab-\ndelzaher, T. F.; and Han, J. 2017. Cotype: Joint extraction\nof typed entities and relations with knowledge bases. In\nProceedings of the 26th International Conference on World\nWide Web, 1015–1024.\nRiedel, S.; Yao, L.; and McCallum, A. 2010. Modeling rela-\ntions and their mentions without labeled text. In Joint Euro-\npean Conference on Machine Learning and Knowledge Dis-\ncovery in Databases, 148–163. Springer.\nSchroff, F.; Kalenichenko, D.; and Philbin, J. 2015. Facenet:\nA uniﬁed embedding for face recognition and clustering. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 815–823.\nSener, O.; and Savarese, S. 2018. Active Learning for Con-\nvolutional Neural Networks: A Core-Set Approach. In In-\nternational Conference on Learning Representations.\nSinha, S.; Ebrahimi, S.; and Darrell, T. 2019. Variational\nadversarial active learning. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, 5972–5981.\nTran, T.; Do, T.-T.; Reid, I.; and Carneiro, G. 2019. Bayesian\nGenerative Active Deep Learning. In International Confer-\nence on Machine Learning, 6295–6304.\n11283\nVu, T.; Phung, D.; and Haffari, G. 2020. Effective Unsuper-\nvised Domain Adaptation with Adversarially Trained Lan-\nguage Models. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing, 6163–\n6173.\nWang, C.; Chiticariu, L.; and Li, Y . 2017. Active learning\nfor black-box semantic role labeling with neural factors. In\nProceedings of the 26th International Joint Conference on\nArtiﬁcial Intelligence, 2908–2914.\nWang, D.; and Shang, Y . 2014. A new active labeling\nmethod for deep learning. In 2014 International joint con-\nference on neural networks (IJCNN), 112–119. IEEE.\nWang, H.; Tan, M.; Yu, M.; Chang, S.; Wang, D.; Xu, K.;\nGuo, X.; and Potdar, S. 2019. Extracting Multiple-Relations\nin One-Pass with Pre-Trained Transformers. In Proceedings\nof the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 1371–1377.\nWang, K.; Zhang, D.; Li, Y .; Zhang, R.; and Lin, L. 2016.\nCost-effective active learning for deep image classiﬁcation.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 27(12): 2591–2600.\nYoo, D.; and Kweon, I. S. 2019. Learning loss for active\nlearning. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 93–102.\nYuan, M.; Lin, H.-T.; and Boyd-Graber, J. 2020. Cold-start\nActive Learning through Self-Supervised Language Model-\ning. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, 7935–7948.\nZeng, K.; Ning, M.; Wang, Y .; and Guo, Y . 2020. Hierarchi-\ncal Clustering With Hard-Batch Triplet Loss for Person Re-\nIdentiﬁcation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 13657–13665.\nZhang, B.; Li, L.; Yang, S.; Wang, S.; Zha, Z.-J.; and Huang,\nQ. 2020. State-Relabeling Adversarial Active Learning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 8756–8765.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nconvolutional networks for text classiﬁcation. Advances in\nneural information processing systems, 28: 649–657.\nZhdanov, F. 2019. Diverse mini-batch Active Learning.\narXiv preprint arXiv:1901.05954.\nZhu, J.; Wang, H.; Yao, T.; and Tsou, B. K. 2008. Active\nlearning with sampling by uncertainty and density for word\nsense disambiguation and text classiﬁcation. In Proceedings\nof the 22nd International Conference on Computational Lin-\nguistics, 1137–1144.\n11284",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7601410150527954
    },
    {
      "name": "Task (project management)",
      "score": 0.7270233035087585
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6844050884246826
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6426810622215271
    },
    {
      "name": "Sample (material)",
      "score": 0.6091774702072144
    },
    {
      "name": "Machine learning",
      "score": 0.579039454460144
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5532655715942383
    },
    {
      "name": "Active learning (machine learning)",
      "score": 0.49668365716934204
    },
    {
      "name": "Sentence",
      "score": 0.49157842993736267
    },
    {
      "name": "Multi-task learning",
      "score": 0.4854314923286438
    },
    {
      "name": "Natural language processing",
      "score": 0.33662253618240356
    },
    {
      "name": "Engineering",
      "score": 0.06603547930717468
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    }
  ]
}