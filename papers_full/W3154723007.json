{
  "title": "TransCrowd: weakly-supervised crowd counting with transformers",
  "url": "https://openalex.org/W3154723007",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2932060598",
      "name": "Liang, Dingkang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2377617746",
      "name": "Chen Xi-wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1612438654",
      "name": "Xu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2037128165",
      "name": "Zhou Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166164994",
      "name": "Bai Xiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953106684",
    "https://openalex.org/W3130620372",
    "https://openalex.org/W3158818292",
    "https://openalex.org/W3097305524",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2463631526",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3119214860",
    "https://openalex.org/W2740959624",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2961566087",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035193053",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2964648978",
    "https://openalex.org/W3112776202",
    "https://openalex.org/W3140576409",
    "https://openalex.org/W3184564979",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2969835258",
    "https://openalex.org/W3101165561",
    "https://openalex.org/W2970646403",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3138486308",
    "https://openalex.org/W2788040570",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2945574898",
    "https://openalex.org/W2950477723",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W2921080178",
    "https://openalex.org/W3087861058",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3129750683",
    "https://openalex.org/W2953456714",
    "https://openalex.org/W3112728669",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3214586131"
  ],
  "abstract": "The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods.",
  "full_text": "TransCrowd: weakly-supervised crowd counting with transformers\nDingkang Liang1, Xiwu Chen1, Wei Xu2, Yu Zhou1, Xiang Bai1\n1Huazhong University of Science and Technology\n2Beijing University of Posts and Telecommunications\nAbstract\nThe mainstream crowd counting methods usually utilize\nthe convolution neural network (CNN) to regress a density\nmap, requiring point-level annotations. However, annotat-\ning each person with a point is an expensive and labori-\nous process. During the testing phase, the point-level an-\nnotations are not considered to evaluate the counting ac-\ncuracy, which means the point-level annotations are redun-\ndant. Hence, it is desirable to develop weakly-supervised\ncounting methods that just rely on count-level annotations,\na more economical way of labeling. Current weakly-\nsupervised counting methods adopt the CNN to regress a\ntotal count of the crowd by an image-to-count paradigm.\nHowever, having limited receptive Ô¨Åelds for context mod-\neling is an intrinsic limitation of these weakly-supervised\nCNN-based methods. These methods thus can not achieve\nsatisfactory performance, with limited applications in the\nreal-word. The Transformer is a popular sequence-to-\nsequence prediction model in NLP , which contains a global\nreceptive Ô¨Åeld. In this paper, we propose TransCrowd,\nwhich reformulates the weakly-supervised crowd counting\nproblem from the perspective of sequence-to-count based\non Transformer. We observe that the proposed TransCrowd\ncan eÔ¨Äectively extract the semantic crowd information by\nusing the self-attention mechanism of Transformer. To the\nbest of our knowledge, this is the Ô¨Årst work to adopt a\npure Transformer for crowd counting research. Exper-\niments on Ô¨Åve benchmark datasets demonstrate that the\nproposed TransCrowd achieves superior performance com-\npared with all the weakly-supervised CNN-based count-\ning methods and gains highly competitive counting perfor-\nmance compared with some popular fully-supervised count-\ning methods. An implementation of our method is available\nat https://github.com/dk-liang/TransCrowd .\n1. Introduction\nCrowd counting is a hot topic in the computer vision\ncommunity, which plays an essential role in video surveil-\nlance, public safety, and crowd analysis. Typical crowd\nCNNRegressor\nCNN FeatureExtractor\n(a) Fully-supervised density map CNN-based method\n(b) Image-to-count weakly-supervised CNN-based method\nPrediction Count\nGT Count\nLoss\nLoss\nPrediction density mapGT density map\nTransCrowd\n(c) Sequence-to-count weakly-supervised Transformer-based method\nPrediction CountGT CountLoss\n‚Ä¶\nFully-connected Layer\nSequence\nFigure 1: (a) Traditional fully-supervised CNN-based\nmethod. All the training images are labeled with point-level\nannotations. (b) Weakly-supervised CNN-based method\nfrom image-to-count perspective, only relying on the an-\nnotated total count of the crowd. (c) The proposed Tran-\nsCrowd, a weakly-supervised method, reformulates the\ncounting problem from the sequence-to-count perspective.\ncounting methods [66, 21, 58, 33, 33] usually utilize the\nconvolution neural network (CNN) to regress a density map,\nwhich has achieved signiÔ¨Åcant progress recently. A stan-\ndard regressor consists of an encoder and decoder: the en-\ncoder extracts the high-level feature information, and the\ndecoder is designed for pixel-level regression based on the\nextracted feature.\nHowever, these density-map regression-based meth-\nods [66, 21, 58, 2] still have some drawbacks. 1) They\napply the point-level annotations to generate ground truth\ndensity maps, which are usually expensive cost. Actu-\nally, some methods [62, 11, 40, 20] discover that we can\ncollect a new crowd dataset by using a more economical\nstrategy, such as mobile crowd-sensing [11] technology or\nGPS-less [40] energy-e Ô¨Écient sensing scheduling. For a\ngiven crowd scene with di Ô¨Äerent viewpoints and the total\ncount keeps the same (such as auditoria, classroom), if we\nknow the total count of one viewpoint, then the total count\nof other viewpoints is known. Besides, we can obtain the\narXiv:2104.09116v3  [cs.CV]  8 Sep 2022\ncrowd number at a glance for some sparse crowd scenes.\n(2) The annotated point label will not be taken to evaluating\nthe counting performance, meaning the point label is redun-\ndant to some extent. Thus, point-level annotations are not\nabsolutely necessary for the crowd counting task.\nBased on the above observations, it is desirable to de-\nvelop the count-level crowd counting method. Following\nprevious works [20, 62], we call the methods which rely on\nthe point-level annotations are fully-supervised paradigm,\nand the methods which only rely on count-level are weakly-\nsupervised paradigm. The fully-supervised methods Ô¨Årst\nutilize the point annotation to generate the ground truth den-\nsity map and then elaborately design a regressor to gener-\nate a prediction density map and Ô¨Ånally apply the L2 loss\nto measure the di Ô¨Äerence between the prediction and the\nground truth, as shown in Fig. 1(a). The existing weakly-\nsupervised methods usually regress the total count of crowd\nimage directly, which is from the image-to-count perspec-\ntive, as shown in Fig. 1(b).\nRecently, Transformer [48], a popular language model\nproposed by Google, has been explored in many vision\ntasks, such as classiÔ¨Åcation [19], detection [4, 68], and\nsegmentation [67]. Unlike the CNN, which utilizes a\nlimited receptive Ô¨Åeld, the Transformer [48] provides the\nglobal receptive Ô¨Åeld, showing excellent advantages over\npure CNN architectures. In this paper, we propose Tran-\nsCrowd, which is the Ô¨Årst to explore the Transformer into\nthe weakly-supervised crowd counting task, establishing the\nperspective of sequence-to-count prediction, as illustrated\nin Fig. 1(c).\nOnly a few methods are proposed with the considerations\nof reducing the annotations burden (e.g., semi, weakly-\nsupervised). L2R [30] facilitates the counting task by rank-\ning the image patch. Wang et al. [56] introduce a synthetic\ncrowd dataset named GCC, and the model is pre-trained on\nthe GCC dataset and then Ô¨Åne-tuned on real data. One of\nthe most relevant works for our method is [62], which pro-\nposes a soft-label network to facilitate the counting task, di-\nrectly regressing the crowd number without the supervision\nof location labels. However, [62] is a CNN-based method,\nwhich has a limited respective Ô¨Åeld. The Transformer has\na global receptive Ô¨Åeld, which eÔ¨Äectively solves the limited\nrespective Ô¨Åeld problem of CNN-based methods once and\nfor all. It means that the Transformer architecture is more\nsuitable for the weakly-supervised counting task since the\ntask aims to directly predict a total count from the whole\nimage and rely on the global perspective.\nIn this paper, we introduce two types of TransCrowd,\nnamed TransCrowd-Token and TransCrowd-GAP, respec-\ntively. TransCrowd-Token utilizes an extra learnable to-\nken to represent the count. TransCrowd-GAP adopts the\nglobal average pooling (GAP) over all items in the output\nsequence of Transformer-encoder to obtain the pooled vi-\nsual tokens. The regression tokens or pooled visual tokens\nare then fed into the regression head to generate the predic-\ntion count. We empirically Ô¨Ånd that the TransCrowd-GAP\ncan obtain more reasonable attention weight, achieve higher\ncount accuracy, and present fast-converging compared with\nTransCrowd-Token.\nIn summary, this work contributes to the following:\n1. TransCrowd is the Ô¨Årst pure transformer-based crowd\ncounting framework. We reformulate the counting\nproblem from a sequence-to-count perspective and\npropose a weakly-supervised counting method, which\nonly utilizes the count-level annotations without the\npoint-level information in the training phase.\n2. We provide two di Ô¨Äerent types of TransCrowd,\nnamed TransCrowd-Token and TransCrowd-GAP, re-\nspectively. We observe that the TransCrowd-GAP can\ngenerate a more reasonable attention weight and re-\nports faster converging and higher counting perfor-\nmance than TransCrowd-Token.\n3. Extensive experiments demonstrate that the proposed\nmethod achieves state-of-the-art counting performance\ncompared with the weakly-supervised methods. Addi-\ntionally, our method has a highly competitive count-\ning performance compared with the fully-supervised\ncounting methods.\n2. Related Works\n2.1. CNN-based crowd counting.\nThe CNN-based crowd counting methods can be cat-\negorized into localization-based methods and regression-\nbased methods. The localization-based methods [38, 28]\nusually learn to predict bounding boxes for each human,\nrelying on box-level annotations. Recently, some meth-\nods [1, 32, 22, 57, 7] try to utilize the pseudo bounding\nboxes based on point-level annotations or design a suit-\nable map to realize counting and localization tasks. How-\never, these localization-based methods usually report unsat-\nisfactory counting performance. The mainstream of crowd\ncounting is the density map CNN-based crowd counting\nmethods [66, 21, 63, 8, 34, 16, 59], whose integral of the\ndensity map gives the total count of a crowd image. Due to\nthe commonly heavy occlusion that existed in crowd im-\nages, multi-scale architecture is developed. SpeciÔ¨Åcally,\nMCNN [66] utilizes multi-size Ô¨Ålters to extract di Ô¨Äerent\nscale feature information. Sam et al. [46] capture the\nmulti-scale information by the proposed contextual pyramid\nCNN. TEDNet [15] assembles multiple encoding-decoding\npaths hierarchically to generate a high-quality density map\nfor accurate crowd counting. Method in [35] proposes a\nscale-aware probabilistic model to handle large scale vari-\nations through DPN, and each level of DPN copes with\na given scale range. Using the perspective information\nto diminish the scale variations is e Ô¨Äective [41, 10, 61].\nPACNN [41] proposes a novel generating ground truth per-\nspective maps strategy and predicts both the perspective\nmaps and density maps at the testing phase. Yanget al. [61]\npropose a reverse perspective network to estimate the per-\nspective factor of the input image and then warp the im-\nage. Appropriate measure matching can help to improve\nthe counting performance. S3 [23] propose a novel mea-\nsure matching based on Sinkhorn divergence, avoiding gen-\nerating the density maps. UOT [36] use unbalanced opti-\nmal transport (UOT) distance to quantify the discrepancy\nbetween two measures, outputting sharper density maps.\nThe Attention-based mechanism is another useful tech-\nnique adopted by many methods [27, 16, 63]. ADCrowd-\nNet [27] generates an attention map for the crowd images\nvia a network called Attention Map Generate (AMG). Jiang\net al. [16] propose a density attention network to generate\nattention masks concerning regions of diÔ¨Äerent density lev-\nels. Zhang et al. [63] propose a Relation Attention Network\n(RANet) that utilizes local self-attention and global self-\nattention to capture long-range dependencies. It is notewor-\nthy that RANet [63] is actually a non-local /self-attention\nmechanism based on CNN instead of pure Transformers,\nand we utilize a pure Transformer without convolution lay-\ners.\n2.2. Weakly-supervised crowd counting.\nOnly a few methods focus on counting with a lack of\nlabeled data. L2R [30] proposes a learning-to-rank frame-\nwork based on an extra collected crowd dataset. Wang et\nal. [56] introduce a synthetic crowd scene for the pre-trained\nmodel. However, these two methods still rely on point-level\nannotations, which are fully-supervised instead of weakly-\nsupervised paradigms.\nThe traditional method [5] relies on hand-crafted fea-\ntures, such as GLCM and edge orientations, which are\nturned to be sub-optimal for this weakly-supervised count-\ning task. MATT [20] learns a model from a small amount\nof point-level annotations (fully-supervised) and a large\namount of count-level annotations (weakly-supervised).\nThe method in [49] proposes a weakly-supervised solution\nbased on the Gaussian process for crowd density estimation.\nShang et al. simultaneous predicted the global count and lo-\ncal count. Wang et al. directly regress the global count, and\nsome negative samples are fed into the network to boost the\nrobustness. Similarly, Yang et al. [62] also directly maps\nthe images to the crowd numbers without the location su-\npervision based on the proposed soft-label sorting network.\nHowever, the counting performance of these count-level\nweakly-supervised counting methods still does not achieve\ncomparable results to the fully-supervised counting meth-\nods, existing massive degradation, limiting the applica-\ntion of weakly-supervised methods in real-world. Di Ô¨Äer-\nent from the previous works, the proposed TransCrowd uti-\nlizes the Transformer architecture to directly regress the\ncrowd number, which formulates the counting problem as\nthe sequence-to-count paradigm and achieves comparable\ncounting performance compared with the popular fully-\nsupervised methods.\n2.3. Visual Transformer.\nThe Transformers [48], dominating the natural language\nmodeling [17, 31], utilize the self-attention mechanism to\ncapture the global dependencies between input and output.\nRecently, many works [4, 19, 67, 54, 6] attempt to apply the\nTransformer into the vision task. SpeciÔ¨Åcally, DETR [4]\nÔ¨Årstly utilizes a CNN backbone to extract the visual fea-\ntures, followed by the Transformer blocks for the box re-\ngression and classiÔ¨Åcation. ViT [19] is the Ô¨Årst which di-\nrectly applies Transformer-encoder [48] to sequences of im-\nages patch to realize classiÔ¨Åcation task. SETR [67] regards\nsemantic segmentation from a sequence-to-sequence per-\nspective with Transformers. IPT [6] develops a pre-trained\nmodel for image processing (low-level task) using the trans-\nformer architecture.\nTo the best of our knowledge, we are the Ô¨Årst to explore\nthe pure Transformer [48] to the counting task.\n3. Our Method\nThe overview of our method consists of the sequence (to-\nkens) of the image, a Transformer-encoder, and a naive re-\ngression head, as shown in Fig. 2(a). SpeciÔ¨Åcally, the in-\nput image is Ô¨Årst transformed into Ô¨Åxed-size patches and\nthen Ô¨Çatten to a sequence of vectors. The sequence is feed\ninto the Transformer-encoder, followed by a naive regres-\nsion head to generate the prediction count.\n3.1. Image to sequence\nIn general, the Transformer adopts a 1 D sequence of\nfeature embeddings Z ‚àà RN√óD as input, where N is the\nlength of the sequence and the D means the input chan-\nnel size. Thus, the Ô¨Årst step of TransCrowd is to trans-\nform the input image I into a sequence of 2D Ô¨Çattened\npatches. SpeciÔ¨Åcally, given an RGB image 1 I ‚ààRH√óW√ó3\n, we reshape the I into a grid of N patches, resulting in\n{xi\np ‚ààRK2¬∑3|i = 1, ...,N}, where N = H\nK √óW\nK and K is the\npre-deÔ¨Åned patch size.\n3.2. Patch Embedding\nNext, we need to map the x into a latent D-dimensional\nembedding feature by a learnable projection, since the\n1H, W, 3 indicate the spatial height, width and channel number, respec-\ntively.\nFlatten and Projection\nImage\n TransformerEncoder\nRegression Head\nPredictionCount\nGTCount\nLoss\n‚Ä¶3‚àóùêæ!\nùêªùëäùêæ!\nRegression token\n‚Ä¶3‚àóùêæ!\nùêªùëäùêæ!\nRegression Head\nPredictionCountGlobal-Average Pooling\n‚Ä¶3‚àóùêæ!\nùêªùëäùêæ!\n‚Ä¶\nRegression Head\nPredictionCount\n(a) The pipeline of  TransCrowd\n‚Ä¶\n(c) Global average pooling (GAP) over all items in the sequence\n3‚àóùêæ!\nùêªùëäùêæ!+1\nùêæ\nùêª\nùëä\n(b) Using an extratoken to represent count  \nFigure 2: (a) The pipeline of TransCrowd. The input image is split into Ô¨Åxed-size patches, each of which is linearly em-\nbedded with position embeddings. Then, the feature embedding sequence is fed into the Transformer-encoder, followed by\na regression head to generate the prediction count. (b) We utilize an extra token to represent the crowd count, similar to\nthe class token in Bert [17] and ViT [19]. (c) A global average pooling is adopted to pool the output visual tokens of the\nTransformer-encoder.\nMLPLayer NormÔºã ÔºãLayer NormMulti-Head Attention\nInput Output\nFigure 3: A standard Transformer layer consists of Multi-\nHead Attention and MLP blocks. Meanwhile, the layer nor-\nmalization (LN) and residual connections are employed.\ntransformer uses constant latent vector size D through all\nof its layers, deÔ¨Åned as\ne = [e1; e2 ¬∑¬∑¬∑ ; eN ] = [x1\npE; x2\npE; ¬∑¬∑¬∑ ; xN\np E], E ‚ààR(K2¬∑3)√óD,\n(1)\nwhere E is a learnable matrix, and e ‚ààRN√óD is the mapped\nfeatures. Thus, we add a speciÔ¨Åc position embedding {pi ‚àà\nRD|i = 1, ...,N}into the e, maintaining position information,\ndeÔ¨Åned as:\nZ0 = [Z1\n0 ; Z2\n0 ; ¬∑¬∑¬∑ ; ZN\n0 ] = [e1+p1; e2+p2; ¬∑¬∑¬∑ ; eN +pN ], (2)\nwhere Z0 is the input of the Ô¨Årst transformer layer.\n3.3. Transformer-encoder\nWe only adopt the Transformer encoder [48], without the\ndecoder, similar to ViT [19]. SpeciÔ¨Åcally, the encoder con-\ntains L layers of Multi-head self-attention (MS A) and Mul-\ntilayer Perceptron ( MLP) blocks. For each layer l, layer\nnormalization (LN) and residual connections are employed.\nA stand transformer layer is shown in Fig. 3, and the output\ncan be written as follows:\nZ‚Ä≤\nl = MS A(LN(Zl‚àí1)) + Zl‚àí1, (3)\nZl = MLP(LN(Z‚Ä≤\nl )) + Z‚Ä≤\nl , (4)\nwhere Zl is the output of layer l. Here, the MLP contains\ntwo linear layers with a GELU [12] activation function. In\nparticular, the Ô¨Årst linear layer of MLP expands the fea-\nture embedding‚Äô dimension fromD to 4D, while the second\nlayer shrinks the dimension from 4D to D.\nMS A is an extension with m independent self-\nattention ( S A) modules: MS A(Zl‚àí1) = [S A1(Zl‚àí1);\nS A2(Zl‚àí1); ¬∑¬∑¬∑ ; S Am(Zl‚àí1)]WO, where WO ‚ààRD√óD is a re-\nprojection matrix. At each independent S A, the input con-\nsists of query (Q), key (K), and value (V), which are com-\nputed from Zl‚àí1:\nQ = Zl‚àí1WQ, K = Zl‚àí1WK , V = Zl‚àí1WV , (5)\nS A(Zl) = so f tmax( QKT\n‚àö\nD\n)V (6)\nwhere WQ/WK /WV ‚àà RD√óD\nm are three learnable matrices.\nThe so f tmax function is applied over each row of the in-\nput matrix and\n‚àö\nD provides appropriate normalization.\n3.4. The input of regression head\nWe introduce two di Ô¨Äerent inputs for the regression\nheads to evaluate the eÔ¨Äectiveness of TransCrowd. The goal\nof the regression head is to generate the prediction count in-\nstead of the density map. We brieÔ¨Çy describe the two types\nof input.\n(1) Regression Token. Similar to the class token in\nBert [17] and ViT [19], we prepend a learnable embedding\nnamed regression token to the input sequence Z0, as shown\nin Fig. 2(b). This architecture forces the self-attention to\nspread information between the patch tokens and the re-\ngression token, making the regression token contain overall\nsemantic crowd information. The regression head is im-\nplemented by MLP containing two linear layers. We re-\nfer to the TransCrowd with the extra regression token as\nTransCrowd-Token.\n(2) Global average pooling.We apply the global aver-\nage pooling (GAP) to shrink the sequence length, as shown\nin Fig. 2(c). Similar to TransCrowd-Token, two linear lay-\ners are used for the regression head. We refer to the Tran-\nsCrowd with global average pooling as TransCrowd-GAP.\nThe global average pooling can eÔ¨Äectively maintain the use-\nful semantic crowd information in patch tokens. We Ô¨Ånd\nthat using pooled visual tokens will generate richer discrim-\ninative semantic crowd patterns and achieve better counting\nperformance than using the extra regression token, the de-\ntailed discussion listed in Sec. 6.\nWe utilizeL1 loss to measure the diÔ¨Äerence between pre-\ndiction and ground truth:\nL1 = 1\nM\nM‚àë\ni=1\n|Pi ‚àíGi|, (7)\nwhere Pi and Gi are the prediction crowd number and the\ncorresponding ground truth of i-th image, respectively. M\nis the batch size of training images.\n4. Experiments\n4.1. Implementation details\nThe Transformer-encoder is similar to ViT [19], which\ncontains 12 transformer layers, and each MS A consists of\n12 S A. We utilize the Ô¨Åxed H and W, both of which are\nset as 384. We set K as 16, which means N is equal to\n576. We use Adam [18] to optimize our model, in which\nthe learning rate and weight decay are set to 1e-5 and 1e-4,\nrespectively. The weights pre-trained on ImageNet are used\nto initialize the Transformer-encoder. During training, the\nwidely adopted data augmentation strategies are utilized, in-\ncluding random horizontal Ô¨Çipping and grayscaling. Due to\nsome datasets having various resolution images, we resize\nall the images into the size of 1152 √ó768. Each resized im-\nage can be regarded as six independent sub-image, and the\nresolution of each sub-image is 384√ó384. We set the batch\nsize as 24 and use a V100 GPU for the experiments. Code\nand models will be released for the community to follow.\n4.2. Dataset\nNWPU-Crowd [55], a large-scale and challenging\ndataset, consists of 5,109 images, 2,133,375 instances an-\nnotated elaborately. To be speciÔ¨Åc, the images are randomly\nsplit into three parts, including training, validation, and test-\ning sets, which contain 3,109, 500, and 1,500 images, re-\nspectively.\nJHU-CROWD++ [44] contains 2,722 training images,\n500 validation images, and 1,600 testing images, collected\nfrom diverse scenarios. The total number of people in each\nimage ranges from 0 to 25,791.\nUCF-QNRF [14] contains 1,535 images captured from\nunconstrained crowd scenes with about one million annota-\ntions. It has a count range of 49 to 12,865, with an aver-\nage count of 815.4. SpeciÔ¨Åcally, the training set consists of\n1,201 images, and the testing set consists of 334 images.\nShanghaiTech [66] contains 1,198 crowd images with\n330,165 annotations. The images of the dataset are divided\ninto two parts: Part A and Part B. In particular, Part A con-\ntains 300 training images and 182 testing images, and Part\nB consists of 400 training images and 316 testing images.\nUCF CC 50 [13] is a small dataset for dense crowd\ncounting, which just contains 50 images with an average\nof 1,280 individuals per image. The images are captured\nin a diverse set of events, and the pedestrians count of each\nimage range between 94 and 4,543.\nWorldExpo‚Äô10[64] contains 1,132 surveillance videos\nfrom 108 cameras. The training set consists of 3,380 im-\nages captured from 103 diÔ¨Äerent scenes, and the testing set\ncontains 600 images from 5 scenes. There are 199,923 an-\nnotations labelled in the whole 3,980 images.\n4.3. Evaluation Metrics\nWe choose Mean Absolute Error (MAE) and Mean\nSquared Error (MSE) to evaluate the counting performance:\nMAE = 1\nN\nN‚àë\ni=1\n|Pi ‚àíGi|, MS E =\nÓµ™Óµ´‚àö\n1\nN\nN‚àë\ni=1\n|Pi ‚àíGi|2, (8)\nwhere N is the number of testing images, Pi and Gi are the\npredicted and ground truth count of the i-th image, respec-\ntively.\n5. Results\nWe conduct extensive experiments to demonstrate the\neÔ¨Äectiveness of the proposed weakly-supervised crowd\ncounting method on Ô¨Åve popular benchmarks. For\nMethod Year Training label UCF-QNRF Part A Part B\nLocation Crowd number MAE MSE MAE MSE MAE MSE\nMCNN [66] CVPR 16 ‚àö ‚àö 277.0 426.0 110.2 173.2 26.4 41.3\nCL [14] ECCV 18 ‚àö ‚àö 132.0 191.0 - - - -\nCSRNet [21] CVPR 18 ‚àö ‚àö - - 68.2 115.0 10.6 16.0\nL2R [30] TPAMI 19 ‚àö ‚àö 124.0 196.0 73.6 112.0 13.7 21.4\nCFF [42] ICCV 19 ‚àö ‚àö - - 65.2 109.4 7.2 12.2\nPGCNet [60] ICCV19 ‚àö ‚àö - - 57.0 86.0 8.8 13.7\nTEDnet [15] CVPR 19 ‚àö ‚àö 113.0 188.0 64.2 109.1 8.2 12.8\nBL [34] ICCV 19 ‚àö ‚àö 88.7 154.8 62.8 101.8 7.7 12.7\nASNet [16] CVPR 20 ‚àö ‚àö 91.5 159.7 57.7 90.1 - -\nLibraNet [25] ECCV20 ‚àö ‚àö 88.1 143.7 55.9 97.1 7.3 11.3\nNoisyCC [50] NeurIPS 20 ‚àö ‚àö 85.8 150.6 61.9 99.6 7.4 11.3\nDM-Count [50] NeurIPS 20 ‚àö ‚àö 85.6 148.3 59.7 95.7 7.4 11.8\nMethod in [35] MM 20 ‚àö ‚àö 84.7 147.2 58.1 91.7 6.5 10.1\nS3 [23] IJCAI 21 ‚àö ‚àö 80.6 139.8 57.0 96.0 6.3 10.6\nUOT [36] AAAI 21 ‚àö ‚àö 83.3 142.3 58.1 95.9 6.5 10.2\nMethod in [62]* ECCV 20 ‚àí ‚àö - - 104.6 145.2 12.3 21.2\nMATT [20]* PR 21 ‚àí ‚àö - - 80.1 129.4 11.7 17.5\nTransCrowd-Token (ours)* - ‚àí ‚àö 98.9 176.1 69.0 116.5 10.6 19.7\nTransCrowd-GAP (ours)* - ‚àí ‚àö 97.2 168.5 66.1 105.1 9.3 16.1\nTable 1: Quantitative comparison (in terms of MAE and MSE) of the proposed method and some popular methods on three\nwidely adopted benchmark datasets. * represents the weakly-supervised method.\nMethod Year Training label Val set\nLow Medium High Overall\nLocation Crowd number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [66] CVPR16 ‚àö ‚àö 90.6 202.9 125.3 259.5 494.9 856.0 160.6 377.7\nCMTL [45] A VSS17 ‚àö ‚àö 50.2 129.2 88.1 170.7 583.1 986.5 138.1 379.5\nDSSI-Net [26] ICCV19 ‚àö ‚àö 50.3 85.9 82.4 164.5 436.6 814.0 116.6 317.4\nCAN [29] CVPR19 ‚àö ‚àö 34.2 69.5 65.6 115.3 336.4 619.7 89.5 239.3\nSANet [3] ECCV18 ‚àö ‚àö 13.6 26.8 50.4 78.0 397.8 749.2 82.1 272.6\nCSRNet [21] CVPR18 ‚àö ‚àö 22.2 40.0 49.0 99.5 302.5 669.5 72.2 249.9\nCG-DRCN [44] PAMI20 ‚àö ‚àö 17.1 44.7 40.8 71.2 317.4 719.8 67.9 262.1\nMBTTBF [47] ICCV19 ‚àö ‚àö 23.3 48.5 53.2 119.9 294.5 674.5 73.8 256.8\nSFCN [56] CVPR19 ‚àö ‚àö 11.8 19.8 39.3 73.4 297.3 679.4 62.9 247.5\nBL [34] ICCV19 ‚àö ‚àö 6.9 10.3 39.7 85.2 279.8 620.4 59.3 229.2\nTransCrowd-Token (ours)*- - ‚àö 7.1 10.7 33.3 54.6 302.5 557.4 58.4 201.1\nTransCrowd-GAP (ours)* - - ‚àö 6.7 9.5 34.5 55.8 285.9 532.8 56.8 193.6\nTable 2: Quantitative results on the JHU-Crowd ++ (val set) dataset. ‚ÄùLow‚Äù, ‚ÄùMedium‚Äù and ‚ÄùHigh‚Äù respectively indicates\nthree categories based on di Ô¨Äerent ranges:[0,50], (50,500], and >500. * represents the weakly-supervised crowd counting\nmethods.\neach dataset, we divide the existing methods into fully-\nsupervised methods (based on point-level annotations) and\nweakly-supervised methods (based on count-level annota-\ntions).\nCompared with the weakly-supervised counting\nmethods. Our method achieves state-of-the-art counting\nperformance on all the conducted datasets, as listed in\nTab. 1 - Tab. 7. SpeciÔ¨Åcally, on ShanghaiTech part A, our\nTransCrowd-GAP improves 17.5% in MAE and 18.8% in\nMSE compared with MATT [20], improves 36.8% in MAE\nand 27.6% in MSE compared with [62]. On ShanghaiTech\npart B, TransCrod-GAP improves 20.5% in MAE and 8.0%\nin MSE compared with MATT [20], improves 24.4% in\nMAE and 24.1% in MSE compared with [62]. Besides,\nthe proposed TransCrowd-Token also achieves signiÔ¨Åcant\nimprovement compared with MATT [20] and [62] in terms\nof MAE and MSE, and only the proposed methods report\ncounting performance close to the fully-supervised meth-\nods. Note that MATT [20] still applies a small number of\nimages, which contain point-level annotations for training.\nCompared with the fully-supervised counting meth-\nods. Although it is unfair to compare the fully-supervised\nand weakly-supervised crowd counting methods, our\nmethod still achieves highly competitive performance on\nthe Ô¨Åve counting datasets, as shown in Tab. 1 - Tab. 7. An\nimpressive phenomenon is that the proposed method even\nMethod Year Training label Testing set\nLow Medium High Overall\nLocation Crowd number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [66] CVPR16 ‚àö ‚àö 97.1 192.3 121.4 191.3 618.6 1,166.7 188.9 483.4\nCMTL [45] A VSS17 ‚àö ‚àö 58.5 136.4 81.7 144.7 635.3 1,225.3 157.8 490.4\nDSSI-Net [26] ICCV19 ‚àö ‚àö 53.6 112.8 70.3 108.6 525.5 1,047.4 133.5 416.5\nCAN [29] CVPR19 ‚àö ‚àö 37.6 78.8 56.4 86.2 384.2 789.0 100.1 314.0\nSANet [3] ECCV18 ‚àö ‚àö 17.3 37.9 46.8 69.1 397.9 817.7 91.1 320.4\nCSRNet [21] CVPR18 ‚àö ‚àö 27.1 64.9 43.9 71.2 356.2 784.4 85.9 309.2\nCG-DRCN [44] PAMI20 ‚àö ‚àö 19.5 58.7 38.4 62.7 367.3 837.5 82.3 328.0\nMBTTBF [47] ICCV19 ‚àö ‚àö 19.2 58.8 41.6 66.0 352.2 760.4 81.8 299.1\nSFCN [56] CVPR19 ‚àö ‚àö 16.5 55.7 38.1 59.8 341.8 758.8 77.5 297.6\nBL [34] ICCV19 ‚àö ‚àö 10.1 32.7 34.2 54.5 352.0 768.7 75.0 299.9\nUOT [36] AAAI21 ‚àö ‚àö 11.2 26.2 28.7 45.3 274.1 648.2 60.5 252.7\nS3 [23] IJCAI21 ‚àö ‚àö - - - - - - 59.4 244.0\nTransCrowd-Token (ours)*- - ‚àö 8.5 23.2 33.3 71.5 368.3 816.4 76.4 319.8\nTransCrowd-GAP (ours)* - - ‚àö 7.6 16.7 34.8 73.6 354.8 752.8 74.9 295.6\nTable 3: Quantitative results on the JHU-Crowd++ (testing set) dataset. ‚ÄùLow‚Äù, ‚ÄùMedium‚Äù and ‚ÄùHigh‚Äù respectively indicates\nthree categories based on di Ô¨Äerent ranges:[0,50], (50,500], and >500. * represents the weakly-supervised crowd counting\nmethods.\nMethod Year Training label Val set Testing set\nOverall Overall Scene Level (only MAE)\nLocation Crowd number MAE MSE MAE MSE Avg. S 0 ‚àºS 4\nC3F-VGG [9] Tech19 ‚àö ‚àö 105.79 504.39 127.0 439.6 666.9 140.9 /26.5/58.0/307.1/2801.8\nCSRNet [21] CVPR18 ‚àö ‚àö 104.89 433.48 121.3 387.8 522.7 176.0 /35.8/59.8/285.8/2055.8\nPCC-Net-VGG [10] CVPR19 ‚àö ‚àö 100.77 573.19 112.3 457.0 777.6 103.9 /13.7/42.0/259.5/3469.1\nCAN [29] CVPR19 ‚àö ‚àö 93.58 489.90 106.3 386.5 612.2 82.6 /14.7/46.6/269.7/2647.0\nSFCN‚Ä† [56] CVPR19 ‚àö ‚àö 95.46 608.32 105.7 424.1 712.7 54.2 /14.8/44.4/249.6/3200.5\nBL [34] ICCV19 ‚àö ‚àö 93.64 470.38 105.4 454.2 750.5 66.5 /8.7/41.2/249.9/3386.4\nKDMG [52] PAMI20 ‚àö ‚àö - - 100.5 415.5 632.7 77.3 /10.3/38.5/259.4/2777.9\nNoisyCC [50] NeurIPS20 ‚àö ‚àö - - 96.9 534.2 608.1 218.7 /10.7/35.2/203.2/2572.8\nDM-Count [53] NeurIPS20 ‚àö ‚àö 70.5 357.6 88.4 388.6 498.0 146.6/7.6/31.2/228.7/2075.8\nS3 [23] IJCAI21 ‚àö ‚àö - - 87.8 387.5 566.5 80.7 / 7.9 / 36.3 / 212.0 / 2495.4\nUOT [36] AAAI21 ‚àö ‚àö - - 83.5 346.9 - -\nTransCrowd-Token (ours)* - ‚àí ‚àö 88.2 446.9 119.6 463.9 736.0 88.0/12.7/47.2/311.2/3216.1\nTransCrowd-GAP (ours)* - ‚àí ‚àö 88.4 400.5 117.7 451.0 737.8 69.3 /12.8/46.0/309.0/3252.2\nTable 4: Comparison of the counting performance on the NWPU-Crowd. S 0 ‚àºS 4 respectively indicate Ô¨Åve categories\naccording to the di Ô¨Äerent number range: 0, (0 , 100], (100, 500], (500, 5000], >5000. * represents the weakly-supervised\ncrowd counting methods.\nsurpasses some popular fully-supervised methods. For ex-\nample, as shown in Tab. 3, our TransCrowd-GAP brings\n11.0 MAE and 13.6 MSE improvement compared with\nCSRNet [21] on the JHU-Crowd ++ (testing set) dataset.\nBL [34], a recent strong counting method, achieves 75.0\nin MAE and 299.9 in MSE, one of the state-of-the-art\nmethods on the JHU-Crowd ++ (testing set) dataset, while\nour TransCrowd-GAP improves 0.1 MAE and 4.3 MSE,\nrespectively. Besides, from the results on UCF-QNRF,\nShanghaiTech, and NWPU-Crowd datasets, we can also\nobserve that our method achieve signiÔ¨Åcant improvement\ncompared to some popular fully-supervised methods (e.g.,\nMCNN [66], CSRNet [21], L2R [30]). We think the rea-\nsons why the proposed method outperforms some fully su-\npervised methods in the NWPU-Crowd and JHU-Crowd++\nmay be two-fold. First, the Transformer is beneÔ¨Åcial to cap-\nture the long-range dependence, and these two datasets con-\ntain many large-scale persons. The proposed TransCrowd\ncan eÔ¨Äectively learn the global crowd semantic feature rep-\nresentation. However, some state-of-the-art methods (e.g.,\nBL [34]) utilize a Ô¨Åxed Gaussian kernel for these datasets,\nand the Ô¨Åxed Gaussian kernel can not e Ô¨Äectively cover the\nlarge scale variations. Second, Dosovitskiy et al. [19] prove\nthat the CNNs outperform Transformer on small datasets\n(despite regularization optimization), but with the larger\ndatasets, Transformer overtakes. For instance, the NWPU-\nCrowd is a large dataset containing 5,190 images, which\nmay help the Transformer better Ô¨Åt the dataset. These im-\npressive results further demonstrate the eÔ¨Äectiveness of the\nproposed method and indicate point-level annotations are\nMethod Training label MAE MSELocation Crowd number\nMCNN [65] ‚àö ‚àö 377.6 509.1\nCSRNet [21] ‚àö ‚àö 266.1 397.5\nADCrowdNet [27] ‚àö ‚àö 257.1 363.5\nMATT [20]* ‚àí ‚àö 355.0 550.2\nTransCrowd-Token (ours)* ‚àí ‚àö 288.9 407.6\nTransCrowd-GAP (ours)* ‚àí ‚àö 272.2 395.3\nTable 5: The performance comparison on the UCF CC 50 dataset. * represents the weakly-supervised crowd counting\nmethods.\nMethod Training label MAE\nLocation Crowd number S1 S2 S3 S4 S5 Ave\nM-CNN [66] ‚àö ‚àö 3.4 20.6 12.9 13.0 8.1 11.6\nCP-CNN[46] ‚àö ‚àö 2.9 14.7 10.5 10.4 5.8 8.8\nLiu et al.[24] ‚àö ‚àö 2.0 13.1 8.9 17.4 4.8 9.2\nIC-CNN[37] ‚àö ‚àö 17.0 12.3 9.2 8.1 4.7 10.3\nCSR-Net[21] ‚àö ‚àö 2.9 11.5 8.6 16.6 3.4 8.6\nSA-Net[3] ‚àö ‚àö 2.6 13.2 9.0 13.3 3.0 8.2\nLSC-CNN[39] ‚àö ‚àö 2.9 11.3 9.4 12.3 4.3 8.0\nMATT[20]* ‚àí ‚àö 3.8 13.1 10.4 15.9 5.3 9.7\nTransCrowd-Token (ours)* ‚àí ‚àö 2.3 14.2 9.9 14.0 4.3 8.9\nTransCrowd-GAP (ours)* ‚àí ‚àö 2.1 13.3 8.9 13.8 4.4 8.5\nTable 6: Comparison results of di Ô¨Äerent methods on 5 scenes in the WorldExpo‚Äô10 dataset. * represents the weakly-\nsupervised crowd counting methods.\nMethod Training label MAE MSELocation Crowd number\nFCN-HA [65] ‚àö ‚àö 4.21 -\nCSRNet [21] ‚àö ‚àö 3.56 -\nADCrowdNet [27] ‚àö ‚àö 2.44 -\nTransCrowd-Token (ours)* ‚àí ‚àö 3.28 4.80\nTransCrowd-GAP (ours)* ‚àí ‚àö 3.23 4.66\nTable 7: The performance comparison on the Trancos\ndataset. * represents the weakly-supervised crowd count-\ning methods.\nMethod Resolution Parameters Backbone FPS\nCSRNET [21] 384 √ó384 16.2 M VGG16 21.67\nBL [34] 384 √ó384 21.6 M VGG19 45.66\nTransCrowd-Token 384√ó384 86.8 M Transformer 46.41\nTransCrowd-GAP 384√ó384 90.4 M Transformer 46.73\nTable 8: Comparison with BL [34] and CSRNET [21] using\nthe same input image resolution on a Titan XP.\nnot entirely necessary for the counting task.\n6. Analysis\n6.1. The input of regression head\nWe introduce two di Ô¨Äerent inputs for the regression\nhead. SpeciÔ¨Åcally, TransCrowd-Token utilizes an extra\nlearnable regression token to perform counting, similar to\nthe class token in Bert [17] and ViT [19]. TransCrowd-\nGAP utilizes global average pooling to obtain the pooled vi-\nsual tokens for count prediction. The result of TransCrowd-\nToken and TransCrowd-GAP are listed in Tab. 1 - Tab. 7.\nWe Ô¨Ånd that the results of TransCrowd-GAP are better\nTransCrowd-Token in all conducted datasets. For exam-\nple, TransCrowd-GAP outperforms TransCrowd-Token by\n2.8 MAE and 11.4 MSE on the ShanghaiTech Part A\ndataset, a signiÔ¨Åcant improvement. TransCrowd-GAP also\nhas steady improvement on ShanghaiTech part B, a sparse\ncrowd dataset. Based on the superior performance, we hope\nthe researchers can design a more reasonable regression\nhead based on the Transformer-encoder in the future.\n6.2. Visualizations\nTo further investigate the proposed TransCrowd, we pro-\nvide qualitative comparison results in Fig. 4 to understand\nwhat the Transformer attends to. We observe that both\nTransCrowd-Token and TransCrowd-GAP can successfully\nfocus on the crowd region, which demonstrates the ef-\nfectiveness of both methods. Moreover, the TransCrowd-\nGAP generates a more reasonable attention map compared\nwith the TransCrowd-Token. SpeciÔ¨Åcally, the TransCrowd-\nToken may pay more attention to the background, lead-\ning to amplifying the counting error. This observation ex-\nplains why the result of TransCrowd-GAP is better than\nTransCrowd-Token.\n6.3. Convergence curves\nWe further compare the convergence curves between the\npopular fully-supervised method (CSRNet [21]) and the\nproposed TransCrowd. Detailed convergence curves are\nGT  118 Pred90\nPred104GT  118\nGT  182 Pred174\nGT  182 Pred180\nGT  144 Pred157\nGT  144 Pred146\nGT  411 Pred445\nGT  411 Pred428\nImageAttention weightAttention mapImageAttention weightAttention map\nTransCrowd-Token\nTransCrowd-GAP\nTransCrowd-GAP\nTransCrowd-Token\nGT  48 Pred51\nGT  48 Pred49\nGT 8 Pred10\nGT  8 Pred8\nTransCrowd-GAP\nTransCrowd-Token\nFigure 4: Examples of attention maps from TransCrowd-Token and TransCrowd-GAP. TransCrowd-GAP generates more\nreasonable attention weights compared with TransCrowd-Token.\nMethod Year Training label None Pre-ImgNet Pre-GCC\nLocation Crowd number MAE MSE MAE MSE MAE MSE\nCSRNet [21] CVPR18 ‚àö ‚àö 120.0 179.4 68.2 115.0 67.4 112.3\nTransCrowd-Token (ours)* - ‚àí ‚àö 142.0 212.5 69.0 116.5 67.2 111.9\nTransCrowd-GAP (ours)* - ‚àí ‚àö 139.9 231.0 66.1 105.1 63.8 102.3\nTable 9: The Ô¨Åne-tuning CSRNet‚Äôs and TransCrowd-GAP‚Äôs results on ShanghaiTech part A dataset by using three diÔ¨Äerent\npre-trained strategies.\nshown in Fig. 5. Based on the convergence curves, we\ncan observe the following phenomena: (1) Compared with\nCSRNet, TransCrowd-GAP achieves better performance\nwith 1.9 √ófewer training epochs. (2) Using global average\npooled visual tokens can converge faster and achieve better\ncount accuracy than using the extra regression token. (3)\nMethod Year Training label Part B ‚ÜíPart A Part A‚ÜíPart B QNRF‚ÜíPart A QNRF‚ÜíPart B\nLocation Crowd number MAE MSE MAE MSE MAE MSE MAE MSE\nMCNN [66] CVPR16 ‚àö ‚àö 221.4 357.8 85.2 142.3 - - - -\nD-ConvNet [43] ECCV18 ‚àö ‚àö 140.4 226.1 49.1 99.2 - - - -\nRRSP[51] CVPR19 ‚àö ‚àö - - 40.0 68.5 - - - -\nBL [34] ICCV19 ‚àö ‚àö - - - - 69.8 123.8 15.3 26.5\nTransCrowd-GAP (ours)* - ‚àí ‚àö 141.3 258.9 18.9 31.1 78.7 122.5 13.5 21.9\nTable 10: Experimental results on the transferability of diÔ¨Äerent methods under cross-dataset evaluation.\n68.269.066.160\n120\n240\n480\n0 20 40 60 80 100120140160\nMAE\nEpochs\nCSRNetTransCrowd-TokenTransCrowd-GAP\nFigure 5: Convergence curves of CSRNet, TransCrowd-\nToken, and TransCrowd-GAP on ShanghaiTech part A\ndataset. The proposed TransCrowd-GAP achieves the best\ncounting performance and is fast-converging.\nBoth TransCrowd-Token and TransCrowd-GAP present a\nsmooth curve and fast converging, while the curve of CSR-\nNet is oscillating. These observations show the potential\nvalue of the Transformer in the counting task.\n6.4. Comparison of run-time.\nAs shown in Tab. 8, we compare with two popular fully\nsupervised counting methods, including BL [34] and CSR-\nNET [21]. The experiment is conducted on a Titan XP\nGPU. Even though both the proposed TransCrowd-Token\nand TransCrowd-GAP contain more parameters than other\nmethods, they still achieve outstanding run-time. This is be-\ncause the fully supervised methods need to maintain high-\nresolution features to generate high-quality density maps\n(e.g., 1/8 size of the input in CSRNET [21] and 1 /16 size\nof the input in BL [34]). Additionally, we can observe\nthat the FPS of VGG19-based BL [34] outperforms the\nVGG16-based CSRNet [21], mainly because the BL gen-\nerates a small-resolution density map (1/16 of the input im-\nage). This phenomenon further demonstrates the inÔ¨Çuence\nof feature resolution on run-time.\n6.5. Comparison of Different Pre-trained strategies.\nIn this section, we study the impact of the pre-trained\nmodel in TransCrowd. We choose the popular CNN-based\nmethod CSRNet [21] as a comparison, and the results are\nlisted in Tab. 9. SpeciÔ¨Åcally, there are three strategies: (1)\nNone: The models are directly trained on ShanghaiTech\npart A. (2) Pre-ImgNet: The models are pre-trained on the\nImageNet and Ô¨Åne-tune on ShanghaiTech part A. (3) Pre-\nGCC: The models are pre-trained on GCC [56], a synthetic\ndataset, and are Ô¨Åne-tuned on ShangahiTech part A dataset.\nFrom the Tab. 9, there are some interesting Ô¨Åndings: 1)\nWithout any pre-trained dataset, the CNN-based method\noutperforms the Transformer-based method. 2) Using the\nextra pre-trained data can e Ô¨Äectively prompt the perfor-\nmance, and the proposed TransCrowd-GAP achieves better\ncounting performance than CSRNet. 3) Besides, when the\nmodel pre-trained on the GCC dataset, the proposed method\ncan even outperform several recent fully-supervised meth-\nods (e.g., CFF [42], TEDNet [15]). Note that the GCC\ndataset is a synthetic crowd dataset, without any annotation\ncost, which means the TransCrowd-GAP can achieve sim-\nilar counting performance to the fully-supervised methods\nby using small count-level labeled real-data and extensive\nfree synthetic data, promoting the practical applications. It\nis noteworthy that the proposed method only uses count-\nlevel annotations of the GCC dataset, diÔ¨Äerent from the pre-\nvious fully-supervised work.\n6.6. Cross-dataset evaluation.\nFinally, we conduct cross-dataset experiments on the\nUCF-QNRF, ShanghaiTech Part A and Part B datasets to ex-\nplore the transferability of the proposed TransCrowd-GAP.\nIn the Cross-dataset evaluation, models are trained on the\nsource dataset and tested on the target dataset without fur-\nther Ô¨Åne-tuning. Quantitative results are shown in Tab. 10.\nAlthough our method is a weakly-supervised paradigm, we\nstill achieve highly competitive performance, which shows\nremarkable transferability.\n7. Conclusion\nIn this work, we present an alternative perspective for\nweakly-supervised crowd counting in images by introduc-\ning a sequence-to-count prediction framework based on\nTransformer-encoder, named TransCrowd. To the best of\nour knowledge, we are the Ô¨Årst to solve the counting prob-\nlem based on the Transformer. We analyze and show that\nthe attention mechanism is very promising to capture the\nsemantic crowd information. Extensive experiments on Ô¨Åve\nchallenging datasets demonstrate that TransCrowd achieves\nsuperior counting performance compared with the state-\nof-the-art weakly-supervised methods and achieves com-\npetitive performance compared with some popular fully-\nsupervised methods. In the future, we plan to make fully-\nsupervised counting using Transformer architecture and ex-\ntend it to video-based counting task.\nReferences\n[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras,\nand Chao Chen. Localization in the crowd with topo-\nlogical constraints. In Proc. of the AAAI Conf. on Ar-\ntiÔ¨Åcial Intelligence, 2021. 2\n[2] Shuai Bai, Zhiqun He, Yu Qiao, Hanzhe Hu, Wei Wu,\nand Junjie Yan. Adaptive dilated network with self-\ncorrection supervision for counting. In Proc. of IEEE\nIntl. Conf. on Computer Vision and Pattern Recogni-\ntion, 2020. 1\n[3] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei\nSu. Scale aggregation network for accurate and e Ô¨É-\ncient crowd counting. In Proc. of European Confer-\nence on Computer Vision, 2018. 6, 7, 8\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In Proc. of European Conference on Com-\nputer Vision, 2020. 2, 3\n[5] Antoni B Chan, Zhang-Sheng John Liang, and Nuno\nVasconcelos. Privacy preserving crowd monitoring:\nCounting people without people models or tracking.\nIn Proc. of IEEE Intl. Conf. on Computer Vision and\nPattern Recognition, 2008. 3\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu,\nYiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing\ntransformer. In Proc. of IEEE Intl. Conf. on Computer\nVision and Pattern Recognition, pages 12299‚Äì12310,\n2021. 3\n[7] Yajie Chen, Dingkang Liang, Xiang Bai, Yongchao\nXu, and Xin Yang. Cell localization and counting us-\ning direction Ô¨Åeld map. IEEE Journal of Biomedical\nand Health Informatics, 2021. 2\n[8] Dawei Du, Longyin Wen, Pengfei Zhu, Heng Fan,\nQinghua Hu, Haibin Ling, Mubarak Shah, Junwen\nPan, Ali Al-Ali, Amr Mohamed, et al. Visdrone-\ncc2020: The vision meets drone crowd counting chal-\nlenge results. In Proc. of European Conference on\nComputer Vision, pages 675‚Äì691. Springer, 2020. 2\n[9] Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu\nGao, and Jun Wen. CÀÜ 3 framework: An open-source\npytorch code for crowd counting. arXiv preprint\narXiv:1907.02724, 2019. 7\n[10] Junyu Gao, Qi Wang, and Xuelong Li. Pcc net: Per-\nspective crowd counting via spatial convolutional net-\nwork. IEEE Transactions on Circuits and Systems for\nVideo Technology, 2019. 3, 7\n[11] Bin Guo, Zhu Wang, Zhiwen Yu, Yu Wang, Neil Y\nYen, Runhe Huang, and Xingshe Zhou. Mobile crowd\nsensing and computing: The review of an emerging\nhuman-powered sensing paradigm. ACM computing\nsurveys (CSUR), 48(1):1‚Äì31, 2015. 1\n[12] Dan Hendrycks and Kevin Gimpel. Bridging nonlin-\nearities and stochastic regularizers with gaussian error\nlinear units. 2016. 4\n[13] Haroon Idrees, Imran Saleemi, Cody Seibert, and\nMubarak Shah. Multi-source multi-scale counting in\nextremely dense crowd images. In Proc. of IEEE Intl.\nConf. on Computer Vision and Pattern Recognition ,\n2013. 5\n[14] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey,\nDong Zhang, Somaya Al-Maadeed, Nasir Rajpoot,\nand Mubarak Shah. Composition loss for count-\ning, density map estimation and localization in dense\ncrowds. In Proc. of European Conference on Com-\nputer Vision, 2018. 5, 6\n[15] Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xi-\nantong Zhen, Xianbin Cao, David Doermann, and\nLing Shao. Crowd counting and density estimation\nby trellis encoder-decoder networks. In Proc. of IEEE\nIntl. Conf. on Computer Vision and Pattern Recogni-\ntion, 2019. 2, 6, 10\n[16] Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu\nZhang, Pei Lv, Bing Zhou, Xin Yang, and Yanwei\nPang. Attention scaling for crowd counting. In Proc.\nof IEEE Intl. Conf. on Computer Vision and Pattern\nRecognition, 2020. 2, 3, 6\n[17] Jacob Devlin Ming-Wei Chang Kenton and\nLee Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171‚Äì4186, 2019. 3, 4, 5, 8\n[18] DP Kingma and JL Ba. Adam: A method for stochas-\ntic optimization 3rd international conference on learn-\ning representations. In Proc. of International Confer-\nence on Learning Representations, 2015. 5\n[19] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk\nWeissenborn, Georg Heigold, Jakob Uszkoreit, Lucas\nBeyer, Matthias Minderer, Mostafa Dehghani, Neil\nHoulsby, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at\nscale. 2021. 2, 3, 4, 5, 7, 8\n[20] Yinjie Lei, Yan Liu, Pingping Zhang, and Lingqiao\nLiu. Towards using count-level weak supervision for\ncrowd counting. Pattern Recognition, 109:107616,\n2021. 1, 2, 3, 6, 8\n[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSR-\nNet: Dilated convolutional neural networks for under-\nstanding the highly congested scenes. InProc. of IEEE\nIntl. Conf. on Computer Vision and Pattern Recogni-\ntion, 2018. 1, 2, 6, 7, 8, 9, 10\n[22] Dingkang Liang, Wei Xu, Yingying Zhu, and Yu\nZhou. Focal inverse distance transform maps for\ncrowd localization and counting in dense crowd.arXiv\npreprint arXiv:2102.07925, 2021. 2\n[23] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei,\nYunfeng Qiu, Yaowei Wang, and Yihong Gong. Direct\nmeasure matching for crowd counting. InProceedings\nof the Thirtieth International Joint Conference on Ar-\ntiÔ¨Åcial Intelligence, 2021. 3, 6, 7\n[24] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexan-\nder G Hauptmann. Decidenet: counting varying den-\nsity crowds through attention guided detection and\ndensity estimation. In Proc. of IEEE Intl. Conf. on\nComputer Vision and Pattern Recognition, 2018. 8\n[25] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong,\nZhiguo Cao, and Chunhua Shen. Weighing counts:\nSequential crowd counting by reinforcement learning.\nIn Proc. of European Conference on Computer Vision,\npages 164‚Äì181. Springer, 2020. 6\n[26] Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu,\nWanli Ouyang, and Liang Lin. Crowd counting with\ndeep structured scale integration network. In Porc. of\nIEEE Intl. Conf. on Computer Vision, 2019. 6, 7\n[27] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu,\nLi Pan, and Hefeng Wu. Adcrowdnet: An attention-\ninjective deformable convolutional network for crowd\nunderstanding. In Proc. of IEEE Intl. Conf. on Com-\nputer Vision and Pattern Recognition, 2019. 3, 8\n[28] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Chris-\ntian Szegedy, Scott Reed, Cheng-Yang Fu, and\nAlexander C Berg. Ssd: Single shot multibox detector.\nIn Proc. of European Conference on Computer Vision,\n2016. 2\n[29] Weizhe Liu, Mathieu Salzmann, and Pascal Fua.\nContext-aware crowd counting. In Proc. of IEEE Intl.\nConf. on Computer Vision and Pattern Recognition ,\n2019. 6, 7\n[30] Xialei Liu, Joost Van De Weijer, and Andrew D Bag-\ndanov. Exploiting unlabeled data in cnns by self-\nsupervised learning to rank.IEEE transactions on pat-\ntern analysis and machine intelligence, 2019. 2, 3, 6,\n7\n[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019. 3\n[32] Yuting Liu, Miaojing Shi, Qijun Zhao, and Xiaofang\nWang. Point in, box out: Beyond counting persons\nin crowds. In Proc. of IEEE Intl. Conf. on Computer\nVision and Pattern Recognition, 2019. 2\n[33] Zhihao Liu, Zhijian He, Lujia Wang, Wenguan Wang,\nYixuan Yuan, Dingwen Zhang, Jinglin Zhang, Pengfei\nZhu, Luc Van Gool, Junwei Han, et al. Visdrone-\ncc2021: The vision meets drone crowd counting chal-\nlenge results. In Porc. of IEEE Intl. Conf. on Computer\nVision, pages 2830‚Äì2838, 2021. 1\n[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong\nGong. Bayesian loss for crowd count estimation with\npoint supervision. In Porc. of IEEE Intl. Conf. on\nComputer Vision, 2019. 2, 6, 7, 8, 10\n[35] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong\nGong. Learning scales from points: A scale-aware\nprobabilistic model for crowd counting. In Proc. of\nACM Multimedia, pages 220‚Äì228, 2020. 2, 6\n[36] Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin,\nYunfeng Qiu, and Yihong Gong. Learning to count\nvia unbalanced optimal transport. In Proc. of the AAAI\nConf. on ArtiÔ¨Åcial Intelligence , volume 35, pages\n2319‚Äì2327, 2021. 3, 6, 7\n[37] Viresh Ranjan, Hieu Le, and Minh Hoai. Iterative\ncrowd counting. In Proc. of European Conference on\nComputer Vision, 2018. 8\n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Proc. of Advances\nin Neural Information Processing Systems, 2015. 2\n[39] Deepak Babu Sam, Skand Vishwanath Peri, Mukun-\ntha Narayanan Sundararaman, Amogh Kamath, and\nVenkatesh Babu Radhakrishnan. Locate, size and\ncount: Accurately resolving people in dense crowds\nvia detection. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2020. 8\n[40] Xiang Sheng, Jian Tang, Xuejie Xiao, and Guoliang\nXue. Leveraging gps-less sensing scheduling for\ngreen mobile crowd sensing. IEEE Internet of Things\nJournal, 1(4):328‚Äì336, 2014. 1\n[41] Miaojing Shi, Zhaohui Yang, Chao Xu, and Qijun\nChen. Revisiting perspective information for e Ô¨Écient\ncrowd counting. In Proc. of IEEE Intl. Conf. on Com-\nputer Vision and Pattern Recognition, 2019. 3\n[42] Zenglin Shi, Pascal Mettes, and Cees GM Snoek.\nCounting with focus for free. In Porc. of IEEE Intl.\nConf. on Computer Vision , pages 4200‚Äì4209, 2019.\n6, 10\n[43] Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yang-\ndong Ye, Ming-Ming Cheng, and Guoyan Zheng.\nCrowd counting with deep negative correlation learn-\ning. In Proc. of IEEE Intl. Conf. on Computer Vision\nand Pattern Recognition, 2018. 10\n[44] Vishwanath Sindagi, Rajeev Yasarla, and Vishal MM\nPatel. Jhu-crowd ++: Large-scale crowd counting\ndataset and a benchmark method. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2020.\n5, 6, 7\n[45] Vishwanath A Sindagi and Vishal M Patel. Cnn-based\ncascaded multi-task learning of high-level prior and\ndensity estimation for crowd counting. In Proc. of\nIEEE Intl. Conf. on Advanced Video and Signal Based\nSurveillance, 2017. 6, 7\n[46] Vishwanath A Sindagi and Vishal M Patel. Generat-\ning high-quality crowd density maps using contextual\npyramid cnns. In Porc. of IEEE Intl. Conf. on Com-\nputer Vision, 2017. 2, 8\n[47] Vishwanath A Sindagi and Vishal M Patel. Multi-level\nbottom-top and top-bottom feature fusion for crowd\ncounting. In Porc. of IEEE Intl. Conf. on Computer\nVision, 2019. 6, 7\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn Proc. of Advances in Neural Information Process-\ning Systems, 2017. 2, 3, 4\n[49] Matthias von Borstel, Melih Kandemir, Philip\nSchmidt, Madhavi K Rao, Kumar Rajamani, and\nFred A Hamprecht. Gaussian process density counting\nfrom weak supervision. In Proc. of European Confer-\nence on Computer Vision , pages 365‚Äì380. Springer,\n2016. 3\n[50] Jia Wan and Antoni Chan. Modeling noisy annotations\nfor crowd counting. Advances in Neural Information\nProcessing Systems, 2020. 6, 7\n[51] Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan,\nand Wei Liu. Residual regression with semantic prior\nfor crowd counting. In Proc. of IEEE Intl. Conf. on\nComputer Vision and Pattern Recognition, 2019. 10\n[52] Jia Wan, Qingzhong Wang, and Antoni B Chan.\nKernel-based density map generation for dense object\ncounting. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2020. 7\n[53] Boyu Wang, Huidong Liu, Dimitris Samaras, and\nMinh Hoai. Distribution matching for crowd counting.\nIn Proc. of Advances in Neural Information Process-\ning Systems, 2020. 7\n[54] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang\nLi. Transformer meets tracker: Exploiting temporal\ncontext for robust visual tracking. In Proc. of IEEE\nIntl. Conf. on Computer Vision and Pattern Recogni-\ntion, pages 1571‚Äì1580, 2021. 3\n[55] Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li.\nNwpu-crowd: A large-scale benchmark for crowd\ncounting and localization. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 2020. 5\n[56] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learn-\ning from synthetic data for crowd counting in the wild.\nIn Proc. of IEEE Intl. Conf. on Computer Vision and\nPattern Recognition, 2019. 2, 3, 6, 7, 10\n[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song\nBai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka.\nAutoscale: Learning to scale for crowd counting. In-\nternational Journal of Computer Vision , pages 1‚Äì30,\n2022. 2\n[58] Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai,\nYongchao Xu, and Xiang Bai. Learn to scale: Gen-\nerating multipolar normalized density map for crowd\ncounting. In Porc. of IEEE Intl. Conf. on Computer\nVision, 2019. 1\n[59] Wei Xu, Dingkang Liang, Yixiao Zheng, Jiahao\nXie, and Zhanyu Ma. Dilated-scale-aware category-\nattention convnet for multi-class object counting.\nIEEE Signal Processing Letters, 28:1570‚Äì1574, 2021.\n2\n[60] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao\nTan, Yezhen Wang, Shilei Wen, and Errui Ding.\nPerspective-guided convolution networks for crowd\ncounting. In Porc. of IEEE Intl. Conf. on Computer\nVision, 2019. 6\n[61] Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming\nHuang, and Nicu Sebe. Reverse perspective net-\nwork for perspective-aware object counting. In Proc.\nof IEEE Intl. Conf. on Computer Vision and Pattern\nRecognition, 2020. 3\n[62] Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming\nHuang, and Nicu Sebe. Weakly-supervised crowd\ncounting learns from sorting rather than locations. In\nProc. of European Conference on Computer Vision ,\n2020. 1, 2, 3, 6\n[63] Anran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong\nZhen, Xianbin Cao, and Ling Shao. Attentional neural\nÔ¨Åelds for crowd counting. In Porc. of IEEE Intl. Conf.\non Computer Vision, 2019. 2, 3\n[64] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xi-\naokang Yang. Cross-scene crowd counting via deep\nconvolutional neural networks. In Proc. of IEEE Intl.\nConf. on Computer Vision and Pattern Recognition ,\npages 833‚Äì841, 2015. 5\n[65] Shanghang Zhang, Guanhang Wu, Joao P Costeira,\nand Jos¬¥e MF Moura. Fcn-rlstm: Deep spatio-temporal\nneural networks for vehicle counting in city cameras.\nIn Porc. of IEEE Intl. Conf. on Computer Vision, 2017.\n8\n[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua\nGao, and Yi Ma. Single-image crowd counting via\nmulti-column convolutional neural network. In Proc.\nof IEEE Intl. Conf. on Computer Vision and Pattern\nRecognition, 2016. 1, 2, 5, 6, 7, 8, 10\n[67] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian\nZhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking\nsemantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proc. of IEEE Intl.\nConf. on Computer Vision and Pattern Recognition ,\npages 6881‚Äì6890, 2021. 2, 3\n[68] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In In-\nternational Conference on Learning Representations ,\n2020. 2",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7466024160385132
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6065621972084045
    },
    {
      "name": "Transformer",
      "score": 0.5666239857673645
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5470049381256104
    },
    {
      "name": "Machine learning",
      "score": 0.41336268186569214
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3878690004348755
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}