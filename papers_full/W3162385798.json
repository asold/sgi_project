{
    "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
    "url": "https://openalex.org/W3162385798",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2743579753",
            "name": "Jiang, Zhengbao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2328387707",
            "name": "Araki Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2354599654",
            "name": "Ding Haibo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2138493030",
            "name": "Neubig, Graham",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3033176962",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W2964212410",
        "https://openalex.org/W2970295111",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2963384319",
        "https://openalex.org/W2557764419",
        "https://openalex.org/W3104939451",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2137556846",
        "https://openalex.org/W3099142828",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035428952",
        "https://openalex.org/W2986266667",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963382396",
        "https://openalex.org/W2987553933",
        "https://openalex.org/W3102476541",
        "https://openalex.org/W1618905105",
        "https://openalex.org/W2061271742",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3021047975",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W2945290257",
        "https://openalex.org/W3034257552",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W3105721709",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W2790319220",
        "https://openalex.org/W2125436846",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W3103751997",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2996848635",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2963818033",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2998557616",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2970745243",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W3035441651",
        "https://openalex.org/W2963675284",
        "https://openalex.org/W2794325560"
    ],
    "abstract": "Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models -- T5, BART, and GPT-2 -- and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
    "full_text": "How Can We Know When Language Models Know?\nOn the Calibration of Language Models for Question Answering\nZhengbao Jiang†, Jun Araki‡, Haibo Ding‡, Graham Neubig†\n†Languages Technologies Institute, Carnegie Mellon University\n‡Bosch Research\n{zhengbaj,gneubig}@cs.cmu.edu\n{jun.araki,haibo.ding}@us.bosch.com\nAbstract\nRecent works have shown that language\nmodels (LM) capture different types of\nknowledge regarding facts or common\nsense. However, because no model is per-\nfect, they still fail to provide appropriate an-\nswers in many cases. In this paper, we ask\nthe question “how can we know when lan-\nguage models know, with conﬁdence, the\nanswer to a particular query?” We exam-\nine this question from the point of view of\ncalibration, the property of a probabilistic\nmodel’s predicted probabilities actually be-\ning well correlated with the probabilities of\ncorrectness. We examine three strong gen-\nerative models – T5, BART, and GPT-2 –\nand study whether their probabilities on QA\ntasks are well calibrated, ﬁnding the an-\nswer is a relatively emphatic no. We then\nexamine methods to calibrate such mod-\nels to make their conﬁdence scores corre-\nlate better with the likelihood of correct-\nness through ﬁne-tuning, post-hoc probabil-\nity modiﬁcation, or adjustment of the pre-\ndicted outputs or inputs. Experiments on\na diverse range of datasets demonstrate the\neffectiveness of our methods. We also per-\nform analysis to study the strengths and lim-\nitations of these methods, shedding light\non further improvements that may be made\nin methods for calibrating LMs. We have\nreleased the code at https://github.\ncom/jzbjyb/lm-calibration.\n1 Introduction\nLanguage models (LMs; Church (1988); Bengio\net al. (2003); Radford et al. (2019)) learn to model\nthe probability distribution of text, and in doing\nso capture information about various aspects of\nthe syntax or semantics of the language at hand.\nRecent works have presented intriguing results\ndemonstrating that modern large-scale LMs also\ncapture a signiﬁcant amount of knowledge, includ-\ning factual knowledge about real-world entities\n(Petroni et al., 2019; Jiang et al., 2020b; Roberts\net al., 2020; Bouraoui et al., 2020), commonsense\nknowledge (Trinh and Le, 2018; Kocijan et al.,\n2019; Talmor et al., 2019a; Bosselut et al., 2019),\nand simple numerical operations (Wallace et al.,\n2019; Talmor et al., 2019a; Geva et al., 2020). No-\ntably, large models trained on massive crawls of\ninternet text (such as T5 (Raffel et al., 2019) and\nGPT-3 (Brown et al., 2020)) have been shown to\nbe able to perform quite sophisticated knowledge-\nbased tasks simply through prompting the model\nto predict the next words given a particular cue.\nHowever, at the same time, LMs are obviously\nnot omnipotent, and still fail to provide appropri-\nate answers in many cases, such as when dealing\nwith uncommon facts (Poerner et al., 2019; Jiang\net al., 2020a) or complex reasoning (Talmor et al.,\n2019a). The high performance on datasets probing\nfactual or numerical knowledge might be achieved\nthrough modeling superﬁcial signals in the train-\ning data that are not generalizable to unseen test\ncases (Poerner et al., 2019; Zhou et al., 2020; Wal-\nlace et al., 2019; Talmor et al., 2019a). Thus, if\nsuch models are to be deployed in real applica-\ntions it is of crucial importance to determine the\nconﬁdence with which they can provide an answer.\nThis is especially true if these models are deployed\nto safety-critical domains such as healthcare and\nﬁnance, where mistaken answers can have serious\nconsequences.1\nIn this paper, we ask the question “how can we\nknow when language models know, with conﬁ-\ndence, the answer to a particular knowledge-based\nquery?” Speciﬁcally, we examine this from the\npoint of view of calibration, whether the model’s\nprobability estimates are well-aligned with the\nactual probability of the answer being correct.\nWe apply the largest publicly available LMs, T5,\n1For example, a mocked-up medical chatbot based on\nGPT-3 answered the question of “should I kill myself?” with\n“I think you should” (Quach, 2020).\narXiv:2012.00955v2  [cs.CL]  20 May 2021\nFormat Input Candidate Answers Original Calibrated\nMultiple-choice\nOxygen and sugar are the products of\n(A) cell division. (B) digestion. (C)\nphotosynthesis. (D) respiration.\ncell division. 0.00 0.02\ndigestion. 0.00 0.01\nphotosynthesis. 0.00 0.83\nrespiration. 1.00 0.14\nExtractive\nWhat type of person can not be at-\ntributed civil disobedience?\nCivil disobedience is usually deﬁned\nas pertaining to a citizen’s relation ...\nhead of government 0.07 0.49\npublic ofﬁcial 0.91 0.26\nhead of government of a country 0.01 0.16\npublic ofﬁcials 0.01 0.09\nTable 1: LM calibration examples for the T5 model with correct answers in bold. “Original” and “calibrated”\nindicate the normalized probability before and after ﬁne-tuning to improve calibration.\nBART, and GPT-2, over a wide range of question\nanswering (QA) datasets (Khashabi et al., 2020)\ncovering diverse domains. We ﬁrst observe that\ndespite the models’ high performance (e.g. T5\neclipses other alternatives such as GPT-3 on some\ndatasets), the models tend to not be well cali-\nbrated; their probability estimates over candidates\nhave far-from-perfect correspondence with the ac-\ntual probability that the answer they provide is cor-\nrect. Some examples of this are demonstrated in\nthe “Original” column of Table 1.\nTo alleviate this problem, we propose meth-\nods to make LMs’ conﬁdence scores correlate bet-\nter with the likelihood of model prediction be-\ning correct. We examined both ﬁne-tuning meth-\nods that modify LMs’ parameters and post-hoc\nmethods that keep LMs ﬁxed and only manipu-\nlate the conﬁdence values or inputs. Speciﬁcally,\nwe ﬁne-tune the LM using softmax- or margin-\nbased objective functions based on multiple candi-\ndate answers. For post-hoc calibration, we exam-\nined temperature-based scaling and feature-based\ndecision trees that take prediction probability and\ninput-related features as input and produce cali-\nbrated conﬁdence (Jagannatha and Yu, 2020; De-\nsai and Durrett, 2020; Kamath et al., 2020). We\nalso study the sensitivity of LMs’ conﬁdence esti-\nmation with respect to language variation by para-\nphrasing candidate answers and augmenting ques-\ntions using retrieved context.\nExperimental results demonstrate that both ﬁne-\ntuning and post-hoc methods can improve calibra-\ntion performance without sacriﬁcing accuracy. We\nfurther perform analysis and ablation studies on\nour methods, inspecting different aspects that may\naffect calibration performance. We found that like\nother neural models, LMs are over-conﬁdent much\nof the time with conﬁdence close to either 0 or\n1. As a result, post-processing conﬁdence with\ntemperature-based scaling and feature-based deci-\nsion trees is universally helpful. We also found\nthat LMs become better calibrated if we phrase\neach answer multiple ways and provide more ev-\nidence through retrieval, indicating that current\nLMs are sensitive to both input and output.\n2 LM-based Question Answering\nLMs are now a ubiquitous tool in not only natu-\nral language generation, but also natural language\nunderstanding (NLU), where they are largely used\nfor unsupervised representation learning in pre-\ntrained models such as BERT (Devlin et al., 2019).\nHowever, recent work has demonstrated that LMs\ncan also be used as-is to solve NLU tasks, by pre-\ndicting the missing words in Cloze-style questions\n(Petroni et al., 2019), or by predicting the contin-\nuation to prompts (Bosselut et al., 2019; Brown\net al., 2020).\nPrevious works that purport to calibrate LMs\n(Desai and Durrett, 2020; Jagannatha and Yu,\n2020; Kamath et al., 2020; Kong et al., 2020)\nmainly focus on the former use case, using repre-\nsentations learned by LMs to predict target classes\n(for tasks such as natural language inference, part-\nof-speech tagging, or text classiﬁcation) or iden-\ntify answer spans (for tasks such as extractive\nQA). In contrast, we focus on the latter case, cali-\nbrating LMs themselves by treating them as natu-\nral language generators that predict the next words\ngiven a particular input.\nTo make our observations and conclusions as\ngeneral as possible, we experiment over a di-\nverse range of QA datasets with broad domain\ncoverage over questions regarding both factual\nand commonsense knowledge (Khashabi et al.,\n2020). We list all the datasets we used in Table 2\nalong with their corresponding domain. Since\nwe focus on calibrating LMs as generators, we\nfollow Khashabi et al. (2020) in converting QA\ndatasets of different formats to a uniﬁed sequence-\nto-sequence format that takes a question X as in-\nput and calculates the probability of a continuation\nY that corresponds to the answer:\nPLM(Y|X) =\n|Y |∏\ni=1\nPLM(yi|X,y<i).\nSpeciﬁcally, we focus on two varieties of QA:\nmultiple-choice and extractive, with examples\nshown in Table 1.2\nMultiple-choice QA For multiple-choice QA,\nwe assume a question and a set of candidate an-\nswers I(X) = {Y(i)}i. Inputs X to LMs are\nquestions concatenated with multiple candidate\nanswers (with each answer prefaced by “(A)”,\n“(B)”, etc.), and context such as a passage that can\nbe used to help answer the question if any exists.\nTo ﬁnd the answer the model will return, we calcu-\nlate the highest-probability answer among the an-\nswer candidates:\nˆY = arg max\nY ′∈I(X)\nPLM(Y′|X).\nWe can also calculate the normalized probability\nPN ( ˆY|X) = PLM( ˆY|X)∑\nY ′∈I(X) PLM(Y′|X), (1)\nwhich provides some idea of the conﬁdence of an-\nswer ˆY with respect to the candidate list.\nExtractive QA For extractive QA, inputs X to\nLMs are questions concatenated with context pas-\nsages from which the answer must be extracted. In\nthis case, every span within the passage is a candi-\ndate answer in I(X). However, enumerating over\nall possible spans of the context passage is com-\nputationally costly. Thus, we follow Jagannatha\nand Yu (2020) in using a manageable set of can-\ndidate outputs to perform calibration. Speciﬁcally,\n2We also considered using free-form (abstractive) QA\ndatasets, where the answers are not constrained to be one of\nseveral choices and can instead be any text. However, we\nfound it hard to evaluate the correctness of generated outputs,\nas paraphrases of the correct answer are still correct, so we do\nnot report results on these datasets in this paper. Solving this\nevaluation problem and evaluating calibration on these tasks\nis an enticing direction for future work.\nFormat Datasets and Domains\nMulti-choice ARC (science (Clark et al., 2018)),\nAI2 Science Questions (science (Clark\net al., 2018)), OpenbookQA (science\n(Mihaylov et al., 2018)), Winogrande\n(commonsense (Sakaguchi et al.,\n2020)), CommonsenseQA (common-\nsense (Talmor et al., 2019b)), MCTest\n(ﬁctional stories (Richardson et al.,\n2013)), PIQA (physical (Bisk et al.,\n2020)), SIQA (social (Sap et al.,\n2019)), RACE (English compre-\nhension (Lai et al., 2017)), QASC\n(science (Khot et al., 2020)), MT-test\n(mixed (Hendrycks et al., 2020))\nExtractive SQuAD 1.1 (wikipedia (Rajpurkar\net al., 2016)), SQuAD 2 (Wikipedia\n(Rajpurkar et al., 2018)), NewsQA\n(news (Trischler et al., 2017)), Quoref\n(wikipedia (Dasigi et al., 2019)),\nROPES (situation understanding (Lin\net al., 2019))\nTable 2: Datasets used in this paper and their domains.\nwe develop a method to efﬁciently calculate prob-\nabilities over promising spans that exist in the in-\nput. First, we calculate the probability of the ﬁrst\ntoken in output Y′, masking out any tokens that\nare not included in the input passage at all. Then,\nfor the top Rscoring tokens, we ﬁnd their location\nin the input passage, and calculate the probability\nof all continuing spans up to a certain length (e.g.,\n20 tokens). We ﬁnally keep the top K spans as\ncandidates I(X) and use all candidates to calcu-\nlate the probability in a manner similar to that of\nmultiple-choice QA.\n3 Background on Calibration\nA model is considered well calibrated if the conﬁ-\ndence estimates of its predictions are well-aligned\nwith the actual probability of the answer being cor-\nrect. Given an input Xand true output Y, a model\noutput ˆY, and a probability PN ( ˆY|X) calculated\nover this output, a perfectly calibrated model sat-\nisﬁes the following condition:\nP( ˆY = Y|PN ( ˆY|X) =p) =p,∀p∈[0,1].\nIn practice, we approximate this probability by\nbucketing predictions into M disjoint equally-\nsized interval bins based on conﬁdence. Guo et al.\n(2017) examined the calibration properties of neu-\nral network classiﬁers, and proposed a widely used\nmeasure of calibration called expected calibration\nerror (ECE), which is a weighted average of the\ndiscrepancy between each bucket’s accuracy and\nconﬁdence:\nM∑\nm=1\n|Bm|\nn |acc(Bm) −conf(Bm)|, (2)\nwhere Bm is the m-th bucket containing samples\nwhose prediction conﬁdence falls into the inter-\nval (m−1\nM , m\nM ], acc (Bm) is the average accuracy\nof this bucket, and conf (Bm) is the average con-\nﬁdence of this bucket. The above equation can\nbe visualized using reliability diagrams (e.g., Fig-\nure 1 in the experiments), where each bar corre-\nsponds to one bucket, and the height is equal to\nthe average accuracy. The diagram of a perfectly\ncalibrated model should have all bars aligned with\nthe diagonal.\nUnfortunately, we found that state-of-the-art\nLM-based methods for question answering (such\nas the UniﬁedQA model of Khashabi et al. (2020))\nwere extraordinarily poorly calibrated, with the\nnormalized probability estimates barely being cor-\nrelated with the likelihood of the outputs being\ncorrect. For the two examples in Table 1, for in-\nstance, we can see that the language model assigns\na very high probability to answers despite the fact\nthat they are wrong. This is particularly impor-\ntant because with T5 (Raffel et al., 2019), GPT-3\n(Brown et al., 2020), and others (Guu et al., 2020;\nLewis et al., 2020c) being provided as a poten-\ntial answer to complex knowledge-based tasks, for\nmodels to actually be used in practical scenarios\nthey must also be able to know when they can-\nnot provide correct information. In the following\nsection, we examine methods to improve the cali-\nbration of pre-trained models through a number of\nmethods.\n4 Calibrating LMs for Question\nAnswering\nOur calibration methods can be grouped into two\ncategories: methods that ﬁne-tune LMs and post-\nhoc methods that keep LMs ﬁxed and only manip-\nulate conﬁdence or inputs.\n4.1 Fine-tuning-based Calibration\nExisting LMs mainly use maximal likelihood es-\ntimation (MLE) during training, which maximizes\nthe probability of ground truth output given the in-\nput. However, it is well-attested that MLE-trained\nlanguage generators are biased, tending to prefer\nshort outputs (Murray and Chiang, 2018), or be-\ning biased towards more frequent vocabulary (Ott\net al., 2018). However, in the case where we know\na set of reasonable candidates I(X), one straight-\nforward way to ﬁne-tune LMs is to only consider\ncandidates in I(X) and directly tune PN ( ˆY|X) to\nbe a good probability estimate of the actual out-\nputs. We propose two ﬁne-tuning objective func-\ntions based on the candidate set.\nSoftmax-based objective functions model can-\ndidates in a one-vs-all setting, where we use the\nsoftmax function to normalize the conﬁdence of\ncandidates and maximize the probability corre-\nsponding to the correct candidate. We use the neg-\native log likelihood as the loss function:\nL(X,Y ) =−log exp(s(Y))∑\nY ′∈I(X) exp(s(Y′)),\nwhere the ground truth Y is one of the candidates\nin I(X), and s(·) is the logit of the corresponding\noutput (omit condition X for simplicity), which is\ncomputed as the log probabilities of all tokens in\nthe output: s(Y) = logPLM(Y|X).\nMargin-based objective functions try to maxi-\nmize the conﬁdence margin between ground truth\noutput and negative results. This is motivated\nby the fact that non-probabilistic objectives such\nas those used by support vector machines pro-\nvide reasonably good probabilistic estimates af-\nter appropriate scaling and adjustment (Platt et al.,\n1999). Speciﬁcally, we use the following objec-\ntive:\nL(X,Y ) =\n∑\nY ′∈I(X)\\Y\nmax(0,τ + s(Y′) −s(Y)).\n4.2 Post-hoc Calibration\nComparing to ﬁne-tuning methods that optimize\nthe parameters in the model, post-hoc calibration\nmethods keep the model as-is and manipulate var-\nious types of information derived from the model\nto derive good probability estimates (Guo et al.,\n2017; Jagannatha and Yu, 2020; Desai and Dur-\nrett, 2020). In this section, we consider two as-\npects of the model: model probabilities PN ( ˆY|X)\nand features of the model inputs X or outputs Y.\nWe attempted two representative methods, namely\ntemperature-based scaling (Guo et al., 2017) and\nfeature-based decision trees (Jagannatha and Yu,\n2020), to study whether post-processing probabil-\nities is an effective method for calibration of LMs\nin the context of QA.\nTemperature-based Scaling methods have\nbeen proposed for classiﬁcation tasks (Guo\net al., 2017; Desai and Durrett, 2020), where\na positive scalar temperature hyperparameter τ\nis introduced in the ﬁnal classiﬁcation layer to\nmake the probability distribution either more\npeaky or smooth: softmax (z/τ). If τ is close to\n0, the class with the largest logit receives most\nof the probability mass, while as τ approaches\n∞, the probability distribution becomes uniform.\nWhen applying this method to our setting, we\nuse log probabilities of the candidates in I(X)\nas logits in computing the softmax function:\nz = log PLM(Y′|X),Y ′ ∈ I(X), and τ is\noptimized with respect to negative log likelihood\non the development split.\nFeature-based Decision Trees methods explore\nnon-linear combinations of features to estimate the\nconﬁdence compared to temperature-based scal-\ning which only considers the raw conﬁdence. We\nfollow previous works (Jagannatha and Yu, 2020;\nDong et al., 2018) and use gradient boosted deci-\nsion trees (Chen and Guestrin, 2016) as our regres-\nsor to estimate the conﬁdence based on features.\nBesides the raw conﬁdence, we consider the fol-\nlowing features and explain their intuitions:\n• Model Uncertainty: We use the entropy of\nthe distribution over the candidate set I(X)\nto inform the regressor of how uncertain the\nLM is with respect to the question.\n• Input Uncertainty : We use the perplexity\nof the LM on the input to indicate the un-\ncertainty over the input. The intuition is that\nhigh perplexity might indicate that the input\ncomes from a distribution different from the\ntraining distribution of the LM.\n• Input Statistics: We also use the length of\nthe input and output as features, motivated by\nour hypothesis that longer text may provide\nmore information to LMs than shorter text.\nWe train the regressor on the development set sim-\nilarly to temperature-based scaling by minimizing\nnegative log likelihood.\nInput How would you describe Addison?\n(A) excited (B) careless (C) devoted.\nAddison had been practicing for the\ndriver’s exam for months. He ﬁnally\nfelt he was ready, so he signed up and\ntook the test.\nParaphrases &\nProbabilities\ndevoted (0.04), dedicated (0.94),\ncommitment (0.11), dedication (0.39)\nTable 3: An example question with the correct answer\nin bold. Different paraphrases of the correct answer\nhave different probabilities.\n4.3 LM-Speciﬁc Methods\nIn addition to standard methods that are applicable\nto most prediction models, we also examine sev-\neral methods that are speciﬁc to the fact that we\nare using LMs for the task of QA.\nCandidate Output Paraphrasing Motivated\nby the fact that LMs are sensitive to language vari-\nation (Jiang et al., 2020b) in tasks like question\nanswering and factual prediction, we hypothesize\nthat one potential reason why the conﬁdence es-\ntimation of LMs is not accurate is that the candi-\ndate output is not worded in such a way that the\nLM would afford it high probability. As shown by\nthe example in Table 3, paraphrasing the correct\nanswer from “devoted” to “dedicated” increases\nthe probability from 0.04 to 0.94. Motivated by\nthis, we use a round-trip translation model to para-\nphrase each candidate output Y′∈I(X) into sev-\neral other expressions by ﬁrst translating it into\nanother language and then back-translating it to\ngenerate a set of paraphrases para (Y′). We then\ncalculate the probability of each candidate out-\nput by summing the probability of all paraphrases\nP(Y′) = ∑\nQ∈para(Y ′) PLM(Q|X) and normalize\nit following Equation 1. By collectively consid-\nering multiple paraphrases, the issue of sensitiv-\nity to the wording can be alleviated somewhat, as\nthere will be a higher probability of observing a\nparaphrase that is afforded high probability by the\nmodel.\nInput Augmentation Previous work has found\nthat LMs’ factual predictions can be improved if\nmore context is provided (Petroni et al., 2020a),\nwhich has inspired many retrieval-augmented\nLMs that retrieve evidence from external resources\nand condition the LMs’ prediction on this evi-\ndence (Guu et al., 2020; Lewis et al., 2020a,c). We\nhypothesize that retrieving extra evidence to aug-\nment the input also has the potential to improve\nthe conﬁdence estimation of LMs as it will provide\nthe model more evidence upon which to base both\nits predictions and its conﬁdence estimates. We\nfollow (Petroni et al., 2020a) to retrieve the most\nrelevant Wikipedia article using TF-IDF-based re-\ntrieval systems used in DrQA (Chen et al., 2017)\nand append the ﬁrst paragraph of the article to the\ninput.\n5 Experiments\n5.1 Experimental Settings\nDatasets We evaluate the calibration perfor-\nmance on both multiple-choice QA datasets and\nextractive QA datasets listed in Table 2. To test\nwhether our calibration methods can generalize\nto out-of-domain datasets, we use a subset of\ndatasets of multiple-choice/extractive QA to train\nour methods, and the remaining subset of datasets\nto evaluate the performance. Speciﬁcally, we\nuse ARC (easy), AI2 Science Question (elemen-\ntary), OpenbookQA, QASC, Winogrande, Com-\nmonsenseQA, and PhysicalIQA as the training\nsubset for multiple-choice QA (denoted as MC-\ntrain), and SQuAD 1.1, NewsQA as the training\nsubset for extractive QA (denoted as Ext-train).\nThe remaining subsets used for evaluation are de-\nnoted as MC-test and Ext-test respectively. We\nalso included a much harder multiple-choice QA\ndataset (denoted as MT-test; Hendrycks et al.\n(2020)) regarding common sense in a number of\ngenres, in which the largest GPT-3 model and Uni-\nﬁedQA both display only low to moderate accu-\nracy. For ﬁne-tuning methods, we use the train\nsplit of MC-train/Ext-train to ﬁne-tune the LMs.\nFor post-hoc methods like temperature-based scal-\ning and decision trees, we follow Guo et al. (2017)\nand use the development split of MC-train/Ext-\ntrain to optimize the parameters.3\nLMs One clear trend of the past several years\nis that the parameter size and training data size\nof pre-trained models plays a signiﬁcant role in\nthe accuracy of models; pre-trained LMs such as\nBERT (Devlin et al., 2019) tend to underperform\nmore recently released larger LMs like Turing-\nNLG4 and GPT-3 (Brown et al., 2020). Thus, we\n3Since not all datasets in MC-test and Ext-test have a test\nsplit, we report the performance on the development split.\n4https://msturing.org/\nuse the largest publicly available LM, which at the\ntime of this writing is Raffel et al. (2019)’s T5\nmodel. The T5 model is a sequence-to-sequence\nmodel with both encoder and decoder using trans-\nformers (Vaswani et al., 2017), and the largest ver-\nsion has 11 billion parameters, allowing it to re-\nalize state-of-the-art performance on tasks such\nas question answering and natural language un-\nderstanding (Roberts et al., 2020; Khashabi et al.,\n2020).\nSpeciﬁcally, we use two varieties of this model.\nThe original T5 model is a sequence-to-sequence\nmodel trained on a large corpus of web text,\nspeciﬁcally trained on a denoising objective that\ngenerates missing tokens given inputs with some\ntokens masked out. The UniﬁedQA model, uses\nthe initial T5 model and ﬁne-tunes on a variety\nof QA datasets by converting multiple-choice, ex-\ntractive QA formats into a uniﬁed sequence-to-\nsequence format, similar to the one that we show\nin Table 1. We use the 3-billion versions in our\nmain experiments in subsection 5.3 (for efﬁciency\npurposes), but also report the performance of the\nlargest 11-billion versions in ablation studies sub-\nsection 5.5.\nFor comparison with LMs of different architec-\ntures trained on different datasets, we also report\nthe performance of two other LMs in section 5.5:\nthe 0.4-billion BART model (Lewis et al., 2020b)\nwhich is a sequence-to-sequence model and the\n0.7-billion GPT-2 large model (Radford et al.,\n2019) which is a conventional language model.\nWe ﬁne-tune them following the same recipe of\nUniﬁedQA (Khashabi et al., 2020).\nEvaluation Metrics We use accuracy to mea-\nsure the prediction performance of our methods,\nand ECE to measure the calibration performance.\nAccuracy is computed as the ratio of question-\nanswer pairs for which the correct answer has\nthe highest probability among all the candidates\nin I(x). ECE is computed using Equation 2 by\nbucketing all candidate answers in I(x) based on\nconﬁdence. For MC-test and Ext-test which in-\nclude multiple datasets, we compute accuracy and\nECE on each dataset separately and average across\nthem to avoid the metrics being dominated by\nlarge datasets.\nImplementation Details We ﬁne-tune\nUniﬁedQA-3B with a batch size of 16 for 3k\nsteps and UniﬁedQA-11B with a batch size of\n3 for 15k steps on a v3-8 TPU. The maximal\nlength of input and output are set to 512 and 128\nrespectively, following the setting of UniﬁedQA\n(Khashabi et al., 2020). For extractive QA\ndatasets, we use top R = 10 ﬁrst tokens and\nﬁnally K = 5 spans are used as candidates.\nFor the paraphrasing-based method, we use the\nWMT-19 English-German and German-English\ntransformer models to perform back translation\n(Ng et al., 2019). The beam size is set to 10 for\nboth directions, which will yield 10 ×10 = 100\nparaphrases in the end. Since some paraphrases\nare duplicated, we count the frequency and use the\ntop 5 unique paraphrases in our main experiments\nsubsection 5.3. We also report the performance of\nusing different numbers of paraphrases in subsec-\ntion 5.5. For the retrieval-based augmentation, we\nuse the KILT toolkit (Petroni et al., 2020b) to re-\ntrieve the most relevant article from the Wikipeida\ndump, and append the ﬁrst three sentences of the\nﬁrst paragraph of the retrieved article to the input.\nFor the feature-based decision trees model, we\nuse XGBoost (Chen and Guestrin, 2016) with\nlogistic binary objective, max depth of 4, number\nof parallel trees of 5, and subsample ratio of\n0.8. We use Temp. to denote temperature-based\nscaling, XGB to denote feature-based decision\ntrees, Para. to denote paraphrasing, Aug. to\ndenote input augmentation, and Combo to denote\nthe combination of Temp., Para., and Aug. in\nthe experimental section. We use the model\nwith the best calibration performance in post-hoc\ncalibration experiments. For multiple-choice QA,\nwe use the UniﬁedQA model after margin-based\nﬁne-tuning. For extractive QA, we use the original\nUniﬁedQA model.\n5.2 Are LM-based QA Models Well\nCalibrated?\nAs shown in Table 4, our baseline models (i.e., T5\nand UniﬁedQA) are strong, achieving state-of-the-\nart accuracy on a diverse range of QA datasets. On\nthe MT-test datasets, the UniﬁedQA model even\noutperforms the largest version of GPT-3 with 175\nbillions parameters (Hendrycks et al., 2020). De-\nspite the impressive performance, these models\nare not well calibrated, with ECE higher than 0.2\non the MT-test dataset. We found that LMs tend\nto be over-conﬁdent about cases they do not know,\nas shown in the conﬁdence distribution in the ﬁrst\nrow of Figure 2 that most predictions have ag-\ngressive conﬁdence being close to 0 or 1. The\nUniﬁedQA model assigns high conﬁdence to the\nwrong answer for examples in Table 1, indicating\nthat its conﬁdence estimates are not trustworthy.\n5.3 Can LM-based QA Models be\nCalibrated?\nWe calibrate the UniﬁedQA model using both\nﬁne-tuning-based methods and post-hoc methods\nand show their performance in Table 4 and Table 5\nrespectively.\nOverall, on multi-choice QA datasets (i.e., MC-\ntest and MT-test), both ﬁne-tuning-based meth-\nods and post-hoc methods can improve ECE while\nmaintaining accuracy compared to the baseline\nUniﬁedQA model. The best-performing method\n(i.e., Combo), which combines margin-based ﬁne-\ntuning, temperature-based scaling, paraphrasing,\nand input augmentation, improves ECE from\n0.095 to 0.044 by over 53%. As shown in the reli-\nability diagrams of the original UniﬁedQA model\n(top-right) and the UniﬁedQA model calibrated\nwith Combo (bottom-left) in Figure 1, calibration\nusing our methods makes the conﬁdence estimates\nof predictions better aligned with their correct-\nness. Comparing those two diagrams, an interest-\ning observation is that our method seems to over-\ncalibrate the LM, making over-estimated bars on\nthe right-hand side of the top-right diagram (bars\nlower than the diagonal) under-estimated and vice\nversa. This is probably caused by the temperature\nbeing too aggressive (i.e., too large), making the\ndistribution too ﬂat. Note that the datasets used\nto learn the temperature (MC-train) and used in\nevaluation (MC-test) are different, which we hy-\npothesize is the reason why the temperature is too\naggressive. We verify this by learning an ora-\ncle temperature on the evaluation datasets (MC-\ntest). The learned temperature indeed becomes\nsmaller (1.35 →1.13), and the reliability dia-\ngram (bottom-right in Figure 1) is almost perfectly\naligned. This demonstrates the challenge of cali-\nbrating LMs across different domains.\nHowever, on extractive QA datasets, the im-\nprovement brought by different calibration meth-\nods is smaller. We hypothesize that this is be-\ncause the candidate set I(X) generated by the\nspan-based decoding method for extractive QA are\nharder to calibrate than the manually curated can-\ndidate answers for multiple-choice QA. We com-\npute the average entropy of the conﬁdence of the\nMethod MC-test MT-test Ext-test\nACC ECE ACC ECE ACC ECE\nT5 0.313 0.231 0.268 0.248 0.191 0.166\nUniﬁedQA 0.769 0.095 0.437 0.222 0.401 0.114\n+ softmax 0.767 0.065 0.433 0.161 0.394 0.110\n+ margin 0.769 0.057 0.431 0.144 0.391 0.112\nTable 4: Performance of different ﬁne-tuning methods.\nMethod MC-test MT-test Ext-test\nACC ECE ACC ECE ACC ECE\nBaseline 0.769 0.057 0.431 0.144 0.401 0.114\n+ Temp. 0.769 0.049 0.431 0.075 0.401 0.107\n+ XGB 0.771 0.055 0.431 0.088 0.402 0.103\n+ Para. 0.767 0.051 0.429 0.122 0.393 0.114\n+ Aug. 0.744 0.051 0.432 0.130 0.408 0.110\n+ Combo 0.748 0.044 0.431 0.079 0.398 0.104\nTable 5: Performance of different post-hoc methods\nusing the UniﬁedQA model after margin-based ﬁne-\ntuning or the original UniﬁedQA model as the base-\nline model. “+Combo” denotes the method using both\nTemp., Para., and Aug.\nUniﬁedQA model over I(X) on both extractive\nQA (Ext-test) and multiple-choice QA datasets\n(MC-test), and found that Ext-test indeed has\nmuch higher entropy compared to MC-test (0.40\nvs 0.13), which partially explains the difﬁculty of\ncalibration on extractive QA datasets.\n5.4 Analysis of Individual Calibration\nMethods\nIn this section, we discuss each method in detail\nand analyze why they can improve calibration per-\nformance.\nObjective Function Matters. The original Uni-\nﬁedQA model is ﬁne-tuned based on MLE, which\nmaximizes the probability of the gold answer\ngiven the question. Both softmax-based and\nmargin-based ﬁne-tuning, which explicitly com-\npare and adjust the probability of candidate an-\nswers, can further improve ECE on multiple-\nchoice datasets. We argue that the softmax-based\nand margin-based objective functions are better\nsuited for questions with potential candidates.\nPost-processing Conﬁdence is Effective Univer-\nsally. Post-processing the raw conﬁdence either\nsolely based on conﬁdence or other features is ef-\nfective across all datasets, which is consistent with\nthe conclusion on other tasks such as structured\n(a) T5\n (b) UniﬁedQA\n(c) UniﬁedQA w/ Combo\n (d) UniﬁedQA w/ Combo and\noracle temperature\nFigure 1: Reliability diagram of the T5 model\n(top-left), the original UniﬁedQA model (top-right),\nthe UniﬁedQA model after calibration with Combo\n(bottom-left), and Combo with oracle temperature\n(bottom-right) on the MC-test datasets.\nprediction and natural language inference (Jagan-\nnatha and Yu, 2020; Desai and Durrett, 2020). We\ndemonstrate the histogram of conﬁdence before\nand after applying temperature-based scaling or\nfeature-based decision trees in Figure 2. LMs tend\nto be over-conﬁdent, with most predictions hav-\ning either extremely high or low conﬁdence. Both\nmethods can successfully re-scale the conﬁdence\nto reasonable ranges, thus improving the calibra-\ntion performance.\nParaphrasing Answers and Input Augmenta-\ntion can Improve Conﬁdence Estimation. The\nimprovement brought by using paraphrasing is\nsigniﬁcant on multiple-choice datasets, demon-\nstrating that using diverse expressions can indeed\nimprove conﬁdence estimation. To better un-\nderstand under what circumstances paraphrasing\nworks, we group candidate answers into two cat-\negories: the ﬁrst group includes candidate an-\nswers that become better calibrated using para-\nphrases; the second group includes candidate an-\nswers whose conﬁdence remains the same using\nparaphrases. We say that a candidate becomes bet-\nter calibrated if its conﬁdence increases/decreases\nby 20% if it is a correct or incorrect answer re-\nspectively. We found that the average length of\nquestions for better calibrated candidates (187) is\nmuch shorter than that of candidates without im-\nprovement (320), indicating that paraphrasing is\n(a) T5\n (b) UniﬁedQA\n(c) UniﬁedQA w/ Temp.\n (d) UniﬁedQA w/ XGB\nFigure 2: The ratio of predictions with respect to conﬁ-\ndence of the T5 model (top-left), the UniﬁedQA model\n(top-right), the UniﬁedQA model after temperature-\nbased calibration (bottom-left), and the UniﬁedQA\nmodel after feature-based calibration (bottom-right) on\nthe MC-test datasets.\nuseful mainly for short questions. We also com-\npute the diversity of word usage in paraphrases\nusing the number of unique words divided by the\ntotal length of paraphrases. We found that better\ncalibrated candidates have slightly higher diversity\n(0.35 vs 0.32), which is consistent with our intu-\nition. Retrieval-based augmentation can also im-\nprove calibration performance on multiple-choice\ndatasets, which is probably because the retrieved\ndocuments can provide extra evidence about the\nquestion, making LMs more robust at conﬁdence\nestimation.\nCalibration Methods are Complementary.\nBy combining margin-based ﬁne-tuning,\ntemperature-based scaling, paraphrasing, and\ninput augmentation, we achieve the best ECE\non MC-test, demonstrating that these calibration\nmethods are complementary to each other.\n5.5 Ablation Study\nIn this section, we perform an ablation study to\nexamine different aspects of LM calibration, in-\ncluding calibration performance of different LMs,\nacross LMs with different sizes, using different\nnumbers of paraphrases, and across datasets with\npotential domain shift.\nPerformance of Different LMs. We report the\nperformance of two other LMs in Table 6. Both\nthe BART and GPT-2 models are smaller than\nMethod BART GPT-2 large\nACC ECE ACC ECE\nOriginal 0.295 0.225 0.272 0.244\n+ UniﬁedQA 0.662 0.166 0.414 0.243\n+ softmax 0.658 0.097 0.434 0.177\n+ margin 0.632 0.090 0.450 0.123\n+ Temp. 0.632 0.064 0.450 0.067\n+ XGB 0.624 0.090 0.440 0.080\n+ Para. 0.624 0.084 0.436 0.104\n+ Aug. 0.600 0.089 0.441 0.126\n+ Combo 0.591 0.065 0.429 0.069\nTable 6: Performance of different LMs on the MC-\ntest dataset. “Original” indicates the original language\nmodel, and “+ UniﬁedQA” indicates ﬁne-tuning fol-\nlowing the recipe of UniﬁedQA.\nT5, thus the overall accuracy and calibration per-\nformance are lower than that of T5. Both ﬁne-\ntuning and post-hoc calibration methods can im-\nprove ECE, indicating that our methods are appli-\ncable to LMs trained with different datasets and\narchitectures.\nPerformance of LMs with Different Sizes. We\nconduct experiments using the largest version (i.e.,\n11B) of the T5 and UniﬁedQA model to ana-\nlyze how calibration performance varies with re-\nspect to the size of the LM in Table 7. We found\nthat larger LMs usually achieve both higher ac-\ncuracy and better calibration performance, which\nis contradictory to the observation in image clas-\nsiﬁcation (Guo et al., 2017) where larger models\nsuch as ResNet (He et al., 2016) are no longer\nwell calibrated compared to smaller models like\nLeNet (Lecun et al., 1998). Given the fact the size\nof both the pre-training corpus and LMs are ex-\ntremely larger compared to previous practice, we\nmight have completely different observations with\nrespect to conﬁdence estimation. Unlike ResNet\ntrained on CIFAR-100, the training of LMs is not\nbottlenecked by the dataset, and larger LMs have\na stronger capacity to model text distribution and\nmemorize facts, which leads to better calibration\nperformance overall (Kaplan et al., 2020). Over-\nall, our methods can improve ECE from 0.067\nto 0.032 using the 11B UniﬁedQA model on the\nMC-test dataset, and from 0.175 to 0.085 on the\nMT-test dataset. However, compared to the 3B\nversion, improvement brought by post-hoc cali-\nbration methods is smaller, which is probably be-\ncause the 11B version is better optimized and more\nMethod MC-test MT-test\nACC ECE ACC ECE\nT5 0.359 0.206 0.274 0.235\nUniﬁedQA 0.816 0.067 0.479 0.175\n+ softmax 0.823 0.041 0.488 0.129\n+ margin 0.819 0.034 0.485 0.107\n+ Temp. 0.819 0.036 0.485 0.098\n+ XGB 0.818 0.065 0.486 0.108\n+ Para. 0.820 0.035 0.484 0.092\n+ Aug. 0.812 0.031 0.493 0.090\n+ Combo 0.807 0.032 0.494 0.085\nTable 7: Performance of the 11B LMs.\n0.04\n0.05\n0.06\n1 3 5 7 9 11 13 15 17 19\nFigure 3: ECE of the UniﬁedQA model using different\nnumbers of paraphrases on the MC-test datasets.\nknowledgeable.\nPerformance using Different Numbers of Para-\nphrases. In Figure 3, we experiment with differ-\nent numbers of paraphrases using the UniﬁedQA\nmodel on MC-test datasets. The overall trend\nis that the more paraphrases we use, the better\ncalibrated the LM, demonstrating that using dif-\nferent variations to express the candidate answer\ncan improve conﬁdence estimation. The improve-\nments using more than 10 paraphrases are subtle,\nso 5-10 paraphrases may represent a good trade-\noff between computational cost and performance\nin practical settings.\nPerformance on Training and Evaluation\nDatasets. As introduced in the experimental\nsection, we perform calibration on the MC-train\ndataset and evaluate the ﬁnal performance on\nthe MC-test dataset to study whether our cali-\nbration methods can generalize to out-of-domain\ndataset. We compare the performance on the train-\ning dataset and the evaluation dataset in Table 8.\nWe found that on both datasets, each individ-\nual method can improve ECE, indicating that our\nmethod can generalize to out-of-domain datasets.\nNote that the improvement on the training dataset\n(0.133 →0.042) is larger than on improvement on\nthe evaluation dataset (0.095 →0.044), which is\nMethod MC-train MC-test\nACC ECE ACC ECE\nT5 0.334 0.228 0.313 0.231\nUniﬁedQA 0.727 0.133 0.769 0.095\n+ softmax 0.735 0.084 0.767 0.065\n+ margin 0.737 0.069 0.769 0.057\n+ Temp. 0.737 0.051 0.769 0.049\n+ XGB 0.737 0.074 0.771 0.055\n+ Para. 0.742 0.053 0.767 0.051\n+ Aug. 0.721 0.059 0.744 0.051\n+ Combo 0.722 0.042 0.748 0.044\nTable 8: Performance comparison between training and\nevaluation datasets.\nprobably caused by the domain shift between the\ntwo datasets.\n6 Related Work\nCalibration Calibration is a well-studied topic\nin other tasks such as medical diagnosis (Jiang\net al., 2012) and image recognition (Guo et al.,\n2017; Lee et al., 2018). Previous works in NLP\nhave examined calibration in structured predic-\ntion problems such as part-of-speech tagging and\nnamed entity recognition (Jagannatha and Yu,\n2020), natural language understanding tasks such\nas natural language inference, paraphrase detec-\ntion, extractive question answering, and text clas-\nsiﬁcation (Desai and Durrett, 2020; Kamath et al.,\n2020; Kong et al., 2020). In contrast, we focus\non calibrating LMs themselves by treating them\nas natural language generators that predict the next\nwords given a particular input.\nLM probing Previous works probe pre-trained\nLMs with respect to syntactic and semantic prop-\nerties (Hewitt and Manning, 2019; Tenney et al.,\n2019), factual knowledge (Petroni et al., 2019; Po-\nerner et al., 2019; Jiang et al., 2020b), common-\nsense knowledge (Trinh and Le, 2018; Kocijan\net al., 2019), and other properties (Talmor et al.,\n2019a). These works usually focus on what LMs\nknow, while in this paper we also consider the\ncases when LMs do not know the answer with con-\nﬁdence.\n7 Conclusion\nIn this paper, we examine the problem of calibra-\ntion in LMs used for QA tasks. We ﬁrst note that\ndespite the impressive performance state-of-the-\nart LM-based QA models tend to be poorly cali-\nbrated in their probability estimates. To alleviate\nthis problem, we attempted several methods to ei-\nther ﬁne-tune the LMs, or adjust the conﬁdence\nby post-processing raw probabilities, augmenting\ninputs, or paraphrasing candidate answers. Ex-\nperimental results demonstrate the effectiveness of\nthese methods. Further analysis reveals the chal-\nlenges of this problem, shedding light on future\nwork on calibrating LMs.\nSome future directions could be developing cal-\nibration methods for LMs on a more ﬁne-grained\nlevel than simply holistic calibration across the en-\ntire dataset. For example, there has been signiﬁ-\ncant interest in how models perform across diverse\nsubsets of the entire training data (Hashimoto\net al., 2018) and how they reﬂect dataset biases\n(Rudinger et al., 2018), and the interaction of\nmodel conﬁdence with these phenomena is of sig-\nniﬁcant interest. It is also interesting to investigate\nthe effect of calibration on users or downstream\ntasks. For instance, providing users with model\nconﬁdences can inﬂuence downstream decisions\n(Zhang et al., 2020), and users may want to adjust\nrequired conﬁdence thresholds on critical domains\n(e.g., health, safety, medicine). All of these are in-\nteresting paths of inquiry for future research.\nAcknowledgements\nThis work was supported in part by a gift from\nBosch research. The authors thank the Google\nCloud and TensorFlow Research Cloud for com-\nputation credits that aided in the execution of this\nresearch.\nReferences\nYoshua Bengio, Réjean Ducharme, Pascal Vin-\ncent, and Christian Jauvin. 2003. A neural\nprobabilistic language model. Journal of ma-\nchine learning research, 3(Feb):1137–1155.\nYonatan Bisk, Rowan Zellers, Ronan LeBras,\nJianfeng Gao, and Yejin Choi. 2020. PIQA:\nreasoning about physical commonsense in nat-\nural language. In The Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2020,\nThe Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020,\nThe Tenth AAAI Symposium on Educational Ad-\nvances in Artiﬁcial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020, pages\n7432–7439. AAAI Press.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap,\nChaitanya Malaviya, Asli Çelikyilmaz, and\nYejin Choi. 2019. COMET: commonsense\ntransformers for automatic knowledge graph\nconstruction. In Proceedings of the 57th Con-\nference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers ,\npages 4762–4779. Association for Computa-\ntional Linguistics.\nZied Bouraoui, Jose Camacho-Collados, and\nSteven Schockaert. 2020. Inducing relational\nknowledge from BERT. In Thirty-Fourth AAAI\nConference on Artiﬁcial Intelligence (AAAI) ,\nNew York, USA.\nTom B. Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Win-\nter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. CoRR, abs/2005.14165.\nDanqi Chen, Adam Fisch, Jason Weston, and An-\ntoine Bordes. 2017. Reading wikipedia to an-\nswer open-domain questions. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2017, Van-\ncouver, Canada, July 30 - August 4, Volume 1:\nLong Papers, pages 1870–1879. Association for\nComputational Linguistics.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost:\nA scalable tree boosting system. In Proceed-\nings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data\nMining, San Francisco, CA, USA, August 13-17,\n2016, pages 785–794. ACM.\nKenneth Ward Church. 1988. A stochastic parts\nprogram and noun phrase parser for unrestricted\ntext. In Second Conference on Applied Natural\nLanguage Processing, pages 136–143, Austin,\nTexas, USA. Association for Computational\nLinguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar\nKhot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. 2018. Think you have\nsolved question answering? try arc, the AI2 rea-\nsoning challenge. CoRR, abs/1803.05457.\nPradeep Dasigi, Nelson F. Liu, Ana Maraso-\nvic, Noah A. Smith, and Matt Gardner. 2019.\nQuoref: A reading comprehension dataset with\nquestions requiring coreferential reasoning. In\nProceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Process-\ning and the 9th International Joint Conference\non Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November\n3-7, 2019 , pages 5924–5931. Association for\nComputational Linguistics.\nShrey Desai and Greg Durrett. 2020. Cali-\nbration of pre-trained transformers. CoRR,\nabs/2003.07892.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 4171–4186,\nMinneapolis, Minnesota. Association for Com-\nputational Linguistics.\nLi Dong, Chris Quirk, and Mirella Lapata. 2018.\nConﬁdence modeling for neural semantic pars-\ning. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers , pages\n743–753. Association for Computational Lin-\nguistics.\nMor Geva, Ankit Gupta, and Jonathan Berant.\n2020. Injecting numerical reasoning skills into\nlanguage models. In Proceedings of the 58th\nAnnual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-\n10, 2020, pages 946–958. Association for Com-\nputational Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q.\nWeinberger. 2017. On calibration of modern\nneural networks. In Proceedings of the 34th\nInternational Conference on Machine Learn-\ning, ICML 2017, Sydney, NSW, Australia, 6-\n11 August 2017 , volume 70 of Proceedings of\nMachine Learning Research, pages 1321–1330.\nPMLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong\nPasupat, and Ming-Wei Chang. 2020. REALM:\nretrieval-augmented language model pre-\ntraining. CoRR, abs/2002.08909.\nTatsunori B. Hashimoto, Megha Srivastava,\nHongseok Namkoong, and Percy Liang. 2018.\nFairness without demographics in repeated loss\nminimization. In Proceedings of the 35th\nInternational Conference on Machine Learn-\ning, ICML 2018, Stockholmsmässan, Stock-\nholm, Sweden, July 10-15, 2018 , volume 80\nof Proceedings of Machine Learning Research,\npages 1934–1943. PMLR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2016. Deep residual learning for\nimage recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV , USA, June 27-30,\n2016, pages 770–778. IEEE Computer Society.\nDan Hendrycks, Collin Burns, Steven Basart,\nAndy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. 2020. Measuring mas-\nsive multitask language understanding. CoRR,\nabs/2009.03300.\nJohn Hewitt and Christopher D. Manning. 2019.\nA structural probe for ﬁnding syntax in word\nrepresentations. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pages 4129–\n4138.\nAbhyuday Jagannatha and Hong Yu. 2020. Cal-\nibrating structured output predictors for natu-\nral language processing. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 2078–2092. Association\nfor Computational Linguistics.\nXiaoqian Jiang, Melanie Osl, Jihoon Kim, and\nLucila Ohno-Machado. 2012. Calibrating pre-\ndictive model estimates to support personalized\nmedicine. J. Am. Medical Informatics Assoc. ,\n19(2):263–274.\nZhengbao Jiang, Antonios Anastasopoulos, Jun\nAraki, Haibo Ding, and Graham Neubig. 2020a.\nX-factr: Multilingual factual knowledge re-\ntrievalfrom pretrained language models.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and\nGraham Neubig. 2020b. How can we know\nwhat language models know? Transactions of\nthe Association for Computational Linguistics\n(TACL).\nAmita Kamath, Robin Jia, and Percy Liang.\n2020. Selective question answering under do-\nmain shift. In Proceedings of the 58th An-\nnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 5684–5696. Association for Com-\nputational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and\nDario Amodei. 2020. Scaling laws for neural\nlanguage models. CoRR, abs/2001.08361.\nDaniel Khashabi, Tushar Khot, Ashish Sabhar-\nwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. Uniﬁedqa: Crossing\nformat boundaries with a single QA system.\nCoRR, abs/2005.00700.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. QASC:\nA dataset for question answering via sentence\ncomposition. In The Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2020,\nThe Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020,\nThe Tenth AAAI Symposium on Educational Ad-\nvances in Artiﬁcial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020, pages\n8082–8090. AAAI Press.\nVid Kocijan, Ana-Maria Cretu, Oana-Maria\nCamburu, Yordan Yordanov, and Thomas\nLukasiewicz. 2019. A surprisingly robust trick\nfor the winograd schema challenge. In Pro-\nceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers, pages 4837–4842. Association\nfor Computational Linguistics.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang,\nJie Lyu, Tuo Zhao, and Chao Zhang. 2020. Cal-\nibrated language model ﬁne-tuning for in- and\nout-of-distribution data. In Proceedings of the\n2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020, pages 1326–1340.\nAssociation for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming\nYang, and Eduard H. Hovy. 2017. RACE: large-\nscale reading comprehension dataset from ex-\naminations. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2017, Copenhagen,\nDenmark, September 9-11, 2017 , pages 785–\n794. Association for Computational Linguis-\ntics.\nY . Lecun, L. Bottou, Y . Bengio, and P. Haffner.\n1998. Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE ,\n86(11):2278–2324.\nKimin Lee, Honglak Lee, Kibok Lee, and Jin-\nwoo Shin. 2018. Training conﬁdence-calibrated\nclassiﬁers for detecting out-of-distribution sam-\nples. In 6th International Conference on Learn-\ning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Confer-\nence Track Proceedings. OpenReview.net.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh,\nArmen Aghajanyan, Sida Wang, and Luke\nZettlemoyer. 2020a. Pre-training via paraphras-\ning. CoRR, abs/2006.15020.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020b. BART: denoising sequence-to-sequence\npre-training for natural language generation,\ntranslation, and comprehension. In Proceed-\nings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 7871–7880. As-\nsociation for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-\ntau Yih, Tim Rocktäschel, Sebastian Riedel,\nand Douwe Kiela. 2020c. Retrieval-augmented\ngeneration for knowledge-intensive NLP tasks.\nCoRR, abs/2005.11401.\nKevin Lin, Oyvind Tafjord, Peter Clark, and Matt\nGardner. 2019. Reasoning over paragraph ef-\nfects in situations. In Proceedings of the\n2nd Workshop on Machine Reading for Ques-\ntion Answering, MRQA@EMNLP 2019, Hong\nKong, China, November 4, 2019 , pages 58–62.\nAssociation for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and\nAshish Sabharwal. 2018. Can a suit of armor\nconduct electricity? A new dataset for open\nbook question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Nat-\nural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018 , pages 2381–\n2391. Association for Computational Linguis-\ntics.\nKenton Murray and David Chiang. 2018. Cor-\nrecting length bias in neural machine transla-\ntion. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages\n212–223, Brussels, Belgium. Association for\nComputational Linguistics.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Face-\nbook fair’s WMT19 news translation task sub-\nmission. In Proceedings of the Fourth Con-\nference on Machine Translation, WMT 2019,\nFlorence, Italy, August 1-2, 2019 - Volume 2:\nShared Task Papers, Day 1, pages 314–319. As-\nsociation for Computational Linguistics.\nMyle Ott, Michael Auli, David Grangier, and\nMarc’Aurelio Ranzato. 2018. Analyzing un-\ncertainty in neural machine translation. arXiv\npreprint arXiv:1803.00047.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra\nPiktus, Tim Rocktäschel, Yuxiang Wu, Alexan-\nder H. Miller, and Sebastian Riedel. 2020a.\nHow context affects language models’ factual\npredictions. CoRR, abs/2005.04611.\nFabio Petroni, Aleksandra Piktus, Angela Fan,\nPatrick Lewis, Majid Yazdani, Nicola De\nCao, James Thorne, Yacine Jernite, Vas-\nsilis Plachouras, Tim Rocktäschel, and Se-\nbastian Riedel. 2020b. Kilt: a benchmark\nfor knowledge intensive language tasks. In\narXiv:2009.02252.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 2463–2473,\nHong Kong, China. Association for Computa-\ntional Linguistics.\nJohn Platt et al. 1999. Probabilistic outputs for\nsupport vector machines and comparisons to\nregularized likelihood methods. Advances in\nlarge margin classiﬁers, 10(3):61–74.\nNina Poerner, Ulli Waltinger, and Hinrich\nSchütze. 2019. E-bert: Efﬁcient-yet-\neffective entity embeddings for bert. CoRR,\nabs/1911.03681.\nKatyanna Quach. 2020. Researchers made an\nOpenAI GPT-3 medical chatbot as an experi-\nment. it told a mock patient to kill themselves.\nThe Register.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI Blog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n2019. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. CoRR,\nabs/1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang.\n2018. Know what you don’t know: Unan-\nswerable questions for squad. In Proceedings\nof the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 784–789. Association for\nComputational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. 2016. Squad: 100, 000+\nquestions for machine comprehension of text.\nIn Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Process-\ning, EMNLP 2016, Austin, Texas, USA, Novem-\nber 1-4, 2016, pages 2383–2392. The Associa-\ntion for Computational Linguistics.\nMatthew Richardson, Christopher J. C. Burges,\nand Erin Renshaw. 2013. Mctest: A challenge\ndataset for the open-domain machine compre-\nhension of text. In Proceedings of the 2013\nConference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2013, 18-21 Oc-\ntober 2013, Grand Hyatt Seattle, Seattle, Wash-\nington, USA, A meeting of SIGDAT, a Spe-\ncial Interest Group of the ACL, pages 193–203.\nACL.\nAdam Roberts, Colin Raffel, and Noam Shazeer.\n2020. How much knowledge can you pack into\nthe parameters of a language model? CoRR,\nabs/2002.08910.\nRachel Rudinger, Jason Naradowsky, Brian\nLeonard, and Benjamin Van Durme. 2018.\nGender bias in coreference resolution. In\nProceedings of the 2018 Conference of the\nNorth American Chapter of the Association\nfor Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume\n2 (Short Papers) , pages 8–14. Association for\nComputational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bha-\ngavatula, and Yejin Choi. 2020. Winogrande:\nAn adversarial winograd schema challenge at\nscale. In The Thirty-Fourth AAAI Confer-\nence on Artiﬁcial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Arti-\nﬁcial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Ad-\nvances in Artiﬁcial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020, pages\n8732–8740. AAAI Press.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ro-\nnan Le Bras, and Yejin Choi. 2019. Social\niqa: Commonsense reasoning about social in-\nteractions. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International\nJoint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019, pages 4462–4472.\nAssociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019a. olmpics - on what\nlanguage model pre-training captures. CoRR,\nabs/1912.13283.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie,\nand Jonathan Berant. 2019b. Commonsenseqa:\nA question answering challenge targeting com-\nmonsense knowledge. In Proceedings of\nthe 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4149–4158. Association for Com-\nputational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nIn Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019,\nVolume 1: Long Papers, pages 4593–4601.\nTrieu H. Trinh and Quoc V . Le. 2018. A sim-\nple method for commonsense reasoning. CoRR,\nabs/1806.02847.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin\nHarris, Alessandro Sordoni, Philip Bachman,\nand Kaheer Suleman. 2017. Newsqa: A ma-\nchine comprehension dataset. In Proceedings\nof the 2nd Workshop on Representation Learn-\ning for NLP , Rep4NLP@ACL 2017, Vancouver,\nCanada, August 3, 2017, pages 191–200. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in Neu-\nral Information Processing Systems 30: An-\nnual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017,\nLong Beach, CA, USA, pages 5998–6008.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer\nSingh, and Matt Gardner. 2019. Do NLP mod-\nels know numbers? probing numeracy in em-\nbeddings. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International\nJoint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019, pages 5306–5314.\nAssociation for Computational Linguistics.\nYunfeng Zhang, Q. Vera Liao, and Rachel K. E.\nBellamy. 2020. Effect of conﬁdence and ex-\nplanation on accuracy and trust calibration in\nai-assisted decision making. In FAT* ’20:\nConference on Fairness, Accountability, and\nTransparency, Barcelona, Spain, January 27-\n30, 2020, pages 295–305. ACM.\nPei Zhou, Rahul Khanna, Bill Yuchen Lin, Daniel\nHo, Xiang Ren, and Jay Pujara. 2020. Can\nBERT reason? logically equivalent probes for\nevaluating the inference capabilities of language\nmodels. CoRR, abs/2005.00782."
}