{
  "title": "Improving protein succinylation sites prediction using embeddings from protein language model",
  "url": "https://openalex.org/W4303645584",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2903365011",
      "name": "Suresh Pokharel",
      "affiliations": [
        "Michigan Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A4286205576",
      "name": "Pawel Pratyush",
      "affiliations": [
        "Michigan Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2098230579",
      "name": "Robert H. Newman",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "North Carolina Agricultural and Technical State University"
      ]
    },
    {
      "id": "https://openalex.org/A1910772782",
      "name": "Dukka B Kc",
      "affiliations": [
        "Michigan Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2903365011",
      "name": "Suresh Pokharel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286205576",
      "name": "Pawel Pratyush",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2897083495",
      "name": "Michael Heinzinger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098230579",
      "name": "Robert H. Newman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1910772782",
      "name": "Dukka B Kc",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3152110994",
    "https://openalex.org/W2770715722",
    "https://openalex.org/W1969899640",
    "https://openalex.org/W2941046634",
    "https://openalex.org/W4285093764",
    "https://openalex.org/W2114836779",
    "https://openalex.org/W2253486751",
    "https://openalex.org/W2914626772",
    "https://openalex.org/W2806089455",
    "https://openalex.org/W2397556949",
    "https://openalex.org/W2209329607",
    "https://openalex.org/W2278741011",
    "https://openalex.org/W3171942304",
    "https://openalex.org/W3082590494",
    "https://openalex.org/W2207892701",
    "https://openalex.org/W2611495100",
    "https://openalex.org/W2790664092",
    "https://openalex.org/W2897458141",
    "https://openalex.org/W4285159613",
    "https://openalex.org/W2986392908",
    "https://openalex.org/W3020537178",
    "https://openalex.org/W4282984452",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W2953008890",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3202848468",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W2739999456",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W2810675213",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1964357740",
    "https://openalex.org/W2070493638",
    "https://openalex.org/W3049692992",
    "https://openalex.org/W28412257",
    "https://openalex.org/W2747278265",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W2971227267"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports\nImproving protein succinylation \nsites prediction using embeddings \nfrom protein language model\nSuresh Pokharel1, Pawel Pratyush1, Michael Heinzinger2,3, Robert H. Newman4,5 & \nDukka B. KC1*\nProtein succinylation is an important post-translational modification (PTM) responsible for many vital \nmetabolic activities in cells, including cellular respiration, regulation, and repair. Here, we present \na novel approach that combines features from supervised word embedding with embedding from \na protein language model called ProtT5-XL-UniRef50 (hereafter termed, ProtT5) in a deep learning \nframework to predict protein succinylation sites. To our knowledge, this is one of the first attempts \nto employ embedding from a pre-trained protein language model to predict protein succinylation \nsites. The proposed model, dubbed LMSuccSite, achieves state-of-the-art results compared to \nexisting methods, with performance scores of 0.36, 0.79, 0.79 for MCC, sensitivity, and specificity, \nrespectively. LMSuccSite is likely to serve as a valuable resource for exploration of succinylation and \nits role in cellular physiology and disease.\nAbbreviations\nPTM  Post-translational modification\nLMs  Language models\npLMs  Protein language models\nDNA  Deoxyribonucleic acid\nCoA  Coenzyme\nMS  Mass spectrometry\nSCX  Strong cation exchange (SCX)\nCNN  Convolutional neural network\nNLP  Natural language processing\nBFD  Big fantastic database\nT5  Text-to-text transfer transformer\nML  Machine learning\nDL  Deep learning\nANN  Artificial neural network\nNN  Neural network\n2D  Two dimensional\nReLU  Rectified linear unit\nTP  True positive\nTN  True negative\nFP  False positive\nFN  False negative\nACC   Accuracy\nMCC  Matthew’s correlation coefficient\nSp  Specificity\nSn  Sensitivity\nAUROC  Area under receiver operating characteristics\nOPEN\n1Department of Computer Science, Michigan Technological University, Houghton, MI, USA. 2Department \nof Informatics, Bioinformatics and Computational Biology - i12, TUM (Technical University of Munich), \nBoltzmannstr. 3, 85748 Garching/Munich, Germany. 3Center of Doctoral Studies in Informatics and Its Applications \n(CeDoSIA), TUM Graduate School, Boltzmannstr. 11, 85748 Garching, Germany. 4Department of Biology, College \nof Science and Technology, North Carolina A&T State University, Greensboro, NC, USA. 5Department of Chemistry, \nUniversity of North Carolina at Chapel Hill, Chapel Hill, NC, USA. *email: dbkc@mtu.edu\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nPrAUC   Precision recall area under curve\nT-SNE  T-distributed stochastic neighbor embedding\nCNN2D  2 Dimensional CNN\nCNN1D  1 Dimensional CNN\nSVM  Support vector machine\nRF  Random forest\nXGBoost  Extreme gradient boosting\nPost-translational modifications (PTMs) are important regulators of proteins that modulate a myriad of physi-\nological and pathological processes such as signal transduction, gene expression, metabolism, DNA repair, \nand cell cycle progression among many  others1. Among more than 400 known PTMs, succinylation has emerged \nas an important PTM that has been implicated in numerous diseases such as hepatic, cardiac, pulmonary and \nneurological  disorders2. Therefore, it is important to identify succinylation sites and how they affect protein \nfunction. Indeed, a better understanding of succinylation could facilitate the development of novel therapeutic \ncompounds to treat diseases. However, comprehensive identification of succinylation sites, as well as understand-\ning of its functional impact, remains elusive.\nSuccinylation, which is among the more recently discovered PTMs, is comparatively  unique3. Like methyla-\ntion, acetylation or ubiquitination, succinylation also occurs on lysine residues. However, compared to methyla-\ntion (14 Da) or acetylation (40 Da), succinylation (100 Da) causes a larger mass change and converts a positively \ncharged side chain to a negatively charged one, causing a two-unit charge shift in the charge of the modified \n residues4. During succinylation, a metabolically derived succinyl CoA modifies protein lysine residues. It has \nbeen shown that succinylation alters the catalytic rates of enzymes and the pathways in which they are involved, \nespecially mitochondrial metabolic  pathways4. This provides an elegant mechanism to coordinate metabolism \nand signaling. Additionally, succinylation is known to provide a link between metabolism and protein function \nin the nervous system and in neurological diseases, rendering deeper understanding of its mechanism highly \n interesting4. Likewise, succinylation has also been shown to be substantially upregulated in the early phases \nof infection by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), suggesting that succinylation \ninhibitors may be able to reduce viral replication to treat COVID-195. Interestingly, succinylation is observed in \ndiverse organisms, including bacteria, yeast, and mouse and these sites are also frequent targets of acetylation \n(another important PTM)6. Given its widespread occurrence, and its implication in many diseases, characteriza-\ntion of succinylation is important for drug discovery.\nDue to its widespread occurrence and its disease relevancy, various experimental techniques have been uti -\nlized to detect protein succinylation. Mass spectrometry (MS) has been one of the most popular experimental \ntechniques for identifying PTMs, including  succinylation3,7–9. For instance, Weinert et al.6 developed an antibody-\nbased affinity enrichment of succinylated peptides followed by strong cation exchange (SCX) chromatography \nand liquid chromatography tandem MS (LC–MS/MS) to characterize succinylation sites.\nSimilarly, to bridge the gap between relatively sparse experimental data and the wealth of protein sequences \nin current databases, a plethora of computational methods have been developed to predict protein succinyla-\ntion sites over the last decade. For instance, various machine learning-based methods like iSuc-PseAAC 10, \niSuc-PseOp11, pSuc-Lys12, MDCAN-Lys13,  HybridSucc14 have been proposed for succinylation site prediction. \nLikewise, a random forest (RF)-based approach called  SuccinSite15, a position specific scoring matrix (PSSM)-\nbased predictor called PSSM-Suc 16, a secondary structure and PSSM-based approach called SSEvol-Suc 17, and \na logistics-based approach called  GPSuc18 have been developed.\nRecently, deep learning-based approaches have also been developed for prediction of protein succinyla -\ntion  sites19. CNN-SuccSite20 uses four feature encoding techniques as input to a convolutional neural network \n(CNN)-based architecture to predict succinylation sites. We also developed a deep learning-based approach that \ndoes not require hand-crafted feature extraction but instead uses techniques from natural language process-\ning (NLP), such as supervised word embedding, to extract vector representations (embeddings) directly from \nprotein sequences. Together with one-hot encoding, those embeddings are used as input to a CNN-architecture \nin  DeepSuccinylSite21. Similarly, MDCAN-Lys13 uses a multilane dense convolutional attention network to pre-\ndict succinylation sites.\nRecently, language models (LMs) have emerged as a powerful paradigm to learn embeddings directly from \nlarge, unlabeled natural language datasets. This is achieved, for example, by reconstructing corrupted tokens \nwithin a sequence of tokens from the remaining, non-corrupted tokens within the same sentence. In contrast to \nuncontextualized word embeddings which will always return the same embedding for a word irrespective of the \nsurrounding words, embeddings from LMs are contextualized in that they render the embedding dependent on \nthe surrounding words. These advances are now being explored in proteins as well as through protein language \nmodels (pLMs)22. These deep language models are an exciting breakthrough in protein sequence modeling as \nthey were shown to capture complex dependencies between protein  residues23 based solely on training using \nlarge, but unlabeled, protein sequence  databases24,25 (rather than sequences belonging to a specific protein family \nor task). Since large protein sequence databases represent vast data goldmines, recently various pLMs have been \ndeveloped for distilling information from  them26. This information can be transferred to other tasks, for example, \nby extracting the hidden states of the last layer of the pLM and using it as input to predict some protein proper-\nties. These  pLMs27–31 were shown to learn features that can be used to better capture sequence relationships.\nSimilar to LMs, pLMs are usually trained by masking some parts (usually single amino acids) of the input \nprotein sequence and reconstructing it from non-corrupted sequence context. Instead of using the final classifica-\ntion output during inference, LMs and pLMs use the output of the last hidden layers of the network as a means of \nrepresenting a protein sequence as numerical vectors called embeddings. This allows information gathered from \n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nlarge but unlabeled protein sequence databases to be transferred to much smaller but labeled sequence data sets \n(via transfer learning). In order to distinguish the different types of embeddings used in this work, we dub one \nstrategy “supervised word embedding” and the other “ProtT5” 32. These embeddings have been used in various \nstructural bioinformatics applications, including the prediction of secondary structure, subcellular localization, \nbinding  residues33 variant effects, or the identification of remote  homologs22,29,31,34.\nIn this work, we utilize the embeddings derived from pLM ProtT5 (based on the NLP sequence-to-sequence \nmodel  T535 but trained on Big Fantastic Database (BFD) and fine-tuned on  Uniref5036 instead of natural lan-\nguage) in conjunction with supervised word embedding to improve the prediction of succinylation sites in \nproteins. The proposed deep-learning approach, called LMSuccSite, combines supervised word embeddings \nwith embeddings from ProtT5 using a simple neural network architecture. This strategy achieves or exceeds \nstate-of-the-art performance in the benchmark dataset. To the best of our knowledge, this is the first work to use \npLM-based embeddings for the prediction of succinylation sites specifically, and PTMs, in general.\nDataset and methods\nDataset. We used the dataset used during the development of  DeepSuccinylSite21 to train and test our \napproach. This dataset consists of experimentally verified succinylation sites provided by Hasan et. al.15 that was \noriginally obtained from the UniProtKB/Swiss-Prot37 database and the NCBI protein sequence database. First, \nthese sequences were obtained and subjected to redundancy removal using a CD-hit 38 algorithm with a cut-off  \nsimilarity of 0.3 to any other proteins in the dataset. This resulted in 5009 succinylated sites from 2322 protein \nsequences. Subsequently, the sequences were randomly separated into a training set consisting of 2192 protein \nsequences and a testing set consisting of 124 proteins. The training and test set sequences contain 4755 and \n254 succinylation sites, respectively. It should be noted that the input to the ProtT5 is the full protein sequence. \nHowever, input to the supervised word embedding is a window sequence created around the central residues \n(positive or negative sites) by flanking it with an equal number of residues to the left and the right.\nSince five of the positive succinylation sites were around the N- or C-termini of the protein, we were unable \nto extract a window of size 33 (corresponding to the optimal window size obtained during the development \nof  DeepSuccinylSite21) around those sites. For this reason, we excluded these five sites, resulting in 4750 suc-\ncinylation sites. After fixing positive training and testing sequences, negative sets were then extracted from the \nsame protein sequences. Specifically, all other lysine (K) residues from the same protein sequences that were \nnot annotated as succinylated (i.e., positive sites) were considered negative succinylation sites. This resulted in \n50,565 negative succinylation sites in the training set and 2977 negative succinylation sites in the test set. To deal \nwith the imbalanced training set, we performed random under sampling on the negative training set to obtain \nthe same number of negative sites (4750) as the number of positive sites in the training set. The independent \ntest set was kept as it is. The final number of sites in this dataset is shown in Table  1. The dataset is provided in \nthe GitHub repository at https:// github. com/ KCLab MTU/ LMSuc cSite. It is worth noting that some of the exist-\ning approaches use an imbalanced training set and use threshold-moving to handle the imbalanced  dataset18,39.\nFeature encoding. Since ML/DL models are only capable of understanding inputs in numerical space, it \nis imperative to convert the protein sequences into vectorized representations (i.e., feature vectors). In fact, the \nquality of these features is directly proportional to the robustness of the predictive models. To address this, we \nleveraged two encoding approaches to identify the best representation of succinylation sites.\nSimilar to  DeepSuccinylSite 21, the first approach uses the representation based on the local interaction of \namino acids in the vicinity of K residues, which is achieved by a supervised word embedding (obtained via \nKeras’s embedding layer)21. In this strategy, the features take into consideration the influence of upstream and \ndownstream flanking residues within the window sequence (or peptide) centered around the site of interest. The \nother encoding approach employs a pLM-based transformer model called  ProtT532 that extracts a contextual -\nized representation (aka embeddings) of the site of interest (i.e., K residues) from full protein sequences. We \nonly extracted embeddings from the encoder-side of ProtT5 in half-precision as it was shown in previous work \nthat this outperforms embeddings of ProtT5’s decoder-side. Please note that both methods directly take protein \nsequences as an input (supervised word embedding takes a window sequence, and ProtT5 takes the full protein \nsequence), eliminating the requirement for handcrafted or manual extraction of features. Each feature encoding \nis described below in detail.\nSupervised word embedding. The first type of feature we used in this work attempts to capture the local infor -\nmation that is extracted using supervised word embedding obtained using Keras’s embedding layer. The embed-\nding encoding used in this work is similar to the embedding encoding in  DeepSuccinylSite21. Essentially, a win-\ndow sequence centered around the site of interest (i.e., a K residue) with an equal number of residues upstream \nand downstream was taken as an input. Based on a comparison of various window sizes (corresponding to odd \nTable 1.  Dataset description of the training and independent test dataset.\nDataset type Positive (succinylated) Negative (non-succinylated)\nTraining data 4750 50,565\nTraining data (after balancing) 4750 4750\nBenchmark independent test data 254 2977\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nnumbers ranging from 11 to 41) performed on a base architecture using tenfold cross-validation, a window size \nof 33 was chosen for subsequent development. The results of experiments performed in order to determine the \noptimal window size is given in Section C of the Supplementary Materials (Supplementary Figure C1 and Sup-\nplementary Table C1). This window size is similar to the optimal window size identified during the development \nof our previous predictor,  DeepSuccinylSite21.\nThis supervised word embedding, which is obtained via Keras’s embedding layer, is able to capture the local \ninformation between amino acids within a fixed-sized window. Embedding layers in Keras work by treating \npeptides as documents and individual amino acids within that peptide as words. Initially, each amino acid in a \npeptide is represented by a unique integer and the embedding layer is initialized with random weights. Hence, \na peptide of length n  will be represented by a vector of length n . The output is a dense vector of dimension n \nx m where m is the size of the predefined vocabulary. The output vector is dynamically updated in subsequent \nepochs during training in a backpropagation fashion. This supervised learning nature of embedding from a set of \nprotein sequences allows the network to learn the semantic similarity of amino acids within the embedded vector \nspace. Furthermore, each vectorized representation is an orthogonal representation in some other dimension, \nthus, preserving the semantic quality of individual amino acids. In practice, the shape of the output vector is a \nparameter to be set before training. We set the size of the vocabulary to 21 as in  DeepSuccinylSite21 to address \n20 canonical amino acids found in proteins, along with a virtual amino acid. Therefore, for a given peptide of \nlength 33, supervised word embedding returns a dense vector of shape (33,21).\nProtT5 encoding. Transfer learning is a promising machine learning methodology that transfers the knowl-\nedge learned in data-rich problems to similar data-limited problems. Protein language models learn summary \nrepresentation that can be used to distill the knowledge from a large dataset, which can be used to improve \ndownstream function prediction through transfer learning. The second type of feature that we use in this work \nis based on a pLM that captures global contextual information. Unlike supervised word embedding, the inputs \nto these models are full-length protein sequences (i.e., no window), generating embeddings for all residues in \na full-length protein sequence. The pLM we utilized is called  ProtT532. Leveraging teacher-forcing and span-gen-\neration, ProtT5, is pre-trained in a self-supervised manner on a massive dataset called  UniRef5036 that consists \nof 45 ×  106 protein sequences. Specifically, ProtT5 was trained by teacher forcing, i.e., input and targets were fed \nto the model with inputs being corrupted protein sequences and targets being identical to inputs but shifted to \nthe right (span generation with span size of 1 for ProtT5). It is based on Google’s t5-3b  model35, which consists \nof a 24 layer encoder-decoder and has around 2.8 ×  109 learnable parameters.\nIn our work, we used the pretrained  ProtT532 model to encode the features. This model takes the overall \nprotein sequence as an input and returns an embedding vector of dimension 1024 for each amino acid. Notably, \nthe ProtT5 is a context-dependent encoding approach. Hence, for succinylation site prediction, using ProtT5, \nwe utilized an embedding vector (length 1024) corresponding to the site of interest (in our case K residue).\nMachine learning model and deep learning models. Below, we define briefly the machine learning \nand deep learning models used in the study.\nRandom Forest (RF)40 classifier is an ensemble method consisting of a number of decision trees. The decision \ntrees independently make the decision and the class predicted with the most votes is considered the final output \nof the Random Forest algorithm.\nSupport Vector Machines(SVM)41 is a supervised learning algorithm commonly used for both classification \nand regression problems. The SVM algorithm maps the data into some higher dimensional feature space such that \nhyperplanes can be constructed to separate the classes. It uses a kernel function to transform the input data into \na higher dimension. The objective is to maximize the margin between the support vectors and the hyperplane.\nExtreme Gradient Boosting (XGBoost) 42 is an optimized version of Gradient Boosting that uses a decision \ntree-based ensemble method to solve classification and regression problems. It uses a gradient descent algorithm \nas the cost function with parallel processing, tree pruning, and regularization techniques.\nCNN1D (1-Dimensional Convolutional Neural Network) is a variant of CNN that performs the convo -\nlution operation along one dimension. CNN1D is mostly used for text and 1D signal data. In this work, we \nimplemented a CNN1D architecture along with max-pooling and dropout layers and trained on ProtT5-based \nfeatures. CNN2D (2-Dimensional Convolutional Neural Network) uses a 2D convolution layer, where a 2D \nkernel convolves over a two-dimensional feature space similar to images. A long short-term memory (LSTM) \nis a variant of artificial neural network feedback connections. LSTM is commonly used for classifying or mak-\ning predictions over time-series data. We used Tensorflow’s Keras API to implement these deep learning-based \nmodels (i.e., ANN, CNN1D, CNN2D, LSTM). The hyperparameters and other details are explained in section \nB (Supplementary Tables B1, B2, B3, B4, B5).\nModel architecture. Our proposed ensemble deep learning model, which we termed LMSuccSite, is \ndesigned to capture knowledge from the supervised word embedding (herein referred to as the embedding \nmodule) using the dataset for succinylation and unsupervised language models learned from a large dataset \nof proteins (obtained via ProtT5). Initially, a 2D-CNN-based architecture was used for supervised embedding \nfeatures while an artificial neural network (ANN)-based module was used for ProtT5 features. Eventually, a \nmeta-classifier based on an ANN using feature sets generated from the outputs of the individual classifiers was \ntrained. Below, we describe the model architecture in detail.\nEmbedding module. The input to the embedding module is the output of the supervised word embedding, \nwhich is a dense vector of shape with dimensions of 33 and 21, where 33 represents the window size and 21 \n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nrepresents the size of the vocabulary. This module consists of a 2D convolutional layer, a 2D max-pooling layer, \nand a fully connected layer (consisting of a flatten layer, a dense layer, and an output layer). Two dropout layers \nwere added to the network to avoid overfitting. Moreover, a rectified linear unit (ReLU) was used as an activa-\ntion function in all the layers due to its representational sparsity and computational efficiency. The architecture \nof the embedding module is shown in Fig. 1. The optimization of parameters in the network was achieved using \nthe Adam optimizer due to its combined benefits with respect to both adaptive gradient descent and root mean \nsquare propagation. Binary cross-entropy or log loss was used as the loss function for training. All the optimal \nhyperparameters used in the module are reported in Supplementary Table B3. This architecture is chosen based \non tenfold cross-validation on the training set using different architectures with different combinations of hyper-\nparameters using grid search.\nProtT5 module. This module takes global contextual features extracted from the pLM ProtT5 for the residue of \ninterest (i.e., K residue) as the input. The size dimension of the feature is 1024, as explained above. The ProtT5 \nmodule is based on an ANN architecture that consists of two hidden layers with sizes 256 and 128, each followed \nby a dropout layer. The architecture of ProtT5 based model is shown in Fig. 2. This architecture was chosen based \non tenfold cross-validation on the training set using different machine learning and deep learning architectures \nwith different combinations of hyperparameters using grid-search. The ReLU activation was used for both lay-\ners and the model was optimized using Adam based on binary cross-entropy loss. The parameters associated \nwith this model are described in Supplementary Table S3 in supplementary materials. Similar to the work of \nVillegas-Morcello et al.43 and Weissenow et al.23, we also observed that these pLM-based features do not require \na complex architecture to obtain competitive performance.\nMeta‑classifier. In order to combine the classifying capability of two different techniques, the embedding mod-\nule and Prot-T5 module were combined using an ANN as a meta-classifier. Rather than using the final output \nof each individual module, we combined the learned features from the second-to-last layers from both modules \n(i.e., the last hidden layer in each module). During training, we paid special attention to the training process in \norder to avoid data leakage. To find the optimal architecture for the meta classifier, we performed tenfold cross-\nvalidation on the training set, ensuring that no data leakage occurred from the target information to the training \nset. In a classical stacked  generalization44 methodology, the meta classifier might only learn from the predictions \nof the individual (base) models resulting in data leakage and over estimation of classification  performance45.\nInitially, all the layers of these two base modules (i.e., the embedding and ProtT5 modules) were frozen and \nthe resulting features were obtained by concatenating the output (meta-feature) of the second-last layers from \nboth modules. Importantly, the input size of the meta-classifier was 144 (16 from the embedding and 128 from \nthe ProtT5 module). This meta-classifier is based on a simple feed forward neural network (NN) architecture. \nThis architecture was chosen based on tenfold cross-validation experiments with different combinations of \nhyperparameters (e.g., number of hidden layers, number of neurons in each layer, regularization parameters) \nusing grid-search. The hyperparameters used in the meta-classifier are provided in Supplementary Table S4. \nFurthermore, underfitting and overfitting in each module were carefully prevented by using early stopping.\nThe final architecture of the meta classifier is simple, consisting of two hidden layers with ReLU activation and \nan output layer with softmax activation, as shown in Fig. 3. Similar to previous base modules, the meta classifier \nwas also optimized using Adam based on binary cross-entropy loss.\nPerformance evaluation. To evaluate the performance of the aforementioned models, we used the stand-\nard confusion matrix for binary classification that consists of four components: True Positives (TP), which rep-\nresent the number of positive sites predicted as succinylated sites; True Negatives (TN), which correspond to the \nFigure 1.  Architecture of supervised word embedding based model using a convolutional neural network.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nnumber of negative sites predicted as non-succinylated sites; False Positives (FP), which represent the number \nof negative sites predicted as succinylated sites; and False Negatives (FN), which describe the number of posi-\ntive sites predicted as non-succinylated sites. Using these four components of the confusion matrix, evaluation \nmetrics such as Accuracy (ACC), Matthew’s Correlation Coefficient (MCC), Sensitivity (Sn), Specificity (Sp), \nand geometric mean (g-mean) were calculated for each experiment. The detailed description of the performance \nmetrics and equations are provided in Supplementary Table A1. We also used area under the receiver operat-\ning characteristic (AUROC) curve and Precision-Recall area under the curve (PrAUC) to further evaluate the \ndiscriminating ability of the models.\nFigure 2.  Architecture of Prot-T5 based model using a two-layer artificial neural network.\nFigure 3.  Model architecture of ensemble mechanism and meta classifier.\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nT-distributed stochastic neighbor embedding. In order to further investigate the proposed models, \nwe performed t-distributed stochastic neighbor embedding (t-SNE) 46 visualization of learned features and the \nablation study.\nt‑SNE visualization of learned features. The feature vector obtained from the final hidden layer can be projected \ninto lower-dimensional latent cartesian space to visualize information available in high-dimensional space. \nTo accomplish this, proteins or residues were colored according to a particular label. The clearer the distinction \nbetween the classes, the more readily available certain information is from the representation of sequences. In \nthis regard, we utilized a t-SNE algorithm (with a perplexity of 30 and learning rate of 100) that can capture non-\nlinear signals within the data robustly, improving the visible boundary of separation. At first, the features gener-\nated from the embedding module and the ProtT5 module were visualized in 2-dimensional scatter plot using \nt-SNE to elucidate their respective boundary of separation between succinylated sites and non-succinylated sites. \nThen, the features learned from the ensemble model were projected onto a t-SNE to investigate if there was an \nimprovement in the visible boundary of separation, which would indicate the usefulness of features obtained \nfrom the pLM.\nSensitivity analysis with respect to data size (Ablation study). Sensitivity analysis is an intuitive technique to \nquantify the performance of a model under different inputs or varying environments. It can be performed by \nusing various what-if analyses that tell how the changing input or other configuration affects the outcome. In \nthis study, we analyzed the trend of model performance when the size of the training data gradually increases.\nResults\nIn this section, we describe the development and evaluation of LMSuccSite, a method that uses embedding from \na pLM to predict protein succinylation sites in proteins using only the primary amino acid sequence as input. \nLMSuccSite combines two modules, one based on supervised word embedding and another based on a ProtT5 \nLM using a meta-classifier. First, we compare the performance of various DL/ML architectures for ProtT5 and \nsupervised word embedding models using cross-validation techniques to find the best performing model. Sub-\nsequently, we compare the performance of various meta-classifiers for the stacked model using cross-validation \nto find the best meta-classifier. Finally, we compare the performance of our approach (LMSuccSite) with existing \nsuccinylation site prediction tools using an independent test set. The details are given in the following subsections.\nTraining and evaluation of embedding module, ProtT5 module and LMSuccSite. As discussed \nin the method section, we performed tenfold cross-validation using the training set on the embedding and \nProt-T5 modules for various ML/DL architectures (Table 2). Since the relative performance of various DL and \nML based models for embedding has already been compared during the development of our previous model, \n DeepSuccinylSite21, we chose to utilize a similar architecture based on CNN2D. For the ProtT5 module, we \nimplemented RF , support vector machines (SVM), XGBoost, CNN1D, and ANN architectures. As can be seen \nin Table 2, the CNN2D and ANN architectures exhibit the best performance in terms of the embedding and \nProtT5 modules, respectively. Since extensive validation of various ML and DL models was already performed \nduring the development of  DeepSuccinylSite21, we only provided the results of CNN2D and LSTM models for \nsupervised word embedding.\nBased on these results, a CNN2D-based architecture performed the best in the category of supervised word \nembedding while ANN performed the best in the category of embedding from ProtT5. Hence, these two mod -\nules are combined using a meta-classifier. In order to determine the best architecture for the ensemble model, \nwe performed tenfold cross validation using various architectures to combine these two best individual models.\nWe also used a class weight method for cost-sensitive learning for both Embedding and ProtT5 encoding \nschemes to train on an imbalanced dataset that includes all available positive and negative sites. Since the results \nof the experiments on an imbalanced dataset were not on par with the results using the balanced dataset, we \nused the balanced dataset in our subsequent experiments. The performance of different models trained on the \nimbalanced dataset using the class weighting method is presented in Table D1 (Supplementary Materials).\nTable 2.  10-Fold cross-validation results on the training set of Embedding module, ProtT5 module with \ndifferent ML and DL models. The highest values in each category are bolded.\nEncoding approach Architecture ACC MCC Sn Sp\nEmbedding\nCNN2D 0.73 ± 0.02 0.47 ± 0.05 0.76 ± 0.01 0.70 ± 0.01\nLSTM 0.71 ± 0.01 0.43 ± 0.02 0.77 ± 0.04 0.66 ± 0.03\nProtT5\nRF 0.62 ± 0.01 0.25 ± 0.01 0.59 ± 0.01 0.65 ± 0.01\nSVM 0.73 ± 0.01 0.46 ± 0.01 0.75 ± 0.02 0.71 ± 0.01\nXGBoost 0.70 ± 0.01 0.41 ± 0.01 0.76 ± 0.01 0.65 ± 0.01\nCNN1D 0.69 ± 0.01 0.38 ± 0.03 0.78 ± 0.08 0.59 ± 0.09\nANN 0.74 ± 0.01 0.47 ± 0.02 0.76 ± 0.02 0.71 ± 0.02\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nAs discussed in the Methods section, the hyperparameters of the final stacked generalization model is \nobtained using grid search. The tenfold cross-validation results using various architectures for the best model \n(in terms of MCC) is shown in Table 3.\nWe also compared the performance of these various models using the ROC curve for the tenfold cross-\nvalidation. The ROC and Pr-Auc curves of the embedding-based CNN2D model, different ProtT5-based models, \nand the combined model of ProtT5-based ANN and Embedding based 2DCNN are shown in Fig. 4. These data \nsuggest that the AUC of the ANN-based combined model is better than those of the individual models. Hence, \nthis model was chosen as the final model, which we termed LMSuccSite. Additionally, LMSuccSite was trained \non the overall training data before being compared against other existing approaches.\nComparison of LMSuccSite with other predictors. In order to compare the performance of LMSuc-\ncSite with the existing succinylation site predictors, we used the independent test set described in Table  1 and \ncomputed parameters such as accuracy, MCC, sensitivity, specificity and g-mean (Table  4). Importantly, the \nsame training and test sets were used for all of the models tested. The results for other predictors were obtained \nfrom the respective articles.\nAs observed in Table 4, LMSuccSite exhibited the highest MCC, Sn, and g-mean scores among the methods \ntested. Moreover, though it did not achieve the highest ACC and Sp scores, LMSuccSite still performed well in \nthese areas, with ACC and Sp scores that were within 10 and 12% of the best performing methods, respectively. \nIn terms of MCC (which is often used as a measure of overall method performance because it takes into account \nboth Sn and Sp), LMSuccSite achieved a 22% increase in MCC compared to the next best succinylation site \npredictor,  GPSUc18 (0.36 vs. 0.30).\nAs the performance of some existing tools have been evaluated by fixing specificity at different values (for \nexample, the SuccinSite2.0 threshold was set at 0.9 specificity while iSuc-PseAAC set a cutoff threshold of  0.35), \nwe also calculated Acc, Sn, and MCC of LMSuccSite at specificity values of 0.85, 0.90, and 0.95. The results are \npresented in Supplementary Table D3.\nFurther analysis of LMSuccSite. To gain insights into the basis for LMSuccSite’s improved performance, \nwe then constructed t-SNE plots in  R2 space for the original data and features learned from supervised word \nembedding, ProtT5 embedding, and the LMSuccSite model (Fig. 5). In contrast to the original data, the features \nlearned from the embedded vector space show the formation of distinct clusters of succinylated (orange data \npoints) and non-succinylated (blue data points) sites. This boundary of separation was even more pronounced \nwhen features were learned from the protT5 model, indicating that contextualized features are useful. Interest-\ningly, the features learned from the ensemble model exhibited prominent distinctions between the clusters of \nsuccinylated sites and non-succinylated sites, as shown in Fig. 5.\nResults of sensitivity analysis with respect to different training data size. To explore the effects of training set size \non model performance, we conducted sensitivity analysis (Fig.  6). To this end, we created four different train-\ning datasets by randomly selecting 20%, 40%, 60%, and 80% of the samples from our training set described in \nTable 1. Subsequently, tenfold cross-validation was performed in each of these random samples using our pro-\nposed model. These studies suggest that, as the size of training data increased, our model exhibited an increasing \ntrend with respect to ACC, MCC, Sn, and Sp. It can be inferred that with the increase in the size of the training \ndata, the results of our model will further improve.\nConclusion and discussion\nSuccinylation, which is a recently discovered PTM, is garnering much interest due to the biological implications \nof introducing a relatively large (100 Da) chemical moiety that changes the charge of the modified residue. Given \nits widespread occurrence and its putative association with many diseases, including SARS-CoV-2 infection and \nthe recent COVID-19 pandemic, the characterization of succinylation sites has important implications for drug \ndiscovery of many diseases as well. In conjunction with technological advances in experimental technologies to \nidentify succinylation sites, various complementary computational approaches have been proposed for predic-\ntion of protein succinylation sites.\nThe available dataset for succinylation site is still relatively small. In the cases where the dataset is scarce, \ntransfer learning becomes very useful. In particular, language models-based approaches that learn summary \nrepresentations can be used to distill the knowledge from a large dataset and this knowledge can then be used \nTable 3.  Performance of best different ML/DL models as a meta classifier. Highest values in each category are \nin [bold].\nEncoding Model ACC MCC Sn Sp\nEmbedding + ProtT5\nSVM 0.76 ± 0.01 0.52 ± 0.02 0.80 ± 0.02 0.71 ± 0.02\nRF 0.75 ± 0.01 0.51 ± 0.02 0.79 ± 0.01 0.71 ± 0.02\nLR 0.74 ± 0.01 0.50 ± 0.03 0.78 ± 0.02 0.71 ± 0.02\nXGBoost 0.73 ± 0.02 0.46 ± 0.04 0.75 ± 0.03 0.71 ± 0.02\nANN(LMSuccSite) 0.77 ± 0.01 0.56 ± 0.02 0.80 ± 0.01 0.76 ± 0.02\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nto improve downstream prediction through transfer learning. In that regard, due to the availability of pLMs, it \nis now possible to use LMs that learn summary representation for feature extraction. In this regard, LMSuccSite \nuses embedding learned from the ProtT5 LM, which is trained on a T5 model on  UniRef5036,48 protein dataset. By \ncombining the embedding learned from a pLM with supervised word embedding using a meta-classifier on the \nexisting succinylation dataset, we obtained overall improvement in the prediction of protein succinylation sites. \nThis combination of supervised word embedding and embedding learned from pLM produced the best results, \nsuggesting that contextual information obtained from pLMs is important for the prediction of succinylation sites.\nAlthough pLMs have been used for other structural bioinformatics tasks, to the best of our knowledge this \nstrategy of using embeddings from pLMs has not been explored for PTM site prediction, in general, and protein \nsuccinylation site prediction, in particular. Our data suggest that pLM-based representations are versatile features \nthat can be used for various structural bioinformatics tasks. In addition, similar to the work of Villegas-Morcello \net al.43 and Weissenow et al.23, we also observed that there is no requirement for complex architecture for these \nLM-based features to obtain competitive performance, as evidenced by the LMSuccSite architecture.\nInterestingly, through t-SNE visualization, we found that the separation between succinylation and non-\nsuccinylation sites for the embedding learned from LMs shows clear boundaries. This boundary becomes more \nFigure 4.  ROC (Receiver Operating Characteristic) curve and PR (Precision Recall) curves with AUC (Area \nUnder Curve) for different models explained in Tables 3, 4. (a) ROC curves for supervised embedding-based \nmodels (b) PR curve for supervised embedding based models (c) ROC curves for Prot-T5 based models (d) PR \ncurves for Prot-T5 based models (e) ROC curves of different meta classifier for combined models (f) PR Curves \nof different meta classifier for combined models.\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nTable 4.  Comparison of our model with existing succinylation prediction tools using an independent test \nset. The numbers are rounded to two significant digits after the decimal. The highest value in each category is \nshown in bold. ACC  Accuracy, MCC Matthew’s Correlation Coefficient, Sn Sensitivity, Sp Specificity, g‑mean \ngeometric mean.\nTo ol ACC MCC Sn Sp g-mean\niSuc-PseAAC 10 0.83 0.01 0.12 0.89 0.33\niSuc-PseOpt11 0.72 0.04 0.30 0.76 0.48\npSuc-Lys12 0.78 0.04 0.22 0.83 0.43\nSuccineSite15 0.84 0.19 0.37 0.88 0.57\nSuccineSite2.047 0.85 0.26 0.40 0.88 0.63\nGPSuc18 0.85 0.30 0.50 0.88 0.66\nPSuccE39 0.85 0.20 0.38 0.89 0.58\nDeepSuccinylSite21 0.70 0.27 0.79 0.69 0.74\nLMSuccSite 0.79 0.36 0.79 0.79 0.79\nFigure 5.  t-SNE to visualize the high-dimensional embedding learned by different features. (a) Before training \nCNN2D Embedding module (b) after training CNN2D with Embedding (c) before training the ANN model \nusing prot-T5 features (d) after training the ANN model using prot-T5 features (e) after training the combined \nmodel. Blue dots represent non-succinylated sites and orange dots represent succinylated sites.\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\ndiscernible when the features learned from the ensemble model show a prominent distinction between the clus-\nters of succinylated sites and non-succinylated sites. Additionally, through sensitivity analysis, it can be inferred \nthat LMSuccSite’s performance is likely to improve with the availability of more training data.\nRecently, numerous LMs have been proposed for learning representation from a vast number of protein \nsequences. In that regard, comparative performance of various LMs for protein succinylation sites will be an \nimportant future work.\nData availability\nThe source code and trained models are publicly available in the GitHub repository at https:// github. com/ KCLab \nMTU/ LMSuc cSite. All the steps to extract the features and executing the programs are discussed in the same \nGitHub repository.\nReceived: 11 August 2022; Accepted: 26 September 2022\nReferences\n 1. Ramazi, S. & Zahiri, J. Post-translational modifications in proteins: resources, tools and prediction methods. Database‑Oxford  \nhttps:// doi. org/ 10. 1093/ datab ase/ baab0 12 (2021).\n 2. Alleyn, M., Breitzig, M., Lockey, R. & Kolliputi, N. The dawn of succinylation: A posttranslational modification. Am. J. Physiol. \nCell Physiol. 314, C228–C232. https:// doi. org/ 10. 1152/ ajpce ll. 00148. 2017 (2018).\n 3. Zhang, Z. et al. Identification of lysine succinylation as a new post-translational modification. Nat. Chem. Biol. 7, 58–63. https:// \ndoi. org/ 10. 1038/ nchem bio. 495 (2011).\n 4. Y ang, Y . & Gibson, G. E. Succinylation links metabolism to protein functions. Neurochem. Res. 44, 2346–2359. https:// doi. org/ 10. \n1007/ s11064- 019- 02780-x (2019).\n 5. Liu, Q. et al. The global succinylation of SARS-CoV-2–infected host cells reveals drug targets. Proc. Natl. Acad. Sci. 119, \ne2123065119 (2022).\n 6. Weinert, B. T. et al. Lysine succinylation is a frequently occurring modification in prokaryotes and eukaryotes and extensively \noverlaps with acetylation. Cell Rep. 4, 842–851. https:// doi. org/ 10. 1016/j. celrep. 2013. 07. 024 (2013).\n 7. Jin, W . & Wu, F . Proteome-wide identification of lysine succinylation in the proteins of tomato (Solanum lycopersicum). PLoS \nONE 11, e0147586. https:// doi. org/ 10. 1371/ journ al. pone. 01475 86 (2016).\n 8. Meng, L. et al. Comparative proteomics and metabolomics of JAZ7-mediated drought tolerance in Arabidopsis. J. Proteom. 196, \n81–91. https:// doi. org/ 10. 1016/j. jprot. 2019. 02. 001 (2019).\n 9. Zhang, N. W . et al. Quantitative global proteome and lysine succinylome analyses reveal the effects of energy metabolism in renal \ncell carcinoma. Proteomics https:// doi. org/ 10. 1002/ pmic. 20180 0001 (2018).\n 10. Xu, Y . et al. iSuc-PseAAC: Predicting lysine succinylation in proteins by incorporating peptide position-specific propensity. Sci. \nRep. 5, 10184. https:// doi. org/ 10. 1038/ srep1 0184 (2015).\n 11. Jia, J., Liu, Z., Xiao, X., Liu, B. & Chou, K. C. iSuc-PseOpt: Identifying lysine succinylation sites in proteins by incorporating \nsequence-coupling effects into pseudo components and optimizing imbalanced training dataset. Anal. Biochem. 497, 48–56. \nhttps:// doi. org/ 10. 1016/j. ab. 2015. 12. 009 (2016).\n 12. Jia, J., Liu, Z., Xiao, X., Liu, B. & Chou, K. C. pSuc-Lys: Predict lysine succinylation sites in proteins with PseAAC and ensemble \nrandom forest approach. J. Theor. Biol. 394, 223–230. https:// doi. org/ 10. 1016/j. jtbi. 2016. 01. 020 (2016).\n 13. Wang, H., Zhao, H., Y an, Z., Zhao, J. & Han, J. MDCAN-Lys: A model for predicting succinylation sites based on multilane dense \nconvolutional attention network. Biomolecules. https:// doi. org/ 10. 3390/ biom1 10608 72 (2021).\n 14. Ning, W . et al. HybridSucc: A hybrid-learning architecture for general and species-specific succinylation site prediction. Genom. \nProteom. Bioinf. 18, 194–207. https:// doi. org/ 10. 1016/j. gpb. 2019. 11. 010 (2020).\n 15. Hasan, M. M., Y ang, S., Zhou, Y . & Mollah, M. N. H. SuccinSite: a computational tool for the prediction of protein succinylation \nsites by exploiting the amino acid patterns and properties. Mol. BioSyst. 12, 786–795 (2016).\n 16. Dehzangi, A. et al. PSSM-Suc: Accurately predicting succinylation using position specific scoring matrix into bigram for feature \nextraction. J. Theor. Biol. 425, 97–102. https:// doi. org/ 10. 1016/j. jtbi. 2017. 05. 005 (2017).\n 17. Dehzangi, A. et al. Improving succinylation prediction accuracy by incorporating the secondary structure via helix, strand and \ncoil, and evolutionary information from profile bigrams. PLoS ONE 13, e0191900. https:// doi. org/ 10. 1371/ journ al. pone. 01919 00 \n(2018).\n 18. Hasan, M. M. & Kurata, H. GPSuc: Global prediction of generic and species-specific succinylation sites by aggregating multiple \nsequence features. PLoS ONE 13, e0200283. https:// doi. org/ 10. 1371/ journ al. pone. 02002 83 (2018).\nFigure 6.  Sensitivity analysis of the final model on varying the size of available training data.\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\n 19. Pakhrin, S. C., Pokharel, S., Saigo, H. & Kc, D. B. Deep learning-based advances in protein posttranslational modification site and \nprotein cleavage prediction. Methods Mol. Biol. 2499, 285–322. https:// doi. org/ 10. 1007/ 978-1- 0716- 2317-6_ 15 (2022).\n 20. Huang, K. Y ., Hsu, J. B. & Lee, T. Y . Characterization and identification of lysine succinylation sites based on deep learning method. \nSci. Rep. 9, 16175. https:// doi. org/ 10. 1038/ s41598- 019- 52552-4 (2019).\n 21. Thapa, N. et al. DeepSuccinylSite: A deep learning based approach for protein succinylation site prediction. BMC Bioinf. 21, 1–10 \n(2020).\n 22. Heinzinger, M. et al. Contrastive learning on protein embeddings enlightens midnight zone. NAR Genom. Bioinform. 4, lqac043. \nhttps:// doi. org/ 10. 1093/ nargab/ lqac0 43 (2022).\n 23. Weissenow, K., Heinzinger, M. & Rost, B. Protein language-model embeddings for fast, accurate, and alignment-free protein \nstructure prediction. Structure https:// doi. org/ 10. 1016/j. str. 2022. 05. 001 (2022).\n 24. Steinegger, M., Mirdita, M. & Soding, J. Protein-level assembly increases protein sequence recovery from metagenomic samples \nmanyfold. Nat Methods 16, 603. https:// doi. org/ 10. 1038/ s41592- 019- 0437-4 (2019).\n 25. Steinegger, M. & Soding, J. Clustering huge protein sequence sets in linear time. Nat. Commun. 9, 2542. https:// doi. org/ 10. 1038/ \ns41467- 018- 04964-5 (2018).\n 26. Bepler, T. & Berger, B. Learning the protein language: Evolution, structure, and function. Cell Syst. 12, 654–669 e653. https:// doi. \norg/ 10. 1016/j. cels. 2021. 05. 017 (2021).\n 27. Bepler, T. & Berger, B. Learning protein sequence embeddings using information from structure. arXiv preprint arXiv: 1902. 08661 \n(2019).\n 28. Rao, R. et al. Evaluating protein transfer learning with TAPE. Adv. Neural Inf. Process. Syst. 32, 9689–9701 (2019).\n 29. Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. \nProc. Natl. Acad. Sci. USA. https:// doi. org/ 10. 1073/ pnas. 20162 39118 (2021).\n 30. Luo, Y . et al. ECNet is an evolutionary context-integrated deep learning framework for protein engineering. Nat. Commun.  12, \n5743. https:// doi. org/ 10. 1038/ s41467- 021- 25976-8 (2021).\n 31. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M. Unified rational protein engineering with sequence-based \ndeep representation learning. Nat. Methods 16, 1315. https:// doi. org/ 10. 1038/ s41592- 019- 0598-1 (2019).\n 32. Elnaggar, A. et al. ProtTrans: Towards cracking the language of Life’s code through self-supervised deep learning and high perfor-\nmance computing. arXiv preprint arXiv: 2007. 06225 (2020).\n 33. Littmann, M., Heinzinger, M., Dallago, C., Weissenow, K. & Rost, B. Protein embeddings and deep learning predict binding residues \nfor various ligand classes. Sci. Rep.‑UK https:// doi. org/ 10. 1038/ s41598- 021- 03431-4 (2021).\n 34. Heinzinger, M. et al. Modeling aspects of the language of life through transfer-learning protein sequences. BMC Bioinf.  20, 723. \nhttps:// doi. org/ 10. 1186/ s12859- 019- 3220-8 (2019).\n 35. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21 (2020).\n 36. Suzek, B. E., Huang, H., McGarvey, P ., Mazumder, R. & Wu, C. H. UniRef: comprehensive and non-redundant UniProt reference \nclusters. Bioinformatics 23, 1282–1288. https:// doi. org/ 10. 1093/ bioin forma tics/ btm098 (2007).\n 37. Consortium, U. UniProt: A hub for protein information. Nucleic Acids Res. 43, D204–D212 (2015).\n 38. Li, W . & Godzik, A. Cd-hit: A fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinfor‑\nmatics 22, 1658–1659 (2006).\n 39. Ning, Q., Zhao, X., Bao, L., Ma, Z. & Zhao, X. Detecting succinylation sites from protein sequences using ensemble support vector \nmachine. BMC Bioinf. 19, 1–9 (2018).\n 40. Breiman, L. Random forests. Mach. Learn. 45, 5–32. https:// doi. org/ 10. 1023/A: 10109 33404 324 (2001).\n 41. Smola, A. J. & Scholkopf, B. A tutorial on support vector regression. Stat. Comput. 14, 199–222. https:// doi. org/ 10. 1023/B: Stco. \n00000 35301. 49549. 88 (2004).\n 42. Friedman, J. H. Stochastic gradient boosting. Comput. Stat. Data. Anal. 38, 367–378. https:// doi. org/ 10. 1016/ S0167- 9473(01) \n00065-2 (2002).\n 43. Villegas-Morcillo, A. et al. Unsupervised protein embeddings outperform hand-crafted sequence and structure features at predict-\ning molecular function. Bioinformatics 37, 162–170. https:// doi. org/ 10. 1093/ bioin forma tics/ btaa7 01 (2021).\n 44. Wolpert, D. H. Stacked Generalization. Neural Netw. 5, 241–259. https:// doi. org/ 10. 1016/ S0893- 6080(05) 80023-1 (1992).\n 45. Raschka, S. STAT 451: Machine Learning Lecture Notes. (2020).\n 46. van der Maaten, L. & Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 2579–2605 (2008).\n 47. Hasan, M. M., Khatun, M. S., Mollah, M. N. H., Y ong, C. & Guo, D. A systematic identification of species-specific protein suc-\ncinylation sites using joint element features information. Int. J. Nanomed. 12, 6303–6315. https:// doi. org/ 10. 2147/ IJN. S1408 75 \n(2017).\n 48. Suzek, B. E. et al. UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches. Bioinfor‑\nmatics 31, 926–932. https:// doi. org/ 10. 1093/ bioin forma tics/ btu739 (2015).\nAcknowledgements\nKC acknowledges support from NSF Grants Nos. 2210356, 1901793.\nAuthor contributions\nD.B.K, S.P . conceived the study and designed the experiments. S.P . performed experiments implementing ProtT5 \nfeatures in different ML/DL based architectures. D.B.K., R.H.N, and M.H. provided guidance throughout the \nproject. S.P ., P .P ., M.H., and D.B.K. wrote the main manuscript and S.P ., P .P ., M.H., R.H.N., D.K. revised the paper. \nD.B.K. oversaw the overall aspects of the project. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 022- 21366-2.\nCorrespondence and requests for materials should be addressed to D.B.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:16933  | https://doi.org/10.1038/s41598-022-21366-2\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Succinylation",
  "concepts": [
    {
      "name": "Succinylation",
      "score": 0.931247353553772
    },
    {
      "name": "Computer science",
      "score": 0.53634113073349
    },
    {
      "name": "Computational biology",
      "score": 0.45282065868377686
    },
    {
      "name": "Protein structure prediction",
      "score": 0.42819371819496155
    },
    {
      "name": "Bioinformatics",
      "score": 0.3355315327644348
    },
    {
      "name": "Protein structure",
      "score": 0.3024096190929413
    },
    {
      "name": "Biology",
      "score": 0.256403386592865
    },
    {
      "name": "Biochemistry",
      "score": 0.14707696437835693
    },
    {
      "name": "Lysine",
      "score": 0.12262815237045288
    },
    {
      "name": "Amino acid",
      "score": 0.10505783557891846
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11957088",
      "name": "Michigan Technological University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I35777872",
      "name": "North Carolina Agricultural and Technical State University",
      "country": "US"
    }
  ],
  "cited_by": 71
}