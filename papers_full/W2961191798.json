{
  "title": "Toward systematic review automation: a practical guide to using machine learning tools in research synthesis",
  "url": "https://openalex.org/W2961191798",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2099227489",
      "name": "Iain J Marshall",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A2673201243",
      "name": "Byron C. Wallace",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2099227489",
      "name": "Iain J Marshall",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2673201243",
      "name": "Byron C. Wallace",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2146668368",
    "https://openalex.org/W2122589015",
    "https://openalex.org/W2593758073",
    "https://openalex.org/W1984615312",
    "https://openalex.org/W1977535059",
    "https://openalex.org/W2783690099",
    "https://openalex.org/W2755149525",
    "https://openalex.org/W166020462",
    "https://openalex.org/W1629765770",
    "https://openalex.org/W4313371821",
    "https://openalex.org/W2060536346",
    "https://openalex.org/W2484269232",
    "https://openalex.org/W2184378182",
    "https://openalex.org/W2399450173",
    "https://openalex.org/W2604568423",
    "https://openalex.org/W4211191796",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2798344278",
    "https://openalex.org/W2798373858",
    "https://openalex.org/W2619694921",
    "https://openalex.org/W2122514515",
    "https://openalex.org/W2897782208",
    "https://openalex.org/W2589937067",
    "https://openalex.org/W4206723194",
    "https://openalex.org/W2512040454",
    "https://openalex.org/W2808847453",
    "https://openalex.org/W2020959315",
    "https://openalex.org/W2600107025",
    "https://openalex.org/W7028469425",
    "https://openalex.org/W2120754707",
    "https://openalex.org/W2180673283",
    "https://openalex.org/W2625124642",
    "https://openalex.org/W2186685500",
    "https://openalex.org/W2413121557",
    "https://openalex.org/W2963082289",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2963354565",
    "https://openalex.org/W2147469877",
    "https://openalex.org/W1663973292",
    "https://openalex.org/W2100053037",
    "https://openalex.org/W2131571251",
    "https://openalex.org/W2914331073",
    "https://openalex.org/W2149684865",
    "https://openalex.org/W2964179635"
  ],
  "abstract": null,
  "full_text": "COMMENTARY Open Access\nToward systematic review automation: a\npractical guide to using machine learning\ntools in research synthesis\nIain J. Marshall 1* and Byron C. Wallace 2\nAbstract\nTechnologies and methods to speed up the production of systematic reviews by reducing the manual labour\ninvolved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic\nreview process, including search, screening, and data extraction. However, how these technologies work in practice\nand when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an\noverview of current machine learning methods that have been proposed to expedite evidence synthesis. We also\noffer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review\nteam might go about using them in practice.\nKeywords: Machine learning, Natural language processing, Evidence synthesis\nBackground\nEvidence-based medicine (EBM) is predicated on the idea\nof harnessing the entirety of the available evidence to in-\nform patient care. Unfortunately, this is a challenging aim\nto realize in practice, for a few reasons. First, relevant evi-\ndence is primarily disseminated in unstructured, natural\nlanguage articles describing the conduct and results of\nclinical trials. Second, the set of such articles is already\nmassive and continues to expand rapidly [ 1].\nA now outdated estimate from 1999 suggests that con-\nducting a single review requires in excess of 1000 h of\n(highly skilled) manual labour [ 2]. More recent work es-\ntimates that conducting a review currently takes, on\naverage, 67 weeks from registration to publication [ 3].\nClearly, existing processes are not sustainable: reviews of\ncurrent evidence cannot be [ 4]produced efficiently and\nin any case often go out of date quickly once they are\npublished . The fundamental problem is that current\nEBM methods, while rigorous, simply do not scale to\nmeet the demands imposed by the voluminous scale of\nthe (unstructured) evidence base. This problem has been\ndiscussed at length elsewhere [ 5– 8].\nResearch on methods for semi-automating systematic\nreviews via machine learning and natural language pro-\ncessing now constitutes its own (small) subfield, with an\naccompanying body of work. In this survey, we aim to\nprovide a gentle introduction to automation technologies\nfor the non-computer scientist. We describe the current\nstate of the science and provide practical guidance on\nwhich methods we believe are ready for use. We also dis-\ncuss how a systematic review team might go about using\nthem, and the strengths and limitations of each. We do\nnot attempt an exhaustive review of research in this bur-\ngeoning field. Perhaps unsurprisingly, multiple systematic\nreviews of such efforts already exist [ 9, 10].\nInstead, we identified machine learning systems that\nare available for use in practice at the time of writing,\nthrough manual screening of records in SR Toolbox 1 on\nJanuary 3, 2019, to identify all systematic review tools\nwhich incorporated machine learning [ 11]. SR Toolbox\nis a publicly available online catalogue of software tools\nto aid systematic review production and is regularly up-\ndated via regular literature surveillance plus direct sub-\nmissions from tool developers and via social media. We\nhave not described machine learning methods from aca-\ndemic papers unless a system to enact them has been\n© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n* Correspondence: iain.marshall@kcl.ac.uk\n1School of Population Health & Environmental Sciences, Faculty of Life\nSciences and Medicine, King ’s College London, 3rd Floor, Addison House,\nGuy’s Campus, London SE1 1UL, UK\nFull list of author information is available at the end of the article\nMarshall and Wallace Systematic Reviews           (2019) 8:163 \nhttps://doi.org/10.1186/s13643-019-1074-9\nmade available; we likewise have not described (the very\nlarge number of) software tools for facilitating system-\natic reviews unless they make use of machine learning.\nBox 1 Glossary of terms used in systematic review\nautomation\nMachine learning: computer algorithms which ‘learn’ to perform a\nspecific task through statistical modelling of (typically large amounts of)\ndata\nNatural language processing: computational methods for automatically\nprocessing and analysing ‘natural’ (i.e. human) language texts\nText classification: automated categorization of documents into groups\nof interest\nData extraction: the task of identifying key bits of structured information\nfrom texts\nCrowd-sourcing: decomposing work into micro-tasks to be performed\nby distributed workers\nMicro-tasks: discrete units of work that together complete a larger\nundertaking\nSemi-automation: using machine learning to expedite tasks, rather than\ncomplete them\nHuman-in-the-loop: workflows in which humans remain involved, rather\nthan being replaced\nSupervised learning: estimating model parameters using manually\nlabelled data\nDistantly supervised: learning from pseudo, noisy ‘labels’ derived\nautomatically by applying rules to existing databases or other structured\ndata\nUnsupervised: learning without any labels (e.g. clustering data)\nMachine learning and natural language\nprocessing methods: an introduction\nText classification and data extraction: the key tasks for\nreviewers\nThe core natural language processing (NLP)\ntechnologies used in systematic reviews are text\nclassification and data extraction . Text classification\nconcerns models that can automatically sort documents\n(here, article abstracts, full texts, or pieces of text within\nthese) into predefined categories of interest (e.g. report\nof RCT vs. not). Data extraction models attempt to\nidentify snippets of text or individual words/numbers\nthat correspond to a particular variable of interest (e.g.\nextracting the number of people randomized from a\nclinical trial report).\nThe most prominent example of text classification in\nthe review pipeline is abstract screening: determining\nwhether individual articles within a candidate set meet\nthe inclusion criteria for a particular review on the basis\nof their abstracts (and later full texts). In practice, many\nmachine learning systems can additionally estimate a\nprobability that a document should be included (rather\nthan a binary include/exclude decision). These\nprobabilities can be used to automatically rank\ndocuments from most to least relevant, thus potentially\nallowing the human reviewer to identify the studies to\ninclude much earlier in the screening process.\nFollowing the screening, reviewers extract the data\nelements that are relevant to their review. These are\nnaturally viewed as individual data extraction tasks. Data\nof interest may include numerical data such as study\nsample sizes and odds ratios, as well as textual data, e.g.\nsnippets of text describing the study randomization\nprocedure or the study population.\nRisk of bias assessment is interesting in that it entails\nboth a data extraction task (identifying snippets of text\nin the article as relevant for bias assessment) and a final\nclassification of an article as being at high or low risk for\neach type of bias assessed [ 12].\nState-of-the-art methods for both text classification\nand data extraction use machine learning (ML)\ntechniques, rather than, e.g. rule-based methods. In ML,\none writes programs that specify parameterized models\nto perform particular tasks; these parameters are then\nestimated using (ideally large) datasets. In practice, ML\nmethods resemble statistical models used in epidemio-\nlogical research (e.g. logistic regression is a common\nmethod in both disciplines).\nWe show a simple example of how machine learning\ncould be used to automate the classification of articles\nas being RCTs or not in Fig. 1. First, a training set of\ndocuments is obtained. This set will be manually\nlabelled for the variable of interest (e.g. as an ‘included\nstudy’or ‘excluded study ’).\nNext, documents are vectorized, i.e. transformed into\nhigh-dimensional points that are represented by se-\nquences of numbers. A simple, common representation\nis known as a bag of words (see Fig. 2). In this approach,\na matrix is constructed in which rows are documents\nand each column corresponds to a unique word. Docu-\nments may then be represented in rows by 1 ’s and 0 ’s,\nindicating the presence or absence of each word, re-\nspectively.2 The resultant matrix will be sparse (i.e. con-\nsist mostly of 0 ’s and relatively few 1 ’s), as any individual\ndocument will contain a small fraction of the full\nvocabulary.3\nNext, weights (or coefficients) for each word are\n‘learned’(estimated) from the training set. Intuitively for\nthis task, we want to learn which words make a\ndocument more, or less, likely to be an RCT. Words\nwhich lower the likelihood of being an RCT should have\nnegative weights; those which increase the likelihood\n(such as ‘random’ or ‘randomly’) should have positive\nweights. In our running example, the model coefficients\ncorrespond to the parameters of a logistic regression\nmodel. These are typically estimated ( ‘learned’) via\ngradient descent-based methods.\nOnce the coefficients are learned, they can easily be\napplied to a new, unlabelled document to predict the\nlabel. The new document is vectorized in an identical\nway to the training documents. The document vector is\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 2 of 10\nthen multiplied 4 by the previously learned coefficients,\nand transformed to a probability via the sigmoid\nfunction.\nMany state-of-the-art systems use more complex\nmodels than logistic regression (and in particular more\nsophisticated methods for representing documents [ 13],\nobtaining coefficients [ 14], or both [ 15]). Neural\nnetwork-based approaches in particular have re-emerged\nas the dominant model class. Such models are composed\nof multiple layers, each with its own set of parameters.\nWe do not describe these methods in detail here, 5 but\nthe general principle is the same: patterns are learned\nfrom numerical representations of documents with\nknown labels, and then, these patterns can be applied to\nnew documents to predict the label. In general, these\nmore complex methods achieve (often modest) improve-\nments in predictive accuracy compared with logistic re-\ngression, at the expense of computational and\nmethodological complexity.\nMethods for automating (or semi-automating) data ex-\ntraction have been well explored, but for practical use\nremain less mature than automated screening technolo-\ngies. Such systems typically operate over either abstracts\nor full-text articles and aim to extract a defined set of\nvariables from the document.\nAt its most basic, data extraction can be seen as a type\nof text classification problem, in which individual words\n(known as tokens) are classified as relevant or not\nwithin a document. Rather than translating the full\ndocument into a vector, a data extraction system might\nencode the word itself, plus additional contextual\ninformation (for example, nearby surrounding words\nand position in the document).\nGiven such a vector representation of the word at\nposition t in document x (notated as xt), an extraction\nsystem should output a label that indicates whether or\nnot this word belongs to a data type of interest (i.e.\nsomething to be extracted). For example, we may want\nto extract study sample sizes. Doing so may entail\nconverting numbers written in English to numerals and\nthen labelling (or ‘tagging’) all numbers on the basis of\nfeature vectors that encode properties that might be\nuseful for making this prediction (e.g. the value of the\nnumber, words that precede and follow it, and so on).\nThis is depicted in Fig. 3. Here, the ‘target’token ( ‘100’)\nis labelled as 1, and others as 0.\nSuch a token by token classification approach often\nfails to capitalize on the inherently structured nature of\nlanguage and documents. For example, consider a model\nfor extracting snippets of text that describe the study\npopulation, intervention/comparators, and outcomes\n(i.e. PICO elements), respectively. Labelling words\nindependently of one another would fail to take into\naccount the observation that adjacent words will have a\ntendency to share designations: if the word at position t\nis part of a description of the study population, that\nFig. 1 Classifying text using machine learning, in this example logistic regression with a ‘bag of words ’ representation of the texts. The system is\n‘trained’, learning a coefficient (or weight) for each unique word in a manually labelled set of documents (typically in the 1000s). In use, the\nlearned coefficients are used to predict a probability for an unknown document\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 3 of 10\nsubstantially raises the odds that the word at position t\n+1 is as well.\nIn ML nomenclature, this is referred to as a structured\nclassification problem. More specifically, assigning the\nwords in a text to categories is an instance of sequence\ntagging. Many models for problems with this structure\nhave been developed. The conditional random field\n(CRF) is amongst the most prominent of these [ 18].\nCurrent state-of-the-art models are based on neural net-\nworks, and specifically recurrent neural networks, or\nRNNs. Long short-term memory networks (LSTMs) [ 19]\ncombined with CRFs (LSTM-CRFs) [ 19– 21] have in par-\nticular shown compelling performance on such tasks\ngenerally, for extraction of data from RCTs specifically\n[22, 23].\nMachine learning tools available for use in\npractice\nSearch\nThe rapidly expanding biomedical literature has made\nsearch an appealing target for automation. Two key\nareas have been investigated to date: filtering articles by\nstudy design and automatically finding relevant articles\nby topic. Text classification systems for identifying RCTs\nare the most mature, and we regard them as ready for\nuse in practice. Machine learning for identifying RCTs\nhas already been deployed in Cochrane; Cochrane\nauthors may access this technology via the Cochrane\nRegister of Studies [ 24].6\nTwo validated systems are freely available for general\nuse [ 16, 25]. Cohen and colleagues have released RCT\ntagger,7 a system which estimates the probability that\nPubMed articles are RCTs [ 25]. The team validated the\nperformance on a withheld portion of the same dataset,\nfinding the system discriminated accurately between\nRCTs and non-RCTs (area under the receiver operating\ncharacteristics curve (AUROC) = 0.973). A search portal\nis available freely at their website, which allows the user\nto select a confidence threshold for their search.\nOur own team has produced RobotSearch 8, which\naims to replace keyword-based study filtering. The sys-\ntem uses neural networks and support vector machines,\nand was trained on a large set of articles with crowd-\nsourced labels by Cochrane Crowd [ 16]. The system was\nvalidated on and achieved state-of-the-art discriminative\nperformance (AUROC = 0.987), reducing the number of\nirrelevant articles retrieved by roughly half compared\nwith the keyword-based Cochrane Highly Sensitive\nSearch Strategy, without losing any additional RCTs.\nThe system may be freely used by uploading an RIS file\nFig. 2 Bag of words modelling for classifying RCTs. Top left: Example of bag of words for three articles. Each column represents a unique word in\nthe corpus (a real example would likely contain columns for 10,000s of words). Top right: Document labels, where 1 = relevant and 0 = irrelevant.\nBottom: Coefficients (or weights) are estimated for each word (in this example using logistic regression). In this example, high +ve weights will\nincrease the predicted probability that an unseen article is an RCT where it contains the words ‘random’ or ‘randomized’. The presence of the\nword ‘systematic’ (with a large negative weight) would reduce the predicted probability that an unseen document is an RCT\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 4 of 10\nto our website; a filtered file containing only the RCTs is\nthen returned.\nStudy design classification is appealing for machine\nlearning because it is a single, generalizable task:\nfiltering RCTs is common across many systematic\nreviews. However, finding articles which meet other\ntopic-specific inclusion criteria is review-specific and\nthus much more difficult — consider that it is unlikely\nthat a systematic review with identical inclusion criteria\nwould have been performed before, and even where it\nhas been, it might yield up to several dozen articles to\nuse a training data, compared with the thousands\nneeded in a typical machine learning system. We discuss\nhow a small set of relevant articles (typically obtained\nthrough screening a proportion of abstracts retrieved by\na particular search) can seed a machine learning system\nto identify other relevant articles below.\nA further application of machine learning in search is\nas a method for producing a semantic search engine, i.e.\none in which the user can search by concept rather than\nby keyword. Such a system is akin to searching PubMed\nby MeSH terms (index terms from a standardized\nvocabulary, which have traditionally been applied\nmanually by PubMed staff). However, such a manual\napproach has the obvious drawback of requiring\nextensive and ongoing manual annotation effort,\nespecially in light of the exponentially increasing volume\nof articles to index. Even putting costs aside, manual\nannotation delays the indexing process, meaning the\nmost recent articles may not be retrievable. Thalia is a\nmachine learning system (based on CRFs, reviewed\nabove) that automatically indexes new PubMed articles\ndaily for chemicals, diseases, drugs, genes, metabolites,\nproteins, species, and anatomical entities. This allows\nthe indexes to be updated daily and provides a user\ninterface to interact with the concepts identified [ 26].\nIndeed, as of October 2018, PubMed itself has adopted\na hybrid approach, where some articles are assigned\nMeSH terms automatically using their Medical Text\nIndexer (MTI) system [ 27], which uses a combination of\nmachine learning and manually crafted rules to assign\nterms without human intervention [ 28].\nScreening\nMachine learning systems for abstract screening have\nreached maturity; several such systems with high levels\nof accuracy are available for reviewers to use. In all of\nthe available systems, human reviewers first need to\nscreen a set of abstracts and then review the system\nrecommendations. Such systems are thus semi-\nautomatic, i.e. keep humans ‘in-the-loop’. We show a\ntypical workflow in Fig. 4.\nAfter conducting a conventional search, retrieved\nabstracts are uploaded into the system (e.g. using the\ncommon RIS citation format). Next, a human reviewer\nmanually screens a sample (often random) of the\nretrieved set. This continues until a ‘sufficient’ number\nof relevant articles have been identified such that a text\nFig. 3 Schematic of a typical data extraction process. The above illustration concerns the example task of extracting the study sample size. In\ngeneral, these tasks involve labelling individual words. The word (or ‘token’) at position t is represented by a vector. This representation may\nencode which word is at this position and likely also communicates additional features, e.g. whether the word is capitalized or if the word is\n(inferred to be) a noun. Models for these kinds of tasks attempt to assign labels all T words in a document and for some tasks will attempt to\nmaximize the joint likelihood of these labels to capitalize on correlations between adjacent labels\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 5 of 10\nclassifier can be trained. (Exactly how many positive\nexamples will suffice to achieve good predictive\nperformance is an empirical question, but a conservative\nheuristic is about half of the retrieved set.) The system\nuses this classifier to predict the relevance of all\nunscreened abstracts, and these are reordered by rank.\nThe human reviewer is hence presented with the most\nrelevant articles first. This cycle then continues, with the\ndocuments being repeatedly re-ranked as additional ab-\nstracts are screened manually, until the human reviewer\nis satisfied that no further relevant articles are being\nscreened.\nThis is a variant of active learning (AL) [ 29]. In AL\napproaches, the model selects which instances are to be\nlabelled next, with the aim of maximizing predictive\nperformance with minimal human supervision. Here, we\nhave outlined a certainty-based AL criterion, in which\nthe model prioritizes for labelling citations that it\nbelieves to be relevant (under its current model\nparameters). This AL approach is appropriate for the\nsystematic review scenario, in light of the relatively small\nnumber of relevant abstracts that will exist in a given set\nunder consideration. However a more standard, general\napproach is uncertainty sampling , wherein the model\nasks the human to label instances it is least certain\nabout.\nThe key limitation of automated abstract screening is\nthat it is not clear at which point it is ‘safe’ for the\nreviewer to stop manual screening. Moreover, this point\nwill vary across reviews. Screening systems tend to rank\narticles by the likelihood of relevance, rather than simply\nproviding definitive, dichotomized classifications.\nHowever, even low ranking articles have some non-zero\nprobability of being relevant, and there remains the\npossibility of missing a relevant article by stopping too\nearly. (It is worth noting that all citations not retrieved\nvia whatever initial search strategy is used to retrieve the\ncandidate pool of articles implicitly assign zero probabil-\nity to all other abstracts; this strong and arguably unwar-\nranted assumption is often overlooked.) Empirical\nstudies have found the optimal stopping point can vary\nsubstantially between different reviews; unfortunately,\nthe optimal stopping point can only be determined de-\nfinitively in retrospect once all abstracts have been\nscreened. Currently available systems include Abstrackr\n[30], SWIFT-Review, 9 EPPI reviewer [ 31], and RobotA-\nnalyst [ 32] (see Table 1).\nData extraction\nThere have now been many applications of data\nextraction to support systematic reviews; for a relatively\nrecent survey of these, see [ 9]. Yet despite advances,\nextraction technologies remain in formative stages and\nare not readily accessible by practitioners. For systematic\nreviews of RCTs, there exist only a few prototype\nplatforms that make such technologies available (ExaCT\n[33] and RobotReviewer [ 12, 34, 35] being among these).\nFor systematic reviews in the basic sciences, the UK\nNational Centre for Text Mining (NaCTeM) has created\na number of systems which use structured models to\nautomatically extract concepts including genes and\nproteins, yeasts, and anatomical entities [ 36], amongst\nother ML-based text mining tools. 10\nExaCT and RobotReviewer function in a similar way.\nThe systems are trained on full-text articles, with sen-\ntences being manually labelled 11 as being relevant (or\nnot) to the characteristics of the studies. In practice,\nboth systems over-retrieve candidate sentences (e.g.\nFig. 4 Typical workflow for semi-automated abstract screening. The asterisk indicates that with uncertainty sampling, the articles which are\npredicted with least certainty are presented first. This aims to improve the model accuracy more efficiently\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 6 of 10\nExaCT retrieves the five sentences predicted most likely,\nwhen the relevant information will generally reside in\nonly one of them). The purpose of this behaviour is to\nmaximize the likelihood that at least one of the sen-\ntences will be relevant. Thus, in practice, both systems\nwould likely be used semi-automatically by a human re-\nviewer. The reviewer would read the candidate sen-\ntences, choose those which were relevant, or consult the\nfull-text paper where no relevant text was identified.\nExaCT uses RCT reports in HTML format and is\ndesigned to retrieve 21 characteristics relating to study\ndesign and reporting based on the CONSORT criteria.\nExaCT additionally contains a set of rules to identify the\nwords or phrase within a sentence which describe the\ncharacteristic of interest. In their evaluation, the ExaCT\nteam found their system had very high recall (72% to\n100% for the different variables collected) when the 5\nmost likely sentences were retrieved.\nRobotReviewer takes RCT reports in PDF format and\nautomatically retrieves sentences which describe the\nPICO (the population, intervention, comparator, and\noutcomes), and also text describing trial conduct\nrelevant to biases (including the adequacy of the random\nsequence generation, the allocation concealment, and\nblinding, using the domains from the Cochrane Risk of\nBias tool). RobotReviewer additionally classifies the\narticle as being as to whether it is at ‘low’risk of bias or\nnot for each bias domain.\nValidation studies of RobotReviewer have found that\nthe article bias classifications (i.e. ‘low’ versus ‘high/\nunclear’ risk of bias) are reasonable but less accurate\nthan those in published Cochrane reviews [ 12, 15].\nHowever, the sentences identified were found to be\nsimilarly relevant to bias decisions as those in Cochrane\nreviews [ 12]. We therefore recommend that the system\nis used with manual input; that the output is treated as a\nsuggestion rather than the final bias assessment. A\nwebtool is available which highlights the text describing\nbiases, and suggests a bias decision aiming to expedite\nthe process compared with fully manual bias assessment.\nOne obstacle to better models for data extraction has\nbeen a dearth of training data for the task. Recall from\nabove the ML systems rely on manual labels to estimate\nmodel parameters. Obtaining labels on individual words\nwithin documents to train extraction models is an\nexpensive exercise. EXaCT, for example, was trained on\na small set (132 total) of full-text articles. RobotReviewer\nwas trained using a much larger dataset, but the ‘labels’\nwere induced semi-automatically, using a strategy known\nas ‘distant supervision ’[35]. This means the annotations\nused for training were imperfect, thus introducing noise\nto the model. Recently, Nye et al. released the EBM-NLP\ndataset [ 23], which comprises ~ 5000 abstracts of RCT\nreports manually annotated in detail. This may provide\ntraining data helpful for moving automated extraction\nmodels forward.\nSynthesis\nAlthough software tools that support the data synthesis\ncomponent of reviews have long existed (especially for\nTable 1 Examples of machine learning systems available for use in systematic reviews\nExample tools Comments\nSearch—\nfinding RCTs\nRobotSearch ( https://robotsearch.vortext.systems)\nCochrane Register of Studies ( https://community.cochrane.org/help/\ntools-and-software/crs-cochrane-register-studies)\nRCT tagger ( http://arrowsmith.psych.uic.edu/cgi-bin/arrowsmith_uic/\nRCT_Tagger.cgi)\n Validated machine learning filters available for identifying\nRCTs and suitable for fully automatic use\n Conventional topic-specific keyword search strategy still\nneeded\n No widely available tools for non-RCT design currently\nSearch—\nliterature\nexploration\nThalia ( http://nactem-copious.man.ac.uk/Thalia/) Allows search of PubMed for concepts (i.e. chemicals, diseases,\ndrugs, genes, metabolites, proteins, species and anatomical\nentities)\nScreening Abstrackr ( http://abstrackr.cebm.brown.edu)[ 30]\nEPPI reviewer ( https://eppi.ioe.ac.uk/cms/er4)[ 31]\nRobotAnalyst ( http://www.nactem.ac.uk/robotanalyst/)[ 32]\nSWIFT-Review ( https://www.sciome.com/swift-review/)\nColandr ( https://www.colandrapp.com)\nRayyan ( https://rayyan.qcri.org)\n Screening systems automatically sort a search retrieval by\nrelevance\n RobotAnalyst and SWIFT-Review also allow topic modelling ,\nwhere abstracts relating to similar topics are automatically\ngrouped, allowing the user to explore the search retrieval.\nData\nextraction\nExaCT ( http://exactdemo.iit.nrc.ca)\nRobotReviewer ( https://robotreviewer.vortext.systems)\nNaCTeM text mining tools for automatically extracting concepts\nrelating to genes and proteins (NEMine), yeast metabolites (Yeast\nMetaboliNER), and anatomical entities (AnatomyTagger) ( http://www.\nnactem.ac.uk/software.php)\n These prototype systems automatically extract data elements\n(e.g. sample sizes, descriptions of PICO elements) from free-\ntexts.\nBias\nassessment\nRobotReviewer ( https://robotreviewer.vortext.systems)  Automatic assessment of biases in reports of RCTs\n System recommended for semi-automatic use (i.e. with\nhuman reviewer checking and correcting the ML\nsuggestions)\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 7 of 10\nperforming meta-analysis), methods for automating this\nare beyond the capabilities of currently available ML and\nNLP tools. Nonetheless, research into these areas con-\ntinues rapidly, and computational methods may allow\nnew forms of synthesis unachievable manually, particu-\nlarly around visualization [ 37, 38] and automatic\nsummarization [ 39, 40] of large volumes of research\nevidence.\nConclusions\nThe torrential volume of unstructured published\nevidence has rendered existing (rigorous, but manual)\napproaches to evidence synthesis increasingly costly and\nimpractical. Consequently, researchers have developed\nmethods that aim to semi-automate different steps of\nthe evidence synthesis pipeline via machine learning.\nThis remains an important research direction and has\nthe potential to dramatically reduce the time required to\nproduce standard evidence synthesis products.\nAt the time of writing, research into machine learning\nfor systematic reviews has begun to mature, but many\nbarriers to its practical use remain. Systematic reviews\nrequire very high accuracy in their methods, which may\nbe difficult for automation to attain. Yet accuracy is not\nthe only barrier to full automation. In areas with a\ndegree of subjectivity (e.g. determining whether a trial is\nat risk of bias), readers are more likely to be reassured\nby the subjective but considered opinion of an expert\nhuman versus a machine. For these reasons, full\nautomation remains a distant goal at present. The\nmajority of the tools we present are designed as ‘human-\nin-the-loop’ systems: Their user interfaces allowing\nhuman reviewers to have the final say.\nMost of the tools we encountered were written by\nacademic groups involved in research into evidence\nsynthesis and machine learning. Very often, these groups\nhave produced prototype software to demonstrate a\nmethod. However, such prototypes do not age well: we\ncommonly encountered broken web links, difficult to\nunderstand and slow user interfaces, and server errors.\nFor the research field, moving from the research\nprototypes currently available (e.g. RobotReviewer,\nExaCT) to professionally maintained platforms remains\nan important problem to overcome. In our own\nexperience as an academic team in this area, the\nresources needed for maintaining professional grade\nsoftware (including bug fixes, server maintenance, and\nproviding technical support) are difficult to obtain from\nfixed term academic grant funding, and the lifespan of\nsoftware is typically many times longer than a grant\nfunding period. Yet commercial software companies are\nunlikely to dedicate their own resources to adopting\nthese machine learning methods unless there was a\nsubstantial demand from users.\nNonetheless, for the pioneering systematic review\nteam, many of the methods described can be used now.\nUsers should expect to remain fully involved in each\nstep of the review and to deal with some rough edges of\nthe software. Searching technologies that expedite\nretrieval of relevant articles (e.g. by screening out non-\nRCTs) are the most fully realized of the ML models\nreviewed here and are more accurate than conventional\nsearch filters. Tools for screening are accessible via us-\nable software platforms (Abstrackr, RobotAnalyst, and\nEPPI reviewer) and could safely be used now as a second\nscreener [ 31] or to prioritize abstracts for manual re-\nview. Data extraction tools are designed to assist the\nmanual process, e.g. drawing the user ’s attention to rele-\nvant text or making suggestions to the user that they\nmay validate, or change if needed. Piloting of some of\nthese technologies by early adopters (with appropriate\nmethodological caution) is likely the key next step to-\nward gaining acceptance by the community.\nEndnotes\n1http://systematicreviewtools.com/\n2Variants of this approach include using word counts\n(i.e. the presence of the word ‘trial’ three times in a\ndocument would result in a number 3 in the associated\ncolumn) or affording greater weight to more\ndiscriminative words (known as term frequency – inverse\ndocument frequency, or tf-idf)\n3We note that while they remain relatively common,\nbag of words representations have been largely\nsupplanted by dense ‘embeddings’ learned by neural\nnetworks.\n4This is a dot product.\n5We refer the interested reader to our brief overview\nof these methods [ 16] for classification and to Bishop\n[17] for a comprehensive, technical take.\n6http://crsweb.cochrane.org\n7http://arrowsmith.psych.uic.edu/cgi-bin/arrowsmith_\nuic/RCT_Tagger.cgi\n8https://robotsearch.vortext.systems/\n9https://www.sciome.com/swift-review/\n10http://www.nactem.ac.uk/\n11More precisely, RobotReviewer generated labels that\ncomprised our training data algorithmically.\nAuthors’ contributions\nThe authors contributed equally to the conception and writing of the\nmanuscript. All authors read and approved the final manuscript.\nAuthors’ information\nIain Marshall is a Clinical Academic Fellow in the School of Population Health\n& Environmental Sciences, Faculty of Life Sciences and Medicine, King ’s\nCollege London, 3rd Floor, Addison House, Guy's Campus, London SE1 1UL.\nEmail: iain.marshall@kcl.ac.uk\nByron Wallace is faculty in the College of Computer and Information Science,\nNortheastern University, 440 Huntington Ave #202, Boston, MA 02115. Email:\nb.wallace@northeastern.edu\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 8 of 10\nFunding\nUK Medical Research Council (MRC), through its Skills Development\nFellowship program, grant MR/N015185/1 (IJM); National Library of Medicine,\ngrant: R01-LM012086-01A1 (both IJM and BCW).\nAvailability of data and materials\nNot applicable.\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nWe consent.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1School of Population Health & Environmental Sciences, Faculty of Life\nSciences and Medicine, King ’s College London, 3rd Floor, Addison House,\nGuy’s Campus, London SE1 1UL, UK. 2Khoury College of Computer Sciences,\nNortheastern University, 202 WVH, 360 Huntington Avenue, Boston, MA\n02115, USA.\nReceived: 18 January 2019 Accepted: 24 June 2019\nReferences\n1. Bastian H, Glasziou P, Chalmers I. Seventy-five trials and eleven systematic\nreviews a day: how will we ever keep up? PLoS Med. 2010;7:e1000326.\n2. Allen IE, Olkin I. Estimating time to conduct a meta-analysis from number of\ncitations retrieved. JAMA. 1999;282:634 – 5.\n3. Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and\nworkers needed to conduct systematic reviews of medical\ninterventions using data from the PROSPERO registry. BMJ Open.\n2017;7:e012545.\n4. Johnston E. How quickly do systematic reviews go out of date? A survival\nanalysis. J Emerg Med. 2008;34:231.\n5. Tsafnat G, Dunn A, Glasziou P, Coiera E. The automation of systematic\nreviews. BMJ. 2013;346: – f139.\n6. O ’Connor AM, Tsafnat G, Gilbert SB, Thayer KA, Wolfe MS. Moving toward\nthe automation of the systematic review process: a summary of discussions\nat the second meeting of International Collaboration for the Automation of\nSystematic Reviews (ICASR). Syst Rev. 2018;7:3.\n7. Thomas J, Noel-Storr A, Marshall I, Wallace B, McDonald S, Mavergames C, et\nal. Living systematic reviews: 2. Combining human and machine effort. J\nClin Epidemiol. 2017;91:31 – 7.\n8. Wallace BC, Dahabreh IJ, Schmid CH, Lau J, Trikalinos TA. Modernizing\nevidence synthesis for evidence-based medicine. Clinical Decision Support;\n2014. p. 339 – 61.\n9. Jonnalagadda SR, Goyal P, Huffman MD. Automating data extraction in\nsystematic reviews: a systematic review. Syst Rev. 2015;4:78.\n10. O ’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text\nmining for study identification in systematic reviews: a systematic review of\ncurrent approaches. Syst Rev. 2015;4:5.\n11. Marshall C, Brereton P. Systematic review toolbox: a catalogue of tools to\nsupport systematic reviews. In: Proceedings of the 19th International\nConference on Evaluation and Assessment in Software Engineering: ACM;\n2015. p. 23.\n12. Marshall IJ, Kuiper J, Wallace BC. RobotReviewer: evaluation of a system for\nautomatically assessing bias in clinical trials. J Am Med Inform Assoc. 2016;\n23:193– 201.\n13. Goldberg Y, Levy O. word2vec explained: deriving Mikolov et al. ’s negative-\nsampling word-embedding method; 2014. p. 1 – 5.\n14. Joachims T. Text categorization with support vector machines:\nlearning with many rele vant features. In: Nédellec C, Rouveirol C,\neditors. Machine learning: ECML-98. Berlin, Heidelberg: Springer Berlin\nHeidelberg; 1998.\n15. Zhang Y, Marshall I, Wallace BC. Rationale-augmented convolutional neural\nnetworks for text classification. Proc Conf Empir Methods Nat Lang Process.\n2016;2016:795– 804.\n16. Marshall IJ, Noel-Storr A, Kuiper J, Thomas J, Wallace BC. Machine learning\nfor identifying randomized controlled trials: an evaluation and practitioner ’s\nguide. Res Synth Methods. 2018; Available from: https://doi.org/10.1002/\njrsm.1287.\n17. Bishop CM. Pattern recognition and machine learning. Springer New York;\n2016.\n18. Sutton C, McCallum A. An introduction to conditional random fields: Now\nPub; 2012.\n19. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput.\n1997;9:1735– 80.\n20. Ma X, Hovy E. End-to-end sequence labeling via bi-directional LSTM-CNNs-\nCRF. Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). 2016. Available from:\nhttp://dx.doi.org/10.18653/v1/p16-1101\n21. Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C. Neural\narchitectures for named entity recognition. Proceedings of the 2016\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. 2016. Available\nfrom: http://dx.doi.org/10.18653/v1/n16-1030\n22. Patel R, Yang Y, Marshall I, Nenkova A, Wallace BC. Syntactic patterns\nimprove information extraction for medical search. Proc Conf. 2018;2018:\n371– 7.\n23. Nye B, Jessy Li J, Patel R, Yang Y, Marshall IJ, Nenkova A, et al. A corpus with\nmulti-level annotations of patients, interventions and outcomes to support\nlanguage processing for medical literature. Proc Conf Assoc Comput\nLinguist Meet. 2018;2018:197 – 207.\n24. Wallace BC, Noel-Storr A, Marshall IJ, Cohen AM, Smalheiser NR, Thomas J.\nIdentifying reports of randomized controlled trials (RCTs) via a hybrid\nmachine learning and crowdsourcing approach. J Am Med Inform Assoc.\n2017;24:1165– 8.\n25. Cohen AM, Smalheiser NR, McDonagh MS, Yu C, Adams CE, Davis JM, et al.\nAutomated confidence ranked classification of randomized controlled trial\narticles: an aid to evidence-based medicine. J Am Med Inform Assoc. 2015;\n22:707– 17.\n26. Soto, Axel J, Przyby ła P, Ananiadou S. “Thalia: Semantic Search Engine for\nBiomedical Abstracts. ” Bioinformatics. 2019;35(10):1799-1801.\n27. Incorporating Values for Indexing Method in MEDLINE/PubMed XML. NLM\nTechnical Bulletin. U.S. National Library of Medicine; 2018 [cited 2019 Jan\n18]; Available from: https://www.nlm.nih.gov/pubs/techbull/ja18/ja18_\nindexing_method.html\n28. Mork J, Aronson A, Demner-Fushman D. 12 years on - is the NLM medical\ntext indexer still useful and relevant? J Biomed Semantics. 2017;8:8.\n29. Settles B. Active learning. Synthesis lectures on artificial intelligence and\nmachine learning. 2012;6:1 – 114.\n30. Wallace BC, Small K, Brodley CE, Lau J, Trikalinos TA. Deploying an\ninteractive machine learning system in an evidence-based practice center:\nAbstrackr. Proceedings of the 2Nd ACM SIGHIT International Health\nInformatics Symposium. New York: ACM; 2012. p. 819 – 24.\n31. Shemilt I, Khan N, Park S, Thomas J. Use of cost-effectiveness analysis to\ncompare the efficiency of study identification methods in systematic\nreviews. Syst Rev. 2016;5:140.\n32. Przyby ła P, Brockmeier AJ, Kontonatsios G, Le Pogam M-A, McNaught J, von\nElm E, et al. Prioritising references for systematic reviews with RobotAnalyst:\na user study. Res Synth Methods. 2018;9:470 – 88.\n33. Kiritchenko S, de Bruijn B, Carini S, Martin J, Sim I. ExaCT: automatic\nextraction of clinical trial characteristics from journal publications. BMC Med\nInform Decis Mak. 2010;10:56.\n34. Marshall IJ, Kuiper J, Banner E, Wallace BC. Automating biomedical evidence\nsynthesis: RobotReviewer. Proc Conf Assoc Comput Linguist Meet. 2017;\n2017:7– 12.\n35. Wallace BC, Kuiper J, Sharma A, Zhu MB, Marshall IJ. Extracting PICO\nsentences from clinical trial reports using supervised distant supervision. J\nMach Learn Res. 2016;17:1 – 25.\n36. Pyysalo S, Ananiadou S. Anatomical entity mention recognition at literature\nscale. Bioinformatics. 2014;30:868 – 75.\n37. Mo Y, Kontonatsios G, Ananiadou S. Supporting systematic reviews using\nLDA-based document representations. Syst Rev. 2015;4:172.\n38. Mu T, Goulermas YJ, Ananiadou S. Data visualization with structural control\nof global cohort and local data neighborhoods. IEEE Trans Pattern Anal\nMach Intell. 2017; Available from: http://dx.doi.org/10.1109/TPAMI.2017.\n2715806\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 9 of 10\n39. Sarker A, Mollá D, Paris C. Query-oriented evidence extraction to support\nevidence-based medicine practice. J Biomed Inform. 2016;59:169 – 84.\n40. Mollá D, Santiago-Martínez ME. Creation of a corpus for evidence based\nmedicine summarisation. Australas Med J. 2012;5:503 – 6.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nMarshall and Wallace Systematic Reviews           (2019) 8:163 Page 10 of 10",
  "topic": "Automation",
  "concepts": [
    {
      "name": "Automation",
      "score": 0.7332677245140076
    },
    {
      "name": "Medicine",
      "score": 0.7295197248458862
    },
    {
      "name": "Systematic review",
      "score": 0.7273409962654114
    },
    {
      "name": "Strengths and weaknesses",
      "score": 0.7012311816215515
    },
    {
      "name": "Process (computing)",
      "score": 0.5735645890235901
    },
    {
      "name": "Data extraction",
      "score": 0.47465163469314575
    },
    {
      "name": "Work (physics)",
      "score": 0.44237253069877625
    },
    {
      "name": "Data science",
      "score": 0.4242267906665802
    },
    {
      "name": "Process management",
      "score": 0.3907003700733185
    },
    {
      "name": "Management science",
      "score": 0.33054906129837036
    },
    {
      "name": "MEDLINE",
      "score": 0.2815024256706238
    },
    {
      "name": "Computer science",
      "score": 0.22290295362472534
    },
    {
      "name": "Engineering",
      "score": 0.10871818661689758
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    }
  ],
  "cited_by": 546
}