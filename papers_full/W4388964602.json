{
    "title": "From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models",
    "url": "https://openalex.org/W4388964602",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4383987833",
            "name": "Englhardt, Zachary",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1511551846",
            "name": "Ma ChengQian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286867160",
            "name": "Morris, Margaret E.",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Xu, Xuhai \"Orson\"",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2153570339",
            "name": "Chang Chun Cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222188487",
            "name": "Qin, Lianhui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747607235",
            "name": "McDuff, Daniel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1970668181",
            "name": "Liu Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3107224813",
            "name": "Patel, Shwetak",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4290627724",
            "name": "Iyer, Vikram",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2726276264",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4321524280",
        "https://openalex.org/W2093973716",
        "https://openalex.org/W3139752690",
        "https://openalex.org/W3122544540",
        "https://openalex.org/W4288707157",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W1894490285",
        "https://openalex.org/W4320854854",
        "https://openalex.org/W4323570543",
        "https://openalex.org/W4307309259",
        "https://openalex.org/W4363671699",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W2516086211",
        "https://openalex.org/W4361230825",
        "https://openalex.org/W233771590",
        "https://openalex.org/W2954300939",
        "https://openalex.org/W2794997027",
        "https://openalex.org/W2865118579",
        "https://openalex.org/W2991609628",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W3158399027",
        "https://openalex.org/W2109664771",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3015745017",
        "https://openalex.org/W4223964620",
        "https://openalex.org/W2597891111",
        "https://openalex.org/W2946439262",
        "https://openalex.org/W2792161256",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W2011934784",
        "https://openalex.org/W4367623495",
        "https://openalex.org/W3178269122",
        "https://openalex.org/W4319837253",
        "https://openalex.org/W2495683356",
        "https://openalex.org/W2097385426",
        "https://openalex.org/W3148026034",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W2015114767",
        "https://openalex.org/W3033293557",
        "https://openalex.org/W4322708560",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3037651932",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4308610353",
        "https://openalex.org/W4378498682",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2986671035",
        "https://openalex.org/W2414738784"
    ],
    "abstract": "Passively collected behavioral health data from ubiquitous sensors holds significant promise to provide mental health professionals insights from patient's daily lives; however, developing analysis tools to use this data in clinical practice requires addressing challenges of generalization across devices and weak or ambiguous correlations between the measured signals and an individual's mental health. To address these challenges, we take a novel approach that leverages large language models (LLMs) to synthesize clinically useful insights from multi-sensor data. We develop chain of thought prompting methods that use LLMs to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety. We first demonstrate binary depression classification with LLMs achieving accuracies of 61.1% which exceed the state of the art. While it is not robust for clinical use, this leads us to our key finding: even more impactful and valued than classification is a new human-AI collaboration approach in which clinician experts interactively query these tools and combine their domain expertise and context about the patient with AI generated reasoning to support clinical decision-making. We find models like GPT-4 correctly reference numerical data 75% of the time, and clinician participants express strong interest in using this approach to interpret self-tracking data.",
    "full_text": "From Classification to Clinical Insights: Towards Analyzing and\nReasoning About Mobile and Behavioral Health Data With Large\nLanguage Models\nZACHARY ENGLHARDT∗ and CHENGQIAN MA∗, University of Washington, USA\nMARGARET E. MORRIS and CHUN-CHENG CHANG, University of Washington, USA\nXUHAI \"ORSON\" XU, Massachusetts Institute of Technology, USA\nLIANHUI QIN, University of California, San Diego, USA\nDANIEL MCDUFF and XIN LIU, University of Washington, USA\nSHWETAK PATEL and VIKRAM IYER,University of Washington, USA\nPassively collected behavioral health data from ubiquitous sensors could provide mental health professionals valuable insights\ninto patient’s daily lives, but such efforts are impeded by disparate metrics, lack of interoperability, and unclear correlations\nbetween the measured signals and an individual’s mental health. To address these challenges, we pioneer the exploration of\nlarge language models (LLMs) to synthesize clinically relevant insights from multi-sensor data. We develop chain-of-thought\nprompting methods to generate LLM reasoning on how data pertaining to activity, sleep and social interaction relate to\nconditions such as depression and anxiety. We then prompt the LLM to perform binary classification, achieving accuracies of\n61.1%, exceeding the state of the art. We find models like GPT-4 correctly reference numerical data 75% of the time.\nWhile we began our investigation by developing methods to use LLMs to output binary classifications for conditions like\ndepression, we find instead that their greatest potential value to clinicians lies not in diagnostic classification, but rather\nin rigorous analysis of diverse self-tracking data to generate natural language summaries that synthesize multiple data\nstreams and identify potential concerns. Clinicians envisioned using these insights in a variety of ways, principally for\nfostering collaborative investigation with patients to strengthen the therapeutic alliance and guide treatment. We describe\nthis collaborative engagement, additional envisioned uses, and associated concerns that must be addressed before adoption in\nreal-world contexts.\nCCS Concepts: • Human-centered computing →Ubiquitous and mobile computing ; • Applied computing →Life\nand medical sciences ; • Computing methodologies →Artificial intelligence.\nAdditional Key Words and Phrases: Passive sensing, large-language-models, clinical insights, mental health\n1 INTRODUCTION\nMobile and wearable sensors that collect health and fitness data have seen explosive growth over the past\nfive years [ 23, 27]. The sensing capabilities of products such as Fitbit and Apple Watch have dramatically\nadvanced beyond simple step counts to include optical heart rate sensors and two lead electrocardiogram (ECG)\nmeasurements that can provide clinicians valuable information about a patient’s symptoms outside their practice.\nBeyond simply tracking a run, or checking for high heart rate, researchers have shown the potential of leveraging\nsuch passive sensing data to model high-level, complex behaviors and mental health. [55, 64, 67, 74].\nDespite their potential, mobile and wearable sensor data use in clinical mental health practice faces four key\nchallenges. First, there is a lack of trust and clinically validated data; for instance, there is high uncertainty\nabout the specific relationships between mobile health signals (such as activity levels) and mental states like\n∗Both authors contributed equally to this research.\nAuthors’ Contact Information: Zachary Englhardt, zacharye@cs.washington.edu; Chengqian Ma, University of Washington, Seattle, Washing-\nton, USA; Margaret E. Morris; Chun-Cheng Chang, University of Washington, Seattle, Washington, USA; Xuhai \"Orson\" Xu, Massachusetts\nInstitute of Technology, Cambridge, Massachusetts, USA; Lianhui Qin, University of California, San Diego, La Jolla, California, USA; Daniel\nMcDuff; Xin Liu, University of Washington, Seattle, Washington, USA; Shwetak Patel; Vikram Iyer, vsiyer@uw.edu, University of Washington,\nSeattle, Washington, USA.\narXiv:2311.13063v3  [cs.AI]  24 Aug 2024\n56:2 • Englhardt and Ma, et al.\ndepression [53]. Second, clinicians struggle with the additional burden of incorporating mobile health data during\ntreatment and often have difficulty interpreting non-standard signals generated by wearable devices and mobile\nphones [28, 62]. Third, conventional machine learning (ML) approaches for interpreting sensor data perform\npoorly on abstract relations. For example, recent work evaluating the ability of 19 different ML models to predict\ndepression using self-tracking data revealed that many achieved accuracies below 50% [74]. Finally, the qualitative\naspects of mental health further limit the utility of binary data classifications since diagnoses on their own\ncontain no context about the patient’s lifestyle and history, which are critical to designing treatment plans [14].\nGiven the challenges inherent in sensor- and ML-based approaches for health data analysis, it becomes\nimperative to explore alternative methods that offer flexibility, adaptability, and deeper analytical capabilities. This\npaper therefore considers the potential of LLMs to transform the way we approach sensor data in health settings.\nUnlike traditional ML techniques, LLMs can generate reasoning that incorporates cross-domain knowledge,\npotentially enabling the processing of multifaceted sensor data with greater contextual understanding [ 11].\nThis shift in methodology from rigid, classification-focused models to LLM’s more versatile and interpretative\ncapabilities offers a promising avenue for advancing health data analysis.\nIn this context, we contribute the following key insights and findings:\n(1) We perform the first exploration of LLM use to process multi-sensor ubiquitous wearable data. We develop\na series of new prompting and model fine tuning strategies that enable LLMs to perform zero-shot\ndepression classification using raw, multi-modal wearable sensor data. We explore the effects of data input\nformats and numerical accuracy on state-of-the-art LLMs like GPT-4 [47], PaLM 2 [16] and GPT-3.5 [49].\nWe demonstrate that these strategies outperform classical ML methods on state-of-the-art depression\nclassification and highlight performance variations across models.\n(2) As an intermediate step to classification, we observe that LLMs can generate text reasoning about multi-\nsensor data, correctly describing trends and anomalies in the data and making connections between\nmultiple input signals and relevant mental health scenarios.\n(3) To evaluate the feasibility of this approach, we undertake an interactive interview study with mental health\nprofessionals to critically assess the practicality and limitations of interpreting mobile health data using\nLLMs, gain insight into the clinical relevance of LLM-generated reasoning, and to consider the prospective\nrole of LLMs in mental health contexts. We find that clinicians express a strong desire to have access to\nLLM-based tools for collaborative investigation of mobile health data with patients, and we outline a series\nof scenarios for clinician-patient-AI collaborative therapy .\n2 RELATED-WORK\n2.1 Multi-sensor Passive Sensing for Health and Well-being\nSmartphones and wearable devices, now ubiquitous in our lives, function as passive sensors, seamlessly capturing\na vast range of data. Their near-constant presence enables unobtrusive and continuous monitoring of behavior,\nactivity, and physiological signals. Over the last decade, significant progress has been made in passive sensing\nand behavioral modeling, impacting areas such as physiological health condition detection [5, 42, 75], monitoring\nmental health status [65, 72], measuring job performance [41, 43], tracking education outcomes [66, 80], and\ntracing social justice [56]. Researchers employ various methods, including statistical analysis and conventional\nML models, to explore these areas.\nIn the mental health context, initial research established statistical correlations between mental health condi-\ntions and mobile sensing data. For instance, Saeb et al. [54] identified significant correlations between depression\nscores and smartphone usage patterns, and Ben-Zeev et al . [7] identified links between changes in depres-\nsion severity levels and features related to sleep duration, speech duration, and mobility. More recent efforts\nFrom Classification to Clinical Insights • 56:3\nhave focused on leveraging these results to build ML models for mental health disorder diagnosis and detec-\ntion [15, 44, 63, 67]. To further growth in this area, Xu et al . [74] collected and released a multi-year passive\nsensing dataset and platform that covers a wide range of physical health, mental health, and social well-being\nmeasurements. However, most research in this domain relies on conventional statistical and ML methods, and\nthe recent improvements in the performance of large foundation models [11] present an opportunity to explore\nnew techniques for analyzing passively collected sensor data.\n2.2 LLMs for Health Applications\nThe success of transformer-based language models, such as BERT [19] and GPT [50], has led to the development\nof larger and more powerful language models (e.g., GPT-3 [10] and T5 [51]). Instruction fine-tuning by including\ninstructions (i.e., prompts) from a range of datasets and task domains during both the training and generation\nphases has led to the development of single models that can perform a wide range of tasks [69]. These instruction-\nfine-tuned LLMs, such as GPT-4 [47], PaLM [17], FLAN-T5 [18], LLaMA [61], and Alpaca [60], contain tens to\nhundreds of billions of parameters and achieve a promising level of performance on a variety of tasks, such as\nquestion answering [46, 52], logic reasoning [70, 82], machine translation [8, 22], and more.\nIn the health sector, these LLMs have been applied in several studies [30, 36, 38, 45, 59, 71]. For example, Singhal\net al. [59] utilized a fine-tuned version of PaLM-2 to score as high as 86.5% on the MedQA dataset. Similarly,\nWu et al. [71] fine-tuned LLaMA on a corpus of academic medical papers and textbooks, yielding promising\nresults on multiple biomedical QA datasets. Jiang et al. [30] trained a medical language model on unstructured\nclinical notes from the electronic health record and fine-tuned for performance across a wide range of clinical\nand operational predictive tasks. These examples underscore the versatility and potential effectiveness of LLMs\nin the medical space.\nIn the mental health domain, LLMs have been explored for applications such as sentiment analysis and\nemotional reasoning [32, 49, 81]. Lamichhane [34], Amin et al. [2]. Yang et al. [78] tested the performance of\nChatGPT on multiple classification tasks (stress, depression, and suicide risk) and found that it shows initial\npotential for these mental health applications, but it has room for significant improvement.\nDespite this past work, scant research focuses specifically on integration with mobile and wearable health data ,\nwith most of the existing literature exploring text data rather than multi-sensor streams. Closer to our work, Liu\net al. [39] demonstrated that with only few-shot tuning, a LLM can ground various physiological and behavioral\ntime-series data and make meaningful inferences on numerous health tasks (e.g., heart rate measurement, atrial\nfibrillation detection, and mood score prediction). However, their work is based on self-curated toy datasets\nconsisting of well-described physiological signals and behaviors.\n3 MATERIALS AND METHODS\nIn this section we outline the procedures used in our experiments. We first explore the use of LLMs for depression\nclassification. In these experiments, we notice that LLMs produce reasoning about the input mobile health data\nbefore providing a classification response. We grade this reasoning for numerical accuracy. Next, we conduct\nan interactive evaluation with mental health professionals to gauge the quality of LLM-generated reasoning as\nit relates to mental health and gain insights into the potential applications for LLM-based tools in therapeutic\ncontexts.\n3.1 Dataset\nIn this paper, we use the GLOBEM dataset [ 76], with its extensive collection of passive sensing data from\nsmartphones and wearables and its wide range of well-being metrics. GLOBEM includes weekly Ecological\nMomentary Assessment (EMA) surveys to capture the recent status of participants’ mental health evaluations.\n56:4 • Englhardt and Ma, et al.\nReasoning\nYes/No\nReasoning\nYes/No\nLLM OutputPrompt ComponentsPrompt Strategy\nDirect \nPrediction\nAsk for\nClassi/f_ication\nForma/t_ted\nData Table\n Yes/NoRole Task\nDescription\nChain-of-\n/T_hought (COT)\nAsk for\nClassi/f_ication\nAsk for\nReasoning\nForma/t_ted\nData Table\nReasoning\nYes/No\nRole Task\nDescription\nCOT +\nVariable \nDescription\nAsk for\nClassi/f_ication\nAsk for\nReasoning\nDescribe\nVariables\nForma/t_ted\nData Table\nRole Task\nDescription\nCOT +\nVariable Desc. +\nDSM-V Def.\nAsk for\nClassi/f_ication\nAsk for\nReasoning\nDSM-V\nDe/f_inition\nDescribe\nVariables\nForma/t_ted\nData Table\nRole Task\nDescription\nFig. 1. The overall workflow of our prompting strategies, detailing the specific components included in each type of prompt.\nFor examples of the text included in each block, see Appendix B.\nWithin EMA measures, we use Patient Health Questionnaire 4 (PHQ-4) [33] as the ground truth in our depression\nclassification task [25, 26, 29]. Paired with the survey data, GLOBEM contains 24×7 sensing data that measures\nuser behavior, such as steps, GPS locations, phone calls, social activity proxies, and more. The dataset then\nextracts hundreds of features (time at home, time asleep, etc.) from these raw measurements.\n3.1.1 Model Input Setup. To constrain the input token length, we select a subset of 16 diverse features, including\nLocation, Phone Usage, Bluetooth, Calls, Physical Activity, and Sleep; we list all feature details in Appendix A.\nWe set the time length of the data as 28 days, with the final day of each window coinciding with the weekly EMA\nassessment. Therefore, the size of mobile and wearable sensor data for each data sample is 28×16 (28 days ×16\nfeatures). Due to the data protection guidelines for the GLOBEM dataset, we are unable to directly open-source\nraw excerpts of sensor data or fine-tuned LLMs from which excerpts of training data may extracted[ 13]. To\naid others in reproducing our results, we provide instructions and code to generate samples prepared from the\nGLOBEM dataset used in the following experiments in a GitHub repository1.\n3.1.2 Classification Label Setup. We focus on a binary classification task based on PHQ-4 scores, which attempt\nto quantify the severity of depression and anxiety from a range of 0 (normal) to 12 (severe) based on responses\nto a questionnaire. As an initial exploration, we avoid the borderline samples with PHQ-4 scores between\n1-5, following PHQ-4 criteria [33] and supported by additional work finding frequent disagreement between\nself-reported and clinician-administered scores for borderline cases [ 3]. Thus, the classification task aims to\ndistinguish samples with PHQ-4 below 1 or above 5. To create a balanced test set, we randomly sampled 30 data\npoints from each year with an equal distribution of labels to equally represent a range of non-borderline cases. In\ntotal, the test data set contains 90 class-balanced samples from three years.\n3.2 Classification with LLMs\n3.2.1 Depression Classification. We develop prompting strategies to enter the raw data into LLMs along with\nvarying amounts of context and instructions to produce a depression classification result. For each strategy, we\ninclude prompt text for the model’s role and task description, followed by the concatenated raw sensor data in\n1https://github.com/ubicomplab/classification-to-clinical\nFrom Classification to Clinical Insights • 56:5\n1. Time Spent at Home:\n    - /T_he time spent at home seems to be decreasing around the \n       middle of the month, notably on 2019-05-14 and 2019-05-15 \n       where it dips to 456 and 167 minutes respectively. /T_his could \n       indicate some unusual activity or a signi/f_icant change in \n       routine.\n2. Sleep Patterns:\n    - /T_he total time asleep varies across the month. On 2019-05-04, \n       the individual slept for only 75 minutes. On the other hand, \n       on 2019-05-13, they slept for 514 minutes. Erratic sleep \n       pa/t_terns can be an indicator of various issues, including \n       stress, anxiety, or other disorders.\n    - /T_he total time spent awake while in bed also sees /f_luctuations,\n       with some nights having prolonged awake times, suggesting\n       possible insomnia or restless sleep.\nGPT-4 Reasoning:Raw Data:\nFig. 2. A plotted excerpt of raw mobile health data and the resulting analysis generated by GPT-4.\nvarying formats, and we proceed to add additional context like variable descriptions and instructions for the\ntask.For example, a prompt would begin with:\nRole: You are a data analyst helping a psychiatrist understand human activity data.\nTask: You will be shown data gathered from a smartphone and smart watch worn by an individual. Your goal is to\nanalyze this data. You are presented with the following: 1. A table consisting of twenty-eight days of collected activity\ntracking data <formatted data> ...\nWe show a diagram of each of the utilized prompting formats in Figure 1 and a detailed description of the text\nformatting of each block in Appendix B. We focus on three different variants of prompting, all on a zero-shot\nsetting detailed below:\n(1) Direct Prediction (DP): we directly ask the LLM to perform depression classification with prompts that\ninclude only the basic role and task information and the formatted sensor data.\n(2) Chain-of-Thought (CoT) prompting: building upon direct prediction, we induce models to perform step-\nby-step reasoning with carefully crafted instructions to hypothesize about the subject’s overall mental\nhealth.\n(3) Reasoning with extra information: based on CoT, we provide extra task-related domain information, such\nas more detailed explanations of input variables (Exp) and the depression criteria from the Diagnostic and\nStatistical Manual of Mental Disorders, Fifth Edition (DSM-V)[1].\nWe test these prompting variants on top of three state-of-the-art LLM models: GPT-3.5, GPT-4 and PaLM\n2. We evaluate the DP method with multiple data input formats to determine which is most suitable for the\nselected LLMs. Specifically, we tried four ways to format these raw data: comma-separated values (CSV), Tabular,\nMarkdown and LaTeX, as shown in Appendix D. We found that Markdown performed best, so we utilize the\nMarkdown table format in all other experiments.\nUsing these strategies, we observe that prior to outputting a classification decision, the models first produce\nintermediate text with reasoning and analysis of the input data. Fig 2 shows an example, with a subset of the raw\ntime series input to the LLM plotted for visualization along with a subset of the analysis text produced by GPT-4.\nThese outputs form the basis for our clinician study and envisioned use cases in which these generative LLM\noutputs can enable new capabilities that go beyond classification to aid clinicians in collaborative therapy. We\ndiscuss these extensively in Sections §3.4 and §4.6.\n56:6 • Englhardt and Ma, et al.\nTo contextualize the classification performance of LLMs, we also introduce a few baselines, including a classic\nML method, Random Forest (RF) [ 9], as well as the state-of-the-art self-supervised learning method Reorder,\nproposed by [74] on this same dataset. Given that these comparison baseline ML models do not support a zero-shot\nsetting and require pre-training, we curated a training dataset for comparative analysis. This dataset comprises\nthe remaining data points that meet the PHQ-4 thresholds, which excludes the data points in the testing dataset,\ntotaling 384 samples. These were formatted into two structures: the first is identical to the test set samples with\na structure of 28 days ×16 features. The second is a set of 16 features computed by taking the average along\nthe time dimension for the Random Forest. We conducted evaluations using Random Forest (RF) and Reorder\nalgorithms, alongside the Reorder algorithm with the original implementation in [73] with 54 features (Reorder-54\nFeatures). These evaluations were performed on the same test set as the LLMs.\n3.2.2 Fine-Tuning. Evaluating the CoT reasoning produced by GPT-3.5 reveals that unlike GPT-4, smaller LLMs\nrarely incorporate analysis that relates specifically to the numerical values of the mobile health data, instead\ngenerating generic statements about mental health. Several studies[24, 58, 77] have shown that the step-by-step\nCoT reasoning capabilities of larger models can be distilled into smaller models through fine-tuning on the output\nof larger models. This raises an important question: can we use instruction fine-tuning to enable smaller LLMs to\nperform more effectively on such challenging wearable classification tasks?\nTo answer this question, we explore using the reasoning responses generated from the GPT-4 (see Appendix\nC) to fine-tune the GPT-3.5 model. For the fine-tuning experiment, the prompt design was kept consistent with\nthe same methodology outlined above. Utilizing GPT-4, we generate a collection of reasoning responses based\non data external to the test set. From this assortment, correctly classified reasoning responses free of numerical\nerrors were selected to form a candidate training set. We make a balanced instruction training set, comprising 70\nsample with high-quality reasoning, evenly distributed between positive and negative examples, and fine-tune\nGPT-3.5 on this dataset.\n3.2.3 Anxiety Classification. In addition to depression, we also explore the generalization abilities of LLMs to\nother mental health classification tasks. Specifically, we use the same CoT prompt design as in the depression\nclassification experiment described in Section 3.2.1, with the modification of asking for anxiety classification\nbased on the PHQ-4 anxiety sub-score instead of the depression classification.\n3.3 Evaluating Numerical Accuracy\nWe begin by investigating the accuracy of the LLMs when referencing the input data. Can the models identify\nreal and specific trends in the data? Do they hallucinate numbers, or are the outputs truly reflective of the input\ndata? To evaluate this, we utilize human graders to objectively evaluate the numbers and stated trends referenced\nin LLM responses against the input timeseries data according to a fixed rubric. We score a total of 480 responses,\nevenly split across four different models: PaLM 2, GPT-3.5, fine-tuned GPT-3.5, and GPT-4.\n3.3.1 Producing Reasoning Samples. Instead of asking to hypothesize about the health of the patient, we tune\nthis prompt to produce analysis on trends in the data. We show the specific prompt used in Appendix E. For\ninput data, we select random samples from the test set with a PHQ-4 score of greater than 5, indicating likely\nmoderate to severe depression. We select these samples since they are most likely to have a trends or anomalies\non which we can evaluate the model. These excerpts consist of the same 16 features used in Section 3.2 formatted\nin markdown format. We produce 8 samples from each of the four models per set of input data.\n3.3.2 Participants. To grade reasoning excerpts, we recruited 15 individuals over the age of 18 through fliers\nplaced around a university campus and word of mouth. Participants were not required to have any domain\nknowledge of mental health to complete the assigned task. To aid in recruitment, compensation of a gift card\nFrom Classification to Clinical Insights • 56:7\n($20 USD) was offered to all participants included in the study. The study protocol was submitted to the IRB at\nthe host institution for the study and deemed exempt from a full review. We follow all IRB procedures to avoid\npotential conflict of interest.\n3.3.3 Prompt Grading Procedure. Each participant was sent an online form that contained a set of 32 randomly\nordered responses (8 from each LLM). To reduce grader burden, all 32 responses were generated from the same\ninput data. Graders were also provided the raw data input to the model in tabular form as well as timeseries plots\nof each data feature. For each reasoning excerpt, graders were asked to answer the following four questions:\n(1) Does this response include numbers?\n(2) Are these numbers consistent with the provided data?\n(3) Does this response identify specific trends?\n(4) Are these trends consistent with the provided data?\nGraders were given specific instructions on how to evaluate each question as well as a series of graded example\nresponses; we show the rubric provided to graders in Appendix F. We provide explicit instructions to evaluate\nsolely the numbers and trends against the provided data table and plots, disregarding any conclusions the\nresponses may make about how these numbers or trends might relate to mental health or other factors. An\nexcerpt of plotted data and LLM-generated reasoning graded by participants is shown in Fig. 2.\n3.4 Clinician Evaluation\nAs shown in Fig 2, the LLMs also generate analyses about the timeseries data. To evaluate the quality of this\nreasoning and understand how these clinical insights could be used in practice, we conducted a user study with\nclinician experts.\n3.4.1 Participants. Eight mental health professionals completed this study: six with PhDs in Clinical Psychology\nand two with master’s degrees. Participants described their approaches as Cognitive-Behavioral Therapy (2),\nAcceptance and Commitment Therapy (1), Dialectical Behavioral Therapy (1), Psychodynamic Therapy (1),\nRelational/Interpersonal Therapy (2), and Family Systems Therapy (1). They reported working in a variety of\nsettings, including academic medicine, group private practice, individual private practice, and community mental\nhealth, across four states in the U.S. Participants were recruited through postings on group practice mailing lists,\nsocial media groups for practicing therapists, and word of mouth. To aid in recruitment, gift cards ($50 USD)\nwere offered to all who participated in the study.\n3.4.2 Ethics. This study protocol was submitted to the IRB at the host institution and was deemed exempt from\nformal IRB review. Participants were sent an information sheet outlining the study process and data management\nprocedures prior to interviews. At the start of the interview but before initiating recording, researchers described\nthe interview process, addressed questions, and obtained spoken consent.\n3.4.3 Data Format. The data analyzed in each interview session consisted of a random 28-day sample from a\nparticipant in the GLOBEM dataset [76]. Only individuals with a PHQ-4 > 5 were selected (as in Section 3.2) to\nobtain a sample representative of individuals who might be likely to seek therapy. Time-series plots that included\neach variable were generated. The generative reasoning evaluation drew on the same 16 features and format\nused for the time series (listed in Appendix A). The chat thread was started with the following prompt, similar to\nthe prompt used in the previous section, but altered slightly for compatibility with the web-based chat interface\nof GPT-4: Below is some data gathered from a fitness tracking smartwatch and a smartphone. Although it does not\ncontain explicit information on mood, trends in physiological signals have been shown to correlate with mental health\nsymptoms. Examine this data and point out any specific trends or data points that could spark fruitful conversation\nwith a mental health professional. <formatted-data> .\n56:8 • Englhardt and Ma, et al.\nStudy Team ClinicianScreen Share of LLM Interface\nFig. 3. Setup for interactive clinician evaluation. Clinicians interacted with live GPT-4 sessions via screen sharing over Zoom.\n3.4.4 Interview and Interactive Exercise. Interviews were one hour in length and conducted over Zoom to enable\nrecording and transcript generation. Participants were not shown the results of other experiments conducted\nin this paper. Interviews were conducted by two researchers, a clinical psychologist and a computer science\ngraduate student.\nInterviews began with a discussion of the clinician’s practice, particularly as it includes patient self-monitoring.\nWe then asked participants to imagine a scenario in which they had received a month of self-tracking data in\nadvance of their first meeting with a patient. To build on this scenario, we shared examples of self-tracking data\nfrom the same publicly available data set used above [74] in two forms. First, we presented a set of 16 time series\nplots (in a shared Google doc), and clinicians were asked to provide feedback on anything they noticed in the\ndata that they might use in a therapeutic context.\nNext, we began a live interactive session with GPT-4 (using ChatGPT) through Zoom screen sharing, as shown\nin Figure 3. The mobile health data was then input into GPT-4 to produce a text response in real-time. We asked\nparticipants to read the response and talk about any reactions they had, including ways the data and GPT-4’s\nobservations about the data might shape their thinking about the patient. Participants were asked to type or\nsay aloud follow-up queries that they had for GPT-4 (which were entered by the interviewer running the GPT-4\nsession). We also discussed their reaction to the responses that GPT-4 gave to these queries. We then asked\nparticipants to make up a hypothetical example in which they used this tool with a therapy patient, prompting\nthem for what types of data or inputs should be available, what kinds of analyses they would like to see, and how\nthey would envision using this tool with the patient. Finally, we asked general questions about treatment and\nhow this tool might affect relationships with patients.\n3.4.5 Post-Interview Survey. After the interview, participants completed a short online survey where they\nindicated their agreement or disagreement with several statements about GPT-4 on a seven-point scale.\n4 RESULTS\nIn this section, we provide results of the depression classification, fine-tuning, and anxiety classification\nexperiments as well as the numerical accuracy and clinician evaluations, all described in Section 3.\n4.1 Depression Classification\nTo evaluate multiple data input formats, we use the classification results of depression through Direct Prediction\n(DP) as a measure of these formats. From Figure 4, we observe that CSV, Tabular and Markdown formats exhibit\ncomparable performance levels, and these results are consistent in both GPT-4 and PaLM 2. In contrast, the LaTeX\nformat demonstrates a performance gap compared to the other three formats. These results align logically with\nexpectations, considering the predominant sources of training data for LLMs. The vast majority of this data is\nFrom Classification to Clinical Insights • 56:9\nGPT3.5 GPT4 Palm 2\n0\n10\n20\n30\n40\n50\n60\n70Percent Correct\nFormat\nCSV\nT abular\nMarkdown\nLaT eX\nFig. 4. Depression classification performance across a range of possible data input formats.\nFig. 5. Comparison of classification results of GPT-3.5, GPT-4, and PaLM 2 across four different prompting strategies, along\nwith results from Reorder and Random Forest models trained on the same dataset. The performance of the Reorder model\ntrained on more features in Xu et al. [74] is included for comparison. Observe how for LLMs, the percent of positive (\"Yes\")\nand negative (\"No\") classifications varies significantly based on the prompting strategy used.\nsourced from the Internet, where formats like CSV and Markdown are far more prevalent than LaTeX. Given\nthis disparity in data availability, it stands to reason that LLMs would exhibit higher accuracy in processing and\ninterpreting CSV and Markdown inputs compared to LaTeX. Since Markdown shows the best overall performance,\nwe chose Markdown as the data format for our subsequent experiments.\nWe tested all prompting strategies with three state-of-the-art LLM models, GPT-3.5, GPT-4 and PaLM 2 in\nMarkdown format. Figure 5 reveals that while GPT-3.5 attains an accuracy rate of 50%, it does not effectively\naddress the question posed. Instead, it consistently defaults to a response of ’No’, which results in an inflated 50%\naccuracy due to our balanced dataset. Figure 5 shows that CoT improves the accuracy of both GPT-4 and PaLM\n2 compared to DP, which aligns with the results found in many related studies [68, 70]. Using the CoT + Exp.\nstrategy, PaLM 2 achieved the highest accuracy of 61.11%. However, adding information, even if accurate and\npertinent to the topic, does not always increase performance. Both PaLM 2 and GPT-4 perform their worst, at\n48.89% and 51.11%, respectively, when provided with the DSM-V description of depression as part of the prompt.\nIn these cases, we see that this results in a significant increase in the percentage of samples classified as depressed,\nwith GPT-4 classifying as high as 98.89% of samples as positive.\nThe performance metrics reveal that the accuracies of Random Forest, Reorder, and Reorder-54 Features are\n53.33%, 51.11%, and 58.89%, respectively. Notably, among the two baseline models utilizing the same set of 16\n56:10 • Englhardt and Ma, et al.\nRandom\nForest\nGPT-3.5 GPT-3.5\nFinetuned\nGPT-4 PaLM 2\n0\n10\n20\n30\n40\n50\n60\n70Percent Correct\nFig. 6. Comparison of depression classification performance for GPT-3.5 fine-tuned for chain-of-thought (CoT) reasoning\ndepression classification against CoT performance of GPT-3.5, GPT-4, and PaLM 2. The performance of the Random Forest\nmodel is included as a baseline.\nRandom\nForest\nGPT-3.5 GPT-3.5\nFinetuned\nGPT-4 PaLM 2\n0\n10\n20\n30\n40\n50\n60\n70Percent Correct\nFig. 7. Comparison of anxiety classification performance for GPT-3.5 fine-tuned for chain-of-thought (CoT) reasoning\ndepression classification against CoT performance of GPT-3.5, GPT-4, and PaLM 2. The performance of the Random Forest\nmodel is included as a baseline.\nfeatures provided to the LLMs, neither could surpass the zero-shot Chain of Thought (CoT) results achieved using\nthe GPT-4 and PaLM 2 models.\n4.2 Fine-Tuning\nAfter fine-tuning GPT-3.5 with 2 epochs using this balanced instruction-tuning set, we see an improvement in\nits performance. Though it initially fails to properly perform classification, our fine-tuned version of GPT-3.5\nachieves an accuracy of 56.67% on the test set, which exceeds the baseline Random Forest model performance of\n53.33% and is closer to the GPT-4 performance of 57.78% on the same dataset and prompt structure.\n4.3 Anxiety Classification\nAs in the depression classification tasks, we observe that GPT-3.5 still always defaults to a \"No\" response. GPT-4\ncan achieve an accuracy of 55.56%, while PaLM 2 achives a slightly higher accuracy of 56.67%. Notably, this result\nmirrors the trend observed in the depression classification results. For our fine-tuned GPT-3.5 model, although\nthe model is fine-tuned for depression classification, it still shows some improvement in the anxiety detection\nFrom Classification to Clinical Insights • 56:11\nReferences\nNumerical\nData\nNumbers\nare All\nCorrect \nIdentifies\nTrends\nAll Trends\nCorrect\n0%\n20%\n40%\n60%\n80%\n100%Percent Correct\nGPT-3.5\nGPT-3.5 Finetuned\nGPT-4\nPaLM 2\nFig. 8. Evaluation of the reasoning excerpts generated by GPT-3.5, Fine-tuned GPT-3.5, GPT-4, and PaLM 2.\ntask compared to the original GPT-3.5. This indicates the potential of fine-tuning to increase performance on\nclassification tasks.\n4.4 Evaluating Numerical Accuracy\nFigure 8 shows the results of our study. Interestingly, we find that while PaLM 2 performs slightly higher on\nthe classification tasks above, GPT-4 performs significantly better across our study rubrics. For example, while\nPaLM 2 also identifies trends at a high rate, GPT-4 is more likely to identify all trends correctly and include\nreferences to the numerical data, achieving scores exceeding 75% accuracy. We further note that the graders\nevaluated data as simple yes/no questions, meaning that all numbers and trends had to be correct. Although we\nobserve some errors in the model outputs, even these responses often contain correct trends as well that may\nhave utility in a collaborative human-AI approach. For example, if the LLM can observe an outlier or anomaly in\nthe data, this is often easily visible to the user as well for confirmation.\n4.5 Clinician Evaluation\nWe discuss findings from clinical interviews in two categories. First, we describe envisioned usages, that is\nhow clinicians thought about incorporating the tool into their practice. Second, we describe concerns that came\nup as clinicians envisioned using the tool. We preface these findings by noting a general interest among clinicians\nin having access to LLM-based tools in their practice (see the post-study survey responses in Fig. 9.\n4.6 Envisioned Clinical Uses\nThree major uses were envisioned by clinicians: collaborative investigation, identifying questions to explore, and\ndocumentation.\n4.6.1 Collaborative In-Session Investigation. Clinicians generally saw the most value in this tool to aid collabora-\ntion with patients by using it as an interactive data explorer during a therapy session . For example, one clinician\nimagined querying the model to identify triggers for panic attacks and other anxiety symptoms. Another clinician\noutlined the high-level steps she would use if incorporating the model: “First, set the goal... What’s bringing them\n56:12 • Englhardt and Ma, et al.\n1 2 3 4 5 6 7\nStrongly Disagree           Neutral              Strongly Agree\nThe model's statements responses were clear and easy to understand.\nIt observed patterns that are relevant to mental health.\nI trusted its observations (e.g. about differences in sleep and activity from day to day).\nIt made reasonable interpretations and conclusions from those patterns.\nThose interpretations are consistent with clinical understanding of mental health conditions.\nMy patients self-track and self-monitor at my encouragement.\nMy patients self-track due to their own interest.\nCurrently, I rely on patient recall or quick glances at self-tracking data to form impressions of it.\nThis type of tool could help me make sense of self-tracking or self-monitoring data.\nThis tool could enhance treatment.\nThis tool could worsen treatment.\nI would like to have access to a tool like this at my practice.\nFig. 9. Post-study survey responses from clinicians, indicating strong disagreement (1) to strong agreement (7) with statements\nabout their experience interacting with GPT-4. We observe positive feedback and enthusiasm across most questions.\nin? Then, agree on some metrics that are relevant. ” (P4). Others imagined using it to find evidence of change early in\ntreatment (e.g., indications of more energy or better sleep) as a patient struggled with whether to continue a\nparticular medication or therapy. One therapist gave an example of how she might, through discussion with a\npatient, tie a concern such as relationship anxiety to the average duration of phone use periods and then use that\nmetric to assess whether therapy was helping with the patient’s anxiety. Several imagined using it not only for\nretrospective analysis but also to forecast improvement . Another envisioned use was asking the model questions\nduring sessions to boost creativity , e.g., when challenging a patient’s worries or negative thoughts.\nAcross these and other forms of collaborative use, some clinicians wanted their patients to be able to query\nthe model . In addition to identifying patterns, joint use was envisioned as a way to build a more general\nfeeling of collaboration in the therapy, something that is important for the therapeutic alliance and positive\noutcomes [12, 40].\n4.6.2 Identifying Issues to Explore with the Patient. Clinicians appreciated the model’s ability to list concerns\nwith references to specific dates (e.g., days with little sleep or little movement). One clinician shared that it would\nbe useful for GPT-4 to prioritize the questions. Clinicians suggested that they would share their observations with\npatients, e.g., about particular days with anomalies such as decreased sleep, as a way of opening up discussions\nand jogging the patient’s memory.\nWhen discussing potential concerns, clinicians wanted the model to raise questions and supply clear data. As\none clinician said, the model \"Could cause less harm if it provided questions for a therapist to ask instead of\nconclusions for a therapist to rely on\" (P8). They did not want the model to apply diagnostic labels to individuals\nor their behaviors.\n4.6.3 Generating Documentation. Clinicians varied in their thoughts on whether models would meaningfully aid\nin documentation. One participant appreciated the neutral boilerplate language the model used and easily imagined\nit as the basis for an intake summary. Another had already started using AI software to aid in documentation\nof sessions. Others bristled at this idea, pointing out that the LLM might miss major insights and obviate the\nanalysis that comes from writing notes.\n4.6.4 Concerns. Clinicians raised additional concerns related primarily to privacy and quality of care.\nFrom Classification to Clinical Insights • 56:13\nPrivacy. As clinicians considered inputs, such as patient mood logs, that would increase the model’s relevance\nto mental health, they grappled with privacy and other ethical concerns. One clinician worried that it could be\nhard to obtain meaningful informed consent. Even if a model were secure and compliant with relevant patient\nprivacy regulations, it could be difficult for therapists to explain how the system worked and what protections it\nafforded.\nQuality of Care. Another concern pertained to relying on the model as a shortcut. Clinicians imagined the\nproblems that could arise from overconfidence in the model’s analysis. One clinician described the possibility of\nmissing the insights that would come from pouring over data herself: “If I was...doing this very quickly, like, before\nI see the client, I could totally see myself or anyone...just relying on on GPT-4. And just think . . . ’This is the answer.\nThis is the knowledge, ’ ... as mental shortcuts as opposed to pouring over the data yourself. . . . And I would want to\nask, ‘Am I missing anything?’ (P6). Another clinician worried that not taking the time to write one’s own notes\nand process each session could degrade the their memory of the session and ultimately shortchange the therapy.\nAnd contrasted her efforts to examine contextual factors, such as family conflict, with a model’s narrower focus\non symptoms. She noted that if relying solely on a model’s observations, a clinician could overlook important\nfactors in assessing and treating mental health struggles.\n5 DISCUSSION\nOur findings reveal a paradigm shift based on the value LLMs can bring to mental health care, switching from a\nfocus of using computational tools for diagnostic classification to using them to generate clinical reasoning. Most\nprior ML research in mental health has focused on classification, or predicting the diagnostic category determined\nby a clinical interview or score on a self-report inventory such as the PHQ. While these coarse categorizations\nhave a role in mental health care, they do not capture the specifics of a particular patient’s struggles and are\ninsufficient to meaningfully guide treatment. Therapeutic change relies on the patient becoming aware of their\npatterns and options for changing them: it is an active collaboration rather than a procedure performed on a\npatient based on a particular diagnosis. Further, therapy is hyper-personalized, grounded in each patient’s specific\nconcerns, goals, contexts, and dynamics.\nOur findings indicate that LLMs have the potential to illuminate an individual’s patterns by analyzing disparate\nsensor data that is otherwise unwieldy for clinicians to interpret. Unlike a diagnostic wizard used before treatment,\nLLMs can bring value in-session and throughout treatment. Instead of being used solely by the clinician or another\nprofessional in a care system, LLMs can be used collaboratively by the patient and clinician . In this vision, the LLM\ndoes not replace the clinician or replace dialogue between the clinician and patient. Instead, the LLM facilitates\ntheir dialogue , the alliance that supports change, and their investigation of the patient’s patterns.\nBelow, we first discuss the challenges of using LLMs for classification followed by further steps needed to\ndevelop end-to-end tools for the proposed clinician-patient-AI collaborative therapy.\n5.1 Challenges of Depression Classification\nThis study illuminated new approaches to and challenges of screening for depression based on mobile health data.\nOur best-performing method for classifying depression was incorporating chain-of-thought (COT) prompting\nand variable explanations with the PaLM 2 model, achieving an accuracy of 61.11% on our class-balanced dataset.\nAlthough this represents a modest improvement over classical ML methods such as Random Forest and the\nstate-of-the-art Reorder [74] method on this dataset,this level of accuracy is still far too low to be useful as a clinical\nscreening tool. It is important to note that personalized depression forecasting approaches where model training\ndata includes historical data from test set individuals report accuracy as high as 90.1% [ 35], but a reliance on\nextensive prior labeled data for each given individual significantly limits real-world utility.\n56:14 • Englhardt and Ma, et al.\nIn our experiments, we utilized a class-balanced dataset, as described in Section 3.1.2. This provides an\nindication of performance across an even distribution of individuals with potential PHQ-4 scores. With an\nunbalanced dataset constructed from all complete samples in the Globem dataset, the distribution would include\n34.8% positive samples as opposed to 50%. With this distribution, we would expect to see a shift in the optimal\nprompting strategies, improving performance for direct prediction and COT prompting methods more likely to\nclassify samples as not depressed and further decreasing performance of prompting strategies, including DSM-V\ndepression criteria.\nWe also attempted experiments to predict severity through a variety of methods, such as asking the LLMs\nto select a PHQ-4 score between 0 and 12 or options ranging from \"none\" to \"severe\" with varying degrees of\nadditional context. In these instances, though, we notice that all three LLMs always select the middle value of the\nprovided range. This result aligns with the findings of other works, which show that GPT-3.5 and GPT-4 both\nrequire few-shot prompting with multiple labelled input examples to perform these types of regression tasks\n[31]. We forego few-shot prompting experiments for this work since they would require either the individual to\ntrack PHQ-4 scores as ground-truth, which is the variable we are trying to predict with the LLM in the first place,\nor use labelled data excerpts from other participants, which raises the challenging question of how to source\nrepresentative samples from existing datasets to use for a new individual.\n5.2 Managing Inaccuracy in a Clinical Context\nClinicians in our study expressed a strong preference for LLMs to generate observations and insights about data\nrelating to a patient rather than apply diagnostic labels. Such usage of LLM-based tools for generative reasoning\navoids the challenges of inaccurate classification, but inaccuracies still need to be managed. In particular, well-\ndocumented numerical inaccuracies may impact the quality of the generated reasoning. In our study, reasoning\nresponses generated by GPT-4, based on a 448-element table (16 features x 28 days), contain at least one numerical\nerror 25% of the time. The necessary accuracy and tolerance for inaccuracy may vary with applications of the LLM.\nFor example, an inaccuracy in a question that is generated for the therapist to ask a patient may pose less risk than\nan error in a definitive statement that informs a clinician’s impressions of a patient’s mental state. These risks of\ninaccuracy may be mitigated by uses that prioritize the therapeutic relationship and the patient’s perspective. The\nclinicians in this study anticipated that they would use LLM-based tools in collaboration with patients, drawing\non the tool’s observations as a starting point for discussion. They emphasized that their treatment decisions and\ndiagnoses would be based on their direct patient interactions.\n5.3 Supporting Clinician-Patient-AI Collaboration\nSupporting the kind of collaboration we propose between clinicians, patients, and AI will require new approaches\nto LLM hosting and data protection. This is due in part to the sensitivity of the data required. To inform mental\nhealth treatment and illuminate the factors associated with a particular individual’s struggles, models need to\nhave data that are directly relevant to mental health and, ideally, personalized to individual patients. The clinicians\nwe interviewed expected the model to have, at a minimum, daily mood and symptom tracking and ideally more\nin-depth data related to mood, social interactions, and behavioral routines. In addition, a clinician may want to\ninput materials such as session notes or transcripts, manuals, or other documents describing relevant mental\nhealth issues and treatments. Much of this potentially personally identifiable data is sensitive and not appropriate\nas an input for existing LLM services such as ChatGPT, which may use this data to improve their models or for\nother commercial purposes. Privately hosted models or robust user data protections are required for this purpose.\nThe solution to the privacy challenges of using LLMs in therapy is not as straightforward as it is for compliant\nmedical record systems. Such record systems primarily serve providers and other employees at medical institutions,\nlimiting patients to read-only access to elements such as test results. In this study, we heard from clinicians that\nFrom Classification to Clinical Insights • 56:15\nboth patients and clinicians should be able to interact with the model. This joint interaction, clinicians anticipated,\ncould build patients’ curiosity about factors associated with their mental health and foster the collaborative\nalliance between patients and therapists that is associated with positive outcome [12, 40].\nSuch models therefore require a very different approach than that used for medical records; in this case, the\npatient should own their data, but both parties (the patient and therapist) should be able to generate data and\nactively interact with it. Meeting the computational requirements of hosting such services may be challenging\nfor providers in smaller practices, while relying on third-party services raises questions about data ownership.\nAddressing these concrete privacy challenges will open the door for broader exploration. AI systems intended not\njust to produce a single diagnosis but rather to engage users in collaborative investigation over time will require new\nmodes of interaction. This will pose research and design challenges in designing systems for use in mental health\ncare and other contexts.\n5.4 Limitations and Future Work\n5.4.1 Dataset Generalization and Balancing. In our experiments, we utilize a class-balanced dataset confined\nto a subset of mobile and behavioral health data collected from undergraduate students at a single university\nover a three-year period. Future evaluations should seek to include both a broader range of data elements as\nwell as data from more diverse population samples to better understand how well these approaches apply to any\npotential individual seeking mental health treatment.\n5.4.2 Model and Prompting Bias. In addition to issues of population representation in the dataset, a growing\nbody of work suggests that state-of-the-art LLMs are prone to producing output containing both implicit and\nexplicit biases [20, 57]. This is further complicated by the sensitivity of classification performance to the specific\ninformation included in the prompt. For example, providing the DSM-V depression criteria results in GPT-4,\nwhich one may assume may improve depression classification, classifying 98.89% of data excerpts as depressed\ndespite a class-balanced dataset. It is almost certain that prompt and model-specific biases exist in the generated\nreasoning about the provided mobile health data as well, but this is a significantly more complex problem to\ncharacterize. Benchmarking techniques are actively being explored in this area [ 48, 79], and further work to\nunderstand how these biases arise and can be mitigated are essential before deployment of LLMs mental health\ncontexts.\n5.4.3 Numerical Errors. Clinicians envisioned inputting both a wide range of additional features as well as a\nlonger time period of data into LLMs for analysis. However, the maximum context window of these models\nis limited, and prior work has shown that the ability of LLMs to accurately recall detailed information from\nthe entirety of the context window decreases as prompt length increases[37]. Pre-computing or summarizing\nportions of data could help address this technical challenge, but requiring this step could significantly reduce the\nability of a tool to generalize across potential input data sources.\nSome errors observed by clinicians involved GPT-4 generating erroneous responses when asked follow-up\nquestions that could not be answered based on the provided mobile health data. This challenge of grounding,\nor constraining LLM output to refer only to a relevant set of input text or data, is not unique to this specific\napplication. Current approaches to improve performance in these areas include model fine-tuning[4]; augmenting\na base general-purpose LLM with a smaller model optimized for a specific task[6]; and utilizing LLMs capable\nof querying external databases and tools to request data and perform arithmetic operations[21]. We view this\nthird option as especially promising since enabling a model to access data from a user-controlled database may\nsimultaneously help address the privacy challenges identified in Section 5.3.\n5.4.4 Evaluation in Clinical Settings. While we find clinicians were frequently able to identify errors generated\nby GPT-4 when they arose during our interviews, in an actual therapy session the clinician may not be able to\n56:16 • Englhardt and Ma, et al.\nbalance the demands of critically evaluating model output while engaging with the patient. Additionally, our\ncurrent work solely evaluates LLM use in therapy from the perspective of clinicians. Further evaluation in a more\nrealistic therapeutic environment is needed to better understand how use of LLMs as part of a mental health\ntreatment program could impact patients and their outcomes.\n6 CONCLUSION\nThis paper examines LLMs in the context of mental health care, specifically psychotherapy. While we begin our\ninvestigation by developing methods to use LLMs to output binary classifications for conditions like depression,\nwe find instead that their greatest potential value to clinicians lies not in diagnostic classification, but rather in\nrigorous analysis of diverse self-tracking data to generate natural language summaries synthesizing multiple\ndata streams and identifying potential concerns. Clinicians envisioned using those insights in a variety of ways,\nprincipally for fostering collaborative investigation with patients. This collaboration was seen as potentially\nvaluable for strengthening the therapeutic alliance and guiding treatment. We describe a human-AI collaborative\nmodel and its requirements for secure management of personal data. These findings highlight directions for\nimpactful future research on human-AI collaborative tools in mental health care and other contexts.\nACKNOWLEDGMENTS\nThis research was partially supported by an Amazon Research Award, a Google Research Scholar Award, The\nWashington Research Foundation, and the Pastry-Powered T(o)uring Machine Endowed Fellowship.\nREFERENCES\n[1] 2013. Diagnostic and statistical manual of mental disorders : DSM-5 (fifth edition. ed.). American Psychiatric Association, Arlington, VA.\n[2] Mostafa M. Amin, Erik Cambria, and Björn W. Schuller. 2023. Will Affective Computing Emerge from Foundation Models and General\nAI? A First Evaluation on ChatGPT. http://arxiv.org/abs/2303.03186\n[3] Chizobam Ani, Mohsen Bazargan, David Hindman, Douglas Bell, Muhammad A. Farooq, Lutful Akhanjee, Francis Yemofio, Richard\nBaker, and Michael Rodriguez. 2008. Depression symptomatology and diagnosis: discordance between patients and physicians in\nprimary care settings. BMC Family Practice 9, 1 (Jan. 2008), 1. https://doi.org/10.1186/1471-2296-9-1\n[4] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and\nSean Welleck. 2023. Llemma: An Open Language Model For Mathematics. https://doi.org/10.48550/arXiv.2310.10631 arXiv:2310.10631\n[cs].\n[5] Sangwon Bae, Denzil Ferreira, Brian Suffoletto, Juan C. Puyana, Ryan Kurtz, Tammy Chung, and Anind K. Dey. 2017. Detecting drinking\nepisodes in young adults using smartphone-based sensors. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 2, Article 5 (Jun\n2017), 36 pages. https://doi.org/10.1145/3090051\n[6] Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, and\nPartha Talukdar. 2024. LLM Augmented LLMs: Expanding Capabilities through Composition. https://doi.org/10.48550/arXiv.2401.02412\narXiv:2401.02412 [cs].\n[7] Dror Ben-Zeev, Emily A. Scherer, Rui Wang, Haiyi Xie, and Andrew T. Campbell. 2015. Next-generation psychiatric assessment: Using\nsmartphone sensors to monitor behavior and mental health. Psychiatric Rehabilitation Journal 38, 3 (2015), 218.\n[8] Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large language models in machine translation. (2007).\n[9] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5–32.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. In Advances in Neural Information Processing Systems , Vol. 33. Curran Associates, Inc., 1877–1901. https://proceedings.neurips.\ncc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n[11] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li,\nScott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early\nexperiments with GPT-4. http://arxiv.org/abs/2303.12712\nFrom Classification to Clinical Insights • 56:17\n[12] Sarah Kate Cameron, Jacqui Rodgers, and Dave Dagnan. 2018. The relationship between the therapeutic alliance and clinical outcomes\nin cognitive behaviour therapy for adults with depression: A meta-analytic review. Clinical psychology & psychotherapy 25, 3 (2018),\n446–456.\n[13] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn\nSong, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX\nSecurity 21) . 2633–2650.\n[14] Stevie Chancellor and Munmun De Choudhury. 2020. Methods in predictive techniques for mental health status on social media: a\ncritical review. npj Digital Medicine 3, 1 (March 2020), 1–11. https://doi.org/10.1038/s41746-020-0233-7\n[15] Prerna Chikersal, Afsaneh Doryab, Michael Tumminia, Daniella K Villalba, Janine M Dutcher, Xinwen Liu, Sheldon Cohen, Kasey G.\nCreswell, Jennifer Mankoff, J. David Creswell, Mayank Goel, and Anind K. Dey. 2021. Detecting Depression and Predicting its Onset\nUsing Longitudinal Symptoms Captured by Passive Sensing. ACM Transactions on Computer-Human Interaction 28, 1 (Jan. 2021), 1–41.\nhttps://doi.org/10.1145/3422821\n[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311\n(2022).\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. http://arxiv.org/abs/2204.02311 arXiv:2204.02311 [cs].\n[18] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani,\nSiddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang,\nAndrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.\nScaling Instruction-Finetuned Language Models. http://arxiv.org/abs/2210.11416 arXiv:2210.11416 [cs].\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv:1810.04805 [cs] (May 2019). http://arxiv.org/abs/1810.04805\n[20] Xiangjue Dong, Yibo Wang, Philip S. Yu, and James Caverlee. 2023. Probing Explicit and Implicit Gender Bias through LLM Conditional\nText Generation. https://doi.org/10.48550/arXiv.2311.00306 arXiv:2311.00306 [cs].\n[21] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-\naided Language Models. InProceedings of the 40th International Conference on Machine Learning . PMLR, 10764–10799. https://proceedings.\nmlr.press/v202/gao23f.html\n[22] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, and Yoshua Bengio. 2017. On integrating a language model into neural\nmachine translation. Computer Speech & Language 45 (2017), 137–148.\n[23] Aodhán Hickey. 2021. The rise of wearables: From innovation to implementation. In Digital health . Elsevier, 357–365.\n[24] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and\nTomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.\narXiv preprint arXiv:2305.02301 (2023).\n[25] Jeremy F Huckins, Alex W DaSilva, Elin L Hedlund, Eilis I Murphy, Courtney Rogers, Weichen Wang, Mikio Obuchi, Paul E Holtzheimer,\nDylan D Wagner, and Andrew T Campbell. 2020. Causal Factors of Anxiety and Depression in College Students: Longitudinal Ecological\nMomentary Assessment and Causal Analysis Using Peter and Clark Momentary Conditional Independence. JMIR Mental Health 7, 6\n(June 2020), e16684. https://doi.org/10.2196/16684\n[26] Jeremy F Huckins, Alex W daSilva, Weichen Wang, Elin Hedlund, Courtney Rogers, Subigya K Nepal, Jialing Wu, Mikio Obuchi, Eilis I\nMurphy, Meghan L Meyer, Dylan D Wagner, Paul E Holtzheimer, and Andrew T Campbell. 2020. Mental Health and Behavior of College\nStudents During the Early Phases of the COVID-19 Pandemic: Longitudinal Smartphone and Ecological Momentary Assessment Study.\nJournal of Medical Internet Research 22, 6 (June 2020), e20185. https://doi.org/10.2196/20185\n[27] R Indrakumari, T Poongodi, P Suresh, and B Balamurugan. 2020. The growing role of Internet of Things in healthcare wearables. In\nEmergence of Pharmaceutical Industry Growth with Industrial IoT Approach . Elsevier, 163–194.\n[28] Nino Isakadze and Seth S. Martin. 2020. How useful is the smartwatch ECG? Trends in Cardiovascular Medicine 30, 7 (Oct. 2020),\n442–448. https://doi.org/10.1016/j.tcm.2019.10.010\n56:18 • Englhardt and Ma, et al.\n[29] Nicholas C. Jacobson and Yeon Joo Chung. 2020. Passive Sensing of Prediction of Moment-To-Moment Depressed Mood among\nUndergraduates with Clinical Levels of Depression Sample Using Smartphones. Sensors 20, 12 (June 2020), 3572. https://doi.org/10.\n3390/s20123572\n[30] Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony\nRiina, Ilya Laufer, Paawan Punjabi, Madeline Miceli, Nora C. Kim, Cordelia Orillac, Zane Schnurman, Christopher Livia, Hannah Weiss,\nDavid Kurland, Sean Neifert, Yosef Dastagirzada, Douglas Kondziolka, Alexander T. M. Cheung, Grace Yang, Ming Cao, Mona Flores,\nAnthony B. Costa, Yindalon Aphinyanaphongs, Kyunghyun Cho, and Eric Karl Oermann. 2023. Health system-scale language models\nare all-purpose prediction engines. Nature (June 2023). https://doi.org/10.1038/s41586-023-06160-y\n[31] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. Health-LLM: Large Language Models for Health\nPrediction via Wearable Sensor Data. https://arxiv.org/abs/2401.06866v1\n[32] Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza,\nArkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.\n[33] K. Kroenke, R. L. Spitzer, J. B.W. Williams, and B. Lowe. 2009. An Ultra-Brief Screening Scale for Anxiety and Depression: The PHQ-4.\nPsychosomatics 50, 6 (Nov. 2009), 613–621. https://doi.org/10.1176/appi.psy.50.6.613\n[34] Bishal Lamichhane. 2023. Evaluation of ChatGPT for NLP-based Mental Health Applications. http://arxiv.org/abs/2303.15727\n[35] Heon-Jeong Lee, Chul-Hyun Cho, Taek Lee, Jaegwon Jeong, Ji Won Yeom, Sojeong Kim, Sehyun Jeon, Ju Yeon Seo, Eunsoo Moon,\nJi Hyun Baek, Dong Yeon Park, Se Joo Kim, Tae Hyon Ha, Boseok Cha, Hee-Ju Kang, Yong-Min Ahn, Yujin Lee, Jung-Been Lee,\nand Leen Kim. 2023. Prediction of impending mood episode recurrence using real-time digital phenotypes in major depression\nand bipolar disorders in South Korea: a prospective nationwide cohort study. Psychological Medicine 53, 12 (Sept. 2023), 5636–5644.\nhttps://doi.org/10.1017/S0033291722002847\n[36] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023. ChatDoctor: A Medical Chat Model Fine-Tuned on a\nLarge Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. http://arxiv.org/abs/2303.14070 arXiv:2303.14070 [cs].\n[37] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle:\nHow Language Models Use Long Contexts. https://doi.org/10.48550/arXiv.2307.03172 arXiv:2307.03172 [cs].\n[38] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille,\nand Shwetak Patel. 2023. Large Language Models are Few-Shot Health Learners. In arXiv.\n[39] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille,\nand Shwetak Patel. 2023. Large Language Models are Few-Shot Health Learners. arXiv preprint arXiv:2305.15525 (2023).\n[40] Daniel J Martin, John P Garske, and M Katherine Davis. 2000. Relation of the therapeutic alliance with outcome and other variables: a\nmeta-analytic review. Journal of consulting and clinical psychology 68, 3 (2000), 438.\n[41] Stephen M Mattingly, Julie M Gregg, Pino Audia, Ayse Elvan Bayraktaroglu, Andrew T Campbell, Nitesh V Chawla, Vedant Das Swain,\nMunmun De Choudhury, Sidney K D’Mello, Anind K Dey, et al. 2019. The Tesserae project: Large-scale, longitudinal, in situ, multimodal\nsensing of information workers. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems . 1–8.\n[42] Jun-Ki Min, Afsaneh Doryab, Jason Wiese, Shahriyar Amini, John Zimmerman, and Jason I. Hong. 2014. Toss “n” turn: Smartphone as\nsleep and sleep quality detector. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario,\nCanada) (CHI ’14) . Association for Computing Machinery, New York, NY, USA, 477–486. https://doi.org/10.1145/2556288.2557220\n[43] Shayan Mirjafari, Kizito Masaba, Ted Grover, Weichen Wang, Pino G. Audia, Andrew T. Campbell, Nitesh V. Chawla, Vedant Das Swain,\nMunmun De Choudhury, Anind K. Dey, et al. 2019. Differentiating higher and lower job performers in the workplace using mobile\nsensing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 2 (2019), 37:1–37:24. https://doi.org/10.1145/3328908\n[44] Stefanie Nickels, Matthew D Edwards, Sarah F Poole, Dale Winter, Jessica Gronsbell, Bella Rozenkrants, David P Miller, Mathias Fleck,\nAlan McLean, Bret Peterson, et al. 2021. Toward a mobile platform for real-world digital measurement of depression: User-centered\ndesign, data quality, and behavioral and clinical modeling. JMIR mental health 8, 8 (2021), e27589.\n[45] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of GPT-4 on Medical Challenge\nProblems. http://arxiv.org/abs/2303.13375 arXiv:2303.13375 [cs].\n[46] Reham Omar, Omij Mangukiya, Panos Kalnis, and Essam Mansour. 2023. Chatgpt versus traditional question answering for knowledge\ngraphs: Current status and future directions towards knowledge graph chatbots. arXiv preprint arXiv:2302.06466 (2023).\n[47] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[48] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R.\nBowman. 2022. BBQ: A Hand-Built Bias Benchmark for Question Answering. https://doi.org/10.48550/arXiv.2110.08193 arXiv:2110.08193\n[cs].\n[49] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is ChatGPT a general-purpose\nnatural language processing task solver? arXiv preprint arXiv:2302.06476 (2023).\n[50] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative\nPre-Training.\nFrom Classification to Clinical Insights • 56:19\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (2020).\n[52] Joshua Robinson and David Wingate. 2023. Leveraging Large Language Models for Multiple Choice Question Answering. In The\nEleventh International Conference on Learning Representations . https://openreview.net/forum?id=yKbprarjc5B\n[53] Darius A. Rohani, Maria Faurholt-Jepsen, Lars Vedel Kessing, and Jakob E. Bardram. 2018. Correlations Between Objective Behavioral\nFeatures Collected From Mobile and Wearable Devices and Depressive Mood Symptoms in Patients With Affective Disorders: Systematic\nReview. JMIR mHealth and uHealth 6, 8 (Aug. 2018), e9691. https://doi.org/10.2196/mhealth.9691\n[54] Sohrab Saeb, Mi Zhang, Christopher J. Karr, Stephen M. Schueller, Marya E. Corden, Konrad P. Kording, and David C. Mohr. 2015.\nMobile phone sensor correlates of depressive symptom severity in daily-life behavior: An exploratory study. Journal of Medical Internet\nResearch 17, 7 (2015), 1–11. https://doi.org/10.2196/jmir.4273\n[55] Asif Salekin, Jeremy W Eberle, Jeffrey J Glenn, Bethany A Teachman, and John A Stankovic. 2018. A Weakly Supervised Learning\nFramework For Detecting Social Anxiety And Depression. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies 2, 2 (2018), 26.\n[56] Yasaman S. Sefidgar, Woosuk Seo, Kevin S. Kuehn, Tim Althoff, Anne Browning, Eve Riskin, Paula S. Nurius, Anind K. Dey, and Jennifer\nMankoff. 2019. Passively-sensed behavioral correlates of discrimination events in college students. Proc. ACM Hum.-Comput. Interact. 3,\nCSCW, Article 114 (Nov 2019), 29 pages. https://doi.org/10.1145/3359216\n[57] Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. 2023. On Second Thought, Let’s Not Think Step by\nStep! Bias and Toxicity in Zero-Shot Reasoning. https://doi.org/10.48550/arXiv.2212.08061 arXiv:2212.08061 [cs].\n[58] Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities into smaller language models. In\nFindings of the Association for Computational Linguistics: ACL 2023 . 7059–7073.\n[59] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene\nNeal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa\nDominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale\nWebster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level\nMedical Question Answering with Large Language Models. http://arxiv.org/abs/2305.09617 arXiv:2305.09617 [cs].\n[60] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.\n2023. Stanford alpaca: An instruction-following llama model.\n[61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and\nEfficient Foundation Language Models. http://arxiv.org/abs/2302.13971 arXiv:2302.13971 [cs].\n[62] Andreas Triantafyllidis, Haridimos Kondylakis, Konstantinos Votis, Dimitrios Tzovaras, Nicos Maglaveras, and Kazem Rahimi. 2019.\nFeatures, outcomes, and challenges in mobile health interventions for patients living with chronic diseases: A review of systematic\nreviews. International Journal of Medical Informatics 132 (Dec. 2019), 103984. https://doi.org/10.1016/j.ijmedinf.2019.103984\n[63] Fabian Wahle, Tobias Kowatsch, Elgar Fleisch, Michael Rufer, and Steffi Weidt. 2016. Mobile Sensing and Support for People With\nDepression: A Pilot Trial in the Wild. JMIR mHealth and uHealth 4, 3 (2016), e111. https://doi.org/10.2196/mhealth.5960\n[64] Rui Wang, Min S. H. Aung, Saeed Abdullah, Rachel Brian, Andrew T. Campbell, Tanzeem Choudhury, Marta Hauser, John Kane,\nMichael Merrill, Emily A. Scherer, Vincent W. S. Tseng, and Dror Ben-Zeev. 2016. CrossCheck: Toward passive sensing and detection of\nmental health changes in people with schizophrenia. Proceedings of the ACM International Joint Conference on Pervasive and Ubiquitous\nComputing (2016), 886–897. https://doi.org/10.1145/2971648.2971740\n[65] Rui Wang, Fanglin Chen, Zhenyu Chen, Tianxing Li, Gabriella Harari, Stefanie Tignor, Xia Zhou, Dror Ben-Zeev, and Andrew T. Campbell.\n2014. StudentLife: Assessing mental health, academic performance and behavioral trends of college students using smartphones. In\nProceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing . ACM, 3–14.\n[66] Rui Wang, Gabriella Harari, Peilin Hao, Xia Zhou, and Andrew T Campbell. 2015. SmartGPA: how smartphones can assess and predict\nacademic performance of college students. In Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous\ncomputing. 295–306.\n[67] Rui Wang, Weichen Wang, Alex daSilva, Jeremy F. Huckins, William M. Kelley, Todd F. Heatherton, and Andrew T. Campbell. 2018.\nTracking Depression Dynamics in College Students Using Mobile Phone and Wearable Sensing. Proceedings of the ACM on Interactive,\nMobile, Wearable and Ubiquitous Technologies 2, 1 (2018), 1–26. https://doi.org/10.1145/3191775\n[68] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).\n[69] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022.\nFinetuned Language Models Are Zero-Shot Learners. http://arxiv.org/abs/2109.01652 arXiv:2109.01652 [cs].\n[70] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-\nThought Prompting Elicits Reasoning in Large Language Models. http://arxiv.org/abs/2201.11903 arXiv:2201.11903 [cs].\n56:20 • Englhardt and Ma, et al.\n[71] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. PMC-LLaMA: Further Finetuning LLaMA on Medical\nPapers. http://arxiv.org/abs/2304.14454 arXiv:2304.14454 [cs].\n[72] Xuhai Xu, Prerna Chikersal, Janine M. Dutcher, Yasaman S. Sefidgar, Woosuk Seo, Michael J. Tumminia, Daniella K. Villalba, Sheldon\nCohen, Kasey G. Creswell, J. David Creswell, Afsaneh Doryab, Paula S. Nurius, Eve Riskin, Anind K. Dey, and Jennifer Mankoff.\n2021. Leveraging Collaborative-Filtering for Personalized Behavior Modeling: A Case Study of Depression Detection among College\nStudents. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 1 (March 2021), 1–27. https:\n//doi.org/10.1145/3448107\n[73] Xuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subigya Nepal, Yasaman Sefidgar, Woosuk Seo, Kevin S Kuehn, Jeremy F Huckins,\nMargaret E Morris, et al. 2023. GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling. Proceedings of the\nACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 6, 4 (2023), 1–34.\n[74] Xuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subigya Nepal, Yasaman Sefidgar, Woosuk Seo, Kevin S. Kuehn, Jeremy F. Huckins,\nMargaret E. Morris, Paula S. Nurius, Eve A. Riskin, Shwetak Patel, Tim Althoff, Andrew Campbell, Anind K. Dey, and Jennifer Mankoff.\n2023. GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling. Proceedings of the ACM on Interactive, Mobile,\nWearable and Ubiquitous Technologies 6, 4 (2023), 1–34. https://doi.org/10.1145/3569485\n[75] Xuhai Xu, Ebrahim Nemati, Korosh Vatanparvar, Viswam Nathan, Tousif Ahmed, Md Mahbubur Rahman, Daniel McCaffrey, Jilong\nKuang, and Jun Alex Gao. 2021. Listen2Cough: Leveraging End-to-End Deep Learning Cough Detection Model to Enhance Lung Health\nAssessment Using Passively Sensed Audio. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 1\n(March 2021), 1–22. https://doi.org/10.1145/3448124\n[76] Xuhai Xu, Han Zhang, Yasaman Sefidgar, Yiyi Ren, Xin Liu, Woosuk Seo, Jennifer Brown, Kevin Kuehn, Mike Merrill, Paula Nurius,\nShwetak Patel, Tim Althoff, Margaret E Morris, Eve Riskin, Jennifer Mankoff, and Anind K Dey. 2022. GLOBEM Dataset: Multi-Year\nDatasets for Longitudinal Human Behavior Modeling Generalization. InThirty-sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track . 18.\n[77] Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, and Chenghua Lin. 2023. Effective distillation of table-based reasoning ability from\nllms. arXiv preprint arXiv:2309.13182 (2023).\n[78] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, and Sophia Ananiadou. 2023. On the Evaluations of ChatGPT and Emotion-\nenhanced Prompting for Mental Health Analysis. http://arxiv.org/abs/2304.03347\n[79] Kai-Ching Yeh, Jou-An Chi, Da-Chen Lian, and Shu-Kai Hsieh. 2023. Evaluating Interfaced LLM Bias. InProceedings of the 35th Conference\non Computational Linguistics and Speech Processing (ROCLING 2023) , Jheng-Long Wu and Ming-Hsiang Su (Eds.). The Association for\nComputational Linguistics and Chinese Language Processing (ACLCLP), Taipei City, Taiwan, 292–299. https://aclanthology.org/2023.\nrocling-1.37\n[80] Han Zhang, Margaret E. Morris, Paula S. Nurius, Kelly Mack, Jennifer Brown, Kevin S. Kuehn, Yasaman S. Sefidgar, Xuhai Xu, Eve A. Riskin,\nAnind K. Dey, and Jennifer Mankoff. 2022. Impact of Online Learning in the Context of COVID-19 on Undergraduates with Disabilities\nand Mental Health Concerns. ACM Transactions on Accessible Computing (July 2022), 3538514. https://doi.org/10.1145/3538514\n[81] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt\nand fine-tuned bert. arXiv preprint arXiv:2302.10198 (2023).\n[82] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc\nLe, and Ed Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. http://arxiv.org/abs/2205.10625\narXiv:2205.10625 [cs].\nFrom Classification to Clinical Insights • 56:21\nA DATA ELEMENTS\nGLOBEM Data Feature Description\ndate date\nf_loc:phone_locations_doryab_totaldistance:allday total distance traveled (meters)\nf_loc:phone_locations_doryab_timeathome:allday time spent at home (minutes)\nf_loc:phone_locations_doryab_locationentropy:allday location entropy\nf_screen:phone_screen_rapids_sumdurationunlock:allday phone screen time (minutes)\nf_screen:phone_screen_rapids_avgdurationunlock:allday average phone unlock duration (minutes)\nf_call:phone_calls_rapids_incoming_sumduration:allday phone call incoming duration (minutes)\nf_call:phone_calls_rapids_outgoing_sumduration:allday phone call outgoing duration (minutes)\nf_blue:phone_bluetooth_doryab_uniquedevicesothers:allday unique Bluetooth devices discovered nearby\nf_steps:fitbit_steps_intraday_rapids_sumsteps:allday step count\nf_steps:fitbit_steps_intraday_rapids_countepisodesedentarybout:allday number of sedentary episodes\nf_steps:fitbit_steps_intraday_rapids_sumdurationsedentarybout:allday total time spent sedentary (minutes)\nf_steps:fitbit_steps_intraday_rapids_countepisodeactivebout:allday number of activity episodes\nf_steps:fitbit_steps_intraday_rapids_sumdurationactivebout:allday total time spent active (minutes)\nf_slp:fitbit_sleep_intraday_rapids_sumdurationasleepunifiedmain:allday total time asleep (minutes)\nf_slp:fitbit_sleep_intraday_rapids_sumdurationawakeunifiedmain:allday total time spent awake while in bed (minutes)\nTable 1. Data Fields and Descriptions\nB PROMPT STRUCTURE\nEach following subsection outlines the specific text used for each component of the prompt. To re-create our\nprompts for a given experiment, concatenate the relevant sections as shown in Fig. 1.\nB.1 Role\nRole:\nYou are a data analyst helping a psychiatrist understand human activity data.\nB.2 Task Description\nThe numerically listed items in this block should be added or removed according to the blocks included in the\nrest of the prompt.\nTask:\nYou will be shown data gathered from a smartphone and smart watch worn by an individual. Your\ngoal is to analyze this data.\nYou are presented with the following:\n1. A table consisting of twenty-eight days of collected activity tracking data [Collected Data]\n2. Explanation of the different types of data. [Description of Variables]\n3. DSM-5 Criteria for depression. [Depression Criteria]\n4. Instructions on how to analyze the data [Instructions]\nB.3 Formatted Data Table\nBegin with a header Collected Data: followed by the data. Each of the data formats used in this paper can be\nfound in Appendix D.\n56:22 • Englhardt and Ma, et al.\nB.4 Describe Variables\nThese descriptions may vary based on the specific dataset used.\nDescription of variables\nFor the duration of the data collection period, data was logged from the individual’s\nsmartphone and a fitness tracking smartwatch. An explanation of the different\ntypes of data logged is included below:\nDate: the data in this row was recorded on that date.\ntotal_Distance_traveled(meters): This represents the total distance traveled as measured\nby the GPS in the participant’s smartphone. This includes walking, driving, and any\nother modes of transportation.\ntime_at_home(minutes): This is the total time that the participant was at home as\ndetermined by their smartphone GPS.\n. . .\nB.5 DSM-V Definition\nDepression Criteria\nHere is the DSM-5 Criteria for depression. The DSM-5 outlines the following criterion to\nmake a diagnosis of depression. The individual must be experiencing five or more symptoms\nduring the same 2-week period and at least one of the symptoms should be either\n(1) depressed mood or (2) loss of interest or pleasure. . .\nB.6 Ask for Reasoning and/or Classification\nSpecify the type(s) of analysis required. The example below includes both reasoning and classification instructions.\nInstructions\n1. Based on the collected data, hypothesize about the overall mental health of the individual\nwith detailed explanations.\n2. Make your best guess if this person is currently experiencing depression. Please answer\nonly \"Yes\" or \"No\" to this question.\nC EXAMPLE GPT-4 RESPONSE\nBelow is a response generated by GPT-4 from a prompt consisting of the Chain of Thought, Variable Description,\nand DSM-V Definition elements:\nTo assess the mental health of the individual, we’ll analyze the provided data and relate it to the DSM-5 criteria\nfor depression. Hypothesis About Overall Mental Health:\n•Physical Activity: The individual’s step count fluctuates significantly, with days of very high physical\nactivity (e.g., 55,755 steps on 2019-05-11) and others with much lower activity (e.g., 2,050 steps on 2019-\n05-20). The inconsistency in physical activity could indicate variability in energy levels or mood.\n•Time at Home : There are significant variances in the amount of time spent at home. Some days show\nextended periods at home (e.g., 1,240 minutes on 2019-05-12), while others show much less (e.g., 5 minutes\non 2019-05-19). Spending extended periods at home might suggest a lack of engagement in external\nactivities, possibly indicating social withdrawal or lack of interest.\nFrom Classification to Clinical Insights • 56:23\n•Phone and Screen Usage : The phone screen time and average phone use unlock duration fluctuate,\nwith some days showing high usage (e.g., 819 minutes on 2019-05-21) and others considerably lower (e.g.,\n138 minutes on 2019-05-19). This could indicate variations in the need for distraction or communication.\n•Sedentary and Active Episodes : There’s inconsistency in the number of sedentary and active episodes.\nSome days have high sedentary time and low activity (e.g., 1,402 sedentary minutes vs. 38 active minutes\non 2019-05-20), which might suggest fatigue or lack of energy.\n•Sleep Patterns: The total time asleep and awake while in bed varies. There are nights with less sleep\n(e.g., 249 minutes on 2019-05-10) and others with more (e.g., 679 minutes on 2019-05-12). Irregular sleep\npatterns could be indicative of disturbed sleep, which is a symptom of depression.\n•Location Entropy: On most days, location entropy is 0, suggesting a lack of diversity in location visits.\nThis could imply a routine life with limited exploration or lack of interest in diverse activities.\n•Phone Call Duration : There is limited data on phone call durations, but on days with data, durations\nvary. This inconsistency might suggest fluctuating social interactions.\nBest Guess if Experiencing Depression: Yes.\nThe data suggests variability in physical activity, time spent at home, phone usage, and sleep patterns. This\ninconsistency could reflect symptoms such as fatigue, loss of interest, and disturbed sleep, aligning with some\ndepression criteria. However, it’s important to note that this analysis is limited and cannot definitively diagnose\ndepression. A comprehensive assessment by a mental health professional is necessary for an accurate diagnosis.\nD DATA INPUT FORMAT\nThe following subsections include snippets detailing the input formats described in Section 3.2.\nD.1 CSV\ndate,total_distance_traveled(meters),time_at_home(minutes),location_entropy,...\n2019-04-29,49037.0,666.0,0.85,298.0,3.0,,,29.0,11430.0,40.0,1290.0,39.0,150.0,306.0,11.0\n2019-04-30,69171.0,555.0,0.87,274.0,4.0,16.0,,13.0,11427.0,49.0,1264.0,48.0,176.0,,\n. . .\nD.2 Markdown\ndate|total_distance_traveled(meters)|time_at_home(minutes)|location_entropy|...\n2019-04-29|49037|666|0.85|298|3|nan|nan|29|11430|40|1290|39|150|306|11|\n2019-04-30|69171|555|0.87|274|4|16|nan|13|11427|49|1264|48|176|nan|nan|\n. . .\nD.3 Tabular\ndate total_distance_traveled(meters) time_at_home(minutes) location_entropy . . .\n2019-04-29 49037.0 666.0 0.85 298.0 3.0 29.0 11430.0 . . .\n2019-04-30 69171.0 555.0 0.87 274.0 4.0 16.0 13.0 11427.0 . . .\n. . .\nD.4 LaTeX\n\\begin{tabular}{lrrrrrrrrrrrrrrr}\n\\toprule\ndate & total_distance_traveled(meters) & time_at_home(minutes) & location_entropy & . . . \\\\\n56:24 • Englhardt and Ma, et al.\n\\midrule\n2019-04-29 & 49037.0 & 666.0 & 0.850 & 298.0 & 3.0 & NaN & . . . \\\\\n2019-04-30 & 69171.0 & 555.0 & 0.870 & 274.0 & 4.0 & 16.0 & . . . \\\\\n\\bottomrule\n\\end{tabular}\nE PRODUCING REASONING PROMPT\nBelow is the prompt used to generate the reasoning samples used in Section 3.3.1:\nRole:\nYou are a data analyst helping a psychiatrist understand human activity data.\nTask:\nYou will be shown data gathered from a smartphone and smart watch worn by an individual. Your\ngoal is to analyze this data.\nYou are presented with the following:\n1. A table consisting of twenty-eight days of collected activity tracking data [Collected Data]\n2. Instructions on how to analyze the data [Instructions]\nCollected Data\ndate|total_distance_traveled(meters)|time_at_home(minutes)|...\n|2019-05-06|11996|1012|...\n. . .\nInstructions\nAlthough the data does not contain explicit information on mood, trends in physiological\nsignals have been shown to correlate with mental health symptoms. Examine this data\nand point out any specific trends or data points that could spark fruitful\nconversation with a mental health professional.\nF REASONING GRADER INSTRUCTIONS\nThank you for taking the time to contribute to this study.\nTo start, please open this document that contains a table of data as well as plots of the data.\nLink to document: [LINK HERE]\nYou will now be asked to grade a series of 32 different statements analyzing this data. Your goal is to check the\naccuracy of these statements to ensure that references to the data are correct\nHere an explanation of the grading rubric. Please read this rubric carefully:\n1. Does this response include numbers? (yes/no)\nYes – at least some part of the response lists or quotes specific numerical data or dates, regardless of correctness\nNo – the response does not include any specific numbers\nNote – numbered lists don’t count as numbers\n2. Are these numbers consistent with the provided data? (yes/no)\nYes – all of the mentioned numbers or dates are included in the provided data\nNo – some or all of the numbers or dates are not consistent with the provided data, or there are no numbers (1\nwas answered “No”)\nFor example:\nFrom Classification to Clinical Insights • 56:25\n• The text statement says the highest sleep time occurred on May 9, but based on the graph you\ncan see it is actually on June 2\n• The text statement lists the lowest distance travelled as 127 meters, but the lowest distance\ntraveled listed in the table is 1270 meters\n3. Does this response identify specific trends? (yes/no)\nYes – the response makes statements relating to concepts like minimum, maximum, averages, variability,\nupward or downward trends, etc. as they pertain to the data\nNo – There is no statement of specific trends that relate to the included data.\nFor example:\n• “An increase in sleep might indicate a disturbance” or \"the individual makes phone calls\" would\nnot be a specific trend relating to the provided data\n• “The time spent asleep increased in the second half of the month” would be a specific trend\nrelating to the data\n4. Are these trends consistent with the provided data? (yes/no)\nYes – the listed trends are plausibly consistent with the provided data table and/or plots\nNo – some or all of the listed trends are contradicted by the provided data and/or plots or there are no specific\ntrends (3 was answered “No”)\nIt is important to note that you should not evaluate further trends or reasoning as they may relate to, for\nexample, mental health. For the purposes of grading these responses, it is only necessary to confirm if the response\ndoes or does not accurately describe the provided data.\nWe anticipate it will take 1.5-2 minutes to grade each statement."
}