{
  "title": "Multi-Task Learning with Language Modeling for Question Generation",
  "url": "https://openalex.org/W2970566942",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101918392",
      "name": "Wenjie Zhou",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100737343",
      "name": "Minghua Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5027803148",
      "name": "Yunfang Wu",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W99485931",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2606333299",
    "https://openalex.org/W2556468274",
    "https://openalex.org/W2889874042",
    "https://openalex.org/W2912913215",
    "https://openalex.org/W2757978590",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W2963275829",
    "https://openalex.org/W2624022918",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2964285114",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2889670144",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2962970841",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2890114324",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2804292122",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2964309167",
    "https://openalex.org/W2757715585"
  ],
  "abstract": "This paper explores the task of answer-aware questions generation. Based on the attention-based pointer generator model, we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi-task learning structure. Our joint-learning model enables the encoder to learn a better representation of the input sequence, which will guide the decoder to generate more coherent and fluent questions. On both SQuAD and MARCO datasets, our multi-task learning model boosts the performance, achieving state-of-the-art results. Moreover, human evaluation further proves the high quality of our generated questions.",
  "full_text": "Multi-Task Learning with Language Modeling for Question Generation\nWenjie Zhou, Minghua Zhang, Yunfang Wu∗\nKey Laboratory of Computational Linguistics, Ministry of Education\nSchool of Electronics Engineering and Computer Science, Peking University, Beijing, China\n{wjzhou013,zhangmh,wuyf}@pku.edu.cn\nAbstract\nThis paper explores the task of answer-aware\nquestions generation. Based on the attention-\nbased pointer generator model, we propose\nto incorporate an auxiliary task of language\nmodeling to help question generation in a hi-\nerarchical multi-task learning structure. Our\njoint-learning model enables the encoder to\nlearn a better representation of the input se-\nquence, which will guide the decoder to gen-\nerate more coherent and ﬂuent questions. On\nboth SQuAD and MARCO datasets, our multi-\ntask learning model boosts the performance,\nachieving state-of-the-art results. Moreover,\nhuman evaluation further proves the high qual-\nity of our generated questions.\n1 Introduction\nQuestion generation (QG) receives increasing in-\nterests in recent years due to its beneﬁts to several\nreal applications: (1) QG can aid in the develop-\nment of annotated questions to boost the question\nanswering systems (Duan et al., 2017; Tang et al.,\n2017); (2) QG enables the dialogue systems to ask\nquestions which make it more proactive (Shum\net al., 2018; Colby, 1975); (3) QG can help to gen-\nerate questions for reading comprehension texts in\nthe education ﬁeld. In this paper, we focus on\nanswer-aware QG. Giving a sentence and an an-\nswer span as input, we want to generate a question\nwhose response is the answer.\nPrevious work on QG was mainly tackled by\ntwo approaches: the rule-based approach and\nneural-based approach. The neural-based ap-\nproach receives a booming development due to\nthe release of large-scale reading comprehension\ndatasets like SQuAD (Rajpurkar et al., 2016) and\nMARCO (Nguyen et al., 2016). Most of the neu-\nral approaches on QG employ the encoder-decoder\n∗Corresponding author.\nframework, which incorporate attention mecha-\nnism to pay more attention to the informative part\nand copy mode to copy some tokens from the input\ntext (Du et al., 2017; Zhou et al., 2017; Song et al.,\n2018; Subramanian et al., 2018; Zhao et al., 2018;\nSun et al., 2018). To make better use of answer\ninformation, Song et al. (2018) leverage multi-\nperspective matching, and Sun et al. (2018) pro-\npose a position-aware model that aims at putting\nmore emphasis on the answer-surrounded context\nwords. Zhao et al. (2018) aggregate paragraph-\nlevel information to help QG. Another line of work\nis to deal with question answering and question\ngeneration as dual tasks (Tang et al., 2017; Duan\net al., 2017). Some other works try to generate\nquestions from a text without answers as input\n(Subramanian et al., 2018; Du and Cardie, 2017).\nAlthough some progress has been made, there is\nstill much room for improvement for QG.\nMulti-task learning is an effective way to im-\nprove model expressiveness via related tasks by\nintroducing more data and fruitful semantic in-\nformation to the model (Caruana, 1998). Many\nworks in NLP have adopted multi-task learning\nand prove its effectiveness on textual entailment\n(Hashimoto et al., 2017), keyphrase generation\n(Ye and Wang, 2018) and document summariza-\ntion (Guo et al., 2018). To the best of our\nknowledge, no work attempts to employ multi-\ntask learning for question generation. Although\nlanguage modeling has been applied to multi-task\nlearning for classiﬁcation tasks, they are different\nfrom our generation task.\nIn this work, we propose to incorporate lan-\nguage modeling as an auxiliary task to help QG\nvia multi-task learning. We adopt the pointer-\ngenerator (See et al., 2017) reinforced with fea-\ntures as the baseline model, which yields state-\nof-the-art result (Sun et al., 2018). The language\nmodeling task is to predict the next word and the\narXiv:1908.11813v1  [cs.CL]  30 Aug 2019\nPeople follow courage\nhlm\n1 hlm\n2 hlm\n3\nhlm\n1 hlm\n2 hlm\n3\nw1hlm\n1w1\nfollow\nLM hidden states\n<s>\nw2hlm\n2w2\ncourage\nw3hlm\n3w3\n</s>follow\nLanguage  \nModeling\nh1\n1 h1\n2 h1\n3\nh2\n1 h2\n2 h2\n3\nAttention\ny1\ny2\ny3\nWhat\ndo\npeople\ny4 follow\nPeople\nAnswer Position\nWord Embedding\nLexical Features\nPcopy\nPvocab\npg\nAnswer Position\nWord Embedding\nLexical Features\ny4 ?\nFigure 1: Overall structure of our joint-learning model.\nprevious word whose input is a plain text without\nrelying on any annotation. The two tasks are then\ncombined with a hierarchical structure, where the\nlow-level language modeling encourages our rep-\nresentation to learn richer language features that\nwill help the high-level network to generate better\nexpressive questions.\nWe conduct extensive experiments on two\nreading comprehension datasets: SQuAD and\nMARCO. We experiment with different settings\nto prove the efﬁcacy of our multi-task learn-\ning model: with/without language modeling and\nwith/without features. Experimental results show\nthat the language modeling consistently yields ob-\nvious performance gain over baselines, for all\nevaluation metrics, including BLEU, perplexity\nand distinct. Our full model outperforms the\nexisting state-of-the-art results on both datasets,\nachieving a high BLEU-4 score of 16.23 on\nSQuAD and 20.88 on MARCO, respectively. We\nalso conduct human evaluation, and our generated\nquestions get higher scores on all three metrics,\nincluding matching, fluency and relevance.\n2 Model Description\nThe baseline model is an attention-based seq2seq\npointer-generator reinforced by lexical features,\nlike the work of Sun et al. (2018). In our proposed\nmodel, we employ multi-task learning with lan-\nguage modeling as an auxiliary task for QG. The\nwhole structure of our model is shown in Figure 1.\n2.1 Feature-enriched Pointer Generator\nThe feature-rich encoder is a bidirectional LSTM\nused to produce a sequence of hidden states hL\nt .\nThe encoder takes a sequence of word-and-feature\nvectors as input ((x1,...,x T )), which concatenates\nthe word embedding et, answer position embed-\nding at and lexical feature embedding lt (xt =\n[et; at; lt]).The lexical feature is composed of POS\ntags, NER tags, and word case.\nThe attention-based decoder is another unidi-\nrectional LSTM, which is conditioned on the pre-\nvious decoder state si−1, decoded word wi−1, and\ncontext vector ci−1 which is generated via atten-\ntion mechanism (Bahdanau et al., 2014):\nsi = LSTM([wi−1; ci−1],si−1) (1)\nFurther, a two-layer feed-forward network is used\nto produce the vocabulary distribution Pvocab.\nThe pointer generator (See et al., 2017) incorpo-\nrates a copy mode Pcopy(w), which allows copy-\ning words from the source text via pointing. The\nﬁnal probability distribution is to combine both\nmodes with a generation probability pg∈[0,1]:\nP(w) =pgPvocab(w) + (1−pg)Pcopy(w) (2)\nThe model is trained to minimize the negative\nlog-likelihood of the target sequence. We denote\nthis loss as E.\n2.2 Language Modeling\nThe language model is to predict the next word\nand previous word in the sequence with a forward\nLSTM and a backward LSTM, respectively. First,\nwe feed the input sequence into a bidirectional\nLSTM to get the hidden representations hlm\nt .\nThen, these states are fed into a softmax layer\nto predict the next and the previous word:\nPlm(wt+1|w<t+1) =softmax(Wf\n−→\nhlm\nt ) (3)\nPlm(wt−1|w>t−1) =softmax(Wb\n←−\nhlm\nt ) (4)\nThe training objective is to minimize the loss\nfunction which is deﬁned as the average of the\nnegative log-likelihood of the next word and the\nprevious word in the sequence:\nElm = − 1\nT −1\nT−1∑\nt=1\nlog(Plm(wt+1|w<t+1))\n− 1\nT −1\nT∑\nt=2\nlog(Plm(wt−1|w>t−1)) (5)\nDataset SQuAD MARCO\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-1 BLEU-2 BLEU-3 BLEU-4\nNQG++ (Zhou et al., 2017) - - - 13.29 - - - -\nmatching strategy (Song et al., 2018) - - - 13.91 - - - -\nMaxout Pointer (sentence) (Zhao et al., 2018)44.51 29.07 21.06 15.82 - - - 16.02\nanswer-focused model (Sun et al., 2018)42.10 27.52 20.14 15.36 46.59 33.46 24.57 18.73\nposition-aware model (Sun et al., 2018)42.16 27.37 20.00 15.23 47.16 34.20 24.40 18.19\nhybrid model (Sun et al., 2018) 43.02 28.14 20.51 15.64 48.24 35.95 25.79 19.45\nOur Model\npointer generator with features (baseline)41.25 26.76 19.53 14.89 54.04 36.68 26.62 20.15\nw/ features + language modeling 42.80 28.43 21.08 16.23 54.47 37.30 27.31 20.88\nw/o features + language modeling 42.72 27.73 20.26 15.43 54.62 37.37 27.18 20.71\nw/ features + 1-layer encoder 42.12 27.48 20.12 15.33 53.51 36.42 26.49 20.11\nTable 1: Experimental results of our model in different settings comparing with previous methods on two datasets.\n2.3 Multi-task Learning\nInstead of sharing representations between two\ntasks (Rei, 2017) or encoding two tasks at the same\nlevel (Liu et al., 2018; Chen et al., 2018; Kendall\net al., 2018), we adopt a hierarchical structure to\ncombine the two tasks, by treating language mod-\neling as a low-level task and pointer generator net-\nwork as high-level, because language modeling\nis fundamental and its semantic information will\nbeneﬁt question generation. In details, we ﬁrst\nfeed the input sequence into the language model-\ning layer to get a sequence of hidden states. Then\nwe concatenate them with the input sequence to\nobtain the input of the feature-rich encoder.\nFinally, the loss of LM is added to the main loss\nto form a combined training objective:\nEtotal = E+ βElm (6)\nwhere β is a hyper-parameter, which is used to\ncontrol the relative importance of two tasks.\n3 Experiments\n3.1 Dataset\nWe conduct experiments on two reading compre-\nhension datasets: SQuAD and MARCO, using\nthe data shared by Zhou et al. (2017) and Sun\net al. (2018), where the lexical features are ex-\ntracted with Stanford CoreNLP. In details, there\nare 86,635, 8,965 and 8,964 sentence-answer-\nquestion triples in the training, development and\ntest set for SQuAD, and 74,097, 4,539 and 4,539\nsentence-answer-question triples in the training,\ndevelopment and test set for MARCO.\n3.2 Experiment Settings\nOur vocabulary contains the most frequent 20,000\nwords in each training set. Word embeddings are\ninitialized with the pre-trained 300-dimensional\nGlove vectors, and are allowed to be ﬁne-tuned\nduring training. The representations of answer po-\nsition, POS tags, NER tags and word cases are ran-\ndomly initialized as 32-dimensional vectors, re-\nspectively. The encoder of our baseline model\nconsists of 2 BiLSTM layers, and the hidden size\nof both the encoder and decoder is set to 512.\nIn our joint model, grid search is used to deter-\nmine βand results are shown in Figure 2. Conse-\nquently, we set the value of βto 0.6.\nFigure 2: The impact of βon BLEU-4\nWe search the best-trained checkpoint base on\nthe dev-set. In order to mitigate the ﬂuctuation of\nthe training procedure, we then average the nearest\n5 checkpoints to obtain a single averaged model.\nBeam search is used with a beam size of 12.\n3.3 Automatic Evaluation\nResults on BLEU The experimental results on\nBLEU (Papineni et al., 2002) are illustrated in\nTable 1. Our full model ( w/ features + lan-\nguage modeling) signiﬁcantly outperforms previ-\nous models and achieves state-of-the-art results\non both datasets, with 16.23 BLEU-4 score on\nSQuAD and 20.88 on MARCO respectively.\nResults without FeaturesTo investigate the ro-\nbustness of our model, we conduct an experiment\nwhose input sequence only takes word embed-\ndings and answer position, but without lexical fea-\nDataset SQuAD MARCO\nModel perplexity distinct-1 distinct-2 perplexity distinct-1 distinct-2\npointer generator with features (baseline) 41.24 9.67 39.46 17.69 16.84 45.61\nw/ features + language modeling 34.09 9.80 40.94 15.17 17.31 47.51\nw/o features + language modeling 38.70 9.73 40.89 14.07 17.13 46.98\nw/ features + 1-layer encoder 38.40 9.56 39.71 17.61 17.10 46.87\nTable 2: Perplexity and distinct of different setting models on two datasets\nDataset SQuAD MARCO\nModel Matching Fluency Relevance Matching Fluency Relevance\npointer generator with features (baseline) 0.983 1.573 1.540 1.133 1.667 1.593\n+ language modeling 1.147 1.690 1.600 1.160 1.720 1.603\nkendall correlation coefﬁcient 0.820 0.814 0.796 0.852 0.792 0.824\nTable 3: Human evaluation results on two datasets.\ntures ( w/o features + language modeling ). We\ncan see that the auxiliary task of language mod-\neling boosts model performance on both datasets,\ndemonstrating that our model guarantees higher\nstability because it does not depend on the quality\nof lexical features. Therefore, our model can ap-\nply to low-resource languages where there is not\nadequate data for training a well-performed model\nfor lexical features extraction.\nResults with a 3-layer EncoderTo validate that\nwe gain the improvement not due to a deeper net-\nwork, we replace the language modeling module\nwith one encoder layer, that is to say, we adopt\na 3-layer encoder. Comparing this model ( w/\nfeatures+1-layer encoder) with the full model (w/\nfeatures+language modeling), we can see that our\njoint-learning model performs better than simply\nadding an extra encoding layer. The results on\nMARCO also clearly show that a deeper network\ndoes not guarantee better performance.\nPerplexity and DiversitySince BLEU only mea-\nsures a hard matching between references and gen-\nerated text, we further adoptperplexity and distinct\n(Li et al., 2016) to judge the quality of generated\nquestions. The results in Table 2 indicate that the\nlanguage modeling task helps the model to gener-\nate more ﬂuent and readable questions. Besides,\nthe generated questions have better diversity.\n3.4 Human Evaluation\nFor a better study on the quality of generations,\nwe perform human evaluation. Three annotators\nare asked to grade the generated questions in three\naspects: matching indicates whether a question\ncan be answered with the given answer; fluency\nindicates whether a question is ﬂuent and gram-\nmatical; relevance indicates whether a question\ncan be answered according to the given context.\nThe rating score ranges from 0 to 2. We randomly\nsample 100 cases from each dataset for evalua-\ntion. Results are displayed in Table 3. The co-\nefﬁcient between human judges is high, validating\na high quality of our annotation. The results show\nthat by incorporating language modeling, the gen-\nerated questions receive higher scores across all\nthree metrics.\nContext: Prior to the early 1960s, access to the forest’s\ninterior was highly restricted, and the forest remained ba-\nsically intact.\nAnswer: The early 1960s\nReference: Accessing the Amazon rainforest was re-\nstricted before what era?\nBaseline: When did access to the forest’s interior?\nJoint-model: When did access to the forest’s interior be-\ncome restricted?\nContext: This teaching by Luther was clearly expressed\nin his 1525 publication on the bondage of the will, which\nwas written in response to on free will by Desiderius Eras-\nmus (1524).\nAnswer: 1525\nReference: When did Luther publish on the bondage of\nthe will?\nBaseline: In what year was the bondage of the will on the\nbondage of the will?\nJoint-model: When was the bondage of the will pub-\nlished?\nTable 4: Examples of generated questions by different\nmodels.\n3.5 Case Study\nFurther, Table 4 gives two examples of the gen-\nerated questions on SQuAD dataset, by the base-\nline model and our joint model respectively. It is\nobvious that questions generated by our proposed\nmodel are more complete and grammatical.\n4 Conclusion\nThis paper proves that equipped with language\nmodeling as an auxiliary task, the neural model\nfor QG can learn better representations that help\nthe decoder to generate more accurate and ﬂuent\nquestions. In future work, we will adopt the aux-\niliary language modeling task to other neural gen-\neration systems to test its generalization ability.\nAcknowledgments\nWe thank Weikang Li and Xin Jia for their valu-\nable comments and suggestions. This work is sup-\nported by the National Natural Science Founda-\ntion of China (61773026, 61572245).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nRich Caruana. 1998. Learning to learn. chapter Mul-\ntitask Learning, pages 95–133. Kluwer Academic\nPublishers, Norwell, MA, USA.\nYing Chen, Wenjun Hou, Xiyao Cheng, and Shoushan\nLi. 2018. Joint learning for emotion classiﬁcation\nand emotion cause detection. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October\n31 - November 4, 2018, pages 646–651.\nKenneth Mark Colby. 1975. Artiﬁcial Paranoia. Else-\nvier Science Inc., New York, NY , USA.\nXinya Du and Claire Cardie. 2017. Identifying where\nto focus in reading comprehension for neural ques-\ntion generation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2017, Copenhagen, Denmark,\nSeptember 9-11, 2017, pages 2067–2073.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1342–1352.\nNan Duan, Duyu Tang, Peng Chen, and Ming Zhou.\n2017. Question generation for question answer-\ning. In Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September\n9-11, 2017, pages 866–874.\nHan Guo, Ramakanth Pasunuru, and Mohit Bansal.\n2018. Soft layer-speciﬁc multi-task summarization\nwith entailment and question generation. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 687–697.\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-\nruoka, and Richard Socher. 2017. A joint many-task\nmodel: Growing a neural network for multiple NLP\ntasks. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2017, Copenhagen, Denmark, Septem-\nber 9-11, 2017, pages 1923–1933.\nAlex Kendall, Yarin Gal, and Roberto Cipolla. 2018.\nMulti-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In 2018\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, pages 7482–7491.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, San Diego California, USA, June 12-17,\n2016, pages 110–119.\nLizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu,\nand Guoping Hu. 2018. Neural multitask learning\nfor simile recognition. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 1543–1553.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings\nof the Workshop on Cognitive Computation: Inte-\ngrating neural and symbolic approaches 2016 co-\nlocated with the 30th Annual Conference on Neu-\nral Information Processing Systems (NIPS 2016),\nBarcelona, Spain, December 9, 2016.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA., pages 311–318.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nMarek Rei. 2017. Semi-supervised multitask learn-\ning for sequence labeling. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2017, Vancouver, Canada,\nJuly 30 - August 4, Volume 1: Long Papers , pages\n2121–2130.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30\n- August 4, Volume 1: Long Papers , pages 1073–\n1083.\nHeung-Yeung Shum, Xiaodong He, and Di Li. 2018.\nFrom eliza to xiaoice: challenges and opportuni-\nties with social chatbots. Frontiers of IT & EE ,\n19(1):10–26.\nLinfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang,\nand Daniel Gildea. 2018. Leveraging context infor-\nmation for natural question generation. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 2 (Short Papers), pages 569–574.\nSandeep Subramanian, Tong Wang, Xingdi Yuan,\nSaizheng Zhang, Adam Trischler, and Yoshua Ben-\ngio. 2018. Neural models for key phrase extrac-\ntion and question generation. In Proceedings of\nthe Workshop on Machine Reading for Question An-\nswering@ACL 2018, Melbourne, Australia, July 19,\n2018, pages 78–88.\nXingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yan-\njun Ma, and Shi Wang. 2018. Answer-focused and\nposition-aware neural question generation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018, pages 3930–\n3939.\nDuyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.\nQuestion answering and question generation as dual\ntasks. CoRR, abs/1706.02027.\nHai Ye and Lu Wang. 2018. Semi-supervised learning\nfor neural keyphrase generation. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, Brussels, Belgium, Octo-\nber 31 - November 4, 2018, pages 4142–4153.\nYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa\nKe. 2018. Paragraph-level neural question gener-\nation with maxout pointer and gated self-attention\nnetworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 3901–3910.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNatural Language Processing and Chinese Comput-\ning - 6th CCF International Conference, NLPCC\n2017, Dalian, China, November 8-12, 2017, Pro-\nceedings, pages 662–671.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8223764896392822
    },
    {
      "name": "Task (project management)",
      "score": 0.6524451375007629
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5866264700889587
    },
    {
      "name": "Multi-task learning",
      "score": 0.5747055411338806
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.5657577514648438
    },
    {
      "name": "Representation (politics)",
      "score": 0.5601105093955994
    },
    {
      "name": "Encoder",
      "score": 0.5495240092277527
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5236836671829224
    },
    {
      "name": "Language model",
      "score": 0.5083596110343933
    },
    {
      "name": "Natural language processing",
      "score": 0.46214163303375244
    },
    {
      "name": "Machine learning",
      "score": 0.3744048476219177
    },
    {
      "name": "Engineering",
      "score": 0.08618757128715515
    },
    {
      "name": "Power (physics)",
      "score": 0.06733930110931396
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}