{
  "title": "Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data",
  "url": "https://openalex.org/W4385764384",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2283893245",
      "name": "Shengchao Chen",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2140909072",
      "name": "Guodong Long",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2014104297",
      "name": "Tao Shen",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2007802817",
      "name": "Jing Jiang",
      "affiliations": [
        "University of Technology Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1789155650",
    "https://openalex.org/W2132558761",
    "https://openalex.org/W2965580012",
    "https://openalex.org/W2137360132",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W4221148875",
    "https://openalex.org/W2163673508",
    "https://openalex.org/W3012549930",
    "https://openalex.org/W6772797770",
    "https://openalex.org/W3006555759",
    "https://openalex.org/W2244217450",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W2914328083",
    "https://openalex.org/W3196053097",
    "https://openalex.org/W3023042748",
    "https://openalex.org/W2541884796",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2172064003",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W2001459659",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W3173539742",
    "https://openalex.org/W3092952717",
    "https://openalex.org/W3092172514",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W4221157218",
    "https://openalex.org/W4311000001",
    "https://openalex.org/W4293141861",
    "https://openalex.org/W3158969156",
    "https://openalex.org/W4281384403",
    "https://openalex.org/W2995956126",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W3103720336",
    "https://openalex.org/W4281753687",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4318072078",
    "https://openalex.org/W4367365932",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3123616420",
    "https://openalex.org/W4285606214",
    "https://openalex.org/W3038022836",
    "https://openalex.org/W4287021126",
    "https://openalex.org/W4303613606",
    "https://openalex.org/W4287888016",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3166441238",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W2904760378",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4312869277",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4385768177",
    "https://openalex.org/W3000500483",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4287757821",
    "https://openalex.org/W4382318655",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W4297899355",
    "https://openalex.org/W2955213239"
  ],
  "abstract": "To tackle the global climate challenge, it urgently needs to develop a collaborative platform for comprehensive weather forecasting on large-scale meteorological data. Despite urgency, heterogeneous meteorological sensors across countries and regions, inevitably causing multivariate heterogeneity and data exposure, become the main barrier. This paper develops a foundation model across regions capable of understanding complex meteorological data and providing weather forecasting. To relieve the data exposure concern across regions, a novel federated learning approach has been proposed to collaboratively learn a brand-new spatio-temporal Transformer-based foundation model across participants with heterogeneous meteorological data. Moreover, a novel prompt learning mechanism has been adopted to satisfy low-resourced sensors' communication and computational constraints. The effectiveness of the proposed method has been demonstrated on classical weather forecasting tasks using three meteorological datasets with multivariate time series.",
  "full_text": "Prompt Federated Learning for Weather Forecasting:\nToward Foundation Models on Meteorological Data\nShengchao Chen, Guodong Long, Tao Shenand Jing Jiang\nAustralian Artificial Intelligence Institute, FEIT, University of Technology Sydney\nshengchao.chen.uts@gmail.com, {guodong.long, tao.shen, jing.jiang}@uts.edu.au\nAbstract\nTo tackle the global climate challenge, it urgently\nneeds to develop a collaborative platform for com-\nprehensive weather forecasting on large-scale me-\nteorological data. Despite urgency, heterogeneous\nmeteorological sensors across countries and re-\ngions, inevitably causing multivariate heterogene-\nity and data exposure, become the main barrier.\nThis paper develops a foundation model across\nregions capable of understanding complex mete-\norological data and providing weather forecast-\ning. To relieve the data exposure concern across\nregions, a novel federated learning approach has\nbeen proposed to collaboratively learn a brand-\nnew spatio-temporal Transformer-based foundation\nmodel across participants with heterogeneous me-\nteorological data. Moreover, a novel prompt learn-\ning mechanism has been adopted to satisfy low-\nresourced sensors’ communication and computa-\ntional constraints. The effectiveness of the pro-\nposed method has been demonstrated on classical\nweather forecasting tasks using three meteorologi-\ncal datasets with multivariate time series.\n1 Introduction\nClimate change will significantly impact all regions; how-\never, the specific effects will vary [Kjellstrom et al., 2016 ].\nIncreasing global temperatures and melting ice will lead to\nalterations in sea levels, ocean currents, weather patterns, and\ncloud cover [Hagemann et al., 2013 ]. To effectively tackle\nthe challenge of global climate change, the implementation of\na large-scale collaborative data-sharing platform is essential.\nAlthough this work is labor-intensive and demands a mul-\ntitude of skilled experts, the utilization of machine learning\ntechniques can enhance efficiency in addressing this problem.\nNonetheless, the machine learning domain faces a challenge\nwhen attempting to employ a centralized uniform model to\nserve all regions due to their heterogeneity. An effective solu-\ntion involves pre-training a foundational model using exten-\nsive weather data and enabling each region to fine-tune the\nmodel using a relatively small data to enhance its ability to\ncapture local weather patterns.\n..........\nClients\nMeteorological sensors\nServer\nGlobal Weather Analysis System \nPre-trained Foundation Model (FM)\nGraph-based Aggregation\nParameters\nLocal FM\nPrompts\n①\n②\n③\nLocal FM Local FM\n..........\nPrompts\nPrompts\nParametersParameters\nFigure 1: Our MetePFL for weather forecasting. i) pre-trained FM\ninitializes the local FM; ii) local FM trains using local data; iii) the\nserver aggregates and transmits local prompts’ parameters.\nWeather forecasting is a fundamental analytical task aimed\nat modeling the dynamic changes of weather on both a global\nand regional scale. Multi-sensor weather forecasting serves\nas a critical tool in mitigating the loss of human lives and\nproperty by providing early warnings for extreme weather\nevents resulting from global climate change [Chattopadhyay\net al., 2020]. The objective of this approach is to capture po-\ntential correlations between multiple meteorological factors\nand the tendency of weather variations in order to gain a com-\nprehensive understanding of specific regions. Unlike conven-\ntional time-series data, weather time-series data in meteorol-\nogy are gathered from sensing devices distributed across di-\nverse geographical locations [Campbell and Diebold, 2005].\nOne-step forecasting [Chen et al., 2022b; Chen et al.,\n2023], empowered by recent advancements in deep learning,\nhave garnered considerable attention due to their high effi-\nciency. However, the performance of these strategies is hin-\ndered by the non-stationary nature of weather changes. This\nlimitation arises from their reliance on fixed patterns derived\nfrom prior knowledge. To address this issue and capture tem-\nporal information, other studies[Karevan and Suykens, 2020;\nAll´eon et al., 2020 ] suggest formulating the tasks as auto-\nregression problems. These studies utilize preceding time\nstep variables to predict variables at the subsequent time\nstep [Chen and Lai, 2011]. This technique is commonly im-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3532\nplemented using RNN or Transformer models. The choice of\nthese models is based on their superior performance in time\nseries analysis [Shi et al., 2015].\nHowever, previous works focus on using an uniform model\nto serve all regions regardless of heterogeneity. In contrast,\nfoundation model (FM) represents a novel service architec-\nture that aims to pre-train a large model with extensive data.\nSubsequently, this model can be fine-tuned for specific tasks\nusing relevant data, such as weather forecasting in a partic-\nular region. The FM has the capability to capture common\nknowledge shared among multiple tasks or participants. Fur-\nther refinements can then enhance its alignment with the spe-\ncific requirements of a given task. The FM has demonstrated\nremarkable success in Natural Language Processing (NLP),\nexemplified by ChatGPT. Notably, recent progress in foun-\ndation models has been observed across diverse domains, in-\ncluding ViT [Xu et al., 2022], BERT [Yates et al., 2021], and\nCLIP [Radford et al., 2021].\nIn contrast to existing FMs, training a FM on weather fore-\ncasting tasks must tackle the following challenges. First,\nsharing raw data across countries/regions will not be easy.\nSecond, transmitting and processing the continuously col-\nlected data is a challenge for low-resourced sensors or de-\nvices. Third, real-time forecasting is critically important. In\nsummary, we need a solution to tackle data security, com-\nmunication, and computation efficiency issues and provide\non-device decisions independently.\nThis paper will design a novel machine-learning approach\nto train foundation models on weather forecasting tasks. The\nmodel will be capable of understanding and constructing\nthe complex spatiotemporal relationship of meteorological\ndata to provide reliable analysis support on weather fore-\ncasting and global climate challenges. Specifically, we pro-\npose a novel Meteorological Prompt Federated Learning\n(MetePFL) approach to collaboratively learn a Transformer-\nbased foundation model (FM) across devices with multivari-\nate time-series data (see Figure 1). TheMetePFL only consid-\ners the model parameters exchange among devices rather than\ndirect data sharing. Considering the low-resourced sensors’\ncommunication efficiency constraint, a brand-new prompt\nlearning mechanism is introduced upon a pre-trained FM\nto comprehensively explore the correlation among weather-\nrelated variables while computing a few parameters.\nThree weather forecasting datasets based on multivariate\ntime series with multiple meteorological factors, i.e., precip-\nitation, temperature, upstream, are leveraged to verify the ef-\nfectiveness of MetePFL. The main contributions of this work\nare summarized in four-fold:\n• This is the first work to explore a foundation model-\nbased solution to enhance weather forecasting tasks to-\nwards a global scale.\n• The proposed prompt federated learning approach is a\nnovel mechanism to collaboratively learns a foundation\nmodel for the applications with many satellite sites or\nstations across regions.\n• A spatio-temporal prompt learning mechanism has been\ndesigned to efficiently tackle multivariable time series.\n• Experiments on three real datasets have demonstrated\nthe effectiveness and superior of our proposed solution.\nIt is worth noting that we obtain excellent performance\nwith only 2.38% of the model’s parameters trained.\n2 Related Work\nWeather Forecasting. Weather forecasting plays a crucial\nrole in the global climate analysis system. Conventional fore-\ncasting methods utilize numerical weather prediction (NWP)\nmodels, which incorporate physical constraints to simulate\nweather phenomena [Bauer et al., 2015 ]. However, with\nthe emergence of data-driven approaches, weather forecast-\ning has shifted towards approaches driven by data, such\nas ARIMA [Chen and Lai, 2011 ], SVM [Sapankevych and\nSankar, 2009], and NNs [V oyantet al., 2012 ]. While these\nbasic models exhibit potential, they encounter difficulties in\ncomprehending nonlinear temporal dynamics. Deep Learn-\ning methods, especially models based on Recurrent Neu-\nral Networks (RNNs), have exhibited promising outcomes\nin weather forecasting [Shi et al., 2015 ]. Recently, Trans-\nformers have demonstrated superior performance compared\nto RNN-based models in time series analysis [Bojesomo et\nal., 2021 ], thereby gaining popularity for weather-related\ntasks [Chen et al., 2023]. However, these models neglect the\ndata exposure concerns when utilizing multi-sensor data for\nreal-world tasks and modeling spatio-temporal correlations.\nFoundation Model. A fully supervised learning paradigm\nneeds a large scale of data. The foundation model provides a\npractical solution for scenario-specific tasks, aiming to pre-\ntrain a model using extensive prior knowledge. The FM\nhas widespread applications in NLP [Yates et al., 2021] and\nCV [Xu et al., 2022; Radford et al., 2021], providing an ef-\nfective cross-task learning strategy. For example, ChatGPT\ncan be used as a baseline model for researchers to fine-tune it\nto achieve more accurate responses for downstream tasks.\nFederated Learning. Federated learning (FL) is a new\nlearning paradigm to embody the collaborative training of\nmodels without requiring data exposure from each partici-\npant (e.g., meteorological sensors) [McMahan et al., 2017;\nLong et al., 2021; Long et al., 2020; Jiang et al., 2020 ].\nVanilla FL suffer from the heterogeneity of data and de-\nvices. Personalized FL aims to solve the problem by mul-\ntiple techniques [Tan et al., 2022a; Chen et al., 2022a;\nWang et al., 2022; Ma et al., 2022; Tan et al., 2021; Zhang et\nal., 2023; Li et al., 2023; Long et al., 2023; Li et al., 2019;\nLi et al., 2020; Gao et al., 2022] via training better personal-\nized model for each client. However, these methods are not\nsuitable for weather forecasting due to all parameters must\nbe considered during communication so that hinder the real-\ntime forecasting. pre-train-based strategy can mitigate the\nproblem [Tan et al., 2022b ] but can not explore the spatio-\ntemporal correlations. Different from the above methods, this\npaper focuses on establishing a high-efficiency FL approach\nthat provides analytical support to across-regional weather\nforecasting systems with heterogeneous meteorological data.\nPrompt Learning. Prompt learning as a lightweight mech-\nanism is widely used in NLP [Li and Liang, 2021; Liu et al.,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3533\n2021], which requires fewer parameters and is more adap-\ntive than fine-tuned pre-trained models by represented by sev-\neral prompt tuning strategies in different applications [Zhou\net al., 2022a ]. Different from language data, understanding\nmultidimensional correlation among multivariate data in the\nweather forecasting task is critical. However, the key point is\noften ignored by the federated prompt learning method [Guo\net al., 2022 ]. The paper introduces a novel prompt mecha-\nnism within the FL framework based on pre-trained FM to\nexplore the temporal dynamics and the potential correlation\namong clients while computing only a few parameters.\n3 Problem Formulation\nGiven N clients that possess individual local private datasets\nD, each client has a multivariate time series denoted asXi ∈\nRm×n. In this notation, each sample at a specific time step t\nis represented as xt ∈ R1×n. Weather forecasting using mul-\ntivariate time series can be defined as the process of utilizing\nhistorical values of all variables for a duration ofP periods to\npredict the values of a specific variable in the future over Q\nperiods, can be defined below:\n[xt−P , xt−P+1, ··· , xt]\nf\n−→\n\u0002\nx′\nt+1, x′\nt+2, ··· , x′\nt+Q\n\u0003\n, (1)\nwhere f is a learning system, and x′\nt ∈ R1×1 is the value of\nthe variable to be forecasting at the t-th time step. Valida FL\nsystem aims to minimize the average loss of the global model\nw on all clients’ local dataset:\nF(w): = arg min\nw1,w2,...,wN\nNX\nk=1\nnk\nn Fk(wk), (2)\nwhere nk is the number of samples hold by the k-th client. n\nis the number of samples held by all clients. Fk(wk) denotes\nthe local objective function of k-th client. The distinguishing\nfactor is that each client possesses a unique pattern, and sen-\nsors are deployed in specific locations, resulting in a statisti-\ncally heterogeneous environment. To address this challenge,\nPFL is typically modeled as a bi-level optimization problem.\nF(v; w): = arg min\n{w1,w2,...,wN},{v1,v2,...,vN}\nNX\nk=1\nGk(vk, w),\ni.e. G k(vk, w) =nk\nn Fk(vk) +λR(vk, w), (3)\nwhere each client hold a personalized model parameterized\nby vi, w denotes the global model. R(·) is the regularization\nterm to control model update, via avoiding the local model\nupdating be far away to the optimal global model.\n4 Meteorological Prompt Federated Learning\nThe framework of MetePFL is depicted in Figure 1. In con-\ntrast to conventional Federated Learning (FL) where random\nglobal parameters are broadcasted to each client, MetePFL\nemploys a fixed FM, thereby reducing computation costs and\nimproving performance without requiring extensive back-\npropagation. During each round, only the prompt parame-\nters of the clients are taken into consideration. The MetePFL\nframework consists of the Spatial-Temporal Prompt (STP)\nand the optimization process.\nFigure 2: Schematic of Spatial-Temporal Prompt Learning.\n4.1 Spatial-Temporal Prompt\nThe Spatial-Temporal Prompt (STP) shown in Figure 2 can\nbe divided into Temporal prompt learning (TPL) and Spatial\nprompt learning (SPL).\nTemporal prompt learning (TPL). To effectively under-\nstand the temporal dynamics under multivariate interactions,\nwe use a multi-step incremental learning mechanism for\nlearning the prompt parameters along the time dimension.\nThe TPL comprise four phases, is shown in Figure 3.\nGiven a time-series X ∈ Rm×n, the m and n represent\nthe number of time steps and variables, respectively. Define\na initial step l, the four temporal prompt format: (1) initial\nprompt ˆP; (2) Temporal prompt-I:P1; (3) Temporal prompt-\nII: P2 ∈ R2l×n; (4) Temporal prompt-III: P3 ∈ R2l×n.\nThe first p steps are fixed as Seq ∈ Rp×n. In addition,\nthe data within the p to k steps are spliced with the ˆP ∈\nR(k−p+l)×n to generate P1 by\n∥X(p∼k), ˆPT ∥ →PT,1, PT,1 ∈ R(2k−2p+l)×n, (4)\nwhere ∥∥ denote the concat operation. To enhance the accu-\nracy of forecasting performance, we adopt a two-stage objec-\ntive in the initial learning phase: ”X (0∼p) 1\n−→X(0∼k) 2\n−→\nX(0∼k+l)”. During this phase, we encourage the FM to en-\nhance prompt parameters associated with prior knowledge in\nX(p∼k) in order to establish a foundation for the subsequent\nstage. In the next stage, the FM learns the remaining PT,1\nparameters in X(k∼k+l). This approach enables the FM to\ndevelop a comprehensive understanding of the relationships\namong variables across different time periods, rather than es-\ntablishing rigid back-and-forth associations. The formulation\nof the initial learning phase is:\nR1 = FM([Seq, embed(PT,1)]), (5)\nwhere the embed(x) represents the learnable position en-\ncoder, and the FM is the pre-trained foundation model.\nSplicing discrete prompts to form a complete series is chal-\nlenging because their parameters cannot match the FM simul-\ntaneously. To establish the relationship between two prompts,\na dual-correct strategy is adopted in the second learning\nphase. This strategy encourages the FM to correctPT,1 based\non the previous PT,1 while learning PT,2. The objectives of\nthe second learning phase are ”X (0∼k+l) −→ X(0∼k+3l).”\nThis process can be formulated as mapping the input data\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3534\nTransformer Foundation Model\nTransformer Foundation Model\nSeq Temproral \nPrompt-I\nTemproral \nPadding\nSeq Input \nSequence\nTemproral \nPadding\nTemproral \nPrompt-II\nSeq Input \nSequence\nTemproral \nPromp-III\nTemproral \nPadding\nTemproral \nPrompt-II\nTemproral \nPrompt-I\nTemproral \nPrompt-III\nC\nFirst learning phase output\nSecond learning phase output\nThird learning phase output\nTransformer Encoder Module\nSecond learning phase output\nThird learning phase output\nFirst learning phase output\nMasked\n+\n++\nC\n×\nTanh\nUniform prompt\nTransformer Foundation Model\nTPL output\nFrozen Network\nUnfrozen Network\nDuplication (Input)\n+ × C Add/Hadamard product/Concat\nDuplication\nDuplication\nFixed Sequence\nInput\nFigure 3: The learning strategy of Temporal Prompt Learning, which consists of four different learning phases.\nfrom X(0∼k+l) to X(0∼k+3l) during the second phase.\n∥PT,1, PT,2∥ →PT,2, PT,2 ∈ R3l×n, (6)\nR2 = FM(∥Seq2, Rp/2∼p\n1 , embed(Rp∼p+l\n1 ), embed(PT,2)∥).\nThe objective of the third learning phase can be expressed\nas “X(0∼k+3l) −→X(0∼k+5l)”, for further correcting these\npreviously learned prompts to improving the continuity of\nseveral prompts and providing smooth transitions between\nthem. The third learning phase can be formulated as:\n∥PT,2, PT,3∥ →PT,3, PT,3 ∈ R4l×n, (7)\nR3 = FM(∥Seq3, ˆP ∗ Rp∼(p+l)\n2 , embed(PT,2), embed(PT,3)∥).\nTo prevent the prompt parameters from being overly biased\ntoward expressing short-term over long-term dependence, the\nfinal stage of learning intends to uniformly adjust these pa-\nrameters. The corresponding values from the three previous\nlearning phases are concatenated along the time dimension\nand then multiplied by the uniform prompt PT,w . The final\nlearning can be expressed as follows.\n∥PT,1, PT,2, PT,3∥ →PT,w ,\n∥PT,1 + PT,2 + PT,3, PT,2 + PT,3, PT,3∥ → ˆPT,w , (8)\nRT = FM(∥Seq, 1\nm − k\nmX\ni=k\n[tanh( ˆPT,w ) ∗ PT,w ]∥).\nSpatial Prompt Learning (SPL). We regard multiple me-\nteorological factors within a specific space. This enables the\nestablishment of correlations between these factors from a\nspatial perspective on the local client. The SPL are shown\nin Figure 4.\nThe trainable parameters that serve as spatial prompts can\nbe represented as PS ∈ R(m−k)×1, where m − k represents\nthe length of the forecasting period. Prior to the initial learn-\ning phase, the first p hours of the data are considered fixed\nand denoted as Seq ∈ Rk×n. Subsequently, Seq are com-\nbined with the initial spatial prompts along the temporal di-\nmension, and any gaps in the spatial dimension are filled with\nzero-valued parameters, resulting in ˆPS ∈ R(m−k)×(n−1).\nThe first learning phase comprises:\n∥PS, ˆPS∥ →PS,1,\nP′\nS,1 = PS,1 ∗ FM(∥Seq, S1∥). (9)\nIn learning spatial prompts for the ith variable, the ith\nlearning phase of SPL can be formulated as follow:\n∥P′\nS,i−1, ˆPS ∈ R(m−k)×(n−i)∥ →PS,i,\nP′\nS,i = PS,i ∗ FM(∥Seq, PS,i∥).\n(10)\nOne advantage of SPL over auto-regression is its continuous\ncorrection of learned prompt parameters from the previous it-\neration. This correction utilizes all the previously predicted\nvalues, leading to improved forecast accuracy. Moreover,\nSPL improves the model’s perception of correlations among\nmultiple variables by considering adjacent variables together.\nThrough multiple iterations of regression, SPL enhances the\nmodel’s ability to account for these spatial correlations.\nTo prevent isolating the TPL and SPL effects within the\nSTP, we combine the result of TPL and SPL empirically using\na Gate operation. The output of the STP is formulated as\nR = [(1− sigmoid(RS)) ∗ tanh(RT )] ∗ W.\n4.2 Optimization of MetePFL\nThe optimization objective of MetePFL is\narg min\nP\nNX\nk=1\nnk\nn [Fk({Pk}) +λR({Pk}, {Pg})] +τG(A) (11)\nwhere {Pk} is prompts including PS, PT , {Pg} is the global\nprompt parameters, G(A) is a regularization term used to\nrepresent the correlation among client according the adjacent\nmatrix A. specifically, the term Fk({P}) +λR({P}, {Pg})\ncan be formulated as Lk = LMSE + λ∥{Pk} − {Pg}∥2\n2. The\noptimization objective on i-th client as:\narg min\nA\nNX\nk=1\nλR({Pk}, {Pg}) +τG(A),\ns.t. Ps ∈ arg min\n{Pk}\nAj,iS({Pj}, {Pi}),\ni.e. Pg = G({Ps\n1 }, {Ps\n2 }, ...,{Ps\nN }),\n(12)\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3535\nFigure 4: The learning strategy of Spatial Prompt Learning, the empty is a location in data where variables need forecasting.\nAlgorithm 1MetePFL algorithm.\nInitialized PT , PS.\nfor each communication round t = 0, 1, 2, ··· , Tdo\nLocal model initialize:\nfor each client i = 0, 1, 2, ..., Nin parallel do\n0 ← PT , PS\nLocal model update:\nfor each client i = 0, 1, 2, ··· , Nin parallel do\nUpdate PT , PS for e local steps:\nTrain PT , PS with loss function L in Eq.(11)\nend for\nEach selected client sends PT , PS to the server\nAggregation:\nA ← GraphGenerator({P1}, {P2}, ··· , {PN })\nA′ ← GAT(A)\nUpdate P for r steps GCN (A′, {Pi}N\ni=1):\n{Ps\ni }N\ni=1 = A′{Pi}N\ni=1\n{Ps\ni }N\ni=1 = α{Ps\ni }N\ni=1 + (1− α){Pi}N\ni=1\nGet Global Prompts {Pg} ←G({Ps\n1 }, {Ps\n2 }, ...,{Ps\nN })\nend for\nwhere the A ∈ {0,1}, S({Pj}, {Pi}) is the similarity of\nprompt parameters of client i and client j measured by co-\nsine or distance, G(·) is the average operation. During the\noptimization of MetePFL, each client update their model via\nsolving the local objective function as Lk after them receive\nthe fixed foundation model at first. Then each client upload\ntheir prompts P rather than complete model to the server\nthat conduct graph-based aggregation, which significantly re-\nduce the communication overhead while exploring the po-\ntential correlations among clients. The aggregation includ-\ning two steps: Graph Attention Network (GAT) [Veliˇckovi´c\net al., 2017]-based graph structure learning that explore the\ndynamic correlations among clients and Graph Convolution\nNetwork (GCN) [Kipf and Welling, 2016] that utilized to pa-\nrameters reconstruction using learned adjacent matrix A and\nthe prompt parameters uploaded by clients. The GCN auto-\nmatically updates the parameters of each node by aggregating\nthe models of its neighbors in the graph.\n5 Experiments\nBaselines. We compare our MetePFL with STGCN [Yu et\nal., 2017 ], LSTM [Graves, 2012 ], ConvLSTM [Shi et al.,\n2015], Transformer [Zerveas et al., 2021 ], Informer [Zhou\net al., 2021 ], Autoformer [Wu et al., 2021 ], and Fed-\nformer [Zhou et al., 2022b]. The LSTM-based models have\nfour layers. The Transformer consists of an eight-layer En-\ncoder, while the Informer, Autoformer, and FEDformer con-\nsist of two encoders and a decoder. The Transforme models\nare trained by data-centric and FL setting, respectively.\nDatasets. We compiled three multivariate time series\ndatasets from NASA1, Average Precipitation (AvePRE), Sur-\nface Temperature (SurTEMP), and Surface Upstream (Su-\nrUPS) collected by 88, 525, and 238 devices, respectively.\nAll three datasets cover the hour-by-hour variability of 12 dif-\nferent weather-related meteorological variables.\nExperimental Setups. Models’ input and output dimen-\nsions are C = 12 and C = 1, respectively. The dataset is split\ninto training, validation, and testing in a 6:2:2 ratio. For pre-\ntraining, we use 2/3 of the training set (i.e., 50% of the entire\ndataset) for training and 1/6 as the validation set (i.e.,10% of\nthe complete dataset) based on the above partition, following\nthe pre-training strategy from Zerveas et al. [Zerveas et al.,\n2021]. For fine-tuning and prompt learning, the last 1/6 of\nthe training set is used for training, while the validation and\ntest sets remain unchanged. In the federated training process,\nwe set k to control the number of clients participating in train-\ning per round, and we use k = 0.1 and k = 0.2 in the experi-\nments. The forecasting uses 12 time steps in history (P =12,\ni.e., the past twelve hours) to predict 15 time steps in the fu-\nture (Q=15, i.e., fifty hours in the future) with a time window\nlength of 27 h. We setl to 3, considering the validity time and\ntrigger threshold of weather events. All models were trained\non an NVIDIA Tesla V100 GPU using an initial learning rate\nof 1e−3 and a batch size of 128, ADAM [Kingma and Ba,\n2014] as the optimizer. The algorithm used in the FL of the\nTransformer-based network is FedAvg. The communication\nround was set to 20, α = 0.5, and the early stopping strat-\negy was applied. Mean absolute error (MAE) and root mean\nabsolute error (RMSE) as evaluation metrics. The code is\navaliable at https://github.com/shengchaochen82/MetePFL .\n5.1 Overall Comparison\nTable 1 presents a performance comparison between our\nMetePFL and baselines. Results indicate the superiority of\nthe Transformer-based model over the LSTM-based model\nand STGCN when trained centrally across all three datasets.\nHowever, when utilized as a FM within the FL framework,\nthe performance of the Transformer-based model is dimin-\n1https://disc.gsfc.nasa.gov/\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3536\nModel T\nemproal Encoding\nAvePRE SurTEMP SurUPS\nMAE RMSE MAE RMSE MAE RMSE\nSTGCN None 0.331\n0.815 0.196 0.257 0.298 0.399\nConvLSTM None 0.305 0.730 0.198 0.266 0.311 0.416\nLSTM None 0.326 0.781 0.212 0.274 0.335 0.431\nTransformer\nLearnable\n0.301 0.714 0.236 0.313 0.369 0.475\nFixed 0.332 0.744 0.239 0.320 0.351 0.449\nFEDformer\nLearnable 0.239 0.547 0.165 0.214 0.205 0.271\nFixed 0.248 0.564 0.165 0.216 0.201 0.264\nAutoformer\nLearnable 0.271 0.589 0.167 0.235 0.201 0.265\nFixed 0.269 0.590 0.176 0.228 0.212 0.279\nInformer\nLearnable 0.213 0.543 0.191 0.245 0.251 0.330\nFixed 0.216 0.547 0.193 0.251 0.240 0.311\nFed-Transformer\nLearnable 0.454/0.402 0.927/0.892 0.780/0.684 0.910/0.793 0.621/0.522 0.769/0.640\nFed-FEDformer Learnable 0.397/0.372 0.791/0.726 0.684/0.530 0.822/0.680 0.612/0.512 0.754/0.647\nFed-Autoformer Learnable 0.425/0.349 0.784/0.724 0.742/0.627 0.924/0.765\n0.578/0.503 0.715/0.602\nFed-Informer Learnable\n0.385/0.361 0.865/0.768\n0.647/0.513 0.790/0.656 0.605/0.543 0.737/0.724\nPromptFL-Transformer\nLearnable 0.427/0.389 0.828/0.786 0.683/0.612 0.824/0.741 0.603/0.519 0.766/0.641\nMetePFL* Learnable 0.389/0.376 0.631/0.626 0.592/0.522 0.611/0.597 0.584/0.485 0.721/0.610\nMetePFL Learnable 0.378/0.342 0.628/0.605 0.556/0.542 0.601/0.569 0.521/0.460 0.642/0.589\nTable 1: Performance comparison of MetePFL with baselines, the first eight models are trained from scratch using full parameters. Fed-\nTransformer refers to training the Transformer model from scratch in a federated learning (FL) setting, the last two models employ FL-based\nprompt learning methods with a pre-trained Transformer as the FM, the symbol∗ indicates a FedAvg-based implementation, underlinemeans\nthe optimal in FL full parameters training, Bold means the optimal in FL prompt learning strategy.\nFM Algorithm\nA\nvePRE SurTEMP SurUPS\nMAE RMSE\nMAE RMSE MAE RMSE\nTransformer*\nFedAtt\n0.507/0.467 0.836/0.823 0.978/0.947 1.279/1.186 0.705/0.686 0.828/0.820\nFedProx 0.567/0.531 0.845/0.827 0.922/0.901 1.141/1.102 0.688/0.672 0.814/0.810\nScaffold 0.567/0.536 0.833/0.811 0.930/0.899 1.232/1.200 0.697/0.676 0.817/0.808\nFedAvg 0.611/0.591 0.823/0.810 0.998/0.896 1.118/1.115 0.706/0.699 0.832/0.821\nTransformer\nMetePFL (FedAtt)\n0.383/0.357 0.735/0.618 0.576/0.520 0.603/0.575 0.511/0.482 0.642/0.610\nMetePFL (FedProx) 0.399/0.385 0.691/0.633 0.564/0.542 0.686/0.667 0.556/0.512 0.702/0.651\nMetePFL (Scaffold) 0.385/0.353 0.755/0.627 0.602/0.531 0.727/0.600 0.560/0.512 0.719/0.645\nMetePFL (FedAvg) 0.389/0.376 0.631/0.626 0.592/0.522 0.611/0.597 0.584/0.485 0.721/0.610\nMetePFL 0.378/0.342 0.628/0.605 0.556/0.542 0.601/0.569 0.521/0.460 0.642/0.589\nTable 2: Performance comparison between fine-tuned Transformer and MetePFL with different FL algorithm, ∗ implying that the model\napplies fine-tuning strategy, the MetePFL (·) means that the implementation based on other FL algorithm, Bold means the optimal results.\nished compared to the trained FM. This reduction can be at-\ntributed to the heterogeneity of weather data collected from\nmultiple sensors. Notably, while the Transformer may be less\neffective than other similar models in centralized training, it\ndemonstrates a significant performance advantage over Fed-\nFEDformer, Fed-Autoformer, and Fed-Informer when em-\nployed as an FM within the MetePFL. This observation sug-\ngests that STP enhances the Transformer’s capability to com-\nprehend spatiotemporal data. Furthermore, our reliable FM\nand aggregation algorithms (Transformer and FedAvg) sur-\npass PromptFL [Guo et al., 2022 ]. This outcome provides\nadditional validation of the effectiveness and superiority of\nour proposed MetePFL.\nTo evaluate the effectiveness and superiority of MetePFL\nover fine-tuning, we implement different MetePFL version\nbased on four FL algoirthms: FedAtt[Jiang et al., 2020], Fed-\nProx [Sahu et al., 2018], Scaffold [Karimireddy et al., 2020],\nand FedAvg [McMahan et al., 2017 ]. We maintained the\nprompt setting and compared the results against fine-tuning.\nThe results are presented in Table 2. Our experiments reveal\nthe following findings: (1) MetePFL outperforms the fine-\ntuning method, highlighting the sensitivity of data correlation\nto the STP approach; (2) the graph-based aggregation used in\nMetePFL surpasses other algorithms, indicating its effective-\nness in mitigating the negative impact of Non-IID.\n5.2 Framework Applicability\nTo determine the applicability of MetePFL, we replaced its\nFM with pre-trained Informer and Autoformer using the same\npre-training strategy. Additionally, we used fine-tuning and\nPromptFL as reference strategies. The results as shown in\nTable 3. Our proposed MetePFL remains valid for other\nTransformer FMs and significantly outperforms fine-tuning\nand PromptFL. By comparing the performance of MetePFL\nunder different FL algorithm, we demonstrate the effective-\nness of the graph-based aggregation once again.\n5.3 Parameter Utilization\nTable 4 compares parameter utilization for different federated\nprompt learning strategies. PromptFL and MetePFL are sig-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3537\nModel Strate\ngy AvePRE SurTEMP SurUPS\nFed-Informer\nFine-tuning (FedA\nvg) 0.397/0.391 0.776/0.759 0.734/0.713 0.874/0.864 0.669/0.631 0.824/0.781\nFine-tuning (FedAtt) 0.403/0.378 0.780/0.724 0.950/0.899 1.057/1.00 0.686/0.675 0.807/0.793\nPromptFL 0.407/0.361\n0.742/0.727 0.722/0.700 0.867/0.832 0.671/0.658 0.786/0.753\nMetePFL (FedAvg) 0.382/0.357 0.631/0.618 0.701/0.692 0.840/0.812 0.698/0.650 0.796/0.725\nMetePFL (FedAtt) 0.392/0.387 0.776/0.758 0.754/0.696 0.905/0.839 0.669/ 0.644 0.772/0.738\nMetePFL 0.363/0.358 0.630/0.601 0.713/0.677 0.838/0.800 0.652/0.644 0.755/0.739\nFed-Autoformer\nFine-tuning (FedA\nvg) 0.378/0.373 0.761/0.758 0.693/0.682 0.848/0.834 0.610/0.523 0.767/0.721\nFine-tuning (FedAtt) 0.372/0.355 0.761/0.754 0.706/0.683 0.848/0.839 0.598/0.551 0.742/0.698\nPromptFL 0.355/0.348 0.759/0.753\n0.672/0.659 0.826/0.807 0.584/0.543 0.724/0.678\nMetePFL (FedAvg) 0.364/0.348 0.780/0.731 0.674/0.630 0.820/0.781 0.564/0.520 0.736/0.656\nMetePFL (FedAtt) 0.372/0.341 0.762/0.754 0.689/0.641 0.824/0.766 0.549/0.525 0.717/0.650\nMetePFL 0.355/0.334 0.750/0.719 0.666/0.630 0.814/0.750 0.547/0.516 0.712/0.649\nTable 3: Performance comparison of pre-trained Fed-Informer and Fed-Autoformer with different learning strategies, MetePFL (FedAvg)\nand MetePFL (FedAtt) implies the FedAvg- and FedAtt-based implementations, respectively,Bold means the optimal performance.\nStrategy # of\nTotal Param # of Training Param # of Participation Param\nFL-Regular\n3,229,857 3,229,857 100%\nFL-Fine Tuning 3,288,886 109,854 30.37%\nPromptFL 3,250,867 71,835 2.22%\nMetePFL (Ours) 3,258,547 77,595 2.38%\nTable 4: Comparison of parameter utilization of MetePFL.\nnificantly more advantageous than regular training (train from\nscratch) and fine-tuning, with parameter utilization of 2.22%\nand 2.38%, respectively - nearly 28% lower than fine-tuning.\nWhile PromptFL is better thanMetePFL in parameter utiliza-\ntion (-0.16%), it performs nearly 20% worse than MetePFL\n(see Table 1). Despite considering only 2.38% parameters,\nMetePFL achieves excellent performance and significantly\nimproves inter-device communication efficiency.\n5.4 Ablation Study\nTo evaluate the effectiveness of STP, we conducted abla-\ntion studies under general scenarios rather than under the\nFL framework. The results are presented in Table 5. The\nEncoder-only and Decoder-only Transformer underwent pre-\ntraining with one-step forecasting and auto-regressive mech-\nanisms similar to previous experiments. The Encoder-only\nTransformer trained with STP outperformed the Decoder-\nonly Transformer, indicating the superior learning mecha-\nnism of the proposed STP over conventional auto-regression.\nMoreover, the performance gap between the pre-trained En-\ncoder with and without STP confirms the efficacy of the pro-\nposed STP in general scenarios.\nModel Strate\ngy MAE RMSE\nEncoder-only\nT\nrain from scratch 0.332 0.744\nTrain from scratch & STP 0.300 0.689\nPre-train 0.295 0.668\nPre-train & STP 0.267 0.571\nDecoder-only\nTrain from scratch 0.304 0.724\nTable 5: Comparison of Encoder/Decoder-only Transformer under\ndifferent strategies in general scenarios based on AvePRE.\nThe effectiveness of SPL, Gate, and their advantage\nover position-aware embedding (PE) was verified using the\nTPL SPL\nGate PE MAE RMSE\nw w\nw/o w/o 0.301 0.608\nw w w w/o 0.267 0.571\nw/o w w/o w/o 0.284 0.580\nw w/o w/o w/o 0.299 0.613\nw/o w/o w/o w 0.284 0.617\nTable 6: Ablation results of the proposed STP in Encoder-only\nTransformer based on the AvePRE in general scenarios.\nEncoder-only Transformer. The results are presented in Ta-\nble 6. SPL demonstrates a greater improvement compared\nto PE. Additionally, SPL enhances the model’s performance,\nwhile the model without Gate experiences a significant de-\ncline. TPL does not perform as effectively as desired in\ngeneric scenarios. Consequently, we conducted ablation ex-\nperiments in the FL setting, as shown in Table 7. The results\nindicate that both TPL and SPL enhance the model’s fore-\ncasting performance. In conclusion, the effectiveness of the\nGate operation is demonstrated, and both TPL and SPL can\nimprove the model’s performance in the FL setting.\nTPL SPL Gate MAE RMSE\nw w w 0.378 0.628\nw/o w w/o 0.407 0.722\nw w/o w/o 0.415 0.740\nTable 7: Ablations on FL setting.\n6 Conclusion\nThis paper proposes a novel machine learning approach to\ntrain foundation models for weather forecasting tasks, capa-\nble of capturing the spatiotemporal relationships of meteoro-\nlogical data based on multivariate time series. To enhance the\nperformance while keeping data secure and reducing com-\nmunication overhead, we introduce a prompt learning mech-\nanism based on the fixed foundation model within the FL\nframework. Additionally, we utilize a graph-based approach\nto mitigate the impact of data heterogeneity on model effec-\ntiveness. Extensive experiments on three real-world weather\ndatasets confirm the effectiveness of our proposed method.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3538\nReferences\n[All´eon et al., 2020] Antoine All ´eon, Gr ´egoire Jauvion,\nBoris Quennehen, and David Lissmyr. Plumenet: Large-\nscale air quality forecasting using a convolutional lstm net-\nwork. arXiv preprint arXiv:2006.09204, 2020.\n[Bauer et al., 2015] Peter Bauer, Alan Thorpe, and Gilbert\nBrunet. The quiet revolution of numerical weather pre-\ndiction. Nature, 525(7567):47–55, 2015.\n[Bojesomo et al., 2021] Alabi Bojesomo, Hasan Al-\nMarzouqi, Panos Liatsis, Gao Cong, and Maya Ramanath.\nSpatiotemporal swin-transformer network for short time\nweather forecasting. In CIKM Workshops, 2021.\n[Campbell and Diebold, 2005] Sean D Campbell and Fran-\ncis X Diebold. Weather forecasting for weather deriva-\ntives. Journal of the American Statistical Association,\n100(469):6–16, 2005.\n[Chattopadhyay et al., 2020] Ashesh Chattopadhyay,\nEbrahim Nabizadeh, and Pedram Hassanzadeh. Analog\nforecasting of extreme-causing weather patterns using\ndeep learning. Journal of Advances in Modeling Earth\nSystems, 12(2):e2019MS001958, 2020.\n[Chen and Lai, 2011] Ling Chen and Xu Lai. Comparison\nbetween arima and ann models used in short-term wind\nspeed forecasting. In 2011 Asia-Pacific Power and Energy\nEngineering Conference, pages 1–4. IEEE, 2011.\n[Chen et al., 2022a] Fengwen Chen, Guodong Long, Zong-\nhan Wu, Tianyi Zhou, and Jing Jiang. Personal-\nized federated learning with graph. arXiv preprint\narXiv:2203.00829, 2022.\n[Chen et al., 2022b] Shengchao Chen, Ting Shu, Huan Zhao,\nQilin Wan, Jincan Huang, and Cailing Li. Dynamic mul-\ntiscale fusion generative adversarial network for radar im-\nage extrapolation. IEEE Transactions on Geoscience and\nRemote Sensing, 60:1–11, 2022.\n[Chen et al., 2023] Shengchao Chen, Ting Shu, Huan Zhao,\nGuo Zhong, and Xunlai Chen. Tempee: Temporal-spatial\nparallel transformer for radar echo extrapolation beyond\nauto-regression. arXiv preprint arXiv:2304.14131, 2023.\n[Gao et al., 2022] Liang Gao, Huazhu Fu, Li Li, Yingwen\nChen, Ming Xu, and Cheng-Zhong Xu. Feddc: Feder-\nated learning with non-iid data via local drift decoupling\nand correction. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n10112–10121, 2022.\n[Graves, 2012] Alex Graves. Long short-term memory. Su-\npervised sequence labelling with recurrent neural net-\nworks, pages 37–45, 2012.\n[Guo et al., 2022] Tao Guo, Song Guo, Junxiao Wang, and\nWenchao Xu. Promptfl: Let federated participants co-\noperatively learn prompts instead of models–federated\nlearning in age of foundation model. arXiv preprint\narXiv:2208.11625, 2022.\n[Hagemann et al., 2013] Stefan Hagemann, Cui Chen, Dou-\nglas B Clark, Sonja Folwell, Simon N Gosling, Ingjerd\nHaddeland, Naota Hanasaki, Jens Heinke, Fulco Ludwig,\nFrank V oss, et al. Climate change impact on available wa-\nter resources obtained using multiple global climate and\nhydrology models. Earth System Dynamics, 4(1):129–\n144, 2013.\n[Jiang et al., 2020] Jing Jiang, Shaoxiong Ji, and Guodong\nLong. Decentralized knowledge acquisition for mobile in-\nternet applications. World Wide Web, 23(5):2653–2669,\n2020.\n[Karevan and Suykens, 2020] Zahra Karevan and Johan AK\nSuykens. Transductive lstm for time-series prediction:\nAn application to weather forecasting. Neural Networks,\n125:1–9, 2020.\n[Karimireddy et al., 2020] Sai Praneeth Karimireddy,\nSatyen Kale, Mehryar Mohri, Sashank Reddi, Sebas-\ntian Stich, and Ananda Theertha Suresh. Scaffold:\nStochastic controlled averaging for federated learning. In\nInternational Conference on Machine Learning, pages\n5132–5143. PMLR, 2020.\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[Kipf and Welling, 2016] Thomas N Kipf and Max Welling.\nSemi-supervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907, 2016.\n[Kjellstrom et al., 2016] Tord Kjellstrom, David Briggs,\nChris Freyberg, Bruno Lemke, Matthias Otto, and Olivia\nHyatt. Heat, human performance, and occupational health:\na key issue for the assessment of global climate change im-\npacts. Annual review of public health, 37:97–112, 2016.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-\ntuning: Optimizing continuous prompts for generation.\narXiv preprint arXiv:2101.00190, 2021.\n[Li et al., 2019] Xiang Li, Kaixuan Huang, Wenhao Yang,\nShusen Wang, and Zhihua Zhang. On the convergence of\nfedavg on non-iid data. arXiv preprint arXiv:1907.02189,\n2019.\n[Li et al., 2020] Tian Li, Anit Kumar Sahu, Manzil Zaheer,\nMaziar Sanjabi, Ameet Talwalkar, and Virginia Smith.\nFederated optimization in heterogeneous networks. Pro-\nceedings of Machine Learning and Systems, 2:429–450,\n2020.\n[Li et al., 2023] Zhiwei Li, Guodong Long, and Tianyi Zhou.\nFederated recommendation with additive personalization.\narXiv preprint arXiv:2301.09109, 2023.\n[Liu et al., 2021] Xiao Liu, Yanan Zheng, Zhengxiao Du,\nMing Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021.\n[Long et al., 2020] Guodong Long, Yue Tan, Jing Jiang, and\nChengqi Zhang. Federated learning for open banking. In\nFederated Learning: Privacy and Incentive , pages 240–\n254. Springer, 2020.\n[Long et al., 2021] Guodong Long, Tao Shen, Yue Tan, Leah\nGerrard, Allison Clarke, and Jing Jiang. Federated learn-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3539\ning for privacy-preserving open innovation future on dig-\nital health. In Humanity Driven AI: Productivity, Well-\nbeing, Sustainability and Partnership, pages 113–133.\nSpringer, 2021.\n[Long et al., 2023] Guodong Long, Ming Xie, Tao Shen,\nTianyi Zhou, Xianzhi Wang, and Jing Jiang. Multi-center\nfederated learning: clients clustering for better personal-\nization. World Wide Web, 26(1):481–500, 2023.\n[Ma et al., 2022] Jie Ma, Guodong Long, Tianyi Zhou,\nJing Jiang, and Chengqi Zhang. On the conver-\ngence of clustered federated learning. arXiv preprint\narXiv:2202.06187, 2022.\n[McMahan et al., 2017] Brendan McMahan, Eider Moore,\nDaniel Ramage, Seth Hampson, and Blaise Aguera y Ar-\ncas. Communication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics, pages 1273–1282. PMLR, 2017.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris\nHallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack\nClark, et al. Learning transferable visual models from nat-\nural language supervision. In International Conference on\nMachine Learning, pages 8748–8763. PMLR, 2021.\n[Sahu et al., 2018] Anit Kumar Sahu, Tian Li, Maziar San-\njabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.\nOn the convergence of federated optimization in heteroge-\nneous networks. arXiv preprint arXiv:1812.06127, 3:3,\n2018.\n[Sapankevych and Sankar, 2009] Nicholas I Sapankevych\nand Ravi Sankar. Time series prediction using support vec-\ntor machines: a survey. IEEE computational intelligence\nmagazine, 4(2):24–38, 2009.\n[Shi et al., 2015] Xingjian Shi, Zhourong Chen, Hao Wang,\nDit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.\nConvolutional lstm network: A machine learning approach\nfor precipitation nowcasting. Advances in neural informa-\ntion processing systems, 28, 2015.\n[Tan et al., 2021] Yue Tan, Guodong Long, Lu Liu, Tianyi\nZhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fed-\nproto: Federated prototype learning over heterogeneous\ndevices. arXiv preprint arXiv:2105.00243, 3, 2021.\n[Tan et al., 2022a] Yue Tan, Yixin Liu, Guodong Long, Jing\nJiang, Qinghua Lu, and Chengqi Zhang. Federated learn-\ning on non-iid graphs via structural knowledge sharing.\narXiv preprint arXiv:2211.13009, 2022.\n[Tan et al., 2022b] Yue Tan, Guodong Long, Jie Ma, Lu Liu,\nTianyi Zhou, and Jing Jiang. Federated learning from pre-\ntrained models: A contrastive learning approach. arXiv\npreprint arXiv:2209.10083, 2022.\n[Veliˇckovi´c et al., 2017] Petar Veliˇckovi´c, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 2017.\n[V oyantet al., 2012] Cyril V oyant, Marc Muselli,\nChristophe Paoli, and Marie-Laure Nivet. Numeri-\ncal weather prediction (nwp) and hybrid arma/ann model\nto predict global radiation. Energy, 39(1):341–355, 2012.\n[Wang et al., 2022] Zhuowei Wang, Tianyi Zhou, Guodong\nLong, Bo Han, and Jing Jiang. Fednoil: a simple two-\nlevel sampling method for federated learning with noisy\nlabels. arXiv preprint arXiv:2205.10110, 2022.\n[Wu et al., 2021] Haixu Wu, Jiehui Xu, Jianmin Wang, and\nMingsheng Long. Autoformer: Decomposition transform-\ners with auto-correlation for long-term series forecast-\ning. Advances in Neural Information Processing Systems,\n34:22419–22430, 2021.\n[Xu et al., 2022] Yufei Xu, Jing Zhang, Qiming Zhang, and\nDacheng Tao. Vitpose+: Vision transformer foundation\nmodel for generic body pose estimation. arXiv preprint\narXiv:2212.04246, 2022.\n[Yates et al., 2021] Andrew Yates, Rodrigo Nogueira, and\nJimmy Lin. Pretrained transformers for text ranking: Bert\nand beyond. In Proceedings of the 14th ACM International\nConference on Web Search and Data Mining, pages 1154–\n1156, 2021.\n[Yu et al., 2017] Bing Yu, Haoteng Yin, and Zhanxing Zhu.\nSpatio-temporal graph convolutional networks: A deep\nlearning framework for traffic forecasting. arXiv preprint\narXiv:1709.04875, 2017.\n[Zerveas et al., 2021] George Zerveas, Srideepika Jayara-\nman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten\nEickhoff. A transformer-based framework for multivariate\ntime series representation learning. In Proceedings of the\n27th ACM SIGKDD Conference on Knowledge Discovery\n& Data Mining, pages 2114–2124, 2021.\n[Zhang et al., 2023] Chunxu Zhang, Guodong Long, Tianyi\nZhou, Peng Yan, Zijian Zhang, Chengqi Zhang, and\nBo Yang. Dual personalization on federated recommen-\ndation. arXiv preprint arXiv:2301.08143, 2023.\n[Zhou et al., 2021] Haoyi Zhou, Shanghang Zhang, Jieqi\nPeng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\nZhang. Informer: Beyond efficient transformer for long\nsequence time-series forecasting. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35,\npages 11106–11115, 2021.\n[Zhou et al., 2022a] Kaiyang Zhou, Jingkang Yang,\nChen Change Loy, and Ziwei Liu. Learning to prompt\nfor vision-language models. International Journal of\nComputer Vision, 130(9):2337–2348, 2022.\n[Zhou et al., 2022b] Tian Zhou, Ziqing Ma, Qingsong Wen,\nXue Wang, Liang Sun, and Rong Jin. Fedformer: Fre-\nquency enhanced decomposed transformer for long-term\nseries forecasting. arXiv preprint arXiv:2201.12740,\n2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3540",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6640303134918213
    },
    {
      "name": "Weather prediction",
      "score": 0.5659691095352173
    },
    {
      "name": "Weather forecasting",
      "score": 0.4513189494609833
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.4512152671813965
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4294775128364563
    },
    {
      "name": "Federated learning",
      "score": 0.423493891954422
    },
    {
      "name": "Machine learning",
      "score": 0.3733779191970825
    },
    {
      "name": "Data science",
      "score": 0.34779685735702515
    },
    {
      "name": "Meteorology",
      "score": 0.2778143882751465
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27755749225616455
    },
    {
      "name": "Geography",
      "score": 0.12937158346176147
    },
    {
      "name": "Cartography",
      "score": 0.06971168518066406
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    }
  ],
  "cited_by": 23
}