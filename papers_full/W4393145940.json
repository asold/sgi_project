{
  "title": "GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting",
  "url": "https://openalex.org/W4393145940",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2743559618",
      "name": "Furong Jia",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2095730837",
      "name": "Kevin Wang",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2292995061",
      "name": "Yixiang Zheng",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A3083293699",
      "name": "Defu Cao",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2100197731",
      "name": "Yan Liu",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2743559618",
      "name": "Furong Jia",
      "affiliations": [
        "Viterbo University",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2095730837",
      "name": "Kevin Wang",
      "affiliations": [
        "University of Southern California",
        "Viterbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2292995061",
      "name": "Yixiang Zheng",
      "affiliations": [
        "University of Southern California",
        "Viterbo University"
      ]
    },
    {
      "id": "https://openalex.org/A3083293699",
      "name": "Defu Cao",
      "affiliations": [
        "University of Southern California",
        "Viterbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2100197731",
      "name": "Yan Liu",
      "affiliations": [
        "Viterbo University",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6750513420",
    "https://openalex.org/W4323556435",
    "https://openalex.org/W3104302219",
    "https://openalex.org/W3120264269",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6753110569",
    "https://openalex.org/W2914951308",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W6789720248",
    "https://openalex.org/W4387355843",
    "https://openalex.org/W3033601430",
    "https://openalex.org/W3016053201",
    "https://openalex.org/W6851432013",
    "https://openalex.org/W6889955440",
    "https://openalex.org/W3045621803",
    "https://openalex.org/W4283794074",
    "https://openalex.org/W2906498146",
    "https://openalex.org/W6848841596",
    "https://openalex.org/W4310416691",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2607045400",
    "https://openalex.org/W6629167549",
    "https://openalex.org/W2990718568",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3173539742",
    "https://openalex.org/W3028192203",
    "https://openalex.org/W4282919422",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W4360831783",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W4287270621",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W4312956471",
    "https://openalex.org/W4318225439",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385965698",
    "https://openalex.org/W4286749450",
    "https://openalex.org/W3007066689",
    "https://openalex.org/W4234761883",
    "https://openalex.org/W4381571703",
    "https://openalex.org/W4321854470",
    "https://openalex.org/W2868496215",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W4306803525",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3123909522",
    "https://openalex.org/W4382239660",
    "https://openalex.org/W1490346241",
    "https://openalex.org/W3166396011"
  ],
  "abstract": "Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.",
  "full_text": "GPT4MTS: Prompt-Based Large Language Model for Multimodal Time-Series\nForecasting\nFurong Jia, Kevin Wang, Yixiang Zheng, Defu Cao, Yan Liu\nDepartment of Computer Science\nViterbi School of Engineering\nUniversity of Southern California\n{florajia, kwang442, yzheng01, defucao, yanliu.cs}@usc.edu\nAbstract\nTime series forecasting is an essential area of machine learn-\ning with a wide range of real-world applications. Most of the\nprevious forecasting models aim to capture dynamic charac-\nteristics from uni-modal numerical historical data. Although\nextra knowledge can boost the time series forecasting per-\nformance, it is hard to collect such information. In addition,\nhow to fuse the multimodal information is non-trivial. In this\npaper, we first propose a general principle of collecting the\ncorresponding textual information from different data sources\nwith the help of modern large language models (LLM). Then,\nwe propose a prompt-based LLM framework to utilize both\nthe numerical data and the textual information simultane-\nously, named GPT4MTS. In practice, we propose a GDELT-\nbased multimodal time series dataset for news impact fore-\ncasting, which provides a concise and well-structured ver-\nsion of time series dataset with textual information for further\nresearch in communication. Through extensive experiments,\nwe demonstrate the effectiveness of our proposed method on\nforecasting tasks with extra-textual information.\nIntroduction\nTime series data has consistently played a pivotal role in\ndiverse fields ranging from finance (Sezer, Gudelek, and\nOzbayoglu 2020) and economics (Kalamara et al. 2022) to\nhealthcare (Kaushik et al. 2020; Cao et al. 2023) and weather\nprediction (Nguyen et al. 2023), etc. The intricate patterns\nembedded within such data often reflect underlying mecha-\nnisms or behaviors, making time series forecasting an indis-\npensable tool for decision-making processes. Meanwhile, in\nour current information-dense era, the influence of textual\ninformation spans from individual decisions to shaping na-\ntional directives. However, due to insufficient data accumu-\nlation and limited resources, limited work has been done on\ndesigning multimodal time series datasets. With the emer-\ngence of Large-scale Language Models (LLMs), we are able\nto fill this gap by proposing an effective pipeline and a new\nparadigm of time series forecasting task as well as dataset as\nshown in Figure 1.\nConventional forecasting methods usually only focus on\nunimodal time series numerical information. Previous works\n(Wu et al. 2020; Jiang and Luo 2022; Cao et al. 2020)\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Difference between our datasets and conventional\ntime series dataset. Conventional datasets contain only nu-\nmerical information, while our dataset leverages textual in-\nformation as well.\nuse Graph Neural Networks (GNNs) for short-term predic-\ntions, transformer variants (Zhou et al. 2021; Wu et al. 2021;\nZhou et al. 2022; Nie et al. 2023) for long-term forecast-\ning. Recently, linear models (Zeng et al. 2023) also exhibit\nstrong performance for time series forecasting tasks. How-\never, these works contain an intrinsic limitation: they over-\nlook the rich contextual information provided by textual\ndata.\nWhile some recent works (Tian Zhou 2023; Sun et al.\n2023; Xue and Salim 2022) have tried to apply language\nmodels for time series tasks, these works either treat time\nseries data as text sequence inputs (Xue and Salim 2022),\nor align the time series input with the textual embedding of\nLLMs. Very few of these approaches utilized a multimodal\ninput containing both time series information and textual in-\nformation. METS (Li et al. 2023) is the only known work,\nbased on our knowledge, that uses a multimodality input\ncontaining both ECG time series data and clinical reports\ntexts. However, METS only focuses on the health-care do-\nmain and cannot be generalized to most of the other time\nseries data.\nIn light of these challenges, we first introduce an inno-\nvative pipeline in Figure 2 that leverages the power of large\nlanguage models (LLMs) to generate textual data along with\nthe time series data: textual information collection, summa-\nrization, re-rank, efficient summary based on re-rank sim-\nilarity. Note that, while not applicable to all domains, this\ngeneration pipeline can still be applied to many other fields\nthrough proper guidance to obtain extra-textual information,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23343\nsuch as financial forecasting and communication.\nIn this paper, based on the proposed pipeline, we created\na GDELT1-based multimodal time series forecasting dataset\nthat contains both time series numerical values and textual\nsummaries of events. GDELT database records global events\nand their associated media coverage, underpinning the pro-\nfound impact that news can have in guiding our lives. It\npropels the development of computational communication\nresearch (Hopp et al. 2019; Lock 2020) under multiple do-\nmains, including social unrest (Galla and Burke 2018), gov-\nernment policies (Schintler and Kulkarni 2014), and finan-\ncial market (Consoli, Pezzoli, and Tosetti 2020). The estab-\nlishment of our dataset can enhance the accessibility of dif-\nferent fields to multimodal time series datasets, as well as\nfoster further research in multimodal computational commu-\nnication analysis.\nTogether with the established dataset, we propose a\nprompt tuning-based LLM, GPT4MTS, for time series fore-\ncasting with multimodal input, which contrasts with conven-\ntional approaches that rely on direct data alignment. For the\nnumerical input, we first split the temporal input into differ-\nent patches with reversible instance normalization (Nie et al.\n2023) and then apply a linear layer to embed the patches into\na hidden space as the time series input embedding. For the\ncorresponding textual information, we propose to obtain the\ntextual embeddings through pre-trained language model and\ntreat them as trainable soft prompts prepended to the tem-\nporal input. In addition, we choose to freeze the attention\nlayer inside the LLM to speed up the training and inference\nstage. Experimental results demonstrate the effectiveness of\nthe proposed method and reveal a potential direction that\ncombining the extra information and pure time series data\ncan further enhance the forecasting performance.\nOur main contributions can be summarized as follows:\n• We propose a general pipeline to incorporate textual\ndata into time series datasets. In addition, we propose\nthe GDELT dataset following the proposed pipeline,\nwhich serves as a practical application of our innovative\npipeline and methodology.\n• Extensive experiments were used to illustrate the effec-\ntiveness of our model based on the multimodal time se-\nries dataset.\nRelated Works\nIn this section, we provide an overview of previous work and\ndiscuss differences between our method and related studies.\nLarge-Scale Language Model (LLM)\nThe introduction of Transformer architecture (Vaswani et al.\n2017) has revolutionized several areas, including Natural\nLanguage Processing (NLP) and Computer Vision (CV).\nCoupled with the rise in computational resources, sev-\neral Large-scale Language Models (LLMs) based on deep\nTransformer architecture have been proposed. Among them,\n1gdeltproject.org\nScrape first 10 articles for each EventRootType\nSummary each scraped article\nRe-rank based on the similarity to hypothetical article of each event type\nGather data from GDELT database\nGroup by Region\nGroup by EventRootType & Sum NumMentions NumArticles NumSources\nSummary over the first 5 related summary\nTime Series Data Collection\nTextual Data Summarization\nFigure 2: GDELT-based dataset data collection.\nBERT is pre-trained on a large corpus and fine-tuned for spe-\ncific NLP tasks (Devlin et al. 2019). T5 model adopts a uni-\nfied text-to-text framework, converting every NLP problem\ninto a text generation task (Raffel et al. 2020). GPT focuses\non the autoregression generation of text, making it suitable\nfor a variety of generative tasks (Radford et al. 2018, 2019;\nBrown et al. 2020). LLaMa models are trained on vast public\ndatasets (Touvron et al. 2023). While LLMs have achieved\nsubstantial success in NLP, their application in areas such as\ntime series forecasting remains underexplored.\nPrompting\nPrompting serves as a methodology for crafting queries or\ncommands that steer models toward generating specific, tar-\ngeted outputs. The CLIP model, for instance, uses textual\nprompts in the format of “a photo of a object” to perform\nimage classification (Radford et al. 2021). T5 incorporates\ntask descriptions into its text-to-text framework, effectively\nusing these as implicit prompts to guide various NLP tasks\n(Raffel et al. 2020). Prompting techniques have also been ex-\ntended to several tasks like object detection (Li et al. 2022),\nimage captioning (Zhang et al. 2022), etc. Distinctively, our\nwork leverages textual prompts to guide the model process-\ning numerical information of time series.\nTime Series Forecasting\nTime series forecasting holds significance in diverse fields,\nsuch as anomaly detection, weather forecasting, and treat-\nment effect modeling. Conventional methods, such as\nARIMA (Box and Jenkins 1968), use statistical models\nto fit time series. As deep learning emerged as a game-\nchanger, several neural architectures, such as CNN-based\n(Munir et al. 2018), RNN-based (Salinas et al. 2020),\nand Transformer-based (Liu et al. 2021; Wu et al. 2021;\nZhou et al. 2021, 2022), demonstrated superiority. While\nTransformer-based methods have shown impressive perfor-\nmance, there are studies that juxtapose them with embarrass-\ningly simple linear models, challenging their priority (Zeng\net al. 2023). In contrast to these approaches, our study uti-\nlizes a pre-trained LLM to address the challenges of time\nseries forecasting.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23344\nFigure 3: One example for textual information generation under our pipeline.\nFurthermore, the prevalent benchmark datasets, like\nWeather and ETT (Zhou et al. 2021), predominantly focus\non numeric series. Our introduction of the GDELT dataset\ndiverges from this trend. By incorporating textual content\ninto time series, we enrich the data, paving the way for\nLLMs to harness additional textual information when mak-\ning predictions based on time series data.\nLLM4TS\nSeveral studies have ventured into using LLMs for time se-\nries forecasting. For instance, one work interprets time se-\nries as patches and employs pre-trained models for fore-\ncasting (Tian Zhou 2023). Another research direction in-\nvolves aligning embeddings between time series and texts.\nThe TEST method adopts contrastive learning to align time\nseries embeddings with the original textual embeddings in\nLLMs (Sun et al. 2023). TimeLLM (Jin et al. 2023) adopts a\ndistinct approach through reprogramming that aligns the em-\nbedding space of time series data with that of textual data.\nOther works convert time series into textual data via prompts\n(Xue and Salim 2022), while some rely exclusively on tex-\ntual information (Yu et al. 2023). Our approach similarly\nharnesses a pre-trained LLM for time series forecasting but\ndifferentiates by using prompts to feed additional textual in-\nformation. Combined with the capabilities of LLMs and the\nintegration of data from both modalities, this approach boost\nthe performance of time series forecasting beyond using ei-\nther alone, emphasizing the importance of both modalities.\nMethods\nDataset Collection\nThis dataset is derived from the Global Database of Events,\nLanguage, and Tone (GDELT) database2, which is one of the\nlargest publicly available databases that monitor news media\nfrom around the world in over 100 languages. The GDELT\ndatabase covers various types of information primarily fo-\ncused on events, thereby offering a rich set of variables for\nunderstanding global societal trends.\nTime series data collection. The pipeline of our data col-\nlection process is shown in Figure 2. In our specific GDELT\ndataset, we focus on extracting key information related to\nthe top 10 popular event types (EventRootCode) and their\nrespective mentions and coverage in the news media. The\nrelationship between EventRootCode and the name of the\nevent can be looked up through Table 3. Specifically, we\nextract three key variables for forecasting: NumMentions\n,\nNumArticles, NumSources . These variables respectively\nrepresent the number of mentions, the number of related ar-\nticles, and the number of sources, which are all relevant to\nthe attention a particular event type receives within a given\ntime frame and geographical region. We divided our dataset\ninto 10 event root types, collecting data for 55 regions under\nthe US and national data for the US. Currently, the data we\nused for training and evaluation spans from 2022-08-17 to\n2023-07-31.\n2gdeltproject.org\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23345\nFigure 4: Overview of our proposed model.\nCorresponding textual data collection. For textual data\nsummarization, we first scrape 10 articles for each event\ntype under the given region and given date. Then a sum-\nmary of each scraped article is generated using T5 (Raffel\net al. 2020). A hypothetical article summary is generated\ngiven the particular event type and its explanations, serving\nas a template for possible summaries under this event type.\nSummaries are then re-ranked based on the similarity to the\nhypothetical article, aiming to keep the summaries that are\nmost relevant to the news under the event type. Finally, an\noverall summary is generated given the top 5 related article\nsummaries using OpenAI ChatGPT3.5 API 3.\nWe demonstrate one example of how textual information\nis collected for our dataset in Figure 3. While the first round\nof summary for each scraped article may contain redundant\ninformation (such as the last summary in our example) as\nthey are processed through T5, by utilizing similarity and re-\nranking, they are less likely to be considered as top-related\narticles to be included into our final summary. Due to a lim-\nited budget, for some regional information, we use T5 in-\nstead of OpenAI ChatGPT3.5 API for the final summary\nof regional data. This can lead to certain mal-summarized\ntexts due to the interference of useless information in the first\nround of summary. Therefore, to mitigate this issue, we re-\nplace these inferior final summaries with summaries of clean\nscraped titles.\n3https://platform.openai.com/docs/guides/gpt\nThe primary goal of this dataset is to help individuals, re-\nsearchers, and policymakers better predict and analyze the\nlevel of attention certain types of events are receiving and\nthe impact they have in a specific region. This can facilitate\nmore informed decision-making in both the public and pri-\nvate sectors as well as provide a concise and effective dataset\nfor computational communication researchers.\nThis dataset also verifies the effectiveness of our multi-\nmodal time series dataset generation pipeline, enhancing the\nfeasibility of establishing similar datasets for other domains.\nProblem Definition\nWe consider the following problem, given a collection of\nmultivariate time series samples and textual summaries with\nlook-back window L : ([x1, s1], . . . ,[xL, sL]) where xt at\ntime step t is a vector of dimension M and st at time step\nt is a textual summary, we would like to forecast T future\nvalues: (xL+1, . . . , xL+T ).\nPrevious time series models learn from unimodal infor-\nmation: they learn a function funi such that\n(xL+1, . . . , xL+T ) =funi([x1, . . . , xL]|θ), (1)\nwhere θ is the parameters of the model. On the contrary,\nour model leverages multimodal information: we learn a\nfunction fmulti such that\n(xL+1, . . . , xL+T ) =fmulti([x1, s1], . . . ,[xL, sL]|θ). (2)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23346\nEvent DLinear NLinear PatchTST Autoformer Informer Transformer FEDformer LLaMa GPT4TS Ours\n01 0.601 0.597 0.649 0.655 2.187 1.699 0.561 0.590 0.582 0.552\n02 0.197 0.196 0.209 0.208 0.354 0.275 0.193 0.194 0.197 0.189\n03 0.217 0.215 0.226 0.222 0.302 0.293 0.208 0.216 0.223 0.213\n04 1.436 1.430 1.542 1.700 7.558 6.518 1.483 1.405 1.382 1.342\n05 0.238 0.234 0.253 0.244 0.362 0.296 0.234 0.236 0.237 0.230\n07 0.127 0.125 0.127 0.138 0.180 0.321 0.135 0.125 0.129 0.121\n08 0.085 0.083 0.086 0.100 0.201 0.405 0.103 0.083 0.085 0.080\n11 0.179 0.177 0.188 0.182 0.213 0.271 0.173 0.177 0.177 0.165\n17 0.204 0.202 0.213 0.201 0.258 0.298 0.196 0.199 0.200 0.190\n19 0.423 0.421 0.435 0.391 0.439 0.449 0.396 0.411 0.403 0.391\nAverage 0.371 0.368 0.392 0.404 1.205 1.083 0.368 0.363 0.362 0.347\nTable 1: Group by Event, MSE. Bold indicates the best result, while Underlines indicates the second best.\nEvent DLinear NLinear PatchTST Autoformer Informer Transformer FEDformer LLaMa GPT4TS Ours\n01 0.566 0.564 0.601 0.615 1.207 1.028 0.562 0.551 0.546 0.544\n02 0.332 0.330 0.347 0.349 0.456 0.413 0.342 0.323 0.325 0.325\n03 0.353 0.349 0.366 0.366 0.424 0.434 0.358 0.348 0.353 0.352\n04 0.910 0.911 0.959 1.017 2.379 2.162 0.942 0.889 0.879 0.881\n05 0.370 0.365 0.388 0.379 0.445 0.412 0.378 0.364 0.363 0.367\n07 0.256 0.249 0.255 0.276 0.340 0.493 0.282 0.250 0.255 0.249\n08 0.213 0.207 0.214 0.240 0.387 0.585 0.255 0.206 0.210 0.206\n11 0.311 0.306 0.323 0.321 0.359 0.430 0.323 0.305 0.305 0.298\n17 0.344 0.340 0.357 0.348 0.398 0.452 0.349 0.337 0.337 0.331\n19 0.416 0.411 0.425 0.407 0.425 0.479 0.420 0.407 0.405 0.390\nAverage 0.407 0.403 0.424 0.432 0.682 0.689 0.421 0.398 0.398 0.394\nTable 2: Group by Event, MAE. Bold indicates the best result, while Underlines indicates the second best.\nProposed Method\nThe architecture of the model we employed is demonstrated\nin Figure 3. We utilize parameters from the pretrained GPT2\nmodel (Radford et al. 2019). In order to understand infor-\nmation from both modalities, we add extra prompt layers to\ntransform time series information and textual information to\nthe input dimension of the pretrained model.\nFrozen Pretrained Model Our model retains the posi-\ntional embedding layers and transformer blocks from the\npretrained GPT2 model, while we freeze the attention lay-\ners and the feed-forward layers and fine-tune the positional\nembeddings and layer normalization layers, following the\nstandard practice (Houlsby et al. 2019; Lu et al. 2022).\nInput Embedding In order to apply both textual input and\ntime series input to pretrained LLM, we need to prepare the\ntwo modalities through separate embedding layers.\nFor the textual information, we apply the BERT (Devlin\net al. 2019) embedding module as a feature extractor to ob-\ntain the representation for the summary texts over the look-\nback window.\nFor the time series information, following previous works\n(Tian Zhou 2023; Nie et al. 2023), we use the following op-\nerations to fit the time series input into the pre-trained GPT2\nmodel:\n• We apply Reversible Instance Normalization (Kim et al.\n2021) to mitigate the distribution shift of the time series\ndata over time, which performs normalization by extract-\ning mean and variance from input time series and adding\nthem back to the predicted output.\n• We then apply patching (Nie et al. 2023) by aggregating\nadjacent timestamps. This enables time series stamps to\ngather context, similar to the continual impact of news\nover a period of time.\n• Since channel-independence has demonstrated its ef-\nfectiveness through previous works (Nie et al. 2023;\nTian Zhou 2023), we treat each multivariate time series\nas multiple independent uni-variate series as well.\nOutput Layer Since the output from the frozen Pre-\ntrained Language Transformer contains hidden states of se-\nquence length (for textual inputs) + patch number (for time\nseries inputs), we apply a linear output layer, which takes\nthe hidden state corresponding to the time series as input\nand transform it into the desired prediction length. As the\nablation study in Table 5 and Table 4 show, this improves\nperformance, as utilizing only the hidden states of the time\nseries input can preserve a better representation of the nu-\nmerical forecasting targets.\nExperiments\nBaselines and Experimental Settings\nTo evaluate the model’s performance on our generated\ndataset, we choose four SOTA Transformer-based models,\nincluding FEDformer (Zhou et al. 2022), Autoformer (Wu\net al. 2021), Informer (Zhou et al. 2021), PatchTST (Nie\net al. 2023), and two Linear-based models, including DLin-\near and NLinear (Zeng et al. 2023) as our baselines. We also\ninclude two pre-trained large language models, LLaMA\n(Touvron et al. 2023), and GPT(Radford et al. 2019) as\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23347\nEvent Number Event Type Name\n01 Make Public Statement\n02 Appeal\n03 Express Intent to Cooperate\n04 Consult\n05 Engage in Diplomatic Cooperation\n07 Provide Aid\n08 Yield\n11 Disapprove\n17 Coerce\n19 Fight\nTable 3: Event number to event type Name table\nour baselines, with both adapted to time series forecasting\ntasks in a similar way as GPT4TS (Tian Zhou 2023). All\nof these models follow the same experimental setup with\na prediction length T = 7, and a look-back window size\nL = 15to forecast all three numerical time-series variables:\nNumMentions\n, NumArticles, NumSources. Under this set-\nting, by dividing data to train, validate, and test with a ratio\nof 7 : 2 : 1, we gathered 343,200 training examples, 98,976\nvalidation examples, and 41,904 testing examples.\nMain Results\nTable 1 and Table 2 show the multivariate forecasting re-\nsults. The corresponding event type name for each event\nnumber is in Table 3. Overall, our model achieves the best\naverage performance among the 10 event types for news\nand steadily exceeds the performance of other SOTA models\nin most event types. Quantitatively, while GPT4TS already\nachieves a SOTA performance, by using textual informa-\ntion as prompts to guide time series forecasting, our model\nachieves an overall 4.14% reduction on MSE and 1.0% re-\nduction on MAE.\nAblation Study\nWe study the effects of only taking hidden representations\nfrom the time series inputs to the output layers. By com-\nparing our methods with taking full hidden representations\n(from both textual inputs and time series inputs) in Table 4\nand Table 5, one can observe that the performance is pol-\nluted by the hidden representations of textual inputs, though\nit is still better than without textual information (GPT4TS).\nThis demonstrates the effectiveness of only taking the hid-\nden representations from time series inputs into our model.\nAnalysis\nThe improvement in performance of our model is under-\nstood as a collaborative result of numerical data, textual in-\nformation, and a prompt-based methodology for multimodal\nintegration:\n• Textual information provides contexts that numerical\ndata alone cannot capture. For instance, in our dataset,\nnews articles featuring prominent figures or with strong\nsentiments or characteristics can obtain a more eminent\nrepresentation and therefore influence the fluctuation or\ntrend in the numerical data.\nEvent Number GPT4TS Ours full selection\n01 0.582 0.552 0.575\n02 0.197 0.189 0.197\n03 0.223 0.213 0.224\n04 1.382 1.342 1.400\n05 0.237 0.230 0.239\n07 0.129 0.121 0.122\n08 0.085 0.080 0.084\n11 0.177 0.165 0.169\n17 0.200 0.190 0.196\n19 0.403 0.391 0.389\nAverage 0.362 0.347 0.360\nTable 4: Ablation studies for GPT4MTS models by MSE.\nTherefore, our model is able to outperform other uni-\nmodality models. As shown in the ablation study, even\nGPT4MTS with full selection can achieve a better MSE\nscore and a similar MAE score compared to GPT4TS,\nwhich already outperforms all other unimodal time series\nmodels. This demonstrates the benefits of having textual\ncontext for time series forecasting tasks.\n• Numerical data offers precise quantitative values, which\nare indispensable for forecasting tasks. It uncovers the\nstatistical patterns inherent in the dataset. Given that our\nforecasting targets are also represented numerically, this\ndata primarily steers the predictions of our model. As\nobserved from table 1 and table 2, when only utiliz-\ning numerical data, linear-based models (DLinear, NLin-\near) generally outperform transformer-based models (In-\nformer, Autoformer, Transformer). This indicates that the\ntransformer models may over-complicate the observed\nnumerical information in our dataset with their com-\nplex architecture. However, pre-trained LLM models,\n(LLaMA and GPT4TS) tend to achieve a better perfor-\nmance than all other models, which demonstrates the\neffectiveness of cross-modality adaptation of these pre-\ntrained Large Language models for time series forecast-\ning tasks.\n• The methodology of prompt-based integration for textual\nand numerical inputs is crucial. Comparative analyses\nbetween GPT4TS and our model highlight the efficacy\nof harnessing additional text representations as prompts.\nConcurrently, our ablation study demonstrates that these\nprompts should act as guiding factors, not as intrinsic\ncomponents of the output representation. This approach\nfacilitates the learning ability of LLMs in processing nu-\nmerical data without being overshadowed by textual in-\nformation, ensuring moderate assimilation of contextual\ninformation. Over-reliance on textual information, how-\never, as demonstrated through the ablation study, could\nharm the predicting performance.\nRelevance for Accessibility in Communication\nIn our current era of information abundance, accessibility\nto this wealth of data becomes crucial, especially for indi-\nviduals in underdeveloped regions, those with reading dis-\nabilities, and non-English speakers. Our research directly\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23348\nEvent Number GPT4TS Ours full selection\n1 0.546 0.544 0.557\n2 0.325 0.325 0.333\n3 0.353 0.352 0.359\n4 0.879 0.881 0.907\n5 0.363 0.367 0.373\n7 0.255 0.249 0.250\n8 0.210 0.206 0.210\n11 0.305 0.298 0.302\n17 0.337 0.331 0.335\n19 0.405 0.390 0.393\nAverage 0.398 0.394 0.402\nTable 5: Ablation studies for GPT4MTS models by MAE.\ncontributes to enhancing communication accessibility by\nproposing a multimodal time series forecasting pipeline and\ndataset, which synergizes numerical data with summarized\ntextual information. This integration enables individuals to\ncomprehend complex global events and news in a more ac-\ncessible and time-efficient manner.\nOur approach effectively addresses language and textual\nbarriers by providing summarized news and key informa-\ntion through numerical data. This enhances the global news\nreach, especially beneficial for non-English speakers and\nminority language users. It provides potential for translated\nsummaries through LLMs and utilizes numerical data to\nhighlight the impact and forecast of various events. This\nmethod not only broadens communication channels but also\nensures that individuals without access to traditional news\nsources can stay informed through oral communication or\ngeneral short summaries. It is particularly useful for those\nwho are illiterate, offering a way to understand world events\nthrough numerical insights on event impacts as well as the\nforecasting results for each kind of event.\nIn fields like finance, our pipeline and methodology can\nempower individuals with limited time encountering acces-\nsibility challenges by providing concise and comprehensi-\nble summaries of complex financial reports, aiding in in-\nformed decision-making. Meanwhile, the forecasting results\ncan provide insights for future trend of events, therefore,\nbeneficial for the financial decisions.\nAdditionally, our dataset and pipeline are invaluable for\ncomputational communication research. They facilitate a\ndeeper understanding of information dissemination and its\nimpact on different populations. This understanding is crit-\nical for developing strategies to enhance news accessibility\nand comprehension across diverse groups.\nThis streamlined approach promises to make information\nmore accessible and understandable, breaking down barriers\nand fostering a more inclusive information ecosystem.\nConclusion and Future Work\nThis paper introduces an effective pipeline for collect-\ning textual information for time series forecasting tasks: a\ndataset is collected under this pipeline, and a correspond-\ning multimodal time series forecasting model is proposed.\nThrough extensive experiments and analysis, we show that\nour dataset and pipeline are reliable and we prove the ca-\npability of improving prediction performance utilizing our\nmodel, GPT4MTS, based on multimodal inputs. This fur-\nther testifies to the benefits of the extra textual information\ncollected under our pipeline.\nWe provide a general pipeline for multimodal dataset gen-\neration and accordingly establish a dataset for the news do-\nmain. Our work also demonstrate the potential of both the\ndataset and our model as foundational elements for future\nmultimodal time series forecasting research. A crucial next\nstep in this field would be to further explore multimodal time\nseries forecasting, particularly leveraging the capabilities of\nLarge Language Models (LLMs). This direction promises\nsignificant advancements in the understanding and applica-\ntion of multimodal data in forecasting tasks.\nReferences\nBox, G. E.; and Jenkins, G. M. 1968. Some recent advances\nin forecasting and control. Journal of the Royal Statistical\nSociety. Series C (Applied Statistics), 17(2): 91–109.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCao, D.; Enouen, J.; Wang, Y .; Song, X.; Meng, C.; Niu,\nH.; and Liu, Y . 2023. Estimating Treatment Effects from Ir-\nregular Time Series Observations with Hidden Confounders.\nIn Williams, B.; Chen, Y .; and Neville, J., eds., Thirty-\nSeventh AAAI Conference on Artificial Intelligence, AAAI\n2023, Thirty-Fifth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2023, Thirteenth Symposium on\nEducational Advances in Artificial Intelligence, EAAI 2023,\nWashington, DC, USA, February 7-14, 2023, 6897–6905.\nAAAI Press.\nCao, D.; Wang, Y .; Duan, J.; Zhang, C.; Zhu, X.; Huang,\nC.; Tong, Y .; Xu, B.; Bai, J.; Tong, J.; and Zhang, Q. 2020.\nSpectral Temporal Graph Neural Network for Multivariate\nTime-series Forecasting. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nConsoli, S.; Pezzoli, L. T.; and Tosetti, E. 2020. Using the\nGDELT Dataset to Analyse the Italian Sovereign Bond Mar-\nket. In Machine Learning, Optimization, and Data Science:\n6th International Conference, LOD 2020, Siena, Italy, July\n19–23, 2020, Revised Selected Papers, Part I 6, 190–202.\nSpringer.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), 4171–4186. Association for\nComputational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23349\nGalla, D.; and Burke, J. 2018. Predicting social unrest using\nGDELT. In International conference on machine learning\nand data mining in pattern recognition, 103–116. Springer.\nHopp, F. R.; Schaffer, J.; Fisher, J. T.; and Weber, R. 2019.\niCoRe: The GDELT interface for the advancement of com-\nmunication research. Computational Communication Re-\nsearch, 1(1): 13–44.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and\nGelly, S. 2019. Parameter-efficient transfer learning for\nNLP. In International Conference on Machine Learning,\n2790–2799. PMLR.\nJiang, W.; and Luo, J. 2022. Graph neural network for traffic\nforecasting: A survey. Expert Systems with Applications,\n207: 117921.\nJin, M.; Wang, S.; Ma, L.; Chu, Z.; Zhang, J. Y .; Shi, X.;\nChen, P.; Liang, Y .; Li, Y .; Pan, S.; and Wen, Q. 2023. Time-\nLLM: Time Series Forecasting by Reprogramming Large\nLanguage Models. CoRR, abs/2310.01728.\nKalamara, E.; Turrell, A.; Redl, C.; Kapetanios, G.; and Ka-\npadia, S. 2022. Making text count: economic forecasting us-\ning newspaper text. Journal of Applied Econometrics, 37(5):\n896–919.\nKaushik, S.; Choudhury, A.; Sheron, P. K.; Dasgupta, N.;\nNatarajan, S.; Pickett, L. A.; and Dutt, V . 2020. AI in health-\ncare: time-series forecasting using statistical, neural, and en-\nsemble architectures. Frontiers in big data, 3: 4.\nKim, T.; Kim, J.; Tae, Y .; Park, C.; Choi, J.-H.; and Choo, J.\n2021. Reversible instance normalization for accurate time-\nseries forecasting against distribution shift. In International\nConference on Learning Representations.\nLi, J.; Liu, C.; Cheng, S.; Arcucci, R.; and Hong, S. 2023.\nFrozen Language Model Helps ECG Zero-Shot Learning.\nCoRR, abs/2303.12311.\nLi, L. H.; Zhang, P.; Zhang, H.; Yang, J.; Li, C.; Zhong, Y .;\nWang, L.; Yuan, L.; Zhang, L.; Hwang, J.-N.; et al. 2022.\nGrounded language-image pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 10965–10975.\nLiu, S.; Yu, H.; Liao, C.; Li, J.; Lin, W.; Liu, A. X.; and Dust-\ndar, S. 2021. Pyraformer: Low-complexity pyramidal atten-\ntion for long-range time series modeling and forecasting. In\nInternational conference on learning representations.\nLock, I. 2020. Debating glyphosate: A macro perspective\non the role of strategic communication in forming and mon-\nitoring a global issue arena using inductive topic modelling.\nInternational Journal of Strategic Communication, 14(4):\n223–245.\nLu, K.; Grover, A.; Abbeel, P.; and Mordatch, I. 2022.\nFrozen pretrained transformers as universal computation en-\ngines. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, 7628–7636.\nMunir, M.; Siddiqui, S. A.; Dengel, A.; and Ahmed, S.\n2018. DeepAnT: A deep learning approach for unsupervised\nanomaly detection in time series.Ieee Access, 7: 1991–2005.\nNguyen, T.; Brandstetter, J.; Kapoor, A.; Gupta, J. K.; and\nGrover, A. 2023. ClimaX: A foundation model for weather\nand climate. In Krause, A.; Brunskill, E.; Cho, K.; En-\ngelhardt, B.; Sabato, S.; and Scarlett, J., eds., International\nConference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, 25904–25938. PMLR.\nNie, Y .; Nguyen, N. H.; Sinthong, P.; and Kalagnanam, J.\n2023. A Time Series is Worth 64 Words: Long-term Fore-\ncasting with Transformers. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023. OpenReview.net.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nSalinas, D.; Flunkert, V .; Gasthaus, J.; and Januschowski, T.\n2020. DeepAR: Probabilistic forecasting with autoregres-\nsive recurrent networks. International Journal of Forecast-\ning, 36(3): 1181–1191.\nSchintler, L. A.; and Kulkarni, R. 2014. Big data for policy\nanalysis: The good, the bad, and the ugly. Review of Policy\nResearch, 31(4): 343–348.\nSezer, O. B.; Gudelek, M. U.; and Ozbayoglu, A. M. 2020.\nFinancial time series forecasting with deep learning: A sys-\ntematic literature review: 2005–2019. Applied soft comput-\ning, 90: 106181.\nSun, C.; Li, Y .; Li, H.; and Hong, S. 2023. TEST: Text Pro-\ntotype Aligned Embedding to Activate LLM’s Ability for\nTime Series. arXiv preprint arXiv:2308.08241.\nTian Zhou, X. W. L. S. R. J., Peisong Niu. 2023. One Fits\nAll: Power General Time Series Analysis by Pretrained LM.\nIn NeurIPS.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23350\nWu, H.; Xu, J.; Wang, J.; and Long, M. 2021. Autoformer:\nDecomposition transformers with auto-correlation for long-\nterm series forecasting. Advances in Neural Information\nProcessing Systems, 34: 22419–22430.\nWu, Z.; Pan, S.; Long, G.; Jiang, J.; Chang, X.; and Zhang,\nC. 2020. Connecting the dots: Multivariate time series fore-\ncasting with graph neural networks. In Proceedings of the\n26th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, 753–763.\nXue, H.; and Salim, F. D. 2022. Prompt-Based Time Se-\nries Forecasting: A New Task and Dataset. arXiv preprint\narXiv:2210.08964.\nYu, X.; Chen, Z.; Ling, Y .; Dong, S.; Liu, Z.; and Lu, Y .\n2023. Temporal Data Meets LLM–Explainable Financial\nTime Series Forecasting. arXiv preprint arXiv:2306.11025.\nZeng, A.; Chen, M.; Zhang, L.; and Xu, Q. 2023. Are trans-\nformers effective for time series forecasting? InProceedings\nof the AAAI conference on artificial intelligence, volume 37,\n11121–11128.\nZhang, H.; Zhang, P.; Hu, X.; Chen, Y .-C.; Li, L.; Dai,\nX.; Wang, L.; Yuan, L.; Hwang, J.-N.; and Gao, J. 2022.\nGlipv2: Unifying localization and vision-language under-\nstanding. Advances in Neural Information Processing Sys-\ntems, 35: 36067–36080.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2021. Informer: Beyond efficient transformer\nfor long sequence time-series forecasting. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 35,\n11106–11115.\nZhou, T.; Ma, Z.; Wen, Q.; Wang, X.; Sun, L.; and Jin, R.\n2022. Fedformer: Frequency enhanced decomposed trans-\nformer for long-term series forecasting. In International\nConference on Machine Learning, 27268–27286. PMLR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23351",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.814415693283081
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.5859264731407166
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.5602949261665344
    },
    {
      "name": "Time series",
      "score": 0.546345591545105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5170905590057373
    },
    {
      "name": "Machine learning",
      "score": 0.5123566389083862
    },
    {
      "name": "Language model",
      "score": 0.44123774766921997
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4135710597038269
    },
    {
      "name": "Data mining",
      "score": 0.3486782908439636
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}