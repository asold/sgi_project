{
    "title": "Gun identification from gunshot audios for secure public places using transformer learning",
    "url": "https://openalex.org/W4289521424",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2642727698",
            "name": "Rahul Nijhawan",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        },
        {
            "id": "https://openalex.org/A3177001352",
            "name": "Sharik Ali Ansari",
            "affiliations": [
                "California State University, Dominguez Hills"
            ]
        },
        {
            "id": "https://openalex.org/A1984592328",
            "name": "Sunil Kumar",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2526289846",
            "name": "Fawaz Alassery",
            "affiliations": [
                "Taif University"
            ]
        },
        {
            "id": "https://openalex.org/A4289592072",
            "name": "Sayed M. El-kenawy",
            "affiliations": [
                "Delta University for Science and Technology",
                "Higher Institute of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2642727698",
            "name": "Rahul Nijhawan",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        },
        {
            "id": "https://openalex.org/A3177001352",
            "name": "Sharik Ali Ansari",
            "affiliations": [
                "California State University, Dominguez Hills"
            ]
        },
        {
            "id": "https://openalex.org/A1984592328",
            "name": "Sunil Kumar",
            "affiliations": [
                "University of Petroleum and Energy Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2526289846",
            "name": "Fawaz Alassery",
            "affiliations": [
                "Taif University"
            ]
        },
        {
            "id": "https://openalex.org/A4289592072",
            "name": "Sayed M. El-kenawy",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2586214300",
        "https://openalex.org/W2466659027",
        "https://openalex.org/W2341764857",
        "https://openalex.org/W2792147589",
        "https://openalex.org/W2981543525",
        "https://openalex.org/W2672451823",
        "https://openalex.org/W4221042810",
        "https://openalex.org/W3210443934",
        "https://openalex.org/W2973147369",
        "https://openalex.org/W3048091370",
        "https://openalex.org/W2972922917",
        "https://openalex.org/W2104698762",
        "https://openalex.org/W2013482694",
        "https://openalex.org/W2107495852",
        "https://openalex.org/W1494733680",
        "https://openalex.org/W2079529251",
        "https://openalex.org/W2059652044",
        "https://openalex.org/W1972567154",
        "https://openalex.org/W2585464829",
        "https://openalex.org/W2561826558",
        "https://openalex.org/W3122866428",
        "https://openalex.org/W2767754137",
        "https://openalex.org/W2899290839",
        "https://openalex.org/W2883651221",
        "https://openalex.org/W2965995832",
        "https://openalex.org/W3032019894",
        "https://openalex.org/W3197301256",
        "https://openalex.org/W3004622726",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W1586885301",
        "https://openalex.org/W3121813932",
        "https://openalex.org/W3200945465",
        "https://openalex.org/W3001377083",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4312919733",
        "https://openalex.org/W6780527031",
        "https://openalex.org/W2038484192",
        "https://openalex.org/W3185838909",
        "https://openalex.org/W4206372017",
        "https://openalex.org/W3121667743",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3179869055",
        "https://openalex.org/W3104866538",
        "https://openalex.org/W4214513770",
        "https://openalex.org/W3152851547",
        "https://openalex.org/W4206993054"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports\nGun identification from gunshot \naudios for secure public places \nusing transformer learning\nRahul Nijhawan1, Sharik Ali Ansari2, Sunil Kumar1*, Fawaz Alassery3 & \nSayed M. El‑kenawy4,5\nIncreased mass shootings and terrorist activities severely impact society mentally and physically. \nDevelopment of real‑time and cost‑effective automated weapon detection systems increases a sense \nof safety in public. Most of the previously proposed methods were vision‑based. They visually analyze \nthe presence of a gun in a camera frame. This research focuses on gun‑type (rifle, handgun, none) \ndetection based on the audio of its shot. Mel‑frequency‑based audio features have been used. We \ncompared both convolution‑based and fully self ‑attention‑based (transformers) architectures. We \nfound transformer architecture generalizes better on audio features. Experimental results using the \nproposed transformer methodology on audio clips of gunshots show classification accuracy of 93.87%, \nwith training loss and validation loss of 0.2509 and 0.1991, respectively. Based on experiments, we are \nconvinced that our model can effectively be used as both a standalone system and in association with \nvisual gun‑detection systems for better security.\nMass shootings in public places like schools, clubs, and roads are becoming common and its rate is alarmingly \nincreasing in the  US1. Even the countries like Canada, Australia, France, UK have seen gun violence in pub-\nlic places. Addressing seriousness of problem, these countries have built unique organizations to tackle mass \n shootings2. According to Mother Jones magazine’s database, Fig. 1 shows us the public gun violence incidences \nacross the US between 1981 and 2021. Almost all major cities get covered under the red zone. In India, Naxalite \nand terrorism problems are very frequent. Clearly, there should be a mechanism to detect gun violence and act \nfast on it irrespective of gun laws. This paper takes a step in this direction to provide a real-time and cost-effective, \nreliable, and robust methodology for detecting gunshots. This will help people spend quality life with family, \nfriends, and colleagues and take immediate action if anything serious happens around them.\nGuns are dangerous, especially when operated by people who want to harm others. Mass shootings in schools \nand public places are becoming a common place. Terrorist attacks are also increasing and creating new security \nchallenges. Such incidents either kill or traumatize survivors for a lifetime, creating a sense of insecurity in soci-\nety. This highlighted the need for a system that increases security in such situations. Even at the military level, it \nwould help develop strategies to protect our troops at the workplace.\nThe computer vision community is contributing, wherever possible to protect the life of people in such \nuneventful situations. Automated camera-based surveillance systems are highly focused previously. These include \na system to detect cold weapons, and guns, differentiating handheld items from weapons that undoubtedly \nincreased such systems’ strength. The problem that these researchers focused on is detecting the weapon by \nvisually seeing it on camera, preventing the mishap, and alerting the authority.\nIf it fails, the attack will occur, and security forces will be called to confront the attackers. At such point, \nforces don’t know what kind of weapons the attackers are using. By knowing their weapon, we can secure and \ngive an edge to our security personnel. Most of the time, attackers fight from spots not covered by surveillance \n(including drones). At such a point, using the audio of the weapon is our only option to get information about \nthe attacker’s weapon. The camera has limited view of capture, audio’s view is not limited. The use of audio will \nimprove these systems by adding a small audio capture device.\nOPEN\n1School of Computer Science, University of Petroleum and Energy Studies, Dehradun, Uttarakhand, \nIndia. 2California State University Dominguez Hills, Carson, USA. 3Department of Computer Engineering, College \nof Computers and Information Technology, Taif University, P .O. Box 11099, Taif 21944, Saudi Arabia. 4Department \nof Communications and Electronics, Delta Higher Institute of Engineering and Technology, Mansoura 35111, \nEgypt. 5Faculty of Artificial Intelligence, Delta University for Science and Technology, Mansoura 35712, \nEgypt. *email: drskumar.cs@gmail.com\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\nPreviously, researchers widely used Convolution neural network (CNN) architectures to solve computer \nvision problems. Although promising, CNN-based architectures have some weaknesses. Firstly, as the convolu-\ntions work with constant window size, the model helps in finding the local information rather than long-range \nspatial relationships between different parts of the image and the complete  image3. Secondly, there is some loss \nof local information through max-pooling4. Inspired by Natural language processing (NLP), Vision transformers \nhave recently been proposed as an alternative to CNNs and have shown promising results in the field of com -\nputer  vision3. Vision transformer is free of convolutions and identifies an image as a sequence of patches, hence \novercoming the issues of locality and translation invariance faced by CNN. This approach has been observed to \nuse the hardware resources more efficiently than CNN and could be pre-trained on the public ImageNet dataset \nwith fewer  resources3.\nIn this paper, we have investigated whether we can classify different gun types from audios of their shots. \nThe hypothesis arises as experienced military personnel can accurately tell which gun the shot is fired from by \nlistening to audio. To test this, we created a dataset that consists of 1-s audio of shots. Transformers have proved \ntheir success from NLP to vision tasks. However, they were never used to classify audio. To replicate a human’s \nsense of understanding of audio, we used Mel-frequency to represent audio and developed a transformer-based \nclassifier to predict the class of gun.\nMajor Challenges faced while working in this area are as follows. firstly, creating such a dataset is difficult and \ncostly due to the legalities and risks involved. We created a dataset using audio from Y ouTube videos. Secondly, \nno such features were proposed earlier for attention-based approaches for gun audio classification. It is not \nknown whether we can use attention directly on shot audio or if there exists some feature that can use attention \nto give a better result.\nOur main contributions in this paper are the following:\n1. We manually created a 1-s gun audio shot dataset and made it available for public use.\n2.We optimized transformer hyperparameters, showing that the vision transformer has significantly improved \nthe gunshot detection accuracy compared to the other state-of-the-art algorithms.\n3. Our proposed approach has the advantage of showing sublime results on every type and aspect of the data \nthat led to obtain unparalleled results for the task of gunshot detection when compared with state-of-the-art \nalgorithms.\nThe rest of the paper is divided as: related work, the methodology that we used and dataset creation, results \nand discussion, conclusion, limitations and future scope.\nRelated work\nChoi et al. highlighted CCTV’s significance for better police  service5. They also studied CCTV-based intelligent \nsecurity systems for constructing crime-zero  zone6. In 2018, they further analyzed the feasibility of security \nsystems based on their economic  value7. Adding an audio-based gun-shot detection method will enhance the \nreach of such systems. With a nominal increase in cost, a completely new sensory ability increases its usefulness. \nLiang et al. 8 proposed a method that can detect the shooter’s position using only a few user-generated videos \nthat include the sound of gunshots. Liang et al. 9 in 2017 solved the issue by developing a temporal localization \nframework for intensive audio events in videos. The localization results are improved by the proposed method \nusing Localized Self-Paced Reranking (LSPaR).\nFigure 1.  Heatmaps shown in the map represent the data collected by Mother Jones. The data shows the \nmass-shootings from years 1981 to 2021. More red areas means, multiple mass-shootings in that area. Software: \nLocation History Visualizer, Source: https:// locat ionhi story visua lizer. com/ heatm ap/.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\nMorshed et al.10 proposed a robust neural network-based approach for audio classification. They employed an \nencoder that contains a sequential stack of neural networks. Further, they also used a temporal-based interpola-\ntion approach for performing the scene-level embeddings. Banoorupa et al. 11 proposed a hybrid fingerprinting \nbased approach for performing audio classification using LSTM. In this approach, the fingerprints were created \nemploying the MFCC spectrum and finally converting the spectrum obtained into digital images.\nPhan et al.12 proposed a multiclass audio classification solution for polyphonic audio event detection. They \ndivided the event categories into multiple sets and created a multi-class problem employing a divide and conquer \napproach. Wang et al.13 proposed a 2D CNN based technique for audio classification.This algorithm was widely \nused in various speech recognition and classification tasks. Zhang et al.14 propose an AED module called Multi-\nScale Time-Frequency Attention (MTFA) it informs the model where to focus along the time and frequency \naxes by collecting data at different resolutions, which has not been taken care in the past. Zhang et al. 15 and \nShen et al.16 proposed a a multiscale time-frequency convolutional recurrent neural network (MTF-CRNN) for \nsound event  detection15.\nDeep learning has evolved a lot over time. Earlier CNNs were being used and continue to work very well. \nRecently, Transformers were introduced, and state-of-the-art methods have proved their performance in both \nNLP and Vision tasks. In this paper, we are searching for the best way to classify guns based on their audio. We \nhave done many experiments for this task with CNNs and transformers.\nTraditional machine learning based researches. This subsection discusses the machine learning-\nbased audio classification approaches, and researcher’s contribution are represented in Table 1. Gunshot audio \ndepends on various aspects, i.e., (1) firing power (size of bullet), (2) length and width of the nozzle, and (3) envi-\nronment (echo). If firing power or length/width of gun nozzle changes, it reflects the weapon’s is changed. Vrysis \net al.42 compared 1D and 2D CNNs for audio classification using various features. 2D CNN with spectrogram \nworked best according to them. Nanni et al 39 used an ensemble of CNNs to classify animal sound using spec-\ntrogram and some handcrafted features. In our work, we have utilized Mel-frequency and spectrogram-based \nfeatures to identify the audio sound.\nTransformer as new state of the art. Transformers are first proposed by Vaswani et al. 43 for the text-\nclassification task of NLP . It is complete attention based, removed the recurrent nature completely; is faster to \ntrain, and offers better performance. Experimenting with pairwise and patch-wise self-attention, Zhao et al. 44 \nfound both outperform CNNs. Dosovitskiy et al.3 used transformers for image recognition. They used a simple \ntransformer as an encoder. Transformer architecture has proven success in different domains. Inspired by this \nsuccess we employed transformers to classify audio of gun shots.\nThe research based on gun identification has been done by Kiktova et al.45 which was an extension of an intel-\nligent acoustic event detection system. The work was based on extracting a variety of features where by using \nmutual information for the feature selection, the length of feature vectors were reduced. Thereafter, the Hidden \nMarkov Model and Viterbi-based decoding algorithm utilized those obtained features. In a recent publication of \n2021,  Dogan46 presented work on predicting the gun model by identifying sound and developed an intelligent \naudio forensics tool. Dogan has used the fractal H-tree pattern-based classification method, where fractal and \nstatistical features were utilized by SVM and kNN classifiers. Researchers have not explored the classification of \ngunshots; research has also been done on measuring and analysing the gunshot sound as they may cause hearing \n impairments47. In their study, acoustic data was collected from four different guns where sound was captured at \na sampling rate of 204.8 kHz. The method developed to measure gunshot was based on using image processing \nTable 1.  This table shows various techniques employed in previous relevant audio related researches along \nwith a research focus (purpose) and publish year. It provides a general outline of previous literature. More \nrelevant previous works are delineated in the literature review section with important detail.\nTechniques Purpose Ye a r Accuracy (%)\nNeural network, SVM, KNN, decision  tree17–20 Audio classification 2003–2007 60.0–80.4\nMulti layer  perceptron21 Audio classification 2008 70.1\nOne-class  SVM22 Audio classification 2009 76.3\nNeural  network23 Feature extraction 2010 80.0\nDeep neural  network24 Audio classification 2013 85.2–8\nCNN, hierarchical neural  network25–27 Audio Classification 2014, 2015 86.1\nLSTM,  RNN28 Audio classification 2016 88.2–89.3\nCNN29–31 Audio tagging, deep feature extraction 2017 90.3–91.5\nDeep unsupervised learning, Unsupervised learning, weakly supervised learning, \nattention  network32–35 Audio event detection, audio representation, audio classification 2018 89.0–92.0\nFew-shot attention, graph neural network, adversarial feature, capsule  network36–38 Audio classification 2019 89.2–91.5\nAttention-based networks, DNN  ensemble39,40 Audio classification 2020 90.2-91.8\nAttention-based networks, zero-shot federated  learning41 Audio classification 2021 91.0-92.5\nProposed approach Audio classification 2022 93.8\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\ntechniques where Short Time Fourier Transform (STFT), was used to get the spectrogram of an audio signal. \nOnce the spectrogram was generated, then kNN and random subspaces were used to classify them. The study \nfound that STFT gives better accuracy than CWT. Mares and Blackburn have focused on reducing gun violence \nby providing an evaluation for St. Louis’ Acoustic Gunshot Detection System’s (AGDS) 48. They have done a \nquasi-experimental longitudinal panel study by conducting various experiments over time. The experimental \nstudy shows a substantial increase (80%) in gunshot responses by the system. Thus, from the overall study and \nliterature survey, it has been observed that gunshot detection plays an important role in the forensic, health sec-\ntor, and security, which motivated us to work in this area.\nMethodology\nProposed approach. In this section, the methodology which has been used to classify gunshots is dis-\ncussed in detail. The proposed work approach has not been used earlier to classify audios and is thus considered \nto be a novelty. The approach used to carry out the work follows some steps, which are given below:\n• Loading the audio samples To load the .mp 3 shot samples we have used the Librosa library. This Library \nproduces a 21,624 or 22,200 lengths Numpy array for a one second sample array. To make its lengths equal to \nnearest whole square (22,500), as deep learning algorithms work on such lengths, we padded it with another \narray containing ‘− 1’ .\n• Extracting features from the audio: Mel-frequency Cepstral Coefficients (MFCCs): Cepstral helps us to \nunderstand periodic structures in frequency which gives us information related to echos. In MFC, frequency \nbands are equally spaced on Mel-scale which maps audio better. MFCCs are coefficients of MFC. To obtain \nMFCC sequence, we have used sampling rate of 22,500 Hz, the number of output MFCC features is set to \n44, length of the FFT window is 2048 samples, the number of samples between successive frames is 512, and \nthe type2 discrete cosine transform.\n• Mel-spectogram: A spectrogram is a visual representation of changes in frequencies of a signal with respect \nto time. To the human ear, the frequencies 600 Hz and 1000 Hz may sound different, but 7600 Hz and 8000 \nHz sound similar. Due to non-linear transformations of the frequency scale, the pitches (frequencies) that \nsound less distant appear less distant on Mel-scale. In Mel-spectrogram, y -axis is Mel-scale, and x -axis \nis time. The Mel-spectrogram is calculated by splitting the audio signal into the windows of length 2048 \nsamples and hop length of 512. Then, applying a fast Fourier transform to each window and separating the \ngiven audio spectrum into evenly spaced frequencies; 128 such Mel bins are taken for this purpose. Finally, \nfor each window, based on Mel-scale frequencies, breaking down the magnitude of the audio signal into its \ncomponents.\n• Vision transformer For the image classification task, a variation is made to the traditional transformer \narchitecture used in NLP . In our approach, the input part is processed by the encoder and the output from \nthe encoder is fed to a Multilayer perceptron after flattening it. No decoder is used, as shown in Fig.  2. The \napproach treats each patch from the image like text.\n  The transformer takes a linear vector with positional embedding as input. Therefore, first, 2D patches from \nthe image are flattened to linear vector. Then, the positional embedding and class token are added to it. As \nshown in Fig. 3, the encoder part has many encoder blocks. Each encoder block has multiple layers of multi-\nheaded self-attention mechanisms. The output from the encoder is normalized and sent to dense layers for \nimage classification as shown in Fig. 3. The model is inspired from Dosovitskiy et al.3.\n  Pre-trained model have proven their success in many researches. After identifying diminishing gradient \nproblem, researchers have proposed using residual connections in Resnet50  paper49. In our experiments, \nwhile training couple of times, we found that even the Resnet50 model quickly overfits. We tried using Drop-\nout with high value, still problem of very high difference in training and validation loss exists. As shown  by50, \ntransformers are better at handling such situations.\nFigure 2.  The figure shows the division of an image into patches of size 32 × 32 . The outputs from the linear \nprojection layer is combined with positional embedding and a learnable class embedding for classification. The \nabove diagram of the transformer encoder was derived from the work of Vaswani et al.43.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\n  Among various available architectures of vision transformers, we have used the L32 model for this research. \nThe input to the transformer is a 3-channel (RGB) image of size 224 × 224 . The image is divided into patches \nof shape 32 × 32 . Each patch is given as input to the linear transformation layer, which changes into a fixed-\nlength vector. Vector of each patch has given its position information by adding a position embedding. As our \nproblem is boiled down to image classification, a class descriptor token is also added to the patches. We pass \nthe combined vector to the transformer encoder block. In the L32 model, we have 24 self-attention encoder \nblocks through which input passes for feature extraction. As our problem needs, we added two dense layers \nwith 400 and 100 nodes, respectively. Both layers use ReLU activation. The dense layers are followed by the \nsoftmax layer, which classifies input into three classes: handgun, rifle, and none. The L32 model we choose is \npre-trained on ImageNet dataset. We fine tune the model keeping each layer trainable.The Adam optimizer \nis used while fine-tuning with learning rate schedule of 1 × 10−3 using a warmup phase followed by a linear \ndecay, and categorical cross-entropy is used as the loss function while training.\nIn this paper, we have used the concept of multi-head attention instead of single head attention by using dmodel \ndimentional keys, values and queries as used by Vaswani et al.43.\nExperimental setup\nDataset collection and pre‑processing. The dataset which has been used to carry out work contains \nshort Gun Shot audio clips of 1 s. First, 200 videos containing multiple gunshots, were collected from Y ouTube. \nAmong these 200 videos, 101 videos are of “rifle” shots, and 99 videos have “handgun” shots. The gun-shot is \nusually of less than 1 s, which is randomly taken audio from before and after the shot act.\nTo segment the 1-s shot audio, manual annotation was corrected up to a millisecond by using a video player. \nThereafter, ffmpeg tool was used to crop annotated audio segments from videos. We kept one to several shots from \neach video, given they are different. In each audio clip, the background noise padding is at a different position.\nTo extract audio from random noise or audio containing no gun-shot, the annotations given to extract \ngunshot audios have been used. We have simply left the audio where the gunshots were present as these were \nalready marked by us manually to create the dataset for the other two classes. A total of 322 audio images with \nnoise was obtained.\nFinally, all the audios were manually verified, and any unclassified audio was removed. Each audio for rifle \nand handgun class contains only one gunshot, which is either handgun or rifle. The dataset contains a total of \n1661 audios comprising 649 handgun sample shots, 693 rifle sample shots, and 322 none or random noise in \n.mp3 format.\nEach audio file for every class is split into set length frames in order to provide the network with sufficient \nand relevant data. We divided the original audio files into two categories as a first step: training samples (which \nconstituted 70% of the original data) and test samples (which constituted 30% of the original data). This is done \nto prevent the network from overfitting and producing inaccurate results when tested on data that was previously \nused to train the network. With K = 5, we performed a K-fold cross-validation to effectively test the proposed \nnetwork, as shown in Table 2.\nComparison with other datasets and system configuration. We test our method on two additional \navailable datasets to confirm the efficacy of the proposed method. (1) TRECVID Gunshot Videos (TREC): we \nFigure 3.  The diagram shows VIT-32 Model Architecture. It contains 24 transformer encoder blocks. The \nencoder block is shown in Fig. 2. in details. The arrows show the forward propagation.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\nhave 57 videos of gunshots from the TRECVID SIN  task51. (3) UrbanSound Gunshot Videos (Urban): from the \nUrbanSound  dataset52, we have extracted 117 audio files that include gunshots. We run experiments on these \ntwo datasets, and Table 4 reports the comparison on testing accuracy for each dataset.\nAll the experiments were conducted inside google collaboratory. The system provided in google collabora -\ntory has Nvidia k80 GPU with 12 GB of VRAM. The system has an Intel Xeon processor with two cores, 12 GB \nof RAM, and 25 GB of disk space. All the implementations were done in python 3.\nResults and discussion\nThis section describe the results. Table  1, shows audio classification algorithms employed in the past, with the \ncomparison of their performance on a yearly basis. Further, we compared our proposed approach with algorithms \ndeveloped by other authors, as shown in Table  4. Also, in order to check the generalizability of our proposed \napproach, we compared the results with two available datasets, (1) TRECVID Gunshot Videos (TREC), and (2) \nUrbanSound Gunshot Videos (Urban). Our proposed approach outperformed the state-of-the-art methods in \nall three datasets, as seen in Table 4. It is to be noted that our proposed approach produced testing accuracy of \n89.0–90.0%. We could see that zero-shot federated learning produced accuracy within 83.5–86.0% (Table  4). \nFurther, DNN Ensemble model produced accuracy of 83.0–84.5%, and capsule network produced the audio \nclassification accuracy between 82.2 and 83.6% as shown in Table 4.\nSo far,  CNNs53–55 have dominated audio classification tasks. CNN’s work is based on where it extracts sig-\nnificant features and edges by applying filters to a section of the  data56. This allows the model to learn only the \nmost important elements from the data rather than the fine details. Moreover, our proposed model works on the \nprinciple where the complete audio data is put into it, rather than only the sections that the filters can extract (or \nfind relevant). This serves as a reason why our proposed approach outperforms the state-of-the-art approaches.\nWe have tried using raw audio signals directly to train Resnet50 as a baseline. When resnet50 is fine-tuned on \nthe raw audio signal, the model overfits quickly. Training accuracy at the 50th epoch is 99.47%, while validation \naccuracy remained just 77.78%. On lower epochs, the validation accuracy is far poor. The training and validation \nloss were 0.0471 and 1.488. We found many variation in training and validation loss and classification accuracy. \nThen MFCC and Mel-Spectrogram features were also tested both individually and combined. When these are \ncombined, we found that better classification accuracy is obtained. So we continued with the combined feature \nas our input.\nWe fine-tuned Resnet50 on MFCC and Mel-spectrogram features. As shown in Fig.  4, the Resnet50 model \nstill has a lot of variations compared to the Vision transformer. The training accuracy and time for this is 98.88% \nand 18 h, respectively. The maximum validation classification was obtained to be 93.87% for both Resnet50 and \nTransformer (Table 3). However, for this accuracy vision transformer has a training loss of 0.2509 and valida -\ntion loss of 0.1991. On the other side, Resnet50 has a lot of variation in Training loss 4.0 × 10 −4 to 0.04 and in \nValidation loss of 0.2768–1.538.\nThe classification accuracy of vision transformer on testing dataset ranges between 89.0 and 90.0% in differ-\nent training testing experiments (Table 4). Comparatively, the accuracy of Resnet50 ranges from 84.0 to 87.0% \n(Table 4). We split the dataset into training and testing (the terms validation and testing are interchangeably \nused). Our dataset size limits us to divide available audio into training testing and validation. While training, we \nused training data and validation data and trained the model for a fixed number of epochs. For the transformer, \nthe best model is obtained at about 100 epochs while fine-tuning. While for resnet50 above approximately 50 \nepoch model start to overfit quickly.\nWe trained and validated models multiple times. We performed 5-fold cross-validation as shown in Table 2.\nInterestingly, the Vision Transformer, which is reputed for quick overfitting behavior, did not overfit when \nthe MFCC+MelSpec feature in the form of an image was passed. But it overfits when raw audio is given as input. \nResnet50 worked well with raw audio but overfitted when MFCC+MelSpec feature as images are passed.\nWhile training on both raw audio and features, we observed that Resnet50 and VT created their features. \nConsidering the recording devices, the environment (echo) was different, and background noise was present.\nVision transformer verses CNN. Vision transformers have shown remarkable performance in several \ncomputer vision-based tasks. These architectures work on multi-head self-attention mechanisms that can accept \na sequence of image patches to encode contextual cues.\nWe are intrigued by the fundamental differences in the operation of convolution and self-attention that have \nnot been extensively explored in the context of robustness and generalization. It is known that convolutions \nlearn local interactions between elements in the input domain. In contrast, self-attention has shown to learn \nglobal exchanges effectively, for example, relationships between far-off  objects57,58. Given a query embedding, \nself-attention finds interactions with the other embeddings in the sequence, thereby conditioning the local \nTable 2.  5-Fold cross validation audio classification results in percentage.\nClass Accuracy Recall Precision F1 score\nRifle 91.52 89.44 89.01 88.40\nHandgun 90.85 89.76 88.65 89.32\nRandom noise 91.21 90.12 88.43 90.41\nAll classes 90.01 90.34 89.87 90.34\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\ncontent while modeling global  relationships59. In contrast, convolutions are content-independent as the exact \nfilter weights are applied to all inputs regardless of their distinct nature. In this paper, our analysis illustrates that \nVision transformer can adjust their receptive field in order to work with the noises in the data and improve the \nexpressivity of the representations.\nFigure 4.  Graph (A) shows the training and validation accuracy curve for Resnet50 on our dataset. Graph (B) \nshows the training and validation loss curve for Resnet50 on our dataset. Graph (C) training and validation \naccuracy curve for VIT-32 on our dataset. Graph (D) shows the training and validation loss curve for VIT-32 on \nour dataset. In all the graphs, the x-axis shows the number of epochs.\nTable 3.  Resnet vs ViT top training accuracy and loss range in nearby epochs.\nRifle vs Handgun shot audio dataset Top accuracy (%) Loss range in nearby epochs\nResnet50+MFCC+MelSpectogram 93.87 0.0004–0.0400\nVIT-32+MFCC+MelSpectogram 93.87 0.2768–1.538\nTable 4.  Comparison of our proposed approach with state of the art algorithms (testing accuracy range), on \ndifferent available datasets.\nModel Our dataset (%) TREC (%) Urban (%)\nResnet50 (Baseline: raw audio) 76.0–78.0 71.0–73.0 70.0–73.4\nCapsule  network37 82.2–83.6 83.1–84.3 80.1–81.9\nDNN  ensemble40 83.0–84.5 82.2–83.4 84.0–85.0\nZero-shot federated  learning41 83.5–86.0 82.9–24.8 83.0–85.0\nResnet50+MFCC+MelSpectogram 84.0–87.0 83.0–84.5 82.9–85.0\n(PA) VT+MFCC+MelSpectogram 89.0–90.0 88.0–89.5 87.4–89.0\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\nConclusion\nThis paper examined the vision transformer-based approach for audio-based gun-type identification tasks. \nVarious features like MFCC and MelSpectogram were experimented with as previous research suggested. Vision \nTransformer was found to work better in terms of closeness of training and validation loss, thus giving us a bet-\nter fitting model. Results indicate that though only a shallow gun audio classification is done in this paper, these \ntechniques can be employed to classify various handguns and rifles based on their shot audio.\nCollecting the dataset for such a project is very difficult. It has both legal and financial issues. However, such \nprojects are necessary.\nOur dataset, though, still captured audio of gunshots in different environments; the plausible audio filters \nused in videos would have disturbed the original audio signal. Due to such disturbance, some critical information \nis missing in the audio. We felt it and therefore limited the research only to classify gun types. Had the audios \nbeen recorded using the same device with no audio filter and in various environments, we could have classified \ndifferent handguns and various rifles based on shot audios. Some attackers use audio suppressors. This audio \nis also classifiable.\nTo increase the audio-based gun identification task’s, the first step will be to collect raw gun audio shots. For \neach gun among various types of handguns and rifles, with and without suppressors, multiple shots in different \nenvironments must be collected. As mentioned in the limitations above, this step requires the support of legal \nauthorities and monitory support.\nAfter dataset collection, we can train a model that will classify different types of guns based on audio of shots. \nLike CCTV cameras, we will attach an audio input device with CCTVs, and any gunshot will be recognized. In \nsuch a way, we can attend to such situations quickly, bypassing human intervention, which usually delays the \nresponse and causes damage to intensify.\nReceived: 7 December 2021; Accepted: 26 July 2022\nReferences\n 1. Schildkraut, J., Elsass, H. J. & Meredith, K. Mass shootings and the media: Why all events are not created equal. J. Crime Justice  \n41(3), 223–243 (2018).\n 2. Chalk, P . Domestic counter-terrorist intelligence structures in the United Kingdom, France, Canada and Australia. Stud. Conflict \nTerrorism 20, 1–33 (2020).\n 3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G. & \nGelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv: 2010. 11929 (arXiv preprint) \n(2020).\n 4. Sabour, S., Frosst, N. & Hinton, G.E. Dynamic routing between capsules. arXiv: 1710. 09829 (arXiv preprint) (2017).\n 5. Y oo, J. S., Min, K. J., Jeong, S. H. & Shin, D. B. Inter-ministerial collaboration to utilize CCTV video service operated by u-city \ncenter of South Korea. Spat. Inf. Res. 24(4), 389–400 (2016).\n 6. Choi, W .-C. & Na, J.-Y . Relative importance for security systems of crime-zero zone based on spatial information. Spat. Inf. Res. \n24(1), 4 (2016).\n 7. Choi, W . C. & Na, J. Y . Evaluating economic values of intelligent security services based on spatial information in South Korea. \nSpat. Inf. Res. 26(4), 347–356 (2018).\n 8. Liang, J., Aronson, J. D. & Hauptmann, A.: Shooter localization using social media videos. In Proceedings of the 27th ACM Inter-\nnational Conference on Multimedia, 2280–2283 (2019).\n 9. Liang, J., Jiang, L. & Hauptmann, A.: Temporal localization of audio events for conflict monitoring in social media. In 2017 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 1597–1601 (IEEE, 2017).\n 10. Morshed, M.M., Ahsan, A.O., Mahmud, H. Hasan, M., et al.: Learning audio representations with mlps. arXiv: 2203. 08490 (arXiv \npreprint) (2022).\n 11. Banuroopa, K. & Shanmuga Priyaa, D. Mfcc based hybrid fingerprinting method for audio classification through lstm. Int. J. \nNonlinear Anal. Appl. 12(Special Issue), 2125–2136 (2022).\n 12. Phan, H., Nguyen, T. N. T., Koch, P . & Mertins, A. Polyphonic audio event detection: Multi-label or multi-class multi-task clas-\nsification problem?. arXiv: 2201. 12557 (arXiv preprint) (2022).\n 13. Wang, X. et al. Rainfall observation using surveillance audio. Appl. Acoust. 186, 108478 (2022).\n 14. Zhang, J., Ding, W ., Kang, J. & He, L.: Multi-scale time-frequency attention for acoustic event detection. arXiv: 1904. 00063 (arXiv \npreprint) (2019).\n 15. Zhang, K., Cai, Y ., Ren, Y ., Y e, R. & He, L. MTF-CRNN: Multiscale time-frequency convolutional recurrent neural network for \nsound event detection. IEEE Access 8, 147337–147348 (2020).\n 16. Shen, Y .-H., He, K.-X & Zhang, W .-Q.: Learning how to listen: A temporal-frequential attention model for sound event detection. \narXiv: 1810. 11939 (arXiv preprint) (2018).\n 17. Shao, X., Xu, C. & Kankanhalli, M. S. Applying neural network on the content-based audio classification. In Fourth International \nConference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. \nProceedings of the 2003 Joint, vol. 3, 1821–1825 (IEEE, 2003).\n 18. Mitra, V . & Wang, C. J. A neural network based audio content classification. In 2007 International Joint Conference on Neural \nNetworks, 1494–1499 (IEEE, 2007).\n 19. Chen, L., Gunduz, S. & Ozsu, M. T. Mixed type audio classification with support vector machine. In 2006 IEEE International \nConference on Multimedia and Expo, 781–784 (IEEE, 2006).\n 20. Zhu, Y ., Ming, Z. & Huang, Q. Svm-based audio classification for content-based multimedia retrieval. In International Workshop \non Multimedia Content Analysis and Mining, 474–482 (Springer, 2007).\n 21. Mitra, Vikramjit & Wang, Chia-Jiu. Content based audio classification: A neural network approach. Soft Comput. 12(7), 639–646. \nhttps:// doi. org/ 10. 1007/ s00500- 007- 0241-4 (2008).\n 22. Jingbin, Y ., Shi, W . & Kheidorov, I. Audio classification based on one-class svm. J. Comput. Appl. 29(5), 1419–1422 (2009).\n 23. Li, T. L., Chan, A. B. & Chun, A. Automatic musical pattern feature extraction using convolutional neural network. Genre 10(2010), \n1–1 (2010).\n 24. Kons, Z., Toledo-Ronen, O. & Carmel, M. Audio event classification using deep neural networks. Interspeech 20, 1482–1486 (2013).\n 25. Dieleman, S. & Schrauwen, B. End-to-end learning for music audio. In 2014 IEEE International Conference on Acoustics, Speech \nand Signal Processing (ICASSP), 6964–6968 (IEEE, 2014).\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\n 26. Ravanelli, M., Elizalde, B., Ni, K. & Friedland, G. Audio concept classification with hierarchical deep neural networks. In 2014 \n22nd European Signal Processing Conference (EUSIPCO), 606–610 (IEEE, 2014).\n 27. Piczak, K. J.: Environmental sound classification with convolutional neural networks. In 2015 IEEE 25th International Workshop \non Machine Learning for Signal Processing (MLSP), 1–6 (IEEE, 2015).\n 28. Dai, J., Liang, S., Xue, W ., Ni, C. & Liu, W . Long short-term memory recurrent neural network based segment features for music \ngenre classification. In 2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP), 1–5 (IEEE, 2016).\n 29. Freitag, M., Amiriparian, S., Pugachevskiy, S., Cummins, N. & Schuller, B. audeep: Unsupervised learning of representations from \naudio with deep recurrent neural networks. J. Mach. Learn. Res. 18(1), 6340–6344 (2017).\n 30. Xu, Y . et al. Unsupervised feature learning based on deep models for environmental audio tagging. IEEE/ACM Trans. Audio Speech \nLang. Process. 25(6), 1230–1241 (2017).\n 31. Oramas, S., Nieto, O., Barbieri, F . & Serra, X. Multi-label music genre classification from audio, text, and images using deep features. \narXiv: 1707. 04916 (arXiv preprint) (2017).\n 32. Morfi, V . & Stowell, D. Data-efficient weakly supervised learning for low-resource audio event detection using deep learning. arXiv: \n1807. 06972 (arXiv preprint) (2018).\n 33. Jansen, A., Plakal, M., Pandya, R., Ellis, D. P ., Hershey, S., Liu, J., Moore, R. C. & Saurous, R. A. Unsupervised learning of semantic \naudio representations. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 126–130 (IEEE, \n2018).\n 34. Amiriparian, S., Schmitt, M., Cummins, N., Qian, K., Dong, F . & Schuller, B. Deep unsupervised representation learning for \nabnormal heart sound classification. In 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology \nSociety (EMBC), 4776–4779 (IEEE, 2018).\n 35. Wu, Y ., Mao, H. & Yi, Z. Audio classification using attention-augmented convolutional neural network. Knowl.-Based Syst. 161, \n90–100 (2018).\n 36. Zhang, S., Qin, Y ., Sun, K. & Lin, Y . Few-shot audio classification with attentional graph neural networks. Interspeech 20, 3649–3653 \n(2019).\n 37. Jain, R.: Improving performance and inference on audio classification tasks using capsule networks. arXiv: 1902. 05069 (arXiv \npreprint) (2019).\n 38. Gao, L. et al. An adversarial feature distillation method for audio classification. IEEE Access 7, 105319–105330 (2019).\n 39. Nanni, L. et al. Ensemble of convolutional neural networks to improve animal audio classification. EURASIP J Audio Speech Music \nProcess. 2020, 1–14 (2020).\n 40. Lu, H., Zhang, H. & Nayak, A. A deep neural network for audio classification with a classifier attention mechanism. arXiv: 2006.  \n09815 (arXiv preprint) (2020).\n 41. Gudur, G. K. & Perepu, S. K. Zero-shot federated learning with new classes for audio classification. arXiv: 2106. 10019 (arXiv \npreprint) (2021).\n 42. Vrysis, L., Tsipas, N., Thoidis, I. & Dimoulas, C. 1d/2d deep cnns vs temporal feature integration for general audio classification. \nJ. Audio Eng. Soc. 68(1/2), 66–77 (2020).\n 43. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. & Polosukhin, I. Attention is all you need. In \nAdvances in Neural Information Processing Systems, 5998–6008 (2017).\n 44. Zhao, H., Jia, J. & Koltun, V . Exploring self-attention for image recognition. In Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, 10076–10085 (2020).\n 45. Kiktova, E., Lojka, M., Pleva, M., Juhar, J. & Cizmar, A. Gun type recognition from gunshot audio recordings. In 3rd International \nWorkshop on Biometrics and Forensics (IWBF 2015), 1–6 (IEEE, 2015).\n 46. Dogan, S. A new fractal h-tree pattern based gun model identification method using gunshot audios. Appl. Acoust.  177, 107916 \n(2021).\n 47. Tardif, B., Lo, D. & Goubran, R. Gunshot sound measurement and analysis. In 2021 IEEE Sensors Applications Symposium (SAS), \n1–6 (IEEE, 2021).\n 48. Mares, D. & Blackburn, E. Acoustic gunshot detection systems: A quasi-experimental evaluation in St. Louis, MO. J. Exp. Criminol. \n17(2), 193–215 (2021).\n 49. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 770–778 (2016).\n 50. Zhang, C., Zhang, M., Zhang, S., Jin, D., Zhou, Q., Cai, Z., Zhao, H., Yi, S., Liu, X. & Liu, Z. Delving deep into the generalization \nof vision transformers under distribution shifts. arXiv: 2106. 07617 (arXiv preprint) (2021).\n 51. Awad, G., Fiscus, J., Joy, D., Michel, M., Smeaton, A., Kraaij, W ., Eskevich, M., Aly, R., Ordelman, R., Ritter, M., et al: Trecvid 2016: \nEvaluating video search, video event detection, localization, and hyperlinking. In TREC Video Retrieval Evaluation (TRECVID)  \n(2016).\n 52. Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo: A dataset and taxonomy for urban sound research. In Proceedings \nof the 22nd ACM International Conference on Multimedia, 1041–1044 (2014).\n 53. Fang, Z. A high-efficient hybrid physics-informed neural networks based on convolutional neural network. IEEE Trans. Neural \nNetw. Learn. Syst. 20, 20 (2021).\n 54. Zheng, W ., Liu, X. & Yin, L. Research on image classification method based on improved multi-scale relational network. PeerJ \nComput. Sci. 7, 613 (2021).\n 55. Zuo, C. et al. Deep learning in optical metrology: A review. Light Sci. Appl. 11(1), 1–54 (2022).\n 56. Liu, R. et al. Sccgan: Style and characters inpainting based on cgan. Mob. Netw. Appl. 26(1), 3–12 (2021).\n 57. Ramachandran, P ., Parmar, N., Vaswani, A., Bello, I., Levskaya, A. & Shlens, J. Stand-alone self-attention in vision models. arXiv: \n1906. 05909 (arXiv preprint) (2019).\n 58. Hu, H., Zhang, Z., Xie, Z. & Lin, S. Local relation networks for image recognition. In Proceedings of the IEEE/CVF International \nConference on Computer Vision, 3464–3473 (2019).\n 59. Vaswani, A., Ramachandran, P ., Srinivas, A., Parmar, N., Hechtman, B. & Shlens, J. Scaling local self-attention for parameter efficient \nvisual backbones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12894–12904 (2021).\nAcknowledgements\nWe deeply acknowledge Taif University for supporting this study through Taif University Researchers Supporting \nProject Number (TURSP-2020/150), Taif University, Taif, Saudi Arabia.\nAuthor contributions\nAll authors contributed equally to this work while preparation of manuscript and revision of work.\nFunding\nFunding for this study is received from Taif University Researchers Supporting Project No. (Project No. TURSP-\n2020/150), Taif University, Taif, Saudi Arabia.\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:13300  | https://doi.org/10.1038/s41598-022-17497-1\nwww.nature.com/scientificreports/\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}