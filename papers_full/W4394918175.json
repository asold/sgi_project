{
  "title": "Large Language Models for Inorganic Synthesis Predictions",
  "url": "https://openalex.org/W4394918175",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2097251220",
      "name": "Seong-Min Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2208335126",
      "name": "Yousung Jung",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A1994813853",
      "name": "Joshua Schrier",
      "affiliations": [
        "Fordham University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283715583",
    "https://openalex.org/W2041726686",
    "https://openalex.org/W2785060548",
    "https://openalex.org/W3204265401",
    "https://openalex.org/W3145890816",
    "https://openalex.org/W3094175649",
    "https://openalex.org/W4318218504",
    "https://openalex.org/W4321494448",
    "https://openalex.org/W2770164889",
    "https://openalex.org/W2999645992",
    "https://openalex.org/W4377862923",
    "https://openalex.org/W4389493050",
    "https://openalex.org/W4394918175",
    "https://openalex.org/W4389407755",
    "https://openalex.org/W4365442601",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W4391035336",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4390511840",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W4394710479",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4389686454",
    "https://openalex.org/W4387393188",
    "https://openalex.org/W4388560379",
    "https://openalex.org/W4390011017",
    "https://openalex.org/W4390966669",
    "https://openalex.org/W4386888220",
    "https://openalex.org/W4388685549",
    "https://openalex.org/W4393924716",
    "https://openalex.org/W4387443463",
    "https://openalex.org/W4391800539",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W4389352201",
    "https://openalex.org/W4386882870",
    "https://openalex.org/W4392678243",
    "https://openalex.org/W14576171",
    "https://openalex.org/W3101215053",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W2278970271",
    "https://openalex.org/W2587621812",
    "https://openalex.org/W2996839165",
    "https://openalex.org/W4365211638",
    "https://openalex.org/W2145827727",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W4393869221",
    "https://openalex.org/W4391871462",
    "https://openalex.org/W2972597827",
    "https://openalex.org/W4220918529",
    "https://openalex.org/W4387078269",
    "https://openalex.org/W4391421491",
    "https://openalex.org/W2899867782",
    "https://openalex.org/W4387779638",
    "https://openalex.org/W4224211223",
    "https://openalex.org/W4392350883",
    "https://openalex.org/W4386167257",
    "https://openalex.org/W3105187533",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W3020918563"
  ],
  "abstract": "We evaluate the effectiveness of pre-trained and fine-tuned large language models (LLMs) for predicting the synthesizability of inorganic compounds and the selection of precursors needed to perform inorganic synthesis. The predictions of fine-tuned LLMs are comparable to—and sometimes better than—recent bespoke machine learning models for these tasks, but require only minimal user expertise, cost, and time to develop. Therefore, this strategy can serve both as an effective and strong baseline for future machine learning studies of various chemical applications and as a practical tool for experimental chemists.",
  "full_text": "Large Language Models for Inorganic Synthesis Predictions  \nSeongmin Kim1, Yousung Jung2,3,4*, and Joshua Schrier5*  \n1 Department of Chemical and Biomolecular Engineering, Korea Advanced Institute of Science \nand Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon 34141, Korea \n2 Department of Chemical and Biological Engineering (BK21 four), Seoul National University, 1 \nGwanak-ro, Gwanak-gu, Seoul 08826, Korea \n3 Institute of Chemical Processes, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul \n08826, Korea \n4 Interdisciplinary Program in Artificial Intelligence, Seoul National University, 1 Gwanak-ro, \nGwanak-gu, Seoul 08826, Korea \n5 Department of Chemistry and Biochemistry, Fordham University, 441 E. Fordham Road, The \nBronx, New York 10458, United States \n \n*Email: jschrier@fordham.edu \n*Email: yousung.jung@snu.ac.kr  \n \n \nAbstract \nWe evaluate the effectiveness of pre-trained and fine-tuned large language models (LLMs) for \npredicting the synthesizability of inorganic compounds and the selection of precursors needed to \nperform inorganic synthesis. The predictions of fine-tuned LLMs are comparable to—and \nsometimes better than—recent bespoke machine learning models for these tasks, but require only \nminimal user expertise, cost, and time to develop. Therefore, this strategy can serve both as an \neffective and strong baseline for future machine learning studies of various chemical applications \nand as a practical tool for experimental chemists. \n \n  \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nSynthesizing novel compositions of matter is a pre-requisite for scientific and practical \nbreakthroughs.1 Discovery would be accelerated if one could predict whether a hypothetical \ncompound could be made and what precursors should be used to make it. Human experts have \ndeveloped physical theories and heuristic rules for these tasks,2–6 but increasingly machine \nlearning (ML) is used to predict synthesizability7–11 and select precursors.12–15 However, \ndeveloping and training ML models requires substantial expertise, slowing adoption by \nexperimental chemists.16,17 \nGeneral purpose large language models (LLMs) are a form of generative artificial \nintelligence (AI),18 pre-trained on a broad dataset so they can be applied to many different tasks \nusing natural language. Pre-trained LLMs have been investigated for a wide variety of chemical \ntasks,19,20 such as extracting structured data from the literature,21–24 writing numerical simulation \nsoftware,25 and education.26 LLM-based workflows have been used to plan syntheses of organic \nmolecules27,28 and metal-organic frameworks (MOFs).21,29–31 Recent work has benchmarked \nmaterials science32,33 and general chemical knowledge34–37 of existing LLMs, and there are \nefforts to develop chemistry/materials-specific LLMs.38,39 Fine-tuning LLMs on modest amounts \nof data improves performance for specific tasks, while still taking advantage of the general pre-\ntraining to provide basic symbol interpretation and output formatting guidance. Chemical \napplications of LLM fine-tuning have addressed property regression and classification of organic \nmolecules,40–43 and been used to improve the water-harvesting behavior of MOFs.29  \nHere, we demonstrate that fine-tuned LLMs can predict inorganic synthesizability and \nprecursor selection with performance comparable to bespoke ML models. We used the GPT-3.5 \n(gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0125-preview) pre-trained LLMs from \nOpenAI, but expect similar results for other LLMs. For each task, we compared the results to \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nrecently published ML models, closely following the datasets and data processing used in those \nearlier studies; detailed descriptions are in the Supporting Information. The available data was \ndivided into 80% training and 20% test. Data is prepared for the fine-tuning process by \norganizing it into pairs of user input and desired response; examples are shown in the \nSupporting Information. Fine-tuning was performed starting from the gpt-3.5-turbo-0125 \nbase model, with OpenAI’s default hyperparameters, using 20% of the test data for validation. \nAll LLM evaluations were performed with the model temperature set to zero to return the most \nprobable response. Data, prompts, and code are available online. \nSynthesizability prediction: Given a chemical formula, predict if the compound could \nbe made. This is a positive-unlabeled (PU) learning problem,44–46 as the available data consists of \nformulas of known (previously made) compounds and unknown (hypothetical) compounds \nwhich may not be synthesizable. Following Jang and Noh et al.,47 we used the 393,053 unique \ninorganic compositions contained in the Materials Project (MP)48 and Open Quantum Materials \nDatabase (OQMD)49 (retrieved 02/2020) to define the set of possibilities; the 40,817 compounds \nwith Inorganic Crystal Structure Database (ICSD) references are positive (synthesized) and the \nremaining 352,236 are unlabeled (hypothesized). The only chemical input to each model is the \nformula, in the format Li1Fe1P1O4. LLMs were provided with the prompt: You are an \nexpert inorganic chemist.  Determine if the following compound is \nlikely to be synthesizable based on its composition, answering only \n\"P\" (for positive or possible) and \"U\" (for unknown or unlikely). LLM \nfine-tuning on all 32,653 positive compounds and an equal number of randomly selected \nunknown compounds in the training set requires <2.5 hours and <25 USD (as of 04/2024). We \ncompared to the stoichiometric convolutional graph neural fingerprint (stoi-CGNF), a \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\ncomposition-based synthesizability classification model trained by semi-supervised PU learning, \nand a stoichiometric similarity baseline model which classifies materials synthesizability based \non similarity cutoff;47 detailed explanations of these models are in the Supporting Information.  \nUnlike traditional binary classifiers with positive and negative data, only the true positive rate \n(TPR) or recall can be measured unambiguously for PU problems, as one lacks true negative \ndata. However, precision (PREC) and false positive rate (FPR) can be estimated by using the \nprior knowledge, α, which is the estimated proportion of the positive among the unlabeled \ndataset.50,51 The detailed α-estimation process is described in the Supporting Information; we \ncomputed α = 0.088, in agreement with the previous result (8.1%).47 \nAs depicted in Figure 1a, GPT-3.5 (FT) and stoi-CGNF metrics are comparable, and both \noutperform the GPT-3.5, GPT-4, and baseline methods in recall. However, their precision is \nlower than the baseline. These results correspond to choosing the highest probability outcome \n(i.e., in the binary classification task, labeling P if the model predicts p(P) > 0.5.) The exact \nmetric values of the 0.5-threshold results were tabulated in Table S1 and￼he probability \ndistributions of each model are show￼ Using the 0.5-threshold for stoi-CGNF and GPT-3.5 (FT) \nresults in 15-19% the unlabeled data predicted as positive (p(P|U)), which is inconsistent with \nour estimated α (8.8%). \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n \nFigure 1.  (a) Comparison of synthesizability predictions upon 0.5-threshold. The precision \n(PREC) was calculated from the α-estimation. (b) Comparison of synthesizability predictions \nafter the recalibration by using the prior knowledge, α=0.088. Thresholds are 0.723, 0.848, \n0.963, and 0.977 for stoi-CGNF, GPT-3.5(FT), GPT-4, and GPT-3.5. (c) The positively predicted \nratio among the unlabeled materials, before and after recalibration.   \n \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTo match the assumed structure of our PU dataset, we adopted higher threshold values, \nwhich corresponds to the ratio of positively predicted unlabeled entries are well-matched in α \nvalue, and recalibrated model performance. (See Figure 1b, 1c, and Table S2.) Probabilities for \nthe GPT models are obtained by querying the log-probabilities assigned to each possible \nresponse; Ramos et al. used a similar strategy for in-context Bayesian optimization of catalysts \nfrom text descriptions.52 Although threshold recalibration decreases recall, the models obtained \nmore balanced performance. (Figure 1b) After recalibration, GPT-3.5 (FT) still outperformed \nbaseline and obtained comparable performance to the recent stoi-CGNF model, despite using \nfewer unlabeled data in training. In contrast, recalibrating the pre-trained GPT-3.5 and GPT-4 \nmodels reduces their performance below the similarity baseline. This demonstrates the necessity \nof fine-tuning, independent of the recalibration strategy.  \nPrecursor Selection: Given the formula for a target compound, predict the complete set \nof precursors that must be provided. The output must exactly match the entire sets of precursors \nin a known example synthesis; because the output is restricted to a predefined list of precursors \nthis is a multi-label prediction problem.53 Following Kim et al.,15 we began with the text-minded \nsynthesis dataset of Kononova et al.,54 removing entries with inconsistent or incomplete data, and \nretaining only reactions that contained precursors used in ≥5 example reactions, which results in \n11,923 unique reactions and 311 precursors. (See Figure S1.)  Again, the only chemical input to \neach model was a chemical formula (e.g., LiFePO4) and the desired output is of the form \nLiFePO4 <- Li2CO3 + FeC2O4 + (NH4)2HPO4. Between 2 - 8 precursors must be \nspecified for a target (Figure S2); most targets have only one unique reaction, but some have as \nmany as 12 unique reactions (Figure S3). Like the synthesizability task, the test data contains \nonly positive examples; “incorrect” predictions may actually work in the laboratory, so our \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nevaluation underestimates true performance. LLMs were given the following system prompt: \nYou are an expert solid-state chemist planning a material synthesis. \nYou are provided with a target compound and must select the precursor \nreagents needed to synthesize the target.  Typically two or more \nprecursors are needed. The precursor reagents must provide all of the \nelements in the target. However, because the reactions are performed \nin an open system where gases can escape, it is acceptable for some \nnon-metal elements (e.g., H, C, N, O, F, Cl, Br) present in the \nprecursors to be absent in the final target product. Your synthesis \ntask is only to identify the correct precursor to use.  Do not provide \nstoichiometry. Only use precursors from the following candidates: … If \nasked to generate more than one reaction recommendation for a target, \neach recommendation should be different, and be separated with a \nnewline. Return only output of the following format for each \nrecommendation: [Target] <- [Precursor] + [Precursor] + [Precursor] \nThey were then asked in the user prompt synthesize [Target] or provide 5 \nsynthesis plans for [Target]to generate the prediction. Because of the smaller \ndataset, we performed a 5-fold cross-validation.  We compared the LLMs to the recent \nElementwise template model (Elemwise)15 and a random statistical baseline in which precursors \nare selected based on their frequency in the dataset for each element. Fine-tuning each GPT-3.5 \nmodel required <90 minutes and <11 USD (as of 04/2024).  \n \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n \nFigure 2:  Precursor selection accuracy, evaluated over 5-fold cross validation for each model. \nDark and light boxes indicate Top-1 and Top-5 performance, respectively, with the white \nhorizonal lines indicating the median.  Dashed lines indicate the minimum and maximum \naccuracy of a hypothetical “perfect” Elemwise method over the cross validation.    \n \nAs shown in Figure 2, the top-1 predictions of the fine-tuned GPT-3.5 and Elemwise \nmodels are comparable; all other methods are inferior. Although the pre-trained models have \nbetter top-1 predictions than the random baseline, they make worse top-5 predictions, suggesting \nthat they do not sample adequately diverse possibilities when generating multiple outputs.  This \nmight be counteracted by increasing the model temperature when generating multiple outputs. \nLLMs can generate precursor sets that are outside the domain of the Elemwise model, which \nassumes that each metal type present in the target corresponds to one precursor.  For example, \ndiammonium phosphate does not contain a metal, and thus cannot be predicted as a precursor by \nthe Elemwise model. The performance of a hypothetical “perfect” Elemwise model is shown as \nthe dashed lines in Figure 2; the top-5 predictions of the fine-tuned GPT-3.5 model slightly \nexceed this limit. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAlthough the overall prediction qualities are comparable, the models make different \npredictions. As depicted in Figure 3, the Top-1 and Top-5 predictions of the two models can vary, \nwhere one is correct and the other is incorrect.  Combining the Top-5 predictions of both models \nwould predict the correct precursors for 93% of target compounds. This suggests the value of \nincluding fine-tuned LLM predictions in ensemble methods. We investigated whether combining \nthe top-5 predictions of the Elemwise (84.5% accuracy) and GPT-3.5 FT (86.0%) and then \nasking the pre-trained GPT-4 model to discuss the feasibility of each plan before selecting the \nbest five syntheses would improve the results.  This provides both human-readable explanations \nand a small, but statistically significant, improvement of the resulting top-5 prediction accuracy \nto 87.6%. (See Supporting Information.)  \n \n \nFigure 3:  Comparison of a) Top-1 and b) Top-5 prediction precursor prediction accuracy by the \nElemwise and GPT-3.5 (FT) models summed over all cross-validation splits.   \n \nIn conclusion, fine-tuned LLMs are comparable to—and in some cases, better than—the \nlatest ML models developed specifically for the synthesizability and precursor selection \nproblems, using only the target chemical formula as input. In the case of the precursor selection \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nproblem, the general output allows for answers that are outside the domain of bespoke ML \nmodels. Because of their simplicity and low cost, we recommend that fine-tuned LLMs be used \nas a strong baseline method against which to compare future bespoke ML models. Previous \nresults on organic molecules have come to similar conclusions for regression and \nclassification;40–43 our results extend that recommendation to inorganic chemistry and PU-\nlearning and multilabel tasks. We also recommend that developers of chemistry- and materials-\nspecific LLMs39,55–57 prioritize the ease with which users can fine-tune the models to unlock this \ncapability. \nA limitation is that this approach relies upon statistical patterns in the training data; biases \npresent in reported syntheses58,59 may hinder extrapolation to novel or rare chemistry.60 Also, \ncommercial LLMs do not disclose their training sets, which may raise the concern of inadvertent \nleakage of test data into the training set. However, the relatively poor performance of the pre-\ntrained models for these tasks suggests this is unlikely. \nAs our goal was to illustrate this approach in the simplest possible way, there are many \nways the performance might be improved. We made no attempt at prompt engineering61 or \nhyperparameter optimization of the fine-tuning process. External function calling of “tools” (e.g., \nperforming numerical or thermodynamic calculations) can be combined with iterative chain-of-\nthought methods (“think step by step”) to further improve problem solving.26,27,62  Finally, we \nexpect continued advances in the underlying pretrained LLMs to improve performance.  \n \n \n \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAcknowledgement \nYJ acknowledges support from NRF (RS-2023-00283902, 2021R1A5A1030054) and IITP \n(2021-0-01343) of Korea government. JS acknowledges Fordham University for granting a \nsabbatical to study LLMs, Seoul National University for a Global Visiting Faculty Fellowship \nduring which the work was initiated, and support by the U.S. Department of Energy, Office of \nScience, Office of Basic Energy Sciences, Heavy Element Chemistry Program under contract \nKC0302031, subcontracted through Los Alamos National Laboratory. \n \nSupporting Information  \nThis Supporting Information is available free of charge at http://pubs.acs.org/[provided by \npublisher] \n• Description of data preparation. Plots of the distribution of number of unique reactions and \nnumber of precursors. Description of model construction and training. LLM prompts. \nDescription for evaluation metrics. Tables of the model performance for the synthesizability \ntask. Description of methods and results for re-evaluating top-5 predictions using GPT-4 and \ncode for associated statistical tests. (PDF) \n \nData Availability \nThe code and data underlying this study are openly available on Github at \nhttps://github.com/jschrier/SynthGPT/, with a persistent archival copy deposited in Zenodo at \nDOI:[to be deposited after article passes peer review]. Access to GPT-3.5 and GPT-4 is \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\ncommercially available to the public at https://openai.com.  The stoi-CGNF source code is \navailable at https://github.com/kaist-amsg/Synthesizability-stoi-CGNF and the Elemwise source \ncode is available at https://github.com/kaist-amsg/ElemwiseRetro/ . \n \nNotes \n The authors declare no competing financial interest. \n  \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nReferences \n(1) Cheetham, A. K.; Seshadri, R.; Wudl, F. Chemical Synthesis and Materials Discovery. Nat. \nSynth 2022, 1 (7), 514–520. https://doi.org/10.1038/s44160-022-00096-3. \n(2) Goldschmidt, V . M. Die Gesetze der Krystallochemie. Naturwissenschaften 1926, 14 (21), \n477–485. https://doi.org/10.1007/BF01507527. \n(3) Bartel, C. J.; Sutton, C.; Goldsmith, B. R.; Ouyang, R.; Musgrave, C. B.; Ghiringhelli, L. \nM.; Scheffler, M. New Tolerance Factor to Predict the Stability of Perovskite Oxides and \nHalides. Science Advances 2019, 5 (2), eaav0693. https://doi.org/10.1126/sciadv.aav0693. \n(4) Ouyang, B.; Wang, J.; He, T.; Bartel, C. J.; Huo, H.; Wang, Y .; Lacivita, V .; Kim, H.; Ceder, \nG. Synthetic Accessibility and Stability Rules of NASICONs. Nat Commun 2021, 12 (1), \n5752. https://doi.org/10.1038/s41467-021-26006-3. \n(5) Woodward, P. M.; Karen, P.; Evans, J. S. O.; V ogt, T. Solid State Materials Chemistry; \nCambridge University Press: Cambridge, 2021. https://doi.org/10.1017/9781139025348. \n(6) West, A. R. Solid State Chemistry and Its Applications, Second edition.; Wiley: Hoboken, \nNJ, 2022. \n(7) Jang, J.; Gu, G. H.; Noh, J.; Kim, J.; Jung, Y . Structure-Based Synthesizability Prediction of \nCrystals Using Partially Supervised Learning. J. Am. Chem. Soc. 2020, 142 (44), 18836–\n18843. https://doi.org/10.1021/jacs.0c07384. \n(8) Gu, G. H.; Jang, J.; Noh, J.; Walsh, A.; Jung, Y . Perovskite Synthesizability Using Graph \nNeural Networks. npj Comput Mater 2022, 8 (1), 1–8. https://doi.org/10.1038/s41524-022-\n00757-z. \n(9) Gleaves, D.; Fu, N.; Siriwardane, E. M. D.; Zhao, Y .; Hu, J. Materials Synthesizability and \nStability Prediction Using a Semi-Supervised Teacher-Student Dual Neural Network. \nDigital Discovery 2023, 2 (2), 377–391. https://doi.org/10.1039/D2DD00098A. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(10) Zhu, R.; Tian, S. I. P.; Ren, Z.; Li, J.; Buonassisi, T.; Hippalgaonkar, K. Predicting \nSynthesizability Using Machine Learning on Databases of Existing Inorganic Materials. \nACS Omega 2023, 8 (9), 8210–8218. https://doi.org/10.1021/acsomega.2c04856. \n(11) Antoniuk, E. R.; Cheon, G.; Wang, G.; Bernstein, D.; Cai, W.; Reed, E. J. Predicting the \nSynthesizability of Crystalline Inorganic Materials from the Data of Known Material \nCompositions. npj Comput Mater 2023, 9 (1), 1–11. https://doi.org/10.1038/s41524-023-\n01114-4. \n(12) Kim, E.; Huang, K.; Jegelka, S.; Olivetti, E. Virtual Screening of Inorganic Materials \nSynthesis Parameters with Deep Learning. npj Comput Mater 2017, 3 (1), 1–9. \nhttps://doi.org/10.1038/s41524-017-0055-6. \n(13) Kim, E.; Jensen, Z.; Van Grootel, A.; Huang, K.; Staib, M.; Mysore, S.; Chang, H.-S.; \nStrubell, E.; McCallum, A.; Jegelka, S.; Olivetti, E. Inorganic Materials Synthesis Planning \nwith Literature-Trained Neural Networks. J. Chem. Inf. Model. 2020, 60 (3), 1194–1201. \nhttps://doi.org/10.1021/acs.jcim.9b00995. \n(14) He, T.; Huo, H.; Bartel, C. J.; Wang, Z.; Cruse, K.; Ceder, G. Inorganic Synthesis \nRecommendation by Machine Learning Materials Similarity from Scientific Literature. \nScience Advances 2023, 9 (23), eadg8180. https://doi.org/10.1126/sciadv.adg8180. \n(15) Kim, S.; Noh, J.; Ho Gu, G.; Chen, S.; Jung, Y . Predicting Synthesis Recipes of Inorganic \nCrystal Materials Using Elementwise Template Formulation. Chemical Science 2024, 15 \n(3), 1039–1045. https://doi.org/10.1039/D3SC03538G. \n(16) Yano, J.; Gaffney, K. J.; Gregoire, J.; Hung, L.; Ourmazd, A.; Schrier, J.; Sethian, J. A.; \nToma, F. M. The Case for Data Science in Experimental Chemistry: Examples and \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nRecommendations. Nat Rev Chem 2022, 6, 357–370. https://doi.org/10.1038/s41570-022-\n00382-w. \n(17) Back, S.; Aspuru-Guzik, A.; Ceriotti, M.; Gryn’ova, G.; Grzybowski, B.; Gu, G. H.; Hein, \nJ.; Hippalgaonkar, K.; Hormázabal, R.; Jung, Y .; Kim, S.; Kim, W. Y .; Moosavi, S. M.; Noh, \nJ.; Park, C.; Schrier, J.; Schwaller, P.; Tsuda, K.; Vegge, T.; Lilienfeld, O. A. von; Walsh, A. \nAccelerated Chemical Science with AI. Digital Discovery 2024, 3 (1), 23–33. \nhttps://doi.org/10.1039/D3DD00213F. \n(18) Anstine, D. M.; Isayev, O. Generative Models as an Emerging Paradigm in the Chemical \nSciences. J. Am. Chem. Soc. 2023, 145 (16), 8736–8750. \nhttps://doi.org/10.1021/jacs.2c13467. \n(19) Jablonka, K. M.; Ai, Q.; Al-Feghali, A.; Badhwar, S.; Bocarsly, J. D.; Bran, A. M.; \nBringuier, S.; Brinson, L. C.; Choudhary, K.; Circi, D.; Cox, S.; Jong, W. A. de; Evans, M. \nL.; Gastellu, N.; Genzling, J.; Gil, M. V .; Gupta, A. K.; Hong, Z.; Imran, A.; Kruschwitz, S.; \nLabarre, A.; Lála, J.; Liu, T.; Ma, S.; Majumdar, S.; Merz, G. W.; Moitessier, N.; Moubarak, \nE.; Mouriño, B.; Pelkie, B.; Pieler, M.; Ramos, M. C.; Ranković, B.; Rodriques, S. G.; \nSanders, J. N.; Schwaller, P.; Schwarting, M.; Shi, J.; Smit, B.; Smith, B. E.; Herck, J. V .; \nVölker, C.; Ward, L.; Warren, S.; Weiser, B.; Zhang, S.; Zhang, X.; Zia, G. A.; Scourtas, A.; \nSchmidt, K. J.; Foster, I.; White, A. D.; Blaiszik, B. 14 Examples of How LLMs Can \nTransform Materials Science and Chemistry: A Reflection on a Large Language Model \nHackathon. Digital Discovery 2023, 2 (5), 1233–1250. \nhttps://doi.org/10.1039/D3DD00113J. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(20) Zhang, J.; Fang, Y .; Shao, X.; Chen, H.; Zhang, N.; Fan, X. The Future of Molecular \nStudies through the Lens of Large Language Models. J. Chem. Inf. Model. 2024, 64 (3), \n563–566. https://doi.org/10.1021/acs.jcim.3c01977. \n(21) Zheng, Z.; Zhang, O.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. ChatGPT Chemistry Assistant \nfor Text Mining and the Prediction of MOF Synthesis. J. Am. Chem. Soc. 2023. \nhttps://doi.org/10.1021/jacs.3c05819. \n(22) Thway, M.; Low, A. K. Y .; Khetan, S.; Dai, H.; Recatala-Gomez, J.; Chen, A. P.; \nHippalgaonkar, K. Harnessing GPT-3.5 for Text Parsing in Solid-State Synthesis – Case \nStudy of Ternary Chalcogenides. Digital Discovery 2024, 3 (2), 328–336. \nhttps://doi.org/10.1039/D3DD00202K. \n(23) Lee, S.; Heinen, S.; Khan, D.; Anatole V on Lilienfeld, O. Autonomous Data Extraction \nfrom Peer Reviewed Literature for Training Machine Learning Models of Oxidation \nPotentials. Mach. Learn.: Sci. Technol. 2024, 5 (1), 015052. https://doi.org/10.1088/2632-\n2153/ad2f52. \n(24) Polak, M. P.; Morgan, D. Extracting Accurate Materials Data from Research Papers with \nConversational Language Models and Prompt Engineering. Nat Commun 2024, 15 (1), \n1569. https://doi.org/10.1038/s41467-024-45914-8. \n(25) D. White, A.; M. Hocky, G.; A. Gandhi, H.; Ansari, M.; Cox, S.; P. Wellawatte, G.; Sasmal, \nS.; Yang, Z.; Liu, K.; Singh, Y .; Ccoa, W. J. P. Assessment of Chemistry Knowledge in \nLarge Language Models That Generate Code. Digital Discovery 2023, 2 (2), 368–376. \nhttps://doi.org/10.1039/D2DD00087C. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(26) Schrier, J. Comment on “Comparing the Performance of College Chemistry Students with \nChatGPT for Calculations Involving Acids and Bases.” J. Chem. Educ. 2024. \nhttps://doi.org/10.1021/acs.jchemed.4c00058. \n(27) Bran, A. M.; Cox, S.; White, A. D.; Schwaller, P. ChemCrow: Augmenting Large-Language \nModels with Chemistry Tools. arXiv April 12, 2023. \nhttps://doi.org/10.48550/arXiv.2304.05376. \n(28) Boiko, D. A.; MacKnight, R.; Kline, B.; Gomes, G. Autonomous Chemical Research with \nLarge Language Models. Nature 2023, 624 (7992), 570–578. \nhttps://doi.org/10.1038/s41586-023-06792-0. \n(29) Zheng, Z.; Alawadhi, A. H.; Chheda, S.; Neumann, S. E.; Rampal, N.; Liu, S.; Nguyen, H. \nL.; Lin, Y .; Rong, Z.; Siepmann, J. I.; Gagliardi, L.; Anandkumar, A.; Borgs, C.; Chayes, J. \nT.; Yaghi, O. M. Shaping the Water-Harvesting Behavior of Metal–Organic Frameworks \nAided by Fine-Tuned GPT Models. J. Am. Chem. Soc. 2023, 145 (51), 28284–28295. \nhttps://doi.org/10.1021/jacs.3c12086. \n(30) Zheng, Z.; Rong, Z.; Rampal, N.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. A GPT-4 Reticular \nChemist for Guiding MOF Discovery. Angewandte Chemie International Edition 2023, 62 \n(46), e202311983. https://doi.org/10.1002/anie.202311983. \n(31) Zheng, Z.; Zhang, O.; Nguyen, H. L.; Rampal, N.; Alawadhi, A. H.; Rong, Z.; Head-\nGordon, T.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. ChatGPT Research Group for \nOptimizing the Crystallinity of MOFs and COFs. ACS Cent. Sci. 2023, 9 (11), 2161–2170. \nhttps://doi.org/10.1021/acscentsci.3c01087. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(32) Zaki, M.; Jayadeva; Mausam; Krishnan, N. M. A. MaScQA: Investigating Materials \nScience Knowledge of Large Language Models. Digital Discovery 2024, 3, 313–327. \nhttps://doi.org/10.1039/D3DD00188A. \n(33) Deb, J.; Saikia, L.; Dihingia, K. D.; Sastry, G. N. ChatGPT in the Material Design: Selected \nCase Studies to Assess the Potential of ChatGPT. J. Chem. Inf. Model. 2024, 64 (3), 799–\n811. https://doi.org/10.1021/acs.jcim.3c01702. \n(34) Hatakeyama-Sato, K.; Yamane, N.; Igarashi, Y .; Nabae, Y .; Hayakawa, T. Prompt \nEngineering of GPT-4 for Chemical Research: What Can/Cannot Be Done? Science and \nTechnology of Advanced Materials: Methods 2023, 3 (1), 2260300. \nhttps://doi.org/10.1080/27660400.2023.2260300. \n(35) Guo, T.; Guo, K.; Nan, B.; Liang, Z.; Guo, Z.; Chawla, N.; Wiest, O.; Zhang, X. What Can \nLarge Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks. In \nAdvances in Neural Information Processing Systems; Oh, A., Neumann, T., Globerson, A., \nSaenko, K., Hardt, M., Levine, S., Eds.; Curran Associates, Inc., 2023; V ol. 36, pp 59662–\n59688. \n(36) AI4Science, M. R.; Quantum, M. A. The Impact of Large Language Models on Scientific \nDiscovery: A Preliminary Study Using GPT-4. arXiv December 8, 2023. \nhttps://doi.org/10.48550/arXiv.2311.07361. \n(37) Mirza, A.; Alampara, N.; Kunchapu, S.; Emoekabu, B.; Krishnan, A.; Wilhelmi, M.; \nOkereke, M.; Eberhardt, J.; Elahi, A. M.; Greiner, M.; Holick, C. T.; Gupta, T.; Asgari, M.; \nGlaubitz, C.; Klepsch, L. C.; Köster, Y .; Meyer, J.; Miret, S.; Hoffmann, T.; Kreth, F. A.; \nRingleb, M.; Roesner, N.; Schubert, U. S.; Stafast, L. M.; Wonanke, D.; Pieler, M.; \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nSchwaller, P.; Jablonka, K. M. Are Large Language Models Superhuman Chemists? arXiv \nApril 1, 2024. https://doi.org/10.48550/arXiv.2404.01475. \n(38) Wang, Z.; Chen, A.; Tao, K.; Han, Y .; Li, J. MatGPT: A Vane of Materials Informatics from \nPast, Present, to Future. Advanced Materials 2003, 36 (6), 2306733. \nhttps://doi.org/10.1002/adma.202306733. \n(39) Zhang, D.; Liu, W.; Tan, Q.; Chen, J.; Yan, H.; Yan, Y .; Li, J.; Huang, W.; Yue, X.; Zhou, D.; \nZhang, S.; Su, M.; Zhong, H.; Li, Y .; Ouyang, W. ChemLLM: A Chemical Large Language \nModel. arXiv February 9, 2024. https://doi.org/10.48550/arXiv.2402.06852. \n(40) Jablonka, K. M.; Schwaller, P.; Ortega-Guerrero, A.; Smit, B. Leveraging Large Language \nModels for Predictive Chemistry. Nat Mach Intell 2024, 1–9. \nhttps://doi.org/10.1038/s42256-023-00788-1. \n(41) Chen, L.; Xie, Z.; Evangelopoulos, X.; Omar, O. H.; Troisi, A.; Cooper, A. Fine-Tuning \nGPT-3 for Machine Learning Electronic and Functional Properties of Organic Molecules. \nChem. Sci. 2024, 15, 500–510. https://doi.org/10.1039/D3SC04610A. \n(42) Zhong, S.; Guan, X. Developing Quantitative Structure–Activity Relationship (QSAR) \nModels for Water Contaminants’ Activities/Properties by Fine-Tuning GPT-3 Models. \nEnviron. Sci. Technol. Lett. 2023, 10 (10), 872–877. \nhttps://doi.org/10.1021/acs.estlett.3c00599. \n(43) Zhong, Z.; Zhou, K.; Mottin, D. Benchmarking Large Language Models for Molecule \nPrediction Tasks. arXiv March 8, 2024. https://doi.org/10.48550/arXiv.2403.05075. \n(44) Elkan, C.; Noto, K. Learning Classifiers from Only Positive and Unlabeled Data. In \nProceedings of the 14th ACM SIGKDD international conference on Knowledge discovery \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nand data mining; ACM: Las Vegas Nevada USA, 2008; pp 213–220. \nhttps://doi.org/10.1145/1401890.1401920. \n(45) Li, X.-L.; Yu, P. S.; Liu, B.; Ng, S.-K. Positive Unlabeled Learning for Data Stream \nClassification. In Proceedings of the 2009 SIAM International Conference on Data Mining; \nSociety for Industrial and Applied Mathematics, 2009; pp 259–270. \nhttps://doi.org/10.1137/1.9781611972795.23. \n(46) Bekker, J.; Davis, J. Learning from Positive and Unlabeled Data: A Survey. Mach Learn \n2020, 109 (4), 719–760. https://doi.org/10.1007/s10994-020-05877-5. \n(47) Jang, J.; Noh, J.; Zhou, L.; Gu, G. H.; Gregoire, J. M.; Jung, Y . Predicting the \nSynthesizability of Materials Stoichiometry. In Review 2024. \n(48) Jain, A.; Ong, S. P.; Hautier, G.; Chen, W.; Richards, W. D.; Dacek, S.; Cholia, S.; Gunter, \nD.; Skinner, D.; Ceder, G.; Persson, K. a. The Materials Project: A Materials Genome \nApproach to Accelerating Materials Innovation. APL Materials 2013, 1 (1), 011002. \nhttps://doi.org/10.1063/1.4812323. \n(49) Kirklin, S.; Saal, J. E.; Meredig, B.; Thompson, A.; Doak, J. W.; Aykol, M.; Rühl, S.; \nWolverton, C. The Open Quantum Materials Database (OQMD): Assessing the Accuracy of \nDFT Formation Energies. npj Comput Mater 2015, 1 (1), 15010. \nhttps://doi.org/10.1038/npjcompumats.2015.10. \n(50) Jain, S.; White, M.; Radivojac, P. Recovering True Classifier Performance in Positive-\nUnlabeled Learning. AAAI 2017, 31 (1). https://doi.org/10.1609/aaai.v31i1.10937. \n(51) Zeiberg, D.; Jain, S.; Radivojac, P. Fast Nonparametric Estimation of Class Proportions in \nthe Positive-Unlabeled Classification Setting. AAAI 2020, 34 (04), 6729–6736. \nhttps://doi.org/10.1609/aaai.v34i04.6151. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(52) Ramos, M. C.; Michtavy, S. S.; Porosoff, M. D.; White, A. D. Bayesian Optimization of \nCatalysts With In-Context Learning. arXiv April 11, 2023. \nhttps://doi.org/10.48550/arXiv.2304.05341. \n(53) Madjarov, G.; Kocev, D.; Gjorgjevikj, D.; Džeroski, S. An Extensive Experimental \nComparison of Methods for Multi-Label Learning. Pattern Recognition 2012, 45 (9), 3084–\n3104. https://doi.org/10.1016/j.patcog.2012.03.004. \n(54) Kononova, O.; Huo, H.; He, T.; Rong, Z.; Botari, T.; Sun, W.; Tshitoyan, V .; Ceder, G. Text-\nMined Dataset of Inorganic Materials Synthesis Recipes. Sci Data 2019, 6 (1), 203. \nhttps://doi.org/10.1038/s41597-019-0224-1. \n(55) Chen, Z.-Y .; Xie, F.-K.; Wan, M.; Yuan, Y .; Liu, M.; Wang, Z.-G.; Meng, S.; Wang, Y .-G. \nMatChat: A Large Language Model and Application Service Platform for Materials \nScience. Chinese Phys. B 2023, 32 (11), 118104. https://doi.org/10.1088/1674-\n1056/ad04cb. \n(56) Hudson, N. C.; Pauloski, J. G.; Baughman, M.; Kamatar, A.; Sakarvadia, M.; Ward, L.; \nChard, R.; Bauer, A.; Levental, M.; Wang, W.; Engler, W.; Price Skelly, O.; Blaiszik, B.; \nStevens, R.; Chard, K.; Foster, I. Trillion Parameter AI Serving Infrastructure for Scientific \nDiscovery: A Survey and Vision. In Proceedings of the IEEE/ACM 10th International \nConference on Big Data Computing, Applications and Technologies; ACM: Taormina \n(Messina) Italy, 2023; pp 1–10. https://doi.org/10.1145/3632366.3632396. \n(57) Yu, B.; Baker, F. N.; Chen, Z.; Ning, X.; Sun, H. LlaSMol: Advancing Large Language \nModels for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning \nDataset. arXiv February 17, 2024. https://doi.org/10.48550/arXiv.2402.09391. \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(58) Jia, X.; Lynch, A.; Huang, Y .; Danielson, M.; Lang’at, I.; Milder, A.; Ruby, A. E.; Wang, H.; \nFriedler, S. A.; Norquist, A. J.; Schrier, J. Anthropogenic Biases in Chemical Reaction Data \nHinder Exploratory Inorganic Synthesis. Nature 2019, 573 (7773), 251–255. \nhttps://doi.org/10.1038/s41586-019-1540-5. \n(59) Beker, W.; Roszak, R.; Wołos, A.; Angello, N. H.; Rathore, V .; Burke, M. D.; Grzybowski, \nB. A. Machine Learning May Sometimes Simply Capture Literature Popularity Trends: A \nCase Study of Heterocyclic Suzuki–Miyaura Coupling. J. Am. Chem. Soc. 2022, 144 (11), \n4819–4827. https://doi.org/10.1021/jacs.1c12005. \n(60) Schrier, J.; Norquist, A. J.; Buonassisi, T.; Brgoch, J. In Pursuit of the Exceptional: \nResearch Directions for Machine Learning in Chemical and Materials Science. J. Am. \nChem. Soc. 2023, 145 (40), 21699–21716. https://doi.org/10.1021/jacs.3c04783. \n(61) Prompt Engineering Guide. https://www.promptingguide.ai/ (accessed 2024-03-03). \n(62) Chiang, Y .; Chou, C.-H.; Riebesell, J. LLaMP: Large Language Model Made Powerful for \nHigh-Fidelity Materials Knowledge Retrieval and Distillation. arXiv January 30, 2024. \nhttps://doi.org/10.48550/arXiv.2401.17244. \n \n \n  \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nFor Table of Contents Only \n \nhttps://doi.org/10.26434/chemrxiv-2024-9bmfj ORCID: https://orcid.org/0000-0002-2071-1657 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0",
  "topic": "Bespoke",
  "concepts": [
    {
      "name": "Bespoke",
      "score": 0.966029703617096
    },
    {
      "name": "Computer science",
      "score": 0.6319088935852051
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5366169214248657
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4409746527671814
    },
    {
      "name": "Machine learning",
      "score": 0.43942949175834656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4131515622138977
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I164389053",
      "name": "Fordham University",
      "country": "US"
    }
  ],
  "cited_by": 3
}