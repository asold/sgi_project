{
  "title": "LightXML: Transformer with Dynamic Negative Sampling for High-Performance Extreme Multi-label Text Classification",
  "url": "https://openalex.org/W3120689745",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5090606731",
      "name": "Ting Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100612161",
      "name": "Deqing Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081275566",
      "name": "Leilei Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5057010946",
      "name": "Huayi Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081856566",
      "name": "Zhengyang Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102969899",
      "name": "Fuzhen Zhuang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970574105",
    "https://openalex.org/W2936191805",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W1781770377",
    "https://openalex.org/W2183087644",
    "https://openalex.org/W2743021690",
    "https://openalex.org/W3037422790",
    "https://openalex.org/W2792287754",
    "https://openalex.org/W2921113176",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2788125153",
    "https://openalex.org/W2061873838",
    "https://openalex.org/W2100743709",
    "https://openalex.org/W2461743311",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2890634844",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W21938781",
    "https://openalex.org/W2520348554",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2068074736",
    "https://openalex.org/W2744136723"
  ],
  "abstract": "Extreme Multi-label text Classification (XMC) is a task of finding the most relevant labels from a large label set. Nowadays deep learning-based methods have shown significant success in XMC. However, the existing methods (e.g., AttentionXML and X-Transformer etc) still suffer from 1) combining several models to train and predict for one dataset, and 2) sampling negative labels statically during the process of training label ranking model, which reduces both the efficiency and accuracy of the model. To address the above problems, we proposed LightXML, which adopts end-to-end training and dynamic negative labels sampling. In LightXML, we use generative cooperative networks to recall and rank labels, in which label recalling part generates negative and positive labels, and label ranking part distinguishes positive labels from these labels. Through these networks, negative labels are sampled dynamically during label ranking part training by feeding with the same text representation. Extensive experiments show that LightXML outperforms state-of-the-art methods in five extreme multi-label datasets with much smaller model size and lower computational complexity. In particular, on the Amazon dataset with 670K labels, LightXML can reduce the model size up to 72% compared to AttentionXML.",
  "full_text": "LightXML: Transformer with Dynamic Negative Sampling for High-Performance\nExtreme Multi-label Text Classiﬁcation\nTing Jiang1, Deqing Wang1, Leilei Sun1,*, Huayi Yang1, Zhengyang Zhao1, Fuzhen Zhuang2,3\n1SKLSDE and BDBC Lab, Beihang University, Beijing, China\n2Key Lab of Intelligent Information Processing of CAS, Institute of Computing Technology, CAS, Beijing, China\n3Beijing Advanced Innovation Center for Imaging Theory and Technology, Academy for Multidisciplinary Studies, Capital\nNormal University, Beijing, China\n{royokong, dqwang, leileisun, yanghy, zzy979}@buaa.edu.cn, zhuangfuzhen@ict.ac.cn\nAbstract\nExtreme Multi-label text Classiﬁcation (XMC) is a task of\nﬁnding the most relevant labels from a large label set. Nowa-\ndays deep learning-based methods have shown signiﬁcant\nsuccess in XMC. However, the existing methods (e.g., Atten-\ntionXML and X-Transformer etc) still suffer from 1) com-\nbining several models to train and predict for one dataset,\nand 2) sampling negative labels statically during the pro-\ncess of training label ranking model, which reduces both\nthe efﬁciency and accuracy of the model. To address the\nabove problems, we proposed LightXML, which adopts end-\nto-end training and dynamic negative labels sampling. In\nLightXML, we use generative cooperative networks to re-\ncall and rank labels, in which label recalling part generates\nnegative and positive labels, and label ranking part distin-\nguishes positive labels from these labels. Through these net-\nworks, negative labels are sampled dynamically during label\nranking part training by feeding with the same text repre-\nsentation. Extensive experiments show that LightXML out-\nperforms state-of-the-art methods in ﬁve extreme multi-label\ndatasets with much smaller model size and lower computa-\ntional complexity. In particular, on the Amazon dataset with\n670K labels, LightXML can reduce the model size up to\n72% compared to AttentionXML. Our code is available at\nhttp://github.com/kongds/LightXML.\nIntroduction\nExtreme Multi-label text Classiﬁcation (XMC) is a task of\nﬁnding the most relevant labels for each text from an ex-\ntremely large label set. It is a very practical problem that\nhas been widely applied in many real-world scenarios, such\nas tagging a Wikipedia article with most relevant labels\n(Dekel and Shamir 2010), dynamic search advertising in E-\ncommerce (Prabhu et al. 2018), and suggesting keywords to\nadvertisers on Amazon (Chang et al. 2020).\nDifferent from the classical multi-label classiﬁcation\nproblem, the candidate label set could be very large, which\nresults in a huge computational complexity. To overcome\nthis difﬁculty, many methods (e.g., Parabel (Prabhu et al.\n2018), DiSMEC (Babbar and Sch ¨olkopf 2017) and Atten-\ntionXML (You et al. 2019) etc) have been proposed in re-\n*Corresponding Author\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ncent years. From the perspective of text representation learn-\ning, these methods could be divided into two categories: 1)\nRaw feature methods, where texts are represented by sparse\nvectors and feed to classiﬁers directly; 2) Semantic feature\nmethods, where deep neural networks are usually employed\nto transfer the texts into semantic representations before the\nclassiﬁcation procedure.\nThe superiority of XMC methods with semantic features\nhave been frequently reported recently. For example, At-\ntentionXML (You et al. 2019) and X-Transformer (Chang\net al. 2020) have achieved signiﬁcant improvements in ac-\ncuracy comparing to the state-of-the-art methods. However,\nthe challenge of how to solve the XMC problem with the\nconstraint computational resources still remains, both At-\ntentionXML and X-Transformer need large computational\nresources to train or to implement. AttentionXML needs\nto train four seperate models for big XMC datasets like\nAmazon-670K. In X-Transformer, it uses a large trans-\nformer model for recalling labels, and simple linear classiﬁ-\ncations are used to rank labels. Both methods split the train-\ning process into multiple stages, and each stage needs a sep-\narate model to train, which takes a lot of computational re-\nsources. Another disadvantage of these methods is the static\nnegative label sampling. Both methods train label ranking\nmodel with negative labels sampled by ﬁne-tuned label re-\ncalling models. This negative sampling strategy makes label\nranking model only focus on a small number of negative la-\nbels, and hard to converge due to these negative labels are\nvery similar to positive labels.\nTo address the above problems, we propose a light\ndeep learning model, LightXML, which ﬁne-tunes single\ntransformer model with dynamic negative label sampling.\nLightXML consists of three parts: text representing, label\nrecalling, and label ranking. For text representing, we use\nmulti-layer features of the transformer model as text repre-\nsentation, which can prove rich text information for the other\ntwo parts. With the advantage of dynamic negative sampling,\nwe propose generative cooperative networks to recall and\nrank labels. For the label recalling part, we use the genera-\ntor network based on label clusters, which is used to recall\nlabels. For the label ranking part, we use the discriminator\nnetwork to distinguish positive labels from recalled labels.\nIn summary, the contributions of this paper are as follows:\narXiv:2101.03305v1  [cs.CL]  9 Jan 2021\n• A novel deep learning method is proposed, which com-\nbines the powerful transformer model with generative co-\noperative networks. This method can fully exploit the ad-\nvantage of transformer model by end-to-end training.\n• We propose dynamic negative sampling by using gener-\native cooperative networks to recall and rank labels. The\ndynamic negative sampling allows label ranking part to\nlearn from easy to hard and avoid overﬁtting, which can\nboost overall model performance.\n• Our extensive experiments show that our model achieves\nthe best results among all methods on ﬁve benchmark\ndatasets. In contrast, our model has much smaller model\nsize and lower computational complexity than current\nstate-of-the-art methods.\nRelated work\nMany novel methods have been proposed to improve accu-\nracy while controlling computational complexity and model\nsizes in XMC. These methods can be broadly categorized\ninto two directions according to the input: One is tradi-\ntional machine learning methods that use the sparse features\nof text like BOW features as input, and the other is deep\nlearning methods that use raw text. For traditional machine\nlearning methods, we can continue to divide these methods\ninto three directions: one-vs-all methods, tree-based meth-\nods and embedding-based methods.\nOne-vs-all methods One-vs-all methods such as DiS-\nMEC (Babbar and Sch ¨olkopf 2017), ProXML (Babbar\nand Sch ¨olkopf 2019), PDSparse (Yen et al. 2016), and\nPPDSparse (Yen et al. 2017) which treat each label as bi-\nnary classiﬁcation problem and classiﬁcation tasks are inde-\npendent of each other. Although many one-vs-all methods\nlike DiSMEC and PPDSparse focus on improving model ef-\nﬁciency, one-vs-all methods still suffer from expensive com-\nputational complexity and large model size. With the cost of\nefﬁciency, these methods can achieve acceptable accuracy.\nTree-based methods Tree-based methods aim to over-\ncome high computational complexity in one-vs-all methods.\nThese methods will construct a hierarchical tree structure by\npartitioning labels like Parabel (Prabhu et al. 2018) or sparse\nfeatures like FastXML (Prabhu and Varma 2014). For Para-\nbel, the label tree is built by label features using balance k-\nmean clustering, each node in the tree contains several clas-\nsiﬁcations to classify whether the text belongs to the children\nnode in inner nodes or the label in leaf nodes. For FastXML,\nFastXML directly optimizes the normalized Discounted Cu-\nmulative Gain (nDCG), and each node of FastXML contains\nseveral binary classiﬁcations to decide which children nodes\nto traverse like Parabel.\nEmbedding-based methods Embedding-based methods\nproject high dimensional label space into a low dimensional\nspace to simplify the XMC problem. And the design of label\ncompression part and label decompression part is signiﬁcant\nfor the performance of these methods. However, no matter\nhow the label compression part is designed, label compres-\nsion will always lose a part of information. It makes these\nmethods achieve worse accuray compared with one-vs-all\nmethods and methods. And some improved embedding-\nbased methods like SLEEC (Bhatia et al. 2015) and An-\nnexML (Tagami 2017) be proposed to solve this problem by\nimproving the label compression and decompression parts,\nbut the problem still remains.\nDeep learning methods With the development of NLP,\ndeep learning methods have shown great improvement in\nXMC which can learn better text representation from raw\ntext. But the main challenge to these methods is how to cou-\nple with millions of labels with limited GPU resources. we\nreview three most representative methods, e.g., XML-CNN\n(Liu et al. 2017), AttentionXML (You et al. 2019) and X-\nTransformers (Chang et al. 2020)..\nXML-CNN is the ﬁrst successful method that showed the\npower of deep learning in XMC. It learns text representation\nby feeding word embedding to CNN networks with end-to-\nend training. For scaling to the datasets with hundreds of\nthousands of labels, XML-CNN proposes a hidden bottle-\nneck layer to project text feature into low dimensional space,\nwhich can reduce the overall model size. But XML-CNN\nonly uses a simple fully connected layer to score all labels\nwith binary entropy loss like simple multi-label classiﬁca-\ntion, which makes it hard to deal with large label sets.\nAfter XML-CNN, AttentionXML shows great success in\nXMC, which overpassed all traditional machine learning\nmethods and proved the superiority of the raw text compare\nto sparse features. Unlike using a simple fully connected\nlayer for label scoring in XML-CNN, AttentionXML adopts\na probabilistic label tree (PLT) that can handle millions of\nlabels. In AttentionXML, it uses RNN networks and atten-\ntion mechanisms to handle raw text with diffferent models\nto each layer of PLT. It needs to train several models for\none dataset. For solving this problem, AttentionXML ini-\ntializes the weight of the current layer model by its upper\nlayer model, which can help the model converge quickly.\nBut It still makes AttentionXML much slow in predicting\nand surffers from big overall model size. In conclusion, At-\ntentionXML is an enlightened method that elegantly com-\nbines PLT with deep learning methods.\nFor X-Transformer, it only uses deep learning models to\nmatch the label clusters for given raw text and ranks these la-\nbels by high dimension linear classiﬁcations with the sparse\nfeature and text representation of deep learning models. X-\nTransformer is the ﬁrst method of using deep transformer\nmodels in XMC. Due to the high computational complexity\nof transformer models, it only ﬁne-tunes transformer mod-\nels as the label clusters matcher, which can not fully exploit\nthe power of transformer models. Although X-Transformer\ncan reach higher accuracy than AttentionXML with the cost\nof high computational complexity and model size, which\nmakes X-Transformer infeasible in XMC applications. Be-\ncause AttentionXML can reach better accuracy with the\nsame computational complexity of X-Transformer by using\nmore models for ensemble.\nFigure 1: An overview of the proposed framework\n.\nMethodology\nProblem Formulation\nGiven a training set {(xi,yi)}N\ni=1 where xi is raw text,\nand yi ∈ {0,1}L is the label of xi represented by L di-\nmensional multi-hot vectors. Our goal is to learn a function\nf(xi) ∈RL which gives scores to all labels, and f needs\nto give high score to l with yil = 1 for xi. We can obtain\ntop-K predicted labels by f(xi). But in many XMC meth-\nods, it only scores the recalled labels to reduce the overall\ncomputational complexity, and the number of these labels in\nthe subset will be much smaller than all labels, which can\nsave much computation time.\nFramework\nThe proposed framework shows in Figure 1. We ﬁrst cluster\nlabels by sparse features of labels. After label clustering, we\nhave a certain number of label clusters, and each label be-\nlongs to one label cluster. Then we employ the transformer\nmodel to embed raw text information into a high dimension\nrepresentation, which is the input of label recalling part and\nlabel ranking part.\nFor label recalling part and label ranking part, with the ad-\nvantage of dynamic negative sampling, we proposed genera-\ntive cooperative networks for XMC. The label recalling part\nis the generator of these networks, which can dynamically\nsample negative labels. The label ranking part is the dis-\ncriminator of these networks, which distinguishes between\nthe negative labels and positive labels. For the cooperation of\nthese networks, the generator assists the discriminator learn-\ning better label representation, and the discriminator helps\nthe generator to consider ﬁne-grained text information. Be-\ncause the discriminator is trained according to speciﬁc label\nrather than label clustering.\nSpeciﬁcally, for the generator, we score every label cluster\nby text representation to obtain top-K label clusters which\nare the labels that we sampled. All positive labels are added\nto this subset in the training stage to make the discriminator\ncan be ﬁne-tuned with generator. For the discriminator, each\nlabel in this label subset will be scored to distinguish the\npositive labels and the negative labels.\nLabel clustering Label clustering is equivalent to two lay-\ners of Probabilistic Label Tree (PLT). Since the deep PLT\nwill harm performance and label clustering is enough to\ncope with extreme numbers of labels, we just use this two-\nlayer PLT with the same constructing methods in Atten-\ntionXML (You et al. 2019).\nMore speciﬁcally, given a maximum number of labels that\neach cluster shas, our goal is to partition labels into K la-\nbel clusters, and the number of labels contained in each la-\nbel cluster is less than sand greater than s/2. To solve this\nproblem, We ﬁrst get each label representation by normal-\nizing the sum of sparse text features with corresponding la-\nbels contain this label. Then, we use balanced k-means (k=2)\nclustering to recursively partition label sets until all label\nsets satisfy the above requirement.\nText representation Transformer models show outstand-\ning performance on a wide array of NLP tasks. In our frame-\nwork, We adopt three pre-trained transformer base mod-\nels: BERT (Devlin et al. 2018), XLNet (Yang et al. 2019)\nand RoBERTa (Liu et al. 2019), which is the same as X-\nTransformer (Chang et al. 2020). But compared to large\ntransformer models(24 layers and 1024 hidden dimension)\nthat X-Transformer uses, we only use base transformer mod-\nels (12 layers and 768 hidden dimension) to reduce compu-\ntational complexity.\nFor input sequence length, the time and space complex-\nity of transformer models will grow exponentially with\nthe growth of text length under self-attention mechanisms\n(Vaswani et al. 2017), which makes transformer models\nhard to deal with long text. In X-Transformer, maximum se-\nquence length is set to 128 for all datasets. However, we set\nmaximum sequence length to 512 for small XMC datasets\nand 128 for large XMC datasets with the advantage of using\nbase models instead of large models.\nFor text embedding of transformer models, different from\nﬁne-tuning transformer models for typical text classiﬁcation\ntasks. In order to make full use of the transformer models in\nXMC, we concatenate the ”[CLS]” token in the hidden state\nof last ﬁve layers as text representation. Let ei ∈Rl be the l\ndimensional hidden state of the ”[CLS]” token in the lasti-th\nlayer of the transformer model. The text is represented by the\nconcatenation of last ﬁve layers e= [e1,..,e 5] ∈R5l. High\ndimensional text representation can enrich text information\nand improve the generalization ability of overall model in\nXMC. And our experiments show it can speed up conver-\ngence and improve the model performance. To avoid over-\nﬁtting, We also use a high rate of dropout to this high dimen-\nsional text representation\nLabel recalling In this part, our goal is not only sam-\npling positive labels, but also negative labels to help the label\nranking part learn. To reduce computational complexity and\nspeed up convergence, we directly sample the label clusters\ninstead of the labels, and use all labels in these clusters as\nthe labels we sample.\nThe generator is a fully connected layer with sigmoid\nG(e) = σ(Wge+ bg) and Greturns K dimensional vector\nrepresentation, which is the scores of all K label clusters.\nWe choose top blabel clusters to generate a subset of labels.\nIn training, all positive labels are added to this subset to force\nteaching the label ranking to distinguish positive and nega-\ntive labels. In predicting, we don’t modify this subset, and\nthis subset may not contain all positive labels.\nFor the generator loss, we don’t calculate loss according\nto the feedback of the discriminator, due to the generator\nis designed to cooperate with the discriminator and make\nmodel easy to convergence. And we can directly calculate\nthe loss by the ground truth. The loss function is described\nas follows:\nLg(G(e),yg) =\nK∑\ni=0\n(1 −yi\ng)(−log(1 −G(e)i))+\nyi\ng(−log(G(e)i)),\n(1)\nwhere yg ∈{0,1}K is the multi-hot representation of label\ncluster for the given text.\nNegative label sampling is a decisive factor for overall\nmodel performance. In AttentionXML (You et al. 2019),\nnegative samples are static, and the model only overﬁts to\ndistinguish speciﬁc negative label samples, which will con-\nstrain the performance. The static negative sampling also\nmakes the model hard to converge, because negative labels\nare very similar to positive labels. We solve this problem by\nemploying the generator with dynamically negative labels\nsampling. For the same training instance, the negative labels\nof this instance is resampled every time by the current gen-\nerator, and negative labels are sampled from easy to difﬁcult\ndistinguishing during the generator ﬁting, which makes the\ndiscriminator converge easily and avoid overﬁtting. The la-\nbel candidates the generator sampled are as follows:\nSg = {li : i∈{i: gc(li) ∈G(e)}}, (2)\nwhere gc is a function to map labels to its clusters and li is\nthe i-th label. In training stage, all positive labels are added\nto Sg.\nLabel ranking Given text representation eand label can-\ndidates Sg, we ﬁrst need to get embeddings of all labels in\nSg which can represent as follows:\nM = [Ei : i∈{i: Sg}] , (3)\nwhere Ei ∈ Rb is the learn-able b dimension embedding\nof i-th label and E ∈RL×b is the overall label embedding\nmatrix which is initialized randomly.\nWe use the same hidden bottleneck layer as XML-CNN\n(Liu et al. 2017) to project text embedding to low dimension.\nThere are two advantages of it:\n• The hidden bottleneck layer makes the overall model size\nsmaller and let the model ﬁt into limited GPU memory.\nThe overall label size Lis usually more than hundreds of\nthousands in XMC. If we remove this layer, this part is\nsize will be O(L×5k), which can take huge GPU mem-\nory. After adding a hidden bottleneck layer, the size will\nbe O((L+ 5k) ×b), and the hyper-parameter bis the di-\nmension of the label embedding, which is much smaller\nthan 5k. According to the size of different datasets, we\ncan set different bto make full use of GPU memory.\n• The hidden bottleneck layer also makes the generator and\nthe discriminator focus on different information of text\nrepresentation. The generator focuses on ﬁne-grained text\ninformation, while the discriminator focuses on coarse-\ngrained text information. And the ﬁnal results will com-\nbine these two types of information.\nThe discriminator can be described as follows:\nD(e,M) = σ(Mσ(Whe+ bh)), (4)\nwhere Wh ∈Rb×5k and bh ∈Rb is the weight of hidden\nbottleneck layer.\nThe object ofD(e,M) is to distinguish positive labels and\nnegative labels that are sampled by the generator. The train-\ning target of D(e,M) is yd where yi\nd = 0 if Si\ng is positive\nlabels and yi\nd = 1 if Si\ng is negative label. Thus the loss of\nthis part is as follows:\nLd(D(e,M),yd) =\nK∑\ni=0\n(1 −yi\nd)(−log(1 −D(e,M)i))+\nyi\nd(−log(D(e,M)i))\n(5)\nThe whole framework of LightXML is shown in Algo-\nrithm 1.\nAlgorithm 1 The proposed framework.\nInput: Training set{X,Y }= {(xi,yi)}N\ni=1, sparse feature\nof training text ˆX;\nOutput: Model;\n1: Construct the label clusters Cby ˆX and Y;\n2: Initialize transformer model T with pre-trained trans-\nformer model;\n3: Initialize discriminator Dbase on C;\n4: Initialize label embedding E, generator G;\n5: while model not converge do\n6: Draw msamples Xbatch and Ybatch from the training\nset {X,Y };\n7: Get text embedding Zby T(Xbatch);\n8: for i=1..mdo\n9: Generate label clusters Sgenerated by G(Zi);\n10: Get negative labels Sneg by Sgenerated and C;\n11: Remove positive labels in Sneg;\n12: end for\n13: Get positive labels Spos according to Ybatch;\n14: for i=1..mdo\n15: Get label embedding M according to Spos and\nSneg;\n16: Generate each label scores by D(Zi,M);\n17: end for\n18: Update parameters of T, Gand Daccording to Eq.1,\nand Eq.5;\n19: end while\n20: return model;\nTraining\nUnlike AttentionXML (You et al. 2019) and X-Transformer\n(Chang et al. 2020), our model can perform end-to-end train-\ning by using generative cooperative networks to handle both\nlabel recalling and label ranking parts, which can reduce\ntraining time and model size. The overall loss function is:\nL= Lg + Ld (6)\nWe directly add the loss of generator part Lg and discrimi-\nnator part Ld as overall loss, and the transformer model that\nused to represent text will update its gradient according to\nboth Lg and Ld, which makes the transformer model learn\nfrom both label recalling and label ranking.\nPrediction\nThe efﬁciency of prediction is essential for XMC applica-\ntions. But deep learning methods reach high accuracy with\nthe huge cost of computational complexity, which makes\nthem infeasible compared to traditional machine learning\nmethods. Although LightXML is based on deep transformer\nmodels, LightXML can make fast prediction in several mil-\nliseconds on the large scale XMC dataset.\nLightXML can also end-to-end predict with raw text. The\nlabel recalling part of LightXML scores all label clusters and\nreturn a subset of all labels containing both positive labels\nand negative labels. The label ranking part scores every label\nin this subset. The ﬁnal scores of labels are the multiplying\nof recalling scores and ranking scores, and we can get top-K\nlabels by these scores.\nExperiments\nDataset\nFive widely-used XMC benchmark datasets are used in our\nexperiments, they are: Eurlex-4K (Mencia and F ¨urnkranz\n2008), Wiki10-31K (Zubiaga 2012), AmazonCat-13K\n(McAuley and Leskovec 2013), Wiki-500K and Amazon-\n670K (McAuley and Leskovec 2013). The detailed informa-\ntion of each dataset is shown in Table 2. The sparse feature\nof text we use for clustering is also contained in datasets.\nEvalutaion Measures\nWe chooseP@kas evaluation metrics, which is widely used\nin XMC and represents percentage of accuracy labels in top\nkscore labels.P@kcan be deﬁned as follows:\nP@k= 1\nk\n∑\ni∈rankk(ˆy)\nyi, (7)\nwhere ˆy is the prediction vector, idenotes the index of the\ni-th highest element in ˆyand y∈{0,1}L.\nBaseline\nWe compared the state-of-the-art and most enlighten-\ning methods including one-vs-all DiSMEC (Babbar and\nSch¨olkopf 2017); label tree based Parabel (Prabhu et al.\n2018), ExtremeText (XT) (Wydmuch et al. 2018), Bonsai\n(Khandagale, Xiao, and Babbar 2019); and deep learning\nbased XML-CNN (Liu et al. 2017), AttentionXML (You\net al. 2019), and X-Transformers (Chang et al. 2020). The\nresults of all baseline are from (You et al. 2019) and (Chang\net al. 2020).\nExperiment Settings\nFor all datasets, we directly use raw texts without prepro-\ncessing, and texts are truncated to the maximum input to-\nkens. We use 0.5 rate of dropout for text representation and\nSW A (stochastic weight averaging) (Izmailov et al. 2018)\nwhich is also used in AttentionXML to avoid overﬁtting.\nLightXML is trained by AdamW (Kingma and Ba 2014)\nwith constant learning rate of 1e-4 and 0.01 weight decay\nfor bias and layer norm weight in model. Automatic Mixed\nPrecision (AMP) is also used to reduce GPU memory usage\nand increase training. Our training stage is end-to-end, and\nthe loss of model is the sum of label recalling loss and label\nranking loss. For datasets with small labels like Eurlex-4k,\nAmazoncat-13k and Wiki10-31k, each label clusters contain\nonly one label and we can get each label scores in label\nrecalling part. For ensemble, we use three different trans-\nformer models for Eurlex-4K, Amazoncat-13K and Wiki10-\n31K, and use three different label clusters with BERT (De-\nvlin et al. 2018) for Wiki-500K and Amazon-670K. Com-\npared to state-of-the-art deep learning methods, all of our\nmodels is trained on single Tesla v100 GPU, and our model\nonly uses less than 16GB of GPU memory for training,\nwhich is much smaller than other methods use. Other hy-\nperparameters is given in Table 1.\nDatasets E B b C L t\nEurlex-4K 20 16 - - 512\nAmazonCat-13K 5 16 - - 512\nWiki-31K 30 16 - - 512\nWiki-500K 10 32 500 60 128\nAmazon-670K 15 16 400 80 128\nTable 1: Hyperparameters of all datasets. E is the number\nof epochs, B is the batch size, b is the dimension of label\nembedding, Cis the number of labels in one label cluster,Lt\nis the maximum length of transformer model is input tokens\nand M is the model size.\nPerformance comparison\nTable 3 shows P@k on the ﬁve datasets. We focus on top\nprediction by varying k at 1, 3 and 5 in P@K which are\nwidely used in XMC. LightXML outperforms all meth-\nods on four datasets. For traditional machine learning\nmethods, DiSMEC has better accuracy compared to these\nmethods (Parabel, Bonsai and XT) with the cost of high\ncomputational complexity, and LightXML has much im-\nprovement on accuracy compared to these methods. For\nX-Transformer, which is also transformer models based\nmethod, LightXML achieve better accuracy on all datasets\nwith much small model size and computational complexity,\nwhich can prove the effectiveness of our method. For Atten-\ntionXML, although AttentionXML has slightly better P@5\nthan LightXML on Wiki-500K, but LightXML achieves\nDatasets Ntrain Ntest D L ¯L ˆL ¯Wtrain ¯Wtest\nEurlex-4K 15,449 3,865 186,104 3,956 5.30 20.79 1248.58 1230.40\nWikil0-31K 14,146 6,616 101,938 30,938 18.64 8.52 2484.30 2425.45\nAmazonCat-13K 1,186,239 306,782 203,882 13,330 5.04 448.57 246.61 245.98\nAmazon-670K 490,449 153,025 135,909 670,091 5.45 3.99 247.33 241.22\nWiki-500K 1,779,881 769,421 2,381,304 501,008 4.75 16.86 808.66 808.56\nTable 2: Detailed datasets statistics. Ntrain is the number of training samples, Ntest is the number of test samples, D is the\ndimension of BOW feature vector, Lis the number of labels, ¯Lis the average number of labels per sample, ˆLis the average\nnumber of samples per label, ¯Wtrain is the average number of words per training sample and ¯Wtest is the average number of\nwords per testing sample.\nDatasets DiSMEC Parabel Bonsai XT XML-CNN AttentionXML X-Transformer LightXML\nEurlex-4K\nP@1 83.21 82.12 82.30 79.17 75.32 87.12 87.22 87.63\nP@3 70.39 68.91 69.55 66.80 60.14 73.99 75.12 75.89\nP@5 58.73 57.89 58.35 56.09 49.21 61.92 62.90 63.36\nAmazonCat-13K\nP@1 93.81 93.02 92.98 92.50 93.26 95.92 96.70 96.77\nP@3 79.08 79.14 79.13 78.12 77.06 82.41 83.85 84.02\nP@5 64.06 64.51 64.46 63.51 61.40 67.31 68.58 68.70\nWiki10-31K\nP@1 84.13 84.19 84.52 83.66 81.41 87.47 88.51 89.45\nP@3 74.72 72.46 73.76 73.28 66.23 78.48 78.71 78.96\nP@5 65.94 63.37 64.69 64.51 56.11 69.37 69.62 69.85\nWiki-500K\nP@1 70.21 68.70 69.26 65.17 - 76.95 77.28 77.78\nP@3 50.57 49.57 49.80 46.32 - 58.42 57.47 58.85\nP@5 39.68 38.64 38.83 36.15 - 46.14 45.31 45.57\nAmazon-670K\nP@1 44.78 44.91 45.58 42.54 33.41 47.58 - 49.10\nP@3 39.72 39.77 40.39 37.93 30.00 42.61 - 43.83\nP@5 36.17 35.98 36.60 34.63 27.42 38.92 - 39.85\nTable 3: Comparisons with different methods. Comparing our model against state-of-the-art XMC methods on Eurlex-4K,\nAmazonCat-13K, Wiki10-31K, Wiki-500K and Amazon-670K. Note that XML-CNN are not scalable on Wiki-500K, and the\nresult of X-Transformer on Amazon-670K has never been reported which is hard to reproduce it limited by our hardware\nconditions.\nmore improvement in P@1 and P@3, and LightXML out-\nperforms AttentionXML on other four datasets.\nPerformance on single model\nWe also examine the single model performance, called\nLightXML-1. Table 4 shows the results of single models on\nAmazon-670K and Wiki-500K. LightXML-1 shows better\naccuracy compare to AttentionXML-1.\nDatasets LightXML-1 AttentionXML-1\nWiki-500K\nP@1 76.19 75.07\nP@3 57.22 56.49\nP@5 44.12 44.41\nAmazon-670K\nP@1 47.14 45.66\nP@3 42.02 40.67\nP@5 38.23 36.94\nTable 4: Performances on single model\nEffect of the dynamic negative sampling\nTo examine the importance of the dynamic negative sam-\npling in negative label sampling, we compare dynamic neg-\native sampling with static negative sampling on Wiki-500K\nand Amazon-670K.\nDatasets D BS S\nWiki-500K\nP@1 76.19 75.30 73.30\nP@3 57.22 56.64 54.60\nP@5 44.12 43.03 42.27\nAmazon-670K\nP@1 47.14 46.27 43.36\nP@3 42.02 41.40 38.55\nP@5 38.23 37.69 34.96\nTable 5: Comparisons with static negative sampling meth-\nods. D is the dynamic negative sampling, BS is the static\nnegative sampling with additional text representation and S\nis the static negative sampling with single text representa-\ntion.\nFor static negative sampling, it’s hard to have a fair com-\nparison with dynamic negative sampling, due to the con-\nstraint of static negative sampling, which needs to train re-\ncalling and ranking in order. So we proposed two version of\nstatic negative sampling: 1) Shas the same model size with\ndynamic negative sampling, and we trained label recalling\nwith the freeze text representation of trained label ranking.\n2) BS has a additional text representation, which can been\nﬁne-tuned in label ranking. And we initialize this with the\ntext representation of trained label recalling.\nThe performance of different negative sampling is shown\nin Table 5. Although both two static negative sampling meth-\nods take longer than dynamic negative sampling methods\nin training time, dynamic negative sampling method still\noutperforms the other two methods. For two static negative\nsampling methods, BS shows better results than Swith ad-\nditional text representation.\nEffect of the multi layers text representation\nThis section analyze how multi layers text representation af-\nfect the model performance, and we choose our model and\nour model with only single last layer of ”[CLS]” token as\ntext representation.\n(a) Wiki-500K\n(b) Amazon-670K\nFigure 2: Effect of the multi layers text representation.\nAs Figure 2 shows, we compare the training loss of multi\nlayers and single layer on Wiki-500K and Amazon-670K. It\ncan be seen that multi layers has low training loss, which\nmeans multi layers text representation can accelerate model\nconvergence, and multi layers can reach same training loss\nof single layer by only using half of total epochs. For ﬁ-\nnal accuracy, multi layers can improve the ﬁnal accuracy of\nP@5 more than 1%.\nComputation Time and Model Size\nComputation time and model size are essential for XMC,\nwhich means XMC is not only a task to pursue high ac-\ncuracy, but also a task to improve efﬁciency. In this sec-\ntion, we compare the computation time and model size\nof LightXML with high performance XMC method At-\ntentionXML. For X-Transformer, it uses large transformer\nmodels and high dimension linear classiﬁcation, which\nmakes it has large model size and high computational com-\nplexity. X-Transformer takes more than 35 hours of training\nwith eight Tesla V100 GPUs on Wiki-500K, and it will need\nmore than one hundred hours for us to reproduce. Due to the\nhard reproducing and bad efﬁciency of the X-Transformer,\nwe don’t compare LightXML with X-Transformer.\nDatasets AttentionXML-1 LightXML-1\nWiki-500K\nTtrain 37.28 34.33\nStest 4.20 3.75\nM 3.11 1.47\nAmazon-670K\nTtrain 26.10 28.75\nStest 8.59 4.09\nM 5.52 1.53\nTable 6: Computation Time and Model Size. Ttrain is the\noverall training hours. Stest is the average time required to\npredict each sample. The unit of Stest is milliseconds per\nsample (ms/sample). M is the model size in GB.\nTabel 6 shows the training time, predicting speed and\nmodel size of AttentionXML-1 and LightXML-1 on Wiki-\n500K and Amazon-670K, and both AttentionXML and\nLightXML uses same hardware with one Tesla V100 GPU.\nLightXML shows signiﬁcant improvements on both predict-\ning speed and model size compare to AttentionXML. For\npredicting speed, LightXML can ﬁnd relevant labels from\nmore than 0.5 million labels in 5 milliseconds with raw text\nas input. For model size, LightXML can reduce 72% model\nsize in Amazon-670K, and 52% on Wiki-500K. For training\ntime, both LightXML and AttentionXML are fast in train-\ning, which can save more than three times training time com-\npare to X-Transformer.\nConclusion\nIn this paper, we proposed a light deep learning model for\nXMC, LightXML, which combines the transformer model\nwith generative cooperative networks. With generative co-\noperative networks, the transformer model can be end-to-\nend ﬁne-tuned in XMC, which makes the transformer model\nlearn powerful text representation. To make LightXMl ro-\nbust in predicting, we also proposed dynamic negative sam-\npling based on these generative cooperative networks. With\nextensive experiments, LightXML shows high efﬁciency on\nlarge scale datasets with the best accuracy comparing to the\ncurrent state-of-the-art methods, which can allow all of our\nexperiments to be performed on the single GPU card with a\nreasonable time. Furthermore, current state-of-the-art deep\nlearning methods have many redundant parameters, which\nwill harm the performance, and LightXML can remain ac-\ncuracy while reducing more than 50% model size than these\nmethods.\nAcknowledgment\nThis work was supported by the National Natural Sci-\nence Foundation of China under Grant Nos. 71901011,\nU1836206, and National Key R&D Program of China un-\nder Grant No. 2019YFA0707204\nReferences\nBabbar, R.; and Sch ¨olkopf, B. 2017. Dismec: Distributed\nsparse machines for extreme multi-label classiﬁcation. In\nProceedings of the Tenth ACM International Conference on\nWeb Search and Data Mining, 721–729.\nBabbar, R.; and Sch ¨olkopf, B. 2019. Data scarcity, robust-\nness and extreme multi-label classiﬁcation. Machine Learn-\ning 108(8-9): 1329–1351.\nBhatia, K.; Jain, H.; Kar, P.; Varma, M.; and Jain, P. 2015.\nSparse local embeddings for extreme multi-label classiﬁca-\ntion. In Advances in neural information processing systems,\n730–738.\nChang, W.-C.; Yu, H.-F.; Zhong, K.; Yang, Y .; and Dhillon,\nI. S. 2020. Taming Pretrained Transformers for Extreme\nMulti-label Text Classiﬁcation. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, 3163–3171.\nDekel, O.; and Shamir, O. 2010. Multiclass-multilabel clas-\nsiﬁcation with more classes than examples. In Proceedings\nof the Thirteenth International Conference on Artiﬁcial In-\ntelligence and Statistics, 137–144.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nIzmailov, P.; Podoprikhin, D.; Garipov, T.; Vetrov, D.;\nand Wilson, A. G. 2018. Averaging weights leads to\nwider optima and better generalization. arXiv preprint\narXiv:1803.05407 .\nKhandagale, S.; Xiao, H.; and Babbar, R. 2019. Bonsai–\nDiverse and Shallow Trees for Extreme Multi-label Classiﬁ-\ncation. arXiv preprint arXiv:1904.08249 .\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\nLiu, J.; Chang, W.-C.; Wu, Y .; and Yang, Y . 2017. Deep\nlearning for extreme multi-label text classiﬁcation. In Pro-\nceedings of the 40th International ACM SIGIR Conference\non Research and Development in Information Retrieval ,\n115–124.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nMcAuley, J.; and Leskovec, J. 2013. Hidden factors and\nhidden topics: understanding rating dimensions with review\ntext. In Proceedings of the 7th ACM conference on Recom-\nmender systems, 165–172.\nMencia, E. L.; and F ¨urnkranz, J. 2008. Efﬁcient pairwise\nmultilabel classiﬁcation for large-scale problems in the le-\ngal domain. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases , 50–65.\nSpringer.\nPrabhu, Y .; Kag, A.; Harsola, S.; Agrawal, R.; and Varma,\nM. 2018. Parabel: Partitioned label trees for extreme classi-\nﬁcation with application to dynamic search advertising. In\nProceedings of the 2018 World Wide Web Conference, 993–\n1002.\nPrabhu, Y .; and Varma, M. 2014. Fastxml: A fast, accurate\nand stable tree-classiﬁer for extreme multi-label learning. In\nProceedings of the 20th ACM SIGKDD international con-\nference on Knowledge discovery and data mining, 263–272.\nTagami, Y . 2017. Annexml: Approximate nearest neighbor\nsearch for extreme multi-label classiﬁcation. In Proceed-\nings of the 23rd ACM SIGKDD international conference on\nknowledge discovery and data mining, 455–464.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWydmuch, M.; Jasinska, K.; Kuznetsov, M.; Busa-Fekete,\nR.; and Dembczynski, K. 2018. A no-regret generaliza-\ntion of hierarchical softmax to extreme multi-label classi-\nﬁcation. In Advances in Neural Information Processing Sys-\ntems, 6355–6366.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In Advances in\nneural information processing systems, 5753–5763.\nYen, I. E.; Huang, X.; Dai, W.; Ravikumar, P.; Dhillon, I.;\nand Xing, E. 2017. Ppdsparse: A parallel primal-dual sparse\nmethod for extreme classiﬁcation. In Proceedings of the\n23rd ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 545–553.\nYen, I. E.-H.; Huang, X.; Ravikumar, P.; Zhong, K.; and\nDhillon, I. 2016. Pd-sparse: A primal and dual sparse ap-\nproach to extreme multiclass and multilabel classiﬁcation.\nIn International Conference on Machine Learning , 3069–\n3077.\nYou, R.; Zhang, Z.; Wang, Z.; Dai, S.; Mamitsuka, H.; and\nZhu, S. 2019. Attentionxml: Label tree-based attention-\naware deep model for high-performance extreme multi-label\ntext classiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, 5820–5830.\nZubiaga, A. 2012. Enhancing navigation on wikipedia with\nsocial tags. arXiv preprint arXiv:1202.5469 .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7380487322807312
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6249490976333618
    },
    {
      "name": "Machine learning",
      "score": 0.5976663827896118
    },
    {
      "name": "Transformer",
      "score": 0.5688513517379761
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.48062652349472046
    },
    {
      "name": "Task (project management)",
      "score": 0.41788309812545776
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4033990800380707
    },
    {
      "name": "Data mining",
      "score": 0.3784008026123047
    },
    {
      "name": "Engineering",
      "score": 0.0625457763671875
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}