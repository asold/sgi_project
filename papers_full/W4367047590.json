{
  "title": "SignBERT+: Hand-Model-Aware Self-Supervised Pre-Training for Sign Language Understanding",
  "url": "https://openalex.org/W4367047590",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5035950233",
      "name": "Hezhen Hu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5057827327",
      "name": "Weichao Zhao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5046805800",
      "name": "Wengang Zhou",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5078141810",
      "name": "Houqiang Li",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6767211374",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6779326418",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W3159461954",
    "https://openalex.org/W3092095626",
    "https://openalex.org/W6758209225",
    "https://openalex.org/W2948242301",
    "https://openalex.org/W2487442924",
    "https://openalex.org/W6782256063",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W6747899497",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3147467731",
    "https://openalex.org/W3034269985",
    "https://openalex.org/W3034765865",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6782324824",
    "https://openalex.org/W3202747033",
    "https://openalex.org/W3196466542",
    "https://openalex.org/W4285191601",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W3133226919",
    "https://openalex.org/W6784631112",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3173290664",
    "https://openalex.org/W2991087169",
    "https://openalex.org/W3092042812",
    "https://openalex.org/W2799020610",
    "https://openalex.org/W4206357202",
    "https://openalex.org/W6797132756",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W6786573146",
    "https://openalex.org/W2895638065",
    "https://openalex.org/W2755802490",
    "https://openalex.org/W6640754710",
    "https://openalex.org/W2950568498",
    "https://openalex.org/W3034442691",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W3108425892",
    "https://openalex.org/W4205517074",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W2807840688",
    "https://openalex.org/W2896487916",
    "https://openalex.org/W2154790436",
    "https://openalex.org/W3045196480",
    "https://openalex.org/W6713392892",
    "https://openalex.org/W2948139159",
    "https://openalex.org/W6756891207",
    "https://openalex.org/W2891726870",
    "https://openalex.org/W2963369114",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W2463640844",
    "https://openalex.org/W3205234797",
    "https://openalex.org/W2746301562",
    "https://openalex.org/W2587277634",
    "https://openalex.org/W3126451397",
    "https://openalex.org/W3193694068",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3170837227",
    "https://openalex.org/W3168702146",
    "https://openalex.org/W2004074725",
    "https://openalex.org/W2020163092",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3081334315",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6740431147",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W2133607347",
    "https://openalex.org/W2972662547",
    "https://openalex.org/W6638273328",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2979577579",
    "https://openalex.org/W3011070051",
    "https://openalex.org/W3107825842",
    "https://openalex.org/W3173262825",
    "https://openalex.org/W2768683308",
    "https://openalex.org/W2227547437",
    "https://openalex.org/W1993962870",
    "https://openalex.org/W2552247836",
    "https://openalex.org/W1990947293",
    "https://openalex.org/W2150457612",
    "https://openalex.org/W2100642335",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W2044235398",
    "https://openalex.org/W2007104354",
    "https://openalex.org/W4385490076",
    "https://openalex.org/W4244078803",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4308663921",
    "https://openalex.org/W3094831145",
    "https://openalex.org/W3126311953",
    "https://openalex.org/W3082302528",
    "https://openalex.org/W4287239859",
    "https://openalex.org/W2903314716",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3169413442",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W4287592659",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3185538031",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W3135189994",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2785325870",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4297748315",
    "https://openalex.org/W3080888993",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2727891399",
    "https://openalex.org/W3109271037",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3093061880",
    "https://openalex.org/W3152253924",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963102968",
    "https://openalex.org/W2403878043"
  ],
  "abstract": "Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 1\nSignBERT+: Hand-model-aware Self-supervised\nPre-training for Sign Language Understanding\nHezhen Hu, Weichao Zhao‚àó, Wengang Zhou, Senior Member, IEEE, and Houqiang Li, Fellow, IEEE\nAbstract‚ÄîHand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for\nsign language understanding (SLU) are prone to over-Ô¨Åtting due to insufÔ¨Åcient sign data resource and suffer limited interpretability. In\nthis paper, we propose the Ô¨Årst self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our\nframework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded\nwith gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we Ô¨Årst perform\nself-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to\nmimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better\ncapture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for\ndownstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks,\ninvolving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results\ndemonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.\nIndex Terms‚ÄîSelf-supervised pre-training, masked modeling strategies, model-aware hand prior, sign language understanding\n!\n1 I NTRODUCTION\nS\nIGN language (SL) serves as a primary communication\ntool for the deaf community. It is a visual language with\nits unique grammar and lexicon, which is non-trivial for\nthe hearing people to master. To facilitate barrier-free com-\nmunication between hearing and deaf people, automatic\nvisual sign language understanding, as a topic with broad\nsocial inÔ¨Çuence, has been widely studied. Visual sign lan-\nguage understanding contains three main tasks, including\nisolated and continuous sign language recognition (SLR)\nand sign language translation (SLT). Isolated SLR focuses on\nword level recognition, which is essentially a Ô¨Åne-grained\nclassiÔ¨Åcation task. Differently, continuous SLR targets at\nrecognizing the sign gloss sequence with its corresponding\noccurring order, which needs to learn the sequence cor-\nrespondence across the visual and textual domains. SLT\nintends to further generate spoken language translations,\nwhich emphasizes natural linguistic expression. These three\ntasks are all important for sign language understanding and\nbring challenges from different perspectives.\nHand gesture plays a dominant role during the meaning\nexpression of sign language. Intrinsically, hand occupies a\nrelatively small spatial size, exhibiting uniform appearance\nand self-occlusion among joints. During SL expression, it\nusually occurs over complex backgrounds and presents\n‚Ä¢ This work was supported by the National Natural Science Foundation of\nChina under Contract U20A20183 and 62021001. It was also supported\nby GPU cluster built by MCC Lab of Information Science and Technology\nInstitution, USTC, and the Supercomputing Center of the USTC.\n‚Ä¢ Hezhen Hu, Weichao Zhao, Wengang Zhou and Houqiang Li are with the\nDepartment of Electronic Engineering and Information Science of Electri-\ncal and Computer Engineering, University of Science and Technology of\nChina, Hefei, Anhui, 230026.\nE-mail: {alexhu, saruka}@mail.ustc.edu.cn, {zhwg, lihq}@ustc.edu.cn\n‚àóEqual contribution with the Ô¨Årst author.\nCorresponding authors: Wengang Zhou and Houqiang Li.\nfast motion. These characteristics lead to difÔ¨Åculty in hand\nrepresentation learning. Current deep-learning-based meth-\nods adaptively learn hand feature representations from the\ncropped RGB sequence. Meanwhile, considering the highly-\narticulated characteristic of hand, some methods propose to\nutilize pose as the hand representation. Pose is a compact\nand semantic representation, which is robust to appear-\nance change and brings potential computation efÔ¨Åciency.\nHowever, current pose-based methods utilize the poses\nextracted from off-the-shelf pose detectors, which usually\nsuffer failure detection due to the motion blur and complex\nbackgrounds, etc. Therefore, their performances usually lag\nlargely behind the RGB-based counterparts. Besides, the\naforementioned methods all follow a data-driven paradigm\nand suffer over-Ô¨Åtting due to limited sign data resource and\ninsufÔ¨Åcient interpretability.\nMeanwhile, the effectiveness of pre-training has been\nvalidated in computer vision (CV) and natural language\nprocessing (NLP) tasks. Recent advances in NLP are largely\nderived from self-supervised pre-training techniques on\nlarge text corpus [1], [2], [3]. Among them, BERT [2] is\none of the most popular methods for its simplicity and\neffectiveness. Its success is largely attributed to the strong\nTransformer backbone [4] and well-designed pre-training\nstrategies to model the context inherent in the text corpus.\nBy adding a simple head (an MLP) on top for Ô¨Åne-tuning,\nit achieves notable performance gains in many downstream\ntasks, especially those with limited data resource. Notably,\nnatural language is represented by a 1D sequence of text\nwords which are characterised with well-deÔ¨Åned semantic\nmeaning. However, sign video is a kind of 3D data with\ncomplex spatial-temporal context, and it is non-trivial to\nanalogically deÔ¨Åne visual word or unit with clariÔ¨Åed seman-\ntics. Therefore, it remains a hard challenge to leverage the\nsuccess of BERT to video-based sign language understand-\narXiv:2305.04868v1  [cs.CV]  8 May 2023\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 2\nEmbedding Layer\nTransformer Encoder\nHand-Model-Aware Decoder\nMask\nSelf-Supervised Pre-Training Downstream Fine-Tuning\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚ë†Isolated SLR\n‚ë°Continuous SLR\n‚ë¢Sign Language Translation\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nEmbedding Layer\nTransformer Encoder\nTask-specific Prediction Head\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nRecons.\nDownstream Task Overview\nDabei kann es vor allem richtung s√ºden zum teil kr√§ftig schneien.\n(It can snow heavily especially towards the south.)\nBESONDERS\n(especially)\nSUED\n(south)\nKOMMEN\n(come)\nKOENNEN\n(can)\nSCHNEE\n(snow)\nSTARK\n(heavily)\nSign Language Video\nSign Language Glosses\nSpoken Language Translation\n‚ë†Isolated SLR;  ‚ë°Continuous SLR;   ‚ë¢Sign Language Translation \n‚ë†\n‚ë°\n‚ë¢\nDownstream Task Prediction\nFig. 1. The overview of our method and sign language understanding tasks (isolated SLR, continuous SLR and SLT).\ning.\nTo tackle the above-mentioned issues, we develop a\nself-supervised pre-trainable framework with model-aware\nhand prior incorporated, namely SignBERT+, as shown in\nFigure 1. Considering the dominance of hand during SL\nexpression, we utilize the compact and expressive hand\npose as a visual token in a frame-by-frame manner. Then, we\ncarefully depict it with gesture state and spatial-temporal\nglobal position information. SignBERT+ Ô¨Årst performs self-\nsupervised pre-training on a large volume of hand pose\ndata, which is derived from an off-the-shelf detector on\nsign videos. Inspired by the success of BERT [2], we pre-\ntrain the encoder-decoder backbone via reconstructing the\nmasked visual tokens from the corrupted input sequence,\nwhich enforces the framework to capture the hierarchical\ncontext in the sign language domain. Considering the noisy\ncharacteristic of detected hand pose data, we carefully de-\nsign multi-scale masking strategies, including joint, frame\nand clip levels. Meanwhile, to better mine the context in\nthe sign video domain, we further incorporate hand prior\nin a model-aware method. After pre-training, we carefully\ndesign simple yet effective task-speciÔ¨Åc prediction heads,\nwhich are jointly Ô¨Åne-tuned with the pre-trained SignBERT+\nencoder to adapt to downstream tasks.\nIn summary, our contributions are three-fold as follows,\n‚Ä¢ To our best knowledge, we propose the Ô¨Årst model-\naware pre-trainable framework, namely SignBERT+.\nIt performs self-supervised pre-training on a large\nvolume of sign pose data, followed by Ô¨Åne-tuning to\nachieve better performance on multiple downstream\ntasks.\n‚Ä¢ To better model the hierarchical context underneath\nthe sign data during pre-training, we design multiple\nmasked modeling strategies ranging from joint to\nclip level, in coordination with incorporated model-\naware hand prior and spatial-temporal position en-\ncoding. For diverse downstream tasks, we design\nsimple yet effective task-speciÔ¨Åc prediction heads on\ntop of the pre-trained SignBERT+ encoder.\n‚Ä¢ We perform extensive experiments to validate the\nfeasibility and effectiveness of our framework. Ex-\nperimental results demonstrate that our method\nachieves new state-of-the-art performance on video-\nbased sign language understanding tasks, including\nisolated SLR, continuous SLR and SLT.\nThis work is an extension of the conference paper [5]\nwith improvement in a number of aspects. 1) Considering\nthe characteristics of sign language, we further introduce\nspatial-temporal global position encoding into embedding,\nalong with the masked clip modeling for modeling temporal\ndynamics. Those new techniques further bring a notable\nperformance gain. 2) We extend the original framework to\ntwo more downstream tasks in video-based sign language\nunderstanding, i.e., continuous SLR and SLT. To this end, we\ndesign simple yet effective task-speciÔ¨Åc prediction heads.\nBesides, we also provide efÔ¨Åcient fusion strategies with the\nRGB modality. Our newly designed framework achieves\nstate-of-the-art performance on all the downstream tasks.\n3) We present more comprehensive discussion on related\nworks and make deep analysis on different components of\nour method to highlight the important ingredients. Besides,\nwe add discussions on future works and broader impact.\n2 R ELATED WORK\nIn this section, we Ô¨Årst give a literature review for video-\nbased sign language understanding. Then we present an\noverview of pre-training strategies. Finally, we introduce\nrelated hand modeling techniques.\n2.1 Video-based Sign Language Understanding\nVideo-based sign language understanding has made re-\nmarkable progress [6], [7], [8], [9]. Generally, it contains\nthree main tasks, including isolated SLR, continuous SLR\nand SLT. These tasks emphasize different aspects, bringing\ntheir speciÔ¨Åc challenges to resolve.\nIsolated sign language recognition.Isolated SLR aims to\nrecognize at the word level, which is essentially a Ô¨Åne-\ngrained classiÔ¨Åcation problem. This task poses a challenge\non learning discriminative visual representation [7], [10],\n[11], [12], [13]. Early works utilize hand-crafted features,\ne.g. HOG [14] and SIFT [10], to represent hand shape, ori-\nentation and motion. Recently, researchers have resorted to\ndeep learning techniques, which adaptively extract features\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 3\nfrom the full video sequence. Based on the input modality,\nthese works can be divided into RGB-based and pose-based\nmethods. RGB-based methods usually adopt Convolutional\nNeural Networks (CNNs) as the backbone. For instance,\nKoller et al. [15] utilize 2D-CNNs with LSTM to sequen-\ntially model the spatial and temporal representations. Some\nother works utilize 3D-CNNs for modeling spatial-temporal\ndependency [7], [13], [16], [17], [18].\nFor the pose-based counterpart, there exist different\nbackbones for feature extraction, including CNNs [13], [19]\nand RNNs [20], [21], [22], etc. Recently, considering its\nwell-structured nature, more and more works have utilized\ngraph convolutional networks (GCNs), which exhibit both\nefÔ¨Åciency and effectiveness [20], [22], [23]. As a represen-\ntative work, ST-GCN [24] organizes the pose sequence as a\npre-deÔ¨Åned graph and adopts GCNs to perform recognition.\nBesides, Tunga et al. further combines Transformer without\npre-training for isolated SLR [23].\nContinuous sign language recognition.It aims to map the\nsign video to the gloss sequence in the same presenting\norder. In this task, the transitions between sign glosses may\ncome with temporal variants, and the sign video usually\nlacks the frame-level gloss annotation. Therefore, it raises\na new challenge on the sequence correspondence learning\nbetween the visual sign representation to the sign glosses. To\nthis end, Koller et al. [25], [26] exploit the integration of 2D-\nCNNs and Hidden Markov Models (HMMs) for modeling\ntransitions. Connectionist Temporal ClassiÔ¨Åcation (CTC) is\na differentiable cost function, which is able to deal with\ntwo unsegmented sequences without precise alignment. It\nusually works with Recurrent Neural Networks (RNNs),\ne.g. BLSTM [27] and GRU [28], and Transformer [4] for\nsequential learning. CTC-based methods make end-to-end\noptimization possible and become the mainstream for its\ncompetitive performance [8], [9], [29], [30], [31]. However,\nthese methods are prone to over-Ô¨Åtting due to limited data\nresources. To tackle this issue, DNF [29] utilizes the itera-\ntive optimization strategy for better feature representation.\nZhou et al. [32] boosts the visual encoder with the partially\nmasked videos under the supervised classiÔ¨Åcation task.\nCMA [30] proposes cross modality augmentation, which\nleverages the pseudo video-text pairs to boost recognition\nperformance. VAC [8] proposes visual alignment constraint\nto enhance the feature extractor.\nSign language translation. This task intends to generate\nthe spoken language translations. It is mainly different from\ncontinuous SLR in the aspect of sequential learning, due to\ndifferent grammar and word order between sign language\nand spoken language [33], [34]. NSLT [33] Ô¨Årst explores this\ntask with an attention-based encoder-decoder and proposes\nRWTH-PhoenixT dataset. This dataset provides both sign\ngloss and translation annotation, and becomes the most\npopular benchmark. Camgoz et al. [9] leverage the strong\nmodeling capability of Transformer into sequential learning.\nTSPNet [34] explores the temporal semantic structures for\nmore discriminative features. STMC [35] fuses information\nfrom multi-cue streams to boost performance. SignBT [36]\nutilizes external text corpus for performance improvement.\nIn this work, we aim to leverage a large volume of sign\ndata via pre-training to beneÔ¨Åt three main sign language\nunderstanding tasks.\n2.2 Pre-Training Strategy\nPre-training, as a common strategy in CV and NLP , aims\nto learn generic representation from massive labeled or\nunlabeled data, which beneÔ¨Åts downstream tasks with\nmarginal Ô¨Åne-tuning cost. For fully supervised pre-training,\nit is common for CV tasks to Ô¨Årst pre-train CNNs under\nlabeled classiÔ¨Åcation benchmarks, e.g. ImageNet [37] and\nKinetics [38], etc. However, given the labeling cost, more and\nmore works turn to self-supervised learning from a large\nvolume of unlabeled data, which is readily available from\nthe Web [39], [40]. Self-supervised learning aims to model\nthe joint probability distribution inherent in data, which is\nbeneÔ¨Åcial to address the following discriminative learning\ntask.\nPioneering works subtly design pretext tasks to perform\nself-supervised pre-training [41], [42], [43], [44], [45], [46],\n[47]. These tasks include predicting colorization [41], rota-\ntion [43], transformation [46] and frame / clip orders [47],\n[48], etc. Recently, some works focus on contrastive learning\nfor pre-training [49], [50], [51]. Typically, it aims to pull\nthe representation of similar instances closer, while pushing\naway negative instances. To obtain informative negative\ninstances for better optimization, some works utilize the\ntechniques like memory banks [49] and large batch size [52].\nThere also exist works further eliminating the requirement\nof negative samples [51], [53].\nAnother interesting strand is the generative self-\nsupervised pre-training, which usually involves training the\nencoder via the reconstruction task. In NLP , one milestone of\npre-training is BERT [2]. BERT is built on the strong Trans-\nformer backbone with masked language modeling (MLM)\nas one of its pre-training tasks. MLM attempts to predict\nthe masked words by leveraging the context cues from the\nremaining tokens. Similar to BERT, some works also adopt\nMLM during pre-training, such as GPT [1], XLNet [3] and\nRoBERTa [54], etc. These pre-training methods generalize\nwell and bring notable performance gains on the down-\nstream tasks. Motivated by the success in NLP , some works\nattempt to leverage the idea of BERT into CV tasks [55], [56],\n[57], [58], [59], [60], which mainly focus on the RGB modal-\nity. BEiT [59] utilizes the discrete tokenized image patches\nas pseudo labels and performs masked modeling similar\nto BERT. MAE [60] directly works in the continuous space,\ni.e., masking and reconstructing the pixel values. However,\nBEiT and MAE only focus on image-based tasks. Actually,\nit is non-trivial to leverage BERT‚Äôs success to video-based\nsign language understanding tasks, which involves special\ndesign of the pretext task and framework architecture.\nPre-training in sign language.Albanie et al. [13] propose\nto perform supervised pre-training on a large-scale anno-\ntated dataset. Li et al. [7] boost isolated SLR via a domain-\ninvariant feature descriptor, which leverages the knowledge\nfrom external subtitled news sign video. To our best knowl-\nedge, there exists no self-supervised pre-training works in\nthe sign language domain.\n2.3 Hand Modeling Techniques\nHand modeling aims to depict the hand with more ex-\npressiveness. Current modeling techniques include sum-\nof-Gaussians [61], shape primitives [62], [63] and sphere-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 4\nHand Pose Seq.\nGesture State Extractor\nGesture State Embed.\nTransformer Encoder\nSpatial-Temporal PE\n1\n+ + +\nHand-Model-Aware Decoder\nMask\nSelf-Supervised Pre-Training Downstream Fine-Tuning\nHand Pose Seq.\nMANO\n‚Ä¶ùúÉùúÉ\nùõΩùõΩ\nùëêùëê\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nLatent Semantic Extractor\nCamera Projection\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nHand-Model-Aware Decoder\nMask Modeling Strategy\nArchitecture Details\nJoint\nFrame\n‚Ä¶\n‚Ä¶\n‚Ä¶ ‚Ä¶\n1 2\n‚Ä¶\n‚Ä¶\nClip\n ‚Ä¶\nk frames\n‚ë†Isolated SLR\n‚ë°Continuous SLR\n‚ë¢Sign Language Translation\n‚Ä¶\n‚Ä¶\n+ +\n2 3 T\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nGesture State Extractor\nTransformer Encoder\n1\n+ + +\nTask-specific Prediction Head\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n+ +\n2 3 T\n‚Ä¶\n‚Ä¶\nRecons.\nT-1 T-1\nDownstream Task Prediction\nFig. 2. Illustration of our SignBERT+ framework details, which contains self-supervised pre-training and Ô¨Åne-tuning for the downstream tasks. We\norganize the pre-extracted 2D poses of both hands as the visual token sequence. For each token, it is embedded with gesture state and spatial-\ntemporal position encoding. During self-supervised pre-training, multi-level masked modeling strategies work with incorporated model-aware hand\nprior, in order to better capture the hierarchical context in the sign domain. Given the downstream-task diversity, we design the task-speciÔ¨Åc\nprediction head and Ô¨Åne-tune it with the pre-trained SignBERT+ encoder.\nmeshes [64], etc. To better reconstruct the hand shape, Ia-\nson et al. [65] deÔ¨Åne scaling terms on bone lengths. Notably,\nsome works [66], [67], [68] attempt to learn hand shape\nvariation with Linear Blend Skinning (LBS) [69]. Among\nthem, MANO [68] becomes the most popular one for its\nwide applications [70], [71], [72], [73]. As a statistical model,\nit learns from a large variety of high-quality hand scans and\nrepresents the geometric changes in the low-dimensional\npose and shape space. With this capability, we adopt it as\na constraint in the decoder to incorporate prior.\n3 O UR APPROACH\nAs shown in Figure 2, our framework contains two stages,\ni.e., self-supervised pre-training and downstream task Ô¨Åne-\ntuning. During sign expression, both hands are involved\nto act as a dominant role. Therefore, we focus on them\nto build the visual token in a frame-wise manner. For\neach visual token, we embed the gesture state and global\nspatial-temporal position information. During pre-training,\nthe whole framework works in a self-supervised paradigm\nby reconstructing the masked visual tokens from the cor-\nrupted input sequence. Jointly with the multi-level masking\nstrategies, we incorporate hand prior to better capture hi-\nerarchical context in the sign domain. Then, we Ô¨Åne-tune\nthe pre-trained SignBERT+ encoder (embedding layer and\nTransformer encoder) with the designed prediction heads\nfor downstream tasks.\nIn the following, we Ô¨Årst introduce the framework archi-\ntecture. After that, we elaborate our pre-training strategy.\nFinally, we discuss the Ô¨Åne-tuning schemes for downstream\ntasks.\n3.1 Framework Architecture\nOur framework contains four main components, i.e., input\nembedding layer, Transformer encoder, hand-model-aware\ndecoder and prediction head.\n3.1.1 Input Embedding Layer\nGiven the dominant role of hand during sign language, we\ncarefully design an embedding layer to capture cues from\nboth hands. It extracts gesture state and spatial-temporal\nposition information from the hand pose sequence in a\nframe-wise manner, which are elaborated in the follow\nparagraphs.\nGesture state embedding. Given its well-structured\ncharacteristics, we Ô¨Årst organize the input 2D hand pose\nÀúJt at frame t as an undirected spatial graph. This graph\nis constructed with the node V and edge E set. The node\nset includes all hand joints, i.e., 21 joints per hand, while the\nedge set contains their physical and symmetrical connection.\nThen, the hand pose sequence is processed by a spectral-\nbased GCN [24], [74], which hierarchically performs graph\nconvolution and graph pooling for gesture state embedding.\nThe graph convolution is formulated as follows,\nfout =\n‚àë\ni\nD\n‚àí1\n2\ni AiD\n1\n2\ni finWi, (1)\nwhere fin and fout are the corresponding input and output\nfeatures, respectively. i indicates the type of neighbors for\neach node. Wi denotes the convolution weight and Ai is\nthe dismantled matrix indicating the edge connection. For\ngraph pooling, we Ô¨Årst cluster the original 21 joint nodes of\neach hand into 6 subsets corresponding to 5 Ô¨Ångers and 1\npalm. Then we perform max-pooling on the nodes in each\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 5\nsubset, leading to 6 nodes. Finally, these nodes are again\nmax-pooled into one and both hands are involved in the\nframe-level gesture state embedding fp,t.\nSpatial-temporal position encoding.Besides the gesture\nstate, hand spatial trajectory and temporal information also\nmatter in video-based sign language understanding. We\ndepict the hand global position in the normalized 2D space\nby introducing the arm joints of both sides. These joints\nare also processed by GCN [24], [74] to extract the frame-\nlevel spatial embedding fs,t. Since the Transformer layers\nprocess the sequence in an order-agnostic way, we add\ntemporal information into the input embedding fe,t, which\nis implemented by the position encoding technique in [4].\n3.1.2 Transformer Encoder\nThe embedded input sequence is fed into the Transformer\nencoder [4] for the latent semantic representation. Its basic\nlayer mainly contains two components, i.e., a multi-head\nself-attention module and a feed-forward network. For each\nlayer, its output retains the same size with the input. The\nwhole encoder is formulated as follows,\nF0 = {fp,t + fs,t + fe,t}T\nt=1,\nÀúFi = L(M(Fi‚àí1) + Fi‚àí1),\nFi = L(C(ÀúFi) + ÀúFi),\n(2)\nwhere i denotes the i-th layer of the Transformer encoder.\nThe whole encoder contains totally N layers. M(¬∑), C(¬∑) and\nL(¬∑) represent the multi-head self-attention, feed-forward\nnetwork and layer normalization, respectively. Fi denotes\nthe output feature from the i-th layer.\n3.1.3 Hand-model-aware Decoder\nTo achieve the reconstruction target during pre-training,\nthe decoder transforms the latent feature back to the pose\nsequence. The decoder works in a model-aware method to\nincorporate prior, which aims to guide the encoder better\ncapturing generic representations in the sign language do-\nmain. SpeciÔ¨Åcally, the latent feature is Ô¨Årst processed by a\nfully-connected layer, which extracts the low-dimensional\nsemantic embeddings depicting the hand status, i.e., hand\npose Œ∏and shape Œ≤, and the camera parameter caligning\nthe image plane, which is formulated as follows,\nFla = {Œ∏,Œ≤,cr,co,cs}T\nt=1 = D(FN), (3)\nwhere D(¬∑) denotes the fully-connected layer.Œ∏and Œ≤‚ààR10\nare the hand pose and shape embeddings for the following\nMANO model, respectively. cr ‚àà R3√ó3, co ‚àà R2, and\ncs ‚àà R are parameters of the weak-perspective camera,\nrepresenting the rotation, translation and scale, respectively.\nThen the MANO model [68] incorporates hand prior and\ndecodes the estimated hand embedding. SpeciÔ¨Åcally, the\ndecoding process is fully-differentiable, which transforms\nthe hand embedding (Œ∏and Œ≤) to the dense triangular hand\nmesh M ‚ààRNv√ó3 (Nv =778 vertices and Nf =1538 faces)\nas follows,\nM(Œ≤,Œ∏) = W(T(Œ≤,Œ∏),J(Œ≤),Œ∏,W), (4)\nT(Œ≤,Œ∏) = ¬ØT + BS(Œ≤) + BP(Œ∏), (5)\nwhere BS(¬∑) and BP(¬∑) represent shape and pose blend\nfunctions, respectively. W is a set of blend weights. The\nhand template ¬ØT is Ô¨Årst posed and skinned based on the\npose and shape corrective blend shapes, i.e., BP(Œ∏) and\nBS(Œ≤). Then the mesh M is generated by rotating each part\naround joints J(Œ≤) based on the linear skinning function\nW(¬∑) [75]. Besides, the sparse joint representation ÀúJ3D is\nalso derived from the mesh. To keep consistent with the\ninput pose format, we further add 5 extra Ô¨Ångertip joints by\nselecting vertices with the index of 333, 443, 555, 678 and\n734. Finally, ÀúJ3D is mapped back to the same 2D plane as\nthe input pose based on the estimated camera parameter as\nfollows,\nÀúJ2D = cs\n‚àè\n(cr ÀúJ3D) + co, (6)\nwhere ‚àè(¬∑) denotes the orthographic projection.\n3.1.4 Prediction Head\nGiven the large diversities among downstream tasks, we\ndesign simple yet effective prediction heads for each task\nin Figure 3. In Section 3.3, we will introduce them in detail\nalong with the task-speciÔ¨Åc Ô¨Åne-tuning settings.\n3.2 Pre-Training SignBERT+\nIn this section, we elaborate our pre-training paradigm. Pre-\ntraining is performed via reconstructing the masked visual\ntokens from the corrupted input sequence, which aims to\nexploit hierarchical context on a large volume of sign pose\ndata. Different from the original BERT working on discrete\nword space, we attempt to pre-train on continuous pose\nspace. Therefore, it raises new issues to resolve, including\nthe design of the masking strategies and objective functions.\n3.2.1 Masking Strategy\nConsidering the noise of the detected input hand pose, the\nmasking strategy needs to be carefully redesigned. Given\nthe hand pose sequence, we Ô¨Årst randomly choose a portion\nR of all tokens. For the chosen token, one of the following\noperations is applied with the equal probability,i.e., masked\njoint modeling, masked frame modeling, masked clip mod-\neling and identity modeling.\nMasked joint modeling.This strategy mimics the fail-\nure cases of pose detectors on some joints. For a chosen\ntoken, we randomly choose m joints. Two operations are\nperformed on these chosen joints with the equal probability,\ni.e., zero masking (masking the coordinates of joints with\nzeros) or random spatial disturbance. This modeling aims\nto guide the framework to infer the gesture state from the\nremaining joints, thus capturing the context at the joint level.\nMasked frame modeling.It aims to deal with the failure\ncase on the whole frame pose, which is often caused by\ncomplex backgrounds. The chosen token is directly zero\nmasked. This strategy enforces the framework to reconstruct\nthe token by leveraging the observation from the remaining\nframes and the other hand. In this way, the temporal con-\ntext for each hand and mutual context between hands are\ncaptured.\nMasked clip modeling.Motion blur, as a factor not to\nbe overlooked, usually causes pose detection failure on a\nvideo clip. To deal with this situation, masked clip modeling\nis designed. We randomly choose k temporally continuous\ntokens, where k ranges from 2 to K. The chosen k tokens\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 6\nare all zero-masked. In order to reconstruct them, the frame-\nwork needs to capture the temporal dynamics by leveraging\nthe motion pattern of existing frames.\nIdentity modeling. Similar to BERT [2], identity mod-\neling directly feeds the unchanged tokens into the frame-\nwork. It is indispensable for the framework to learn identity\nsemantic encoding on those unmasked tokens.\n3.2.2 Objective Functions\nDuring pre-training, its objective is to maximize the likeli-\nhood of the joint probability distribution to reconstruct the\nhand pose sequence. To achieve the reconstruction target,\nthe classiÔ¨Åcation objective in the original BERT is substan-\ntially changed into regression. To this end, we design the\nobjective function as follows,\nL= Lrec + ŒªLreg, (7)\nwhere Lrec and Lreg are reconstruction and regularization\nloss terms, respectively. Œªdenotes the weighting factor. We\nonly include the corresponding output of the masked tokens\nduring the loss calculation.\nReconstruction loss Lrec. Since the utilized pose usu-\nally contains noise due to failure detection, we utilize the\ndetection conÔ¨Ådence score as the Ô¨Ålter to eliminate these\ninÔ¨Çuences. The reconstruction loss is calculated as follows,\nLrec =\n‚àë\nt,j\n1(s(t,j) >= œµ)s(t,j)\nÓµπÓµπÓµπÀúJ2D(t,j) ‚àíJ2D(t,j)\nÓµπÓµπÓµπ\n1\n, (8)\nwhere 1(¬∑) denotes the indicator function, and s(t,j) de-\nnotes the detection conÔ¨Ådence score of the J2D with joint j\nat time t. The joints with the conÔ¨Ådence lower than œµare not\nincluded in the loss calculation.\nRegularization loss Lreg. To ensure this decoder work-\ning properly, the regularization loss is added. It is imple-\nmented by constraining the magnitude and derivative of\nthe MANO input, which is responsible for generating the\nplausible mesh and keeping the signer identity unchanged.\nThis loss term is computed as follows,\nLreg =\n‚àë\nt\n(‚à•Œ∏t‚à•2\n2 + wŒ≤‚à•Œ≤t‚à•2\n2 + wŒ¥‚à•Œ≤t ‚àíŒ≤t‚àí1‚à•2\n2), (9)\nwhere wŒ≤ and wŒ¥ denote the weighting factors.\n3.3 Fine-Tuning SignBERT+\nAfter pre-training SignBERT+ for generic visual represen-\ntation in sign language, it is relatively simple to Ô¨Åne-tune\nit for various downstream tasks. During Ô¨Åne-tuning, the\ntask-speciÔ¨Åc prediction head is added on top of the pre-\ntrained SignBERT+ encoder, as illustrated in Figure 3. The\nframework is supervised under the task-speciÔ¨Åc loss.\nSince only the hand pose modality is insufÔ¨Åcient to con-\nvey the full meaning of sign language, we further provide\nthe task-speciÔ¨Åc fusion strategy with the method based on\nthe full RGB frames. For clarity, the baseline RGB method\nutilized for fusion will be marked in the experiment section.\nBesides, we denote our vanilla and fused framework as\nOurs and Ours (+ R), respectively.\nIsolated SLR\nSignBERT+ Encoder\nTemporal Merging \nVisual Token Sequence\n‚Ä¶\nContinuous SLR\nSign Gloss Sequence\nSignBERT+ Encoder\nTemporal Pooling\nVisual Token Sequence\n‚Ä¶\n‚Ä¶\nSLT\nSignBERT+ Encoder\nSemantic Modulator\nVisual Token Sequence\n‚Ä¶\n‚Ä¶\nDecoder\nSpoken Translation\nCTC Decoder\n‚Ä¶\n‚Ä¶\n‚Ä¶\nIsolated Word\nClassifier\n‚Ä¶\nFig. 3. Illustration of the settings on three downstream tasks, i.e., iso-\nlated SLR, continuous SLR and SLT. The box in purple denotes our\ndesigned task-speciÔ¨Åc prediction head. It is Ô¨Åne-tuned with the pre-\ntrained SignBERT+ encoder.\n3.3.1 Isolated Sign Language Recognition\nIsolated SLR is a Ô¨Åne-grained classiÔ¨Åcation problem, which\ncategorizes a sign video to the corresponding isolated word.\nFor this task, the designed prediction head consists of a\ntemporal merging module and a classiÔ¨Åer. The former mod-\nule utilizes a simple attention mechanism to highlight the\ndiscriminative cues in certain frames during the merging\nprocess as follows,\no= Softmax(FW + b) ¬∑F, (10)\nwhere F and o denote the input feature sequence and\nmerged feature, respectively. Then the merged feature o\nis passed through a classiÔ¨Åer (MLP and softmax layer)\nto output the probability matrix. Since isolated SLR is a\nclassiÔ¨Åcation problem, we utilize the cross-entropy loss to\nsupervise the Ô¨Åne-tuning process. We use the simple late\nfusion strategy with the RGB method, which directly sums\ntheir prediction scores and chooses the class with the highest\nscore as the Ô¨Ånal recognition result.\n3.3.2 Continuous Sign Language Recognition\nContinuous SLR aims to recognize the gloss sequence g\nin the same presenting order as the sign actions in the\ninput sign video V with T frames. The prediction head for\nthis task contains a temporal pooling module and a CTC\ndecoder. The temporal pooling module aggregates frame-\nlevel visual features to the clip level, which outputs the one-\nquarter temporal length of the input. Then it is fed into the\nconnectionist temporal classiÔ¨Åcation (CTC) decoder to deal\nwith the mapping between two unsegmented sequences\nwithout explicit alignment.\nThe objective of CTC is to maximize the posterior prob-\nability over all alignments from the source to the target. It\nextends the vocabulary with a blank label to cover the cases\nof transition and silence. Denote each alignment path of\nthe input sequence as œÄ = {œÄt|T\nt=1}with T as temporal\nduration. Under the time independence assumption, its\nprobability is computed as follows,\np(œÄ|V) =\nT‚àè\nt=1\np(œÄt|V). (11)\nTypically, there exists many-to-one mapping from multiple\ninput sequences to one target, which is achieved by remov-\ning all blanks and repetition. In this way, we calculate the\nconditional probability of the target gloss sequence g by\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 7\nsumming the probabilities of all possible mapping paths as\nfollows,\np(g|V) =\n‚àë\nœÄ‚ààB‚àí1(s)\np(œÄ|V), (12)\nwhere B(¬∑) denotes the many-to-one mapping function.\nB‚àí1(¬∑) is the inverse mapping of B(¬∑). During training, the\nobjective is deÔ¨Åned by the negative log probability ofp(g|V)\nas follows,\nLCSLR = ‚àíln p(g|V). (13)\nDuring inference, the CTC decoder obtains a series of sen-\ntences via beam search and chooses the one with the highest\ndecoding probability as the Ô¨Ånal prediction.\nFor fusion, similar to [76], we Ô¨Årst concatenate the en-\ncoded feature from the RGB baseline and our method. Then\nwe utilize a BLSTM sequential model and a CTC decoder to\nmap the merged feature to the gloss sequence.\n3.3.3 Sign Language Translation\nGiven the input sign video V with T frames, SLT aims\nto generate the spoken language translation s = {si}N\ni=1\nwith N words via maximizing the conditional probability\np(s|V). For this task, our designed prediction head contains\na semantic modulator and a decoder as shown in Figure 3.\nConsidering the token length diversity between the\nsource and target ( T >> N ), the semantic modulator\nattempts to bridge this gap and generates suitable semantics\nM = {mi}T1\ni=1 for the decoder. SpeciÔ¨Åcally, it Ô¨Årst performs\naverage temporal pooling to reduce the source visual token\nsequence from the length T to the length T1 = T/4. This\noperation makes the visual representation more compact,\nbut its output lacks temporal dependency modeling. To\nmitigate this issue, a Transformer encoder is adapted to\nfurther modulate the pooled visual sequence and generate\nsuitable semantics.\nAfter that, a decoder is adopted to perform mapping\nbetween sign language and spoken translation while con-\nsidering their different grammar. The decoder contains two\nmain components, i.e, a word embedding layer and an\nautoregressive Transformer decoder. The word embedding\nlayer embeds each word in the target sequences, along with\nthe added position encoding as follows,\nwi = WordEmbed(si) + PE(i), (14)\nwhere si denotes the input word, WordEmbed(¬∑) and\nPE(¬∑) are the word embedding and position encoding\nfunctions, respectively.\nThe autoregressive property means the model leverages\ngenerated text as additional input when generating the\nnext. The Transformer decoder architecture is also a stack\nof basic blocks. The basic block contains three compo-\nnents, i.e., masked multi-head self-attention module, multi-\nhead cross-attention module and feed-forward network. The\nmask adopted on self-attention ensures the information Ô¨Çow\nin the rightward direction to preserve the autoregressive\nproperty [4], [77]. This operation is necessary for the SLT\ninference, since the framework is not accessible to the output\ntokens which are decoded currently or in the future. Cross-\nattention module leverages the contextual cues from the\nmodulated visual semantics M and predecessors words w.\nThe whole Transformer decoder is formulated as follows,\nD0 = w,\nQi = L( ÀúM(Di‚àí1) + Di‚àí1),\nÀúDi = L(MHA(Qi,Mk,Mv) + Qi),\nDi = L(C( ÀúDi) + ÀúDi),\n(15)\nwhere i denotes the i-th layer of the Transformer de-\ncoder. The whole encoder contains totally N layers. ÀúM(¬∑),\nMHA(¬∑), C(¬∑) and L(¬∑) represent the masked multi-head\nself-attention, multi-head cross-attention, feed-forward net-\nwork and layer normalization, respectively. Di denotes the\noutput feature from the i-th layer.\nDuring decoding, the sentence is Ô¨Årst preÔ¨Åxed with the\nword ‚Äú[bos]‚Äù to indicate the beginning. Then each word in\nthe target sequence sis embedded. The embedded sequence\nis then fed into the Transformer decoder TransD(¬∑). This\ndecoder additionally performs cross attention by leveraging\nthe contextual cues from the modulated visual semantics m\nand predecessors words w. Finally, its output is fed into a\nfully-connected network and a softmax layer to generate the\nprobability matrix of the output word.\nIn summary, the whole decoding process is formulated\nas follows,\nhi = TransD(w1:i‚àí1,m1:T1 ), (16)\noi = Softmax(Whi + b). (17)\nThe conditional probability p(s|V) is calculated as follows,\np(s|V) =\nN‚àè\ni=1\np(si|s1:i‚àí1,V) =\nN‚àè\ni=1\noi,si . (18)\nFinally, the objective function is formulated as follows,\nLSLT = ‚àíln p(s|V), (19)\nwhich is equivalent to calculating the cross-entropy loss on\neach word. We adopt the S2T setting [33], which directly\nmaps the sign embedding to spoken translation in an end-\nto-end manner. During inference, the framework predicts\nthe word one-by-one based on the beam search [78].\nFor fusion, we leverage the latent visual features from\nboth RGB and pose modalities, i.e., Mr and Mp, to the same\ndecoder. SpeciÔ¨Åcally, we replace the original cross attention\nin the decoder with the cascaded one, which is formulated\nas follows,\nÀúDi = MHA(MHA(Qi,Mk\np,Mv\np),Mk\nr,Mv\nr), (20)\nwhere MHA(¬∑) denotes the multi-head cross-attention\nlayer, Qi denotes the feature from the previous decoder\nlayer, and ÀúDi is the output of the cascaded attention.\n4 E XPERIMENT\nIn this section, we Ô¨Årst introduce the experiment setup,\ni.e., datasets, evaluation metrics and implementation details.\nThen we perform ablation studies on the framework ef-\nfectiveness from multiple perspectives. Finally, we perform\nextensive experiments to make comparison with state-of-\nthe-art methods on multiple downstream tasks.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 8\n4.1 Experiment Setup\n4.1.1 Datasets\nWe Ô¨Årst perform experiments on the dataset with hand pose\nannotations available to evaluate the framework feasibility.\nHANDS17 [79] is a video-level hand pose estimation dataset\nwith 292,820 frames from 99 video sequences. For each\nvideo, the Ô¨Årst 70% and remaining 30% frames are separated\nfor training and testing, respectively.\nWe evaluate our proposed method on three main video-\nbased sign language understanding tasks, i.e., isolated SLR,\ncontinuous SLR and SLT. The corresponding datasets for\neach task are discussed in the follow.\nFor isolated SLR, we make evlauation on three datasets,\ni.e., MSASL [17], WLASL [18], and SLR500 [16]. MSASL [17]\nis an American sign language (ASL) dataset containing a\nvocabulary of 1,000, with 25,512 samples. Besides, it also re-\nleases two subsets (MSASL100 and MSASL200) by choosing\nthe Top-K most frequent signs. WLASL [18] is another ASL\ndataset with 2,000 signs and 21,083 samples. It also contains\ntwo subsets, i.e., WLASL100 and WLASL300. MSASL and\nWLASL are both collected from the Web, which are recorded\nin unconstrained real-life conditions with unbalanced sam-\nples for each sign word. These factors bring new challenges\non accurate recognition. SLR500 [16] is the largest CSL\ndataset, which contains 500 daily signs and 125,000 samples\nrecording at the resolution of 1280 √ó720. These samples\nare split into 90,000 and 35,000 for training and testing,\nrespectively.\nFor continuous SLR, the evaluation is conducted on two\ndatasets, i.e., RWTH-Phoenix [14] and RWTH-PhoenixT [33].\nRWTH-Phoenix [14] is a popular German sign language\ndataset collected from the weather forecast broadcast. It\ncontains 6,841 samples, with 5,672, 540 and 629 videos\nfor training, validation and testing, respectively. RWTH-\nPhoenixT [33] is included for evaluation, which is intro-\nduced in ‚ÄúSLT datasets‚Äù.\nFor SLT, we make evaluation on RWTH-PhoenixT [33]\ndataset, which is treated as the extended version of RWTH-\nPhoenix. It provides parallel sign gloss and translation an-\nnotations, to evaluate both continuous SLR and SLT tasks.\nIt contains 8,257 videos, which are divided into three sets:\n7,096 for training, 519 for validation, and 642 for testing.\nRWTH-Phoenix and RWTH-PhoenixT are both recorded at\nthe resolution of 210 √ó260.\n4.1.2 Evaluation Metrics\nTo evaluate whether our framework works during pre-\ntraining, we adopt the metrics for evaluating pose esti-\nmation accuracy. SpeciÔ¨Åcally, we report the Percentage of\nCorrect Keypoints (PCK) score and the Area Under the\nCurve (AUC) on the PCK threshold ranging from 20 to\n40 pixels. PCK deÔ¨Ånes the keypoint to be correct if the\nEuclidean distance between this keypoint and ground truth\nis lower than the threshold. The distance metric is expressed\nin pixels.\nFor isolated SLR, We utilize the accuracy metrics, includ-\ning per-instance (P-I) and per-class (P-C) metrics. P-I and\nP-C denote the average accuracy over all the instances and\nclasses, respectively. Following previous works [5], [13], we\nreport Top-1 and Top-5 P-I and P-C metrics on MSASL\nand WLASL. Since each class in SLR500 contains the same\nnumber of samples, P-I is equal to P-C and we only report\none of them.\nFor continuous SLR, we utilize Word Error Rate (WER) as\nthe evaluation metric. WER is the editing distance, which\nmeasures the least operations (substitution, deletion and\ninsertion) to transform the hypothesis to the reference gloss\nsentence as follows,\nWER = Ni + Nd + Ns\nL , (21)\nwhere Ni, Nd, and Ns are the number of operations for\ninsertion, deletion, and substitution, respectively. Ldenotes\nthe length of the reference sentence.\nFor SLT, we adopt BLEU [80] and ROUGE [81] metrics\nwhich are commonly utilized in neural machine translation.\nBLEU calculates the overlap rate of n-gram between the\ngenerated text and the reference text, and nranges from 1 to\n4. ROUGE is a metric based on the recall rate and measures\nthe sentence-level structure similarity. In this work, we refer\nto the ROUGE-L F1-Score.\n4.1.3 Implementation Details\nIn our experiment, all the models are implemented by\nPyTorch [82] and trained on NVIDIA RTX 3090. Since sign\nlanguage datasets contain no available pose annotations,\nwe utilize MMPose [83] to extract 133 full 2D keypoints,\nconsisting of 23 body joints, 68 face joints and 42 hand\njoints. The hyper-parameters œµ, Œª, wŒ≤ and wŒ¥ are set as\n0.5, 0.01, 10.0 and 100.0, respectively. During decoding, the\nbeam width is set as 10 and 3 for continuous SLR and SLT,\nrespectively.\nDuring the pre-training stage, the utilized data includes\nthe training data from all aforementioned sign datasets,\nalong with other collected data from [84], [85]. In total,\nthe pre-training data volume is 230,246 videos. The Adam\noptimizer is adopted to train the framework for 60 epochs\nwith the weight decay set as 0.01. The learning rate warms\nup over the Ô¨Årst 10% of the training process, and then decays\nlinearly from the peak rate (1e-4). All the frames are fed into\nthe framework.\nFor all the downstream tasks, the Adam optimizer is still\nadopted. The learning rate for isolated SLR, continuous SLR\nand SLT are 1e-4, 1e-4 and 5e-5, respectively. For continuous\nSLR and SLT, we follow the setting [9]. Spatial-temporal\ndata augmentation is utilized during training. Spatially,\nfollowing [24], we adopt random moving augmentation to\nsimulate spatial disturbance induced by rotation, translation\nand scaling factors. Temporally, for isolated SLR, we extract\n32 frames from the origin video using random and center\nsampling for training and testing, respectively. While for\ncontinuous SLR and SLT, we randomly sample 80% frames\nduring training and utilize all the frames during testing.\n4.2 Ablation Study\nIn this section, we Ô¨Årst validate the framework feasibility.\nThen we perform detailed ablation studies on different\ncomponents of our framework.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 9\nTABLE 1\nFramework feasibility validation on HANDS17. ‚ÄúP@20‚Äù denotes the\nPCK metrics with the error threshold set as 20 pixel. ‚ÄúJoint‚Äù, ‚ÄúFrame‚Äù\nand ‚ÄúClip‚Äù denote the masked joint modeling, masked frame modeling\nand masked clip modeling, respectively. ‚ÄúInput‚Äù and ‚ÄúOutput‚Äù represent\nthe corrupted input pose and the reconstructed pose sequence by our\nframework, respectively.\nMask Input Output\nJoint Frame Clip P@20 AUC P@20 AUC\n‚úì 90.06 89.99 94.60 95.15\n‚úì 74.83 74.80 93.30 95.13\n‚úì 60.99 60.00 91.94 93.47\n‚úì ‚úì ‚úì 66.65 66.63 94.00 94.74\nTABLE 2\nImpact of the Transformer layersN on MSASL dataset. N denotes the\nnumber of the Transformer encoder layers in our framework.\nN 100 200 1000\nP-I P-C P-I P-C P-I P-C\n2 82.56 82.35 74.47 75.51 59.20 56.70\n3 84.94 85.23 78.51 79.35 62.42 60.15\n4 83.75 83.56 76.97 77.74 60.69 57.34\n5 83.88 84.23 77.04 77.93 61.27 58.30\n4.2.1 Framework Feasibility\nWe validate the framework feasibility via observing its\npose reconstruction capability, on HANDS17 dataset with\nhand pose annotation available. In this setting, we adopt\nall masked modeling strategies to train our framework\non this dataset. During validation, we perform different\nmasking cases on the input sequence and evaluate the\nframework output quality. As shown in Table 1, the output\nmetrics are higher than those of the input under all masking\ncases, which validates our framework feasibility. Besides,\nwe qualitatively visualize the hand pose reconstruction in\nFigure 4. It can be observed that the reconstructed hand pose\nsequence is consistent with the ground truth, even under the\nseverely corrupted input situation. It is largely attributed to\ninherent contextual cues captured by our framework via our\ndesigned pretext task.\n4.2.2 Ablation Study\nTo study the impact of different hyper-parameters and set-\ntings in our approach, we conduct experiments on MSASL\nand its subset with per-instance and per-class Top-1 accu-\nracy as the performance indicator.\nImpact of the Transformer layers N. As shown in Ta-\nble 2, the accuracy gets improved when the number N of\nTransformer layers increases. It reaches the peak when N is\nequal to 3. There exists difference in the best N between the\noriginal BERT and ours, which may be attributed to different\ncharacteristics between the sign pose and text domains. In\nall the experiments, we set N as 3 unless stated.\nImpact of the poseŒ∏dimension in the hand-model-aware\ndecoder. From Table 3, the pose Œ∏dimension represents the\nMANO characterization capability of the hand gesture. The\nincrease of the pose dimension brings enhanced capability\nand accuracy improvement on the downstream SLR. It\nreaches the top when the dimension is equal to 25. However,\nTABLE 3\nImpact of the pose Œ∏ dimension in the hand-model-aware decoder on\nMSASL dataset.\nDimension 100 200 1000\nP-I P-C P-I P-C P-I P-C\n15 82.83 82.83 76.01 76.50 61.65 58.59\n25 84.94 85.23 78.51 79.35 62.42 60.15\n35 83.88 84.20 77.19 77.99 61.60 59.04\nTABLE 4\nImpact of the temporal span K in masked clip modeling on MSASL\ndataset. K represents that the masked clip duration ranges from 2 to\nK.\nK 100 200 1000\nP-I P-C P-I P-C P-I P-C\n4 81.90 82.17 73.58 74.17 60.43 57.51\n8 83.88 83.55 76.60 77.57 61.94 59.76\n12 82.96 82.83 74.98 75.16 60.93 58.67\nTABLE 5\nImpact of different temporal information extraction on MSASL dataset.\nMethod 100 200 1000\nP-I P-C P-I P-C P-I P-C\nPE 84.94 85.23 78.51 79.35 62.42 60.15\nGCN Tem-3 83.36 83.41 76.09 76.85 60.21 57.62\nGCN Tem-5 83.62 84.36 76.09 77.12 60.55 58.13\nfurther increasing does not bring more performance gain,\nwhich may be caused by the optimization difÔ¨Åculty.\nImpact of the temporal spanK in masked clip modeling.\nIn Table 4, it can be observed that the accuracy reaches the\ntop when K is equal to 8. The suitable temporal mask span\nenforces the framework to capture the temporal dynamics\nduring sign language. In the following, we set Kas 8 unless\nstated.\nImpact of different temporal information extraction on\nMSASL dataset. There are many alternative methods to\nextract temporal information. Besides temporal position\nencoding, directly extracting temporal information is also\na solution. To this end, we modify the current GCN into\nthe temporal variant following the practice in [74]. Specif-\nically, the original spatial GCN graph is replaced with\nthe spatial-temporal one via adding the local connections\nalong the temporal dimension. With this modiÔ¨Åcation, the\ngesture state extractor embeds the temporal receptive Ô¨Åeld\nof additional kadjacent input frames and thus captures the\ntemporal information. Besides, since our pretext task needs\nto recover the masked pose token in the corresponding\noutput, we utilize padding to keep the sequence length after\nthe gesture extractor the same as the input.\nAs shown in Table 5, we perform comparison on these\ndifferent temporal extraction methods. ‚ÄúPE‚Äù denotes utiliz-\ning the position encoding for temporal information extrac-\ntion. For ‚ÄúGCN Tem-k‚Äù, we remove the temporal position\nencoding and directly extract temporal information via our\nmodiÔ¨Åed GCN backbone. k represents the number of adja-\ncent frames. These settings achieve comparable performance\non the downstream SLR. It can be explained that the fol-\nlowing Transformer encoder contains the strong capability\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 10\nTABLE 6\nEffectiveness of the spatial-temporal position encoding (‚ÄúPE‚Äù) on\nMSASL dataset.\nPE 100 200 1000\nTemporal Spatial P-I P-C P-I P-C P-I P-C\n77.68 77.93 73.07 73.50 51.80 48.76\n‚úì 79.00 79.05 74.39 74.51 52.97 49.95\n‚úì 82.96 82.75 75.94 77.15 60.95 58.82\n‚úì ‚úì 84.94 85.23 78.51 79.35 62.42 60.15\nTABLE 7\nEffectiveness of the masking ratio R on MSASL dataset.\nR 100 200 1000\nP-I P-C P-I P-C P-I P-C\n20% 78.47 78.46 69.09 70.62 59.52 56.15\n30% 82.30 82.33 76.23 76.79 61.07 57.92\n40% 84.94 85.23 78.51 79.35 62.42 60.15\n50% 82.96 83.26 76.31 77.40 60.93 57.95\n60% 80.18 81.06 75.64 76.21 59.80 57.25\nTABLE 8\nEffectiveness of the masking strategy on MSASL dataset. The Ô¨Årst row\ndenotes the baseline, i.e., our framework is trained without pre-training.\n‚ÄúJoint‚Äù, ‚ÄúFrame‚Äù and ‚ÄúClip‚Äù denote the masked joint modeling, masked\nframe modeling and masked clip modeling, respectively.\nMask 100 200 1000\nJoint Frame Clip P-I P-C P-I P-C P-I P-C\n78.20 78.10 72.77 73.21 53.31 50.43\n‚úì 82.03 82.78 74.39 74.62 58.44 55.24\n‚úì 82.69 82.85 76.53 77.24 60.23 58.02\n‚úì 83.88 83.55 76.60 77.57 61.94 59.76\n‚úì ‚úì ‚úì 84.94 85.23 78.51 79.35 62.42 60.15\nof capturing long-term sequential dependencies. The simple\nposition encoding is sufÔ¨Åcient to indicate this encoder with\nthe temporal order. Unless stated, we utilize the temporal\nposition encoding as the temporal order indicator.\nEffectiveness of the spatial-temporal position encoding.\nAs shown in Table 6, we validate the effectiveness of spatial-\ntemporal position encoding. The Ô¨Årst row denotes the re-\nsults without any position encoding. In the second row,\nwe incorporate the temporal inforamtion with the temporal\nposition encoding. In the third row, we incorporate the\nglobal hand information in the form of hand spatial position\nencoding and involve all arm joints (totally 7 joints). Com-\npared with the temporal counterpart, our designed spatial\nposition encoding incorporates the important hand global\nposition information and brings a larger performance gain.\nFurthermore, these two encoding schemes exhibit comple-\nmentary effects, bringing even 10.62% per-instance Top-1\naccuracy improvement over the baseline on the full set.\nEffectiveness of the masking ratio R. We study the in-\nÔ¨Çuence of the masking ratio in Table 7. It is observed that\nthe performance reaches the top when the masking ratio is\nequal to 40%, which is inconsistent with BERT [2]. It may\nbe attributed to the information density difference between\nsign pose and predeÔ¨Åned NLP word tokens.\nEffectiveness of the masking strategy.As demonstrated in\nTable 8, the Ô¨Årst row denotes the baseline method, which\nTABLE 9\nEffectiveness of the model-aware decoder on MSASL dataset. We\ncompare ours with different pose decoders.\nDecoder 100 200 1000\nP-I P-C P-I P-C P-I P-C\n1-layer fc 82.56 83.11 74.32 75.26 59.88 57.17\n2-layer fc 83.62 83.82 75.06 75.92 60.50 57.65\nOurs 84.94 85.23 78.51 79.35 62.42 60.15\nTABLE 10\nEffectiveness of the ratio of pre-training data scale on MSASL dataset.\nRatio 100 200 1000\nP-I P-C P-I P-C P-I P-C\n0% 78.20 78.10 72.77 73.21 53.31 50.43\n25% 81.11 81.44 74.83 75.51 58.53 56.04\n50% 81.24 81.79 75.35 76.35 59.61 56.71\n75% 83.09 83.36 76.01 76.60 60.43 57.63\n100% 84.94 85.23 78.51 79.35 62.42 60.15\nis Ô¨Åne-tuned on MSASL without pre-training. Our designed\nthree masked modeling strategies target at different levels\nof context contained in the sign language domain, which\nindividually bring 5.13%, 6.92% and 8.63% per-instance Top-\n1 accuracy improvement over the baseline on the full set,\nrespectively. Among them, masked clip modeling brings the\nlargest performance gain. When all these masking strategies\nare utilized, it reaches the best performance.\nEffectiveness of the model-aware decoder. We compare\nthe effects of different pose decoders in Table 9. The Ô¨Årst\ntwo rows represent that we utilize fully-connected layers\nto directly regress the keypoints for pose reconstruction\nduring pre-training. Compared with the direct regression\nmethod, our decoder incorporates prior via regressing the\ncompact gesture embedding, which eases optimization and\nbrings a larger performance gain on the downstream SLR.\nBesides, our decoder also exhibits the additional beneÔ¨Åt,\nwhich inÔ¨Çates the input 2D sequence to the corresponding\n3D mesh as the intermediate representation.\nEffectiveness of the pre-training data scale. We study\nthe impact of the pre-training data scale in Table 10. We\nrandomly extract a portion of all training data, indicated in\nthe ‚ÄúRatio‚Äù column. Then we pre-train and Ô¨Åne-tune our\nframework based on the same setup. It is observed that\nthe accuracy grows monotonically when the pre-training\ndata volume increases, which suggests our framework may\nbeneÔ¨Åt from even more pre-training data.\n4.3 Qualitative Visualization\nWe visualize the reconstruction results under the sign data\nsources after pre-training in Figure 5 and 6. In this setting,\nposes are detected by an off-the-shelf extractor and fed\ninto the framework. For clarity, we only plot one hand and\nvisualize its reconstructed poses and the intermediate hand\nmeshes, which are produced by our framework. We start by\ndemonstrating some general cases in Figure 5. Under loss\nof hand joints in several frames, our framework is able to\ncapture the context to reconstruct the pose and mesh, which\nalign the 2D image plane well. The hand mesh, as the inter-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 11\nIn\n2D\nOut\n2D\nGT\n2D\nTimestamp\nOut\nMesh\n2,3,10, 15,17,19, 20,21,25,32,33,34,35,38,39,50,51,52,54,56,57,58\nTimestamp\nIn\n2D\nOut\n2D\nGT\n2D\nOut\nMesh\nFig. 4. Qualitative illustration of the framework feasibility on HANDS17. We exhibit 15 continuous frames of a video. Four rows represent the ground\ntruth pose, input pose disturbed by all kinds of masked modeling strategies (joint, frame and clip), reconstructed sequence and the middle mesh\nrepresentation, respectively. Notably, blanks in the second row represent all joints in the corresponding frames are masked.\nIn\nOut\nMesh\nTimestamp\n Timestamp\nIn\nOut\nMesh\nIn\nOut\nMesh\nIn\nOut\nMesh\nFig. 5. More visualization samples under sign data sources with no hand pose annotation. For each sample, 8 continuous frames are visualized.\n‚ÄúIn‚Äù, ‚ÄúOut‚Äù and ‚ÄúMesh‚Äù denote the input hand pose, the reconstructed hand pose and the intermediate hand mesh, respectively. For clarity, we\nvisualize all the poses and meshes on their aligned RGB image planes.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 12\nOut\nMesh\nTimestamp\nIn\nOut\nMesh\nIn\nTimestamp Timestamp\n(a) Hand-to-hand Interaction\nOut\nMesh\nTimestamp\nIn\nOut\nMesh\nIn\nTimestamp Timestamp\n(b) Hand-to-face Interaction\nFig. 6. More visualization samples on two types of hard interaction cases during sign language expression,i.e., hand-to-hand interaction and hand-\nto-face interaction. For each sample, 5 continuous frames are visualized. ‚ÄúIn‚Äù, ‚ÄúOut‚Äù and ‚ÄúMesh‚Äù denote the input hand pose, the reconstructed hand\npose and the intermediate hand mesh, respectively. For clarity, we only plot one hand and visualize its poses and meshes on their aligned RGB\nimage planes.\nmediate representation, also improves the interpretability of\nour method.\nAs illustrated in Figure 6, we further demonstrate quali-\ntative results on two types of hard cases, i.e., hand-to-hand\ninteraction and hand-to-face interaction. Due to the similar\nappearance of hand and face and complex self- and mutual\nocclusion, these interactions bring inherent ambiguity and\ncause failure in hand pose estimation. Even under these\nhard cases, our framework can rectify the noisy inputs and\ninfer all the poses which well align the image plane. This\nstrong hallucination capability may be largely attributed to\nthe well-modeled statistics in the sign language domain.\n4.4 Comparison with Other Pre-Training Strategies\nAs demonstrated in Table 11, we compare with other pre-\ntraining strategies, including supervised and state-of-the-art\nself-supervised pre-training methods. For fair comparison,\nwe pre-train on the same SignBERT+ encoder backbone (em-\nbedding layer and Transformer encoder).\nSimilar to [86], supervised pre-training denotes that we\nadd a classiÔ¨Åer (an MLP and softmax layer) on top of the\nbackbone and perform pre-training under the classiÔ¨Åcation\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 13\nTABLE 11\nComparison with other pre-training strategies on downstream tasks. For fair comparison, all the pre-training methods are performed on the same\nbackbone, i.e., embedding layer and Transformer encoder. The Ô¨Årst row represents the framework is directly Ô¨Åne-tuned on the downstream tasks\nwithout pre-training. ‚ÄúPartial‚Äù and ‚ÄúAll‚Äù denote the corresponding classiÔ¨Åcation data and all pre-training data, respectively. The data volumes of\n‚ÄúPartial‚Äù and ‚ÄúAll‚Äù are about 160k and 230k videos, respectively. (‚Üëdenotes the higher the better, while ‚Üìrepresents the lower the better.)\nMethod Pre-Train\nMSASL WLASL SLR500 RWTH-PhoenixRWTH-PhoenixT\nP-I P-C P-I P-C P-I Dev Test Dev Test\nTop-1‚Üë Top-5‚Üë Top-1‚Üë Top-5‚Üë Top-1‚Üë Top-5‚Üë Top-1‚Üë Top-5‚Üë Top-1‚Üë WER‚Üì WER‚Üì WER‚Üì WER‚Üì\nBaseline Scratch 53.31 75.98 50.43 74.12 38.33 72.59 36.40 71.23 92.1 43.6 43.4 42.2 42.6\nSupervised Partial 58.82 80.42 56.27 79.54 46.00 79.95 43.63 78.32 94.7 42.5 42.6 40.8 41.7\nV-MoCo [86]Partial 54.22 78.26 51.31 77.03 39.12 72.79 36.93 71.15 94.1 42.1 43.4 40.3 40.8\nOurs Partial 60.13 82.12 57.19 80.72 47.46 81.62 45.03 80.31 95.1 36.7 36.2 35.0 35.2\nV-MoCo [86]All 55.27 79.72 52.06 78.31 40.17 75.19 37.71 73.38 94.8 41.1 41.3 39.2 40.0\nOurs All 62.42 83.49 60.15 82.44 48.85 82.48 46.37 81.33 95.4 34.0 34.1 32.9 33.6\nMethod Pre-Train\nRWTH-PhoenixT\nDev Test\nROUGE‚Üë BLEU-1‚Üë BLEU-2‚Üë BLEU-3‚Üë BLEU-4‚Üë ROUGE‚Üë BLEU-1‚Üë BLEU-2‚Üë BLEU-3‚Üë BLEU-4‚Üë\nBaseline Scratch 40.23 39.09 26.40 19.63 15.50 39.83 39.27 26.98 20.10 15.90\nSupervised Partial 41.63 40.87 28.06 20.84 16.49 41.58 41.60 28.82 21.68 17.29\nV-MoCo [86] Partial 42.44 41.94 28.97 21.43 16.84 41.75 41.83 28.82 21.36 16.82\nOurs Partial 44.15 44.03 31.18 23.71 19.00 44.33 43.51 30.92 23.59 19.10\nV-MoCo [86] All 42.79 42.55 29.40 22.13 17.53 41.95 42.68 29.49 21.89 17.36\nOurs All 45.53 44.45 31.88 24.59 19.86 44.89 44.35 32.09 24.92 20.41\ntask. SpeciÔ¨Åcally, supervised pre-training is conducted on a\nportion of the original pre-training data (denoted as ‚ÄúPar-\ntial‚Äù), i.e., the corresponding classiÔ¨Åcation (isolated SLR)\nbenchmarks. The ‚ÄúPartial‚Äù and ‚ÄúAll‚Äù data volumes are\n160,113 and 230,246 videos, respectively.\nFor fair comparison with supervised pre-training, we\ndeÔ¨Åne two evaluation settings for self-supervised methods,\ni.e., pre-training on the ‚ÄúPartial‚Äù and ‚ÄúAll‚Äù data volumes.\nWe adopt the state-of-the-art self-supervised pre-training\nmethod, i.e., V-MoCo [86]. It is based on contrastive learning,\nand can be treated as the extended version of MoCo [49]\ninto the video domain. Since it originally works on the\nRGB domain, we make a few modiÔ¨Åcations by replacing\nits backbone with ours, which is able to process the pose\nmodality. During pre-training, we randomly sample two\nclips with 32 consecutive frames from the same sign pose\nsequence, as the query and positive samples. The negative\nsamples are obtained from the clips of other videos. During\ntraining, its objective is to maximize the similarity between\nthe query and positive samples, which aims to learn tempo-\nrally persistent features of the same video.\nWe evaluate the effectiveness of these pre-training\nmethods on all three downstream tasks, i.e., isolated\nSLR (MSASL, WLASL, SLR500), continuous SLR (RWTH-\nPhoenix and RWTH-PhoenixT) and SLT (RWTH-PhoenixT).\nIn isolated SLR, supervised pre-training brings a larger per-\nformance gain than V-MoCo. While for continuous SLR and\nSLT, the supervised pre-training method brings a relatively\nsmaller performance gain, whose performance is worse\nthan that of V-MoCo. Supervised pre-training exhibits the\nlimited generalization capability to the downstream tasks,\nwhose objective is inconsistent with classiÔ¨Åcation. For self-\nsupervised pre-training methods, i.e., V-MoCo and Ours,\nthey do not rely on annotated data and scale well with larger\npre-training data volume. Notably, when compared with\nother pre-training strategies, our method achieves the best\nperformance on all downstream tasks with notable gains.\n4.5 Comparison with State-of-the-art Methods\nIn this section, we compare our method with previous state-\nof-the-art methods on three main downstream tasks, includ-\ning isolated SLR, continuous SLR and SLT. For comparison,\nwe group them into pose-based and RGB-based methods.\n4.5.1 Isolated Sign Language Recognition\nEvaluation on MSASL [17].MSASL introduces new chal-\nlenges given its unconstrained recording conditions. As\nillustrated in Table 12, the accuracy of previous pose-based\nmethods lags largely behind the RGB-based counterpart. It\nis mainly attributed to the pose detection failure caused by\npartially occluded upper body, motion blur and complex\nbackground, etc. TCK [7] and BSL [13] propose different\npre-training techniques on the I3D backbone by leverag-\ning external sign data. Our method achieves new state-\nof-the-art performance under both pose-based and RGB-\nbased comparison settings with a notable gain. Notably,\nwhen compared with previous SignBERT [5], our frame-\nwork outperforms it by 12.88% per-instance Top-1 accuracy\nimprovement on the full set.\nEvaluation on WLASL [18]. WLASL is also the uncon-\nstrained recording setting. Compared with MSASL, it is\nmore challenging with fewer samples and doubled vocab-\nulary size. As shown in Table 13, it is worth mentioning\nthat our single pose-based method even outperforms the\nmost challenging RGB-based method [7], [13], with over 2%\nTop-1 per-instance accuracy improvement on all sets. When\nfused with the RGB baseline, the performance of our method\nfurther gets improved.\nEvaluation on SLR500 [16].As demonstrated in Table 14,\nSTIP [87] and GMM-HMM [88] are the traditional meth-\nods based on hand-crafted features. Since this dataset is\nrecorded under the controlled setting, the performance is\nquite saturated with only Top-1 accuracy reported. GLE-\nNet [84] is a challenging method, which performs feature\nenhancement from the global and local views. Our method\nachieves 97.8% Top-1 accuracy, which is new state-of-the-art\nperformance.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 14\nTABLE 12\nEvaluation of isolated SLR on MSASL dataset (the higher the better). [17] denotes the RGB baseline for fusion.\nMethod\nMSASL100 MSASL200 MSASL\nPer-instance Per-class Per-instance Per-class Per-instance Per-class\nTop-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5\nPose-based\nST-GCN [24] 59.84 82.03 60.79 82.96 52.91 76.67 54.20 77.62 36.03 59.92 32.32 57.15\nSignBERT [5] 76.09 92.87 76.65 93.06 70.64 89.55 70.92 90.00 49.54 74.11 46.39 72.65\nOurs 84.94 95.77 85.23 95.76 78.51 92.49 79.35 93.03 62.42 83.49 60.15 82.44\nRGB-based\nI3D [17] - - 81.76 95.16 - - 81.97 93.79 - - 57.69 81.05\nHMA [72] 73.45 89.70 74.59 89.70 66.30 84.03 67.47 84.03 49.16 69.75 46.27 68.60\nTCK [7] 83.04 93.46 83.91 93.52 80.31 91.82 81.14 92.24 - - - -\nBSL [13] - - - - - - - - 64.71 85.59 61.55 84.43\nSignBERT (+ R) [5] 89.56 97.36 89.96 97.51 86.98 96.39 87.62 96.43 71.24 89.12 67.96 88.40\nOurs (+ R) [5] 90.75 97.75 91.52 97.73 88.08 96.47 88.62 96.47 73.71 90.12 70.77 89.30\nTABLE 13\nEvaluation of isolated SLR on WLASL dataset (the higher the better). I3D [18] denotes the RGB baseline for fusion.\nMethod\nWLASL100 WLASL300 WLASL\nPer-instance Per-class Per-instance Per-class Per-instance Per-class\nTop-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5\nPose-based\nST-GCN [24] 50.78 79.07 51.62 79.47 44.46 73.05 45.29 73.16 34.40 66.57 32.53 65.45\nPose-TGCN [18] 55.43 78.68 - - 38.32 67.51 - - 23.65 51.75 - -\nPSLR [23] 60.15 83.98 - - 42.18 71.71 - - - - - -\nSignBERT [5] 76.36 91.09 77.68 91.67 62.72 85.18 63.43 85.71 39.40 73.35 36.74 72.38\nOurs 79.84 92.64 80.72 93.08 73.20 90.42 73.77 90.58 48.85 82.48 46.37 81.33\nRGB-based\nI3D [18] 65.89 84.11 67.01 84.58 56.14 79.94 56.24 78.38 32.48 57.31 - -\nHMA [72] - - - - - - - - 37.91 71.26 35.90 70.00\nTCK [7] 77.52 91.08 77.55 91.42 68.56 89.52 68.75 89.41 - - - -\nBSL [13] - - - - - - - - 46.82 79.36 44.72 78.47\nSignBERT (+ R) [5] 82.56 94.96 83.30 95.00 74.40 91.32 75.27 91.72 54.69 87.49 52.08 86.93\nOurs (+ R) 84.11 96.51 85.05 96.83 78.44 94.31 79.12 94.43 55.59 89.37 53.33 88.82\nTABLE 14\nEvaluation of isolated SLR on SLR500 dataset (the higher the better).\n[84] denotes the RGB baseline for fusion.\nMethod Accuracy\nPose-based\nST-GCN [24] 90.0\nSignBERT [5] 94.5\nOurs 95.4\nRGB-based\nSTIP [87] 61.8\nGMM-HMM [88] 56.3\n3D-R50 [89] 95.1\nHMA [72] 95.9\nGLE-Net [84] 96.8\nSignBERT (+ R) [5] 97.7\nOurs (+ R) 97.8\nIn summary, our method greatly shrinks the perfor-\nmance gap between pose-based and RGB-based methods on\nisolated SLR. Under the challenging in-the-wild conditions,\nour method even outperforms the challenging RGB-based\nmethods. It can be attributed to our designed masking mod-\neling strategies and incorporated prior during pre-training.\n4.5.2 Continuous Sign Language Recognition\nEvaluation on RWTH-Phoenix [14]. As demonstrated in\nTable 16, we exhibit experiment results on RWTH-Phoenix\ndataset. Due to the lack of pose-based methods, we adopt\ntwo representative RGB-based methods [9], [29] by only\nchanging its visual encoder with the GCN to process pose\nmodality, denoted as ‚ÄúP-BLSTM‚Äù and ‚ÄúP-Trans‚Äù. Pose-based\nmethods lag largely behind RGB-based methods, which is\nlargely caused by pose failure caused by the low-quality\ndata and motion blur. Among pose-based methods, our\nmethod largely outperforms the most challenging competi-\ntor P-BLSTM with 5.6% and 5.2% WER improvement on\nthe dev and test set, respectively. When fused with the\nRGB baseline, our method achieves new state-of-the-art\nperformance, i.e., 19.9% and 20.0% WER on the dev and test\nset, respectively.\nEvaluation on RWTH-PhoenixT [33].We make comparison\non RWTH-PhoenixT in Table 17. This dataset additionally\nprovides spoken German translation corresponding to the\nsign gloss annotation. [15] utilizes the spoken translation to\ninfer the mouth shape label, which provides auxiliary cues\nto recognition. Besides, it releases multi-stream versions for\nfurther performance improvement. STMC [35] also lever-\nages the multi-cue information from the full frame, hand,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 15\nTABLE 15\nEvaluation of SLT on RWTH-PhoenixT dataset (the higher the better). [36] denotes the RGB baseline method for fusion.\nMethod Dev Test\nROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE BLEU-1 BLEU-2 BLEU-3 BLEU-4\nPose-based\nSkeletor [90] 32.66 31.97 19.53 14.01 10.91 31.80 31.86 19.11 13.49 10.35\nOurs 45.53 44.45 31.88 24.59 19.86 44.89 44.35 32.09 24.92 20.41\nRGB-based\nSign2Text [33] 31.80 31.87 19.11 13.16 9.94 31.80 32.24 19.03 12.83 9.58\nTSPNet [34] - - - - - 34.96 36.10 23.12 16.88 13.41\nMCT [91] 45.90 - - - 19.51 43.57 - - - 18.51\nSL-Trans [9] - 47.26 34.40 27.05 22.38 - 46.61 33.73 26.19 21.32\nBN-TIN-Trans [36] 46.87 46.90 33.98 26.49 21.78 46.98 47.57 34.64 26.78 21.68\nSimulSLT [92] 36.04 36.01 22.60 16.05 12.39 35.13 35.92 22.70 16.03 12.27\nPiSLTRc-T [92] 47.89 46.51 33.78 26.78 21.48 48.13 46.22 33.56 26.04 21.29\nSTMC [35] 48.24 47.60 36.43 29.18 24.08 46.65 46.98 36.09 28.70 23.65\nSignBT [36] 50.29 51.11 37.90 29.80 24.45 49.54 50.80 37.75 29.72 24.32\nOurs (+ R) 51.12 51.46 38.28 30.30 24.95 50.63 52.01 39.19 31.06 25.70\nTABLE 16\nEvaluation of continuous SLR on RWTH-Phoenix dataset (the lower the\nbetter). [29] denotes the RGB baseline for fusion.\nMethods Dev Test\ndel / ins WER del / ins WER\nPose-based\nP-BLSTM [29] 13.4 / 3.5 39.6 12.3 / 3.4 39.3\nP-Trans [9] 16.0 / 3.2 40.9 14.9 / 3.4 40.4\nOurs 9.0 / 6.3 34.0 7.9 / 6.0 34.1\nRGB-based\nCMLLR [14] 21.8 / 3.9 55.0 20.3 / 4.5 53.0\n1-Million-Hand [93] 16.3 / 4.6 47.1 15.2 / 4.6 45.1\nCNN-Hybrid [94] 12.6 / 5.1 38.3 11.1 / 5.7 38.8\nSubUNets [76] 14.6 / 4.0 40.8 14.3 / 4.0 40.7\nRCNN [95] 13.7 / 7.3 39.4 12.2 / 7.5 38.7\nRe-Sign [25] - 27.1 - 26.8\nHybrid CNN-HMM [26] - 31.6 - 32.5\nCNN-LSTM-HMM [15] - 26.0 - 26.0\nCTF [96] 12.8 / 5.2 37.9 11.9 / 5.6 37.8\nDilated [97] 8.3 / 4.8 38.0 7.6 / 4.8 37.3\nIAN [98] 12.9 / 2.6 37.1 13.0 / 2.5 36.7\nDNF [29] 7.8 / 3.5 23.8 7.8 / 3.4 24.4\nFCN [99] - 23.7 - 23.9\nCMA [30] 7.3 / 2.7 21.3 7.3 / 2.4 21.9\nPiSLTRc-R [100] 8.1 / 3.4 23.4 7.6 / 3.3 23.2\nSTMC [35] 7.7 / 2.4 21.7 7.4 / 2.6 20.7\nVAC [8] 7.9 / 2.5 21.2 8.4 / 2.6 22.3\nOurs (+ R) 4.8 / 3.7 19.9 4.2 / 3.8 20.0\nface and pose and becomes the most challenging competitor.\nOur method (Ours (+ R)) outperforms it while only utilizing\nthe full video and pose information.\n4.5.3 Sign Language Translation\nEvaluation on RWTH-PhoenixT [33].As shown in Table 15,\nwe perform comparison on RWTH-PhoenixT dataset, which\nis the current most popular benchmark for evaluating SLT.\nContemporaneous with ours, Skeletor [90] is an inÔ¨Çuential\nwork that conducts BERT style pre-training but in another\nÔ¨Åeld of pose estimation. SpeciÔ¨Åcally, it inÔ¨Çates the detected\nposes to 3D ones and conducts BERT-style pre-training\nwith the aim of reÔ¨Åning 3D poses. Then it validates its\neffectiveness on downstream SLT with the reÔ¨Åned poses as\ninput. Compared with this challenging pose-based method,\nTABLE 17\nEvaluation of continuous SLR on RWTH-PhoenixT dataset (the lower\nthe better). [29] denotes the RGB baseline for fusion.\nMethods Dev Test\ndel / ins WER del / ins WER\nPose-based\nP-BLSTM [29] 13.8 / 3.3 40.2 12.9 / 3.1 40.2\nP-Trans [9] 12.9 / 3.7 39.4 11.4 / 3.8 39.8\nOurs 9.2 / 4.9 32.9 8.4 / 5.3 33.6\nRGB-based\n1-stream [15] - 24.5 - 26.5\n3-stream [15] - 22.1 - 24.1\nDNF [29] 10.5 / 1.9 22.7 9.8 / 2.4 23.5\nSL-Trans [9] 11.7 / 6.5 24.9 11.2 / 6.1 24.6\nFCN [99] - 23.3 - 25.1\nPiSLTRc-R [100] 4.9 / 4.2 21.8 5.1 / 4.4 22.9\nSTMC [35] - 19.6 - 21.0\nOurs (+ R) 4.8 / 3.3 18.8 4.3 / 3.9 19.9\nour framework directly models the SL statics in the latent\nsemantics space and surpasses it with a larger performance\ngain, i.e., 8.95% and 10.06% BLEU-4 improvement on the\ndev and test set, respectively. When compared with RGB-\nbased methods, Ours (+ R) also achieves new state-of-the-\nart performance, achieving 24.95% and 25.70% BLEU-4 on\nthe dev and test set, respectively.\n4.6 Evaluation with Deaf Community\nThe end goal of automatic sign language understanding is to\nmake the daily life of the deaf community more convenient.\nXu et al. [101] make the Ô¨Årst attempt of user study to\nevaluate the built sign gloss dictionary for sign language\nlearners. Evaluation participation of the deaf community\nis also crucial to better analyze our method and outline\nfuture work. In our work, the effectiveness of pre-training is\nevaluated on the downstream tasks and it is also desirable\nto perform more direct evaluation on pre-training.\nTo this end, we conduct a user study with the Institu-\ntional Review Board (IRB) approval from our college with\ngranted number No.202200603. This user study aims to ana-\nlyze the robustness of our framework under different input\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 16\nTABLE 18\nUser study with deaf community on robustness of our pre-training\nmodel under different input noise levels. The rate represents the\naverage correct rate which means the deaf volunteer is able to correctly\nidentify the semantic meaning via observing the output pose of our\nframework.\nNoise Intensity Correct Rate\nInput Output\n0.1 23% 92%\n0.2 16% 85%\n0.3 10% 79%\n0.4 4% 75%\nnoise levels via evaluating the semantic preservation of the\npre-training framework output (pose sequence). Different\nnoise levels are achieved via choosing different portions of\nthe input tokens to add noise. There are 10 deaf volunteers\nparticipating in this study. In the study, each volunteer is\nasked to judge whether the corresponding sign gloss can be\nidentiÔ¨Åed via observing the framework output. We report\nthe correct rate to indicate semantic preservation. Totally,\n100 real-world sign videos are collected and involved in\nthis study. For each video, there are 8 samples, i.e., 4 (noise\nlevels) √ó2 (input and output) needed to be evaluated.\nAs shown in Table 18, it can be observed that the se-\nmantics of the output are well-preserved when the input\nnoise intensity increases, which validates the robustness of\nour framework. Meanwhile, the correct rate of the output\nis consistently better than that of the input under all noise\nlevels. It reveals that the modeled statistics via our pre-\ntraining can bring positive gains on the semantics. Besides,\nthese deaf participants also give us feedback on the reasons\nof failed recognition from their perspectives, e.g. pose jit-\ntering, nonstandard gesture, etc. These results can give us\nsome hints on further improving the pre-training design,\ne.g. inserting the basic gesture types of sign language as the\nconstraint.\n4.7 Analysis & Future Work\nThe core of our work is modeling the statistics in the sign\nlanguage domain via maximizing the likelihood of the joint\nprobability distribution, which beneÔ¨Åts the downstream\nsign language understanding tasks. Despite the success of\nBERT in NLP , it is non-trivial to leverage its masked mod-\neling pretext task into sign language understanding due to\ndifferent characteristics between these two domains. Among\nthem, the major one is information density [60]. Originally,\nthe languages are highly semantic and well represented\nwith 1D sequences of text words, which are deÔ¨Åned with\nclariÔ¨Åed semantics. In contrast, sign pose, expressed in 3D\ncontinuous coordinates, is a kind of well-structured data\nwith both spatial and temporal redundancy. Besides, this\nkind of signal usually contains noise due to failure estima-\ntion. This fundamental difference raises the following issues\nto resolve, which outlines potential future work.\n‚Ä¢ Token embedding & Position encoding. These em-\nbeddings are needed to be carefully designed con-\nsidering the hand pose characteristics in the sign\nlanguage domain, e.g. how to effectively represent\nspatial-temporal positions of hands.\n‚Ä¢ Masking strategy. It aims to capture the hierarchical\ncontext in the sign data, which needs to consider the\ncharacteristic of sign pose data.\n‚Ä¢ Decoder design & Pre-training objective. The de-\ncoder in the pre-training stage performs mapping\nfrom the latent feature back to the input. In NLP ,\nthe decoder predicts the masked discrete words with\nthe cross-entropy objective. While for this task, the\ngoal is to reconstruct the continuous sign hand pose\nsequence. The involved pre-training objective and\ndecoder are needed to design.\nMore discussion. In this work, we provide our solution\nto the above issues and validate the effectiveness of our\nframework. Pre-training on pose has its pros and cons. Pose\ndata is a semantic and compact representation, which is\nrobust to appearance or background changes and brings\npotential computation efÔ¨Åciency. On the other hand, our\nadopted pose input is estimated by the off-the-shelf pose\ndetector. Although our framework embeds the capability to\ncapture the cues from the corrupted input pose sequence,\nits bottleneck is somewhat limited by the quality of the\ndetected pose. Jointly optimizing the pose detector with\nour framework may be a possible solution. It is also desir-\nable to extend masked-modeling-based self-supervised pre-\ntraining to RGB data. Besides, pre-training can go beyond\nself-supervised learning, e.g. multilingual information may\nbe merged as an auxiliary indicator for better performance\non downstream tasks.\n5 C ONCLUSION\nIn this paper, we propose the Ô¨Årst self-supervised pre-\ntrainable framework with hand prior incorporated, namely\nSignBERT+. Given the dominant role of hand during sign\nlanguage, we take both hands as visual tokens and care-\nfully embed each visual tokens with gesture state and\nspatial-temporal position information. Our framework Ô¨Årst\nperforms pre-training on a large volume of sign data via\nreconstructing the masked tokens from the corrupted input\nsequence. SpeciÔ¨Åcally, we subtly design hierarchical masked\nmodeling strategies (joint, frame and clip). These strate-\ngies explicitly consider hand pose characteristics to cap-\nture multi-level contextual information. Furthermore, we\ndesign the hand-model-aware decoder to incorporate prior\nfor better optimization and context modeling. Then, the\npre-trained SignBERT+ is Ô¨Åne-tuned for downstream tasks.\nGiven the task diversities, we design simple yet effective\nprediction heads on top of the SignBERT+ encoder during\nÔ¨Åne-tuning. Extensive experiments are conducted among\nthree main video-based sign language understanding tasks,\ni.e, isolated SLR, continuous SLR and SLT. Our experiment\nresults demonstrate the effectiveness of our method, achiev-\ning new state-of-the-art performance with a notable gain.\nBroader Impact.It is estimated by World Health Organiza-\ntion (WHO) that by 2050 over 700 million people will have\ndisabling hearing loss, which accounts for 10% of global\npopulation [102]. The community with hearing loss may\nfeel isolated, lonely and other mental issues when they face\nthe communication barrier in daily life. One way to assist\nthem is to bridge this gap via the automatic sign language\nunderstanding technique. Our framework is able to promote\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 17\nits development. However, our technique is not intended\nfor the potential privacy issue, such as surveillance on sign\nlanguage communication.\nREFERENCES\n[1] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‚ÄúIm-\nproving language understanding by generative pre-training,‚Äù\narxiv, pp. 1‚Äì12, 2018.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-\ntraining of deep bidirectional Transformers for language un-\nderstanding,‚Äù in Conference of the North American Chapter of the\nAssociation for Computational Linguistics (NAACL), 2018, pp. 4171‚Äì\n4186.\n[3] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V .\nLe, ‚ÄúXLNet: Generalized autoregressive pretraining for language\nunderstanding,‚Äù in Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2019, pp. 1‚Äì18.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in Neural Information Processing Systems (NeurIPS) ,\n2017, pp. 5999‚Äì6009.\n[5] H. Hu, W. Zhao, W. Zhou, Y. Wang, and H. Li, ‚ÄúSignBERT:\nPre-training of hand-model-aware representation for sign lan-\nguage recognition,‚Äù in International Conference on Computer Vi-\nsion (ICCV), 2021, pp. 11 087‚Äì11 096.\n[6] O. Koller, ‚ÄúQuantitative survey of the state of the art in sign\nlanguage recognition,‚Äù arXiv, pp. 1‚Äì40, 2020.\n[7] D. Li, C. Rodriguez, X. Yu, and H. Li, ‚ÄúTransferring cross-domain\nknowledge for video sign language recognition,‚Äù in IEEE/CVF\nInternational Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020, pp. 6205‚Äì6214.\n[8] Y. Min, A. Hao, X. Chai, and X. Chen, ‚ÄúVisual alignment con-\nstraint for continuous sign language recognition,‚Äù in International\nConference on Computer Vision (ICCV), 2021, pp. 11 542‚Äì11 551.\n[9] N. C. Camgoz, O. Koller, S. HadÔ¨Åeld, and R. Bowden, ‚ÄúSign\nlanguage Transformers: Joint end-to-end sign language recog-\nnition and translation,‚Äù in IEEE/CVF International Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020, pp. 10 023‚Äì\n10 033.\n[10] T. PÔ¨Åster, J. Charles, and A. Zisserman, ‚ÄúLarge-scale learning of\nsign language by watching TV (using co-occurrences).‚Äù in British\nMachine Vision Conference (BMVC), 2013, pp. 1‚Äì11.\n[11] J. F. Lichtenauer, E. A. Hendriks, and M. J. Reinders, ‚ÄúSign\nlanguage recognition by combining statistical DTW and inde-\npendent classiÔ¨Åcation,‚Äù IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TP AMI), vol. 30, no. 11, pp. 2040‚Äì2046, 2008.\n[12] Y. C. Bilge, R. G. Cinbis, and N. Ikizler-Cinbis, ‚ÄúTowards zero-\nshot sign language recognition,‚Äù IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TP AMI), pp. 1‚Äì16, 2022.\n[13] S. Albanie, G. Varol, L. Momeni, T. Afouras, J. S. Chung, N. Fox,\nand A. Zisserman, ‚ÄúBSL-1k: Scaling up co-articulated sign lan-\nguage recognition using mouthing cues,‚Äù in European Conference\non Computer Vision (ECCV), 2020, pp. 35‚Äì53.\n[14] O. Koller, J. Forster, and H. Ney, ‚ÄúContinuous sign language\nrecognition: Towards large vocabulary statistical recognition sys-\ntems handling multiple signers,‚Äù Computer Vision and Image Un-\nderstanding (CVIU), vol. 141, pp. 108‚Äì125, 2015.\n[15] O. Koller, C. Camgoz, H. Ney, and R. Bowden, ‚ÄúWeakly super-\nvised learning with multi-stream CNN-LSTM-HMMs to discover\nsequential parallelism in sign language videos,‚ÄùIEEE Transactions\non Pattern Analysis and Machine Intelligence (TP AMI), vol. 42, no. 9,\npp. 2306‚Äì2320, 2020.\n[16] J. Huang, W. Zhou, H. Li, and W. Li, ‚ÄúAttention based 3D-\nCNNs for large-vocabulary sign language recognition,‚Äù IEEE\nTransactions on Circuits and Systems for Video Technology (TCSVT) ,\nvol. 29, no. 9, pp. 2822‚Äì2832, 2019.\n[17] H. R. V . Joze and O. Koller, ‚ÄúMS-ASL: A large-scale data set and\nbenchmark for understanding american sign language,‚Äù British\nMachine Vision Conference (BMVC), pp. 1‚Äì16, 2019.\n[18] D. Li, C. Rodriguez, X. Yu, and H. Li, ‚ÄúWord-level deep sign\nlanguage recognition from video: A new large-scale dataset and\nmethods comparison,‚Äù in IEEE Winter Conference on Applications\nof Computer Vision (WACV), 2020, pp. 1459‚Äì1469.\n[19] C. Li, Q. Zhong, D. Xie, and S. Pu, ‚ÄúCo-occurrence feature\nlearning from skeleton data for action recognition and detection\nwith hierarchical aggregation,‚Äù in International Joint Conference on\nArtiÔ¨Åcial Intelligence (IJCAI), 2018, pp. 786‚Äì792.\n[20] Y. Du, W. Wang, and L. Wang, ‚ÄúHierarchical recurrent neural\nnetwork for skeleton based action recognition,‚Äù in IEEE/CVF\nInternational Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2015, pp. 1110‚Äì1118.\n[21] Y. Min, Y. Zhang, X. Chai, and X. Chen, ‚ÄúAn efÔ¨Åcient PointL-\nSTM for point clouds based gesture recognition,‚Äù in IEEE/CVF\nInternational Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020, pp. 5761‚Äì5770.\n[22] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, ‚ÄúAn end-to-end\nspatio-temporal attention model for human action recognition\nfrom skeleton data,‚Äù in AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence (AAAI), 2017, pp. 4263‚Äì4270.\n[23] A. Tunga, S. V . Nuthalapati, and J. Wachs, ‚ÄúPose-based sign\nlanguage recognition using GCN and BERT,‚Äù inWACV Workshop,\n2020, pp. 31‚Äì40.\n[24] S. Yan, Y. Xiong, and D. Lin, ‚ÄúSpatial temporal graph convolu-\ntional networks for skeleton-based action recognition,‚Äù in AAAI\nConference on ArtiÔ¨Åcial Intelligence (AAAI) , 2018, pp. 7444‚Äì7452.\n[25] O. Koller, S. Zargaran, and H. Ney, ‚ÄúRe-sign: Re-aligned end-to-\nend sequence modelling with deep recurrent CNN-HMMs,‚Äù in\nIEEE/CVF International Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017, pp. 4297‚Äì4305.\n[26] O. Koller, S. Zargaran, H. Ney, and R. Bowden, ‚ÄúDeep sign:\nEnabling robust statistical continuous sign language recognition\nvia hybrid CNN-HMMs,‚Äù International Journal of Computer Vi-\nsion (IJCV), vol. 126, no. 12, pp. 1311‚Äì1325, 2018.\n[27] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù\nNeural Computation, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.\n[28] K. Cho, B. Van Merri ¬®enboer, D. Bahdanau, and Y. Bengio, ‚ÄúOn\nthe properties of neural machine translation: Encoder-decoder\napproaches,‚Äù in Workshop on Syntax, Semantics and Structure in\nStatistical Translation (SSST), 2014, pp. 103‚Äì111.\n[29] R. Cui, H. Liu, and C. Zhang, ‚ÄúA deep neural framework for\ncontinuous sign language recognition by iterative training,‚ÄùIEEE\nTransactions on Multimedia (TMM) , vol. 21, no. 7, pp. 1880‚Äì1891,\n2019.\n[30] J. Pu, W. Zhou, H. Hu, and H. Li, ‚ÄúBoosting continuous sign\nlanguage recognition via cross modality augmentation,‚Äù in ACM\nInternational Conference on Multimedia (ACM MM), 2020, pp. 1497‚Äì\n1505.\n[31] B. Shi, A. M. D. Rio, J. Keane, D. Brentari, G. Shakhnarovich,\nand K. Livescu, ‚ÄúFingerspelling recognition in the wild with\niterative visual attention,‚Äù in International Conference on Computer\nVision (ICCV), 2019, pp. 5400‚Äì5409.\n[32] Z. Zhou, V . W. L. Tam, and E. Y. Lam, ‚ÄúSignbert: A bert-based\ndeep learning framework for continuous sign language recogni-\ntion,‚Äù IEEE Access, vol. 9, pp. 161 669‚Äì161 682, 2021.\n[33] N. Cihan Camgoz, S. HadÔ¨Åeld, O. Koller, H. Ney, and R. Bowden,\n‚ÄúNeural sign language translation,‚Äù in IEEE/CVF International\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2018, pp. 7784‚Äì7793.\n[34] D. Li, C. Xu, X. Yu, K. Zhang, B. Swift, H. Suominen, and H. Li,\n‚ÄúTSPNet: Hierarchical feature learning via temporal semantic\npyramid for sign language translation,‚Äù in Advances in Neural\nInformation Processing Systems (NeurIPS), 2020, pp. 1‚Äì12.\n[35] H. Zhou, W. Zhou, Y. Zhou, and H. Li, ‚ÄúSpatial-temporal multi-\ncue network for sign language recognition and translation,‚ÄùIEEE\nTransactions on Multimedia (TMM), vol. 24, pp. 768‚Äì779, 2021.\n[36] H. Zhou, W. Zhou, W. Qi, J. Pu, and H. Li, ‚ÄúImproving\nsign language translation with monolingual data by sign back-\ntranslation,‚Äù in IEEE/CVF International Conference on Computer\nVision and Pattern Recognition (CVPR), 2021, pp. 1316‚Äì1325.\n[37] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúIm-\nageNet: A large-scale hierarchical image database,‚Äù in IEEE/CVF\nInternational Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2009, pp. 248‚Äì255.\n[38] J. Carreira and A. Zisserman, ‚ÄúQuo vadis, action recognition? A\nnew model and the Kinetics dataset,‚Äù in IEEE/CVF International\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2017, pp. 6299‚Äì6308.\n[39] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and\nJ. Tang, ‚ÄúSelf-supervised learning: Generative or contrastive,‚Äù\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 18\nIEEE Transactions on Knowledge and Data Engineering (TKDE) , pp.\n1‚Äì20, 2021.\n[40] L. Jing and Y. Tian, ‚ÄúSelf-supervised visual feature learning with\ndeep neural networks: A survey,‚Äù IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TP AMI) , vol. 43, no. 11, pp.\n4037‚Äì4058, 2020.\n[41] R. Zhang, P . Isola, and A. A. Efros, ‚ÄúColorful image colorization,‚Äù\nin European Conference on Computer Vision (ECCV), 2016, pp. 649‚Äì\n666.\n[42] J. Wang, J. Jiao, L. Bao, S. He, W. Liu, and Y.-H. Liu, ‚ÄúSelf-\nsupervised video representation learning by uncovering spatio-\ntemporal statistics,‚Äù IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TP AMI), pp. 1‚Äì16, 2021.\n[43] S. Gidaris, P . Singh, and N. Komodakis, ‚ÄúUnsupervised repre-\nsentation learning by predicting image rotations,‚Äù in International\nConference on Learning Representations (ICLR), 2018, pp. 1‚Äì16.\n[44] M. Noroozi and P . Favaro, ‚ÄúUnsupervised learning of visual rep-\nresentations by solving jigsaw puzzles,‚Äù inEuropean Conference on\nComputer Vision (ECCV), 2016, pp. 69‚Äì84.\n[45] K. Wang, L. Lin, C. Jiang, C. Qian, and P . Wei, ‚Äú3D human pose\nmachines with self-supervised learning,‚Äù IEEE Transactions on\nPattern Analysis and Machine Intelligence (TP AMI) , vol. 42, no. 5,\npp. 1069‚Äì1082, 2019.\n[46] G.-J. Qi, L. Zhang, F. Lin, and X. Wang, ‚ÄúLearning general-\nized transformation equivariant representations via autoencod-\ning transformations,‚Äù IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TP AMI), vol. 44, no. 4, pp. 2045‚Äì2057, 2022.\n[47] I. Misra, C. L. Zitnick, and M. Hebert, ‚ÄúShufÔ¨Çe and learn: unsu-\npervised learning using temporal order veriÔ¨Åcation,‚Äù in European\nConference on Computer Vision (ECCV), 2016, pp. 527‚Äì544.\n[48] D. Xu, J. Xiao, Z. Zhao, J. Shao, D. Xie, and Y. Zhuang, ‚ÄúSelf-\nsupervised spatiotemporal learning via video clip order predic-\ntion,‚Äù in IEEE/CVF International Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 10 334‚Äì10 343.\n[49] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, ‚ÄúMomentum\ncontrast for unsupervised visual representation learning,‚Äù in\nIEEE/CVF International Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020, pp. 9729‚Äì9738.\n[50] L. Linguo, W. Minsi, N. Bingbing, W. Hang, Y. Jiancheng, and\nZ. Wenjun, ‚Äú3D human action representation learning via cross-\nview consistency pursuit,‚Äù in IEEE/CVF International Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n[51] X. Chen and K. He, ‚ÄúExploring simple siamese representation\nlearning,‚Äù in IEEE/CVF International Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021, pp. 15 750‚Äì15 758.\n[52] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple\nframework for contrastive learning of visual representations,‚Äù\nin International Conference on Machine Learning (ICML) , 2020, pp.\n1597‚Äì1607.\n[53] J.-B. Grill, F. Strub, F. Altch ¬¥e, C. Tallec, P . H. Richemond,\nE. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar\net al. , ‚ÄúBootstrap your own latent: A new approach to self-\nsupervised learning,‚Äù in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, pp. 21 271‚Äì21 284.\n[54] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoBERTa: A robustly\noptimized bert pretraining approach,‚Äù arXiv, pp. 1‚Äì13, 2019.\n[55] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,\n‚ÄúVideoBERT: A joint model for video and language repre-\nsentation learning,‚Äù in International Conference on Computer Vi-\nsion (ICCV), 2019, pp. 7464‚Äì7473.\n[56] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, ‚ÄúVL-\nBERT: Pre-training of generic visual-linguistic representations,‚Äù\nin International Conference on Learning Representations (ICLR), 2020,\npp. 1‚Äì16.\n[57] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, ‚ÄúGenerative pretraining from pixels,‚Äù inInternational\nConference on Machine Learning (ICML), 2020, pp. 1691‚Äì1703.\n[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., ‚ÄúAn image is worth 16x16 words: Transformers for image\nrecognition at scale,‚Äù in International Conference on Learning Repre-\nsentations (ICLR), 2020, pp. 1‚Äì21.\n[59] H. Bao, L. Dong, and F. Wei, ‚ÄúBEiT: Bert pre-training of image\ntransformers,‚Äù in International Conference on Learning Representa-\ntions (ICLR), 2022, pp. 1‚Äì16.\n[60] K. He, X. Chen, S. Xie, Y. Li, P . Doll ¬¥ar, and R. Girshick, ‚ÄúMasked\nautoencoders are scalable vision learners,‚Äù arXiv, pp. 1‚Äì8, 2021.\n[61] S. Sridhar, A. Oulasvirta, and C. Theobalt, ‚ÄúInteractive markerless\narticulated hand motion tracking using RGB and depth data,‚Äù in\nInternational Conference on Computer Vision (ICCV), 2013, pp. 2456‚Äì\n2463.\n[62] I. Oikonomidis, M. I. Lourakis, and A. A. Argyros, ‚ÄúEvolution-\nary quasi-random search for hand articulations tracking,‚Äù in\nIEEE/CVF International Conference on Computer Vision and Pattern\nRecognition (CVPR), 2014, pp. 3422‚Äì3429.\n[63] C. Qian, X. Sun, Y. Wei, X. Tang, and J. Sun, ‚ÄúRealtime and robust\nhand tracking from depth,‚Äù in IEEE/CVF International Conference\non Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1106‚Äì\n1113.\n[64] A. Tkach, M. Pauly, and A. Tagliasacchi, ‚ÄúSphere-meshes for\nreal-time hand modeling and tracking,‚Äù ACM Transactions on\nGraphics (TOG), vol. 35, no. 6, pp. 1‚Äì11, 2016.\n[65] I. Oikonomidis, N. Kyriazis, and A. A. Argyros, ‚ÄúEfÔ¨Åcient model-\nbased 3D tracking of hand articulations using Kinect,‚Äù in British\nMachine Vision Conference (BMVC), 2011, pp. 1‚Äì11.\n[66] L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Pollefeys,\n‚ÄúMotion capture of hands in action using discriminative salient\npoints,‚Äù in European Conference on Computer Vision (ECCV) , 2012,\npp. 640‚Äì653.\n[67] D. Tzionas, L. Ballan, A. Srikantha, P . Aponte, M. Pollefeys, and\nJ. Gall, ‚ÄúCapturing hands in action using discriminative salient\npoints and physics simulation,‚Äù International Journal of Computer\nVision (IJCV), vol. 118, no. 2, pp. 172‚Äì193, 2016.\n[68] J. Romero, D. Tzionas, and M. J. Black, ‚ÄúEmbodied hands: Model-\ning and capturing hands and bodies together,‚Äù ACM Transactions\non Graphics (TOG), vol. 36, no. 6, pp. 1‚Äì17, 2017.\n[69] J. P . Lewis, M. Cordner, and N. Fong, ‚ÄúPose space deformation:\na uniÔ¨Åed approach to shape interpolation and skeleton-driven\ndeformation,‚Äù in Annual Conference on Computer Graphics and\nInteractive Techniques (SIGGRAPH), 2000, pp. 165‚Äì172.\n[70] M. Habermann, W. Xu, M. Zollhofer, G. Pons-Moll, and\nC. Theobalt, ‚ÄúDeepCap: Monocular human performance capture\nusing weak supervision,‚Äù in IEEE/CVF International Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2020, pp. 5052‚Äì\n5063.\n[71] A. Boukhayma, R. d. Bem, and P . H. Torr, ‚Äú3D hand shape\nand pose from images in the wild,‚Äù in IEEE/CVF International\nConference on Computer Vision and Pattern Recognition (CVPR) ,\n2019, pp. 10 843‚Äì10 852.\n[72] H. Hu, W. Zhou, and H. Li, ‚ÄúHand-model-aware sign language\nrecognition,‚Äù in AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI) ,\n2021, pp. 1558‚Äì1566.\n[73] G. Moon, S.-I. Yu, H. Wen, T. Shiratori, and K. M. Lee, ‚ÄúInter-\nhand2. 6m: A dataset and baseline for 3D interacting hand pose\nestimation from a single RGB image,‚Äù in European Conference on\nComputer Vision (ECCV), 2020, pp. 548‚Äì564.\n[74] Y. Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thal-\nmann, ‚ÄúExploiting spatial-temporal relationships for 3D pose\nestimation via graph convolutional networks,‚Äù in International\nConference on Computer Vision (ICCV), 2019, pp. 2272‚Äì2281.\n[75] L. Kavan and J. ÀáZ¬¥ara, ‚ÄúSpherical blend skinning: a real-time de-\nformation of articulated models,‚Äù in ACM SIGGRAPH Symposium\non Interactive 3D Graphics and Games (I3D) , 2005, pp. 9‚Äì16.\n[76] N. C. Camgoz, S. HadÔ¨Åeld, O. Koller, and R. Bowden, ‚ÄúSub-\nUNets: End-to-end hand shape and continuous sign lan-\nguage recognition,‚Äù in International Conference on Computer Vi-\nsion (ICCV), 2017, pp. 3075‚Äì3084.\n[77] A. Graves, ‚ÄúGenerating sequences with recurrent neural net-\nworks,‚Äù arXiv, pp. 1‚Äì43, 2013.\n[78] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey et al., ‚ÄúGoogle‚Äôs neural\nmachine translation system: Bridging the gap between human\nand machine translation,‚Äù arXiv, pp. 1‚Äì23, 2016.\n[79] S. Yuan, Q. Ye, G. Garcia-Hernando, and T.-K. Kim, ‚ÄúThe 2017\nhands in the million challenge on 3D hand pose estimation,‚Äù\narXiv, pp. 1‚Äì7, 2017.\n[80] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBLEU: a method\nfor automatic evaluation of machine translation,‚Äù inAnnual Meet-\ning of the Association for Computational Linguistics (ACL) , 2002, pp.\n311‚Äì318.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, APRIL 2023 19\n[81] C.-Y. Lin, ‚ÄúROUGE: A package for automatic evaluation of sum-\nmaries,‚Äù in ACL workshop on Text Summarization Branches Out ,\n2004, pp. 74‚Äì81.\n[82] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , ‚ÄúPyTorch:\nAn imperative style, high-performance deep learning library,‚Äù in\nAdvances in Neural Information Processing Systems (NeurIPS), 2019,\npp. 8026‚Äì8037.\n[83] M. Contributors, ‚ÄúOpenMMLab pose estimation toolbox and\nbenchmark,‚Äù https://github.com/open-mmlab/mmpose, 2020.\n[84] H. Hu, W. Zhou, J. Pu, and H. Li, ‚ÄúGlobal-local enhancement net-\nwork for NMFs-aware sign language recognition,‚Äù ACM Trans-\nactions on Multimedia Computing, Communications, and Applica-\ntions (TOMM), vol. 17, no. 3, pp. 1‚Äì18, 2021.\n[85] A. Duarte, S. Palaskar, L. Ventura, D. Ghadiyaram, K. DeHaan,\nF. Metze, J. Torres, and X. Giro-i Nieto, ‚ÄúHow2sign: a large-scale\nmultimodal dataset for continuous american sign language,‚Äù in\nIEEE/CVF International Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021, pp. 2735‚Äì2744.\n[86] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, ‚ÄúA\nlarge-scale study on unsupervised spatiotemporal representation\nlearning,‚Äù in IEEE/CVF International Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021, pp. 3299‚Äì3309.\n[87] I. Laptev, ‚ÄúOn space-time interest points,‚Äù International Journal of\nComputer Vision (IJCV), vol. 64, no. 2-3, pp. 107‚Äì123, 2005.\n[88] A. Tang, K. Lu, Y. Wang, J. Huang, and H. Li, ‚ÄúA real-time hand\nposture recognition system using deep neural networks,‚Äù ACM\nTransactions on Intelligent Systems and Technology (TIST) , vol. 6,\nno. 2, pp. 1‚Äì23, 2015.\n[89] Z. Qiu, T. Yao, and T. Mei, ‚ÄúLearning spatio-temporal repre-\nsentation with pseudo-3D residual networks,‚Äù in International\nConference on Computer Vision (ICCV), 2017, pp. 5533‚Äì5541.\n[90] T. Jiang, N. C. Camgoz, and R. Bowden, ‚ÄúSkeletor: Skeletal trans-\nformers for robust body-pose estimation,‚Äù in CVPR Workshop ,\n2021, pp. 3394‚Äì3402.\n[91] N. C. Camgoz, O. Koller, S. HadÔ¨Åeld, and R. Bowden, ‚ÄúMulti-\nchannel transformers for multi-articulatory sign language trans-\nlation,‚Äù in ECCV Workshop, 2020, pp. 301‚Äì319.\n[92] A. Yin, Z. Zhao, J. Liu, W. Jin, M. Zhang, X. Zeng, and X. He,\n‚ÄúSimulSLT: End-to-end simultaneous sign language translation,‚Äù\nin ACM International Conference on Multimedia (ACM MM) , 2021,\npp. 4118‚Äì4127.\n[93] O. Koller, H. Ney, and R. Bowden, ‚ÄúDeep hand: How to train\na cnn on 1 million hand images when your data is continuous\nand weakly labelled,‚Äù in IEEE/CVF International Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2016, pp. 3793‚Äì\n3802.\n[94] O. Koller, O. Zargaran, H. Ney, and R. Bowden, ‚ÄúDeep sign:\nHybrid CNN-HMM for continuous sign language recognition,‚Äù\nin British Machine Vision Conference (BMVC), 2016, pp. 1‚Äì12.\n[95] R. Cui, H. Liu, and C. Zhang, ‚ÄúRecurrent convolutional neural\nnetworks for continuous sign language recognition by staged\noptimization,‚Äù in IEEE/CVF International Conference on Computer\nVision and Pattern Recognition (CVPR), 2017, pp. 7361‚Äì7369.\n[96] S. Wang, D. Guo, W.-g. Zhou, Z.-J. Zha, and M. Wang, ‚ÄúConnec-\ntionist temporal fusion for sign language translation,‚Äù in ACM\nInternational Conference on Multimedia (ACM MM), 2018, pp. 1483‚Äì\n1491.\n[97] J. Pu, W. Zhou, and H. Li, ‚ÄúDilated convolutional network with\niterative optimization for continuous sign language recognition,‚Äù\nin International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI) ,\n2018, pp. 885‚Äì891.\n[98] ‚Äî‚Äî, ‚ÄúIterative alignment network for continuous sign language\nrecognition,‚Äù in IEEE/CVF International Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 4165‚Äì4174.\n[99] K. L. Cheng, Z. Yang, Q. Chen, and Y.-W. Tai, ‚ÄúFully convolu-\ntional networks for continuous sign language recognition,‚Äù in\nEuropean Conference on Computer Vision (ECCV) , 2020, pp. 697‚Äì\n714.\n[100] P . Xie, M. Zhao, and X. Hu, ‚ÄúPiSLTRc: Position-informed sign\nlanguage transformer with content-aware convolution,‚Äù IEEE\nTransactions on Multimedia (TMM), pp. 1‚Äì13, 2021.\n[101] C. Xu, D. Li, H. Li, H. Suominen, and B. Swift, ‚ÄúAutomatic\ngloss dictionary for sign language learners,‚Äù in ACL: System\nDemonstrations, 2022, pp. 83‚Äì92.\n[102] W. H. Organization. Deafness and hearing loss. (2021,\nApril 1). [Online]. Available: https://www.who.int/news-room/\nfact-sheets/detail/deafness-and-hearing-loss",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8827089071273804
    },
    {
      "name": "Interpretability",
      "score": 0.7037789225578308
    },
    {
      "name": "Sign language",
      "score": 0.6837233304977417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6437600255012512
    },
    {
      "name": "Gesture",
      "score": 0.5484758019447327
    },
    {
      "name": "Language model",
      "score": 0.524903416633606
    },
    {
      "name": "Security token",
      "score": 0.511214017868042
    },
    {
      "name": "Machine learning",
      "score": 0.4979698657989502
    },
    {
      "name": "Deep learning",
      "score": 0.4288361966609955
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}