{
  "title": "Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition",
  "url": "https://openalex.org/W3115058362",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5050564586",
      "name": "Cedric Lothritz",
      "affiliations": [
        "University of Luxembourg"
      ]
    },
    {
      "id": "https://openalex.org/A5079777016",
      "name": "Kevin Allix",
      "affiliations": [
        "University of Luxembourg"
      ]
    },
    {
      "id": "https://openalex.org/A5000702987",
      "name": "Lisa Veiber",
      "affiliations": [
        "University of Luxembourg"
      ]
    },
    {
      "id": "https://openalex.org/A5082835974",
      "name": "Tegawendé F. Bissyandé",
      "affiliations": [
        "University of Luxembourg"
      ]
    },
    {
      "id": "https://openalex.org/A5040326968",
      "name": "Jacques Klein",
      "affiliations": [
        "University of Luxembourg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963940534",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2007119365",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2589025090",
    "https://openalex.org/W2782403934",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2738180183",
    "https://openalex.org/W2151846280",
    "https://openalex.org/W1986544831",
    "https://openalex.org/W2855916967",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and has remained an active research field. In recent years, transformer models and more specifically the BERT model developed at Google revolutionised the field of NLP. While the performance of transformer-based approaches such as BERT has been studied for NER, there has not yet been a study for the fine-grained Named Entity Recognition (FG-NER) task. In this paper, we compare three transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based models (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each model to a multitude of distinct domains. We find that transformer-based models incrementally outperform the studied non-transformer-based models in most domains with respect to the F1 score. Furthermore, we find that the choice of domains significantly influenced the performance regardless of the respective data size or the model chosen.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3750–3760\nBarcelona, Spain (Online), December 8-13, 2020\n3750\nEvaluating Pretrained Transformer-based Models on the Task of\nFine-Grained Named Entity Recognition\nCedric Lothritz\nUniversity of Luxembourg\ncedric.lothritz@uni.lu\nKevin Allix\nUniversity of Luxembourg\nkevin.allix@uni.lu\nLisa Veiber\nUniversity of Luxembourg\nlisa.veiber@uni.lu\nTegawend´e F. Bissyand´e\nUniversity of Luxembourg\ntegawende.bissyande@uni.lu\nJacques Klein\nUniversity of Luxembourg\njacques.klein@uni.lu\nAbstract\nNamed Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and\nhas remained an active research ﬁeld. In recent years, transformer models and more speciﬁcally\nthe BERT model developed at Google revolutionised the ﬁeld of NLP. While the performance of\ntransformer-based approaches such as BERT has been studied for NER, there has not yet been a\nstudy for the ﬁne-grained Named Entity Recognition (FG-NER) task. In this paper, we compare\nthree transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based\nmodels (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each model to a multitude of\ndistinct domains. We ﬁnd that transformer-based models incrementally outperform the studied\nnon-transformer-based models in most domains with respect to the F1 score. Furthermore, we\nﬁnd that the choice of domain signiﬁcantly inﬂuenced the performance regardless of the respec-\ntive data size or the model chosen.\n1 Introduction\nNamed Entity Recognition (NER) is part of the fundamental tasks in Natural Language Processing\n(NLP). The main objective of NER is to detect and classify proper names (named entities) in a free\ntext. Typically, named entities can be subdivided into four broad categories: persons, i.e., ﬁrst and last\nnames, locations such as countries or landscapes, organisations such as companies or political parties,\nand miscellaneous entities which serves as a catch-all category for other named entities such as brands,\nmeals, or social events. NER is an active research ﬁeld and state-of-the-art solutions such as spaCy 1,\nﬂair (Akbik et al., 2018), and Primer 2 manage to achieve near-human performance. However, classical\nNER (which we refer to as coarse-grained NER in this paper) models typically distinguish between only\na small number of entity types, usually fewer than a dozen distinct categories.\nWhile this kind of shallow classiﬁcation is sufﬁcient for many applications, there are industrial use-\ncases in which more precise information is necessary such as ﬁnancial documents processing in the\nbanking and ﬁnance context. For instance, application forms for a business loan are usually supplied\nwith several supporting textual documents. These can contain the names of different types of persons,\nsuch as the owner or the CEO of the applying company, the contact person(s) at the issuing bank, ﬁnance\nanalysts, or lawyers. The same is true for organisation names such as the name of the issuing bank, a\ngovernment agency, or the name of the applying company or third-party companies. It is necessary to\nnot only detect entity names, but to also qualify and differentiate between various entity types. Indeed,\nin many contexts the actual name of an entity is important only if it can be associated to a role, or any\nother relevant quality. In the banking and ﬁnance world for example, the strict regulatory requirements\ncannot be satisﬁed with just a list of who is involved; knowing how entities are involved is a necessity.\nThe term ”Fine-Grained Named Entity Recognition” (FG-NER) was ﬁrst coined by Fleischman and\nHovy (2002). It describes a subtask of NER, where the objective remains the same as standard NER,\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n1https://spacy.io\n2https://primer.ai/blog/a-new-state-of-the-art-for-named-entity-recognition/\n3751\nbut where the number of entity types is considerably higher. In extreme cases, FG-NER models such as\nthe ﬁne-grained entity recognizer (FIGER) (Ling and Weld, 2012) are able to distinguish between more\nthan 100 distinct labels.\nConditional Random Field (CRF) models (Lafferty et al., 2001) have been popular for numerous\nsequence-to-sequence tasks such as NER. They perform reasonably well and can serve as a baseline for\nthe task of FG-NER.\nIn a previous study, Mai et al. (2018) compared the performance of several FG-NER approaches for\nthe English and Japanese languages. They found that the BiLSTM-CNN-CRF model devised by Ma\nand Hovy (2016) combined with gazetteers performed the best in terms of F1 score for the English\nlanguage. They also found that BiLSTM-CNN-CRF performed well without the use of gazetteers. In\nfact, among the models that did not make use of gazetteers, BiLSTM-CNN-CRF achieved the highest F1\nscore. In 2017, the introduction of the transformer model (Vaswani et al., 2017) revolutionised the NLP\nlandscape and led to a number of novel language modeling approaches which manage to outperform\nstate-of-the-art models in numerous tasks. In 2018, Devlin et al. (2019) developed the Bidirectional\nEncoder Representations from Transformers (BERT) model, a powerful language modeling technique\nwhich is considered as one of the most signiﬁcant breakthroughs in NLP in recent memory. BERT\nmodels are pretrained on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks.\nDevlin et al. (2019) ﬁne-tuned the resulting models on several fundamental NLP tasks such as the GLUE\nlanguage understanding tasks (Wang et al., 2018), the SQuAD question answering task (Rajpurkar et al.,\n2016), and the SW AG Common Sense Inference task (Zellers et al., 2018), for which BERT manages to\nachieve state-of-the-art performances. Furthermore, Devlin et al. (2019) reported an F1 score of 92.8%\nwhen ﬁne-tuned on the CoNLL-2003 dataset for NER (Sang and De Meulder, 2003), achieving similar\nresults as state-of-the-art models such as Contextual String Embeddings (Akbik et al., 2018) and ELMo\nEmbeddings (Peters et al., 2017).\nImproving on the BERT model, Liu et al. (2019) at Facebook AI 3 developed a Robustly optimized\nBERT approach (RoBERTa). They claim that the standard BERT models were undertrained and pro-\nposed a new version of BERT that was trained for a longer time, on longer sequences, on more data,\nand with larger batches. Furthermore, they trained only on the MLM task and with dynamic changes\nof the masking patterns applied to training data. BERT’s pretraining steps was performed on the same\ndataset using the same masked locations for the entire MLM task. RoBERTa mitigated that problem by\nduplicating their dataset ten times, and using different masking patterns for each duplicate. They report\nthat ﬁne-tuned models derived from RoBERTa either matched or improved on BERT models in terms of\nperformance, although they did not perform tests speciﬁcally on the NER task.\n2019 also saw an attempt to solve the shortcomings of BERT in terms of the training approach. Yang\net al. (2019) presented XLNet. During the MLM pretraining task of BERT, a special [MASK] token\nis introduced in the training set. According to (Yang et al., 2019), BERT models neglect dependencies\nbetween the masked tokens. Furthermore, this token is absent in the ﬁne-tuning tasks, resulting in a\npretrain/ﬁne-tune discrepancy. XLNet avoids this shortcoming as it does not mask its tokens, and instead\npermutes the order of token predictions. Yang et al. (2019) reports that XLNet outperforms BERT in\n20 NLP tasks, speciﬁcally language understanding, reading comprehension, text classiﬁcation and doc-\nument ranking tasks. They do not report any results on sequence-to-sequence tasks like NER.\nWhile BERT, RoBERTa, and XLNet (which we refer to as transformer-based models throughout the pa-\nper) achieve state-of-the-art performances in numerous Natural Language Understanding (NLU) tasks,\nwe observe a lack of research in the area of FG-NER. In this paper, we present an empirical study of\nthe performance of FG-NER approaches derived from a pretrained BERT, a pretrained RoBERTa, and a\npretrained XLNet model as well as a comparison to a simple CRF model and the model presented by Ma\nand Hovy (2016). Furthermore, we apply these approaches to a large number of distinct domains, with\nvarying numbers of data samples and entity categories.\nSpeciﬁcally, we will address the following research questions:\n•RQ1: Do transformer-based models outperform the state-of-the-art model for the FG-NER task?\n3https://ai.facebook.com/\n3752\nID domain #sentences#words #named entities#entity types#entity types\nbefore removalafter removal\n1 physics 68 1916 144 6 5\n2 fashion 1043 27 598 2182 68 20\n3 ﬁnance 1723 42 834 4121 75 24\n4 exhibitions 1829 40 162 2960 131 34\n5 meteorology 2838 69 551 7659 92 32\n... ... ... ... ... ... ..\n30 food 41 160 1 034 233 100 445 415 50\n31 media 49 714 1 269 641 142 084 959 50\n32 biology 53 042 1 248 434 142 084 246 50\n33 travel 59 965 1 467 691 152 712 750 50\n34 business 68 244 1 688 935 182 306 1009 50\n... ... ... ... ... ... ...\n45 government 331 720 8 380 706 1 170 947 1182 51\n46 ﬁlm 430 693 9 557 747 1 720 973 1134 51\n47 music 441 22010 116 628 1 684 479 918 50\n48 people 442 68311 452 451 1 762 255 1825 50\n49 location 443 64612 525 545 1 472 198 1603 50\nTable 1: Statistics for a selection of datasets\n•RQ2: What are the strengths, weaknesses, and trade-offs of each investigated model?\n•RQ3: How does the choice of the domain inﬂuence the performance of the models?\nWe use the EWNERTC dataset published by Sahin et al. (2017a), containing roughly 7 million data\nsamples in 49 different domains. To the best of our knowledge, our study is the ﬁrst aiming to precisely\nevaluate the performance of these existing approaches on the FG-NER task.\n2 Experimental Setup\nIn this section, we present the dataset used in this study and we introduce the different models that we\ncompare against each other.\n2.1 Dataset\nFor this study, we apply the selected models to the English Wikipedia Named Entity Recognition and\nText Categorization (EWNERTC) dataset 4 published by Sahin et al. (2017b). It is a collection of auto-\nmatically categorised and annotated sentences from Wikipedia articles. The original dataset consists of\nroughly 7 million annotated sentences, divided into 49 separate domains. These 49 domains vary sig-\nniﬁcantly in overall size and number of entity types. The physics domain is the smallest subset with 68\nsentences, 144 entities and merely 6 distinct entity types. In contrast, the location domain is the largest\nsubset with 443 646 sentences, 1 472 198 entities, and 1603 types. Table 1 contains statistics for a small\nselection of domains.5 Physics, fashion, ﬁnance, exhibitions, and meteorology are the ﬁve smallest sets,\nconsisting of fewer than 3000 sentences each. Food, media, biology, travel, and business are medium-\nsized sets, comprising between 40 000 and 70 000 sentences. Finally, government, ﬁlm, music, people,\nand location are the largest sets with more than 300 000 sentences each.\nIt is noteworthy that thephysics dataset is an obvious outlier in terms of size (since the second smallest\ndataset is the fashion dataset, which contains an order of magnitude more sentences). It is possible that\nthe size of the physics subset is too small to produce meaningful results.\nFor this study, the number of entity types was drastically reduced. This measure was taken for two\nreasons: most entity types appear only a few times in any given subset. Furthermore, the training time\nfor CRF models tends to explode when dealing with a high number of entity types according to Mai et\nal. (2018). We limited the number of entity types per domain to the top 50 and, if necessary, added a\nmiscellaneous type as a catch-all for all remaining named entities.\n4https://data.mendeley.com/datasets/cdcztymf4k/1\n5Link to the full table: https://github.com/lothritz/FG-NER-data-statistics/blob/master/results ewnertc.csv\n3753\n2.2 Approaches\nIn this section, we present the ﬁve models that we investigate for this study in more detail and we specify\nthe conﬁguration of each model.\n2.2.1 CRF\nAs CRF models remain largely popular solutions for sequence-to-sequence tasks, we use a simple CRF\nmodel as a baseline. We use a large number of context and word shape features such as casing infor-\nmation and whether or not the word contains numerical characters. While simple CRF models generally\nperform well for coarse-grained NER, they require custom-made features and their usefulness is limited\nfor FG-NER according to Mai et al. (2018) who observed that CRF models tend to require too much\ntime to ﬁnish when handling a large number of labels. We use the sklearn crfsuite API6 for python with\nthe following hyperparameters for training: gradient descent using the L-BFGS method as the training\nalgorithm with a maximum of 100 iterations. The coefﬁcients for L1 and L2 regularisation are ﬁxed to\nC1 = 0.4 and C2 = 0.0. We use the following features: the word itself, casing information, is the word\nalphabetical, numerical or alphanumerical, sufﬁxes and preﬁxes, as well as the words and features in a\ntwo-words context window. Considering that the datasets are numerous and very diverse, we decided\nagainst using specialised gazetteers/dictionaries for this study, despite their proven usefulness in earlier\nstudies (Mai et al., 2018).\n2.2.2 BiLSTM-CNN-CRF\nAs our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b) 7 of the\nBiLSTM-CNN-CRF model proposed by Ma and Hovy (2016). The model consists of a combination of\na convolutional neural network (CNN) layer, a bidirectional long short-term memory (BiLSTM) layer,\nand a CRF layer. In a ﬁrst step, the CNN is used to extract character-level representations of given words\nwhich are then concatenated with word embeddings to create word level representations of the input\ntokens. These representations are fed into a forward and a backward LSTM layer, creating a bidirectional\nencoding of the input sequence. Finally, a CRF layer decodes the resulting representations into the\nmost probable label sequence (Ma and Hovy, 2016). Mai et al. (2018) achieved the best performance\nwith a combination of gazetteers and BiLSTM+CNN+CRF, but as was mentioned above, we do not\nuse gazetteers for this study due to the diverse nature of our datasets. We use the hyperparameters\nrecommended by Reimers and Gurevych (2017a) as they were shown to be useful for coarse-grained\nNER. We also useGlobal Vectors (GLoVe)8 word embeddings with 300 dimensions for the same reason.\n2.2.3 BERT\nPretraining a language model can take several days due to its large amount of trainable parameters. Fur-\nthermore, a sizable amount of data is required to achieve good results. Indeed, we tried to train a few\nlanguage models using the EWNERTC dataset, but it is too small and the resulting models were essen-\ntially unusable as they yielded very low F1 scores. Fortunately, Google provides a variety of pretrained\nmodels that have been trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, amounting\nto a grand total of 3.3 billion words. We use the Transformers library 9 provided by Huggingface (Wolf\net al., 2019) which allows to pretrain and ﬁne-tune BERT models with a simpliﬁed procedure using CLI\ncommands. For this study, we ﬁne-tune an English BERT Base model using each dataset separately.\nAs we compare models for FG-NER, we chose the cased model as recommended, in order to preserve\ncasing information. The BERT Base model contains 12 transformer blocks, 768 hidden layers, 12 self-\nattention blocks, and 110 million parameters in total. While the BERT Large model yields better results\nin every task that Devlin et al. (2019) investigated, the BERT Base model can be useful for determining\na lower boundary for the performance. Devlin et al. (2019) report that the recommended hyperparame-\nters vary depending on the NER task, but generally the best performances are observed for a batch size\n6https://github.com/TeamHG-Memex/sklearn-crfsuite\n7https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf\n8https://github.com/stanfordnlp/GloVe\n9https://github.com/huggingface/transformers\n3754\nin {16, 32}, a learning rate in {2−5, 3−5, 5−5}, and training epochs in {2, 3, 4}. After testing on three\nspeciﬁc domains (comic books, symbols, and ﬁctional universe with 21 262, 21 171 and 39 781 sentences\nrespectively), we found that a batch size of 16, a learning rate of 5−5, and 5 training epochs yielded the\nhighest F1 scores.\n2.2.4 RoBERTa\nRoBERTa presents similar challenges as BERT as it needs a large amount of resources, time and data. Liu\net al. (2019) provide pretrained models, trained on 160GB of text, which represents about 3-4 times the\namount of data used for pretraining BERT. We use the RoBERTa Base model, which contains 12 trans-\nformer blocks, 768 hidden layers, 12 self-attention heads, and 125 million trainable parameters. We ﬁne-\ntune it on each dataset separately. Similar to the pretrained BERT model, the pretrained RoBERTa model\nis also cased, making it appropriate for ﬁne-tuning on NER tasks. Liu et al. (2019) trained RoBERTa\nusing the same hyperparameters as BERT, except for the number of training epochs which they ﬁxed\nto ten. We perform a similar grid search as for BERT, i.e., a batch size in {16, 32}, and a learning\nrate in {2−5, 3−5, 5−5}, but training epochs in {2, 4, 6, 8, 10}. Testing on the comic books, symbols,\nand ﬁctional universe, we found that a batch size of 16 , a learning rate of 5−5, and 10 training epochs\nperformed best with regards to F1 score.\n2.2.5 XLNet\nWhile the pretraining approach of the XLNet model differs signiﬁcantly from BERT models, the pre-\ntraining step still requires a vast amount of resources and time. Thus, we once again use a pretrained\nmodel rather than training one ourselves. For the comparison, we use the cased XLNet Base model with\n12 transformer blocks, 768 hidden layers, 12 self-attention heads, and 110 million parameters. Yang\net al. (2019) ﬁne-tuned their pretrained model using the same hyperparameters as the BERT models to\ncompare their performances. We perform the same hyperparameter grid search as for BERT, and get the\nbest F1 score with a batch size of 16, a learning rate of 5−5 and 5 training epochs for the domains comic\nbooks, symbols, and ﬁctional universe.\n3 Experimental Results\nIn this section, we will answer the three research questions that we formulated for this study (cf. Sec-\ntion 1). Table 2 shows the performance of the ﬁve models for each domain. In order to account for\nthe imbalanced distribution of the entity types, we opt to calculate micro-averaged performance scores\nwhich takes into account the frequency of every entity type. To facilitate reading, we highlight (in bold)\nthe highest F1 score for each domain.\n3.1 RQ1: Do transformer-based models outperform the state-of-the-art model for the FG-NER\ntask?\nThe results indicate that, overall, the transformer-based models outperform CRF and BiLSTM-CNN-\nCRF in most domains in terms of F1 score. Speciﬁcally, the results show that the BERT and RoBERTa\nmodels yield the highest and second-highest F1 scores for almost every domain. BERT has the highest\nF1 score in 36 out of 49 domains, while RoBERTa achieves the best F1 score in 10 out of 49 domains.\nWhile XLNet outperforms BiLSTM-CNN-CRF in most domains, its performance scores are slightly\nlower than the ones of both the BERT and RoBERTa models. It is also noteworthy that XLNet performs\nconsistently worse than BiLSTM-CNN-CRF in the ten smallest domains.\nFigure 1a provides the boxplots showing the distributions of the F1 scores over all the domains\nacross the ﬁve models. We can make two observations. The boxplots indicate that, on average, all\nof the transformer-based models achieve higher performances than both CRF and BiLSTM-CNN-CRF.\nFurthermore, we can observe that the ranges, and, more importantly, the interquartile ranges of the\ntransformer-based models are smaller. This indicates that their performances are more stable and less\nsensitive to the choice of domain than the performances of CRF and BiLSTM-CNN-CRF.\n3755\nCRF BiLSTM-CNN-CRFBERT RoBERTa XLNetID domain #sentencesprec rec F1 prec rec F1 prec rec F1 prec rec F1 prec rec F11 physics 68 1 0.7780.8751 0.8330.9090.8570.6670.75 0.5 0.4440.4710.7060.6670.6862 fashion 10430.92 0.7650.8360.8940.7760.8310.8490.8010.8240.8160.8160.8160.8250.77 0.7973 ﬁnance 17230.8590.7080.7760.83 0.7310.7770.8070.7960.8020.7940.8390.8150.7680.7590.7644 exhibitions 18290.9010.7370.8110.8310.7440.7850.7650.7540.76 0.7880.7820.7850.7590.74 0.755 meteorology 28380.7480.6750.7090.75 0.7530.7510.7460.79 0.7670.7550.7920.7730.7220.7420.7326 interests 34620.9430.8110.8720.9120.8430.8760.8770.8680.8720.8870.8750.8810.8730.8380.8557 measurement unit38640.8220.7070.76 0.8120.7720.7910.7940.8060.8 0.79 0.7950.7920.7730.7850.7798 internet 39150.83 0.63 0.7160.7680.6570.7090.7270.7120.7190.7490.7250.7370.73 0.6870.7089 engineering 44750.8560.63 0.7260.7640.6910.7260.7340.7220.7280.7390.7250.7320.6940.6890.69110 chemistry 48830.8690.7360.7970.8740.7680.8180.8360.8230.8290.8150.8230.8190.81 0.8050.80811 astronomy 82980.85 0.7430.7920.8250.7810.8020.8250.8330.8290.8310.8310.8310.8210.8140.81712 automotive 10 3490.7990.7350.7660.7880.7790.7840.7920.8160.8030.7730.7970.7850.7720.8010.78613 soccer 11 3980.7660.6470.7020.7790.6810.7270.77 0.7730.7720.7560.7690.7630.7610.7640.76314 opera 11 5590.8650.74 0.7980.8250.7760.8 0.8270.8470.8370.83 0.8390.8340.8140.8240.81915 law 11 8130.7920.64 0.7080.7560.7010.7270.75 0.7590.7540.7580.7520.7550.7610.7450.75316 visual art 12 0590.8610.6490.74 0.81 0.6740.7360.7660.7250.7450.7740.7210.7470.7610.7180.73817 basketball 12 6040.8360.7960.8150.8320.83 0.8310.8330.8490.8410.8280.85 0.8390.8240.8440.83418 computer 12 9550.8140.6730.7370.7680.74 0.7540.7620.7730.7670.7550.7670.7610.7480.7570.75219 theater 15 3400.79 0.6080.6880.7330.6580.6940.7090.7190.7140.7190.7250.7220.7 0.6970.69820 symbols 21 1710.72 0.5710.6370.7150.62 0.6640.7230.7270.7250.7240.7120.7180.7110.6990.70521 comic books 21 2620.8540.7110.7760.8080.7490.7770.8080.8290.8180.8180.8210.82 0.7960.8150.80522 language 21 3060.8030.74 0.77 0.79 0.7640.7770.81 0.8160.8130.7990.8090.8040.7870.8 0.79323 religion 27 9770.8050.6970.7470.7870.7610.7740.8080.81 0.8090.8 0.7960.7980.7870.7910.78924 time 28 9030.7170.5650.6320.6970.63 0.6620.7160.7220.7190.7040.7040.7040.7040.7050.70525 royalty 30 5870.8040.7250.7620.7850.76 0.7720.7860.7980.7920.7790.7880.7840.7740.7850.77926 games 31 4200.8390.7410.7870.7960.77 0.7830.79 0.8130.8010.7890.81 0.7990.7680.7910.77927 aviation 36 9240.7950.7120.7510.7790.73 0.7540.7890.8070.7980.7810.7970.7890.7740.79 0.78228 medicine 37 7290.8480.6970.7650.7970.7550.7760.8020.7880.7950.7910.7880.7890.7990.7910.79529 ﬁctional universe39 7810.8740.7560.8110.8450.7810.8120.8430.8550.8490.8410.8480.8450.8370.8420.83930 food 41 1600.8010.6480.7170.7460.69 0.7170.7760.7880.7820.76 0.7660.7630.7520.7740.76331 media common49 7140.8620.7230.7860.8190.7550.7860.8060.8250.8150.8070.8190.8130.8030.8120.80732 biology 53 0420.8540.7710.8110.8430.8070.8250.8340.8470.84 0.8320.8370.8340.8360.8370.83633 travel 59 9650.8220.6960.7540.8030.7190.7590.7840.79 0.7870.7640.7720.7680.7790.7770.77834 business 68 2440.8030.6340.7090.7560.6660.7080.7650.7710.7680.7550.7590.7570.7520.7540.75335 architecture 76 3220.7090.5880.6430.6850.6270.6540.7070.7220.7150.6880.7010.6940.6850.6950.6936 geography 94 7120.8130.7280.7680.8010.7520.7760.7980.8150.8060.7950.8040.7990.7890.7990.79437 military 95 8090.8360.7310.78 0.82 0.7780.7980.8160.8270.8210.8110.8210.8160.8090.8230.81638 transportation 111 8640.8280.7380.7810.8340.8040.8190.8450.8570.8510.8450.85 0.8480.8390.8440.84139 award 117 2800.7020.6170.6570.7020.6710.6860.6850.7160.7 0.6820.7070.6940.6890.7030.69540 book 135 8650.7610.6040.6750.7170.6390.6760.7110.73 0.7210.7080.7230.7160.7160.7220.71941 organization 146 5830.7690.64 0.6980.7650.6740.7170.7670.7760.7710.7560.7660.7610.7620.7680.76542 tv 154 1520.7250.5740.6410.7330.6030.6620.6970.6960.6960.6880.6860.6870.7020.6840.69343 sports 171 6450.7810.7050.7410.7990.7670.7830.8060.8220.8140.8010.8160.8080.8070.8190.81344 education 212 4230.7340.6530.6910.7470.7060.7260.7690.78 0.7740.7630.7740.7690.7690.7740.77145 government 331 7200.81 0.7250.7650.8150.7640.7890.8210.8280.8250.8160.8240.82 0.8240.8250.82446 ﬁlm 478 4790.75 0.68 0.7130.7430.6950.7180.7690.7730.7710.7660.7670.7660.7720.7680.7747 music 462 9490.7860.6540.7140.78 0.6680.72 0.7440.7440.7440.7390.7360.7370.7520.7360.74448 people 442 6830.8360.7710.8020.8470.7950.82 0.83 0.83 0.83 0.8250.8210.8230.8340.8250.82949 location 443 6460.8090.7030.7520.8 0.7130.7540.79 0.7890.79 0.7750.7720.7740.7840.7750.78\nTable 2: Micro-averaged results of each model for every domain. Bold text indicates the highest F1 score\nfor the domain.\n3.2 RQ2: What are the strengths, weaknesses, and trade-offs of each investigated model?\nWhile the transformer-based models clearly outperform the other models with regards to the F1 score,\nit is worth examining the precision and recall scores as well. Regarding the precision, the CRF model\nalmost consistently outperforms all of the other models as shown in Table 2. When compared to the\nBiLSTM-CNN-CRF model, the transformer-based models perform worse in most domains in terms of\nprecision. In fact, BERT outperforms BiLSTM-CNN-CRF in less than half of the domains, RoBERTa\noutperforms BiLSTM-CNN-CRF in only a third of the domains and XLNet outperforms it in only a ﬁfth\nof the domains. Figure 1b shows the distribution of the precision scores over all the domains across the\nﬁve models. The boxplots conﬁrm the strength of CRF over the other models. Furthermore, they show\nthat BiLSTM-CNN-CRF performs slightly better than the transformer-based models, albeit at a loss of\nstability as indicated by the large range.\nOn the other hand, the transformer-based models signiﬁcantly outperform the other models with re-\ngards to recall as seen in Table 2. In fact, both BERT and RoBERTa signiﬁcantly outperform CRF and\nBiLSTM-CNN-CRF in almost every domain, while XLNet outperforms them in most. The same result\n3756\n(a) Distribution of F1 scores\n (b) Distribution of Precision\n (c) Distribution of Recall\nFigure 1: Distribution the performance of the ﬁve models used\ncan be observed in Figure 1c. The transformer-based models not only outperform the other models, but\ntheir interquartile ranges are signiﬁcantly smaller as well. This difference in recall score also explains\nthe higher F1 scores for the transformer-based models.\nTo summarise, CRF shows its strength in terms of precision, BERT, RoBERTa, and XLNet perform\nwell with regards to both recall and F1 score, with BERT usually achieving the highest performances.\nThe BiLSTM-CNN-CRF model acts as a trade-off between CRF and the transformer-based models.\n3.3 RQ3: How does the choice of the domain inﬂuence the performance of the models?\nFigure 1a shows that while different models may achieve signiﬁcantly different performance, no ap-\nproach yields a signiﬁcant breakthrough, w.r.t the others, for the task at hand, and all leave room for\nimprovement. The ﬁve tested models obtained relatively stable performances, as is visible from the fact\nthat boxes, which represent the performance measurements of 50% of the domains, cover only a ±0.05\nband around the average.\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n0 5 10 15 20 25 30 35 40 45 50\nDom ain I D (ordered by size)\nC RF\n B-C -C\n BERT\n RoBERTa\n XLNet\n C RF\n B-C -C\n BERT\n RoBERTa\n XLNet\nFigure 2: Performances of the ﬁve models for every domain.\nFigure 2, that plots the F1 scores for every domain (ordered by size), reveals however that all mod-\nels are similarly impacted by domains: with the exceptions of the four smallest domains (left-most on\nFigure 2), when one model achieves a lower performance than its overall average, all models are also\nperforming worse than their overall averages. We also note that the per-domain variations in performance\ncannot be explained by the size of the domains (since the performance looks erratic across all domain\n3757\nsizes). Overall, the results are a clear indication that most domains are either: (a) relatively hard for\nevery model, or (b) relatively easy for every model. This suggests that no model manages to acquire a\nmassively better language understanding that would make it able to avoid the difﬁculties faced by the\nother models, at least in the context of FG-NER.\nFurthermore, the ranking of the ﬁve models is very stable across domains: given the fact that one spe-\nciﬁc model performs the best (resp. the worst) for one domain, it can reliably be predicted that this model\nwill also perform the best (resp. the worst) across all domains. It follows that some models do bring a\nsometime incremental, but nonetheless measurable improvement over other models. Nevertheless, we\nnote that for the four smallest domains, the difference in performance from one model to another is more\nimportant, and no ranking pattern is visible.\nThe performance variations between domains that we see in our results have also been reported in the\nstudy by Guo et al. (2006), who investigated the stability of coarse-grained NER across domains for the\nChinese language. Notably, when trained on the sports domain, their baseline has a signiﬁcantly higher\nF1-score than the other domains. The same is true here, but it has to be noted that they use the classic\nNER-labels, i.e., person, location, organisation, and miscellaneous, rather than domain-speciﬁc labels.\nTake-Home Messages: To summarise, the transformer-based models do indeed outperform the\nBiLSTM-CNN-CRF model with regards to F1 score, with BERT yielding the highest results over-\nall. The simple CRF model achieved the best performance in terms of precision, while performing\nthe worst in terms of recall. Compared to both CRF and BiLSTM-CNN-CRF, the transformer-based\nmodels achieved signiﬁcantly higher recall scores. Furthermore, we observe signiﬁcant discrepancies\nwhen applying the models to different domains. Moreover, when a model is performing better (resp.\nworse) on one domain, the other models also perform better (resp. worse). This suggests that while\ntransformer-based models can indeed bring signiﬁcant performance improvements, their language\nunderstanding may not be outstandingly different. Indeed, if they were clearly different, we could\nhave reasonably expected to note different patterns in the performance for the FG-NER task (i.e., they\nwould not systematically perform well/badly for the same domains).\n4 Related Work\n4.1 Fine-Grained Named Entity Recognition\nEarly efforts to develop a ﬁne-grained approach to NER were made by B ´echet et al. (2000), where they\nfocused on differentiating between ﬁrst names, last names, countries, towns, and organisations. While\nthis would be considered coarse-grained by today’s standards, they do split the classical NER labels\nperson and location into more nuanced labels. FG-NER was ﬁrst described as ”ﬁne grained classiﬁcation\nof named entities” by Fleischman and Hovy (2002). They focused on a ﬁne-grained label set for personal\nnames, dividing the generic person label into eight subcategories, i.e., athlete, politician/government,\nclergy, businessperson, entertainer/artist, lawyer, doctor/scientist, and police. They experimented with\na variety of classic machine learning approaches for this task, and achieved promising results of 68.1%,\n69.5%, and 70.4% in terms of accuracy for SVM, a feed-forward neural network, and a C4.5 decision\ntree, respectively. Furthermore, Ling and Weld (2012) introduced their ﬁne-grained entity recognizer\n(FIGER), which can distinguish between 112 different labels and handle multi-label classiﬁcation.\nMai et al. (2018) presented an empirical study on FG-NER prior to the rise of transformer-based mod-\nels (which are the focus of our study). They targeted an English dataset containing 19 800 sentences and\na Japanese dataset which contained 19 594 sentences, dividing the named entities into 200 categories.\nThey compared performances for FIGER, BiLSTM-CNN-CRF, and a hierarchical CRF+SVM classiﬁer,\nwhich classiﬁes an entity into a coarse-grained category before further classifying it into a ﬁne-grained\nsubcategory. Furthermore, they combine some of the aforementioned methods with gazetteers and cat-\negory embeddings to further improve the performance of the models. They found that the BiLSTM-\nCNN-CRF model by Ma and Hovy (2016) combined with gazetteer information performed the best for\nthe English language with an F1 score of 83.14% while BiLSTM-CNN-CRF with both gazetteers and\n3758\ncategory embeddings yielded an F1 score of 82.29%, and 80.93% without either gazetteers or category\nembeddings.\n4.2 The Rise of Transformers\nVaswani et al. (2017) ﬁrst described the transformer model which superseded the popular LSTM model\nin favour of the attention mechanism (Bahdanau et al., 2014). As transformers do not need to process\nsentences in sequence, they allow for more parallelisation than LSTMs or other recurrent neural network\nmodels. Due to this advantage, transformers have become fundamental for state-of-the-art models in\nthe NLP ﬁeld. One early notable model that employed transformers is the Generative Pretraining Trans-\nformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of\ntwelve NLU tasks. Devlin et al. (2019) further revolutionised the NLP landscape by introducing BERT.\nUnlike the unidirectional GPT model, BERT is a deeply bidirectional transformer model, pretrained on\nthe MLM and NSP tasks. Fine-tuned BERT models managed to outperform state-of-the-art models in\neleven NLP tasks, including the GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) bench-\nmarks. The success of BERT led to a large variety of similar models, which were pretrained on different\ndatasets. Most notably, RoBERTa (Liu et al., 2019) and XLNet managed to further outperform BERT in\na large number of tasks. Speciﬁcally, Yang et al. (2019) introduced XLNet, replacing the MLM task with\na permutation-based autoregression task, effectively predicting sentence tokens in random order. XLNet\nmanages to outperform BERT in 20 tasks, including the GLUE, SQuAD and RACE (Lai et al., 2017)\nbenchmarks. Meanwhile, the RoBERTa model was trained on more data, for longer periods of time,\ntweaked the MLM pretraining task, and removed the NSP task. Liu et al. (2019) reported that RoBERTa\noutperforms BERT on the GLUE, SQuAD, and RACE benchmarks.\n5 Threats to Validity\nThis study was conducted on the EWNERTC dataset (Sahin et al., 2017a) which was annotated automat-\nically. We are operating under the assumption that the annotations are accurate. However, while Sahin\net al. (2017b) conducted an evaluation for the Turkish counterpart of the dataset (TWNERTC), they did\nnot evaluate the English one. Nevertheless, EWNERTC is the largest publicly available dataset that we\ncould ﬁnd and that is relevant for FG-NER studies. We further proposed to reduce the potential noise in\nlabelling by considering only the subset associated to top labels (cf. Section 2.1).\nPerformance measurements can be impacted by sub-optimal implementation of algorithms. To miti-\ngate this threat, we collected the models’ implementations that were released by their original authors,\nand already leveraged in previous studies, and we reused them in the settings they were designed for.\nWhile we conducted grid searches to determine optimised hyperparameters for the CRF, BERT,\nRoBERTa and XLNet models, we did not speciﬁcally optimise the hyperparameters for the the BiLSTM-\nCNN-CRF model due to the induced computational costs. Furthermore, as pointed out in section 2, due\nto the large number of domains, we decided against using gazetteers even though they would likely have\nincreased the F1-scores of the non-transformer-based models.\n6 Conclusion\nIn this paper, we presented an empirical study of the performance of various transformer-based models\nfor the FG-NER task on a multitude of domains and compared them to both CRF and BiLSTM-CNN-\nCRF models (which are commonly used in the literature for the NER task).\nWe concluded that while the transformer-based models did not manage to outperform non-transformer-\nbased models in terms of precision, we observed a consistent increase in recall and F1 scores in most\ndomains. We noticed, however, signiﬁcant differences in performance for a selection of domains that\ncould not be explained by the size of the respective datasets. This study yields the main insight that\nwhile transformer-based models can indeed bring signiﬁcant performance improvements, they do not\nnecessarily revolutionise the achievements in FG-NER to the same extent they did in other NLP tasks.\n3759\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\nProceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473.\nFr´ed´eric B´echet, Alexis Nasr, and Franck Genet. 2000. Tagging unknown proper names using decision trees. In\nProceedings of the 38th Annual Meeting on Association for Computational Linguistics , pages 77–84. Associa-\ntion for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 , pages\n4171–4186.\nMichael Fleischman and Eduard Hovy. 2002. Fine grained classiﬁcation of named entities. In Proceedings of\nthe 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computa-\ntional Linguistics.\nHong Lei Guo, Li Zhang, and Zhong Su. 2006. Empirical study on the performance stability of named entity\nrecognition model across domains. In Proceedings of the 2006 Conference on Empirical Methods in Natural\nLanguage Processing, pages 509–516.\nJohn D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference\non Machine Learning, ICML ’01, page 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading compre-\nhension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 785–794.\nXiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the Twenty-Sixth AAAI\nConference on Artiﬁcial Intelligence, AAAI’12, page 94–100. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 1064–1074,\nBerlin, Germany, August. Association for Computational Linguistics.\nKhai Mai, Thai-Hoang Pham, Minh Trung Nguyen, Tuan Duc Nguyen, Danushka Bollegala, Ryohei Sasano, and\nSatoshi Sekine. 2018. An empirical study on ﬁne-grained named entity recognition. In Proceedings of the 27th\nInternational Conference on Computational Linguistics, pages 711–722, Santa Fe, New Mexico, USA, August.\nAssociation for Computational Linguistics.\nMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence\ntagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1756–1765.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nby generative pre-training. http://openai-assets.s3.amazonaws.com/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2383–2392.\nNils Reimers and Iryna Gurevych. 2017a. Optimal hyperparameters for deep lstm-networks for sequence labeling\ntasks. arXiv preprint arXiv:1707.06799.\nNils Reimers and Iryna Gurevych. 2017b. Reporting score distributions makes a difference: Performance study of\nlstm-networks for sequence tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 338–348, Copenhagen, Denmark, 09.\n3760\nH. Bahadir Sahin, Mustafa Tolga Eren, Caglar Tirkaz, Ozan Sonmez, and Eray Yildiz. 2017a. English/turkish\nwikipedia named-entity recognition and text categorization dataset.\nH Bahadir Sahin, Caglar Tirkaz, Eray Yildiz, Mustafa Tolga Eren, and Ozan Sonmez. 2017b. Automatically\nannotated turkish corpus for named entity recognition and text categorization using large-scale gazetteers.arXiv\npreprint arXiv:1702.02363.\nErik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learn-\ning at HLT-NAACL 2003, pages 142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY , USA. Curran Associates\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art\nnatural language processing. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xl-\nnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SW AG: A large-scale adversarial dataset for\ngrounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 93–104, Brussels, Belgium, October-November. Association for Computational\nLinguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\n2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27.",
  "topic": "Named-entity recognition",
  "concepts": [
    {
      "name": "Named-entity recognition",
      "score": 0.8837159276008606
    },
    {
      "name": "Transformer",
      "score": 0.8601914644241333
    },
    {
      "name": "Computer science",
      "score": 0.7383635640144348
    },
    {
      "name": "Language model",
      "score": 0.5814256072044373
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5780884027481079
    },
    {
      "name": "Natural language processing",
      "score": 0.5396881103515625
    },
    {
      "name": "Speech recognition",
      "score": 0.3237990736961365
    },
    {
      "name": "Task (project management)",
      "score": 0.3069099187850952
    },
    {
      "name": "Engineering",
      "score": 0.10949316620826721
    },
    {
      "name": "Voltage",
      "score": 0.06250190734863281
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I186903577",
      "name": "University of Luxembourg",
      "country": "LU"
    }
  ]
}