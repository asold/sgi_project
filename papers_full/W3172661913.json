{
  "title": "Fully Transformer Networks for Semantic Image Segmentation",
  "url": "https://openalex.org/W3172661913",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222589338",
      "name": "Wu Sitong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743457425",
      "name": "Wu, Tianyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222589336",
      "name": "Lin, Fangjian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2368967476",
      "name": "Tian, Shengwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489541830",
      "name": "Guo, Guodong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3034521057",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3107497254",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3126080715",
    "https://openalex.org/W3109772732",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W2158967129",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3108139472",
    "https://openalex.org/W3094918035",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2561196672",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2963374347",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2800507189",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W3034755976",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2183341477"
  ],
  "abstract": "Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated that combining such Transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure Transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, meanwhile reducing the computation complexity of the standard Visual Transformer (ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve better results on multiple challenging semantic segmentation and face parsing benchmarks, including PASCAL Context, ADE20K, COCOStuff, and CelebAMask-HQ. The source code will be released on https://github.com/BR-IDL/PaddleViT.",
  "full_text": "1\nFully Transformer Networks for Semantic Image\nSegmentation\nSitong Wu1, 2 ∗, Tianyi Wu1, 2 ∗, Fangjian Lin 1, 2, 3 ∗, Shengwei Tian 3, Guodong Guo 1,2 †\nAbstract—Transformers have shown impressive performance\nin various natural language processing and computer vision\ntasks, due to the capability of modeling long-range dependencies.\nRecent progress has demonstrated that combining such Trans-\nformers with CNN-based semantic image segmentation models\nis very promising. However, it is not well studied yet on how\nwell a pure Transformer based approach can achieve for image\nsegmentation. In this work, we explore a novel framework for\nsemantic image segmentation, which is encoder-decoder based\nFully Transformer Networks (FTN). Speciﬁcally, we ﬁrst pro-\npose a Pyramid Group Transformer (PGT) as the encoder for\nprogressively learning hierarchical features, meanwhile reducing\nthe computation complexity of the standard Visual Transformer\n(ViT). Then, we propose a Feature Pyramid Transformer (FPT)\nto fuse semantic-level and spatial-level information from multiple\nlevels of the PGT encoder for semantic image segmentation.\nSurprisingly, this simple baseline can achieve better results on\nmultiple challenging semantic segmentation and face parsing\nbenchmarks, including PASCAL Context, ADE20K, COCO-\nStuff, and CelebAMask-HQ. The source code will be released\non https://github.com/BR-IDL/PaddleViT.\nIndex Terms—Semantic Segmentation, Pyramid Group Trans-\nformer, Feature Pyramid Transformer.\nI. I NTRODUCTION\nS\nEMANTIC image segmentation aims to assign a semantic\nlabel to each pixel in an image, which is an indispens-\nable component for many applications, such as autonomous\nvehicles [13], human-computer interaction [22], and medical\ndiagnosis [20]. Since the era of deep learning, convolutional\nneural networks (CNNs) have been the cornerstone of tremen-\ndous image segmentation models.\nInspired by successful applications of Transformer [53] on\nnatural language processing tasks (machine translation [42],\ntext classiﬁcation [25]), and computer vision tasks (image\nclassiﬁcation [16], [33], [37], [52], [55], [58], [74]), Trans-\nformer has attracted more and more interests in the context\nof semantic image segmentation [37], [46], [55], [64], [77]\nrecently. These methods combined the Transformer block into\nthe encoder-decoder architecture, where the encoder reduces\nthe feature maps and captures rich semantic representations,\nwhile the decoder gradually recovers the spatial information\nManuscript created November, 2021. ∗denotes equal contribution. (Corre-\nsponding author: Guodong Guo).\nSitong Wu, Tianyi Wu, Fangjian Lin, and Guodong Guo are with In-\nstitute of Deep Learning, Baidu Research, Beijing 100085, China, and\nalso with National Engineering Laboratory for Deep Learning Technology\nand Application, Beijing 100085, China (email: wusitong@baidu.com; wu-\ntianyi01@baidu.com; linfangjian@baidu.com; guoguodong01@baidu.com).\nFangjian Lin and Shengwei Tian are with School of Software, XinJiang\nUniversity, urumqi, China. (email: tianshengwei@163.com).\nor fuses multi-scale features for generating accurate pixel-\nlevel predictions. These approaches can be divided into three\ncategories: (1) Transformer-CNN architecture [37], [46],\n[55], [77]. It contains a Transformer encoder and CNN de-\ncoder. Images are ﬁrst divided into a set of patches and\nprojected to patch embeddings, which are then passed through\na Transformer encoder and a traditional CNN decoder to\nrecover the pixel-level prediction, as shown in Figure 1(a). (2)\nHybrid-CNN architecture [77]. As shown in Figure 1(b),\nits decoder is the traditional CNN, while the encoder is a\ncombination of CNN (e.g., ResNet [24]) and Transformer\nlayers, in which the output of CNN with a lower resolution\nis directly used as the patch embeddings for Transformer\nlayers. (3) Hybrid-Transformer architecture [64]. It further\nreplaced the CNN decoder of Hybrid-CNN architecture with\na Transformer decoder, as shown in Figure 1(c).\nDifferent from the architectures mentioned above, we pro-\npose Fully Transformer Networks (FTN) for semantic image\nsegmentation, without relying on CNN. As shown in Figure\n1(d), both the encoder and decoder in the proposed FTN are\ncomposed of multiple Transformer layers. Firstly, we propose\na new Vision Transformer encoder called Pyramid Group\nTransformer (PGT). Inspired by the hierarchical nature of\nthe features in deep networks [70]), we propose to gradu-\nally increase the receptive ﬁelds rather than keep it global\nthroughout the whole network (such as ViT [16]). Speciﬁcally,\nPGT spatially divides the feature maps into multiple groups,\nand performs multi-head self-attention within each group. The\nreceptive ﬁelds can be easily controlled by the group number,\nthus we progressively increase the number of groups, forming\na pyramid pattern, to achieve gradually increasing receptive\nﬁelds. Beneﬁted by such design, our PGT is good at learning\nhierarchical features, including low-level (edge/color conjunc-\ntions), mid-level (complex invariances, similar textures), and\nhigh-level (entire objects). Furthermore, PGT can also reduce\nthe unaffordable computational and memory costs of the stan-\ndard Visual Transformer (ViT), which is essential for pixel-\nlevel prediction tasks. Secondly, we propose a Transformer-\nbased decoder, named Feature Pyramid Transformer (FPT) to\nfuse semantic-level and spatial-level information from multiple\nlevels of the PGT encoder. Beneﬁted by the long-range mod-\neling capacity of Transformer architecture, FPT can capture\nricher context information. Based on the proposed encoder and\ndecoder, we develop Fully Transformer Networks (FTN) for\nsemantic image segmentation, and evaluated the effectiveness\nof our approach by conducting extensive ablation studies. Fur-\nthermore, we evaluate our FTN on three challenging semantic\nsegmentation benchmarks, including PASCAL-Context [43],\narXiv:2106.04108v3  [cs.CV]  28 Dec 2021\n2\nFig. 1. Pipelines of the previous Transformer-based segmentation models. The blue bar represents Transformer blocks and the green bar indicates CNN layers.\nThe encoder and decoder are separated by a red snip.\nADE20K [78], and COCO-Stuff [5], achieving the state-of-\nthe-art performance (mIoU) of 56.05%, 51.36%, and 45.89%,\nrespectively.\nOur main contributions include:\n• We propose the Pyramid Group Transformer (PGT) to\nlearn hierarchical representations by gradually increasing\nthe receptive ﬁelds via a pyramid pattern. Meanwhile,\nit can reduce the computation and memory costs of the\nstandard ViT signiﬁcantly.\n• We develop a Feature Pyramid Transformer (FPT) de-\ncoder to fuse semantic-level and spatial-level information\nfrom multiple levels of the PGT encoder. It is able to cap-\nture richer contextual information beneﬁted by the long-\nrange dependencies modeling capability of Transformer\narchitecture.\n• Based on the proposed PGT and FPT, we create Fully\nTransformer Networks (FTN) for semantic image seg-\nmentation, which achieves superior performance than\nprevious approaches on multiple challenging benchmarks,\nincluding PASCAL Context, ADE20K, COCO-Stuff, and\nCelebAMask-HQ.\nII. RELATED WORKS\nSemantic Image Segmentation. The early FCN [38] used\na fully convolutional network to generate dense predictions\nfor images with an arbitrary size, which is regarded as a\nmilestone in semantic segmentation. Since then, lots of works\nhave been designing segmentation networks based on CNN\narchitecture [2], [7], [15], [19], [23], [27], [30], [54], [60],\n[61], [75], [76]. For more precise segmentation, some works\n[2], [44] were devoted to involving ﬁne-grained details via\nvarious hierarchies of decoders. Considering that the effective\nreceptive ﬁelds of CNN architecture are proved to be much\nsmaller than its theoretical value [41], some works focused\non how to capture more contextual information for a better\nscene understanding [7]–[9], [61], [76]. For example, DeepLab\nfamily [7]–[9] replaced partial pooling layers with atrous\nconvolution to expand the receptive ﬁeld without reducing the\nresolution. PSPNet [76] proposed a general pyramid pooling\nmodule to fuse features under four different pyramid scales.\nCGNet [61] learns the joint features of both local features\nand the surrounding context in all stages of the network.\nRecently, in order to aggregate more precise contextual in-\nformation, [68] and [73] devote to capture class-dependent\ncontext information. [29], [36], and [66] proposed to augment\neach pixel representation by aggregating intra-class and inter-\nclass context respectively. Besides, there are also some works\ncommitted to improving the segmentation on the boundary\nregion [4], [11], [69].\nSelf-attention in Semantic Image Segmentation. Inspired by\nthe success of Transformer [53] for NLP tasks, DANet [18]\nemployed the self-attention [56] on the CNN-based backbone,\nwhich adaptively integrated local features with their global\ndependencies. The traditional self-attention serves as a partic-\nularly useful technique for semantic segmentation while being\ncriticized for its prohibitive computation burdens and memory\nusages. To address this issue, [28] and [79] introduced a criss-\ncross attention mechanism and pyramid sampling module into\nthe self-attention, respectively, aiming to reduce the computa-\ntion cost and memory usage while keeping the performance.\nAANet [3] proposed an attention-augmented convolution ar-\nchitecture by concatenating convolutional feature maps with a\nset of feature maps produced via self-attention. In contrast\nto these methods which augmented a CNN-based encoder\nwith a self-attention module for semantic segmentation, we\npropose the novel Fully Transformer Networks, where both\nencoder and decoder are based on pure Transformer with our\nspecialized design.\nTransformer in Vision Tasks.DETR [6] combined a common\nCNN with Transformer for predicting objects, which formu-\nlates object detection as a dictionary lookup problem with\nlearnable queries. Later, many researchers explored to adopt a\npure Transformer architecture for computer vision tasks [16],\n[17], [33], [37], [52], [55], [58], [74]. Vision Transformer\n(ViT) [16] is the ﬁrst work to introduce a pure Transformer\ninto image classiﬁcation by treating an image as a sequence of\n3\npatches. Recently, Transformer has attracted more and more\nattention in semantic segmentation [46], [64], [77]. SETR [77]\nattempted to introduce Transformer for semantic segmentation\nby using ViT as the encoder and employing CNN-based\ndecoders for generating segmentation results. Trans2Seg [64]\nstacked a convolution-free encoder-decoder network on top of\na common CNN encoder for transparent object segmentation.\nDPT [46] assembled tokens from multiple stages of ViT and\nprogressively combined them into full-resolution predictions\nusing a convolutional decoder. Different from these approaches\nthat used Transformer as an encoder or decoder, we proposed\nthe encoder-decoder based Fully Transformer Networks for\nsemantic segmentation.\nIII. METHOD\nIn this section, we ﬁrst describe our framework, Fully\nTransformer Networks (FTN). Then, we present the encoder\narchitecture of FTN, i.e., Pyramid Group Transformer (PGT),\nwhich aims to extract hierarchical representations. Finally, we\npresent the decoder architecture of FTN, i.e., Feature Pyramid\nTransformer (FPT), which fuses the multi-scale features from\nthe encoder and generates the ﬁne pixel-level predictions.\nA. Framework of FTN\nThe overall framework of our FTN is shown in Figure 2,\nwhich consists of Pyramid Group Transformer (PGT) encoder\nand Feature Pyramid Transformer (FPT) decoder. PGT aims\nto extract hierarchical representations. It is conﬁgured with\nfour stages for learning hierarchical features, similar to some\nclassic backbones [24], [65]. Each stage of PGT has a similar\nstructure, which contains a patch transform layer and multiple\nPGT blocks. The patch transform layer is employed to reduce\nthe number of tokens. In particular, given an input image\nx ∈RH×W×3, it is ﬁrst transformed into HW\n42 patches with\ndimension C by the patch transform layer in stage 1, then\nthe output is fed into N1 PGT blocks, where N1 indicates\nthe number of PGT blocks in stage 1. The output of the last\nblock in stage 1 is F1 ∈ R\nHW\n42 ×C. For the last 3 stages,\nthe patch transform layer merges each 2 × 2 non-overlapping\nneighboring patches to reduce the resolution by 1/2 and\nexpand the dimension twice. The output feature in stage i\nis Fi ∈ R\nHW\n22i+2 ×2i−1C. After getting multi-scale features,\nwe employ FPT decoder to fuse semantic-level and spatial-\nlevel information from multiple levels. Finally, the output of\nFPT is fed into a linear layer followed by a simple bilinear\nupsampling to generate the probability map for each pixel.\nThe pixel-level segmentation result is obtained by argmax\noperation on the probability map.\nB. Pyramid Group Transformer\nAs shown in Figure 2, PGT has four hierarchical stages\nthat generate features with multiple scales. At the beginning\nof each stage, the features are ﬁrst shrunk spatially and\nenlarged in channel dimension by the patch transform layer,\nand then be fed into the subsequent PGT blocks to learn\ndiscriminative representations. PGT progressively increases\nthe receptive ﬁeld of self-attention when the stage increases, so\nas to learn low-level spatial details at shallow layers and high-\nlevel semantic features at deep layers. Such a mechanism is\nsuperior to the previous Vision Transformer (ViT) [16] where\nthe self-attention with a global receptive ﬁeld is performed on\nthe whole input features at all layers. Besides, our approach\ncan reduce the computational and memory cost of the whole\ntransformer block, which is desired especially for dense pre-\ndiction tasks.\nSpeciﬁcally, each feature map is ﬁrstly divided into non-\noverlapping grids and each grid is regarded as a group. Then,\nself-attention is performed among patches within each group.\nThus, patches in one group are unacceptable for patches in\nother groups, which is equivalent to local receptive ﬁelds.\nExplicitly, the size of each receptive ﬁeld can be conveniently\ncontrolled by the group numbers. As shown in Figure 3,\ndifferent from the global receptive ﬁelds in all stages of ViT\n[16] and PVT [55], the receptive ﬁelds at different stages of\nour PGT present a pyramid pattern, and we set a consistent\nreceptive ﬁeld inside each stage. It is notable that the global\nreceptive ﬁeld is better to be applied in both stage 3 and stage\n4 for the reason that features with stride 16 and 32, usually\ncontain rich semantics. For the l-th PGT block, its computation\ncan be formulated as follows:\nˆZl = Zl−1 + PG-MSA(LN(Zl−1)),\nZl = ˆZl + MLP(LN( ˆZl)),\n(1)\nwhere Zl−1 is the output of the (l−1)-th PGT block. LN\nand MLP refer to layer normalization [1] and multi-layer\nperceptron, respectively. Furthermore, the core of PGT blocks\nis the Pyramid Group Multi-Self Attention (PG-MSA), which\ncan be formulated as follow:\nPG-MSA(Z) = Concat(h0,h1,hH−1),\nhi = Reshape(head0\ni ,head1\ni ,...,head G−1\ni ),\nheadj\ni = Attention(Qi[j],Ki[j],Vi[j]),\n(2)\nwhere i∈{0,1,...,H −1}is the head index; j ∈{0,1,...,G −\n1} is the group index; Attention(·) is the self-attention\noperation [53]. Qi = ZWq\ni , Ki = ZWk\ni , and Vi = ZWv\ni\nindicate the query, key, value embedding of the i-th head,\nrespectively.\nPGT Variants. We designed 4 variants with different sizes,\nnamed PGT-T, PGT-S, PGT-B, and PGT-L respectively. For a\nfair comparison, the sizes of these variants are similar to the\nprevious works [37], [52], [55]. The detailed conﬁguration of\nour PGT variants are shown in Table I, where the deﬁnitions\nof hyper-parameters are listed below:\n• Pi\n2: the token number reduction factor of the patch\ntransform layer in stage i;\n• Ci: the dimension of tokens in stage i;\n• Ni: the number of PGT blocks in stage i;\n• Gi: the group number of PG-MSA in stage i;\n• Hi: the head number of PG-MSA in stage i;\n• Ei: the dimension expansion ratio of MLP in stage i;\nAccording to the differences among these variants, the\nrepresentation capacity is only enlarged by increasing the\ndepth of PGT blocks and the dimension of patch tokens\n4\nFig. 2. The overall architecture of our Fully Transformer Networks (FTN). Both the encoder and decoder of FTN are based on Transformer architectures.\n(a) ViT\n (b) PVT\n (c) PGT (ours)\nFig. 3. Comparisons of different self-attention mechanisms in Transformers.\n(a) ViT performed the global self-attention within the single-scale feature\nmaps throughout the whole network. (b) PVT introduced the pyramid structure\nto generate multi-scale feature maps and applied the global self-attention to\nfeatures as well. (c) In PGT, the feature map is ﬁrst divided into groups in\nthe spatial domain and then the global self-attention is only performed within\neach group, i.e., a red grid in the ﬁgure. The receptive ﬁelds (group size) of\nattention are progressively increased in stages as a pyramid pattern.\n(i.e. deeper and wider). For example, PGT-S is obtained by\nincreasing the dimensions of PGT-T, and PGT-B is a deeper\nversion of PGT-S. PGT-L is the deepest and widest among\nthese all. For all variants, the expansion ratio of MLP is set\nto 4, and the dimension of each head is 32. The pyramid\nstructure of the receptive ﬁelds in attention is also consistent\nfor different variants, with group numbers 64, 16, 1, and 1,\nrespectively, in four stages.\nCompared with Swin Transformer. For Swin Transformer\n[37], the window size is set as a ﬁxed number for input image\nwith arbitrary size. While, in our PGT, the group number is\nconstant for each stage and gradually decreases from shallow\nstage to deep stage. The group size depends on the size of\nthe input image. Therefore, as the image size increases, the\nreceptive ﬁeld of PGT enlarges faster than that of Swin.\nCompared with PVT. PVT [55] proposed Spatial Reduction\nAttention (SRA) module to reduce the computational and\nmemory cost of global attention, which makes it affordable\nfor high-resolution features. Nevertheless, the receptive ﬁelds\nin all stages of PVT are still global. Different from that, we\npropose to gradually increase the receptive ﬁelds rather than\nglobal throughout the network. Thus, our PGT does better\nin learning low-level features with detailed information, and\nhigh-level features with rich semantics.\nC. Feature Pyramid Transformer\nTo generate ﬁner semantic image segmentation, we propose\na Feature Pyramid Transformer (FPT) to aggregate the infor-\nmation from multiple levels of the PGT encoder, as shown in\nFigure 2. Inspired by the spirit of convolution-based FPN [34],\nFPT is expected to fuse the semantic-level representations and\nspatial-level information into a high-resolution and high-level\nsemantic output.\nSpeciﬁcally, given the output hierarchical features {Fi},i ∈\n{1,2,...,L }of the encoder, we ﬁrst use a light-weight fusion\nmodule, containing lateral connections and a top-down path-\nway, to introduce strong semantics into all levels,\nˆFi =\n{\nφ(Fi) + Upsample\n(\nˆFi+1\n)\n, 1 ≤i<L\nφ(Fi), i = L\n(3)\nwhere φis used to align the channel dimension, implemented\nby a linear projection layer. Upsample denotes bilinear inter-\npolation. Note that ˆFi has the same size of Fi. Then, each level\nfeature is passed through the spatial reduction Transformer\nblocks [55] and progressively upsampled until it reaches 1/4\nscale,\n˜Fi =\n\n\n\nΨ( ˆFi), i = 1[\nUpsample\n(\nΨ( ˆFi)\n)]\n×(i−1)\n, 1 <i ≤L (4)\n5\nTABLE I\nDETAILED CONFIGURATION OF OUR PGT VARIANTS .\nStage Output\nStride Layer PGT-T PGT-S PGT-B PGT-L\n1 4\nPatch\nTransform\nP1 = 4\nC1 = 64\nP1 = 4\nC1 = 96\nP1 = 4\nC1 = 96\nP1 = 4\nC1 = 128\nPGT\nBlock\n\n\nG1 = 64\nH1 = 2\nE1 = 4\n\n ×2\n\n\nG1 = 64\nH1 = 3\nE1 = 4\n\n ×2\n\n\nG1 = 64\nH1 = 3\nE1 = 4\n\n ×2\n\n\nG1 = 64\nH1 = 4\nE1 = 4\n\n ×2\n2 8\nPatch\nTransform\nP2 = 2\nC2 = 128\nP2 = 2\nC2 = 192\nP2 = 2\nC2 = 192\nP2 = 2\nC2 = 256\nPGT\nBlock\n\n\nG2 = 16\nH2 = 4\nE2 = 4\n\n ×2\n\n\nG2 = 16\nH2 = 6\nE2 = 4\n\n ×2\n\n\nG2 = 16\nH2 = 6\nE2 = 4\n\n ×2\n\n\nG2 = 16\nH2 = 8\nE2 = 4\n\n ×2\n3 16\nPatch\nTransform\nP3 = 2\nC3 = 256\nP3 = 2\nC3 = 384\nP3 = 2\nC3 = 384\nP3 = 2\nC3 = 512\nPGT\nBlock\n\n\nG3 = 1\nH3 = 8\nE3 = 4\n\n ×6\n\n\nG3 = 1\nH3 = 12\nE3 = 4\n\n ×6\n\n\nG3 = 1\nH3 = 12\nE3 = 4\n\n ×18\n\n\nG3 = 1\nH3 = 16\nE3 = 4\n\n ×18\n4 32\nPatch\nTransform\nP4 = 2\nC4 = 512\nP4 = 2\nC4 = 768\nP4 = 2\nC4 = 768\nP4 = 2\nC4 = 1024\nPGT\nBlock\n\n\nG4 = 1\nH4 = 16\nE4 = 4\n\n ×2\n\n\nG4 = 1\nH4 = 24\nE4 = 4\n\n ×2\n\n\nG4 = 1\nH4 = 24\nE4 = 4\n\n ×2\n\n\nG4 = 1\nH4 = 32\nE4 = 4\n\n ×2\nwhere Ψ represents Transformer blocks and Upsample is im-\nplemented by bilinear interpolation. [ f ]×N denotes executing\nfunction f by N times. Finally, these multi-level and high-\nresolution representations are fused by simple element-wise\nsummation, and further passed through a per-pixel classiﬁca-\ntion head to predict pixel-level prediction and then upsampled\nto the original size of input image via bilinear interpolation.\nY = Upsample\n(\nψ\n( L∑\ni=1\n˜Fi\n))\n, (5)\nwhere ψ denotes per-pixel classiﬁcation head, which is im-\nplemented by a linear projection layer. Beneﬁted by the top-\ndown connections and multi-level aggregation, our model is\nable to enhance the capability of fusing multi-level semantic\nfeatures with different resolutions, which is essential for dense\nprediction tasks.\nIV. E XPERIMENTS\nIn this section, we ﬁrst compare our PGT with the state-\nof-the-art methods on the image classiﬁcation task. To fur-\nther demonstrate the effectiveness of our FTN, we conduct\nexperiments on three widely-used segmentation benchmarks,\nPASCAL Context [43], ADE20K [78], and COCO-Stuff [5].\nDetailed ablation studies are also provided to understand the\nimportance and impact of individual components and settings\nin our method.\nA. Image Classiﬁcation\nIn order to train our FTN on semantic image segmentation\nbenchmarks, we ﬁrst pretrained our PGT on the ImageNet\ndataset [47] for obtaining pretrained weights. ImageNet dataset\ncontains 1.3 million images with 1,000 classes. It is referred\nto as ImageNet-1K in this work.\nExperiment Settings. All the variants are trained for 300\nepochs on 8 V100 GPUs from scratch, with a total batch size\nof 512. We use AdamW [39] as the optimizer with an initial\nlearning rate of 0.0005, cosine decay scheduler, weight decay\n0.05, and 5-epoch linear warmup. For data augmentation, we\nfollow most of the widely used settings: random horizontal\nﬂipping [48], color jitter, Mixup [72] and AutoAugment [14]\nand randomly cropping to 224 × 224. We also use some\nregularization strategies in [52], such as Label-Smoothing [49]\nand stochastic depth [26]. During the evaluation, the input\nimage is resized and center-cropped to the size of 224 × 224.\nThe top-1 accuracy is reported for comparisons.\nResults on ImageNet-1K. Table II shows the comparison with\nthe state-of-the-art CNN and Transformer backbones. Compare\nto the dominant CNNs, our PGT consistently performs better\nthan the state-of-the-art RegNet [45] and conventional ResNet\n[24] with similar complexity by a large margin. For example,\nour PGT-S is +2% higher than RegNetY-4G and +3.5%\nhigher than ResNet-50. The advantage of PGT is especially\nobvious on tiny models, with PGT-T +4.8% higher than PVT-\nT. For small variants, our PGT-S surpasses the nearest CvT-\n13 and Swin-T by +0.4% and +0.7%, respectively. Compared\nwith larger models, our PGT-B and PGT-L exceed PVT\nsigniﬁcantly by 1 ∼2%, and outperform the state-of-the-art\nSwin Transformer with similar numbers of parameters and\ncomputation budgets, i.e., 83.6 vs. 83.3 and 83.4 vs. 83.0.\n6\nTABLE II\nCOMPARISONS OF DIFFERENT BACKBONES FOR THE IMAGE NET-1K\nCLASSIFICATION . OUR PGT CONSISTENTLY OUTPERFORMS OTHER\nMODELS WITH SIMILAR NUMBERS OF PARAMETERS AND COMPUTATIONAL\nBUDGETS .\nMethod Input size Params FLOPs Top-1 Acc. (%)\nResNet-18 [24] 224 12M 1.8G 68.5\nPVT-T [55] 224 13M 1.9G 75.1\nPGT-T (ours) 224 13M 2.1G 79.9\nResNet-50 [24] 224 26M 4.1G 78.5\nDeiT-S [52] 224 22M 4.6G 79.8\nPVT-S [55] 224 25M 3.8G 79.8\nRegNetY-4G [45] 224 21M 4.0G 80.0\nT2T-ViTt-14 [67] 224 22M 6.1G 80.7\nTNT-S [21] 224 24M 5.2G 81.3\nSwin-T [37] 224 29M 4.5G 81.3\nCvT-13 [58] 224 20M 4.5G 81.6\nPGT-S (ours) 224 28M 4.6G 82.0\nResNet-101 [24] 224 45M 7.9G 79.8\nPVT-M [55] 224 44M 6.7G 81.2\nT2T-ViTt-19 [67] 224 39M 9.8G 81.4\nRegNetY-8G [45] 224 39M 8.0G 81.7\nCvT-21 [58] 224 32M 7.1G 82.5\nSwin-S [37] 224 50M 8.7G 83.0\nPGT-B (ours) 224 50M 9.1G 83.4\nPVT-L [55] 224 61M 9.8G 81.7\nDeiT-B [52] 224 86M 17.5G 81.8\nT2T-ViTt-24 [67] 224 64M 15.0G 82.2\nTNT-B [21] 224 66M 14.1G 82.8\nRegNetY-16G [45] 224 84M 16.0G 82.9\nSwin-B [37] 224 88M 15.4G 83.3\nPGT-L (ours) 224 88M 15.9G 83.6\nViT-B/16 [16] 384 86M 55.4G 77.9\nViT-L/16 [16] 384 307M 190.7G 76.5\nB. Semantic Image Segmentation\nWe evaluate the segmentation performance of our FTN on\nPASCAL Context [43], ADE20K [78], and COCO-Stuff [5]\ndatasets.\nDatasets. PASCAL Context [43] is an extension of the PAS-\nCAL VOC 2010 detection challenge. Due to the sparsity of\nsome categories, a subset of 60 classes is more commonly\nused. For fair comparisons, we also use this 60 classes (59\nclasses and background) subset, which contains 4998 and 5105\nimages for training and validation, respectively. ADE20K [78]\nis a more challenging scene parsing dataset, which is split\ninto 20210, 2000, and 3352 images for training, validation\nand testing, respectively, with 150 ﬁne-grained object cate-\ngories. COCO-Stuff-10K [5] has 9000 training images and\n1000 testing images with 182 categories, which is referred as\nCOCO-Stuff in this paper.\nExperimental Details. We optimize our models using\nAdamW [39] with a batch size of 16. The total iterations are\nset to 80k, 160k and 100k for PASCAL Context, ADE20K,\nand COCO-Stuff, respectively. The initial learning rate is set\nto 6e-5 with a polynomial decay scheduler and weight decay\n0.01. During training, data augmentation in all the experiments\nconsists of three steps: (i) random horizontal ﬂipping, (ii)\nrandom scale with factors between 0.5 and 2, (iii) random\ncropping 480 for PASCAL Context and 512 for ADE20K and\nCOCO-Stuff. During inference, the multi-scale (MS) (factors\nvary from 0.5 to 1.75 with 0.25 as interval) mIoU is reported.\nFor a fair comparison, we simply apply the cross-entropy loss\nand synchronized BN. Similar to Swin [37], we add auxiliary\nloss to the output of stage 3 of PGT with the weight of 0.4.\nResults on PASCAL Context. Table III shows the results on\nPASCAL Context dataset. Our FTN models with PGT variants\nas the encoder are signiﬁcantly superior to the corresponding\nUperNet(Swin) models with similar numbers of parameters\nand computation cost. For example, our FTN(PGT-S) is +4.1%\nhigher than UperNet(Swin-T) (53.09 vs.48.99). Our FTN(PGT-\nL) achieves 56.05% mIoU, outperforming the UperNet(Swin-\nB) with 52.57% mIoU by a large margin. Surprisingly, our\nFTN(PGT-L) which is pretrained on ImageNet-1K even sur-\npasses the SETR-MLA(ViT-L/16) by +0.22%, whose encoder\npretrained on a larger ImageNet-21K dataset.\nFig. 4. Visualization comparison on PASCAL Context dataset.\nResults on ADE20K. As shown in Table III, we compare\nthe performance of FTN with the famous and state-of-the-art\nmodels on the more challenging ADE20K validation dataset.\nCompared with CNN-based models, our FTN has achieved\noverwhelming advantages with even a lower complexity, such\nas (48.68% vs. 46.35%). Compared with other Transformer\nsegmentation models, our FTN models still outperform Uper-\nNet(Swin) under the similar computational burden. Besides,\nour FTN(PGT-B) achieves 50.88% mIoU, surpassing the more\ncomputationally intensive SETR-MLA(ViT-L/16).\nResults on COCO-Stuff. The state-of-the-art results on\nCOCO-Stuff dataset are shown in Table III. RecoNet with\nResNet-101 as the backbone achieves a mIoU of 41.50%,\nwhich is the most advanced CNN model. Our FTN(PGT-\nT) is comparable to the most advanced CNN model (41.57\nvs. 41.50) with three times reduction in the numbers of\nparameters. Our FTN(PGT) models are signiﬁcantly superior\nto the promising Swin Transformers, with the advantages of\nabout 2 ∼3%.\nVisualization. We illustrate the visualization results on\nPASCAL-Context [43], ADE20K [78] and COCO-Stuff [5]\nin Figure 4, Figure 5 and Figure 6, respectively. We mainly\ncompare our FTN with UperNet [63], which performs best in\nprevious Transformer-based segmentation methods. According\nto these visualizations, we can see that our FTN(PGT-L)\nperforms better than UperNet(Swin-B) under similar compu-\ntational burden in distinguishing the confusing regions with\n7\nTABLE III\nCOMPARISONS WITH THE STATE -OF-THE -ART SEGMENTATION MODELS ON PASCAL C ONTEXT , ADE20K AND COCO-S TUFF DATASETS . ALL THE\nMIOU IS OBTAINED BY MULTI -SCALE INFERENCE . “‡” INDICATES PRETRAINING ON LARGER IMAGE NET-21 K DATASET UNDER THE INPUT SIZE OF\n384 ×384. “−” MEANS NO PUBLIC RESULTS AVAILABLE .\nMethod Backbone mIoU\nName Params PASCAL Context ADE20K COCO-Stuff\nFCN [38] ResNet-101 [24] 45M 45.63 41.40 -\nCGBNet [15] ResNet-101 [24] 45M 53.40 44.90 37.70\nPSPNet [76] ResNet-101 [24] 45M 47.78 45.35 -\nDeepLabV3+ [9] ResNet-101 [24] 45M 48.47 46.35 -\nCFNet [62] ResNet-101 [24] 45M 52.4 - 36.6\nDANet [18] ResNet-101 [24] 45M 52.60 45.02 39.70\nOCRNet [68] ResNet-101 [24] 45M 54.80 45.28 39.50\nEfﬁcient-FCN [35] ResNet-101 [24] 45M 53.30 45.28 -\nGINet [59] ResNet-101 [24] 45M 54.90 45.54 40.60\nRecoNet [10] ResNet-101 [24] 45M 54.80 45.54 41.50\nGPSNet [19] ResNet-101 [24] 45M - 45.76 -\nFPT [71] ResNet-101 [24] 45M - 45.90 -\nCPNet [66] ResNet-101 [24] 45M 53.9 46.27 -\nISNet [29] ResNet-101 [24] 45M - 47.55 42.08\nUperNet [63] Swin-T [37] 28M 48.99 45.81 39.13\nUperNet [63] Swin-S [37] 50M 51.66 49.47 41.58\nUperNet [63] Swin-B [37] 88M 52.57 49.72 42.20\nSETR-MLA [77] ViT-L/16 ‡ [16] 307M 55.83 50.28 -\nFTN (ours) PGT-T (ours) 13M 51.15 47.12 41.57\nFTN (ours) PGT-S (ours) 28M 53.09 48.68 43.63\nFTN (ours) PGT-B (ours) 50M 54.93 50.88 44.82\nFTN (ours) PGT-L (ours) 88M 56.05 51.36 45.89\nFig. 5. Visualization comparison on ADE20K dataset.\nsimilar colors but different categories. For example, in Line\n4 of Figure 4, UperNet(Swin-B) confused the cat with its\nsurrounding clothing, while our FTN(PGT-L) distinguish them\nclearly. However, UperNet(Swin-B) will confuse in some\nvisually easy areas. For example, in the second and third row\nof Figure 4, the bird and its background are visually distinct,\nwhich is visually a simple scene, while UperNet(Swin-B)\nmisjudges some part of the background surprisingly.\nFig. 6. Visualization comparison on COCO-Stuff dataset.\nC. Face Parsing\nIn order to evaluate the generalization of our FTN on other\ndense prediction tasks, we also apply our FTN(PGT-L) on the\nface parsing task, which aims to distinguish between different\nparts of the face, such as eyes, nose, and mouth.\nDataset. We compare our model with the state-of-the-art\nmethods on CelebAMask-HQ dataset [32], which is a large-\nscale face parsing dataset with 19 categories including some\nadditional eyeglass, earring, necklace, neck, and cloth beyond\nthe traditional facial components. It contains 24183, 2993, and\n2824 images for training, validation, and testing respectively.\n8\nTABLE IV\nCOMPARISONS WITH STATE -OF-THE -ARTS ON THE CELEB AMASK -HQ DATASET (IN F1 SCORE ).\nMethod Face Nose Glasses L-Eye R-Eye L-Brow R-Brow L-Ear R-Ear MeanI-Mouth U-Lip L-Lip Hair Hat Earring Necklace Neck Cloth\nPSPNet [76] 94.8 90.3 75.8 79.9 80.1 77.3 78.0 75.6 73.1 76.289.8 87.1 88.8 90.4 58.2 65.7 19.4 82.7 64.2\nMaskGAN [32] 95.5 85.6 92.9 84.3 85.2 81.4 81.2 84.9 83.1 80.363.4 88.9 90.1 86.6 91.3 63.2 26.1 92.8 68.3\nEHANet [40] 96.0 93.7 90.6 86.2 86.5 83.2 83.1 86.5 84.1 84.093.8 88.6 90.3 93.9 85.9 67.8 30.1 88.8 83.5\nWei et al. [57] 96.4 91.9 89.5 87.1 85.0 80.8 82.5 84.1 83.3 82.190.6 87.9 91.0 91.1 83.9 65.4 17.8 88.1 80.6\nEAGRNet [51] 96.2 94.0 92.3 88.6 88.7 85.7 85.2 88.0 85.7 85.195.0 88.9 91.2 94.9 87.6 68.3 27.6 89.4 85.3\nAGRNet [50] 96.5 93.9 91.8 88.7 89.1 85.5 85.6 88.1 88.7 85.592.0 89.1 91.1 95.2 87.2 69.6 32.8 89.9 84.9\nUperNet(Swin-B) [37] 96.5 94.0 92.1 89.7 89.9 86.2 85.9 88.3 87.9 86.292.1 89.4 91.2 95.5 90.1 70.5 35.4 90.3 87.1\nFTN-L (ours) 96.6 94.3 93.3 90.0 90.3 87.0 86.6 88.9 88.5 87.492.4 89.9 91.4 96.1 92.2 72.0 40.2 92.2 91.0\nFig. 7. Visualization comparison on CelebAMask-HQ dataset.\nSettings. We conduct experiments on 2 V100 GPUs with a to-\ntal batch size of 16. The models are trained for 160k iterations\nwith AdamW [39] as the optimizer (weight decay=0.01). The\ninitial learning rate is set to 6e-5 and decay via polynomial\nscheduler. Synchronized BN and cross-entropy loss are used\nduring training.\nResults. Table IV compares the F1 score of recent ap-\nproaches. Compared with previous best CNN-based method,\nour FTN(PGT-L) outperforms AGRNet [50] by a large mar-\ngin (87.4 vs. 85.5). Our FTN(PGT-L) also surpasses the\nUperNet(Swin-B) by +1.2%. Note that the backbone of\nboth UperNet(Swin-B) and FTN(PGT-L) are pretrained on\nImageNet-1k with 224 ×224 input size. Figure 7 presents the\nvisualization comparisons, which shows that the segmentation\nof our FTN(PGT-L) is more detailed.\nD. Ablation Study\nIn this section, we ﬁrst replace the encoder and decoder of\nour FTN with other models for comparisons to demonstrate the\neffectiveness of PGT and FPT, respectively. Then, we ablate\nthe key designs of PGT, i.e., group numbers, position encoding\nand class token. Finally, we provide more ablation experiments\nfor some design choices of FPT.\nEffectiveness of PGT. Our proposed FTN is a general and\nﬂexible fully Transformer framework for image segmentation,\nin which the encoder can be replaced by any other Vision\nTransformer backbones. In order to prove the effectiveness\nof PGT, we compare the recent promising Transformer back-\nbones, such as ViT [16], PVT [55] and Swin Transformer\n[37] with our PGT under various decoders on PASCAL\nContext. As shown in Table V, no matter which decoder is\nused, our PGT variants are about +3 ∼6% higher than PVT\nvariants and +2 ∼4% higher than Swin variants on average\nunder similar computation burdens, which demonstrates the\nsuperiority of our PGT. It is remarkable that our PGT-L\neven surpasses the more computationally intensive ViT-L\n(pretrained on ImageNet-21K) by +1 ∼2% improvement with\nconsiderably fewer amount of pretraining data.\nEffectiveness of FPT. Meanwhile, the decoder part of FTN is\nalso compatible with various CNN and Transformer-based de-\ncoders. To evaluate the effectiveness of our FPT, we compare\nFPT with the decoders used in previous Transformer segmen-\ntation models on PASCAL Context dataset. Experiments in\nTable V show that no matter which backbone is applied, our\nFPT is consistently superior to other popular decoders, about\n+1% higher than Semantic FPN and about +0.5% higher than\nUperNet used in [37]. Compared with another query-based\nTransformer decoder proposed in Trans2Seg [64], our FPT\nalso performs better by a large margin (55.24 vs. 54.13).\nGroup Choices of PGT. As mentioned in Section III-B, the\nreceptive ﬁelds of PGT can be adjusted by the number of\ngroups in four encoder stages. Excessive receptive ﬁelds will\nimpose a heavy computational burden, while if the receptive\nﬁeld is too small, it will be insufﬁcient for context modeling.\nFigure VI shows the comparison with different choices of PGT\ngroup numbers in four encoder stages. PGT <64-16-1-1> is\n+0.38% better than PGT <64-16-4-1> with fewer additional\n9\nTABLE V\nCOMBINATIONS OF DIFFERENT ENCODERS AND DECODERS ON PASCAL C ONTEXT VALIDATION DATASET . ‡REFERS TO PRETRAINING ON A LARGER\nIMAGE NET-21 K DATASET UNDER THE INPUT SIZE OF 384 ×384. FOR A FAIR COMPARISON , ALL THE SETTINGS ARE SAME AS MENTIONED IN SECTION\nIV-B, EXCEPT THAT THE AUXILIARY LAYERS ARE REMOVED AND THE TOTAL ITERATION IS HALVED TO 40K.\nEncoder Decoder\nCNN Transformer\nModel Params Semantic FPN [31] SETR-PUP [77] SETR-MLA [77] DPT [46] UperNet [63] Trans2Seg [64] FPT(ours)\nPVT-T [55] 13M 43.34 41.13 43.50 43.67 43.92 43.03 44.62\nPGT-T (ours) 13M 49.04 46.59 49.21 48.63 49.95 49.56 50.48\nPVT-S [55] 25M 47.53 45.44 47.63 47.13 47.95 46.19 48.43\nSwin-T [37] 28M 47.01 44.43 47.29 47.25 48.95 47.42 49.42\nPGT-S (ours) 28M 50.37 47.94 50.91 50.67 51.58 50.83 51.93\nSwin-S [37] 50M 50.03 47.10 50.77 50.51 51.76 50.17 52.43\nPVT-L [55] 61M 49.70 47.20 50.27 49.52 50.53 49.72 51.08\nPGT-B (ours) 50M 53.26 50.29 53.73 53.24 54.02 53.44 54.06\nSwin-B [37] 88M 50.31 47.52 51.18 52.17 52.51 50.80 53.35\nViT-L/16‡ [16] 307M 52.65 52.42 52.87 52.65 52.93 52.78 53.43\nPGT-L (ours) 88M 54.09 52.61 54.60 54.31 54.73 54.13 55.24\nTABLE VI\nDIFFERENT RECEPTIVE FIELDS OF PGT. T HE A , B, C, D IN <A-B-C-D>\nREPRESENT THE GROUP NUMBER IN STAGE 1∼4 RESPECTIVELY .\nPyramid Group Numbers Top-1 Acc. (%)\n<64-16-4-1> 77.81\n<64-16-1-1> 78.19\n<16-4-1-1> 78.21\nTABLE VII\nDIFFERENT POSITION ENCODING APPROACHES IN PGT.\nPosition Encoding Top-1 Acc. (%)\nno pos. 76.33\nAPE 77.49\nCPE 78.19\nAPE&CPE 77.85\ncomputation, which demonstrates that global context modeling\non the 1/16 resolution features in stage3 is critical to the\nperformance. PGT<16-4-1-1> is slightly +0.02% better than\nPGT<64-16-1-1>, but much more expensive for memory cost.\nAs a result, we use PGT <64-16-1-1> in our PGT variants.\nPosition Encoding of PGT. Positional embeddings are used to\nintroduce the awareness of spatial location for self-attention,\nwhich have been proved to be critical to Transformers [12],\n[16], [37], [53]. To understand the effect of position encoding\nmethods for our PGT, we compare four different settings,\nnamely no position encoding (no pos.), learnable absolute\nposition encoding (APE) [16], conditional position encoding\n(CPE) [12] and a combination of APE and CPE (APE&CPE).\nAs shown in Figure VII, APE achieves +1.16% higher than\nno pos., which shows the importance of position encoding.\nCPE achieves +0.7% higher than APE and +0.34% higher than\nAPE&CPE. Furthermore, CPE is compatible with variable-\nlength input sequences, which is an obvious limitation of\nabsolute position encoding. Therefore, we use CPE in our PGT\nas an implicit position encoding scheme.\nCLS-Token of PGT. CLS-Token plays the role of a global\nimage representation, which is expected to contain all of\nTABLE VIII\nDIFFERENT CLS-T OKEN IN PGT.\nCLS-Token Top-1 Acc. (%)\nstage 4 77.82\nstage 1∼4 77.71\navgpooling 78.19\nTABLE IX\nCOMPARISONS OF DIFFERENT SETTINGS OF FPT. <A-B-C> REPRESENTS\nTHE SETTINGS FOR THE TRANSFORMER BLOCKS WHICH ARE PERFORMED\nON THE FEATURE MAPS WITH STRIDE 32(” A”), 16(” B”) AND 8(”C”),\nRESPECTIVELY . THE M IOU IS OBTAINED BY MULTI -SCALE INFERENCE ON\nCOCO-S TUFF VALIDATION DATASET .\nDepth Embedding dim SR-MSA ratio Fusion mode mIoU (%)\n<1-1-1> 256 <2-2-2> add 42.41\n<1-1-1> 512 <2-2-2> add 43.37\n<1-2-1> 512 <2-2-2> add 43.11\n<1-1-1> 512 <1-2-2> add 43.26\n<1-1-1> 512 <1-1-2> add 43.33\n<1-1-1> 512 <2-2-2> concat 43.35\ndiscriminate information for ﬁnal classiﬁcation. Some of Vi-\nsion Transformers kept CLS-Token throughout the network\n[16], while some works only involved it at the last stage\n[55]. Considering that CLS-Token may have missed some key\ninformation, [37] directly removed the CLS-Token and applied\naverage pooling on the last output of the backbone instead.\nInspired by above all, we study the impact of four CLS-token\npatterns for PGT. As shown in Figure VIII, average pooling\nperforms the best, achieving +0.37% and +0.48% higher than\nthe CLS-token in only stage4 and all stages respectively. Thus,\nin PGT, we simply apply average pooling on the ﬁnal output\nof stage 4 to generate a global token rather than CLS-Token.\nScales of FPT.With the limited segmentation training data, the\nscale of decoder is not the larger the better for segmentation\n[64], so that there is an important trade-off between the scale\nof FPT and the performance. In our FPT, the scale is mainly\ndetermined by depth, embedding dim, and the reduction ratio\nof SR-MSA. We compare several combinations of these hyper-\nparameters with PGT-S as the backbone on COCO-Stuff\n10\nvalidation set. All the settings are the same as Section 4.2,\nexcept the abandoned auxiliary layers.\nConsidering that features with stride 16 have a good trade-\noff in terms of containing semantic and detailed information,\nwe try to enlarge the depth (number of blocks) for Transformer\nblocks with 1/16 features in FPT. As shown in Table IX,\nincreasing the depth for 1/16 features results in a 0.26% mIoU\ndrop (43.37 vs. 43.11), which indicates that the FPT decoder\nis not the deeper the better.\nEmbedding dim refers to the dimension of tokens in trans-\nformer blocks of FPT. Excessive small embedding dim might\nlead to the lack of representation ability, and a too large one\nmay result in channel redundancy and increased computational\nburden. Considering the constraints of computing resources,\nwe tried two computation-friendly choices of embedding dims\n(dim=256 and 512). As shown in Table IX, dim=512 is\nsigniﬁcantly superior to dim=256 (43.37 vs. 42.41). Therefore,\nwe set the embedding dim to 512 in our FPT.\nSR-MSA is used to reduce memory and computation cost\nby spatially reducing the number of key and value tokens,\nespecially for high-resolution representations. Considering that\nthe token numbers of the low-resolution features are relatively\nfewer, we attempt to remove the spatial reduction for trans-\nformer blocks with 1/32 and 1/16 features. Table IX compares\nthe different choices of reduction ratio of SR-MSA in FPT.\nIt turns out that without using SR-MSA on low-resolution\nrepresentations, the computation cost is increased, while there\nis no signiﬁcant performance gain as expected (43.37 vs. 43.33\nvs. 43.26). On the contrary, not using SR-MSA results in slight\nperformance degradation.\nFusion Mode of FPT. In our FPT, the multi-level high-\nresolution features of each branch need to be fused to generate\nﬁner prediction. We explore two simple fusion mode, i.e.,\nelement-wise summation and channel-wise concatenation. As\nshown in Table IX, summation performs +0.02% slightly\nbetter than concatenation, thus we fuse the multi-level rep-\nresentations by the simpler and more efﬁcient element-wise\nsummation.\nV. C ONCLUSION\nWe have developed the Fully Transformer Networks (FTN)\nfor semantic image segmentation. The core contributions of\nFTN are the proposed Pyramid Group Transformer (PGT)\nencoder and Feature Pyramid Transformer (FPT) decoder.\nThe former learns hierarchical representations, which creates\nPyramid Group Multi-head Self Attention to dramatically\nimprove the efﬁciency and reduce the computation costs of\nthe traditional Multi-head Self Attention. The latter fuses\nsemantic-level and spatial-level information from multiple\nstages of the encoder, which can promotes to the generation\nof ﬁner segmentation results. Extensive experimental results\non PASCAL-Context, ADE20K, and COCO-Stuff have shown\nthat our FTN can outperform the state-of-the-art methods in\nsemantic image segmentation, demonstrating that the Fully\nTransformer Networks can achieve better results than hybrid\nTransformer and CNN approaches.\nREFERENCES\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer\nnormalization. arXiv preprint arXiv:1607.06450 , 2016.\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A\ndeep convolutional encoder-decoder architecture for image segmenta-\ntion. IEEE transactions on pattern analysis and machine intelligence ,\n39(12):2481–2495, 2017.\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V\nLe. Attention augmented convolutional networks. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision , pages 3286–\n3295, 2019.\n[4] Shubhankar Borse, Ying Wang, Yizhe Zhang, and Fatih Porikli. In-\nverseform: A loss function for structured boundary-aware segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5901–5911, 2021.\n[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing\nand stuff classes in context. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1209–1218, 2018.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko. End-to-end object detection\nwith transformers. In European Conference on Computer Vision , pages\n213–229. Springer, 2020.\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Mur-\nphy, and Alan L Yuille. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully connected\ncrfs. IEEE transactions on pattern analysis and machine intelligence ,\n40(4):834–848, 2017.\n[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig\nAdam. Rethinking atrous convolution for semantic image segmentation.\narXiv preprint arXiv:1706.05587 , 2017.\n[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff,\nand Hartwig Adam. Encoder-decoder with atrous separable convolution\nfor semantic image segmentation. In Proceedings of the European\nconference on computer vision (ECCV) , pages 801–818, 2018.\n[10] Wanli Chen, Xinge Zhu, Ruoqi Sun, Junjun He, Ruiyu Li, Xiaoyong\nShen, and Bei Yu. Tensor low-rank reconstruction for semantic seg-\nmentation. In European Conference on Computer Vision , pages 52–69.\nSpringer, 2020.\n[11] Bowen Cheng, Ross Girshick, Piotr Doll ´ar, Alexander C Berg, and\nAlexander Kirillov. Boundary iou: Improving object-centric image\nsegmentation evaluation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 15334–15342, 2021.\n[12] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei,\nHuaxia Xia, and Chunhua Shen. Conditional positional encodings for\nvision transformers. arXiv preprint arXiv:2102.10882 , 2021.\n[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,\nMarkus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and\nBernt Schiele. The cityscapes dataset for semantic urban scene under-\nstanding. In Proceedings of the IEEE conference on computer vision\nand pattern recognition , pages 3213–3223, 2016.\n[14] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and\nQuoc V Le. Autoaugment: Learning augmentation strategies from data.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 113–123, 2019.\n[15] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang.\nSemantic segmentation with context encoding and multi-path decoding.\nIEEE Transactions on Image Processing , 29:3520–3533, 2020.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference on Learning\nRepresentations, 2021.\n[17] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng\nYan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision\ntransformers. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 6824–6835, October 2021.\n[18] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang,\nand Hanqing Lu. Dual attention network for scene segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3146–3154, 2019.\n[19] Qichuan Geng, Hong Zhang, Xiaojuan Qi, Gao Huang, Ruigang Yang,\nand Zhong Zhou. Gated path selection network for semantic segmenta-\ntion. IEEE Transactions on Image Processing , 30:2436–2449, 2021.\n[20] Jatin Gupta, Sumindar Kaur Saini, and Mamta Juneja. Survey of\ndenoising and segmentation techniques for mri images of prostate for\nimproving diagnostic tools in medical applications. Materials Today:\nProceedings, 28:1667–1672, 2020.\n11\n[21] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe\nWang. Transformer in transformer. arXiv preprint arXiv:2103.00112 ,\n2021.\n[22] Matthias Harders and Gabor Szekely. Enhancing human-computer inter-\naction in medical segmentation. Proceedings of the IEEE , 91(9):1430–\n1442, 2003.\n[23] Jun-Yan He, Shi-Hua Liang, Xiao Wu, Bo Zhao, and Lei Zhang. Mgseg:\nMultiple granularity-based real-time semantic segmentation network.\nIEEE Transactions on Image Processing , 30:7200–7214, 2021.\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 770–778,\n2016.\n[25] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-\ntuning for text classiﬁcation. In ACL, 2018.\n[26] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-\nberger. Deep networks with stochastic depth. In European conference\non computer vision , pages 646–661. Springer, 2016.\n[27] Zilong Huang, Chunyu Wang, Xinggang Wang, Wenyu Liu, and Jing-\ndong Wang. Semantic image segmentation by scale-adaptive networks.\nIEEE Transactions on Image Processing , 29:2066–2077, 2020.\n[28] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao\nWei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmen-\ntation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 603–612, 2019.\n[29] Zhenchao Jin, Bin Liu, Qi Chu, and Nenghai Yu. Isnet: Integrate\nimage-level and semantic-level context for semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision, pages 7189–7198, 2021.\n[30] Longlong Jing, Yucheng Chen, and Yingli Tian. Coarse-to-ﬁne semantic\nsegmentation from image-level labels. IEEE Transactions on Image\nProcessing, 29:225–236, 2020.\n[31] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll ´ar.\nPanoptic feature pyramid networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 6399–\n6408, 2019.\n[32] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan:\nTowards diverse and interactive facial image manipulation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5549–5558, 2020.\n[33] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool.\nLocalvit: Bringing locality to vision transformers. arXiv preprint\narXiv:2104.05707, 2021.\n[34] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariha-\nran, and Serge Belongie. Feature pyramid networks for object detection.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2117–2125, 2017.\n[35] Jianbo Liu, Junjun He, Jiawei Zhang, Jimmy S Ren, and Hongsheng Li.\nEfﬁcientfcn: Holistically-guided decoding for semantic segmentation. In\nEuropean Conference on Computer Vision , pages 1–17. Springer, 2020.\n[36] Mingyuan Liu, Dan Schonfeld, and Wei Tang. Exploit visual dependency\nrelations for semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 9726–\n9735, 2021.\n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages 10012–\n10022, October 2021.\n[38] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 3431–\n3440, 2015.\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regulariza-\ntion. In ICLR, 2019.\n[40] Ling Luo, Dingyu Xue, and Xinglong Feng. Ehanet: An effective\nhierarchical aggregation network for face parsing. Applied Sciences ,\n10(9):3135, 2020.\n[41] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard S. Zemel. Un-\nderstanding the effective receptive ﬁeld in deep convolutional neural\nnetworks. In NIPS, 2016.\n[42] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.\nLearned in translation: Contextualized word vectors. In NIPS, 2017.\n[43] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-\nWhan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of\ncontext for object detection and semantic segmentation in the wild. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 891–898, 2014.\n[44] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning de-\nconvolution network for semantic segmentation. In Proceedings of the\nIEEE international conference on computer vision , pages 1520–1528,\n2015.\n[45] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He,\nand Piotr Doll ´ar. Designing network design spaces. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10428–10436, 2020.\n[46] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision trans-\nformers for dense prediction. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV) , pages 12179–12188,\nOctober 2021.\n[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev\nSatheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,\nMichael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision , 115(3):211–252,\n2015.\n[48] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew\nRabinovich. Going deeper with convolutions. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages 1–\n9, 2015.\n[49] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and\nZbigniew Wojna. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818–2826, 2016.\n[50] Gusi Te, Wei Hu, Yinglu Liu, Hailin Shi, and Tao Mei. Agrnet: Adaptive\ngraph representation learning and reasoning for face parsing. IEEE\nTransactions on Image Processing , 2021.\n[51] Gusi Te, Yinglu Liu, Wei Hu, Hailin Shi, and Tao Mei. Edge-\naware graph representation learning and reasoning for face parsing. In\nEuropean Conference on Computer Vision , pages 258–274. Springer,\n2020.\n[52] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,\nAlexandre Sablayrolles, and Herv ´e J´egou. Training data-efﬁcient image\ntransformers & distillation through attention. In International Confer-\nence on Machine Learning , pages 10347–10357. PMLR, 2021.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in neural information processing systems ,\npages 5998–6008, 2017.\n[54] Qiurui Wang, Chun Yuan, and Yan Liu. Learning deep conditional neural\nnetwork for image segmentation. IEEE Transactions on Multimedia ,\n21(7):1839–1852, 2019.\n[55] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\nLiang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer:\nA versatile backbone for dense prediction without convolutions. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), pages 568–578, October 2021.\n[56] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-\nlocal neural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 7794–7803, 2018.\n[57] Zhen Wei, Si Liu, Yao Sun, and Hefei Ling. Accurate facial image\nparsing at real-time speed. IEEE Transactions on Image Processing ,\n28(9):4659–4670, 2019.\n[58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai,\nLu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 22–31, October 2021.\n[59] Tianyi Wu, Yu Lu, Yu Zhu, Chuang Zhang, Ming Wu, Zhanyu Ma, and\nGuodong Guo. Ginet: Graph interaction network for scene parsing. In\nEuropean Conference on Computer Vision, pages 34–51. Springer, 2020.\n[60] Tianyi Wu, Sheng Tang, Rui Zhang, Juan Cao, and Jintao Li. Tree-\nstructured kronecker convolutional network for semantic segmentation.\nIn 2019 IEEE International Conference on Multimedia and Expo\n(ICME), pages 940–945. IEEE, 2019.\n[61] Tianyi Wu, Sheng Tang, Rui Zhang, Juan Cao, and Yongdong Zhang.\nCgnet: A light-weight context guided network for semantic segmenta-\ntion. IEEE Transactions on Image Processing , 30:1169–1179, 2020.\n[62] Tianyi Wu, Sheng Tang, Rui Zhang, and Guodong Guo. Consensus\nfeature network for scene parsing. IEEE Transactions on Multimedia ,\n2021.\n[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.\nUniﬁed perceptual parsing for scene understanding. In Proceedings of\nthe European Conference on Computer Vision (ECCV) , pages 418–434,\n2018.\n[64] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding\nLiang, and Ping Luo. Segmenting transparent objects in the wild with\ntransformer. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth\nInternational Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pages\n12\n1194–1200. International Joint Conferences on Artiﬁcial Intelligence\nOrganization, 8 2021. Main Track.\n[65] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming\nHe. Aggregated residual transformations for deep neural networks. In\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\n[66] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen,\nand Nong Sang. Context prior for scene segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 12416–12425, 2020.\n[67] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang\nJiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-\ntoken vit: Training vision transformers from scratch on imagenet. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), pages 558–567, October 2021.\n[68] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual repre-\nsentations for semantic segmentation. In ECCV, 2020.\n[69] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segﬁx: Model-\nagnostic boundary reﬁnement for segmentation. In European Conference\non Computer Vision , pages 489–506. Springer, 2020.\n[70] Matthew D Zeiler and Rob Fergus. Visualizing and understanding\nconvolutional networks. In European conference on computer vision ,\npages 818–833. Springer, 2014.\n[71] Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng\nHua, and Qianru Sun. Feature pyramid transformer. In European\nConference on Computer Vision , pages 323–339. Springer, 2020.\n[72] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-\nPaz. mixup: Beyond empirical risk minimization. In International\nConference on Learning Representations , 2018.\n[73] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang\nWang, Ambrish Tyagi, and Amit Agrawal. Context encoding for\nsemantic segmentation. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition , pages 7151–7160, 2018.\n[74] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei\nZhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision\ntransformer for high-resolution image encoding. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV) , pages\n2998–3008, October 2021.\n[75] Yang Zhang, Moyun Liu, Jingwu He, Fei Pan, and Yanwen Guo. Afﬁnity\nfusion graph-based framework for natural image segmentation. IEEE\nTransactions on Multimedia , pages 1–1, 2021.\n[76] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and\nJiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 2881–\n2890, 2017.\n[77] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo,\nYabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr,\net al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages 6881–6890,\n2021.\n[78] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Semantic understanding of scenes\nthrough the ade20k dataset. International Journal of Computer Vision ,\n127(3):302–321, 2019.\n[79] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xiang Bai.\nAsymmetric non-local neural networks for semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on Computer\nVision, pages 593–602, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7448713183403015
    },
    {
      "name": "Encoder",
      "score": 0.6230189800262451
    },
    {
      "name": "Segmentation",
      "score": 0.5968136787414551
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5737714767456055
    },
    {
      "name": "Transformer",
      "score": 0.5407528877258301
    },
    {
      "name": "Computer vision",
      "score": 0.45410996675491333
    },
    {
      "name": "Image segmentation",
      "score": 0.4466026723384857
    },
    {
      "name": "Parsing",
      "score": 0.42596665024757385
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38375023007392883
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}