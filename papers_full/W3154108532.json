{
  "title": "Changing the Mind of Transformers for Topically-Controllable Language Generation",
  "url": "https://openalex.org/W3154108532",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4316344404",
      "name": "Haw-Shiuan Chang",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2920778353",
      "name": "Jiaming Yuan",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2068391019",
      "name": "Mohit Iyyer",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2077104176",
      "name": "Andrew McCallum",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914855263",
    "https://openalex.org/W2888213795",
    "https://openalex.org/W3174847985",
    "https://openalex.org/W3021582395",
    "https://openalex.org/W2741986794",
    "https://openalex.org/W3021347125",
    "https://openalex.org/W2963614567",
    "https://openalex.org/W25648700",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2621430944",
    "https://openalex.org/W2989202909",
    "https://openalex.org/W2805486818",
    "https://openalex.org/W2790165607",
    "https://openalex.org/W2962753250",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W2572589325",
    "https://openalex.org/W2236262502",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W3099872554",
    "https://openalex.org/W2986530058",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2151373442",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W3037337776",
    "https://openalex.org/W2963951835",
    "https://openalex.org/W3103662468",
    "https://openalex.org/W2962895717",
    "https://openalex.org/W2997764164",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3142516437",
    "https://openalex.org/W1995638668",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2116216716",
    "https://openalex.org/W2992543296",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2965962253",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Large Transformer-based language models can aid human authors by suggesting plausible continuations of text written so far. However, current interactive writing assistants do not allow authors to guide text generation in desired topical directions. To address this limitation, we design a framework that displays multiple candidate upcoming topics, of which a user can select a subset to guide the generation. Our framework consists of two components: (1) a method that produces a set of candidate topics by predicting the centers of word clusters in the possible continuations, and (2) a text generation model whose output adheres to the chosen topics. The training of both components is self-supervised, using only unlabeled text. Our experiments demonstrate that our topic options are better than those of standard clustering approaches, and our framework often generates fluent sentences related to the chosen topics, as judged by automated metrics and crowdsourced workers.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2601–2611\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n2601\nChanging the Mind of Transformers\nfor Topically-Controllable Language Generation\nHaw-Shiuan Chang Jiaming Yuan Mohit Iyyer Andrew McCallum\nCICS, University of Massachusetts Amherst\nhschang@cs.umass.edu,jiamingyuan@umass.edu,\n{mccallum,miyyer}@cs.umass.edu\nAbstract\nLarge Transformer-based language models\ncan aid human authors by suggesting plausi-\nble continuations of text written so far. How-\never, current interactive writing assistants do\nnot allow authors to guide text generation in\ndesired topical directions. To address this lim-\nitation, we design a framework that displays\nmultiple candidate upcoming topics, of which\na user can select a subset to guide the gener-\nation. Our framework consists of two compo-\nnents: (1) a method that produces a set of can-\ndidate topics by predicting the centers of word\nclusters in the possible continuations, and (2) a\ntext generation model whose output adheres to\nthe chosen topics. The training of both compo-\nnents is self-supervised, using only unlabeled\ntext. Our experiments demonstrate that our\ntopic options are better than those of standard\nclustering approaches, and our framework of-\nten generates ﬂuent sentences related to the\nchosen topics, as judged by automated metrics\nand crowdsourced workers.\n1 Introduction\nRecently, Transformer-based language models\n(LMs) have achieved impressive performance in\nlanguage generation tasks (Radford et al., 2019;\nDai et al., 2019) such as open-domain story genera-\ntion (See et al., 2019a). When writing with the LM,\nusers often desire an intuitive and effective way to\ncontrol what a LM is going to generate (Keskar\net al., 2019). To address this need, interactive writ-\ning assistants provide options to reveal possible\ndevelopments of the story and generate continua-\ntions guided by the user-selected options.\nInteractive writing assistants have wide applica-\ntions in creative writing (Roemmele and Gordon,\n2015; Clark et al., 2018; Akoury et al., 2020), ed-\nucation (Luo et al., 2015), and gaming (Walton,\n2020). Nevertheless, the existing systems’ options\nusually do not provide ﬁne-grained control and/or\nStep 2: Might say these topics\nStep 1: Let’s see what \nlanguage models would say\nStep 3: Please \ntalk more about \nthese topics\n1  book  books  novels\n2  Essays  Perspectives  Perspective\n3  University  faculty  undergraduate\n4  Reid  Sen.  McConnell\n5  humanity  life  spirituality\n6 2011 2010 2009\n7  know  sure  want\n8  insistence  disdain  dismissive\n9  election  elections  Democratic\n10  U.S.  States  United\nInput Prompt: “Barack \nObama writes a new book”\nOutput Continuation:  “: The Future of a Democratic \nElection. The book tells the story of the 2008 election.”\nTransformer\n-based \nLanguage \nModels\nUser\nStep 4: Let me try. \nWhat does this \ncontinuation sound?\nFigure 1: Given an input prompt, the Transformer-\nbased LM provides K = 10 topics that might be men-\ntioned next and each topic is represented by M = 3\nwords. The user could guide the generation process by\nchoosing a subset of topics.\nrequire substantial human labor. In some prior\nwork (Keskar et al., 2019; Tu et al., 2019), users\nchoose among a static set of predeﬁned attributes\n(e.g., sentiment) that only provide coarse-grained\ncontrol. Other work (Roemmele and Gordon, 2015;\nClark et al., 2018) presents users with multiple\ngenerated continuations, which requires substan-\ntial reading effort and might not contain topics that\nusers want to see. Finally, options could be nodes in\na plot graph that are handcrafted (Luo et al., 2015)\nor derived from a collaboration between humans\nand machine (Li et al., 2013), but such choices are\nusually limited due to the high cost of preparing\nthe options.\nTo address these limitations, we propose an in-\nteractive writing framework that provides a set of\ntopics and guides the text generation by the user-\nchosen topics. The topic options are generated\ndynamically based on the input prompt to pro-\n2602\nInput Prompt: Barack Obama writes a new book\n: The Future of a Democratic Election. The \nbook tells the story of the 2008 election. \nTopic: election, elections, Democratic \nTopic: book, books, novels Topic: humanity, life, spirituality\non spirituality and the role of \nreligion in society \nTopic: God, Christ, eternal\n, entitled  My Living With God , and \nwrites that he will give the  gift of grace\n. In it he describes why many \nAmericans believe in political parties.\nTopic: understand, know, realize \nWord: story\nWord: zombie\nabout the United States entitled I \nDon't Care...You Bet I'm a Zombie.\nTopic: American, America, U.S. \nTopic: political, ideology, politics\n. In the United States, many people \nknow the story of the human race\nFigure 2: Examples of our generated options and continuations. We highlight the words in the continuation that\nare related to the chosen topics or to the speciﬁed word.\nvide ﬁne-grained control, and our models are self-\nsupervised without the need to deﬁne the attributes\nor collect annotations. As depicted in Figure 1, a\nuser can peek at the most probableK topics (shown\nas bags of words) appearing after the input prompt\nand control the generation by choosing the topics.\nIn Figure 2, we compare multiple generated sen-\ntences conditioned on different chosen topic(s) or\nspeciﬁed word(s). For example, if the user chooses\na topic about humanity, life, and spirituality, our\nsystem continues the input prompt “Barack Obama\nwrites a new book ” with “on spirituality and the\nroles of religion in society”. Then, we can use the\ngenerated text as the new input prompt and update\nthe set of topics to include other more relevant top-\nics such as God, Christ, and eternal. The process\ncan be repeated to create a plot tree.\nA user can also control the generation by spec-\nifying word(s) if the user wants to see the words\nthat are not in the topic list or seeks a transition\nto a word that is not directly related to the input\nprompt. For example, a user can ask our system to\ngenerate a sentence about zombie. Consequently,\nthe continuation of “Barack Obama writes a new\nbook” becomes “about the United States entitled I\nDon’t Care...You Bet I’m a Zombie”.\nThe system is realized by two components: an\noption generator and a conditional text generator.\nGiven a prompt, the option generator suggests a set\nof K topics. After a user chooses a subset of the\ntopics and speciﬁes some words, the embedding of\nevery word or topic will guide the conditional text\ngenerator to produce the continuation that is both\nconsistent with the existing prompt and relevant to\nthe chosen topics and words.\nBoth components are self-supervised and use\npretrained GPT2 models (Radford et al., 2019) to\nencode the input prompt. During training, the op-\ntion generator predicts the cluster centers of fu-\nture words, which are in the continuation of the\nprompt, based on the contextualized embeddings\nfrom GPT2. The conditional text generator ﬁne-\ntunes GPT2 to predict the next words given the\nprompt and a few subsequent words. Since both\ncomponents’ input and output only come from the\nprompt and its continuation, training the system\nonly requires a raw corpus, word tokenizers, and a\nlist of stop words. This makes the proposed method\nsuitable for open-domain story generation and eas-\nily being ﬁne-tuned for a speciﬁc domain.\nIn experiments, we demonstrate that our system\nrecommends high-quality topics and often generate\nsentences that follow the chosen topics. We com-\npare our option generator with global topic models\nsuch as LDA (Blei et al., 2001) or local topic mod-\nels such as clustering the words in the input prompt.\nThe results show that the proposed method gener-\nates signiﬁcantly more topics that are plausible and\npromote the narrative. Moreover, we compare our\nconditional text generator with PPLM (Plug and\nPlay Language Models) (Dathathri et al., 2020) and\ndemonstrate that our generation is more ﬂuent and\nrelevant to the chosen topics. Our code is available\nat https://github.com/iesl/interactive_LM.\n2 Method\nThe proposed framework consists of two compo-\nnents: option generator and conditional text gen-\nerator. In Figure 3, we illustrate the two compo-\nnents and their interaction. First, given the prompt\nx1, ..., xI inputted by a user, the option generator\nat the bottom of the ﬁgure outputs K topics. After\nthe user chooses two topics about book and elec-\ntion and speciﬁes one extra word story, the topics\n2603\nGPT2 Encoder\nbook, books, \nnovels\nt5\nLinear Layer\nWeighted average \nof GloVe\nt7\nelection, elections, \nDemocratic\ntw\nstory\nSample based \non probability\nGloVe\nGPT2 Encoder\nTransformer\nL1 L2 L3 L4 L5 L6 L7 L8 L9 L10\n3. write\nT1 T2 T3 T4 T5 T6 T7 T8 T9 T10\n3. choose\n2. show\n1. write\n4. show\n(b) Option \nGenerator\n(a) Conditional \nText Generator\nUser\nbook :newBarack …\n…\nbookObama newBarack writes a\nSoftmax\npw5 pw6…pw1 pf6 pf6 pf6\n+ +++++\nx1 x2 x3 x4 x5 x6\n̂ y 1\nc1 c2 c3 c4 c5 c6 c7 c8 c9 c10\nClosest M Words\nFigure 3: Our model architectures for (a) conditional\ntext generator and (b) option generator. During testing,\nthe information ﬂows from the bottom to the top.\nand word are passed to our text generator as the\ngeneration guidance. Accordingly, the generator\ncontinues to write the next token ˆy1.1\nIn the following subsections, we introduce our\nmodel designs and the way to train each component.\nMore implementation details are described in the\nappendix.\n2.1 Option Generator\nWhen we do not have labeled attributes in a corpus,\nwe can create options by clustering all the words in\na corpus into topics (Tu et al., 2019). The clustering\ncould be done by topic modeling approaches such\nas LDA (Blei et al., 2001). The resulting topics\nare static (i.e., the clustering is performed globally\n1The framework is ﬂexible. For example, the GPT2 en-\ncoders in the two components could be shared. Besides topics,\nthe option generator could be extended to predict likely at-\ntributes in the continuation such as positive sentiment and\nevent frames (Tu et al., 2019) if the corresponding label data\nare available in the training corpus.\nwithout considering the prompt). However, the\nprompt might have a narrow focus and the related\nwords of interest are all clustered into a single topic.\nA simple remedy is to cluster only the words\nin the prompt rather than all the words in the cor-\npus. The topics are created dynamically and locally\ngiven a prompt and can capture more ﬁne-grained\naspects in the continuations. However, the top-\nics derived from the prompt might provide less\ninspiration because the users have seen the prompt.\nAnother major drawback of the approach is that\nthe generated topics might encourage the LM to\ngenerate repetitive sentences or make a narrative\ncircle inside a loop.\nMotivated by the challenges, we propose an op-\ntion generator that predicts the cluster centers based\non the prompt instead of clustering the words in\nthe prompt during testing.\n2.1.1 Model Prediction\nThe goal of our option generator is to predict the\nK cluster centers of words in the possible continu-\nations and use the cluster centers as the topics user\ncould choose from. As in Figure 3 (b), the option\ngenerator uses GPT2 to encode the input prompt\nx1, ..., xI and passes the output embedding to K\ndifferent linear layers L1, ..., LK. To model the\ndependency of clusters, a Transformer (Vaswani\net al., 2017) takes the K embeddings as input and\npredicts the cluster centers c1, ...cK in GloVe (Pen-\nnington et al., 2014) space. During testing, each\npredicted cluster center is normalized by its L2\nnorm, and we use the M closest words in the\nnormalized GloVe space to represent the topicTi,\nwhich users can choose.\nWe choose to learn the cluster centers in GloVe\nspace rather than GPT2 or BERT (Devlin et al.,\n2019) space because the non-contextualized word\nembeddings are easier to visualize. Users can eas-\nily understand the meaning of a cluster center by\nseeing nearby words. We normalize GloVe space in\nthis work to make the squared L2 distance equal to\ntwice the cosine distance between two embeddings.\nOur architecture is similar to the one in Chang\net al. (2021), but we use a pretrained GPT2 en-\ncoder rather than train a BERT-like Transformer\nfrom scratch. Another difference is that we ignore\nthe connection between the second Transformer\nand the output of GPT2 to save GPU memory for\nhandling a longer input prompt.\n2604\nGloVe\n-nullnullnull\n- American … 2008 … election … of severe ...\nAfrican... \nLeak information Randomly selected words\nGPT2 Encoder + Lk + Transformer\nc1\n(b) Option Generator\nAfricanObama firstBarack becomes the\n(a) Conditional Text Generator\n-  American president in 2008 ,  in an election held\nagainst the backdrop of severe economic problems caused by policies started or worsened under  …\nAfrican\nTell GPT2 that the selected \nwords will appear in the future\n2008 severe...... of\nfirstBarack …\nAmerican\nAmericansAmerica\nelection\nelections\n2008\n2009\n2007\nvoters\nRepublicans\nDemocrats\neconomic\nnorth\nbus\nA randomly \nsampled word\nPush \naway\nPull closer\nPush away\nc2 c3 c4 c5 c6 c7 c8 c9 c10\nPull closer\nx1 x2 x3 x4 x5 x6 y1           y2                   y3          y4     y5    y6  y7 y8         y9           y10\nGPT2 Encoder + Softmax\nA word in continuation\nFigure 4: Training our two components using the same sentence. (a) We randomly pick n = 3 words in the actual\ncontinuation as our conditions for the text generator, and the null labels mean their predicted probabilities are\nignored in our loss. (b) We visualize 5 out of K = 10 generated topics in a normalized GloVe space. Red words\nare the ones that appear in the continuation and pull the nearby cluster centers closer during training.\n2.1.2 Model Training\nIn Figure 4 (b), we visualize our training proce-\ndure. For each input prompt in the training corpus,\nwe run a forward pass through the Transformers\nand get predicted cluster centers c1, ...cK. Next,\nwe collect 50 words in the continuation (except\nstop words) as positive examples and match the\nwords with cluster centers as in the E-step of the\nEM algorithm (Dempster et al., 1977). We mini-\nmize the distances between the centers and their\nnearby positive examples by backpropagating the\ngradients through the matching and updating our\nTransformer models. Furthermore, we randomly\nsample some words as negative examples and max-\nimize the distances between the cluster centers and\ntheir nearby embeddings from negative examples.\nUsing Figure 4 (b) as an example, the orange\ncluster center is pulled closer toward the embed-\nding of 2008, which appears in the continuation.\nThe green cluster center is pushed away from the\nembedding of north, a randomly sampled word.\nSince each output embedding ck is pulled by only\nthe nearby embeddings of words in the continua-\ntion, the output embedding will naturally become\nthe cluster center of the nearby continuation word\nembeddings. Notice that the related topics like\nDemocrats and Republicans are not observed in the\nprompt and continuation, but our model can predict\na red cluster center close to them because the model\ncan learn from other similar input prompts whose\ncontinuation mentions words like Democrats.\nChang et al. (2021) discover that non-negative\nsparse coding (NNSC) (Hoyer, 2002) could en-\ncourage the Transformers to predict more diverse\nand relevant topics compared with Kmeans, so we\nadopt NNSC as our clustering loss, and its formu-\nlation could be found in Chang et al. (2021).\n2.2 Conditional Text Generator\nAfter the user chooses topic(s) or speciﬁes word(s),\neach topic or word is converted to a GloVe em-\nbedding. The component aims to generate the text\ngiven the input prompt and the GloVe embeddings\nof the topics or words we prefer to see in the con-\ntinuation.\nUsers only see the M words closest to the kth\npredicted cluster center ck from our option genera-\ntor, so we compute the kth topic embedding as\ntk =\n∑M\nm=1 cos(ew\nm, ck)ew\nm\n||∑M\nm=1 cos(ewm, ck)ewm||\n, (1)\nwhere ew\nm is the normalized GloVe embedding of\nthe mth closet word and cos(ew\nm, ck) is the cosine\nsimilarities between the mth word embedding and\nthe embedding ck.\n2605\n2.2.1 Model Prediction\nDuring testing, the topic embeddings tk or em-\nbedding of the speciﬁed words are inserted into\nGPT2 encoder before xI, the last word piece in the\nprompt. The inserted embeddings nudge the GPT2\nto generate the sentences containing the desired\nwords with a higher probability.\nAs Figure 3 (a) shows, the GloVe embeddings\nare ﬁrst passed through a linear layer to make their\ndimension become the same as the hidden state\nsize of GPT2. Then, the transformed embeddings\nare added with special positional embeddings pf\nI ,\nwhich are different from those for the prompt pw\ni .\nThe special positional embedding tells GPT2 that\nthe inserted embeddings have a different meaning\nand where the conditional generation starts.\nThe GPT2 encoder’s output goes through a soft-\nmax layer, which computes the probability of each\ntoken being observed as the ﬁrst word piece in the\ncontinuation y1. We adopt top-k sampling (Fan\net al., 2018), which reduces the chance of sampling\nwords with low probability, to pick the next word,\nand autoregressively sample one token ˆyo at a time\nto generate the continuation ˆy1, ...,ˆyO.\n2.2.2 Model Training\nWe train the generator using the continuation\nof a prompt and some randomly selected non-\nstop words in the continuation as its generation\nconditions. Since the continuation contains the\nrandomly-selected words, the generator would be\nheavily penalized if it ignores the conditions by\nassigning low probabilities to the selected words in\nall the continuation positions.\nAn example is illustrated in Figure 4 (a). Given\nan input prompt in the training set, we randomly\npick a number n from 0 to K and sample n words\nfrom the next O = 25 words (except stop words).\nNext, the normalized GloVe embeddings of n\nwords are inserted to the GPT2 encoder before the\nlast word piece in the prompt, and we ignore the\noutput probabilities corresponding to the inserted\npositions during training. To speed up the training,\nwe conduct the future word insertion in multiple\npositions of each training text sequence.\nWe insert the future words just before the text\nthat might contain the words rather than at the be-\nginning as in the classic seq2seq model, because\nwe do not want the model to learn to generate the\ncontinuation based on the future topics that have\nnot yet be speciﬁed by the users (e.g., The GPT2\nshould not know that it will see election in the fu-\nture when it learns to generate Barack Obama ...\nduring training).\nBy allowing the LM to see the upcoming words\nearlier, we leak partial label information to the LM\ninput. Consequently, GPT2 learns to utilize the\ninformation and generate the sentence containing\nthe desired words to achieve a lower perplexity\nloss. Notice that the training method allows us to\nspecify our topical preference without signiﬁcantly\nscarifying generation efﬁciency and ﬂuency, but it\ncannot guarantee to generate all the desired topics,\nespecially when we specify multiple ones.\nOne concern of the method is that the LM cannot\nsee all possible sets of topics or words users might\nspecify during training. Besides, each GloVe em-\nbedding used to supervise LM comes from a single\nword, but we ask the LM to condition on average\nGloVe embedding of the top M words during test-\ning. Nevertheless, we observe that the LM is often\nable to generalize well in our experiments because\nsimilar words have similar GloVe embeddings, lots\nof training instances could be easily prepared by\nthe self-supervised method, and our option gener-\nator usually provides the topics mentioned in the\ncontinuation in our training corpus.\n3 Experiments\nWe evaluate two components separately, and both\nevaluations include automated metrics and human\njudgment. Throughout the evaluation, the number\nof topics K = 10 and the length of generations\nis 50 word pieces. We ﬁnd that ﬁxing K = 10\nworks well in our experiments. If the possible\ncontinuations cover more than 10 topics, our option\ngenerator tends to output the important topics. If\nthey cover fewer topics, our option generator tends\nto output the related topics that are not explicitly\nmentioned in the prompt or the duplicated topics.\nMore experiment setup details could be found in\nthe appendix.\n3.1 Datasets\nWe use 90% of English Wikipedia 2016 as our train-\ning set for both components, 5% as our validation\nset to determine the hyperparameters such as the\nnumber of epochs, and the remaining 5% as our\ntest set to perform the automated evaluation.\nFor human evaluation, we collect labels from\nAmazon Mechanical Turk (MTurk). We randomly\nsample sentences from the training set of STS\nbenchmark (STSb) (Cer et al., 2017) as our input\n2606\nprompts. Compared with Wikipedia, the sentences\nfrom STSb are easier to understand for annotators\nbecause a large portion of sentences in Wikipedia\ninvolves terminologies, depends on a longer con-\ntext, or might even just be a list of names.\nIn STSb, we sample 24 sentences as our prompts,\nand each method generates one continuation for\neach input prompt. Each generated continuation or\ntopics will be scored by three different workers.\n3.2 Option Generator Evaluation\nWe evaluate the topics from different option genera-\ntors by judging whether the topics will appear in the\ncontinuation and whether the topics would promote\nthe narrative. The goal is to have topics that are\nrelevant and provide new information. The topics\nthat are too similar to the prompt words might be\nredundant and not helpful because the users have\nalready seen the prompt.\n3.2.1 Automatic Evaluation Metrics\n• Sim: If the generated topics T can help users to\nwrite the continuation, the embedding of every\nnon-stop word in the actual continuation should\nbe similar to the embeddings of a generated topic.\nThus, we compute\nSim( ¯Y , T) =\nO′\n∑\no=1\nK\nmax\nk=1\n(tk)T e¯y\no, (2)\nwhere ¯Y = {¯yo}O′\no=1 is a set of non-stop words in\nthe continuation and O′ = 25. tk is the normal-\nized embedding of kth topic in T from equation 1\nand e¯y\no is the oth word in ¯Y .\n• Sim Short: When computing Sim, we use the in-\nput prompts containing around 180 words on av-\nerage. To examine the topic quality at the start of\nwriting, where the authors might need assistance\nthe most, we also report Sim( ¯Y , T) on short in-\nput prompts (with 35 words on average).\n• Sim Diff: The options that are helpful to users\nshould be sufﬁciently different from the words\nin the input prompt to promote the narrative\nand avoid generating repeated content. Thereby,\nwe also evaluate methods using Sim Diff =\nSim( ¯Y , T) - Sim( ¯X, T), where ¯X = {¯xi}I′\ni=1\nare the non-stop words in the input prompt.\n3.2.2 Human Evaluation\nOur questionnaire shows the prompt and asks\nwhich generated topics are likely to appear in\nScope Method Sim Sim Short Sim Diff\nGlobal\nSample 14.63 14.42 0.16\nLDA 36.86 36.02 -2.82\nKmeans 40.65 39.91 -3.40\nLocal\nSample 41.50 41.23 -12.51\nNNSC 46.70 42.80 -15.94\nKmeans 47.94 43.89 -16.12\nOurs 48.38 46.29 0.45\nTable 1: Comparison of the option generators using au-\ntomatic metrics. The best numbers within each scope\nare highlighted.\nScope Method L TP L&TP\nGlobal LDA 5.76±0.50 6.24±0.33 5.26±0.31\nKmeans6.94±0.36 6.13±0.30 5.96±0.31\nLocal Kmeans8.65±0.16 5.31±0.50 5.14±0.50\nOurs 7.85±0.25 6.96±0.26 6.75±0.28\nTable 2: Comparison of option generators using human\njudgment (mean ±standard error). L and TP refer to\nlikelihood and topic promotion, respectively.\na reasonable continuation and which topics pro-\nmote the narrative. For each method, we re-\nport the average number of its topics that are\nlikely to appear (L), promote the topic (TP), and\nboth (L&TP). For example, an MTurk worker is\nshown three topics generated by a method given\na prompt: ABC. The worker thinks A is likely\nto appear in the continuation and AB promote\nthe topic. Then, L= |{A}|=1, TP=|{AB}|=2, and\nL&TP=|{A}∩{AB}|=|{A}|=1 for this prompt.\n3.2.3 Option Generator Baselines\nWe compare our generator with two types of meth-\nods.2 The ﬁrst type performs the clustering glob-\nally and selects the most relevant topics to the input\nprompt from the static set of clusters. We cluster\nall the words into J = 150 topics by LDA (Blei\net al., 2001) ( LDA-global) and into J = 1000\ntopics by Kmeans on the normalized GloVe em-\nbedding space (Tu et al., 2019) (Kmeans-global).\nWe also randomly sampleK words from the whole\nvocabulary as our cluster centers (Sample-global).\nSimilar to equation 1, we ﬁnd the M words with\nthe closest embeddings to each cluster center to\nrepresent the topic and compute the topic embed-\nding tj as the weighted average embedding of M\nwords in the jth topic. Among all J cluster cen-\nters, we pick the K topics with the closest tj to the\n2Another alternative is to generate many continuations and\ncluster the words in the generation. However, the method takes\ntime, which might be prohibited by limited computational\nresources and the real-time interaction requirement.\n2607\nInput Prompt The study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s.LDA-global Kmeans-local Ours1 population, households 6 company, companies1 Norway, Sweden 6 also, however1 research, scientiﬁc 6 1980s, 1970s2 patients, treatment 7 Norwegian, Norway2 tripled, doubled 7 since, Since2 tissues, tissue 7 even, though3 psychology, research 8 story, book3 nearly, almost 8 Sweden, Finland3 patients, diagnosis 8 susceptibility, pathogenic4 police, prison 9 hospital, Hospital4 cancer, skin 9 study, studies4 DNA, gene 9 decreased, increased5 chemical, carbon 10 Icelandic, Iceland5 1950s, 1940s 10 found, discovered5 orange, purple 10 Sweden, Norway\nTable 3: Comparison of all K topics for the input prompt using M = 2 words closest to each topic.\nInput Prompt The study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s.\nGenerator Generated TextOption Text\nLDA-global Ours A study of the Norwegian police has conﬁrmed the cancer case. The law in Norway was the subject of the\nKmeans-local OursThe study also found that skin cancer nearly tripled in Norway and Sweden since the 1950s. As well, skin\nOurs PPLM In this study, a study was conducted conducted in Italy and in Finland. From the 1990s to the 1970s, there\nNone GPT2 The study also revealed that only 20% of the deaths in Norway were caused by a sudden cardiac response\nOurs Ours Recent studies have shown that melanin causes a decrease in genetic susceptibility in people in Norway,\nTable 4: The continuations that are generated by conditioning on all of K topics from different option generators.\nThe input prompt comes from STSb.\nprompt embedding, where the prompt embedding\nis the average embedding of all words in the input\nprompt.\nThe second type of methods discovers the K\ntopics from the input prompt. We cluster non-\nstop words in the prompt using non-negative sparse\ncoding (Hoyer, 2002) (NNSC-local) and Kmeans\n(Kmeans-local). We also sample K non-stop\nwords from the prompt and call it Sample-local.\nSimilar to equation 1, we represent each topic us-\ning M words and compute the weighted average\nof their embeddings tk as the input of our text gen-\nerator. Notice that the locally clustering methods\nproduce similar results when the prompts come\nfrom STSb due to their short lengths, so we only\ntest Kmeans-local in our human evaluation.\n3.2.4 Results\nIn Table 1, we show that local methods generate\nthe options more relevant to the input prompt than\nthe global methods due to signiﬁcantly higher Sim\nand Sim Short. Our method performs better com-\npared to other local methods, especially in Sim Diff,\nwhich highlights the high novelty of our generated\ntopics. The improvement on Sim Short is larger\nthan that on Sim because our method could suggest\nthe related topics that are not explicitly mentioned\nin the short prompt (e.g., U.S. in Figure 1).\nThe human evaluation results are presented in\nTable 2. Our method wins in terms of generat-\ning relevant topics that promote the narrative. The\nKmeans-local performs better in L because most\nof the words in the input prompts could be men-\ntioned again in the next sentence. However, it often\nleads to the redundant topics that are too similar to\nthe prompt.\nTable 3 compares the options generated by dif-\nferent methods while Table 4 compares the text\ngenerated using different option generators and text\ngenerators. More examples are presented in the ap-\npendix. In Table 3, we can see that most topics in\nKmeans-local do not promote the narrative, which\nmakes the generated continuation become a copy\nof the input prompt in Table 4. We will quantita-\ntively evaluate the generated continuations using\ndifferent option generators in the appendix. No-\ntice that the high redundancy problem is hard to be\nsolved by a conditional text generator because the\nrelatedness between the prompt and the generated\ntext is hard to be controlled (See et al., 2019b).\n3.3 Conditional Text Generator Evaluation\nTo demonstrate our text generator’s effectiveness,\nwe use our option generator to prepare the topic\nembeddings and randomly select n topics as our\nconditions to simulate the user’s choice, wheren\nis a random number from 1 to K. The sentences\ngenerated by different methods are compared.\n3.3.1 Automatic Evaluation Metrics\nWe match the union of M ×K top words in the\nchosen topics with the words in the generated con-\ntinuations and count the number of tokens that are\nmatched exactly (token), the number of matched\nword types (word), and the number of topics that\ncontain at least one matched word (topic) to mea-\nsure the relevancy between the continuations and\nthe chosen topics. Notice that the scores are under-\nestimated because the generation might mention\nwords in different morphological variations or other\n2608\nText Automatic Metrics Inference Human Judgement\nGeneration Relevancy Hit Quality Time Relevancy Fluency\nMethod Token Word Topic PPL (↓) Dist-1 Dist-2 s (↓) Recall Precision Score\nPPLM 1.48 0.99 0.77 18.49 40.29 80.83 17.74 30.56±2.96 56.01±4.41 3.83±0.13\nOurs 2.36 1.79 1.40 16.39 37.98 79.65 1.02 41.46±3.47 56.41±4.41 4.07±0.10\nGPT2 1.27 0.84 0.64 14.24 39.80 80.22 1.00 24.49±2.77 48.69±4.61 4.15±0.11\nTable 5: Comparison of conditional text generators. The numbers in Dist-1, Dist-2, Recall, and Precision are\npercentages. Lower perplexity (PPL) and inference time are better. The better performances between PPLM and\nour method are highlighted. In human evaluation, we report the mean ±standard error of each method.\nwords related to the topics.\nThe ﬂuency of the generated text is measured\nusing the perplexity (Serban et al., 2016) of the\noriginal GPT2 (with 345M parameters) without\nbeing ﬁne-tuned on Wikipedia. Dist-n (Li et al.,\n2016) is the ratio between the number of unique\nn-grams and the number of all n-grams in the con-\ntinuations, where n=1 or 2. Higher Dist-n implies\nmore diverse generations. The average inference\ntime per input prompt is also presented.\n3.3.2 Human Evaluation\nWe present the prompt and the generated continu-\nation and ask the worker to score the generation’s\nﬂuency from 1 (not ﬂuent at all) to 5 (very ﬂuent).\nNext, we show K topics and ask which topics are\nmentioned in the generation. Treating the worker’s\nchoices as prediction and the topics our model con-\nditions on as ground truth, we report the average\nprecision and recall of the prediction.\n3.3.3 Conditional Text Generator Baselines\nWe compare our method with PPLM (Plug and\nPlay Language Models) (Dathathri et al., 2020) due\nto its strong performance against the weighted de-\ncoding approach from Ghazvininejad et al. (2017)\nwhen the condition is a bag of words.\nThe condition for PPLM is the union of the top\nM words in the chosen topics and each word’s\nweight is neglected. We use our generation model\nwithout conditioning on any word (i.e., n = 0 )\nduring testing3 as the base model of PPLM. We\nalso present the performance of the base model\nitself as a reference to know the signiﬁcance of our\nimprovement (denoted as GPT2).\n3.3.4 Results\nTable 5 indicates that our model outperforms\nPPLM in all metrics except in Dist-1 and Dist-2.\nWe suspect that our model generates slightly less\n3We ﬁnd the model performs similarly compared with the\nGPT2 with no condition during training.\ndiverse sentences in order to make the generation\nmore relevant to the given topics.\nThe generation might mention a topic even if it\nis not chosen as a condition, so we achieve similar\nprecision compared to PPLM in human evalua-\ntion. The recall of PPLM means that only around\n30% of given topics are mentioned. The low recall\nindicates the difﬁculty of mentioning multiple ran-\ndomly selected topics in the next 50 word pieces\nwhile keeping the sentence ﬂuent. By contrast,\nachieving 40% on recall demonstrates the effective-\nness of our conditional text generator.\nCompared with PPLM, our model requires an\nadditional training step but achieves low inference\ntime and high relevancy to the given topics/words\nonce the training is ﬁnished. The beneﬁts make it\npreferable in our interactive writing application.\n4 Related Work\nDifferent interactive writing assistants provide dif-\nferent forms of options to let users express their\npreferences. The options could be manually de-\nﬁned classes (e.g., sentiment) (Keskar et al., 2019;\nDathathri et al., 2020), semantic frames (Tu et al.,\n2019), or event structures such as (subject, verb,\nobject, modiﬁer) (Martin et al., 2018; Tambwekar\net al., 2019; Ammanabrolu et al., 2020). The forms\nof options allow users to control the attributes of\nthe generated text but require labels or classiﬁers\nthat map the text to the attributes/options.\nThe options could also be a single query word at\nthe beginning (Austin, 2019), the article title (Yan,\n2016), politeness (Niu and Bansal, 2018) or speci-\nﬁcity (See et al., 2019b) of the text, or the length of\nthe generated sentence (Tu et al., 2019). However,\nthe options cannot provide ﬁne-grained control on\ntopical directions of the generated contents.\nA related research direction is the multi-stage\nstory generation. To make a long story more co-\nherent, recent work proposes to generate a skele-\nton and then generate the full text guided by\n2609\nthe skeleton. The skeleton could be a sequence\nof SRL frames (Fan et al., 2019), a sequence\nof event structure (subject, verb, object, prepo-\nsition, modiﬁer) (Ammanabrolu et al., 2020), a\nstory premise (Fan et al., 2018), or a story sum-\nmary (Chen et al., 2019). Users can revise the\nskeleton to control the generated text, but the ap-\nproaches assume the existence of the skeleton ex-\ntractor or labels in the training corpus. Besides,\nthe systems cannot suggest options given the par-\ntial text, which is one of the main focuses of our\ninteractive writing assistant.\nThe skeleton could also be multiple keyphrases.\nThe keyphrases are extracted based on word fre-\nquency (Ippolito et al., 2019; Tan et al., 2020; Wu\net al., 2020), an off-the-shelf keyword extraction\nmethod (Peng et al., 2018; Goldfarb-Tarrant et al.,\n2019; Yao et al., 2019; Rashkin et al., 2020; Zhang\net al., 2020), a sentence compression dataset and\nreinforcement learning (Xu et al., 2018), or image\ncaption datasets and ConceptNet (Lin et al., 2020).\nMost of the studies focus on modeling the long-\nterm dependency among the keyphrases and/or\nforcing the generation to contain the keyphrases.\nInstead, we focus on allowing users to determine\nthe topical directions of the generation. Compared\nwith conditioning on keyphrases, our interactive\nwriting assistant is especially helpful when users\ndo not know the exact phrases they want to see or\nwhen the given keyphrase extractor does not detect\nthe desired topics.\n5 Conclusion\nWe propose an interactive writing assistant that\ngenerates topic options given an input prompt and\ngenerates the continuation of the prompt given the\ntopics chosen by a user. We decompose the frame-\nwork into two components and propose a novel\nmodel for each component. The automated evalua-\ntion and human evaluation indicate that our system\ngenerates many topics that are related to but differ-\nent from the prompt, and generates the sentences\nthat are ﬂuent and relevant to the chosen topics.\nAcknowledgements\nWe thank Ao Liu for his preliminary exploration\nof this project and Nader Akoury for his helpful\nfeedbacks. We also thank the anonymous reviewers\nfor their constructive feedback.\nThis work was supported in part by the Cen-\nter for Data Science and the Center for Intelligent\nInformation Retrieval, in part by the Chan Zucker-\nberg Initiative under the project Scientiﬁc Knowl-\nedge Base Construction, in part using high per-\nformance computing equipment obtained under a\ngrant from the Collaborative R&D Fund managed\nby the Massachusetts Technology Collaborative,\nin part by the National Science Foundation (NSF)\ngrant numbers DMR-1534431 and IIS-1514053.\nAny opinions, ﬁndings, conclusions, or recom-\nmendations expressed in this material are those of\nthe authors and do not necessarily reﬂect those of\nthe sponsor.\nReferences\nNader Akoury, Shufan Wang, Josh Whiting, Stephen\nHood, Nanyun Peng, and Mohit Iyyer. 2020. STO-\nRIUM: A Dataset and Evaluation Platform for\nMachine-in-the-Loop Story Generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6470–6484, Online. Association for Computational\nLinguistics.\nPrithviraj Ammanabrolu, Ethan Tien, Wesley Cheung,\nZhaochen Luo, William Ma, Lara J. Martin, and\nMark O. Riedl. 2020. Story realization: Expand-\ning plot events into sentences. In The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 7375–7382. AAAI Press.\nJohn Austin. 2019. The book of endless history: Au-\nthorial use of GPT2 for interactive storytelling. In\nInteractive Storytelling - 12th International Confer-\nence on Interactive Digital Storytelling, ICIDS 2019,\nLittle Cottonwood Canyon, UT, USA, November 19-\n22, 2019, Proceedings , volume 11869, pages 429–\n432. Springer.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2001. Latent dirichlet allocation. In Advances in\nNeural Information Processing Systems 14 [Neural\nInformation Processing Systems: Natural and Syn-\nthetic, NIPS 2001, December 3-8, 2001, Vancouver,\nBritish Columbia, Canada] , pages 601–608. MIT\nPress.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nHaw-Shiuan Chang, Amol Agrawal, and Andrew Mc-\nCallum. 2021. Extending multi-sense word embed-\nding to phrases and sentences for unsupervised se-\nmantic applications. In Proceedings of the Twenty-\nSeventh AAAI Conference on Artiﬁcial Intelligence.\n2610\nGang Chen, Yang Liu, Huanbo Luan, Meng Zhang,\nQun Liu, and Maosong Sun. 2019. Learning to\npredict explainable plots for neural story generation.\narXiv preprint arXiv:1912.02395.\nElizabeth Clark, Anne Spencer Ross, Chenhao Tan,\nYangfeng Ji, and Noah A Smith. 2018. Creative writ-\ning with a machine in the loop: Case studies on slo-\ngans and stories. In 23rd International Conference\non Intelligent User Interfaces.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nArthur P Dempster, Nan M Laird, and Donald B Rubin.\n1977. Maximum likelihood from incomplete data\nvia the em algorithm. Journal of the Royal Statisti-\ncal Society: Series B (Methodological), 39(1):1–22.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2650–\n2660, Florence, Italy. Association for Computa-\ntional Linguistics.\nMarjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and\nKevin Knight. 2017. Hafez: an interactive poetry\ngeneration system. In Proceedings of ACL 2017,\nSystem Demonstrations , pages 43–48, Vancouver,\nCanada. Association for Computational Linguistics.\nSeraphina Goldfarb-Tarrant, Haining Feng, and\nNanyun Peng. 2019. Plan, write, and revise: an\ninteractive system for open-domain story generation.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n89–97, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPatrik O Hoyer. 2002. Non-negative sparse coding. In\nProceedings of the 12th IEEE Workshop on Neural\nNetworks for Signal Processing.\nDaphne Ippolito, David Grangier, Chris Callison-\nBurch, and Douglas Eck. 2019. Unsupervised hier-\narchical story inﬁlling. In Proceedings of the First\nWorkshop on Narrative Understanding , pages 37–\n43, Minneapolis, Minnesota. Association for Com-\nputational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R\nVarshney, Caiming Xiong, and Richard Socher.\n2019. CTRL: A conditional transformer language\nmodel for controllable generation. arXiv preprint\narXiv:1909.05858.\nBoyang Li, Stephen Lee-Urban, George Johnston, and\nMark Riedl. 2013. Story generation with crowd-\nsourced plot graphs. In Proceedings of the Twenty-\nSeventh AAAI Conference on Artiﬁcial Intelligence,\nJuly 14-18, 2013, Bellevue, Washington, USA. AAAI\nPress.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nLinbo Luo, Wentong Cai, Suiping Zhou, Michael Lees,\nand Haiyan Yin. 2015. A review of interactive narra-\ntive systems and technologies: a training perspective.\nSimulation, 91(2):126–147.\nLara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang,\nWilliam Hancock, Shruti Singh, Brent Harrison, and\nMark O. Riedl. 2018. Event representations for au-\ntomated story generation with deep neural nets. In\nProceedings of the Thirty-Second AAAI Conference\non Artiﬁcial Intelligence, (AAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 868–\n875. AAAI Press.\nTong Niu and Mohit Bansal. 2018. Polite dialogue gen-\neration without parallel data. Transactions of the As-\nsociation for Computational Linguistics, 6:373–389.\n2611\nNanyun Peng, Marjan Ghazvininejad, Jonathan May,\nand Kevin Knight. 2018. Towards controllable story\ngeneration. In Proceedings of the First Workshop on\nStorytelling, pages 43–49, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state\ntracking. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4274–4295, Online. Associa-\ntion for Computational Linguistics.\nMelissa Roemmele and Andrew S. Gordon. 2015. Cre-\native help: A story writing assistant. In Interac-\ntive Storytelling - 8th International Conference on\nInteractive Digital Storytelling, ICIDS 2015, Copen-\nhagen, Denmark, November 30 - December 4, 2015,\nProceedings, volume 9445, pages 81–92. Springer.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019a. Do\nmassively pretrained language models make better\nstorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 843–861, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nAbigail See, Stephen Roller, Douwe Kiela, and Ja-\nson Weston. 2019b. What makes a good conver-\nsation? how controllable attributes affect human\njudgments. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1702–1723, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-\ngio, Aaron C. Courville, and Joelle Pineau. 2016.\nBuilding end-to-end dialogue systems using gener-\native hierarchical neural network models. In Pro-\nceedings of the Thirtieth AAAI Conference on Arti-\nﬁcial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA, pages 3776–3784. AAAI Press.\nPradyumna Tambwekar, Murtaza Dhuliawala, Lara J.\nMartin, Animesh Mehta, Brent Harrison, and\nMark O. Riedl. 2019. Controllable neural story plot\ngeneration via reward shaping. In Proceedings of\nthe Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2019, Macao, China,\nAugust 10-16, 2019, pages 5982–5988. ijcai.org.\nBowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P\nXing, and Zhiting Hu. 2020. Progressive generation\nof long text. arXiv preprint arXiv:2006.15720.\nLifu Tu, Xiaoan Ding, Dong Yu, and Kevin Gimpel.\n2019. Generating diverse story continuations with\ncontrollable semantics. In Proceedings of the 3rd\nWorkshop on Neural Generation and Translation ,\npages 44–58, Hong Kong. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nNick Walton. 2020. AI dungeon.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Osten-\ndorf, et al. 2020. A controllable model of\ngrounded response generation. arXiv preprint\narXiv:2005.00613.\nJingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-\naoyan Cai, and Xu Sun. 2018. A skeleton-based\nmodel for promoting coherence among sentences in\nnarrative story generation. In Proceedings of the\n2018 Conference on Empirical Methods in Natu-\nral Language Processing , pages 4306–4315, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nRui Yan. 2016. i, poet: Automatic poetry composition\nthrough recurrent neural networks with iterative pol-\nishing schema. In Proceedings of the Twenty-Fifth\nInternational Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI 2016, New York, NY, USA, 9-15 July\n2016, pages 2238–2244. IJCAI/AAAI Press.\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling.\nIn The Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2019, Honolulu, Hawaii, USA,\nJanuary 27 - February 1, 2019 , pages 7378–7385.\nAAAI Press.\nYizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan,\nChris Brockett, and Bill Dolan. 2020. POINTER:\nconstrained progressive text generation via insertion-\nbased generative pre-training. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 8649–8670. Associ-\nation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8258914947509766
    },
    {
      "name": "Transformer",
      "score": 0.7342886924743652
    },
    {
      "name": "Cluster analysis",
      "score": 0.6633529663085938
    },
    {
      "name": "Text generation",
      "score": 0.6273365020751953
    },
    {
      "name": "Natural language processing",
      "score": 0.5671936869621277
    },
    {
      "name": "Language model",
      "score": 0.5411658883094788
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5358608365058899
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47586172819137573
    },
    {
      "name": "Programming language",
      "score": 0.2398805320262909
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}