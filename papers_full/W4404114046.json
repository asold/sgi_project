{
  "title": "An Application Programming Interface (API) Sensitive Data Identification Method Based on the Federated Large Language Model",
  "url": "https://openalex.org/W4404114046",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2102111143",
      "name": "Jianping Wu",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109759291",
      "name": "Lifeng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111633910",
      "name": "Siyuan Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097339356",
      "name": "Chunming Wu",
      "affiliations": [
        "Zhejiang University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200514128",
    "https://openalex.org/W4302600279",
    "https://openalex.org/W4205650511",
    "https://openalex.org/W4389945750",
    "https://openalex.org/W4399205773",
    "https://openalex.org/W2783522756",
    "https://openalex.org/W3163893137",
    "https://openalex.org/W3170070992",
    "https://openalex.org/W2541884796",
    "https://openalex.org/W4402794779",
    "https://openalex.org/W4362704894",
    "https://openalex.org/W4386309371",
    "https://openalex.org/W4318466533",
    "https://openalex.org/W6853251322"
  ],
  "abstract": "The traditional methods for identifying sensitive data in APIs mainly encompass rule-based and machine learning-based approaches. However, these methods suffer from inadequacies in terms of security and robustness, exhibit high false positive rates, and struggle to cope with evolving threat landscapes. This paper proposes a method for detecting sensitive data in APIs based on the Federated Large Language Model (FedAPILLM). This method applies the large language model Qwen2.5 and the LoRA instruction tuning technique within the framework of federated learning (FL) to the field of data security. Under the premise of protecting data privacy, a domain-specific corpus and knowledge base are constructed for pre-training and fine-tuning, resulting in a large language model specifically designed for identifying sensitive data in APIs. This paper conducts comparative experiments involving Llama3 8B, Llama3.1 8B, and Qwen2.5 14B. The results demonstrate that Qwen2.5 14B can achieve similar or better performance levels compared to the Llama3.1 8B model with fewer training iterations.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8350111246109009
    },
    {
      "name": "Programming language",
      "score": 0.5895532369613647
    },
    {
      "name": "Interface (matter)",
      "score": 0.5776221752166748
    },
    {
      "name": "Application programming interface",
      "score": 0.4793761074542999
    },
    {
      "name": "Interface description language",
      "score": 0.4531455636024475
    },
    {
      "name": "Operating system",
      "score": 0.2075251042842865
    },
    {
      "name": "User interface",
      "score": 0.17856234312057495
    },
    {
      "name": "Maximum bubble pressure method",
      "score": 0.0
    },
    {
      "name": "Bubble",
      "score": 0.0
    }
  ]
}