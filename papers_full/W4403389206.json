{
  "title": "Evaluating Zero-Shot Large Language Models Recommenders on Popularity Bias and Unfairness: A Comparative Approach to Traditional Algorithms",
  "url": "https://openalex.org/W4403389206",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Gustavo Mendon√ßa Ortega",
      "affiliations": [
        "Universidad San Pedro"
      ]
    },
    {
      "id": "https://openalex.org/A5100577313",
      "name": "Rodrigo Ferrari de Souza",
      "affiliations": [
        "Universidad San Pedro"
      ]
    },
    {
      "id": "https://openalex.org/A4221295405",
      "name": "Marcelo Garcia Manzato",
      "affiliations": [
        "Universidad San Pedro"
      ]
    },
    {
      "id": null,
      "name": "Gustavo Mendon√ßa Ortega",
      "affiliations": [
        "Brazilian Society of Computational and Applied Mathematics"
      ]
    },
    {
      "id": "https://openalex.org/A5100577313",
      "name": "Rodrigo Ferrari de Souza",
      "affiliations": [
        "Brazilian Society of Computational and Applied Mathematics"
      ]
    },
    {
      "id": "https://openalex.org/A4221295405",
      "name": "Marcelo Garcia Manzato",
      "affiliations": [
        "Brazilian Society of Computational and Applied Mathematics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134330728",
    "https://openalex.org/W4403724173",
    "https://openalex.org/W3158895417",
    "https://openalex.org/W4386081001",
    "https://openalex.org/W4392846385",
    "https://openalex.org/W1994389483",
    "https://openalex.org/W4367155933",
    "https://openalex.org/W4398186459",
    "https://openalex.org/W2911936302",
    "https://openalex.org/W4386730022",
    "https://openalex.org/W3175865138"
  ],
  "abstract": "Large Language Models (LLMs), such as ChatGPT, have transcended technological boundaries and are now widely used across various domains to enhance productivity. This widespread application highlights their versatility, with a notable presence as recommender systems. Existing literature already showcases their capabilities in this area. In this paper, we present a detailed empirical evaluation of the effectiveness of Zero-Shot LLMs, specifically ChatGPT 3.5 Turbo, without special settings, in calibrating popularity bias and ensuring fairness in movie and TV show recommendations when prompted. We particularly focus on how these models adapt their output, comparing them to traditional post-processing algorithms. Our findings indicate that LLMs, evaluated through metrics such as Mean Average Precision (MAP) and Mean Rank Miscalibration (MRMC), not only perform well but also have the potential to surpass conventional recommender systems models like Singular Value Decomposition (SVD) when paired with calibration methods. The results underscore the advantages of using LLMs in more advanced scenarios due to their ease of implementation and performance.",
  "full_text": "Evaluating Zero-Shot Large Language Models Recommenders on\nPopularity Bias and Unfairness: A Comparative Approach to\nTraditional Algorithms\nGustavo Mendon√ßa Ortega, Rodrigo Ferrari de Souza, Marcelo Garcia Manzato\ngustavo_ortega@usp.br,rodrigofsouza@usp.br,mmanzato@icmc.usp.br\nInstituto de Ci√™ncias Matem√°ticas e de Computa√ß√£o\nUniversidade de S√£o Paulo\nS√£o Carlos, SP\nABSTRACT\nLarge Language Models (LLMs), such as ChatGPT, have transcended\ntechnological boundaries and are now widely used across various\ndomains to enhance productivity. This widespread application high-\nlights their versatility, with a notable presence as recommender\nsystems. Existing literature already showcases their capabilities in\nthis area. In this paper, we present a detailed empirical evaluation\nof the effectiveness of Zero-Shot LLMs, specifically ChatGPT 3.5\nTurbo, without special settings, in calibrating popularity bias and\nensuring fairness in movie and TV show recommendations when\nprompted. We particularly focus on how these models adapt their\noutput, comparing them to traditional post-processing algorithms.\nOur findings indicate that LLMs, evaluated through metrics such\nas Mean Average Precision (MAP) and Mean Rank Miscalibration\n(MRMC), not only perform well but also have the potential to sur-\npass conventional recommender systems models like Singular Value\nDecomposition (SVD) when paired with calibration methods. The\nresults underscore the advantages of using LLMs in more advanced\nscenarios due to their ease of implementation and performance.\nKEYWORDS\nRecommender Systems, LLM, Zero-Shot, Popularity Bias, Fairness.\n1 INTRODUCTION\nTraditional recommendation algorithms often struggle with pop-\nularity bias and unfairness, impacting user satisfaction and trust.\nPopularity bias, where popular items are disproportionately recom-\nmended, limits the exposure of less popular but potentially relevant\nitems, reducing the diversity of recommendations [9]. Unfairness\narises when certain groups of users consistently receive lower-\nquality recommendations, leading to unequal access to information\nand opportunities. Addressing these issues is critical for creating\nequitable and effective recommender systems that serve all users\nfairly [14].\nDespite the availability of well-established recommender algo-\nrithms and bias/unfairness mitigation [15, 17, 18], the emergence\nand rapid popularization of Large Language Models (LLMs) have in-\ntroduced novel approaches for a wide range of tasks. These models\nIn: IV Concurso de Trabalhos de Inicia√ß√£o Cient√≠fica (CTIC 2024). Anais Estendidos do\nXXX Simp√≥sio Brasileiro de Sistemas Multim√≠dia e Web (CTIC‚Äô2024). Juiz de Fora/MG,\nBrazil. Porto Alegre: Brazilian Computer Society, 2024.\n¬© 2024 SBC ‚Äì Sociedade Brasileira de Computa√ß√£o.\nISSN 2596-1683\nare being rigorously tested in various domains, including classifi-\ncation problems [7], anomaly detection [2], mathematical problem\nsolving [19], and recommender systems [21].\nUnlike traditional algorithms that require extensive training and\nlarge collections of interactions, LLMs primarily use their extensive\ncontent and contextual understanding to generate recommenda-\ntions [8]. However, given their growing prominence, it is crucial\nto study LLMs in the context of bias and unfairness to ensure they\nprovide equitable and diverse recommendations without perpetu-\nating existing biases. Several recent works examine these aspects\n[5, 6, 10, 12, 16, 20], but there is a lack of studies comparing LLMs\nwith traditional approaches to bias and unfairness mitigation. This\ncomparison is essential to understand the effectiveness of LLMs\nin this task and to position these models relative to traditional\nmethods developed so far.\nGiven the current landscape, this paper presents a detailed empir-\nical evaluation of Zero-Shot LLMs, specifically ChatGPT 3.5 Turbo1,\nwithout any special settings, in calibrating popularity bias and en-\nsuring fairness in movie and TV show recommendations. We focus\non how these models adapt their outputs compared to traditional\npost-processing algorithms. By evaluating metrics such as Mean\nAverage Precision (MAP) and Mean Rank Miscalibration (MRMC),\nwe demonstrate that LLMs not only perform well but also have the\npotential to surpass conventional recommender system models like\nSingular Value Decomposition (SVD) when paired with calibration\nmethods.\nThis paper is organized as follows. Section 2 reviews the related\nwork and background information necessary for understanding the\ncontext of this study. Section 3 discusses the traditional approaches\nused in the field and highlights their strengths and limitations. Sec-\ntion 4 describes the experimental setup, including the data, methods,\nand tools used to conduct the research. Section 5 presents the re-\nsults of the experiments and provides an analysis of the findings.\nSection 6 summarizes the study‚Äôs main contributions and outlines\npotential directions for future research.\n2 RELATED WORK\nThe field of recommender systems has been significantly influenced\nby large language models (LLMs) like GPT (Generative Pre-trained\nTransformer) [3]. The literature reports works analyzing LLMs in a\ngeneral context [13], and in a specific context of bias [5, 10, 16] and\nunfairness [6, 12, 20] in recommender systems. These studies high-\nlight LLMs‚Äô potential and limitations, showing their performance in\n1https://platform.openai.com/docs/models/gpt-3-5-turbo\n45\nCTIC‚Äô2024, Juiz de Fora/MG, Brazil Gustavo Mendon√ßa Ortega, Rodrigo Ferrari de Souza, Marcelo Garcia Manzato\nrating prediction, explanation generation, and personalized recom-\nmendations while identifying significant biases. Various strategies,\nincluding tailored prompting and framework designs, are proposed\nto mitigate these biases and improve fairness. This work compares\ntraditional bias mitigation and fairness approaches with those pro-\nduced by LLMs, comprehensively evaluating their effectiveness in\naddressing these critical issues in recommendation systems.\n3 TRADITIONAL APPROACHES\nThere are several traditional approaches for mitigating bias and\nunfairness in recommendation systems. As noted in the literature\n[14], works proposing recommendation calibration can apply this\nstrategy at three stages: pre-processing, in-processing, and post-\nprocessing. In this paper, we chose the post-processing calibration\napproach due to its simplicity and independence from the training\ndata.\nWe selected four calibration approaches according to their rele-\nvance and recency:\n(1) CP: Proposed by [1], this method implements a calibration\ntechnique for popularity, similar to our proposed popularity\ncalibration, but using the Jensen-Shannon divergence metric\nfor comparing the profile and recommendation distributions.\nIn our experiments, we followed the authors‚Äô method for\nboth datasets to split the popularity into groups and ex-\nploited the parameter ùúÜ ‚àà[0,1]. This method is compared\nagainst our proposals using the SVD++ recommender. In this\napproach, the author divides users into popularity groups\nand uses a divergence measure to return the best recommen-\ndations for each user.\n(2) Steck‚Äôs Calibration: proposed by [18], this method works\nas a post-processing step for genre calibration. This approach\naims to provide the best possible recommendation list for the\nuser based on their genre preferences, ensuring it matches\ntheir interest proportion.\n(3) Personalized: proposed by [15], this method implements a\nswitch-based calibration, where some users receive the genre\ncalibration, and others receive the popularity calibration. In\nour experiments, we followed the authors‚Äô methodology\nand exploited the parameter ùúÜ ‚àà [0,1]. In this approach,\nthe authors consider that users should receive recommenda-\ntions calibrated based on popularity if they consume many\npopular items. This method works similarly to the one imple-\nmented by Steck [18]. Still, it uses some weights to balance\nrecommendations between accuracy and calibration and also\nconsiders the popularity of items in the calibration process.\n(4) Two Stage: proposed by [ 17], this method implements a\npipeline of two calibrations based on genres and popularity.\nIn our experiments, we followed the authors‚Äô methodology.\nFirst, it generates a recommended list from the model, fol-\nlowed by the best possible list that matches the user‚Äôs interest\nproportion regarding item popularity. After that, a second\ncalibration is done to produce a new list that meets the user‚Äôs\ninterests in terms of genre.\nAs a recommender algorithm, we selected the SVD++ [11] as the\nmodel to be combined with these approaches because it effectively\nFigure 1: Overview of proposed evaluation scenario.\nincorporates both explicit ratings and implicit feedback, leading to\nmore accurate and personalized recommendations.\n4 EXPERIMENTAL SETUP\nTo thoroughly measure the effectiveness of LLMs in this task and\ncompare the performance and the impact of calibration techniques,\nthe LLM model was evaluated as illustrated in Figure 1.\nThe evaluation process involves two main models: Zero-Shot\nChatGPT 3.5 Turbo and SVD++. Data from a dataset is fed into\nboth models to generate recommendations. ChatGPT provides rec-\nommendations based on its inherent capabilities without specific\npre-training, while SVD++ uses four post-processing calibration\ntechniques (CP, Steck‚Äôs, Personalized, and Two Stage) to improve\nfairness and mitigate bias. The performance of both models is then\ncompared, focusing on personalized and fair recommendations for\ndifferent user profiles, to assess the effectiveness of LLMs versus\ntraditional calibrated recommendation algorithms.\nConcerning the LLM, we designed three scenarios:\n(1) Baseline: no post-processing algorithms were applied. The\nprompt consists of a user‚Äôs historical profile, and the LLM is\nrequested for recommendations.\n(2) Popularity Debiasing: the LLM is informed through the\nprompt about the items‚Äô popularity and their distribution.\nThe LLM is requested to provide recommendations without\npopularity bias.\n(3) Fairness Recalibration: a genre distribution of the user‚Äôs\nprofile is added to the prompt, and the LLM is requested to\nprovide recommendations according to this distribution.\nThis structured approach facilitates a comprehensive understand-\ning of the calibration effectiveness and enables the empirical data\ncollection demonstrating the LLM‚Äôs ability to adjust its outputs\nbased on input prompts.\n4.1 Dataset\nTo conduct the tests, a subset of the MovieLens-20M2 dataset\nwas used, containing 2,809,860 interactions. Users with at least 30\ninteractions were selected, with the last 30 interactions divided into\n2https://grouplens.org/datasets/movielens/20m\n46\nEvaluating Zero-Shot Large Language Models Recommenders on Popularity Bias and Unfairness CTIC‚Äô2024, Juiz de Fora/MG, Brazil\n10 for the test set and 20 for the training set. From this, 4,000 users\nwere randomly selected for the test sample.\nThe training sample contains 2,769,860 interactions, 138,493 dis-\ntinct users, and 12,366 unique movies/TV shows. The test sample\ncomprises 40,000 interactions with 4,000 unique users and 599 items.\nBoth training and test sets use additional information related\nto genres and popularity for calibration techniques. The dataset\nincludes 19 genres (e.g., Action, Drama, Thriller), used to ensure fair-\nness in genres. A genre distribution is computed for each user based\non the training data, utilized by post-processing calibration meth-\nods for traditional approaches or incorporated into the prompts for\nthe LLM.\nFor popularity, items are divided into three categories:Head (H)\nfor the top 20% of interactions, Tail (T) for the bottom 20%, and\nMid (M) for the rest, based on Pareto‚Äôs principle.\nUsers are categorized into three groups based on [ 9]: Block-\nBuster (BB) for users consuming at least 50% of the most popular\nitems, Niche (N) for users consuming at least 50% of the least pop-\nular items, and Diverse (D) for users with preferences differing\nfrom the other two groups.\n4.2 ChatGPT 3.5 Turbo\nOur experiments aim to emulate a traditional model closely, ensur-\ning the capabilities of the LLM are measurable while minimizing\nprompt engineering influence. Recommendations are generated\nthrough a minimal prompt and the user‚Äôs past interactions without\na conversational context.\nFor bias evaluation, items are categorized into blockbuster, medium,\nand niche groups for popularity bias, and similar approaches are\napplied for gender fairness. Four prompts were designed and tested\nwith 15 random users for each scenario to avoid bias from random\nprompts.\nFor the Baseline execution (1), the prompt with the highest Mean\nAverage Precision (MAP) score was chosen. The same process was\nused for scenarios (2) and (3), with prompts designed to address\npopularity bias or fairness constraints. The best prompt for each\nscenario was selected based on empirical evaluation, using the\nMRMC of popularity for scenario (2) and the MRMC of genre for\nscenario (3). Figure 2 shows the three resulting prompts used in\nour experiments3.\nFinally, a mechanism was implemented to ensure that the rec-\nommendations returned by the model are contained within the\ndataset so that the metrics are not influenced. The process starts\nwith a request to the OpenAI API4. The response is checked to see\nif at least 5 titles are in the dataset. If fewer than 5 titles exist, it\nchecks whether fewer than 2 interactions exist for the same user.\nIf this condition is true, it increments the prompt, reports which\nrecommendations were in the dataset, and then repeats the process.\nIf there are at least 5 titles in the dataset or no fewer than 2 interac-\ntions for the same user, the recommendations are saved, and the\nprocess iterates to the following user.\n3All tested prompts can be found in the project‚Äôs GitHub: https://github.com/\nCuriousGu/llm_zeroshot_calibration.\n4https://api.openai.com/v1/\nFigure 2: Template of prompts used in three different con-\ntexts.\nIt is worth mentioning that the structures are very similar to\nensure that the effects were caused by the model and not only by\nthe prompt differences.\n4.3 Metrics\nIn our experiments, we evaluated the effects of different calibrations\nin terms of Mean Reciprocal Rank (MRR) for accuracy, Mean Rank\nMiscalibration (MRMC) [4] for fairness, and Long Tail Coverage\n(LTC) [15] for popularity bias. MRR ranges from 0 to 1, where higher\nvalues are better and Lower values for LTC mean recommended\nitems are popular. In the case of MRMC, which covers the interval\n[0,1](lower is better), we also use the harmonic mean (or F1 score)\nbetween MRMC of genres and popularity, where higher values are\nbetter:\nùêπ1 = 2 (1 ‚àíùëÄùëÖùëÄùê∂ùê∫ùëíùëõùëüùëí )‚àó( 1 ‚àíùëÄùëÖùëÄùê∂ ùëÉùëúùëù)\n(1 ‚àíùëÄùëÖùëÄùê∂ùê∫ùëíùëõùëüùëí )+( 1 ‚àíùëÄùëÖùëÄùê∂ ùëÉùëúùëù) (1)\n5 RESULTS\nTable 5 presents the results of our experiments. Analyzing theLTC\nmetric, which indicates long-tail coverage, it is evident that the\nSVD++ recommender algorithm calibrated with Steck‚Äôs [18] and\nPersonalized [15] approaches achieved the best values, meaning\nthey returned more diverse recommendations. Regarding the LLM,\nthe Fairness Recalibration LLM obtained better results. In this ap-\nproach, the prompt asks the model to recalibrate recommendations\naccording to a genre distribution, resulting in a better coverage of\nthe long-tail curve.\nRegarding MRMC, this metric evaluates how much the recom-\nmendations distribution differs from the user‚Äôs profile in terms of\ngenre distribution (MRMCùëî) and popularity (MRMCùëù ). In the first\ncase, the results show that the two traditional approaches involving\ngenre calibration, SVD++ calibrated with Steck‚Äôs [18] and SVD++\n47\nCTIC‚Äô2024, Juiz de Fora/MG, Brazil Gustavo Mendon√ßa Ortega, Rodrigo Ferrari de Souza, Marcelo Garcia Manzato\ncalibrated with Two Stage [17], achieved the best results. Among\nthe LLM approaches, Popularity Debiasing yielded the most fair\nvalues in this aspect. Concerning popularity, the table shows that\nthe LLM effectively provided calibrated recommendations to users\naccording to their profiles. We note a slight improvement in MRMC\nPop for the Popularity Debiasing approach compared to the other\ntwo LLM-based approaches. Regarding the traditional methods,\nSVD++ calibrated with CP [1] performed better.\nThe F1 Score metric confirms the good results of the LLM ap-\nproaches concerning genres and popularity fairness. The Popu-\nlarity Debiasing approach obtained the best values in this aspect.\nAmong the traditional approaches, the SVD++ calibrated with CP\nperformed better.\nAnother important aspect to analyze is accuracy, which can\nbe verified by the MRR metric. This metric shows that the LLM\napproaches could recommend more relevant items to the users,\nwith the Baseline and Fairness Recalibration performing almost\nidentically. On the other hand, the traditional approaches had signif-\nicantly lower values in terms of accuracy, indicating that the LLM\napproaches managed to balance fairness, diversity, and accuracy in\nreturning recommendations.\nTable 1: Comparison of LLM approaches with traditional\napproaches on the MovieLens 20M dataset.\nAlgorithm LTC MRMC ùëî MRMCùëù F1 MRR\nBaseline LLM 0.034 0.318 0.022 0.803 0.076\nPop. Debiasing LLM 0.024 0.300 0.020 0.816 0.075\nFairness Recalib. LLM 0.046 0.327 0.027 0.796 0.078\nSVD++ + CP 0.032 0.530 0.189 0.595 0.061\nSVD++ + Genres 0.050 0.204 0.649 0.484 0.018\nSVD++ + Personalized 0.050 0.217 0.647 0.486 0.015\nSVD++ + Two Stage 0.049 0.200 0.653 0.484 0.011\n6 CONCLUSION\nThis paper has presented a comprehensive empirical evaluation\nof Zero-Shot LLMs, particularly focusing on ChatGPT 3.5 Turbo,\nin addressing popularity bias and ensuring fairness in movie and\nTV show recommendations. Our study fills a critical gap in the\nexisting literature by directly comparing the performance of LLMs\nwith traditional post-processing algorithms used in recommender\nsystems.\nOur experiments show that LLMs demonstrated robust perfor-\nmance in calibrating recommendations, with improvements in MRR,\nMAP, and MRMC metrics. On the contrary, the long-tail coverage\nwas improved by traditional methods, indicating better diversity\non recommendations.\nIn future work, we plan to explore the application of Zero-Shot\nLLMs across a wider range of domains and content types to vali-\ndate and extend our findings. We also plan to conduct user-centric\nstudies to assess the real-world impact of LLM-generated recom-\nmendations on user satisfaction and trust.\nACKNOWLEDGMENT\nThe authors would like to thank the financial support from FAPESP\nand CNPq.\nREFERENCES\n[1] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher,\nand Edward Malthouse. 2021. User-centered evaluation of popularity bias in\nrecommender systems. InProceedings of the 29th ACM conference on user modeling,\nadaptation and personalization. 119‚Äì129.\n[2] Sarah Alnegheimish, Linh Nguyen, Laure Berti-Equille, and Kalyan Veeramacha-\nneni. 2024. Large language models can be zero-shot anomaly detectors for time\nseries? arXiv preprint arXiv:2405.14755(2024).\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems33 (2020), 1877‚Äì1901.\n[4] Diego Corr√™a da Silva, Marcelo Garcia Manzato, and Frederico Ara√∫jo Dur√£o. 2021.\nExploiting personalized calibration and metrics for fairness recommendation.\nExpert Systems with Applications181 (2021), 115112.\n[5] Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Nar-\nducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. Evaluating chatgpt as\na recommender system: A rigorous approach. arXiv preprint arXiv:2309.03613\n(2023).\n[6] Mateo Gutierrez Granada, Dina Zilbershtein, Daan Odijk, and Francesco Barile.\n2023. VideolandGPT: A User Study on a Conversational Recommender System.\narXiv preprint arXiv:2309.03645(2023).\n[7] Joshua Harris, Timothy Laurence, Leo Loman, Fan Grayson, Toby Nonnenmacher,\nHarry Long, Loes WalsGriffith, Amy Douglas, Holly Fountain, Stelios Georgiou,\net al. 2024. Evaluating Large Language Models for Public Health Classification\nand Extraction Tasks. arXiv preprint arXiv:2405.14766(2024).\n[8] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,\nBodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large\nlanguage models as zero-shot conversational recommenders. InProceedings of the\n32nd ACM international conference on information and knowledge management.\n720‚Äì730.\n[9] Abdollahpouri Himan, Mansoury Masoud, Burke Robin, and Mobasher Bamshad.\n2019. The unfairness of popularity bias in recommendation. arXiv preprint\narXiv:1907.13286 (2019).\n[10] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\nand Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for\nrecommender systems. In European Conference on Information Retrieval. Springer,\n364‚Äì381.\n[11] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted\ncollaborative filtering model. InProceedings of the 14th ACM SIGKDD international\nconference on Knowledge discovery and data mining. 426‚Äì434.\n[12] Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023. A preliminary study\nof chatgpt on news recommendation: Personalization, provider fairness, fake\nnews. arXiv preprint arXiv:2306.10702(2023).\n[13] Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang.\n2023. Is chatgpt a good recommender? a preliminary study. arXiv preprint\narXiv:2304.10149 (2023).\n[14] Evaggelia Pitoura, Kostas Stefanidis, and Georgia Koutrika. 2022. Fairness in\nrankings and recommendations: an overview. The VLDB Journal(2022), 1‚Äì28.\n[15] Andre Sacilotti, Rodrigo Ferrari de Souza, and Marcelo Garcia Manzato. 2023.\nCounteracting popularity-bias and improving diversity through calibrated rec-\nommendations. In Proceedings.\n[16] Yubo Shu, Hansu Gu, Peng Zhang, Haonan Zhang, Tun Lu, Dongsheng Li, and\nNing Gu. 2023. RAH! RecSys-Assistant-Human: A Human-Central Recommen-\ndation Framework with Large Language Models. arXiv preprint arXiv:2308.09904\n(2023).\n[17] Rodrigo Souza and Marcelo Manzato. 2024. A Two-Stage Calibration Approach\nfor Mitigating Bias and Fairness in Recommender Systems. In Proceedings of the\n39th ACM/SIGAPP Symposium on Applied Computing. 1659‚Äì1661.\n[18] Harald Steck. 2018. Calibrated recommendations. In Proceedings of the 12th ACM\nconference on recommender systems. 154‚Äì162.\n[19] Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, and Yang Wang.\n2024. Can LLMs Solve longer Math Word Problems Better? arXiv preprint\narXiv:2405.14804 (2024).\n[20] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. Is chatgpt fair for recommendation? evaluating fairness in large lan-\nguage model recommendation. In Proceedings of the 17th ACM Conference on\nRecommender Systems. 993‚Äì999.\n[21] Lemei Zhang, Peng Liu, Yashar Deldjoo, Yong Zheng, and Jon Atle Gulla. 2024.\nUnderstanding Language Modeling Paradigm Adaptations in Recommender\nSystems: Lessons Learned and Open Challenges. arXiv preprint arXiv:2404.03788\n(2024).\n48",
  "topic": "Popularity",
  "concepts": [
    {
      "name": "Popularity",
      "score": 0.7141543626785278
    },
    {
      "name": "Computer science",
      "score": 0.6676343679428101
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5759618878364563
    },
    {
      "name": "Shot (pellet)",
      "score": 0.48438650369644165
    },
    {
      "name": "Algorithm",
      "score": 0.47988736629486084
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3867032825946808
    },
    {
      "name": "Linguistics",
      "score": 0.09246706962585449
    },
    {
      "name": "Psychology",
      "score": 0.08766999840736389
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}