{
    "title": "Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA",
    "url": "https://openalex.org/W2982834534",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5072304825",
            "name": "Ronghang Hu",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A5100672985",
            "name": "Amanpreet Singh",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A5029105520",
            "name": "Trevor Darrell",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5024481540",
            "name": "Marcus Rohrbach",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2899505139",
        "https://openalex.org/W2964067226",
        "https://openalex.org/W2967593235",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2963622213",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2542835211",
        "https://openalex.org/W2938082352",
        "https://openalex.org/W2963717374",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W2144554289",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2979382951",
        "https://openalex.org/W2795151422",
        "https://openalex.org/W2954165458",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2008806374",
        "https://openalex.org/W2988326850",
        "https://openalex.org/W2053317383",
        "https://openalex.org/W2968880719",
        "https://openalex.org/W2597425697",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2809273748",
        "https://openalex.org/W2952913664",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W2950738719",
        "https://openalex.org/W2963383024",
        "https://openalex.org/W2963521239",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W3004349648",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2253806798",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2970192826",
        "https://openalex.org/W2963668159",
        "https://openalex.org/W3004268082",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2888894220",
        "https://openalex.org/W2012689760"
    ],
    "abstract": "Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks. For example, a deep water label on a warning sign warns people about the danger in the scene. Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question. However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task. In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images. Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context. Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification. Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.",
    "full_text": "Iterative Answer Prediction with Pointer-Augmented\nMultimodal Transformers for TextVQA\nRonghang Hu1,2 Amanpreet Singh1 Trevor Darrell2 Marcus Rohrbach1\n1Facebook AI Research (FAIR) 2University of California, Berkeley\n{ronghang,trevor}@eecs.berkeley.edu, {asg,mrf}@fb.com\nAbstract\nMany visual scenes contain text that carries crucial in-\nformation, and it is thus essential to understand text in im-\nages for downstream reasoning tasks. For example, a deep\nwater label on a warning sign warns people about the dan-\nger in the scene. Recent work has explored the TextVQA\ntask that requires reading and understanding text in im-\nages to answer a question. However, existing approaches\nfor TextVQA are mostly based on custom pairwise fusion\nmechanisms between a pair of two modalities and are re-\nstricted to a single prediction step by casting TextVQA as a\nclassiﬁcation task. In this work, we propose a novel model\nfor the TextVQA task based on a multimodal transformer\narchitecture accompanied by a rich representation for text\nin images. Our model naturally fuses different modalities\nhomogeneously by embedding them into a common seman-\ntic space where self-attention is applied to model inter- and\nintra- modality context. Furthermore, it enables iterative\nanswer decoding with a dynamic pointer network, allowing\nthe model to form an answer through multi-step prediction\ninstead of one-step classiﬁcation. Our model outperforms\nexisting approaches on three benchmark datasets for the\nTextVQA task by a large margin.\n1. Introduction\nAs a prominent task for visual reasoning, the Visual\nQuestion Answering (VQA) task [4] has received wide at-\ntention in terms of both datasets (e.g. [4, 17, 22, 21, 20]) and\nmethods (e.g. [14, 3, 6, 25, 33]). However, these datasets\nand methods mostly focus on the visual components in the\nscene. On the other hand, they tend to ignore a crucial\nmodality – text in the images – that carries essential in-\nformation for scene understanding and reasoning. For ex-\nample, in Figure 1, deep water on the sign warns people\nabout the danger in the scene. To address this drawback,\nnew VQA datasets [44, 8, 37] have been recently proposed\nwith questions that explicitly require understanding and rea-\nsoning about text in the image, which is referred to as the\nTextVQA task.\njoint embedding+ multimodal transformer\nt=0\nt=1\nanswer: “deep water”\n“deep”“water”iterative answer decoding\ndynamic pointers\nOur model\nquestion words\nvisual objects\ntext in image (richfeatures)\nQuestion: What is the danger?Previous work: waterOur model: deepwater\nTextVQA\nFigure 1. Compared to previous work (e.g. [44]) on the TextVQA\ntask, our model, accompanied by rich features for image text,\nhandles all modalities with a multimodal transformer over a joint\nembedding space instead of pairwise fusion mechanisms between\nmodalities. Furthermore, answers are predicted through iterative\ndecoding with pointers instead of one-step classiﬁcation over a\nﬁxed vocabulary or copying single text token from the image.\nThe TextVQA task distinctively requires models to see,\nread and reason over three modalities: the input question,\nthe visual contents in the image such as visual objects, and\nthe text in the image. Several approaches [44, 8, 37, 7] have\nbeen proposed for the TextVQA task, based on OCR results\nof the image. In particular, LoRRA [44] extends previous\nVQA models [43] with an OCR attention branch and adds\nOCR tokens as a dynamic vocabulary to the answer classi-\nﬁer, allowing copying a single OCR token from the image\nas the answer. Similarly in [37], OCR tokens are grouped\ninto blocks and added to the output space of a VQA model.\nWhile these approaches enable reading text in images to\nsome extent, they typically rely on custom pairwise mul-\ntimodal fusion mechanisms between two modalities (such\nas single-hop attention over image regions and text tokens,\nconditioned on the input question), which limit the types\nof possible interactions between modalities. Furthermore,\nthey treat answer prediction as a single-step classiﬁcation\nproblem – either selecting an answer from the training set\n1\narXiv:1911.06258v3  [cs.CV]  24 Mar 2020\nanswers or copying a text token from the image – making\nit difﬁcult to generate complex answers such as book titles\nor signboard names with multiple words, or answers with\nboth common words and speciﬁc image text tokens, such as\nMcDonald’s burger where McDonald’sis from text in the\nimage and burger is from the model’s own vocabulary. In\naddition, the word embedding based image text features in\nprevious work have limited representation power and miss\nimportant cues such as the appearance ( e.g. font and color)\nand the location of text tokens in images. For example, to-\nkens that have different fonts and are spatially apart from\neach other usually do not belong to the same street sign.\nIn this paper, we address the above limitations with our\nnovel Multimodal Multi-Copy Mesh (M4C) model for the\nTextVQA task, based on the transformer [48] architecture\naccompanied by iterative answer decoding through dynamic\npointers, as shown in Figure 1. Our model naturally fuses\nthe three input modalities and captures intra- and inter-\nmodality interactions homogeneously within a multimodal\ntransformer, which projects all entities from each modality\ninto a common semantic embedding space, and applies the\nself-attention mechanism [38, 48] to collect relational rep-\nresentations for each entity. Instead of casting answer pre-\ndiction as a classiﬁcation task, we perform iterative answer\ndecoding in multiple steps and augment our answer decoder\nwith a dynamic pointer network that allows selecting text\nin the image in a permutation-invariant way without rely-\ning on any ad-hoc position indices in previous work such as\nLoRRA [44]. Furthermore, our model is capable of combin-\ning its own vocabulary with text in the image in a generated\nanswer, as shown in examples in Figure 4 and 5. Finally,\nwe introduce a rich representation for text tokens in the im-\nages based on multiple cues, including its word embedding,\nappearance, location, and character-level information.\nOur contributions in this paper are as follows: 1) We\nshow that multiple (more than two) input modalities can\nbe naturally fused and jointly modeled through our multi-\nmodal transformer architecture. 2) Unlike previous work\non TextVQA, our model reasons about the answer beyond a\nsingle classiﬁcation step and predicts it through our pointer-\naugmented multi-step decoder. 3) We adopt a rich feature\nrepresentation for text tokens in images and show that it is\nbetter than features based only on word embedding in previ-\nous work. 4) Our model signiﬁcantly outperforms previous\nwork on three challenging datasets for the TextVQA task:\nTextVQA [44] (+25% relative), ST-VQA [8] (+65% rela-\ntive), and OCR-VQA [37] (+32% relative).\n2. Related work\nVQA based on reading and understanding image text.\nRecently, a few datasets and methods [44, 8, 37, 7] have\nbeen proposed for visual question answering based on text\nin images (referred to as the TextVQA task). LoRRA [44],\na prominent prior work on this task, extends the Pythia\n[43] framework for VQA and allows it to copy a single\nOCR token from the image as the answer, by applying a\nsingle attention hop (conditioned on the question) over the\nOCR tokens and including the OCR token indices in the an-\nswer classiﬁer’s output space. A conceptually similar model\nis proposed in [37], where OCR tokens are grouped into\nblocks and added to both the input features and the output\nanswer space of a VQA model. In addition, a few other\napproaches [8, 7] enable text reading by augmenting exist-\ning VQA models with OCR inputs. However, these exist-\ning methods are limited by their simple feature represen-\ntation of image text, multimodal learning approaches, and\none-step classiﬁcation for answer outputs. In this work, we\naddress these limitations with our M4C model.\nMultimodal learning in vision-and-language tasks.\nEarly approaches on vision-and-language tasks often com-\nbined the image and text through attention over one modal-\nity conditioned on the other modality, such as image atten-\ntion based on text ( e.g. [51, 34]). Some approaches have\nexplored multimodal fusion mechanisms such as bilinear\nmodels (e.g. [14, 25]), self-attention ( e.g. [15]), and graph\nnetworks (e.g. [30]). Inspired by the success of Transformer\n[48] and BERT [13] architectures in natural language tasks,\nseveral recent works [33, 1, 47, 31, 29, 45, 53, 11] have\nalso applied transformer-based fusion between image and\ntext with self-supervision on large-scale datasets. However,\nmost existing works treat each modality with a speciﬁc set\nof parameters, which makes them hard to scale to more in-\nput modalities. On the other hand, in our work we project\nall entities from each modality into a joint embedding space\nand treat them homogeneously with a transformer architec-\nture over the list of all things. Our results suggest that joint\nembedding and self-attention are efﬁcient when modeling\nmultiple (more than two) input modalities.\nDynamic copying with pointers. Many answers in the\nTextVQA task come from text tokens in the image such as\nbook titles or street signs. As it is intractable to have every\npossible text token in the answer vocabulary, copying text\nfrom the image would often be an easier option for answer\nprediction. Prior work has explored dynamically copying\nthe inputs in different tasks such as text summarization [42],\nknowledge retrieval [52], and image captioning [35] based\non Pointer Networks [50] and its variants. For the TextVQA\ntask, recent works [44, 37] have proposed to copy OCR to-\nkens by adding their indices to classiﬁer outputs. However,\napart from their limitation of copying only a single token\n(or block), one drawback of these approaches is that they re-\nquire a pre-deﬁned number of OCR tokens (since the classi-\nﬁer has a ﬁxed output dimension) and their output is depen-\ndent on the ordering of the tokens. In this work, we over-\ncome this drawback using a permutation-invariant pointer\nnetwork together with our multimodal transformer.\n2\n3. Multimodal Multi-Copy Mesh (M4C)\nIn this work, we present Multimodal Multi-Copy Mesh\n(M4C), a novel approach for the TextVQA task based on\na pointer-augmented multimodal transformer architecture\nwith iterative answer prediction. Given a question and an\nimage as inputs, we extract feature representations from\nthree modalities – the question, the visual objects in the im-\nage, and the text present in the image. These three modali-\nties are represented respectively as a list of question words\nfeatures, a list of visual object features from an off-the-shelf\nobject detector, and a list of OCR token features based on\nan external OCR system.\nOur model projects the feature representations of enti-\nties (in our case, question words, detected objects, and de-\ntected OCR tokens) from the three modalities as vectors in\na learned common embedding space. Then, a multi-layer\ntransformer [48] is applied on the list of all projected fea-\ntures, enriching their representations with intra- and inter-\nmodality context. Our model learns to predict the an-\nswer through iterative decoding accompanied by a dynamic\npointer network. During decoding, it feeds in the previous\noutput to predict the next answer component in an auto-\nregressive manner. At each step, it either copies an OCR\ntoken from the image, or selects a word from its ﬁxed an-\nswer vocabulary. Figure 2 shows an overview of our model.\n3.1. A common embedding space for all modalities\nOur model receives inputs from three modalities – ques-\ntion words, visual objects, and OCR tokens. We extract fea-\nture representations for each modality and project them into\na common d-dimensional semantic space through domain-\nspeciﬁc embedding approaches as follows.\nEmbedding of question words. Given a question as a se-\nquence of K words, we embed these words into the corre-\nsponding sequence of d-dimensional feature vectors{xques\nk }\n(where k = 1, ··· , K) using a pretrained BERT model\n[13].1 During training, the BERT parameters are ﬁne-tuned\nusing the question answering loss.\nEmbedding of detected objects. Given an image, we\nobtain a set of M visual objects through a pretrained\ndetector (Faster R-CNN [41] in our case). Follow-\ning prior work [3, 43, 44], we extract appearance fea-\nture xfr\nm using the detector’s output from the m-th ob-\nject (where m = 1 , ··· , M). To capture its location\nin the image, we introduce a 4-dimensional location fea-\nture xb\nm from m-th object’s relative bounding box coor-\ndinates [xmin/Wim, ymin/Him, xmax/Wim, ymax/Him], where\nWim and Him are image width and height respectively.\nThen, the appearance feature and the location feature are\n1In our implementation, we extract question word features from the\nﬁrst 3 layers of BERT-BASE. We ﬁnd it sufﬁcient to use its ﬁrst few layers\ninstead of using all its 12 layers, which saves computation.\nprojected into thed-dimensional space with two learned lin-\near transforms (where d is the same as in the question word\nembedding above), and are summed up as the ﬁnal object\nembedding {xobj\nm }as\nxobj\nm = LN(W1xfr\nm) +LN(W2xb\nm) (1)\nwhere W1 and W2 are learned projection matrices. LN (·)\nis layer normalization [5], added on the output of the linear\ntransforms to ensure that the object embedding has the same\nscale as the question word embedding. We ﬁne-tune the last\nlayer of the Faster R-CNN detector during training.\nEmbedding of OCR tokens with rich representations.\nIntuitively, to represent text in images, one needs to en-\ncode not only its characters, but also its appearance ( e.g.\ncolor, font, and background) and spatial location in the im-\nage (e.g. words appearing on the top of a book cover are\nmore likely to be book titles). We follow this intuition in\nour model and use a rich OCR representation consisting\nof four types of features, which is shown in our experi-\nments to be signiﬁcantly better than word embedding (such\nas FastText) alone in prior work [44]. After obtaining a set\nof N OCR tokens in an image through external OCR sys-\ntems, from the n-th token (where n = 1, ··· , N) we ex-\ntract 1) a 300-dimensional FastText [9] vector xft\nn, which\nis a word embedding with sub-word information, 2) an ap-\npearance feature xfr\nn from the same Faster R-CNN detec-\ntor in the object detection above, extracted via RoI-Pooling\non the OCR token’s bounding box, 3) a 604-dimensional\nPyramidal Histogram of Characters (PHOC) [2] vector xp\nn,\ncapturing what characters are present in the token – this\nis more robust to OCR errors and can be seen as a coarse\ncharacter model, and 4) a 4-dimensional location feature\nxb\nn based on the OCR token’s relative bounding box co-\nordinates [xmin/Wim, ymin/Him, xmax/Wim, ymax/Him]. We\nlinearly project each feature into d-dimensional space, and\nsum them up (after layer normalization) as the ﬁnal OCR\ntoken embedding {xocr\nn }as below\nxocr\nn = LN(W3xft\nn + W4xfr\nn + W5xp\nn) +LN(W6xb\nn) (2)\nwhere W3, W4, W5 and W6 are learned projection matrices\nand LN(·) is layer normalization.\n3.2. Multimodal fusion and iterative answer predic-\ntion with pointer-augmented transformers\nAfter embedding all entities (question words, visual ob-\njects, and OCR tokens) from each modality as vectors in\nthe d-dimensional joint embedding space as described in\nSec. 3.1, we apply a stack of L transformer layers [48] with\na hidden dimension of d over the list of all K + M + N en-\ntities from {xques\nk }, {xobj\nm }, and {xocr\nn }. Through the multi-\nhead self-attention mechanism in transformers, each entity\nis allowed to freely attend to all other entities, regardless of\n3\nquestion:  what   is   the   speed   limit   of   this   road   ?answer:   75mphdetected objects: carroadsign…OCR tokens: speed   limit   75   exit  …\nquestionword 2questionword K… <begin>\n…\n… OCRtoken 1OCRtoken 2OCRtoken Ndetected object 1detected object 2detected object M\nmultimodal transformer layers\n…\nquestionword 1 previous output 1previous output T-1\ndynamic pointer network\n…\n…\nquestion word embedding\ndetected object embedding\nOCR token embedding\nprevious prediction embedding\nvocab scores 1\nOCRscore 1\nvocab scores 2\nvocab scores T\n……\n75speed limit56exitaragonite1 mile\nOCRscores 2OCRscores T\noutput 1output 2<end>…\nFigure 2. An overview of our M4C model. We project all entities (question words, detected visual objects, and detected OCR tokens) into\na common d-dimensional semantic space through domain-speciﬁc embedding approaches and apply multiple transformer layers over the\nlist of projected things. Based on the transformer outputs, we predict the answer through iterative auto-regressive decoding, where at each\nstep our model either selects an OCR token through our dynamic pointer network, or a word from its ﬁxed answer vocabulary.\nwhether they are from the same modality or not. For ex-\nample, an OCR token is allowed to attend to another OCR\ntoken, a detected object, or a question word. This enables\nmodeling both inter- and intra- modality relations in a ho-\nmogeneous way through the same set of transformer param-\neters. The output from our multimodal transformer is a list\nof d-dimensional feature vectors for entities in each modal-\nity, which can be seen as their enriched embedding in mul-\ntimodal context.\nWe predict an answer to the question through iterative\ndecoding, using exactly the same transformer layers as a\ndecoder. We decode the answer word by word in an auto-\nregressive manner for a total ofT steps, where each decoded\nword may be either an OCR token in the image or a word\nfrom our ﬁxed vocabulary of frequent answer words. As il-\nlustrated in Figure 2, at each step during decoding, we feed\nin an embedding of the previously predicted word, and pre-\ndict the next answer word based on the transformer output\nwith a dynamic pointer network.\nLet {zocr\n1 , ··· , zocr\nN }be the d-dimensional transformer\noutputs of theN OCR tokens in the image. Assume we have\na vocabulary of V words that frequently appear in the train-\ning set answers. At the t-th decoding step, the transformer\nmodel outputs a d-dimensional vector zdec\nt corresponding\nto the input xdec\nt at step t (explained later in this section).\nFrom zdec\nt , we predict both the V -dimensional scores yvoc\nt\nof choosing a word from ﬁxed answer vocabulary and the\nN-dimensional scores yocr\nt of selecting an OCR token from\nthe image at decoding step t. In our implementation, the\nﬁxed answer vocabulary score yvoc\nt,i for the i-th word (where\ni = 1, ··· , V) is predicted as a simple linear layer as\nyvoc\nt,i = (wvoc\ni )T zdec\nt + bvoc\ni (3)\nwhere wvoc\ni is a d-dimensional parameter for the i-th word\nin the answer vocabulary, and bvoc\ni is a scalar parameter.\nTo select a token from the N OCR tokens in the im-\nage, we augment the transformer model with a dynamic\npointer network, predicting a copying score yocr\nt,n (where\nn = 1, ··· , N) for each token via bilinear interaction be-\ntween the decoding output zdec\nt and each OCR token’s out-\nput representation zocr\nn as\nyocr\nt,n = (Wocrzocr\nn + bocr)T (\nWdeczdec\nt + bdec)\n(4)\nwhere Wocr and Wdec are d ×d matrices, and bocr and bdec\nare d-dimensional vectors.\nDuring prediction, we take the argmax on the concate-\nnation yall\nt = [yvoc\nt ; yocr\nt ] of ﬁxed answer vocabulary scores\nand dynamic OCR-copying scores, selecting the top scoring\nelement (either a vocabulary word or an OCR token) from\nall V + N candidates.\nIn our iterative auto-regressive decoding procedure, if\nthe prediction at decoding time-step t is an OCR token, we\nfeed in its OCR representation xocr\nn as the transformer input\nxdec\nt+1 to the next prediction step t + 1. Otherwise (the previ-\nous prediction is a word from the ﬁxed answer vocabulary),\nwe feed in its corresponding weight vector wvoc\ni in Eqn. 3\nas the next step’s inputxdec\nt+1. In addition, we add two extra\nd-dimensional vectors as inputs – a positional embedding\nvector corresponding to step t, and a type embedding vec-\ntor corresponding to whether the previous prediction is a\nﬁxed vocabulary word or an OCR token. Similar to ma-\nchine translation, we augment our answer vocabulary with\ntwo special tokens,<begin> and <end>. Here <begin>\nis used as the input to the ﬁrst decoding step, and we stop\nthe decoding process after <end> is predicted.\nTo ensure causality in answer decoding, we mask the\nattention weights in the self-attention layers of the trans-\nformer architecture [48] such that question words, detected\nobjects and OCR tokens cannot attend to any decoding\nsteps, and all decoding steps can only attend to previous de-\ncoding steps in addition to question words, detected objects\nand OCR tokens. This is similar to preﬁx LM in [40].\n4\n3.3. Training\nDuring training, we supervise our multimodal trans-\nformer at each decoding step. Similar to sequence pre-\ndiction tasks such as machine translation, we use teacher-\nforcing [28] (i.e. using ground-truth inputs to the decoder)\nto train our multi-step answer decoder, where each ground-\ntruth answer is tokenized into a sequence of words. Given\nthat an answer word can appear in both ﬁxed answer vocab-\nulary and OCR tokens, we apply multi-label sigmoid loss\n(instead of softmax loss) over the concatenated scores yall\nt .\n4. Experiments\nWe evaluate our model on three challenging datasets for\nthe TextVQA task, including TextVQA [44], ST-VQA [8],\nand OCR-VQA [37] (we use these datasets for research pur-\nposes only). Our model outperforms previous work by a\nsigniﬁcant margin on all the three datasets.\n4.1. Evaluation on the TextVQA dataset\nThe TextVQA dataset [44] contains 28,408 images from\nthe Open Images dataset [27], with human-written ques-\ntions asking to reason about text in the image. Similar to\nVQAv2 [17], each question in the TextVQA dataset has 10\nhuman annotated answers, and the ﬁnal accuracy is mea-\nsured via soft voting of the 10 answers.2\nWe use d = 768 as the dimensionality of the joint\nembedding space and extract question word features with\nBERT-BASE using the 768-dimensional outputs from its\nﬁrst three layers, which are ﬁne-tuned during training.\nFor visual objects, following Pythia [43] and LoRRA\n[44], we detect objects with a Faster R-CNN detector [41]\npretrained on the Visual Genome dataset [26], and keeps\n100 top-scoring objects per image. Then, the fc6 feature\nvector is extracted from each detected object. We apply the\nFaster R-CNN fc7 weights on the extracted fc6 features to\noutput 2048-dimensional fc7 appearance features and ﬁne-\ntune fc7 weights during training. However, we do not use\nthe ResNet-152 convolutional features [19] as in LoRRA.\nFinally, we extract text tokens on each image using the\nRosetta OCR system [10]. Unlike the prior work LoRRA\n[44] that uses a multilingual Rosetta version, in our model\nwe use an English-only version of Rosetta that we ﬁnd has\nhigher recall. We refer to these two versions as Rosetta-\nml and Rosetta-en, respectively. As mentioned in Sec. 3.1,\nfrom each OCR token we extract FastText [9] feature, ap-\npearance feature from Faster R-CNN ( FRCN), PHOC [2]\nfeature, and bounding box (bbox) feature.\nIn our multimodal transformer, we use L = 4 layers\nof multimodal transformer with 12 attention heads. Other\nhyper-parameters (such as dropout ratio) follow BERT-\nBASE [13]. However, we note that the multimodal trans-\n2See https://visualqa.org/evaluation for details.\nformer parameters are initialized from scratch rather than\nfrom a pretrained BERT model. We use T = 12maximum\ndecoding step in answer prediction unless otherwise speci-\nﬁed, which is sufﬁcient to cover almost all answers.\nWe collect the top 5000 frequent words from the answers\nin the training set as our answer vocabulary. During train-\ning, we use a batch size of 128, and train for a maximum of\n24,000 iterations. Our model is trained using the Adam op-\ntimizer, with a learning rate of 1e-4 and a staircase learning\nrate schedule, where we multiply the learning rate by 0.1 at\n14000 and at 19000 iterations. The best snapshot is selected\nusing the validation set accuracy. The entire training takes\napproximately 10 hours on 4 Nvidia Tesla V100 GPUs.\nAs a notable prior work on this dataset, we show a step-\nby-step comparison with the LoRRA model [44]. LoRRA\nuses two single-hop attention layers over image visual fea-\ntures and OCR features. The attended visual and OCR fea-\ntures are then fused with a vector encoding of the question\nand fed into a single-step classiﬁer to select either a frequent\nanswer from the training set or a single OCR token from the\nimage. Unlike our rich OCR representation in Sec. 3.1, in\nthe LoRRA model each OCR token is only represented as a\n300-dimensional FastText vector.\nAblations on pretrained question encoding and OCR\nsystems. We ﬁrst experiment with a restricted version of\nour model using the multimodal transformer architecture\nbut without iterative decoding in answer prediction, i.e.\nM4C (w/o dec.) in Table 1. In this setting, we only de-\ncode for one step, and either select a frequent answer3 from\nthe training set or copy a single OCR token in the image as\nthe answer. As a step-by-step comparison with LoRRA, we\nstart with extracting OCR tokens from Rosetta-ml, repre-\nsenting OCR tokens only with FastText vectors, and initial-\nizing question encoding parameters in Sec. 3.1 from scratch\n(rather than from a pretrained BERT-BASE model). The re-\nsult is shown in line 3 of Table 1. Compared with LoRRA\nin line 1, this restricted version of our model already out-\nperforms LoRRA by around 3% (absolute) on TextVQA\nvalidation set. This result shows that our multimodal trans-\nformer architecture is more efﬁcient for jointly modeling the\nthree input modalities. We also experiment with initializing\nthe word embedding from GloVe [39] as in LoRRA and the\nremaining parameters from scratch, shown in line 2. How-\never, we ﬁnd that this setting slightly under-performs ini-\ntializing everything from scratch, which we suspect is due\nto different question tokenization between LoRRA and the\nBERT tokenizer used in our model. We then switch to a pre-\ntrained BERT for question encoding in line 4, and Rosetta-\nen for OCR extraction in line 5. Comparing line 3 to 5,\nwe see that a pretrained BERT leads to around 0.6% higher\naccuracy, and Rosetta-en gives another 1% improvement.\n3In this case, we predict the entire (multi-word) answer, instead of a\nsingle word from our answer word vocabulary as in our full model.\n5\n# Method Question enc. OCR OCR token Output Accu. Accu.\npretraining system representation module on val on test\n1 LoRRA [44] GloVe Rosetta-ml FastText classiﬁer 26.56 27.63\n2 M4C w/o dec. GloVe Rosetta-ml FastText classiﬁer 29.36 –\n3 M4C w/o dec. (none) Rosetta-ml FastText classiﬁer 29.55 –\n4 M4C w/o dec. BERT Rosetta-ml FastText classiﬁer 30.15 –\n5 M4C w/o dec. BERT Rosetta-en FastText classiﬁer 31.28 –\n6 M4C w/o dec. BERT Rosetta-en FastText + bbox classiﬁer 33.32 –\n7 M4C w/o dec. BERT Rosetta-en FastText + bbox + FRCN classiﬁer 34.38 –\n8 M4C w/o dec. BERT Rosetta-en FastText + bbox + FRCN + PHOC classiﬁer 35.70 –\n9 M4C (ours - ablation) (none) Rosetta-ml FastText + bbox + FRCN + PHOC decoder 36.06 –\n10 M4C (ours - ablation) BERT Rosetta-ml FastText + bbox + FRCN + PHOC decoder 37.06 –\n11 M4C (ours) BERT Rosetta-en FastText + bbox + FRCN + PHOC decoder 39.40 39.01\n12 DCD ZJU (ensemble) [32] – – – – 31.48 31.44\n13 MSFT VTI [46] – – – – 32.92 32.46\n14 M4C (ours; w/ ST-VQA) BERT Rosetta-en FastText + bbox + FRCN + PHOC decoder 40.55 40.46\nTable 1. On the TextVQA dataset, we ablate our M4C model and show a detailed comparison with prior work LoRRA [44]. Our multimodal\ntransformer (line 3 vs 1), our rich OCR representation (line 8 vs 5) and our iterative answer prediction (line 11 vs 8) all improve the accuracy\nsigniﬁcantly. Notably, our model still outperforms LoRRA by 9.5% (absolute) even when using fewer pretrained parameters (line 9 vs 1).\nOur ﬁnal model achieves 39.01% (line 11) and 40.46% (line 14) test accuracy without and with the ST-VQA dataset as additional training\ndata respectively, outperforming the challenge-winning DCD ZJU method by 9% (absolute). See Sec. 4.1 for details.\n1 2 4 6 8 10 12\nmaximum decoding steps T\n30\n35\n40\n45\n50\n55\n60\n65accuracy\nTextVQA\nST-VQA\nOCR-VQA\nFigure 3. Accuracy under different maximum decoding stepsT on\nthe validation set of TextVQA, ST-VQA, and OCR-VQA. There is\na major gap between single-step (T = 1) and multi-step (T >1)\nanswer prediction. We use 12 steps by default in our experiments.\nAblations on OCR feature representation We analyze\nthe impact of our rich OCR representation in Sec. 3.1\nthrough ablations in Table 1 line 5 to 8. We see that OCR\nlocation (bbox) features and the RoI-pooled appearance fea-\ntures (FRCN) both improve the performance by a noticeable\nmargin. In addition, we ﬁnd that PHOC is also helpful as\na character-level representation of the OCR token. Our rich\nOCR representation gives around 4% (absolute) accuracy\nimprovement compare with using only FastText features as\nin LoRRA (line 8 vs 5). We note that our extra OCR fea-\ntures do not require more pretrained models, as we apply\nexactly the same Faster R-CNN model use in object detec-\ntion for OCR appearance features, and PHOC is a manually-\ndesigned feature that does not need pretraining.\nIterative answer decoding. We then apply our full M4C\nmodel with iterative answer decoding to the TextVQA\ndataset. The results are shown in Table 1 line 11, which\nis around 4% (absolute) higher than its counterpart in line\n8 using a single-step classiﬁer and 13% (absolute) higher\nthan LoRRA in line 1. In addition, we ablate our model us-\ning Rosetta-ml and randomly initialized question encoding\nparameters in line 9 and 10. Here, we see that our model in\nline 9 still outperforms LoRRA (line 1) by as much as 9.5%\n(absolute) when using the same OCR system as LoRRA and\neven fewer pretrained components. We also analyze the per-\nformance of our model with respect to the maximum de-\ncoding steps, shown in Figure 3, where decoding for multi-\nple steps greatly improves the performance compared with\na single step. Figure 4 shows qualitative examples (more\nexamples in appendix) of our M4C model on the TextVQA\ndataset in comparison to LoRRA [44], where our model is\ncapable of selecting multiple OCR tokens and combining\nthem with its ﬁxed vocabulary in predicted answers.\nQualitative insights. When inspecting the errors, we ﬁnd\nthat a major source of errors is OCR failure ( e.g. in the last\nexample in Figure 4, we ﬁnd that the digits on the watch are\nnot detected). This suggests that the accuracy of our model\ncould be improved with better OCR systems, as supported\nby the comparison between line 10 and 11 in Table 1. An-\nother possible future direction is to dynamically recognize\ntext in the image based on the question (e.g. if the question\nasks about the price of a product brand, one may want to\ndirectly localize the brand name in the image). Some other\nerrors of our model include resolving relations between ob-\njects and text or understanding large chunks of text in im-\nages (such as book pages). However, our model is able to\ncorrect a large number of mistakes in previous work where\ncopying multiple text tokens is required to form an answer.\n6\nWhat does the light sign read on the\nfarthest right window?\nWho is usa today’s bestselling au-\nthor?\nWhat is the name of the band? what is the time?\nLoRRA: exit LoRRA: roger zelazny LoRRA: 7 LoRRA: 1:45\nM4C (ours): bud light M4C (ours): cathy williams M4C (ours): soul doubt M4C (ours): 3:44\nhuman: bud light; all 2 liters human: cathy williams human: soul doubt; h. michael\nkarshis; unanswerable\nhuman: 5:40; 5:41; 5:42; 8:00\nFigure 4. Qualitative examples from our M4C model on the TextVQA validation set (orange words are from OCR tokens and blue words\nare from ﬁxed answer vocabulary). Compared to the previous work LoRRA [44] which selects one answer from training set or copies only\na single OCR token, our model can copy multiple OCR tokens and combine them with its ﬁxed vocabulary through iterative decoding.\nTextVQA Challenge 2019. We also compare to the win-\nning entries in the TextVQA Challenge 2019.4 We compare\nour method to DCD [32] (the challenge winner, based on\nensemble) and MSFT VTI [46] (the top entry after the chal-\nlenge), both relying on one-step prediction. We show that\nour single model (line 11) signiﬁcantly outperforms these\nchallenge winning entries on the TextVQA test set by a\nlarge margin. We also experiment with using the ST-VQA\ndataset [8] as additional training data (a practice used by\nsome of the previous challenge participants), which gives\nanother 1% improvement and 40.46% ﬁnal test accuracy –\na new state-of-the-art on the TextVQA dataset.\n4.2. Evaluation on the ST-VQA dataset\nThe ST-VQA dataset [8] contains natural images from\nmultiple sources including ICDAR 2013 [24], ICDAR 2015\n[23], ImageNet [12], VizWiz [18], IIIT STR [36], Visual\nGenome [26], and COCO-Text [49].5 The format of the ST-\nVQA dataset is similar to the TextVQA dataset in Sec. 4.1.\nHowever, each question is accompanied by only one or two\nground-truth answers provided by the question writer. The\ndataset involves three tasks, and its Task 3 - Open Dictio-\nnary (containing 18,921 training-validation images and test\n2,971 images) corresponds to our general TextVQA setting\nwhere no answer candidates are provided at test time.\nThe ST-VQA dataset adopts Average Normalized Lev-\nenshtein Similarity (ANLS)6 as its ofﬁcial evaluation met-\nric, deﬁned as scores 1 −dL(apred, agt)/ max(|apred|, |agt|)\n(where apred and agt are prediction and ground-truth answers\nand dL is edit distance) averaged over all questions. Also,\nall scores below the threshold 0.5 are truncated to 0 before\naveraging. To facilitate comparison, we report both accu-\n4https://textvqa.org/challenge\n5We notice that many images from COCO-Text [49] in the down-\nloaded ST-VQA data (around 1/3 of all images) are resized to256×256 for\nunknown reasons, which degrades the image quality and distorts their as-\npect ratios. In our experiments, we replace these images with their original\nversions from COCO-Text as inputs to object detection and OCR systems.\n6https://rrc.cvc.uab.es/?ch=11&com=tasks\n# Method Output Accu. ANLS ANLS\nmodule on val on val on test\n1 SAN+STR [8] – – – 0.135\n2 VTA [7] – – – 0.282\n3 M4C w/o dec. classiﬁer 33.52 0.397 –\n4 M4C (ours) decoder 38.05 0.472 0.462\nTable 2. On the ST-VQA dataset, our restricted model without de-\ncoder (M4C w/o dec.) already outperforms previous work by a\nlarge margin. Our ﬁnal model achieves +0.18 (absolute) ANLS\nboost over the challenge winner, VTA [7]. See Sec. 4.2 for details.\nracy and ANLS in our experiments.\nAs the ST-VQA dataset does not have an ofﬁcial split\nfor training and validation, we randomly select 17,028 im-\nages as our training set and use the remaining 1,893 images\nas our validation set. We train our model on the ST-VQA\ndataset following exactly the same setting (line 11 in Ta-\nble 1) as in our TextVQA experiments in Sec. 4.1, where\nwe extract image text tokens using Rosetta-en, use FastText\n+ bbox + FRCN + PHOC as our OCR representation, and\ninitialize question encoding parameters from a pretrained\nBERT-BASE model. The results are shown in Table 2.\nAblations of our model. We train two versions of our\nmodel, one restricted version (M4C w/o dec. in Table 2)\nwith a ﬁxed one-step classiﬁer as output module (similar to\nline 8 in Table 1) and one full version (M4C) with iterative\nanswer decoding. Comparing the results of these two mod-\nels, it can be seen that there is a large improvement from\nour iterative answer prediction mechanism.\nComparison to previous work. We compare with two\nprevious methods on this dataset: 1) SAN+STR [8], which\ncombines SAN for VQA [51] and Scene Text Retrieval [16]\nfor answer vocabulary retrieval, and 2) VTA [7], the ICDAR\n2019 ST-VQA Challenge6 winner, based on BERT [13] for\nquestion encoding and BUTD [3] for VQA. From Table 2,\nit can be seen that our restricted model (M4C w/o dec.) al-\nready achieves higher ANLS than these two models, and\nour full model achieves as much as +0.18 (absolute) ANLS\n7\nWhat is the name of the street on which\nthe Stop sign appears?\nWhat does the white sign\nsay?\nHow many cents per pound are the ba-\nnanas?\nWhat kind of stop sign is in the im-\nage?\nprediction: 45th parallel dr prediction: tokyo station prediction: 99 prediction: stop all way\nGT: 45th parallel dr GT: tokyo station GT: 99 GT: all way\nFigure 5. Qualitative examples from our M4C model on the ST-VQA validation set (orange words from OCR tokens and blue words from\nﬁxed answer vocabulary). Our model can select multiple OCR tokens and combine them with its ﬁxed vocabulary to predict an answer.\n# Method Output Accu. Accu.\nmodule on val on test\n1 BLOCK [37] – – 42.0\n2 CNN [37] – – 14.3\n3 BLOCK+CNN [37] – – 41.5\n4 BLOCK+CNN+W2V [37] – – 48.3\n5 M4C w/o dec. classiﬁer 46.3 –\n6 M4C (ours) decoder 63.5 63.9\nTable 3. On the OCR-VQA dataset, we experiment with using ei-\nther an iterative decoder (our full model) or a single-step classiﬁer\n(M4C w/o dec.) as the output module, where our iterative decoder\ngreatly improves the accuracy and largely outperforms the base-\nline methods. See Sec. 4.3 for details.\nWho is the author of this book? Is this a pharmaceutical book?\nprediction: the new york times prediction: no\nGT: the new york times GT: no\nFigure 6. Qualitative examples from our M4C model on the OCR-\nVQA validation set ( orange words from OCR tokens and blue\nwords from ﬁxed answer vocabulary).\nboost over the best previous work.\nWe also ablate the maximum copying number in our\nmodel in Figure 3, showing that it is beneﬁcial to decode for\nmultiple (as opposed to one) steps. Figure 5 shows qualita-\ntive examples of our model on the ST-VQA dataset.\n4.3. Evaluation on the OCR-VQA dataset\nThe OCR-VQA dataset [37] contains 207,572 images of\nbook covers, with template-based questions asking about\nthe title, author, edition, genre, year or other information\nabout the book. Each question is has a single ground-truth\nanswer, and the dataset assumes that the answers to these\nquestions can be inferred from the book cover images.\nWe train our model using the same hyper-parameters\nas in Sec. 4.1 and 4.2, but use 2×the total iterations and\nadapted learning rate schedule since the OCR-VQA dataset\ncontains more images. The results are shown in Table 3.\nCompared to using a one-step classiﬁer (M4C w/o dec.),\nour full model with iterative decoding achieves signiﬁcantly\nbetter accuracy, which coincides with Figure 3 that having\nmultiple decoding steps is greatly beneﬁcial on this dataset.\nThis is likely because the OCR-VQA dataset often contains\nmulti-word answers such as book titles and author names.\nWe compare to four baseline approaches from [37],\nwhich are VQA systems based on 1) visual features from\na convolutional network (CNN), 2) grouping OCR tokens\ninto text blocks (BLOCK) with manually deﬁned rules, 3)\nan averaged word2vec (W2V) feature over all the OCR to-\nkens in the image, and 4) their combinations. Note that\nwhile the BLOCK baseline can also select multiple OCR\ntokens, it relies on manually deﬁned rules to merge tokens\ninto groups and can only select one group as answer, while\nour method learns from data how to copy OCR tokens to\ncompose answers. Compare to these baselines, our M4C\nhas over 15% (absolute) higher test accuracy. Figure 6\nshows qualitative examples of our model on this dataset.\n5. Conclusion\nIn this paper, we present Multimodal Multi-Copy Mesh\n(M4C) for visual question answering based on understand-\ning and reasoning about text in images. M4C adopts rich\nrepresentations for text in the images, jointly models all\nmodalities through a pointer-augmented multimodal trans-\nformer architecture over a joint embedding space, and pre-\ndicts the answer through iterative decoding, outperform-\ning previous work by a large margin on three challeng-\ning datasets for the TextVQA task. Our results suggest\nthat it is efﬁcient to handle multiple modalities through\ndomain-speciﬁc embedding followed by homogeneous self-\nattention and to generate complex answers as multi-step de-\ncoding instead of one-step classiﬁcation.\n8\nReferences\n[1] Chris Alberti, Jeffrey Ling, Michael Collins, and David Re-\nitter. Fusion of detected objects in text for visual question\nanswering. arXiv preprint arXiv:1908.05054, 2019. 2\n[2] Jon Almaz ´an, Albert Gordo, Alicia Forn ´es, and Ernest Val-\nveny. Word spotting and recognition with embedded at-\ntributes. IEEE transactions on pattern analysis and machine\nintelligence, 36(12):2552–2566, 2014. 3, 5\n[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n6077–6086, 2018. 1, 3, 7\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2425–\n2433, 2015. 1\n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[6] Hedi Ben-Younes, R ´emi Cadene, Matthieu Cord, and Nico-\nlas Thome. Mutan: Multimodal tucker fusion for visual\nquestion answering. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision , pages 2612–2620,\n2017. 1\n[7] Ali Furkan Biten, Ruben Tito, Andres Maﬂa, Lluis Gomez,\nMarc ¸al Rusi˜nol, Minesh Mathew, CV Jawahar, Ernest Val-\nveny, and Dimosthenis Karatzas. Icdar 2019 competition\non scene text visual question answering. arXiv preprint\narXiv:1907.00490, 2016. 1, 2, 7\n[8] Ali Furkan Biten, Ruben Tito, Andres Maﬂa, Lluis Gomez,\nMarc ¸al Rusi˜nol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 2019. 1, 2, 5, 7\n[9] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. Enriching word vectors with subword infor-\nmation. Transactions of the Association for Computational\nLinguistics, 5:135–146, 2017. 3, 5\n[10] Fedor Borisyuk, Albert Gordo, and Viswanath Sivakumar.\nRosetta: Large scale system for text detection and recogni-\ntion in images. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data\nMining, pages 71–79. ACM, 2018. 5\n[11] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Learning universal image-text representations. arXiv\npreprint arXiv:1909.11740, 2019. 2\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 7\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, 2019. 2, 3, 5, 7\n[14] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,\nTrevor Darrell, and Marcus Rohrbach. Multimodal com-\npact bilinear pooling for visual question answering and vi-\nsual grounding. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing , pages\n457–468, 2016. 1, 2\n[15] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\nSteven CH Hoi, Xiaogang Wang, and Hongsheng Li. Dy-\nnamic fusion with intra-and inter-modality attention ﬂow for\nvisual question answering. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n6639–6648, 2019. 2\n[16] Llu ´ıs G´omez, Andr´es Maﬂa, Marc ¸al Rusinol, and Dimosthe-\nnis Karatzas. Single shot scene text retrieval. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 700–715, 2018. 7\n[17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6904–6913, 2017. 1,\n5\n[18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 3608–\n3617, 2018. 7\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 5\n[20] Drew A Hudson and Christopher D Manning. Gqa: a\nnew dataset for compositional question answering over real-\nworld images. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2019. 1\n[21] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,\nLi Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A\ndiagnostic dataset for compositional language and elemen-\ntary visual reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n2901–2910, 2017. 1\n[22] Kushal Kaﬂe and Christopher Kanan. An analysis of visual\nquestion answering algorithms. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1965–\n1973, 2017. 1\n[23] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos\nNicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-\nmura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-\ndrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust\nreading. In 2015 13th International Conference on Docu-\nment Analysis and Recognition (ICDAR), pages 1156–1160.\nIEEE, 2015. 7\n9\n[24] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,\nMasakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles\nMestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-\nmazan, and Lluis Pere De Las Heras. Icdar 2013 robust\nreading competition. In 2013 12th International Conference\non Document Analysis and Recognition , pages 1484–1493.\nIEEE, 2013. 7\n[25] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilin-\near attention networks. In Advances in Neural Information\nProcessing Systems, pages 1564–1574, 2018. 1, 2\n[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vi-\nsion, 123(1):32–73, 2017. 5, 7\n[27] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Tom Duerig, et al. The open im-\nages dataset v4: Uniﬁed image classiﬁcation, object detec-\ntion, and visual relationship detection at scale.arXiv preprint\narXiv:1811.00982, 2018. 5\n[28] Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying\nZhang, Saizheng Zhang, Aaron C Courville, and Yoshua\nBengio. Professor forcing: A new algorithm for training re-\ncurrent networks. In Advances In Neural Information Pro-\ncessing Systems, pages 4601–4609, 2016. 5\n[29] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming\nZhou. Unicoder-vl: A universal encoder for vision and\nlanguage by cross-modal pre-training. arXiv preprint\narXiv:1908.06066, 2019. 2\n[30] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-\naware graph attention network for visual question answering.\narXiv preprint arXiv:1903.12314, 2019. 2\n[31] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and perfor-\nmant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019. 2\n[32] Yuetan Lin, Hongrui Zhao, Yanan Li, and Donghui Wang.\nDCD ZJU, TextVQA Challenge 2019 winner. https://\nvisualqa.org/workshop.html. 6, 7\n[33] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems, 2019. 1, 2\n[34] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nHierarchical question-image co-attention for visual question\nanswering. In Advances In Neural Information Processing\nSystems, pages 289–297, 2016. 2\n[35] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural baby talk. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 7219–\n7228, 2018. 2\n[36] Anand Mishra, Karteek Alahari, and CV Jawahar. Image re-\ntrieval using textual cues. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 3040–3047,\n2013. 7\n[37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In Proceedings of the Interna-\ntional Conference on Document Analysis and Recognition ,\n2019. 1, 2, 5, 8\n[38] Ankur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob\nUszkoreit. A decomposable attention model for natural lan-\nguage inference. arXiv preprint arXiv:1606.01933, 2016. 2\n[39] Jeffrey Pennington, Richard Socher, and Christopher Man-\nning. Glove: Global vectors for word representation. In\nProceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP) , pages 1532–1543,\n2014. 5\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019. 4\n[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91–99, 2015. 3, 5\n[42] Abigail See, Peter J Liu, and Christopher D Manning. Get to\nthe point: Summarization with pointer-generator networks.\narXiv preprint arXiv:1704.04368, 2017. 2\n[43] Amanpreet Singh, Vivek Natarajan, Yu Jiang, Xinlei Chen,\nMeet Shah, Marcus Rohrbach, Dhruv Batra, and Devi\nParikh. Pythia-a platform for vision & language research.\nIn SysML Workshop, NeurIPS, volume 2018, 2018. 1, 2, 3, 5\n[44] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8317–8326, 2019. 1, 2, 3, 5, 6, 7, 12\n[45] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. arXiv preprint arXiv:1908.08530,\n2019. 2\n[46] Anonymous submission. MSFT VTI, TextVQA Chal-\nlenge 2019 top entry (post-challenge). https:\n//evalai.cloudcv.org/web/challenges/\nchallenge-page/244/. 6, 7\n[47] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 2\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2,\n3, 4\n[49] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,\nand Serge Belongie. Coco-text: Dataset and benchmark\nfor text detection and recognition in natural images. arXiv\npreprint arXiv:1601.07140, 2016. 7\n[50] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer\nnetworks. In Advances in Neural Information Processing\nSystems, pages 2692–2700, 2015. 2\n10\n[51] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and\nAlex Smola. Stacked attention networks for image question\nanswering. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 21–29, 2016. 2,\n7\n[52] Semih Yavuz, Abhinav Rastogi, Guan-Lin Chao, and Dilek\nHakkani-Tur. Deepcopy: Grounded response genera-\ntion with hierarchical pointer networks. arXiv preprint\narXiv:1908.10731, 2019. 2\n[53] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J Corso, and Jianfeng Gao. Uniﬁed vision-language\npre-training for image captioning and vqa. arXiv preprint\narXiv:1909.11059, 2019. 2\n11\nIterative Answer Prediction with Pointer-Augmented\nMultimodal Transformers for TextVQA\n(Supplementary Material)\nA. Hyper-parameters in M4C\nWe summarize the hyper-parameters in our M4C model\nin Table A.1. Most hyper-parameters are the same across\nall the three datasets (TextVQA, ST-VQA, and OCR-VQA),\nexcept that we use 2×the total iterations and adapted learn-\ning rate schedule on the OCR-VQA dataset since it contains\nmore images.\nHyper-parameter Value\nmax question word num K 20\ndetected object num M 100\nmax OCR num N 50\nmax decoding steps T 12\nembedding dim d 768\nmultimodal transformer layers L 4\nmultimodal transformer attention heads 12\nmultimodal transformer FFN dim 3072\nmultimodal transformer dropout 0.1\noptimizer Adam\nbatch size 128\nbase learning rate 1e-4\nwarm-up learning rate factor 0.2\nwarm-up iterations 2000\nmax gradient L2-norm for clipping 0.25\nlearning rate decay 0.1\nlearning rate steps (TextVQA, ST-VQA) 14000, 19000\nlearning rate steps (OCR-VQA) 28000, 38000\nmax iterations (TextVQA, ST-VQA) 24000\nmax iterations (OCR-VQA) 48000\nTable A.1. Hyper-parameters of our M4C.\nB. Additional ablation analysis\nDuring the iterative answer decoding process, at each\nstep our M4C model can decode an answer word either from\nthe model’s ﬁxed vocabulary, or from the OCR tokens ex-\ntracted from the image. We ﬁnd in our experiments that it\nis necessary to haveboth the ﬁxed vocabulary space and the\nOCR tokens.\nTable B.1 shows our ablation study where we remove\nthe ﬁxed answer vocabulary or the dynamic pointer net-\nwork for OCR copying from our M4C. Both these two ab-\nlated versions have a large accuracy drop compared to our\nfull model. However, we note that even without ﬁxed an-\nswer vocabulary, our restricted model (M4C w/o ﬁxed vo-\ncabulary in Table B.1) still outperforms the previous work\nLoRRA [44], suggesting that it is particularly important to\nlearn to copy multiple OCR tokens to form an answer (a key\nfeature in our model but not in LoRRA).\n# Method TextVQA Val Accuracy\n1 LoRRA [44] 26.56\n2 M4C w/o ﬁxed vocabulary 31.76\n3 M4C w/o OCR copying 14.94\n4 M4C (ours) 39.40\nTable B.1. We ablate our M4C model by removing its ﬁxed answer\nvocabulary (M4C w/o ﬁxed vocabulary) or its dynamic pointer net-\nwork for OCR copying (M4C w/o OCR copying) on the TextVQA\ndataset. We see that our full model has signiﬁcantly higher accu-\nracy than these ablations, showing that it is important to haveboth\na ﬁxed and a dynamic vocabulary (i.e. OCR tokens).\nC. Additional qualitative examples\nAs mentioned in Sec. 4.1 in the main paper, we ﬁnd that\nOCR failure is a major source of error for our M4C model’s\npredictions. Figure C.1 shows cases on the TextVQA\ndataset where the OCR system fails to precisely localize the\ncorresponding text tokens in the image, suggesting that our\nmodel’s accuracy can be improved with better OCR sys-\ntems.\nFigure C.2, C.3, and C.4 shows additional qualitative\nexamples from our M4C model on the TextVQA dataset,\nST-VQA, and OCR-VQA datasets, respectively. While our\nmodel occasionally fails when reading a large piece of text\nor resolving the relation between text and objects as in Fig-\nure C.2 (f) and (h), in most cases it learns to identify and\ncopy text tokens from the image and combine them with its\nﬁxed vocabulary to predict an answer.\n12\n(a) what candy bar is down there on the bottom? (b) what is the year on the calender?\nprediction: unanswerable prediction: 2005\nhuman: hershey’s; hersheys human: 2010; unanswerable\n(c) what is the largest measurement we can see on this ruler? (d) how much is the coin worth?\nprediction: 40 prediction: one dollar\nhuman: 50 human: 20; 25; 25 paise\n(e) what is the name of the bar? (f) what time is it?\nprediction: 15 prediction: 76\nhuman: moo bar; moon; moon bar human: 13:50; 13:57; ;5713; mathematic; wiﬁ\nFigure C.1. Examples where OCR failure is the main source of errors (from our M4C model on the TextVQA validation set). The red\nboxes show the OCR results (orange words from OCR tokens and blue words from ﬁxed answer vocabulary).\n13\n(a) what is the brand of this cam-\nera?\n(b) does it say happy birthday? (c) what is the title of the album? (d) what is the 4 digit number\nwritten at the bottom of the black\nbook?\nM4C: dakota digital M4C: yes M4C: slide:ology M4C: 9350\nhuman: dakota digital ; dakota;\nclos culombu; nous les gosses\nhuman: yes human: slide:ology; sideology human: 9350; 9,350\n(e) what airline is the plane from? (f) what was mr. green’s ﬁrst\nname?\n(g) what time is displayed on the\nphone’s screen?\n(h) what number is on the bike on\nthe right?\nM4C: lufthansa M4C: charles M4C: 9:09 M4C: 30\nhuman: lufthansa human: basil human: 9:09; no human: 317\nFigure C.2. Additional qualitative examples from our M4C model on the TextVQA validation set. The red boxes show the OCR results\n(best viewed in 400%; orange words from OCR tokens and blue words from ﬁxed answer vocabulary).\n(a) What is this building used for\naccording to the sign above it?\n(b) What can you get 6 of for $5? (c) where can I buy shoes here? (d) What is the license plate num-\nber on the red car?\nM4C: post ofﬁce M4C: donuts M4C: public market M4C: gsv 820\nGT: post ofﬁce GT: donuts GT: footaction GT: gsv 820\n(e) What does the large pink text\nsay?\n(f) What brand of typewriter is be-\ning used?\n(g) What 4-digit number is on the\nyellow stick in front of the green\ncar?\n(h) What brand is the bike in\nfront?\nM4C: me M4C: olympia M4C: 4764 M4C: ducati\nGT: pardon me prime minister GT: olympia GT: 4764 GT: ducati\nFigure C.3. Additional qualitative examples from our M4C model on the ST-VQA validation set. The red boxes show the OCR results\n(best viewed in 400%; orange words from OCR tokens and blue words from ﬁxed answer vocabulary).\n14\n(a) Who is the author of this book? (b) Which year’s calendar is this? (c) What is the title of this book? (d) What is the genre of this book?\nM4C: sueellen ross M4C: 2016 M4C: sailing to the mark 2013\ncalendar\nM4C: arts & photography\nGT: sueellen ross GT: 2016 GT: sailing to the mark 2013 cal-\nendar\nGT: calendars\nFigure C.4. Additional qualitative examples from our M4C model on the OCR-VQA validation set. The red boxes show the OCR results\n(best viewed in 400%; orange words from OCR tokens and blue words from ﬁxed answer vocabulary).\n15"
}