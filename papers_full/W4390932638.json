{
  "title": "Bitemporal Attention Transformer for Building Change Detection and Building Damage Assessment",
  "url": "https://openalex.org/W4390932638",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2032273238",
      "name": "Wen Lu",
      "affiliations": [
        "Auckland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107741239",
      "name": "Lu Wei",
      "affiliations": [
        "Wuchang University of Technology",
        "Wuchang Shouyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2101202397",
      "name": "Minh Nguyen",
      "affiliations": [
        "Auckland University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908320224",
    "https://openalex.org/W2090590748",
    "https://openalex.org/W1964669384",
    "https://openalex.org/W195968276",
    "https://openalex.org/W6794968592",
    "https://openalex.org/W4297883960",
    "https://openalex.org/W6771401702",
    "https://openalex.org/W6775948734",
    "https://openalex.org/W3161846845",
    "https://openalex.org/W4376648503",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W2988020997",
    "https://openalex.org/W2951991161",
    "https://openalex.org/W3205440207",
    "https://openalex.org/W3150964127",
    "https://openalex.org/W3195032332",
    "https://openalex.org/W4296999582",
    "https://openalex.org/W3175213166",
    "https://openalex.org/W4382243959",
    "https://openalex.org/W2896365540",
    "https://openalex.org/W6853405587",
    "https://openalex.org/W3120467244",
    "https://openalex.org/W4318953874",
    "https://openalex.org/W4362699455",
    "https://openalex.org/W3036453075",
    "https://openalex.org/W3027225766",
    "https://openalex.org/W3157062364",
    "https://openalex.org/W3099503507",
    "https://openalex.org/W3004423752",
    "https://openalex.org/W3130754787",
    "https://openalex.org/W4318766188",
    "https://openalex.org/W4386299129",
    "https://openalex.org/W3216244838",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3193414609",
    "https://openalex.org/W4385153992",
    "https://openalex.org/W4378804791",
    "https://openalex.org/W4377079808",
    "https://openalex.org/W3164715703",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3184566187",
    "https://openalex.org/W4390874337",
    "https://openalex.org/W3139912591",
    "https://openalex.org/W4312259879",
    "https://openalex.org/W4312549298",
    "https://openalex.org/W4287890462",
    "https://openalex.org/W4226361741",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2896092083",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4295532878",
    "https://openalex.org/W6793164127",
    "https://openalex.org/W2047839530",
    "https://openalex.org/W2037320310",
    "https://openalex.org/W3206533587",
    "https://openalex.org/W3133438312",
    "https://openalex.org/W4292348104",
    "https://openalex.org/W4285298122",
    "https://openalex.org/W4381789302",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3015433531",
    "https://openalex.org/W4379087169",
    "https://openalex.org/W4288020654",
    "https://openalex.org/W4287642547"
  ],
  "abstract": "Building change detection (BCD) holds significant value in the context of monitoring land use, whereas building damage assessment (BDA) plays a crucial role in expediting humanitarian rescue efforts post-disasters. To address these needs, we propose the bitemporal attention module (BAM) as an innovative cross-attention mechanism aimed at effectively capturing spatio-temporal semantic relations between a pair of bitemporal remote sensing images. Within BAM, a shifted windowing scheme has been implemented to confine the scope of the cross-attention mechanism to a specific range, not only excluding remote and irrelevant information but also contributing to computational efficiency. Moreover, existing methods for BDA often overlook the inherent order of ordinal labels, treating the BDA task simplistically as a multiclass semantic segmentation problem. Recognizing the vital significance of ordinal relationships, we approach the BDA task as an ordinal regression problem. To address this, we introduce a rank-consistent ordinal regression loss function to train our proposed change detection network, bitemporal attention transformer. Our method achieves state-of-the-art accuracy on two BCD datasets (LEVIR-CD+ and S2Looking), as well as the largest BDA dataset (xBD).",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024 4917\nBitemporal Attention Transformer for Building\nChange Detection and Building Damage Assessment\nWen Lu ,L uW e i, and Minh Nguyen\nAbstract—Building change detection (BCD) holds signiﬁcant\nvalue in the context of monitoring land use, whereas building\ndamage assessment (BDA) plays a crucial role in expediting hu-\nmanitarian rescue efforts post-disasters. To address these needs,\nwe propose the bitemporal attention module (BAM) as an inno-\nvative cross-attention mechanism aimed at effectively capturing\nspatio-temporal semantic relations between a pair of bitemporal\nremote sensing images. Within BAM, a shifted windowing scheme\nhas been implemented to conﬁne the scope of the cross-attention\nmechanism to a speciﬁc range, not only excluding remote and irrele-\nvant information but also contributing to computational efﬁciency.\nMoreover, existing methods for BDA often overlook the inherent\norder of ordinal labels, treating the BDA task simplistically as a\nmulticlass semantic segmentation problem. Recognizing the vital\nsigniﬁcance of ordinal relationships, we approach the BDA task\nas an ordinal regression problem. To address this, we introduce\na rank-consistent ordinal regression loss function to train our\nproposed change detection network, bitemporal attention trans-\nformer. Our method achieves state-of-the-art accuracy on two BCD\ndatasets (LEVIR-CD+ and S2Looking), as well as the largest BDA\ndataset (xBD).\nIndex Terms —Building change detection (BCD), building\ndamage assessment (BDA), ordinal regression, transformer.\nI. INTRODUCTION\nB\nOTH building change detection (BCD) and building dam-\nage assessment (BDA) are subtasks of change detection.\nBCD aims at identifying structural alterations of buildings over\ntime, it involves allocating binary labels (changed or unchanged)\non a pixel level through the analysis of aligned images ac-\nquired at different moments. BCD ﬁnds applications in urban\nplanning [1], land-cover monitoring [2], [3], and other ﬁelds\nwhere tracking changes in built structures is crucial for in-\nformed decision-making. BDA can be viewed as a multiclass\nchange detection (MCD) task speciﬁcally concentrating on\nManuscript received 26 November 2023; revised 1 January 2024; accepted\n11 January 2024. Date of publication 16 January 2024; date of current version\n22 February 2024.(Corresponding author: Minh Nguyen.)\nWen Lu is with the School of Engineering, Computer & Mathematical\nSciences, Auckland University of Technology, Auckland 1010, New Zealand,\nand also with FHE Electrical Ltd., T/A Kinetic Electrical East Tamaki, Auckland\n2013, New Zealand (e-mail: wen.lu@autuni.ac.nz).\nMinh Nguyen is with the School of Engineering, Computer & Mathematical\nSciences, Auckland University of Technology, Auckland 1010, New Zealand\n(e-mail: minh.nguyen@aut.ac.nz).\nLu Wei is with the School of Information Science and Engineering, Wuchang\nShouyi University, Wuhan 430064, China (e-mail: weilu@wsyu.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2024.3354310\nthe land cover category “building.” In this context, it identi-\nﬁes individual buildings and assigns predeﬁned damage de-\ngree labels to them. Timely humanitarian assistance and dis-\naster response, especially within the ﬁrst 72 h, is very crucial\nfor saving lives [4]. By locating and evaluating the damage\nseverity of the buildings, BDA provides critical information\nfor emergency responders to identify damaged zones, plan\naid routing, and optimize the deployment of rescue resources\nwithin impacted regions. High spatial resolution satellite and\naerial imagery can accurately reﬂect the Earth’s surface and\nrapidly provide large area observations for BCD and BDA\ntasks. However, analysis of the imagery by experts is laborious\nand time-consuming, therefore, automatic BCD and BDA are\nimperative.\nIt is worth noting that BDA differs from semantic change\ndetection (SCD), which broadens the MCD task by offer-\ning not just the locations of changes, but also detailed land\ncover and land use (LCLU) categories before and after the\nobservation periods. The typical predeﬁned labels for BDA\ninclude no damage, minor damage, major damage, destroyed,\nand background. Consequently, the predeﬁned LCLU cate-\ngories consist of only two: “building” and “other.” Whereas,\nthe predeﬁned LCLU categories are more diverse for SCD.\nFor example, the SEmantic Change detectiON Dataset (SEC-\nOND) [5] includes LCLU categories: “non-vegetated ground\nsurface,” “tree,” “low vegetation,” “water,” “buildings,” and\n“playgrounds.” The Landsat-SCD dataset[6] includes LCLU\ncategories: “farmland,” “desert,” “building,” and “water.” In\nBDA task, the LCLU categories remain constant, as the pri-\nmary focus is on evaluating the severity of building damage.\nConversely, SCD is designed to identify alterations in LCLU\ncategories.\nIn contrast to the categorical labels in tasks such as semantic\nsegmentation, land cover classiﬁcation, and SCD, the labels\nin the BDA exhibit an ordinal relationship. The joint damage\nscale [7], developed in collaboration with experts from NASA,\nCAL FIRE, FEMA, and the California Air National Guard,\nclassiﬁes building damage into four distinct degrees.\n1) No damage is deﬁned as no sign of water, structural or\nshingle damage, or burn marks.\n2) Minor damageis deﬁned as building partially burnt, water\nsurrounding structure, volcanic ﬂow nearby, roof elements\nmissing, or visible cracks.\n3) Major damageis deﬁned as partial wall or roof collapse,\nencroaching volcanic ﬂow, or surrounded by water/mud.\n© 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\n4918 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 1. Left side shows inconsistent predictions, whereas the right side demonstrates ideal predictions where the probabilities consistently decrease.\n4) Destroyed is deﬁned as scorched, completely collapsed,\npartially/completely covered with water/mud, or other-\nwise, no longer present.\nEvidently, the differentiation betweenNo damage and De-\nstroyed is more conspicuous than that betweenMajor damage\nand Destroyed. In real-world scenarios, a model that erro-\nneously predicts aDestroyed building asMajor damagewould\nbe more accurate and valuable for locating injured individ-\nuals than one that misclassiﬁes a Destroyed building as No\ndamage.\nHowever, existing BDA methods[7], [8], [9], [10], [11], [12],\n[13], [14], [15], [16], [17], [18], [19] overlook the intrinsic\norder among ordinal labels and simplistically treat the BDA\ntask as a multiclass semantic segmentation problem. These\nmethods employ traditional classiﬁcation loss functions, such\nas crossentropy loss, dice loss, and focal loss, to train their\nchange detection networks. Nevertheless, these loss functions\nhave a drawback: When a building is labeled as destroyed,\npredicting no damage or major damage incurs the same loss,\ndisregarding the more signiﬁcant difference betweenno dam-\nage and destroyed. The BDA task cannot be approached as a\nmetric regression problem either because the distance between\nordinal ranks cannot be quantiﬁed. For example, the difference\nbetween no damageand minor damagecannot be quantitatively\ncompared with the difference betweenminor damageand major\ndamage.\nTo correctly utilize the ordering information, we approach the\nBDA task as an ordinal regression problem. In this framework,\nwe transform the K ranks into K −1 binary classiﬁcation\nproblems, where each kth task predicts whether the damage\nlevel exceeds rank rk (k =1 ,...,K −1). While all K −1\ntasks share the same intermediate layers, they possess distinct\nweight parameters in the output layer. However, traditional\nordinal regression methods do not guarantee consistent predic-\ntions, leading to potential disagreement among the predictions\nfor individual binary tasks. This inconsistency arises when,\nfor example, the kth binary task indicates that the damage\nlevel surpassesmajor damage, whereas a preceding binary task\nsuggests that the damage level falls belowminor damage,a s\nillustrated in Fig.1. In order to address these inconsistencies, we\nemploy the conditional ordinal regression for neural networks\n(CORN) [20] as the loss function to ensure rank-monotonicity\nand maintain consistent conﬁdence scores. CORN achieves rank\nconsistency through an innovative training scheme that utilizes\nconditional training sets to obtain unconditional rank proba-\nbilities by applying the chain rule for conditional probability\ndistributions.\nWith aligned preceding and subsequent images, a key question\nis how to effectively model the spatio-temporal semantic rela-\ntions between the bitemporal pair? Some convolutional neural\nnetworks (CNNs) simply concatenate or subtract bitemporal\nfeatures to extract change-related information[8], [9], [10], [21],\n[22]. While concatenation allows for the preservation of the\noriginal semantic information within monotemporal images, it\nfails to incorporate prior knowledge of changes. On the other\nhand, subtraction enables the acquisition of prior knowledge of\nchanges, but at the cost of losing the original semantic infor-\nmation. Some CNN methods utilize attention mechanisms for\nbitemporal feature fusion. However, they either apply attention\nseparately to enhance features in each monotemporal image[23],\n[24], [25], or use attention to reweight the fused bitemporal\nfeatures in the channel or spatial dimensions[26], [27], [28],\n[29], [30], [31], [32], [33], [34], instead of using attention mech-\nanisms to model the correlation within the bitemporal image\npair.\nAs shown in Fig.2, during natural disasters, such as hurri-\ncanes and volcanic eruptions, certain damaged buildings exhibit\nno discernible differences between predisaster and postdisas-\nter images; therefore, the damage levels are evaluated based\non their surrounding water or lava. Due to inherently limited\nreceptive ﬁeld, the CNN features of such a damaged build-\ning in both predisaster and postdisaster images would exhibit\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4919\nFig. 2. No discernible difference in the roofs of the damaged buildings within the purple rectangles when comparing predisaster and postdisaster images. The\nassessment of building damage levels is based on the presence of surrounding water or lava. The images are sourced from the xBD dataset[7].\na high degree of similarity. Consequently, without access to\nextensive receptive context information, CNN faces difﬁculties\nin accurately differentiating between varying degrees of dam-\nage. Following its dominance in the ﬁeld of natural language\nprocessing, the transformer architecture has shown superior\nperformance compared with CNN in monotemporal computer\nvision tasks, such as image classiﬁcation, object detection, and\nsemantic segmentation. Moreover, the transformer has achieved\nremarkable success in multimodal computer vision tasks, includ-\ning visual question answering, visual commonsense reasoning,\ncrossmodal retrieval, and image captioning[35]. Compared with\nCNN, the transformer, with its nonlocal attention mechanism, is\nbetter suited for change detection tasks. However, conventional\ntransformer models possess three limitations. First, their self-\nattention mechanism is designed for monotemporal computer\nvision tasks, rendering it incapable of capturing the tempo-\nral relationships inherent in a bitemporal image pair. Second,\ntheir global attention mechanism has quadratic computational\ncomplexity relative to the image size. Given the large scale\nof remote sensing imagery and dense prediction tasks, such\nas change detection, this computational expense becomes un-\naffordable. Third, as illustrated in Fig.2, the surroundings of\ndamaged buildings are often submerged in water or covered in\nlava, whereas the adjacent areas remain unaffected, with intact\nbuildings. Consequently, distant information becomes irrelevant\nor possibly misleading, necessitating a focus on middle-range\ncontext rather than long-range.\nIn recent years, speciﬁc cross-attention (CA) mechanisms\nhave emerged, tailored to model the temporal relationships\nwithin a bitemporal image pair[36], [37], [38], [39], [40], [41].\nCA mechanisms enable models to focus on relevant areas in\nboth images and learn the spatio-temporal relationships between\nthem. To effectively and efﬁciently model the spatio-temporal\nsemantic relations between bitemporal images, we propose a\nnovel CA mechanism called the bitemporal attention module\n(BAM), which detects discrepancies through bitemporal mutual\ninformation. Initially, both the preceding image and the sub-\nsequent image are processed by encoder to extract features.\nThe extracted features from one temporal image are used to\nattend to speciﬁc regions in the other temporal image. For each\n4920 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\npixel in the ﬁrst image, the CA mechanism identiﬁes the most\nrelevant pixels in the second image. The similarity in feature\nspace is used to compute this relevance. Once the CA mech-\nanism has associated pixels in both images, change detection\ncan be performed. Therefore, the BAM is effective in handling\nmisalignments, allowing for more accurate and robust change\ndetection. From another viewpoint, the BAM treats the change\ndetection problem as a question-and-answer (Q&A) scenario.\nSpeciﬁcally, a Query token in the subsequent image serves as\na question, asking the Key and Value tokens in the preceding\nimage to which change level it belongs. Similarly, a Query\ntoken in the preceding image can be seen as a question, asking\nthe Key and Value tokens in the subsequent image how much\nchange has happened to it. To mitigate computational complex-\nity while achieving the desired middle-range context, the BAM\nalso integrates the shifted windowing scheme proposed by Swin\nTransformer [42]. This strategy conﬁnes attention computation\nwithin nonoverlapping local windows that partition the image,\nwith a ﬁxed number of patches per window, resulting in linear\ncomputational complexity relative to image size.\nSome BCD datasets, such as the S2Looking dataset[43],\ninclude more than just the normalbuilding change labels, it\nalso provides the demolished labels, and the newly built la-\nbels. However, existing change detection models [11], [21],\n[23], [24], [25], [27], [31], [33], [40], [44], [45], [46], [47],\n[48], [49] solely support one type of label. They only use the\nbuilding change labels and disregard the valuable information\nprovided by the other two types of labels. Therefore, these\nmethods are limited to predicting onlybuilding change labels\nand lack the capability to predictdemolished or newly built\nlabels. In contrast, our BAM can support all three types of\nlabels.\nThe contributions of this work can be summarized in the\nfollowing three aspects.\n1) We propose the BAM, a novel CA mechanism designed\nto effectively and efﬁciently model the spatio-temporal\nsemantic relations between a pair of bitemporal remote\nsensing images.\n2) We construct an efﬁcient semantic segmentation backbone\nto extract features from buildings and their surroundings,\nand then integrate the BAM into a Siamese network\ncomposed of the backbones, forming a change detection\nnetwork named bitemporal attention transformer (BAT).\n3) We recognize the signiﬁcance of ordinal relationships and\napproach the BDA task as an ordinal regression problem.\nTo avoid potential disagreement among the predictions\nfor individual binary tasks, our framework employs a\nregression loss function with strong theoretical guarantees\nfor rank-monotonicity.\nII. RELATED WORK\nThis section commences with an overview of CNN-based\nBCD and BDA methods. Following that, we introduce various\nrecently published CA mechanisms and conduct a comparative\nanalysis with our proposed CA mechanism, the BAM.\nA. CNN-Based BCD and BDA Methods\nFully convolutional network (FCN)[50] introduced an end-\nto-end paradigm for pixelwise prediction, starting a new era\nof applying deep learning for change detection tasks. Change\ndetection through deep learning can be broadly classiﬁed into\nearly fusion and late fusion approaches. The early fusion ap-\nproach is rooted in semantic segmentation, wherein bitemporal\nimages are concatenated and input to a deep learning network,\nundergoing direct training using ground truth. However, dis-\ntinct from semantic segmentation, change detection entails the\nextraction of not just the semantics present in a single image,\nbut also the change-related information derived from dual-phase\nsemantics. In the early fusion approach, the semantic informa-\ntion belonging to an individual temporal image is mixed and\nconfused with the change-related information between the two\ntemporal images. To overcome the issue of semantic confusion,\nthe late fusion approach decouples feature extraction and feature\nfusion. It commences by individually extracting features from\neach temporal image, subsequently conducting predictions using\neither metric-based or classiﬁcation-based strategy. The metric-\nbased entails the construction of a parameterized embedding\nspace, characterized by a large distance between the changed\npixels and a small distance between the unchanged pixels.\nOn the other hand, the classiﬁcation-based strategy fuses the\ntwo temporal features to generate a probability map wherein\npositions with changes receive higher scores compared with un-\nchanged positions. Regarding the loss function, the former strat-\negy commonly employs the contrastive loss functions such as\ntriplet loss[51], whereas the latter strategy utilizes conventional\nclassiﬁcation loss functions, such as crossentropy loss or dice\nloss.\nIn late fusion approaches, Weber and Kané[8] fused the\nbitemporal features by concatenation, whereas Gupta and\nShah [9] fused them by subtraction. Concatenation can effec-\ntively retain building features, but lacks prior knowledge of\nchanges. Conversely, subtraction enables the acquisition of prior\nknowledge of changes, but leads to the loss of building features\nand is incapable of handling pseudochanges originating from\nseasonal variations, weather conditions, differences in illumina-\ntion, or disparities in image sources. As a strategy of allocating\nlarger weights to informative parts of a feature map, various\nattention mechanisms have replaced the aforementioned simple\nfusion methods in recent research studies. For example, DSIFN\nincorporates a channel attention module and a spatial attention\nmodule subsequent to the fusion of bitemporal features and\nupper-level change features [26]. In STANet, a self-attention\nmechanism is employed to compute attention weights among\npairs of pixels across different temporal instances and spatial\nlocations [27]. ADS-Net introduced a dual-stream attention\nmechanism subsequent to the fusion of bitemporal features\nalong with their subtraction outcomes[28]. BDANet proposed a\ncrossdirectional attention module to explore the correlations be-\ntween prediaster and postdisaster images[18]. Siam-U-Net-Attn\nintroduced a self-attention module to incorporate long-range\ninformation from the entire image[14]. Deng and Wang[17]\nused shufﬂe attention to correlate buildings before and after\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4921\nthe disaster. LGPNet incorporated two general attention mech-\nanisms, the position attention module and the channel attention\nmodule [34]. These mechanisms facilitate adaptive selection\nand enhancement of building features exhibiting high semantic\nresponses. To further enhance its focus on buildings and alleviate\nthe inﬂuence of other ground targets, the LGPNet adopts a\ncross-task transfer learning strategy. This strategic approach\nsigniﬁcantly boosts the network’s performance in isolating and\nanalyzing building features.\nHowever, these methods either apply attention separately to\nenhance features in each monotemporal image[23], [24], [25],\nor use attention to reweight the fused bitemporal features in the\nchannel or spatial dimensions[26], [27], [28], [29], [30], [31],\n[32], [33], [34], instead of using attention mechanisms to model\nthe correlation within the bitemporal image pair.\nB. CA Mechanisms\nIn recent years, speciﬁc CA mechanisms have emerged,\ntailored to model the temporal relationships within a bitem-\nporal image pair. CA mechanisms enable models to focus on\nrelevant areas in both images and learn the spatio-temporal\nrelationships between them. For example, changer includes a\nseries of alternative interaction layers in the feature extractor\nand proposes a ﬂow-based dual-alignment fusion module, which\nallows interactive alignment and feature fusion[40]. FCCDN\ndesigns a dense connection-based feature fusion module to\nfuse bitemporal features[41]. PGLF proposes a multiscale spa-\ntiotemporal interaction module to model and enhance the spatial\nand temporal correlations between paired change features and\nextract robust change representations under the constraint of\nbidirectional temporal direction changes.\nTo enable deep and long-range modeling of temporal cor-\nrelations in the semantic space, SCanNet[36] employed the\ncross-shaped window transformer mechanism[52], which par-\ntitions the input features into vertical and horizontal stripes.\nHowever, the vertical and horizontal stripes not only retain\ndistant and unrelated information but also omit crucial features in\nthe adjacent diagonal area. On the contrary, our BAM adopts the\nshifted windowing scheme that conﬁnes attention computation\nwithin nonoverlapping local windows that partition the image,\nnot only excluding remote and irrelevant information but also\ncontaining all the adjacent features.\nCTD-Former utilized a CA mechanism that is based on the dif-\nferences between similarity matrices of bitemporal images[39].\nBiSRNet employed a crosstemporal semantic reasoning (Cot-\nSR) block to model the temporal correlations [37]. Within\nthe CA mechanism of Cot-SR, the encoded features from one\ntemporal image initially attend to speciﬁc regions within itself\nto generate the attention matrice. Subsequently, the generated\nattention matrice is matrix multiplied with the encoded features\nfrom the other temporal image. Its CA mechanism can be repre-\nsented as CA(Qpre,Kpre,Vpost) and CA(Qpost,Kpost,Vpre).T h i s\nCA mechanism’s limitation resides in computing weighted sums\nof values, determined by relevance derived from attention scores\nbetween Queries and Keys within the same temporal feature\nmap. Consequently, it exclusively assesses feature similarities\nand spatial correlations within a single temporal image, lacking\ndirect comparisons of corresponding features across different\ntemporal images.\nIn contrast, within the CA mechanism of our BAM, the\nencoded features from one temporal image initially attend to\nspeciﬁc regions in the other temporal image to generate the\nattention matrice. Our CA mechanism can be represented as\nCA(Qpre,Kpost,Vpost) and CA(Qpost,Kpre,Vpre). Our CA mech-\nanism offers an advantage by directly comparing the similarity\nof corresponding features across different temporal images,\nmirroring human behavior and aligning more intuitively with\nthe concept of “cross-attention.” Detecting changes necessitates\nidentifying the same building captured from different angles and\nlocating any updated components. The registration process for\nthe bitemporal image pair suffers from inherent inaccuracies ow-\ning to varying side-looking angles and terrain undulations[43].\nThis serves as a test of a change detection model’s ability to\ntolerate minor registration inaccuracies. Another strength of our\nCA mechanism lies in the integration of a Query vector from\none temporal image into the computation of attention scores\nwith Key vectors within a local window from the other temporal\nimage. This integration enhances the mechanism’s ability to\naccommodate minor registration inaccuracies more effectively.\nIII. PROPOSED METHOD\nThis section begins by presenting the novel CA mechanism\ncalled the BAM. Next, we introduce our change detection net-\nwork, BAT. Finally, we propose an ordinal regression training\npipeline along with an object-based prediction pipeline tailored\nfor BDA.\nA. Bitemporal Attention Module\nIn contrast to bitemporal fusion methods, such as concate-\nnation, subtraction, channel attention, and self-attention, BAM\ndetects discrepancies through bitemporal mutual information.\nInitially, both the preceding image and the subsequent image are\nprocessed by encoder to extract features. The extracted features\nfrom one temporal image are used to attend to speciﬁc regions\nin the other temporal image. For each pixel in the ﬁrst image, the\nCA mechanism identiﬁes the most relevant pixels in the second\nimage. The similarity in feature space is used to compute this\nrelevance. Once the CA mechanism has associated pixels in both\nimages, change detection can be performed. Therefore, the BAM\nis effective in handling misalignments, allowing for more accu-\nrate and robust change detection. From another viewpoint, the\nBAM captures change features by treating the change detection\nproblem as a Q&A scenario. In this scenario, a Query token\nin the subsequent image asks the Key and Value tokens in the\npreceding image about its change level. Similarly, a Query token\nin the preceding image asks the Key and Value tokens in the\nsubsequent image about the amount of change that has occurred\nto it. The upper part of Fig.3 illustrates how the CA mechanism\nin the BAM designates one temporal image as the source and\nthe other as the inquirer, enabling the detection of differences\nthrough mutual information. Furthermore, to eliminate remotely\n4922 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 3. Structure of BAM. Initially, a bitemporal feature pair undergoes\npartitioning into nonoverlapping local windows/shifted windows, followed by\nCA within these windows.\nirrelevant information and reduce computational complexity, the\nBAM integrates the shifted windowing scheme proposed by\nSwin Transformer [42] to restrict the CA mechanism within\na speciﬁc range. This strategy conﬁnes attention computation\nwithin nonoverlapping local windows that partition the image,\nwith a ﬁxed number of patches per window, resulting in lin-\near computational complexity relative to image size. The CA\nmechanism within a local window is formulated as\nCA(Qpre,Kpost,Vpost)= σ(QpreKT\npost/\n√\nd+ B)Vpost (1)\nCA(Qpost,Kpre,Vpre)= σ(QpostKT\npre/\n√\nd+ B)Vpre (2)\nwhere Qpre,Kpre,Vpre ∈ RM2×d are the predisaster Query, Key,\nand Value matrices,Qpost,Kpost,Vpost ∈ RM2×d are the postdis-\naster Query, Key, and Value matrices,σ is softmax operation,\nd is the Query/Key dimension,M2 is the number of patches in\na window, andB is the relative position bias. In the CA mech-\nanism, QpreKT\npost and QpostKT\npre are considered as bitemporal\nmutual information, whereasVpost and Vpre are considered as the\nmonotemporal image features.\nAs illustrated in the lower part of Fig.3, the CA mechanism ac-\ncepts either a window or shifted window partitioned bitemporal\nfeature pair as input, producing a change feature pair (consisting\nof a prevalue change feature map and a postvalue change feature\nmap) that is also window or shifted window partitioned. After-\nward, to establish connections among the windows, the window\npartitioned change features and the shifted window partitioned\nchange features are combined through addition, resulting in\ncombined prevalue change features and combined postvalue\nchange features.\nB. Bitemporal Attention Transformer\nTo avoid the issue of semantic confusion in the early fusion\napproach, we adopt a late fusion approach that separates the\nprocesses of feature extraction and feature fusion. Our approach\nutilizes a Siamese architecture that begins by extracting features\nindividually from each monotemporal image, and subsequently\nintegrates the bitemporal features through CA. Speciﬁcally,\nwe construct an efﬁcient semantic segmentation backbone to\nextract features from buildings and their surroundings, and then\nintegrate the BAM into a Siamese network composed of the\nbackbones, forming a change detection network named BAT.\nExisting methods[8], [14], [18] utilize a heavyweight back-\nbone for the Siamese branch, which necessitates cropping an\noriginal 1024×1024 pixel image into 512×512 pixel or 256×\n256 pixel patches due to limitations in GPU memory. Nonethe-\nless, this image preprocessing operation inevitably leads to the\nsplitting of some complete buildings into partial fragments,\nresulting in the loss of middle-range context information. More-\nover, resizing the predicted results back to their original size\nintroduces additional latency[53]. In order to overcome these\nlimitations, we construct a lightweight semantic segmentation\nbackbone incorporated as a branch within the Siamese archi-\ntecture. This hybrid backbone combines the high efﬁciency of\nCNNs with the powerful and nonlocal modeling capability of\ntransformers.\nAs illustrated in the upper part of Fig.4, the semantic segmen-\ntation backbone utilizes EfﬁcientNetV2[54] as the encoder to\nextract multiscale features, and then, employs sequential Swin\nTransformer blocks[42] as the decoder to fuse these features.\nThe EfﬁcientNetV2 Stage 5 output features are upsampled 2×\nand concatenated with Stage 3 output features before sending to\nthe decoder for fusion and window and shifted window attention.\nThe decoder output is considered as the monotemperal features.\nAs illustrated in the lower part of Fig. 4, both Siamese\nbranches share a common backbone, facilitating the equitable\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4923\nFig. 4. Architecture of the BAT. BAT utilizes a Siamese architecture com-\nposed of weights sharing backbones to extract features individually from\neach monotemporal image, and subsequently integrates the bitemporal features\nthrough the BAM.\nextraction of features. Subsequently, the monotemperal features\nfrom each Siamese branch are routed to the BAM, responsible\nfor extracting spatio-temporal information through window and\nshifted window CA.\nThe outputs of the BAM consist of combined prevalue change\nfeatures and combined postvalue change features. Due to their\nstronger association with preceding image features, we train the\ncombined prevalue change features using thedemolished labels.\nConversely, the combined postvalue change features, which\nexhibit a closer relationship with subsequent image features,\nare trained using thenewly builtlabels. We also fuse the com-\nbined prevalue change and combined postvalue change features\nthrough concatenation, forming the dual-perspective change\nsensitivity features, and train them using the normalbuilding\nchange labels. The pseudocode of the “forward” method of BAT\nis shown as follows.\nAs i m p l e1×1 convolution layer serves as either the auxiliary\nor the main change detection head, with logits upsampled by a\nfactor of 8 through bilinear interpolation before being directed\nto the auxiliary or the main loss function. The total lossLt is the\nweighted sum of the main lossLm and the two auxiliary losses\nLa\nLt = w1 ×Lm + w2 ×La. (3)\nFine-tuning the hyperparametersw1 and w2 typically leads\nto improved results. However, to ensure generality, we refrained\nfrom ﬁne-tuning the hyperparameters in our experiments. In-\nstead, we used straightforward and intuitive values. Given that\nbuilding change detection is the primary task for most datasets,\nwe assigned greater emphasis to the main loss. This was achieved\nby settingw1 =1 and w2 =0 .5 in the subsequent experiments.\nIn the case of certain BCD datasets, such as the S2Looking\ndataset [43], which include not only the normalbuilding change\nlabels but also thedemolished labels and thenewly builtlabels,\nour BAT distinguishes itself from existing change detection\nmodels (e.g.,[11], [21], [23], [27], [31], [40], [45], [46], [47],\n[48], and[49]), which exclusively supportbuilding changela-\nbels. BAT is capable to harness the valuable information offered\nby these additional label types, enabling it to predictdemolished\nand newly builtlabels.\nFor other BCD or BDA datasets, such as LEVIR-CD+[43]\nBCD dataset and xBD BDA dataset[7], which include only a\nsingle type of labels, BAT’s auxiliary change detection heads\nand corresponding loss functions are omitted.\nThe BAT is suitable for both the BCD and BDA tasks. Given\nthat BCD is a binary-class semantic segmentation problem,\nconventional loss functions, such as binary crossentropy loss\nand dice loss are applicable. However, it is inappropriate to\noversimplify BDA by regarding it as a multiclass semantic\nsegmentation task and applying conventional classiﬁcation loss\nfunctions, such as crossentropy loss, dice loss, and focal loss,\ndue to the intrinsic ordinal relationships within the labels. Con-\nsequently, we address the BDA task as an ordinal regression\n4924 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nproblem and propose the subsequent ordinal regression training\npipeline along with an object-based prediction pipeline tailored\nfor BDA.\nC. Ordinal Regression Training Pipeline for BDA\nSince BDA is a combination of two subtasks: Building ex-\ntraction and damage classiﬁcation. For the task of building\nextraction, the backbone of BAT along with a simple 1×1 con-\nvolution layer served as semantic segmentation head are utilized\nand trained with predisaster images and their corresponding\nbuilding footprint labels by binary crossentropy loss. Another\ntwo semantic segmentation heads are added upon the Stages 3\nand 5 output features in the training phase, respectively, serving\nas auxiliary losses to enhance feature extraction ability. In the\ninference phase, the two auxiliary heads are discarded without\nincurring the additional computational cost.\nGiven that BDA is a combination of two subtasks: Building\nextraction and damage classiﬁcation. For the building extraction\ntask, we employ the backbone of BAT (as illustrated in the\nupper part of Fig.4), augmented by a simple 1×1 convolution\nlayer functioning as the semantic segmentation head. During\nthe training phase, two additional semantic segmentation heads\nare introduced, one connected to the Stage 3 and the other to the\nStage 5 output features. These heads function as auxiliary losses,\nenhancing feature extraction capability. In the inference phase,\nthe two auxiliary heads are discarded, incurring no additional\ncomputational cost. This network is trained using predisaster\nimages and their corresponding building footprint labels, and\nemploys binary crossentropy as loss function.\nFor the damage classiﬁcation task, we leverage the trained\nweights acquired from the building extraction task to initialize\nthe BAT backbone. BAT is trained using bitemporal image pairs\nand their corresponding building damage labels, and employs a\nregression loss function named CORN, which guarantees rank-\nmonotonicity to avoid rank inconsistencies among the binary\ntasks.\nLet D = {x[i],y[i]}N\ni=1 denote a dataset containingN sam-\nples, in whichx[i] ∈X denotes the inputs of theith sample,\ny[i] denotes its corresponding class label, andK denotes the\nnumber of classes. In ordinal regression, y[i] is referred as\nrank, where y[i] ∈Y = {r1,r2,...r K} with rank order r1 ≺\nr2 ≺ ... ≺ rK−1 ≺ rK. CORN applies a label extension to the\nrank labelsy[i], such that the resulting binary labely[i]\nk ∈{ 0,1}\nindicates whethery[i] exceeds rankrk. CORN employsK −1\nbinary tasks in the output layer of a neural network. CORN\nestimates a series of conditional probabilities using conditional\ntraining subsets, such that the output of thekth (k> 1) binary\ntask fk(x[i]) represents the conditional probability\nfk(x[i])= ˆP(y[i] >r k|y[i] >r k−1) (4)\nwhere the events are nested,{y[i] >r k}⊆{ y[i] >r k−1}.\nWhen k =1 , fk(x[i]) represents the initial unconditional\nprobability\nf1(x[i])= ˆP(y[i] >r 1). (5)\nThe equivalent unconditional probabilities are computed by\napplying the chain rule\nˆP(y[i] >r k)=\nk∏\nj=1\nfj(x[i]). (6)\nSince ∀j,0 ≤ fj(x[i]) ≤ 1,w eh a v e\nˆP(y[i] >r 1) ≥ ˆP(y[i] >r 2) ≥ ... ≥ ˆP(y[i] >r K−1) (7)\nwhich guarantees rank consistency among theK −1 binary\ntasks.\nThe neural network aims to estimate the initial uncondi-\ntional probability f1(x[i]) and the conditional probabilities\nf2(x[i]),...,f K−1(x[i]). Estimatingf1(x[i])= ˆP(y[i] >r 1) is\na classic binary classiﬁcation task with the binary labely[i]\n1 .\nTo estimate the conditional probability fk(x[i])= ˆP(y[i] >\nrk|y[i] >r k−1), only the subset of the dataset wherey[i] >r k−1\nis needed.\nLet fj(x[i]) denote the predicted value of thejth node in the\noutput layer of the network, and let|Sj| denote the size of its\nconditional training set. The loss functionL(X,y)is\n− 1∑ K−1\nj=1 |Sj|\nK−1∑\nj=1\n|Sj |∑\ni=1\n[\nlog\n(\nfj(x[i])\n)\n·1\n{\ny[i] >r j\n}\n+l o g\n(\n1 −fj(x[i])\n)\n·1\n{\ny[i] ≤ rj\n}]\n(8)\nwhere 1 denotes indicator function.\nTo improve the numerical stability of the loss gradients during\ntraining, the following alternative formulationL(Z,y) of the\nloss is implemented:\n− 1∑ K−1\nj=1 |Sj|\nK−1∑\nj=1\n|Sj |∑\ni=1\n[\nlog\n(\nσ(z[i])\n)\n·1\n{\ny[i] >r j\n}\n+\n(\nlog\n(\nσ(z[i])\n)\n−z[i]\n)\n·1\n{\ny[i] ≤ rj\n}]\n(9)\nwhere Z are the inputs of the last layer,σ is softmax operation,\nand log(σ(z[i])) = log(fj(x[i])).\nThe rank indexqof theith sample is obtained by thresholding\nthe predicted probabilities corresponding to theK −1 binary\ntasks and summing the binary labels as follows:\nq[i] =1+\nK−1∑\nj=1\n1\n(\nˆP(y[i] >r j) > 0.5\n)\n(10)\nwhere the predicted rank isrq[i] .\nBesides taking advantage of ordinal information, another ben-\neﬁt of employing CORN as the loss function lies in the reduced\nnumber of output logits channels, which isK −1, compared\nwith the conventional loss functions where it isK. Consequently,\nCORN consumes less memory than alternative loss functions.\nIn order to make BAT focus on building change detection,\nonly the pixels within the building footprint contribute to the loss\ncalculation, whereas those in the background are disregarded.\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4925\nFig. 5. Object-based prediction pipeline for BDA.\nD. Object-Based Prediction Pipeline for BDA\nThe object-based prediction pipeline comprises the sequential\nsteps depicted in Fig.5 and detailed in Algorithm1.\nStep 1: Building Extraction. The predisaster image is input\nto a BAT backbone network with pretrained frozen\nweights for the ﬁrst subtask, yielding a building ex-\ntraction map.\nStep 2: Instance Segmentation. The connected component\nlabeling algorithm[55], [56] is applied on the build-\ning extraction map from Step 1 to assign a distinct\nlabel to each extracted building. This process trans-\nforms the semantic segmentation map from Step 1\ninto an instance segmentation map.\nStep 3: Pixel-based Building Damage Classiﬁcation. The\nbitemporal image pair is fed into a BAT with pre-\ntrained frozen weights for the second subtask, yield-\ning a pixel-based building damage prediction map.\nStep 4: Background Removal. To eliminate the background,\nthe building extraction map from Step 1 serves as a\nmask, which is then applied to the building damage\nprediction map from Step 3 through multiplication,\nresulting in a masked building damage prediction\nmap.\nStep 5: Object-based Building Damage Classiﬁcation. In or-\nder to ensure label consistency within individual\nbuildings, majority voting is carried out within in-\ndividual building instances. This process transforms\nthe pixel-based predictions from Step 4 into object-\nbased predictions.\nAlgorithm 1: Object-Based Prediction Pipeline for BDA.\nInput: Pre-disaster ImageX1, Post-disaster ImageX2,\nConnected Component Labeling AlgorithmCCL.\nOutput: Object-based PredictionˆY.\n#Step1: Building Extraction\nYb ← BAT′sb a c k b o n e(X1)\n#Step2: Instance Segmentation\nYi ← CCL(Yb)\n#Step3:\nPixel −based Building Damage Classification\nYd ← BAT(X1,X2)\n#Step4: BackgroundRemoval\nYm ← Yb ⊗Yd\n#Step5:\nObject −based Building Damage Classification\nˆY ← Vote(Ym,Yi)\nreturn ˆY\n⊗ denotes elementwise multiplication.\nIV . BCD EXPERIMENTAL RESULTS\nTo assess BAT’s efﬁcacy in BCD tasks, we conducted experi-\nments on two BCD datasets, comparing it with various methods.\nA. Experimental Setup\n1) BAT Parameter Setting: We employed EfﬁcientNetV2S\nas the backbone encoder, and conﬁgured the CA mechanism in\nBAM with a window size of 16.\n4926 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\n2) Training Details: Network training was conducted on an\nNVIDIA RTX 3090 GPU within a PyTorch environment. We\nemployed AdamW as the optimizer, with a batch size of 7 and\na base learning rate of 0.0001, utilizing cosine decay. Crossen-\ntropy was used as the loss function. The networks underwent\n200 epochs of training, incorporating a warmup strategy during\nthe initial 50 epochs.\n3) Data Augmentation: During the training process, we ap-\nplied random ﬂipping, random rotation, random scaling with\nrates (0.8, 0.9, 1.0, 1.25, 1.5), random cropping to the size\nof 768×768 pixels, and color jittering operations to the input\nimages.\n4) Evaluation Metrics: In accordance with previous re-\nsearch, we employed Precision, Recall, and F1-score to assess\nthe effectiveness of different methods. The three metrics are\ndeﬁned as\nPrecision = TP\nTP + FP (11)\nRecall = TP\nTP + FN (12)\nF1=2 · Precision ·Recall\nPrecision + Recall (13)\nwhere TP represents the count of true-positive pixels, FP rep-\nresents the count of false-positive pixels, and FN represents the\ncount of false-negative pixels.\nRecall measures the method’s effectiveness in identifying\nthe regions that have undergone changes. Precision assesses\nhow effectively the method ﬁlters out irrelevant and unchanged\nstructures from the prediction results. The F1-score offers a\ncomprehensive assessment of the prediction results.\nB. Compared Methods\nWe make a comparison to representative and state-of-the-art\nBCD methods, which are described as follows.\nFC-EF , FC-Siam-Conc, and FC-Siam-Diff[21] belong to\nthe category of classiﬁcation-based UNet-like models. FC-EF\nemploys early fusion by directly concatenating bitemporal im-\nages, whereas FC-Siam-Conc utilizes Siamese encoders and\nconcatenation for feature fusion. FC-Siam-Diff, on the other\nhand, employs Siamese encoders and difference for feature\nfusion.\nDTCDSCN [23] incorporates a dual-attention module for cap-\nturing interdependencies among channels and spatial positions,\nthus, enhancing the representation of features.\nSTANet [27] is a Siamese network with a self-attention mech-\nanism to compute attention weights among pairs of pixels across\ndifferent temporal instances and spatial locations. STANet-\nBAM includes the basic spatial-temporal attention module,\nwhereas STANet-PAM includes the pyramid spatial-temporal\nattention module.\nCDNet [45] is a Siamese CNN with instance-level data\naugmentation. Through generative adversarial training, the\ninstance-level data augmentation can generate bitemporal im-\nages that contain changes involving numerous and diverse syn-\nthesized building instances.\nIFNet [26] extends the design principles of FC-Siam-Conc, it\nincorporates a channel attention module and a spatial attention\nmodule subsequent to the fusion of bitemporal features and\nupper-level change features.\nSNUNet [31] employs a nested U-Net architecture, incorpo-\nrating dense skip connections between the Siamese encoder and\nmultiple subdecoders. This design choice is made to mitigate\nthe loss of spatial position information within the deep decoder\nlayers.\nChangerEx [40] includes a series of alternative interaction\nlayers in the feature extractor and proposes a ﬂow-based dual-\nalignment fusion module, which allows interactive alignment\nand feature fusion.\nChangeStar [44] presents a scalable multitemporal remote\nsensing change data generator via generative modeling, it de-\ncouples the complex simulation problem into change event\nsimulation and semantic change synthesis.\nBiT [11] is a hybrid of CNN and transformer, it employs con-\nvolutional blocks in the shallow layers and transformer blocks\nin the deeper layers.\nChangeFormer[47] is a Siamese network based on the trans-\nformer architecture. It integrates a hierarchical transformer en-\ncoder with an MLP decoder to capture and represent long-range\ndetails.\nTransUNetCD [49] is a hybrid of UNet and transformer, it\nuses a difference enhancement module to generate a difference\nfeature map containing rich change information.\nC. Experimental Results on the LEVIR-CD+ Dataset\nAs the expansion of the LEVIR-CD dataset[27], the LEVIR-\nCD+ BCD dataset[43] comprises 985 near-nadir satellite image\npairs, each with dimensions of 1024×1024 pixels and a spatial\nresolution of 0.5 m/pixel. It spans 20 regions within various\ncities in Texas, USA. Each pair of bitemporal images captures\na time span of ﬁve years. The dataset is ofﬁcially divided into a\ntraining set, consisting of 637 pairs of bitemporal images, and a\ntest set, comprising 348 pairs of bitemporal images. Following\nestablished practices, we employed the ofﬁcial training set for\nnetwork training and the ofﬁcial test set for reporting results.\nAs this dataset only provides the normalbuilding changelabels,\nwe excluded the two auxiliary heads and corresponding losses,\ntraining the BAT solely with this single type of label.\nAs demonstrated in TableI, BAT exhibits a notably higher\nrecall rate, substantiating the efﬁcacy of the CA mechanism in\nmodeling the change detection process by the Q&A scenario.\nDespite a higher recall rate, BAT attains a precision rate on par\nwith the leading performance methods, ultimately resulting in\nthe highest F1 Score.\nFig. 6 displays the predicted building changes. This visualiza-\ntion highlights BATs’ ability to not only differentiate building\nalterations from seasonal and land-cover variations, but also\naccurately reconstruct the boundary details of the modiﬁed\nstructures.\nIn addition, we conducted an assessment of BAT’s inference\nspeed employing an NVIDIA RTX 2060 Max-Q 6 G Mo-\nbile GPU with a computational capacity of 4.55 TFLOPS in\nFP32, which closely matches the computational capability of\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4927\nTABLE I\nCOMPARISON OFBAT WITH OTHER METHODS ON THELEVIR-CD+ DATASET\nFig. 6. Predicted building changes by BAT on the LEVIR-CD+ dataset.\nan NVIDIA Jetson AGX Orin embedded system (FP32: 5.33\nTFLOPS). BAT achieves an inference speed of 6.1 frames per\nsecond (FPS) when analyzing bitemporal image pairs with di-\nmensions of 1024×1024 pixels. This high efﬁciency is attributed\nto the shifted windowing scheme that is integrated in the BAM,\nwhich mitigates computational complexity from quadratic in\nterms of image size to linear in terms of image size while\nachieving the desired middle-range context.\nD. Experimental Results on the S2Looking Dataset\nThe S2Looking dataset [43] stands out as both the largest\nand the most challenging BCD dataset to date. Comprising\n5000 bitemporal image pairs, this dataset is ofﬁcially divided\ninto a training set of 3500 pairs, a validation set of 500 pairs,\nand a test set of 1000 pairs. In contrast with the LEVIR-CD+\ndataset, which focuses on urban areas at near-nadir angles, the\nS2Looking dataset primarily centers on rural areas captured\nfrom varying large off-nadir angles. The S2Looking dataset\nexhibits signiﬁcantly sparser changes in buildings compared\nwith the LEVIR-CD+ dataset. Following established practices,\nwe utilized the ofﬁcial training set for training the network, the\nofﬁcial validation set for validation purposes, and the ofﬁcial\ntest set for reporting the results. As this dataset provides not only\nthe normalbuilding changelabels but also thedemolished labels\nand thenewly builtlabels, we kept the two auxiliary heads and\ncorresponding losses, training the BAT with all the three types\nof labels, as illustrated in the lower part of Fig.4. To evaluate\nthe impact of the two auxiliary heads and corresponding losses,\nwe constructed a variant network that lacked these components.\nThis variant network is referred to asBAT without aux heads.\nAs indicated in Table II, on this particularly challenging\ndataset, BAT signiﬁcantly strengthens its superiority in terms of\nrecall rate, ultimately achieving the highest F1 Score. Although\nFC-EF and FC-Siam-Diff attain a high level of precision, their\nrecall rates are notably deﬁcient, resulting in the lowest F1\nScores. This illustrates their limited capacity to detect only the\nmost conspicuous building changes. In contrast, our BAT can\nidentify a substantial portion of building changes while main-\ntaining a precision rate comparable to that of the top-performing\nmethods. This advantage can be attributed to the generation\nof change-sensitive features by the CA mechanism within the\nBAM.\nAs presented in TableII, BAT with aux heads stands out as\nthe sole model capable of disentangling demolished structures\nfrom newly constructed ones. This capability empowers urban\nadministrators to analyze these distinct changes independently.\nThrough the extraction of change-related information embedded\nwithin thedemolished and newly builtlabels, the auxiliary heads,\nalong with their associated loss functions, effectively enhance\nboth precision and recall rates.\nAs shown in Fig.7, BAT effectively identiﬁes the majority\nof building changes, while excluding unchanged structures and\npseudochanges originating from seasonal variations, weather\nconditions, differences in illumination, or disparities in image\nsources.\nFig. 8 illustrates the comparison of predicted building changes\nusing different methods. In the ﬁrst set of bitemporal images,\nBAT exhibits superior building boundary recovery and a reduced\noccurrence of pseudochanges. For the second pair of bitemporal\nimages, BAT stands out as the only model whose predictions\nclosely align with the ground truth, whereas other methods ex-\nhibit numerous pseudochanges but fail to capture real alterations.\nIn the case of the third set of bitemporal images, BAT effectively\nupholds a high recall rate for detecting building changes while\nmitigating the occurrence of pseudochanges.\nV. B DA EXPERIMENTAL RESULTS\nA. Experimental Setup\nIn order to evaluate the effectiveness of BAT, along with\nour proposed training and prediction pipelines, we conducted\n4928 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE II\nCOMPARISON OFBAT WITH OTHER METHODS ON THES2LOOKING DATASET\nFig. 7. Predicted building changes by BAT on the S2looking dataset.\nexperiments on the xBD dataset[7], the most extensive BDA\ndataset available in the ﬁeld, and compared its performance\nagainst various methods.\nThe xBD dataset encompasses more than 800 000 building\nannotations, spanning across an area of over 45 000 square\nkilometers. Designed to facilitate the development of a versatile\nmodel suitable for a wide range of disaster scenarios, xBD com-\nprises a diverse collection of disasters that occurred in various\nglobal regions between 2011 and 2019. These disasters encom-\npass volcano eruptions, hurricanes, wildﬁres, ﬂoods, tsunamis,\nearthquakes, monsoons, and tornadoes. The dataset categorizes\nbuilding damage into four classes:no damage, minor damage,\nmajor damage, and destroyed. The xBD dataset is ofﬁcially\ndivided into a training set, a test set, and a holdout set. The\ntraining set comprises 9168 pairs of aligned predisaster and\npostdisaster images, each sized 1024×1024 pixels. The test and\nholdout sets each consist of 933 pairs. Notably, the distribution\nof building damage classes is heavily skewed towardno damage,\nwhich is represented over eight times more than the other classes.\nFig. 9 displays some predisaster images, postdisaster images,\nand corresponding ground truth masks. As prior studies, we\nutilized the ofﬁcial training set for network training, employed\nthe ofﬁcial holdout set for validation, and used the ofﬁcial test\nset for reporting the results.\nThe xBD dataset ofﬁcial evaluation metric (F1s) is a weighted\naverage of the building segmentation F1 score (F1b) and the\nharmonic mean of classwise damage classiﬁcation F1 scores\n(F1d)\nF1s =0 .3 ×F1b +0 .7 ×F1d (14)\nin whichF1d is deﬁned as\nF1d = n∑ n\ni=1 1/F1Ci\n(15)\nwhere F1Ci denotes the F1 score of each damage class.\nThis weighted F1 score, which balances precision and recall\nin a harmonic mean, proves particularly effective for assessing\nimbalanced datasets such as xBD. Using accuracy alone as a\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4929\nFig. 8. Comparison of predicted building changes by various methods on the S2looking dataset.\nmetric is problematic, as a classiﬁer that consistently predicted\n“no damage” for all images would yield a misleadingly high 75%\naccuracy. Hence, this metric is reasonable as well as challeng-\ning because it heavily penalizes overﬁtting to overrepresented\nclasses.\nAs detailed in Section III-C, we utilized the BAT’s back-\nbone, complemented by two auxiliary heads (as depicted in\nthe upper portion of Fig. 4) for building extraction. This\nnetwork was trained using predisaster images and their cor-\nresponding building footprint labels, and employed binary\ncrossentropy as loss function. Based on the dataset’s spa-\ntial resolution, the window size of sequential Swin Trans-\nformer blocks within the decoder was set to 16. The en-\ncoder in BAT’s backbone was initialized with weight values\nfrom EfﬁcientNetV2 pretrained on ImageNet [62], and re-\nmained frozen for the initial 20 epochs. The network was\ntrained for 120 epochs with a warmup strategy in the ﬁrst 20\nepochs. We applied identical settings for other training details\nand data augmentation as those used in the preceding BCD\nexperiments.\n4930 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 9. Predisaster images, postdisaster images, and ground truth of the xBD dataset.\nAs explained in SectionIII-C, for the damage classiﬁcation\ntask, we utilized the pretrained weights from the building extrac-\ntion task to initialize BAT’s backbone. BAT was trained with\nbitemporal image pairs and their associated building damage\nlabels, utilizing CORN as the loss function. The network was\ntrained for 150 epochs with a warmup strategy in the ﬁrst 30\nepochs. We also applied the same settings for other training\ndetails and data augmentation as those employed in the previous\nBCD experiments.\nB. Compared Methods\nWe make a comparison to representative and state-of-the-art\nBDA methods, which are described as follows.\nSiam-U-Net-Attn [14] not only employs the U-Net architec-\nture for local information extraction but also utilizes skip con-\nnections to preserve global information. In addition, it incorpo-\nrates a self-attention module to capture long-range information\nspanning the entire image.\nRescueNet [9] employs a segmentation head and a change\ndetection head on the dilated ResNet50 backbone. The network\nis simultaneously trained to fulﬁll the tasks of building extraction\nand damage classiﬁcation.\nD a ie ta l .[15] employs SE-ResNeXt-50 along with an atten-\ntion gate module in the initial stage for building segmentation.\nIn the subsequent stage, adjustments are made to the network’s\noutput layer to accommodate the damage classiﬁcation task.\nDeng and Wang [17] develops a two-stage BDA network\nbased on the U-Net architecture. The initial stage employs an in-\ndependent U-Net for precise building segmentation, succeeded\nby a Siamese U-Net dedicated to building damage classiﬁcation.\nTo enhance the network’s capability in segmenting buildings\nacross various scales, the architecture incorporates extra skip\nconnections and asymmetric convolution blocks. In addition,\nthe network employs shufﬂe attention to focus on the correlation\nbetween buildings before and after the disaster.\nChangeOS [16] integrates building localization and dam-\nage classiﬁcation within a cohesive framework using a partial\nSiamese FCN architecture. This approach facilitates interaction\nat the feature representation level. ChangeOS offers an advan-\ntage by enabling end-to-end training and inference.\nBDANet [18] uses a two-branch multiscale U-Net as back-\nbone, where pre and postdisaster images are fed into the network\nseparately. To investigate correlations between these images,\na crossdirectional attention module has been introduced. In\naddition, the application of CutMix data augmentation addresses\nthe difﬁculties associated with challenging classes.\nC. Experimental Results for Stage 1: Building Extraction\nAs displayed in TableIII, the backbone of BAT demonstrates\nsuperior performance, yielding the highest building segmen-\ntation F1 score (F1b). This superiority is attributed both to\nthe encoder’s powerful feature extraction capability and to the\ndecoder’s constrained self-attention within window partitions,\nwhich are commensurate in size with buildings and their sur-\nrounding environments. Because building damage degrees are\npredicted based on the extracted buildings, enhanced accuracy\nin building extraction serves as a crucial prerequisite for the\nsubsequent stage.\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4931\nTABLE III\nCOMPARISON OFBAT’S BACKBONE WITH OTHER METHODS IN STAGE 1:\nBUILDING EXTRACTION\nTABLE IV\nCOMPARISON OFBAT WITH OTHER METHODS IN STAGE 2: DAMAGE\nCLASSIFICATION\nD. Experimental Results for Stage 2: Damage Classiﬁcation\nWeber and Kané[8] employed a straightforward bitemporal\nfeature fusion strategy via concatenation. Despite this simplicity,\nas presented in TableIV, their method attains competitive F1\nscores for the evident damage levels (no damageand destroyed).\nHowever, distinguishing between the intermediate damage lev-\nels (minor damageand major damage) remains challenging due\nto subtle visual discrepancies, resulting in signiﬁcant confusion\namong these classes for most methods. In contrast, BAT exhibits\na more balanced performance compared with other methods\nacross all building damage degrees, resulting in the highest\nF1d score (the harmonic mean of classwise damage classiﬁ-\ncation F1 scores). Furthermore, it attains the highest accuracy\nin recognizing the intermediate damage levels (minor dam-\nage and major damage), which pose a greater challenge com-\npared with the more evident building statuses (no damageand\ndestroyed).\nFig. 10 displays BAT’s predictions for building damage clas-\nsiﬁcation across diverse disasters. Due to the strong building\nextraction capabilities of its backbone, BAT precisely recon-\nstructs boundaries even for very small structures. Some damaged\nbuildings exhibit intact roofs, posing a signiﬁcant challenge that\nrequires a change detection model to assess damage degrees\nby considering the environmental factors, such as accumulated\nwater, around the structures. BAT addresses this challenge by\nFig. 10. Predicted building damage classiﬁcation by BAT.\nFig. 11. Comparison of predicted BDAs by various methods on the xBD\ndataset.\nconﬁning the CA mechanism to local windows that enclose\nboth the buildings and their surrounding environments. Con-\nsequently, BAT accurately identiﬁes the building damages that\nare subtle from an aerial perspective. In addition, our ordinal\nregression approach plays a crucial role in categorizing damage\nseverity levels. For instance, it accurately labels buildings with\na small portion of roof damaged in hurricanes asminor damage,\nthose with a signiﬁcant portion of roof damaged in hurricanes as\nmajor damage, and those burnt down in wildﬁres asdestroyed.\nFig. 11 presents the predicted BDAs by various methods,\nit is obvious that BAT correctly labels the damage levels of\nthe majority of buildings situated at the boundary between\nthe undamaged and destroyed zones. Predictions from other\n4932 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE V\nCOMPARISON OFBAT WITH OTHER METHODS IN TERMS OF THETOTAL SCORE\nmethods often exhibit inconsistent pixelwise labels within in-\ndividual buildings, which presents a challenge to statistics. We\nadopt an object-based prediction pipeline where pixelwise labels\nwithin individual buildings achieve consensus through major-\nity voting, thereby advancing predictions to an instancewise\nlevel.\nE. Experimental Results for the Entire Task\nAs presented in TableV, BAT’s superior performance in both\nthe building extraction stage and the damage classiﬁcation stage\nleads to the highest total score.\nVI. ABLATION ANALYSIS\nThe BAM, ordinal regression approach, and object-based\nprediction are core components in our pipeline. Consequently,\nwe conducted ablation analysis on the xBD dataset to assess\ntheir effectiveness.\nA. Ablation Analysis of the BAM\nBi-SRNet [37] also utilizes a CA mechanism (Cot-SR) to\nmodel the temporal correlations. Within the CA mechanism\nof Cot-SR, the encoded features from one temporal image\ninitially attend to speciﬁc regions within itself to generate the\nattention matrice. Subsequently, the generated attention ma-\ntrice is matrix multiplied with the encoded features from the\nother temporal image. Its CA mechanism can be represented\nas CA(Qpre,Kpre,Vpost) and CA(Qpost,Kpost,Vpre). In contrast,\nwithin the CA mechanism of our BAM, the encoded features\nfrom one temporal image initially attend to speciﬁc regions\nin the other temporal image to generate the attention matrice.\nOur CA mechanism can be represented as CA(Qpre,Kpost,Vpost)\nand CA(Qpost,Kpre,Vpre). To compare the effectiveness of these\ntwo CA mechanisms, we only replaced the CA mechanism of\nBAM with that of Cot-SR and formed a variant network. As\ndemonstrated in Table VI, the CA mechanism of BAM can\nbetter model the spatio-temporal semantic relations than that\nof Cot-SR.\nTABLE VI\nCOMPARISON OF THECA MECHANISMS OF COT-SR AND BAM\nTABLE VII\nCOMPARISON OFDIFFERENT LOSS FUNCTIONS EMPLOYED BYBATIN STAGE 2:\nDAMAGE CLASSIFICATION\nTABLE VIII\nCOMPARISON OFSEMANTIC SEGMENTATION ANDINSTANCE SEGMENTATION\nFig. 12. Comparison of semantic segmentation approach and instance\nsegmentation approach on the xBD dataset.\nB. Ablation Analysis of the Ordinal Regression Approach\nA ss h o w ni nT a b l eVII, when compared with conventional\nmulticlass classiﬁcation loss functions, such as dice and crossen-\ntropy, the ordinal regression loss function CORN prominently\nboosts accuracy for intermediate damage levels (minor damage\nand major damage). This demonstrates that BDA is more ap-\npropriately treated as an ordinal regression problem rather than\na multiclass classiﬁcation problem.\nC. Ablation Analysis of the Object-Based Prediction\nAs presented in TableVIII, the object-based postprocessing\nprocedure primarily improves accuracy for intermediate damage\nlevels, resulting in F1 score increases of 1.6% forminor damage\nand 1.1% formajor damage. This is evidenced by Fig.12, where\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4933\ninconsistent pixelwise predictions within individual buildings\nare rectiﬁed through a majority voting mechanism.\nVII. DISCUSSION\nChange detection ﬁnds applications in various domains, en-\nhancing decision-making and facilitating the understanding of\ndynamic processes. With aligned preceding and subsequent\nimages, a key question is how to effectively model the spatio-\ntemporal semantic relations between the bitemporal image pair.\nIn response to this question, we propose a novel CA mecha-\nnism that emulates the question-and-answer pattern observed\nin human interactions. We integrate this CA mechanism into a\nSiamese network, forming a change detection network named\nBAT. Subsequently, we applied BAT to BCD task to evaluate its\neffectiveness. Considering the building scale, irrelevant infor-\nmation removal, and computational efﬁciency, we introduce the\nshifted windowing scheme to the CA mechanism, conﬁning it to\na deﬁned range. BAT distinguishes itself from existing change\ndetection models, which exclusively supportbuilding changela-\nbels. BAT is capable to harness the valuable information offered\nby demolished and newly builtlabels, enabling it to predict these\nadditional label types.\nIn contrast to existing BDA methods, which overlook the\nintrinsic order among ordinal targets and simplistically treat the\nBDA task as a multiclass semantic segmentation problem, we\nrecognize the signiﬁcance of ordinal relationships and approach\nthe BDA task as an ordinal regression problem. Therefore, we\ndesign an ordinal regression training pipeline and an object-\nbased prediction pipeline for BDA. The effectiveness of our\nproposed pipelines is not only substantiated through quantitative\nmetrics and ablation analysis, but also evidenced by visual\ninterpretations.\nDespite the aforementioned advantages, a limitation worth\nnoting is that our CA mechanism is constrained to analyzing\nbitemporal image pairs and incapable to model interrelationships\nwithin a multitemporal image set. A further limitation to be ac-\nknowledged is that the backbone of BAT consumes a majority of\nthe inference time, accounting for 71.1%, whereas the CA mech-\nanism occupies only 28.5% of the inference time. Therefore, the\npursuit of real-time processing speed on embedded devices may\nbe advanced by designing a powerful yet lightweight backbone\nto supplant the existing one. Optimizing BAT with TensorRT\nmay also result in a signiﬁcant speed boost.\nVIII. CONCLUSION\nIn this work, we propose a novel CA mechanism to effectively\nand efﬁciently model the spatio-temporal semantic relations\nbetween a pair of bitemporal remote sensing images. We also\nrecognize the signiﬁcance of ordinal relationships and approach\nthe BDA task as an ordinal regression problem. Our method\nachieves state-of-the-art accuracy on two BCD datasets (LEVIR-\nCD+ and S2Looking), as well as the largest BDA dataset\n(xBD). This study focuses on the BCD and BDA tasks, our\nfuture research direction involves the application, adaptation,\nand enhancement of the BAT framework for addressing change\ndetection tasks within the domains of agriculture and climate\nchange.\nACKNOWLEDGMENT\nConceptualization, Wen Lu and Minh Nguyen; Data curation,\nWen Lu; Formal analysis, Wen Lu; Funding acquisition, Minh\nNguyen; Investigation, Wen Lu; Methodology, Wen Lu; Project\nadministration, Minh Nguyen; Resources, Minh Nguyen; Soft-\nware, Wen Lu; Supervision, Minh Nguyen; Validation, Minh\nNguyen; Visualization, Wen Lu; Writing—original draft, Wen\nLu; Writing—review and editing, Lu Wei and Minh Nguyen.\nAll authors have read and agreed to the published version\nof the manuscript. We gratefully acknowledge the support of\nFHE Electrical Ltd. T/A Kinetic Electrical East Tamaki for\ntheir invaluable contributions to this research. Their technical\nassistance has been instrumental in the successful completion of\nthis study. We also extend our appreciation to Andrew Bryson\nfor his expertise and guidance throughout the project. This work\nwould not have been possible without his assistance.\nREFERENCES\n[1] S. Ji, S. Wei, and M. Lu, “Fully convolutional networks for multisource\nbuilding extraction from an open aerial and satellite imagery data set,”\nIEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 574–586, Jan. 2019.\n[2] H. A. Aﬁfy, “Evaluation of change detection techniques for monitoring\nland-cover changes: A case study in new Burg EL-Arab area,”Alexandria\nEng. J., vol. 50, no. 2, pp. 187–195, 2011.\n[3] B. Demir, F. Bovolo, and L. Bruzzone, “Updating land-cover maps by clas-\nsiﬁcation of image time series: A novel change-detection-driven transfer\nlearning approach,” IEEE Trans. Geosci. Remote Sens., vol. 51, no. 1,\npp. 300–312, Jan. 2013.\n[4] M. E. Hodgson, B. A. Davis, and J. Kotelenska, “Remote sensing\nand GIS data/information in the emergency response/recovery phase,”\nGeospatial Techniques in Urban Hazard and Disaster Analysis. 2010,\npp. 327–354.\n[5] K. Yang et al., “Semantic change detection with asymmetric siamese\nnetworks,” 2020,arXiv:2010.05687.\n[6] P. Yuan, Q. Zhao, X. Zhao, X. Wang, X. Long, and Y . Zheng, “A\ntransformer-based Siamese network and an open optical dataset for se-\nmantic change detection of remote sensing images,”Int. J. Digit. Earth,\nvol. 15, no. 1, pp. 1506–1525, 2022.\n[7] R. Gupta et al., “xBD: A dataset for assessing building damage from\nsatellite imagery,” 2019,arXiv:1911.09296.\n[8] E. Weber and H. Kané, “Building disaster damage assessment in satellite\nimagery with multi-temporal fusion,” 2020,arXiv:2004.05525.\n[9] R. Gupta and M. Shah, “RescueNet: Joint building segmentation and\ndamage assessment from satellite imagery,” inProc. 25th Int. Conf. Pattern\nRecognit., 2021, pp. 4405–4411.\n[10] Y . Zhang et al., “An efﬁcient change detection method for disaster-affected\nbuildings based on a lightweight residual block in high-resolution remote\nsensing images,” Int. J. Remote Sens., vol. 44, no. 9, pp. 2959–2981,\n2023.\n[11] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection\nwith transformers,”IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–14,\n2022.\n[12] B. Hou, Q. Liu, H. Wang, and Y . Wang, “From W-Net to CDGAN:\nBitemporal change detection via deep learning techniques,”IEEE Trans.\nGeosci. Remote Sens., vol. 58, no. 3, pp. 1790–1802, Mar. 2020.\n[13] D. Peng, Y . Zhang, and H. Guan, “End-to-end change detection for high\nresolution satellite images using improved UNet,”Remote Sens., vol. 11,\nno. 11, 2019, Art. no. 1382.\n[14] H. Hao et al., “An attention-based system for damage assessment using\nsatellite imagery,” inProc. IEEE Int. Geosci. Remote Sens. Symp., 2021,\npp. 4396–4399.\n[15] B. Dai, H. Xiao, and M. Zhang, “A novel two-stage network for building\nlocalization and damage level assessment,” inProc. 6th Int. Conf. Big Data\nInf. Analytics, 2020, pp. 265–269.\n[16] Z. Zheng, Y . Zhong, J. Wang, A. Ma, and L. Zhang, “Building damage\nassessment for rapid disaster response with a deep object-based semantic\nchange detection framework: From natural disasters to man-made disas-\nters,” Remote Sens. Environ., vol. 265, 2021, Art. no. 112636.\n[17] L. Deng and Y . Wang, “Post-disaster building damage assessment based\non improved U-Net,”Sci. Rep., vol. 12, no. 1, 2022, Art. no. 15862.\n4934 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\n[18] Y . Shen et al., “BDANet: Multiscale convolutional neural network\nwith cross-directional attention for building damage assessment from\nsatellite images,” IEEE Trans. Geosci. Remote Sens. , vol. 60, 2021,\nArt. no. 5402114.\n[19] J. Koo, J. Seo, K. Yoon, and T. Jeon, “Dual-HRNet for building\nlocalization and damage classiﬁcation,” 2020. [Online]. Available:\nhttps://github.com/DIUxxView/xView2_ﬁfth_place/blob/master/ﬁgures/\nxView2_White_Paper_SI_Analytics.pdf\n[20] X. Shi, W. Cao, and S. Raschka, “Deep neural networks for rank-consistent\nordinal regression based on conditional probabilities,”Pattern Anal. Appl.,\nvol. 26, no. 3, pp. 941–955, 2023.\n[21] R. C. Daudt, B. L. Saux, A. Boulch, and Y . Gousseau, “Urban change\ndetection for multispectral Earth observation using convolutional neu-\nral networks,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2018,\npp. 2115–2118.\n[22] Z. Li, C. Tang, X. Li, W. Xie, K. Sun, and X. Zhu, “Towards accurate and\nreliable change detection of remote sensing images via knowledge review\nand online uncertainty estimation,” 2023.arXiv:2305.19513.\n[23] Y . Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, “Building change\ndetection for remote sensing images using a dual-task constrained deep\nSiamese convolutional network model,”IEEE Geosci. Remote Sens. Lett.,\nvol. 18, no. 5, pp. 811–815, May 2021.\n[24] W. Gao, Y . Sun, X. Han, Y . Zhang, L. Zhang, and Y . Hu, “AMIO-\nNet: An attention-based multiscale input–output network for building\nchange detection in high-resolution remote sensing images,” IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens., vol. 16, pp. 2079–2093,\n2023.\n[25] C. Xu et al., “Progressive context-aware aggregation network combining\nmulti-scale and multi-level dense reconstruction for building change de-\ntection,” Remote Sens., vol. 15, no. 8, 2023, Art. no. 1958.\n[26] C. Zhang et al., “A deeply supervised image fusion network for change\ndetection in high resolution bi-temporal remote sensing images,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 166, pp. 183–200, 2020.\n[27] H. Chen and Z. Shi, “A spatial-temporal attention-based method and a\nnew dataset for remote sensing image change detection,”Remote Sens.,\nvol. 12, no. 10, 2020, Art. no. 1662.\n[28] D. Wang, X. Chen, M. Jiang, S. Du, B. Xu, and J. Wang, “ADS-Net:\nAn attention-based deeply supervised network for remote sensing image\nchange detection,” Int. J. Appl. Earth Observ. Geoinf., vol. 101, 2021,\nArt. no. 102348.\n[29] X. Peng, R. Zhong, Z. Li, and Q. Li, “Optical remote sensing im-\nage change detection based on attention mechanism and image differ-\nence,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7296–7307,\nSep. 2020.\n[30] H. Jiang, X. Hu, K. Li, J. Zhang, J. Gong, and M. Zhang, “PGA-SiamNet:\nPyramid feature-based attention-guided Siamese network for remote sens-\ning orthoimagery building change detection,”Remote Sens., vol. 12, no. 3,\n2020, Art. no. 484.\n[31] S. Fang, K. Li, J. Shao, and Z. Li, “SNUNet-CD: A densely connected\nSiamese network for change detection of VHR images,”IEEE Geosci.\nRemote Sens. Lett., vol. 19, 2022, Art. no. 8007805.\n[32] Z. Li et al., “Lightweight remote sensing change detection with progressive\nfeature aggregation and supervised attention,”IEEE Trans. Geosci. Remote\nSens., vol. 61, 2023, Art. no. 5602812.\n[33] C. Han, C. Wu, H. Guo, M. Hu, J. Li, and H. Chen, “Change guiding\nnetwork: Incorporating change prior to guide change detection in remote\nsensing imagery,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 16, pp. 8395–8407, 2023.\n[34] T. Liu et al., “Building change detection for VHR remote sensing images\nvia local–global pyramid network and cross-task transfer learning strat-\negy,”IEEE Trans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4704817.\n[35] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,”ACM Comput. Surv., vol. 54, no. 10s,\npp. 1–41, 2022.\n[36] L. Ding, J. Zhang, K. Zhang, H. Guo, B. Liu, and L. Bruzzone, “Joint\nspatio-temporal modeling for semantic change detection in remote sensing\nimages,” 2022,arXiv:2212.05245.\n[37] L. Ding, H. Guo, S. Liu, L. Mou, J. Zhang, and L. Bruzzone, “Bi-\ntemporal semantic reasoning for the semantic change detection in HR\nremote sensing images,”IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5620014.\n[38] S. Hou et al., “Stable prototype guided single-temporal supervised learning\nfor change detection and extraction of building,”IEEE Trans. Geosci.\nRemote Sens., vol. 61, 2023, Art. no. 4406622.\n[39] K. Zhang, X. Zhao, F. Zhang, L. Ding, J. Sun, and L. Bruzzone, “Re-\nlation changes matter: Cross-temporal difference transformer for change\ndetection in remote sensing images,”IEEE Trans. Geosci. Remote Sens.,\nvol. 61, 2023, Art. no. 5611615.\n[40] S. Fang, K. Li, and Z. Li, “Changer: Feature interaction is what you need\nfor change detection,”IEEE Trans. Geosci. Remote Sens., vol. 61, 2023,\nArt. no. 5610111.\n[41] P. Chen, B. Zhang, D. Hong, Z. Chen, X. Yang, and B. Li, “FCCDN:\nFeature constraint network for VHR image change detection,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 187, pp. 101–119, 2022.\n[42] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 9992–10002.\n[43] L. Shen et al., “S2Looking: A satellite side-looking dataset for building\nchange detection,”Remote Sens., vol. 13, no. 24, 2021, Art. no. 5094.\n[44] Z. Zheng, S. Tian, A. Ma, L. Zhang, and Y . Zhong, “Scalable multi-\ntemporal remote sensing change data generation via simulating stochas-\ntic change process,” inProc. IEEE/CVF Int. Conf. Comput. Vis., 2023,\npp. 21761–21770.\n[45] H. Chen, W. Li, and Z. Shi, “Adversarial instance augmentation for\nbuilding change detection in remote sensing images,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, 2022, Art. no. 5603216.\n[46] R. Hu, G. Pei, P. Peng, T. Chen, and Y . Yao, “Feature difference enhance-\nment fusion for remote sensing image change detection,” inProc. Chin.\nConf. Pattern Recognit. Comput. Vis., S. Yu et al., Eds., 2022, pp. 510–523.\n[47] W. G. C. Bandara and V . M. Patel, “A transformer-based siamese network\nfor change detection,” inProc. IEEE Int. Geosci. Remote Sens. Symp.,\n2022, pp. 207–210.\n[48] G. Pei and L. Zhang, “Feature hierarchical differentiation for remote\nsensing image change detection,”IEEE Geosci. Remote Sens. Lett., vol. 19,\n2022, Art. no. 6514105.\n[49] Q. Li, R. Zhong, X. Du, and Y . Du, “TransUNetCD: A hybrid transformer\nnetwork for change detection in optical remote-sensing images,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 5622519.\n[50] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2015, pp. 3431–3440.\n[51] M. Zhang, G. Xu, K. Chen, M. Yan, and X. Sun, “Triplet-based semantic\nrelation learning for aerial remote sensing image change detection,”IEEE\nGeosci. Remote Sens. Lett., vol. 16, no. 2, pp. 266–270, Feb. 2019.\n[52] X. Dong et al., “CSWin transformer: A general vision transformer back-\nbone with cross-shaped windows,” inProc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2022, pp. 12114–12124.\n[53] Z. Zhang, W. Lu, J. Cao, and G. Xie, “MKANet: An efﬁcient network\nwith sobel boundary loss for land-cover classiﬁcation of satellite remote\nsensing imagery,”Remote Sens., vol. 14, no. 18, 2022, Art. no. 4514.\n[54] M. Tan and Q. Le, “EfﬁcientNetV2: Smaller models and faster training,”\nin Proc. Int. Conf. Mach. Learn., 2021, pp. 10096–10106.\n[55] C. Fiorio and J. Gustedt, “Two linear time union-ﬁnd strategies for image\nprocessing,” Theor . Comput. Sci., vol. 154, no. 2, pp. 165–181, 1996.\n[56] K. Wu, E. Otoo, and A. Shoshani, “Optimizing connected component\nlabeling algorithms,” inP r o c .M e d .I m a g . :I m a g eP r o c e s s ., 2005, pp. 1965–\n1976.\n[57] M. Liu and Q. Shi, “DSAMNet: A deeply supervised attention metric\nbased network for change detection of high-resolution images,” inProc.\nIEEE Int. Geosci. Remote Sens. Symp., 2021, pp. 6159–6162.\n[58] M. Papadomanolaki, M. Vakalopoulou, and K. Karantzalos, “A deep\nmultitask learning framework coupling semantic segmentation and\nfully convolutional LSTM networks for urban change detection,”\nIEEE Trans. Geosci. Remote Sens. , vol. 59, no. 9, pp. 7651–7668,\nSep. 2021.\n[59] Z. Li, C. Tang, L. Wang, and A. Y . Zomaya, “Remote sensing change\ndetection via temporal feature interaction and guided reﬁnement,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 5628711.\n[60] M. Liu, Z. Chai, H. Deng, and R. Liu, “A CNN-transformer network\nwith multiscale context aggregation for ﬁne-grained cropland change\ndetection,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15,\npp. 4297–4306, 2022.\n[61] Y . Zhou, C. Huo, J. Zhu, L. Huo, and C. Pan, “DCAT: Dual cross-attention-\nbased transformer for change detection,”Remote Sens., vol. 15, no. 9, 2023,\nArt. no. 2395.\n[62] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” inProc. Annu. Conf. Neural\nInf. Process. Syst., 2012.\nLU et al.: BITEMPORAL ATTENTION TRANSFORMER FOR BUILDING CHANGE DETECTION AND BUILDING DAMAGE ASSESSMENT 4935\nWen Lu received the B.Eng. degree in materi-\nals physics from Wuhan University of Technology,\nWuhan, China, in 2007, the M.Eng. degree in com-\nputer science and technology from Hubei Univer-\nsity of Technology, Wuhan, in 2023. He is currently\nworking toward the Ph.D. degree in computer and\ninformation sciences with the School of Engineer-\ning, Computer and Mathematical Sciences, Auckland\nUniversity of Technology, Auckland, New Zealand.\nHis research interests include computer vision, re-\nmote sensing, machine learning, and deep learning.\nLu Wei received the B.Eng. degree in software en-\ngineering from Wuhan University of Technology,\nWuhan, China, in 2006, and the M.Eng. degree\nin computer technology from Wuhan University,\nWuhan, in 2019.\nShe was a Senior Engineer with Huawei Tech-\nnologies Corporation, and is currently an Associate\nProfessor with School of Information Science and En-\ngineering, Wuchang Shouyi University, Wuhan. Her\nresearch interests include image radiance correction,\nmultisensor image fusion, image quality assessment,\nand intelligent image processing.\nMinh Nguyen received the B.Sc. degree in computer\nscience, and the M.Sc. and Ph.D. degrees in computer\nvision from The University of Auckland, Auckland,\nNew Zealand, in 2007, 2010, and 2014 respectively.\nSince 2017, he has codirected the Centre for\nRobotics and Vision with Auckland University of\nTechnology (AUT). Currently, he is the Head of\nthe Department of Computer Science and Software\nEngineering with AUT, leading a team of 40 faculty\nmembers. His research interests include computer vi-\nsion, AI, virtual/augmented reality, computer–human\ninteraction, and knowledge representation and machine learning.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5406524538993835
    },
    {
      "name": "Transformer",
      "score": 0.514149010181427
    },
    {
      "name": "Change detection",
      "score": 0.4380117654800415
    },
    {
      "name": "Reliability engineering",
      "score": 0.32593613862991333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.20927342772483826
    },
    {
      "name": "Engineering",
      "score": 0.14180392026901245
    },
    {
      "name": "Electrical engineering",
      "score": 0.13054883480072021
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39854758",
      "name": "Auckland University of Technology",
      "country": "NZ"
    },
    {
      "id": "https://openalex.org/I4400600924",
      "name": "Wuchang Shouyi University",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210121573",
      "name": "Wuchang University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 26
}