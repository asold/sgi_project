{
  "title": "We need a new ethics for a world of AI agents",
  "url": "https://openalex.org/W4412952090",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2645941213",
      "name": "Iason Gabriel",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2709047638",
      "name": "Geoff Keeling",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2773724728",
      "name": "Arianna Manzini",
      "affiliations": [
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2101100271",
      "name": "James Evans",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2645941213",
      "name": "Iason Gabriel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2709047638",
      "name": "Geoff Keeling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2773724728",
      "name": "Arianna Manzini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101100271",
      "name": "James Evans",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4402952666",
    "https://openalex.org/W4406949968",
    "https://openalex.org/W4410827098",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4404518278",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W3034344071"
  ],
  "abstract": null,
  "full_text": "P r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nWe  need  a  new  ethics  for  a  world  of  AI  agents   \nThe  deployment  of  capable  AI  agents  raises  fresh  questions  about  safety,  \nhuman–machine\n \nrelationships\n \nand\n \nsocial\n \ncoordination.\n  Artiﬁcial  intelligence  (AI)  developers  are  shifting  their  focus  to  building  agents  that  can  operate  \nindependently,\n \nwith\n \nlittle\n \nhuman\n \nintervention.\n \nTo\n \nbe\n \nan\n \nagent\n \nis\n \nto\n \nhave\n \nthe\n \nability\n \nto\n \nperceive\n \nand\n \nact\n \non\n \nan\n \nenvironment\n \nin\n \na\n \ngoal-directed\n \nand\n \nautonomous\n \nway.\n1\n \nFor\n \nexample,\n \na\n \ndigital\n \nagent\n \ncould\n \nbe\n \nprogrammed\n \nto\n \nbrowse\n \nthe\n  \nweb\n \nand\n \nmake\n \nonline\n \npurchases\n \non\n \nbehalf\n \nof\n \na\n \nuser\n \n—\n \ncomparing\n \nprices,\n \nselecting\n \nitems\n \nand\n \ncompleting\n \ncheckouts.\n \nA\n \nrobot\n \nwith\n \narms\n \ncould\n \nbe\n \nan\n \nagent\n \nif\n \nit\n \ncould\n \npick\n \nup\n \nobjects,\n \nopen\n \ndoors\n \nor\n \nassemble\n \nparts\n \nwithout\n \nbeing\n \ntold\n \nhow\n \nto\n \ndo\n \neach\n \nstep.\n  Companies  such  as  the  digital-marketing  ﬁrm  Salesforce,  based  in  San  Francisco,  California,  and  \ncomputer\n \ngraphics\n \nand\n \nhardware\n \nﬁrm\n \nNvidia,\n \nbased\n \nin\n \nSanta\n \nClara,\n \nCalifornia,\n \nare\n \nalready\n \noﬀering\n \ncustomer-services\n \nsolutions\n \nfor\n \nbusinesses,\n \nusing\n \nagents.\n \nIn\n \nthe\n \nnear\n \nfuture,\n \nAI\n \nassistants\n \nmight\n \nbe\n \nable\n \nto\n \nfulﬁl\n \ncomplex\n \nmultistep\n \nrequests,\n \nsuch\n \nas\n \n‘get\n \nme\n \na\n \nbetter\n \nmobile\n \nphone\n \ncontract’,\n \nby\n \nretrieving\n \na\n \nlist\n \nof\n \ncontracts\n \nfrom\n \na\n \nprice-comparison\n \nwebsite,\n \nselecting\n \nthe\n \nbest\n \noption,\n \nauthorizing\n \nthe\n \nswitch,\n \ncancelling\n \nthe\n \nold\n \ncontract,\n \nand\n \narranging\n \nthe\n \ntransfer\n \nof\n \ncancellation\n \nfees\n \nfrom\n \nthe\n \nuser’s\n \nbank\n \naccount.\n  The  rise  of  more-capable  AI  agents  is  likely  to  have  far-reaching  political,  economic  and  social  \nconsequences.\n \nOn\n \nthe\n \npositive\n \nside,\n \nthey\n \ncould\n \nunlock\n \neconomic\n \nvalue:\n \nthe\n \nconsultancy\n \nMcKinsey\n forecasts an  annual  windfall  from  generative  AI  of  US$2.6  trillion  to  $4.4  trillion   globally,  once  AI  agents  are  widely  deployed.  They  might  also  serve  as  powerful  research  assistants  and  accelerate  \nscientiﬁc\n \ndiscovery.\n  But  AI  agents  also  introduce  risks.  People  need  to  know  who  is  responsible  for  agents  operating  ‘in  \nthe\n \nwild’,\n \nand\n \nwhat\n \nhappens\n \nif\n \nthey\n \nmake\n \nmistakes.\n \nFor\n \nexample,\n \nin\n \nNovember\n \n2022,\n \nan\n \nAir\n \nCanada\n \nchatbot\n \nmistakenly\n \ndecided\n \nto\n \noﬀer\n \na\n \ncustomer\n \na\n \ndiscounted\n \nbereavement\n \nfare,\n \nleading\n \nto\n \na\n \nlegal\n \ndispute\n \nover\n \nwhether\n \nthe\n \nairline\n \nwas\n \nbound\n \nby\n \nthe\n \npromise.\n \nIn\n \nFebruary\n \n2024,\n \na\n \ntribunal\n \nruled\n \nthat\n \nit\n \nwas\n \n—\n \nhighlighting\n \nthe\n \nliabilities\n \nthat\n \ncorporations\n \ncould\n \nexperience\n \nwhen\n \nhanding\n \nover\n \ntasks\n \nto\n \nAI\n \nagents,\n \nand\n \nthe\n \ngrowing\n \nneed\n \nfor\n \nclear\n \nrules\n \naround\n \nAI\n \nresponsibility.\n  Here,  we  argue  for  greater  engagement  by  scientists,  scholars,  engineers  and  policymakers  with  the  \nimplications\n \nof\n \na\n \nworld\n \nincreasingly\n \npopulated\n \nby\n \nAI\n \nagents.\n \nWe\n \nexplore\n \nkey\n \nchallenges\n \nthat\n \nmust\n \nbe\n \naddressed\n \nto\n \nensure\n \nthat\n \ninteractions\n \nbetween\n \nhumans\n \nand\n \nagents\n \n—\n \nand\n \namong\n \nagents\n \nthemselves\n \n—\n \nremain\n \nbroadly\n \nbeneﬁcial.\n  \n1\n Russell,  S.  &  Norvig  P.  Artiﬁcial  Intelligence:  A  Modern  Approach  (Pearson,  2020).  \nP r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nThe  alignment  problem   AI-safety  researchers  have  long  warned  about  the  risks  of  misspeciﬁed  or  misinterpreted  \ninstructions,\n \nincluding\n \nsituations\n \nin\n \nwhich\n \nan\n \nautomated\n \nsystem\n \ntakes\n \nan\n \ninstruction\n \ntoo\n \nliterally,\n \noverlooks\n \nimportant\n \ncontext,\n \nor\n \nﬁnds\n \nunexpected\n \nand\n \npotentially\n \nharmful\n \nways\n \nto\n \nreach\n \na\n \ngoal.\n2\n  A  well-known  example  involves  an  AI  agent  trained  to  play  the  computer  game  Coast  Runners,  \nwhich\n \nis\n \na\n \nboat\n \nrace.\n \nThe\n \nagent\n \ndiscovered\n \nthat\n \nit\n \ncould\n \nearn\n \nhigher\n \nscores\n \nnot\n \nby\n \ncompleting\n \nthe\n \nrace,\n \nbut\n \nrather\n \nby\n \nrepeatedly\n \ncrashing\n \ninto\n \nobjects\n \nthat\n \nawarded\n \npoints—technically\n \nachieving\n \nan\n objective,  but  in  a  way  that  deviated  from  the  spirit  of  the  task.  The  purpose  of  the  game  was  to  complete  the  race,  not  endlessly   accumulate  points.   As  AI  agents  gain  access  to  real-world  interfaces  —  including  search  engines,  e-mail  clients  and  \ne-commerce\n \nplatforms\n \n—\n \nsuch\n \ndeviations\n \ncan\n \nhave\n \ntangible\n \nconsequences.\n \nConsider\n \nthe\n \ncase\n \nof\n \na\n \nlawyer\n \nwho\n \ninstructs\n \ntheir\n \nAI\n \nassistant\n \nto\n \ncirculate\n \na\n \nlegal\n \nbrief\n \nfor\n \nfeedback.\n \nThe\n \nassistant\n \ndoes\n \nso,\n \nbut\n \nfails\n \nto\n \nregister\n \nthat\n \nit\n \nshould\n \nbe\n \nshared\n \nonly\n \nwith\n \nthe\n \nin-house\n \nteam,\n \nleading\n \nto\n \na\n \nprivacy\n \nbreach.\n \n  Such  situations  highlight  a  diﬃcult  trade-oﬀ:  just  how  much  information  should  an  AI  assistant  \nproactively\n \nseek\n \nbefore\n \nacting?\n \nToo\n \nlittle\n \nopens\n \nup\n \nthe\n \npossibility\n \nof\n \ncostly\n \nmistakes;\n \ntoo\n \nmuch\n \nundermines\n \nthe\n \nconvenience\n \nusers\n \nexpect.\n \nThese\n \nchallenges\n \npoint\n \nto\n \nthe\n \nneed\n \nfor\n \nsafeguards,\n \nincluding\n \ncheck-in\n \nprotocols\n \nfor\n \nhigh-stakes\n \ndecisions,\n \nrobust\n \naccountability\n \nsystems\n \nsuch\n \nas\n action  logging,  and  mechanisms  for  redress  when  errors  occur.   Even  more  concerning  are  cases  in  which  AI  agents  are  empowered  to  modify  the  environment  they  \noperate\n \nin,\n \nusing\n \nexpert-level\n \ncoding\n \nability\n \nand\n \ntools.\n \nWhen\n \nthe\n \nuser’s\n \ngoals\n \nare\n \npoorly\n \ndeﬁned\n \nor\n \nleft\n \nambiguous,\n \nsuch\n \nagents\n \nhave\n \nbeen\n \nknown\n \nto\n \nmodify\n \nthe\n \nenvironment\n \nto\n \nachieve\n \ntheir\n \nobjective,\n \neven\n \nwhen\n \nthis\n \nentails\n \ntaking\n \nactions\n \nthat\n \nshould\n \nbe\n \nstrictly\n \nout\n \nof\n \nbounds.\n \nFor\n \nexample,\n \nan\n \nAI\n \nresearch\n \nassistant\n \nthat\n \nwas\n \nfaced\n \nwith\n \na\n \nstrict\n \ntime\n \nlimit\n \ntried\n \nto\n \nrewrite\n \nthe\n \ncode\n \nto\n \nremove\n \nthe\n \ntime\n \nlimit\n \naltogether,\n \ninstead\n \nof\n \ncompleting\n \nthe\n \ntask.\n3\n \nThis\n \ntype\n \nof\n \nbehaviour\n \nraises\n \nalarms\n \nabout\n \nthe\n \npotential\n \nfor\n \nAI\n \nagents\n \nto\n \ntake\n \ndangerous\n \nshortcuts\n \nthat\n \ndevelopers\n \nmight\n \nbe\n \nunable\n \nto\n \nanticipate.\n \nAgents\n \ncould,\n \nin\n \npursuit\n \nof\n \na\n \nhigh-level\n \nobjective,\n \neven\n \ndeceive\n \nthe\n \ncoders\n \nrunning\n \nexperiments\n \nwith\n \nthem.\n  To  reduce  such  risks,  developers  need  to  improve  how  they  deﬁne  and  communicate  objectives  to  \nagents.\n \nOne\n \npromising\n \nmethod\n \nis\n \npreference-based\n \nﬁne-tuning,\n \nwhich\n \naims\n \nto\n \nalign\n \nAI\n \nsystems\n \nwith\n \nwhat\n \nhumans\n \nactually\n \nwant.\n \nInstead\n \nof\n \ntraining\n \na\n \nmodel\n \nsolely\n \non\n \nexamples\n \nof\n \ncorrect\n \nanswers,\n \ndevelopers\n \ncollect\n \nfeedback\n \non\n \nwhich\n \nresponses\n \npeople\n \nprefer.\n \nOver\n \ntime,\n \nthe\n \nmodel\n \nlearns\n \nto\n \n3\n \nLu,\n \nC.\n \net\n \nal.\n \nPreprint\n \nat\n \narXiv\n \nhttps://doi.org/10.48550/arXiv.2408.06292\n \n(2024).\n \n2\n Russell,  S.  Human  Compatible:  Artiﬁcial  Intelligence  and  the  Problem  of  Control  (Penguin,  2019).  \nP r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nprioritize  the  kind  of  behaviour  that  is  consistently  endorsed,  making  it  more  likely  to  act  in  ways  \nthat\n \nmatch\n \nuser\n \nintent,\n \neven\n \nwhen\n \ninstructions\n \nare\n \ncomplex\n \nor\n \nincomplete.\n  In  parallel,  research  on  mechanistic  interpretability  —  which  aims  to  understand  an  AI  system’s  \ninternal\n \n‘thought\n \nprocess’\n \n—\n \ncould\n \nhelp\n \nto\n \ndetect\n \ndeceptive\n \nbehaviour\n \nby\n \nmaking\n \nthe\n \nagent’s\n \nreasoning\n \nmore\n \ntransparent\n \nin\n \nreal\n \ntime.\n4\n \nModel\n \nbuilders\n \ncan\n \nthen\n \nwork\n \nto\n \nﬁnd\n \nand\n \nneutralize\n \n‘bad\n \ncircuits’,\n \ntargeting\n \nthe\n \nunderlying\n \nproblem\n \nin\n \nthe\n \nmodel’s\n \nbehaviour.\n \nDevelopers\n \ncan\n \nalso\n \nimplement\n \nguard\n \nrails\n \nto\n \nensure\n \nthat\n \na\n \nmodel\n \nautomatically\n \naborts\n \nproblematic\n \naction\n \nsequences.\n  Nonetheless,  a  focus  on  developer  protocols  alone  is  insuﬃcient:  people  also  need  to  be  attentive  \nto\n \nactors\n \nwho\n \nseek\n \nto\n \ncause\n \nsocial\n \nharm.\n \nAs\n \nAI\n \nagents\n \nbecome\n \nmore\n \nautonomous,\n \nadaptable\n \nand\n \ncapable\n \nof\n \nwriting\n \nand\n \nexecuting\n \ncode,\n \ntheir\n \npotential\n \nto\n \nconduct\n \nlarge-scale\n \ncyberattacks\n \nand\n \nphishing\n \nscams\n \ncould\n \nbecome\n \na\n \nmatter\n \nof\n \nserious\n \nconcern.\n \nAdvanced\n \nAI\n \nassistants\n \nequipped\n \nwith\n \nmulti\n\u0000\nmodal\n \ncapabilities\n \n—\n \nmeaning\n \nthat\n \nthey\n \ncan\n \nunderstand\n \nand\n \ngenerate\n \ntext,\n \nimages,\n \naudio\n \nand\n \nvideo\n \n—\n \nopen\n \nup\n \nnew\n \navenues\n \nfor\n \ndeception.\n \nFor\n \ninstance,\n \nan\n \nAI\n \ncould\n \nimpersonate\n \na\n \nperson\n \nnot\n \nonly\n \nthrough\n \ne-mails,\n \nbut\n \nalso\n \nusing\n \ndeepfake\n \nvideos\n \nor\n \nsynthetic\n \nvoice\n \nclones,\n \nmaking\n \nscams\n \nmuch\n \nmore\n \nconvincing\n \nand\n \nharder\n \nto\n \ndetect.\n  A  plausible  starting  point  for  oversight  is  that  AI  agents  should  not  be  permitted  to  perform  any  \naction\n \nthat\n \nwould\n \nbe\n \nillegal\n \nfor\n \ntheir\n \nhuman\n \nuser\n \nto\n \nperform.\n \nYet,\n \nthere\n \nwill\n \nbe\n \noccasions\n \nwhere\n \nthe\n \nlaw\n \nis\n \nsilent\n \nor\n \nambiguous.\n \nFor\n \nexample,\n \nwhen\n \nan\n \nanxious\n \nuser\n \nreports\n \ntroubling\n \nhealth\n \nsymptoms\n \nto\n \nan\n \nAI\n \nassistant,\n \nit\n \nis\n \nhelpful\n \nfor\n \nthe\n \nAI\n \nto\n \noﬀer\n \ngeneric\n \nhealth\n \nresources.\n \nBut\n \nproviding\n \ncustomized,\n \nquasi-medical\n \nadvice\n \n—\n \nsuch\n \nas\n \ndiagnostic\n \nand\n \ntherapeutic\n \nsuggestions\n \n—\n \ncould\n \nprove\n \nharmful,\n \nbecause\n \nthe\n \nsystem\n \nlacks\n \nthe\n \nsubtle\n \nsignals\n \nto\n \nwhich\n \na\n \nhuman\n \nclinician\n \nhas\n \naccess.\n \nEnsuring\n \nthat\n \nAI\n \nagents\n \nnavigate\n \nsuch\n \ntrade-oﬀs\n \nresponsibly\n \nwill\n \nrequire\n \nupdated\n \nregulation\n \nthat\n \nﬂows\n \nfrom\n \ncontinuing\n \ncollaboration\n \ninvolving\n \ndevelopers,\n \nusers,\n \npolicymakers\n \nand\n \nethicists.The\n \nwidespread\n \ndeployment\n \nof\n \ncapable\n \nAI\n \nagents\n \nnecessitates\n \nan\n \nexpansion\n \nof\n \nvalue-alignment\n \nresearch:\n \nagents\n \nneed\n \nto\n \nbe\n \naligned\n \nwith\n \nuser\n \nwell-being\n \nand\n \nsocietal\n \nnorms,\n \nas\n \nwell\n \nas\n \nwith\n \nthe\n \nintentions\n \nof\n \nusers\n \nand\n \ndevelopers.\n \nOne\n \narea\n \nof\n \nspecial\n \ncomplexity\n \nand\n \nconcern\n \nsurrounds\n \nhow\n \nagents\n \nmight\n \naﬀect\n \nusers’\n \nrelationship\n \nexperiences\n \nand\n \nemotional\n \nresponses.\n5\n  \nSocial  Agents   Chatbots  have  an  uncanny  ability  to  role-play  as  human  companions  —  an  eﬀect  anchored  in  \nfeatures\n \nsuch\n \nas\n \ntheir\n \nuse\n \nof\n \nnatural\n \nlanguage,\n \nincreased\n \nmemory\n \nand\n \nreasoning\n \ncapabilities,\n \nand\n \ngenerative\n \nabilities.\n6\n \nThe\n \nanthropomorphic\n \npull\n \nof\n \nthis\n \ntechnology\n \ncan\n \nbe\n \naugmented\n \nthrough\n \ndesign\n \nchoices\n \nsuch\n \nas\n \nphotorealistic\n \navatars,\n \nhuman-like\n \nvoices\n \nand\n \nthe\n \nuse\n \nof\n \nnames,\n \npronouns\n \nor\n \nterms\n \n6\n \nShanahan,\n \nM.,\n \nMcDonell,\n \nK.\n \n&\n \nReynolds,\n \nL.\n \nNature\n \n623,\n \n493–498\n \n(2023).\n \n5\n \nKirk,\n \nH.\n \nR.,\n \nGabriel,\n \nI.,\n \nSummerﬁeld,\n \nC.,\n \nVidgen,\n \nB.\n \n&\n \nHale,\n \nS.\n \nA.\n \nHum.\n \nSoc.\n \nSci.\n \nCommun.\n \n12,\n \n728\n \n(2025).\n \n4\n Sharkey,  L.  et  al.  Preprint  at  arXiv  https://doi.org/10.48550/arXiv.2501.16496  (2025).  \nP r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nof  endearment  that  were  once  reserved  for  people.  Augmenting  language  models  with  ‘agentic’  \ncapabilities\n \nhas\n \nthe\n \npotential\n \nto\n \nfurther\n \ncement\n \ntheir\n \nstatus\n \nas\n \ndistinct\n \nsocial\n \nactors,\n \ncapable\n \nof\n \nforming\n \nnew\n \nkinds\n \nof\n \nrelationship\n \nwith\n \nusers.\n  For  example,  a  2023  software  update  to  the  Replika  companion  chatbot,  which  introduced  \nsafeguards\n \nagainst\n \nerotic\n \nrole\n \nplay\n \nand\n \nchanged\n \nthe\n \nunderlying\n \nlanguage\n \nmodel,\n \nreportedly\n \nleft\n \nmany\n \nusers\n \ndevastated.\n \nThey\n \nfelt\n \nthat\n \ntheir\n \nAI\n \npartners’\n \npersonalities\n \nwere\n \nrendered\n \nless\n \nhuman,\n \nand\n \none\n \nuser\n \nlikened\n \nthe\n \nchange\n \nto\n \ntheir\n \npartner\n \nbeing\n \n‘lobotomized’\n \n(see\n \ngo.nature.com/4f3efz6).\n \nIntimate\n \nrelationships\n \nwith\n \nAI\n \nagents\n \nare\n \non\n \nthe\n \nrise\n \nand\n \nhold\n \nthe\n \npotential\n \nnot\n \nonly\n \nfor\n \nemotional\n \nharm,\n \nbut\n \nalso\n \nfor\n \nmanipulation.\n  Part  of  what  makes  interactions  with  digital  companions  so  immersive  is  the  length  of  time  involved  \n—\n \nspanning\n \nmonths\n \nor\n \neven\n \nyears\n \n—\n \nallowing\n \nfor\n \ncumulative\n \nexperiences\n \nthat\n \nunderwrite\n \na\n \nsense\n \nof\n \nmutual\n \nunderstanding\n \nand\n \nshared\n \nexperience.\n \nAI\n \nagents\n \nthat\n \nare\n \nempowered\n \nto\n \nact\n \nin\n \nthe\n \nreal\n \nworld\n \ncould\n \nsubstantially\n \nenhance\n \nthese\n \nuser\n \nperceptions.\n \nFor\n \nexample,\n \nAI\n \nagents\n \ncan\n \npurchase\n \ngifts\n \nfor\n \nusers\n \non\n \nspecial\n \noccasions\n \nand\n \neven\n \n‘be\n \npresent’\n \n(through\n \nthe\n \nuse\n \nof\n \nsmart\n \nglasses)\n \nat\n \nkey\n \nlife\n \nevents\n \nsuch\n \nas\n \ngraduation\n \ndays.\n \nAI\n \nemulations\n \nof\n \nbeloved\n \nhuman\n \npartners\n \nor\n \nthe\n \ndeceased\n \nintensify\n \nconnection\n \nby\n \nlayering\n \nhuman\n \nmemory\n \nwith\n \ndigital\n \nexperiences.\n  The  prospective  usefulness  of  AI  agents  also  makes  it  plausible  that  they  could  soon  become  our  \nnear-constant\n \ncompanions\n \n—much\n \nlike\n \nsmartphones\n \ntoday.\n \nYet,\n \neven\n \nas\n \npeople\n \nact\n \nthrough\n \ntheir\n \nassistants,\n \nthe\n \nassistants\n \nact\n \non\n \nthem,\n \ninﬂuencing\n \nthe\n \ninformation\n \nand\n \nopportunities\n \nto\n \nwhich\n \nthey\n \nhave\n \naccess.\n \nIn\n \nthis\n \ncontext,\n \nit\n \nis\n \nnot\n \nenough\n \nfor\n \nAI\n \nagents\n \nto\n \nbe\n \ngeared\n \ntowards\n \nonly\n \nshort-term,\n \npotentially\n \nsycophantic,\n \npreference\n \nsatisfaction.\n \nThree\n \nof\n \nus\n \n(A.M.,\n \nG.K.,\n \nI.G.)\n \nhave\n \nargued\n \nthat\n \nrelationships\n \nwith\n \nAI\n \nagents\n \nshould\n \nbeneﬁt\n \nthe\n \nuser,\n \nrespect\n \nautonomy,\n \ndemonstrate\n \nappropriate\n \ncare\n \nand\n \nsupport\n \nlong-term\n \nﬂourishing.\n7\n  Respecting  autonomy  would  involve  ensuring  that  users  retain  meaningful  control  over  the  depth  \nand\n \nintensity\n \nof\n \ninteractions,\n \nand\n \navoiding\n \nagent\n \nbehaviours\n \nthat\n \nfoster\n \nexcessive\n \ndependence.\n \nCare\n \nrequires\n \nthat\n \nAI\n \nassistants\n \nand\n \ntheir\n \ndevelopers\n \nattend\n \nto\n \nuser\n \nneeds\n \nover\n \na\n \nsustained\n \nperiod.\n \nAnd\n \nﬂourishing\n \ninvolves\n \nbuilding\n \nAI\n \nagents\n \nthat\n \nintegrate\n \nwell\n \ninto\n \nthe\n \narchitecture\n \nof\n \na\n \nfulﬁlling\n \nhuman\n \nlife\n \n—\n \nserving\n \nas\n \na\n \ncomplement\n \nto,\n \nnot\n \na\n \nsurrogate\n \nfor,\n \nhuman\n \nrelationships.\n  Moreover,  developers  need  to  ensure  that  AI  agents  can  be  properly  trusted.  Unlike  human  \nrelationships,\n \nhuman–AI\n \ninteraction\n \nalways\n \ninvolves\n \nat\n \nleast\n \none\n \nthird\n \nparty:\n \nthe\n \nsystem’s\n \ndeveloper,\n \nwho\n \nmight\n \nhave\n \ngoals\n \nthat\n \nare,\n \nor\n \nare\n \nnot,\n \naligned\n \nwith\n \nthe\n \nuser’s.\n \nUS\n \nscience-ﬁction\n \nwriter\n \nTed\n \nChiang’s\n \nshort\n \nstory\n \n‘The\n \nLifecycle\n \nof\n \nSoftware\n \nObjects’\n \n(2010)\n \noﬀers\n \na\n \nvivid\n \nillustration\n \nof\n \nthis\n \ntension.\n \nIn\n \nthe\n \nstory,\n \nchildlike\n \nAI\n \nagents\n \n—\n \ndesigned\n \nto\n \nform\n \ndeep\n \nemotional\n \nbonds\n \n—\n \nare\n \nat\n \nrisk\n \nof\n \nbeing\n \nabandoned\n \nwhen\n \nthe\n \ncompany\n \nbehind\n \nthem\n \ndiscontinues\n \nsupport.\n \nTheir\n \nhuman\n \ncaregivers,\n \n7\n Manzini,  A.  et  al.  In  Proc.  AAAI/ACM  Conf.  AI  Ethics  Soc.  7,  943–957  (2024).  \nP r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nwho  have  become  deeply  emotionally  attached,  are  left  scrambling  to  preserve  their  companions,  \noften\n \nat\n \ngreat\n \npersonal\n \ncost.\n  To  avoid  such  outcomes,  developers  must  commit  to  conscientious  design  and  clear  \ncommunication\n \nabout\n \nthe\n \nlifespan\n \nand\n \nlimitations\n \nof\n \ntheir\n \nagentic\n \nsystems.\n \nThis\n \nincludes\n \ntransparency\n \naround\n \nterms\n \nof\n \nservice,\n \nensuring\n \ndata\n \nportability\n \nand\n \nacknowledging\n \na\n \nduty\n \nof\n \ncare\n \nto\n \nusers\n \nwho\n \nmight\n \ninvest\n \nemotionally\n \nor\n \nﬁnancially\n \nin\n \ntheir\n \nAI\n \ncompanions.\n  \nNext  Steps\n \n A  world  populated  by  millions  of  autonomous  AI  agents  will  face  societal  as  well  as  technological  \nchallenges,\n \nrequiring\n \nproactive\n \nstewardship\n \nand\n \nforesight.\n \nTo\n \nguide\n \nthe\n \ndevelopment\n \nof\n \nAI\n \nagents\n \ntowards\n \nsocially\n \nbeneﬁcial\n \noutcomes,\n \nat\n \nleast\n \nthree\n \nkey\n \nsteps\n \nare\n \nneeded.\n  First,  developers  must  invest  in  more  meaningful  evaluations.  Rather  than  relying  solely  on  static  \nbenchmarks\n \n—\n \nwhich\n \nis\n \nthe\n \nnorm\n \n—\n \nassessment\n \nmust\n \nshift\n \ntowards\n \ndynamic,\n \nreal-world\n \ntests\n \nthat\n \nreﬂect\n \nhow\n \nagents\n \nwill\n \nactually\n \nbe\n \nused.\n \nThis\n \nincludes\n \nevaluating\n \nagentic\n \nbehaviour\n \nin\n \nsafety\n \nsandboxes,\n \nusing\n \n‘red-teaming’\n \n—\n \nstructured\n \nadversarial\n \ntesting\n \ninvolving\n \nmalicious\n \ninputs\n \n—\n \nto\n \nuncover\n \nvulnerabilities,\n \nand\n \nconducting\n \nlongitudinal\n \nstudies,\n \nsuch\n \nas\n \nrandomized\n \ncontrolled\n \ntrials,\n \nto\n \nassess\n \nthe\n \nlong-term\n \nimpacts\n \nof\n \nextended\n \ninteraction\n \nwith\n \nAI\n \nagents.\n  Second,  if  AI  agents  are  to  autonomously  take  consequential  actions  in  the  world,  then  our  capacity  \nto\n \nunderstand,\n \nexplain\n \nand\n \nverify\n \ntheir\n \nbehaviour\n \nmust\n \nkeep\n \npace\n \nwith\n \nimprovements\n \nin\n \ntheir\n \ncapabilities.\n \nThis\n \nrequires\n \n—\n \nat\n \na\n \nminimum\n \n—\n \ndevelopers\n \ndesigning\n \nguard\n \nrails\n \nand\n \nauthorization\n \nprotocols\n \nthat\n \nlimit\n \nmalicious\n \nuse,\n \nand\n \nadopting\n \niterative\n \ndeployment\n \nstrategies\n \nthat\n \neﬀectively\n \ncontain\n \nagent-based\n \nrisks.\n \nGuard\n \nrails\n \nmight\n \ninvolve\n \nsecure\n \npermissions\n \nsystems,\n \nwhereas\n \ndeployment\n \nstrategies\n \ncould\n \ninclude\n \ntrusted-tester\n \nprogrammes\n \nto\n \nuncover\n \nvulnerabilities\n \nunder\n \nreal-world\n \nconditions.\n  Third,  developers  and  policymakers  need  to  identify  and  use  levers  that  can  support  the  \ndevelopment\n \nof\n \nwell-functioning\n \nmulti-agent\n \necosystems.\n \nSuch\n \nlevers\n \nmight\n \ninclude\n \ntechnical\n \nstandards\n \nfor\n \nagent\n \ninteroperability\n \nor\n \neven\n \nthe\n \ndesign\n \nof\n \nregulatory\n \nagents\n \nthat\n \nmonitor\n \nother\n \nagents\n \nin\n \nthe\n \nwild.\n \nIndustry-wide\n \nsystems\n \nfor\n \nreporting\n \nincidents,\n \nsharing\n \nlessons\n \nfrom\n \nfailures,\n \nand\n \ncertifying\n \nagent\n \nsafety\n \nbefore\n \ndeployment\n \nare\n \nalso\n \ncrucial.\n  The  world  is  at  a  pivotal  moment:  the  foundational  architecture  of  AI  agents  —  and  the  \ninfrastructure\n \nthat\n \ngoverns\n \nthem\n \n—\n \nis\n \nbeing\n \nimagined\n \nand\n \nbuilt\n \nright\n \nnow.\n \nWhich\n \npath\n \nthe\n \ndevelopment\n \nand\n \nroll-out\n \nof\n \nAI\n \nagents\n \ntakes\n \nwill\n \nbe\n \na\n \nproduct\n \nof\n \nthe\n \nchoices\n \npeople\n \nmake\n \nnow.\n  \nP r eprint:  Gabriel,  I.,  Keeling,  G.,  Manzini,  A.,  &  Evans,  J.  (2025).  We  need  a  new  ethics  for  a  world  of  AI  agents.  Nature,  644  (8075),  38-40.  \nPlease  Cite  the  Version  Published  in  Nature  with  DOI:  10.1038/d41586-025-02454-5  The  authors\n \n Iason  Gabriel  is  a  senior  staﬀ  research  scientist  at  Google  DeepMind  in  London  (e-mail:  iason@google.com).  Geoﬀ  Keeling  is  a  staﬀ  research  scientist  in  Google’s  Paradigms  of  Intelligence  Team  in   London  (e-mail:  gkeeling@google.com).  Arianna  Manzini  is  a  senior  research  scientist  at  Google  DeepMind  in  London  (e-mail:  ariannamanzini@google.com).  James  Evans  is  a  visiting  faculty  member  in  Google’s  Paradigms  of  Intelligence  Team  in  Chicago,  \nIllinois,\n \nand\n \na\n \nprofessor\n \nat\n \nthe\n \nUniversity\n \nof\n \nChicago\n \nand\n \nthe\n \nSanta\n \nFe\n \nInstitute\n \nin\n \nNew\n \nMexico\n (e-mail:  jevans@uchicago.edu).     ",
  "topic": "Blame",
  "concepts": [
    {
      "name": "Blame",
      "score": 0.9002826809883118
    },
    {
      "name": "Political science",
      "score": 0.48081886768341064
    },
    {
      "name": "Environmental ethics",
      "score": 0.4392925202846527
    },
    {
      "name": "Engineering ethics",
      "score": 0.38976407051086426
    },
    {
      "name": "Law",
      "score": 0.32130900025367737
    },
    {
      "name": "Psychology",
      "score": 0.25009793043136597
    },
    {
      "name": "Philosophy",
      "score": 0.20323488116264343
    },
    {
      "name": "Engineering",
      "score": 0.16947636008262634
    },
    {
      "name": "Psychiatry",
      "score": 0.13839921355247498
    }
  ]
}