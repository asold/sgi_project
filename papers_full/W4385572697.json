{
  "title": "When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain",
  "url": "https://openalex.org/W4385572697",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2143724091",
      "name": "Raj Shah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2402573852",
      "name": "Kunal Chawla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5026077706",
      "name": "Dheeraj Eidnani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2311082944",
      "name": "Agam Shah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2883119698",
      "name": "Wendi Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160633987",
      "name": "Sudheer Chava",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2487203670",
      "name": "Natraj Raman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172758514",
      "name": "Charese Smiley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2946229205",
      "name": "Jiaao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2611208736",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2888160375",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3203486519",
    "https://openalex.org/W2798658104",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2554619162",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W4287614078",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3034417881",
    "https://openalex.org/W3124135940",
    "https://openalex.org/W2606088639",
    "https://openalex.org/W2126267628",
    "https://openalex.org/W2335156248",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W3194848415",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3124532889",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3098394092",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3124689972",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3121493459",
    "https://openalex.org/W4312847632",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2787423662"
  ],
  "abstract": "Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, Diyi Yang. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2322–2335\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nWHEN FLUE M EETS FLANG: Benchmarks and Large Pre-trained\nLanguage Model for Financial Domain\nRaj Sanjay Shah†, Kunal Chawla†∗, Dheeraj Eidnani†∗, Agam Shah†∗, Wendi Du†\nSudheer Chava†, Natraj Raman♣, Charese Smiley♣, Jiaao Chen†, Diyi Yang♡\n†Georgia Institute of Technology\n♣JPMorgan AI Research\n♡Stanford University\nAbstract\nPre-trained language models have shown im-\npressive performance on a variety of tasks and\ndomains. Previous research on financial lan-\nguage models usually employs a generic train-\ning scheme to train standard model architec-\ntures, without completely leveraging the rich-\nness of the financial data. We propose a novel\ndomain specific Financial LANGuage model\n(FLANG) which uses financial keywords and\nphrases for better masking, together with span\nboundary objective and in-filing objective. Ad-\nditionally, the evaluation benchmarks in the\nfield have been limited. To this end, we con-\ntribute the Financial Language Understanding\nEvaluation (FLUE), an open-source compre-\nhensive suite of benchmarks for the financial\ndomain. These include new benchmarks across\n5 NLP tasks in financial domain as well as com-\nmon benchmarks used in the previous research.\nExperiments on these benchmarks suggest that\nour model outperforms those in prior literature\non a variety of NLP tasks. Our models, code\nand benchmark data are publicly available on\nGithub and Huggingface1\n1 Introduction\nEfficient financial markets incorporate all price rel-\nevant information available to investors at that point\nof time. Unstructured data, such as textual data,\nhelp complement structured data traditionally used\nby investors. For example, in addition to quanti-\ntative data such as firm’s financial performance,\nthe tone and sentiment of firms’ financial reports,\nearnings calls and social media posts can also in-\nfluence the stock price movement (Bochkay et al.,\nEmail IDs of the authors: {rajsanjayshah, ku-\nnalchawla, deidnani, ashah482, wendi.du, schava6,\njchen896}@gatech.edu, natraj.raman@jpmorgan.com,\ncharese.h.smiley@jpmchase.com, diyiy@cs.stanford.edu\n* These authors contributed equally to this work\n1The website can be found at https://salt-\nnlp.github.io/FLANG/. All the FLANG models are\navailable on the Huggingface SALT-NLP site.\n2020). We aim to capture these textual features\nwith the help of pre-trained deep learning mod-\nels, which have shown superior performance in\na variety of Natural Language Processing (NLP)\ntasks (Radford et al., 2019; Devlin et al., 2018; Liu\net al., 2019; Lewis et al., 2020). However, the lan-\nguage used in finance and economics is likely to\nbe different from the language of common usage.\nA statement like “The crude oil prices are going\nup” has a negative sentiment for the financial mar-\nkets, but it does not contain traditionally negative\nwords such as danger, hate, fear, etc. (Loughran\nand McDonald, 2011). Therefore, it is necessary to\ndevelop a domain-specific language model training\nmethodology that improves the performance in the\ndownstream NLP tasks like managers’ sentiment\nanalysis and financial news classification.\nPrevious research, for example, Yang et al.\n(2020); Araci (2019) have pre-trained the state-of-\nthe-art language models like BERT (Devlin et al.,\n2018) with financial documents, but suffer from\ntwo major limitations. First, financial domain\nknowledge and adaptation are not utilized in the\npre-training process. We argue that the financial\nterminologies play a critical role in understanding\nthe language used in financial markets, and expect\na performance improvement after incorporating the\nfinancial domain knowledge into the pre-training\nprocess. Second, the lack of different evaluation\nbenchmarks limit the test the language models’ per-\nformance in finance-related tasks.\nIn this work, we propose a simple yet effec-\ntive language model pre-training methodology\nwith preferential token masking and prediction of\nphrases. This helps capture the fact that many\nfinancial terms are actually multi-token phrases,\nsuch as margin call and break-even analysis. We\ncontribute and make public two language models\ntrained using this technique. Financial LANGuage\nModel (FLANG-BERT) is based on BERT-base archi-\ntecture (Devlin et al., 2018), which has a relatively\n2322\nFigure 1: Architecture of our model. We use finance specific datasets and general English datasets (Wikpedia and\nBooksCorpus) for training the model. We follow the training strategy of ELECTRA (Clark et al., 2020) with span\nboundary task which first predicts masked tokens using language model and then uses a discriminator to assess if a\ntoken is original or replaced. The generator and discriminator are trained end-to-end, and both words and phrases\nfrom financial vocabulary are used for masking. The final discriminator is then fine-tuned on individual tasks on our\ncontributed benchmark suite, Financial Language Understanding Evaluation (FLUE). Note that our method is not\nspecific to ELECTRA and can be generalized to other models.\nsmall memory footprint and inference time. It\nalso enables comparison with previous works, most\nof which are based on BERT. We also contribute\nFLANG-ELECTRA, our best performing model, based\non the ELECTRA-base architecture (Clark et al.,\n2020), where we introduce a span boundary ob-\njective on the ELECTRA generator pre-training\ntask to learn robust financial multi-word represen-\ntations while masking contiguous spans of text. We\nshow that FLANG-BERT outperforms all previous\nworks in nearly all our benchmarks, and FLANG-\nELECTRA further improves the performance giv-\ning two new state-of-the-art models. Our training\nmethodology can be extended to other domains that\nwould benefit from domain adaptation.\nFinancial domain benchmarks are critical to eval-\nuate the newly developed financial language mod-\nels. Inspired by GLUE (Wang et al., 2018), a\nset of comprehensive benchmarks across multiple\nNLP tasks, we construct Financial Language Un-\nderstanding Evaluation (FLUE) benchmarks. FLUE\nconsists of 5 financial domain tasks: financial sen-\ntiment analysis, news headline classification, name\nentity recognition, structure boundary detection,\nand question answering. We intend for this bench-\nmark suite to be a standard for evaluation of natural\nlanguage tasks in financial domain, subject to ap-\npropriate license and privacy considerations. All\nproposed benchmarks will be made publicly avail-\nable on Github and Huggingface.\nOur contributions are as follows:\n• We propose masking finance-specific words\nand phrase masking for pre-training language\nmodel, as well as a span boundary objective\nto build robust multi-word representations.\n• We contribute finance-related benchmarks\nwith 5 NLP tasks: financial sentiment anal-\nysis, news headline classification, named en-\ntity recognition, structure boundary detection,\nquestion answering. This results in a com-\nprehensive suite of finance benchmarks, with\nlicensing details in Table 1.\n• We make all our models and code publicly\navailable, for easier development and further\nresearch by the NLP and Finance community.\nSpecifically, we contribute FLANG-BERT\nand FLANG-ELECTRA language models,\nand all the benchmarks in FLUE.\n2 Related Work\nPre-trained language models Language models\npre-trained on unlabeled textual data, such as BERT\n(Devlin et al., 2018), ELMo (Peters et al., 2018)\nand ROBERTA (Liu et al., 2019), have significantly\nimproved the state-of-the-art in many natural lan-\nguage tasks. Newer models introduce different\ntraining objectives: BART (Lewis et al., 2020) uses\ndenoising auto-encoder objective for sequence-to-\nsequence pre-training; Span-BERT (Joshi et al.,\n2019) uses a pre-training methodology that pre-\ndicts spans of text; ELECTRA (Clark et al., 2020)\nuses token detection for training, where it corrupts\nsome tokens using a generator network and predicts\nif the tokens are corrupted using a discriminator.\nMasked Language Modeling Most language\nmodels use Masked Language Modeling (MLM)\n(Devlin et al., 2018) as a training objective. It\ntypically involves randomly masking a percentage\nof tokens in a text, and using surrounding text to\n2323\nName Task Source Dataset Size Metric License EthicalRisksTrain Valid TestFPB Sentiment Classification (Malo et al., 2014) 3488 388 969 Accuracy CC BY-SA 3.0 LowFiQA SA Sentiment Analysis (FiQA)FiQA 2018 822 117 234 MSE Public LowHeadline News Headlines Classification (Sinha and Khandait, 2020) 7,989 1,141 2,282 Avg F-1 score CC BY-SA 3.0 LowNER Named Entity Recognition (Alvarado et al., 2015) 932 232 302 F-1 score CC BY-SA 3.0 LowFinSBD3 Structure Boundary Detection (FinSBD3, 2021)FinWeb-2021 460 165 131 F-1 score CC BY-SA 3.0 LowFiQA QA Question Answering (FiQA)FiQA 2018 5676 631 333 nDCG, MRR Public Low\nTable 1: Summary of benchmarks in FLUE. Dataset size denotes the number of samples in the benchmark. Metric\ndenotes the evaluation metric used. Here MSE denotes Mean Squared Error, nDCG denotes Normalized Discounted\nCumulative Gain and MRR denotes Mean Reciprocal Rank.\npredict the masked tokens. A variety of masking\ntechniques have been used for domain-specific pre-\ntraining. While some works (Glass et al., 2020; Sun\net al., 2019b) propose rule based masking strategies\nthat work better than random masking, other works\n(Kang et al., 2020) attempt to find optimal mask-\ning policy automatically using techniques such as\nreinforcement learning.\nDomain-specific Language Models While the\nmodels trained on general English language per-\nform well, domain-specific pre-training can fur-\nther increase the performance on a particular do-\nmain of text (Sun et al., 2019a; Gururangan et al.,\n2020). For example, BioBERT (Lee et al., 2019)\non biomedical domain, ClinicalBERT (Alsentzer\net al., 2019) on clinical domain, SciBERT (Beltagy\net al., 2019) on scientific publications domain, etc.\nThere have been some works on financial domain\nas well: previous works by Araci (2019); Yang\net al. (2020) directly fine-tune BERT trained on fi-\nnancial corpus for sentiment analysis and question\nanswering tasks respectively. FinBERT (Liu et al.,\n2020) uses multi-task pre-training to improve per-\nformance. The previous works in financial domain\nrely on basic architectures/ training schemes and do\nnot use finance-specific knowledge. Furthermore,\nFinBERT is pre-trained with the objective of opti-\nmizing performance for sentiment analysis, while\nwe build a generalizable model performing well\non a diverse set of tasks. We use and demonstrate\nthat finance specific knowledge and vocabulary can\nfurther improve the performance of the model.\nFinance Benchmarks Wang et al. (2018) created\nGeneral Language Understudy Evaluation(GLUE),\na collection of benchmark tasks for training, evalu-\nating, and analyzing language model designed for\nnon-domain specific tasks. For financial domain,\nthe benchmark suite isn’t as exhaustive. Malo\net al. (2014) created Financial PhraseBank dataset\nfor Sentiment analysis classification. Maia et al.\n(2018) created two tasks in (FiQA)FiQA 2018:\nTask-1 for Sentiment Analysis Regression and Task\n2 dataset for Question Answering task in finance.\nOther datasets include gold news headline datase\n(Sinha and Khandait, 2020), financial NER (Al-\nvarado et al., 2015) and Structure Boundary Detec-\ntion (FinSBD3, 2021). Recent financial language\nmodels (Araci, 2019; Yang et al., 2020) evaluate\ntheir efficacy only on sentiment analysis tasks. We\nuse datasets from existing literature and create a\nset of heterogeneous benchmark tasks FLUE (Fi-\nnancial Language Understanding Evaluation) for\nbetter comprehensive evaluation.\n3 Benchmarks (FLUE) and Datasets\n3.1 FLUE\nWe introduce Financial Language Understanding\nEvaluation (FLUE), a set of comprehensive bench-\nmarks across 5 financial tasks. The statistics for\nFLUE are summarized in Table 1 along with the\nlicensing details for public use. All FLUE bench-\nmark datasets have low ethical risks and do not\nexpose any sensitive information of any organiza-\ntion/ individual. Additionally, we have obtained\napproval for the authors of each dataset for this\nFLUE benchmark.\n3.1.1 Financial Sentiment Analysis\nServing as a fundamental task for textual analy-\nsis, this task received a lot of attention in finance\ndomain (Loughran and McDonald, 2011; Garcia,\n2013). In our FLUE benchmark, we include both\nsentiment analysis tasks: regression and classifica-\ntion. For classification, we use Financial Phrase-\nBank dataset (Malo et al., 2014) which provides the\nsentiment labels annotated by humans for financial\nnews sequences. For regression, we use FiQA 2018\ntask-1 (Aspect-based financial sentiment analysis)\n2324\ndataset (Maia et al., 2018), which contains both\nheadlines and microblogs.\n3.1.2 News Headline Classification\nThe financial phrases contain information on multi-\nple dimensions other than the sentiment. Financial\nnews headlines contain important time sensitive in-\nformation on price changes. To explore our model\non those dimensions, we use the Gold news head-\nline dataset created by Sinha and Khandait (2020).\nThe dataset is a collection of 11,412 news head-\nlines, with 9 binary labels.\n3.1.3 Named Entity Recognition\nName entity recognition (NER) is key task to\nanalysing any financial text as it can be used along\nwith the Knowledge Graphs to better understand in-\nterdependence of different financial entities linked\nthrough location, organisation and person. Given\na text, NER can identify and classify tokens into\nspecified categories such as person, organisation,\nlocation and miscellaneous. We use dataset re-\nleased by Alvarado et al. (2015) for NER task on\nfinancial domain text.\n3.1.4 Structure Boundary Detection\nBoundary detection of different structure is funda-\nmental challenge in processing text data. Here we\nemploy the dataset shared in the task FinSBD-3\nof (FinSBD3, 2021)FinWeb-2021 workshop. The\ngoal of the task is to find the boundaries of different\ncomponents of text (sentences, lists and list items,\nincluding structure elements like footer, header, ta-\nbles). We chose this dataset as it not only identifies\nboundaries of sentences but also identifies bound-\naries of other structural elements.\n3.1.5 Question Answering\nQuestion answering system which can answer the\nfinance domain question is essential to any digital\nassistant. To evaluate our language model’s ability\non QA task we employ the dataset (\"Opinion-based\nQA over financial data\") released in (FiQA) FiQA\n2018 open challenge Task 2 (Maia et al., 2018).\n3.2 Pre-training Datasets\nFor pre-training, we use a mix of general English\nlanguage datasets and finance specific datasets. For\nEnglish, we use BooksCorpus (Zhu et al., 2015)\n(800M words) and English Wikipedia (2500M\nwords). For the domain specific datasets, we use\nsix publicly available datasets, they are: 1) SEC\n10-K and 10-Q financial reports, 2) Earning Con-\nference calls, 3) Analyst Reports, 4) Reuters Fi-\nnancial News, 5) Bloomberg Financial News, and\n6) Investopedia. The details for these datasets are\nsummarized in Table 13 and a brief description of\neach dataset is given in the Appendix Section 7.1.\n4 Model\nFor FLANG-BERT, we add financial word and phrase\nmasking, while for FLANG-ELECTRA, we also add a\nspan boundary objective. The addition of financial\nword and phrasal masking is model agnostic and\ncan be used for any model with a generator.\n4.1 Financial Word Masking\nPrevious works (Liu et al., 2020; Yang et al., 2020;\nAraci, 2019) on financial language modeling use\nMLM objective for pre-training, which masks some\ntokens randomly and uses the prediction of those\ntokens as a training objective. However, there\nis empirical evidence (Sun et al., 2019b; Kang\net al., 2020; Glass et al., 2020) that masking some\nwords strategically which carry more information\nimproves performance on downstream tasks.\nHence, we propose masking financial words\npreferentially. To this end, we use Investopedia\nFinancial Term Dictionary (Investopedia) to cre-\nate a comprehensive financial dictionary, which\nlists the commonly used technical terms in finan-\ncial markets and literature. We expand our list by\nadding words/phrases from other financial vocabu-\nlary lists available online (V ocabulary.com; MyV o-\ncabulary.com; TheStreet).\nOur dictionary contains more than 8200 words\nand phrases. For preferential masking, we mask the\nsingle word financial tokens with a probability 30%\nand randomly mask other tokens with 70% per-\ncent probability. Like original BERT pre-training\nscheme, we mask a cumulative total of 15% of all\ntokens, such that the total number of tokens being\nmasked in each round is same as the original BERT\npre-training approach. Table 10 shows that mask-\ning financial terms with a 30% probability gives\nthe lowest perplexity score when pre-training either\nBERT and ELECTRA with additional vocabulary.\n4.2 Phrase Masking\nMany financial terms are phrases with multiple to-\nkens. It has been shown (Sun et al., 2019b; Joshi\net al., 2019) that masking phrases instead of words\ncould leads to better learning of the phrase content.\n2325\nModel FPB FiQA SA Headline NER FinSBD3 FiQA QA\nMetric Accuracy MSE Mean F-1 F-1 F-1 nDCG\nBERT-base 0.856 0.073 0.967 0.79 0.95 0.46\nFinBERT (Yang et al., 2020) 0.872 0.070 0.968 0.80 0.89 0.42\nFLANG-BERT(ours) 0.912 0.054 0.972 0.83 0.96 0.51\nELECTRA 0.881 0.066 0.966 0.78 0.94 0.52\nFLANG-ELECTRA(ours) 0.919 0.034 0.98 0.82 0.97 0.55\nTable 2: Summary of results of our models and baselines on benchmarks. FLANG (Financial Language Model)\ndenotes our final model. Average of 3 seeds was used for each model and benchmark.\nModel MSE R2\nSC-V (Yang et al., 2018) 0.080 0.40\nRCNN (Piao and Breslin, 2018) 0.090 0.41\nBERT 0.074 0.59\nFinBERT 0.070 0.57\nFLANG-BERT 0.052 0.67\nELECTRA 0.046 0.72\nFLANG-ELECTRA 0.039 0.77\nTable 3: Resuls on FiQA Sentiment Regression.\nModel F-1 Scores\nMulti-token No Yes\nCRFs 0.83\nBERT 0.805 0.788\nFinBERT 0.795 0.800\nFLANG-BERT 0.836 0.831\nELECTRA 0.797 0.777\nFLANG-ELECTRA0.822 0.818\nTable 4: Results on Named Entity Recognition. Yes: Set\nother tokens in word to same label. CRF result is taken\nfrom (Alvarado et al., 2015), but they don’t specify that\nwhether they set other tokens in word to same label.\nBuilding on that, we use phrase-based masking in\nthe language model. We perform a two-phase train-\ning: in the first phase, we only use word masking to\nmask single tokens and train the language model;\nin the second phase, we add phrase masking.\nFor a financial term of token length n, we mask\nit with a probability of 30%. We replace all tokens\nin a financial phrase with a single [MASK] token.\nWe add all the financial phrases in the model vocab-\nulary and predict the phrase with the usual masked\nlanguage modeling objective.\n4.3 Span Boundary Objective\nWe add the Span Boundary Objective to the loss\nfunction along with the MLM loss in the pre-\ntraining stage, in addition to the word and the\nphrasal level masking and the modified vocabulary.\nOur final loss has three parts:\nMasked Language Modeling Loss is the Maxi-\nmum Likelihood Loss of the ELECTRA generator\nModel Accuracy %∆MP\nBERT 85.6\nFinBERT 87.2\nFLANG-BERT 91.2 31.25\nELECTRA 88.1 7.03\nw/ AD 91.1 30.47\nw/ AD + PFV 91.4 32.81\nw/ AD + PFV + SBO 91.9 36.71\nw/ AD + PFV + SBO + SCL92.1 38.28\nTable 5: Results on Financial Phrase Bank Sentiment\nClassification Dataset (Malo et al., 2014). Accuracy is\ngiven as a percentage. Average of 3 seeds was used for\nall models. Marginal increase in performance is calcu-\nlated for FLANG-ELECTRA with respect to FinBERT.\nFV means using Financial V ocabulary for masking, PFV\nmeans using both words and phrases in the financial dic-\ntionary for multi-stage masking in the pre-training task,\nSCL means the use of Supervised Contrastive Learning\nduring the fine-tuning stage.\n(G). We also modify the token masking to randomly\nmask contiguous spans from a geometric distribu-\ntion of length L∼Geo(p), which is skewed towards\nsmaller spans. We follow the results of Joshi et al.\n(2019) and set p = 0.2.\nLMLM (x,θG) = E(\n∑\ni∈masks\n−log(PG(xi|xmasked))\nDiscriminator loss This loss term is the standard\nELECTRA implementation. LDisc penalizes if the\ndiscriminator detects a token generated by the gen-\nerator as replaced when it is a non-corrupt token\nor if the token generated by G is corrupt and the\ndiscriminator detects it as original.\nSpan Boundary Objective This term penalizes\nthe low probability of a token being generated\ngiven span boundaries (the the representations\nof tokens present before and after the masked\ncontiguous span). The position of the left boundary\ntoken is xstart−1 and the position of the right\nboundary token is xend+1. By looking at words\nbefore and after spans and then trying to generate\nthe tokens in the span, this term helps the model to\n2326\nCategory SVM BERT FinBERT FLANG-BERTELECTRA FLANG-ELECTRA%∆MP\nPrice or Not 0.965 0.955 0.956 0.960 0.951 0.964 18.18\nPrice Up 0.924 0.939 0.945 0.951 0.946 0.964 34.54\nPrice Constant 0.715 0.980 0.978 0.981 0.977 0.987 40.90\nPrice Down 0.932 0.950 0.958 0.965 0.959 0.974 38.09\nPast Price 0.965 0.947 0.952 0.955 0.943 0.975 47.91\nFuture Price 0.732 0.987 0.985 0.988 0.984 0.988 20.00\nPast News - 0.950 0.951 0.952 0.945 0.956 10.20\nFuture News - 0.989 0.993 0.993 0.991 0.994 14.28\nAsset Comparison0.994 0.998 0.998 0.999 0.996 0.998 0\nMean F-1 Score 0.890(¯7) 0.967 0.968 0.973 0.966 0.978 31.25\nTable 6: Results on News Headline Classification. SVM results are taken from (Sinha and Khandait, 2020). All\nvalues are F1 scores. FLANG denotes our model. Average of 3 seeds was used for all models. FLANG-ELECTRA\nalso uses Supervised Contrastive Learning while fine-tuning. Marginal increase in performance is calculated for\nFLANG-ELECTRA with respect to FinBERT.\nModel F-1 Scores\nSpecial tokens No Yes\nBERT 0.950 0.948\nFinBERT 0.872 0.890\nFLANG-BERT 0.964 0.958\nELECTRA 0.938 0.968\nFLANG-ELECTRA 0.966* 0.967*\nTable 7: Results on Structure Boundary Detection. *in-\ndicates the best model when the combined F1 score of\nboth special tokens is considered. Yes and No are addi-\ntional special tokens. Average of 3 seeds was used.\nbuild multi-word representations of financial terms\nthat are not captured in our vocabulary.\nLSBO (x,θG) = E(\n∑\ni∈masks\n−log(PG(xi|yi)))\nwhere yi = f(xstart−1,xend+1,posi−start+1)\nHere the function f(c˙) is the representation func-\ntion for the ith token in the span and is defined by\ntwo feed forward layers:\nyi = LayerNorm(GELU(w2 ∗h1)\nwhere h1 = LayerNorm(Gelu(w1 ∗h0))\nand h0 = [xstart−1,xend+1,posi]\nOur model is then pre-trained and optimized based\non this combined loss function.\nTotal Loss = LMLM (x,θG) + λ1LSBO (x,θG)\n+λ2LDisc(x,θD)\nModel nDCG MRR Precision\nBERT 0.46 0.42 0.35\nFinBERT 0.42 0.37 0.29\nFLANG-BERT 0.51 0.46 0.36\nSpanBERT + AD + FV + PFV0.57 0.54 0.50\nELECTRA 0.52 0.49 0.43\nFLANG-ELECTRA 0.55 0.51 0.45\nTable 8: Results on Question Answering benchmark.\nAverage of 3 seeds was used for all models.\n4.4 Contrastive Loss for Fine-tuning\nWhile most language models are fine-tuned for\nsupervised classification by using cross-entropy\nloss (Devlin et al., 2018; Liu et al., 2019; Rad-\nford et al., 2019), we use additional supervised\ncontrastive learning loss for fine-tuning for clas-\nsification (Gunel et al., 2021). This loss function\ncaptures the similarities between examples of the\nsame class and contrasts them with the examples\nfrom other classes. Details about Supervised Con-\ntrastive Loss are given in Appendix Section 7.3.\nHere, we only add this loss to the fine-tuning of\nFinancial Phrasebank Dataset and the Headlines\nDataset as shown in Tables 5 and 6.\n5 Experiments\n5.1 Experiment Setup\nAll experiments were conducted with PyTorch\n(Paszke et al., 2019) on NVIDIA V100 GPUs. We\ninitialized each model with their respective pre-\ntrained version on the Huggingface’s Transformers\nlibrary (Wolf et al., 2020). We further pre-trained\neach model for 4 more epochs on the training data.\nWe used 2 epochs with only single token masking\nand the later 2 epochs for both word and phrase\nmasking. Using this multi-stage setup gives the\nlowest model perplexity as shown in Table 11.\nWe used ELECTRA-base pre-trained model as\n2327\nour base architecture. ELECTRA corrupts the in-\nput by replacing tokens with words sampled from\na generator and trains a discriminator model that\npredicts whether each token in the corrupted input\nwas replaced by a generator sample. This enables\nit to learn from all input tokens rather than just\nmasked out tokens and is a good fit for our prefer-\nential masking approach. We compare our results\nthe following models:\n• BERT-base and ELECTRA-base: We use the\nBERT-base model (Devlin et al., 2018) and\nthe ELECTRA-base model (Clark et al., 2020)\nfrom Huggingface (Wolf et al., 2020) and fine-\ntuned it directly for our tasks.\n• finBERT (Yang et al., 2020): We used fin-\nBERT model and fine-tune on our tasks.\n• FLANG-BERT (ours)(Financial LANGuage\nModel based on BERT): For direct compari-\nson with finBERT, we use our method to train\na BERT-base model on our training corpus\nin a multi-stage manner (Table 11), masking\nsingle tokens from financial vocabulary in the\nfirst stage and then masking both words and\nphrases in the second stage.\n• ELECTRA w/ AD (Additional Data): The\nELECTRA base model pre-trained on our fi-\nnancial training corpus.\n• ELECTRA w/ AD + FV (Financial V ocab-\nulary): The ELECTRA Base model is pre-\ntrained on our training corpus, while masking\nsingle tokens from financial vocabulary with\na higher probability.\n• ELECTRA w/ AD + PFV (Phrase Financial\nV ocabulary). The ELECTRA Base model pre-\ntrained on our training corpus in a multi-stage\nmanner (Table 11), masking only single-word\ntokens from financial vocabulary in the first\nstage and masking both words and phrases in\nthe second stage.\n• FLANG-ELECTRA (Financial LANGuage\nModel based on ELECTRA): ELECTRA w/\nAD + PFV (Phrase Financial V ocabulary) +\nSBO (Span Boundary Objective). It is pre-\ntrained on our training corpus in the described\nmulti-stage manner with the span boundary\nand in-filling training objective.\n• ELECTRA w/ AD + PFV + SBO + SCL\n(Contrastive Loss): We use our final language\nmodel (FLANG-ELECTRA) but add a con-\ntrastive loss term to fine-tune on supervised\nclassification tasks.\n5.2 Benchmark Results\nSummarized results on all benchmarks of our\nmodel and baselines are shown in Table 2.\n5.2.1 FPB Sentiment Classification\nThe results of sentiment classification on Financial\nPhrase Bank sentiment dataset are shown in Table 2.\nFrom the accuracy numbers listed in the Table 2, it\nis evident that FLANG-BERT improves hugely on\nperformance of FinBERT and our final language\nmodel(FLANG-ELECTRA) significantly outper-\nforms all the baseline models on the sentiment clas-\nsification task on the Financial Phrase Bank dataset,\nachieving state of the art results. Results in Table\n5 highlight the importance of each step in our ex-\nperiment setup described in Section 5.1. As the\nprevious state of art performance on this dataset\nis already in the higher 80s, we use an additional\nmetric: marginal increase in performance over Fin-\nBERT (∆MP) to demonstrate our techniques. We\ncalculate (∆MP) as given in equation 1:\n∆MP = MetricModel −MetricFinBERT\n1 −MetricFinBERT\n(1)\nwhere the Metric is Accuracy for the Financial\nPhrasebank Dataset and is F1 score for News Head-\nlines Dataset.\n5.2.2 FiQA Sentiment Regression\nThe results of sentiment regression analysis on\nthe FiQA dataset are shown in Table 3. Evalua-\ntion of models is done on two regression evalu-\nation measures Mean Squared Error (MSE) and\nR Square (R2). Our transformer based architec-\ntures outperform conventional techniques like SC-\nV and RCNN. FLANG-BERT model achieves sig-\nnificant improvement on both BERT and finBERT\nand FLANG-ELECTRA outperforms all models\nand achieves state of art result for the sentiment\nregression analysis task on the FIQA dataset.\n5.2.3 News Headline Classification\nThe results of news headline classification for 9\nbinary classification tasks on Gold headline dataset\nare shown in Table 6. All the deep learning based\nlanguage models perform much better than Support\nVector Machines. Our ELECTRA-based language\nmodel (FLANG-ELECTRA) achieves the highest\n2328\nModel FBP Headline NER FinSBD3 FIQA SA FIQA QA\nMetric Accuracy Mean F-1F-1 F-1 MSE nDCG\nBERT 0.856 0.967 0.79 0.949 0.073 0.46\nBERT + AD 0.902 0.968 0.811 0.954 0.058 0.47\nBERT + AD + FV + PFV (FLANG-BERT) 0.912 0.972 0.834 0.962 0.054 0.51\nDistilbert 0.844 0.963 0.776 0.934 0.075 0.45\nDistilbert + AD 0.898 0.965 0.806 0.944 0.064 0.46\nDistilbert + AD + FV + PFV 0.901 0.965 0.812 0.958 0.057 0.49\nSpanBERT 0.852 0.962 0.774 0.935 0.078 0.53\nSpanBERT + AD 0.901 0.962 0.789 0.951 0.063 0.55\nSpanBERT + AD + FV + PFV 0.904 0.969 0.792 0.959 0.056 0.57\nELECTRA 0.881 0.966 0.782 0.954 0.066 0.52\nELECTRA + AD 0.911 0.973 0.803 0.959 0.052 0.53\nELECTRA + AD + FV + PFV 0.914 0.977 0.825 0.962 0.038 0.55\nELECTRA + AD + FV + PFV + SBO (FLANG-ELECTRA)0.919 0.978 0.816 0.967 0.034 0.56\nTable 9: Ablation Studies: Average of three seeds were used for each model and benchmark\nModel Perplexities BERT ELECTRA\n% of Financial\nTerms Masked\nFV PFV FV PFV\n10 23.02 22.88 19.10 18.96\n20 21.45 21.30 18.44 18.42\n30 20.29 19.53 17.87 17.52\n40 20.80 20.11 18.67 17.98\nTable 10: Model Perplexities when different percent-\nages of Financial terms are masked. FV means using\nFinancial V ocabulary for masking, PFV means using\nboth words and phrases in the financial dictionary for\nmulti-stage masking in the pre-training task.\nmean F-1 score compared to other language models.\nFLANG-BERT performs better than BERT, which\nagain highlights the importance of our setup.\n5.2.4 Named Entity Recognition\nThe results of NER on financial NER dataset pro-\nvided by (Alvarado et al., 2015) are shown in Ta-\nble 4. The margin of improvement is more muted\nin this benchmark. Our models outperform the\nbaselines in a multi-token setting. The multi-token\nsetting refers to all tokens in a word being set to\nthe same label when a word is split into multiple\ntokens, instead of only labeling the first token and\nignoring the rest. Our hypothesis is that when the\ntask doesn’t require domain specific knowledge,\nlike NER, pre-training language model on domain\nspecific data does not help.\n5.2.5 Structure Boundary Detection\nThe results of structure boundary detection task on\nFinSBD3 dataset from (FinSBD3, 2021)FinWeb-\n2021 are shown in Table 7. In this table, note that\nthe \"Special Tokens\" setting refers to adding spe-\ncial tokens that are commonly used by pre-trained\ntransformers such as [CLS] to the input. Our mod-\nNumber of Epochs Model Perplexity\nFV FV + PFV BERT ELECTRA\n4 0 20.29 17.87\n3 1 20.11 17.82\n2 2 19.53 17.52\n1 3 20.13 17.80\n0 4 20.05 17.69\nTable 11: Model Perplexities when using multi-stage\nfinancial term masking for pre-training. FV means using\nFinancial V ocabulary for masking, PFV means using\nboth words and phrases in the financial dictionary for\nmulti-stage masking.\nels perform similarly or slightly better to baseline\narchitectures. This could be because SBD, like\nNER, relies more on language cues rather than fi-\nnance keywords for inference and further gives evi-\ndence to the hypothesis that when the task doesn’t\nrequire domain specific knowledge, one should not\nget improvement by pre-training a language model\non domain specific data. However, our model still\nperforms significantly better than FinBERT.\n5.2.6 Question Answering\nOn Question-Answering, our models outperform\nthe previous works, as shown in Table 8. For evalu-\nation, we compare the following metrics (Michael\nand Joseph): Precision, nDCG—A higher value\nmeans that more relevant documents are retrieved\nfirst, and MRR—A higher value means that the first\nrelevant item is retrieved earlier. FLANG-BERT,\nFLANG-ELECTRA outperform other models on\nall metrics by a huge margin, but do not outperform\nSpanBERT pre-trained with Additional Data.\n5.3 Ablation Studies\nWe conduct multiple ablation studies to understand\nthe individual impact of our techniques on perfor-\n2329\nModel Perplexity Size\nBERT-base 23.66 110M\nFinBERT 21.11 110M\nFLANG-BERT 19.53 110M\nElectra 20.10 110M\nw/ AD 19.20 110M\nw/ AD + FV 17.87 110M\nw/ AD + PFV 17.52 110M\nw/ AD + PFV + SBO 17.34 110M\nTable 12: Comparison of perplexity of our model and\nbaselines. The model size is given in terms of number of\nparameters, and perplexity is averaged over all sentences\nin the validation dataset. Average of 3 runs was used\nfor perplexity numbers. Here AD means Additional\nfinancial data, FV means using Financial V ocabulary\nfor masking, PFV means using both words and phrases\nin the financial dictionary for multi-stage masking, and\nSBO means using the span boundary objective in the\npre-training task.\nmance. Our studies in Table 10 show that preferen-\ntially masking 30% of the financial tokens gives the\nleast perplexity for each model. Furthermore, we\nfind that using single-word financial terminologies\nin the first two pre-training epochs and multi-word\nterminologies in the next two gives the lowest per-\nplexity score (Table 11). Table 9 shows that the use\nof additional data and domain specific preferential\nmasking give substantial increase in performance\nfor our FLUE tasks. Addition of the Span Bound-\nary Objective on the ELECTRA generator gives\nthe best performing model when compared to other\nsimilar encoder based architectures like SpanBERT,\nDistilBERT and BERT. In Table 12, we also show\nthat pre-training models using our methodology\ngives the lowest perplexity scores when compared\nto prior baselines. The details for the studies can\nbe found in Table 9 and Appendix Section 7.2.\n5.4 Discussion\nIn conclusion, both FLANG-ELECTRA and\nFLANG-BERT outperform the base architectures\n(ELECTRA and BERT, respectively). FLANG-\nBERT also outperforms FinBERT on all the bench-\nmarks, with the same number of parameters. Addi-\ntionally, on relatively domain-agnostic tasks such\nas Named Entity Recognition, the improvements\nare muted. The performance is hugely improved\nin tasks which utilize finance specific language,\nsuch as sentiment analysis, sentence classification\nand question answering. Overall, the dramatic im-\nprovement in most benchmarks suggests that our\ntechnique yields state-of-the-art financial language\nmodels. We also note that our vocabulary based\npreferential masking training methodology is both\narchitecture and domain independent and can be\ngeneralized to other language models and domains.\n6 Conclusion\nWe contribute two language models in the fi-\nnance domain, which use domain-specific word\nand phrase masking as a pre-training objective. Ad-\nditionally, we contribute a comprehensive suite of\nbenchmarks in finance domain across 5 natural\nlanguage tasks, including new benchmarks using\npublic sources. Our language model outperforms\nprevious language models on all the benchmarks.\nWe will release our models, code and benchmark\ndata on acceptance. We also note that our method\nis not specific to finance and can be used for any\ndomain-specific language model training.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their comments. We appreciate the generous\nsupport of Azure credits from Microsoft made avail-\nable for this research via the Georgia institute of\nTechnology Cloud Hub. This work is supported\nin part by the J.P. Morgan AI Faculty Research\nAward. Any opinions, findings, and conclusions in\nthis paper are those of the authors only and do not\nnecessarily reflect the views of the sponsors.\nEthics Statement\nWe give full credit to the respective authors of each\ndataset included in our FLUE benchmark and have\nobtained their permissions for the inclusion of each\ndataset in FLUE. All FLUE benchmark datasets\nhave low ethical risks and do not expose any sensi-\ntive or personal identifiable information. We also\nobtain explicit permissions to use the datasets given\nin section 13 for pre-training of the FLANG models\nfrom the respective sources.\nWe understand that training large language mod-\nels has big carbon-footprint and we have tried\nto minimize the number of full-scale pre-training\nruns. The addition of preferential masking and\nthe span boundary objective have minimal com-\nputation overhead when compared to pre-training\ntraditional BERT/ELECTRA. We hope that future\nmodels work towards lower carbon footprint to re-\nduce the environment costs of pre-training for more\nsustainable and ethical AI.\n2330\nLimitations\nSome limitations to our work are: 1) We have not\nincluded abstractive generation or summarization\ntasks in the FLUE benchmark, due to a lack of large,\nannotated datasets. Future work can be directed\ntowards summarization efforts for the financial do-\nmain. 2) We do not include social media data like\ntwitter and reddit in our pre-training step, despite\nthe heavy impact of social media on some financial\nmarkets like crypto currencies. This is because of\nthe informal usage of textual data which impedes\nthe formal and syntactical correctness of most fi-\nnancial documents. 3) The models are trained\nand tested on English tasks and may not perform\nwell on non-English text. The limited availability\nof non-English domain specific vocabulary makes\nbuilding multi-lingual FLANG models difficult. 4)\nWhile the methodologies presented in this paper\ncan work well for any similarly structured domain\nlike clinical data, it is often difficult to obtain a\nvocabulary term lists and dictionaries for certain\ndomains. 5) We limit ourselves to using encoder\nbased architectures due to the nature of the popular\nfinancial domain specific tasks. Future works can\nexplore the use of other models like GPT3 and T5\nfor the domain.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical BERT\nembeddings. In Proceedings of the 2nd Clinical Nat-\nural Language Processing Workshop, pages 72–78,\nMinneapolis, Minnesota, USA. Association for Com-\nputational Linguistics.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop 2015, pages 84–90.\nDogu Araci. 2019. Finbert: Financial sentiment\nanalysis with pre-trained language models. ArXiv,\nabs/1908.10063.\nPaul Asquith, Michael B Mikhail, and Andrea S Au.\n2005. Information content of equity analyst reports.\nJournal of financial economics, 75(2):245–282.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: Pretrained language model for scientific text. In\nEMNLP.\nKhrystyna Bochkay, Jeffrey Hales, and Sudheer Chava.\n2020. Hyperbole or reality? investor response to\nextreme language in earnings conference calls. The\nAccounting Review, 95(2):31–60.\nRobert M Bowen, Angela K Davis, and Dawn A Mat-\nsumoto. 2002. Do conference calls affect analysts’\nforecasts? The Accounting Review, 77(2):285–316.\nMatthias MM Buehlmaier and Toni M Whited. 2018.\nAre financial constraints priced? evidence from\ntextual analysis. The Review of Financial Studies ,\n31(7):2693–2728.\nBrian J Bushee, Dawn A Matsumoto, and Gregory S\nMiller. 2003. Open versus closed conference calls:\nthe determinants and effects of broadening access to\ndisclosure. Journal of accounting and economics ,\n34(1-3):149–180.\nSudheer Chava, Wendi Du, and Baridhi Malakar. 2020.\nDo managers walk the talk on environmental and\nsocial issues?\nSudheer Chava, Wendi Du, and Nikhil Paradkar. 2019.\nBuzzwords? Available at SSRN 3862645.\nSudheer Chava, Wendi Du, Agam Shah, and Linghang\nZeng. 2022. Measuring firm-level inflation expo-\nsure: A deep learning approach. Available at SSRN\n4228332.\nSudheer Chava and Nikhil Paradkar. 2016. December\ndoldrums, investor distraction, and stock market reac-\ntion to unscheduled news events. Available at SSRN\n2962476.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nArXiv, abs/2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.\n2014. Using structured events to predict stock price\nmovement: An empirical investigation. In Proceed-\nings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1415–\n1425.\nFinSBD3. 2021. Financial sbd 3. https:\n//sites.google.com/nlg.csie.ntu.edu.tw/\nfinweb2021/shared-task-finsbd-3 .\nFiQA. Financial question answering. https://sites.\ngoogle.com/view/fiqa.\nDiego Garcia. 2013. Sentiment during recessions. The\nJournal of Finance, 68(3):1267–1300.\nMichael R. Glass, A. Gliozzo, Rishav Chakravarti, An-\nthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Di-\nnesh Garg, and Avirup Sil. 2020. Span selection\npre-training for question answering. In ACL.\n2331\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves\nStoyanov. 2021. Supervised contrastive learning\nfor pre-trained language model fine-tuning. ArXiv,\nabs/2011.01403.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nInvestopedia. Financial term dictionary from in-\nvestopedia. https://www.investopedia.com/\nfinancial-term-dictionary-4769738 .\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. CoRR, abs/1907.10529.\nMinki Kang, Moonsu Han, and Sung Ju Hwang. 2020.\nNeural mask generator: Learning to generate adap-\ntive word maskings for language model adaptation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 6102–\n6120.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics.\nM. Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. ArXiv, abs/1910.13461.\nFeng Li. 2010. The information content of forward-\nlooking statements in corporate filings—a naïve\nbayesian machine learning approach. Journal of Ac-\ncounting Research, 48(5):1049–1102.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2020. Finbert: A pre-trained finan-\ncial language representation model for financial text\nmining. In Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI, pages 5–10.\nTim Loughran and Bill McDonald. 2011. When is a\nliability not a liability? textual analysis, dictionaries,\nand 10-ks. The Journal of finance, 66(1):35–65.\nMacedo Maia, Siegfried Handschuh, André Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www’18 open challenge:\nFinancial opinion mining and question answering. In\nCompanion Proceedings of the The Web Conference\n2018, WWW ’18, page 1941–1942, Republic and\nCanton of Geneva, CHE. International World Wide\nWeb Conferences Steering Committee.\nPekka Malo, Ankur Sinha, Pyry Takala, Pekka Korho-\nnen, and Jyrki Wallenius. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the American Society for Informa-\ntion Science and Technology.\nEkstrand Michael and Konstan Joseph.\nRank-aware top-n metrics. https://www.\ncoursera.org/lecture/recommender-metrics/\nrank-aware-top-n-metrics-Wk98r .\nMyV ocabulary.com. Business, finance\nand economics vocabulary word list.\nhttps://myvocabulary.com/word-list/\nbusiness-finance-and-economics-vocabulary/ .\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nXiao Ding Philippe Remy. 2015. Financial news dataset\nfrom bloomberg and reuters. https://github.com/\nphilipperemy/financial-news-dataset.\nGuangyuan Piao and John G Breslin. 2018. Financial\naspect and sentiment predictions with deep neural\nnetworks: an ensemble approach. In Companion\nProceedings of the The Web Conference 2018, pages\n1973–1977.\nAlec Radford, Jeff Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nAnkur Sinha and Tanmay Khandait. 2020. Impact of\nnews on the commodity market: Dataset and results.\narXiv preprint arXiv:2009.04202.\n2332\nChi Sun, Xipeng Qiu, Yige Xu, and X. Huang. 2019a.\nHow to fine-tune bert for text classification? ArXiv,\nabs/1905.05583.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019b. Ernie: Enhanced rep-\nresentation through knowledge integration. ArXiv,\nabs/1904.09223.\nTheStreet. Financial word dictionary.\nhttps://www.thestreet.com/topic/46001/\nfinancial-glossary.html.\nV ocabulary.com. Personal finance and financial literacy.\nhttps://www.vocabulary.com/lists/1504643.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP@EMNLP.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSteve Yang, Jason Rosenfeld, and Jacques Maku-\ntonin. 2018. Financial aspect-based sentiment anal-\nysis using deep representations. arXiv preprint\narXiv:1808.07931.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model for\nfinancial communications. CoRR, abs/2006.08097.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n7 Appendix\n7.1 Pre-training datasets\nTable 13 summarizes the financial datasets used\nfor pre-training. It also presents the percentage of\neach dataset sampled in one training epoch. A brief\ndescription of each dataset used for pre-training is\ngiven below:\n7.1.1 SEC Financial Reports\nMost U.S. public firms are required by the U.S.\nSecurities and Exchange Commission (SEC) to\nsubmit annual report (10-K) and quarterly report\n(10-Q), to provide detailed information about the\nfirm’s business, risk factors, and financial perfor-\nmance. 10-K and 10-Q filings were analyzed in (Li,\n2010; Loughran and McDonald, 2011; Buehlmaier\nand Whited, 2018; Chava and Paradkar, 2016). We\ndownload the 10-K and 10-Q filings from SEC\nEDGAR during 1993–2020.\n7.1.2 Earnings Conference Calls\nThe earnings conference calls are held by pub-\nlic companies to covey critical corporate informa-\ntion to the investors and analysts (Bushee et al.,\n2003; Bowen et al., 2002). SeekingAlpha, as a\ncrowd-sourced website in the United States, pro-\nvides investing information for a large number of\npublic companies and publishes textual transcripts\nof many earnings conference calls. Bochkay et al.\n(2020) use the earnings conference call transcripts\nto analyze the stock market response to the lan-\nguage extremity. (Chava et al., 2019) use BERT to\nconstruct emerging technology related discussions\nin earnings calls and evaluate whether it is just\nhype. (Chava et al., 2020) employ RoBERTa to ex-\ntract environmental related discussion in earnings\ncalls and analyze whether managers walk their talk.\nWe collect 151,359 earnings call transcripts from\nSeekingAlpha from Jan. 2000 to Jul. 2019. (Chava\net al., 2022) use BERT to construct a text-based\nfirm-level inflation exposure measure on earning\ncall transcripts.\n7.1.3 Analyst Reports\nSecurity analysts generate reports related to a firms’\nfuture performance after collecting and analyzing\nthe relevant information. Most analyst reports con-\ntains earnings forecast, stock recommendation, and\nstock price target (Asquith et al., 2005). We collect\naround 201 analyst reports on public firms from\nLexisNexis. This corpus contains the language the\nanalysts use to disseminate the new information\nand their interpretation of previous released infor-\nmation to the investors.\n7.1.4 Reuters Financial News\nFinancial news corpus is helpful in analyzing the\nlanguage used in business society. The Thomson\nReuters Text Research Collection (TRC2) contains\nover 1.8M financial news stories during 2008–2009,\n2333\nName Source Size Time Period %age sampled\n10-K SEC EDGAR 13660 1993-2020 8\n10-Q SEC EDGAR 36402 1993-2020 5\nEarning Call Transcripts SeekingAlpha 151359 2007-2019 1.5\nFinancial News Reuters TRC2 Corpus 106521 2007 10\nFinancial News Bloomberg Corpus 387220 2009 5\nAnalyst Reports LexisNexis 201 2017-2020 100\nInvestopedia Articles Investopedia 638 NA 100\nTable 13: Summary of financial datasets used for pre-training. Model size denotes the number of samples in the\ndataset. %age sampled denotes the percentage of each dataset we sampled in a single training epoch.\nwhich is deployed in prior literature (Araci, 2019).\nWe use 10% of this corpus to pre-train our model.\n7.1.5 Bloomberg Financial News\nBloomberg disseminates business and market news\nto the market investors. We obtain the publicly\navailable Bloomberg news articles provided by\nPhilippe Remy (2015), which is used in Ding et al.\n(2014) to predict the return of Standard & Poor’s\n500 stock (S&P 500) index.\n7.1.6 Investopedia\nInvestopedia is a financial website which serves as\na comprehensive financial dictionary and provides\ndefinition and explanation for financial terminolo-\ngies used in business world. We download the 638\narticles for the financial concepts, and use them to\npre-train our model. These articles not only pro-\nvide definitions of financial terms, but also show\nhow they are interrelated to each other.\n7.2 Ablation Studies\n7.2.1 Preferential Masking with Financial\nVocabulary\nFor the first study, we try different configurations\nwhile preferentially masking financial terms in the\npre-training. Table 10 shows the impact of mask-\ning different percentages of Financial Terms on the\nmodel perplexity. The perplexities are calculated\nwhile keeping the total percentage of masked to-\nkens for all vocabulary at 15 percent. Table 10\nshows that masking 30 percent of financial terms\ngives the least perplexity on the validation set.\nWe also experiment with the multi-stage masking,\nwhere in the first stage (first n epochs) we use only\nthe single-word financial tokens and in the sec-\nond stage (next m epochs) we use both: word and\nphrasal financial vocabulary masking. Table 11\nshows that masking single-word financial vocabu-\nlary in the first 2 epochs and masking all financial\nterms has the lowest perplexity score.\n7.2.2 Perplexity on Validation Set\nFor the second study, we compute perplexity of\nthe language model on the validation set after pre-\ntraining. We report the perplexity scores in Ta-\nble 12. We notice that FLANG-BERT significantly\nlowers the perplexity on validation set, relative to\nBERT and FinBERT (Araci, 2019). Despite all\nmodels having the same number of parameters,\nELECTRA based models show lower perplexity\nscores. For ablation study, we keep ELECTRA ar-\nchitecture fixed and notice that pre-training with fi-\nnancial data along with general English data lowers\nperplexity compared to base ELECTRA. Further\nreduction is seen when using our token masking\napproach with financial keywords, suggesting that\ndomain specific masking is helpful for domain spe-\ncific language models. Pre-training with phrase\nbased masking with the span boundary objective in\nthe generator stage results in the best performance,\nvalidating the performance of our technique.\n7.2.3 FPB Sentiment Classification\nFor the third study, we fine-tune the models for\nsentiment analysis on the Financial PhraseBank\nDataset (Malo et al., 2014) and report the accu-\nracy in Table 5. We perform a detailed ablation\nstudy on ELECTRA architectures with our various\ntechniques. The results suggest that pre-training\non financial data improves accuracy from 88.1%\nto 91.1%, and using a financial vocabulary for to-\nken masking further improves the performance to\n91.4%. Span boundary objective is even more ef-\nfective, improving accuracy to > 91.5%. Using\ncontrastive learning for fine-tuning further enables\nan accuracy of 92.1%, which is significantly higher\nthan previous works.\n2334\n7.3 Supervised Contrastive Loss\nLanguage models are usually fine-tuned (Devlin\net al., 2018; Liu et al., 2019; Radford et al., 2019)\nfor supervised classification tasks by using cross\nentropy loss LCE :\nLCE = −1\nN\nN∑\ni=1\nC∑\ni=1\nyi,c log ˆyi,c (2)\nwhere N is the number of samples, Cis the num-\nber of classes, (xi,yi) are the sentence and label\npairs for sample iand ˆyi,c is the model output for\nprobability of sample ihaving class c.\nGunel et al. (2021) showed that using an addi-\ntional supervised contrastive learning loss LSCL\nfor fine-tuning pre-trained language models im-\nproves performance. The loss is meant to capture\nthe similarities between examples of the same class\nand contrast them with the examples from other\nclasses:\nLSCL =\nN∑\ni=1\n− 1\nNyi −1\nN∑\nj=1\n⊮i̸=j⊮yi=yj (\nlog exp(ϕ(xi) ˙ϕ(xj))∑N\nk=1 ⊮i̸=kexp(ϕ(xi) ˙ϕ(xk)))\n)\n(3)\nwhere Nc is the number of samples of class c.\nOverall loss is given by:\nL= λLCE + (1 −λ)LSCL (4)\nwhere λis a variable for weighing the two losses.\n2335",
  "topic": "Smiley",
  "concepts": [
    {
      "name": "Smiley",
      "score": 0.753261923789978
    },
    {
      "name": "Computer science",
      "score": 0.6255806088447571
    },
    {
      "name": "Natural language processing",
      "score": 0.5255847573280334
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5127637982368469
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4936843812465668
    },
    {
      "name": "Chen",
      "score": 0.4776272177696228
    },
    {
      "name": "Natural language",
      "score": 0.4388558864593506
    },
    {
      "name": "Speech recognition",
      "score": 0.33143115043640137
    },
    {
      "name": "Mathematics",
      "score": 0.1471659541130066
    },
    {
      "name": "Operating system",
      "score": 0.09562596678733826
    },
    {
      "name": "Geology",
      "score": 0.08290675282478333
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1305429384",
      "name": "JPMorgan Chase & Co (United States)",
      "country": "US"
    }
  ],
  "cited_by": 48
}