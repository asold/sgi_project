{
    "title": "What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity",
    "url": "https://openalex.org/W3169037993",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5062667199",
            "name": "Alessio Miaschi",
            "affiliations": [
                "Institute for Computational Linguistics “A. Zampolli”",
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A5057659257",
            "name": "Dominique Brunato⋄",
            "affiliations": [
                "Institute for Computational Linguistics “A. Zampolli”",
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A5084812833",
            "name": "Felice Dell’Orletta⋄",
            "affiliations": [
                "Institute for Computational Linguistics “A. Zampolli”",
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A5080111149",
            "name": "Giulia Venturi⋄",
            "affiliations": [
                "Institute for Computational Linguistics “A. Zampolli”",
                "University of Pisa"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2733060938",
        "https://openalex.org/W3035064549",
        "https://openalex.org/W2740917210",
        "https://openalex.org/W2949629417",
        "https://openalex.org/W2579343286",
        "https://openalex.org/W2250263931",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W3029139904",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2299976354",
        "https://openalex.org/W3034718411",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2139450036",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W131522978",
        "https://openalex.org/W3118017018",
        "https://openalex.org/W3168987555"
    ],
    "abstract": "This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2's perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs.",
    "full_text": "Proceedings of Deep Learning Inside Out (DeeLIO):\nThe 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,pages 40–47\nOnline, June 10, 2021. ©2021 Association for Computational Linguistics\n40\nWhat Makes My Model Perplexed?\nA Linguistic Investigation on Neural Language Models Perplexity\nAlessio Miaschi⋆ ⋄, Dominique Brunato⋄, Felice Dell’Orletta⋄, Giulia Venturi⋄\n⋆Department of Computer Science, University of Pisa\n⋄Istituto di Linguistica Computazionale “Antonio Zampolli”, Pisa\nItaliaNLP Lab – www.italianlp.it\nalessio.miaschi@phd.unipi.it, {name.surname}@ilc.cnr.it\nAbstract\nThis paper presents an investigation aimed\nat studying how the linguistic structure of\na sentence affects the perplexity of two of\nthe most popular Neural Language Models\n(NLMs), BERT and GPT-2. We ﬁrst com-\npare the sentence–level likelihood computed\nwith BERT and the GPT-2’s perplexity show-\ning that the two metrics are correlated. In ad-\ndition, we exploit linguistic features capturing\na wide set of morpho-syntactic and syntactic\nphenomena showing how they contribute to\npredict the perplexity of the two NLMs.\n1 Introduction and Motivation\nPerplexity is one of the most standard metrics to as-\nsess the quality of a language model. It is also used\nin different scenarios, such as to classify formal\nand colloquial tweets (González, 2015), to detect\nthe boundaries between varieties belonging to the\nsame language family (Gamallo et al., 2017), to\nidentify speech samples produced by subjects with\ncognitive and/or language diseases e.g. dementia,\n(Cohen and Pakhomov, 2020) or to assess whether\nit matches various human behavioural measures,\nsuch as gaze duration during reading (Demberg\nand Keller, 2008; Goodkind and Bicknell, 2018).\nWith the recent success gained by Neural Language\nModels (NLMs) across a variety of NLP tasks, the\nnotion of perplexity has started being investigated\nalso to dig into issues related to the interpretability\nof contextual word representations, with the aim of\nunderstanding whether there is a relationship be-\ntween this metric and the grammatical abilities im-\nplicitly encoded by a NLM (Gulordava et al., 2018;\nMarvin and Linzen, 2018; Kuncoro et al., 2019).\nIn this context, Hu et al. (2020) and Warstadt et al.\n(2020) observed a dissociation between the per-\nplexity of a NLM and its performance on targeted\nsyntactic assessments probing the model’s ability\nto encode a range of subtle syntactic phenomena.\nThese ﬁndings seem to be valid for models tested\nacross languages (Mueller et al., 2020).\nIn this paper, we address this scenario but from a\ndifferent perspective. Rather than studying the rela-\ntion between the NLM’s perplexity and its linguis-\ntic competences assessed on sentences undergoing\ncontrolled syntactic modiﬁcations, we focus on sen-\ntences representative of real usage. Our purpose\nindeed is to understand which linguistic phenom-\nena of the input sentence may make perplexed a\nNLM and whether they can effectively predict the\nassigned perplexity score. To have a in-depth under-\nstanding of the relation between linguistic structure\nand perplexity, we rely on a wide spectrum of lin-\nguistic features modeling a variety of phenomena,\nspeciﬁcally morpho-syntactic and syntactic ones.\nAs we also intend to evaluate the possible inﬂuence\nof the NLM architecture on this relation, in all our\nexperiments we consider two of the most popular\nNLMs, a traditional unidirectional one, i.e. GPT-2\n(Radford et al., 2019), and a bidirectional model\nsuch as BERT (Devlin et al., 2019).\nContributions In this paper: (i) we showed that\na sentence-level likelihood computed by masking\neach word sequentially for the BERT model has a\nrobust correlation with GPT-2’s perplexity scores;\n(ii) we veriﬁed whether it is possible to predict\nNLMs’ perplexities using a wide set of linguistic\nfeatures extracted by a sentence; (iii) we identiﬁed\nthe linguistic properties of a sentence that mostly\ncause perplexity, reporting differences and similar-\nities between the two models.\n2 Our Approach\nWe deﬁned two sets of experiments. The ﬁrst\nconsists in investigating the relationship between\nBERT and GPT-2 sentence-level perplexity (PPL)\nscores. To do so, we ﬁrst computed BERT and\nGPT-2 PPL scores for sentences contained in the\nEnglish Universal Dependencies (UD) treebank\n(Nivre et al., 2016) and we assessed their corre-\n41\nlation. In the second set of experiments, we studied\nwhether a simple regression model that takes as\ninput a wide range of linguistic features automat-\nically extracted from each UD sentence is able to\npredict the two NLMs sentence-level perplexities.\nTo understand which linguistic phenomena con-\ntribute to the prediction of BERT and GPT-2 PPLs,\nand how these features differ between them, we\nperformed an in-depth investigation training the\nregression model with one feature at a time.\n2.1 Linguistic Features\nThe set of considered linguistic features is based on\nthe ones described in Brunato et al. (2020) which\nare acquired from raw, morpho-syntactic and syn-\ntactic levels of annotation for a total of 78 features\nthat can be categorised in 9 groups corresponding\nto different linguistic phenomena. A summary of\nthe linguistic features is reported in Table 1, while\nthe whole list is provided in Appendix A.\nAs shown in Table, these features model lin-\nguistic phenomena ranging from raw text one,\nto morpho–syntactic information and inﬂectional\nproperties of verbs, to more complex aspects of\nsentence structure modeling global and local prop-\nerties of the whole parsed tree and of speciﬁc sub-\ntrees, such as the order of subjects and objects with\nrespect to the verb, the distribution of UD syntactic\nrelations, also including features referring to the\nuse of subordination and to the structure of verbal\npredicates.\nAll these features have been shown to play a\nhighly predictive role when leveraged by traditional\nlearning models on a variety of classiﬁcation prob-\nlems, also including the development of probes as\nreported by Miaschi et al. (2020), who showed that\nthese features can be effectively used to proﬁle the\nknowledge encoded in the language representations\nof a pretrained NLM.\n2.2 Models and Data\nFor our experiments, we rely on the pre-trained ver-\nsion of the two NLMs previously deﬁned. BERT\n(Devlin et al., 2019) is a Transformer-based masked\nlanguage model, pretrained on BookCorpus (Zhu\net al., 2015) and English Wikipedia. GPT-2 (Rad-\nford et al., 2018) is a large transformer-based lan-\nguage model trained using the language modeling\ntask (LM) on 8 million documents for a total of 40\nGB of text.\nWe ﬁrst computed GPT-2’s sentence-level per-\nplexities by dividing the sum of all sub-word con-\nLinguistic Feature\nRaw Text Properties\nSentence Length\nWord Length\nVocabulary Richness\nType/Token Ratio for words and lemmas\nMorphosyntactic information\nDistibution of UD and language–speciﬁc POS\nLexical density\nInﬂectional morphology\nInﬂectional morphology of auxiliary verbs\nVerbal Predicate Structure\nDistribution of verbal heads and verbal roots\nVerb arity and distribution of verbs by arity\nGlobal and Local Syntactic Tree Structures\nDepth of the whole syntactic tree\nAverage length of dependency links and of the longest link\nAverage length of prepositional chains and distribution by depth\nClause length\nRelative order of elements\nOrder of subject and object\nSyntactic Relations\nDistribution of dependency relations\nUse of Subordination\nDistribution of subordinate and principal clauses\nAverage length of subordination chains and distribution by depth\nRelative order of subordinate clauses\nTable 1: Linguistic Features used in the experiments.\nditional log-probabilities by the total number of\nwords for each sentence in the UD dataset. On the\nother hand, since BERT masked language model-\ning task does not allow to compute well-formed\nprobability distributions over sentences, we mea-\nsure BERT sentence-level likelihood by masking\neach word sequentially and computing the proba-\nbility as follows:\np(S) ≈\nk∏\ni=1\np(wi|context)\nwhere context, given the deep bidirec-\ntionality of the model, corresponds to\nw1,...,w i−1,wi+1,...,w k. The perplexity is\nthen computed as follows:\nPPLS = e( p(S)\nN )\nwhere N correspond to the length of sentence\nS. In order to uniform the terminology, in what\nfollows we will refer to the BERT sentence-level\nlikelihood as perplexity.\nIn order to evaluate our approach on gold an-\nnotated sentences, we relied on the three English\nUniversal Dependencies (UD) treebanks: the En-\nglish version of ParTUT (Sanguinetti and Bosco,\n2015), the UD version of the GUM corpus (Zeldes,\n2017) and of the English Web Treebank (EWT)\n(Silveira et al., 2014). Overall, the ﬁnal dataset\nconsists of 22,505 sentences.\n42\nLengths ρ score # samples\nAll 0.63 22,505\nn=10 0.66 847\nn=15 0.60 793\nn=20 0.64 643\nn=25 0.53 422\nn=30 0.54 277\nTable 2: Spearman correlations between BERT and\nGPT-2 perplexities computed for all UD sentences (All)\nand sentences with ﬁxed-length n.\n3 A Linguistic Investigation on\nPerplexity\nAs a ﬁrst step, we assessed whether there is a re-\nlationship between the perplexity of a traditional\nNLM and of a masked NLM. We thus calculated\nBERT and GPT-2 perplexity scores for each UD\nsentence and measured the correlation between\nthem. Since PPL scores are highly affected by\nthe length of the input sequence, we computed ρ\ncorrelation coefﬁcients also considering groups of\nsentences with ﬁxed length. Speciﬁcally, we relied\non Spearman correlation because we were inter-\nested in measuring how the variations in perplexity\nscores relate each other, rather than focusing on the\nactual PPL values. Results are reported in Table\n2. As we can notice, even considering samples\nwith ﬁxed length, the two NLMs’ perplexities ex-\nhibit moderate to substantial correlation (with p <\n0.001), thus showing that BERT an GPT-2 do not\ndiverge excessively in their ability of predicting\nthe likelihood of the input sentences. Moreover,\nthis allows us to conﬁrm that, although the deep\nbidirectional structure of BERT does not permit\nto compute a well-formed probability distribution\nover a sentence (see Section 2.2), this metric could\nbe considered as a valid approximation of the per-\nplexity computed with a unidirectional NLM.\nOnce established the correlation between the per-\nplexities of the two NLMs, we performed a second\nexperiment to investigate (i) if the considered set\nof linguistic features plays a role in predicting their\nperplexity and (ii) which are the features that con-\ntribute more to the prediction task. To do so, we\ntrained a LinearSVR model that predicts perplex-\nity’s scores using our set of linguistic properties as\ninput features. Since most of them refer to syntac-\ntic properties of sentence that are strongly corre-\nlated with its length, we considered as a baseline a\nSVR model that takes sentence length as input and\noutputs BERT/GPT-2 sentence’s perplexity. Re-\ngression results deriving by considering both the\nFigure 1: BERT and GPT-2 ρ scores (multiplied by\n100) obtained with the LinearSVR model using linguis-\ntic features, for the whole UD dataset and groups of\nsentences with ﬁxed length.\nwhole set (All) and each of the 9 groups of linguis-\ntic features separately are reported in Figure 1. As\na general remark, for the whole UD dataset, we can\nobserve that the results considering both all and the\n9 groups of linguistic features outperform the re-\nsults obtained by the baseline, i.e.ρ=0.38 for BERT\nand 0.22 for GPT-2 respectively. This demonstrates\nthat the considered features are able to model as-\npects involved in NLM’s perplexity that go beyond\nthe simple length of sentence. This is particularly\nthe case of GPT-2, suggesting that the probabil-\nity assigned to a sentence by a traditional NLM\nis more explainable in terms of linguistic phenom-\nena mainly affecting morpho-syntactic and syntac-\ntic structure. Consequently, the baseline score is\nhigher for BERT. If we consider the scores obtained\nfor each group of sentences with ﬁxed length, we\ncan see that higher scores are obtained for groups\ncontaining shorter sentences, for both NLMs. This\nis quite expected since in these sentences the possi-\nble output space is smaller for almost all features,\n43\nFigure 2: BERT and GPT-2 ρscores obtained with the\nLinearSVR model, for the whole UD dataset and 16\ntoken-long sentences. Scores are reported for the 20\ntop-ranked features for BERT. Numbers in brackets cor-\nrespond to the relative in the GPT-2 ranking.\nthus making them more predictive. Also in this\ncase, the impact of the linguistic features is always\nhigher for the prediction of GPT-2’s perplexity.\nA more in-depth analysis of these results shows\nthat the distribution of the morpho-syntactic char-\nacteristics of a sentence (POS) and of the syntactic\ndependency relations (SyntacticDep) are the two\nmost predictive sources of linguistic information.\nAs Figure 1 reports, this holds for the two NLM\nmodels and it remains constant throughout all the\ngroups of sentences with ﬁxed lengths. Interest-\ningly, if we consider the whole set of sentences,\nthe effect of the morpho-syntactic information on\nthe prediction of GPT-2’s perplexity is exactly the\nsame of that of the whole set of linguistic features.\nFor some sentence lengths (15, 20, 30) the scores\nobtained using only this type of information out-\nperform even those obtained considering the whole\nset of features. Note that this last remark is true\nalso in the prediction of BERT’s perplexity. As\nexpected the other most predictive group is the one\n(RawText) that includes the length of sentence.\n3.1 Focus on the contribution of individual\nfeatures\nTo investigate more in depth which linguistic phe-\nnomena are more involved in the perplexity of the\ntwo models, we trained the LinearSVR model using\neach individual feature at a time. This was done for\nboth the whole dataset and the subset of sentences\n(i.e. 758 sentences) having a length of 16 tokens,\nwhich corresponds to the mean sentence length of\nthe UD dataset. A subset of results is reported in\nFigure 2, while the whole results are provided in\nAppendix B. As we can see in the left-side of the\nheatmap, the two models share many features in the\nﬁrst ten positions, thus showing that the two NLM\narchitectures are made perplexed by similar linguis-\ntic characteristics of a sentence. In particular, for\nboth of them, the two most predictive features cor-\nrespond to the lexical density and the presence of\npronouns conﬁrming the highly predictive power of\nmorpho-syntactic information. They are followed\nby features related to the presence of verbs and\nto their internal structure (i.e. verbal_heads and\navg_verb_edges), and, as it was expected, by the\nlength of the sentence. Despite these similarities,\nwe can see that the scores obtained by the regres-\nsion model to predict BERT’s perplexity are on\naverage higher than GPT-2’s scores. Considering\nthat we obtained higher scores using all (or groups\nof) features in the prediction of GPT-2’ perplexity\n(see Figure 1), this latter result may suggest that\nthe interaction among features is less relevant in\nthe prediction of BERT’s perplexity. Differences\namong the two models concern features that are\nhighly sensitive to sentence length, which result to\nbe more predictive of BERT’s perplexity. This is\nthe case of syntactic features capturing global and\nlocal aspects of sentence structure, i.e. the depth of\nthe whole syntactic tree (parse_depth), the maxi-\nmum length of dependency links (max_links_len)\nand the length of verbal clauses ( clause_length).\nAlso, the canonical order of nuclear sentence ele-\nments such as pre-verbal subjects contribute more\nto predict BERT’s than GPT-2’s perplexity. Instead,\nthe distribution of proper nouns (%_upos_PROPN),\nin particular in their singular form (%_xpos_NNP),\nthe length of token (char_per_tok) and vocabulary\nrichness are more predictive of GPT-2’s perplex-\nity. Although we cannot say from ranking results\nwhether features highly ranked are positively or\nnegatively correlated with perplexity, we can hy-\npothesize that knowing the distribution of tokens\nbelonging to open lexical categories (e.g. proper\nnouns vs determiners) make the perplexity easier\nto identify.\nThe right-side heatmap shows the top-ranked\nfeatures used to predict the two models perplexity\nfor sentences 16-token long. As expected, when\nsentence length is controlled, the role of other fea-\ntures less related to length becomes predominant.\n44\nIn particular, morpho-syntactic information is still\nhighly predictive for the two models, with lexical\nparts-of-speech showing to be relevant not only for\nGPT-2’s but also of BERT’s perplexity.\n4 Conclusion\nIn this paper we proposed an investigation of the\nlinguistic phenomena characterizing the perplexity\nof a undirectional and a bidirectional Neural Lan-\nguage Model, GPT-2 and BERT. We ﬁrst reported\nrobust correlations between GPT-2’s perplexity and\nthe sentence-level likelihood computed with BERT.\nThis is a quite prominent result, especially consider-\ning that these two metrics are differently computed\nas a consequence of the two NLMs architectures.\nInterestingly, we found the effectiveness of lin-\nguistic features modelling a wide set of morpho-\nsyntactic and syntactic phenomena in predicting the\nperplexity of the two NLMs, especially for shorter\nsentences. Despite similar trends, we observed\nsome differences between the two NLMs both at\nthe level of regression accuracy and in the rankings\nof the features exploited in the prediction of per-\nplexity. GPT-2’s perplexity is better captured by\nthe considered features and it resulted to be more\naffected by lexical parts-of-speech and features cap-\nturing the vocabulary richness of a sentence. On\nthe contrary, BERT’s perplexity seems to be best\npredicted by syntactic features highly sensitive to\nsentence length.\nReferences\nDominique Brunato, Andrea Cimino, Felice\nDell’Orletta, Giulia Venturi, and Simonetta\nMontemagni. 2020. Proﬁling-ud: a tool for linguis-\ntic proﬁling of texts. In Proceedings of The 12th\nLanguage Resources and Evaluation Conference,\npages 7147–7153, Marseille, France. European\nLanguage Resources Association.\nTrevor Cohen and Serguei Pakhomov. 2020. A tale\nof two perplexities: Sensitivity of neural language\nmodels to lexical retrieval deﬁcits in dementia of the\nAlzheimer’s type. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1946–1957, Online. Association\nfor Computational Linguistics.\nV . Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntactic\nprocessing complexity. Cognition, 109:193–210.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nPablo Gamallo, Jose Ramom Pichel, and Iñaki Alegria.\n2017. A perplexity-based method for similar lan-\nguages discrimination. In VarDial2017 workshop at\nEACL 2017. Proceedings of the Fourth Workshop\non NLP for Similar Languages, Varieties and\nDialects, pages 109–114,Valencia, Spain, April\n3, 2017. c©2017 Association for Computational\nLinguistics (http://web.science.mq.edu.au/ smal-\nmasi/vardial4/index.html).\nM. González. 2015. An analysis of twitter corpora\nand the differences between formal and colloquial\ntweets. In TweetMT@SEPLN.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics, CMCL 2018, Salt\nLake City, Utah, USA, January 7, 2018, pages 10–\n18. Association for Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable syntax-\naware language models using knowledge distillation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3472–3484, Florence, Italy. Association for Compu-\ntational Linguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202.\nAlessio Miaschi, Dominique Brunato, Felice\nDell’Orletta, and Giulia Venturi. 2020. Lin-\nguistic proﬁling of a neural language model. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 745–756,\n45\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5523–5539, Online. Association for\nComputational Linguistics.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajic, Christopher D Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, et al. 2016. Universal dependencies\nv1: A multilingual treebank collection. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC’16), pages\n1659–1666.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nManuela Sanguinetti and Cristina Bosco. 2015. Parttut:\nThe turin university parallel treebank. In Harmo-\nnization and Development of Resources and Tools\nfor Italian Natural Language Processing within the\nPARLI Project, pages 51–69. Springer.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nDe Marneffe, Samuel R Bowman, Miriam Connor,\nJohn Bauer, and Christopher D Manning. 2014. A\ngold standard dependency corpus for english. In\nLREC, pages 2897–2904.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nAmir Zeldes. 2017. The GUM corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n46\nA Appendix A\nfRaw Text Properties\n[sent_length]: average length of sentences in a document, calculated in terms of the number of words per sentence\n[char_per_tok]: average number of characters per word (excluded punctuation)\nVocabulary Richness\n[ttr_lemma]: Type/Token Ratio (TTR) calculated with respect to the lemmata in a sentence. It ranges between 1 (high lexical variety) and 0\n(low vocabulary richness)\n[ttr_form]: Type/Token Ratio (TTR) calculated with respect to the word forms in a sentence. It ranges between 1 (high lexical variety) and 0\n(low vocabulary richness)\nMorphosyntactic information\n[%_upos_*]: distribution of the part-of-speech categories deﬁned in the Universal POS tags, as detailed at the following link: https://\nuniversaldependencies.org/u/pos/index.html\n[%_xpos_*]: distribution of the part-of-speech categories deﬁned in the Penn Treebank POS tags, as detailed at the following link: https:\n//www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n[lexical_density]: the value corresponds to the ratio between content words (nouns, proper nouns, verbs, adjectives, adverbs) over the total\nnumber of words in a sentence\nInﬂectional morphology\n[%_aux_tense_*]: distribution of auxiliary verbs according to their tense: https://universaldependencies.org/u/feat/\nTense.html\n[%_aux_mood_dist_*]: distribution of auxiliary verbs according to their moods: https://universaldependencies.org/u/feat/\nMood.html\n[%_aux_form_*]: distribution of auxiliary verbs according to their forms: https://universaldependencies.org/u/feat/\nVerbForm.html\n[verbs_gender_dist_*]: distribution of verbs according to the gender of participle forms, for the languages that have this features: https:\n//universaldependencies.org/u/feat/Gender.html\n[%_aux_num_pers_*]: distribution of auxiliary verbs according to their number and person:https://universaldependencies.org/\nu/feat/Person.html\nVerbal Predicate Structure\n[verbal_head]: average distribution of verbal heads in the document, out ot the total of heads.\n[%_verbal_roo]: average distribution of roots headed by a lemma tagged as verb, out of the total of sentence roots;\n[avg_verb_edges]: verbal arity, calculated as the average number of instantiated dependency links (covering both arguments and modiﬁers)\nsharing the same verbal head, excluding punctuation and auxiliaries bearing the syntactic role of copula according to the UD scheme\n[verbal_arity*]: distribution of verbs for arity class (e.g. verbs with arity 1, 2, ...)\nGlobal and Local Syntactic Tree Structures\n[parse_depth]: mean of the maximum tree depths of the sentence. The maximum depth is calculated as the longest path (in terms of occurring\ndependency links) from the root of the dependency tree to some leaf\n[clause_length]: average clause length, calculated in terms of the average number of tokens per clause, where a clause is deﬁned as the ratio\nbetween the number of tokens in a sentence and the number of either verbal or copular head\n[avg_links_len]: average number of words occurring linearly between each syntactic head and its dependent (excluding punctuation dependen-\ncies)\n[max_links_len]: the value of the longest dependency link in the document, calculated in number of tokens\n[prep_1]: distribution of prepositional chains 1-complement long. A prepositional chain is calculated as the number of embedded prepositional\ncomplements dependent on a noun\nRelative order of elements\n[%_obj_post]: distribution of objects following the verb\n[%_subj_pre]: distribution of subjects preceding the verb\nSyntactic Relations\n[%_dep_*]: average distribution of the 37 universal syntactic relations used in UD (https://universaldependencies.org/u/dep/\nindex.html)\nUse of Subordination\n[principal_prop_dist]: distribution of principal clauses\n[%_subord_prop]: distribution of subordinate clauses, as deﬁned in the UD scheme: https://universaldependencies.org/u/\noverview/complex-syntax.html#subordination\n[subord_post]: distribution of subordinate clauses following the main clause\n[avg_subord_chain]: average length of subordinate chains, where a subordinate ’chain’ is calculated as the number of subordinate clauses\nembedded on a ﬁrst subordinate clause\n[subord_1]: distribution of subordinate chains 1-clause long\nTable 3: Linguistic features used in the experiments.\n47\nB Appendix B\nFigure 3: BERT and GPT-2ρscores obtained with the LinearSVR model using one feature at a time, for the whole\nUD dataset and sentences with lengths = 16."
}