{
  "title": "A vision transformer for decoding surgeon activity from surgical videos",
  "url": "https://openalex.org/W4362458937",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4226556328",
      "name": "Kiyasseh, Dani",
      "affiliations": [
        "California Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3023081013",
      "name": "Ma, Runzhuo",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4281406114",
      "name": "Haque, Taseen F.",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": null,
      "name": "Miles, Brian J.",
      "affiliations": [
        "Methodist Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2136572785",
      "name": "Wagner Christian",
      "affiliations": [
        "St.-Antonius-Hospital Gronau"
      ]
    },
    {
      "id": null,
      "name": "Donoho, Daniel A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747916978",
      "name": "Anandkumar, Animashree",
      "affiliations": [
        "California Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4281406118",
      "name": "Hung, Andrew J.",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4226556328",
      "name": "Kiyasseh, Dani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3023081013",
      "name": "Ma, Runzhuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281406114",
      "name": "Haque, Taseen F.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Miles, Brian J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136572785",
      "name": "Wagner Christian",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Donoho, Daniel A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747916978",
      "name": "Anandkumar, Animashree",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281406118",
      "name": "Hung, Andrew J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2131149153",
    "https://openalex.org/W3000214625",
    "https://openalex.org/W2023276160",
    "https://openalex.org/W2964026410",
    "https://openalex.org/W3087831331",
    "https://openalex.org/W2980110287",
    "https://openalex.org/W3118577024",
    "https://openalex.org/W4226457954",
    "https://openalex.org/W3121789984",
    "https://openalex.org/W3179083309",
    "https://openalex.org/W4210453703",
    "https://openalex.org/W3187783200",
    "https://openalex.org/W3209901185",
    "https://openalex.org/W4285803680",
    "https://openalex.org/W3156751451",
    "https://openalex.org/W4362506180",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W4362506590",
    "https://openalex.org/W3158454357",
    "https://openalex.org/W2790209545",
    "https://openalex.org/W2580456502",
    "https://openalex.org/W2072678279",
    "https://openalex.org/W2788941211",
    "https://openalex.org/W2809241522",
    "https://openalex.org/W2916629625",
    "https://openalex.org/W4223432361",
    "https://openalex.org/W100441416",
    "https://openalex.org/W3013053228",
    "https://openalex.org/W3091747220",
    "https://openalex.org/W3090913119",
    "https://openalex.org/W3136964767",
    "https://openalex.org/W3203457317",
    "https://openalex.org/W1989912492",
    "https://openalex.org/W3112106476",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3100964249",
    "https://openalex.org/W3203204495",
    "https://openalex.org/W3198379050",
    "https://openalex.org/W3185114238",
    "https://openalex.org/W3069635843",
    "https://openalex.org/W2081197657",
    "https://openalex.org/W4305072987",
    "https://openalex.org/W3200295862",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3210506295",
    "https://openalex.org/W4225907609",
    "https://openalex.org/W4210582445",
    "https://openalex.org/W3134650884",
    "https://openalex.org/W4316128849"
  ],
  "abstract": "Abstract The intraoperative activity of a surgeon has substantial impact on postoperative outcomes. However, for most surgical procedures, the details of intraoperative surgical actions, which can vary widely, are not well understood. Here we report a machine learning system leveraging a vision transformer and supervised contrastive learning for the decoding of elements of intraoperative surgical activity from videos commonly collected during robotic surgeries. The system accurately identified surgical steps, actions performed by the surgeon, the quality of these actions and the relative contribution of individual video frames to the decoding of the actions. Through extensive testing on data from three different hospitals located in two different continents, we show that the system generalizes across videos, surgeons, hospitals and surgical procedures, and that it can provide information on surgical gestures and skills from unannotated videos. Decoding intraoperative activity via accurate machine learning systems could be used to provide surgeons with feedback on their operating skills, and may allow for the identification of optimal surgical behaviour and for the study of relationships between intraoperative factors and postoperative outcomes.",
  "full_text": "Nature Biomedical Engineering | Volume 7 | June 2023 | 780–796 780\nnature biomedical engineering\nArticle\nhttps://doi.org/10.1038/s41551-023-01010-8\nA vision transformer for decoding surgeon \nactivity from surgical videos\nDani Kiyasseh    1 , Runzhuo Ma2, Taseen F. Haque    2, Brian J. Miles3, \nChristian Wagner4, Daniel A. Donoho5, Animashree Anandkumar1 & \nAndrew J. Hung    2 \nThe intraoperative activity of a surgeon has substantial impact on \npostoperative outcomes. However, for most surgical procedures, the \ndetails of intraoperative surgical actions, which can vary widely, are not \nwell understood. Here we report a machine learning system leveraging a \nvision transformer and supervised contrastive learning for the decoding \nof elements of intraoperative surgical activity from videos commonly \ncollected during robotic surgeries. The system accurately identified surgical \nsteps, actions performed by the surgeon, the quality of these actions and \nthe relative contribution of individual video frames to the decoding of the \nactions. Through extensive testing on data from three different hospitals \nlocated in two different continents, we show that the system generalizes \nacross videos, surgeons, hospitals and surgical procedures, and that it \ncan provide information on surgical gestures and skills from unannotated \nvideos. Decoding intraoperative activity via accurate machine learning \nsystems could be used to provide surgeons with feedback on their operating \nskills, and may allow for the identification of optimal surgical behaviour \nand for the study of relationships between intraoperative factors and \npostoperative outcomes.\nThe overarching goal of surgery is to improve postoperative patient \noutcomes1,2. It was recently demonstrated that such outcomes are \nstrongly influenced by intraoperative surgical activity3, that is, what \nactions are performed by a surgeon during a surgical procedure and \nhow well those actions are executed. For the vast majority of surgical \nprocedures, however, a detailed understanding of intraoperative surgi-\ncal activity remains elusive. This scenario is all too common in other \ndomains of medicine, where the drivers of certain patient outcomes \neither have yet to be discovered or manifest differently. The status quo \nwithin surgery is that intraoperative surgical activity is simply not meas-\nured. Such lack of measurement makes it challenging to capture the \nvariability in the way surgical procedures are performed across time, \nsurgeons and hospitals, to test hypotheses associating intraoperative \nactivity with patient outcomes, and to provide surgeons with feedback \non their operating technique.\nIntraoperative surgical activity can be decoded from videos com-\nmonly collected during robot-assisted surgical procedures. Such \ndecoding provides insight into what procedural steps (such as tissue \ndissection and suturing) are performed over time, how those steps \nare executed (for example, through a set of discrete actions or ges-\ntures) by the operating surgeon, and the quality with which they are \nexecuted (that is, mastery of a skill; Fig. 1). Currently, if a video were to \nbe decoded, it would be through a manual retrospective analysis by an \nexpert surgeon. However, this human-driven approach is subjective, as \nReceived: 22 June 2022\nAccepted: 15 February 2023\nPublished online: 30 March 2023\n Check for updates\n1Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA, USA. 2Center for Robotic Simulation and \nEducation, Catherine & Joseph Aresty Department of Urology, University of Southern California, Los Angeles, CA, USA. 3Department of Urology, Houston \nMethodist Hospital, Houston, TX, USA. 4Department of Urology, Pediatric Urology and Uro-Oncology, Prostate Center Northwest, St. Antonius-Hospital, \nGronau, Germany. 5Division of Neurosurgery, Center for Neuroscience, Children’s National Hospital, Washington, DC, USA.  e-mail: danikiy@hotmail.com;  \najhung@gmail.com\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 781\nArticle https://doi.org/10.1038/s41551-023-01010-8\ngeneralize to, or perform well in, new settings, such as with unseen \nvideos from different surgeons, surgical procedures and hospitals. \nSuch a rigorous evaluation is critical to ensuring the development of \nsafe and trustworthy AI systems.\nIn this study, we propose a unified surgical AI system (SAIS) that \ndecodes multiple elements of intraoperative surgical activity from vid-\neos collected during surgery. Through rigorous evaluation on data from \nthree hospitals, we show that SAIS reliably decodes multiple elements \nof intraoperative activity, from the surgical steps performed to the \ngestures that are executed and the quality with which they are executed \nby a surgeon. This reliable decoding holds irrespective of whether vid-\neos are of different surgical procedures and from different surgeons \nacross hospitals. We also show that SAIS decodes such elements more \nreliably than state-of-the-art AI systems, such as Inception3D (I3D;  \nref. 6), which have been developed to decode only a single element \nit depends on the interpretation of activity by the reviewing surgeon; \nunreliable, as it assumes that a surgeon is aware of all intraoperative \nactivity; and unscalable, as it requires the presence of an expert surgeon \nand an extensive amount of time and effort. These assumptions are \nparticularly unreasonable where expert surgeons are unavailable (as in \nlow-resource settings) and already pressed for time. As such, there is a \npressing need to decode intraoperative surgical activity in an objective, \nreliable and scalable manner.\nGiven these limitations, emerging technologies such as artificial \nintelligence (AI) have been used to identify surgical activity4, gestures5, \nsurgeon skill levels6,7 and instrument movements 8 exclusively from \nvideos. However, these technologies are limited to decoding only a \nsingle element of intraoperative surgical activity at a time (such as only \ngestures), limiting their utility. These technologies are also seldom \nrigorously evaluated, where it remains an open question whether they \na\nVideo of a surgical procedure\nTime\nDissection gestures\n(how of surgery)\nStitch\nSubphases\n(what of surgery)\nNeedle\nhandling\nNeedle\ndriving\nNeedle\nwithdrawal\nDissection Suturing\nSAIS\nIntraoperative surgical activity\nUnified AI system that decodes \nintraoperative surgical activity from a video\nHook Clip Cut\nMove Peel Retract\nSkill level\n(how of surgery)\nLow High\nSubphase\nrecognition\nSAIS \nSurgical video\nGesture\nclassification\nSAIS\nSurgical video\nSkill\nassessment\nSAIS\nSurgical video\nSame \narchitecture\nSame \narchitecture\nb\nFig. 1 | An AI system that decodes intraoperative surgical activity from \nvideos. a, Surgical videos commonly collected during robotic surgeries are \ndecoded via SAIS into multiple elements of intraoperative surgical activity: what \nis performed by a surgeon, such as the suturing subphases of needle handling, \nneedle driving and needle withdrawal, and how that activity is executed by a \nsurgeon, such as through discrete gestures and at different levels of skill. b, SAIS \nis a unified system since the same architecture can be used to independently \ndecode different elements of surgical activity, from subphase recognition to \ngesture classification and skill assessment.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 782\nArticle https://doi.org/10.1038/s41551-023-01010-8\n(such as surgeon skill). We also show that SAIS, through deployment \non surgical videos without any human-driven annotations, provides \ninformation about intraoperative surgical activity, such as its quality \nover time, that otherwise would not have been available to a surgeon. \nThrough a qualitative assessment, we demonstrate that SAIS provides \naccurate reasoning behind its decoding of intraoperative activity. \nWith these capabilities, we illustrate how SAIS can be used to provide \nsurgeons with actionable feedback on how to modulate their intraop-\nerative surgical behaviour.\nResults\nSAIS reliably decodes surgical subphases\nWe decoded the ‘what’ of surgery by tasking SAIS to distinguish between \nthree surgical subphases: needle handling, needle driving and nee -\ndle withdrawal (Fig. 1). For all experiments, we trained SAIS on video \nsamples exclusively from the University of Southern California (USC) \n(Table 1). A description of the surgical procedures and subphases is \nprovided in Methods.\nGeneralizing across videos. We deployed SAIS on the test set of video \nsamples from USC, and present the receiver operating characteristic \n(ROC) curves stratified according to the three subphases (Fig. 2a). \nWe observed that SAIS reliably decodes surgical subphases with area \nunder the receiver operating characteristic curve (AUC) of 0.925, 0.945 \nand 0.951, for needle driving, needle handling and needle withdrawal, \nrespectively. We also found that SAIS can comfortably decode the \nhigh-level steps of surgery, such as suturing and dissection (Supple -\nmentary Note 3 and Supplementary Fig. 2).\nGeneralizing across hospitals. T o determine whether SAIS can gen-\neralize to unseen surgeons at distinct hospitals, we deployed it on \nvideo samples from St. Antonius Hospital (SAH) (Fig. 2b) and Houston \nMethodist Hospital (HMH) (Fig. 2c). We found that SAIS continued to \nexcel with AUC ≥0.857 for all subphases and across hospitals.\nBenchmarking against baseline models. We deployed SAIS to decode \nsubphases from entire videos of the vesico-urethral anastomosis (VUA) \nsuturing step (20 min long) without any human supervision (inference \nsection in Methods). We present the F110 score (Fig. 2e), a commonly \nreported metric9, and contextualize its performance relative to that of \na state-of-the-art I3D network6. We found that SAIS decodes surgical \nsubphases more reliably than I3D, with these models achieving F110 of \n50 and 40, respectively.\nThe performance of SAIS stems from attention mechanism \nand multiple data modalities\nT o better appreciate the degree to which the components of SAIS con-\ntributed to its overall performance, we trained variants of SAIS, after \nhaving removed or modified these components (ablation section in \nMethods), and report their positive predictive value (PPV) when decod-\ning the surgical subphases (Fig. 2d).\nWe found that the self-attention (SA) mechanism was the largest \ncontributor to the performance of SAIS, where its absence resulted in \n∆PPV of approximately −20. This finding implies that capturing the \nrelationship between, and temporal ordering of, frames is critical for \nthe decoding of intraoperative surgical activity. We also observed that \nthe dual-modality input (red–green–blue, or RGB, frames and flow) \nhas a greater contribution to performance than using either modality \nof data alone. By removing RGB frames (‘without RGB’) or optical flow \n(‘without flow’), the model exhibited an average ∆PPV of approximately \n−3 relative to the baseline implementation. Such a finding suggests that \nthese two modalities are complementary to one another. We therefore \nused the baseline model (SAIS) for all subsequent experiments.\nSAIS reliably decodes surgical gestures\nIn the previous section, we showed the ability of SAIS to decode surgical \nsubphases (the ‘what’ of surgery) and to generalize to video samples \nfrom unseen surgeons at distinct hospitals, and also quantified the mar-\nginal benefit of its components via an ablation study. In this section, we \nTable 1 | Total number of videos and video samples associated with each of the hospitals and tasks\nTask Activity Details Hospital Videos Video samples Surgeons Generalization to\nSubphase recognition Suturing VUA USC 78 4,774 19 Videos\nSAH 60 2,115 8 Hospitals\nHMH 20 1,122 5 Hospitals\nUSC 48 Inference on entire videos\nGesture classification Suturing VUA USC 78 1,241 19 Videos\nLaboratory JIGSAWS 39 793 8 Users\nDVC UCL 36 1,378 8 Videos\nDissection NS USC 86 1,542 15 Videos\nSAH 60 540 8 Hospitals\nUSC 154 Inference on entire unlabelled videos\nRAPN USC 27 339 16 Procedures\nSkill assessment Suturing Needle handling USC 78 912 19 Videos\nSAH 60 240 18 Hospitals\nHMH 20 184 5 Hospitals\nNeedle driving USC 78 530 19 Videos\nSAH 60 280 18 Hospitals\nHMH 20 220 5 Hospitals\nNote that we train our model, SAIS, exclusively on data from hospitals whose names are shown in bold following a ten-fold Monte Carlo cross-validation setup. For an exact breakdown of the \nnumber of video samples in each fold and training, validation and test split, please refer to Supplementary Tables 1–5. The data from the remaining hospitals are exclusively used for inference. \nWe perform inference on entire videos from hospitals whose names are shown in italics. Except for the task of subphase recognition, SAIS is always trained and evaluated on a class-balanced \nset of data whereby each category (low skill and high skill) contains the same number of samples. This prevents SAIS from being negatively affected by a sampling bias during training, and \nallows for a more intuitive appreciation of the evaluation results.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 783\nArticle https://doi.org/10.1038/s41551-023-01010-8\nexamine the ability of SAIS to decode surgical gestures (the ‘how’ of sur-\ngery) performed during both tissue suturing and dissection activities \n(the description of gestures and activities is provided in Methods). For \nthe suturing activity (VUA), we trained SAIS to distinguish between four \ndiscrete suturing gestures: right forehand under (R1), right forehand \nover (R2), left forehand under (L1) and combined forehand over (C1). \nFor the dissection activity, known as nerve sparing (NS), we trained SAIS \nto distinguish between six discrete dissection gestures: cold cut (c), \nhook (h), clip (k), camera move (m), peel (p) and retraction (r). We note \nthat training was performed on video samples exclusively from USC.\nGeneralizing across videos. We deployed SAIS on the test set of video \nsamples from USC, and present the ROC curves stratified according \nto the discrete suturing gestures (Fig. 3a) and dissection gestures \n(Fig. 3b). There are two main takeaways here. First, we observed that \nSAIS can generalize well to both suturing and dissection gestures in \nunseen videos. This is exhibited by the high AUC achieved by SAIS \nacross the gestures. For example, in the suturing activity, AUC was \n0.837 and 0.763 for the right forehand under (R1) and combined fore-\nhand over (C1) gestures, respectively. In the dissection activity, AUC \nwas 0.974 and 0.909 for the clip (k) and camera move (m) gestures, \nrespectively. These findings bode well for the potential deployment \nof SAIS on unseen videos for which ground-truth gesture annotations \nare unavailable, an avenue we explore in a subsequent section. Second, \nwe found that the performance of SAIS differs across the gestures. For \nexample, in the dissection activity, AUC was 0.701 and 0.974 for the \nretraction (r) and clip (k) gestures, respectively. We hypothesize that \nthe strong performance of SAIS for the latter stems from the clear visual \npresence of a clip in the surgical field of view. On the other hand, the \nubiquity of retraction gestures in the surgical field of view could be a \nsource of the relatively lower ability of SAIS in decoding retractions, \nas explained next. Retraction is often annotated as such when it is \nactively performed by a surgeon’s dominant hand. However, as a core \ngesture that is used to, for example, improve a surgeon’s visualization \nof the surgical field, a retraction often complements other gestures. \nAs such, it can occur simultaneously with, and thus be confused for, \nother gestures by the model.\nGeneralizing across hospitals. T o measure the degree to which SAIS \ncan generalize to unseen surgeons at a distinct hospital, we deployed \nit on video samples from SAH (Fig. 3c and video sample count in  \nTable 1). We found that SAIS continues to perform well in such a set -\nting. For example, AUC was 0.899 and 0.831 for the camera move (m) \nand clip (k) gestures, respectively. Importantly, such a finding sug -\ngests that SAIS can be reliably deployed on data with several sources \nof variability (surgeon, hospital and so on). We expected, and indeed \nobserved, a slight degradation in performance in this setting relative \nto when SAIS was deployed on video samples from USC. For example, \nAUC was 0.823 → 0.702 for the cold cut (c) gesture in the USC and SAH \ndata, respectively. This was expected due to the potential shift in the \ndistribution of data collected across the two hospitals, which has been \ndocumented to negatively affect network performance 10. Potential \nsources of distribution shift include variability in how surgeons per -\nform the same set of gestures (for instance, different techniques) and \na b c\nd e\n1.0\nTrain\nVUA–USC\nTest\nVUA–USC\nTrain\nVUA–USC\nTest\nVUA–SAH\nTrain\nVUA–USC\nTest\nVUA–HMH\n0.8\n0.6\nSensitivity 0.4\n0.2\n0\n1.0\n0.8\n0.6\nSensitivity 0.4\n0.2\n0\n1.0\n0.8\n0.6\nSensitivity 0.4\n0.2\n0 0.2 0.4 0.6\n1 – specificity\nAblation study Benchmarking models for subphase recognition\nBaseline 80\n60\n40\nF1 at 10\n20\n0\nWithout TTA\nWithout RGB\nWithout flow\nWithout SA\n–20 –10 0 I3D SAIS\nAI systemPPV diﬀerence compared with baseline\nAUC\nNeedle withdrawal 0.951\nNeedle handling 0.945\nNeedle driving 0.925\nAUC\nNeedle driving 0.898\nNeedle withdrawal 0.870\nNeedle handling 0.857\nAUC\nNeedle withdrawal 0.964\nNeedle driving 0.957\nNeedle handling 0.937\n0.8 1.0 0 0.2 0.4 0.6\n1 – specificity\n0.8 1.0 0 0.2 0.4 0.6\n1 – specificity\n0.8 1.0\nFig. 2 | Decoding surgical subphases from videos. a–c, SAIS is trained on \nvideo samples exclusively from USC and evaluated on those from USC (a), SAH \n(b) and HMH (c). Results are shown as an average (±1 standard deviation) of ten \nMonte Carlo cross-validation steps. d, We trained variants of SAIS to quantify \nthe marginal benefit of its components on its PPV. We removed test-time \naugmentation (‘without TTA’), RGB frames (‘without RGB’), flow maps (‘without \nflow’) and the self-attention mechanism (‘without SA’). We found that the \nattention mechanism and the multiple modality input (RGB and flow) are the \ngreatest contributors to PPV. e, We benchmarked SAIS against an I3D model \nwhen decoding subphases from entire VUA videos without human supervision. \nEach box reflects the quartiles of the results, and the whiskers extend to 1.5× the \ninterquartile range.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 784\nArticle https://doi.org/10.1038/s41551-023-01010-8\nin the surgical field of view (for example, clear view with less blood). \nFurthermore, our hypothesis for why this degradation affects certain \ngestures (such as cold cuts) more than others (such as clips) is that \nthe latter exhibits less variability than the former, and is thus easier to \nclassify by the model.\nGeneralizing across surgical procedures. While videos of different \nsurgical procedures (such as nephrectomy versus prostatectomy) \nmay exhibit variability in, for example, anatomical landmarks (such as \nkidney versus prostate), they are still likely to reflect the same tissue \ndissection gestures. We explored the degree to which such variability \naffects the ability of SAIS to decode dissection gestures. Specifically, we \ndeployed SAIS on video samples of a different surgical step: renal hilar \ndissection (HD), from a different surgical procedure: robot-assisted \npartial nephrectomy (RAPN) (Fig. 3d and Table 1  for video sample \ncount). We observed that SAIS manages to adequately generalize to an \nunseen surgical procedure, albeit exhibiting degraded performance, \nas expected (0.615 < AUC < 0.858 across the gestures). Interestingly, \na\nb c\ne f\nd\n1.0\nTrain\nVUA–USC\nAUC\nTest\nVUA–USC\n0.8\nR1 - 0.837\nC1 - 0.763\nL1 - 0.739\nR2 - 0.705\n0.6\nSensitivity 0.4\n0.2\n0 0.2 0.4 0.6\n1 – specificity\n0.8 1.0\n1.0\nTrain\nNS–USC\nAUC\nTest\nNS–USC\nTrain\nNS–USC\nTest\nHD–USC\nTrain\nNS–USC\nTest\nNS–St. Antonius\n0.8\nk—0.974\nm—0.909\nc—0.823\np—0.776\nh—0.768\nr—0.701\nAUC\nm—0.899\nk—0.831\np—0.765\nc—0.702\nh—0.696\nr—0.680\nAUC\nm—0.858\np—0.681\nc—0.665\nr—0.660\nh—0.615\n0.6\nSensitivity\n0.4\n0.2\n0\n1.0\n0.8\n0.6\nSensitivity\n0.4\n0.2\n0\n1.0\n0.8\n0.6\nSensitivity\n0.4\n0.2\n0\n0 0.2 0.4 0.6\n1 – specificity\nc k p\nh m r\n0.8 1.0 0 0.2 0.4 0.6\n1 – specificity\nHook (h) Clip (k) Cut (c)\n0.8 1.0 0 0.2 0.4 0.6\n1 – specificity\n0.8 1.0\n1.0\n0.9\nc\nh\nk\nm\np\n00:00\nTime (min)\nCamera removal Camera inspection Camera insertion\n05:00 10:00 15:00 20:00 25:00 30:00\nr\n0.8\nPrecision\n0.6\n0.7\n0.5\n0.4\n0.3\nLeft\nAnatomical location of neurovascular bundle\nRight\nFig. 3 | Decoding surgical gestures from videos. a, SAIS is trained and evaluated \non the VUA data exclusively from USC. The suturing gestures are right forehand \nunder (R1), right forehand over (R2), left forehand under (L1) and combined \nforehand over (C1). b–d, SAIS is trained on the NS data exclusively from USC and \nevaluated on the NS data from USC (b), NS data from SAH (c) and HD data from \nUSC (d). The dissection gestures are cold cut (c), hook (h), clip (k), camera move \n(m), peel (p) and retraction (r). Note that clips (k) are not used during the HD \nstep. Results are shown as an average (±1 standard deviation) of ten Monte Carlo \ncross-validation steps. e, Proportion of predicted gestures identified as correct \n(precision) stratified on the basis of the anatomical location of the neurovascular \nbundle in which the gesture is performed. f, Gesture profile where each row \nrepresents a distinct gesture and each vertical line represents the occurrence of \nthat gesture at a particular time. SAIS identified a sequence of gestures (hook, clip \nand cold cut) that is expected in the NS step of RARP procedures, and discovered \noutlier behaviour of a longer-than-normal camera move gesture corresponding to \nthe removal, inspection and re-insertion of the camera into the patient’s body.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 785\nArticle https://doi.org/10.1038/s41551-023-01010-8\nthe hook (h) gesture experienced the largest degradation in perfor -\nmance (AUC 0.768 → 0.615). We hypothesized that this was due to the \ndifference in the tissue in which a hook is performed. Whereas in the \nNS dissection step, a hook is typically performed around the prostatic \npedicles (a region of blood vessels), in the renal HD step, it is performed \nin the connective tissue around the renal artery and vein, delivering \nblood to and from the kidney, respectively.\nValidating on external video datasets. T o contextualize our work with \nprevious methods, we also trained SAIS to distinguish between sutur-\ning gestures on two publicly available datasets: JHU-ISI gesture and \nskill assessment working set ( JIGSAWS)11 and dorsal vascular complex \nUniversity College London (DVC UCL)12 (Methods). While the former \ncontains videos of participants in a laboratory setting, the latter con-\ntains videos of surgeons in a particular step (dorsal vascular complex) \nof the live robot-assisted radical prostatectomy (RARP) procedure. \nWe compare the accuracy of SAIS with that of the best-performing \nmethods on JIGSAWS (Supplementary Table 6) and DVC UCL (Sup -\nplementary Table 7).\nWe found that SAIS, despite not being purposefully designed \nfor the JIGSAWS dataset, performs competitively with the baseline \nmethods (Supplementary Table 6). For example, the best-performing \nvideo-based method achieved accuracy of 90.1, whereas SAIS achieved \naccuracy of 87.5. It is conceivable that incorporating additional modali-\nties and dataset-specific modifications into SAIS could further improve \nits performance. As for the DVC UCL dataset, we followed a different \nevaluation protocol from the one originally reported12 (see Implemen-\ntation details of training SAIS on external video datasets in Methods) \nsince only a subset of the dataset has been made public. T o fairly com-\npare the models in this setting, we quantify their improvement relative \nto a naive system that always predicts the majority gesture (Random) \n(Supplementary Table 7). We found that SAIS leads to a greater improve-\nment in performance relative to the state-of-the-art method (MA-TCN) \non the DVC UCL dataset. This is evident by the three-fold and four-fold \nincrease in accuracy achieved by MA-TCN and SAIS, respectively, rela-\ntive to a naive system.\nSAIS provides surgical gesture information otherwise \nunavailable to surgeons\nOne of the ultimate, yet ambitious, goals of SAIS is to decode surgeon \nactivity from an entire surgical video without annotations and with \nminimal human oversight. Doing so would provide surgeons with infor-\nmation otherwise less readily available to them. In pursuit of this goal, \nand as an exemplar, we deployed SAIS to decode the dissection gestures \nfrom entire NS videos from USC (20–30 min in duration) to which it has \nnever been exposed (Methods).\nQuantitative evaluation.  T o evaluate this decoding, we randomly \nselected a prediction made by SAIS for each dissection gesture category \nin each video (n = 800 gesture predictions in total). This ensured we \nretrieved predictions from a more representative and diverse set of \nvideos, thus improving the generalizability of our findings. We report \nthe precision of these predictions after manually confirming whether \nor not the corresponding video samples reflected the correct gesture \n(Fig. 3e). We further stratified this precision on the basis of the ana -\ntomical location of the neurovascular bundle relative to the prostate \ngland. This allowed us to determine whether SAIS was (a) learning an \nunreliable shortcut to decoding gestures by associating anatomical \nlandmarks with certain gestures, which is undesirable, and (b) robust \nto changes in the camera angle and direction of motion of the gesture. \nFor the latter, note that operating on the left neurovascular bundle \noften involves using the right-hand instrument and moving it towards \nthe left of the field of view (Fig. 3f, top row of images). The opposite is \ntrue when operating on the right neurovascular bundle.\nWe found that SAIS is unlikely to be learning an anatomy-specific \nshortcut to decoding gestures and is robust to the direction of motion of \nthe gesture. This is evident by its similar performance when deployed on \nvideo samples of gestures performed in the left and right neurovascular \nbundles. For example, hook (h) gesture predictions exhibited precision \nof ~0.75 in both anatomical locations. We also observed that SAIS was able \nto identify an additional gesture category beyond those it was originally \ntrained on. Manually inspecting the video samples in the cold cut (c) \ngesture category with a seemingly low precision, we found that SAIS was \nidentifying a distinct cutting gesture, also known as a hot cut, which, in \ncontrast to a cold cut, involves applying heat/energy to cut tissue.\nQualitative evaluation. T o qualitatively evaluate the performance of \nSAIS, we present its gesture predictions for a single 30-min NS video \n(Fig. 3f). Each row represents a distinct gesture, and each vertical \nline represents the occurrence of this gesture at a particular time. We \nobserved that, although SAIS was not explicitly informed about the \nrelationship between gestures, it nonetheless correctly identified a \npattern of gestures over time which is typical of the NS step within \nRARP surgical procedures. This pattern constitutes a (a) hook, (b) \nclip and (c) cold cut and is performed to separate the neurovascular \nbundle from the prostate while minimizing the degree of bleeding \nthat the patient incurs.\nWe also found that SAIS can discover outlier behaviour, despite not \nbeing explicitly trained to do so. Specifically, SAIS identified a contigu-\nous 60-s interval during which a camera move (m) was performed, and \nwhich is 60× longer than the average duration (1 s) of a camera move. \nSuspecting outlier behaviour, we inspected this interval and discovered \nthat it coincided with the removal of the camera from the patient’s \nbody, its inspection by the operating surgeon, and its re-insertion into \nthe patient’s body.\nSAIS reliably decodes surgical skills\nAt this point, we have demonstrated that SAIS, as a unified AI system, \ncan independently achieve surgical subphase recognition (the what of \nsurgery) and gesture classification (the how of surgery), and generalize \nto samples from unseen videos in the process. In this section, we exam-\nine the ability of SAIS to decode skill assessments from surgical videos. \nIn doing so, we also address the how of surgery, however through the \nlens of surgeon skill. We evaluated the quality with which two suturing \nsubphases were executed by surgeons: needle handling and needle \ndriving (Fig. 1a, right column). We trained SAIS to decode the skill level \nof these activities using video samples exclusively from USC.\nGeneralizing across videos. We deployed SAIS on the test set of video \nsamples from USC, and present the ROC curves associated with the \nskills of needle handling (Fig. 4a) and needle driving (Fig. 4b ). We \nfound that SAIS can reliably decode the skill level of surgical activity, \nachieving AUC of 0.849 and 0.821 for the needle handling and driving \nactivity, respectively.\nFig. 4 | Decoding surgical skills from videos and simultaneous provision of \nreasoning. a,b, We train SAIS on video samples exclusively from USC to decode \nthe skill-level of needle handling (a) and needle driving (b), and deploy it on video \nsamples from USC, SAH and HMH. Results are an average (±1 standard deviation) \nof ten Monte Carlo cross-validation steps. c,d, We also present the attention \nplaced on frames by SAIS for a video sample of low-skill needle handling (c) \nand needle driving (d). Images with an orange bounding box indicate that SAIS \nplaces the highest attention on frames depicting visual states consistent with \nthe respective skill assessment criteria. These criteria correspond to needle \nrepositions and needle adjustments, respectively. e, Surgical skills profile \ndepicting the skill assessment of needle handling and needle driving from a \nsingle surgical case at SAH. f,g, Ratio of low-skill needle handling (f) and needle \ndriving (g) in each of the 30 surgical cases at SAH. The horizontal dashed lines \nrepresent the average ratio of low-skill activity at USC.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 786\nArticle https://doi.org/10.1038/s41551-023-01010-8\nGeneralizing across hospitals. We also deployed SAIS on video sam-\nples from unseen surgeons at two hospitals: SAH and HMH (Fig. 4a,b \nand Table 1 for video sample count). This is a challenging task that \nrequires SAIS to adapt to the potentially different ways in which surgi-\ncal activities are executed by surgeons with different preferences. We \nfound that SAIS continued to reliably decode the skill level of needle \na b\n1.0\nAUC\nSAH 0.880\nUSC 0.849\nHMH 0.804\n0.8\n0.6\nSensitivity\n0.4\n0.2\n1.0\n0.8\n0.6\nSensitivity 0.4\n0.2\n0\n0 0.2 0.4 0.6\n1 – specificity\n0.8 1.0 0 0.2 0.4 0.6\n 1 – specificity\n0.8 1.0\nAUC\nUSC 0.821\nSAH 0.797\nHMH 0.719\nc\ne\ndNeedle reposition 1 Needle driving 1\nLow importance\nGround-truth skill assessment\nHigh\nLow\nHandling skillDriving skill\nHigh\nLow\n1 4 7 10 13 16\nStitch number\n19 22 25 28\nAI skill assessment\nHigh importance\nFrameFrame\nNeedle adjustment\nNeedle reposition 2 Needle withdrawal Needle driving 2\nf g\nRatio of low-skill handling\n0.4\nAverage ratio of\nlow-skill handling\nat USC\nAverage ratio of\nlow-skill driving\nat USC\nSurgical case ID\nCases with higher\nthan average ratio\nof low-skill driving\n0.3\n0.2\n0.1\n0\nRatio of low-skill driving\n0.4\n0.5\n0.3\n0.2\n0.1\n0\n29 35 60 48 67 104 4148 99 16 60\nSurgical case ID\n77 32 26\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 787\nArticle https://doi.org/10.1038/s41551-023-01010-8\nhandling (SAH: AUC 0.880, HMH: AUC 0.804) and needle driving (SAH: \nAUC 0.821, HMH: AUC 0.719). The ability of SAIS to detect consistent \npatterns across hospitals points to its potential utility for the objective \nassessment of surgical skills.\nBenchmarking against baseline models. Variants of the 3D convolu-\ntional neural network (3D-CNN) have achieved state-of-the-art results \nin decoding surgical skills on the basis of videos of either a laboratory \ntrial6 or a live procedure13. As such, to contextualize the utility of SAIS, \nwe fine-tuned a pre-trained I3D model (see Implementation details \nof I3D experiments in Methods) to decode the skill level of needle \nhandling and needle driving (Table 2). We found that SAIS consistently \noutperforms this state-of-the-art model when decoding the skill level \nof surgical activities across hospitals. For example, when decoding the \nskill level of needle handling, SAIS and I3D achieved AUC of 0.849 and \n0.681, respectively. When decoding the skill level of needle driving, \nthey achieved AUC of 0.821 and 0.630, respectively. We also found \nthat I3D was more sensitive to the video samples it was trained on \nand the initialization of its parameters. This is evident by the higher \nstandard deviation of its performance relative to that of SAIS across \nthe folds (0.12 versus 0.05 for needle driving at USC). Such sensitivity \nis undesirable as it points to the lack of robustness and unpredictable \nbehaviour of the model.\nSAIS provides accurate reasoning behind decoding of  \nsurgical skills\nThe safe deployment of clinical AI systems often requires that they are \ninterpetable14. We therefore wanted to explore whether or not SAIS was \nidentifying relevant visual cues while decoding the skill level of surgeons. \nThis would instill machine learning practitioners with confidence that \nSAIS is indeed latching onto appropriate features, and can thus be trusted \nin the event of future deployment within a clinical setting. We first \nretrieved a video sample depicting a low-skill activity (needle handling \nor needle driving) that was correctly classified by SAIS. By inspecting the \nattention placed on such frames by the attention mechanism (architec-\nture in Fig. 5), we were able to quantify the importance of each frame. \nIdeally, high attention is placed on frames of relevance, where relevance \nis defined on the basis of the skill being assessed.\nWe present the attention (darker is more important) placed on \nframes of a video sample of needle handling (Fig. 4c) and needle driving \n(Fig. 4d) and that was correctly classified by SAIS as depicting low skill. We \nfound that SAIS places the most attention on frames that are consistent \nwith the skill assessment criteria. For example, with the low-skill needle \nhandling activity based on the number of times a needle is re-grasped by a \nsurgeon, we see that the most important frames highlight the time when \nboth robotic arms simultaneously hold onto the needle, which is charac-\nteristic of a needle reposition manoeuvre (Fig. 4c). Multiple repetitions \nof this behaviour thus align well with the low-skill assessment of needle \nhandling. Additionally, with needle driving assessed as low-skill based \non the smoothness of its trajectory, we see that the needle was initially \ndriven through the tissue, adjusted, and then completely withdrawn \n(opposite to direction of motion) before being re-driven through the \ntissue seconds later (Fig. 4d). SAIS placed a high level of attention on the \nwithdrawal of the needle and its adjustment and was thus in alignment \nwith the low-skill assessment of needle driving. More broadly, these \nexplainable findings suggest that SAIS is not only capable of providing \nsurgeons with a reliable, objective, and scalable assessment of skill but \ncan also pinpoint the important frames in the video sample. This capa-\nbility addresses why a low-skill assessment was made and bodes well for \nwhen SAIS is deployed to provide surgeons with targeted feedback on \nhow to improve their execution of surgical skills.\nSAIS provides surgical skill information otherwise unavailable \nto surgeons\nWe wanted to demonstrate that SAIS can also provide surgeons with \ninformation about surgical skills that otherwise would not have been \navailable to them. T o that end, we tasked SAIS with assessing the skill of \nall needle handling and needle driving video samples collected from SAH.\nWith needle handling (and needle driving) viewed as a subphase of \na single stitch and knowing that a sequence of stitches over time makes \nup a suturing activity (such as VUA) in a surgical case, SAIS can generate \na surgical skills profile for a single case (Fig. 4e) for needle handling \nand needle driving. We would like to emphasize that this profile, when \ngenerated for surgical cases that are not annotated with ground-truth \nskill assessments, provides surgeons with actionable information \nthat otherwise would not have been available to them. For example, a \ntraining surgeon can now identify temporal regions of low-skill stitch \nactivity, relate that to anatomical locations perhaps, and learn to focus \non such regions in the future. By decoding profiles for different skills \nwithin the same surgical case, a surgeon can now identify whether \nsubpar performance for one skill (such as needle handling) correlates \nwith that for another skill (such as needle driving). This insight will help \nguide how a surgeon practises such skills.\nSAIS can also provide actionable information beyond the indi -\nvidual surgical case level. T o illustrate this, we present the proportion of \nneedle handling (Fig. 4f) and needle driving (Fig. 4g) actions in a surgi-\ncal case that were deemed low-skill, for all 30 surgical cases from SAH. \nWe also present the average low-skill ratio observed in surgical videos \nfrom USC. With this information, the subset of cases with the lowest \nrate of low-skill actions can be identified and presented to training \nsurgeons for educational purposes. By comparing case-level ratios to \nthe average ratio at different hospitals (Fig. 4g), surgeons can identify \ncases that may benefit from further surgeon training.\nSAIS can provide surgeons with actionable feedback\nWe initially claimed that the decoding of intraoperative surgical activ-\nity can pave the way for multiple downstream applications, one of \nwhich is the provision of postoperative feedback to surgeons on their \noperating technique. Here we provide a template of how SAIS, based \non the findings we have presented thus far, can deliver on this goal. In \nreliably decoding surgical subphases and surgical skills while simultane-\nously providing its reasoning for doing so, SAIS can provide feedback \nof the following form: ‘when completing stitch number three of the \nsuturing step, your needle handling (what—subphase) was executed \npoorly (how—skill). This is probably due to your activity in the first \nand final quarters of the needle handling subphase (why—attention)’ . \nSuch granular and temporally localized feedback now allows a surgeon \nto better focus on the element of intraoperative surgical activity that \nrequires improvement, a capability that was not previously available.\nThe skill assessments of SAIS are associated with patient \noutcomes\nWhile useful for mastering a surgical technical skill itself, surgeon \nfeedback becomes more clinically meaningful when grounded in \nTable 2 | SAIS outperforms a state-of-the-art model when \ndecoding the skill level of surgical activity. SAIS is trained \non video samples exclusively from USC. We report the \naverage AUC (±1 standard deviation) on the test-set across \nall of the ten folds\nActivity Hospital I3D (ref. 6) SAIS\nNeedle handling USC 0.681 (0.07) 0.849 (0.06)\nSAH 0.730 (0.04) 0.880 (0.02)\nHMH 0.680 (0.04) 0.804 (0.03)\nNeedle driving USC 0.630 (0.12) 0.821 (0.05)\nSAH 0.656 (0.08) 0.797 (0.04)\nHMH 0.571 (0.07) 0.719 (0.06)\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 788\nArticle https://doi.org/10.1038/s41551-023-01010-8\npatient outcomes. For example, if low-skill assessments are associated \nwith poor outcomes, then a surgeon can begin to modulate specific \nbehaviour to improve such outcomes. T o that end, we conducted a \npreliminary analysis regressing the surgeon skill assessments of SAIS \nat USC onto a patient’s binary recovery of urinary continence (ability \nto voluntarily control urination) 3 months after surgery (Methods). \nWhen considering all video samples (multiple per surgical case), and \ncontrolling for surgeon caseload and patient age, we found that urinary \ncontinence recovery was 1.31× (odds ratio (OR), confidence interval (CI) \n1.08–1.58, P = 0.005) more likely when needle driving was assessed as \nhigh skill than as low skill by SAIS. When aggregating the skill assess-\nments of video samples within a surgical case, that relationship is fur-\nther strengthened (OR 1.89, CI 0.95–3.76, P = 0.071). These preliminary \nfindings are consistent with those based on manual skill assessments \nfrom recent studies\n15,16.\nDiscussion\nOnly in the past decade or so has it been empirically demonstrated that \nintraoperative surgical activity can have a direct influence on postop-\nerative patient outcomes. However, discovering and acting upon this \nrelationship to improve outcomes is challenging when the details of \nintraoperative surgical activity remain elusive. By combining emerging \ntechnologies such as AI with videos commonly collected during robotic \nsurgeries, we can begin to decode multiple elements of intraoperative \nsurgical activity.\nWe have shown that SAIS can decode surgical subphases, gestures \nand skills, on the basis of surgical video samples, in a reliable, objective \nand scalable manner. Although we have presented SAIS as decoding \nthese specific elements in robotic surgeries, it can conceivably be \napplied to decode any other element of intraoperative activity from \ndifferent surgical procedures. Decoding additional elements of sur -\ngery will simply require curating a dataset annotated with the surgical \nelement of interest. T o facilitate this, we release our code such that \nothers can extract insight from their own surgical videos with SAIS. In \nfact, SAIS and the methods that we have presented in this study apply \nto any field in which information can be decoded on the basis of visual \nand motion cues.\nCompared with previous studies, our study offers both transla -\ntional and methodological contributions. From a translational stand-\npoint, we demonstrated the ability of SAIS to generalize across videos, \nsurgeons, surgical procedures and hospitals. Such a finding is likely \nto instil surgeons with greater confidence in the trustworthiness of \nSAIS, and therefore increases their likelihood of adopting it. This is \nin contrast to previous work that has evaluated AI systems on videos \ncaptured in either a controlled laboratory environment or a single \nhospital, thereby demonstrating limited generalization capabilities.\nFrom a methodological standpoint, SAIS has much to offer com-\npared with AI systems previously developed for decoding surgical \nactivity. First, SAIS is unified in that it is capable of decoding multiple \nelements of intraoperative surgical activity without any changes to its \nunderlying architecture. By acting as a dependable core architecture \naround which future developments are made, SAIS is likely to reduce \nthe amount of resources and cognitive burden associated with devel-\noping AI systems to decode additional elements of surgical activity. \nThis is in contrast to the status quo in which the burdensome process \nof developing specialized AI systems must be undertaken to decode \njust a single element. Second, SAIS provides explainable findings in that \nit can highlight the relative importance of individual video frames in \ncontributing to the decoding. Such explainability, which we system-\natically investigate in a concurrent study 17 is critical to gaining the \ntrust of surgeons and ensuring the safe deployment of AI systems for \nhigh-stakes decision making such as skill-based surgeon credentialing. \nThis is in contrast to previous AI systems such as MA-TCN12, which is \nonly capable of highlighting the relative importance of data modalities  \nPre-trained \nViT\nPre-trained \nViT\nPre-trained \nViT\nTransformer \nencoders\nPre-trained \nViT\nPre-trained \nViT\nPre-trained \nViT\nTransformer\nencoders\nProjection \nhead\nCategory 1 Category 2\nAttraction Repulsion\nStep 1:\nextract spatial features\nStep 2:\ncapture relationship \nbetween frames\nStep 3:\naggregate modality-\nspecific features\nModality-specific \nframe feature\nModality-specific\nvideo feature \nVideo feature Prototype\nStep 4:\nidentify prototype \nclosest to video feature\nSurgical video frames over time Optical flow maps over time\nFig. 5 | A vision-and-attention-based AI system. SAIS consists of two parallel \nstreams that process distinct input data modalities: RGB surgical videos and \noptical flow. Irrespective of the data modality, features are extracted from each \nframe via a ViT pre-trained in a self-supervised manner on ImageNet. Features \nof video frames are then input into a stack of transformer encoders to obtain a \nmodality-specific video feature. These modality- specific features are aggregated \nand passed into a projection head to obtain a single video feature, which is either \nattracted to, or repelled from, the relevant prototype. Although we illustrate \ntwo prototypes to reflect binary categories (high-skill activity versus low-skill \nactivity), we would have C prototypes in a setting with C categories.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 789\nArticle https://doi.org/10.1038/s41551-023-01010-8\n(for example, images versus kinematics), and therefore lacks the finer \nlevel of explainability of SAIS.\nSAIS is also flexible in that it can accept video samples with an \narbitrary number of video frames as input, primarily due to its trans-\nformer architecture. Such flexibility, which is absent from previous \ncommonly used models such as 3D-CNNs, confers benefits to train -\ning, fine-tuning and performing inference. During training, SAIS can \naccept a mini-batch of videos each with a different number of frames. \nThis can be achieved by padding videos in the mini-batch (with zeros) \nthat have fewer frames, and appropriately masking the attention \nmechanism in the transformer encoder (see Implementation details \nand hyperparameters in Methods). This is in contrast to existing AI \nsystems, which must often be presented with a mini-batch of equally \nsized videos. Similarly, during fine-tuning or inference, SAIS can be \npresented with an arbitrary number of video frames, thus expanding \nthe spectrum of videos that it can be presented with. This is in contrast \nto existing setups that leverage a 3D-CNN that has been pre-trained \non the Kinetics dataset18, whereby video samples must contain either \n16 frames or multiples thereof 6,13. Abiding by this constraint can be \nsuboptimal for achieving certain tasks, and departing from it implies \nthe inability to leverage the pre-trained parameters that have proven \ncritical to the success of previous methods. Furthermore, SAIS is \narchitecturally different from previous models in that it learns proto-\ntypes via supervised contrastive learning to decode surgical activity, \nan approach that has yet to be explored with surgical videos. Such \nprototypes pave the way for multiple downstream applications from \ndetecting out-of-distribution video samples, to identifying clusters \nof intraoperative activity, and retrieving samples from a large surgi-\ncal database19.\nWe also showed that SAIS can provide information that otherwise \nwould not have been readily available to surgeons. This includes sur-\ngical gesture and skill profiles, which reflect how surgical activity is \nexecuted by a surgeon over time for a single surgical case and across \ndifferent cases. Such capabilities pave the way for multiple downstream \napplications that otherwise would have been difficult to achieve. For \nexample, from a scientific perspective, we can now capture the vari -\nability of surgical activity across time, surgeons and hospitals. From a \nclinical perspective, we can now test hypotheses associating intraop-\nerative surgical activity with long-term patient outcomes. This brings \nthe medical community one step closer to identifying, and eventually \nmodulating, causal factors responsible for poor outcomes. Finally, \nfrom an educational perspective, we can now monitor and provide \nsurgeons with feedback on their operating technique. Such feedback \ncan help surgeons master necessary skills and contribute to improved \npatient outcomes.\nThere are important challenges our work does not yet address. \nFirst, our framework, akin to others in the field, is limited to only decod-\ning the elements of surgical activity that have been previously outlined \nin some taxonomy (such as gestures). In other words, it cannot decode \nwhat it does not know. Although many of these taxonomies have been \nrigorously developed by teams of surgeons and through clinical experi-\nence, they may fail to shed light on other intricate aspects of surgical \nactivity. This, in turn, limits the degree to which automated systems can \ndiscover novel activity that falls beyond the realm of existing protocol. \nSuch discovery can lend insight into, for example, optimal but as-of-yet \nundiscovered surgical behaviour. In a similar vein, SAIS is currently \nincapable of decoding new elements of surgical activity beyond those \ninitially presented to it. Such continual learning capabilities10 are criti-\ncal to adapting to an evolving taxonomy of surgical activity over time.\nThe goal of surgery is to improve patient outcomes. However, it \nremains an open question whether or not the decoded elements of \nintraoperative surgical activity: subphases, gestures and skills, are the \nfactors most predictive of postoperative patient outcomes. Although \nwe have presented preliminary evidence in this direction for the case of \nsurgical skills, large-scale studies are required to unearth these relation-\nships. T o further explore these relationships and more reliably inform \nfuture surgical practice, we encourage the public release of large-scale \nsurgical video datasets from different hospitals and surgical specialties. \nEquipped with such videos and SAIS, researchers can begin to decode \nthe various elements of surgery at scale.\nMoving forward, we look to investigate whether SAIS has the \nintended effect on clinical stakeholders. For example, we aim to deploy \nSAIS in a controlled laboratory environment to assess the skill level \nof activity performed by medical students and provide them with \nfeedback based on such assessments. This will lend practical insight \ninto the utility of AI-based skill assessments and its perception by sur-\ngical trainees. We also intend to explore the interdependency of the \nelements of intraoperative surgical activity (subphase recognition, \ngesture classification and skill assessment). This can be achieved, for \nexample, by training a multi-task variant of SAIS in which all elements \nare simultaneously decoded from a video. In such a setting, positive \ninterference between the tasks could result in an even more reliable \ndecoding. Alternatively, SAIS can be trained to first perform subphase \nrecognition (a relatively easy task) before transferring its parameters \nto perform skill assessment (a relatively harder task). This is akin to cur-\nriculum learning20, whereby an AI system is presented with increasingly \ndifficult tasks during the learning process in order to improve its overall \nperformance. In a concurrent study 21, we also investigate whether \nSAIS exhibits algorithmic bias against various surgeon subcohorts22. \nSuch a bias analysis is particularly critical if SAIS is to be used for the \nprovision of feedback to surgeons. For example, it may disadvantage \ncertain surgeon subcohorts (such as novices with minimal experience) \nand thus affect their ability to develop professionally.\nMethods\nEthics approval\nAll datasets (data from USC, SAH, and HMH) were collected under \ninstitutional review board approval in which informed consent was \nobtained (HS-17-00113). These datasets were de-identified before \nmodel development.\nPrevious work\nComputational methods.  Previous work has used computational \nmethods, such as AI, to decode surgery 23,24. One line of research has \nfocused on exploiting robot-derived sensor data, such as the displace-\nment and velocity of the robotic arms (kinematics), to predict clinical \noutcomes25–28. For example, researchers have used automated per -\nformance metrics to predict a patient’s postoperative length of stay \na\nb\nFig. 6 | ViT feature extractor places the highest importance on instrument \ntips, needles and anatomical edges. We present two sample RGB video frames \nof the needle handling activity and the corresponding spatial attention placed by \nViT on patches of these frames.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 790\nArticle https://doi.org/10.1038/s41551-023-01010-8\nwithin a hospital 26. Another line of research has instead focused on \nexclusively exploiting live surgical videos from endoscopic cameras \nto classify surgical activity4,29, gestures5,30–33 and skills6,7,13,34,35, among \nother tasks36,37. For information on additional studies, we refer readers \nto a recent review9. Most recently, attention-based neural networks \nsuch as transformers38 have been used to distinguish between distinct \nsurgical steps within a procedure39–42.\nEvaluation setups.  Previous work often splits their data in a way \nthat has the potential for information ‘leakage’ across training and \ntest sets. For example, it is believed that the commonly adopted \nleave-one-user-out evaluation setup on the JIGSAWS dataset11 is rigor-\nous. Although it lends insight into the generalizability of a model to \na video from an unseen participant, this setup involves reporting a \ncross-validation score, which is often directly optimized by previous \nmethods (for example, through hyperparameter tuning), therefore \nproducing an overly optimistic estimate of performance. As another \nexample, consider the data split used for the CholecT50 dataset43. Here \nthere is minimal information about whether videos in the training and \ntest sets belong to the same surgeon. Lastly, the most recent DVC UCL \ndataset12 consists of 36 publicly available videos for training and 9 pri-\nvate videos for testing. After manual inspection, we found that these \nnine videos come from six surgeons whose data are also in the training \nset. This is a concrete example of surgeon data leakage, and as such, \nwe caution the use of such datasets for benchmarking purposes. It is \ntherefore critical to more rigorously evaluate the performance of SAIS, \nand in accordance with how it is likely to be deployed in a clinical setting.\nDescription of surgical procedures and activities\nWe focused on surgical videos depicting two types of surgical activity \ncommonly performed within almost any surgery: tissue dissection and \nsuturing, which we next outline in detail.\nTissue dissection. Tissue dissection is a fundamental activity in almost \nany surgical procedure and involves separating pieces of tissue from \none another. For example, the RARP surgical procedure, where a can-\ncerous prostate gland is removed from a patient’s body, entails several \ntissue dissection steps, one of which is referred to as nerve-sparing, or \nNS. NS involves preserving the neurovascular bundle, a mesh of vascu-\nlature and nerves to the left and right of the prostate, and is essential \nfor a patient’s postoperative recovery of erectile function for sexual \nintercourse. Moreover, an RAPN surgical procedure, where a part of \na cancerous kidney is removed from a patient’s body, entails a dissec-\ntion step referred to as hilar dissection, or HD. HD involves removing \nthe connective tissue around the renal artery and vein to control any \npotential bleeding from these blood vessels.\nThese dissection steps (NS and HD), although procedure specific \n(RARP and RAPN), are performed by a surgeon through a common \nvocabulary of discrete dissection gestures. In our previous work, we \ndeveloped a taxonomy44 enabling us to annotate any tissue dissection \nstep with a sequence of discrete dissection gestures over time.\nTissue suturing. Suturing is also a fundamental component of sur -\ngery45 and involves bringing tissue together. For example, the RARP \nprocedure entails a suturing step referred to as vesico-urethral anasto-\nmosis, or VUA. VUA follows the removal of the cancerous prostate gland \nand involves connecting, via stitches, the bladder neck (a spherical \nstructure) to the urethra (a cylindrical structure), and is essential for \npostoperative normal flow of urine. The VUA step typically consists \nof an average of 24 stitches where each stitch can be performed by a \nsurgeon through a common vocabulary of suturing gestures. In our \nprevious work, we developed a taxonomy 5 enabling us to annotate \nany suturing activity with a sequence of discrete suturing gestures. \nWe note that suturing gestures are different to, and more subtle than, \ndissection gestures.\nEach stitch can also be deconstructed into the three recurring sub-\nphases of (1) needle handling, where the needle is held in preparation \nfor the stitch, (2) needle driving, where the needle is driven through \ntissue (such as the urethra), and (3) needle withdrawal, where the needle \nis withdrawn from tissue to complete a single stitch. The needle han-\ndling and needle driving subphases can also be evaluated on the basis \nof the skill level with which they are executed. In our previous work, we \ndeveloped a taxonomy46 enabling us to annotate any suturing subphase \nwith a binary skill level (low skill versus high skill).\nSurgical video samples and annotations\nWe collected videos of entire robotic surgical procedures from three \nhospitals: USC, SAH and HMH. Each video of the RARP procedure, for \nexample, was on the order of 2 h. A medical fellow (R.M.) manually \nidentified the NS tissue dissection step and VUA tissue suturing step \nin each RARP video. We outline the total number of videos and video \nsamples from each hospital in Table 1. We next outline how these steps \nwere annotated with surgical subphases, gestures and skill levels.\nIt is important to note that human raters underwent a training \nphase whereby they were asked to annotate the same set of surgical vid-\neos, allowing for the calculation of the inter-rater reliability (between \n0 and 1) of their annotations. Once this reliability exceeded 0.8, we \ndeemed the training phase complete47.\nSurgical gesture annotations. Each video of the NS dissection step \n(on the order of 20 min) was retrospectively annotated by a team of \ntrained human raters (R.M., T.H. and others) with tissue dissection ges-\ntures. This annotation followed the strict guidelines of our previously \ndeveloped taxonomy of dissection gestures44. We focused on the six \nmost commonly used dissection gestures: cold cut (c), hook (h), clip \n(k), camera move (m), peel (p) and retraction (r). Specifically, upon \nobserving a gesture, a human rater recorded the start time and end \ntime of its execution by the surgeon. Therefore, each NS step resulted \nin a sequence of n ≈ 400 video samples of gestures (from six distinct \ncategories) with each video sample on the order of 0–10 s in duration. \nMoreover, each video sample mapped to one and only one gesture. The \nsame strategy was followed for annotating the VUA suturing step with \nsuturing gestures. This annotation followed the strict guidelines of our \npreviously developed taxonomy of suturing gestures5. We focused on \nthe four most commonly used suturing gestures: right forehand under \n(R1), right forehand over (R2), left forehand under (L1) and combined \nforehand over (C1).\nSurgical subphase and skill annotations.  Each video of the VUA \nsuturing step (on the order of 20 min) was retrospectively annotated by \na team of trained human raters (D.K., T.H. and others) with surgical sub-\nphases and skills. This annotation followed the strict guidelines of our \npreviously developed taxonomy referred to as the end-to-end assess-\nment of suturing expertise or EASE 46. Since the VUA step is a recon -\nstructive one in which the bladder and urethra are joined together, it \noften requires a series of stitches (on the order of 24 stitches: 12 on the \nbladder side and another 12 on the urethral side).\nWith a single stitch consisting of the three subphases of needle \nhandling, needle driving and needle withdrawal (always in that order), \na human rater would first identify the start time and end time of each \nof these subphases. Therefore, each VUA step may have n = 24 video \nsamples of the needle handling, needle driving and needle withdrawal \nsubphases with each video sample on the order of 10–30 s. The dis -\ntribution of the duration of such video samples is provided in Sup -\nplementary Note 2.\nHuman raters were also asked to annotate the quality of the nee-\ndle handling or needle driving activity (0 for low skill and 1 for high \nskill). For needle handling, a high-skill assessment is based on the \nnumber of times the surgeon must reposition their grip on the needle \nin preparation for driving it through the tissue (the fewer the better). \nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 791\nArticle https://doi.org/10.1038/s41551-023-01010-8\nFor needle driving, a high-skill assessment is based on the smoothness \nand number of adjustments required to drive the needle through the \ntissue (the smoother and fewer number of adjustments the better). \nSince each video sample was assigned to multiple raters, it had multi-\nple skill assessment labels. In the event of potential disagreements in \nannotations, we considered the lowest (worst) score. Our motivation \nfor doing so was based on the assumption that if a human rater penal-\nized the quality of the surgeon’s activity, then it must have been due \nto one of the objective criteria outlined in the scoring system, and \nis thus suboptimal. We, in turn, wanted to capture and encode this \nsuboptimal behaviour.\nMotivation behind evaluating SAIS with Monte Carlo \ncross-validation\nIn all experiments, we trained SAIS on a training set of video samples \nand evaluated it using ten-fold Monte Carlo cross- validation where \neach fold’s test set consisted of subphases from videos unseen during \ntraining. Such an approach contributes to our goal of rigorous evalua-\ntion by allowing us to evaluate the ability of SAIS to generalize to unseen \nvideos (hereon referred to as across videos). This setup is also more \nchallenging and representative of real-world deployment than one \nin which an AI system generalizes to unseen samples within the same \nvideo. As such, we adopted this evaluation setup for all experiments \noutlined in this study, unless otherwise noted. A detailed breakdown of \nthe number of video samples used for training, validation and testing \ncan be found in Supplementary Note 1.\nData splits. For all the experiments conducted, unless otherwise noted, \nwe split the data at the case video level into a training (90%) and test set \n(10%). We used 10% of the videos in the training set to form a validation \nset with which we performed hyperparameter tuning. By splitting at \nthe video level, whereby data from the same video do not appear across \nthe sets, we are rigorously evaluating whether the model generalizes \nacross unseen videos. Note that, while it is possible for data from the \nsame surgeon to appear in both the training and test sets, we also \nexperiment with even more rigorous setups: across hospitals—where \nvideos are from entirely different hospitals and surgeons—and across \nsurgical procedures—where videos are from entirely different surgi-\ncal procedures (such as nephrectomy versus prostatectomy). While \nthere are various ways to rigorously evaluate SAIS, we do believe that \ndemonstrating its generalizability across surgeons, hospitals and \nsurgical procedures, as we have done, is a step in the right direction. \nWe report the performance of models as an average, with a standard \ndeviation, across the folds.\nLeveraging both RGB frames and optical flow\nT o capture both visual and motion cues in surgical videos, SAIS oper-\nated on two distinct modalities: live surgical videos in the form of RGB \nframes and the corresponding optical flow of such frames. Surgical \nvideos can be recorded at various sampling rates, which have the units \nof frames per second (fps).\nKnowledge of the sampling rate alongside the natural rate with \nwhich activity occurs in a surgical setting is essential to multiple deci-\nsions. These can range from the number of frames to present to a deep \nlearning network, and the appropriate rate with which to downsample \nvideos, to the temporal step size used to derive optical flow maps, as \noutlined next. Including too many frames where there is very little \nchange in the visual scene leads to a computational burden and may \nresult in over-fitting due to the inclusion of highly similar frames (low \nvisual diversity). On the other hand, including too few frames might \nresult in missing visual information pertinent to the task at hand. Simi-\nlarly, deriving reasonable optical flow maps, which is a function of a pair \nof images which are temporally spaced, is contingent upon the time that \nhas lapsed between such images. T oo short of a timespan could result \nin minimal motion in the visual scene, thus resulting in uninformative \noptical flow maps. Analogously, too long of a timespan could mean \nmissing out on informative intermediate motion in the visual scene. \nWe refer to these decisions as hyperparameters (see Implementation \ndetails and hyperparameters section in Methods). Throughout this \npaper, we derived optical flow maps by deploying a RAFT model 48, \nwhich we found to provide reasonable maps.\nSAIS is a model for decoding activity from surgical videos\nOur AI system—SAIS—is vision based and unified (Fig. 5 ). It is vision \nbased as it operates exclusively on surgical videos routinely collected \nas part of robotic surgical procedures. It is unified as the same archi-\ntecture, without any modifications, can be used to decode multiple \nelements of intraoperative surgical activity (Fig. 1b ). We outline the \nbenefits of such a system in Discussion.\nSingle forward pass through SAIS\nExtracting spatial features. We extract a sequence of D-dimensional \nrepresentations, {vt ∈ℝ D}\nT\nt=1, from T temporally ordered frames via a \n(frozen) vision transformer (ViT) pre-trained on the ImageNet dataset \nin a self-supervised manner49. In short, this pre-training setup, entitled \nDINO, involved optimizing a contrastive objective function whereby \nrepresentations of the same image, augmented in different ways (such \nas random cropping), are encouraged to be similar to one another. For \nmore details, please refer to the original paper50.\nViTs convert each input frame into a set of square image patches \nof dimension H  × H and introduce a self-attention mechanism that \nattempts to capture the relationship between image patches (that \nis, spatial information). We found that this spatial attention picks up \non instrument tips, needles, and anatomical edges (Fig. 6). We chose \nthis feature extractor on the basis of (a) recent evidence favouring \nself-supervised pre-trained models relative to their supervised counter-\nparts and (b) the desire to reduce the computational burden associated \nwith training a feature extractor in an end-to-end manner.\nExtracting temporal features. We append a learnable D-dimensional \nclassification embedding, ecls ∈ℝ D, to the beginning of the sequence \nof frame representations, {vt}\nT\nt=1. T o capture the temporal ordering of \nthe frames of the images, we add D-dimensional temporal positional \nembeddings, {et ∈ℝ D}\nT\nt=1, to the sequence of frame representations \nbefore inputting the sequence into four Transformer encoder layers. \nSuch an encoder has a self-attention mechanism whereby each frame \nattends to every other frame in the sequence. As such, both short- and \nlong-range dependencies between frames are captured. We summarize \nthe modality-specific video through a modality-specific video repre-\nsentation, hcls ∈ℝ D, of the classification embedding, e cls, at the final \nlayer of the Transformer encoder, as is typically done. This process is \nrepeated for the optical flow modality stream.\nAggregating modality-specific features. The two modality-specific \nvideo representations, hRGB and hFlow, are aggregated as follows:\nhagg = hRGB +hFlow (1)\nThe aggregated representation, hagg, is passed through two projection \nheads, in the form of linear layers with a non-linear activation function \n(ReLU), to obtain an E-dimensional video representation, hVideo ∈ℝ E.\nTraining protocol for SAIS.  T o achieve the task of interest, the \nvideo-specific representation, hVideo, undergoes a series of attractions \nand repulsions with learnable embeddings, which we refer to as pro -\ntotypes. Each prototype, p, reflects a single category of interest and is \nof the same dimensionality as hVideo. The representation, hVideo ∈ℝ E, of \na video from a particular category, c, is attracted to the single proto -\ntype, pc ∈ℝ E, associated with the same category, and repelled from all \nother prototypes, {pj}\nC\nj=1,j≠ c, where C is the total number of categories. \nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 792\nArticle https://doi.org/10.1038/s41551-023-01010-8\nWe achieve this by leveraging contrastive learning and minimizing the \nInfoNCE loss, ℒNCE:\nℒNCE =−\nB\n∑\ni=1\nlog\nes(hVideo,pc)\n∑j es(hVideo,pj)\ns(hVideo,pj) =\nhVideo⋅pj\n||hVideo||||pj|\n|\n(2)\nDuring training, we share the parameters of the Transformer \nencoder across modalities to avoid over-fitting. As such, we learn, in an \nend-to-end manner, the parameters of the Transformer encoder, the clas-\nsification token embedding, the temporal positional embeddings, the \nparameters of the projection head and the category-specific prototypes.\nEvaluation protocol for SAIS. T o classify a video sample into one of \nthe categories, we calculate the similarity (that is, cosine similarity) \nbetween the video representation, hVideo, and each of the prototypes, \n{pj}\nC\nj=1. We apply the softmax function to these similarity values in order \nto obtain a probability mass function over the categories. By identifying \nthe category with the highest probability mass (argmax), we can make \na classification.\nThe video representation, hVideo, can be dependent on the choice \nof frames (both RGB and optical flow) which are initially input into the \nmodel. Therefore, to account for this dependence and avoid missing \npotentially informative frames during inference, we deploy what is \nknown as test-time augmentation (TTA). This involves augmenting the \nsame input multiple times during inference, which, in turn, outputs \nmultiple probability mass functions. We can then average these prob-\nability mass functions, analogous to an ensemble model, to make a \nsingle classification. In our context, we used three test-time inputs; the \noriginal set of frames at a fixed sampling rate, and those perturbed by \noffsetting the start frame by K frames at the same sampling rate. Doing \nso ensures that there is minimal frame overlap across the augmented \ninputs, thus capturing different information, while continuing to span \nthe most relevant aspects of the video.\nImplementation details and hyperparameters\nDuring training and inference, we use the start time and end time of \neach video sample to guide the selection of video frames from that \nsample. For gesture classification, we select ten equally spaced frames \nfrom the video sample. For example, for a video sample with a frame \nrate of 30 Hz and that is 3 s long, then from the original 30 × 3 = 90 \nframes, we would only retrieve frames ∈ [0, 9, 18, …]. In contrast, for \nsubphase recognition and skill assessment, we select every other tenth \nframe. For example, for the same video sample above, we would only \nretrieve frames ∈ [0, 10, 20,…]. We found that these strategies resulted \nin a good trade-off between computational complexity and capturing \nsufficiently informative signals in the video to complete the task. Simi-\nlarly, optical flow maps were based on pairs of images that were 0.5 s \napart. Shorter timespans resulted in frames that exhibited minimal \nmotion and thus uninformative flow maps. During training, to ensure \nthat the RGB and optical flow maps were associated with the same \ntimespan, we retrieved maps that overlapped in time with the RGB \nframes. During inference, and for TTA, we offset both RGB and optical \nflow frames by K = 3 and K = 6 frames.\nWe conduct our experiments in PyT orch51 using a V100 GPU on \na DGX machine. Each RGB frame and optical flow map was resized to \n224 × 224 (from 960 × 540 at USC and SAH and 1,920 × 1,080 at SAH) \nbefore being input into the ViT feature extractor. The ViT feature extrac-\ntor pre-processed each frame into a set of square patches of dimension \nH = 16 and generated a frame representation of dimension D  = 384. \nAll video representations and prototypes are of dimension E = 256. In \npractice, we froze the parameters of the ViT, extracted all such repre-\nsentations offline (that is, before training), and stored them as h5py \nfiles. We followed the same strategy for extracting representations \nof optical flow maps. This substantially reduced the typical bottle -\nneck associated with loading videos and streamlined our training and \ninference process. This also facilitates inference performed on future \nvideos. Once a new video is recorded, its features can immediately be \nextracted in an offline manner, and stored for future use.\nUnless otherwise stated, we trained SAIS using a mini-batch size \nof eight video samples and a learning rate of 1e −1, and optimize its \nparameters via stochastic gradient descent. Mini-batch samples are \noften required to have the same dimensionality ( B × T × D) where B \nis the batch size, T is the number of frames and D is the dimension of \nthe stored representation. Therefore, when we encountered video \nsamples in the same mini-batch with a different number of temporal \nframes (such as T  = 10 versus T = 11), we first appended placeholder \nrepresentations (tensors filled with zeros) to the end of the shorter \nvideo samples. This ensured all video samples in the mini-batch had \nthe same dimension. T o avoid incorporating these padded representa-\ntions into downstream processing, we used a masking matrix (matrix \nwith binary entries) indicating which representations the attention \nmechanism should attend to. Importantly, padded representations \nare not attended to during a forward pass through SAIS.\nDescription of ablation study\nWe trained several variants of SAIS to pinpoint the contribution of each \nits components on overall performance. Specifically, the model variants \nare trained using SAIS (baseline), evaluated without test-time augmenta-\ntion (‘without TTA’), and exposed to only optical flow (‘without RGB’) or \nRGB frames (‘without flow’) as inputs. We also removed the self-attention \nmechanism which captured the relationship between, and temporal \nordering of, frames (‘without SA’). In this setting, we simply averaged \nthe frame features. Although we present the PPV in Results, we arrived \nat similar findings when using other evaluation metrics.\nImplementation details of inference on entire videos\nAfter we trained and evaluated a model on video samples (on the order \nof 10–30 s), we deployed it on entire videos (on the order of 10–30 min) \nto decode an element of surgical activity without human supervision. \nWe refer to this process as inference. As we outline next, a suitable \nimplementation of inference is often dependent on the element of \nsurgical activity being decoded.\nSuturing subphase recognition.  Video samples used for training \nand evaluating SAIS to decode the three suturing subphases of needle \nhandling, needle driving and needle withdrawal spanned, on average, \n10–30 s (Supplementary Note 2). This guided our design choices for \ninference.\nCurating video samples for inference. During inference, we adopted \ntwo complementary approaches, as outlined next. Approach 1: we \npresented SAIS with 10-s video samples from an entire VUA video \nwith 5-s overlaps between subsequent video samples, with the latter \nensuring we capture boundary activity. As such, each 10-s video sample \nwas associated with a single probabilistic output, {sNH, sND, sNW}, reflect-\ning the probability, s , of needle handling (NH), needle driving (ND) \nand needle withdrawal (NW). Approach 2: we presented SAIS with 5-s \nnon-overlapping video samples from the same video. The motivation \nfor choosing a shorter video sample is to capture a brief subphase that \notherwise would have bled into another subphase when using a longer \nvideo sample. As such, each 5-s video sample was associated with a \nsingle probabilistic output. Note that we followed the same approach \nfor selecting frames from each video sample as we did during the \noriginal training and evaluation setup (see Implementation details \nand hyperparameters).\nAs an example of these approaches, the first video sample \npresented to SAIS in approach 1 spans 0–10 s whereas the first two \nvideo samples presented to SAIS in approach 2 span 0–5 s and 5–10 s, \nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 793\nArticle https://doi.org/10.1038/s41551-023-01010-8\nrespectively. When considering both approaches, the timespan 0–10 s \nis thus associated with three unique probabilistic outputs (as is every \nother 10-s timespan).\nUsing ensemble models. Recall that we trained SAIS using ten-fold \nMonte Carlo cross-validation, resulting in ten unique models. T o \nincrease our confidence in the inference process, we performed infer-\nence following the two aforementioned approaches with each of the \nten models. As such, each 10-s timespan was associated with 3 proba-\nbilistic outputs (P) × 10 folds (F) × 3 TTAs = 90 probabilistic outputs in \ntotal. As is done with ensemble models, we then averaged these proba-\nbilistic outputs (a.k.a. bagging) to obtain a single probabilistic out -\nput, {sNH,sND,sNW}, where the j th probability value for j  ∈ [1, C ]  \n(C categories) is obtained as follows:\n̄si =\n3\n∑\nP=1\n10\n∑\nF=1\n3\n∑\nTTA=1\nsTTA,F,P\nj ∀j∈ [1,C] (3)\nAbstaining from prediction.  In addition to ensemble models often \noutperforming their single model counterparts, they can also provide \nan estimate of the uncertainty about a classification. Such uncertainty \nquantification can be useful for identifying out-of-distribution video \nsamples52 such as those the model has never seen before or for high -\nlighting video samples where the classification is ambiguous and \nthus potentially inaccurate. T o quantify uncertainty, we took inspira-\ntion from recent work\n53 and calculated the entropy, S, of the resultant \nprobabilistic output post bagging. With high entropy implying high \nuncertainty, we can choose to abstain from considering classifications \nwhose entropy exceeds some threshold, Sthresh:\nS=−\nc\n∑\nj=1\nsjlogsj > Sthresh (4)\nAggregating predictions over time.  Once we have filtered out the \npredictions which are uncertain (that is, exhibit high entropy), we were \nleft with individual predictions for each subphase spanning at most \n10 s (because of how we earlier identified video samples). However, we \nknow from observation that certain subphases can be longer than 10 s \n(Supplementary Note 2). T o account for this, we aggregated subphase \npredictions that were close to one another over time. Specifically, we \naggregated multiple predictions of the same subphase into a single \nprediction if they were less than 3 s apart, in effect chaining the predic-\ntions. Although this value is likely to be dependent on other choices \nin the inference process, we found it to produce reasonable results.\nNS dissection gesture classification. Video samples used for training \nand evaluating SAIS to decode the six dissection gestures spanned, \non average, 1–5 s. This also guided our design choices for inference.\nIdentifying video samples for inference. During inference, we found \nit sufficient to adopt only one of the two approaches for inference \ndescribed earlier (inference for subphase recognition). Specifically, \nwe presented SAIS with 1-s non-overlapping video samples of an entire \nNS video. As such, each 1-s video sample was associated with a single \nprobabilistic output, \n{sj}6\nj=1 reflecting the probability, s, of each of the \nsix gestures.\nUsing ensemble models.  As with inference for suturing subphase \nrecognition, we deployed the ten SAIS models (from the ten Monte \nCarlo folds) and three TTAs on the same video samples. As such, each \n1-s video sample was associated with 10 × 3 = 30 probabilistic outputs. \nThese are then averaged to obtain a single probabilistic output, { ̄sj}6\nj=1.\nAbstaining from prediction.  We also leveraged the entropy of ges -\nture classifications as a way to quantify uncertainty and thus abstain \nfrom making highly uncertain gesture classifications. We found that \nSthresh = 1.74 led to reasonable results.\nAggregating predictions over time. T o account for the observation \nthat gestures can span multiple seconds, we aggregated individual 1-s \npredictions that were close to one another over time. Specifically, we \naggregated multiple predictions of the same gesture into a single predic-\ntion if they were less than 2 s apart. For example, if a retraction gesture \n(r) is predicted at intervals 10–11 s, 11–12 s and 15–16 s, we treated this as \ntwo distinct retraction gestures. The first one spans 2 s (10–12 s) while \nthe second one spans 1 s (15–16 s). This avoids us tagging spurious and \nincomplete gestures (for example, the beginning or end of a gesture) \nas an entirely distinct gesture over time. Our 2-s interval introduced \nsome tolerance for a potential misclassification between gestures of \nthe same type and allowed for the temporal continuity of the gestures.\nImplementation details of training SAIS on external video \ndatasets\nWe trained SAIS on two publicly available datasets: JIGSAWS11 and DVC \nUCL12. In short, these datasets contain video samples of individuals \nperforming suturing gestures either in a controlled laboratory set -\nting or during the dorsal vascular complex step of the RARP surgical \nprocedure. For further details on these datasets, we refer readers to \nthe original respective publications.\nJIGSAWS dataset.  We followed the commonly adopted \nleave-one-user-out cross-validation setup11. This involves training on \nvideo samples from all but one user and evaluating on those from the \nremaining user. These details can be found in a recent review9.\nDVC UCL dataset. This dataset, recently released as part of the Endo-\nscopic Vision Challenge 2022 at MICCAI, consists of 45 videos from a \ntotal of eight surgeons performing suturing gestures during the dorsal \nvascular complex step of the RARP surgical procedure12. The publicly \navailable dataset, at the time of writing, is composed of 36 such videos \n(Table 1). Similar to the private datasets we used, each video (on the order \nof 2–3 min) is annotated with a sequence of eight unique suturing ges-\ntures alongside their start time and end time. Note that these annotations \ndo not follow the taxonomy we have developed and are therefore distinct \nfrom those we outlined in the Surgical video samples and annotations \nsection. The sole previous method to evaluate on this dataset does so \non a private test set. As this test set is not publicly available, we adopted \na leave-one-video-out setup and reported the ten-fold cross-validation \nperformance of SAIS (Supplementary Table 3 for the number of video \nsamples in each fold). Such a setup provides insight into how well SAIS \ncan generalize to unseen videos. Furthermore, in light of the few samples \nfrom one of the gesture categories (G5), we distinguished between only \nseven of the gestures. T o facilitate the reproducibility of our findings, we \nwill release the exact data splits used for training and testing.\nImplementation details of I3D experiments\nWe trained the I3D model to decode the binary skill level of needle \nhandling and needle driving on the basis of video samples of the VUA \nstep. For a fair comparison, we presented the I3D model with the same \nexact data otherwise presented to SAIS (our model). In training the I3D \nmodel, we followed the core strategy proposed in ref. 6. For example, \nwe loaded the parameters pre-trained on the Kinetics dataset and froze \nall but the last three layers (referred to as Mixed5b, Mixed5c and logits).\nHowever, having observed that the I3D model was quite sensitive \nto the choice of hyperparameters, we found it necessary to conduct \nan extensive number of experiments to identify the optimal setup \nand hyperparameters for decoding surgical skill, the details of which \nare outlined next. First, we kept the logits layer as is, resulting in a \n400-dimensional representation, and followed it with a non-linear \nclassification head to output the probability of, for example, a high-skill \nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 794\nArticle https://doi.org/10.1038/s41551-023-01010-8\nactivity. We also leveraged both data modalities (RGB and flow) which \nwe found to improve upon the original implementation that had used \nonly a single modality. Specifically, we added the two 400-dimensional \nrepresentations (one for each modality) to one another and passed \nthe resultant representation through the aforementioned classifica-\ntion head. With the pre-trained I3D expecting an input with 16 frames \nor multiples thereof, we provided it with a video sample composed \nof 16 equally spaced frames between the start time and end time of \nthat sample. While we also experimented with a different number of \nframes, we found that to yield suboptimal results. T o train I3D, we used \na batch-size of 16 video samples and a learning rate of 1e−3.\nAssociation between the skill assessments of SAIS and patient \noutcomes\nT o determine whether the skill assessments of SAIS are associated with \npatient outcomes, we conducted an experiment with two variants. We \nfirst deployed SAIS on the test set of video samples in each fold of the \nMonte Carlo cross-validation setup. This resulted in an output, Z1 ∈ \n[0, 1], for each video sample reflecting the probability of a high-skill \nassessment. In the first variant of this experiment, we assigned each \nvideo sample, linked to a surgical case, a urinary continence recovery \n(3 months after surgery) outcome, Y . T o account for the fact that a \nsingle outcome, Y , is linked to an entire surgical case, in the second \nvariant of this experiment, we averaged the outputs, Z , for all video \nsamples within the same surgical case. This, naturally, reduced the \ntotal number of samples available.\nIn both experiments, we controlled for the total number of robotic \nsurgeries performed by the surgeon (caseload, Z2) and the age of the \npatient being operated on (Z3), and regressed the probabilistic outputs \nof SAIS to the urinary continence recovery outcome using a logistic \nregression model (SPSS), as shown below (σ is the sigmoid function). \nAfter training this model, we extracted the coefficient, b1, and report \nthe odds ratio (OR) and the 95% confidence interval (CI).\nY= σ(b0 +b1Z1 +b2Z2 +b3Z3)\nOR=\noddshigh−skill\noddslow−skill\n= eb1\n(5)\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nData supporting the results in this study involve surgeon and patient \ndata. As such, while the data from SAH and HMH are not publicly avail-\nable, de-identified data from USC can be made available upon reason-\nable request from the authors.\nCode availability\nCode is made available at https://github.com/danikiyasseh/SAIS.\nReferences\n1. Weiser, T. G. et al. An estimation of the global volume of  \nsurgery: a modelling strategy based on available data. Lancet \n372, 139–144 (2008).\n2. Sheetz, K. H., Claflin, J. & Dimick, J. B. Trends in the adoption of \nrobotic surgery for common surgical procedures. JAMA Netw. \nOpen 3, e1918911–e1918911 (2020).\n3. Birkmeyer, J. D. et al. Surgical skill and complication rates after \nbariatric surgery. N. Engl. J. Med. 369, 1434–1442 (2013).\n4. Zia, A., Hung, A., Essa, I. & Jarc, A. Surgical activity recognition \nin robot-assisted radical prostatectomy using deep learning. \nIn International Conference on Medical Image Computing and \nComputer-Assisted Intervention, 273–280 (Springer, 2018).\n5. Luongo, F., Hakim, R., Nguyen, J. H., Anandkumar, A. & Hung, \nA. J. Deep learning-based computer vision to recognize and \nclassify suturing gestures in robot-assisted surgery. Surgery 169, \n1240–1244 (2021).\n6. Funke, I. et al. Using 3d convolutional neural networks to \nlearn spatiotemporal features for automatic surgical gesture \nrecognition in video. In International Conference on Medical \nImage Computing and Computer-Assisted Intervention 467–475 \n(Springer, 2019); https://doi.org/10.1007/978-3-030-32254-0_52\n7. Lavanchy, J. L. et al. Automation of surgical skill assessment using \na three-stage machine learning algorithm. Sci. Rep. 11, 1–9 (2021).\n8. Goodman, E. D. et al. A real-time spatiotemporal AI model \nanalyzes skill in open surgical videos. Preprint at arXiv  \nhttps://arxiv.org/abs/2112.07219 (2021).\n9. van Amsterdam, B., Clarkson, M. & Stoyanov, D. Gesture \nrecognition in robotic surgery: a review. IEEE Trans. Biomed. Eng. \n68, 2021–2035 (2021).\n10. Kiyasseh, D., Zhu, T. & Clifton, D. A clinical deep learning framework \nfor continually learning from cardiac signals across diseases, time, \nmodalities, and institutions. Nat. Commun. 12, 1–11 (2021).\n11. Gao, Y. et al. JHU-ISI gesture and skill assessment working \nset (JIGSAWS): a surgical activity dataset for human motion \nmodeling. In Proceedings of the Modeling and Monitoring of \nComputer Assisted Interventions (M2CAI)—MICCAI Workshop,  \nVol. 3 (CIRL, Johns Hopkins University, 2014).\n12. Van Amsterdam, B. et al. Gesture recognition in robotic  \nsurgery with multimodal attention. IEEE Trans. Med. Imaging 41, \n1677–1687 (2022).\n13. Kitaguchi, D. et al. Development and validation of a 3-dimensional \nconvolutional neural network for automatic surgical skill \nassessment based on spatiotemporal video analysis. JAMA Netw. \nOpen 4, e2120786–e2120786 (2021).\n14. Ghassemi, M., Oakden-Rayner, L. & Beam, A. L. The false hope of \ncurrent approaches to explainable artificial intelligence in health \ncare. Lancet Digit. Health 3, e745–e750 (2021).\n15. Sanford, D. et al. Association of suturing technical skill \nassessment scores between virtual reality simulation and live \nsurgery. J. Endourol. 36, 1388–1394 (2022).\n16. Trinh, L. et al. Survival analysis using surgeon skill metrics and \npatient factors to predict urinary continence recovery after robot- \nassisted radical prostatectomy. Eur. Urol. Focus. 8, 623–630 (2022).\n17. Kiyasseh D. et al. A multi-institutional study using artificial \nintelligence to provide reliable and fair feedback to surgeons. \nCommun. Med. https://doi.org/10.1038/s43856-023-00263-3 \n(2023).\n18. Carreira, J. & Zisserman, A. Quo vadis, action recognition? A new \nmodel and the kinetics dataset. In Proceedings of IEEE Computer \nSociety Conference on Computer Vision and Pattern Recognition, \n6299–6308 (IEEE, 2017).\n19. Kiyasseh, D., Zhu, T. & Clifton, D. CROCS: clustering and retrieval \nof cardiac signals based on patient disease class, sex, and age. \nAdv. Neural Inf. Process. Syst. 34, 15557–15569 (2021).\n20. Bengio, Y., Louradour, J., Collobert, R. & Weston, J. Curriculum \nlearning. In Proceedings of the 26th Annual International \nConference on Machine Learning, 41–48 (Association for \nComputing Machinery, 2009).\n21. Kiyasseh D. et al. Human visual explanations mitigate bias in \nAI-based assessment of surgeon skills. NPJ Digit. Med.  \nhttps://doi.org/10.1038/s41746-023-00766-2 (2023).\n22. Collins, J. W. et al. Ethical implications of AI in robotic surgical \ntraining: a Delphi consensus statement. Eur. Urol. Focus. 8, \n613–622 (2021).\n23. Hashimoto, D. A., Rosman, G., Rus, D. & Meireles, O. R. Artificial \nintelligence in surgery: promises and perils. Ann. Surg. 268,  \n70 (2018).\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796\n 795\nArticle https://doi.org/10.1038/s41551-023-01010-8\n24. Maier-Hein, L. et al. Surgical data science for next-generation \ninterventions. Nat. Biomed. Eng. 1, 691–696 (2017).\n25. Weede, O. et al. Workflow analysis and surgical phase recognition \nin minimally invasive surgery. In 2012 IEEE International \nConference on Robotics and Biomimetics (ROBIO) 1080–1074 \n(IEEE, 2012).\n26. Hung, A. J. et al. Utilizing machine learning and automated \nperformance metrics to evaluate robot-assisted radical \nprostatectomy performance and predict outcomes. J. Endourol. \n32, 438–444 (2018).\n27. Hung, A. J., Chen, J. & Gill, I. S. Automated performance \nmetrics and machine learning algorithms to measure surgeon \nperformance and anticipate clinical outcomes in robotic surgery. \nJAMA Surg. 153, 770–771 (2018).\n28. Hung, A. J. et al. Deep learning on automated performance \nmetrics and clinical features to predict urinary continence \nrecovery after robot-assisted radical prostatectomy. BJU Int. 124, \n487 (2019).\n29. Nwoye, C. I. et al. CholecTriplet2021: a benchmark challenge for \nsurgical action triplet recognition. Preprint at arXiv https://arxiv.\norg/abs/2204.04746 (2022).\n30. Béjar Haro, B., Zappella, L. & Vidal, R. Surgical gesture \nclassification from video data. In International Conference on \nMedical Image Computing and Computer-Assisted Intervention, \n34–41 (Springer-Verlag, 2012).\n31. Khalid, S., Goldenberg, M., Grantcharov, T., Taati, B. & Rudzicz, \nF. Evaluation of deep learning models for identifying surgical \nactions and measuring performance. JAMA Netw. Open 3, \ne201664–e201664 (2020).\n32. van Amsterdam, B., Clarkson, M. J. & Stoyanov, D. Multi-task \nrecurrent neural network for surgical gesture recognition and \nprogress prediction. In 2020 IEEE International Conference on \nRobotics and Automation (ICRA), 1380–1386 (IEEE, 2020).\n33. Gao, X., Jin, Y., Dou, Q. & Heng, P.-A. Automatic gesture \nrecognition in robot-assisted surgery with reinforcement learning \nand tree search. In 2020 IEEE International Conference on \nRobotics and Automation (ICRA), 8440–8446 (IEEE, 2020).\n34. Wu, J. Y., Tamhane, A., Kazanzides, P. & Unberath, M. Cross-modal \nself-supervised representation learning for gesture and skill \nrecognition in robotic surgery. Int. J. Comput. Assist. Radiol. Surg. \n16, 779–787 (2021).\n35. Wagner, M. et al. Comparative validation of machine learning \nalgorithms for surgical workflow and skill analysis with the \nheichole benchmark. Med. Image Anal. 86, 102770 (2023).\n36. Zappella, L., Béjar, B., Hager, G. & Vidal, R. Surgical gesture \nclassification from video and kinematic data. Med. Image Anal. 17, \n732–745 (2013).\n37. Bar, O. et al. Impact of data on generalization of AI for surgical \nintelligence applications. Sci. Rep. 10, 1–12 (2020).\n38. Vaswani, A. et al. Attention is all you need. In Advances in Neural \nInformation Processing Systems (Eds Guyon, I. et al.) Vol. 30  \n(NIPS, 2017).\n39. Garrow, C. R. et al. Machine learning for surgical phase \nrecognition: a systematic review. Ann. Surg. 273, 684–693  \n(2021).\n40. Czempiel, T. et al. Opera: attention-regularized transformers \nfor surgical phase recognition. In International Conference on \nMedical Image Computing and Computer-Assisted Intervention, \n604–614 (Springer, 2021).\n41. Nwoye, C. I. et al. Rendezvous: attention mechanisms for the \nrecognition of surgical action triplets in endoscopic videos.  \nMed. Image Anal. 78, 102433 (2022).\n42. Aspart, F. et al. ClipAssistNet: bringing real-time safety feedback \nto operating rooms. Int. J. Comput. Assist. Radiol. Surg. 17,  \n5–13 (2022).\n43. Nwoye, C. I. & Padoy, N. Data splits and metrics for method \nbenchmarking on surgical action triplet datasets. Preprint at arXiv \nhttps://arxiv.org/abs/2204.05235 (2022).\n44. Ma, R. et al. A novel dissection gesture classification to \ncharacterize robotic dissection technique for renal hilar \ndissection. J. Urol. 205, 271–275 (2021).\n45. Moy, R. L., Waldman, B. & Hein, D. W. A review of sutures and \nsuturing techniques. J. Dermatol. Surg. Oncol. 18, 785–795 (1992).\n46. Haque, T. F. et al. An assessment tool to provide targeted \nfeedback to robotic surgical trainees: development and validation \nof the end-to-end assessment of suturing expertise (ease). Urol. \nPract. 9, 532–539 (2022).\n47. Hung, A. J. et al. Road to automating robotic suturing skills \nassessment: battling mislabeling of the ground truth. Surgery 171, \n915–919 (2022).\n48. Teed, Z. & Deng, J. Raft: recurrent all-pairs field transforms \nfor optical flow. In European Conference on Computer Vision, \n402–419 (Springer, 2020).\n49. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers \nfor image recognition at scale. In International Conference on \nLearning Representations (ICLR, 2021).\n50. Caron, M. et al. Emerging properties in self-supervised vision \ntransformers. In IEEE/CVF International Conference on Computer \nVision, 9650–9660 (IEEE, 2021).\n51. Paszke, A. et al. Pytorch: an imperative style, high-performance \ndeep learning library. In Advances in Neural Information \nProcessing Systems (Eds Wallach, H. et al.) Vol. 32 (NIPS, 2019).\n52. Roy, A. G. et al. Does your dermatology classifier know what it \ndoesn’t know? Detecting the long-tail of unseen conditions. Med. \nImage Anal. 75, 102274 (2022).\n53. Lakshminarayanan, B., Pritzel, A. & Blundell, C. Simple and \nscalable predictive uncertainty estimation using deep ensembles. \nIn Advances in Neural Information Processing Systems (Eds Guyon, \nI. et al.) Vol. 30 (NIPS, 2017).\nAcknowledgements\nWe are grateful to T. Chu for the annotation of videos with gestures. \nWe also thank J. Laca and J. Nguyen for early feedback on the \npresentation of the manuscript. A.J.H. discloses support for the \nresearch described in this study from the National Cancer Institute \nunder award no. R01CA251579-01A1 and a multi-year Intuitive Surgical \nClinical Research Grant.\nAuthor contributions\nD.K. and A.J.H. contributed to the conception of the study. D.K. \ncontributed to the study design, developed the deep learning models \nand wrote the manuscript. R.M. and T.H. provided annotations for the \nvideo samples. D.A.D. provided extensive feedback on the manuscript. \nB.J.M. provided data for the study. C.W. collected data from SAH \nand provided feedback on the manuscript. A.J.H. and A.A. provided \nsupervision and contributed to edits of the manuscript.\nCompeting interests\nD.K. is a paid employee of Vicarious Surgical and a consultant of \nFlatiron Health. C.W. is a paid consultant of Intuitive Surgical. A.A. is \nan employee of Nvidia. A.J.H. is a consultant of Intuitive Surgical. The \nother authors declare no competing interests.\nAdditional information\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41551-023-01010-8.\nCorrespondence and requests for materials should be addressed  \nto Dani Kiyasseh or Andrew J. Hung.\nNature Biomedical Engineering | Volume 7 | June 2023 | 780–796 796\nArticle https://doi.org/10.1038/s41551-023-01010-8\nPeer review information Nature Biomedical Engineering thanks \nMasaaki Ito, Jie Ying Wu and the other, anonymous, reviewer(s) for  \ntheir contribution to the peer review of this work. Peer reviewer reports \nare available.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2023\n1 nature research  |  reporting summaryApril 2020\nCorresponding author(s): Dani Kiyasseh\nLast updated by author(s): 2023/01/13\nReporting Summary\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection No software was used to collect data.\nData analysis Custom code, Python 3, PyTorch 1.8.0. Code is available at https://github.com/danikiyasseh/SAIS.\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A list of figures that have associated raw data \n- A description of any restrictions on data availability\nThe data supporting the results in this study involve surgeon and patient data. The data from St. Antonius Hospital and Houston Methodist Hospital are not publicly \navailable, yet de-identified data from the University of Southern California can be made available by the corresponding authors on reasonable request.\n2 nature research  |  reporting summaryApril 2020\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size All experiments were conducted using 10-fold Monte Carlo cross-validation. Details of the number of samples and surgical videos used during \ntraining, validation and testing are provided in the Supplementary Information.\nData exclusions We did not exclude any patients for training, validation or testing. \nReplication The protocol is described in the paper, and in-depth implementation details are provided in the Supplementary Information.\nRandomization The experiments were conducted across 10 folds. Each fold consisted of a different subset of data on which to train and evaluate the model. \nThe initialization of the parameters of the network was also different across folds. Each fold was set based on a seed (from 0 to 10). \nBlinding Blinding was not applicable. When comparing different methods, we maintained the same experimental settings to allow for a fair \ncomparison. \nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nHuman research participants\nClinical data\nDual use research of concern\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nHuman research participants\nPolicy information about studies involving human research participants\nPopulation characteristics Table 1 provides a summary of the number of video samples used for the different experimental tasks (sub-phase \nrecognition, gesture classification, and skill assessment). We also include the total number of surgeons from each hospital \nwho were associated with these samples. \nRecruitment Surgeons from different hospitals were recruited as part of Award No. R01CA251579-01A1 by the National Cancer Institute. \nVideos from hospitals exclusively reflected two different types of surgical procedures: robot-assisted radical prostatectomy \nand robot-assisted partial nephrectomy. Details of these procedures can be found in Methods.\nEthics oversight All datasets (data from the University of Southern California, St. Antonius Hospital, and Houston Methodist Hospital) were \ncollected under Institutional Review Board (IRB) approval, in which informed consent was obtained (HS-17-00113). These \ndatasets were de-identifed prior to model development.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.7169675827026367
    },
    {
      "name": "Gesture",
      "score": 0.5449842214584351
    },
    {
      "name": "Surgical procedures",
      "score": 0.5372519493103027
    },
    {
      "name": "Medicine",
      "score": 0.44575902819633484
    },
    {
      "name": "Identification (biology)",
      "score": 0.4122428894042969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.410990834236145
    },
    {
      "name": "Surgical robot",
      "score": 0.41045069694519043
    },
    {
      "name": "Computer science",
      "score": 0.38675758242607117
    },
    {
      "name": "Medical physics",
      "score": 0.36651623249053955
    },
    {
      "name": "Surgery",
      "score": 0.3388875126838684
    },
    {
      "name": "Robot",
      "score": 0.18399381637573242
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}