{
  "title": "Automated Radiology Report Labeling in Chest X-Ray Pathologies: Development and Evaluation of a Large Language Model Framework",
  "url": "https://openalex.org/W4406971480",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2049227219",
      "name": "ABDULLAH ABDULLAH",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102032674",
      "name": "Seong Tae Kim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2611650229",
    "https://openalex.org/W2338526423",
    "https://openalex.org/W3103694015",
    "https://openalex.org/W3012070096",
    "https://openalex.org/W4297811187",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2234417453",
    "https://openalex.org/W2584937702",
    "https://openalex.org/W2911410812",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W2765803301",
    "https://openalex.org/W2953196299",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2963373823",
    "https://openalex.org/W3004844052",
    "https://openalex.org/W2768567289",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2043768386",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2785863263",
    "https://openalex.org/W4387225871",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4385573131",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4386076004",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3101156210"
  ],
  "abstract": "Abstract Background Labeling unstructured radiology reports is crucial for creating structured datasets that facilitate downstream tasks, such as training large-scale medical imaging models. Current approaches typically rely on Bidirectional Encoder Representations from Transformers (BERT)-based methods or manual expert annotations, which have limitations in terms of scalability and performance. Objective This study aimed to evaluate the effectiveness of a generative pretrained transformer (GPT)-based large language model (LLM) in labeling radiology reports, comparing it with 2 existing methods, CheXbert and CheXpert, on a large chest X-ray dataset (MIMIC Chest X-ray [MIMIC-CXR]). Methods In this study, we introduce an LLM-based approach fine-tuned on expert-labeled radiology reports. Our model’s performance was evaluated on 687 radiologist-labeled chest X-ray reports, comparing F 1 scores across 14 thoracic pathologies. The performance of our LLM model was compared with the CheXbert and CheXpert models across positive, negative, and uncertainty extraction tasks. Paired t tests and Wilcoxon signed-rank tests were performed to evaluate the statistical significance of differences between model performances. Results The GPT-based LLM model achieved an average F 1 score of 0.9014 across all certainty levels, outperforming CheXpert (0.8864) and approaching CheXbert’s performance (0.9047). For positive and negative certainty levels, our model scored 0.8708, surpassing CheXpert (0.8525) and closely matching CheXbert (0.8733). Statistically, paired t tests indicated no significant difference between our model and CheXbert ( P =.35) but a significant improvement over CheXpert ( P =.). Wilcoxon signed-rank tests corroborated these findings, showing no significant difference between our model and CheXbert ( P =.4) but confirming a significant difference with CheXpert ( P =.). The LLM also demonstrated superior performance for pathologies with longer and more complex descriptions, leveraging its extended context length. Conclusions The GPT-based LLM model demonstrates competitive performance compared with CheXbert and outperforms CheXpert in radiology report labeling. These findings suggest that LLMs are a promising alternative to traditional BERT-based architectures for this task, offering enhanced context understanding and eliminating the need for extensive feature engineering. Furthermore, with large context length LLM-based models are better suited for this task as compared with the small context length of BERT based models.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.7462542057037354
    },
    {
      "name": "Computer science",
      "score": 0.5240134000778198
    },
    {
      "name": "Medicine",
      "score": 0.4751546084880829
    },
    {
      "name": "Radiology",
      "score": 0.4579848349094391
    },
    {
      "name": "Medical physics",
      "score": 0.40601807832717896
    },
    {
      "name": "World Wide Web",
      "score": 0.15631142258644104
    }
  ]
}