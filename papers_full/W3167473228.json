{
    "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks",
    "url": "https://openalex.org/W3167473228",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5055885317",
            "name": "Rabeeh Karimi Mahabadi",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5037310413",
            "name": "Sebastian Ruder",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5102906366",
            "name": "Mostafa Dehghani",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5084321238",
            "name": "James Henderson",
            "affiliations": [
                "Idiap Research Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3026404337",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W3123161422",
        "https://openalex.org/W3100124407",
        "https://openalex.org/W2980113592",
        "https://openalex.org/W3003689215",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2804236178",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W3105421296",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W3034665873",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2963921132",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W2089468765",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W2070246124",
        "https://openalex.org/W3098068947",
        "https://openalex.org/W2996144997",
        "https://openalex.org/W2624871570",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W2788496822",
        "https://openalex.org/W2963245493",
        "https://openalex.org/W2888456631",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3113529090",
        "https://openalex.org/W2963453233",
        "https://openalex.org/W2946659172",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 565–576\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n565\nParameter-efficient Multi-task Fine-tuning for Transformers\nvia Shared Hypernetworks\nRabeeh Karimi Mahabadi∗\nEPFL University, Idiap Research Institute\nrabeeh.karimimahabadi@epfl.ch\nSebastian Ruder\nDeepMind\nruder@google.com\nMostafa Dehghani\nGoogle Brain\ndehghani@google.com\nJames Henderson\nIdiap Research Institute\njames.henderson@idiap.ch\nAbstract\nState-of-the-art parameter-efficient fine-tuning\nmethods rely on introducing adapter modules be-\ntween the layers of a pretrained language model.\nHowever, such modules are trained separately for\neach task and thus do not enable sharing infor-\nmation across tasks. In this paper, we show that\nwe can learn adapter parameters for all layers\nand tasks by generating them using shared hyper-\nnetworks, which condition on task, adapter posi-\ntion, and layer id in a transformer model. This\nparameter-efficient multi-task learning frame-\nwork allows us to achieve the best of both worlds\nby sharing knowledge across tasks via hypernet-\nworks while enabling the model to adapt to each\nindividual task through task-specific adapters.\nExperiments on the well-known GLUE bench-\nmark show improved performance in multi-task\nlearning while adding only0.29% parameters per\ntask. We additionally demonstrate substantial per-\nformance improvements in few-shot domain gen-\neralization across a variety of tasks. Our code\nis publicly available inhttps://github.com/\nrabeehk/hyperformer.\n1 Introduction\nTransfer learning from pretrained large-scale language\nmodels yields state-of-the-art results in a variety of\ntasks (Devlin et al., 2019; Radford et al., 2018; Liu\net al., 2019b). As a highly expressive and abstract\nframework, Raffel et al. (2020) explored the land-\nscape of transfer learning by converting text-based\nnatural language processing (NLP) problems into a\nsequence-to-sequence format to train a unified model\non several tasks simultaneously. Multi-task learning\nwith pretrained language models (Ruder, 2017) is\nappealing for multiple reasons: 1) Training individual\nmodels per task results in higher computational costs,\nwhich hinders deployment and maintenance. These\ncosts are substantially reduced by training a single\n∗Work done while the author was at Google.\nMulti-head attention\nFeed forward\nAdapter\n+ \nLayer norm\nLayer norm\nFeed forward\nAdapter\n+ \nFeed forward down\nprojection\nNonlinearity\nFeed forward up\nprojection\nLayer norm\n+ \nAdapter LayerTransformer Layer\nFigure 1: Left: Adapter integration in the T5 model.\nRight: Our H YPERFORMER adapter architecture.\nFollowing Houlsby et al. (2019), we include adapter\nmodules after the two feed-forward layers. The Adapter\nhypernetworkhl\nA produces the weights (Ul\nτ and Dl\nτ) for\ntask-specific adapter modules conditioned on an input\ntask embedding Iτ. Similarly, the layer normalization\nhypernetwork hl\nLN generates the conditional layer nor-\nmalization parameters (βτ and γτ). During training, we\nonly update layer normalizations in T5, hypernetworks,\nand task embeddings. The compact H YPERFORMER++\nshares the same hypernetworks across all layers and tasks\nand computes the task embedding based on task, layer id,\nand position of the adapter module (§2.4).\nmodel. 2) Fine-tuning the model across multiple tasks\nallows sharing information between the different\ntasks and positive transfer to other related tasks.\nSpecifically, when target datasets have limited training\ndata, multi-task learning improves the performance\ncompared to individually trained models (Liu et al.,\n2019a; Ratner et al., 2018). However, multi-task\nfine-tuning can result in models underperforming on\nhigh-resource tasks due to constrained capacity (Ari-\nvazhagan et al., 2019; McCann et al., 2018). An\nadditional issue with multi-task fine-tuning is the\npotential for task interferenceor negative transfer,\n566\nwhere achieving good performance on one task can\nhinder performance on another (Wang et al., 2019c).\nAs an alternative to fine-tuning (Howard and Ruder,\n2018), adapter layers (Houlsby et al., 2019) insert\na small number of additional parameters per task\ninto the model. During fine-tuning, only the adapter\nmodules, layer normalizations, and parameters of\nthe final classification layer are updated, while the\noriginal pretrained model parameters remain frozen.\nSuch task-specific adapters eliminate negative task\ninterference by encapsulating task-specific informa-\ntion (Pfeiffer et al., 2020). However, so far there has\nnot been an effective and parameter-efficient way to\nshare information across multiple adapters to enable\npositive transfer to low-resource and related tasks.\nTo address this problem and to enable sharing in-\nformation across tasks while reaping the benefits of\nadapter layers, as depicted in Figure 1, we propose\nHYPERFORMER++, which employs a compact hyper-\nnetwork (Ha et al., 2017; Oswald et al., 2020) shared\nacross tasks and layers. The hypernetwork learns to\ngenerate task and layer-specific adapter parameters,\nconditioned on task and layer id embeddings. The hy-\npernetwork is jointly learned between all tasks and is\nthus able to share information across them, while neg-\native interference is minimized by generating separate\nadapter layers for each task. For each new task, our\nmodel only requires learning an additional task em-\nbedding, reducing the number of trained parameters.\nWe use the encoder-decoder T5 model (Raffel et al.,\n2020) as the underlying model for our experiments\nand evaluate on the standard GLUE benchmark (Wang\net al., 2019b). We achieve strong gains over both\nthe T5BASE model as well as adapters (Houlsby et al.,\n2019). To our knowledge, this is the first time that\nadapters have been successfully integrated into a state-\nof-the-art encoder-decoder model beyond machine\ntranslation (Bapna and Firat, 2019), demonstrating\nthat our method effectively balances sharing informa-\ntion across tasks while minimizing negative transfer.\nIn summary, we make the following contributions:\n(1) We propose a parameter-efficient method for multi-\ntask fine-tuning based on hypernetworks and adapter\nlayers. (2) We demonstrate that our method scales\nmore efficiently than prior work. (3) We provide em-\npirical results on GLUE demonstrating the effective-\nness of the proposed method on multi-task learning.\n(4) We perform extensive few-shot domain transfer\nexperiments, which reveal that the captured shared\nknowledge can positively transfer to unseen in-domain\ntasks. We release our code to facilitate future work.\n2 H YPERFORMER\nIn this section, we present our HYPERFORMER\nmodel, which integrateshypernetwork-based adapter\nlayers into a multi-task transformermodel. In §2.4,\nwe introduce a parameter-efficient variant of this\nmodel, called HYPERFORMER++.\nProblem formulation: We consider a general\nmulti-task learning problem, where we are given the\ndata from a set of tasks {Dτ}T\nτ=1, where T is the\ntotal number of tasks andDτ={(xi\nτ,yi\nτ)}Nτ\ni=1 shows\nthe training data forτ-th task withNτ samples. We\nassume we are also given a large-scale pretrained\nlanguage model fθ(.) parameterized by θ that\ncomputes the output for inputxi\nτ. Standard multi-task\nfine-tuning minimizes the following loss on the\ntraining set:\nL(θ,{Dτ}T\nτ=1)=\nT∑\nτ=1\n∑\n(xiτ,yiτ)∈Dτ\nwτl\n(\nfθ(xi\nτ),yi\nτ\n)\n, (1)\nwhere l is typically the cross-entropy loss, andwτ\nshows the sampling weight forτ-th task. Our goal\nis to finetune the pretrained model in a multi-task\nlearning setup efficiently, while allowing sharing\ninformation across tasks and at the same time,\nenabling the model to adapt to each individual task.\nThe key idea of our approach, depicted in Figure\n1, is to learn a parametric task embedding{Iτ}T\nτ=1\nfor each task, and then feed these task embeddings\nto hypernetworks parameterized byνthat generate\nthe task-specific adapter layers (Houlsby et al.,\n2019). We insert adapter modules within the layers\nof a pretrained model, making the final model of\nXν(xi\nτ,θ,Iτ) parameterized by ν that computes\nthe output for input xi\nτ. During training, we only\ntrain hypernetwork parametersν, task embeddings\n{Iτ}T\nτ=1, and layer normalizations infθ(.), while the\nrest of the pretrained model parametersθare fixed:\nL(ν,{Iτ}T\ni=1,{Dτ}T\nτ=1)=\nT∑\nτ=1\n∑\n(xiτ,yiτ)∈Dτ\nwτl\n(\nXν(xi\nτ,θ,Iτ),yi\nτ\n)\n, (2)\nThe hypernetworks capture the shared information\nacross tasks in a multi-task learning model enabling\npositive transfer between related domains and trans-\nferable tasks, while adapters are reducing negative\ninterference, encapsulating task-specific information.\nBase model: All of our models are built on top\nof the state-of-the-art T5 transformer model (Raffel\n567\net al., 2020). This model frames text-based language\ntasks as sequence-to-sequence problems. T5 consists\nof an encoder-decoder Transformer (V aswani et al.,\n2017) with minor modifications (Raffel et al., 2020).\nThe model is trained simultaneously on multiple\ntasks, obtaining state-of-the-art performance across\na diverse set of tasks. We use the T5 framework as\nit enables training a universal model that interfaces\nwith many language tasks. Our model has three\nmain components: 1) task conditional adapter layers;\n2) task conditional layer normalizations; and 3)\nhypernetworks that generate task-specific parameters.\nWe next describe these components.\n2.1 Task Conditional Adapter Layers\nPrior work has shown that fine-tuning all parameters\nof the model can result in a sub-optimal solution,\nparticularly for resource-limited datasets (Peters et al.,\n2019). As an alternative to fine-tuning all the model’s\nparameters, prior work (Houlsby et al., 2019; Rebuffi\net al., 2018; Stickland and Murray, 2019) inserted\nsmall modules calledadapter layerswithin layers of\na pretrained model, as shown in Figure 1. Adapters\nintroduce no change to the structure or parameters\nof the original model.\nIn this work, we propose conditional adapter\nmodules, in which we generate the adapters weights\nbased on input task embeddings using shared\nhypernetworks (Ha et al., 2017), which capture\ninformation across tasks that can be used to positively\ntransfer to other relevant tasks.\nEach layer of a transformer model consists of\nan attention block and a feed-forward block, each\nfollowed by a skip connection. Following Houlsby\net al. (2019), as depicted in Figure 1, we introduce\na conditional adapter layer after each block before the\nskip connection. The conditional adapter layerAl\nτ\nfor layerlconsists of a down-projection,Dl\nτ∈Rh×d,\nGeLU non-linearity (Hendrycks and Gimpel, 2016),\nand up-projectionUl\nτ ∈Rd×h, where his the input\ndimension, anddis the bottleneck dimension for the\nadapter layer, mathematically defined as:\nAl\nτ(x)= LNl\nτ\n(\nUl\nτ(GeLU(Dl\nτ(x)))\n)\n+x, (3)\nwhere xis the input hidden state and LNl\nτ is the\nconditional layer norm defined in the next section.\nWe generate adapter weights (Ul\nτ, Dl\nτ) through a\nhypernetwork described in §2.3.\n2.2 Task Conditional Layer Normalization\nConventional layer normalization (Ba et al., 2016) is\ndefined as:\nLNl\nτ(xi\nτ)= γl\nτ⊙xi\nτ−µτ\nστ\n+βl\nτ, (4)\nwhere⊙is the element-wise multiplication between\ntwo vectors, andγl\nτ and βl\nτ are learnable parameters\nwith the same dimension asxi\nτ. V alues ofµτ and\nστ show the mean and standard deviation of training\ndata for theτ-th task.\nTo allow the layer normalization inside adapters\nto adapt to each task, inspired by Perez et al. (2018);\nDe Vries et al. (2017), we generate γl\nτ, βl\nτ via a\nhypernetwork as a function of task embeddings (§2.3).\n2.3 Task Conditioned Hypernetworks\nIn order to have a model that can share information\nwhile being able to adapt to each individual task, we\ngenerate the parameters of task conditional adapter\nlayers and layer normalization using hypernetworks.\nA hypernetwork is a network that generates the\nweights of another network (Ha et al., 2017).\nThe hypernetworks capture the shared information,\nwhile the generated task conditional adapters and\nlayer normalization allow the model to adapt to each\nindividual task to reduce negative task interference.\nLearned task embedding:We first compute a task\nembeddingIτ∈Rt for each individual task using a\ntask projector networkhI(.), which is a multi-layer\nperceptron consisting of two feed-forward layers and\na ReLU non-linearity:\nIτ=hI(zτ), (5)\nwhere zτ∈Rt′\ncan be a learnable parameter or any\npretrained task features (Vu et al., 2020), and the task\nprojector networkhI(.) learns a suitable compressed\ntask embedding from input task features. In this work,\nwe consider a parametric zτ to allow end-to-end\ntraining which is convenient in practice.1\nRemoving task prefixes:The T5 model prepends\ntask-specific prefixes to the input sequence for\nconditioning. For instance, when training on\nCoLA (Warstadt et al., 2019), cola sentence: is\nprepended to each sample. Instead, we remove task\nprefixes and use task embeddings for conditioning.\nTask conditioned hypernetworks:We consider\nsimple linear layers as hypernetworks that are\nfunctions of input task embeddingsIτ. We introduce\nthese hypernetworks in each layer of the transformer.\nWe define hypernetworkhl\nA(.) that generates task\nconditional adapter weights (Ul\nτ, Dl\nτ):\n1We ran some pilot experiments with pretrained task\nembeddings (Vu et al., 2020), but did not observe extra benefits.\n568\n(Ul\nτ,Dl\nτ):= hl\nA(Iτ)=\n(\nWUl\n,WDl\n)\nIτ, (6)\nwhere WUl\n∈R(d×h)×t and WDl\n∈R(h×d)×t\nare the respective hypernetwork parameters. We\nadditionally define the hypernetwork hl\nLN(.) that\ncomputes the layer normalization parameters:\n(γl\nτ,βl\nτ):= hl\nLN(Iτ)=\n(\nWγl\n,Wβl\n)\nIτ, (7)\nwhereWγl\n∈Rh×t and Wβl\n∈Rh×t.\n2.4 H YPERFORMER++\nA downside of introducing a separate hypernetwork\nin each layer of the Transformer is that it increases the\noverall number of parameters. We, therefore, propose\nto share hypernetworks across transformer layers.\nBy having a shared hypernetwork that is reusable,\nthis strategy results in a substantial reduction in the\nnumber of parameters. However, reapplying the same\nhypernetwork across all the layers introduces weight\nsharing across target parameters, which may not be\ndesirable. To allow for a flexible parameterization of\ntask conditional adapters/layer normalization, for a\ntransformer ofLlayers, we introduce a set oflayer\nid embeddingsI= {li}L\ni=1, and adapter position\nembeddingsP={pj}2\nj=1, which specify the position\nof adapter layers in each transformer block (after\nthe attention layer or feed-forward layer), which are\nused as additional inputs to the hypernetworks. For\nsimplicity, we considerli∈Rt, pj∈Rt, and zτ∈Rt.\nWe feed a concatenation of(zτ,li,pj) to a similar\ntask projector networkh′\nI as in Eq. (5):\nIτ=h′\nI(zτ,li,pj), (8)\nwhich is then followed by a shared layer normaliza-\ntion to compute final task embeddingsIτ∈Rt to the\nhypernetwork. This way, the hypernetwork is able\nto produce distinct weights for each task, adapter po-\nsition, and layer of a transformer. Furthermore, layer\nid and adapter position embeddings are parameters\nthat are learned via back-propagation, allowing us to\ntrain the whole model end-to-end conveniently.\n3 Experiments\nDatasets: Following Raffel et al. (2020), we\nevaluate the performance of the models on the GLUE\nbenchmark (Wang et al., 2019b). This benchmark\ncovers multiple tasks of paraphrase detection (MRPC,\nQQP), sentiment classification (SST-2), natural\nlanguage inference (MNLI, RTE, QNLI), and\nlinguistic acceptability (CoLA).2 The original test\n2Following Raffel et al. (2020); Devlin et al. (2019), as a\ncommon practice, due to the adversarial nature of WNLI with\nrespect to the training set, we do not experiment with WNLI.\nsets are not publicly available, and following Zhang\net al. (2021), for datasets fewer than 10K samples\n(RTE, MRPC, STS-B, CoLA), we divide the original\nvalidation set in half, using one half for validation and\nthe other for the test. For the other larger datasets, we\nsplit 1k samples from the training set as our validation\ndata and test on the original validation set.\nExperimental details: We use the HuggingFace\nimplementation (Wolf et al., 2020a) of the T5\nmodel (Raffel et al., 2020). We fine-tune all\nmodels with a constant learning rate of0.0003 and\nfollowing Raffel et al. (2020), we use218 = 262144\nsteps in all experiments. We save a checkpoint\nevery1000 steps for all models (see also§A). Raffel\net al. (2020) report the results based on the best\ncheckpoint for each task independently. In contrast,\nwe focus on the more realistic setting where we report\nthe results on a single checkpoint with the highest\naverage validation performance across all tasks. The\nhyperparameters are selected in the same manner.\nIn contrast to prior work (Houlsby et al., 2019), we\ndo not learn a separate output layer for each task but\ninstead share a frozen output layer for all the tasks,\nwhich makes our setting more parameter-efficient\nthan prior work and is an advantage of multi-task\nlearning with encoder-decoder models.3\nBaselines: We compare to the strong adapter base-\nline (Houlsby et al., 2019). Following Houlsby et al.\n(2019), we add adapters modules for each task after\nthe two feed-forward modules in each transformer\nblock of the T5 model. As suggested in Houlsby et al.\n(2019), we train the layer normalization parameters\ninside the T5 model, per task. We refer to this method\nas Adapters. We additionally propose a variant of\nthis model, in which we share all layer normalization\nparameters (T5 and adapters) across all tasks. We\nrefer to this model as Adapters†. We compare our\nmodels to the state-of-the-art T5 model, in which we\nfine-tune all parameters of the model on all tasks. We\nrefer to this method as T5SMALL/T5BASE in experiments.\nSampling tasks: During training, we sample tasks\nwith conventional temperature-based sampling with\ntemperatureT= 10for all methods. We sample dif-\nferent tasks proportional top1/T\nτ wherepτ= Nτ∑T\ni=1Nτ\nand Nτ is the number of training samples for theτ-\nth task. We did not experiment with more complex\nsampling strategies (Raffel et al., 2020) or tuning ofT.\n3According to our initial experiments, fine-tuning the final out-\nput layer did not improve performance for adapter-based methods.\n569\nModel #Total\nparams\n#Trained\nparams /\nper task\nCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTEAvg\nSingle-Task Training\nT5SMALL 8.0× 100% 46.81 90.47 86.21/90.67 91.02/87.96 89.11/88.70 82.09 90.21 59.4282.06\nAdaptersSMALLg 1+8×0.01 0.74% 40.12 89.44 85.22/89.29 90.04/86.68 83.93/83.62 81.58 89.11 55.8079.53\nT5BASE 8.0× 100% 54.85 92.19 88.18/91.61 91.46/88.61 89.55/89.41 86.4991.60 67.3984.67\nAdaptersBASEg 1+8×0.01 0.87% 59.49 93.4688.18/91.55 90.94/88.01 87.44/87.18 86.3892.26 68.8484.88\nMulti-Task Training\nT5SMALLm 1.0× 12.5% 50.67 91.39 84.73/88.8989.53/86.31 88.70/88.2781.04 89.67 59.42 81.69\nAdapters†SMALL 1.05× 0.68% 39.87 90.01 88.67/91.81 88.51/84.77 88.15/87.89 79.95 89.60 60.1480.85\nHYPERFORMERSMALL 1.45× 5.80% 47.64 91.39 90.15/92.9688.68/85.08 87.49/86.9681.24 90.3965.2282.47\nHYPERFORMER++SMALL 1.04× 0.50% 53.96 90.59 84.24/88.81 88.44/84.46 87.73/87.26 80.6990.39 71.0182.51\nT5BASEm 1.0× 12.5% 54.88 92.54 90.15/93.0191.13/88.0788.84/88.53 85.66 92.04 75.3685.47\nAdapters†BASE 1.07× 0.82% 61.53 93.00 90.15/92.91 90.47/87.26 89.86/89.44 86.0993.17 70.2985.83\nHYPERFORMERBASE 1.54× 6.86% 61.32 93.80 90.64/93.3390.13/87.18 89.55/89.0386.33 92.79 78.2686.58\nHYPERFORMER++BASE 1.02× 0.29% 63.73 94.0389.66/92.63 90.28/87.2090.00/89.6685.74 93.02 75.36 86.48\nTable 1: Performance of all models on the GLUE tasks. For each method, we report the total number of parameters\nacross all tasks and the number of parameters that are trained for each task as a multiple and proportion respectively of\nthe corresponding single-task T5 model. For MNLI, we report accuracy on the matched validation set. For MRPC and\nQQP , we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation coefficients. For CoLA, we\nreport Matthews correlation. For all other tasks, we report accuracy. Adapters†refers to our proposed variant of adapters\nwith shared layer normalizations. Our HYPERFORMER++ obtains a better score on average compared to full fine-tuning\nand Adapters†, while being more parameter-efficient.m: Our re-implementation of Raffel et al. (2020),g: Applying\nmethod of Houlsby et al. (2019) on T5. Bold fonts indicate the best results in each block.\n3.1 Results on the GLUE Benchmark\nTable 1 shows the results onGLUE for single-task\nand multi-task training. We experiment with reduc-\ntion factors of r= {8,16,32}for all adapter-based\nmethods, wherer= h\nd. We report the results both\nwith T5SMALL (6 layers and 60M parameters) and\nT5BASE models (12 layers and 222M parameters).\nOverall, our proposedHYPERFORMER++ obtains\nstrong gains over Adapters (82.51 versus 79.53 for\nT5SMALL and 86.48 versus 84.88 for T5 BASE) while\nbeing more parameter-efficient.\nOur variant of Adapters†, which shares layer norms\nacross tasks, outperforms prior work (Houlsby et al.,\n2019), which does not share such information (80.85\nversus 79.53 for T5SMALL and 85.83 versus 84.88 for\nT5BASE). This demonstrates that in encoder-decoder\nmodels such as T5 more sharing of information\nacross tasks is beneficial.\nOur proposedHYPERFORMER obtains consistent\nimprovement over our proposed Adapters†method.\nWe attribute this improvement to the ability to learn\nthe shared information across tasks through our hyper-\nnetworks. Interestingly,HYPERFORMER++ obtains\nsimilar performance asHYPERFORMER while being\nmore than an order of magnitude more parameter-\nefficient. Adapter modules thus seem to be similar\nenough so that much of their information can be mod-\neled by a single, appropriately conditioned network.\nCompared to single-task fine-tuning of all param-\neters, our methods on average improve the results by\n0.45 for T5SMALL and 1.81 for T5BASE with substantial\nimprovement on low-resource datasets like CoLA\n(63.73 versus 54.85) and RTE (75.36 versus 67.39)\ndue to shared hypernetworks that capture the shared\ninformation and enable positive transfer effects.\nWe also report the total number of parameters and\ntrainable parameters for all methods in Table 1. For\nadapter-based methods, the number of parameters\nvaries based on the adapter size (we report all numbers\nwith r=32). The multiple in terms of the number of\nparameters ofHYPERFORMER++BASE with regard to\nT5BASE is 1.02×with only0.29% trainable parameters\nper task. Note that by keeping the output layer frozen\nfor AdaptersSMALL and AdaptersBASE, they require\n5.51×and 2.53×fewer parameters respectively com-\npared to a direct application of prior work (Houlsby\net al., 2019). Despite using more efficient baselines,\ncompared to AdaptersBASE, HYPERFORMER++BASE re-\nquires3×fewer trainable parameters.\n3.2 Few-shot Domain Transfer\nFinally, we assess how well a trainedHYPERFORMER\ncan generalize to new tasks. We evaluate performance\non 5 tasks and 7 datasets. In particular, we consider\n1) the natural language inference (NLI) datasets\nSciTail (Khot et al., 2018), and CB (De Marneffe\net al., 2019) from SuperGLUE (Wang et al., 2019a)\n2) the question answering (QA) dataset BoolQ (Clark\n570\net al., 2019a); 3) the sentiment analysis datasets\nIMDB (Maas et al., 2011) and Y elp Polarity (Zhang\net al., 2015); and 4) the paraphrase detection dataset\nPAWS (Baldridge et al., 2019); 5) the question\nclassification dataset TREC (Li and Roth, 2002).\nFor CB and BoolQ, since test sets are not available,\nwe divide the validation sets in half, using one half\nfor validation and the other for testing. For Y elp\npolarity, TREC, and IMDB, since validation sets are\nnot available, we similarly divide the test sets to form\nvalidation sets. For the rest, we report on the original\ntest sets.\nWe consider the models trained on GLUE reported\nin Table 1 and evaluate them on the test set after the\nfew-shot fine-tuning on each target training data. For\nAdapters†and our method, we use the adapter and the\ntask embedding respectively trained on the most sim-\nilar GLUE task for initialization, i.e. MNLI for NLI,\nQNLI for QA, SST -2 for sentiment analysis, and QQP\nfor paraphrase detection. Following prior evidence\nof positive transfer from NLI to other tasks (Conneau\nand Kiela, 2018; Yin et al., 2020; Phang et al., 2018),\nwe initialize the out-of-domain TREC from MNLI.\nWe show the results of full fine-tuning of all model’s\nparameters, Adapters†, and HYPERFORMER++4\nin Table 2. Our method significantly surpasses the\nbaselines on the majority of settings.\n3.3 Low-resource Fine-tuning\n0 1000 2000 3000 4000\n# Samples per task\n60\n65\n70\n75\n80\n85Average scores on GLUE\nT5BASE\nHyperFormer++BASE\nFigure 2: Results on GLUE for the various number of\ntraining samples per task(100,500,1000,2000,4000). We\nshow mean and standard deviation across 5 seeds.\nGiven that our modelHYPERFORMER++BASE has\nsubstantially fewer trainable parameters than T5BASE,\nwe investigate whether it generalizes better in a\nlow-resource setting. We subsample each individual\ntask in GLUE for varying training sizes. We train\nthe models for 15,000 steps, which we found to be\n4We finetune hypernetworks and task embeddings parameters.\nWe also tried only fine-tuning the task embedding but found\nthat this achieves lower performance in the few-shot setting and\ncomparable performance with more samples.\nDataset # SamplesT5\nBASE\nAdapters\n†BASE\nHYPER\nFORMER\n++\nBASE\nNatural Language Inference\nSciTail\n4 79.60 ±3.3 79.54±2.8 82.00±4.9\n16 80.03 ±2.3 83.25±1.7 86.55±1.4\n32 81.97 ±1.3 85.06±1.1 85.85±1.4\n100 84.04 ±0.7 88.22±1.3 88.52±0.7\n500 88.07 ±0.7 91.27±0.8 91.44±0.6\n1000 88.77 ±1.0 91.75±0.8 92.34±0.5\n2000 91.01 ±1.0 92.72±0.5 93.40±0.2\nCB\n4 57.78 ±10.9 51.11±9.2 60.74±16.66\n16 77.04±7.2 74.81±5.4 76.29±4.45\n32 80.0 ±7.6 74.81±5.9 81.48±6.2\n100 85.93 ±5.4 80.74±7.6 87.41±2.96\n250 85.19 ±4.7 86.67±5.0 89.63±4.32\nQuestion Classification\nTREC\n4 28.11 ±5.9 23.61±7.7 28.85±6.9\n16 40.08 ±12.6 43.45±14.0 49.40±9.5\n32 62.49 ±6.2 59.6±7.0 68.94±7.5\n100 87.79 ±0.7 78.07±3.8 88.42±1.7\n500 93.57 ±1.3 93.65±1.7 94.78±1.4\n1000 95.5 ±0.9 96.06±0.4 96.72±1.3\n2000 96.87 ±1.3 97.03±0.7 96.92±0.9\nQuestion Answering\nBoolQ\n4 50.49 ±11.1 53.48±2.8 48.03±4.8\n16 56.50±7.1 51.37±6.5 50.21±7.9\n32 58.43±4.9 54.52±5.1 58.37±3.7\n100 60.10 ±2.4 58.60±1.6 62.03±2.0\n500 66.49 ±1.2 66.72±0.7 70.04±1.4\n1000 69.01 ±1.1 70.21±1.3 72.35±1.7\n2000 71.58 ±0.8 73.60±0.8 74.94±0.6\nSentiment Analysis\nIMDB\n4 77.23 ±3.0 81.55±1.9 81.77±1.8\n16 82.74 ±1.7 82.54±1.0 84.06±0.7\n32 83.42 ±1.0 83.39±0.8 84.64±0.4\n100 84.58 ±0.6 83.35±0.8 84.74±0.4\n500 84.99 ±0.3 85.37±0.5 86.00±0.2\n1000 85.50 ±0.1 86.27±0.4 86.37±0.4\n2000 86.01 ±0.2 86.57±0.2 86.60±0.1\nY elp polarity\n4 76.85 ±14.3 81.37±13.1 90.25±1.0\n16 87.84 ±1.5 91.08±0.2 90.36±1.2\n32 89.22 ±0.7 91.09±0.5 91.15±0.5\n100 90.19 ±0.7 90.15±0.7 91.06±0.6\n500 90.92 ±0.2 91.52±0.2 92.09±0.4\n1000 91.32 ±0.2 92.26±0.6 92.50±0.2\n2000 91.68 ±0.1 92.36±0.4 92.70±0.1\nParaphrase Detection\nPAWS\n4 53.89 ±3.6 55.69±9.0 55.58±7.5\n16 54.18 ±1.0 63.38±5.3 72.71±1.1\n32 55.23 ±3.2 68.78±1.5 73.39±2.1\n100 71.51 ±2.4 73.82±1.6 78.24±2.1\n500 82.81 ±1.0 85.36±0.6 86.3±1.1\n1000 85.67 ±0.7 87.89±0.6 89.12±0.5\n2000 88.33 ±0.6 90.41±0.6 90.87±0.3\nTable 2: Few-shot domain transfer results of the models\ntrained on GLUE averaged across 5 seeds. We compute\naccuracy for all datasets.\n571\nsufficient to allow them to converge. Figure 2 shows\nthe results. HYPERFORMER++BASE substantially\nimproves results with limited training data, indicating\nmore effective fine-tuning in this regime.\n4 Analysis\n4.1 Parameter Efficiency\nIn this section, we compare the number of parameters\nof HYPERFORMER++ with Adapters.\nAdapters parameters: The standard setting\n(Houlsby et al., 2019) employs two adapters per\nlayer for each task. Each adapter layer has 2hd\nparameters for projection matrices (Ul\nτ and Dl\nτ) and\n2hparameters for the layer normalization. The total\nnumber of parameters for Adapters forLTransformer\nlayers in both an encoder and a decoder acrossTtasks\nis, therefore,4TL(2hd+ 2h), which scales linearly\nwith the number of tasks times the number of layers.\nHYPERFORMER++ parameters: Our approach\nlearns a task feature embedding per task, consisting\nof Tt parameters. We additionally employ layer id\nand adapter position embeddings in the encoder and\ndecoder, which require2(2+L)tparameters, with a\nfixed embedding size oftfor all these feature embed-\ndings. We consider a separate task projector networks\nh′\nI for encoder and decoder, which is in both cases\na two-layer MLP , consisting of a total of2(3te+et)\nparameters, wheree= 128is the hidden dimension\nfor the task-projector network. Our hypernetwork\nfor adapters in encoder/decoder consists of2(2thd)\nparameters and our layer normalization hypernetwork\nconsists of 2(2th) parameters. In total, this results\nin t(T+4+2L)  \nTask features\n+ 8te+2t(2hd+2h)  \nHypernetworks\nparameters.\nThe total number of parameters for hypernetworks\nremains constant, while the task feature parameters\nscale with the number of tasks or layers times t,\nwheret=64 in our experiments.\nIn settings with a large number of layers and a large\nnumber of tasks, sincet≪2hd+2hand T+L≪TL,\nour method is much more parameter-efficient com-\npared to Adapters. In the current setting, the termhd\nis the largest term, and the factor2TL for Adapters\nis larger than the factortfor HYPERFORMER++.\n4.2 Do Extra Parameters Make a Difference?\nWhile our HYPERFORMER++ is more parameter-\nefficient than the baselines, the number of parameters\nof HYPERFORMER per task is higher compared to\nAdapters†. To confirm that the improvements of\nModel GLUE #Total\nparams\n#Trained\nparams/task\nAdapters†SMALL 80.97 1.83x 10.44%\nHYPERFORMERSMALL 82.47 1.45x 5.80 %\nAdapters†BASE 85.84 2.02x 12.73%\nHYPERFORMERBASE 86.58 1.54x 6.86%\nTable 3: Averaged test results on GLUE for H YPER-\nFORMER and Adapters†, where Adapters†has a higher\nnumber of parameters compared to HYPERFORMER.\nModel variant GLUE\nHYPERFORMER SMALL 82.47\n−Adapter blocks 68.37\n−Conditional layer norm 79.83\n−Task projector 81.56\n−T5 Layer norm 81.29\n−Conditional layer norm, T5 Layer norm 78.92\nTable 4: Impact when removing different components of\nour framework. We report the average results on GLUE.\nHYPERFORMER are due to its capability of sharing\ninformation across tasks and not the number of\nparameters, as an ablation, we run the Adapters †\nwith r = {2,4}and choose the model performing\nthe best on the validation set. This allows Adapters†\nto have a higher number of parameters compared to\nHYPERFORMER. We report the results in Table 3\nand compare them with results ofHYPERFORMER\nin Table 1. The results demonstrate that even with\nan increased number of parameters, Adapters†is not\nable to reach the performance ofHYPERFORMER,\nand HYPERFORMER performs substantially better.\n4.3 Impact of the Framework Components\nWe investigate the impact of the components of our\nframework including: (1) task conditional adapter\nblocks; (2) task conditional layer normalization;\n(3) task projection network; (4) fine-tuning of\nlayer normalizations in the T5 model; (5) task\nconditional layer normalization in adapter modules\nand fine-tuning of layer normalizations inside the T5\nmodel. We consider our small model of Table 1 and\ntrain different variants of it. Table 4 shows the results\non GLUE, demonstrating that each component of the\nmodel contributes positively to its final performance.\n4.4 Visualization of Task Embeddings\nTo analyze whatHYPERFORMER++BASE has learned\nabout the relations between different tasks, we visual-\nize the learned task embeddings for the models trained\n572\n4\n 2\n 0 2 4 6\n4\n2\n0\n2\n4\n6\nco\nst\nss\nye\nim\nmn sc\ncb\nrt\nmr\nqq pa\nqn\nbo\ntr\nco: CoLA\nst: STS-B\nss: SST-2\nye: Yelp polarity\nim: IMDB\nmn: MNLI\nsc: SciTail\ncb: CB\nrt: RTE\nmr: MRPC\nqq: QQP\npa: PAWS\nqn: QNLI\nbo: BoolQ\ntr: TREC\nFigure 3: Visualization of learned task embeddings by\nHYPERFORMER++BASE.\nwith the largest number of samples in Table 1 and 2.\nFigure 3 illustrates the 2D vector projections of task\nembeddings using PCA (Wold et al., 1987). Interest-\ningly, the observed groupings correspond to similar\ntasks. This shows that learned task embeddings by\nHYPERFORMER++BASE are meaningful. For CB, an\nNLI dataset despite being initialized from MNLI, af-\nter few-shot training the task embedding is closest\nto RTE, another NLI dataset. This is plausible as\npremises and hypotheses in both the discourse-based\nCB and the news and Wikipedia-based RTE are more\ncomplex compared to MNLI. The sentence similarity\ndataset STS-B is grouped close to the MRPC para-\nphrase dataset. CoLA, which focuses on linguistic\nacceptability is very different from other tasks and is\nnot grouped with any of the observed task embeddings.\nIn addition, the task embeddings for 1) all the senti-\nment analysis datasets namely SST-2, Y elp polarity,\nand IMDB; 2) the two large-scale NLI datasets namely\nMNLI and SciTail; 3) question answering datasets, i.e.\nBoolQ and QNLI; and 4) paraphrase datasets namely\nQQP and PAWS are each grouped together.\n5 Related Work\nMulti-task learning: Multi-task learning, i.e.,\nlearning a unified model to perform well on multiple\ndifferent tasks, is a challenging problem in NLP .\nIt requires addressing multiple challenges such as\ncatastrophic forgetting, and handling disproportionate\ntask sizes resulting in a model overfitting in low-\nresource tasks while underfitting in high-resource\nones (Arivazhagan et al., 2019). Liu et al. (2019a) pro-\nposed Multi-Task Deep Neural Network (MTDNN)\nfor learning from multiple NLU tasks. Although\nMTDNN obtains impressive results on GLUE, it\napplies multi-task learning as a form of pretraining\nfollowed by task-specific fine-tuning. Concurrently\nwith us, Tay et al. (2021) propose a multi-task learning\nmethod by training task-conditioned hyper networks;\nhowever, their method is 43x less parameter efficient\ncompared to ours. In another line of research, Clark\net al. (2019b) proposed to learn multi-task models\nwith knowledge distillation. Houlsby et al. (2019)\ntrained adapters for each task separately, keeping\nthe model fixed. Stickland and Murray (2019) share\nthe model parameters across tasks and introduce\ntask-specific adapter parameters, which is more\nparameter-inefficient than our method.\nHypernetworks and contextual parameter\ngeneration: Our work is closely related to hyper-\nnetworks (Ha et al., 2017). In a continual learning\nsetup, where tasks are learned sequentially, Oswald\net al. (2020) proposed a task-conditioned hypernet-\nwork to generate all the weights of the target model.\nOur method is substantially more efficient as we do\nnot generate all the weights of the target model but a\nvery small number of parameters for adapter modules\nto allow the model to adapt to each individual task\nefficiently. Similarly, Jin et al. (2020) generate the\nfull model from task-specific descriptions in different\ndomains whereas we efficiently generate only small\nadapter modules for each task.\nPrior work also proposed meta-learning or\nBayesian approaches to generate softmax layer\nparameters for new settings (Bansal et al., 2020;\nPonti et al., 2020). Meta-learning approaches are\nnotoriously slow to train. In addition, generating\nsoftmax parameters requires a substantially higher\nnumber of parameters, leaves the method unable to\nadapt the lower layers of the model, and restricts their\napplication to classification tasks.\nIn contemporaneous work, ¨Ust¨un et al. (2020)\nproposed a multilingual dependency parsing method\nbased on adapters and contextual parameter generator\nnetworks (Platanios et al., 2018) where they generate\nadapter parameters conditioned on trained input\nlanguage embeddings. Their study is limited to\nmultilingual dependency parsing, while our work\nstudies multi-task learning and applies to several tasks\nthanks to the general sequence-to-sequence nature\nof our model. Moreover, their number of trainable\nparameters is 2.88×larger than their base model\nsince they employ a contextual parameter generator\nin each layer. In contrast, we use a single compact\nhypernetwork allowing us to efficiently condition on\nmultiple tasks and layers of a transformer model.\n573\n6 Conclusion\nWe propose a parameter-efficient method for\nmulti-task fine-tuning. Our approach is to train shared\nhypernetworks to generate task-specific adapters\nconditioned on the task, layer id, and adapter position\nembeddings. The shared hypernetworks capture the\nknowledge across tasks and enable positive transfer\nto low-resource and related tasks, while task-specific\nlayers allow the model to adapt to each individual\ntask. Extensive experiments show that our method\nobtains strong improvement over multi-task learning\non the GLUE benchmark, and substantially improves\nthe in-domain task generalization.\nAcknowledgments\nWe are grateful to Dani Y ogatama, Neil Houlsby, and\nColin Raffel for feedback on a draft of this paper.\nWe would like to also thank Adam Paszke, Jamie\nKiros, and George Dahl for useful comments and\ndiscussions.\nReferences\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry\nLepikhin, Melvin Johnson, Maxim Krikun, Mia Xu\nChen, Y uan Cao, George Foster, Colin Cherry, et al.\n2019. Massively multilingual neural machine trans-\nlation in the wild: Findings and challenges. arXiv\npreprint arXiv:1907.05019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nJason Baldridge, Luheng He, and Y uan Zhang. 2019.\nPaws: Paraphrase adversaries from word scrambling.\nIn NAACL.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum.\n2020. Learning to Few-Shot Learn Across Diverse\nNatural Language Classification Tasks. InCOLING.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. InEMNLP.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom\nKwiatkowski, Michael Collins, and Kristina Toutanova.\n2019a. Boolq: Exploring the surprising difficulty of\nnatural yes/no questions. InNAACL.\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal,\nChristopher D Manning, and Quoc Le. 2019b. Bam!\nborn-again multi-task networks for natural language\nunderstanding. InACL.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representations.\nIn LREC.\nMarie-Catherine De Marneffe, Mandy Simons, and Judith\nTonhauser. 2019. The commitmentbank: Investigat-\ning projection in naturally occurring discourse. In\nproceedings of Sinn und Bedeutung.\nHarm De Vries, Florian Strub, J ´er´emie Mary, Hugo\nLarochelle, Olivier Pietquin, and Aaron C Courville.\n2017. Modulating early visual processing by language.\nIn NeurIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL.\nDavid Ha, Andrew Dai, and Quoc V . Le. 2017. Hypernet-\nworks. In ICLR.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian error\nlinear units (gelus).arXiv preprint arXiv:1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. InICML.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classification.\nIn ACL.\nTian Jin, Zhun Liu, Shengjia Y an, Alexandre Eichen-\nberger, and Louis-Philippe Morency. 2020. Language\nto network: Conditional parameter adaptation with\nnatural language descriptions. InACL.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. InAAAI.\nXin Li and Dan Roth. 2002. Learning question classifiers.\nIn COLING.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks for\nnatural language understanding. InACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov. 2019b. Roberta:\nA robustly optimized bert pretraining approach.arXiv\npreprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan\nHuang, Andrew Y . Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. InACL.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language\ndecathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nJohannes V on Oswald, Christian Henning, Jo˜ao Sacra-\nmento, and Benjamin F Grewe. 2020. Continual\nlearning with hypernetworks. InICLR.\n574\nEthan Perez, Florian Strub, Harm De Vries, Vincent\nDumoulin, and Aaron Courville. 2018. Film: Visual\nreasoning with a general conditioning layer. InAAAI.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. InRepL4NLP.\nJonas Pfeiffer, Andreas R¨uckl´e, Clifton Poth, Aishwarya\nKamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. AdapterHub: A\nframework for adapting transformers. In EMNLP:\nSystem Demonstrations.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nEmmanouil Antonios Platanios, Mrinmaya Sachan,\nGraham Neubig, and Tom Mitchell. 2018. Contextual\nparameter generation for universal neural machine\ntranslation. InEMNLP.\nEdoardo M Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela\nParovic, Roi Reichart, and Anna Korhonen. 2020.\nParameter space factorization for zero-shot learn-\ning across tasks and languages. arXiv preprint\narXiv:2001.11453.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Y anqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer.\nJMLR.\nAlex Ratner, Braden Hancock, Jared Dunnmon, Roger\nGoldman, and Christopher R´e. 2018. Snorkel metal:\nWeak supervision for multi-task learning. In DEEM\nworkshop.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nV edaldi. 2018. Efficient parametrization of multi-\ndomain deep neural networks. InCVPR.\nSebastian Ruder. 2017. An Overview of Multi-Task\nLearning in Deep Neural Networks. InarXiv preprint\narXiv:1706.05098.\nAsa Cooper Stickland and Iain Murray. 2019. Bert and\npals: Projected attention layers for efficient adaptation\nin multi-task learning. InICML.\nYi Tay, Zhe Zhao, Dara Bahri, Don Metzler, and Da-\nCheng Juan. 2021. Hypergrid transformers: Towards a\nsingle model for multiple tasks. InICLR.\nAhmet ¨Ust¨un, Arianna Bisazza, Gosse Bouma, and Gert-\njan van Noord. 2020. Udapter: Language adaptation\nfor truly universal dependency parsing. InEMNLP.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In NeurIPS.\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro\nSordoni, Adam Trischler, Andrew Mattarella-Micke,\nSubhransu Maji, and Mohit Iyyer. 2020. Exploring and\npredicting transferability across NLP tasks. InEMNLP.\nAlex Wang, Y ada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language\nunderstanding systems. InNeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. 2019b. GLUE:\nA multi-task benchmark and analysis platform for\nnatural language understanding. InICLR.\nZirui Wang, Zihang Dai, Barnab´as P´oczos, and Jaime Car-\nbonell. 2019c. Characterizing and avoiding negative\ntransfer. InCVPR.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nSvante Wold, Kim Esbensen, and Paul Geladi. 1987.\nPrincipal component analysis. Chemometrics and\nintelligent laboratory systems.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma,\nY acine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020a. Transformers: State-\nof-the-art natural language processing. In EMNLP:\nSystem Demonstrations.\nThomas Wolf, Quentin Lhoest, Patrick von Platen,\nY acine Jernite, Mariama Drame, Julien Plu, Julien\nChaumond, Clement Delangue, Clara Ma, Abhishek\nThakur, Suraj Patil, Joe Davison, Teven Le Scao,\nVictor Sanh, Canwen Xu, Nicolas Patry, Angie\nMcMillan-Major, Simon Brandeis, Sylvain Gugger,\nFranc ¸ois Lagunas, Lysandre Debut, Morgan Funtow-\nicz, Anthony Moi, Sasha Rush, Philipp Schmidd,\nPierric Cistac, Victor Mu ˇstar, Jeff Boudier, and\nAnna Tordjmann. 2020b. Datasets. GitHub. Note:\nhttps://github.com/huggingface/datasets.\nWenpeng Yin, Nazneen Fatema Rajani, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2020. Universal\nnatural language processing with limited annotations:\nTry few-shot textual entailment as a start. InEMNLP.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Y oav Artzi. 2021. Revisiting few-sample\nbert fine-tuning. InICLR.\nXiang Zhang, Junbo Zhao, and Y ann Lecun. 2015.\nCharacter-level convolutional networks for text\nclassification. InNeurIPS.\n575\nA Experimental Details\nComputing infrastructure: We run the experi-\nments in Table 1 on 4 GPUs, and the rest of the experi-\nments on 1 GPU on a heterogeneous cluster with Tesla\nV100, Tesla A100, Tesla P4, and GTX1080ti GPUs.\nHyperparameters: We use a batch size of 64 for\nT5SMALL and 32 for T5BASE to fit the GPU memory. We\nset the dimension of the task feature embedding (zτ)\nto t′=512, and the dimension of the task embedding\n(Iτ) to t= 64. For low-resource fine-tuning in§3.3,\nwe use reduction factors of{16,32,64}.\nData pre-processing: We download all datasets\nfrom the HuggingFace Datasets library (Wolf et al.,\n2020b). Following Raffel et al. (2020), we cast all\ndatasets into a sequence-to-sequence format, and\nrecast STS-B as a 21-class classification task by round-\ning its target scores to their nearest increment of 0.2.\nPerformance evaluation: Table 5 and 6 present\nthe efficiency evaluation in terms of memory, and\ntime for all the methods measured on the GLUE\nbenchmark. We report the time for 1000 training steps.\nOur approach has several attractive properties. Our\nHYPERFORMER++BASE approach offers a much better\nmemory usage with low-overhead, while HYPER-\nFORMER BASE and T5BASE cause substantial memory\noverhead. In dealing with large-scale transformer mod-\nels like T5, efficient memory usage is of paramount\nimportance. Second, in terms of training time, our\nmethod is much faster than Adapters†BASE. Relative to\nT5BASE, HYPERFORMER++BASE increases the training\ntime by 30.49%, while Adapters †BASE causes the\nsubstantial training time overhead of 84.93%.\nModel Memory ∆%\nT5BASE 7.76 (GB) -\nAdapters†BASE 5.95 (GB) -23.32%\nHYPERFORMER BASE 7.60 (GB) -2.06%\nHYPERFORMER++BASE 5.81 (GB) -25.13\nTable 5: The required memory for all methods. ∆% is\nthe relative difference with respect to T5BASE.\nModel Time ∆%\nT5BASE 5.51 (min) -\nAdapters†BASE 10.19 (min) 84.93%\nHYPERFORMER BASE 7.92 (min) 43.74%\nHYPERFORMER++BASE 7.19 (min) 30.49%\nTable 6: Training time for all methods.∆% is the relative\ndifference with respect to T5BASE.\nImpact of adapter’s bottleneck size on the perfor-\nmance Similar to (Houlsby et al., 2019), adapter’s\nreduction factor needs to be set per dataset. Ta-\nble 7 shows the validation performance ofHYPER-\nFORMER++ on the GLUE tasks for different adapters’\nreduction factors. While the pattern may not be al-\nways consistent, generally, smaller datasets seem to\nbenefit more from smaller bottleneck size, i.e., less pa-\nrameters for adapters, while the opposite is the case for\nlarger datasets, which require more modeling capacity.\n576\nModel r CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg\nHYPERFORMER++SMALL 8 42.13 98.60 82.76/87.72 90.69/87.55 84.92/84.18 82.3 95.40 78.83 83.19\nHYPERFORMER++SMALL 16 42.60 97.8 84.73/89.12 88.99/85.33 85.69/85.12 81.96 93.69 75.91 82.81\nHYPERFORMER++SMALL 32 49.90 96.00 83.74/88.50 89.29/85.79 85.99/85.41 81.28 91.79 72.99 82.79\nHYPERFORMER++BASE 8 54.86 97.30 88.18/91.55 94.59/92.91 89.77/89.69 85.89 96.10 84.67 87.77\nHYPERFORMER++BASE 16 53.83 98.00 88.18/91.61 94.89/93.33 90.12/89.65 85.94 96.50 83.94 87.82\nHYPERFORMER++BASE 32 55.58 97.20 89.66/92.42 93.19/91.08 88.96/88.57 85.82 94.19 81.75 87.13\nTable 7: V alidation performance of HYPERFORMER++ on the GLUE tasks for different reduction factorsr={8,16,32}.\nFor MNLI, we report accuracy on the matched validation set. For MRPC and QQP , we report accuracy and F1. For\nSTS-B, we report Pearson and Spearman correlation coefficients. For CoLA, we report Matthews correlation. For all\nother tasks, we report accuracy."
}