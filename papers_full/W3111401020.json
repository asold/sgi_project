{
  "title": "Flight of the PEGASUS? Comparing Transformers on Few-shot and Zero-shot Multi-document Abstractive Summarization",
  "url": "https://openalex.org/W3111401020",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3203926103",
      "name": "Travis Goodwin",
      "affiliations": [
        "United States National Library of Medicine",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A5088942774",
      "name": "Max Savery",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2054753577",
      "name": "Dina Demner-Fushman",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6813275036",
    "https://openalex.org/W6887725390",
    "https://openalex.org/W2401044082",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3021199973",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1974339500",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2949925034",
    "https://openalex.org/W3027635833",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3090073303",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2978708643",
    "https://openalex.org/W3100439863"
  ],
  "abstract": "Recent work has shown that pre-trained Transformers obtain remarkable performance on many natural language processing tasks, including automatic summarization. However, most work has focused on (relatively) data-rich single-document summarization settings. In this paper, we explore highly-abstractive multi-document summarization, where the summary is explicitly conditioned on a user-given topic statement or question. We compare the summarization quality produced by three state-of-the-art transformer-based models: BART, T5, and PEGASUS. We report the performance on four challenging summarization datasets: three from the general domain and one from consumer health in both zero-shot and few-shot learning settings. While prior work has shown significant differences in performance for these models on standard summarization tasks, our results indicate that with as few as 10 labeled examples, there is no statistically significant difference in summary quality, suggesting the need for more abstractive benchmark collections when determining state-of-the-art.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5640–5646\nBarcelona, Spain (Online), December 8-13, 2020\n5640\nFlight of the PEGASUS? Comparing Transformers on Few-Shot and\nZero-Shot Multi-document Abstractive Summarization\nTravis R. Goodwinand Max E. Saveryand Dina Demner-Fushman\nU.S. National Library of Medicine\nNational Institutes of Health\n{firstname.lastname}@nih.gov\nAbstract\nRecent work has shown that pre-trained Transformers obtain remarkable performance on many\nnatural language processing tasks, including automatic summarization. However, most work has\nfocusedon(relatively)data-richsingle-documentsummarizationsettings. Inthispaper,weexplore\nhighly-abstractive multi-document summarization, where the summary is explicitly conditioned\non a user-given topic statement or question. We compare the summarization quality produced\nby three state-of-the-art transformer-based models: BART, T5, and PEGASUS. We report the\nperformance on four challenging summarization datasets: three from the general domain and one\nfrom consumer health in both zero-shot and few-shot learning settings. While prior work has\nshown signiﬁcant diﬀerences in performance for these models on standard summarization tasks,\nour results indicate that with as few as 10 labeled examples, there is no statistically signiﬁcant\ndiﬀerence in summary quality, suggesting the need for more abstractive benchmark collections\nwhen determining state-of-the-art.\n1 Introduction\nSince its inception (Luhn, 1958), automatic summarization has focused on summarizing documents either\nin a generic way – conveying the main points of the document to any reader regardless of their information\nneed – or in a task-speciﬁc way – distilling the important points of the document with respect to a speciﬁc\ninformation need such as a question or topic statement (Mani, 2009). In the latter case, the selection of the\nmost salient points in the document (i.e., content selection) as well as the expression of those points (i.e.,\nsurface realization) must be explicitly conditioned on a user-given natural language context statement,\nsuch as a question or topic of interest. In this setting, a single passage may be summarized in diﬀerent\nways depending on the context description. Consequently, obtaining reference summaries is often time- or\ncost-prohibitive, particularly when dealing with specialized domains such as healthcare.\nThe Document Understanding Conference has explored Topic-driven summarization (DUC) and its\nsuccessor, the Text Analysis Conference (TAC), which both ran community evaluations of topic- or\nquestion-based summarization. Speciﬁcally, participants were asked to develop automatic summarization\napproaches for generating single- or multi-document summaries that summarized a set of documents with\nrespect to a given topic description or question, as shown in Figure 1. Human assessors manually judged\nsubmitted summaries.\nIn this work, we revisit the multi-document topic-driven abstractive summarization datasets produced\nfrom DUC 2007, TAC 2009, and TAC 2010, as well as question-driven summarization from consumer\nhealth. Because these datasets are relatively small (approximately 45 topics each), we explore modern\ntransformer-based models’ performance in the zero-shot and few-shot (10 examples) learning settings.\nSpeciﬁcally, we explore the quality of multi-document abstractive summarization generated by T5 (Raﬀel\net al., 2019), BART (Lewis et al., 2019), and PEGASUS (Zhang et al., 2019).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\n//creativecommons.org/licenses/by/4.0/.\n5641\nQuestion: What became of the cast and oth-\ners related to the “Seinfeld” TV series after\nit ended? What actions were taken by others\nin response to the show’s closing?\nSummary: After“Seinfeld”ended,JerrySe-\ninfeld did a stand-up tour, a 10-show Broad-\nway run, and an HBO special, “I’m Telling\nYou This for the Last Time”. He [...]\n(a) DUC 2007\nTopic: Describe eﬀorts made toward peace\nin the India-Pakistan conﬂict over Kashmir.\nSummary: Since they became separate\nnations in 1947, India and Pakistan have\nfought two wars over Kashmir, the Hi-\nmalayan province which was split between\nthem. KashmirisIndia’sonlymajorityIslam\nprovince, and an Islam [...]\n(b) TAC 2009\nQuestion: What is Nephrotic Syndrome.\nWhat are its causes and cures?\nSummary: Nephrotic syndrome is caused\nby diﬀerent disorders that damage the kid-\nneys. In adults, most commonly by glomeru-\nlonephritis. This damage leads to the release\nof too much protein in the urine [...]\n(c) MEDIQA\nFigure1: Exampletopic-andquestion-drivenmulti-documentabstractivesummaries(documentsomitted).\n2 Background\nRecent work has indicated that transfer learning (pre-training a model on data-rich tasks before ﬁne-tuning\nit on a downstream task) obtains remarkable performance on many natural language processing tasks\n(Yang et al., 2019; Dong et al., 2019; Liu et al., 2019b). The most successful models are obtained through\nself-supervised pre-training with massive datasets to obtain transferable knowledge for new tasks (i.e.,\nﬁne-tuning) with less abundant data (Devlin et al., 2019; Lewis et al., 2019; Keskar et al., 2019; Raﬀel et\nal., 2019). More recently, research has indicated that these models can generate language conditioned on a\nuser-given prompt or context. For example, this prompt can guide the model’s content selection towards a\nparticular topic (Keskar et al., 2019) or inform surface realization for a speciﬁc task (Lewis et al., 2019;\nRaﬀel et al., 2019). In Liu et al. (2020), the authors condition an extractive transformer using “control\ncodes” to specify the position, importance, and diversity of the sentences in the source text. In this work,\nwe adapt this paradigm to train and evaluate BART, T5, and PEGASUS for abstractive multi-document\nsummarization.\nAlthough zero-shot learning (ZSL) has received considerable attention in the image processing\ncommunity, there has been comparatively little work on zero-shot learning speciﬁcally for summarization:\nDuan et al. (2019) explore zero-shot learning for cross-lingual sentence summarization and Liu et al.\n(2019a) explored zero-shot abstractive summaries of ﬁve-sentence stories. We extend these works by\nevaluating zero-shot and few-shot learning for multi-document abstractive summarization.\n3 Models\nIn this work, we compare three of the most prominent conditional language generation models: T5, BART,\nand PEGASUS. To facilitate comparison, for each model we chose the variant with the most similar\narchitecture(suchthateachconsistsof12transformerlayersandasimilarnumberoflearnableparameters).\nEach model is pre-trained with unique strategies as described below.\nBART (Bidirectional and Auto-Regressive Transformers) is pre-trained on document rotation, sentence\npermutation, text-inﬁlling, and token masking and deletion objectives (Lewis et al., 2019). In our\nexperiments, we usedBART-Large.\nT5 (Text-to-TextTransferTransformer)ispre-trainedonseveralunsupervisedandsupervisedobjectives,\nsuch as token and span masking, as well as translation, classiﬁcation, reading comprehension, and\nsummarization. Importantly, each objective is treated as a language-generation task, where the model\nis conditioned to generate the correct output based on a textual prompt included in the input sequence\n(Raﬀel et al., 2019). In this work, we usedT5-Base.\nPEGASUS (Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-\nsequence)wasspeciﬁcallydesignedforabstractivesummarizationandispre-trainedwithaself-supervised\ngap-sentence-generation objective (Zhang et al., 2019). In this task, entire sentences are masked from\nthe source document, concatenated, and used as the target “summary”. We usedPEGASUS-Base in our\nexperiments.\n5642\n4 Experiments\nWe evaluated T5, BART, and PEGASUS in zero-shot (ZSL) and few-shot (FSL) learning settings on four\ndatasets. Summary quality was measured using ROUGE-1, ROUGE-2, and ROUGE-L\u001b1-scores (Lin,\n2004); BLEU-4 (Papineni et al., 2002); and Repetition Rate (in unigrams). Implementation details are\nprovided in Appendix A.\n4.1 Answer Summarization at DUC 2007\nThe 2007 challenge of the Document Understanding Conference (DUC) focused on answering 45 natural\nlanguagequestionsbysummarizingsetsof10documentsfromtheAQUAINTEnglishnewscorpus(Graﬀ,\n2002). Reference summaries were between 230 and 250 words. We used 30 topics for testing (with 10 for\ntraining and 5 for validation under FSL). Table 1 presents these results, showing that BART obtains the\nhighest quality summaries in both settings, though FSL provides a signiﬁcant increase for all models.\nSystem ROUGE-1 ROUGE-2 ROUGE-L BLEU-4 Repetition\nT5 (ZSL) 21.21 (20.37 –22.04) 4.35 (3.82 – 4.91) 11.59 (11.17 –12.03) 1.45 (1.24 –1.73) 33.21 (31.82 –34.60)\nT5 (FSL) 36.35 (34.96 –37.66) 9.12 (8.27 – 9.94) 17.46 (16.85 –18.10) 4.81 (4.22 –5.51) 54.20 (52.27 –56.24)\nBART (ZSL) 37.36 (36.18 –38.59) 8.08 (7.34 – 8.88) 16.62 (16.08 –17.18) 5.14 (4.52 –5.84) 44.91 (44.05 –45.83)\nBART (FSL) 40.86 (39.84 –41.81) 9.40 (8.69 –10.08) 18.38 (17.93 –18.85) 6.06 (5.46 –6.68) 53.96 (53.17 –54.69)\nPEGASUS (ZSL) 26.36 (25.05 –27.64) 5.01 (4.38 – 5.70) 14.69 (13.95 –15.34) 2.18 (1.83 –2.58) 65.52 (60.81 –70.36)\nPEGASUS (FSL) 36.02 (34.63 –37.33) 7.95 (7.26 – 8.65) 18.88 (18.27 –19.49) 5.21 (4.57 –5.85) 74.29 (71.73 –76.92)\nTable 1: Abstract multi-document summarization on DUC 2007 with 95% conﬁdence intervals.\n4.2 Update Summarization at TAC 2009\nIn 2009, the Text Analysis Conference (TAC) summarization evaluation explored summarizing sets of\n10 newswire articles with respect to a given topic description in approximately 100 words under the\nassumption that a user had already read a given set of earlier articles (Dang and Owczarzak, 2009). Of44\ntopics used in 2009, we used 30 for testing (with 10 for training and 4 for validation under FSL). Table 2\npresents these results. While T5 had the highest performance in zero-shot performance, there was no\nstatistically signiﬁcant diﬀerence in terms of ROUGE after few-shot training, although T5 did obtain\nimproved BLEU.\nSystem ROUGE-1 ROUGE-2 ROUGE-L BLEU-4 Repetition\nT5 (ZSL) 29.97 (28.55 –31.38) 9.03 (7.78 –10.30) 17.98 (16.89 –19.21) 3.67 (3.10 –4.36) 27.96 (26.76 –29.31)\nT5 (FSL) 38.36 (36.92 –39.85) 11.56 (10.25 –12.95) 21.06 (19.74 –22.59) 8.55 (7.32 –9.78) 33.91 (32.75 –35.03)\nBART (ZSL) 12.82 (11.68 –13.97) 3.73 (3.27 – 4.20) 9.43 (8.76 –10.12) 0.57 (0.44 –0.74) 7.32 (5.43 – 9.41)\nBART (FSL) 39.28 (38.10 –40.46) 11.33 (10.37 –12.44) 21.11 (20.27 –22.02) 7.30 (6.49 –8.11) 45.30 (44.24 –46.33)\nPEGASUS (ZSL) 25.69 (23.88 –27.66) 5.70 (4.74 – 6.69) 16.72 (15.77 –17.65) 3.31 (2.81 –3.91) 75.56 (71.07 –80.36)\nPEGASUS (FSL) 38.96 (37.64 –40.17) 10.44 (9.51 –11.40) 21.92 (20.84 –22.95) 7.00 (6.24 –7.88) 43.59 (41.50 –45.87)\nTable 2: Abstract multi-document summarization on TAC 2009 with 95% conﬁdence intervals.\n4.3 Guided Summarization at TAC 2010\nSimilar to the 2009 evaluation, the summarization track’s goal in TAC 2010 was to produce 100-word\nsummaries of sets of 10 newswires articles for 46 given topics. However, in 2010 each topic was assigned\ntooneofﬁvepre-deﬁnedcategories,andsummarieswereexpectedtocoverallaspectsassociatedwiththat\ncategory (e.g., forAccidents and Natural Disasters, summaries should cover (a) what happened, (b) when\nit happened, (c) the reasons for the accident or disaster, (d) casualties, (e) damages, and (f) rescue eﬀorts\nor countermeasures) (Owczarzak and Dang, 2010). We used 30 topics for testing (with 10 for training and\n6 for validation). Results are illustrated in Table 3. In this case, BART had the highest performance in\nboth ZSL and FSL settings, although FSL provided signiﬁcant improvements for all models, allowing T5\nto obtain similar ROUGE-2 and ROUGE-L performance.\n5643\nSystem ROUGE-1 ROUGE-2 ROUGE-L BLEU-4 Repetition\nT5 (ZSL) 27.01 (25.65 –28.35) 6.25 (5.35 – 7.29) 15.72 (14.84 –16.75) 2.06 (1.72 –2.45) 30.47 (29.07 –31.91)\nT5 (FSL) 34.13 (32.72 –35.77) 8.36 (7.32 – 9.50) 17.35 (16.44 –18.28) 5.59 (4.70 –6.54) 32.60 (31.25 –34.05)\nBART (ZSL) 28.97 (27.48 –30.70) 6.32 (5.58 – 7.24) 15.64 (14.80 –16.40) 3.62 (3.11 –4.22) 27.96 (26.06 –29.74)\nBART (FSL) 38.22 (36.98 –39.41) 10.15 (9.17 –11.18) 20.11 (19.27 –20.94) 6.85 (5.99 –7.68) 39.91 (38.67 –41.11)\nPEGASUS (ZSL) 24.87 (23.14 –26.48) 4.99 (4.31 – 5.77) 14.80 (13.97 –15.65) 2.66 (2.26 –3.19) 57.15 (51.71 –62.81)\nPEGASUS (FSL) 36.31 (34.95 –37.63) 9.21 (8.27 –10.15) 20.35 (19.56 –21.26) 5.81 (5.09 –6.62) 40.39 (37.73 –43.31)\nTable 3: Abstract multi-document summarization on DUC 2007 with 95% conﬁdence intervals.\n4.4 MEDIQA Summarization\nThe MEDIQA collection contains consumer health questions, sets of passages extracted from reliable\nwebsites relevant to the question, and human-authored multi-document summaries of the passages\nintended to provide consumer-friendly answers (Savery et al., 2020). Of the 156 available abstractive\nmulti-document summaries, we used 141 questions for testing (with 10 for training and 5 for validation\nunder FSL). Table 4 provides these results. While FSL provided a clear improvement for all models,\nthere were no statistically signiﬁcant diﬀerences in summary quality between the three models using FSL.\nExample summaries from all systems for a single MEDIQA question are provided in Figure 3.\nSystem ROUGE-1 ROUGE-2 ROUGE-L BLEU-4 Repetition\nT5 (ZSL) 31.09 (28.46 –33.72) 14.63 (11.77 –17.58) 22.52 (20.15 –25.19) 7.12 (5.07 – 9.36) 31.00 (29.06 –32.96)\nT5 (FSL) 38.56 (35.94 –41.13) 18.52 (15.54 –21.50) 26.00 (23.67 –28.76) 10.90 (9.08 –13.07) 36.19 (34.73 –37.78)\nBART (ZSL) 33.51 (31.21 –36.14) 13.87 (11.52 –16.31) 20.87 (18.92 –22.88) 8.21 (6.38 –10.18) 38.24 (36.60 –39.79)\nBART (FSL) 37.65 (35.07 –40.37) 17.01 (14.38 –20.12) 23.54 (21.34 –26.00) 10.83 (8.83 –13.04) 41.48 (40.13 –42.87)\nPEGASUS (ZSL) 29.75 (26.20 –32.89) 12.17 (9.44 –15.12) 20.88 (18.19 –23.49) 8.61 (6.53 –10.84) 63.87 (58.64 –69.69)\nPEGASUS (FSL) 37.02 (33.86 –40.33) 17.04 (13.95 –20.12) 24.90 (22.18 –27.68) 12.40 (9.96 –15.08) 46.81 (43.40 –50.27)\nTable 4: Abstract multi-document summarization on MEDIQA with 95% conﬁdence intervals.\n4.5 Few-shot and Zero-shot Learning\nFigure 2 compares the performance of each model in FSL and ZSL settings. FSL provided signiﬁcant\nincreases in performance on all tasks for PEGASUS, all but MEDIQA for BART, and only two tasks for\nT5, suggesting that while FSL is clearly useful for all three models, it most beneﬁts PEGASUS.\n0 10 20 30\nDUC 2007\nTAC 2009\nTAC 2010\nMEDIQA\n BART\n0 10 20 30\n T5\n0 10 20 30\n PEGASUS\nSetting\nZSL\nFSL\nFigure 2: Rouge-L of each model trained under the few-shot (FSL) and zero-shot (ZSL) learning settings.\n5 Conclusion\nWe evaluated the summarization quality produced by three state-of-the-art transformers: BART, T5,\nand PEGASUS on four challenging summarization dataset in both zero-shot and few-shot learning\nsettings. Ourresultsindicatethat,whiletherearestatisticallysigniﬁcantdiﬀerencesbetweenthemodelsin\nzero-shot settings, after few-shot learning with as few as 10 examples, there is little discernible diﬀerence\nbetween them. This suggests that while large improvements have been made on standard single-document\nbenchmarks, highly abstractive multi-document summarization remains challenging.\nAcknowledgements\nThis work was supported by the intramural research program at the U.S. National Library of Medicine,\nNational Institutes of Health, and utilized the computational resources of the NIH HPC Biowulf cluster\n(http://hpc.nih.gov).\n5644\nQuestion: What are the causes of childhood obesity?\nContext: OverweightandObesity(RiskFactors): Therearemanyriskfactorsforoverweightandobesity. Someriskfactorscan\nbe changed, such as unhealthy lifestyle habits and environments. Other risk factors, such as age, family history and genetics,\nrace and ethnicity, and sex, cannot be changed. Heathy lifestyle changes can decrease your risk for developing overweight and\nobesity. Unhealthy lifestyle habits Lack of physical activity, unhealthy eating patterns, not enough sleep, and high amounts of\nstress can increase your risk for overweight and obesity. Lack of physical activity Lack of physical activity due to high amounts\nof TV, computer, videogame or other screen usage has been associated with a high body mass index.[abridged]\nReference Summary:There are many factors that contribute to increasing your child’s risk of becoming overweight. Regularly\neating high calorie foods can cause your child to gain weight. Lack of physical exercise can contribute to childhood obesity. If\nyour child comes from a family of overweight people, he or she may be more likely to put on weight. Personal, parental, and\nfamily stress can also increase a child’s risk of obesity.\nBART (FSL):There are many risk factors for overweight and obesity. Some risk factors can be changed, such as age, family\nhistory and genetics, race and ethnicity, and sex. However, some risk factors cannot be changed. These include: poor diet, lack\nof physical activity, unhealthy eating patterns, not enough sleep, and high amounts of stress. social factors such as having a low\nsocioeconomic status or an unhealthy social or unsafe environment in the neighborhood. Eating too much saturated and trans\nfats, eatingfoodshighinaddedsugars, andhavinganunhealthyeatingpatterncanincreaseyourriskforobesity. Avoidingfoods\nthat are high in fat and saturated fat, eating smaller meals, and reducing screen time\nBART (ZSL):question: what are the causes of childhood obesity summarize: Overweight and Obesity (Risk Factors): There\nare many risk factors for overweight and obesity. Some risk factors can be changed, such as unhealthy lifestyle habits and\nenvironments. Heathy lifestyle changes can decrease your risk for becoming overweight and obese, and can help you lose\nweight and maintain a healthy weight. Other risk factors, including age, family history and genetics, race and ethnicity, and sex,\ncannot be changed. Age Childhood obesity remains a serious problem in the United States, and some populations are more at\nrisk for childhood obesity than others. The risk of unhealthy weight gain increases as you age. Adults who have\nT5 (FSL):There are many risk factors for overweight and obesity. Some risk factors can be changed, such as unhealthy lifestyle\nhabits and environments. Other risk factors such as age, family history and genetics, race and ethnicity, and sex, cannot be\nchanged. Unhealthy lifestyle habits Lack of physical activity, unhealthy eating patterns, not enough sleep, and high amounts of\nstress can increase your risk for overweight or obesity. Lack of exercise due to TV, computer, videogame, or other screen usage\nhas been associated with a high body mass index. Eating too much saturated and trans fats Eating foods high in added sugars\nVisit Heart-healthy eating for more information about healthy\nT5 (ZSL):Unhealthy lifestyle habits Lack of physical activity, unhealthy eating patterns, not enough sleep, and high amounts of\nstress can increase your risk for overweight and obesity . age Childhood obesity remains a serious problem in the united states,\nand some populations are more at risk for childhood obesity than others .\nPEGASUS (FSL):Lifestyle changes can decrease your risk for developing overweight and obesity. Unhealthy lifestyle habits,\nsuch as eating more calories than you use, and not enough sleep can increase your risk for overweight and obesity. Lack of\nphysical activity due to high amounts of TV, computer, videogame or other screen usage has been associated with a high body\nmass index. Healthy lifestyle changes, such as being physically active and reducing screen time, can help you aim for a healthy\nweight.\nPEGASUS (ZSL):Unhealthy lifestyle habits Lack of physical activity, unhealthy eating patterns, not enough sleep, and high\namounts of stress can increase your risk for overweight and obesity. Unhealthy environments Many environmental factors can\nincrease your risk for overweight and obesity: social factors such as having a low socioeconomic status or an unhealthy social or\nunsafe environment in the neighborhood built environment factors such as easy access to unhealthy fast foods, limited access to\nrecreational facilities or parks, and few safe or easy ways to get exercise. Read more Overweight and obesity can be prevented\nby eating healthy foods, getting enough sleep, and reducing stress. Read more Overweight and obesity can be prevented by\neating healthy foods, getting enough sleep, and reducing stress. Read\nFigure 3: Example summaries for the question,What are the causes of childhood obesity?\nReferences\nHoaTrangDangandKarolinaOwczarzak. 2009. OverviewoftheTAC2009summarizationtrack. In Proceedings\noftheSecondTextAnalysisConference ,volume2,Gaithersburg,Maryland,USA.NationalInstituteofStandards\nand Technology.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. InProceedings of the 2019 Conference of the North American\nChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Longand\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-\nWuen Hon. 2019. Uniﬁed language model pre-training for natural language understanding and generation. In\nH.Wallach, H.Larochelle, A.Beygelzimer, F.d/quotesingle.ts1Alché-Buc, E.Fox, andR.Garnett, editors,AdvancesinNeural\nInformation Processing Systems 32, pages 13063–13075. Curran Associates, Inc.\n5645\nXiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, and Weihua Luo. 2019. Zero-shot cross-lingual\nabstractivesentencesummarizationthroughteachinggenerationandattention. In Proceedingsofthe57thAnnual\nMeeting of the Association for Computational Linguistics, pages 3162–3172, Florence, Italy, July. Association\nfor Computational Linguistics.\nDavid Graﬀ. 2002. The aquaint corpus of English news text: portions© 1998-2000 New York Times, Inc.,©\n1998-2000 Associated Press, Inc.,© 1996-2000 Xinhua News Service. Linguistic Data Consortium.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A\nconditional transformer language model for controllable generation.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoy-\nanov, and Luke Zettlemoyer. 2019. BART: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension.\nChin-YewLin. 2004. ROUGE:Apackageforautomaticevaluationofsummaries. In TextSummarizationBranches\nOut, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.\nPeter J. Liu, Yu-An Chung, and Jie Ren. 2019a. SummAE: Zero-shot abstractive text summarization using\nlength-agnostic auto-encoders.\nXiaodongLiu,PengchengHe,WeizhuChen,andJianfengGao. 2019b. Multi-taskdeepneuralnetworksfornatural\nlanguage understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 4487–4496, Florence, Italy, July. Association for Computational Linguistics.\nZhengyuan Liu, Ke Shi, and Nancy F. Chen. 2020. Conditional neural generation using sub-aspect functions for\nextractive news summarization.\nHansPeterLuhn. 1958. Theautomaticcreationofliteratureabstracts. IBMJournalofResearchandDevelopment ,\n2(2):159–165.\nInderjeetMani. 2009. Summarizationevaluation: anoverview. InProceedingsoftheNTCIRWorkshop ,volume2.\nKarolinaOwczarzakandHoaTrangDang. 2010. OverviewoftheTAC2010summarizationtrack. In Proceedings\nof the Third Text Analysis Conference, volume 3, Gaithersburg, Maryland, USA. National Institute of Standards\nand Technology.\nKishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. 2002. Bleu: amethodforautomaticevaluationof\nmachinetranslation. InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics ,\npages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.\nColinRaﬀel, NoamShazeer, AdamRoberts, KatherineLee, SharanNarang, MichaelMatena, YanqiZhou, WeiLi,\nand Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nMax Savery, Asma Ben Abacha, Soumya Gayen, and Demner-Fushman. 2020. Question-driven summarization of\nanswers to consumer health questions. https://doi.org/10.17605/OSF.IO/FYG46.\nTensorFlow Datasets, a collection of ready-to-use datasets.https://www.tensorflow.org/datasets.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTimRault,RémiLouf,MorganFuntowicz,andJamieBrew. 2019. Huggingface’stransformers: State-of-the-art\nnatural language processing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet:\nGeneralized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d/quotesingle.ts1Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems 32,\npages 5753–5763. Curran Associates, Inc.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization.\n5646\nAppendix A Implementation Details\nMetrics, conﬁdence intervals, and the PEGASUS implementation were provided byhttps://github.\ncom/google-research/pegasus. All models were trained with a batch size of 8, maximum sequence\nlength of 512 tokens, and 3 warm-up epochs followed by 20 training epochs using single V100X GPUs\n(32 GB VRAM) on a shared cluster. Validation loss was measured every epoch and the snapshot with\nlowest validation loss was used for FSL evaluation. Documents were sorted by similarity to their reference\nsummaries when provided to the model. T5 and BART implementations were provided by HuggingFace’s\nTransformers package (Wolf et al., 2019). Existing datasets were obtained using the TensorFlow DataSets\ncatalogue. ThesourcecodeforthispaperisavailableonGitHubat https://github.com/h4ste/mdas.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9839794635772705
    },
    {
      "name": "Computer science",
      "score": 0.8022480010986328
    },
    {
      "name": "Transformer",
      "score": 0.752700686454773
    },
    {
      "name": "Natural language processing",
      "score": 0.6260495781898499
    },
    {
      "name": "Information retrieval",
      "score": 0.4995098114013672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4945642650127411
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.48529142141342163
    },
    {
      "name": "Engineering",
      "score": 0.07994738221168518
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800548410",
      "name": "United States National Library of Medicine",
      "country": "US"
    }
  ],
  "cited_by": 36
}