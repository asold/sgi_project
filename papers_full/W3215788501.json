{
  "title": "PU-Transformer: Point Cloud Upsampling Transformer",
  "url": "https://openalex.org/W3215788501",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2226394061",
      "name": "Qiu Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164584947",
      "name": "Anwar, Saeed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746654427",
      "name": "Barnes, Nick",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2991216808",
    "https://openalex.org/W3192980497",
    "https://openalex.org/W2069479606",
    "https://openalex.org/W2950493473",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963509914",
    "https://openalex.org/W3203890361",
    "https://openalex.org/W2997337685",
    "https://openalex.org/W2886499109",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W3040726448",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W2586114507",
    "https://openalex.org/W1987985833",
    "https://openalex.org/W3157424380",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W2963680153",
    "https://openalex.org/W2609719703",
    "https://openalex.org/W3203898101",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W3184736166",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3195219304",
    "https://openalex.org/W3203038692",
    "https://openalex.org/W3175676582",
    "https://openalex.org/W2614059183",
    "https://openalex.org/W1992642990",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W2990045899",
    "https://openalex.org/W3112996878",
    "https://openalex.org/W3034429258",
    "https://openalex.org/W2084595284",
    "https://openalex.org/W2788158258",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2963390820",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3118806719",
    "https://openalex.org/W3039448353",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2037402385",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W3171215128",
    "https://openalex.org/W2158455546",
    "https://openalex.org/W2963091558"
  ],
  "abstract": "Given the rapid development of 3D scanners, point clouds are becoming popular in AI-driven machines. However, point cloud data is inherently sparse and irregular, causing significant difficulties for machine perception. In this work, we focus on the point cloud upsampling task that intends to generate dense high-fidelity point clouds from sparse input data. Specifically, to activate the transformer's strong capability in representing features, we develop a new variant of a multi-head self-attention structure to enhance both point-wise and channel-wise relations of the feature map. In addition, we leverage a positional fusion block to comprehensively capture the local context of point cloud data, providing more position-related information about the scattered points. As the first transformer model introduced for point cloud upsampling, we demonstrate the outstanding performance of our approach by comparing with the state-of-the-art CNN-based methods on different benchmarks quantitatively and qualitatively.",
  "full_text": "PU-Transformer: Point Cloud Upsampling\nTransformer\nShi Qiu1,2, Saeed Anwar1,2, and Nick Barnes1\n1 Australian National University\n2 Data61-CSIRO, Australia\n{shi.qiu, saeed.anwar, nick.barnes}@anu.edu.au\nAbstract. Given the rapid development of 3D scanners, point clouds\nare becoming popular in AI-driven machines. However, point cloud data\nis inherently sparse and irregular, causing significant difficulties for ma-\nchine perception. In this work, we focus on the point cloud upsampling\ntask that intends to generate dense high-fidelity point clouds from sparse\ninput data. Specifically, to activate the transformer‚Äôs strong capability\nin representing features, we develop a new variant of a multi-head self-\nattention structure to enhance both point-wise and channel-wise rela-\ntions of the feature map. In addition, we leverage a positional fusion\nblock to comprehensively capture the local context of point cloud data,\nproviding more position-related information about the scattered points.\nAs the first transformer model introduced for point cloud upsampling, we\ndemonstrate the outstanding performance of our approach by comparing\nwith the state-of-the-art CNN-based methods on different benchmarks\nquantitatively and qualitatively.\n1 Introduction\n3D computer vision has been attracting a wide range of interest from academia\nand industry since it shows great potential in many fast-developing AI-related\napplications such as robotics, autonomous driving, augmented reality, etc. As\na basic representation of 3D data, point clouds can be easily captured by 3D\nsensors [1,2], incorporating the rich context of real-world surroundings.\nUnlike well-structured 2D images, point cloud data has inherent properties of\nirregularity and sparsity, posing enormous challenges for high-level vision tasks\nsuch as point cloud classification [3,4,5], segmentation [6,7,8], and object detec-\ntion [9,10,11]. For instance, Uy et al. [12] fail to classify the real-world point\nclouds while they apply a pre-trained model of synthetic data; and recent 3D\nsegmentation and detection networks [8,13,14] achieve worse results on the dis-\ntant/smaller objects ( e.g., bicycles, traffic-signs) than the closer/larger objects\n(e.g., vehicles, buildings). If we mitigate point cloud data‚Äôs irregularity and spar-\nsity, further improvements in visual analysis can be obtained (as verified in [15]).\nThus, point cloud upsampling deserves a deeper investigation.\nAs a basic 3D low-level vision task, point cloud upsampling aims to gener-\nate dense point clouds from sparse input, where the generated data should re-\ncover the fine-grained structures at a higher resolution. Moreover, the upsampled\narXiv:2111.12242v2  [cs.CV]  3 Oct 2022\n2 S. Qiu et al.\nPU-Transformer TailPU-Transformer Head PU-Transformer Body\n‚Ä¶\nN√ó3 ùëüN√ó3\nMLP Transformer\nEncoder\nTransformer\nEncoder MLPShuffle\nÔÅÜùëô‚àí1\nPositional\nFusion Norm\nShifted Channel\nMulti-head\nSelf-Attention\nNorm MLP ÔÅÜùëô\nÔÅê\nOverall architecture of PU-Transformer\nTransformer Encoder\nÔÅáùëô ÔÅáùëô‚Ä≤\n1 ‚Ä¶ ùêø\n(the ùëô-th)\nFig. 1: The details of PU-Transformer. The upper chart shows the overall architecture\nof the PU-Transformer model containing three main parts: the PU-Transformer head\n(Sec. 4.1), body (Sec. 4.2), and tail (Sec. 4.3). The PU-Transformer body includes a\ncascaded set of Transformer Encoders (e.g., L in total), serving as the core component\nof the whole model. Particularly, the detailed structure of each Transformer Encoder is\nshown in the lower chart, where all annotations are consistent with Line 3-5 in Alg. 1.\npoints are expected to lie on the underlying surfaces in a uniform distribution,\nbenefiting downstream tasks for both 3D visual analysis [16,17] and graphic mod-\neling [18,19]. Following the success of Convolution Neural Networks (CNNs) in\nimage super-resolution [20,21,22] and Multi-Layer-Perceptrons (MLPs) in point\ncloud analysis [3,6], previous methods tended to upsample point clouds via com-\nplex network designs ( e.g., Graph Convolutional Network [23], Generative Ad-\nversarial Network [24]) and dedicated upsampling strategies ( e.g., progressive\ntraining [25], coarse-to-fine reconstruction [26], disentangled refinement [27]). As\nfar as we are concerned, these methods share a key to point cloud upsampling:\nlearning the representative features of given points to estimate the distribution\nof new points. Considering that regular MLPs have limited-expression and gen-\neralization capability, we need a more powerful tool to extract fine-grained point\nfeature representations for high-fidelity upsampling. To this end, we introduce\na succinct transformer model, PU-Transformer, to effectively upsample point\nclouds following a simple pipeline as illustrated in Fig. 1. The main reasons for\nadopting transformers to point cloud upsampling are as follows:\nPlausibility in theory. As the core operation of transformers, self-attention [28]\nis a set operator [29] calculating long-range dependencies between elements re-\ngardless of data order. On this front, self-attention can easily estimate the point-\nwise dependencies without any concern for the inherent unorderedness. However,\nto comprehensively represent point cloud features, channel-wise information is\nalso shown to be a crucial factor in attention mechanisms [5,11]. Moreover, such\nchannel-wise information enables an efficient upsampling via a simple periodic\nshuffling [30] operated on the channels of point features, saving complex de-\nsigns [26,24,27,25] for upsampling strategy. Given these facts, we propose a\nShifted Channel Multi-head Self-Attention (SC-MSA) block, which strength-\nens the point-wise relations in a multi-head form and enhances the channel-wise\nconnections by introducing the overlapping channels between consecutive heads.\nPU-Transformer: Point Cloud Upsampling Transformer 3\nFeasibility in practice. Since the transformer model was originally invented\nfor natural language processing; its usage has been widely recognized in high-\nlevel visual applications for 2D images [31,32,33]. More recently, Chen et al. [34]\nintroduced a pre-trained transformer model achieving excellent performance on\nimage super-resolution and denoising. Inspired by the transformer‚Äôs effectiveness\nfor image-related low-level vision tasks, we attempt to create a transformer-based\nmodel for point cloud upsampling. Given the mentioned differences between 2D\nimages and 3D point clouds, we introduce the Positional Fusion block as a re-\nplacement for positional encoding in conventional transformers: on the one hand,\nlocal information is aggregated from both the geometric and feature context of\nthe points, implying their 3D positional relations; on the other hand, such local\ninformation can serve as complementary to subsequent self-attention operations,\nwhere the point-wise dependencies are calculated from a global perspective.\nAdaptability in various applications. Transformer-based models are consid-\nered as a luxury tool in computer vision due to the huge consumption of data,\nhardware, and computational resources. However, our PU-Transformer can be\neasily trained with a single GPU in a few hours, retaining a similar model com-\nplexity to regular CNN-based point cloud upsampling networks [35,25,27]. More-\nover, following a patch-based pipeline [25], the trained PU-Transformer model\ncan effectively and flexibly upsample different types of point cloud data, includ-\ning but not limited to regular object instances or large-scale LiDAR scenes (as\nshown in Fig. 3, 4 and 6). Starting with the upsampling task in low-level vision,\nwe expect our approach to transformers will be affordable in terms of resource\nconsumption for more point cloud applications. Our main contributions are:\n‚Äì To the best of our knowledge, we are the first to introduce a transformer-\nbased model3 for point cloud upsampling.\n‚Äì We quantitatively validate the effectiveness of the PU-Transformer by signif-\nicantly outperforming the results of state-of-the-art point cloud upsampling\nnetworks on two benchmarks using three metrics.\n‚Äì The upsampled visualizations demonstrate the superiority of PU-Transformer\nfor diverse point clouds.\n2 Related Work\nPoint Cloud Networks:In early research, the projection-based methods [36,37]\nused to project 3D point clouds into multi-view 2D images, apply regular 2D\nconvolutions and fuse the extracted information for 3D analysis. Alternatively,\ndiscretization-based approaches [38] tended to convert the point clouds to vox-\nels [39] or lattices [40], and then process them using 3D convolutions or sparse\ntensor convolutions [41]. To avoid context loss and complex steps during data\nconversion, the point-based networks [3,6,4] directly process point cloud data via\nMLP-based operations. Although current mainstream approaches in point cloud\nupsampling prefer utilizing MLP-related modules, in this paper, we focus on an\n3The project page is: https://github.com/ShiQiu0419/PU-Transformer.\n4 S. Qiu et al.\nadvanced transformer structure [28] in order to further enhance the point-wise\ndependencies between known points and benefit the generation of new points.\nPoint Cloud Upsampling: Despite the fact that current point cloud research\nin low-level vision [35,42] is less active than that in high-level analysis [3,8,9],\nthere exists many outstanding works that have contributed significant devel-\nopments to the point cloud upsampling task. To be specific, PU-Net [35] is a\npioneering work that introduced CNNs to point cloud upsampling based on a\nPointNet++ [6] backbone. Later, MPU [25] proposed a patch-based upsampling\npipeline, which can flexibly upsample the point cloud patches with rich local de-\ntails. In addition, PU-GAN [24] adopted the architecture of Generative Adversar-\nial Networks [43] for the generation problem of high-resolution point clouds, while\nPUGeo-Net [44] indicated a promising combination of discrete differential geome-\ntry and deep learning. More recently, Dis-PU [27] applies disentangled refinement\nunits to gradually generate the high-quality point clouds from coarse ones, and\nPU-GCN [23] achieves good upsampling performance by using graph-based net-\nwork constructions [4]. Moreover, there are some papers exploring flexible-scale\npoint cloud upsampling via meta-learning [15], self-supervised learning [45], de-\ncoupling ratio with network architecture [46], or interpolation [47], etc. As the\nfirst work leveraging transformers for point cloud upsampling, we focus on the\neffectiveness of PU-Transformer in performing the fundamental fixed-scale up-\nsampling task, and expect to inspire more future work in relevant topics.\nTransformers in Vision: With the capacity in parallel processing as well as\nthe scalability to deep networks and large datasets [48], more visual transform-\ners have achieved excellent performance on image-related tasks including either\nlow-level [49,34] or high-level analysis [32,33,31,50]. Due to the inherent gaps be-\ntween 3D and 2D data, researchers introduce the variants of transformer for point\ncloud analysis [51,52,53,54,55], using vector-attention [29], offset-attention [56],\nand grid-rasterization [57], etc. However, since these transformers still operate\non an overall classical PointNet [3] or PointNet++ architecture [6], the im-\nprovement is relatively limited while the computational cost is too expensive for\nmost researchers to re-implement. To simplify the model‚Äôs complexity and boost\nits adaptability in point cloud upsampling research, we only utilize the general\nstructure of transformer encoder [32] to form the body of our PU-Transformer.\n3 Methodology\n3.1 Overview\nAs shown in Fig. 1, given a sparse point cloud P ‚ààRN√ó3, our proposed PU-\nTransformer can generate a dense point cloud S ‚ààRrN√ó3, where r denotes\nthe upsampling scale. Firstly, the PU-Transformer head extracts a preliminary\nfeature map from the input. Then, based on the extracted feature map and the\ninherent 3D coordinates, the PU-Transformer body gradually encodes a more\ncomprehensive feature map via the cascaded Transformer Encoders. Finally, in\nthe PU-Transformer tail, we use the shuffle operation [30] to form a dense feature\nmap and reconstruct the 3D coordinates of S via an MLP.\nPU-Transformer: Point Cloud Upsampling Transformer 5\nAlgorithm 1:PU-Transformer Pipeline\ninput: a sparse point cloud P ‚ààRN√ó3\noutput: a dense point cloud S ‚ààRrN√ó3\n# PU-Transformer Head\n1 F0 = MLP(P)\n# PU-Transformer Body\n2 for each Transformer Encoderdo\n# l = 1 ... L\n# the l-th Transformer Encoder\n3 Gl = PosFus(P, Fl‚àí1);\n4 Gl\n‚Ä≤ = SC-MSA\n\u0000\nNorm(Gl)\n\u0001\n+ Gl;\n5 Fl = MLP\n\u0000\nNorm(Gl\n‚Ä≤)\n\u0001\n+ Gl\n‚Ä≤;\n6 end for\n# PU-Transformer Tail\n7 S = MLP\n\u0000\nShuffle(FL)\n\u0001\nIn Alg. 1, we present the basic operations that are employed to build our\nPU-Transformer. As well as the operations (‚ÄúMLP‚Äù [3], ‚ÄúNorm‚Äù [58], ‚ÄúShuf-\nfle‚Äù [30]) that have been widely used in image and point cloud analysis, we\npropose two novel blocks targeting a transformer-based point cloud upsampling\nmodel i.e., the Positional Fusion block (‚Äú PosFus‚Äù in Alg. 1), and the Shifted-\nChannel Multi-head Self-Attention block (‚ÄúSC-MSA‚Äù in Alg. 1). In the rest of\nthis section, we introduce these two blocks in detail. Moreover, for a compact\ndescription, we only consider the case of anarbitrary Transformer Encoder; thus,\nin the following, we discard the subscripts that are annotated in Alg. 1 denoting\na Transformer Encoder‚Äôs specific index in the PU-Transformer body.\n3.2 Positional Fusion\nUsually, a point cloud consisting of N points has two main types of context: the\n3D coordinates P ‚ààRN√ó3 that are explicitly sampled from synthetic meshes or\ncaptured by real-world scanners, showing the original geometric distribution of\nthe points in 3D space; and the feature context,F ‚ààRN√óC, that is implicitly en-\ncoded by convolutional operations in C-dimensional embedding space, yielding\nrich latent clues for visual analysis. Older approaches [35,25,24] to point cloud\nupsampling generate a dense point set by heavily exploiting the encoded features\nF, while recent methods [44,23] attempt to incorporate more geometric infor-\nmation. As the core module of the PU-Transformer, the proposed Transformer\nEncoder leverages a Positional Fusion block to encode and combine both the\ngiven P and F4 of a point cloud, following the local geometric relations between\nthe scattered points.\nBased on the metric of 3D-Euclidean distance, we can search for neighbors\n‚àÄpj ‚àà Ni(pi) for each point pi ‚àà R3 in the given point cloud P, using the\nk-nearest-neighbors (knn) algorithm [4]. Coupled with a grouping operation,\nwe thus obtain a matrix Pj ‚àà RN√ók√ó3, denoting the 3D coordinates of the\nneighbors for all points. Accordingly, the relative positions between each point\n4equivalent to ‚ÄúFl‚àí1‚Äù in Alg. 1\n6 S. Qiu et al.\nand its neighbors can be formulated as:\n‚àÜP = Pj ‚àí P, ‚àÜ P ‚ààRN√ók√ó3; (1)\nwhere k is the number of neighbors. In addition to the neighbors‚Äô relative posi-\ntions showing each point‚Äôs local detail, we also append the centroids‚Äô positions\nin 3D space, indicating the global distribution for all points. By duplicating P\nin a dimension expanded k times, we concatenate the local geometric context:\nGgeo = concat\n\u0002\ndup\nk\n(P); ‚àÜP\n\u0003\n‚àà RN√ók√ó6. (2)\nFurther, for the feature matrix Fj ‚àà RN√ók√óC of all searched neighbors, we\nconduct similar operations (Eq. 1 and 2) as on the counterpart Pj, computing\nthe relative features as:\n‚àÜF = Fj ‚àí F, ‚àÜ F ‚ààRN√ók√óC; (3)\nand representing the local feature context as:\nGfeat = concat\n\u0002\ndup\nk\n(F); ‚àÜF\n\u0003\n‚àà RN√ók√ó2C. (4)\nAfter the local geometric context Ggeo and local feature context Gfeat are con-\nstructed, we then fuse them for a comprehensive point feature representation.\nSpecifically, Ggeo and Gfeat are encoded via two MLPs, MŒ¶ and MŒò, respec-\ntively; further, we comprehensively aggregate the local information,G ‚ààRN√óC‚Ä≤5,\nusing a concatenation between the encoded two types of local context, followed\nby a max-pooling function operating over the neighborhoods. The above opera-\ntions can be summarized as:\nG = max\nk\n\u0010\nconcat\n\u0002\nMŒ¶(Ggeo); MŒò(Gfeat )\n\u0003\u0011\n. (5)\nUnlike the local graphs in DGCNN [4] that need to be updated in every\nencoder based on the dynamic relations in embedding space, both of our Ggeo\nand Gfeat are constructed (i.e., Eq. 2 and 4) and encoded (i.e., MŒ¶ and MŒò in\nEq. 5) in the same way, followingfixed 3D geometric relations (i.e., ‚àÄpj ‚àà Ni(pi)\ndefined upon 3D-Euclidean distance). The main benefits of our approach can be\nconcluded from two aspects: (i) it is practically efficient since the expensive knn\nalgorithm just needs to be conducted once, while the searching results can be\nutilized in all Positional Fusion blocks of the PU-Transformer body; and (ii) the\nlocal geometric and feature context are represented in a similar manner following\nthe same metric, contributing tofairly fusing the two types of context. A detailed\nbehavior analysis of this block is provided in the supplementary material.\nOverall, the Positional Fusion block can not only encode the positional in-\nformation about a set of unordered points for the transformer‚Äôs processing, but\nalso aggregate comprehensive local details for accurate point cloud upsampling.\n5equivalent to ‚ÄúGl‚Äù in Alg. 1\nPU-Transformer: Point Cloud Upsampling Transformer 7\nAlgorithm 2:Shifted Channel\nMulti-head Self-Attention (SC-MSA)\ninput: a point cloud feature map: I ‚ààRN√óC‚Ä≤\noutput: the refined feature map: O ‚ààRN√óC‚Ä≤\nothers: channel-wise split width: w\nchannel-wise shift interval: d, d < w\nthe number of heads: M\n1 Q = Linear(I) # Query Mat Q ‚ààRN√óC‚Ä≤\n2 K = Linear(I) # Key Mat K ‚ààRN√óC‚Ä≤\n3 V = Linear(I) # Value Mat V ‚ààRN√óC‚Ä≤\n4 for m ‚àà {1, 2, ..., M} do\n5 Qm = Q[ : , (m ‚àí 1)d : (m ‚àí 1)d + w];\n6 Km = K[ : , (m ‚àí 1)d : (m ‚àí 1)d + w];\n7 Vm = V[ : , (m ‚àí 1)d : (m ‚àí 1)d + w];\n8 Am = softmax(QmKm\nT );\n9 Om = AmVm;\n10 end for\n11 obtain: {O1, O2, ...,OM}\n12 O = Linear\n\u0010\nconcat\n\u0002\n{O1, O2, ...,OM}\n\u0003\u0011\n‚Ä¶\n‚Ä¶ÔÅë1 ÔÅë2 ÔÅëùê∂‚Ä≤\nùë§\n·âä\nQuery Matrix \nÔÅë ‚àà ‚ÑùùëÅ√óùê∂‚Ä≤\nùë§\n·âä\nùë§\n·âä\nùë§\n·âêùëÅ\n‚Ä¶ÔÅë1 ÔÅë2 ÔÅëùëÄ\n·âä\nQuery Matrix \nÔÅë ‚àà ‚ÑùùëÅ√óùê∂‚Ä≤\nùë§\n·âä\nùë§\n·âä\nùë§\n·âêùëÅ\n·âäùëë\nshifting along\nMSA SC-MSA\n‚Ä¶\nthe channels\nFig. 2: Examples of how regular\nMSA [28] and our SC-MSA generate\nthe low-dimensional splits of query ma-\ntrix Q for multi-head processing (the\nsame procedure applies to K and V).\n3.3 Shifted Channel Multi-head Self-Attention\nDifferent from previous works that applied complex upsampling strategies ( e.g.,\nGAN [24], coarse-to-fine [26], task-disentangling [27]) to estimate new points,\nwe prefer generating dense points in a simple way. Particularly, PixelShuffle [30]\nis a periodic shuffling operation that efficiently reforms the channels of each\npoint feature to represent new points without introducing additional parameters.\nHowever, with regular multi-head self-attention (MSA) [28] serving as the main\ncalculation unit in transformers, only point-wise dependencies are calculated in\neach independent head of MSA, lacking integration of channel-related informa-\ntion for shuffling-based upsampling. To tackle this issue, we introduce a Shifted\nChannel Multi-head Self-Attention (SC-MSA) block for the PU-Transformer.\nAs Alg. 2 states, at first, we apply linear layers (denoted as ‚ÄúLinear‚Äù, and\nimplement as a 1 √ó1 convolution) to encode the query matrix Q, key matrix K,\nand value matrix V. Then, we generate low-dimensional splits of Qm, Km, Vm for\neach head. Particularly, as shown in Fig. 2, regular MSA generates the indepen-\ndent splits for the self-attention calculation in corresponding heads. In contrast,\nour SC-MSA applies a window (dashed square) shift along the channels to ensure\nthat any two consecutive splits have an overlap of (w‚àíd) channels (slashed area),\nwhere w is the channel dimension of each split and d represents the channel-wise\nshift interval each time. After generating the Qm, Km, Vm for each head in the\nmentioned manner, we employ self-attention (Alg. 2 steps 8-9) to estimate the\npoint-wise dependencies as the output Om of each head. Considering the fact\nthat any two consecutive heads have part of the input in common (i.e., the over-\nlap channels), thus the connections between the outputs{O1, O2, ...,OM } (Alg. 2\nstep 11) of multiple heads are established. There are two major benefits of such\nconnections: (i) it is easier to integrate the information between the connected\nmulti-head outputs (Alg. 2 step 12), compared to using the independent multi-\n8 S. Qiu et al.\nhead results of regular MSA; and (ii) as the overlapping context is captured\nfrom the channel dimension, our SC-MSA can further enhance the channel-\nwise relations in the final output O, better fulfilling an efficient and effective\nshuffling-based upsampling strategy than only using regular MSA‚Äôs point-wise\ninformation. These benefits contribute to a faster training convergence and a\nbetter upsampling performance, especially when we deploy fewer Transformer\nEncoders. More practical evidence is provided in the supplementary material.\nIt is worth noting that SC-MSA requires the shift interval to be smaller than\nthe channel-wise width of each split ( i.e., d < w as in Alg. 2) for a shared\narea between any two consecutive splits. Accordingly, the number of heads in\nour SC-MSA is higher than regular MSA ( i.e., M > C‚Ä≤/w in Fig. 2). More\nimplementation detail and the choices of parameters are provided in Sec. 4.2.\n4 Implementation\n4.1 PU-Transformer Head\nAs illustrated in Fig. 1, our PU-Transformer model begins with the head to\nencode a preliminary feature map for the following operations. In practice, we\nonly use a single layer MLP (i.e., a single 1 √ó1 convolution, followed by a batch\nnormalization layer [59] and a ReLU activation [60]) as the PU-Transformer\nhead, where the generated feature map size is N √ó 16.\n4.2 PU-Transformer Body\nTo balance the model complexity and effectiveness, empirically, we leverage five\ncascaded Transformer Encoders (i.e., L = 5 in Alg. 1 and Fig. 1) to form the PU-\nTransformer body, where the channel dimension of each output follows: 32 ‚Üí\n64 ‚Üí 128 ‚Üí 256 ‚Üí 256. Particularly, in each Transformer Encoder, we only\nuse the Positional Fusion block to encode the corresponding channel dimension\n(i.e., C‚Ä≤ in Eq. 5), which remains the same in the subsequent operations. For all\nPositional Fusion blocks, the number of neighbors is empirically set to k = 20\nas used in previous works [4,23].\nIn terms of the SC-MSA block, the primary way of choosing the shift-related\nparameters is inspired by the Non-local Network [61] and ECA-Net [62]. Specif-\nically, a reduction ratio œà [61] is introduced to generate the low-dimensional\nmatrices in self-attention; following a similar method, the channel-wise width\n(i.e., channel dimension) of each split in SC-MSA is set as w = C‚Ä≤/œà. Moreover,\nsince the channel dimension is usually set to a power of 2 [62], we simply set the\nchannel-wise shift interval d = w/2. Therefore, the number of heads in SC-MSA\nbecomes M = 2œà ‚àí 1. In our implementation, œà = 4 is adopted in all SC-MSA\nblocks of PU-Transformer.\n4.3 PU-Transformer Tail\nBased on the practical settings above, the input to the PU-Transformer tail (i.e.,\nthe output of the last Transformer Encoder) has a size of N √ó 256. Then, the\nPU-Transformer: Point Cloud Upsampling Transformer 9\nperiodic shuffling operation [30] reforms the channels and constructs a dense\nfeature map of rN √ó 256/r, where r is the upsampling scale. Finally, another\nMLP is applied to estimate the upsampled point cloud‚Äôs 3D coordinates (rN √ó3).\n5 Experiments\n5.1 Settings\nTraining Details: In general, our PU-Transformer is implemented using Ten-\nsorflow [63] with a single GeForce 2080 Ti GPU running on the Linux OS. In\nterms of the hyperparameters for training, we heavily adopt the settings from\nPU-GCN [23] and Dis-PU [27] for the experiments in Tab. 1 and Tab. 2, respec-\ntively. For example, we have a batch size of 64 for 100 training epochs, an initial\nlearning rate of 1 √ó 10‚àí3 with a 0.7 decay rate, etc. Moreover, we only use the\nmodified Chamfer Distance loss [25] to train the PU-Transformer, minimizing\nthe average closest point distance between the input set P ‚ààRN√ó3 and the\noutput set S ‚ààRrN√ó3 for efficient and effective convergence.\nDatasets: Basically, we apply two 3D benchmarks for our experiments:\n‚Äì PU1K: This is a new point cloud upsampling dataset introduced in PU-\nGCN [23]. In general, the PU1K dataset incorporates 1,020 3D meshes for\ntraining and 127 3D meshes for testing, where most 3D meshes are collected\nfrom ShapeNetCore [64] covering 50 object categories. To fit in with the\npatch-based upsampling pipeline [25], the training data is generated from\npatches of 3D meshes via Poisson disk sampling. Specifically, the training\ndata includes 69,000 samples, where each sample has 256 input points (low\nresolution) and a ground-truth of 1,024 points (4 √ó high resolution).\n‚Äì PU-GAN Dataset: This is an earlier dataset that was first used in PU-\nGAN [24] and generated in a similar way as PU1K but on a smaller scale. To\nbe concrete, the training data comprises 24,000 samples (patches) collected\nfrom 120 3D meshes, while the testing data only contains 27 meshes. In ad-\ndition to the PU1K dataset consisting of a large volume of data targeting the\nbasic 4√ó upsampling experiment, we conduct both 4 √ó and 16√ó upsampling\nexperiments based on the compact data of the PU-GAN dataset.\nEvaluation Metrics: As for the testing process, we follow common practice\nthat has been utilized in previous point cloud upsampling works [25,24,27,23].\nTo be specific, at first, we cut the input point cloud into multiple seed patches\ncovering all the N points. Then, we apply the trained PU-Transformer model to\nupsample the seed patches with a scale of r. Finally, the farthest point sampling\nalgorithm [3] is used to combine all the upsampled patches as a dense output\npoint cloud with rN points. For the 4√ó upsampling experiments in this paper,\neach testing sample has a low-resolution point cloud with 2,048 points, as well\nas a high-resolution one with 8,196 points. Coupled with the original 3D meshes,\nwe quantitatively evaluate the upsampling performance of our PU-Transformer\nbased on three widely used metrics: (i) Chamfer Distance (CD), (ii) Hausdorff\nDistance [65] (HD), and (iii) Point-to-Surface Distance (P2F). A lower value\nunder these metrics denotes better upsampling performance.\n10 S. Qiu et al.\nTable 1: Quantitative comparisons (4√ó Upsampling) to state-of-the-art methods on the\nPU1K dataset [23]. (‚Äú CD‚Äù: Chamfer Distance; ‚Äú HD‚Äù: Hausdorff Distance; ‚Äú P2F‚Äù:\nPoint-to-Surface Distance. ‚ÄúModel‚Äù: model size; ‚Äú Time‚Äù: average inference time per\nsample; ‚ÄúParam.‚Äù: number of parameters.‚àó: self-reproduced results, ‚Äì: unknown data.)\nMethods Model Time Param.Results (√ó10‚àí3)\n(MB) (√ó10‚àí3s) (√ó103) CD‚Üì HD‚ÜìP2F‚Üì\nPU-Net [35] 10.1 8.4 812.0 1.155 15.170 4.834\nMPU [25] 6.2 8.3 76.2 0.935 13.327 3.551\nPU-GACNet [66]‚Äì ‚Äì 50.7 0.665 9.053 2.429\nPU-GCN [23] 1.8 8.0 76.0 0.585 7.577 2.499\nDis-PU‚àó [27] 13.2 10.8 1047.0 0.485 6.145 1.802\nOurs 18.4 9.9 969.9 0.4513.8431.277\nTable 2: Quantitative comparisons to state-of-the-art methods on the PU-GAN\ndataset [24]. (All metric units are 10 ‚àí3. The best results are denoted in bold.)\nMethods 4√ó Upsampling 16√ó Upsampling\nCD‚Üì HD‚Üì P2F‚Üì CD‚Üì HD‚Üì P2F‚Üì\nPU-Net [35] 0.844 7.061 9.431 0.699 8.594 11.619\nMPU [25] 0.632 6.998 6.199 0.348 7.187 6.822\nPU-GAN [24]0.483 5.323 5.053 0.269 7.127 6.306\nPU-GCN‚àó [23] 0.357 5.229 3.628 0.256 5.938 3.945\nDis-PU [27] 0.315 4.201 4.149 0.1994.716 4.249\nOurs 0.2732.6051.836 0.2412.3101.687\n5.2 Point Cloud Upsampling Results\nPU1K: Table 1 shows the quantitative results of our PU-Transformer on the\nPU1K dataset. It can be seen that our approach outperforms other state-of-the-\nart methods on all three metrics. In terms of the Chamfer Distance metric, we\nachieve the best performance among all the tested networks, since the reported\nvalues of others are all higher than ours of 0.451. Under the other two metrics,\nthe improvements of PU-Transformer are particularly significant: compared to\nthe performance of the recent PU-GCN [23], our approach can almost halve the\nvalues assessed under both the Hausdorff Distance (HD: 7.577 ‚Üí3.843) and the\nPoint-to-Surface Distance (P2F: 2.499‚Üí1.277).\nPU-GAN Dataset: We also conduct point cloud upsampling experiments using\nthe dataset introduced in PU-GAN [24]. under more upsampling scales. As shown\nin Table 2, we achieve best performance under all three evaluation metrics for the\n4√ó upsampling experiment. However, in the 16√ó upsampling test, we (CD: 0.241)\nare slightly behind the latest Dis-PU network [27] (CD: 0.199) evaluated under\nthe Chamfer Distance metric: the Dis-PU applies two CD-related items as its\nloss function, hence getting an edge for CD metric only. As for the results under\nHausdorff Distance and Point-to-Surface Distance metrics, our PU-Transformer\nshows significant improvements again, where some values ( e.g., P2F in 4 √ó, HD\nand P2F in 16 √ó) are even lower than half of Dis-PU‚Äôs results.\nPU-Transformer: Point Cloud Upsampling Transformer 11\nTable 3: Ablation study of the PU-Transformer‚Äôs components tested on the PU1K\ndataset [23]. Specifically, models A1-A3 investigate the effects of the Positional Fusion\nblock, models B1-B3 compare the results of different self-attention approaches, and\nmodels C1-C3 test the upsampling methods in the tail.\nmodels PU-Transformer Body PU-Transformer TailResults (√ó10‚àí3)\nPositional FusionAttention Type CD‚ÜìHD‚ÜìP2F‚Üì\nA1 None SC-MSA Shuffle 0.605 6.477 2.038\nA2 Ggeo SC-MSA Shuffle 0.558 5.713 1.751\nA3 Gfeat SC-MSA Shuffle 0.497 4.164 1.511\nB1 Ggeo&Gfeat SA [61] Shuffle 0.526 4.689 1.492\nB2 Ggeo&Gfeat OSA [56] Shuffle 0.509 4.823 1.586\nB3 Ggeo&Gfeat MSA [28] Shuffle 0.498 4.218 1.427\nC1 Ggeo&Gfeat SC-MSA MLPs [35] 1.070 8.732 2.467\nC2 Ggeo&Gfeat SC-MSA DupGrid [25] 0.485 3.966 1.380\nC3 Ggeo&Gfeat SC-MSA NodeShuffle [23]0.505 4.157 1.404\nFull Ggeo&Gfeat SC-MSA Shuffle 0.4513.8431.277\nOverall Comparison: The experimental results in Table 1 and 2 indicate the\ngreat effectiveness of our PU-Transformer. Moreover, given quantitative compar-\nisons to CNN-based (e.g., GCN [67], GAN [43]) methods under different metrics,\nwe demonstrate the superiority of transformers for point cloud upsampling by\nonly exploiting the fine-grained feature representations of point cloud data.\n5.3 Ablation Studies\nEffects of Components: Table 3 shows the experiments that replace PU-\nTransformer‚Äôs major components with different options. Specifically, we test\nthree simplified models ( A1-A3) regarding the Positional Encoding block out-\nput (Eq. 5), where employing both local geometric Ggeo and feature Gfeat con-\ntext (model ‚ÄúFull‚Äù) provides better performance compared to the others. As for\nmodels B1-B3, we apply different self-attention approaches to the Transformer\nEncoder, where our proposed SC-MSA (Sec. 3.3) block shows higher effective-\nness on point cloud upsampling. In terms of the upsampling method used in the\nPU-Transformer tail, some learning-based methods are evaluated as in models\nC1-C3. Particularly, with the help of our SC-MSA design, the simple yet efficient\nperiodic shuffling operation ( i.e., PixelShuffle [30]) indicates good effectiveness\nin obtaining a high-resolution feature map.\nRobustness to Noise: As the PU-Transformer can upsample different types\nof point clouds, including real scanned data, it is necessary to verify our model‚Äôs\nrobustness to noise. Concretely, we test the pre-trained models by adding some\nrandom noise to the sparse input data, where the noise is generated from a stan-\ndard normal distribution N(0, 1) and multiplied with a factor Œ≤. In practice, we\nconduct the experiments under three noise levels: Œ≤ = 0.5%, 1% and 2%. Table 4\nquantitatively compares the testing results of state-of-the-art methods. In most\ntested noise cases, our proposed PU-Transformer achieves the best performance,\nwhile Dis-PU [27] shows robustness under the CD metric as explained in Sec. 5.2.\n12 S. Qiu et al.\nTable 4: The model‚Äôs robustness to random noise tested on the PU1K dataset [23],\nwhere the noise follows a normal distribution of N(0, 1) and Œ≤ is the noise level.\nMethods Œ≤ = 0.5% Œ≤ = 1% Œ≤ = 2%\nCD‚Üì HD‚Üì P2F‚Üì CD‚Üì HD‚Üì P2F‚Üì CD‚Üì HD‚Üì P2F‚Üì\nPU-Net [35]1.006 14.640 5.2531.017 14.998 6.8511.333 19.964 10.378\nMPU [25] 0.869 12.524 4.0690.907 13.019 5.6251.130 16.252 9.291\nPU-GCN [23]0.621 8.011 3.524 0.762 9.553 5.585 1.107 13.130 9.378\nDis-PU [27] 0.496 6.268 2.604 0.5917.944 4.417 0.85810.960 7.759\nOurs 0.4534.0522.127 0.610 5.7873.965 1.058 9.9487.551\nTable 5: Model Complexity of PU-Transformer using different numbers of Transformer\nEncoders. (Tested on the PU1K dataset [23] with a single GeForce 2080 Ti GPU.)\n# Transformer# ParametersModel SizeTraining SpeedInference SpeedResults (√ó10‚àí3)\nEncoders (per batch)(per sample)CD‚ÜìHD‚ÜìP2F‚Üì\nL= 3 438.3k 8.5M 12.2s 6.9ms 0.487 4.081 1.362\nL= 4 547.3k 11.5M 15.9s 8.2ms 0.472 4.010 1.284\nL= 5 969.9k 18.4M 23.5s 9.9ms 0.4513.8431.277\nL= 6 2634.4k 39.8M 40.3s 11.0ms 0.4343.9961.210\nModel Complexity: Generally, our PU-Transformer is a light ( <1M parame-\nters) transformer model compared to image transformers [48,33,32] that usually\nhave more than 50M parameters. In particular, we investigate the complexity\nof our PU-Transformer by utilizing different numbers of the Transformer En-\ncoders. As shown in Table 5, with more Transformer Encoders being applied,\nthe model complexity increases rapidly, while the quantitative performance im-\nproves slowly. For a better balance between effectiveness and efficiency, we adopt\nthe model with five Transformer Encoders (L = 5) in this work. Overall speak-\ning, the PU-Transformer is a powerful and affordable transformer model for the\npoint cloud upsampling task.\n5.4 Visualization\nQualitative Comparisons: The qualitative results of different point cloud up-\nsampling models are presented in Fig. 3 and 4. Since we utilize the self-attention\nbased structure to capture the point-wise dependencies from a global perspective,\nthe PU-Transformer‚Äôs output can better illustrate the overall contours of input\npoint clouds producing fewer outliers (as shown in the zoom-in views of Fig. 3).\nParticularly, based on the rich local context encoded by our Positional Fusion\nblock, the PU-Transformer precisely upsamples the real point clouds (compared\nin Fig. 4), retaining a uniform distribution and much structural detail.\nUpsampling Different Input Sizes: Fig. 5 shows the results of upsampling\ndifferent sizes of point cloud data using PU-Transformer. Given a relatively low-\nresolution point cloud (e.g., 256 or 512 input points), our proposed model is still\nable to generate dense output with high-fidelity context ( e.g., the head/foot of\nPU-Transformer: Point Cloud Upsampling Transformer 13\n(a) Input                   (b) PU-GAN               (c) PU-GCN                   (d) Dis-PU            (e) PU-Transformer   (f) Ground-Truth\nFig. 3: Comparisons to state-of-the-art methods (PU-GAN [24], PU-GCN [23], Dis-\nPU [27]) in (4 √ó) upsampling synthetic point cloud data using 2048 input points.\n(a) Input                           (b) PU-GAN                        (c) PU-GCN                         (d) Dis-PU             (e) PU-Transformer\nFig. 4: Comparisons to state-of-the-art methods (PU-GAN [24], PU-GCN [23], Dis-\nPU [27]) in (4 √ó) upsampling real point cloud data from ScanObjectNN [12] dataset\nand SemanticKITTI [68] dataset.\n‚ÄúPanda‚Äù). As the input size increases, the new points are uniformly distributed,\ncovering the main flat areas ( e.g., the body of ‚ÄúPanda‚Äù).\nUpsampling Real Point Clouds: In addition to Fig. 4, we provide more\nupsampling results (4 √ó and 16 √ó) on real point cloud samples ( i.e., ‚Äúchair‚Äù,\n‚Äúoffice‚Äù, ‚Äúroom‚Äù, ‚Äústreet‚Äù) from ScanObjectNN [12], S3DIS [69], ScanNet [70],\nand SemanticKITTI [68], respectively. As Fig. 6 clearly illustrates, by addressing\nthe sparsity and non-uniformity of raw inputs, not only is the overall quality of\npoint clouds significantly improved, but also the representative features of object\ninstances are enhanced. Particularly, the contours of upsampled object instances\n(e.g., tables in ‚Äúoffice/room‚Äù, cars in ‚Äústreet‚Äù) are clearly distinct from the\ncomplex surroundings, obtaining high-fidelity details for visual analysis. More\nexamples for visualization are included in the supplementary material.\n14 S. Qiu et al.\nInputsPU-Transformer\n256 points 512 points 1024 points 2048 points\nFig. 5:PU-Transformer‚Äôs 4√ó upsampling\nresults, given different sizes of input\npoint cloud data.\n(a) Real-scanned Point Clouds\n(b) 4√ó Upsampling Results\n(c) 16√óUpsampling Results\nFig. 6: PU-Transformer‚Äôs 4 √ó and 16 √ó\nupsampling results, given different real\npoint clouds.\n6 Limitations and Future Work\nUpsampling Efficiency: Compared to the recent works such as Point Trans-\nformer [29] ( ‚àº7.76M parameters) or PoinTr [55] ( ‚àº22.7M), PU-Transformer\n(‚àº0.97M) is an efficient transformer for point clouds. However, it still consumes\nmore parameters than some CNN-based counterparts [35,9,23,27] shown in Ta-\nble 1. As for inference speed, our approach is very close to others due to the\nsuccinct pipeline design, while methods that exploit complex network [24], up-\nsampling strategy [27] or geometric calculations [44] will be a bit slower.\nUpsampling Flexibility: To generate different resolutions of output, our PU-\nTransformer may require some post-processing such as multiple inference iter-\nations and farthest point sampling [3]. For flexible point cloud upsampling, in\nfuture work, we will improve the adaptability of the PU-Transformer‚Äôs body.\nFuture Work: As a light-weight transformer targeting point clouds, our PU-\nTransformer has great potential in practice. For example, we could design a\nmulti-functional tail to solve different low-level vision problems such as upsam-\npling, completion, and denoising. Moreover, we could further optimize the effi-\nciency of the PU-Transformer in learning fine-grained point feature representa-\ntions, benefiting the high-level visual analysis of large-scale point clouds.\n7 Conclusions\nThis paper focuses on low-level vision for point cloud data in order to tackle its\ninherent sparsity and irregularity. Specifically, we propose a novel transformer-\nbased model, PU-Transformer, targeting the fundamental point cloud upsam-\npling task. Our PU-Transformer shows significant quantitative and qualitative\nimprovements on different point cloud datasets compared to state-of-the-art\nCNN-based methods. By conducting related ablation studies and visualizations,\nwe also analyze the effects and robustness of our approach. In the future, we\nexpect to further optimize its efficiency for real-time applications and extend its\nadaptability in high-level 3D visual tasks.\nPU-Transformer: Point Cloud Upsampling Transformer 15\nReferences\n1. Endres, F., Hess, J., Sturm, J., Cremers, D., Burgard, W.: 3-d mapping with an\nrgb-d camera. IEEE transactions on robotics 30 (2013) 177‚Äì187\n2. Jaboyedoff, M., Oppikofer, T., Abell¬¥ an, A., Derron, M.H., Loye, A., Metzger, R.,\nPedrazzini, A.: Use of lidar in landslide investigations: a review. Natural hazards\n61 (2012) 5‚Äì28\n3. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for\n3d classification and segmentation. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. (2017) 652‚Äì660\n4. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic\ngraph cnn for learning on point clouds. ACM Transactions on Graphics (TOG) 38\n(2019) 146\n5. Qiu, S., Anwar, S., Barnes, N.: Geometric back-projection network for point cloud\nclassification. IEEE Transactions on Multimedia (2021)\n6. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-\ning on point sets in a metric space. In: Advances in neural information processing\nsystems. (2017) 5099‚Äì5108\n7. Qiu, S., Anwar, S., Barnes, N.: Dense-resolution network for point cloud classifica-\ntion and segmentation. In: Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision (WACV). (2021) 3813‚Äì3822\n8. Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A.:\nRandla-net: Efficient semantic segmentation of large-scale point clouds. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. (2020) 11108‚Äì11117\n9. Qi, C.R., Litany, O., He, K., Guibas, L.J.: Deep hough voting for 3d object detec-\ntion in point clouds. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. (2019) 9277‚Äì9286\n10. Qi, C.R., Chen, X., Litany, O., Guibas, L.J.: Imvotenet: Boosting 3d object detec-\ntion in point clouds with image votes. In: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition. (2020) 4404‚Äì4413\n11. Qiu, S., Wu, Y., Anwar, S., Li, C.: Investigating attention mechanism in 3d point\ncloud object detection. In: International Conference on 3D Vision (3DV), IEEE\n(2021)\n12. Uy, M.A., Pham, Q.H., Hua, B.S., Nguyen, T., Yeung, S.K.: Revisiting point cloud\nclassification: A new benchmark dataset and classification model on real-world\ndata. In: Proceedings of the IEEE/CVF International Conference on Computer\nVision. (2019) 1588‚Äì1597\n13. Qiu, S., Anwar, S., Barnes, N.: Semantic segmentation for real point cloud scenes\nvia bilateral augmentation and adaptive fusion. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR). (2021) 1757‚Äì\n1767\n14. Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is pseudo-lidar needed for\nmonocular 3d object detection? In: Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV). (2021) 3142‚Äì3152\n15. Ye, S., Chen, D., Han, S., Wan, Z., Liao, J.: Meta-pu: An arbitrary-scale upsam-\npling network for point cloud. IEEE Transactions on Visualization and Computer\nGraphics (2021)\n16. Liu, Y., Fan, B., Xiang, S., Pan, C.: Relation-shape convolutional neural network\nfor point cloud analysis. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. (2019) 8895‚Äì8904\n16 S. Qiu et al.\n17. Qiu, S., Anwar, S., Barnes, N.: Pnp-3d: A plug-and-play for 3d point clouds. arXiv\npreprint arXiv:2108.07378 (2021)\n18. Mitra, N.J., Nguyen, A.: Estimating surface normals in noisy point cloud data.\nIn: Proceedings of the nineteenth annual symposium on Computational geometry,\nACM (2003) 322‚Äì328\n19. Mitra, N.J., Gelfand, N., Pottmann, H., Guibas, L.: Registration of point cloud\ndata from a geometric optimization perspective. In: Proceedings of the 2004 Eu-\nrographics/ACM SIGGRAPH symposium on Geometry processing, ACM (2004)\n22‚Äì31\n20. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo-\nlutional networks. IEEE transactions on pattern analysis and machine intelligence\n38 (2015) 295‚Äì307\n21. Kim, J., Lee, J.K., Lee, K.M.: Accurate image super-resolution using very deep\nconvolutional networks. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. (2016) 1646‚Äì1654\n22. Anwar, S., Khan, S., Barnes, N.: A deep journey into super-resolution: A survey.\nACM Computing Surveys (CSUR) 53 (2020) 1‚Äì34\n23. Qian, G., Abualshour, A., Li, G., Thabet, A., Ghanem, B.: Pu-gcn: Point cloud\nupsampling using graph convolutional networks. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. (2021) 11683‚Äì11692\n24. Li, R., Li, X., Fu, C.W., Cohen-Or, D., Heng, P.A.: Pu-gan: a point cloud upsam-\npling adversarial network. In: Proceedings of the IEEE International Conference\non Computer Vision. (2019) 7203‚Äì7212\n25. Yifan, W., Wu, S., Huang, H., Cohen-Or, D., Sorkine-Hornung, O.: Patch-based\nprogressive 3d point set upsampling. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. (2019) 5958‚Äì5967\n26. Liu, X., Liu, X., Han, Z., Liu, Y.S.: Spu-net: Self-supervised point cloud upsampling\nby coarse-to-fine reconstruction with self-projection optimization. arXiv preprint\narXiv:2012.04439 (2020)\n27. Li, R., Li, X., Heng, P.A., Fu, C.W.: Point cloud upsampling via disentangled\nrefinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. (2021) 344‚Äì353\n28. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. (2017) 5998‚Äì6008\n29. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision. (2021)\n16259‚Äì16268\n30. Shi, W., Caballero, J., Husz¬¥ ar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert,\nD., Wang, Z.: Real-time single image and video super-resolution using an efficient\nsub-pixel convolutional neural network. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. (2016) 1874‚Äì1883\n31. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision, Springer (2020) 213‚Äì229\n32. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\nPU-Transformer: Point Cloud Upsampling Transformer 17\n33. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV). (2021)\n10012‚Äì10022\n34. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., Gao,\nW.: Pre-trained image processing transformer. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. (2021) 12299‚Äì12310\n35. Yu, L., Li, X., Fu, C.W., Cohen-Or, D., Heng, P.A.: Pu-net: Point cloud upsampling\nnetwork. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. (2018) 2790‚Äì2799\n36. Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view convolutional\nneural networks for 3d shape recognition. In: Proceedings of the IEEE international\nconference on computer vision. (2015) 945‚Äì953\n37. Lawin, F.J., Danelljan, M., Tosteberg, P., Bhat, G., Khan, F.S., Felsberg, M.: Deep\nprojective 3d semantic segmentation. In: International Conference on Computer\nAnalysis of Images and Patterns, Springer (2017) 95‚Äì107\n38. Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.: Deep learning for\n3d point clouds: A survey. IEEE transactions on pattern analysis and machine\nintelligence (2020)\n39. Huang, J., You, S.: Point cloud labeling using 3d convolutional neural network. In:\n2016 23rd International Conference on Pattern Recognition (ICPR), IEEE (2016)\n2670‚Äì2675\n40. Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M.H., Kautz, J.:\nSplatnet: Sparse lattice networks for point cloud processing. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition. (2018) 2530‚Äì2539\n41. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski convolu-\ntional neural networks. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. (2019) 3075‚Äì3084\n42. Yuan, W., Khot, T., Held, D., Mertz, C., Hebert, M.: Pcn: Point completion\nnetwork. In: 2018 International Conference on 3D Vision (3DV), IEEE (2018)\n728‚Äì737\n43. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural\ninformation processing systems 27 (2014)\n44. Qian, Y., Hou, J., Kwong, S., He, Y.: Pugeo-net: A geometry-centric network for 3d\npoint cloud upsampling. In: European Conference on Computer Vision, Springer\n(2020) 752‚Äì769\n45. Zhao, Y., Hui, L., Xie, J.: Sspu-net: Self-supervised point cloud upsampling via\ndifferentiable rendering. In: Proceedings of the 29th ACM International Conference\non Multimedia. (2021) 2214‚Äì2223\n46. Luo, L., Tang, L., Zhou, W., Wang, S., Yang, Z.X.: Pu-eva: An edge-vector based\napproximation solution for flexible-scale point cloud upsampling. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. (2021) 16208‚Äì\n16217\n47. Qian, Y., Hou, J., Kwong, S., He, Y.: Deep magnification-flexible upsampling over\n3d point clouds. IEEE Transactions on Image Processing 30 (2021) 8354‚Äì8367\n48. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin vision: A survey. arXiv preprint arXiv:2101.01169 (2021)\n49. Yang, F., Yang, H., Fu, J., Lu, H., Guo, B.: Learning texture transformer net-\nwork for image super-resolution. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. (2020) 5791‚Äì5800\n18 S. Qiu et al.\n50. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159\n(2020)\n51. Yew, Z.J., Lee, G.H.: Regtr: End-to-end point cloud correspondences with trans-\nformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. (2022) 6677‚Äì6686\n52. Fan, H., Yang, Y., Kankanhalli, M.: Point 4d transformer networks for spatio-\ntemporal modeling in point cloud videos. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition. (2021) 14204‚Äì14213\n53. Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., Lu, J.: Point-bert: Pre-training\n3d point cloud transformers with masked point modeling. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2022)\n19313‚Äì19322\n54. Fan, H., Yang, Y., Kankanhalli, M.: Point spatio-temporal transformer networks for\npoint cloud video modeling. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (2022)\n55. Yu, X., Rao, Y., Wang, Z., Liu, Z., Lu, J., Zhou, J.: Pointr: Diverse point cloud\ncompletion with geometry-aware transformers. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. (2021) 12498‚Äì12507\n56. Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.: Pct: Point\ncloud transformer. Computational Visual Media 7 (2021) 187‚Äì199\n57. Mazur, K., Lempitsky, V.: Cloud transformers: A universal approach to point cloud\nprocessing tasks. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. (2021) 10715‚Äì10724\n58. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n59. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n60. Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann ma-\nchines. In: Icml. (2010)\n61. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n(2018) 7794‚Äì7803\n62. Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., Hu, Q.: Eca-net: Efficient channel\nattention for deep convolutional neural networks. In: IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). (2020)\n63. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe-\nmawat, S., Irving, G., Isard, M., et al.: Tensorflow: A system for large-scale ma-\nchine learning. In: 12th {USENIX} symposium on operating systems design and\nimplementation ({OSDI} 16). (2016) 265‚Äì283\n64. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\nSavarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich\n3d model repository. arXiv preprint arXiv:1512.03012 (2015)\n65. Berger, M., Levine, J.A., Nonato, L.G., Taubin, G., Silva, C.T.: A benchmark for\nsurface reconstruction. ACM Transactions on Graphics (TOG) 32 (2013) 1‚Äì17\n66. Han, B., Zhang, X., Ren, S.: Pu-gacnet: Graph attention convolution network for\npoint cloud upsampling. Image and Vision Computing (2022) 104371\n67. Li, G., Muller, M., Thabet, A., Ghanem, B.: Deepgcns: Can gcns go as deep as\ncnns? In: Proceedings of the IEEE/CVF International Conference on Computer\nVision. (2019) 9267‚Äì9276\nPU-Transformer: Point Cloud Upsampling Transformer 19\n68. Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall,\nJ.: Semantickitti: A dataset for semantic scene understanding of lidar sequences.\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2019)\n9297‚Äì9307\n69. Armeni, I., Sax, S., Zamir, A.R., Savarese, S.: Joint 2d-3d-semantic data for indoor\nscene understanding. arXiv preprint arXiv:1702.01105 (2017)\n70. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. (2017) 5828‚Äì5839",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.8634545803070068
    },
    {
      "name": "Upsampling",
      "score": 0.7367777228355408
    },
    {
      "name": "Computer science",
      "score": 0.6948140859603882
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5485464334487915
    },
    {
      "name": "Transformer",
      "score": 0.5457659363746643
    },
    {
      "name": "Cloud computing",
      "score": 0.476790189743042
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4534280002117157
    },
    {
      "name": "Fidelity",
      "score": 0.44352155923843384
    },
    {
      "name": "Computer vision",
      "score": 0.34831082820892334
    },
    {
      "name": "Engineering",
      "score": 0.156415194272995
    },
    {
      "name": "Electrical engineering",
      "score": 0.10228285193443298
    },
    {
      "name": "Telecommunications",
      "score": 0.0942075252532959
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}