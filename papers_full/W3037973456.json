{
  "title": "PowerNorm: Rethinking Batch Normalization in Transformers",
  "url": "https://openalex.org/W3037973456",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2120683661",
      "name": "Shen Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226671959",
      "name": "Yao, Zhewei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751132899",
      "name": "Gholami, Amir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222709623",
      "name": "Mahoney, Michael W.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743083855",
      "name": "Keutzer, Kurt",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970213198",
    "https://openalex.org/W2982012700",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2970233825",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2144935315",
    "https://openalex.org/W2796438033",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2995333370",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2957004239",
    "https://openalex.org/W2963836885",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996657533",
    "https://openalex.org/W2795783309",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2963399222",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2963300719",
    "https://openalex.org/W2546302380",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2401231614",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2996731312",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3011650341",
    "https://openalex.org/W2970903692",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2948798935"
  ],
  "abstract": "The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \\url{https://github.com/sIncerass/powernorm}.",
  "full_text": "PowerNorm: Rethinking Batch Normalization in Transformers\nSheng Shen * 1 Zhewei Yao* 1 Amir Gholami 1 Michael W. Mahoney1 Kurt Keutzer 1\nAbstract\nThe standard normalization method for neural\nnetwork (NN) models used in Natural Language\nProcessing (NLP) is layer normalization (LN).\nThis is different than batch normalization (BN),\nwhich is widely-adopted in Computer Vision. The\npreferred use of LN in NLP is principally due to\nthe empirical observation that a (naive/vanilla)\nuse of BN leads to signiﬁcant performance degra-\ndation for NLP tasks; however, a thorough un-\nderstanding of the underlying reasons for this is\nnot always evident. In this paper, we perform\na systematic study of NLP transformer models\nto understand why BN has a poor performance,\nas compared to LN. We ﬁnd that the statistics\nof NLP data across the batch dimension exhibit\nlarge ﬂuctuations throughout training. This re-\nsults in instability, if BN is naively implemented.\nTo address this, we propose Power Normaliza-\ntion (PN), a novel normalization scheme that re-\nsolves this issue by (i) relaxing zero-mean nor-\nmalization in BN, (ii) incorporating a running\nquadratic mean instead of per batch statistics to\nstabilize ﬂuctuations, and (iii) using an approxi-\nmate backpropagation for incorporating the run-\nning statistics in the forward pass. We show\ntheoretically, under mild assumptions, that PN\nleads to a smaller Lipschitz constant for the loss,\ncompared with BN. Furthermore, we prove that\nthe approximate backpropagation scheme leads\nto bounded gradients. We extensively test PN for\ntransformers on a range of NLP tasks, and we\nshow that it signiﬁcantly outperforms both LN\nand BN. In particular, PN outperforms LN by\n0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0\nPPL on PTB/WikiText-103. We make our code\npublicly available at https://github.com/\nsIncerass/powernorm.\n*Equal contribution 1UC Berkeley. Correspondence to: Amir\nGholami <amirgh@berkeley.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\n1. Introduction\nNormalization has become one of the critical components\nin Neural Network (NN) architectures for various machine\nlearning tasks, in particular in Computer Vision (CV) and\nNatural Language Processing (NLP). However, currently\nthere are very different forms of normalization used in CV\nand NLP. For example, Batch Normalization (BN) (Ioffe\n& Szegedy, 2015) is widely adopted in CV , but it leads to\nsigniﬁcant performance degradation when naively used in\nNLP. Instead, Layer Normalization (LN) (Ba et al., 2016) is\nthe standard normalization scheme used in NLP. All recent\nNLP architectures, including Transformers (Vaswani et al.,\n2017), have incorporated LN instead of BN as their default\nnormalization scheme. In spite of this, the reasons why BN\nfails for NLP have not been clariﬁed, and a better alternative\nto LN has not been presented.\nIn this work, we perform a systematic study of the chal-\nlenges associated with BN for NLP, and based on this we\npropose Power Normalization (PN), a novel normalization\nmethod that signiﬁcantly outperforms LN. In particular, our\ncontributions are as follows:\n• We ﬁnd that there are clear differences in the batch\nstatistics of NLP data versus CV data. In particular, we\nobserve that batch statistics for NLP data have a very\nlarge variance throughout training. This variance exists\nin the corresponding gradients as well. In contrast,\nCV data exhibits orders of magnitude smaller variance.\nSee Figure 2 and 3 for a comparison of BN in CV and\nNLP.\n• To reduce the variation of batch statistics, we mod-\nify typical BN by relaxing zero-mean normalization,\nand we replace the variance with the quadratic mean.\nWe denote this scheme as PN-V . We show theoreti-\ncally that PN-V preserves the ﬁrst-order smoothness\nproperty as in BN; see Lemma 2.\n• We show that using running statistics for the quadratic\nmean results in signiﬁcantly better performance, up to\n1.5/2.0 BLEU on IWSLT14/WMT14 and 7.7/3.4 PPL\non PTB/WikiText-103, as compared to BN; see Table 1\nand 2. We denote this scheme as PN. Using running\nstatistics requires correcting the typical backpropaga-\ntion scheme in BN. As an alternative, we propose an\narXiv:2003.07845v2  [cs.CL]  28 Jun 2020\nPowerNorm: Rethinking Batch Normalization in Transformers\napproximate backpropagation to capture the running\nstatistics. We show theoretically that this approximate\nbackpropagation leads to bounded gradients, which is\na necessary condition for convergence; see Theorem 4.\n• We perform extensive tests showing that PN also\nimproves performance on machine translation and\nlanguage modeling tasks, as compared to LN. In\nparticular, PN outperforms LN by 0.4/0.6 BLEU\non IWSLT14/WMT14, and by 5.6/3.0 PPL on\nPTB/WikiText-103. We emphasize that the improve-\nment of PN over LN is without any change of hyper-\nparameters.\n• We analyze the behaviour of PN and LN by comput-\ning the Singular Value Decomposition of the resulting\nembedding layers, and we show that PN leads to a\nmore well-conditioned embedding layer; see Figure 6.\nFurthermore, we show that PN is robust to small-batch\nstatistics, and it still achieves higher performance, as\nopposed to LN; see Figure 5.\nFeature\nBatch Dimension\nSentence Length\nLayer Normalization\n1\nFeature\nBatch Dimension\nSentence Length\nBatch/Power Normalization\n1\nFigure 1.The illustration of layer normalization (left) and\nbatch/power normalization (right). The entries colored in blue\nshow the components used for calculating the statistics.\n2. Related Work\nNormalization is widely used in modern deep NNs such\nas ResNet (He et al., 2016), MobileNet-V2 (Sandler et al.,\n2018), and DenseNet (Huang et al., 2017) in CV , as well\nas LSTMs (Hochreiter & Schmidhuber, 1997; Ba et al.,\n2016), transformers (Vaswani et al., 2017), and transformer-\nbased models (Devlin et al., 2019; Liu et al., 2019) in NLP.\nThere are two main categories of normalization: weight\nnormalization (Salimans & Kingma, 2016; Miyato et al.,\n2018; Qiao et al., 2019) and activation normalization (Ioffe\n& Szegedy, 2015; Jarrett et al., 2009; Krizhevsky et al.,\n2012; Ba et al., 2016; Ulyanov et al., 2016; Wu & He, 2018;\nLi et al., 2019). Here, we solely focus on the latter, and we\nbrieﬂy review related work in CV and NLP.\nNormalization in Computer Vision Batch Normaliza-\ntion (BN) (Ioffe & Szegedy, 2015) has become the de-facto\nnormalization for NNs used in CV . BN normalizes the ac-\ntivations (feature maps) by computing channel-wise mean\nand variance across the batch dimension, as schematically\nshown in Figure 1. It has been found that BN leads to\nrobustness with respect to sub-optimal hyperparameters\n(e.g., learning rate) and initialization, and it generally re-\nsults in more stable training for CV tasks (Ioffe & Szegedy,\n2015). Following the seminal work of (Ioffe & Szegedy,\n2015), there have been two principal lines of research: (i)\nextensions/modiﬁcations of BN to improve its performance,\nand (ii) theoretical/empirical studies to understand why BN\nhelps training.\nWith regard to (i), it was found that BN does not per-\nform well for problems that need to be trained with small\nbatches, e.g., image segmentation (often due to memory\nlimits) (Zagoruyko & Komodakis, 2016; Lin et al., 2017;\nGoldberger et al., 2005). The work of (Ioffe, 2017) proposed\nbatch renormalization to remove/reduce the dependence of\nbatch statistics to batch size. It was shown that this approach\nleads to improved performance for small batch training as\nwell as cases with non-i.i.d. data. Along this direction, the\nwork of (Singh & Shrivastava, 2019) proposed “EvalNorm,”\nwhich uses corrected normalization statistics. Furthermore,\nthe recent work of (Yan et al., 2020) proposed “Moving\nAverage Batch Normalization (MABN)” for small batch BN\nby replacing batch statistics with moving averages.\nThere has also been work on alternative normalization tech-\nniques, and in particular Layer Normalization (LN), pro-\nposed by (Ba et al., 2016). LN normalizes across the chan-\nnel/feature dimension as shown in Figure 1. This could be\nextended to Group Norm (GN) (Wu & He, 2018), where\nthe normalization is performed across a partition of the fea-\ntures/channels with different pre-deﬁned groups. Instance\nNormalization (IN) (Ulyanov et al., 2016) is another tech-\nnique, where per-channel statistics are computed for each\nsample.\nWith regard to (ii), there have been several studies to under-\nstand why BN helps training in CV . The original motivation\nwas that BN reduces the so-called “Internal Covariance\nShift” (ICS) (Ioffe & Szegedy, 2015). However, this expla-\nnation was viewed as incorrect/incomplete (Rahimi, 2017).\nIn particular, the recent study of (Santurkar et al., 2018)\nargued that the underlying reason that BN helps training is\nthat it results in a smoother loss landscape. This was later\nconﬁrmed for deep NN models by measuring the Hessian\nspectrum of the network with/without BN (Yao et al., 2019).\nNormalization in Natural Language Processing De-\nspite the great success of BN in CV , the large computation\nand storage overhead of BN at each time-step in recurrent\nPowerNorm: Rethinking Batch Normalization in Transformers\nneural networks (RNNs) made it impossible/expensive to de-\nploy for NLP tasks (Cooijmans et al., 2017). To address this,\nthe work of (Cooijmans et al., 2017; Hou et al., 2019) used\nshared BN statistics across different time steps of RNNs.\nHowever, it was found that the performance of BN is signif-\nicantly lower than LN for NLP. For this reason, LN became\nthe default normalization technique, even for the recent\ntransformer models introduced by (Vaswani et al., 2017).\nOnly limited recent attempts were made to compare LN\nwith other alternatives or investigate the reasons behind the\nsuccess of LN in transformer models. For instance, (Zhang\n& Sennrich, 2019) proposes RMSNorm, which removes the\nre-centering invariance in LN and performs re-scaling invari-\nance with the root mean square summed of the inputs. They\nshowed that this approach achieves similar performance\nto LN, but with smaller (89% to 64%) overhead. Further-\nmore, (Nguyen & Salazar, 2019) studies different variants\nof weight normalization for transformers in low-resource\nmachine translation. The recent work of (Xu et al., 2019)\nstudies why LN helps training, and in particular it ﬁnds that\nthe derivatives of LN help recenter and rescale backward\ngradients. From a different angle, (Zhang et al., 2019b;a)\ntry to ascribe the beneﬁts of LN to solving the exploding\nand vanishing gradient problem at the beginning of train-\ning. They also propose two properly designed initialization\nschemes which also enjoy that property and are able to\nstabilize training for transformers.\nHowever, most of these approaches achieve similar or\nmarginal improvement over LN. More importantly, there is\nstill not a proper understanding of why BN performs poorly\nfor transformers applied to NLP data. Here, we address\nthis by systematically studying the BN behavior through-\nout training; and, based on our results, we propose Power\nNormalization (PN), a new normalization method that sig-\nniﬁcantly outperforms LN for a wide range of tasks in NLP.\n3. Batch Normalization\nNotation. We denote the input of a normalization layer as\nX PRBˆd, where dis the embedding/feature size and B\nis the batch size1. We denote Las the ﬁnal loss of the NN.\nThe i-th row (column) of a matrix, e.g., X, is denoted by\nXi,: (X:,i). We also write the i-th row of the matrix as\nits lower-case version, i.e., xi “Xi,:. For a vector y, yi\ndenotes the i-th element in y.\nWithout other speciﬁcation: (i) for two vector x PRd and\ny PRd, we denote xy as the element-wise product, x `y\nas the element-wise sum, and xx,yyas the inner product;\n1For NLP tasks, we ﬂatten sentences/word in one dimension,\ni.e., the batch size actually corresponds to all non-padded words\nin a training batch.\nAlgorithm 1 Batch Normalization (Every Iteration)\nbegin Forward Propagation:\nInput: X PRBˆd\nOutput: Y PRBˆd\nµB “ 1\nB\nřB\ni“1 xi // Get mini-batch mean\nσ2\nB “ 1\nB\nřB\ni“1pxi´µBq2 // Get mini-batch variance\n|X “ X´µB\nσB\n// Normalize\nY “γd|X `β // Scale and shift\nµ“αµ`p1 ´αqµB // Update running mean\nσ2 “ασ2 `p1 ´αqσ2\nB // Update running variance\nbegin Backward Propagation:\nInput: BL\nBY PRBˆd\nOutput: BL\nBX PRBˆd\nBL\nBX based on Eq. 3 // Gradient of X\nInference: Y “γdX´µ\nσ `β\n(ii) for a vector y PRd and a matrix X PRBˆd, we denote\nydX as ry1X:,1,...,y dA:,dsand y`X as ry`X1,:; ...; y`\nXB,:s; and (iii) for a vector y PRd, y ąCmeans that each\nentry of y is larger than the constant C, i.e., yi ąCfor all\ni.\n3.1. Formulation of Batch Normalization\nWe brieﬂy review the formulation of BN (Ioffe & Szegedy,\n2015). Let us denote the mean (variance) of X along the\nbatch dimension as µB PRd (σ2\nB PRd). The batch dimen-\nsion is illustrated in Figure 1. The BN layer ﬁrst enforces\nzero mean and unit variance, and it then performs an afﬁne\ntransformation by scaling the result by γ,β PRd, as shown\nin Algorithm 1.\nThe Forward Pass (FP) of BN is performed as follows. Let\nus denote the intermediate result of BN with zero mean and\nunit variance as |X, i.e.,\n|X “ X ´µB\nσB\n. (1)\nThe ﬁnal output of BN, Y , is then an afﬁne transformation\napplied to |X:\nY “γd|X `β. (2)\nThe corresponding Backward Pass (BP) can then be derived\nas follows. Assume that the derivative of Lwith respect to\nY is given, i.e., BL\nBY is known. Then, the derivative with\nrespect to input can be computed as:\nBL\nBxi\n“ γ\nσB\nBL\nByi\n´ γ\nσBB\nÿ\njPB\np BL\nByjloomoon\nfrom µB: gµ\n` BL\nByj\nˇxjˇxi\nloooomoooon\nfrom σ2\nB: gσ2\nq. (3)\nSee Lemma 6 in Appendix C for details. We denote the\nterms contributed by µB and σ2\nB as gµ and gσ2 , respectively.\nPowerNorm: Rethinking Batch Normalization in Transformers\n20% 40% 60% 80% 100%\nPercent of Training Epochs\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n1\nd || B||\nCIFAR10\nIWSLT14\n20% 40% 60% 80% 100%\nPercent of Training Epochs\n0\n1\n2\n3\n4\n5\n6\n1\nd || 2 2\nB||\nCIFAR10\nIWSLT14\nFigure 2.The average Euclidean distance between the batch statistics (µB, σ2\nB) and the running statistics (µ, σ2) stored in ﬁrst BN during\nforward pass for ResNet20 on Cifar-10 and Transformer on IWLST14. We can clearly see that the ResNet20 statistics have orders of\nmagnitude smaller variation than the running statistics throughout training. However, the corresponding statistics in Transformer BN\nexhibit very high variance with extreme outliers. This is true both for the mean (shown in the left) as well as variance (shown in right).\nThis is one of the contributing factors to the low performance of BN in transformers.\nIn summary, there are four batch statistics in BN, two in\nFP and two in BP. The stability of training is highly depen-\ndent on these four parameters. In fact, naively implement-\ning the BN as above for transformers leads to poor perfor-\nmance. For example, using transformer with BN (denoted\nas TransformerBN ) results in 1.1 and 1.4 lower BLEU score,\nas compared to the transformer with LN (TransformerLN ),\non IWSLT14 and WMT14, respectively; see Table 1.\nThis is signiﬁcant performance degradation, and it stems\nfrom instabilities associated with the above four batch\nstatistics. To analyze this, we studied the batch statistics\nusing the standard setting of ResNet20 on Cifar-10 and\nTransformerBN on IWSLT14 (using a standard batch size\nof 128 and tokens of 4K, respectively). In the ﬁrst experi-\nment, we probed the ﬂuctuations between batch statistics,\nµB/σB, and the corresponding BN running statistics, µ/σ,\nthroughout training. This is shown for the ﬁrst BN layer\nof ResNet20 on Cifar-10 and TransformerBN on IWSLT14\nin Figure 2. Here, the y-axis shows the average Euclidean\ndistance between batch statistics (µB, σB) and the running\nstatistics (µ, σ), and the x-axis is different epochs of train-\ning, where we deﬁne the average Euclidean distance as\ndistpµB,µq“ 1\nd}µB ´µ}.\nThe ﬁrst observation is that Transformer BN shows signiﬁ-\ncantly larger distances between the batch statistics and the\nrunning statistics than ResNet20 on Cifar-10, which exhibits\nclose to zero ﬂuctuations. Importantly, this distance between\nσB and σ signiﬁcantly increases throughout training, but\nwith extreme outliers. During inference, we have to use the\nrunning statistics. However, such large ﬂuctuations would\nlead to a large inconsistency between statistics of the testing\ndata and the BN’s running statistics.\nThe second observation comes from probing the norm of gµ\nand gσ2 deﬁned in Eq. 3, which contribute to the gradient\nbackpropagation of input. These results are shown in Figure\n3, where we report the norm of these two parameters for\nResNet20 and TransformerBN . For TransformerBN , we can\nsee very large outliers that actually persist throughout train-\ning. This is in contrast to ResNet20, for which the outliers\nvanish as training proceeds.\n4. Power Normalization\nBased on our empirical observations, we propose Power\nNormalization (PN), which effectively resolves the perfor-\nmance degradation of BN. This is achieved by incorporating\nthe following two changes to BN. First, instead of enforcing\nunit variance, we enforce unit quadratic mean for the acti-\nvations. The reason for this is that we ﬁnd that enforcing\nzero-mean and unit variance in BN is detrimental due to the\nlarge variations in the mean, as discussed in the previous\nsection. However, we observe that unlike mean/variance,\nthe unit quadratic mean is signiﬁcantly more stable for trans-\nformers. Second, we incorporate running statistics for the\nquadratic mean of the signal, and we incorporate an approxi-\nmate backpropagation method to compute the corresponding\ngradient. We ﬁnd that the combination of these two changes\nleads to a signiﬁcantly more effective normalization, with\nresults that exceed LN, even when the same training hyper-\nparameters are used. Below we discuss each of these two\ncomponents.\n4.1. Relaxing Zero-Mean and Enforcing Quadratic\nMean\nHere, we describe the ﬁrst modiﬁcation in PN. As shown\nin Figure 2 and 3, µB and gµ exhibit signiﬁcant number of\nlarge outliers, which leads to inconsistencies between train-\ning and inference statistics. We ﬁrst address this by relaxing\nthe zero-mean normalization, and we use the quadratic mean\nof the signal, instead of its variance. The quadratic mean\nexhibits orders of magnitude smaller ﬂuctuations, as shown\nPowerNorm: Rethinking Batch Normalization in Transformers\n20% 40% 60% 80% 100%\nPercent of Training Epochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1\nd ||g ||\nCIFAR10\nIWSLT14\n20% 40% 60% 80% 100%\nPercent of Training Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1\nd ||g 2||\nCIFAR10\nIWSLT14\nFigure 3.The average gradient norm of the input of the ﬁrst BN layer contributed by µB and σB for ResNet20 on Cifar10 and\nTransformerBN on IWSLT14 during the BP (note that d “ 16 for Cifar-10 and d “ 512 for IWSLT experiment). It can be clearly\nseen that the norm of gµ and gσ2 for ResNet20 has orders of magnitude smaller variation throughout training, as compared to that for\nTransformerBN . Also, the outliers for ResNet20 vanish at the end of training, which is in contrast to TransformerBN , for which the outliers\npersist. This is true both for gµ (shown in left) as well as gσ2 (shown in right).\n20% 40% 60% 80% 100%\n0\n1\n2\n3\n4\n5\n6\n1\nd || 2 2\nB||\nBN\nPN-V\n0\n1\n2\n3\n4\n5\n6\n1\nd || 2 2\nB||\n20% 40% 60% 80% 100%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1\nd ||g 2||\nBN\nPN-V\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1\nd ||g 2||\nFigure 4.Results for Transformer on IWSLT14. (Left) The average Euclidean distance between batch statistics (σB, ψB) and the running\nstatistics (σ, ψ) stored in ﬁrst BN/PN-V during forward propagation (FP). (Right) The average norm of gradient of the input of the ﬁrst\nBN/PN-V contributed by σB/ψB. During FP, ψB has much smaller variations of running statistics, as compared to σB, as shown in left,\nIt can also be clearly seen that during BP, the norm of gψ2 exhibits many fewer outliers, as compared to gσ2 , throughout the training.\nin Figure 4. We refer to this normalization (i.e., no zero\nmean and unit quadratic mean enforcement) as PN-V , de-\nﬁned as follows.\nDeﬁnition 1 (PN-V ). Let us denote the quadratic mean of\nthe batch as ψB\n2 “ 1\nB\nřB\ni“1 x2\ni. Furthermore, denote xX\nas the signal scaled by ψB, i.e.,\nxX “ X\nψB\n. (4)\nThen, the output of PN-V is deﬁned as\nY “γdxX `β, (5)\nwhere γ PRd and β PRd are two parameters (vectors) in\nPN-V (which is the same as in the afﬁne transformation\nused in BN).\nNote that here we use the same notation Y as the output\nin Eq. 2 without confusion.\nThe corresponding BP of PN-V is as follows:\nBL\nBxi\n“ γ\nψB\nBL\nByi\n´ γ\nBψB\nÿ\njPB\nBL\nByj\nˆxjˆxi\nloooomoooon\nfrom ψB2: gψ2\n. (6)\nSee Lemma 8 in Appendix C for the full details. Here,\ngψ2 is the gradient attributed by ψB\n2. Note that, compared\nto BN, there exist only two batch statistics in FP and BP:\nψB\n2 and gψ2 . This modiﬁcation removes the two unstable\nfactors corresponding to µB and σB in BN ( gµ, and gσ2\nin Eq. 3). This modiﬁcation also results in signiﬁcant per-\nformance improvement, as reported in Table 1 for IWSLT14\nand WMT14. By directly replacing BN with PN-V (de-\nnoted as TransformerPN-V ), the BLEU score increases from\n34.4 to 35.4 on IWSLT14, and 28.1 to 28.5 on WMT14.\nThese improvements are signiﬁcant for these two tasks. For\nexample, (Zhang et al., 2019b;a) only improves the BLEU\nscore by 0.1 on IWSLT14.\nAs mentioned before, ψB exhibits orders of magnitude\nsmaller variations, as compared to σB. This is shown in Fig-\nPowerNorm: Rethinking Batch Normalization in Transformers\nure 4, where we report the distance between the running\nstatistics for σ, distpσ2\nB,σ2q, and ψ, distpψB\n2,ψ2q. Simi-\nlarly during BP, we compute the norm ofgσ2 and gψ2 , and\nwe report it in Figure 4 throughout training. It can be clearly\nseen that during BP, the norm of gψ2 exhibits many fewer\noutliers as compared to gσ2 .\nIn (Santurkar et al., 2018), the authors provided theoretical\nresults suggesting that employing BN in DNNs can lead to\na smaller Lipschitz constant of the loss.\nIt can be shown that PN-V also exhibits similar behaviour,\nunder mild assumptions. In more detail, let us denote the\nloss of the NN without normalization as pL. With mild\nassumptions, (Santurkar et al., 2018) shows that the norm\nof BL\nBX (with BN) is smaller than the norm of BpL\nBX . Here, we\nshow that, under the same assumptions, PN-V can achieve\nthe same results that BN does. See Appendix C for details,\nincluding the statement of Assumption 9.\nLemma 2 (The effect of PN-V on the Lipschitz constant of\nthe loss). Under Assumption 9, we have\n} BL\nBX:,i\n}2 “ γ2\ni\npψBq2\ni\n˜\n} BpL\nBX:,i\n}2 ´x BpL\nBX:,i\n,\nxX:,i?\nB\ny2\n¸\n. (7)\nSee the proof in Appendix C. Note thatx BpL\nBX:,i\n,\nxX:,i?\nBy2 is non-\nnegative, and hence the Lipschitz constant of Lis smaller\nthan that of pLif γi ďpψBqi. This is what we observe in\npractice, as shown in Appendix B.\n4.2. Running Statistics in Training\nHere, we discuss the second modiﬁcation in PN. First\nnote that even though Transformer PN-V outperforms\nTransformerBN , it still can not match the performance of\nLN. This could be related to the larger number of outliers\npresent in ψB, as shown in Figure 4. A straightforward\nsolution to address this is to use running statistics for the\nquadratic mean (denoted as ψ2), instead of using per batch\nstatistics, since the latter changes in each iteration. How-\never, using running statistics requires modiﬁcation of the\nbackpropagation, which we described below.\nDeﬁnition 3 (PN). Denote the inputs/statistics at the t-th\niteration by ¨ptq, e.g., Xptqis the input data at t-th iteration.\nIn the forward propagation, the following equations are\nused for the calculation:\nxXptq “ Xptq\nψpt´1q, (8)\nY ptq “γdxXptq`β, (9)\npψptqq2 “pψpt´1qq2 `p1 ´αqpψB\n2 ´pψpt´1qq2q. (10)\nHere, 0 ăα ă1 is the moving average coefﬁcient in the\nforward propagation, and ψB is the statistic for the current\nbatch. Since the forward pass evolves running statistics,\nAlgorithm 2 Power Normalization (Every Iteration)\nbegin Forward Propagation:\nInput: X PRBˆd\nOutput: Y PRBˆd\nψB\n2 “ 1\nB\nřB\ni“1 x2\ni // Get mini-batch statistics\nxX “ X\nψ // Normalize\nY “γdxX `β // Scale and shift\nψ2 “αψ2 `p1 ´αqψB\n2 // Update running\nstatistics\nbegin Backward Propagation:\nInput: BL\nBY PRBˆd\nOutput: BL\nBX PRBˆd\nBL\nBxX “γd BL\nBY // Intermediate Gradient\nĂX1 “ BL\nBxX ´νxX // Intermediate Estimated Gradient\nBL\nBX “\nĂX1\nψ // Gradient of X\nν “νp1 ´p1 ´αqΓq`p 1 ´αqΛ // See\nDefinition 3 for Γ and Λ\nInference: Y “γdX\nψ `β\nthe backward propagation cannot be accurately computed—\nnamely, the accurate gradient calculation needs to track\nback to the ﬁrst iteration. Here, we propose to use the\nfollowing approximated gradient in backward propagation:\npĂXptqq1 “ BL\nBxXptq\n´νpt´1qdxXptq, (11)\nBL\nBXptqq “ pĂXptqq1\nψpt´1q , (12)\nνptq “νpt´1qp1 ´p1 ´αqΓptqq`p 1 ´αqΛptq, (13)\nwhere Γptq “ 1\nB\nřB\ni“1 ˆxptq\ni ˆxptq\ni and Λptq “\n1\nB\nřB\ni“1\nBL\nBˆxptq\ni\nˆxptq\ni .\nThis backpropagation essentially uses running statistics by\ncomputing the gradient of the loss w.r.t. the quadratic mean\nof the current batch, rather than using the computationally\ninfeasible method of computing directly the gradient w.r.t.\nrunning statistics of the quadratic mean. Importantly, this\nformulation leads to bounded gradients which is necessary\nfor convergence, as shown below.\nTheorem 4 (Gradient of Lw.r.t. X is bounded in PN). For\nany datum point of ĂX (i.e. ĂXi,:), the gradients computed\nfrom Eq. 11 are bounded by a constant.\nFurthermore, the gradient of Xi,: is also bounded, as\ngiven Eq. 12.\nSee the proof in Appendix C. The pseudo-code for PN\nalgorithm is presented in Algorithm 2.\nPowerNorm: Rethinking Batch Normalization in Transformers\n5. Results\n5.1. Experiment Setup\nWe compare our PN method with LN and BN for a variety\nof sequence modeling tasks: Neural Machine Translation\n(MT); and Language Modeling (LM). We implement our\ncode for MT using fairseq-py (Ott et al., 2019), and (Ma\net al., 2019) for LM tasks. For a fair comparison, we directly\nreplace the LN in transformers (TransformerLN ) with BN\n(TransformerBN ) or PN (TransformerPN ) without varying\nthe position of each normalization layer or changing the\ntraining hyperparameters.\nFor all the experiments, we use the pre-normalization setting\nin (Wang et al., 2019), where the normalization layer is\nlocated right before the multi-head attention module and\npoint-wise feed-forward network module. Following (Wang\net al., 2019), we generally increase the learning rate by a\nfactor of 2.0, relative to the common post-normalization\ntransformer (Vaswani et al., 2017). Below we discuss tasks\nspeciﬁc settings. 2\nNeural Machine Translation We evaluate our methods\non two widely used public datasets: IWSLT14 German-\nto-English (De-En) and WMT14 English-to-German (En-\nDe) dataset. We follow the settings reported in (Ott et al.,\n2018). We use transformer big architecture for WMT14\n(4.5M sentence pairs) andsmall architecture for IWSLT14\n(0.16M sentence pairs). For inference, we average the last\n10 checkpoints, and we set the length penalty to 0.6/1.0, and\nthe beam size to 4/5 for WMT/IWSLT, following (Ott et al.,\n2019). All the other hyperparamters (learning rate, dropout,\nweight decay, warmup steps, etc.) are set identically to the\nones reported in the literature for LN (i.e., we use the same\nhyperparameters for BN/PN).\nLanguage Modeling We experiment on both\nPTB (Mikolov et al., 2011) and Wikitext-103 (Mer-\nity et al., 2017), which contain 0.93M and 100M tokens,\nrespectively. We use three layers tensorized transformer\ncore-1 for PTB and six layers tensorized transformer\ncore-1 for Wikitext-103, following (Ma et al., 2019).\nFurthermore, we apply the multi-linear attention mech-\nanism with masking, and we report the ﬁnal testing set\nperplexity (PPL).3\n5.2. Experiment Results\nNeural Machine Translation We use BLEU (Papineni\net al., 2002) as the evaluation metric for MT. Following\nstandard practice, we measure tokenized case-sensitive\n2More detailed experimental settings and comparisons between\nnormalization methods are provided in Appendix A, B.3.\n3We also report the validation perplexity in Appendix B.\nModel IWSLT14 WMT14\nsmall big\nTransformer (Vaswani et al., 2017) 34.4 28.4\nDS-Init (Zhang et al., 2019a) 34.4 29.1\nFixup-Init (Zhang et al., 2019b) 34.5 29.3\nScaling NMT (Ott et al., 2018) / 29.3\nDynamic Conv (Wu et al., 2019) 35.2 29.7\nTransformer + LayerDrop (Fan et al., 2020) / 29.6\nPre-Norm TransformerLN 35.5 29.5\nPre-Norm TransformerBN 34.4 28.1\nPre-Norm TransformerPN-V 35.5 28.5\nPre-Norm TransformerPN 35.9 30.1\nTable 1.MT performance (BLEU) on IWSLT14 De-En and\nWMT14 En-De testsets. Using PN-V instead of BN signiﬁcantly\nimproves the performance, but LN still outperforms. However,PN\nachieves much higher BLEU scores, as compared to LN.\nBLEU and case-insensitive BLEU for WMT14 En-De and\nIWSLT14 De-En, respectively. For a fair comparison, we\ndo not include other external datasets. All the transform-\ners in Table 1 are using six encoder layers and six decoder\nlayers.\nThe results are reported in Table 1. In the ﬁrst section of\nrows, we report state-of-the-art results for these two tasks\nwith comparable model sizes. In the second section of rows,\nwe report the results with different types of normalization.\nNotice the signiﬁcant drop in BLEU score when BN is used\n(34.4/28.1), as opposed to LN (35.5/29.5). Using PN-V in-\nstead of BN helps reduce this gap, but LN still outperforms.\nHowever, the results corresponding to PN exceeds LN re-\nsults by more than 0.4/0.6 points, This is signiﬁcant for these\ntasks. Comparing with other concurrent works like DS-Init\nand Fixup-Init (Zhang et al., 2019a;b), the improvements in\nTransformerPN are still signiﬁcant.\nModel PTB WikiText-103\nTest PPL Test PPL\nTied-LSTM (Inan et al., 2017) 48.7 48.7\nAWD-LSTM-MoS (Yang et al., 2018) 56.0 29.2\nAdaptive Input (Baevski & Auli, 2019) 57.0 20.5\nTransformer-XLbase(Dai et al., 2019) 54.5 24.0\nTransformer-XLlarge(Dai et al., 2019) – 18.3\nTensor-Transformer1core(Ma et al., 2019) 57.9 20.9\nTensor-Transformer2core(Ma et al., 2019) 49.8 18.9\nTensor-Transformer1core+ LN 53.2* 20.9*\nTensor-Transformer1core+ BN 60.7 27.2\nTensor-Transformer1core+ PN-V 55.3 21.3\nTensor-Transformer1core+ PN 47.6 17.9\nTable 2. Results with state-of-the-art methods on PTB and\nWikiText-103. ’-’ indicates no reported results in that setting,\n’˚’ indicates the results are from our own implementation. PN\nachieves 5.6/3 points lower testing PPL on PTB and WikiTest-103,\nrespectively, compared to LN.\nPowerNorm: Rethinking Batch Normalization in Transformers\nLanguage Modeling We report the LM results in Table 2,\nusing the tensorized transformer proposed in (Ma et al.,\n2019). Here we observe a similar trend. Using BN re-\nsults in a signiﬁcant degradation, increasing testing PPL by\nmore than 7.5/6.3 for PTB/WikiText-103 datasets (achiev-\ning 60.7/27.2 as opposed to 53.2/20.9). However, when we\nincorporate the PN normalization, we achieve state-of-the-\nart results for these two tasks (for these model sizes and\nwithout any pre-training on other datasets). In particular,\nPN results in 5.6/3 points lower testing PPL, as compared\nto LN. Importantly, note that using PN we achieve better\nresults than (Ma et al., 2019), with the same number of\nparameters.\n5.3. Analysis\n512 1k 2k 4k\nBatch Size (#tokens in each batch) for IWSLT14\n34.6\n34.8\n35.0\n35.2\n35.4\n35.6\n35.8BLEU\nPN\nLN\nPN-V\nBN\nFigure 5.Ablation study of the performance of PN, PN-V , LN\nand BN on IWSLT14 trained using different batch sizes. Note\nthat the performance of PN consistently outperforms LN. In the\nmeanwhile, PN-V can only match the result of LN when mini-\nbatch gets to 4K. Among all the settings, BN behaves poorly and\nabnormally across different mini-batches.\nThe Effect of Batch Size for Different Normalization\nTo understand better the effects of our proposed methods\nPN and PN-V , we change the batch size used to collect\nstatistics in BN, LN, and PN. To this end, we keep the total\nbatch size constant at 4K tokens, and we vary the mini-batch\nsize used to collect statistics from 512 to 4K. Importantly,\nnote that we keep the total batch size constant at 4K, and\nwe use gradient accumulation for smaller mini-batches. For\nexample, for the case with mini-batch of 512, we use eight\ngradient accumulations. The results are reported in Figure\n5. We can observe that BN behaves poorly and abnormally\nacross different mini-batches. Noticeably, after relaxing\nthe zero-mean normalization in BN and replacing the vari-\nance estimation with quadratic mean, PN-V matches the\nperformance of LN for 4K mini-batch and consistently out-\nperforms BN. However, it underperforms LN. In contrast,\nwe can see that PN consistently achieves higher results\nunder different mini-batch settings.\nRepresentation Power of learned Embedding To inves-\ntigate further the performance gain of PN, we compute the\nSingular Value Decomposition of the embedding layers, as\nproposed by (Gao et al., 2019), which argued that the singu-\nlar value distribution could be used as a proxy for measuring\nrepresentational power of the embedding layer. It has been\nargued that having fast decaying singular values leads to\nlimiting the representational power of the embeddings to a\nsmall sub-space. If this is the case, then it may be preferable\nto have a more uniform singular value distribution (Wang\net al., 2020). We compute the singular values for word em-\nbedding matrix of LN and PN, and we report the results\nin Figure 6. It can be clearly observed that the singular\nvalues corresponding to PN decay more slowly than those\nof LN. Intuitively, one explanation for this might be that\nPN helps by normalizing all the tokens across the batch\ndimension, which can result in a more equally distributed\nembeddings. This may illustrate one of the reasons why PN\noutperforms LN.\n0 200 400 600 800 1000\nThe Index of Sigular Value\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Singular Value\nPN\nLN\nFigure 6.Singular values of embedding matrix trained with\nLN/PN on WMT14. We normalize the singular values of each\nmatrix so that they are comparable with the largest one as 1. Note\nthat the singular values corresponding to PN decay more slowly\nthan those of LN.\n6. Conclusion\nIn this work, we systematically analyze the ineffectiveness\nof vanilla batch normalization (BN) in transformers. Com-\nparing NLP and CV , we show evidence that the batch statis-\ntics in transformers on NLP tasks have larger variations.\nThis further leads to the poor performance of BN in trans-\nformers. By decoupling the variations into FP and BP com-\nputation, we propose PN-V and PN to alleviate the vari-\nance issue of BN in NLP. We also show the advantages of\nPN-V and PN, both theoretically and empirically. Theoreti-\ncally, PN-V preserves the ﬁrst-order smoothness property\nas in BN. The approximate backpropagation of PN leads to\nbounded gradients. Empirically, we show that PN outper-\nforms LN in neural machine translation (0.4/0.6 BLEU on\nPowerNorm: Rethinking Batch Normalization in Transformers\nIWSLT14/WMT14) and language modeling (5.6/3.0 PPL\non PTB/WikiText-103) by a large margin. We also conduct\nfurther analysis of the effect of PN-V /PN/BN/LN under\ndifferent batch size settings to show the signiﬁcance of sta-\ntistical estimations, and we investigate the representation\npower of learned embeddings matrix by LN/PN to illustrate\nthe effectiveness of PN.\nAcknowledgments\nThis work was supported by funds from Intel and Samsung.\nWe are grateful to support from Google Cloud, Google\nTFTC team, as well as support from the Amazon AWS.\nWe would like to acknowledge ARO, DARPA, NSF, and\nONR for providing partial support of this work. We are also\ngrateful to Zhuohan Li, Zhen Dong, Yang Liu, the members\nof Berkeley NLP, and the members of the Berkeley RISE\nLab for their valuable feedback.\nReferences\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBaevski, A. and Auli, M. Adaptive input representations for\nneural language modeling. In ICLR, 2019.\nCooijmans, T., Ballas, N., Laurent, C., G ¨ulc ¸ehre, C ¸., and\nCourville, A. Recurrent batch normalization. In ICLR,\n2017.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. In ACL, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In NAACL, 2019.\nFan, A., Grave, E., and Joulin, A. Reducing transformer\ndepth on demand with structured dropout. In ICLR, 2020.\nGao, J., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.\nRepresentation degeneration problem in training natural\nlanguage generation models. In ICLR, 2019.\nGoldberger, J., Hinton, G. E., Roweis, S. T., and Salakhut-\ndinov, R. R. Neighbourhood components analysis. In\nNeurIPS, 2005.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In CVPR, 2016.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 1997.\nHou, L., Zhu, J., Kwok, J., Gao, F., Qin, T., and Liu, T.-\ny. Normalization helps training of quantized lstm. In\nNeurIPS, 2019.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nCVPR, 2017.\nInan, H., Khosravi, K., and Socher, R. Tying word vectors\nand word classiﬁers: A loss framework for language\nmodeling. In ICLR, 2017.\nIoffe, S. Batch renormalization: Towards reducing mini-\nbatch dependence in batch-normalized models. In\nNeurIPS, 2017.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn ICML, 2015.\nJarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y .\nWhat is the best multi-stage architecture for object recog-\nnition? In ICCV, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NeurIPS, 2012.\nLi, B., Wu, F., Weinberger, K. Q., and Belongie, S. Posi-\ntional normalization. In NeurIPS, 2019.\nLin, T.-Y ., Goyal, P., Girshick, R., He, K., and Doll ´ar, P.\nFocal loss for dense object detection. In ICCV, 2017.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nMa, X., Zhang, P., Zhang, S., Duan, N., Hou, Y ., Zhou,\nM., and Song, D. A tensorized transformer for language\nmodeling. In NeurIPS, 2019.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models. In ICLR, 2017.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and\nˇCernock`y, J. Empirical evaluation and combination of\nadvanced language modeling techniques. In INTER-\nSPEECH, 2011.\nMiyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . Spec-\ntral normalization for generative adversarial networks. In\nICLR, 2018.\nNguyen, T. Q. and Salazar, J. Transformers without tears:\nImproving the normalization of self-attention. arXiv\npreprint arXiv:1910.05895, 2019.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling neu-\nral machine translation. In Machine Translation, 2018.\nPowerNorm: Rethinking Batch Normalization in Transformers\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\nN., Grangier, D., and Auli, M. fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL: Demonstra-\ntions, 2019.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a\nmethod for automatic evaluation of machine translation.\nIn ACL, 2002.\nQiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Weight\nstandardization. arXiv preprint arXiv:1903.10520, 2019.\nRahimi, A. Nuerips 2017 test-of-time award presentation,\nDecember 2017.\nSalimans, T. and Kingma, D. P. Weight normalization: A\nsimple reparameterization to accelerate training of deep\nneural networks. In NeurIPS, 2016.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\nbottlenecks. In CVPR, 2018.\nSanturkar, S., Tsipras, D., Ilyas, A., and Madry, A. How\ndoes batch normalization help optimization? In NeurIPS,\n2018.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In ACL,\n2016.\nSingh, S. and Shrivastava, A. Evalnorm: Estimating batch\nnormalization statistics for evaluation. In ICCV, 2019.\nUlyanov, D., Vedaldi, A., and Lempitsky, V . Instance nor-\nmalization: The missing ingredient for fast stylization.\narXiv preprint arXiv:1607.08022, 2016.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis all you need. In NeurIPS, 2017.\nWang, L., Huang, J., Huang, K., Hu, Z., Wang, G., and Gu,\nQ. Improving neural language generation with spectrum\ncontrol. In ICLR, 2020.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F.,\nand Chao, L. S. Learning deep transformer models for\nmachine translation. In ACL, 2019.\nWu, F., Fan, A., Baevski, A., Dauphin, Y ., and Auli, M. Pay\nless attention with lightweight and dynamic convolutions.\nIn ICLR, 2019.\nWu, Y . and He, K. Group normalization. InECCV, 2018.\nXu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. Under-\nstanding and improving layer normalization. In NeurIPS,\n2019.\nYan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y ., and Sun, J.\nTowards stabilizing batch statistics in backward propaga-\ntion of batch normalization. In ICLR, 2020.\nYang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. W.\nBreaking the softmax bottleneck: A high-rank rnn lan-\nguage model. In ICLR, 2018.\nYao, Z., Gholami, A., Keutzer, K., and Mahoney, M. W. Py-\nHessian: Neural networks through the lens of the Hessian.\narXiv preprint arXiv:1912.07145, 2019.\nZagoruyko, S. and Komodakis, N. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.\nZhang, B. and Sennrich, R. Root mean square layer normal-\nization. In NeurIPS, 2019.\nZhang, B., Titov, I., and Sennrich, R. Improving deep\ntransformer with depth-scaled initialization and merged\nattention. In EMNLP, 2019a.\nZhang, H., Dauphin, Y . N., and Ma, T. Residual learning\nwithout normalization via better initialization. In ICLR,\n2019b.\nPowerNorm: Rethinking Batch Normalization in Transformers\nA. Training Details\nA.1. Machine Translation.\nDataset The training/validation/test sets for the IWSLT14 dataset contain about 153K/7K/7K sentence pairs, respectively.\nWe use a vocabulary of 10K tokens based on a joint source and target byte pair encoding (BPE) (Sennrich et al., 2016). For\nthe WMT14 dataset, we follow the setup of (Vaswani et al., 2017), which contains 4.5M training parallel sentence pairs.\nNewstest2014 is used as the test set, and Newstest2013 is used as the validation set. The 37K vocabulary for WMT14 is\nbased on a joint source and target BPE factorization.\nHyperparameter Given the unstable gradient issues of decoders in NMT (Zhang et al., 2019a), we only change all the\nnormalization layers in the 6 encoder layers from LN to BN/ PN, and we keep all the 6 decoder layers to use LN. For\nTransformerPN-V big and TransformerBN big (not TransformerPN big), we use the synchronized version, where each\nFP and BP will synchronize the mean/variance/quadratic mean of different batches at different nodes. For PN, we set the\nαin the forward and backward steps differently, and we tune the best setting over 0.9/0.95/0.99 on the validation set. To\ncontrol the scale of the activation, we also involve a layer-scale layer (Zhang & Sennrich, 2019) in each model setting before\nthe normalization layer. The warmup scheme for accumulating ψis also employed, as suggested in (Yan et al., 2020) .\nSpeciﬁcally, we do not tune the warmup steps, but we set it identical to the warmup steps for the learning rate schedule in\nthe optimizer (Vaswani et al., 2017). We set dropout as 0.3/0.0 for Transformerbig/small model, respectively. We use\nthe Adam optimizer and follow the optimizer setting and learning rate schedule in (Wang et al., 2019). We set the maximum\nnumber of updates following (Ott et al., 2018) to be 300k for WMT and 100k for IWSLT. We used early stopping to stop\nthe experiments by showing no improvement over the last 10/5 epochs. For the big model, we enlarge the batch size and\nlearning rate, as suggested in (Ott et al., 2019), to accelerate training. We employ label smoothing of value ϵls “0.1 in all\nexperiments. We implement our code for MT using fairseq-py (Ott et al., 2019).\nEvaluation We use BLEU4 (Papineni et al., 2002) as the evaluation metric for MT. Following standard practice, we\nmeasure tokenized case-sensitive BLEU and case-insensitive BLEU for WMT14 En-De and IWSLT14 De-En, respectively.\nFor a fair comparison, we do not include other external datasets. For inference, we average the last 10 checkpoints, and we\nset the length penalty to 0.6/1.0 and beam size to 4/5 for WMT/IWSLT, following (Ott et al., 2019).\nA.2. Language Modeling.\nDataset PTB (Mikolov et al., 2011) has 0.93M training tokens, 0.073M validation words, and 0.082M test word. Wikitext-\n103 (Merity et al., 2017) contains 0.27M unique tokens, and 100M training tokens from 28K articles, with an average length\nof 3.6K tokens per article. We use the same evaluation scheme that was provided in (Dai et al., 2019).\nHyperparameter We use three layers tensorized transformer core-1 for PTB and six layers tensorized transformer core-1\nfor Wikitext-103, following (Ma et al., 2019). This means there exists only one linear projection in multi-linear attention.\nWe replace every LN layer with a PN layer. For PN, we set the αin forward and backward differently, and we tune the best\nsetting over 0.9/0.95/0.99 on the validation set. The warmup scheme and layer-scale are also the same as the hyperparameter\nsetting introduced for machine translation. We set the dropout as 0.3 in all the datasets. The model is trained using 30\nepochs for both PTB and WikiText-103. We use the Adam optimizer, and we follow the learning rate setting in (Ma et al.,\n2019). We set the warmup steps to be 4000 and label smoothing to be ϵls “0.1 in all experiments.\nB. Extra Results\nB.1. Empirical Results for Lemma 2.\nUnder Assumption 9, mentioned in Section 4.1 and discussed in Appendix C.1, we show\n} BL\nBX:,i\n}2 “ γ2\ni\npψBq2\ni\n`\n} BpL\nBX:,i\n}2 ´x BpL\nBX:,i\n,\nxX:,i?\nB\ny2˘\n.\nGiven that x BpL\nBX:,i\n,\nxX:,i?\nBy2 is non-negative, the Lipschitz constant of Lis smaller than that of pLif γi ďpψBqi. Here, we\n4https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nPowerNorm: Rethinking Batch Normalization in Transformers\nreport the empirical results to show that γi ďpψBqi holds for each iPt1,2,...,d uon IWSLT14; see Figure 7. Observe\nthat the Lipschitz constant of Lis smaller than that of pLempirically in our setting.\nLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nB\n= B\nFigure 7.The empirical results of the distribution of γ\npψBq P Rd in different layers of Transformer PN-V on IWSLT14. Given that\nγi ďpψBqi holds for each iPt1,2,...,d u, Lemma 2 holds as well.\nB.2. Validation Results on Language Modeling.\nModel PTB WikiText-103\nVal PPL Test PPL Val PPL Test PPL\nTied-LSTM (Inan et al., 2017) 75.7 48.7 – 48.7\nAWD-LSTM-MoS (Yang et al., 2018) 58.1 56.0 29.0 29.2\nAdaptive Input (Baevski & Auli, 2019) 59.1 57.0 19.8 20.5\nTransformer-XLbase (Dai et al., 2019) 56.7 54.5 23.1 24.0\nTransformer-XLlarge (Dai et al., 2019) – – – 18.3\nTensor-Transformer1core (Ma et al., 2019) 55.4 57.9 23.6 20.9\nTensor-Transformer2core (Ma et al., 2019) 54.3 49.8 19.7 18.9\nTensor-Transformer1core + LN 58.0* 53.2* 22.7* 20.9*\nTensor-Transformer1core + BN 71.7 60.7 28.4 27.2\nTensor-Transformer1core + PN-V 59.7 55.3 23.6 21.3\nTensor-Transformer1core + PN 51.6 47.6 18.3 17.9\nTable 3. Additional Validation and Test results with state-of-the-art results on PTB and WikiText-103. ’-’ indicates no reported results in\nthat setting, ’˚’ indicates that the results are from our own implementation.PN achieves 5.6/3.0 points lower testing PPL on PTB and\nWikiTest-103, respectively, as compared to LN.\nB.3. More Comparisons.\nAs shown in B.3, we present more comparison results for different normalization method including Moving Average Batch\nNormalizatio (MABN) (Yan et al., 2020), Batch Renormalization (BRN) (Ioffe, 2017), and Group Normalization (GN)\n(Wu & He, 2018). We can observe that BRN/MABN/PN-V is better than BN but worse than LN, which suggests the small\nbatch-size setting (main focus of (Yan et al., 2020; Ioffe, 2017; Wu & He, 2018)) may have similar characteristic of the\nsetting in NLP, where there exists large variance across batches. Obviously, GN performs the best among the previous\nproposed methods given LN can be viewed as the special case of GN (group number as 1). 5 Throughout the comparisons,\n5We empirically found that setting group number the same as head number leads to the best performance.\nPowerNorm: Rethinking Batch Normalization in Transformers\nModel IWSLT14 PTB\nTransformerBN 34.4 60.7\nTransformerBRN 34.7 58.3\nTransformerMABN 34.9 57.2\nTransformerLN 35.5 53.2\nTransformerGN 35.7 51.7\nTransformerPN-V 35.5 55.3\nTransformerPN 35.9 47.6\nTable 4. (Left) NMT performance (BLEU) on IWSLT14 De-En. (Right) LM performance (Test PPL) on PTB.\nPN still performs the best in the two tasks, which may validate the effectiveness of our method.\nC. Theoretical Results\nIn this section, we discuss the theoretical results on BN and PN. We assume γand βto be constants for our analysis on BN,\nPN-V and PN.\nSince the derivative of loss Lw.r.t. Y is known as BL\nBY , trivially, we will have γd BL\nB|X “ BL\nBY . Also, it is not hard to get the\nfollowing fact.\nFact 5. The derivatives of µB and σ2\nB w.r.t.xi are\nBµB\nBxi\n“ 1\nB and Bσ2\nBxi\n“ 2\nBpxi ´µBq. (14)\nWe are now ready to show the derivative ofLw.r.t. xi under BN.\nLemma 6 (Derivative of Lw.r.t. xi in BN). Based on the Fact 5, it holds that\nBL\nBxi\n“ 1\nσB\nBL\nBˇxi\n´ 1\nσBB\nÿ\njPB\nBL\nBˇxj\np1 `ˇxjˇxiq. (15)\nProof. Based on chain rule, we will have\nBL\nBxi\n“ BL\nBˇxi\nBˇxi\nBxi\n`\nÿ\njPB\npBL\nBˇxj\nBˇxj\nBµB\nBµB\nBxi\n` BL\nBˇxj\nBˇxj\nBσB\nBσB\nBxi\nq\n“ 1\nσB\nBL\nBˇxi\n`\nÿ\njPB\nBL\nBˇxj\npBˇxj\nBµB\n1\nB ` Bˇxj\nBσ2\nB\n2\nBpxi ´µBqq\n“ 1\nσB\nBL\nBˇxi\n´ 1\nσBB\nÿ\njPB\nBL\nBˇxj\np1 `xi ´µB\nσB\nxj ´µB\nσB\nq\n“ 1\nσB\nBL\nBˇxi\n´ 1\nσBB\nÿ\njPB\nBL\nBˇxj\np1 `ˇxjˇxiq.\n(16)\nReplacing BL\nB|X by γd BL\nBY , we can get Eq. 3.\nIn the following, we will ﬁrst discuss the theoretical properties of PN-V in Appendix C.1; and then we discuss how to use\nrunning statistics in the forward propagation and how to modify the corresponding backward propagation in Appendix C.2.\nC.1. Proof of PN-V\nBefore showing the gradient of Lw.r.t. xi under PN-V, we note the following fact, which is not hard to establish.\nPowerNorm: Rethinking Batch Normalization in Transformers\nFact 7. The derivatives of ψB w.r.t.xi are,\nBψB\n2\nBxi\n“ 2\nBxi. (17)\nWith the help of Fact 7, we can prove the following lemma\nLemma 8 (Derivative of Lw.r.t. xi in PN-V) . Based on the Fact 7, it holds that that\nBL\nBxi\n“ 1\nψB\nBL\nBˆxi\n´ 1\nBψB\nÿ\njPB\nBL\nBˆxj\nˆxjˆxi. (18)\nProof. Based on chain rule, we will have\nBL\nBxi\n“ BL\nBˆxi\nBˆxi\nBxi\n`\nÿ\njPB\nBL\nBˆxj\nBˆxj\nBψB\n2\nBψB\n2\nBxi\n“ BL\nBˆxi\nBˆxi\nBxi\n`\nÿ\njPB\nBL\nBˆxj\np´1\n2\nxj\nψB\n3 q2xi\nB\n“ 1\nψB\nBL\nBˆxi\n´ 1\nBψB\nÿ\njPB\nBL\nBˆxj\nˆxjˆxi.\n(19)\nReplacing BL\nBxX by γd BL\nBY , we can get Eq. 6.\nIn order to show the effect of PN-V on the Lipschitz constant of the loss, we make the following standard assumption, as\nin (Santurkar et al., 2018).\nAssumption 9. Denote the loss of the non-normalized neural network, which has the same architecture as the PN-V\nnormalized neural network, as pL. We assume that\nBL\nByi\n“ BpL\nBxi\n, (20)\nwhere yi is the i-th row of Y .\nBased on these results, we have the following proof of Lemma 2, which was stated in Section 4.\nProof of Lemma 2. Since all the computational operator of the derivative is element-wise, here we consider d “ 1 for\nnotational simplicity6. When d“1, Lemma 8 can be written as\nBL\nBxi\n“ 1\nψB\nBL\nBˆxi\n´ 1\nBψB\nxBL\nBxX\n,xXyˆxi. (21)\nTherefore, we have\nBL\nBX “ 1\nψB\nBL\nBxX\n´ 1\nBψB\nxBL\nBxX\n,xXyxX. (22)\nSince\n}xX}2 “\nř\niPB ˆxi\n1\nB\nř\niPB ˆxi\n“B, (23)\n6For dě2, we just need to separate the entry and prove them individually.\nPowerNorm: Rethinking Batch Normalization in Transformers\nthe following equation can be obtained\n}BL\nBX }2 “ 1\nψB\n2 }BL\nBxX\n´x BL\nBxX\n,\nxX?\nB\ny\nxX?\nB\n}2\n“ 1\nψB\n2\n`\n}BL\nBxX\n}2 ´2xBL\nBxX\n,xBL\nBxX\n,\nxX?\nB\ny\nxX?\nB\ny`}x BL\nBxX\n,\nxX?\nB\ny\nxX?\nB\ny}2˘\n“ 1\nψB\n2\n`\n}BL\nBxX\n}2 ´x BL\nBxX\n,\nxX?\nB\ny2˘\n“ γ2\nψB\n2\n`\n}BL\nBY }2 ´x BL\nBY ,\nxX?\nB\ny2˘\n“ γ2\nψB\n2\n`\n}BpL\nBX }2 ´x BpL\nBX ,\nxX?\nB\ny2˘\n.\n(24)\nC.2. Proof of PN\nIn order to prove that after the replacement of BL\nBpXptqq with Eq. 12, the gradient of the input is bounded, we need the\nfollowing assumptions.\nAssumption 10. We assume that\n}ˆxi}ď C1 and }BL\nBˆxi\n}ď C2, (25)\nfor all input datum point and all iterations. We also assume that the exponentially decaying average of each element of ˆxi is\nbounded away from zero,\np1 ´αq\ntÿ\nj“0\nαt´jˆxiˆxi ąC3 ą0, @t, (26)\nwhere we denote αas the decay factor for the backward pass. In addition, we assume that αsatisﬁes\npC1q2 ă 1\n1 ´α. (27)\nW.l.o.g., we further assume that every entry ofψptqis bounded below, i.e.\nC0 ăψptq, @t. (28)\nIf we can prove or νptqis bounded by some constant C4 (the ofﬁcial proof is in Lemma 11), then it is obvious to prove the\neach datum point of ĂX1is bounded.\nBased on these results, we have the following proof of Theorem 4, which was stated in Section 4.\nPowerNorm: Rethinking Batch Normalization in Transformers\nProof of Theorem 4. It is easy to see that\n}ĂX1\ni,:}2 “} BL\nBˆxptq\ni\n´νpt´1qˆxptq\ni }2\n“x BL\nBˆxptq\ni\n´νpt´1qˆxptq\ni , BL\nBˆxptq\ni\n´νpt´1qˆxptq\ni y\n“} BL\nBˆxptq\ni\n}2 `}νpt´1qˆxptq\ni }2 ´2x BL\nBˆxptq\ni\n,νpt´1qˆxptq\ni y\nď} BL\nBˆxptq\ni\n}2 `}νpt´1q}2}ˆxptq\ni }2 ´2x BL\nBˆxptq\ni\n,νpt´1qˆxptq\ni y\nď} BL\nBˆxptq\ni\n}2 `}νpt´1q}2}ˆxptq\ni }2 `2} BL\nBˆxptq\ni\n}}νpt´1qˆxptq\ni }\nď} BL\nBˆxptq\ni\n}2 `}νpt´1q}2}ˆxptq\ni }2 `2} BL\nBˆxptq\ni\n}}νpt´1q}}ˆxptq\ni }\nďpC2q2 `pC1q2pC4q3 `C1C2C4\nAll these inequalities come from Cauchy-Schwarz inequity and the fact that\npa1b1q2 `...`padbdq2 ďpa2\n1 `...`a2\ndqpb2\n1 `...`b2\ndq.\nIn the ﬁnal step of Theorem 4, we directly use that νptqis uniformly bounded (each element of νptqis bounded) by C4. The\nexact proof is shown in below.\nLemma 11. Under Assumption 10, νptqis uniformly bounded.\nProof. For simplicity, denote BL\nBˆxi\nas ˆx1\ni. It is not hard to see,\n}Γptq}2 “ 1\nB2 }\nBÿ\ni“1\nˆxptq\ni ˆxptq\ni }2\n“ 1\nB2 x\nBÿ\ni“1\nˆxptq\ni ˆxptq\ni ,\nBÿ\ni“1\nˆxptq\ni ˆxptq\ni y\nď 1\nB2 pB2 max\nj\ntxˆxptq\nj ˆxptq\nj ,ˆxptq\ni ˆxptq\ni yuq\nďpC1q2.\nSimilarly, we will have }Λptq}ď C1C2 as well as p1 ´αqřt\nj“0 αt´jΓpjqΓpjq ąC3. We have\nνptq “p1 ´p1 ´αqΓptqqνpt´1q`p1 ´αqΛptq\n“p1 ´p1 ´αqΓptqqpp1 ´p1 ´αqΓpt´1qqνpt´2q`p1 ´αqΛpt´1qq`p 1 ´αqΛptq\n...\n“p1 ´αq\ntÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq.\nThen,\n1\np1 ´αq2 }νptq}2 “x\ntÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq,\ntÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jqy.\nPowerNorm: Rethinking Batch Normalization in Transformers\nNotice that with the deﬁnition,\nΓpmq “ 1\nB\nBÿ\ni“1\nˆxpmq\ni ˆxpmq\ni , (29)\nwe will have that all entries of Γpmq are positive, for m P t0,1,...,t u. It is clear that when all entries of Λpmq, for\nmPt0,1,...,t u, have the same sign (positive or negative), the above equation achieves its upper bound. W.l.o.g., we assume\nthey are all positive.\nSince 0 ăαă1, it is easy to see that, when K “rplogpp1´αqC3\n2C1C1\nq{logpαqqs, then the following inequality holds,\np1 ´αq\n8ÿ\nj“K\nαj ă C3\n2C1C1\n. (30)\nSince }Γpkq}ď C1, the value of any entry of Γpkqis also bounded by C1. Therefore, based on this and Eq. 30, when tąK,\nwe will have\np1 ´αq\ntÿ\nk“t´K`1\nαt´k ΓpkqΓpkq “p1 ´αq\ntÿ\nk“0\nαt´k ΓpkqΓpkq´p1 ´αq\nt´Kÿ\nk“0\nαt´k ΓpkqΓpkq\nąC3⃗1 ´p1 ´αq\nt´Kÿ\nk“0\nαt´k }Γpkq}}Γpkq}\nąC3⃗1 ´p1 ´αqC1C1⃗1\nt´Kÿ\nk“0\nαt´k\n“C3⃗1 ´p1 ´αqC1C1⃗1\ntÿ\nk“K\nαk\nąC3⃗1 ´p1 ´αqC1C1⃗1\n8ÿ\nk“K\nαk\nąC3⃗1 ´C3⃗1\n2\n“ C3⃗1\n2 ,\n(31)\nwhere ⃗1 is the unit vector. Then, for tąK, we can bound from below the arithmetic average of the Kcorresponding items\nof Γ,\npC1q2 ą 1\nK\nK´1ÿ\nk“0\nΓpt´kqΓpt´kq ą 1\nαK´1\nK´1ÿ\nk“0\nαkΓpt´kqΓpt´kq\n“ 1\nαK´1\ntÿ\nk“t´K`1\nαt´1ΓpkqΓpkq\ną C3\n2p1 ´αqαK´1 “C5 ą0.\n(32)\nThis inequality shows that after the ﬁrst K items, for any K consecutive Γpkq, the average of them will exceeds a constant\nnumber, C5. Therefore, for any tąT ąK, we will have\n1\nT ´K\nT´Kÿ\nk“0\nΓpt´kqΓpt´kq ąt T ´K\nK upK 1\nT ´KqC5 ą C5\n2 . (33)\nLet us split řt\nj“0\n`śj´1\nk“0p1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jqinto two parts: (i) řt\nj“K\n`śj´1\nk“0p1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq,\nand (ii) řK´1\nj“0\n`śj´1\nk“0p1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq. From so on, we will discuss how we deal with these two parts\nrespectively.\nPowerNorm: Rethinking Batch Normalization in Transformers\nCase 1: řt\nj“K\n`śj´1\nk“0p1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq Notice that for 0 ăaj ă1, the following inequality can be proven\nwith simply induction,\nk´1ź\nj“0\np1 ´ajqďp 1 ´1\nk\nk´1ÿ\nj“0\nαjqk. (34)\nReplacing aj with p1 ´αqΓpt´j`1q, we will have\ntÿ\nj“K\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq ď\ntÿ\nj“K\n`\np1 ´p1 ´αq\nj\nj´1ÿ\nk“0\nΓpt´k`1qq\n˘j\nΛpt´jq\nď\ntÿ\nj“K\n`\np1 ´p1 ´αqC5\n2 q\n˘j\nΛpt´jq\nď\ntÿ\nj“K\n`\np1 ´p1 ´αqC5\n2 q\n˘j\nC1C2\nď 2\np1 ´αqC5\nC1C2 “C6.\n(35)\nHere the second inequality comes from Eq. 33, and the third inequality comes form the the fact each entry ofΛpmqis smaller\nthan C1C2, given }Λpmq}ď C1C2. The ﬁnal inequality comes from Eq. 31, where 0 ăC5 ăpC1q2 ă1{p1 ´αq, then we\ncan have 0 ă\n`\n1 ´p1 ´αqC5{2\n˘\nă1.\nCase 2: řK´1\nj“0\n`śj´1\nk“0p1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq It is easy to see\nK´1ÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq ď\nK´1ÿ\nj“0\n`j´1ź\nk“0\np⃗1q\n˘\nΛpt´jq\nďKC1C2.\n(36)\nCombining Case 1 and 2, we have\n1\np1 ´αq2 }νptq}2 “x\ntÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jq,\ntÿ\nj“0\n`j´1ź\nk“0\np1 ´p1 ´αqΓpt´k`1qq\n˘\nΛpt´jqy\nďxC6⃗1 `KC1C2⃗1,C6⃗1 `KC1C2⃗1yă C7,\n(37)\nwhich indicates }νptq}is bounded and C4 “p1 ´αq?C7.",
  "topic": "Normalization (sociology)",
  "concepts": [
    {
      "name": "Normalization (sociology)",
      "score": 0.7550361752510071
    },
    {
      "name": "Transformer",
      "score": 0.5945984125137329
    },
    {
      "name": "Computer science",
      "score": 0.5874610543251038
    },
    {
      "name": "Backpropagation",
      "score": 0.567211925983429
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5585148930549622
    },
    {
      "name": "Artificial neural network",
      "score": 0.49547064304351807
    },
    {
      "name": "Algorithm",
      "score": 0.38025450706481934
    },
    {
      "name": "Machine learning",
      "score": 0.3475974500179291
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.324779748916626
    },
    {
      "name": "Voltage",
      "score": 0.16299661993980408
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1297971548",
      "name": "International Computer Science Institute",
      "country": "US"
    }
  ],
  "cited_by": 16
}