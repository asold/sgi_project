{
    "title": "The Trade-offs of Domain Adaptation for Neural Language Models",
    "url": "https://openalex.org/W3199990076",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2959607770",
            "name": "David Grangier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2564041968",
            "name": "Dan Iter",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1848260265",
        "https://openalex.org/W2885771996",
        "https://openalex.org/W4250589301",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2117278770",
        "https://openalex.org/W4294635920",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2597603852",
        "https://openalex.org/W2796514099",
        "https://openalex.org/W1905522558",
        "https://openalex.org/W2107741520",
        "https://openalex.org/W2156876426",
        "https://openalex.org/W3101656801",
        "https://openalex.org/W2145147745",
        "https://openalex.org/W4239943352",
        "https://openalex.org/W1971735090",
        "https://openalex.org/W2962890089",
        "https://openalex.org/W2892283076",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3087148478",
        "https://openalex.org/W3211848854",
        "https://openalex.org/W3127307050",
        "https://openalex.org/W4322588812",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W1574901103",
        "https://openalex.org/W2963366389",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2914746235",
        "https://openalex.org/W17525587",
        "https://openalex.org/W2573665256",
        "https://openalex.org/W2162867699",
        "https://openalex.org/W4289543315",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963933682",
        "https://openalex.org/W2963371670",
        "https://openalex.org/W2949847538",
        "https://openalex.org/W3034640977",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W1916559533",
        "https://openalex.org/W3023528699",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2113651538",
        "https://openalex.org/W2795282075",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3198953535",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3014390292",
        "https://openalex.org/W2962863357",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2296319761",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W3134228182"
    ],
    "abstract": "This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pre-training before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3802 - 3813\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nThe Trade-offs of Domain Adaptation\nfor Neural Language Models\nDavid Grangier\nGoogle, Mountain View, CA\ngrangier@google.com\nDan Iter∗\nStanford, Palo Alto, CA\ndaniter@stanford.edu\nAbstract\nThis work connects language model adapta-\ntion with concepts of machine learning theory.\nWe consider a training setup with a large out-\nof-domain set and a small in-domain set. We\nderive how the beneﬁt of training a model on\neither set depends on the size of the sets and\nthe distance between their underlying distri-\nbutions. We analyze how out-of-domain pre-\ntraining before in-domain ﬁne-tuning achieves\nbetter generalization than either solution inde-\npendently. Finally, we present how adapta-\ntion techniques based on data selection, such\nas importance sampling, intelligent data selec-\ntion and inﬂuence functions, can be presented\nin a common framework which highlights their\nsimilarity and also their subtle differences.\n1 Introduction\nNeural Language Models (LMs) trained on large\ngeneric training sets – over a billion sen-\ntences (Kaplan et al., 2020; Roziewski and\nKozłowski, 2021) – have been shown to be ef-\nfective at adapting to smaller, speciﬁc target do-\nmains for language modeling and other down-\nstream tasks (Bommasani et al., 2021). Neural LM\nadaptation is commonly performed via ﬁne tun-\ning (Devlin et al., 2018; Liu et al., 2019; Raffel\net al., 2019; Radford et al., 2019), data selection\n(van der Wees et al., 2017) or their combination\n(Wang et al., 2018; Aharoni and Goldberg, 2020;\nGururangan et al., 2020). However, the trade-\noffs between ﬁne-tuning and reweighting of pre-\ntraining data is not well understood and a theoreti-\ncal framework for reasoning about the generaliza-\ntion performance of these methods is needed.\nIn this paper, we connect language model adap-\ntation with concepts of machine learning theory.\nOur derivations support past empirical observa-\ntions: it has been observed that the size of the\nout-of-domain pre-training set is important for in\n∗Work performed while interning at Google.\ndomain generalization (Raffel et al., 2019; Devlin\net al., 2018) or that domain adaptation is more ef-\nfective on domains which are well represented in\nthe the pre-training data (Radford et al., 2019).\nOur study consider a training setup with a large\nout-of-domain set and a small in-domain set. As\na ﬁrst contribution, we derive how the beneﬁt of\ntraining a model on either set depends on the size\nof the sets and the distance between their underly-\ning distribution. We also expose how ﬁne-tuning\ncan be viewed as a regularization method that can\nachieve a better trade-off than training only on ei-\nther set.\nThe research on data selection for LM adap-\ntion originates mainly from intelligent selec-\ntion (Moore and Lewis, 2010; Axelrod et al.,\n2011). This method examines the out-of-domain\ntraining data to emphasize a subset deemed more\nlikely by an in-domain model than by an out-of-\ndomain model. Although intuitive, the connection\nof this method with statistical estimation is un-\nclear, which makes studying its impact on general-\nization error difﬁcult. Another family of selection\nmethods stems from inﬂuence functions (Koh and\nLiang, 2017; Wang et al., 2021) which estimate\nwhether the model updates from out-of-domain\ntraining examples are aligned with the in-domain\nupdates. This approach is more principled and\nits impact on the generalization error is easier to\nstudy. In this work, as a second contribution, we\nshow how intelligent selection and inﬂuence func-\ntion methods are linked in the case of neural LMs.\nIn particular, we show that they both can be de-\nrived from importance sampling (Owen, 2013), a\nclassical, well-studied statistical estimation tech-\nnique.\nThe rest of our paper is organized as follows.\nWe ﬁrst presents the theoretical trade-offs between\nin-domain and out-of-domain training. We high-\nlight the importance of the relative sizes of in-\ndomain and out-of-domain training sets along with\n3802\nthe distance between their underlying distribu-\ntions. We also present how ﬁne-tuning with a\nlimited number of updates can be seen as a train-\ning method regularized with respect to the out-of-\ndomain prior. Finally, we present data selection\nmethods under a unifying framework.\n2 Neural Language Modeling\nLanguage modeling refers to the generative mod-\neling of natural language (Manning and Schutze,\n1999). Commonly, natural language is represented\nas a sequence of symbols, tokens, from a ﬁnite\nvocabulary. For instance, language can be rep-\nresented as a sequence of characters, a sequence\nof words or alternative units. A neural language\nmodel (LM) decomposes the estimates the log\nprobability of a text y= (y1,...y n), as\nlog P(y; θ) =\nn∑\ni=1\nlog P(yi|yi−1\n1 ; θ)\nwhere Pθ maps a parameter vector θ along with\na sequence of past tokens yi−1\n1 onto a probability\ndistribution over the vocabulary. Different types of\nneural architectures have been used for neural lan-\nguage modeling. Most architectures used for LMs\nre-use intermediate computations from the previ-\nous steps for the next steps when estimating proba-\nbilities for successive tokens in the same sequence.\nPopular architectures include recurrent neural net-\nworks (Mikolov et al., 2010; Sundermeyer et al.,\n2012), convolutional networks (Dauphin et al.,\n2017) and transformer networks (Vaswani et al.,\n2017; Radford et al., 2019).\nThe parameter vector θ ∈Θ of a neural LM is\nidentiﬁed by maximizing the log likelihood over a\ntraining set D sampled from the true distribution\nDusing variants of stochastic gradient descent.\nThe log likelihood of a held-out set, sampled from\nthe same distribution, can evaluate model qual-\nity. One often reports perplexity, the exponenti-\nated negative average log likelihood per token.\nConditional LMs model the distribution of a\ntext ygiven a conditioning input x.\nlog P(y|x; θ) =\nn∑\ni=1\nlog P(yi|yi−1\n1 ,x; θ)\nThis type of model is used for translation where\n(x,y) pairs are sentences in the source and target\nlanguage (Koehn, 2009; Bahdanau et al., 2015) or\nsummarization where (x,y) pairs are correspond-\ning articles and summaries (See et al., 2017).\nFor both conditional and regular LMs, the size\nof the training data is important to achieve a low\nheld-out perplexity. This is an obstacle for do-\nmains with limited available training data. This\nissue has led to various model adaptation ap-\nproaches. These methods leverage large amounts\nof generic training data D along with a small\namount of target domain training data T from the\ndomain of interest. Fine tuning is a popular do-\nmain adaptation method which trains a neural lan-\nguage model in two phases, ﬁrst maximizing the\nlikelihood of the generic set D (pre-training) be-\nfore optimizing the likelihood of the target do-\nmain set T (ﬁne-tuning). As an alternative to\nﬁne-tuning, some methods consider leveraging the\nsmall target-domain training set to identify and\nemphasize similar data in the larger generic train-\ning set. These emphasis methods can be used\nindividually or in conjunction with ﬁne-tuning.\nEmphasis methods include importance sampling,\ncontrastive data selection and inﬂuence functions.\nThis paper shows that these methods – although\nproposed in different context – can be presented in\na uniﬁed way which allows light to be cast on their\nsubtle differences.\n3 Training Strategies\nThis section ﬁrst examines in-domain training,\ni.e. when the training and test data are sampled\nfrom the same distribution. It then studies out-of-\ndomain training, i.e. when the training and test\ndata distribution differs. Finally, it examines out-\nof-domain pre-training followed by in-domain ﬁne\ntuning. For the three cases, we decompose the loss\nrelying on classical concepts from learning theory\nand study the trade-offs involved in each setup.\n3.1 In-Domain Training\nGiven a training setDsampled from a distribution\nD, learning an LM typically aims at minimizing\nthe negative log-likelihood of D, also referred to\nas the cross-entropy loss i.e.\nL(θ;D) =−1\n|D|\n∑\ny∈D\nlogP(y|θ) = E\ny∼D\n[−logP(y|θ)].\nThis empirical risk is the average over the ﬁnite\nset D, which acts as a proxy for the expectation\n3803\nover the true, unavailable distribution P(y|D),\nL(θ; D) = −\n∑\ny∈Ω\nlog P(y|θ)P(y|D)\n= E\ny∼D\n[−log P(y|θ)],\nwhere the distribution’s support Ω is the set of\nall ﬁnite sequences. The true expected loss is\nbounded by the entropy of the distributionP(·|D),\ni.e.\nL(θ; D) ≥LH(D) = H(P(·|D))\nsince H(P(·|D)) = min qEy∼D[−log q(y)].The\ngap between the best likelihood from a neural net-\nwork with the chosen parameterization and the en-\ntropy is called the approximation error\nLapp(D,Θ) = min\nθ∈Θ\nL(θ; D) −H(P(·|D)).\nThis gap accounts for the fact that P(·|D) gen-\nerally cannot be represented by a parameterized\nfunction from the chosen family spanned by Θ.\nIn addition to the approximation error, one should\nconsider the estimation error to account that one\nrelies on the empirical risk from the ﬁnite set D,\nLest(D,Θ,D) = L(θD; D) −min\nθ\nL(θ; D)\nwith θD = arg minθ∈Θ L(θ; D). Therefore, the\nloss of θD over Ddecomposes as (Bottou and\nBousquet, 2007)\nL(θD; D) =\nLH(D) + Lapp(D,Θ) + Lest(D,Θ,D) (1)\nwhere the three terms accounts for the intrinsic un-\ncertainty of D, the chosen neural architecture and\nthe ﬁnite training set Drespectively.\nThe approximation error Lapp(D,Θ) depends\non the selected model family Θ. It can be reduced\nby selecting a more expressive family, i.e. a neu-\nral architecture with more capacity, a largerΘ, e.g.\narchitectures with more, wider layers. The esti-\nmation error Lest(D,Θ,D) depends both on the\nselected model family Θ and the size of the train-\ning data D. Increasing model capacity will result\nin a higher estimation error for the same training\nset size, but training over a larger training set will\ndecrease estimation error. Therefore, for a given\ntraining set size, capacity needs to be chosen to\nidentify a good trade-off between the two error\ntypes.\nTwo important properties of neural networks\nneed to be kept in mind when examining this\ntrade-off. The universal approximation prop-\nerty (Lecun, 1987; Funahashi, 1989) means that\nfor any approximation error ϵand any distribution\nD, there exists a capacity setting C(ϵ,D) at which\na neural network θ ∈C(ϵ,D) whose error is be-\nlow ϵ, i.e.\n∀ϵ> 0,∃Cs.t. Lapp(D,C) ≤ϵ.\nIn layman terms, the universal approximation\nproperty means that for sufﬁciently large capac-\nity settings, the approximation error can become\narbitrary low. The statistical consistency property\nmeans that for any ϵ,ϵ′ > 0, there exist a train-\ning set size N(ϵ,D) such that sampling a training\nset of size N(ϵ,ϵ′,D) from Dwill result in an es-\ntimation error less than ϵ′with probability 1 −ϵ,\n∀ϵ,ϵ′>0,∃N s.t ,\nP(D∼DN : Lest(D,Θ,D) <ϵ′) = 1 −ϵ\nIn layman terms, the statistical consistency prop-\nerty means that for sufﬁciently large training sets,\nthe probability to get an estimation error below\nany positive value can be arbitrary close to 1.\nUniversal approximation and consistency im-\nplies that, in the asymptotic case (i.e. as the size\nof Dtends to inﬁnity), the last two terms in Eq. 1\ncan be arbitrary close to zero with the appropri-\nate model capacity (with high probability). In that\ncase, the likelihood L(θD; D) amounts to the in-\ntrinsic entropy of Dwith the appropriate model\ncapacity.\n3.2 Out-of-Domain Training\nThis section considers a setup where one needs\na specialized language model in a domain T and\ntwo training sets are available: a small training set\nT sampled from Tand a large training set Dsam-\npled from D, a generic domain different from the\nspecialized domain.\nIn that context, the simplest options are either to\ntrain a model over T or Dalone. Training only on\nthe small set T results in the generalization loss\nL(θT; T)\n= LH(T) + Lapp(T,Θ) + Lest(T,Θ,T)\nwith θT = arg minθ∈Θ L(θ; T) as in the previous\nsection. Training on the larger set Dresults in\nL(θD; T)\n= LH(T) + Lapp(T,Θ) + Lest(T,Θ,D).\n3804\nTwo factors are important to compare these two\noptions: the size of the specialized set T relative\nto the size of the generic set D and the similarity\nbetween T and Ddistributions.\nWhen the T and Ddistributions are identical,\nD and T are sampled from the same distribution\nand training a model on the larger training setDis\nadvantageous. For a constant capacity, this option\nwill get a lower estimation error. When varying\ncapacity, one might identify a setting with an even\nbetter trade-off in the compound loss of Eq. (1)\nwith the larger training set D.\nWhen the distributions T and Ddiffer and the\nsize of D is ﬁxed, the size of T determines which\noption to prefer. Statistical consistency means that\nLest(T,Θ,T) will converge to zero in probability\nas the size of T grows. This means that when the\nsize of T is greater than N(ϵ,Lest(T,Θ,D),D),\nthe probability that training on T results in a bet-\nter generalization loss than training on D is above\n1 −ϵ.\nWhen the distributionsTand Ddiffer, the Kull-\nback–Leibler (KL) divergence between the two\ndistributions plays a key role.\nTheorem 1 The generalization of the loss of θD\nover T is upper bounded as\n∀ϵ> 0, ∃N s.t. ∀D∼Dn,\nL(θD; T) ≤H(T) + KL(T,D) + ϵ (2)\nwith probability 1 −ϵ. This bound justiﬁes the\nintuition that, if given the choice between two\ngeneric domains Dand D′, training over the one\nwith the lowest KL divergence to T will result in\na better asymptotic behaviour. The proof of this\nbound is presented in Appendix A.\n3.3 Fine-Tuning & Multitask Learning\nFine-tuning for domain adaptation trains a model\non a small in-domain set initializing optimiza-\ntion from the parameters of a model trained\non a large out-of-domain set. Formally, ﬁne-\ntuning minimizes L(θ; T) the loss over T for\na few steps, starting the optimization from\nθD = arg minθ∈Θ L(θ; D). This strategy implic-\nitly targets a trade-off between the empirical losses\nover T and D. This trade-off is controlled by\nthe number of ﬁne tuning steps nft. Few steps\nmeans that the identiﬁed parameters θft achieve\na low loss over D, while many steps expresses\nthat the parameters achieve a low loss over T.\nThis strategy leverages the regularization effect of\nearly stopping (Caruana et al., 2001), i.e. the so-\nlution found by gradient descent is guaranteed to\nbe in an Euclidean ball centered around the ini-\ntialization whose radius grows with the number of\nsteps (Grangier and Bengio, 2008), i.e.\n∥θft −θD∥2 ≤λnft gmax\nwhere λ refers to the (maximum) learning rate\nand gmax to an upper bound on the update norm.\nThe small distance between θft and θD guaran-\ntees that the loss L(θft; D) is close to the optimum\nL(θD; D) when θ →L(θ; D) is a smooth func-\ntion, e.g. a Lipschitz function.\nFor the basic ﬁne-tuning setup, several vari-\nants have been introduced. Some approaches (De-\nvlin et al., 2018; Liu et al., 2019; Raffel et al.,\n2019) consider leaving some parameters un-tuned\nor frozen which is the extreme case of regulariza-\ntion for these weights, penalizing any deviation\nfrom initialization. Other approaches consider in-\ntroducing novel (unregularized) weights for ﬁne\ntuning, often referred as adapter layers (Houlsby\net al., 2019; Stickland et al., 2019; Pfeiffer et al.,\n2020). Other forms of regularization, such as\ndropout, have also been considered in conjunction\nwith ﬁne tuning (Miceli Barone et al., 2017).\nThe selection of the regularization strength\nin ﬁne-tuning is computationally efﬁcient since\nit successively visits an optimization path from\nthe most regularized model ( θD trained only on\nD, Sec. 3.2) to the unregularized θT (Sec. 3.1).\nThis is more efﬁcient compared to explicit reg-\nularization methods, including multitask learn-\ning (Caruana, 1998; Collobert and Weston,\n2008; Pilault et al., 2021), i.e. optimizing\nLmulti(θ; T,D,α ) = L(θ; T) + αL(θ; D).\n4 Data Selection\nData selection aims to improve out-of-domain\ntraining by selecting or giving stronger weights\nto some data points. The identiﬁcation of these\npoints aims to emphasize out-of-domain exam-\nples which have an impact on the model similar\nto the impact of the in-domain training examples.\nWe study three independently proposed selection\nmethods, importance sampling, contrastive data\nselection and inﬂuence functions. We show that\nthese methods all train models through weighted\nlog-likelihood training,\nL(θ; D,T,w ) = − 1\n|D|\n∑\ny∈D\nw(y; T,D) logP(y|θ)\n3805\nbut introduce their weights w(y; T,D) with dif-\nferent justiﬁcations. Despite these differences, we\nshow that these methods result in surprisingly sim-\nilar selection weights in the speciﬁc case of neural\nlanguage models.\nData selection is particularly suited when the\nout-of-domain training distribution and the test\ndistribution have a large KL divergence but the\nout-of-domain training set is large. In that case,\nthe generalization of a model trained on out-of-\ndomain data is poor due to the large KL divergence\nbetween T and D, see Eq. (2). When this KL di-\nvergence is large but out-of-domain data is abun-\ndant, data selection methods propose to select a\nsubset of the out-of-domain data DT ⊂D. Ide-\nally, the training loss over such a subset L(θ,DT)\nwould be a better proxy for the generalization loss\nover T, L(θ,T), than the training loss over the full\nset D, L(θ,D).\nSelection involves a delicate trade-off though.\nOne one hand, data selection is attractive since it\nreplaces the training set with another set closer to\nthe test domain. On the other hand, this training\nset is smaller, which increases the impact of esti-\nmation errors. Additionally, data selection is im-\nperfect since the target domain distribution T is\nonly known through a small target training set T.\nThis section successively presents importance\nsampling, contrastive data selection and inﬂuence\nfunctions and connect them into a single frame-\nwork.\n4.1 Importance Sampling\nAlthough intelligent selection also called con-\ntrastive data selection is more common (Moore\nand Lewis, 2010; Wang et al., 2018), we ﬁrst ex-\namine importance sampling since this method will\nguide our understanding of other selection meth-\nods.\nImportance sampling is a generic statistical\ntechnique (Owen, 2013). In our case, it can\nbe used to estimate the expectation of the cross-\nentropy loss over T while having access to sam-\nples from D. It relies on the identity\nL(θ; T) = E\ny∼T\n[−log P(y|θ)]\n= −\n∑\ny∈Ω\nlog P(y|θ)P(y|T)\n= −\n∑\ny∈Ω\nlog P(y|θ)P(y|T)\nP(y|D)P(y|D)\n= E\ny∼D\n[−w(y; T,D) logP(y|θ)]\nwhere w(y; T,D) = P(y|T)\nP(y|D) , assuming full sup-\nport on D, i.e. ∀y∈Ω, P(y|D) > 0. In practice,\none has not access toTand Dbut to ﬁnite samples\nT and D. With importance sampling, we can con-\nsider two alternative estimators of L(θ; T), either\nthe empirical risk over T,\nL(θ; T) = −1\n|T|\n∑\ny∈T\nlog P(y|θ)\nor the mean of the importance weighted cross en-\ntropy over D, i.e.\nLimp(θ; D,T, ˆw) = − 1\n|D|\n∑\ny∈D\nˆw(y; T,D) logP(y|θ)\nwhere ˆwestimates of the weightswfrom the train-\ning sets Dand T. The trade-off between these two\nestimators depends on the relative size ofT and D,\nthe imbalance of the weights wand the quality of\ntheir estimate ˆw.\nImportance sampling is interesting when the\ngeneralization error L(θimp(D,T); T) of the model\nθimp(D,T) = arg min\nθ\nLimp(θ; D,T, ˆw)\nis less than the generalization error of θT selected\nby minimizing L(θ; T), i.e. classical empirical\nrisk minimization. This error decomposes as,\nL(θimp(D,T); T)\n= LH(T) + Lapp(T,Θ) + Limp\nest (T,Θ,D,T ).\nWe further decompose the estimation error in two\nterms,\nLimp\nest (T,Θ,D,T )\n= Lest/w(T,D,Θ,D) + Lest/ˆ w(T,Θ,D,T )\nwhere Lest/w(T,D,Θ,D) refers to the estimation\nerror resulting from the ﬁnite size of D, assum-\ning access to the true importance weights, and\n3806\nLest/ˆ w(T,Θ,D,T ) isolate the residual error re-\nsulting from the estimation of w. We have\nLest/w(T,D,Θ,D)\n= L(θimp(D,D); D) −min\nθ\nL(θ; T),\nLest/ˆ w(T,Θ,D,T )\n= L(θimp(D,T); D) −L(θimp(D,T); D)\nwith θimp(D,D) = arg minθLimp(θ; D,T, ˆw)\nThe ﬁrst term depends on the size of D and\nthe imbalance of the weights. For instance, if the\nweights are mostly concentrated over a small sub-\nset of D, this estimation error will be high. If\nthis subset is smaller than T, estimation errors\nfrom Limp(θ; D,T, ˆw) will be higher than from\nL(θ; T). The notion of effective sample size has\nbeen deﬁned to quantify this effect (Kish, 1965).\nIt is deﬁned by examining the variance of the\nweighted sum of nindependent random variables\nZi with mean µZ and variance σ2\nZ, Sw =\n∑\niwiZi∑\niwi\n.\nThis variance is\nσ2\nSw =\n∑\niw2\ni\n(∑\niw)2 σ2\nZ\nwhich can be compared to σ2\nS = 1\nnσ2\nZ in the un-\nweighted case. This means that the weighted sum\nvariance matches the variance of an unweighted\ncase with\nne = (∑\niw)2\n∑\niw2\ni\n.\nAssuming that losses over Dand T have compa-\nrable means and variances, the expected loss esti-\nmate with importance weighting overDhas lower\nvariance than the mean over T only when,\nne = (w)2\nw2 |D|≫| T|\nwhere w = 1\n|D|\n∑\ny∈Dw(y) and w2 =\n1\n|D|\n∑\ny∈Dw2(y) are the sample mean and vari-\nance of the weights over D. This means\nthat the ﬁrst term in the estimation error is\nLest/w(T,Θ,D,T ) advantageous compared to\nthe estimation error from classical empirical risk\nminimization over T when T is small.\nUnfortunately, the second estimation error\nterm Lest/ˆ w(T,Θ,D,T ) gets larger as T gets\nsmaller since estimating the importance weights\nw(y; T,D) = P(y|T)\nP(y|D) from data is challenging\nwhen T is small. One can remark that language\nmodeling is actually the very problem of identify-\ning a model to estimate the probabilities in this ra-\ntio, P(y|T) and P(y|D), from ﬁnite samples from\nthe distributions T and D. Discriminative classi-\nﬁers are also relevant to estimate this ratio since\nw(y; T,D) ∝P(T|y)\nP(D|y).\nIn fact the multiplying constant (prior ratio) does\nnot matter since multiplying the weighted loss by\na positive constant has no impact on optimization.\nWhen importance weights are estimated with an\nLM, one can estimate P(·|T) by ﬁne tuning on T\na model pre-trained on D. The number of tun-\ning steps nft gives controls on ∥θft −θD∥. When\nnft = 0 , ˆw = 1 and the importance sampling\nloss corresponds to L(θ,D). As nft grows, the\nestimate P(y|θft) could overﬁt and assigns most\nof the probability mass to a small neighborhood\naround samples in T. The weights ˆw will in turn\nbe concentrated in this small neighborhood, mak-\ning the minimizer of the importance sampling loss\nclose to the minimizer of the empirical loss over\nT. Therefore, ﬁne-tuning a language model for\nestimating the importance weights allow to pro-\ngressively transition between the in-domain and\nthe out-of-domain empirical loss minimizers seen\nin Section 3.2. In the next sections, we refer to the\nestimated importance sampling weights as\nwimp\nD,T(y) = ˆw(y; T,D).\nImportance sampling has been used for model\ntraining for various application: either to im-\nprove training speed (Johnson and Guestrin, 2018;\nKatharopoulos and Fleuret, 2018) or to adapt to\na changing training distribution (Mahmood et al.,\n2014; Metelli et al., 2018). Importance sampling\nhas rarely been used to modify the training dis-\ntribution of language models (Foster et al., 2010;\nFernandez and Downey, 2018) as intelligent selec-\ntion methods are more common.\n4.2 Intelligent Selection\nIntelligent selection (Moore and Lewis, 2010; Ax-\nelrod et al., 2011) and contrastive data selection,\nits extension to neural networks (van der Wees\net al., 2017; Wang et al., 2018), have been intro-\nduced in the language modeling literature. We\nshow that these methods are closely related to im-\nportance sampling, even if their original papers\ndoes not mention this link.\n3807\nIntelligent selection selects training samples\nfrom an out-of-domain dataset according to the\nlog-odd between an in-domain LM and an out-of-\ndomain LM. Typically, a binary decision is taken\nper sentence by comparing the average log-odd to\na threshold τ,\nLIntSel(θ,D,T ) = −\n∑\ny∈D\nbIntSel\nD,T (y) logP(y|θ)\nwhere bIntSel\nD,T (y) is deﬁned as\nI {log P(y|θT) −log P(y|θD) >τ }. Com-\npared to importance sampling, the weights are\nbinarized, i.e.\nbIntSel\nD,T (y) = I\n{\nlog wimp\nD,T(y) >τ\n}\n.\nThe binarization decision was certainly driven by\nconvenience as most n-gram LM training pack-\nages did not support weighted likelihood opti-\nmization when intelligent selection was intro-\nduced. Binarization also has the advantage of\ndown-weighting extremely positive weight values\nfrom large log P(y|θT) due to over-ﬁtting on the\nsmall set T.\nMore recently, intelligent selection has been\nextended to neural models (van der Wees et al.,\n2017; Wang et al., 2018). Contrastive data selec-\ntion (Wang et al., 2018) suggests to ﬁne tune the\nin-domain model log P(y|θT) from log P(y|θD)\nand also observes that selection scores can efﬁ-\nciently be estimated from a model with a much\nsmaller capacity than the ﬁnal trained model. Dy-\nnamic selection (van der Wees et al., 2017) pro-\nposes to increase the selection threshold τt as\ntraining progresses, gradually transitioning from\ngeneric to in-domain training. This gradual adap-\ntation of neural network is related to curriculum\nlearning (Bengio et al., 2009) which studies the or-\ndering of examples and tasks during model train-\ning.\nIntelligent selection methods have been applied\nboth for unconditional models (language model-\ning) and conditional models (machine translation).\nIn the conditional case, intelligent selection com-\nputes\nbIntSel\nD,T (x,y) = I\n{\nlog wIntSel\nD,T (x,y) >τ\n}\nwith wIntSel\nD,T (x,y) = P(y|x,θT)\nP(y|x,θD).\nThis ratio of conditional probabilities is different\nfrom the ratio of joint probabilities stemming from\nimportance sampling, i.e.\nLimp(θ; D,T, ˆw) =\n− 1\n|D|\n∑\ny∈D\nP(x,y|T)\nP(x,y|D) log P(y|x,θ).\nThe two ratios match when P(x|T) = P(x|D)\nsince\nwimp\nD,T(x,y) = P(x,y|T)\nP(x,y|D)\n= P(x|T)\nP(x|D) wIntSel\nD,T (x,y).\nThe formulation of intelligent selection therefore\nneglects the domain mismatch from the input dis-\ntribution in the conditional case. This formula-\ntion aligns with the denoising goal (Wang et al.,\n2018) which assumes that Dcontains label noise,\ni.e. mistranslation in that case.\n4.3 Inﬂuence Functions\nAs mentioned above, importance sampling and\nintelligent selection weights can be estimated by\ncontrasting the log probabilities from a base model\nwith those from a ﬁne-tuned model. This use of\nﬁne-tuning connects intelligent selection to inﬂu-\nence function and gradient alignment techniques.\nInﬂuence functions (Koh and Liang, 2017; Pruthi\net al., 2020) have been used as a diagnostic tool\nto identify the training instances which support or\ncontradict with a given test label. This task is re-\nlated to the selection of training data when the ob-\njective is to ﬁnd instances in a generic training set\nD whose training updates increase the likelihood\nof a set T from a different domain.\nThe inﬂuence of a training pointyon a test point\ny′is deﬁned as\nI(y,y′) = −∂ℓ\n∂θ(y′; θ)⊤H−1\nθ\n∂ℓ\n∂θ(y; θ)\nwhere ℓ(y,θ) refers to the loss at y for a model\nwith parameters θand Hθ refers to the Hessian of\nthe model loss at θ. This quantity can be derived\nby considering the impact of reducing the weight\nof point yduring training on the test loss at y′. If\nwe increase the weight of a training example by ϵ,\nθD,ϵ = min\nθ\n1\n|D|\n∑\nz∈D\nℓ(z; θ) + ϵℓ(y; θ)\n3808\nFrom (Cook and Weisberg, 1982), we derive\n∂θD,ϵ\n∂ϵ\n⏐⏐⏐⏐\nϵ=0\n= −H−1\nθ\n∂ℓ\n∂θ(y; θ)\n⏐⏐⏐⏐\nθ=θD\n.\nComposing with the test loss on (x′,y′), we get\n∂ℓ(y′; θD,ϵ)\n∂ϵ\n⏐⏐⏐⏐\nϵ=0\n= −∂ℓ(y′; θ)⊤\n∂θ\n⏐⏐⏐⏐\nθ=θD\nH−1\nθ\n∂ℓ(y; θ)\n∂θ\n⏐⏐⏐⏐\nθ=θD\nwhich matches the expression of inﬂuence intro-\nduced above.\nWe now connect inﬂuence with the precedent\nsections on importance sampling and contrastive\ndata selection. We consider an LM with weights\nθD, trained on the generic training set D. Its ﬁrst\norder Taylor expansion at θD is\nlog P(y|θD + ∆θ) =\nlog P(y|θD) + ∆θ⊤g(y; θD) + O\n(\n∥∆θ∥2)\n(3)\nwhere g(y; θD) = ∂\n∂θ log P(y|θ)\n⏐⏐\nθ=θD\n. If the\nmodel pre-trained on D is ﬁne-tuned on T by\nperforming a single step of gradient descent with\nlearning rate λ, we get\nθT = θD −λ ∂\n∂θL(T; θ)\n⏐⏐⏐⏐\nθ=θD\n= θD + λ E\ny∼T\n[g(y; θD)] .\nIn that case, the log-odd of the two models there-\nfore has the following Taylor expansion,\nlog P(y|θT) −log P(y|θD)\n= λ E\ny′∼T\n[\ng(y′; θD)⊤g(y; θD)\n]\n+ O\n(\n∥θD −θT∥2)\n.\nIf we assume that the model’s Hessian is the iden-\ntity, Hθ = 1 , we therefore have\nlog P(y|θT) −log P(y|θD) =\n−λ E\ny′∼T\n[\nI(y,y′)\n]\n+ O\n(\n∥θD −θT∥2)\n.\nThe Hessian assumption might be dropped when\nthe model is ﬁne-tuned with a Newton-style up-\ndate (Boyd and Vandenberghe, 2014). The above\nrelation means that the negative mean inﬂuence of\na point y ∈D over the set T also corresponds to\nthe log of the estimated importance weights intro-\nduced in Section 4.1, i.e.\nlog wimp\nD,T(y) =\n−λ E\ny′∼T\n[\nI(y,y′)\n]\n+ O\n(\n∥θD −θT∥2)\n.\nOf course, this relation holds only in the case\nwhere a single gradient step is performed for ﬁne-\ntuning. This relation allows estimating the re-\nduction in test loss (here over T) when removing\ntraining samples from D with positive inﬂuence\nwhich is also the goal of intelligent data selection.\nThis strategy has been applied to label noise ﬁlter-\ning (Koh and Liang, 2017), class rebalancing (Ren\net al., 2018) and domain adaptation (Wang et al.,\n2021).\n4.4 Comparing Data Selection Methods\nOur analysis connects importance sampling, con-\ntrastive data selection and inﬂuence functions. In\npractice, contrastive data selection is the most pop-\nular approach. Unlike inﬂuence functions, con-\ntrastive data selection weights rely on ﬁne tun-\ning the generic model for more than one step on\nthe in-domain data T. This has two effects. On\none hand the contrastive data selection weights\ncan be more reliable, closer to the ideal weights\nw(y; T,D) = P(y|T)\nP(y|D) . On the other hand, multi-\nple steps increase the risk of over-ﬁtting to T. In\nthe case where one ﬁrst trains with data selection\nbefore ﬁne tuning on T, it might actually be help-\nful to limit the inﬂuence of T on selected data,\nto increase the complementary effect of ﬁne tun-\ning (Iter and Grangier, 2021).\nWhen comparing contrastive data selection with\nimportance sampling, the weight binarization is\nthe main difference. This binarization might also\nhave two opposite effects. On the positive side, it\nacts has a regularizer since binary weights are less\nlikely to reﬂect statistics speciﬁc to T compared to\nunquantized ones. On the negative side, it cancels\nlow weights which might collectively represent\nmost of the weighted cross entropy. This interpre-\ntation of contrastive data selection as a regularized\nversion of importance sampling opens the door to\nexploring more sophisticated regularization alter-\nnative to regularization, e.g. using a lower capac-\nity model or different input features to estimate se-\nlection weights.\n5 Conclusions\nThis work focuses on domain adaptation for neural\nlanguage modeling. It compares the generaliza-\ntion properties of a model trained over a large out-\nof-domain corpus as opposed to a model trained\nover a small in-domain corpus. It shows how ﬁne-\ntuning, the most common approach for neural LM\n3809\nadaptation can achieve better trade-offs than ei-\nther solution. We then focus on adaptation via\ndata selection techniques, i.e. techniques to em-\nphasize in-domain data in an out-of-domain train-\ning set. We show that common techniques, con-\ntrastive data selection and inﬂuence function se-\nlection, can both be derived from importance sam-\npling.\nOur analysis currently assumes a pure language\nmodeling setup, i.e. an auto-regressive model\ntrained aiming high log-likelihood, both for out-\nof-domain and in-domain data. In the future, we\nwant to extend our analysis of domain adapta-\ntion techniques to the popular setting (Bommasani\net al., 2021) where model training combines lan-\nguage modeling over out-of-domain data and a\ndifferent ﬁnal task on in-domain data.\nOur theoretical work also raises empirical ques-\ntions. The binarization of importance sampling\nweights in intelligent selection is a simple vari-\nance reduction technique and more sophisticated\nalternative might be beneﬁcial empirically. The\nlink between inﬂuence functions and importance\nsampling suggests that examples with importance\nsampling weights lower than one have only a neg-\native effect on the in-domain likelihood, which is\nnot a typical observation in practice. This con-\ntradiction suggests expanding inﬂuence scores to\ntake into account effects beyond a single update.\nAcknowledgements\nWe thanks Wei Wang, Bowen Liang, Kelvin Guu\nand Nicolas Le Roux for their suggestions and\ncomments.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsuper-\nvised domain clusters in pretrained language mod-\nels. arXiv preprint arXiv:2004.02105.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 355–362, Edinburgh, Scotland, UK. As-\nsociation for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Con-\nference on Machine Learning, ICML 2009, Mon-\ntreal, Quebec, Canada, June 14-18, 2009 , volume\n382 of ACM International Conference Proceeding\nSeries, pages 41–48. ACM.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, Erik Brynjolfsson, Shya-\nmal Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri Chatterji, Annie S. Chen, Kathleen Creel,\nJared Quincy Davis, Dorottya Demszky, Chris Don-\nahue, Moussa Doumbouya, Esin Durmus, Ste-\nfano Ermon, John Etchemendy, Kawin Ethayarajh,\nLi Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gille-\nspie, Karan Goel, Noah D. Goodman, Shelby Gross-\nman, Neel Guha, Tatsunori Hashimoto, Peter Hen-\nderson, John Hewitt, Daniel E. Ho, Jenny Hong,\nKyle Hsu, Jing Huang, Thomas Icard, Saahil Jain,\nDan Jurafsky, Pratyusha Kalluri, Siddharth Karam-\ncheti, Geoff Keeling, Fereshte Khani, Omar Khat-\ntab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, and et al. 2021. Foundation\nmodels. CoRR, abs/2108.07258.\nLéon Bottou and Olivier Bousquet. 2007. The trade-\noffs of large scale learning. In Advances in Neu-\nral Information Processing Systems 20, Proceedings\nof the Twenty-First Annual Conference on Neural\nInformation Processing Systems, Vancouver, British\nColumbia, Canada, December 3-6, 2007 , pages\n161–168. Curran Associates, Inc.\nStephen P. Boyd and Lieven Vandenberghe. 2014.\nConvex Optimization. Cambridge University Press.\nRich Caruana. 1998. Multitask learning. In Sebas-\ntian Thrun and Lorien Y . Pratt, editors, Learning to\nLearn, pages 95–133. Springer.\nRich Caruana, Steve Lawrence, and Lee Giles. 2001.\nOverﬁtting in neural nets: Backpropagation, conju-\ngate gradient, and early stopping. Advances in neu-\nral information processing systems, pages 402–408.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160–167.\nR Dennis Cook and Sanford Weisberg. 1982. Residu-\nals and inﬂuence in regression . New York: Chap-\nman and Hall.\nYann N. Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling with\ngated convolutional networks. In Proceedings of the\n34th International Conference on Machine Learn-\ning, ICML 2017, Sydney, NSW, Australia, 6-11 Au-\ngust 2017 , volume 70 of Proceedings of Machine\nLearning Research, pages 933–941. PMLR.\n3810\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJared Fernandez and Doug Downey. 2018. Sampling\ninformative training data for RNN language mod-\nels. In Proceedings of ACL 2018, Student Research\nWorkshop, pages 9–13, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nGeorge Foster, Cyril Goutte, and Roland Kuhn. 2010.\nDiscriminative instance weighting for domain adap-\ntation in statistical machine translation. In Proceed-\nings of the 2010 Conference on Empirical Meth-\nods in Natural Language Processing , pages 451–\n459, Cambridge, MA. Association for Computa-\ntional Linguistics.\nKen-Ichi Funahashi. 1989. On the approximate real-\nization of continuous mappings by neural networks.\nNeural networks, 2(3):183–192.\nD. Grangier and S. Bengio. 2008. A discrimina-\ntive kernel-based model to rank images from text\nqueries. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI).\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks.\nCoRR, abs/2004.10964.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nCoRR, abs/1902.00751.\nDan Iter and David Grangier. 2021. On the comple-\nmentarity of data selection and ﬁne tuning for do-\nmain adaptation. arXiv, 2109.07591.\nTyler B Johnson and Carlos Guestrin. 2018. Training\ndeep models faster with robust, approximate impor-\ntance sampling. Advances in Neural Information\nProcessing Systems, 31:7265–7275.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nAngelos Katharopoulos and François Fleuret. 2018.\nNot all samples are created equal: Deep learn-\ning with importance sampling. In International\nconference on machine learning, pages 2525–2534.\nPMLR.\nLeslie Kish. 1965. Survey sampling. new york: John\nwesley & sons.\nPhilipp Koehn. 2009. Statistical machine translation.\nCambridge University Press.\nPang Wei Koh and Percy Liang. 2017. Understand-\ning black-box predictions via inﬂuence functions.\nIn Proceedings of the 34th International Conference\non Machine Learning , volume 70 of Proceedings\nof Machine Learning Research , pages 1885–1894,\nInternational Convention Centre, Sydney, Australia.\nPMLR.\nYann Lecun. 1987. Modeles connexionnistes de\nl’apprentissage. Universite Pierre et Marie Curie\n(Paris 6).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAshique Rupam Mahmood, Hado Van Hasselt, and\nRichard S Sutton. 2014. Weighted importance sam-\npling for off-policy learning with linear function ap-\nproximation. In NIPS, pages 3014–3022.\nChristopher Manning and Hinrich Schutze. 1999.\nFoundations of statistical natural language process-\ning. MIT press.\nAlberto Maria Metelli, Matteo Papini, Francesco Fac-\ncio, and Marcello Restelli. 2018. Policy opti-\nmization via importance sampling. arXiv preprint\narXiv:1809.06098.\nAntonio Valerio Miceli Barone, Barry Haddow, Ulrich\nGermann, and Rico Sennrich. 2017. Regularization\ntechniques for ﬁne-tuning in neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1489–1494, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nTomás Mikolov, Martin Karaﬁát, Lukás Burget, Jan\nCernocký, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 ,\npages 1045–1048. ISCA.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nArt B. Owen. 2013. Monte Carlo theory, methods and\nexamples. Stanford.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2020.\nAdapterfusion: Non-destructive task composition\nfor transfer learning.\nJonathan Pilault, Amine El hattami, and Christopher\nPal. 2021. Conditionally adaptive multi-task learn-\ning: Improving transfer learning in {nlp} using\nfewer parameters & less data. In International Con-\nference on Learning Representations.\n3811\nGarima Pruthi, Frederick Liu, Satyen Kale, and\nMukund Sundararajan. 2020. Estimating training\ndata inﬂuence by tracing gradient descent. In Ad-\nvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners (gpt).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel\nUrtasun. 2018. Learning to reweight examples for\nrobust deep learning. In International Conference\non Machine Learning, pages 4334–4343. PMLR.\nSzymon Roziewski and Marek Kozłowski. 2021. Lan-\nguagecrawl: a generic tool for building language\nmodels upon common crawl. Language Resources\nand Evaluation, pages 1–29.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. CoRR, abs/1704.04368.\nAsa Cooper Stickland, Iain Murray, someone, and\nsomeone. 2019. BERT and PALs: Projected at-\ntention layers for efﬁcient adaptation in multi-task\nlearning. volume 97 of Proceedings of Machine\nLearning Research, pages 5986–5995, Long Beach,\nCalifornia, USA. PMLR.\nMartin Sundermeyer, Ralf Schlüter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In INTERSPEECH 2012, 13th Annual Confer-\nence of the International Speech Communication As-\nsociation, Portland, Oregon, USA, September 9-13,\n2012, pages 194–197. ISCA.\nMarlies van der Wees, Arianna Bisazza, and Christof\nMonz. 2017. Dynamic data selection for neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1410, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nWei Wang, Taro Watanabe, Macduff Hughes, Tetsuji\nNakagawa, and Ciprian Chelba. 2018. Denois-\ning neural machine translation training with trusted\ndata and online data selection. In Proceedings of\nthe Third Conference on Machine Translation: Re-\nsearch Papers, pages 133–143, Brussels, Belgium.\nAssociation for Computational Linguistics.\nXinyi Wang, Ankur Bapna, Melvin Johnson, and Orhan\nFirat. 2021. Gradient-guided loss masking for neu-\nral machine translation.\n3812\nA Proof of Theorem 1\nWhen the distributions T and Ddiffer, the Kull-\nback–Leibler (KL) divergence between the two\ndistributions is considered. We show that the\ngeneralization of the loss of θD over T is upper\nbounded\n∀ϵ> 0, ∃N s.t. ∀D∼Dn,\nL(θD; T) ≤H(T) + KL(T,D) + ϵ (4)\nwith probability 1 −ϵ This bound justiﬁes intu-\nition that if given the choice between two generic\ndomain Dand D′, training over the one with the\nlowest KL divergence to T will result a in better\nasymptotic behaviour.\nProof. We consider the asymptotic case for the\nsize of D. For any ϵ >0, the universal approxi-\nmation property allows us to consider a model ca-\npacity large enough such that\nLapp(D,Θ) < ϵ\n2\nUsing consistency, we can also consider a training\nset Dlarge enough such that\nLest(D,Θ,D) < ϵ\n2\nwith probability 1 −ϵ. With the same probability,\nL(θD; D) <LH(D) + ϵ\nwhich can be rewritten as a bound on the\nKullback-Leibler divergence,\nKL(P(·|D),P(·|θD)) = L(θD; D)−LH(D) <ϵ.\nThis bound can help connecting the generalization\nloss of θD over T with the Kullback-Leibler diver-\ngence of T and D,\nL(θD; T)\n=\n∑\ny∈Ω\nP(y|T) logP(y|θD)\n=\n∑\ny∈Ω\nP(y|T) log(P(y|D) +P(y|θD) −P(y|D))\n≤\n∑\ny∈Ω\nP(y|T) log(P(y|D) +|P(y|D) −P(y|θD)|)\n≤\n∑\ny∈Ω\nP(y|T) log(P(y|D) + 2ϵ2) (5)\n≤\n∑\ny∈Ω\nP(y|T) log(P(y|D)) + log(1 + 2mϵ2)\n≤ H(T) +KL(T,D) + log(1 + 2mϵ2)\nwhere m = 1 /minyP(y|D) assumes that\nP(·|D) has full support, and (5) relies on\nPinsker’s inequality, i.e. maxy|P(y) −Q(y)|<\n2KL(Q,Y )2.\n3813"
}