{
  "title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor",
  "url": "https://openalex.org/W3176724088",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2305010929",
      "name": "Md. Kamrul Hasan",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2996514779",
      "name": "Sangwu Lee",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2715050629",
      "name": "Wasifur Rahman",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2343600773",
      "name": "Amir Zadeh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068190112",
      "name": "Rada Mihalcea",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A4212702429",
      "name": "Louis-Philippe Morency",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753530250",
      "name": "Ehsan Hoque",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2305010929",
      "name": "Md. Kamrul Hasan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2996514779",
      "name": "Sangwu Lee",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2715050629",
      "name": "Wasifur Rahman",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2343600773",
      "name": "Amir Zadeh",
      "affiliations": [
        "Age Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2068190112",
      "name": "Rada Mihalcea",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A4212702429",
      "name": "Louis-Philippe Morency",
      "affiliations": [
        "Age Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2753530250",
      "name": "Ehsan Hoque",
      "affiliations": [
        "University of Rochester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2574717657",
    "https://openalex.org/W2951804840",
    "https://openalex.org/W6744211137",
    "https://openalex.org/W2757724864",
    "https://openalex.org/W2767249564",
    "https://openalex.org/W6751488361",
    "https://openalex.org/W2016730668",
    "https://openalex.org/W1595126664",
    "https://openalex.org/W6805520290",
    "https://openalex.org/W2937328183",
    "https://openalex.org/W2805662932",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3130174139",
    "https://openalex.org/W2109020278",
    "https://openalex.org/W6620808201",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2798894561",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W7020755954",
    "https://openalex.org/W6690956907",
    "https://openalex.org/W2798357113",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2772633765",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2954170085",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W6604407656",
    "https://openalex.org/W6691711314",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W6658014623",
    "https://openalex.org/W2787581402",
    "https://openalex.org/W6658616807",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W109058817",
    "https://openalex.org/W2095176743",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2807126412",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2090915937",
    "https://openalex.org/W640026160",
    "https://openalex.org/W4287780442",
    "https://openalex.org/W2033175753",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2251785914",
    "https://openalex.org/W1833933305",
    "https://openalex.org/W2808359495",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962718314",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4230165320",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970252517",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2754644668",
    "https://openalex.org/W2029996593",
    "https://openalex.org/W2963349408",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2251092808",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3023708404",
    "https://openalex.org/W3104536742",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2804900514"
  ],
  "abstract": "Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets -- achieving a new state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that our model can capture interpretable, humor-inducing patterns from all modalities.",
  "full_text": "Humor Knowledge Enriched Transformer for\nUnderstanding Multimodal Humor\nMd Kamrul Hasan1, Sangwu Lee 1, Wasifur Rahman 1, Amir Zadeh 2,\nRada Mihalcea 3, Louis-Philippe Morency 2, Ehsan Hoque 1\n1 Department of Computer Science, University of Rochester, USA,\n2 Language Technologies Institute, CMU, USA,\n3 Computer Science & Engineering, University of Michigan, USA\nmhasan8@cs.rochester.edu, slee232@u.rochester.edu, echowdh2@ur.rochester.edu, abagherz@cs.cmu.edu,\nmihalcea@umich.edu, morency@cs.cmu.edu, mehoque@cs.rochester.edu\nAbstract\nRecognizing humor from a video utterance requires under-\nstanding the verbal and non-verbal components as well as\nincorporating the appropriate context and external knowl-\nedge. In this paper, we propose Humor Knowledge enriched\nTransformer (HKT) that can capture the gist of a multimodal\nhumorous expression by integrating the preceding context\nand external knowledge. We incorporate humor centric ex-\nternal knowledge into the model by capturing the ambigu-\nity and sentiment present in the language. We encode all the\nlanguage, acoustic, vision, and humor centric features sepa-\nrately using Transformer based encoders, followed by a cross\nattention layer to exchange information among them. Our\nmodel achieves 77.36% and 79.41% accuracy in humorous\npunchline detection on UR-FUNNY and MUStaRD datasets\n– achieving a new state-of-the-art on both datasets with the\nmargin of 4.93% and 2.94% respectively. Furthermore, we\ndemonstrate that our model can capture interpretable, humor-\ninducing patterns from all modalities.\nIntroduction\nEver wondered the difﬁculty associated for a computer al-\ngorithm to recognize the punchline of a joke? People who\nare funny tend to be creative as well. They experiment\nwith words (language), gestures (vision), prosody (acoustic)\nand their (mis)alignments to build up a story while cross-\nreferencing multiple sources and appropriately delivering\nthe punchline. It is an important communication skill that\nremoves barriers in conversations, builds trust (Vartabedian\nand Vartabedian 1993) and creates positive impact on mental\nhealth (Lefcourt and Martin 2012). Humor has various styles\nlike sarcasm, exaggeration, irony, satire etc. Understanding\nhumor in everyday communication can enable machines to\nadapt its behavior seamlessly while interacting with the hu-\nmans, leading to a smooth and enriched user experience.\nA humorous punchline is often built around background\ncontext and external commonsense knowledge. Speakers de-\nliberately use ambiguous and sentiment evoking words to\nprime the audience to elicit a delightful laughter. They build\nup expectations in the minds of their audiences and at the\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nopportune moment, introduce a sudden twist, funny ges-\nture or sarcastic tone to deviate from the expectation of the\nstory (Ramachandran 1998). Humans can naturally process\nall these information subconsciously. However, building an\nalgorithm that can potentially do the same requires appropri-\nate integration of all these disparate sources of information.\nWe propose to model humor centric features by captur-\ning the diversity of meanings and the sentiment expressed\nin each word using ConceptNet (Speer, Chin, and Havasi\n2017) and the NRC\nV AD (Mohammad 2018) lexicon re-\nspectively. We also capture modality speciﬁc nuances for\nlanguage, acoustic, vision, and humor centric features sep-\narately. To represent the language modality, we ﬁne-tune a\npre-trained Albert model (Lan et al. 2019). For other modal-\nities, we train Transformer (Vaswani et al. 2017) based en-\ncoders from scratch. We enrich the language modality with\nhumor-centric features, and then capture the cross-modality\ninteractions using a Bimodal Cross Attention Layer to build\na singular representation of a data instance. Since modali-\nties often carry both complementary and supplementary in-\nformation, it is crucial to model them jointly to capture\ntheir underlying interactions. During all these phases, we\nmodel the punchline in light of the context so that we can\naccurately capture the inter-dependency between them. We\ncall our model Humor Knowledge enriched Transformer\n(HKT). The main contributions of our paper are:\n• We derive humor centric features (HCF) at word level by\nincorporating several humor theories and common sense\nknowledge.\n• We propose HKT – a transformer based model that learns\nto represent a punchline in light of the background con-\ntext. The model has modality speciﬁc encoders for attend-\ning to each modality and aBimodal Cross Attention Layer\nto jointly represent pairs of modality groups effectively.\n• The HKT model outperforms the state-of-the-art base-\nlines on two different multimodal datasets of humor\n(Hasan et al. 2019) and sarcasm (Castro et al. 2019) de-\ntection. We perform extensive experiments to demonstrate\nthat humor centric features, background context and cues\nfrom all three modalities are important to understand the\nhumor.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n12972\nBackground\nIn this section, we focus on the recent research on both text-\nbased and multimodal humor understanding. As our model\nis largely based on Transformer (Vaswani et al. 2017) archi-\ntectures, we discuss some examples on how they have been\nexpanded to model multiple modalities.\nText-based Humor Analysis: Early works have focused on\nextracting and analyzing interpretable features based on sev-\neral humor theories of incongruity, ambiguity, superiority,\nand phonetic style (Yang et al. 2015; Miller and Gurevych\n2015; Mihalcea and Strapparava 2005; Liu, Zhang, and Song\n2018; Zhang and Liu 2014). The discourse of sentiments in\ntext also plays important role in recognizing humor (Liu,\nZhang, and Song 2018). One noteworthy effort has been\ntaken by Yang et al. (Yang et al. 2015) to identify humor an-\nchors – the most pivotal text segments in generating humor.\nRecent works have started to focus on deep learning based\nmodels for humor detection. Convolutional Neural Network\n(CNN) based model (Chen and Lee 2017) is used to detect\nlaughter in TedTalk transcript. As a follow up, Chen et al.\n(Chen and Soo 2018) designs a CNN based model with high-\nway network that achieves state-of-the-art performance on\nfour text based humor datasets. Language model ﬁne-tuning\nis applied to classify humor in a large dataset containing\n300k Russian short texts (Blinov, Bolotova-Baranova, and\nBraslavski 2019). Transformer and BERT based architec-\ntures are also used to study humor in text (Weller and Seppi\n2019; Annamoradnejad 2020).\nMultimodal Humor Analysis: Due to the availability of\nlarge number of video content, researchers have started to\nstudy humor in a multimodal manner. Bertero et al. (Bert-\nero and Fung 2016) introduce a dataset containing acous-\ntic and text from the TV-show “Big Bang Theory” and ap-\nply Recurrent Neural Network to detect humor. UR-FUNNY\n(Hasan et al. 2019) is a publicly available multimodal (tex-\ntual, acoustic and visual) humor dataset that is collected\nfrom TED-Talks. The dataset has punchline-context setup\nand the authors extend Memory Fusion Network (Zadeh\net al. 2018) to incorporate context information for predicting\nhumorous punchline. A multimodal sarcasm dataset named\nMUStARD (Castro et al. 2019) is collected from popular\nsitcom TV shows. They also provide the preceding context\nof each sarcastic punchline and conduct extensive evalua-\ntion. MISA (Hazarika, Zimmermann, and Poria 2020) ag-\ngregates modality-invariant and modality-speciﬁc represen-\ntations and has been applied to predict humor in the UR-\nFUNNY dataset.\nMultimodal Analysis: Learning the joint representation of\nmultimodal data has been an active research area in NLP\ncommunity (Wang et al. 2019; Pham et al. 2019; Hazarika\net al. 2018; Poria et al. 2017; Zadeh et al. 2017; Liang et al.\n2018; Tsai et al. 2018; Liu et al. 2018; Zadeh et al. 2018; Is-\nlam and Iqbal 2021). Recently, Transformer (Vaswani et al.\n2017) based models have gained success in modeling mul-\ntiple modalities. Sun et al. (Sun et al. 2019) learn joint rep-\nresentation of video segments and their accompanying texts\nfrom a cooking video dataset. Multimodal Transformer (Tsai\net al. 2019) uses a set of transformer encoders to capture\nboth unimodal and cross modal interactions. Similarly, Tan\nand Bansal (Tan and Bansal 2019) learn joint representation\nof text and visual through a Cross-modality Encoder. Rah-\nman et al. (Rahman et al. 2020) integrate acoustic and visual\ninformation in the pre-trained transformers like BERT (De-\nvlin et al. 2018) and XLNet (Yang et al. 2019). Word rep-\nresentations of the language models are shifted conditioned\non nonverbal features by ﬁne-tuning.\nHumor Centric Features Extraction\nWe extract humor-centric features based on the ambiguity\nand superiority theories.\nAmbiguity\nAmbiguity occurs when a sentence expresses multiple\nmeanings simultaneously. It can be achieved through craft-\ning a sentence with ambiguous words. Such sentences can\nhave both serious and funny interpretations, generating hu-\nmor in that process (Charina 2017). One such example is:\nDid you hear about the guy whose whole left side was cut\noff? He’s all right now.In this example, the word ‘right’ can\nhave two primary meanings: ‘good’ and ‘direction’. Based\non the meaning we perceive to be true, we will interpret\nthe sentence very differently, and may experience ambigu-\nity driven humor.\nYang et al. (Yang et al. 2015) extracted the count of senses\n(meanings) of each word from WordNet (Fellbaum 2012)\nand used it as a feature for capturing ambiguity. Although\nthe count of senses is a good starting point, we believe that\nﬁnding the most frequently used senses of the words and the\ndiversity among the senses are important to capture the am-\nbiguity in that sentence. To that end, we use ConceptNet (Liu\nand Singh 2004) – a large-scale semantic structure that ex-\npresses the relationship between words and phrases through\ngraphs. The nodes in the graph denote concepts (words or\nphrases) and the weighted labeled edges denote how the\nwords are related and the conﬁdence score of the relation.\nFor each word, we extract neighbouring concepts after ﬁl-\ntering out the edges with conﬁdence score less than 1. For\nexample, top weighted neighbours of the word ‘right’ are:\nturn, direction, correct, good, proper, etc.\nFor each word w in our dataset, we extract Ns\nsenses (/concepts) and their corresponding Glove embed-\ndings (Pennington, Socher, and Manning 2014). The Glove\nembeddings provide a 300 dimensional vector for each\nword: similar words will have similar vector representations.\nThe matrix of all the senses of a word w is S 2RNs\u0002300;\nwhere Ns is the number of senses/concepts. We take the\nsummation of cosine distances of all pair of senses as a met-\nric of ambiguity. Since cosine distance between two iden-\ntical distance is zero, the metric will have higher value for\nmore ambiguous words.\nSentiment\nAccording to the superiority theory (Gruner 2017), hu-\nmorous text often contains sentiment information and the\ntransition of sentiments can be valuable in humor recog-\nnition (Liu, Zhang, and Song 2018). We extract valence\n(negative-positive), arousal (calm-excited), and dominance\n12973\nALBERT\nHCF\nEncoder\nAcoustic\nEncoder\nVisual\nEncoder\nUl\nUh\nUa\nUv\nHCF\nEnriched \nLanguage\nNon-verbal\nEmbedding\nCross \nAttention\nSelf\nAttention FF\nBimodal Cross \nAttention Layer\n[CLS] Because the other day \nwhen I said my husband is an \nangel; a woman complained. \nHumor Centric Featurescontextpunchline\ncontext [SEP] punchline\nZ\nUl,h\nUa,v\ncontext [SEP] punchline \ncontext [SEP] punchline \nInput Representations Unimodal Representations Multimodal Fusion\nMultimodal\nRepresentation\nCross \nAttention\n[SEP] She said you're so \nlucky, mine's still alive. \nFigure 1: Humor Knowledge enriched Transformer (HKT)\n(submissive-dominant) scores of each word from the NRC\nV AD dictionary (Mohammad 2018). This dictionary pro-\nvides the above mentioned scores (in the range [0,1]) for\n20k English words.\nWe denote the ambiguity, valence, arousal and dominance\nscores extracted for each word asHCF - Humor Centric Fea-\ntures (h) to be used as an additional information.\nHumor Knowledge Enriched Transformer\n(HKT) Model\nIn this section, we outline the Humor Knowledge enriched\nTransformer (HKT) model (Figure 1). First, a set of en-\ncoders create unimodal representations of punchline condi-\ntioned on context. Then, humor centric feature enriched lan-\nguage and non-verbal embedding go through Bimodal Cross\nAttention layers (details in Figure 2) to create multimodal\nfusion.\nTask Deﬁnition\nIf our dataset has N data-points, we can represent the i-th\ndata as fXi = (Ci;Pi);Y igwhere i2[1;:::N]. Here, Ci,\nPi and Yi are the context, punchline and the label (Humor/\nnot Humor) associated with i-th datapoint respectively. Both\nthe punchline and context sequences consist of four modali-\nties: language (l), acoustic (a), vision (v) and humor centric\nfeatures (h). We align the acoustic and visual features with\ntheir corresponding tokens in the language modality; there-\nfore, the language, acoustic and vision sequences have the\nsame length.\nThe total length of the i-th datapoint is \u001ci = \u001ci\np + \u001ci\nc,\nwhere \u001ci\np= punchline sequence length and \u001ci\nc= context se-\nquence length. Since \u001ci will have different values for differ-\nent ivalues, we can truncate context or pad them with zero\n(to the right) to make sure that all datapoints have a ﬁxed\nlength \u001c. We represent the punchline of i-th datapoint as\nPi = (Pi\nl;Pi\na;Pi\nv;Pi\nh); where Pi\nl 2R\u001ci\np\u0002dl , Pi\na 2R\u001ci\np\u0002da ,\nPi\nv 2R\u001ci\np\u0002dv and Pi\nh 2R\u001ci\np\u0002dh . Here, dl;da, dv and dh are\nthe dimensions of the language, acoustic, visual and HCF\nfeatures respectively. Similarly, the context of i-th datapoint\ncan be represented as Ci = (Ci\nl;Ci\na;Ci\nv;Ci\nh); where Ci\nl 2\nR\u001ci\nc\u0002dl , Ci\na 2R\u001ci\nc\u0002da , Cv 2R\u001ci\nc\u0002dv and Ch 2R\u001ci\nc\u0002dh .\nGiven a context (Ci) and punchline (Pi), our task is\nto predict whether the label (Yi) is humorous or not. For\nachieving that goal, we will maximize the following func-\ntion \u001e:\n\u001e=\nNY\ni=1\np(YijPi;Ci; \u0012) (1)\nIn Eq. 1, \u001erepresents the product of the conditional proba-\nbilities of determining the correct label given the punchline\nand context; \u0012denotes the model parameters that we want to\ntrain.\nUnimodal Representation Learning\nWe ﬁne-tune a pre-trained Albert (Lan et al. 2019) en-\ncoder for representing the language (l ) only. For the other\nthree modalities, we train a modiﬁed transformer encoder\n(Vaswani et al. 2017).\nLanguage Representation: To convert the text token\ninto vectors, we feed both the context and punchline to-\ngether to Albert. We represent our language modality as:\nXl = [CLS]Cl[SEP]Pl ; where Xl 2 R\u001c\u0002dl , Cl=tokens\nof context, Pl=tokens of punchline and \u001c= total length of\nthe token sequence. In essence, the [CLS] token appended at\nthe beginning will be used by the Albert encoder to create\n12974\na vector representing the whole input Xl, and the [SEP] to-\nken separates out the two sources of information – context\nand punchline – so that the punchline is modelled in light\nof the context. This representation is same as the one used\nin (Devlin et al. 2018) to model question-answering task.\nAlbert encoder output the unimodal language representation\nUl = ALBERT(Xl); where Ul 2R\u001c\u0002du\nl and du\nl = output\ndimension of the Albert encoder.\nAcoustic, Visual and Humor Centric Feature Repre-\nsentations: Transformer encoders are used to learn the uni-\nmodal representations of acoustic (a), vision (v ) and HCF\n(h).\nTransformer Encoder Layer contains a Multihead self-\nattention sub-layer and a Feed Forward (FF) sub-layer. The\nself-attention layers calculate the weighted summation of\nvalues; where the weights are computed from the scale dot\nproduct of query and key vector.\nAttention(Q;K;V ) =softmax(QKT\npdh\n)V (2)\nMultiple self-attention layers operating in parallel, hence the\nname Multi-Head Self Attention – each potentially focusing\non complementary aspects of the input. Following the con-\nvention of (Vaswani et al. 2017), we add layer normalization\nand residual connections after each sub-layers. Na, Nv and\nNh is the number of encoder layers are used in the Acoustic,\nVisual and HCF encoder respectively. To align the input rep-\nresentation with language, we create an input representation\nXm for the modality m: Xm = [PAD]Cm[SEP]Pm.\nm = fa;v;hg represents the acoustic, visual and HCF\nrespectively; where Xm 2R\u001c\u0002dm and dm = the dimen-\nsion of features in the corresponding modalities. [PAD] is\nused as a placeholder token mimicking the[CLS] token used\nfor language, and [SEP] token is used to separate the con-\ntext tokens from the punchline tokens. We send each input\nsequence Xm to the modality-speciﬁc encoder to obtain the\nunimodal representation Um = TransformerEncoder(Xm).\nIn the above equation, Um 2R\u001c\u0002du\nm and du\nm = output di-\nmension of the transformer encoder in the corresponding\nmodality m2[a;v;h].\nGrouping together modality information:As discussed\nin the preceding sections, Albert and the Transformer En-\ncoders give the unimodal representations of language (U l),\nacoustic (Ua), vision (Uv) and HCF (Uh). To infuse the lan-\nguage information with the knowledge gained from humor\ncentric features, we create an HCF-Enriched language rep-\nresentation Ul;h = Ul \bUh ; \brepresents concatenation\nand Ul;h 2 R\u001c\u0002(du\nl +du\nh). Similarly, we combine acoustic\nand visual representations to create a non-verbal embedding\nUa;v = Ua \bUv where Ua;v 2R\u001c\u0002(du\na+du\nv ).\nBimodal Cross Attention Layer\nA Bimodal Cross Attention Layer is added to learn the\njoint representation of Ul;h and Ua;v (Figure 1). We mod-\niﬁed the original transformer encoder layer (Vaswani et al.\n2017) to fuse the information across two modalities (De-\ntailed architecture is shown in Figure 2). For the sake of\nbrevity, the rest of this section will assume that we instan-\ntiate this layer with two vectors (U m1,Um2): representing\nAdd & Norm\nFeed Forward\nAdd & Norm\nUm1 Um2\nCross Alignment Fusion\nZm1,m2\nQm2 Km1 Vm1Qm1 Km2 Vm2\nUm2\n~ ~Um1\nMulti-Head \nSelf Attention\nMulti-Head \nCross Attention\nMulti-Head \nCross Attention\nAdd & Norm Add & Norm\nFigure 2: Bimodal Cross Attention Layer\nmodalities m1 and m2. However, these vectors can repre-\nsent a group of modalities as well. For example, we assume\nthat Um1 = Ul;h and Um2 = Ua;v in the model presented in\nFigure 2.\nTo exchange information between the two vectors\n(Um1,Um2), we create two sets of queries (Qm1;Qm2), keys\n(Km1;Km2) and values (Vm1;Vm2) matrices following the\nconvention of standard transformer (Vaswani et al. 2017).\nEach set is be attached to one of the two Multihead Cross\nAttention sub-layers. The sub-layers exchange the key and\nvalue matrices to compute the cross-aligned heads using the\nEquation 2. In essence, each sub-layer will create a vec-\ntor representation while absorbing information from its pair.\nSimilar approach of accomplishing cross-alignment among\nmodalities has been studied in other language-vision tasks\nas well (Tan and Bansal 2019; Lu et al. 2019).\nAlthough each of the outputs of the two Multihead Cross\nAttentions, ( ~Um1; ~Um2) contain information about the other,\nwe need to facilitate further exchange of information to build\na more contextual and uniﬁed vector representation. For\nachieving that goal, we concatenated and passed them to a\nMultihead Self Attention followed by a Feed Forward sub-\nlayer. Multihead Self Attention updates the representation of\neach element of input in light of the information gained from\nall the other elements and help us create the ﬁnal joint repre-\nsentation Zm1;m2. We add layer normalization and residual\nconnections after each sub-layers as well.\nMultimodal Fusion\nThe Bimodal Cross Attention Layer is used to cre-\nate a fusion vector of humor-centric-feature enriched\nlanguage embeddings (U l;h) and non-verbal embeddings\n(Ua;v) shown in Figure 1. That fusion vector is: Z =\n12975\nBimodalCrossAttention(Ul;h;Ua;v). In order to create a sin-\ngle vector representation unifying all components of our\nmodel, we create ﬁve vectors:[e l;ea;ev;eh;ez]; el is the\nvector corresponding to the [CLS] token in Albert model,\nand the rest of them are created by applying Max-pooling\nlayers on [Ua;Uv;Uh;Z] respectively. Max-pooling gives us\na computationally efﬁcient method of extracting the most\nsalient features across the time dimension and yields a ﬁxed\ndimensional vector. Finally, we concatenate all these repre-\nsentations to get the ﬁnal embedding o = el \bea \bev \b\neh \bez; o 2Rdo . The output probability is computed as\np = softmax(oW + b), where W 2Rdo\u0002l and b 2Rl de-\nnote model parameters and ldenotes the number of classes.\nExperiments\nIn this section, we discuss our experimental methodology:\ndatasets we use, features we extract and baselines we com-\npare with.\nDatasets\nWe work with datasets that have language, acoustic and\nvision modalities and the preceding context of the punch-\nline with the humor(/sarcastic) label. Only UR-FUNNY and\nMUStaRD fulﬁll these criteria.\nUR-FUNNY:The UR-FUNNY (Hasan et al. 2019) is col-\nlected from TED talk videos and therefore, has language,\nacoustic and visual modalities and the context preceding\nthe punchline. Punchline is extracted using the ‘laughter’\nmarkup – indicating when audience laughed during the talk\n– in the transcripts. The sentences preceding the punchline\nform the context. Negative samples are also extracted in sim-\nilar manner where target punchline utterances are not fol-\nlowed by ‘laughter’. In total, the dataset consists of 5K hu-\nmor and 5K non-humor instances (from 1741 distinct speak-\ners). Version 2 of the UR-FUNNY dataset is used for all ex-\nperiments.\nMUStARD: Multimodal Sarcasm Detection Dataset\n(MUStARD) (Castro et al. 2019) is compiled from popular\nTV shows like Friends, The Big Bang Theory, The golden\nGirls and Sarcasmaholics. It provides 690 video segments\nthat are manually annotated with sarcastic/non-sarcastic la-\nbels. They provide target punchline utterance and the asso-\nciated historical dialogues as context.\nFeature Extraction\nThe following standard features are used by the baseline\nmodels and our HKT model.\nLanguage: Albert (Lan et al. 2019) language model is\nﬁne tuned to learn the contextual word representations.\nP2FA forced alignment model (Yuan and Liberman 2008)\nis used to extract the timing of all the words used in punch-\nline and context. Once we extract the acoustic and visual\nfeatures for the whole video, we use each word’s timing to\nslice off the relevant range of acoustic and visual features\nfor that word. Those two feature arrays are averaged out\nacross the time dimension separately and in the end, we get\nthe acoustic and visual feature vectors for each word (Chen\net al. 2017). We extract ‘senses’ of word from ConceptNet\nand use GloVe embeddings (Pennington, Socher, and Man-\nning 2014) to measure ambiguity.\nAcoustic: We use COV AREP (Degottex et al. 2014)\nto extract low-level acoustic features. This feature set\nincludes Melcepstral coefﬁcients, fundamental frequency,\nvoiced/unvoiced segments , normalized amplitude quotient,\nquasi open quotient (Kane and Gobl 2013), glottal source\nparameters (Drugman et al. 2012), harmonic model and\nphase distortions, the formants etc.\nVisual: OpenFace 2 (Baltrusaitis et al. 2018) is used to\nextract facial Action Units (AU) features and Rigid and non-\nrigid facial shape parameters. Facial action unit features are\nbased on the Facial Action Coding System (FACS) (Ekman\n1997) which are widely used in human affect analysis.\nBaseline Models\nThe performance of our HKT model is compared with the\nfollowing baselines:\nContextual Memory Fusion Network (C-MFN)(Hasan\net al. 2019) was used to detect humor punchlines in UR-\nFUNNY dataset. They extended the Memory Fusion Net-\nwork (Zadeh et al. 2018) by incorporating the information\nfrom the preceding context.\nSupport Vector Machines (SVM) was used as the base-\nline model for MUStARD dataset (Castro et al. 2019). They\nused ResNet (He et al. 2016) and Librosa (McFee et al.\n2018) to extract visual and acoustic features respectively\nand did not align all three modalities. While we have at-\ntempted to extract the acoustic and visual features using the\nCovarep and Openface and align the modalities, we lost 14\nsamples. To present a fair comparison, we extract all the fea-\ntures (mentioned above) and retrain an SVM model on the\ntrain, dev, and test sets that we deﬁne for MUStARD.\nMISA (Hazarika, Zimmermann, and Poria 2020)\nachieved SOTA performance on the UR-FUNNY dataset\nby projecting their data to a modality invariant and three\nmodality-speciﬁc spaces and then aggregating all those\nprojections. They used BERT language encoder and worked\nwith punchline only. For fair comparison, we rerun MISA\nby concatenating both punchline and context. As our model\nuse ALBERT language encoder, so we run a variant of\nMISA with ALBERT. We experiment with these variants of\nMISA on both UR-FUNNY and MUStARD datasets.\nMAG-Transformer (Rahman et al. 2020) introduced\nMultimodal Adaption Gate (MAG) to fuse acoustic and vi-\nsual information in pretrained language transformers. Dur-\ning ﬁne tuning, the MAG shifts the internal representa-\ntions of BERT and XLNet in the presence of the visual\nand acoustic modalities. Both the MAG-BERT and MAG-\nXLNet achieved SOTA performance in CMU-MOSI and\nCMU-MOSEI datasets of multimodal sentiment analysis.\nMoreover, the MAG-XLNet achieved human level perfor-\nmance on the CMU-MOSI dataset and outperformed all the\nstate of the art multimodal fusion models. We apply the\nMAG-XLNet on both UR-FUNNY and MUStARD datasets\ndue to it’s superior performance. For fair comparison with\nour model, we also run a variant of MAG-Transformer\nwhere we use ALBERT pretrained transformer.\n12976\nMultimodal Models UR-FUNNY MUStARD\nC-MFN (Glove) 65.23 -\nC-MFN (Albert) 61.72 -\nSVM - 71.6\nMISA (BERT) [punchline only] 70.61 -\nMISA (BERT) 69.62 66.18\nMISA (ALBERT) 69.82 66.18\nMAG-ALBERT 67.20 69.12\nMAG-XLNet 72.43 76.47\nHKT 77.36 79.41\n\u0001 SOTA 4.93 \" 2.94 \"\nTable 1: Performances (binary accuracy) of multimodal\nmodels on the UR-FUNNY & MUStARD datasets.\nExperimental Design\nAdam optimizer and Linear scheduler are used to train\nthe HKT model. We use different learning rates for lan-\nguage, acoustic, visual and HCF encoders. The search space\nof the learning rates is f0:001;0:0001;0:00001;0:000001g.\nBinary cross entropy is used as loss function. We experiment\nwith f1;2;3;4;5;6;7;8gencoder layers and f1;2;3;4;6g\ncross attention heads for the language, acoustic, visual and\nHCF encoders. For the Bimodal Cross Attention we exper-\niment f1;2glayers and f1;2;4gattention heads. Dropout\n[0:05 \u00000:30] (uniform distribution) is used to regularize the\nmodel. For other baseline models, we ﬁrst experiment with\nbest conﬁgurations that were presented in their respective\npapers. In addition, we run experiments with extensive hyper\nparameter search for fair comparison. We provide details of\nthe best model conﬁgurations and hyper-parameters search\nspaces in the supplementary material 1. In our framework, it\nis possible to reproduce the same experiment on K80 gpu for\nspeciﬁc hyper-parameters and seed. Both the UR-FUNNY\nand MUStARD have balanced test set. Hence, we use Bi-\nnary Accuracy as our performance metric.\nResults and Discussions\nIn this section, we compare the performance of HKT model\nwith the baselines, conduct ablation studies to show the im-\nportance of including multiple modalities and HCF features,\nand demonstrate our model’s capability of capturing multi-\nmodal humor anchors.\nComparison with Baselines\nTable 1 shows that the HKT model outperforms the base-\nlines signiﬁcantly on the UR-FUNNY dataset (4.93% in-\ncrease) and MUStARD dataset (2.94% increase). The orig-\ninal C-MFN model was trained with Glove embeddings on\nUR-FUNNY dataset. We re-train the C-MFN model with the\nembeddings extracted from the pre-trained Albert model to\nensure a fair comparison. However, it performed poorly on\nhumorous punchline detection in UR-FUNNY . The MISA\nbaseline model only used punchline of UR-FUNNY to\npredict humor (Hazarika, Zimmermann, and Poria 2020).\n1https://github.com/matalvepu/HKT\nModels UR-FUNNY MUStARD\nHKT 77.36 79.41\n- acoustic (a) 74.14 76.47\n- visual (v) 76.06 76.47\n- HCF (h) 76.36 75.00\nlanguage only (l) 73.54 73.53\nacoustic only (a) 64.99 73.53\nvisual only (v) 55.84 64.71\nHCF only (h) 56.54 60.29\nTable 2: Role of modalities in our HKT model. Here ‘-’ de-\nnotes removal of the corresponding feature set. Binary accu-\nracy is reported here as performance.\nTherefore, we train a variation of our HKT model by remov-\ning the context and achieved 71.33% accuracy (0:72% in-\ncrease compare to MISA). However, our full model achieves\n77:36% accuracy on UR-FUNNY that indicates the impor-\ntance of context in detecting humorous punchline.\nMISA (BERT) and MISA (ALBERT) that are trained on\nthe full sequence of context and punchline do not achieve\nbetter performance. They perform worse than the punchline\nonly MISA (BERT) model in UR-FUNNY dataset. The pos-\nsible explanation is that MISA encodes the full temporal un-\nmiodal sequence into a single latent space and reconstructs\nthe unimodal embeddings from the latent space. So, infor-\nmation might get lost during encoding the long sequence.\nThe authors also worked with punchline only in their experi-\nments for the UR-FUNNY dataset. Similarly, MISA variants\nperform worse than the SVM baseline in MUStARD dataset.\nThis generative approach does not work well on the small\ndataset like MUStARD. In all of theses cases, we exper-\niment with their reported best model conﬁgurations. Then\nadditional extensive hyper-parameter search is done for fair\ncomparison. However, MISA variants do not achieve rea-\nsonable performance on the long sequence of punchline fol-\nlowed by a context.\nMAG-XLNet is the current state-of-the-art model for\nmultimodal sentiment analysis in CMU-MOSI and CMU-\nMOSEI datasets. In here, it also achieves competitive perfor-\nmance compare to other baselines in both datasets. However,\nMAG-ALBERT does not achieve similar performance. Our\nHKT model achieves better result than MAG-XLNet and\nshows the importance of this kind of architecture for mod-\neling multimodal humorous punchline in the light of back-\nground context and humor centric features.\nRole of Different Components\nRole of Modalities: we retrain our model by removing a\nset of modalities (one at a time) and thus show its impor-\ntance in Table 2. Language manifests itself as the dominant\nmodality in the UR-FUNNY dataset. On the other hand, both\nlanguage and acoustic shows highest importance in MUS-\ntARD dataset. Although removing acoustic and visual fea-\ntures drops the performance for both datasets, the drop is\nvery negligible for visual in the UR-FUNNY dataset. Since\nthe cameras move a lot and seldom focuses on the faces\n12977\nl a v h\nl\na\nv\nh\nl a v h\nl\na\nv\nh\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nUR-FUNNY MUStARD\nFigure 3: Correlation among the prediction outputs (dev &\ntest set) of unimodal models.\nof the speaker in TED-Talk recordings, the visual features\ntend to carry a lot of noises (Hasan et al. 2019). We ﬁnd that\n70% of the visual vectors are zero in UR-FUNNY dataset.\nRemoving HCF is also consequential, especially for the\nMUStARD dataset. In our early exploration, we have exper-\nimented with transformer-based language encoder trained\non Glove embeddings. ALBERT language encoder achieves\nbetter results (8% increase on UR-FUNNY), which is why\nwe use the ALBERT encoder in our HKT model.\nWe try to understand how much modality-speciﬁc infor-\nmation our unimodal encoders can capture. Each encoders\nare trained separately with its modality-speciﬁc information\nonly (while detaching it from other parts of the model). Next,\nwe take the outputs from each of those of the unimodal en-\ncoders to predict humor/sarcasm (Table 2). These results are\nin accordance with the results mentioned in the preceding\nparagraph. The acoustic modality alone works surprisingly\nwell for MUStARD dataset: we observe that actors exag-\ngerate their voice to deliver sarcastic punchlines in sitcom\nshows, which can explain the superior performance of the\nacoustic modality. HCF features also capture some mean-\ningful insight from the MUStARD dataset. Figure 3 shows\nhow much complementary information is present in each\nmodality with respect to the other modalities. We extract the\npredictions from each unimodal encoders and calculate the\nPearson correlation among them. The correlation values are\nlow, which indicates that each modality covers different as-\npects of information. The HKT model brings all this comple-\nmentary information together, which can explain its higher\naccuracy when compared to previous models.\nBimodal Cross Attention Layers: We experiment with\ndifferent numbers of Bimodal Cross Attention Layers. How-\never, in both dataset we observe that increasing the num-\nber layers do not improve performance (highest 76.47%\naccuracy on MUStARD and highest 75.75% accuracy on\nUR-FUNNY). Speciﬁcally, in MUStARD dataset the model\noverﬁts very quickly due to high number of parameters com-\npare to the small amount of data.\nHCF: Integrated gradients technique (Sundararajan, Taly,\nand Yan 2017) is used to analyze the relative importance\nof the humor centric features in the model’s inference. The\nmodel puts more weight (1.88 times higher) on the “Senti-\nment” features compared to the “Ambiguity”. The test and\ndev split of the MUStARD dataset are used to compare.\n Anyway  if  you don’t feel  \nlike being  alone  tonight \n Joey  and Chandler  are coming \nover   to  help  put  together  my  \nnew furniture  sd  \n Yes,  we  are very  \nexcited about  it d\nTEXT \nAnyway if you don’t feel\n like being  alone  tonight s\nJoey and Chandler are  coming  over to \nhelp put  together  my  new furniture\n Yes,  we are very \nexcited about it \nAnyway if you don’t feel \nlike being  alone  tonight s\n Joey and  Chandler  are  coming \nover to  help  put  together my \nnew furniture\n Yes, we  are very \nexcited about it s  \nVISUAL \nACOUSTIC\nHCF\nAnyway if you don’t  feel \nlike being  alone  tonight\nJoey and Chandler are coming \nover to help put together my \nnew  furniture \nYes, we are very  \nexcited  about it\nRaised Eyebrow Displeased Facial Expression Exaggerated Expression \nStress on Tone Started Sarcastic Tone\nHigh Valence & ArousalLow Arousal & Dominance\n      Low \nAttribution\n   High \nAttribution\n0.0 0.02\nFigure 4: Multimodal Humor Anchors extracted by the HKT\nmodel. Integrated gradients (Sundararajan, Taly, and Yan\n2017) method is used to decipher how each input token\n(across modalities) contributed to the model’s ﬁnal decision.\nSince acoustic, visual and HCF features are aligned on word\nlevel, we color coded the words to indicate the timestamps\nwhere model put attention on the corresponding modalities.\nFor example, model puts highest attention to the visual fea-\ntures corresponding to the time-periods when the words ‘be-\ning’ and ‘yes’ were spoken. (Best view in color and zoomed)\nMultimodal Humor Anchors\nHumor anchors are the input tokens that play a pivotal role\nin creating humor (Yang et al. 2015). We want to see if our\nHKT model can identify humor anchors present in other\nmodalities than text. Figure 4 shows the visualization of an\nexample from MUStARD dataset (video id: 2\n524). We use\nintegrated gradients method (Sundararajan, Taly, and Yan\n2017) to ﬁnd the candidate tokens which have high impact in\nthe model’s decision making process. As the acoustic and vi-\nsual modalities are aligned with text in word level, we know\nthe timestamps where each feature resides. We get the at-\ntribution value (measure of impact) for each feature vector.\nThen we manually go through the video to understand how\nthose high-attribution achieving features are related to hu-\nmor. We have found that the model puts high attribution to\nmeaningful patterns like eye brow raise, exaggerated facial\nexpressions, stress on tone, high valence and arousal.\nConclusion\nIn this paper, we introduce HKT – a humor knowledge en-\nriched multimodal model that can effectively learn the multi-\nmodal representation of a punchline conditioned on the con-\ntext story. Our experiments show signiﬁcant improvements\nin the task of humorous/sarcastic punchline detection on two\npublicly available datasets: UR-FUNNY and MUStARD.\nWe demonstrate that context, humor centric features based\non humor theories and cues from all three modalities are im-\nportant for recognizing the humorous punchline. Addition-\nally, we demonstrate that our model is able to ﬁnd meaning-\nful humor anchors across the modalities\n12978\nAcknowledgments\nThis research was supported in part by grant W911NF-19-\n1-0029 with the US Defense Advanced Research Projects\nAgency (DARPA) and the Army Research Ofﬁce (ARO).\nAuthors AZ and LPM were supported in part by the National\nScience Foundation (Awards #1750439, #1722822) and Na-\ntional Institutes of Health.\nReferences\nAnnamoradnejad, I. 2020. ColBERT: Using BERT Sentence Em-\nbedding for Humor Detection. arXiv preprint arXiv:2004.12765\n.\nBaltrusaitis, T.; Zadeh, A.; Lim, Y . C.; and Morency, L.-P. 2018.\nOpenface 2.0: Facial behavior analysis toolkit. In 2018 13th IEEE\nInternational Conference on Automatic Face & Gesture Recogni-\ntion (FG 2018), 59–66. IEEE.\nBertero, D.; and Fung, P. 2016. Deep learning of audio and lan-\nguage features for humor prediction. In Proceedings of the Tenth\nInternational Conference on Language Resources and Evaluation\n(LREC’16), 496–501.\nBlinov, V .; Bolotova-Baranova, V .; and Braslavski, P. 2019. Large\nDataset and Language Model Fun-Tuning for Humor Recognition.\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 4027–4032.\nCastro, S.; Hazarika, D.; P ´erez-Rosas, V .; Zimmermann, R.;\nMihalcea, R.; and Poria, S. 2019. Towards Multimodal Sar-\ncasm Detection (An\nObviously Perfect Paper). arXiv preprint\narXiv:1906.01815 .\nCharina, I. N. 2017. Lexical and Syntactic Ambiguity in Humor.\nInternational Journal of Humanity Studies (IJHS) 1(1): 120–131.\nChen, L.; and Lee, C. 2017. Predicting Audience’s Laughter Dur-\ning Presentations Using Convolutional Neural Network. In Pro-\nceedings of the 12th Workshop on Innovative Use of NLP for Build-\ning Educational Applications, 86–90.\nChen, M.; Wang, S.; Liang, P. P.; Baltru ˇsaitis, T.; Zadeh, A.; and\nMorency, L.-P. 2017. Multimodal sentiment analysis with word-\nlevel fusion and reinforcement learning. InProceedings of the 19th\nACM International Conference on Multimodal Interaction, 163–\n171.\nChen, P.-Y .; and Soo, V .-W. 2018. Humor recognition using deep\nlearning. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 2 (Short Papers),\n113–117.\nDegottex, G.; Kane, J.; Drugman, T.; Raitio, T.; and Scherer, S.\n2014. COV AREP—A collaborative voice analysis repository for\nspeech technologies. In 2014 ieee international conference on\nacoustics, speech and signal processing (icassp), 960–964. IEEE.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805 .\nDrugman, T.; Thomas, M.; Gudnason, J.; Naylor, P.; and Dutoit,\nT. 2012. Detection of glottal closure instants from speech signals:\nA quantitative review. IEEE Transactions on Audio, Speech, and\nLanguage Processing 20(3): 994–1006.\nEkman, R. 1997. What the face reveals: Basic and applied studies\nof spontaneous expression using the Facial Action Coding System\n(FACS). Oxford University Press, USA.\nFellbaum, C. 2012. WordNet. The encyclopedia of applied linguis-\ntics .\nGruner, C. R. 2017. The game of humor: A comprehensive theory\nof why we laugh. Routledge.\nHasan, M. K.; Rahman, W.; Bagher Zadeh, A.; Zhong, J.; Tanveer,\nM. I.; Morency, L.-P.; and Hoque, M. E. 2019. UR-FUNNY: A\nMultimodal Language Dataset for Understanding Humor. In Pro-\nceedings of the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP), 2046–\n2056. Hong Kong, China: Association for Computational Linguis-\ntics. doi:10.18653/v1/D19-1211. URL https://www.aclweb.org/\nanthology/D19-1211.\nHazarika, D.; Poria, S.; Zadeh, A.; Cambria, E.; Morency, L.-P.;\nand Zimmermann, R. 2018. Conversational memory network for\nemotion recognition in dyadic dialogue videos. In Proceedings of\nthe 2018 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), volume 1, 2122–2132.\nHazarika, D.; Zimmermann, R.; and Poria, S. 2020. MISA:\nModality-Invariant and-Speciﬁc Representations for Multimodal\nSentiment Analysis. arXiv preprint arXiv:2005.03545 .\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 770–778.\nIslam, M. M.; and Iqbal, T. 2021. Multi-GAT: A Graphical\nAttention-based Hierarchical Multimodal Representation Learning\nApproach for Human Activity Recognition. IEEE Robotics and\nAutomation Letters 1–1. doi:10.1109/LRA.2021.3059624.\nKane, J.; and Gobl, C. 2013. Wavelet maxima dispersion for\nbreathy to tense voice discrimination.IEEE Transactions on Audio,\nSpeech, and Language Processing 21(6): 1170–1179.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Sori-\ncut, R. 2019. Albert: A lite bert for self-supervised learning of\nlanguage representations. arXiv preprint arXiv:1909.11942 .\nLefcourt, H. M.; and Martin, R. A. 2012. Humor and life stress:\nAntidote to adversity. Springer Science & Business Media.\nLiang, P. P.; Liu, Z.; Zadeh, A.; and Morency, L.-P. 2018. Multi-\nmodal language analysis with recurrent multistage fusion. arXiv\npreprint arXiv:1808.03920 .\nLiu, H.; and Singh, P. 2004. ConceptNet—a practical common-\nsense reasoning tool-kit. BT technology journal 22(4): 211–226.\nLiu, L.; Zhang, D.; and Song, W. 2018. Modeling sentiment asso-\nciation in discourse for humor recognition. In Proceedings of the\n56th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), 586–591.\nLiu, Z.; Shen, Y .; Lakshminarasimhan, V . B.; Liang, P. P.; Zadeh,\nA.; and Morency, L.-P. 2018. Efﬁcient low-rank multimodal fusion\nwith modality-speciﬁc factors. arXiv preprint arXiv:1806.00064 .\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vilbert: Pre-\ntraining task-agnostic visiolinguistic representations for vision-\nand-language tasks. In Advances in Neural Information Processing\nSystems, 13–23.\nMcFee, B.; McVicar, M.; Balke, S.; Thom ´e, C.; Lostanlen, V .;\nRaffel, C.; Lee, D.; Nieto, O.; Battenberg, E.; Ellis, D.; et al.\n2018. WZY . Rachel Bittner, Keunwoo Choi, Pius Friesch,\nFabian-Robert Stter, Matt Vollrath, Siddhartha Kumar, nehz, Si-\nmon Waloschek, Seth, Rimvydas Naktinis, Douglas Repetto, Cur-\ntis” Fjord” Hawthorne, CJ Carr, Joo Felipe Santos, JackieWu,\nErik, and Adrian Holovaty,“librosa/librosa: 0.6 2.\n12979\nMihalcea, R.; and Strapparava, C. 2005. Making computers laugh:\nInvestigations in automatic humor recognition. In Proceedings of\nthe Conference on Human Language Technology and Empirical\nMethods in Natural Language Processing, 531–538. Association\nfor Computational Linguistics.\nMiller, T.; and Gurevych, I. 2015. Automatic disambiguation of\nEnglish puns. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Processing (Volume\n1: Long Papers), 719–729.\nMohammad, S. 2018. Obtaining reliable human ratings of valence,\narousal, and dominance for 20,000 English words. In Proceedings\nof the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 174–184.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014\nconference on empirical methods in natural language processing\n(EMNLP), 1532–1543.\nPham, H.; Liang, P. P.; Manzini, T.; Morency, L.-P.; and Poczos,\nB. 2019. Found in Translation: Learning Robust Joint Represen-\ntations by Cyclic Translations Between Modalities. arXiv preprint\narXiv:1812.07809 .\nPoria, S.; Cambria, E.; Hazarika, D.; Mazumder, N.; Zadeh, A.;\nand Morency, L.-P. 2017. Multi-level multiple attentions for con-\ntextual multimodal sentiment analysis. In 2017 IEEE International\nConference on Data Mining (ICDM), 1033–1038. IEEE.\nRahman, W.; Hasan, M. K.; Lee, S.; Zadeh, A. B.; Mao, C.;\nMorency, L.-P.; and Hoque, E. 2020. Integrating Multimodal In-\nformation in Large Pretrained Transformers. In Proceedings of the\n58th Annual Meeting of the Association for Computational Lin-\nguistics, 2359–2369.\nRamachandran, V . S. 1998. The neurology and evolution of humor,\nlaughter, and smiling: the false alarm theory. Medical hypotheses\n51(4): 351–354.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5: An open\nmultilingual graph of general knowledge. In Thirty-First AAAI\nConference on Artiﬁcial Intelligence.\nSun, C.; Myers, A.; V ondrick, C.; Murphy, K.; and Schmid, C.\n2019. Videobert: A joint model for video and language representa-\ntion learning. In Proceedings of the IEEE International Conference\non Computer Vision, 7464–7473.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic attribu-\ntion for deep networks. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70, 3319–3328. JMLR.\norg.\nTan, H.; and Bansal, M. 2019. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490 .\nTsai, Y .-H. H.; Bai, S.; Liang, P. P.; Kolter, J. Z.; Morency, L.-P.;\nand Salakhutdinov, R. 2019. Multimodal transformer for unaligned\nmultimodal language sequences. arXiv preprint arXiv:1906.00295\n.\nTsai, Y .-H. H.; Liang, P. P.; Zadeh, A.; Morency, L.-P.; and\nSalakhutdinov, R. 2018. Learning factorized multimodal represen-\ntations. arXiv preprint arXiv:1806.06176 .\nVartabedian, R. A.; and Vartabedian, L. K. 1993. Humor in the\nWorkplace: A Communication Challenge. .\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. In Advances in neural information processing sys-\ntems, 5998–6008.\nWang, Y .; Shen, Y .; Liu, Z.; Liang, P. P.; Zadeh, A.; and Morency,\nL.-P. 2019. Words Can Shift: Dynamically Adjusting Word\nRepresentations Using Nonverbal Behaviors. arXiv preprint\narXiv:1811.09362 .\nWeller, O.; and Seppi, K. 2019. Humor Detection: A Transformer\nGets the Last Laugh. arXiv preprint arXiv:1909.00252 .\nYang, D.; Lavie, A.; Dyer, C.; and Hovy, E. 2015. Humor recog-\nnition and humor anchor extraction. In Proceedings of the 2015\nConference on Empirical Methods in Natural Language Process-\ning, 2367–2376.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov, R. R.;\nand Le, Q. V . 2019. Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural information\nprocessing systems, 5753–5763.\nYuan, J.; and Liberman, M. 2008. Speaker identiﬁcation on the\nSCOTUS corpus. Journal of the Acoustical Society of America\n123(5): 3878.\nZadeh, A.; Chen, M.; Poria, S.; Cambria, E.; and Morency, L.-P.\n2017. Tensor fusion network for multimodal sentiment analysis.\narXiv preprint arXiv:1707.07250 .\nZadeh, A.; Liang, P. P.; Mazumder, N.; Poria, S.; Cambria, E.; and\nMorency, L.-P. 2018. Memory fusion network for multi-view se-\nquential learning. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\nZhang, R.; and Liu, N. 2014. Recognizing humor on twitter. InPro-\nceedings of the 23rd ACM International Conference on Conference\non Information and Knowledge Management, 889–898. ACM.\n12980",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7986640930175781
    },
    {
      "name": "Computer science",
      "score": 0.6860575079917908
    },
    {
      "name": "Ambiguity",
      "score": 0.649995744228363
    },
    {
      "name": "Utterance",
      "score": 0.5631625652313232
    },
    {
      "name": "Novelty",
      "score": 0.5271962285041809
    },
    {
      "name": "ENCODE",
      "score": 0.5232231020927429
    },
    {
      "name": "Encoder",
      "score": 0.49478524923324585
    },
    {
      "name": "Natural language processing",
      "score": 0.43395358324050903
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3642299473285675
    },
    {
      "name": "Speech recognition",
      "score": 0.3355598449707031
    },
    {
      "name": "Psychology",
      "score": 0.1871868073940277
    },
    {
      "name": "Engineering",
      "score": 0.08249986171722412
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}