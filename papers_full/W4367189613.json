{
    "title": "Generative Relevance Feedback with Large Language Models",
    "url": "https://openalex.org/W4367189613",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4281181919",
            "name": "Mackie, Iain",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A4289375834",
            "name": "Chatterjee, Shubham",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A4281181924",
            "name": "Dalton, Jeffrey",
            "affiliations": [
                "University of Glasgow"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2117473841",
        "https://openalex.org/W4284669679",
        "https://openalex.org/W6600466347",
        "https://openalex.org/W2000411838",
        "https://openalex.org/W3154280800",
        "https://openalex.org/W4320813768",
        "https://openalex.org/W3021397474",
        "https://openalex.org/W3180230246",
        "https://openalex.org/W3184918446",
        "https://openalex.org/W3217485291",
        "https://openalex.org/W3209981429",
        "https://openalex.org/W3152887675",
        "https://openalex.org/W2156769220",
        "https://openalex.org/W2070740689",
        "https://openalex.org/W2107370612",
        "https://openalex.org/W3152151101",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4327644588",
        "https://openalex.org/W4251326898",
        "https://openalex.org/W4293248017",
        "https://openalex.org/W4309698332",
        "https://openalex.org/W4385573600",
        "https://openalex.org/W2067506377",
        "https://openalex.org/W2102563107",
        "https://openalex.org/W3134665270",
        "https://openalex.org/W3210968241",
        "https://openalex.org/W3003611599",
        "https://openalex.org/W1964348731",
        "https://openalex.org/W4250909839",
        "https://openalex.org/W4284697472",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W4322617770",
        "https://openalex.org/W2164547069",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W1969340322",
        "https://openalex.org/W2014415866",
        "https://openalex.org/W4306311899",
        "https://openalex.org/W4225338712",
        "https://openalex.org/W4385565351",
        "https://openalex.org/W2069065514",
        "https://openalex.org/W4318239801",
        "https://openalex.org/W3034439313",
        "https://openalex.org/W3175111331",
        "https://openalex.org/W159389352",
        "https://openalex.org/W4313680149",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4315481736",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3189106975",
        "https://openalex.org/W3093955333"
    ],
    "abstract": "Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve state-of-the-art recall across all datasets.",
    "full_text": "Generative Relevance Feedback with Large Language Models\nIain Mackie\nUniversity of Glasgow\ni.mackie.1@research.gla.ac.uk\nShubham Chatterjee\nUniversity of Glasgow\nshubham.chatterjee@glasgow.ac.uk\nJeffrey Dalton\nUniversity of Glasgow\njeff.dalton@glasgow.ac.uk\nABSTRACT\nCurrent query expansion models use pseudo-relevance feedback to\nimprove first-pass retrieval effectiveness; however, this fails when\nthe initial results are not relevant. Instead of building a language\nmodel from retrieved results, we propose Generative Relevance\nFeedback (GRF) that builds probabilistic feedback models from long-\nform text generated from Large Language Models. We study the\neffective methods for generating text by varying the zero-shot gen-\neration subtasks: queries, entities, facts, news articles, documents,\nand essays. We evaluate GRF on document retrieval benchmarks\ncovering a diverse set of queries and document collections, and the\nresults show that GRF methods significantly outperform previous\nPRF methods. Specifically, we improve MAP between 5-19% and\nNDCG@10 17-24% compared to RM3 expansion, and achieve the\nbest R@1k effectiveness on all datasets compared to state-of-the-art\nsparse, dense, and expansion models.\nCCS CONCEPTS\nâ€¢ Information systems â†’Information retrieval.\nKEYWORDS\nPseudo-Relevance Feedback; Text Generation; Document Retrieval\nACM Reference Format:\nIain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative\nRelevance Feedback with Large Language Models. In Proceedings of the\n46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR â€™23), July 23â€“27, 2023, Taipei, Taiwan. ACM,\nNew York, NY, USA, 6 pages. https://doi.org/10.1145/3539618.3591992\n1 INTRODUCTION\nRecent advances in Large Language Models (LLMs) such as GPT-3\n[4], PaLM [6], and ChatGPT demonstrate new capabilities to gen-\nerate long-form fluent text. In addition, LLMs are being combined\nwith search engines, including BingGPT or Bard, to create sum-\nmaries of search results in interactive forms. In this work, we use\nthese models not to generate end-user responses but as input to\nthe core retrieval algorithm.\nThe classical approach to address the vocabulary mismatch\nproblem [2] is query expansion using Pseudo-Relevance Feedback\n(PRF) [1, 30, 31, 51], where the query is expanded using terms from\nthe top-ğ‘˜documents in a feedback set. This feedback set is obtained\nusing a first-pass retrieval, and the expanded query is then used\nfor a second-pass retrieval. While query expansion with PRF often\nimproves recall, its effectiveness hinges on the quality of the first-\npass retrieval. Non-relevant results in the feedback set introduce\nnoise and may pull the query off-topic.\nTo address this problem, we propose Generative Relevance Feed-\nback (GRF) that uses LLMs to generate text independent of first-pass\nretrieval. Figure 1 shows how we use an LLM to generate diverse\ntypes of query-specific text, before using these â€œgenerated docu-\nmentsâ€ as input for proven query expansion models [1]. We experi-\nment using the following types of generated text: keywords, entities,\nchain-of-thought reasoning, facts, news articles, documents, and\nessays. Furthermore, we find that combining text across all genera-\ntion subtasks results in 2-7% higher MAP versus the best standalone\ngeneration.\nFigure 1: GRF uses diverse LLM-generate text content for rel-\nevance feedback to contextualise the query.\nWe evaluate GRF1 on four established document ranking bench-\nmarks (see Section 4.1) and outperform several state-of-the-art\nsparse [1, 37], dense [19, 32, 41], and learned sparse [17] PRF mod-\nels. We find that long-form text generation (i.e. news articles, docu-\nments, and essays) is 7-14% more effective as a feedback set com-\npared to shorter texts (i.e. entities and keywords). Furthermore, the\ncloser the generation subtask is to the style of the target dataset (i.e.\nnews generation for newswire corpus or document generation for\nweb document corpus), the more effective GRF is. Lastly, combining\ntext across all generation subtasks results in 2-7% improvement in\nMAP over the best standalone generation subtask.\nThe contributions of this work are:\nâ€¢We propose GRF, a generative relevance feedback approach\nwhich builds a relevance model using text generated from an\nLLM.\nâ€¢We show LLM generated long-form text in the style of the target\ndataset is the most effective. Furthermore, combing text across\nmultiple generation subtasks can further improve effectiveness.\nâ€¢We demonstrate that GRF improves MAP between 5-19% and\nNDCG@10 between 17-24% compared to RM3 expansion, and\nachieves the best Recall@1000 compared to state-of-the-art sparse,\ndense and learned sparse PRF retrieval models.\n1Prompts and generated data for reproducibility: link\narXiv:2304.13157v1  [cs.IR]  25 Apr 2023\n2 RELATED WORK\nQuery Expansion : Lexical mismatch is a crucial issue in infor-\nmation retrieval, whereby a user query fails to capture their com-\nplete information need [2]. Query expansion methods [38] tackle\nthis problem by incorporating terms closer to the userâ€™s intended\nmeaning. One popular technique for automatic query expansion is\npseudo-relevance feedback (PRF), where the top ğ‘˜ documents from\nthe initial retrieval are assumed to be relevant. For example, Roc-\nchio [38], KL expansion [51], relevance modelling [30], LCE [31],\nand RM3 expansion [1]. Additionally, we have seen approaches that\nexpand queries with KG-based information [9, 29, 45, 47] or utilize\nquery-focused LLM vectors for query expansion [32].\nRecent advancements in dense retrieval [16, 21, 46] have led to\nthe development of vector-based PRF models [19], such as ColBERT\nPRF [41], ColBERT-TCT PRF [49], and ANCE-PRF [ 49]. Further-\nmore, SPLADE [11] is a neural retrieval model that uses BERT and\nsparse regularization to learn query and document sparse expan-\nsions. Recent work has leveraged query expansion with PRF of\nlearned sparse representations [17]. Unlike prior work, GRF does\nnot rely on pseudo-relevance feedback, instead generating relevant\ntext context for query expansion using LLMs.\nLLM Query Augmentation The emergence of LLMs has shown\nprogress across many different aspects of information retrieval [48].\nThis includes using LLMs to change the query representation, such\nas query generation and rewriting [15, 24, 34, 39, 44, 50], context\ngeneration [12, 23], and query-specific reasoning [10, 36]. For ex-\nample, Nogueira et al. [34] fine-tune a T5 model to generate queries\nfor document expansion for passage retrieval. More recent work by\nBonifacio et al. [3] shows that GPT3 can be effectively leveraged for\nfew-short query generation for dataset generation. Furthermore,\nLLMs have been used for conversational query re-writing [44] and\ngenerating clarifying questions [50].\nWe have also seen facet generation using T5 [24] and GPT3 [39]\nto improve the relevance or diversity of search results. While in\nQA, Liu et al. [23] sample various contextual clues from LLMs and\naugment and fuse multiple queries. For passage ranking, HyDe [12]\nuses InstructGPT [35] to generate hypothetical document embed-\ndings and use Contriever [14] for dense retrieval. Lastly, works have\nshown LLM generation used for query-specific reasoning [10, 36]\nto improve ranking effectiveness. Our approach differs from prior\nLLM augmentation approaches as we use LLMs to generate long-\nform text to produce a probabilistic expansion model to tackle\nquery-document lexical mismatch.\n3 GENERATIVE RELEVANCE FEEDBACK\nGenerative Relevance Feedback (GRF) tackles query-document lex-\nical mismatch using text generation for zero-shot query expansion.\nUnlike traditional PRF approaches for query expansion [1, 30, 31],\nGRF is not reliant on first-pass retrieval effectiveness to find useful\nterms for expansion. Instead, we leverage LLMs [ 5] to generate\nzero-shot relevant text content.\nWe build upon prior work on Relevance Models [1] to incorpo-\nrate the probability distribution of the terms generated by our LLM.\nThis approach enriches the original query with useful terms from\ndiverse generation subtasks, including keywords, entities, chain-\nof-thought reasoning, facts, news articles, documents, and essays.\nWe find that the most effective query expansions are: (1) long-form\ntext generations and (2) text content closer in style to the target\ndataset. In essence, we show that LLMs can effectively generate\nzero-shot text context close to the target relevant documents.\nLastly, we propose our full GRF method that combines text con-\ntent across all generation subtasks. The intuition behind this ap-\nproach is that if terms are used consistently generated across sub-\ntasks (i.e. within the entity, fact, and news generations), then these\nterms are likely useful for expansion. Additionally, multiple diverse\nsubtasks also help expose tail knowledge or uncommon synonyms\nhelpful for retrieval. We find this approach is more effective than\nany standalone generation subtasks.\n3.1 GRF Query Expansion\nFor a given query ğ‘„, Equation 1 shows how ğ‘ƒğºğ‘…ğ¹ (ğ‘¤|ğ‘…)is the\nprobability of a term,ğ‘¤, being in a relevant document, ğ‘…. Similar to\nRM3 [1], GRF expansion combines the probability of a term given\nthe original query ğ‘ƒ(ğ‘¤|ğ‘„)with the probability of a term within\nour LLM-generated document, ğ‘ƒ(ğ‘¤|ğ·ğ¿ğ¿ğ‘€), which we assume is\nrelevant. ğ›½(original query weight) is a hyperparameter to weigh the\nrelative importance of our generative expansion terms. Additionally,\nğœƒ (number of expansion terms) is a hyperparameter with ğ‘Šğœƒ being\nthe set of most probable LLM-generated terms.\nğ‘ƒğºğ‘…ğ¹ (ğ‘¤|ğ‘…)= ğ›½ğ‘ƒ(ğ‘¤|ğ‘„)+\n(\n(1 âˆ’ğ›½)ğ‘ƒ(ğ‘¤|ğ·ğ¿ğ¿ğ‘€), if ğ‘¤ âˆˆğ‘Šğœƒ.\n0, otherwise. (1)\n3.2 Generation Subtasks\nWe study how LLMs can generate relevant text, ğ·ğ¿ğ¿ğ‘€, across di-\nverse generation subtasks for GRF expansion. The 10 query-specific\ngeneration subtasks are:\nâ€¢Keywords (64 tokens) : Generates a list of the important words\nor phrases for the topic, similar to facet generation [24, 39].\nâ€¢Entities (64 tokens) : Generates a list of important concepts or\nnamed entities, similar to KG-based expansion approaches [9].\nâ€¢CoT-Keywords (256 tokens): Generate chain-of-thought (CoT) [43]\nreasoning to explain â€œwhyâ€ a list of keywords are relevant.\nâ€¢CoT-Entities (256 tokens): Generate CoT reasoning to explain\nâ€œwhyâ€ a list of entities are relevant.\nâ€¢Queries (256 tokens) : Generate a list of queries based on the\noriginal query, similar to [3].\nâ€¢Summary (256 tokens) : Generate a concise summary (or an-\nswer) to satisfy the query.\nâ€¢Facts: Generate a knowledge-intensive list of text-based facts on\nthe topic, which is close to [23].\nâ€¢Document (512 tokens) : Generate a relevant document based\non the query closest to a long-form web document.\nâ€¢Essay (512 tokens) : Generate a long-form essay-style response.\nâ€¢News (512 tokens) : Generate text in the style of a news article.\nThe full GRF expansion model concatenates text generated\nacross all subtasks to produceğ·ğ¿ğ¿ğ‘€. We then calculateğ‘ƒ(ğ‘¤|ğ·ğ¿ğ¿ğ‘€)\nusing this aggregated text, as outlined above. Section 5 shows that\nthe combination using the text across all types is most effective.\n4 EXPERIMENTAL SETUP\n4.1 Datasets\n4.1.1 Retrieval Corpora. TREC Robust04 [40] was created to in-\nvestigate methods targeting poorly performing topics. This dataset\ncomprises 249 topics, containing short keyword â€œtitlesâ€ and longer\nnatural-language \"descriptions\" queries. Relevance judgments are\nover a newswire collection of 528k long documents (TREC Disks 4\nand 5), i.e. FT, Congressional Record, LA Times, etc.\nCODEC [28] is a dataset that focuses on the complex informa-\ntion needs of social science researchers. Domain experts (economists,\nhistorians, and politicians) generate 42 challenging essay-style top-\nics. CODEC has a focused web corpus of 750k long documents,\nwhich includes news (BBC, Reuters, CNBC etc.) and essay-based\nweb content (Brookings, Forbes, eHistory, etc.).\nTREC Deep Learning (DL) 19/20 [7, 8] builds upon the MS\nMARCO web queries and documents [33]. The TREC DL dataset\nuses NIST annotators to provide judgments pooled to a greater\ndepth, containing 43 topics for DL-19 and 45 topics for DL-20. Both\nquery sets are predominately factoid-based [27].\n4.1.2 Indexing and Evaluation. For indexing we use Pyserini ver-\nsion 0.16.0 [20], removing stopwords and using Porter stemming.\nWe use cross-validation and optimise R@1k on standard folds for\nRobust04 [13] and CODEC [28]. On DL-19, we cross-validated on\nDL-20 and use the average parameters zero-shot on DL-19 (and\nvice versa for DL-20). We assess the system runs to a run depth\nof 1,000. With GRF being an initial retrieval model, recall-oriented\nevaluation is important, such as Recall@1000 and MAP to identify\nrelevant documents. We also analyse NDCG@10 to show precision\nin the top ranks. We use ir-measures for all our evaluations [25]\nand a 95% confidence paired-t-test for significance.\n4.2 GRF Implementation\nLLM Generation. For our text generation we use the GPT3 API [5].\nSpecifically, we use thetext-davinci-002 model with parameters:\ntemperature of 0.7, top_p of 1.0, frequency_penalty of 0.0, and pres-\nence_penalty of 0.0. We release all code, generation subtask prompts,\ngenerated text content and runs for reproducibility.\nRetrieval and Expansion To avoid query drift, all GRF runs in\nthe paper use a tuned BM25 system for the input initial run [37]. We\ntune GRF hyperparameters: the number of feedback terms (ğœƒ) and\nthe interpolation between the original terms and generative expan-\nsion terms (ğ›½). The tuning methodology is the same as BM25 and\nBM25 with RM3 expansion to make the GRF directly comparable;\nsee below for details.\n4.3 Comparison Methods\nBM25 [37]: Sparse retrieval method, we tune ğ‘˜1 parameter (0.1 to\n5.0 with a step size of 0.2) and ğ‘(0.1 to 1.0 with a step size of 0.1).\nBM25+RM3 [1]: For BM25 with RM3 expansion, we tuneğ‘“ğ‘_ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘ \n(5 to 95 with a step of 5), ğ‘“ğ‘_ğ‘‘ğ‘œğ‘ğ‘  (5 to 50 with a step of 5), and\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™_ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦_ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ (0.2 to 0.8 with a step of 0.1).\nCEQE [32]: Utilizes query-focused vectors for query expansion.\nWe use the CEQE-MaxPool runs provided by the author.\nSPLADE+RM3: We use RM3 [1] expansion with SPLADE [11]. We\nuse naver/splade-cocondenser-ensembledistil checkpoint and\nPyseriniâ€™s [ 20] â€œimpactâ€ searcher for max-passage aggregation.\nWe tune ğ‘“ğ‘_ğ‘‘ğ‘œğ‘ğ‘  (5,10,15,20,25,30), ğ‘“ğ‘_ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘  (20,40,60,80,100), and\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™_ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦_ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ (0.1 to 0.9 with a step of 0.1).\nTCT+PRF: [18] is a Roccio PRF approach using ColBERT-TCT [22].\nWe employ a max-passage approach with TCT-ColBERT-v2-HNP\ncheckpoint. We tune Roccio PRF parameters: ğ‘‘ğ‘’ğ‘ğ‘¡â„ (2,3,5,7,10,17),\nğ›¼ (0.1 to 0.9 with a step of 0.1), and ğ›½ (0.1 to 0.9 with a step of 0.1).\nColBERT+PRF [41]: We use the runs provided by Wang et al. [42],\nwhich use pyterrier framework [26] for ColBERT-PRF retrieval.\n5 RESULTS & ANALYSIS\n5.1 RQ1: What generative content is most\neffective for query expansion?\nTable 1 shows the effectiveness of generative feedback with varying\nunits of text (Keywords-News) and our full hybrid method that uses\ntext from all subtasks. We test for significant improvements against\nBM25 with RM3 expansion, to ascertain whether our zero-shot\ngenerative feedback methods improve over RM3 expansion.\nGeneration subtasks that target short text span or lists (Key-\nwords, Entities, Keywords-COT, Entities-COT, and Queries) do not\nTable 1: GRF with different generation subtasks. Significant improvements against BM25+RM3 (â€œ+â€) and best system ( bold).\nRobust04 -Title CODEC DL-19 DL-20\nNDCG@10 MAP R@1k NDCG@10 MAP R@1k NDCG@10 MAP R@1k NDCG@10 MAP R@1k\nBM25 0.445 0.252 0.705 0.316 0.214 0.783 0.531 0.335 0.703 0.546 0.413 0.811\nBM25+RM3 0.451 0.292 0.777 0.326 0.239 0.816 0.541 0.383 0.745 0.513 0.418 0.825\nGRF-Keywords 0.435 0.252 0.717 0.327 0.218 0.748 0.565 0.377 0.749 0.554 0.435 0.822\nGRF-Entities 0.452 0.252 0.698 0.341 0.216 0.750 0.531 0.363 0.741 0.544 0.414 0.824\nGRF-CoT-Keywords 0.436 0.248 0.704 0.327 0.239 0.774 0.550 0.382 0.748 0.542 0.423 0.817\nGRF-CoT-Entities 0.450 0.252 0.714 0.355 0.243 0.789 0.563 0.389 0.757 0.552 0.430 0.832\nGRF-Queries 0.450 0.257 0.710 0.347 0.233 0.773 0.551 0.367 0.760 0.568 0.439 0.851\nGRF-Summary 0.491+ 0.277 0.730 0.398+ 0.260 0.796 0.577 0.414 0.761 0.585+ 0.472+ 0.865\nGRF-Facts 0.501+ 0.284 0.744 0.353 0.255 0.795 0.569 0.401 0.769 0.583+ 0.459+ 0.871\nGRF-Document 0.480+ 0.276 0.728 0.376+ 0.265 0.795 0.618+ 0.428+ 0.787+ 0.589+ 0.476+ 0.872\nGRF-Essay 0.494+ 0.284 0.736 0.405+ 0.270+ 0.803 0.609+ 0.421+ 0.779+ 0.551 0.440 0.859\nGRF-News 0.501+ 0.287 0.745 0.398+ 0.270+ 0.828 0.609 0.409 0.777 0.578+ 0.457 0.853\nGRF 0.528+ 0.307 0.788 0.405+ 0.285+0.830 0.620+ 0.441+0.797+ 0.607+ 0.486+0.879+\nTable 2: GRF against state-of-the-art PRF models. Significant improvements against BM25+RM3 (â€œ+â€) and best system ( bold).\nRobust04 -Title CODEC DL-19 DL-20\nnDCG@10 MAP R@1k nDCG@10 MAP R@1k nDCG@10 MAP R@1k nDCG@10 MAP R@1k\nBM25+RM3 0.451 0.292 0.777 0.326 0.239 0.816 0.541 0.383 0.745 0.513 0.418 0.825\nCEQE-MaxPool 0.474 0.310+0.764 - - - 0.518 0.378 0.746 0.473 0.396 0.841\nSPLADE+RM3 0.418 0.248 0.703 0.311 0.216 0.770 0.566 0.328 0.651 0.533 0.379 0.784\nTCT+PRF 0.493 0.274 0.684 0.358 0.239 0.757 0.670+ 0.378 0.684 0.618+ 0.442 0.784\nColBERT-PRF 0.467 0.272 0.648 - - - 0.668+ 0.385 0.625 0.615+ 0.489+0.813\nGRF (Ours) 0.528+ 0.307 0.788 0.405+ 0.285+0.830 0.620+ 0.441+0.797+0.607+ 0.486+ 0.879+\noffer significantly improves over RM3 expansion. Conversely, sub-\ntasks targeting long text generation (Summary, Facts, Document,\nEssay, News) significantly improve at least two datasets over RM3\nexpansions. This indicates that more terms generated from the\nLLM provide a better relevance model, and increases MAP between\n7-14% when we compare these two categories.\nFurthermore, we find most effective generation subtasks are\naligned with the style of the target dataset. For example, Facts\nand News are the best standalone generation methods across all\nmeasures on Robust04, where the dataset contains fact-heavy topics\nand a newswire corpus. Additionally, Essay and News are the best\ngeneration subtasks on CODEC across all measures, which aligns\nwith its essay-style queries over a news (BBC, Reuters, CNBC, etc.)\nand essay-style (Brookings, Forbes, eHistory, etc.) corpus. Lastly,\nDocument is the best generation subtask across DL-19 and DL-20,\naligning with MS Marcos web document collection. Overall, this\nfinding supports that LLM generative content in the styles of the\ntarget dataset is the most effective.\nAlthough we see significant improvements from some stan-\ndalone generation subtasks, particularly NDCG@10 (15/40 subtasks\nacross the datasets), the full GRF method is consistently as good if\nnot better than any standalone subtask. Specifically, GRF improves\nNDCG by 0.0-5.4%, MAP by 2.1-7.0% and R@1k by 0.2-5.8% across\nthe datasets. This shows that combining LLM-generated text from\nvarious generation subtasks is a robust and effective method of\nrelevance modelling.\nLastly, these results show that GRF expansion from generated\ntext is consistently better, often significantly, than RM3 expansion\nthat uses documents from the target corpus. Specifically, we find\nsignificant improvement on all measures across DL-19 and DL-20,\nNDCG@10 and MAP on CODEC, and NDCG@10 on Robust04\ntitles. Although not included for space limitations, on Robust04\ndescription queries, GRF shows significant improvements with an\nNDCG of 0.550, MAP of 0.318, and R@1k of 0.776.\nThese results strongly support that LLM generation is an effec-\ntive query expansion method without relying on first-pass retrieval\neffectiveness. For example, we look at the hardest 20% of Robust04\ntopics ordered by NDCG@10; we find that RM3 offers minimal\nuplift and only improves NDCG@10 by +0.006, MAP by +0.008,\nand R@1k by +0.052. In contrast, GRF is not reliant on first-pass\nretrieval effectiveness, and GRF improves NDCG@10 by +0.145,\nMAP by +0.068, and R@1k by +0.165 (a relative improvement of\n+100-200% on NDCG@10 and MAP).\n5.2 RQ2: How does GRF compare to\nstate-of-the-art PRF models?\nTable 2 shows GRF against state-of-the-art sparse, dense, and learned\nsparse PRF models across target datasets. This allows us to directly\ncompare GRFâ€™s unsupervised term-based queries against PRF meth-\nods that use more complex LLM-based embeddings. We conduct\nsignificance testing against BM25 with RM3 expansion.\nGRF has the best R@1k across all datasets and has comparable\nand often better effectiveness in the top ranks. Specifically, on more\nchallenging datasets, such as CODEC and Robust04 titles, GRF is the\nbest system across all measures, except Robust04 titles MAP, which\nis 0.003 less than CEQE. Although not included for space limitations,\nGRF is also the most effective system on Robust04 descriptions\nacross all measures. GRF vastly outperforms dense retrieval and\ndense PRF on these challenging datasets, with a performance gap\nof 7-14% on NDCG@20, 13-21% MAP, and 10-22% R@1k.\nDense retrieval has been shown to be highly effective on the more\nfactoid-focused datasets, such as DL-19 and DL-20. However, as well\nas the best R@1k, our unsupervised GRF queries have comparable\nNDCG@10 and MAP scores to dense PRF models. This is juxtaposed\nto other sparse methods (BM25 and BM25 with RM3 expansions)\nor LLM expansion (CEQE), which have much poorer precision in\nthe top ranks. Overall, this supports that generative expansion is a\nhighly effective initial retrieval method across various collections\nand query types.\n6 CONCLUSION\nTo our knowledge, this is the first work to study the use of long-form\ntext generated from large-language models for query expansion.\nWe show that generating long-form text in news-like and essay-\nlike formats is effective input for probabilistic query expansion\napproaches. The results on document retrieval on multiple corpora\nshow that the proposed GRF approach outperforms models that use\nretrieved documents (PRF). The results show GRF improves MAP\nbetween 5-19% and NDCG@10 between 17-24% when compared to\nRM3 expansion, and achieves the best Recall@1000 compared to\nstate-of-the-art PRF retrieval models. We envision GRF as one of\nthe many new emerging methods that use LLM-generated content\nto improve the effectiveness of core retrieval tasks.\n7 ACKNOWLEDGEMENTS\nThis work is supported by the 2019 Bloomberg Data Science Re-\nsearch Grant and the Engineering and Physical Sciences Research\nCouncil grant EP/V025708/1.\nREFERENCES\n[1] Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey,\nXiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. UMass at TREC 2004:\nNovelty and HARD. Computer Science Department Faculty Publication Series\n(2004), 189.\n[2] Nicholas J Belkin, Robert N Oddy, and Helen M Brooks. 1982. ASK for information\nretrieval: Part I. Background and theory. Journal of documentation (1982).\n[3] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nInpars: Unsupervised dataset generation for information retrieval. In Proceedings\nof the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 2387â€“2392.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems , H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877â€“1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al . 2020. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165 (2020).\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling\nwith Pathways. arXiv:cs.CL/2204.02311\n[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview\nof the TREC 2020 deep learning track. InText REtrieval Conference (TREC) . TREC.\n[8] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M\nVoorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint\narXiv:2003.07820 (2020).\n[9] Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion\nusing knowledge base links. In Proceedings of the 37th international ACM SIGIR\nconference on Research & development in information retrieval . 365â€“374.\n[10] Fernando Ferraretto, Thiago Laitz, Roberto Lotufo, and Rodrigo Nogueira.\n2023. ExaRanker: Explanation-Augmented Neural Ranker. arXiv preprint\narXiv:2301.10521 (2023).\n[11] Thibault Formal, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE:\nSparse lexical and expansion model for first stage ranking. In Proceedings of\nthe 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 2288â€“2292.\n[12] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot\nDense Retrieval without Relevance Labels.arXiv preprint arXiv:2212.10496 (2022).\n[13] Samuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison\nof retrieval models using term dependencies. Ir, University of Massachusetts\n(2014).\n[14] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Infor-\nmation Retrieval with Contrastive Learning. https://doi.org/10.48550/ARXIV.\n2112.09118\n[15] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nJakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Mod-\nels as Efficient Dataset Generators for Information Retrieval. arXiv preprint\narXiv:2301.01820 (2023).\n[16] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert. In Proc. of SIGIR . 39â€“48.\n[17] Carlos Lassance and StÃ©phane Clinchant. 2023. Naver Labs Europe (SPLADE)@\nTREC Deep Learning 2022. arXiv preprint arXiv:2302.12574 (2023).\n[18] Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and G. Zuccon.\n2021. Pseudo Relevance Feedback with Deep Language Models and Dense Re-\ntrievers: Successes and Pitfalls. ArXiv abs/2108.11044 (2021).\n[19] Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, and\nGuido Zuccon. 2022. Improving Query Representations for Dense Retrieval with\nPseudo Relevance Feedback: A Reproducibility Study. In European Conference on\nInformation Retrieval . Springer, 599â€“612.\n[20] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,\nand Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible infor-\nmation retrieval research with sparse and dense representations. In Proceedings\nof the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 2356â€“2362.\n[21] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling dense\nrepresentations for ranking using tightly-coupled teachers. arXiv preprint\narXiv:2010.11386 (2020).\n[22] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021. In-batch negatives\nfor knowledge distillation with tightly-coupled teachers for dense retrieval. In\nProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-\n2021). 163â€“173.\n[23] Linqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel, and Pontus Stenetorp.\n2022. Query Expansion Using Contextual Clue Sampling with Language Models.\narXiv preprint arXiv:2210.07093 (2022).\n[24] Sean MacAvaney, Craig Macdonald, Roderick Murray-Smith, and Iadh Ounis.\n2021. IntenT5: Search Result Diversification using Causal Language Models.\narXiv preprint arXiv:2108.04026 (2021).\n[25] Sean MacAvaney, Craig Macdonald, and Iadh Ounis. 2022. Streamlining Evalua-\ntion with ir-measures. In European Conference on Information Retrieval . Springer,\n305â€“310.\n[26] Craig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. 2021.\nPyTerrier: Declarative experimentation in Python from BM25 to dense retrieval. In\nProceedings of the 30th ACM International Conference on Information & Knowledge\nManagement. 4526â€“4533.\n[27] Iain Mackie, Jeffrey Dalton, and Andrew Yates. 2021. How deep is your learn-\ning: The DL-HARD annotated deep learning dataset. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 2335â€“2341.\n[28] Iain Mackie, Paul Owoicho, Carlos Gemmell, Sophie Fischer, Sean MacAvaney,\nand Jeffery Dalton. 2022. CODEC: Complex Document and Entity Collection.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval .\n[29] Edgar Meij, Dolf Trieschnigg, Maarten De Rijke, and Wessel Kraaij. 2010. Con-\nceptual language models for domain-specific retrieval. Information Processing &\nManagement 46, 4 (2010), 448â€“469.\n[30] Donald Metzler and W Bruce Croft. 2005. A markov random field model for\nterm dependencies. In Proceedings of the 28th annual international ACM SIGIR\nconference on Research and development in information retrieval . 472â€“479.\n[31] Donald Metzler and W Bruce Croft. 2007. Latent concept expansion using\nmarkov random fields. In Proceedings of the 30th annual international ACM SIGIR\nconference on Research and development in information retrieval . 311â€“318.\n[32] Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, and James Allan. 2021. Ceqe:\nContextualized embeddings for query expansion. In Advances in Information\nRetrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event,\nMarch 28â€“April 1, 2021, Proceedings, Part I 43 . Springer, 467â€“482.\n[33] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. Ms marco: A human-generated machine reading\ncomprehension dataset. (2016).\n[34] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\ndocTTTTTquery. Online preprint 6 (2019).\n[35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730â€“27744.\n[36] Jayr Pereira, Robson Fidalgo, Roberto Lotufo, and Rodrigo Nogueira. 2023. Vis-\nconde: Multi-document QA with GPT-3 and Neural Reranking. InAdvances in\nInformation Retrieval: 45th European Conference on Information Retrieval, ECIR\n2023, Dublin, Ireland, April 2â€“6, 2023, Proceedings, Part II . Springer, 534â€“543.\n[37] Stephen E Robertson and Steve Walker. 1994. Some simple effective approxi-\nmations to the 2-poisson model for probabilistic weighted retrieval. In SIGIRâ€™94.\nSpringer, 232â€“241.\n[38] Joseph Rocchio. 1971. Relevance feedback in information retrieval. The Smart\nretrieval system-experiments in automatic document processing (1971), 313â€“323.\n[39] Chris Samarinas, Arkin Dharawat, and Hamed Zamani. 2022. Revisiting Open\nDomain Query Facet Extraction and Generation. In Proceedings of the 2022 ACM\nSIGIR International Conference on Theory of Information Retrieval . 43â€“50.\n[40] Ellen M. Voorhees. 2004. Overview of the TREC 2004 Robust Track. InProceedings\nof the Thirteenth Text REtrieval Conference (TREC 2004) . Gaithersburg, Maryland,\n52â€“69.\n[41] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2022. ColBERT-\nPRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document\nRetrieval. ACM Transactions on the Web (2022).\n[42] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2023. ColBERT-\nPRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document\nRetrieval. ACM Transactions on the Web 17, 1 (2023), 1â€“39.\n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi,\nQuoc V Le, Denny Zhou, et al. [n. d.]. Chain-of-Thought Prompting Elicits Rea-\nsoning in Large Language Models. In Advances in Neural Information Processing\nSystems.\n[44] Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hannaneh Hajishirzi, Mari\nOstendorf, and Gaurav Singh Tomar. 2022. CONQRR: Conversational Query\nRewriting for Retrieval with Reinforcement Learning. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing . Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates, 10000â€“10014.\nhttps://aclanthology.org/2022.emnlp-main.679\n[45] Chenyan Xiong and Jamie Callan. 2015. Query Expansion with Freebase. In\nProceedings of the 2015 International Conference on The Theory of Information\nRetrieval (ICTIR â€™15) . Association for Computing Machinery, New York, NY, USA,\n111â€“120. https://doi.org/10.1145/2808194.2809446\n[46] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett,\nJunaid Ahmed, and Arnold Overwijk. [n. d.]. Approximate Nearest Neighbor Neg-\native Contrastive Learning for Dense Text Retrieval. In International Conference\non Learning Representations .\n[47] Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009. Query Dependent Pseudo-\nRelevance Feedback Based on Wikipedia. In Proceedings of the 32nd International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR â€™09) . Association for Computing Machinery, New York, NY, USA, 59â€“66.\nhttps://doi.org/10.1145/1571941.1571954\n[48] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. In WSDM. 1154â€“1156.\n[49] HongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. Improving Query Repre-\nsentations for Dense Retrieval with Pseudo Relevance Feedback. InProceedings of\nthe 30th ACM International Conference on Information & Knowledge Management .\n3592â€“3596.\n[50] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck.\n2020. Generating clarifying questions for information retrieval. In Proceedings of\nthe web conference 2020 . 418â€“428.\n[51] Chengxiang Zhai and John Lafferty. 2001. Model-based feedback in the lan-\nguage modeling approach to information retrieval. In Proceedings of the tenth\ninternational conference on Information and knowledge management . 403â€“410."
}