{
    "title": "How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging",
    "url": "https://openalex.org/W4387559910",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5005627888",
            "name": "Qianou Ma",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5101501252",
            "name": "Hua Shen",
            "affiliations": [
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5062550465",
            "name": "Kenneth R. Koedinger",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5004225142",
            "name": "Tongshuang Wu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2869536260",
        "https://openalex.org/W4245480127",
        "https://openalex.org/W4291476001",
        "https://openalex.org/W4241080928",
        "https://openalex.org/W4288059420",
        "https://openalex.org/W2060765209",
        "https://openalex.org/W3138026483",
        "https://openalex.org/W2078518223",
        "https://openalex.org/W2560703437",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W4283775237",
        "https://openalex.org/W4384659608",
        "https://openalex.org/W2990670679",
        "https://openalex.org/W4381956199",
        "https://openalex.org/W4307473961",
        "https://openalex.org/W4381587445",
        "https://openalex.org/W4380186444",
        "https://openalex.org/W2809259768",
        "https://openalex.org/W2090266317",
        "https://openalex.org/W4283157303",
        "https://openalex.org/W2085617435",
        "https://openalex.org/W4310744613",
        "https://openalex.org/W2102734438",
        "https://openalex.org/W4386584914",
        "https://openalex.org/W1574060188"
    ],
    "abstract": "Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as \"AI pair programmers,\" it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.",
    "full_text": "How to Teach Programming in the AI Era?\nUsing LLMs as a Teachable Agent for Debugging\nQianou Ma1, Hua Shen2, Kenneth Koedinger1, and Sherry Tongshuang Wu1\n1 Carnegie Mellon University, Pittsburgh PA, USA\n{qianoum,krk,sherryw}@cs.cmu.edu\n2 University of Michigan, Ann Arbor MI, USA\nhuashen@umich.edu\nAbstract. Large Language Models (LLMs) now excel atgenerative skills\nand can create content at impeccable speeds. However, they are imper-\nfect and still make various mistakes. In a Computer Science education\ncontext, as these models are widely recognized as “AI pair program-\nmers,” it becomes increasingly important to train students on evalu-\nating and debugging the LLM-generated code. In this work, we intro-\nduce HypoCompass, a novel system to facilitate deliberate practice on\ndebugging, where human novices play the role of Teaching Assistants\nand help LLM-powered teachable agents debug code. We enable effective\ntask delegation between students and LLMs in this learning-by-teaching\nenvironment: students focus on hypothesizing the cause of code errors,\nwhile adjacent skills like code completion are offloaded to LLM-agents.\nOur evaluations demonstrate that HypoCompass generates high-quality\ntraining materials (e.g., bugs and fixes), outperforming human counter-\nparts fourfold in efficiency, and significantly improves student perfor-\nmance on debugging by 12% in the pre-to-post test.\nKeywords: LLM · teachable agent · debugging · CS1.\n1 Introduction\nLLMs are becoming an integral part of software development — commercialized\ntools like GitHub Copilot are now advertised as “your AI pair programmer”\nand generate up to 46% of users’ code [6]. Despite their prevalence, LLMs of-\nten produce unpredictable mistakes [11], e.g., GPT-4 can still make mistakes\n17% of the time in coding tasks for introductory and intermediate program-\nming courses [22]. The impressive yet imperfect generative capabilities of LLMs,\ncoupled with the associated risks of excessive reliance on these models, under-\nscore the importance of teaching evaluation skills to students. In the context of\nprogramming, students must improve their debugging and testing skills [2].\nHowever, debugging tends to be overlooked in formal educational curricula,\nespecially in introductory Computer Science classes ( i.e., CS1) [21]. Prior re-\nsearch has outlined various factors contributing to the absence of debugging\ninstruction, such as instructors’ limited time budget for developing specialized\narXiv:2310.05292v5  [cs.HC]  10 Oct 2024\n2 Q. Ma et al.\nSend\nA B C\nC\nD\nE\nF\nFig. 1: In HypoCompass, given a programming problem description (A), a stu-\ndent user (in the role of a Teaching Assistant) needs to compile a test suite (B)\nand assist multiple LLM-simulated agents (e.g., Bob, Chelsea, Dave) in an Office\nHour Queue (C) through a chat interface (E). Each LLM-agent acts as a novice\nseeking help with a buggy solution (D) and provides feedback to the user (F).\ndebugging materials and assessments [19]. Consequently, students primarily learn\ndebugging from working on their own mistakes, which can be rather frustrating\n— they must invest substantial time and effort inhypothesizing the cause of bugs\nwhile grappling with other cognitively demanding tasks, such as understanding\nand writing code. These challenges prompt us to ask:\nResearch Question: Can we train students to improve debugging skills by\nproviding explicit and scaffolded practice with minimal cost to instructor time?\nIn this work, we focus on training students’ abilities in hypothesis construc-\ntion, a critical step in debugging as established by prior work [29,30]. We in-\ntroduce HypoCompass (Figure 1, Section 3), an interactive, LLM-augmented\nintelligent tutoring system for debugging. Leveraging LLMs’material generation\ncapability, we have these models imitate CS1 students who have written buggy\ncode and require assistance from Teaching Assistants (TAs). Human novice stu-\ndents assume the role of the TA, who helps troubleshoot these bugs. This enables\nstudents to deliberately practice the skill of hypothesizing about the defects of\nLLM-generated code, delegating other tasks not core to hypothesis construc-\ntion (e.g., code completion) to the LLM. As a result, HypoCompass fosters an\nengaging learning environment using the teachable agent framework [3] and pro-\nvides students with guided exposure to LLM-generated bugs. We also employ\nprompting strategies such as focused task formation and over-generate-then-\nselect to improve LLM generation quality in HypoCompass (Section 4).\nWe conducted two evaluation studies and found that HypoCompass saves\ninstructors’ time in material generation and is beneficial to student learning .\nHypoCompass: LLM-based Hypothesis Construction Tutor 3\nIn our LLM evaluation study (Section 5), expert inspections on six practice\nproblems and 145 buggy programs showed that HypoCompass achieved a 90%\nsuccess rate in generating and validating a complete set of materials, four times\nfaster than human generation. Our learning evaluation study with 19 novices\n(Section 6) showed that HypoCompass significantly improved students’ pre-to-\npost test performance by 12% and decreased their completion time by 14%.\nIn summary, we contribute:\n– A pragmatic solution that balances the benefits and risks of LLMs in learning.\nWe use LLMs to prepare students to engage with imperfect LLMs, and we\nhighlight the importance of role-playing for practical LLM application and\ntask delegation to help students focus on essential skills.\n– A theoretically grounded instructional design to enhance debugging skills. To\nthe best of our knowledge, we are the first to provide aligned instruction and\nassessments on the hypothesis construction learning objectives, i.e., forming\nhypotheses about the source of error, a core bottleneck in debugging [25].\n2 Related Works\nThe Debugging Process. Debugging is a complicated process of various cog-\nnitively demanding tasks, including understanding the code, finding bugs, and\nfixing bugs, with the first two considered primary bottlenecks [19,25]. While\nmany studies have attempted to improve students’ code understanding [12],\nthere is limited instruction on bug finding. Researchers characterize the cognitive\nmodel of bug finding as a hypothesis construction process, including initializing,\nmodifying, selecting, and verifying hypotheses (Figure 2B) [29]. This process\nis challenging: prior works show that novices struggle to systematically gener-\nate comprehensive hypotheses and identify the right hypothesis, in contrast to\nexperts [8,7]. Hence, we emphasize teaching students to construct accurate hy-\npotheses about bugs and develop comprehensive hypotheses about potential bugs .\nTutors and Tools for Debugging Training. Prior studies [19] and on-\nline discussions [21] indicate that teaching debugging is challenging and is rarely\nincluded in CS1 curricula, due to logistical challenges like the lack of instruc-\ntional time and resources [5,10]. Existing tools demand instructor effort and\noften focus on the full debugging process, improving bug-fixing accuracy and\nefficiency [1,15]. In contrast, few studies emphasize accurate or comprehensive\nhypothesis construction (and they tend to be language-specific) [13,25]. To fill\nin the gap, we design HypoCompass to provide deliberate practice [9] on hy-\npothesis construction, and use the LLM generation capability to provide easily\nadaptable and targeted exercises with immediate feedback.\nLLM Capabilities for CS Learning. LLMs can perform well in a CS1\nclassroom [22], but concerns about misuse and LLM errors limit their use in\neducation [2]. Therefore, current deployments tend to focus on generating in-\nstructional materials (e.g., questions [24]). In our work, HypoCompass uses the\nLLM to generate inter-dependent materials in an integrated process and frame\n4 Q. Ma et al.\nDebug. Process Model\nInitialize hypothesis set\nModify hypothesis set\nSelect a hypothesis\nVerify hypothesis\nLO1: Comprehensive \nhypothesis construction\nWrite test suite\nAdd test case\nEvaluate expl.\nSelect expl.\nMake test suite more complete\nCorrectly map explanations to bugs\nLO2: Accurate \nhypothesis construction\nHypoCompass\nBug ﬁxes\nBug explanations\nLearning Objectives Students’ primary tasks\nStudent ﬂow LLM generation1 2\nA B C D\nTest category hint\nTest case hint\nBuggy codes\nFig. 2: To enable deliberate practice, we establish a close mapping between the\n(A) learning objectives, (B) the cognitive debugging process model, (C) the\nHypoCompass interaction flow, and (D) the primary tasks students perform in\nHypoCompass. We offload various material generation tasks to LLMs (C 2).\nthe LLM as a student asking for help [3], such that human novices can em-\nbrace imperfections in LLMs. Two unique capabilities of LLMs power this: (1)\nLLMs can simulate different personas and tutoring interactions [18]; (2) LLMs\nmake common mistakes and natural bugs similar to humans [20], which can be\nused as buggy code practice examples. We adapt and develop various prompting\nmethods [27] to enhance the quality of LLM generations.\n3 The Design of HypoCompass\nGrounded in the cognitive process [29] and the novice-expert difference in hypothesis-\ndriven debugging (Section 2), we specify two crucial learning components for\nHypoCompass: comprehensive and accurate hypothesis construction. Prior work\nshows that hypothesis construction is closely connected with testing [30]: each\nadditional test case should, ideally, be a hypothesis about what can go wrong in\nthe program. In turn, a comprehensive test suite (i.e., a set of test cases) should\nallow an effective debugger to construct a accurate hypothesis about why the\nprogram is wrong. We thus design toward two learning objectives (Figure 2A,D):\nLO1 Comprehensive Hypothesis Construction : Construct a comprehen-\nsive test suite that well covers the possible errors for the given problem.\nLO2 Accurate Hypothesis Construction : Given the failed test cases, con-\nstruct an accurate explanation of how the program is wrong.\nInterface and Key Components. We designed HypoCompass through an\niterative development process with 10 pilots, including CS1 students, TAs, and\ninstructors. In the resulting interface (Figure 1), a human student would be asked\nto play the role of a TA where they help an LLM-simulated student (LLM-agent)\nin debugging. They need to write and sort test cases into categories (Figure 1B)\nthat represent different hypotheses of what inputs may trigger errors in code.\nOnce the student is satisfied with their test suite,HypoCompass shows them\nan Office Hour Queue (OHQ) simulator (Figure 1C). As the student interacts\nHypoCompass: LLM-based Hypothesis Construction Tutor 5\nFeedback to incorrect test cases Feedback to correct explanation selection\nFeedback to wrong explanation selection\nSend\nStarter test category hint Explanations to choose from\nDescriptive test case hint\n1\n2\n3\n1 2\n3\nCorrect explanation\nDistracting explanations\nFig. 3(a): HypoCompass offers (1) test category hintsto help write a comprehensive\ntest suite systematically; (2) test case hintsto help students add missing test scenarios;\n(3) candidate explanation poolto clarify misconceptions of alternative explanations.\nFeedback to incorrect test cases Feedback to correct explanation selection\nFeedback to wrong explanation selection\nSend\nStarter test category hint Explanations to choose from\nDescriptive test case hint\n1\n2\n3\n1 2\n3\nCorrect explanation\nDistracting explanations\nFig. 3(b): HypoCompass provides immediate feedback to (1) incorrect test cases, en-\nsuring students understand the code behavior; (2) correct explanations, as correct code\nfixes; (3) incorrect explanations, as confusion messages from the LLM-agent.\nwith each LLM-agent, the agent presents a buggy code snippet (Figure 1D). The\nstudent guides the LLM-agent in debugging code through a dialog interface (Fig-\nure 1E), selecting or creating test cases that reflect their hypotheses of the bug,\nand selecting explanations for the bug among a pool of candidate natural lan-\nguage explanations. These candidates each explain a different bug, representing\nalternative hypotheses that may confuse students ( e.g., Figure 3a3).\nThe LLM-agent then uses the test case and explanation to revise the code,\nproviding immediate feedback to the student (Figure 3b). If the explanation is\ncorrect, the agent will conduct minimal code fixes, and present the color-coded\nedits as feedback (Figure 1F, a zoomed-in view is in Figure 3b 2). Otherwise,\nthe LLM-agent will ask the student to reflect on their hypothesis by responding\nwith a confusion message that highlights the discrepancy between the student’s\nexplanation and the actual code behavior (Figure 3b 3).\nOnce the student correctly confirms that all the bugs are fixed, they can move\nto help the next LLM-agent (Figure 1C). Upon completion, HypoCompass\nwill provide the next round of exercises with another programming problem.\nWhile the numbers are configurable, by default HypoCompass includes two\nprogramming exercises, each with three LLM-agents (buggy programs).\n6 Q. Ma et al.\ndef first_num_greater_than (numbers_list, key):\n  for i in range(len(numbers_list)):\n    if numbers_list[i] > key:\n      return num\n    else:\n      return None\nExplanation: Your code returns None if the ﬁrst number \nin the list is not greater than the key. It doesn't check the \nrest of the numbers in the list \nFix: Move “return None” out of the for loop so that it only \nexecutes after all numbers have been checked.\nGenerate explanations and ﬁx statements, per bug\nWrite a Python function first_num_greater_than \n(numbers_list, key) that takes a list of integers…\ndef first_num_greater_than (numbers_list, key):\n  for i in range(len(numbers_list)):\n    if numbers_list[i] > key:\n      return num\n     return None\nGenerate multiple buggy codes per problem\nGenerate bug ﬁxes, per (bug, ﬁx statement)\nWrite a test case to cover the scenario where the \nkey number is presented in the list, but there are \nnumbers greater than the key\nassert(first_num_greater_than([2,3],2) == 3)\nGenerate descriptive test case hint, per test case\nOutput: Multiple \npractice suites\nNo numbers in list greater than key\nassert(first_num_greater_than([1],5) == 0)\nassert(first_num_greater_than([],2) == 0)\nGenerate starter test case categories hint, per cluster\nInput: Problem descriptions      + pre-deﬁned test suites\nLLM-generated materials\nFig. 4: Examples of inputs and outputs to the LLM material generation pipeline.\nWe highlight the two most essential components of the interaction:\n– Frame imperfect LLMs through role-play. We use the LLM to simulate\nstudents who wrote bugs and have human novices offer help. This teachable\nagent setup supports learning, helping students reflect on their knowledge\nand reason through diverse bugs [23]. Having students work through “other\npeople’s errors” also boosts their motivation and protects their self-efficacy [3].\nMore importantly, it actively involves novices in identifying bugs in LLM-\ngenerated code, enabling guided exposure to LLM imperfectness .\n– Task delegation between students and LLMs. To ensure deliberate\npractice on comprehensive and accurate hypothesis construction, students\nprimarily engage in two tasks corresponding to each learning objective (Fig-\nure 2D): (1) making the test suite more complete (LO1); and (2) correctly\nmapping explanations to bugs (LO2). We align student interaction flow (Fig-\nure 2C1) with the cognitive model of debugging [29] (Figure 2B). LLMs take\nover other tasks that areindirectly related to the core learning goals, including\ngenerating diverse bugs and fixes, which frees students from code writing. We\nalso use LLMs to support scaffolding, generate hints (Figure 3a), and provide\nimmediate feedback throughout the practice (Figure 3b).\n4 LLM Integration\nAs shown in Figure 2C2, we use LLM to generate five types of materials: (1) test\ncase category hints, (2) test case hints, (3) buggy programs, (4) explanations\nof bugs, and (5) programs with bugs fixed. We reduce instructor workload by\ngenerating practices using just a problem description, a reference solution, and\na reference test suite with about 10 inputs, and we further minimize human\nverification overhead with optimized prompts and automated algorithms. Our\ngeneration process is detailed in Figure 4, example prompts are in Table 1, and\nHypoCompass: LLM-based Hypothesis Construction Tutor 7\nTable 1: Prompts and temperatures (Temp.) for generating bugs, explanations,\nand fixes. The temperature is set higher for more diverse and random outputs.\nMaterial Generation goal Temp.\nBuggy code To over-generate bugs with mixed quality for further selection. 0.7\n[Sys.] You are a novice student in intro CS, you make mistakes and write buggy code.\n[User] Problem Description: {problem description}\nWrite different buggy solutions with common mistakes like novice students:\nBug expl. &\nfix instruct.\nTo describe each unique bug, and write a corresponding fix instruction. If there are\nmultiple bugs in the code, generate their explanations and fixes separately.\n0.3\n[Sys.] You are a helpful and experienced TA of an introductory programming class.\n[User] Hi, I’m a student in your class. I’m having trouble with this problem in the programming\nassignment: {problem description} Here’s my buggy code: {buggy code} What’s wrong with my\ncode? List all the unique bugs included, but do not make up bugs. For each point, put in the\nformat of: {explanation: accurate and concise explanation of what the code does and what the\nbug is, for a novice, fix: how to fix the bug, within 30 words}\nOnly return the bullet list. Do not write any other text or code.\nBug fix To edit the buggy code according to the fix instruction, w/o over- or under- fix. 0.3\n[Sys.] You fix bugs in Python code closely following the instructions.\n[User] Original code: {buggy code}; Code modification: {explanation}\nTranslate the statement into actual, minimal code change in this format:\n{original code snippet: \"\"copy the lines of code that need editing\"\"\n-> edited code snippet: \"\"write the edited code snippet\"\"}\n[LLM ] {old to new snippet in JSON, e.g., numbers list[i] <= key ) numbers list[i] > key }\n[User] Old Code:{buggy code}; Instruction:{Old snippet to new snippet}; New Code:\nfull prompts are in Table 3 in Supplements3. OpenAI’s gpt-3.5-turbo is used for\nall materials, except for explanation generation, which uses gpt-4 for enhanced\nreasoning capabilities. Below are key factors to the success of generation:\nTask Formation and Decomposition. We iterate on our prompts according\nto the nature of the task. First, as LLMs behave inconsistently when the user\ntasks conflict with LLMs inherent training objectives [28], we carefully formulate\nthe task to avoid introducing competing tasks. Take Local Bug Fix (Table 1) as\nan example: when we directly ask the LLM to fix a bug according to an explana-\ntion, we observe that the model almost always over-fix all bugs irrespective of the\nprovided instructions. This is because LLMs can be biased towards generating\nfully correct code (part of the LLM pre-training) and away from local bug fixing\n(changing only the buggy snippet described by the instruction, the desired task).\nHence, we re-frame it as a translation task, converting bug-fixing instructions to\nits code format old → new code snippet. This task re-framing mitigates\nthe model’s inherent bias, reducing over-fixing errors by 70%.\nSecond, for multi-step tasks (e.g., Local Bug Fix), we adoptLLM-chains [27],\ndecomposing tasks into sub-tasks handled by separate steps, such that each step\ncontributes to stable performance. Third, we also address prompt complexity by\nexplicitly prioritizing essential requirements. For tasks like generating Bug Ex-\nplanations and Fix Instructions (Table 1), we prioritize precise bug extraction,\ninstructing the model to list all unique bugs upfront. Secondary requirements\n(e.g., word limits) are specified only in the output format. This hierarchical\ndisentanglement significantly improves success rates by over 40%.\n3 Supplemental materials are at: http://tinyurl.com/hypocompass-sup\n8 Q. Ma et al.\n1 1 0 1 1 0\nRef. test cases\nCandidate bugs\n1 1 1 1 1 1\n1 0 1 0 1 1\n1 0 1 1 1 1\n1 1 1 1 0 1\n1 1 1 1 0 1\n1 1 1 0 0 1\n1\n2\n3\n4\n5\n6\n7\na b c d e f\n1\n3\n5\n3\n4\n7\na b cd e f\nSelect test category \nTest case clustering via AHC\nSelect buggy code \nBehaviorally diverse codes\nSelect explanation candidate \nBehavioral close codes ➜ expl.\nOver-generate buggy code \n& map to test case behaviorA\nB\nC\nD\nFig. 5: Over-generate and automatically select materials with pedagogical values.\nOver-Generate-then-Select. While LLMs can easily generate random mate-\nrials, it is nontrivial to ensure that their generations have pedagogical values. For\nexample, behaviorally distinct bugs help students practice with varied instances,\nbut it is hard to enforce through prompting as it requires LLMs to “know” bug\nbehaviors. Nonetheless, we can configure the non-deterministic LLMs to over-\ngenerate multiple solutions with mixed qualities [17], andthen select a subset\nof desired ones (Figure 5). We apply this strategy in multiple places:\n(1) To expose students to behaviorally distinct bugs, we over-generate buggy\ncode (Table 1). We filter out correct code, and we vectorize buggy code’s behavior\nbased on the reference test suite (Figure 5A, 0 being failed tests). We then\ngreedily choose a diverse subset of buggy programs with the maximum pairwise\ndistance, using Euclidean distance on the error vectors (Figure 5B).\n(2) To help students clarify misconceptions (Figure 5C), we want distract-\ning explanations that look similar to the actual explanation for each practice\nbuggy code. We choose from the over-generated buggy code pool, find two with\nthe smallest Euclidean distance to the target code, and use their corresponding\nexplanations as distractors. The mapping also helps generate the confusion mes-\nsages (Figure 3b3) — when a student selects the distractor explanation, we use\nits corresponding buggy code to find test cases to present to students.\n(3) To capture key testing aspects in our test category hints (Figure 5D), we\ncluster reference test cases into semantically meaningful groups. We build den-\ndrograms from test case vectors with Agglomerative Hierarchical Clustering [14],\nwhich guide the selection of test category hints from the over-generated pool.\nHuman-in-the-Loop Verification. As shown in Figure 4, while the hints\nfor test cases and categories are generated separately, the materials relevant to\nbugs are generated in sequential order. We perform human verification per step\nto mitigate the risk of cascading errors in subsequent steps. We provide more\ndetails on human verification and editing times in Section 5.\n5 LLM Evaluation: Generation Efficiency and Quality\nWe evaluated the generations on six different problems from prior work [4] and\nour own problems (detailed in Table 4 in Supplements). On average, for each\nHypoCompass: LLM-based Hypothesis Construction Tutor 9\nTable 2: LLM Evaluation: Time, Success rate, and Inter-Rater Reliability scores\n(i.e., IRR% = #agreements / #total labels, κ is Cohen’s Kappa coefficient). 4\nMaterial Raw LLM outputs Human verification\n# Generation Avg. gen time Success% Avg. edit time IRR% κ\nTest case description hint 61 0:00:37 98.36% 0:00:08 100% -\nTest case category hint 18 0:00:10 94.44% 0:00:10 100% -\nBuggy code 145 0:01:30 57.93% 0:00:02 n/a n/a\nBug explanation and fix 145 0:03:36 91.72% 0:00:52 90% 0.875\nBug fix 195 0:02:45 86.15% 0:00:37 92% 0.752\nproblem, we generated 3 test category hints, 10 test case hints, 24 buggy pro-\ngrams, explanation and fix instructions, and 33 bug fixes. The total number and\nthe success rates are summarized in Table 2. We provided the success criteria\nfor all types of materials in Table 5 in Supplements.\nMethod. Two authors annotated 10% of the generations at each step individ-\nually, and discussed to resolve the disagreement and update the codebook. An\nexternal instructor annotated the same 10% of LLM-generated materials, using\nthe updated codebook. We calculated the inter-rater reliability (IRR) between\nthe external instructor and the resolved annotation among the two authors us-\ning percent IRR and Cohen’s Kappa. As shown in Table 2, the agreements are\nsatisfactory across different model generations (IRR% > 90% and κ >0.75)4.\nOne author annotated the rest of the materials to calculate the success rates.\nWe log the verification and editing time, as proxies to the instructor overhead.\nTo compare LLM and human generations, we recruited two experienced CS\nTAs to each create practice materials for a specific problem. Each TA received\nthe same input as LLMs, was asked to produce one set of materials matching\nthe amount of content LLMs produced, and was compensated for their time.\nResult: Efficient and High-Quality Generation. We achieve high-quality\ngeneration: a complete set of practice materials with 9 buggy programs (3 for\npractice and 6 more as distractors), 9 bug explanations, 9 bug fixes, 10 test case\nhints, and 3 test category hints can be generated with a 90% success rate and\nonly takes 15 minutes to label and edit. As we over-generate and automatically\nselect buggy code, a success rate over 50% is reasonable for practical use.\nEmploying LLMs can also be significantly more efficient. In total, a TA spent\naround 60 minutes to generate one set of practice materials for HypoCompass.\nOne TA noted the difficulty in consistently creating unique and high-quality ma-\nterials after 30 minutes, saying that “the importance of the bug I create would\nstart to decline.” The same author evaluated the TAs’ generations using the\nannotation codebook, which had a 100% success rate and took 11 minutes. The\ntime invested in generating and editing instructional materials for HypoCom-\npass using LLMs was 4.67 times less than that of the human TAs.\n4 Buggy programs undergo automatic testing, so human verification is unnecessary\n(n/a). If both raters unanimously agree in one category, kappa is undefined (-), so\nκ is only noted when there’s less than 100% IRR agreement on a single label.\n10 Q. Ma et al.\nPython\nQuestion7Selectoneanswer.\nTestcase2: assert(remove_extras_code2([1,1,2,3])==[1,2,3])\nActualbehavior:'TypeError''int'objectisnotiterable.\n1 defremove_extras_code2(lst):\n2 new_lst= []\n3 fori inlst:\n4 ifi ==lst[i+1]:\n5 continue\n6 else:\n7 new_lst+=i\n8 returnnew_lst\nWhat'sthebugexposedbythistestcase?\nA. Thebugoccursbecausetheloopvariableiismistakenlyusedasboththeelementand\nindexofthelist.ThisleadstoincorrectcomparisonsandtriggersaTypeErrorin\nlst[i+1]becauseiisanelementofthelist,notanindex.\nB. Thebugiscausedbynotinitializingthenew_lstproperly.Thecodefailstoexplicitly\nassignanemptylisttonew_lst,sowhenconcatenatingelementstonew_lstusingthe\n+=operator,aTypeErroroccursbecausenew_lstisnotiterable.\nC. Thebugisduetoanincorrectconditionalstatement.Thecodeincorrectlycomparesi\nwithlst[i+1]insteadofcomparingadjacentelementsofthelist,whichtriggers\nTypeErrorwhentryingtocompareanintegeriwithalistelement.\nD. Thebugoccursbecausethecodeincorrectlyassumesthatiisiterablewhen\nconcatenatingittonew_lstwiththe+=operator.Inthiscase,iisaninteger,whichis\nnotiterable,anditcausesaTypeError.\nAnswer:\nFig. 6: Pre-post test question examples for LO1 comprehensive (Q3.1 and Q3.2)\nand LO2 accurate hypothesis construction (Q7).\n6 Learning Evaluation: Pre- / Post-Test Study\nCan novices better formulate hypotheses after engaging with HypoCompass?\nWe conducted a learning evaluation with 19 students and compared the differ-\nence in speed and performance from the pre-test to the post-test.\nAssessment. To best capture student learning gains on our learning objectives,\nwe took a backward design method [26] to create an aligned assessment for the\ncomprehensive LO1 and accurate LO2 hypothesis construction skills. We con-\nducted multiple rounds of pilots to refine our intervention and pre-post tests. Our\nfinal tests are based on two programming exercises with comparable difficulties.\nWe counterbalanced pre-post tests’ problems to control for problem sequence in-\nfluence. Each test consists of seven questions, with three assessing LO1 and four\nfor LO2. Figure 6 provides a sample for each. For instance, Question 3.1 asks\nstudents to identify the more suitable test case to add to an existing test suite,\nevaluating their ability to construct comprehensive hypotheses (LO1). We mea-\nsure students’ performance using their test scores based on a standard rubric.\nWe also log the pre-post tests’ completion time as a proxy for proficiency.\nMethod: Study Procedure and Participants. Our hour-long user study\nconstituted a pre-survey, pre-test, interaction with HypoCompass, post-test,\nand a post-survey. Participants began with a pre-survey, which asked demo-\ngraphic information and 7-level Likert Scale questions on their debugging expe-\nriences. Then, participants had up to 20 minutes for the pre-test. The system\ninteraction consisted of two problems, where participants needed to write a test\nsuite and explain bugs in three different buggy programs for each problem. The\nfirst problem was the same as in the pre-test, and the second problem matched\nthe screening survey’s exercise. By reusing problems that students have seen, we\nisolate our learning objectives from the program comprehension skills. After a\nsubsequent 20-minute post-test, participants filled out a post-survey with Lik-\nert Scale and open-ended questions on their experience and perceptions using\nHypoCompass. Participants received a $15 Gift Card for their time.\nHypoCompass: LLM-based Hypothesis Construction Tutor 11\nWe recruited a diverse group of undergraduate and graduate students from\nfour public or private US institutions. Interested participants completed a screen-\ning survey, which included a programming exercise that also served as the second\nexercise in our study. To ensure a suitable skill range, we excluded those with\nextensive programming experience or who quickly solved the exercise. After fil-\ntering, 19 participants (S1-19) were included in the study — 12 females, 6 males,\n1 non-binary, and 8 non-native English speakers, with an average age of 20.7.\nQuantitative Result: Learning Gains. A two-tailed paired t-test showed\nthat students’ pre-test to post-test scores significantly improved by 11.7% ( p =\n0.033 < 0.05), and the time of completion significantly reduced by 13.6% ( p =\n0.003), indicating success in learning through HypoCompass interaction. Note\nthat the bugs used in pre-post tests are generated by humans and are not the\nsame as in HypoCompass. As such, the significant learning gains indicate that\nstudents could learn debugging skills transferable to real-world bugs.\nWhere does the learning gain come from? We break down the analyses by\nlearning objectives. We found a small 6.1% improvement in the score and a\nlarge 23.6% time reduction for comprehensive hypothesis construction (LO1),\nand a large 15.8% improvement in the score and a small 9.0% time reduction\nfor accurate hypothesis construction (LO2). Therefore, students showed more\nefficiency enhancement in LO1, and more learning gains in LO2. Note that these\nimprovements may confound with problem difficulty, as the items corresponding\nto LO1 (pre-test µ = 54%) seem easier than the ones for LO2 (pre-testµ = 38%).\nQualitative Result: Student Perceptions.We further unpack howHypoCom-\npass contributed to learning by analyzing the survey responses. Students valued\nbeing able to offload some debugging subtasks to HypoCompass, such as writ-\ning code and explanations. For example, S1 said“looking at the test behavior and\nthe explanation options really helps relieve that burden.” Students also generally\nfelt that the LLM-generated bugs and fixes were authentic. Most participants\ncould not tell if their practiced programs were written by students or AI because\nof their experiences making or seeing similar mistakes from peers.\nMoreover, students reported that HypoCompass was engaging, fun, not\nfrustrating, and helped build confidence in debugging. A Wilcoxon signed-rank\ntest shows a significant increase in self-rated confidence in debugging by 15%\n(p = 0.007). Students rated HypoCompass as significantly more engaging (6.0\nout of 7), fun (6.0), and less frustrating (2.5) than their conventional way of\nlearning debugging and testing ( p < 0.005 for each). S8 especially liked the\nteachable agent setup: “the role play just feels more natural because it feels like\nexplaining to a rubber duck instead of to talking to myself” .\n7 Discussion\nTeachable Agent for Appropriate Reliance with Imperfect AIs. Our\nwork illustrates a scenario in whichLLM-generated bugs are not seen as problems\n12 Q. Ma et al.\nbut rather as features. HypoCompass’s teachable agent setup provides students\nwith moderated exposure to imperfect LLMs, and may help them learn that LLMs\nare fallible and calibrate trust accordingly. Future iterations could remove mate-\nrial validation and allow direct exposure to unfiltered LLM mistakes in real-time\ninteractions, taking full advantage of the teachable agent framework. Students\nwill naturally expect that the LLM-agent seeking help may make mistakes (e.g.,\nfail to follow bug-fixing explanations). This approach, however, requires a more\nsophisticated design for scaffolding students in recognizing LLM errors.\nTask Delegation for Shifting Learning Focus. Our exploration lays the\nfoundation for a paradigm shift toward cultivating higher-order evaluation skills\nin the generative AI era. Essentially, we asked: what skills should we offload,\nand what should we learn? Most students in our study appreciated offloading\nsubtasks to LLM (Section 6); however, some need more scaffolds, while others\nprefer less. Future research can investigate more personalized task delegation.\nFor example, students who need more help can use LLMs to facilitate code\ntracing, and students can also write their own explanations for bugs based on\ntheir proficiency. Deciding the bare minimum programming skills and human-AI\ncollaboration skills to teach also warrants further exploration [16].\nModularize to Adapt to Different Needs. Though most students and\ninstructors found HypoCompass engaging, some expressed concerns about the\ndeployment and maintenance cost of a new tool. To maximize utility to diverse\nusers, we can modularize different components in HypoCompass. Instructors\nwho prefer to distribute training materials as handouts can rely entirely on the\nmaterial generation module. In contrast, instructors who want to experiment\nwith TA training can employHypoCompass with practice generated using their\ntraining questions. Future studies may perform ablation studies to evaluate dif-\nferent HypoCompass components with more extensive classroom deployment.\nLimitation. We primarily evaluatedwhether HypoCompass can bring learn-\ning and efficiency gains through small in-lab experiments. With this prerequisite,\nwe plan to conduct future classroom deployment with controlled comparisons.\nThere is also a limitation regarding the reported efficiency of the LLM-assisted\ninstructional material development, as the instructors need some familiarization\ntime with the tool and the process.\n8 Conclusion\nIn an attempt to answer how LLMs can reshape programming education’s fo-\ncus, we introduce a novel system, HypoCompass, and new instructional designs\nfor hypothesis construction skills. We aim to provide engaging and deliberate\npractice on debugging to novices, using our theoretically motivated and empir-\nically tested teachable agent augmented by LLM. Our evaluations show that\nHypoCompass can efficiently help instructors create high-quality instructional\nmaterials, effectively train novices on comprehensive and accurate hypothesis\nconstruction, and facilitate students’ confidence and engagement in debugging.\nHypoCompass: LLM-based Hypothesis Construction Tutor 13\nAcknowledgments Thanks to the participants, reviewers, Vicky Zhou, Kelly\nRivers, Michael Taylor, Michael Hilton, Michael Xieyang Liu, Kexin Yang, Jiong-\nhao Lin, Erik Harpstead, and other Ken’s lab members for insights and help.\nThanks to the gift funds from Adobe, Oracle, and Google; Thanks to the Na-\ntional Science Foundation (award CNS-2213791) for partial support of this work.\nReferences\n1. Ardimento, P., Bernardi, M.L., Cimitile, M., Ruvo, G.D.: Reusing bugged source\ncode to support novice programmers in debugging tasks. ACM Trans. Comput.\nEduc. 20(1), 1–24 (2019). https://doi.org/10.1145/3355616\n2. Becker, B.A., Denny, P., Finnie-Ansley, J., Luxton-Reilly, A., Prather, J., Santos,\nE.A.: Programming is hard-or at least it used to be: Educational opportunities\nand challenges of ai code generation. In: Proceedings of the 54th ACM Technical\nSymposium on Computer Science Education V. 1. pp. 500–506 (2023)\n3. Blair, K., Schwartz, D.L., Biswas, G., Leelawong, K.: Pedagogical agents for learn-\ning by teaching: Teachable agents. Educ. Technol. Res. Dev. 47(1), 56–61 (2007)\n4. Dakhel, A.M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M.C., Jiang,\nZ.M.J.: Github copilot ai pair programmer: Asset or liability? Journal of Systems\nand Software 203, 111734 (2023). https://doi.org/10.48550/ARXIV.2206.15331\n5. Desai, C., Janzen, D.S., Clements, J.: Implications of integrating test-driven de-\nvelopment into CS1/CS2 curricula. SIGCSE Bull. 41(1), 148–152 (Mar 2009)\n6. Dohmke, T.: GitHub copilot x: The AI-powered developer experience. https://\ngithub.blog/2023-03-22-github-copilot-x-the-ai-powered-developer-experience/\n(Mar 2023), accessed: 2023-9-5\n7. Edwards, S.H., Shams, Z.: Comparing test quality measures for assessing student-\nwritten tests. In: Companion Proceedings of the 36th International Conference\non Software Engineering. pp. 354–363. ICSE Companion 2014, Association for\nComputing Machinery, New York, NY, USA (May 2014)\n8. Edwards, S.H., Shams, Z.: Do student programmers all tend to write the same soft-\nware tests? In: Proceedings of the 2014 conference on Innovation & technology in\ncomputer science education. pp. 171–176. ITiCSE ’14, Association for Computing\nMachinery, New York, NY, USA (Jun 2014)\n9. Ericsson, A., Pool, R.: Peak: Secrets from the new science of expertise. Random\nHouse (2016)\n10. Fitzgerald, S., McCauley, R., Hanks, B., Murphy, L., Simon, B., Zander, C.: De-\nbugging from the student perspective. IEEE Trans. Educ. 53(3), 390–396 (2010)\n11. Ganguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Chen, A., Conerly,\nT., Dassarma, N., Drain, D., Elhage, N., et al.: Predictability and surprise in\nlarge generative models. In: Proceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency. pp. 1747–1764 (2022)\n12. Kallia, M.: The search for meaning: Inferential strategic reading comprehension\nin programming. In: Proceedings of the 2023 ACM Conference on International\nComputing Education Research (ICER ’23). ACM (May 2023)\n13. Ko, A.J., Myers, B.A.: Debugging reinvented: asking and answering why and why\nnot questions about program behavior. In: Proceedings of the 30th international\nconference on Software engineering. pp. 301–310. ICSE ’08, Association for Com-\nputing Machinery, New York, NY, USA (May 2008)\n14 Q. Ma et al.\n14. Lukasov´ a, A.: Hierarchical agglomerative clustering procedure. Pattern Recogni-\ntion 11(5-6), 365–381 (1979)\n15. Luxton-Reilly, A., McMillan, E., Stevenson, E., Tempero, E., Denny, P.: Lade-\nbug: an online tool to help novice programmers improve their debugging skills.\nIn: Proceedings of the 23rd Annual ACM Conference on Innovation and Technol-\nogy in Computer Science Education. pp. 159–164. ITiCSE 2018, Association for\nComputing Machinery (Jul 2018)\n16. Ma, Q., Wu, T., Koedinger, K.: Is AI the better programming partner? Human-\nHuman pair programming vs. Human-AI pAIr programming. arXiv preprint\narXiv:2306.05153 (2023), http://arxiv.org/abs/2306.05153\n17. MacNeil, S., Tran, A., Mogil, D., Bernstein, S., Ross, E., Huang, Z.: Generating\ndiverse code explanations using the gpt-3 large language model. In: Proceedings\nof the 2022 ACM Conference on International Computing Education Research-\nVolume 2. pp. 37–39 (2022)\n18. Markel, J.M., Opferman, S.G., Landay, J.A., Piech, C.: GPTeach: Interactive TA\ntraining with GPT-based students. In: Proceedings of the Tenth ACM Conference\non Learning @ Scale. pp. 226–236. L@S ’23, Association for Computing Machinery,\nNew York, NY, USA (Jul 2023). https://doi.org/10.1145/3573051.3593393\n19. McCauley, R., Fitzgerald, S., Lewandowski, G., Murphy, L., Simon, B., Thomas, L.,\nZander, C.: Debugging: A review of the literature from an educational perspective.\nComputer Science Education 18(2), 67–92 (Jun 2008)\n20. Mozannar, H., Bansal, G., Fourney, A., Horvitz, E.: Reading between the lines:\nModeling user behavior and costs in ai-assisted programming. arXiv preprint\narXiv:2210.14306 (2022)\n21. News, Y.H.: Why don’t schools teach debugging? https://news.ycombinator.\ncom/item?id=7215870 (Feb 2014), accessed: 2023-9-8\n22. Savelka, J., Agarwal, A., An, M., Bogart, C., Sakr, M.: Thrilled by your progress!\nlarge language models (gpt-4) no longer struggle to pass assessments in higher\neducation programming courses. arXiv preprint arXiv:2306.10073 (2023)\n23. Shahriar, T., Matsuda, N.: What and how you explain matters: Inquisitive teach-\nable agent scaffolds knowledge-building for tutor learning. In: International Con-\nference on Artificial Intelligence in Education. pp. 126–138. Springer (2023)\n24. Wang, Z., Valdez, J., Basu Mallick, D., Baraniuk, R.G.: Towards Human-Like edu-\ncational question generation with large language models. In: Artificial Intelligence\nin Education. pp. 153–166. Springer International Publishing (2022)\n25. Whalley, J., Settle, A., Luxton-Reilly, A.: Analysis of a process for introductory\ndebugging. In: Proceedings of the 23rd Australasian Computing Education Con-\nference. pp. 11–20. ACE ’21, Association for Computing Machinery (Mar 2021)\n26. Wiggins, G.P., McTighe, J.: Understanding by Design. ASCD (2005)\n27. Wu, T., Terry, M., Cai, C.J.: AI chains: Transparent and controllable Human-AI\ninteraction by chaining large language model prompts. In: Proceedings of the 2022\nCHI Conference on Human Factors in Computing Systems. pp. 1–22. No. Article\n385 in CHI ’22, Association for Computing Machinery, New York, NY, USA (2022)\n28. Xie, J., Zhang, K., Chen, J., Lou, R., Su, Y.: Adaptive chameleon or stubborn sloth:\nUnraveling the behavior of large language models in knowledge clashes (2023)\n29. Xu, S., Rajlich, V.: Cognitive process during program debugging. In: Proceedings\nof the Third IEEE International Conference on Cognitive Informatics, 2004. pp.\n176–182. IEEE (Aug 2004)\n30. Zeller, A.: Why programs fail: A guide to systematic debugging. Morgan Kauf-\nmann, Oxford, England, 2 edn. (Jun 2009)"
}