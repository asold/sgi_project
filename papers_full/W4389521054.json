{
  "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
  "url": "https://openalex.org/W4389521054",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2997959453",
      "name": "Zhoujun Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2760024017",
      "name": "Jungo Kasai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097645590",
      "name": "Tao Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2044423413",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W2105717194",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4206688402",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W3135335819",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4385573392",
    "https://openalex.org/W4309087688",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4303441863"
  ],
  "abstract": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly (up to 5× with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code is released at the site https://github.com/xlang-ai/batch-prompting.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 792–810\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBatch Prompting: Efficient Inference with Large Language Model APIs\nZhoujun Cheng\nShanghai Jiao Tong University\nblankcheng@sjtu.edu.cn\nJungo Kasai\nUniversity of Washington\njkasai@cs.washington.edu\nTao Yu\nUniversity of Hong Kong\ntyu@cs.hku.hk\nAbstract\nPerforming inference on large volumes of sam-\nples with large language models (LLMs) can\nbe computationally and financially costly in in-\ndustry and real-world use. We propose batch\nprompting, a simple yet effective prompting\napproach that enables the LLM to run infer-\nence in batches, instead of one sample at a\ntime. Our method reduces both token and\ntime costs while retaining downstream per-\nformance. We theoretically demonstrate that\nunder a few-shot in-context learning setting,\nthe inference costs decrease almost inverse\nlinearly with the number of samples in each\nbatch. We extensively validate the effective-\nness of batch prompting on ten datasets across\ncommonsense QA, arithmetic reasoning, and\nNLI/NLU: batch prompting significantly (up\nto 5×with six samples in batch) reduces the\nLLM (Codex) inference token and time costs\nwhile achieving better or comparable perfor-\nmance. For state-of-the-art Chat-based LLMs,\ne.g., GPT-3.5 and GPT-4, we show the benefits\nof batch prompting also hold. Further analysis\nshows that the number of samples in each batch\nand the complexity of tasks affect its perfor-\nmance. Moreover, batch prompting can be ap-\nplied across different reasoning methods using\nLLMs. Our code can be found at the site https:\n//github.com/xlang-ai/batch-prompting.\n1 Introduction\nLarge language models (LLMs) have shown their\nstrong capabilities under zero/few-shot settings\nwith in-context learning (Brown et al., 2020; Chen\net al., 2021; Chowdhery et al., 2022; Ouyang et al.,\n2022). Much recent work has made progress in in-\ncontext learning by eliciting reasoning steps (Wei\net al., 2022; Wang et al., 2022; Khot et al., 2022;\nCheng et al., 2022; Yao et al., 2022), selecting rep-\nresentative in-context exemplars (Liu et al., 2022;\nSu et al., 2022; Agrawal et al., 2022), and designing\nprompt templates (Jiang et al., 2020; Bach et al.,\n2022; Arora et al., 2022).\nStandard Prompting\nBatch Prompting\n# K-shot in-context exemplars\nQ: {question}\nA: {answer}\nQ: {question}\nA: {answer}\n…\n# One sample to inference\nQ: Ali had $21. Leila gave him half of her\n   $100. How much does Ali have now?\n-----------------------------------------------\n# Response\nA: Leila gave 100/2=50 to Ali. Ali now has \n   $21+$50 = $71. The answer is 71.\n# K-shot in-context exemplars in K/b batches\nQ[1]: {question}\nQ[2]: {question}\nA[1]: {answer}\nA[2]: {answer}\n…\n# b samples in a batch to inference\nQ[1]: Ali had $21. Leila gave him half of her \n      $100. How much does Ali have now?\nQ[2]: A robe takes 2 bolts of blue fiber and \n      half that white fiber. How many bolts? \n-----------------------------------------------\n# Responses to a batch\nA[1]: Leila gave 100/2=50 to Ali. Ali now has \n      $21+$50 = $71. The answer is 71.\nA[2]: It takes 2/2=1 bolt of white fiber. The \n      total amount is 2+1=3. The answer is 3.\nb(=2) samples \nin one batch\nFigure 1: Illustration of batch prompting compared with\nstandard prompting. Batch prompting groups multiple\nsamples in one batch (b= 2in the figure) and lets the\nLLM generate multiple responses (highlighted in yel-\nlow) for the batch in inference.\nUsing LLMs can be costly in terms of token and\ntime usage, especially when large volumes of LLM\ncalls are needed, such as benchmarking a large\ndataset or addressing a high volume of customer\ninquiries for businesses. For example, the widely-\nadopted OpenAI API service 1 of LLMs requires\nabout $40 and 8 hours to perform inference on\n10K samples using gpt-3.5-turbo; and the expense\nsignificantly escalates when using gpt-4, exceeding\na substantial $600. 2 If the rate limits of maximum\n1https://openai.com/api/.\n2Assume each LLM call consumes2, 000 tokens (common\nfor few-shot or long instruction), including both the input\n792\nAPI requests per minute are also considered, the\ncosts will be even higher, preventing users from\nbuilding massive LLM applications.\nWe propose batch prompting, a simple yet effec-\ntive approach for prompting LLMs, which allows\nthe model to perform inference on multiple sam-\nples at once, instead of one sample at a time. This\nreduces token and time costs while still retaining\ndownstream performance, without any change in\nAPIs. As shown in Figure 1, standard prompting\ngenerates a response (answer) to one sample at a\ntime, which takes N inference runs of an LLM for\na test set of size N. For our batch prompting, on\nthe other hand, an LLM generates responses to b\nsamples in a single inference run and only takes\nN/bruns for the same N samples.\nWe first demonstrate theoretically that under the\nfew-shot in-context learning setting, most tokens\nconsumed during the API call are the few-shot ex-\nemplars, and only a small portion of token budgets\nare used for the particular inference sample(s) (Sec-\ntion 2). Therefore, increasing bin batch prompt-\ning reduces the token and time costs in an inverse\nlinear fashion. We extensively validate the effec-\ntiveness of batch prompting on diverse downstream\ndatasets across commonsense QA, arithmetics, and\nNLI/NLU using Codex, a strong variant of GPT-3\nfinetuned on code data (Section 3). We also test\nbatch prompting on the state-of-the-art GPT-3.5\nand GPT-4 models. Batch prompting significantly\ndecreases the tokens and run time of using LLMs\nwhile achieving comparable or even better perfor-\nmance on all ten datasets.\nIn further analysis (Section 4), we find the num-\nber of samples in batch and the complexity of tasks\naffect its performance. Moreover, we show that\nbatch prompting works well across different reason-\ning methods (e.g., end-to-end, Chain-of-Thought,\nand code generation), suggesting that batch prompt-\ning is an efficient drop-in substitute for conven-\ntional prompting.\n2 Approach\nWe first introduce batch prompting, an efficient\nalternative to standard prompting. We then com-\npare the token and time costs of batch and stan-\ndard prompting, demonstrating the efficiency of\nour method.\nprompt tokens and generated tokens, and each call takes 3\nseconds to finish (a plausible average time in real use).\n2.1 Problem Setup\nThe conventional paradigm (i.e., standard prompt-\ning in Figure 1) to prompt LLMs for in-context\nlearning is as follows: K in-context few-shot ex-\nemplars with both a context ( e.g., question) and\nan output (e.g., answer) are selected to build the\ninput prompt, one test sample with context only is\nappended at the end of the prompt, and the LLM is\nused to generate the response for the test sample.\nIn this paper, we focus on a realistic scenario\nwith N test samples in total, which is common\nwhen benchmarking on a dataset or handling a large\nvolume of customer requests. In this case, it takes\nN separate calls of the LLM inference under the\nstandard prompting paradigm.\n2.2 Batch Prompting\nBatch prompting enables the LLM to generate re-\nsponses for multiple samples in one batch in a sin-\ngle inference run, so that it reduces the LLM infer-\nence time from N to N/b, where bis the number\nof samples in one batch. Specifically, as shown\nin Figure 1, our prompt groups the K in-context\nexemplars into K/bbatches with bexemplars each\nas demonstrations. In every batch, demonstration\ncontexts are arranged in a specific order at the be-\nginning, with their corresponding outputs placed\nin the same order afterwards. Then, b test sam-\nple contexts are grouped together at the end of the\ninput prompt. In this way, the LLM learns from\nthe in-context demonstrations and generates cor-\nresponding responses for the entire batch of test\nsamples. We add a position identifier “ [index]”\nwithin each batch to 1) assist the LLM with iden-\ntifying the order correspondence of input contexts\nand generated responses and 2) ease the process of\nparsing the generated responses.\n2.3 Token Cost\nThe costs of one LLM call scale linearly with the\nnumber of tokens, including both the input prompt\ntokens (few-shot and instruction) and generated\ntokens (according to, for example, OpenAI’s pric-\ning). Most tokens are consumed by the prompt\ntokens in standard prompting because the num-\nber of prompt tokens is usually far more than the\nnumber of generated tokens so that the LLM can\nbetter learn from in-context exemplar. Thus, the\nlarger the portion of tokens spent on generated to-\nkens, the more economical the total cost is.\nWe define token efficiency η as the portion of\n793\n(a) Token(CommonsenseQA)\n (b) Token(GSM8K)\n (c) Token(RTE)\n(d) Time(CommonsenseQA)\n (e) Time(GSM8K)\n (f) Time(RTE)\nFigure 2: Token and time costs per sample on three datasets for illustrations (other datasets show similar trends).\nBatch prompting significantly lowers both token and time costs as the number of samples in each batch increases.\ntokens spent on generated tokens in one LLM call.\nFor standard prompting and batch prompting (the\ninstruction tokens are omitted if any for brevity):\nηstandard = 1\nK+ 1\nηbatch = b\nK+ b\n(1)\nWhen K ≫1 and b<K , ηbatch scales almost\ninverse linearly with b, and thus increasing b of\nbatch prompting can greatly reduce token costs.\n2.4 Time Cost\nIntuitively, batch prompting reduces the inference\ntime by decreasing the number of API calls from\nN to N/b. Considering the Transformer (Vaswani\net al., 2017) decoding time, the cost will increase\nwith bin batch prompting due to the generation of\nlonger responses compared to standard prompting.\nWe give a detailed derivation from Transformer\narchitecture perspective in Appendix A.\nHowever, as most end-users are accustomed to\nand only have access to LLM API services, this\npart of time cost is marginal (observed in main\nexperiments), relative to the overhead of API call\nand request rate limits per minute set by a company,\nsuch as OpenAI. Besides, cases may occur when\nnetwork connections are unstable or slow, and the\nusers seek to finish a task with as few LLM calls\nas possible.\nTherefore, in practice, reducing the number of\ncalls from N to N/b with batch prompting can\nessentially lower the time costs. Note that when\nthe API call overhead and rate limits are no longer\nthe major bottlenecks of time costs in the future,\nthen the increased decoding time to generate longer\nsequences discussed in Appendix A cannot be over-\nlooked, and the time reduction of batch prompting\nwill not be as pronounced.\nSince LLM infrastructure/services can change\nover time, the token cost comparison is more reli-\nable and durable to measure than time costs.\n3 Experiments\nWe extensively evaluate batch prompting across\nten diverse datasets. Our results suggest that batch\nprompting can achieve at most 5×token and time\nefficiency (with six samples in batches) improve-\nment with similar or even better downstream per-\nformance.\n3.1 Datasets\nWe evaluate batch prompting on ten datasets\nacross commonsense question answering, arith-\nmetic reasoning, and natural language under-\nstanding/inference: CommonsenseQA (Talmor\net al., 2019), StrategyQA (Geva et al., 2021),\nGSM8K (Cobbe et al., 2021), SV AMP (Patel et al.,\n794\nTask Dataset Standard Batch\nCommonsense CSQA 77.2 77.4(+0.2)\nStrategyQA 73.3 71.0(−2.3)\nArithmetic GSM8K 55.7 58.7(+3.0)\nSV AMP 83.7 81.3(−2.4)\nAQuA 46.1 42.1(−4.0)\nAddSub 86.6 84.8(−1.8)\nMultiArith 97.5 98.7(+1.2)\nNLI/NLU RTE 76.9 74.7(−2.2)\nMNLI 65.3 65.7(+0.4)\nSST-5 51.3 49.7(−1.6)\nTable 1: Accuracy of standard and batch prompting on\nten datasets. Batch prompting shows comparable or\neven better performance.\n2021), AQuA (Ling et al., 2017), AddSub (Hos-\nseini et al., 2014), MultiArith (Roy and Roth, 2015),\nRTE (Bentivogli et al., 2009), MNLI (Williams\net al., 2018), and SST-5 (Socher et al., 2013). For\nCommonsenseQA, AQuA, AddSub, MultiArith,\nand RTE, we evaluate the whole dev/test sets. For\nthe other five datasets, we evaluate the first300 test\nsamples considering the costs of LLM APIs.\n3.2 Experimental Setups\nWe evaluate OpenAI Codex (code-davinci-002)\nas the LLM in our main experiments across ten\ndatasets. Codex was provided for free when the\npaper was written, but the token consumption re-\nduction is the same as the other LLMs, ensuring\nthat the token costs in experiments are general.\nWe also test the batch prompting performance on\nother state-of-the-art LLMs, including GPT-3(text-\ndavinci-003), GPT-3.5 (gpt-3.5-turbo), and GPT-\n4 (gpt-4). For GPT-4, we test the first 100 samples\nfor each dataset, considering the budget. The de-\ncoding temperature is set as 0. For each dataset, we\nmanually select 12-shot samples from the train-\ning set as in-context exemplars, with Chain-of-\nThought (Wei et al., 2022, CoT) reasoning steps in\nthe answers (in Section 4.4, other reasoning meth-\nods beyond CoT are discussed). We choose 12\nexemplars because 12 is the least common multiple\nof 2,3,4,6, and thus it is easy to analyze the effects\nof grouping them into batches of 2,3,4,6 samples\nin our ablation studies. More experimental details\nand full results are listed in Appendix B.\n3.3 Main Results\nFigure 2 compares the token and time costs of\nstandard and batch prompting. As shown, batch\nprompting substantially (up to 5×with 6 samples\nin each batch) reduces both the token and time\nDataset GPT-3 GPT-3.5 GPT-4\nStandardBatch StandardBatch StandardBatch\nCSQA 78.3 75 .8 72 .9 75 .4 84 .0 86 .0\nGSM8K 58.0 55 .0 71 .0 76 .7 96 .0 93 .0\nSV AMP 86.7 85 .8 84 .7 81 .3 98 .0 95 .0\nAddSub 99.2 98 .3 89 .3 92 .0 99 .0 99 .0\nRTE 88.3 88 .3 77 .6 81 .6 92 .0 90 .0\nTable 2: Accuracy of different LLMs with standard\nprompting and batch prompting using CoT prompts.\nLanguage models are GPT-3 (text-davinci-003), GPT-\n3.5 (gpt-3.5-turbo), and GPT-4 (gpt-4). Batch prompt-\ning can be applied well on different LLMs with good\nperformance.\ncosts of standard prompting with Codex. Further,\nthe decrease of costs scales almost inverse linearly\nwith the number of samples in each batch, verify-\ning our analysis in Sections 2.3 and 2.4. Note the\ntime costs include the API call overhead and rate\nlimit blocks, which exist in the commonly-used\nOpenAI and other LLM services. For LLM ser-\nvices where these are not bottlenecks of time, the\ndecoding time increase from larger b should not\nbe overlooked as discussed in Section 2.4. As the\nLLM infrastructure can change anytime, the token\nefficiency improvement is easier to compare than\ntime; the token reduction in Figure 2 should hold\nfor any LLM over time.\nTable 1 shows that batch prompting (with the\nbest b, i.e., the number of samples in each batch)\nperforms comparably or even better than standard\nprompting over all ten datasets. We thus recom-\nmend that LLM users consider applying batch\nprompting to save money and time while main-\ntaining good performance in realistic applications.\n3.4 Results across More LLMs\nWe experiment batch prompting with some other\nstate-of-the-art LLMs, including GPT-3, GPT-\n3.5 (ChatGPT) and GPT-4.\nTable 2 shows performance from these LLMs.\nAll tested LLMs demonstrate capabilities similar to\nCodex: batch prompting retains downstream perfor-\nmance across datasets. Actually, batch prompting\nChat-based models tend to gain performance im-\nprovements. We deduce the reason is that GPT-3.5\nand GPT-4 accept a specific role of system mes-\nsage as instruction, which makes them better follow\nbatch prompting instructions to input and output\nin batches. As discussed in Section 2, the token\nefficiency of batch prompting should hold for dif-\nferent LLMs, though the decrease in time may vary\ndepending on the LLM inference implementation.\n795\nFigure 3: Accuracy over varying numbers of batch sam-\nples bon five datasets using batch prompting. The per-\nformance decreases with larger b.\nDataset Random Similar Diverse\nCSQA 77.4 77 .4 78 .2\nGSM8K 58.7 57 .7 55 .7\nSV AMP 81.3 81 .3 80 .7\nAddSub 84.8 83 .2 84 .1\nRTE 74.7 70 .4 70 .8\nTable 3: Accuracy from various batching methods on\nfive representative datasets. Similarity or diversity-\nbased methods do not achieve performance gains.\n4 Analysis\nIn this section, we assess factors influencing batch\nprompting performance and the tradeoff between\ncosts and performance. We also demonstrate that\nbatch prompting can be applied to various LLM\nprompting methods, such as end-to-end and code\ngeneration.\n4.1 Number of Batch Samples\nFigure 3 illustrates the impact of the number of\nsamples per batch, b, on batch prompting perfor-\nmance. Performance typically decreases as bin-\ncreases, with a significant drop at b= 6across four\nout of five datasets. However, the optimal perfor-\nmance isn’t always at b = 2. Selecting b = 3or\nb = 4often yields good performance while con-\nserving more tokens and time. The time/token cost\nreductions diminish as bgrows, suggesting b< 6\n(given 12 in-context examples in experiments) as a\ngood balance between costs and performance.\n4.2 Selection of Batch Samples\nHere we examine whether the selection of samples,\ni.e. how samples are grouped into batches, will\naffect the performance of batch prompting. We\nstudy two widely-adopted sample selection meth-\nods in in-context learning when grouping the test\nFigure 4: Accuracy on WikiTQ of various table input\nstrategies and b(the number of samples in each batch).\nThis studies how the input length affects batch prompt-\ning performance. b = 1 means standard prompting.\nAverage input tokens per table are 24, 58, and 216 to-\nkens. As the number of batch samples increases, batch\nprompting suffers in downstream performance.\nsamples: grouping more similar (Rubin et al., 2021;\nLiu et al., 2022) and more diverse (Su et al., 2022;\nAgrawal et al., 2022) samples into batches. Specifi-\ncally, given N test samples, to group similar ones,\nwe use k-means clustering and post-process each\ncluster into equal size bby moving redundant sam-\nples to their closest groups with size <b. To group\ndiverse ones, we apply the vote-k method (Su et al.,\n2022) to iteratively select diverse and representa-\ntive groups of samples.\nAs listed in Table 3, both similarity and diversity-\nbased selections do not show improvements over\nrandom grouping. We suspect that the reason may\nbe that both methods assume in-batch samples can\nbenefit from previous similar or diverse samples,\ni.e., samples in the front of the batch. However,\nthese earlier samples without ground truth outputs\nmay bring error propagation to the rest of the in-\nbatch samples. Developing effective strategies for\nselecting samples for batch prompting could be a\npromising area for future research to further en-\nhance the performance of batch prompting.\n4.3 Complexity of Tasks\nIn Table 1, the steepest drop (from 46.1 to 42.1)\noccurs on AQuA dataset: an arithmetic reasoning\ntask in a multi-choice QA format. One possible\ninterpretation is that AQuA is more difficult than\nother datasets with the lowest absolute accuracy\n46.1%, and thus LLMs are more likely to be dis-\nturbed when input contexts are grouped together.\nWe further study another task aspect that may\n796\nDataset End-to-end Program\nStandard Batch Standard Batch\nCSQA 81.5 80 .4 - -\nGSM8K 21.3 17 .3 72 .7 73 .0\nSV AMP 70.7 68 .3 86 .0 86 .3\nRTE 85.2 83 .4 - -\nWikiTQ - - 54.3 50 .7\nTable 4: Accuracy of different reasoning methods with\nstandard and batch prompting. Batch prompting can be\napplied well showing similar or better performance.\naffect performance: batch prompting tends to de-\ngrade performance more significantly with longer\ninput contexts. We validate our assumption with\nWikiTQ (Pasupat and Liang, 2015), a challeng-\ning Table QA dataset. Tables contain longer input\ntokens for their multiple rows and columns. We\nexperiment with increasing table input lengths: a\nsimplified table schema (i.e., column names with-\nout column types; avg. 24 tokens/table), a table\nschema (avg. 58 tokens/table), and a table schema\nwith three table rows (avg. 216 tokens/table).\nAs shown in Figure 4, in standard prompting\n(b= 1), inputting table schemas with three rows\ndominates QA performance. However, it also sees\nthe steepest performance drop when bincreases us-\ning batch prompting. The shorter the input contexts,\nthe steadier the performance with batch prompting.\nThis suggests that long task inputs are more likely\nto lead to confusion and performance drops when\nbatch prompting is applied.\n4.4 Reasoning Methods\nIn our main experiments (Section 3), we used the\nChain-of-Thought (CoT) for all ten datasets. Here\nwe examine whether batch prompting is suitable\nfor other common LLM reasoning methods. We\nexperiment with two more reasoning methods: end-\nto-end (i.e., directly prompt the LLM to output the\nanswers without intermediate steps) and program-\nbased, (i.e., prompt the LLM to generate programs\nto answer the question). For the program-based\nmethods, we adopt Binder (Cheng et al., 2022)\non WikiTQ and Program-of-Thought (Chen et al.,\n2022, PoT) on GSM8K and SV AMP.\nAs seen in Table 4, both end-to-end and program-\nbased methods can benefit from the efficiency of\nbatch prompting while maintaining similar or even\nbetter performance on the task. This indicates batch\nprompting is a drop-in replacement that can be\ncombined with various reasoning methods under\ndiverse scenarios.\n5 Related Work\nImprove In-Context Learning. The impressive\ncapabilities of large language models (Brown et al.,\n2020; Chen et al., 2021; Chowdhery et al., 2022,\nLLM) have sparked a surge of recent research aim-\ning to enhance in-context learning (ICL) perfor-\nmance. Several works propose different reasoning\nmethods to prompt LLMs (Wei et al., 2022; Zhou\net al., 2022; Khot et al., 2022), showing great im-\nprovements over directly prompting LLMs to out-\nput answers. Other works (Chen et al., 2022; Gao\net al., 2022; Cheng et al., 2022) generate programs\nto solve reasoning tasks. Another line of work (Liu\net al., 2022; Su et al., 2022; Agrawal et al., 2022)\nfocuses on selecting better in-context exemplars.\nThis work adds a new dimension to ICL for large-\nscale real-world applications: batch prompting to\nsave budget and time while achieving good or even\nbetter performance.\nEfficient Language Generation. Much recent\nwork proposed methods for efficient language\ngeneration, including machine translation (Ka-\nsai et al., 2020, 2021a,b) and language model-\ning (Katharopoulos et al., 2020; Peng et al., 2021,\n2022), and model cascading (Varshney and Baral,\n2022). Many of them introduce alternative archi-\ntectures to the standard transformer to achieve such\nefficiency gains, which makes them hard to apply\nor deploy to real-world scenarios. Our method is\na simple yet effective alternative to recent prompt-\ning methods, and thus it is applicable to any off-\nthe-shelf language model APIs, such as OpenAI,\nGoogle, Anthropic, or any other available private\nLLM APIs, without any additional training or cus-\ntomized model hosting.\n6 Limitation\nBatch prompting has proven to be an efficient\nmethod for time and token reduction. Nonetheless,\nthere are several critical considerations to keep in\nmind when implementing it across various scenar-\nios. First, to optimize its benefits, the length of\nthe input prompt tokens should be (significantly)\ngreater than that of the output tokens. Thus, it\nmight not be suitable for “heavy output\" tasks like\nstory generation. It is important to note that while\nour experiments are conducted with few-shot in-\ncontext learning, this method is also applicable to\nthe instruction-following paradigm, either on its\nown or in combination, by simply substituting or\n797\nadding the few-shot inputs with instructions. The\nonly crucial factor is the length of the shared input\ntokens of inference samples. Secondly, it is possi-\nble to observe performance declines. Our exper-\niments indicate that task complexity and lengthy\ninput contexts can negatively impact performance.\nAlthough we have not identified a definitive guide-\nline for predicting performance, we advise users\nto initiate testing with a smaller subset to gauge\nthe effectiveness of batch prompting before imple-\nmenting it on a larger scale.\n7 Conclusion\nWe present batch prompting, a new way to prompt\nLLMs that performs inference on samples in a\nbatched fashion. With batch prompting, multi-\nple samples can be handled in one API call so\nthat the costs of tokens and time can be signif-\nicantly reduced. Extensive experiments on ten\ndatasets across commonsense QA, arithmetics, and\nNLI/NLU show that batch prompting can achieve\nbetter or similar performance compared to standard\nprompting, with much lower token and time costs.\nWe hope batch prompting offers pragmatic value\nto efficient real-world LLM usage.\n798\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\ncontext examples selection for machine translation.\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\nrel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\neric Sala, and Christopher Ré. 2022. Ask me any-\nthing: A simple strategy for prompting language mod-\nels.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert\nWebson, Colin Raffel, Nihal V Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFévry, et al. 2022. PromptSource: An integrated\ndevelopment environment and repository for natural\nlanguage prompts. In Proc. of ACL.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In Proc. of TAC.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proc. of\nNeurIPS.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, R.K. Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir R. Radev, Marilyn Ostendorf, Luke Zettle-\nmoyer, Noah A. Smith, and Tao Yu. 2022. Binding\nlanguage models in symbolic languages.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. PaLM: Scaling language\nmodeling with pathways.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. PAL: Program-aided language\nmodels.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. TACL.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn Proc. of EMNLP.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? TACL.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In Proc.\nof ICML.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah A. Smith. 2021a. Deep encoder, shallow\ndecoder: Reevaluating non-autoregressive machine\ntranslation. In Proc. of ICLR.\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama,\nGabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu\nChen, and Noah A. Smith. 2021b. Finetuning pre-\ntrained transformers into RNNs. In Proc. of EMNLP.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proc. of ICML.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n2022. Decomposed prompting: A modular approach\nfor solving complex tasks.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proc. of ACL.\n799\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for gpt-3?\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProc. of ACL.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proc. of NAACL.\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani\nYogatama, Zhaofeng Wu, Lingpeng Kong, Roy\nSchwartz, and Noah A. Smith. 2022. ABC: Attention\nwith bounded-memory control. In Proc. of ACL.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In Proc. of ICLR.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proc. of EMNLP.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proc. of EMNLP.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, and Tao Yu. 2022.\nSelective annotation makes language models better\nfew-shot learners.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proc. of NAACL.\nNeeraj Varshney and Chitta Baral. 2022. Model cas-\ncading: Towards jointly improving efficiency and\naccuracy of nlp systems. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 11007–11021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In Proc. of NeurIPS.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proc. of\nNAACL.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-\nto-most prompting enables complex reasoning in\nlarge language models.\n800\nA Time Cost Analysis Regarding\nTransformer Architecture\nIn batch prompting, assume there are Kin-context\nexemplars (Ctokens per sample on average),bsam-\nples in a batch to be inference. Standard prompting\nis a special case where b= 1. Since most current\nLLMs (e.g.,GPT-3, Codex, PaLM) are based on the\nTransformer decoder-only architecture, we focus\non the time cost of the auto-regressive decoder.\nThe plain transformer time complexity for decod-\ning one token is O(n2d), i.e., the time for encoding\nthe embeddings of input tokens, where n is the\nlength of input tokens and dis the dimension of\nembeddings. With the caching of previous tokens,\nthe time complexity to decode each of the rest to-\nkens is O(nd). We omit d since it is a constant.\nThus, the time of one inference to decode C ·b\ntokens:\nTencode = (CK)2\nTdecode = (CK + 1) +... (CK + Cb)\nT = Tencode + Tdecode\n(2)\nwhere Tencode is the time for encoding the input\ntokens in the decoder, and Tdecode is the time for\ndecoding the rest tokens. C can be seen as a con-\nstant. One inference time T regarding K and b\nis:\nT = C2K2 + Cb·CK + Cb(Cb+ 1)\n2\n= C2(K2 + bK+ b2\n2 ) +Cb\n2\n(3)\nThus, increasing bin batch prompting will also in-\ncrease the time cost of one inference. The influence\nof balso increases with its value and is relatively\nmarginal when bis small, especially when b≪K,\nwhich is a common practice ( b= 1) in few-shot\nin-context learning.\nWe can see a few examples by settingK=12 (as\nin experiments), C=100 with varying bin Table 5\naccording to equation 3.\nb Time per inference\n1 1565050\n2 1700100\n3 1845150\n4 2000200\n6 2340300\n12 3600600\nTable 5: Time(no unit) per inference with K = 12,\nC=100 and various b.\nThough the numbers are not accurate consider-\ning the constant coefficients of Big O time com-\nplexity, we can learn the decoding time increase\ncan not be overlooked as b becomes large. We\ndo not emphasize this part in Section 2.4 because\nthe overhead and rate limit blocking time of the\nOpenAI API make up the most proportion of time\ncost, and thus reducing the N times of API calls to\nN/btimes almost inverse linearly reduce the time\ncost (see Figure 2).\nHowever, if the overhead and rate limits are no\nlonger the bottlenecks, e.g., rate limits are strict\nfor Codex (code-davinci-002), GPT-3.5 (gpt-3.5-\nturbo) and GPT-4 (gpt-4) but not a big issue to\nGPT-3 (text-davinci-003), then the decoding time\nincrease will be non-negligible.\nB More Experimental Results\nWe list results for all experiments (Tables 6-9).\nFor the WikiTQ experiment with Binder, the LLM\ngeneration temperature is 0.4 following its paper.\nFor the other experiments, the temperature is 0.\nFor all experiments, top_p = 1, sampling_n = 1,\nlogprobs =1, and stop_tokens =\\n\\n. Five Ope-\nnAI keys are used as a polling pool on rotation to\nrequest the OpenAI API of Codex (the rate limit er-\nrors still occur in the experiments and are counted\ninto time cost since it is a practical issue). If fewer\nOpenAI keys are used, there should be more rate\nlimit errors because the request interval for one key\nwill be shorter.\nC Prompts\nIn the section, we list the prompt templates we\nuse for each dataset (Tables 10-16). We follow\nCoT (Wei et al., 2022) to build the prompts of\nCommonsenseQA, StrategyQA, GSM8K, SV AMP,\nAQuA, AddSub, MutliArith. We follow\nBinder (Cheng et al., 2022) and Program-of-\nThought (Chen et al., 2022) to build the prompts\nof WikiTQ, GSM8K (program), and SV AMP (pro-\ngram). For RTE, MNLI, SST-5, we design the\nprompts ourselves using Chain-of-Thought. For\nprompts with fewer than 12 in-context exemplars,\nwe manually add to 12 samples using samples from\nthe training set. We show batch prompting prompts\nwith b= 4as examples. For different b, we group\nthe same 12 samples according to b. When using\nChatGPT in Section 3.4, the prompt format differs\nfrom Codex and GPT-3 because its conversational\ncapability. See Table 17.\n801\nTask Dataset Standard Prompting Batch Prompting\nb=2 3 4 6\nCommonsense CSQA 77.2 76 .0 77 .4 77 .4 77 .2\nStrategyQA 73.3 69 .0 67 .7 71 .0 67 .7\nArithmetic GSM8K 55.7 55 .7 58 .7 55 .0 49 .7\nSV AMP 83.7 81 .3 80 .7 75 .7 76 .0\nAQuA 46.1 41 .3 42 .1 33 .1 37 .4\nAddSub 86.6 84 .8 80 .8 80 .3 68 .1\nMultiArith 97.5 98 .0 98 .7 96 .5 96 .3\nNLI/NLU RTE 76.9 70 .8 71 .8 74 .7 67 .1\nMNLI 65.3 65 .7 64 .7 65 .3 64 .7\nSST-5 51.3 48 .0 45 .0 49 .7 48 .7\nTable 6: Batch prompting accuracy with different b(the number of samples in batch) compared with standard\nprompting on ten datasets. All use Codex (code-davinci-002) as the LLM and Chain-of-Thought as the reasoning\nmethod.\nTask Dataset Standard Promting Batch Prompting\nb=2 3 4 6\nCommonsense CSQA 7.37 3 .77 2 .57 1 .96 1 .40\nStrategyQA 7.62 3 .63 2 .85 2 .42 1 .99\nArithmetic GSM8K 8.78 4 .55 3 .91 3 .75 3 .61\nSV AMP 7.25 3 .69 2 .46 2 .50 1 .92\nAQuA 7.02 3 .62 2 .60 2 .45 1 .77\nAddSub 7.79 4 .32 2 .41 1 .58 1 .45\nMultiArith 6.80 3 .56 2 .51 1 .89 1 .38\nNLI/NLU RTE 6.50 4 .56 2 .73 2 .40 1 .29\nMNLI 7.11 3 .78 2 .54 2 .22 1 .32\nSST-5 7.42 3 .23 2 .69 2 .22 1 .18\nTable 7: Batch prompting time per sample with different b(the number of samples in batch) compared with standard\nprompting on ten datasets. All use Codex (code-davinci-002) as the LLM and Chain-of-Thought as the reasoning\nmethod.\nTable Input Standard Prompting Batch Prompting\nb=2 3 4 6\nSchema(Simple) 45.7 41 .7 42 .0 40 .0 41 .3\nSchema 54.3 50 .7 48 .7 48 .7 47 .3\nSchema(3 table rows) 60.3 51 .3 46 .3 50 .3 38 .0\nTable 8: Accuracy on WikiTQ of various table input strategies and b (number of samples in batch) using\nBinder (Cheng et al., 2022) to generate programs with Codex (code-davinci-002).\nDataset Standard Prompting Batch Prompting\nb=2 3 4 6\nGSM8K 72.7 66 .3 70 .7 73 .0 51 .5\nSV AMP 86.0 86 .3 83 .0 80 .7 84 .3\nTable 9: Accuracy on GSM8K and SV AMP with varying b (number of samples in batch) using Program-of-\nThought (Chen et al., 2022) to generate programs with Codex (code-davinci-002).\n802\nCommonsenseQA Prompt\nQ[1]: What do people use to absorb extra ink from a fountain pen?\nAnswer Choices[1]: (a) shirt pocket (b) calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\nQ[2]: What home entertainment equipment requires cable?\nAnswer Choices[2]: (a) radio shack (b) substation (c) television (d) cabinet\nQ[3]: The fox walked from the city into the forest, what was it looking for?\nAnswer Choices[3]: (a) pretty flowers (b) hen house (c) natural habitat (d) storybook\nQ[4]: Sammy wanted to go to where the people were. Where might he go?\nAnswer Choices[4]: (a) populated areas (b) race track (c) desert (d) apartment (e) roadblock\nA[1]: The answer must be an item that can absorb ink. Of the above choices, only blotters are used to\nabsorb ink. So the answer is (e).\nA[2]: The answer must require cable. Of the above choices, only television requires cable. So the answer\nis (c).\nA[3]: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest.\nSo the answer is (b).\nA[4]: The answer must be a place with a lot of people. Of the above choices, only populated areas have a\nlot of people. So the answer is (a).\nQ[1]: Where do you put your grapes just before checking out?\nAnswer Choices[1]: (a) mouth (b) grocery cart (c)supermarket (d) fruit basket (e) fruit market\nQ[2]: Google Maps and other highway and street GPS services have replaced what?\nAnswer Choices[2]: (a) united states (b) mexico (c) countryside (d) atlas\nQ[3]: Before getting a divorce, what did the wife feel who was doing all the work?\nAnswer Choices[3]: (a) harder (b) anguish (c) bitterness (d) tears (e) sadness\nQ[4]: James went to the tennis court that was located in his home what?\nAnswer Choices[4]: (a) country club (b) park (c) michigan (d) sports (e) town\nA[1]: The answer should be the place where grocery items are placed before checking out. Of the above\nchoices, grocery cart makes the most sense for holding grocery items. So the answer is (b).\nA[2]: The answer must be something that used to do what Google Maps and GPS services do, which is to\ngive directions. Of the above choices, only atlases are used to give directions. So the answer is (d).\nA[3]: The answer should be the feeling of someone getting divorced who was doing all the work. Of the\nabove choices, the closest feeling is bitterness. So the answer is (c).\nA[4]: The answer must be a place where tennis courts are located. Of the above choices, only home town\nhas tennis courts. So the answer is (e).\nQ[1]: What does you body do when you exercise?\nAnswer Choices[1]: (a) need for food (b) thirst (c) work out (d) sweating (e) injury\nQ[2]: In order to see a story on the big screen what must you do?\nAnswer Choices[2]: (a) go to movies (b) visualize (c) reading (d) open book (e) sketching a picture\nQ[3]: He followed the train tracks hoping to get home, he had gotten lost in the Yooperland where?\nAnswer Choices[3]: (a) ghetto (b) michigan (c) new york (d) canada (e) train station\nQ[4]: What would you get if you want a painting but cannot afford the original?\nAnswer Choices[4]: (a) reproduction (b) derivative (c) reproduction (d) simile (e) remake\nA[1]: The answer must be something that happens when you exercise. Of the above choices, only sweating\nhappens when you exercise. So the answer is (d).\nA[2]: The answer must be something that you do to see a story on the big screen. Of the above choices,\nonly going to movies makes sense. So the answer is (a).\nA[3]: The answer should be a place that relates to Yooperland. Of the above choices, only michigan is\nrelated to Yooperland. So the answer is (b).\nA[4]: The answer must be something that is similar to the original. Of the above choices, only\nreproduction is similar to the original. So the answer is (a).\nTable 10: CommonsenseQA Prompt.\n803\nStrategyQA Prompt\nQ[1]: Do hamsters provide food for any animals?\nQ[2]: Could Brooke Shields succeed at University of Pennsylvania?\nQ[3]: Hydrogen’s atomic number squared exceeds number of Spice Girls?\nQ[4]: Is it common to see frost during some college commencements?\nA[1]: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some\nanimals. So the answer is yes.\nA[2]: Brooke Shields went to Princeton University. Princeton University is about as academically\nrigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of\nPennsylvania. So the answer is yes.\nA[3]: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s\natomic number squared is less than 5. So the answer is no.\nA[4]: College commencement ceremonies can happen in December, May, and June. December is in the\nwinter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\nQ[1]: Could a llama birth twice during War in Vietnam (1945-46)?\nQ[2]: Would a pear sink in water?\nQ[3]: Can an Arvanite Greek understand some of the Albanian Declaration of Independence?\nQ[4]: Can Burundi’s communicate with citizens of New Brunswick?\nA[1]: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than\n6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\nA[2]: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water\nfloat. Thus, a pear would float. So the answer is no.\nA[3]: The Arvanite Greek’s are a major Tosk speaking group of southern Albania. Thus, they can understand\nsome of the Albanian Declaration of Independence. So the answer is yes.\nA[4]: French is one of the official languages of Burundi. Thus, Burundi’s can communicate with citizens of\nNew Brunswick. So the answer is yes.\nQ[1]: Are quadrupeds represented on Chinese calendar?\nQ[2]: Can actress Dafne Keen win the Eurovision Song Contest finals in 2020?\nQ[3]: Would a student in eleventh grade be unable to run for president of the United States?\nQ[4]: Does the judo rank system reach the triple digits?\nA[1]: The Chinese calendar has a number of symbols including monkeys, goats, and tigers. Tigers have four\npaws and balance themselves by walking on their toes. Thus, quadrupeds are represented on the Chinese\ncalendar. So the answer is yes.\nA[2]: Contestants must be at least 16 years of age to compete in the finals of Eurovision Song Contest.\nDafne Keen is 15 years old in 2020. Thus, Dafne Keen cannot win the Eurovision Song Contest finals in 2020.\nSo the answer is no.\nA[3]: Students in the eleventh grade are typically 16–17 years of age. To serve as president, one must be at\nleast 35 years old. Thus, a student in eleventh grade would be unable to run for president of the United States.\nSo the answer is yes.\nA[4]: A triple digit number would be equal to at least 100. The judo dan-rank system was capped at 10th\ndan after the death of judo’s founder, Kan¯o Jigor¯o. Thus, the judo rank system does not reach the triple\ndigits. So the answer is no.\nTable 11: StrategyQA Prompt.\n804\nGSM8K, SV AMP, AddSub, MultiArith Prompt\nQ[1]: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nQ[2]: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nQ[3]: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left\nin total?\nQ[4]: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\ndid Jason give to Denny?\nA[1]: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have\nbeen 21 - 15 = 6. The answer is 6.\nA[2]: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nA[3]: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35,\nthey had 74 - 35 = 39. The answer is 39.\nA[4]: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12\n= 8. The answer is 8.\nQ[1]: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nQ[2]: There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nQ[3]: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nQ[4]: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA[1]: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\nThe answer is 9.\nA[2]: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20\ncomputers were added. 9 + 20 is 29. The answer is 29.\nA[3]: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had\n35 - 2 = 33 golf balls. The answer is 33.\nA[4]: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left.\n23 - 15 is 8. The answer is 8.\nQ[1]: A garden produced 237 potatoes, 60 fewer cucumbers and twice as many peppers than the cucumbers. How\nmany vegetables did the garden produce?\nQ[2]: John’s cow weighs 400 pounds. It increased its weight to 1.5 times its starting weight. He is able to sell the cow\nfor $3 per pound. How much more is it worth after gaining the weight?\nQ[3]: John writes 20 pages a day. How long will it take him to write 3 books that are 400 pages each?\nQ[4]: James has a rainwater collection barrel. For each inch of rain he collects 15 gallons. On Monday it rained 4 inches\nand on Tuesday it rained 3 inches. He can sell water for $1.2 per gallon. How much money did he make from selling\nall the water?\nA[1]: The garden produced 237 - 60 = 177 cucumbers. The garden produced 177 * 2 = 354 peppers. The garden\nproduced 237 + 177 + 354 = 768 vegetables. The answer is 768.\nA[2]: The cow initially weighs 400 * 1.5 = 600 pounds. So it gained 600 - 400 = 200 pounds. It is worth 200 * 3 = 600\ndollars more. The answer is 600.\nA[3]: He wants to write 3 * 400 = 1200 pages. So it will take him 1200 / 20= 60 days The answer is 60.\nA[4]: It rained 3 + 4 = 7 inches So he collected 7 * 15 = 105 gallons So he makes 105 * 1.2 = 126 from selling the\nwater. The answer is 126.\nTable 12: GSM8K, SV AMP, AddSub, MultiArith Prompt.\n805\nAQuA Prompt\nQ[1]: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the\nnumbers is? Answer Choices[1]: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\nQ[2]: If a / b = 3/4 and 8a + 5b = 22,then find the value of a.\nAnswer Choices[2]: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\nQ[3]: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?\nAnswer Choices[3]: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\nQ[4]: How many keystrokes are needed to type the numbers from 1 to 500?\nAnswer Choices[4]: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\nA[1]: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean\nwould be 50. The answer is (a).\nA[2]: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means\n44a / 3 = 22. So a is equal to 3/2. The answer is (b).\nA[3]: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).\nA[4]: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are\n401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).\nQ[1]: A number X equals 80% of the average of 5, 7, 14 and a number Y . If the average of X and Y is 26, the\nvalue of Y is?\nAnswer Choices[1]: (a) 13 (b) 26 (c) 39 (d)36 (e) None of these\nQ[2]: A shopkeeper gave an additional 20 per cent concession on the reduced price after giving 30 per\ncent standard concession on an article. If Arun bought that article for 1,120, what was the original price?\nAnswer Choices[2]: (a) 3,000 (b) 4,000 (c) 2,400 (d) 2,000 (e) None of these\nQ[3]: A and B invests Rs.3000 and Rs.7000 respectively in a business. If A doubles his capital after 6 months.\nIn what ratio should A and B divide that year’s profit?\nAnswer Choices[3]: (a) 9:6 (b) 9:8 (c) 9:14 (d) 9:9 (e) 9:5\nQ[4]: The angle between two hands at 3.45 is?\nAnswer Choices[4]: (a) 110 degree (b) 115 degree (c) 112 1/2 degree (d) 117 degree (e) 157 1/2 degree\nA[1]: Average of 5, 7, 14 and Y = (5 + 7 + 14 + Y) / 4. Therefore, X = 80% of (5 + 7 + 14 + y) / 4 = (80/100)\nx (26 + Y)/4 => X = (26 + Y)/5, i.e., 5X - Y = 26. Also, (X + Y) / 2 = 26. Thus, (26 + Y) / 5 + Y = 52, then Y\n= 39. The answer is (c).\nA[2]: The total discount should be (1 - 0.3) * (1 - 0.2) = 0.56. Thus, the original price should be 1120 / 0.56\n= 2000. The answer is (d).\nA[3]: The ratio should be (3 * 6 + 6 * 6): (7 * 12) = 54:84. It simplifies to 9:14. The answer is (c).\nA[4]: The hour hand is (45/60) * (360/12) = 22.5 degree from 3 o’clock. So the angle between the hour hand and\nthe minute hand is (9-3) * (360/12) - 22.5 = 157.5. The answer is (e).\nQ[1]: Find the sum of first 30 natural numbers.\nAnswer Choices[1]: (a) 470 (b) 468 (c) 465 (d) 463 (e) 487\nQ[2]: What will come in place of the x in the following Number series? 46080, 3840, ?, 48, 8, 2, 1.\nAnswer Choices[2]: (a) 1 (b) 384 (c) 5 (d) 7 (e) 9\nQ[3]: A password of a computer used two digits where they are from 0 and 9. What is the probability that the\npassword solely consists of prime numbers and zero?\nAnswer Choices[3]: (a) 1/32 (b) 1/16 (c) 1/8 (d) 2/5 (e) 1/4\nQ[4]: Ifk3 is divisible by 120, what is the least possible value of integer k?\nAnswer Choices[4]: (a) 12 (b) 30 (c) 60 (d) 90 (e) 120\nA[1]: The sum of first 30 natural numbers is 30 * (30 + 1) / 2 = 465. The answer is (c).\nA[2]: The ratio of the numbers is 10:8:6:4:2:1. So the next number should be 384. The answer is (b).\nA[3]: 0, 2, 3, 5, 7 are five prime digits(including zero). So there are 5 * 5 = 25 two-digit numbers with\nonly prime numbers and zero. The probability is 25/100 = 1/4. The answer is (e).\nA[4]: 120 can be factored as 2 * 2 * 2 * 3 * 5. So the least k be 2 * 3 * 5 = 30. The answer is (b).\nTable 13: AQuA Prompt.\n806\nRTE Prompt\nPremise[1]: No Weapons of Mass Destruction Found in Iraq Yet.\nHypothesis[1]: Weapons of Mass Destruction Found in Iraq.\nPremise[2]: A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman\nCatholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.\nHypothesis[2]: Pope Benedict XVI is the new leader of the Roman Catholic Church.\nPremise[3]: Libya’s case against Britain and the US concerns the dispute over their demand\nfor extradition of Libyans charged with blowing up a Pan Am jet over Lockerbie in 1988.\nHypothesis[3]: One case involved the extradition of Libyan suspects in the Pan Am Lockerbie bombing.\nPremise[4]: Argentina sought help from Britain on its privatization program and encouraged British\ninvestment.\nHypothesis[4]: Argentina sought UK expertise on privatization and agriculture.\nAnswer[1]: No Weapons of Mass Destruction Found, which contradicts the hypothesis. So the\nanswer is False.\nAnswer[2]: As Roman Catholic faithful gathered in downtown Chicago to mark the installation of new\nPope Benedict XVI. So the answer is True.\nAnswer[3]: Libya’s case suspects in the Pan Am Lockerbie bombing. So the answer is True.\nAnswer[4]: Argentina sought help from Britain on its privatization program, not agriculture, which\ncontradicts the hypothesis. So the answer is False.\nPremise[1]: Startling new research into mobile phones claims they may reduce a man’s sperm count by\nup to 30%.\nHypothesis[1]: Male fertility may be affected by use of a mobile phones.\nPremise[2]: It rewrites the rules of global trade, established by the General Agreement on Tariffs and\nTrade, or GATT, in 1947, and modified in multiple rounds of negotiations since then.\nHypothesis[2]: GATT was formed in 1947.\nPremise[3]: The cost of the consumer of the United States fell in June.\nHypothesis[3]: U.S. consumer spending dived in June.\nPremise[4]: Israeli Prime Minister Ariel Sharon has said that Mahmoud Abbas is a man that Israel can do\nbusiness with. Hypothesis[4]: Palestinian leader, Mahmoud Abbas, may be someone Israel can talk with.\nAnswer[1]: New research claims mobile phones reduce a man’s sperm count, i.e., affects male fertility. So\nthe answer is True.\nAnswer[2]: GATT is rewritten in 1947, not formed in 1947, which contradicts the hypothesis. So the answer\nis False.\nAnswer[3]: The consumer cost fell in June, not the spending, which contradicts the hypothesis. So the\nanswer is False.\nAnswer[4]: Mahmoud Abbas is a man that Israel can do business with, i.e., he may be someone Israel\ncan talk with. So the answer is True.\nPremise[1]: In October, however, amid rising tensions between the government and opposition groups,\na car bomb seriously injured an opposition politician and killed his driver, in Beirut.\nHypothesis[1]: A member of the opposition was injured in a car bomb attack in Beirut.\nPremise[2]: Ruth’s 1927 single season record of 60 home runs stood unsurpassed until Roger Maris hit 61 in 1961.\nHypothesis[2]: Babe Ruth hit 60 home runs in his lifetime.\nPremise[3]: The German technology was employed to build Shanghai’s existing maglev line, the first\nin the world to be used commercially.\nHypothesis[3]: Maglev is commercially used.\nPremise[4]: Twelve of Jupiter’s moons are relatively small and seem to have been more likely captured\nthan to have been formed in orbit around Jupiter.\nHypothesis[4]: Jupiter has Twelve moons.\nAnswer[1]: A car bomb seriously injured an opposition politician in Beirut. So the answer the True.\nAnswer[2]: Babe Ruth hit 60 home runs in a single season, not his lifetime, which contradicts the hypothesis.\nSo the answer is False.\nAnswer[3]: The German technology was employed to build Shanghai’s existing maglev line, i.e., Maglev\nis commercially used. So the answer is True.\nAnswer[4]: Twelve of Jupiter’s moons are relatively small, not Jupiter has Twelve moons, which contradicts\nthe hypothesis. So the answer is False.\nTable 14: RTE Prompt.\n807\nMNLI Prompt\nPremise[1]: Conceptually cream skimming has two basic dimensions - product and geography.\nHypothesis[1]: Product and geography are what make cream skimming work.\nPremise[2]: One of our number will carry out your instructions minutely.\nHypothesis[2]: A member of my team will execute your orders with immense precision.\nPremise[3]: Analyzing Postal Service accounts for depreciation, fuel, and maintenance for\ncity delivery carriers, we have estimated the average city delivery vehicle cost per route.\nHypotheis[3]: Driving cost estimates can be averaged with sufficient data.\nPremise[4]: Consider the United States Postal Service.\nHypothesis[4]: Forget the United States Postal Service.\nAnswer[1]: The answer is Neutral.\nAnswer[2]: The answer is True.\nAnswer[3]: The answer is Neutral.\nAnswer[4]: The answer is False.\nPremise[1]: Take a remarkable statistic that Shesol cites but lets pass relatively unexamined.\nHypothesis[1]: They had data that was very relevant but under used.\nPremise[2]: The man on the ground thinks for a moment and yells back, You must work in management.\nHypothesis[2]: There was no one on the ground, man or woman.\nPremise[3]: Hello, Ben.\nHypothesis[3]: I ignored Ben.\nPremise[4]: How can you prove it?\nHypothesis[4]: Can you tell me how to prove it?\nAnswer[1]: The answer is True.\nAnswer[2]: The answer is False.\nAnswer[3]: The answer is False.\nAnswer[4]: The answer is True.\nPremise[1]: In the midst of this amazing amalgam of cultures is a passion for continuity.\nHypothesis[1]: A passion for continuity is not the most important of these cultures.\nPremise[2]: Poirot, I exclaimed, with relief, and seizing him by both hands, I dragged him into the room.\nHypothesis[2]: Poirot was now back and I was sorry that he would take over what I now considered\nmy own investigation.\nPremise[3]: There’s a uh a couple called um oh i’m going to forgot his name now uh Dirkson.\nHypothesis[3]: I can’t remember their name.\nPremise[4]: It’s not that the questions they asked weren’t interesting or legitimate (though most did fall\nunder the category of already asked and answered).\nHypothesis[4]: All of the questions were interesting according to a focus group consulted on the subject.\nAnswer[1]: The answer is Neutral.\nAnswer[2]: The answer is False.\nAnswer[3]: The answer is True.\nAnswer[4]: The answer is Neutral.\nTable 15: MNLI Prompt.\n808\nSST-5 Prompt\nQ[1]: a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s\nhorror films.\nQ[2]: they presume their audience wo n’t sit still for a sociology lesson, however entertainingly\npresented, so they trot out the conventional science-fiction elements of bug-eyed monsters and\nfuturistic women in skimpy clothes.\nQ[3]: um , no..\nQ[4]: jonathan parker’s bartleby should have been the be-all-end-all of the modern-office anomie films.\nA[1]: The tone is very positive.\nA[2]: The tone is negative.\nA[3]: The tone is neutral.\nA[4]: The tone is positive.\nQ[1]: lacks the inspiration of the original and has a bloated plot that stretches the running time\nabout 10 minutes past a child’s interest and an adult’s patience.\nQ[2]: the santa clause 2 proves itself a more streamlined and thought out encounter than the original\ncould ever have hoped to be.\nQ[3]: you might say tykwer has done all that heaven allows, if you wanted to make as anti-kieslowski\na pun as possible.\nQ[4]: otto-sallies has a real filmmaker’s eye.\nA[1]: The tone is very negative.\nA[2]: The tone is positive.\nA[3]: The tone is neutral.\nA[4]: The tone is positive.\nQ[1]: with a confrontational stance, todd solondz takes aim on political correctness and suburban\nfamilies.\nQ[2]: verall , cletis tout is a winning comedy that excites the imagination and tickles the funny bone.\nQ[3]: with its parade of almost perpetually wasted characters ... margarita feels like a hazy high\nthat takes too long to shake.\nQ[4]: an ugly-duckling tale so hideously and clumsily told it feels accidental.\nA[1]: The tone is neutral.\nA[2]: The tone is very positive.\nA[3]: The tone is negative.\nA[4]: The tone is very negative.\nTable 16: SST-5 Prompt.\n809\nChat Prompt for GSM8K\nSystem\nThe answers should respond to each question the user inputs. One line one answer starting with A[idx],\nno other line break.\nUser\nQ[1]: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nQ[2]: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nQ[3]: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left\nin total?\nQ[4]: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\ndid Jason give to Denny?\nAssistant\nA[1]: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have\nbeen 21 - 15 = 6. The answer is 6.\nA[2]: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nA[3]: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35,\nthey had 74 - 35 = 39. The answer is 39.\nA[4]: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12\n= 8. The answer is 8.\nUser\nQ[1]: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nQ[2]: There were nine computers in the server room. Five more computers were installed each day, from monday\nto thursday. How many computers are now in the server room?\nQ[3]: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\nballs did he have at the end of wednesday?\nQ[4]: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAssistant\nA[1]: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\nThe answer is 9.\nA[2]: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20\ncomputers were added. 9 + 20 is 29. The answer is 29.\nA[3]: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had\n35 - 2 = 33 golf balls. The answer is 33.\nA[4]: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left.\n23 - 15 is 8. The answer is 8.\nUser\nQ[1]: A garden produced 237 potatoes, 60 fewer cucumbers and twice as many peppers than the cucumbers. How\nmany vegetables did the garden produce?\nQ[2]: John’s cow weighs 400 pounds. It increased its weight to 1.5 times its starting weight. He is able to sell the cow\nfor $3 per pound. How much more is it worth after gaining the weight?\nQ[3]: John writes 20 pages a day. How long will it take him to write 3 books that are 400 pages each?\nQ[4]: James has a rainwater collection barrel. For each inch of rain he collects 15 gallons. On Monday it rained 4 inches\nand on Tuesday it rained 3 inches. He can sell water for $1.2 per gallon. How much money did he make from selling\nall the water?\nAssistant\nA[1]: The garden produced 237 - 60 = 177 cucumbers. The garden produced 177 * 2 = 354 peppers. The garden\nproduced 237 + 177 + 354 = 768 vegetables. The answer is 768.\nA[2]: The cow initially weighs 400 * 1.5 = 600 pounds. So it gained 600 - 400 = 200 pounds. It is worth 200 * 3 = 600\ndollars more. The answer is 600.\nA[3]: He wants to write 3 * 400 = 1200 pages. So it will take him 1200 / 20= 60 days The answer is 60.\nA[4]: It rained 3 + 4 = 7 inches So he collected 7 * 15 = 105 gallons So he makes 105 * 1.2 = 126 from selling the\nwater. The answer is 126.\nUser\n{four test questions}\nAssistant\n{four test answers.}\nTable 17: An example GPT-3.5 (ChatGPT) and GPT-4 prompt we use for batch prompting. Specifically, the task\ninstruction is given in the system message. In the next a few rounds, one batch of in-context exemplars is input in\none round as the role “user\", and the answers are output as the role “assistant\". In the final round, test samples’\ncontexts are input and the model outputs the answers.\n810",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.7775391340255737
    },
    {
      "name": "Computer science",
      "score": 0.7454957365989685
    },
    {
      "name": "Security token",
      "score": 0.7197469472885132
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5970213413238525
    },
    {
      "name": "Language model",
      "score": 0.547543466091156
    },
    {
      "name": "Code (set theory)",
      "score": 0.539032518863678
    },
    {
      "name": "Sample (material)",
      "score": 0.46280235052108765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4186258912086487
    },
    {
      "name": "Batch processing",
      "score": 0.4171905517578125
    },
    {
      "name": "Natural language processing",
      "score": 0.3668076992034912
    },
    {
      "name": "Machine learning",
      "score": 0.3543137311935425
    },
    {
      "name": "Programming language",
      "score": 0.17646071314811707
    },
    {
      "name": "Computer security",
      "score": 0.0972302258014679
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}