{
  "title": "The Opposite of Smoothing: A Language Model Approach to Ranking Query-Specific Document Clusters",
  "url": "https://openalex.org/W1563094057",
  "year": 2011,
  "authors": [
    {
      "id": null,
      "name": "O. Kurland",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": null,
      "name": "E. Krikon",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6677351378",
    "https://openalex.org/W2160746453",
    "https://openalex.org/W2028122423",
    "https://openalex.org/W2066636486",
    "https://openalex.org/W6653691414",
    "https://openalex.org/W6631459773",
    "https://openalex.org/W2089561267",
    "https://openalex.org/W6759216221",
    "https://openalex.org/W2014158545",
    "https://openalex.org/W6666945086",
    "https://openalex.org/W6639773293",
    "https://openalex.org/W6683348788",
    "https://openalex.org/W2099871636",
    "https://openalex.org/W1992795877",
    "https://openalex.org/W42510783",
    "https://openalex.org/W2084048649",
    "https://openalex.org/W6682477757",
    "https://openalex.org/W2041565863",
    "https://openalex.org/W1981202432",
    "https://openalex.org/W2145674033",
    "https://openalex.org/W2075246941",
    "https://openalex.org/W2027445772",
    "https://openalex.org/W2950763224",
    "https://openalex.org/W2950669208",
    "https://openalex.org/W2068905009",
    "https://openalex.org/W6640155274",
    "https://openalex.org/W1514403774",
    "https://openalex.org/W6684900693",
    "https://openalex.org/W6631512791",
    "https://openalex.org/W2061198046",
    "https://openalex.org/W1803641895",
    "https://openalex.org/W6679397115",
    "https://openalex.org/W6651262116",
    "https://openalex.org/W1687668752",
    "https://openalex.org/W6676979179",
    "https://openalex.org/W2144270295",
    "https://openalex.org/W7015831105",
    "https://openalex.org/W2018557178",
    "https://openalex.org/W2157820455",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W6682715607",
    "https://openalex.org/W2021986193",
    "https://openalex.org/W2129971563",
    "https://openalex.org/W2155194165",
    "https://openalex.org/W1965893683",
    "https://openalex.org/W6826403133",
    "https://openalex.org/W6605815150",
    "https://openalex.org/W6661223370",
    "https://openalex.org/W514365",
    "https://openalex.org/W4231856373",
    "https://openalex.org/W2168286517",
    "https://openalex.org/W2136542423"
  ],
  "abstract": "Exploiting information induced from (query-specific) clustering of top-retrieved documents has long been proposed as a means for improving precision at the very top ranks of the returned results. We present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. While most previous cluster ranking approaches focus on the cluster as a whole, our model utilizes also information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identifying clusters containing a high relevant-document percentage. Furthermore, using the model to produce document ranking yields precision-at-top-ranks performance that is consistently better than that of the initial ranking upon which clustering is performed. The performance also favorably compares with that of a state-of-the-art pseudo-feedback-based retrieval method.",
  "full_text": "Journal of Artiﬁcial Intelligence Research 41 (2011) 367–3 95 Submitted 03/2011; published 07/2011\nThe Opposite of Smoothing: A Language Model Approach\nto Ranking Query-Speciﬁc Document Clusters\nOren Kurland kurland@ie.technion.ac.il\nEyal Krikon krikon@tx.technion.ac.il\nFaculty of Industrial Engineering and Management\nTechnion — Israel Institute of Technology\nAbstract\nExploiting information induced from ( query-speciﬁc ) clustering of top-retrieved docu-\nments has long been proposed as a means for improving precisi on at the very top ranks of\nthe returned results. We present a novel language model approach to ranking query-speciﬁc\nclusters by the presumed percentage of relevant documents t hat they contain. While most\nprevious cluster ranking approaches focus on the cluster as a whole, our model utilizes\nalso information induced from documents associated with th e cluster. Our model substan-\ntially outperforms previous approaches for identifying cl usters containing a high relevant-\ndocument percentage. Furthermore, using the model to produ ce document ranking yields\nprecision-at-top-ranks performance that is consistently better than that of the initial rank-\ning upon which clustering is performed. The performance als o favorably compares with\nthat of a state-of-the-art pseudo-feedback-based retriev al method.\n1. Introduction\nUsers of search engines want to see the results most pertaining to their queries at the highest\nranks of the returned document lists. However, attaining hi gh precision at top ranks is still\na very diﬃcult challenge for search engines that have to cope with various (types of) queries\n(Buckley, 2004; Harman & Buckley, 2004).\nHigh precision at top ranks is also important for applicatio ns that rely on search as\nan intermediate step; for example, question answering syst ems (Voorhees, 2002; Collins-\nThompson, Callan, Terra, & Clarke, 2004). These systems hav e to provide an answer to\na user’s query rather than return a list of documents. Often, question answering systems\nemploy a search over the given document corpus using the ques tion at hand as a query\n(Voorhees, 2002). Then, passages of the highest ranked docu ments are analyzed for ex-\ntracting (compiling) an answer to the question. Hence, it is important that the documents\ncontain question-pertaining information.\nTo cope with the fact that a search engine can often return at t he highest ranks of the\nresult list quite a few documents that are not relevant to the user’s query, researchers have\nproposed, among others, cluster-based result interfaces (Hearst & Pedersen, 1996; Leuski,\n2001). That is, the documents that are initially highest ran ked are clustered into clusters\nof similar documents. Then, the user can potentially exploi t the clustering information to\nmore quickly locate relevant documents from the initial res ult list. An important question\nin devising cluster-based result interfaces is the order by which to present the clusters to\nthe users (Leuski, 2001). This order should potentially reﬂ ect the presumed percentage of\nrelevant documents in the clusters.\nc⃝ 2011 AI Access F oundation. All rights reserved.\nKurland & Krikon\nClusters of top-retrieved documents (a.k.a. query-speciﬁc clusters ) can also be utilized\nwithout the user (or an application that uses search as an int ermediate step) being aware\nthat clustering has been performed. Indeed, researchers ha ve proposed using information\ninduced from the clusters to automatically re-rank the init ially retrieved list so as to im-\nprove precision at top ranks (Preece, 1973; Willett, 1985; H earst & Pedersen, 1996; Liu &\nCroft, 2004; Kurland & Lee, 2006; Yang, Ji, Zhou, Nie, & Xiao, 2006; Liu & Croft, 2008).\nMuch of the motivation for employing clustering of top-retrieved documents comes from van\nRijsbergen’s cluster hypothesis (van Rijsbergen, 1979), which states that “closely associated\ndocuments tend to be relevant to the same requests”. Indeed, it was shown that applying\nvarious clustering techniques to the documents most highly ranked by some initial search\nproduces some clusters that contain a very high percentage o f relevant documents (Hearst\n& Pedersen, 1996; Tombros, Villa, & van Rijsbergen, 2002; Ku rland, 2006; Liu & Croft,\n2006a). Moreover, positioning these clusters’ constituen t documents at the very top ranks\nof the returned results yields precision-at-top-ranks per formance that is substantially bet-\nter than that of state-of-the-art document-based retrieva l approaches (Hearst & Pedersen,\n1996; Tombros et al., 2002; Kurland, 2006).\nThus, whether used for creating eﬀective result interfaces or for automatic re-ranking of\nsearch results, whether utilized so as to help serve users of search engines or applications\nthat rely on search, query-speciﬁc clustering (i.e., clust ering of top-retrieved documents)\ncan result in much merit. Yet, a long standing challenge — pro gress with which can yield\nsubstantial retrieval eﬀectiveness improvements over state-of-the-art retrieval approaches as\nwe show — is the ability to identify query-speciﬁc clusters t hat contain a high percentage\nof documents relevant to the query.\nWe present a novel language-model-based approach to rankin g query-speciﬁc clusters\nby the presumed percentage of relevant documents that they c ontain. The key insight\nthat guides the derivation of our cluster-ranking model is t hat documents that are strongly\nassociated with a cluster can serve as proxies for ranking it . Since documents can be\nconsidered as more focused units than clusters, they can ser ve, for example, as mediators\nfor estimating the cluster-query “match”. Thus, while most previous approaches to ranking\nvarious types of clusters focus on the cluster as a whole unit (Jardine & van Rijsbergen,\n1971; Croft, 1980; Voorhees, 1985; Willett, 1985; Kurland & Lee, 2004; Liu & Croft,\n2004, 2006b), our model integrates whole-cluster-based information with that induced from\ndocuments associated with the cluster. Hence, we conceptua lly take the opposite approach\nto that of cluster-based smoothing of document language models that has recently been\nproposed for document ranking (Azzopardi, Girolami, & van R ijsbergen, 2004; Kurland &\nLee, 2004; Liu & Croft, 2004; Tao, Wang, Mei, & Zhai, 2006; Wei & Croft, 2006); that\nis, using cluster-based information to enrich a document re presentation for the purpose of\ndocument ranking.\nOur model integrates two types of information induced from c lusters and their proxy\n(associated) documents. The ﬁrst is the estimated similarity to the query. The second is the\ncentrality of an element (document or cluster) with respect to its refer ence set (documents\nin the initially-retrieved list or clusters of these docume nts); centrality is deﬁned in terms\nof textual similarity to other central elements in the refer ence set (Kurland & Lee, 2005).\nUsing either, or both, types of information just described — induced from a cluster as a\nwhole and/or from its proxy documents — yields several novel cluster ranking criteria that\n368\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\nLM 45. 7 50. 0 53. 6 33. 9\nRelevance model 50. 3 54. 4 58. 8 35. 7\nOptimal cluster 79. 6 83. 6 81. 5 65. 9\nTable 1: The resultant p@5 performance of ﬁnding the “optima l cluster” in comparison to\nthat of the initial LM-based ranking upon which clustering i s performed, and that\nof an optimized relevance model.\nare integrated in our model. We study the relative contribut ion of each of these criteria to\nthe overall eﬀectiveness of our approach. Furthermore, we s how that previously proposed\ncluster ranking methods, which were developed independent ly, can be derived or explained\nusing our ranking framework.\nEmpirical evaluation shows that our cluster ranking model consistently and substantially\noutperforms previously proposed methods in identifying cl usters that contain a high per-\ncentage of relevant documents. Furthermore, positioning t he constituent documents of the\ncluster most highly ranked by our model at the top of the list o f results yields precision-at-\ntop-ranks performance that is substantially better than that of the initial document ranking\nupon which clustering was performed. The resultant perform ance also favorably compares\nwith that of a state-of-the-art pseudo-feedback-based document retrieval method; and, with\nthat of approaches that utilize inter-document similariti es (e.g., using clusters) to directly\n(re-)rank documents.\n2. Motivation\nWe ﬁrst start by demonstrating the performance merits of the ability to eﬀectively rank\nquery-speciﬁc clusters by the presumed percentage of relevant documents that they contain.\nSuppose that we have an initial list of documents that were re trieved in response to a\nquery by using a standard language model (LM) approach (Ponte & Croft, 1998; Laﬀerty &\nZhai, 2001). Suppose also that some clustering algorithm is used to cluster the 50 highest\nranked documents, and that the resultant clusters contain 5 documents each. (Speciﬁc\ndetails of the experimental setup are provided in Section 5.2.) We deﬁne the optimal cluster\nas the one that contains the highest percentage of relevant d ocuments. If we position the\nconstituent documents of this cluster at the ﬁrst ﬁve ranks o f the document list returned in\nresponse to the query, then the resultant precision at 5 (p@5) performance is the percentage\nof relevant documents in this cluster. We contrast the p@5 pe rformance with that of the\ninitial LM-based ranking. As an additional reference compa rison we use an optimized\nrelevance model , RM3, which is a state-of-the-art pseudo-feedback-based q uery expansion\napproach (Lavrenko & Croft, 2001; Abdul-Jaleel et al., 2004 ). The performance numbers\nfor four TREC corpora are presented in Table 1. (Details regarding the corpora and queries\nused are provided in Section 5.2.)\nThe message rising from Table 1 is clear. If we were able to aut omatically identify\nthe optimal cluster, then the resultant performance would h ave been much better than\nthat of the initial LM-based ranking upon which clustering i s performed. Furthermore, the\n369\nKurland & Krikon\nperformance is also substantially better than that of a stat e-of-the-art retrieval approach.\nSimilar conclusions were echoed in previous work on using cl usters of top-retrieved doc-\numents (Hearst & Pedersen, 1996; Tombros et al., 2002; Crest ani & Wu, 2006; Kurland,\n2006; Liu & Croft, 2006a; Kurland & Domshlak, 2008).\n3. Ranking F ramework\nThroughout this section we assume that the following have be en ﬁxed: a query q, a corpus\nof documents D, and an initial list of N documents DN\ninit ⊂ D (henceforth Dinit) that are the\nhighest ranked by some search performed in response toq. We assume that Dinit is clustered\ninto a set of document clustersC l(Dinit) = {c1, . . . , c M } by some clustering algorithm1. Our\ngoal is to rank the clusters in C l(Dinit) by the presumed percentage of relevant documents\nthat they contain. In what follows we use the term “cluster” t o refer either to the set of\ndocuments it is composed of, or to a (language) model induced from it. We use py (x) to\ndenote the language-model-based similarity between y (a document or a cluster) and x (a\nquery or a cluster); we describe our language-model inducti on method in Section 5.1.\n3.1 Cluster Ranking\nSimilarly to the language model approach to ranking documen ts (Ponte & Croft, 1998;\nCroft & Laﬀerty, 2003), and in deference to the recent growin g interest in automatically\nlabeling document clusters and topic models (Geraci, Pelle grini, Maggini, & Sebastiani,\n2006; Treeratpituk & Callan, 2006; Mei, Shen, & Zhai, 2007), we state the problem of\nranking clusters as follows: estimate the probability p(c|q) that cluster c can be labeled\n(i.e., its content can be described) by the terms in q. We hypothesize that the higher this\nprobability is, the higher the percentage of documents pert aining to q that c contains.\nSince q is ﬁxed, we use the rank equivalence\np(c|q) rank= p(q|c) ·p(c)\nto rank the clusters in C l(Dinit). Thus, c is ranked by combining the probability p(q|c) that\nq is “generated”2 as a label for c with c’s prior probability (p(c)) of “generating” any label.\nIndeed, most prior work on ranking various types of clusters (Jardine & van Rijsbergen,\n1971; Croft, 1980; Willett, 1985; Voorhees, 1985; Kurland & Lee, 2004; Liu & Croft, 2004)\nimplicitly uses uniform distribution for p(c), and estimates p(q|c) (in spirit) by comparing\na representation of c as a whole unit with that of q.\nHere, we suggest to incorporate a document mediated approac h to estimating the prob-\nability p(q|c) of generating the label q for cluster c. Since documents can be considered as\nmore coherent units than clusters, they might help to genera te more informative/focused\nlabels than those generated by using representations of clu sters as whole units. Such an\n1. Clustering the documents most highly ranked by a search performed in response to a query is often termed\nquery-speciﬁc clustering (Willett, 1985). We do not assume, however, that the cluster ing algorithm has\nknowledge of the query in hand.\n2. While the term “generate” is convenient, we do not assume that clusters or documents literally generate\nlabels, nor do we assume an underlying generative theory as that presented by Lavrenko and Croft (2001)\nand Lavrenko (2004), inter alia .\n370\nA Language Model Approach to Ranking Query-Specific Document Clusters\napproach is conceptually the opposite of smoothing a document representation (e.g., lan-\nguage model) with that of a cluster (Azzopardi et al., 2004; K urland & Lee, 2004; Liu &\nCroft, 2004; Wei & Croft, 2006). In what follows we use p(q|d) to denote the probability\nthat q is generated as a label describing document d’s content — cf., the language model-\ning approach to ranking documents (Ponte & Croft, 1998; Crof t & Laﬀerty, 2003). Also,\nwe assume that p(d) — the prior probability that document d generates any label — is a\nprobability distribution over the documents in the corpus D.\nWe let all, and only, documents in the corpus D to serve as proxies for label generation\nfor any cluster in C l(Dinit). Consequently, we assume that p(d|c), the probability that d\nis chosen as a proxy of c for label generation, is a probability distribution deﬁned over the\ndocuments in D. Then, we can write using some probability algebra\np(c|q) rank= p(c)\n∑\ndi ∈D\np(q|c, d i)p(di|c). (1)\nWe use λp (q|c) + (1−λ )p(q|di ), where λ is a free parameter, as an estimate for p(q|c, d i )\n(Si, Jin, Callan, & Ogilvie, 2002; Kurland & Lee, 2004) in Equ ation 1, and by applying\nprobability algebra we get the following scoring principle 3 for clusters\nλp (c)p(q|c) + (1− λ )\n∑\ndi ∈D\np(q|di)p(c|di )p(di). (2)\nEquation 2 scores c by a mixture of (i) the probability that q is directly generated from c\ncombined with c’s prior probability of generating any label, and (ii) the (average) probability\nthat q is generated by documents that are both “strongly associate d” with c (as measured\nby p(c|di)) and that have a high prior probability p(di) of generating labels.\nWe next derive speciﬁc ranking algorithms from Equation 2 bymaking some assumptions\nand estimation choices.\n3.2 Algorithms\nWe ﬁrst make the assumption, which underlies (in spirit) mos t pseudo-feedback-based re-\ntrieval models (Buckley, Salton, Allan, & Singhal, 1994; Xu & Croft, 1996; Lavrenko &\nCroft, 2003), that the probability of generating q directly from di (p(q|di)) is quite small\nfor documents di that are not in the initially retrieved list Dinit; hence, these documents\nhave relatively little eﬀect on the summation in Equation 2. Furthermore, if the clusters in\nC l(Dinit) are produced by a “reasonable” clustering algorithm, then p(c|di ) — the cluster-\ndocument association strength — might be assumed to be signiﬁcantly higher for documents\nfrom Dinit that are in c than for documents from Dinit that are not in c. Consequently, we\ntruncate the summation in Equation 2 by allowing only c’s constituent documents to serve\nas it proxies for generating q. Such truncation does not only alleviate the computational\ncost of estimating Equation 2, but can also yield improved eﬀ ectiveness as we show in Sec-\ntion 5.3. In addition, we follow common practice in the langu age model framework (Croft\n3. The shift in notation and terminology from “p(c|q)\nrank\n= ” to “score of c” echoes the transition from using\n(model) probabilities to estimates of such probabilities.\n371\nKurland & Krikon\n& Laﬀerty, 2003), speciﬁcally, in work on utilizing cluster -based language models for docu-\nment retrieval (Liu & Croft, 2004; Kurland & Lee, 2004), and use language-model estimates\nfor conditional probabilities to produce our primary ranki ng principle:\nS core(c)\ndef\n= λp (c)pc(q) + (1− λ )\n∑\ndi ∈c\npdi (q)pdi (c)p(di ). (3)\nNote that using pd(c) for p(c|d) means that we use the probability of generating the “label”c\n(i.e., some term-based representation ofc) from document d as a surrogate for the document-\ncluster association strength.\nThe remaining task is to estimate the document and cluster pr iors, p(d) and p(c), re-\nspectively.\n3.2.1 Document and cluster biases\nFollowing common practice in work on language-model-based retrieval we can use a uni-\nform distribution for the document prior p(d) (Croft & Laﬀerty, 2003), and similarly assume\na uniform distribution for the cluster prior p(c). Such practice would have been a natu-\nral choice if the clusters we want to rank were produced in a query-independent fashion.\nHowever, we would like to exploit the fact that the clusters i n C l(Dinit) are composed of\ndocuments in the initially retrieved list Dinit. A case in point, since Dinit was retrieved in\nresponse to q, documents in Dinit that are considered as reﬂecting Dinit’s content might be\ngood candidates for generating the label q (Kurland & Lee, 2005); a similar argument can\nbe made for clusters in C l(Dinit) that reﬂect its content. Therefore, instead of using “true ”\nprior distributions, we use biases that represent the centrality (Kurland & Lee, 2005) of\ndocuments with respect to Dinit and the centrality of clusters with respect to C l(Dinit).4\nWe adopt a recently proposed approach to inducing document c entrality that is based\non measuring the similarity of a document in Dinit to other central documents in Dinit (Kur-\nland & Lee, 2005). To quantify this recursive centrality deﬁ nition, we compute PageRank’s\n(Brin & Page, 1998) stationary distribution over a graph whe rein vertices represent docu-\nments in Dinit and edge-weights represent inter-document language-model-based similarities\n(Kurland & Lee, 2005). We then set p(d)\ndef\n= Cent(d) for d ∈ Dinit and 0 otherwise, where\nCent(d) is d’s PageRank score; hence, p(d) is a probability distribution over the entire\ncorpus D.\nAnalogously, we set p(c)\ndef\n= Cent(c) for c ∈ C l(Dinit), where Cent(c) is c’s PageRank\nscore as computed over a graph wherein vertices are clusters in C l(Dinit) and edge-weights\nrepresent language-model-based inter-cluster similarities; therefore, p(c) is a probability dis-\ntribution over the given set of clusters C l(Dinit). The construction method of the document\nand cluster graphs follows that of constructing document-s olely graphs (Kurland & Lee,\n2005), and is elaborated in Appendix A.\nUsing the document and cluster induced biases we can now full y instantiate Equation 3\nto derive ClustRanker, our primary cluster ranking algorithm:\n4. The biases are not “true” prior distributions, because of the virtue by which Dinit was created, that is,\nin response to the query. However, we take care that the biase s form valid probability distributions as\nwe show later.\n372\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAlgorithm Scoring function ( S core(c))\nClustCent Cent(c)\nClustQueryGen pc(q)\nClustCent ∧ ClustQueryGen Cent(c)pc(q)\nDocCent ∑\ndi ∈c pdi (c)Cent(di )\nDocQueryGen ∑\ndi ∈c pdi (q)pdi (c)\nDocCent ∧ DocQueryGen ∑\ndi ∈c pdi (q)pdi (c)Cent(di)\nClustCent ∧ DocCent λCent (c) + (1− λ ) ∑\ndi ∈c pdi (c)Cent(di )\nClustQueryGen ∧ DocQueryGen λp c(q) + (1− λ ) ∑\ndi ∈c pdi (q)pdi (c)\nClustRanker λCent (c)pc(q) + (1 − λ ) ∑\ndi ∈c pdi (q)pdi (c)Cent(di )\nTable 2: Summary of methods for ranking clusters.\nS coreClustRanker (c)\ndef\n= λCent (c)pc(q) + (1− λ )\n∑\ndi ∈c\npdi (q)pdi (c)Cent(di). (4)\n3.2.2 Methods for ranking clusters\nThe ClustRanker algorithm ranks cluster c by integrating several criteria: (i) ClustCent\n— c’s centrality ( Cent(c)), (ii) ClustQueryGen — the possibility to generate the label\nq directly from c as measured by pc(q), (iii) DocCent — the centrality of c’s constituent\ndocuments ( Cent(d)), and (iv) DocQueryGen — the possibility to generate q by c’s\nconstituent documents as measured by pd(q). (Note that the latter two are combined with\nthe cluster-document association strength, pd(c)).\nTo study the eﬀectiveness of each of these criteria (and some of their combinations)\nfor ranking clusters, we apply the following manipulations to the ClustRanker algorithm:\n(i) setting λ to 1 (0) to have only the cluster (documents) generate q, (ii) using uniform\ndistribution for Cent(c) (overC l(Dinit)) and/or forCent(d) (overDinit) hence assuming that\nall clusters in C l(Dinit) and/or documents in Dinit are central to the same extent; we assume\nthat the number of clusters in C l(Dinit) is the same as the number of documents in Dinit,\nas is the case for the clustering method that we employ in Sect ion 5; hence, the document\nuniform prior and the cluster uniform prior are the same; and (iv) setting pc(q) (pd(q)) to\nthe same constant value thereby assuming that for all cluste rs in C l(Dinit) (documents in\nDinit) the probability of directly generating q is the same. For instance, setting λ to 0 and\npd(q) to some constant, we rank c by DocCent — the weighted-average of the centrality\nvalues of its constituent documents: ∑\ndi ∈c pdi (c)Cent(di). Table 2 presents the resultant\ncluster ranking methods that we explore. (“∧” indicates that a method utilizes two criteria.)\n3.3 Explaining Previous Methods for Ranking Clusters\nThe ClustRanker method, or more generally, Equation 3 on whi ch it is based, can be used\nso as to help explain, and derive, some previously proposed m ethods for ranking clusters.\nWhile these methods were developed independently, and not a s part of a single framework,\ntheir foundations can be described in terms of our approach. In the following discussion\nwe use uniform prior for documents and clusters, and rank clu ster c, using Equation 3, by\n373\nKurland & Krikon\nλp c(q)+(1 −λ ) ∑\ndi ∈c pdi (q)pdi (c). Furthermore, recall that our framework is not committed\nto language models; i.e., px(y), which is a language-model-based estimate for p(y|x), can be\nreplaced with another estimate.\nNow, setting λ = 1, and consequently considering the cluster only as a whole unit, yields\nthe most common cluster ranking method. That is, ranking the cluster based on the match\nof its representation as a whole unit with that of the query; t he cluster can be represented,\nfor example, by using the concatenation of its constituent d ocuments (Kurland & Lee,\n2004; Liu & Croft, 2004) or by a centroid-based representati on of its constituent document\nrepresentations (Voorhees, 1985; Leuski, 2001; Liu & Croft , 2008). The ClustQueryGen\nmethod from Table 2, which was also used in previous work (Liu & Croft, 2004; Kurland\n& Lee, 2004; Liu & Croft, 2006b; Kurland & Lee, 2006), is an exa mple of this approach in\nthe language modeling framework.\nOn the other hand, setting λ = 0 results in ranking c by using its constituent documents\nrather than using c as a whole unit: ∑\ndi∈c pdi (q)pdi (c). Several cluster ranking methods\nthat were proposed in past literature ignore the document-c luster association strength.\nThis practice amounts to setting pdi (c) to the same constant for all clusters and documents.\nAssuming also that all clusters contain the same number of do cuments, as is the case in\nour experimental setup in Section 5, we then rank c by the arithmetic mean of the “query-\nmatch” values of its constituent documents, 1\n|c|\n∑\ndi ∈c pdi (q); |c| is the number of documents\nin c. The arithmetic mean can be bounded from above by max di ∈c pdi (q), which was used\nin some previous work on ranking clusters (Leuski, 2001; Sha nahan, Bennett, Evans, Hull,\n& Montgomery, 2003; Liu & Croft, 2008), or from below by the ge ometric mean of the\ndocument-query match values, |c|\n√ ∏\ndi ∈c pdi (q) (Liu & Croft, 2008; Seo & Croft, 2010). 5\nAlternatively, the minimal query-document match value, mi ndi ∈c pdi (q), which was also\nutilized for ranking clusters (Leuski, 2001; Liu & Croft, 20 08), also constitutes a lower\nbound for the arithmetic mean.\n4. Related Work\nQuery-speciﬁc clusters are often used to visualize the resu lts of search so as to help users\nto quickly detect the relevant documents (Hearst & Pedersen , 1996; Leuski & Allan, 1998;\nLeuski, 2001; Palmer et al., 2001; Shanahan et al., 2003). Le uski (2001), for example,\norders (hard) clusters in an interactive retrieval system b y the highest and lowest query-\nsimilarity exhibited by any of their constituent documents . We showed in Section 3.3\nthat these ranking methods, and others, that were used in sev eral reports on using query-\nspeciﬁc clustering (Shanahan et al., 2003; Liu & Croft, 2006 a), can be explained using our\nframework. Furthermore, in Section 5.3 we demonstrate the m erits of ClustRanker with\nrespect to these approaches.\nSome work uses information from query-speciﬁc clusters to smooth language models\nof documents in the initial list so as to improve the document -query similarity estimate\n(Liu & Croft, 2004; Kurland, 2009). In a related vein, graph- based approaches for re-\nranking the initial list, some using document clusters, that utilize inter-document similarity\n5. Liu and Croft (2008) and Seo and Croft (2010) used the geome tric-mean-based language model repre-\nsentation of clusters, rather than the geometric mean of the “query-match” values.\n374\nA Language Model Approach to Ranking Query-Specific Document Clusters\ninformation were also proposed (Diaz, 2005; Kurland & Lee, 2 005, 2006; Yang et al., 2006).\nThese approaches can potentially help to improve the perfor mance of our ClustRanker\nalgorithm, as they provide a higher quality document rankin g to begin with. Graph-based\napproaches for modeling inter-item textual similarities,some similar in spirit to our methods\nof inducing document and cluster centrality, were also usedfor text summarization, question\nanswering, and clustering (Erkan & Radev, 2004; Mihalcea, 2 004; Mihalcea & Tarau, 2004;\nOtterbacher, Erkan, & Radev, 2005; Erkan, 2006a, 2006b).\nRanking query-speciﬁc (and query-independent) clusters in response to a query has tra-\nditionally been based on comparing a cluster representation with that of the query (Jardine\n& van Rijsbergen, 1971; Croft, 1980; Voorhees, 1985; Willet t, 1985; Kurland & Lee, 2004;\nLiu & Croft, 2004, 2006b, 2006a). The ClustQueryGen criteri on, which was used in work\non ranking query-speciﬁc clusters in the language model fra mework (Liu & Croft, 2004;\nKurland, 2009), is a language-model manifestation of this r anking approach. We show that\nthe eﬀectiveness of ClustQueryGen is inferior to that of Clu stRanker in Section 5.3.\nSome previous cluster-based document-ranking models (Kurland & Lee, 2004; Kurland,\n2009) can be viewed as the conceptual opposite of our ClustRa nker method as they use\nclusters as proxies for ranking documents. However, these m odels use only query-similarity\ninformation while ClustRanker integrates such informatio n with centrality information. In\nfact, we show in Section 5.3 that centrality information is o ften more eﬀective than query-\nsimilarity (generation) information for ranking query-sp eciﬁc clusters; and, that their inte-\ngration yields better performance than that of using each al one.\nRecently, researchers have identiﬁed some properties of query-speciﬁc clusters that con-\ntain a high percentage of relevant documents (Liu & Croft, 20 06b; Kurland & Domsh-\nlak, 2008); among which are the cluster-query similarity (C lustQueryGen) (Liu & Croft,\n2006b), the query similarity of the cluster’s constituent d ocuments (DocQueryGen) (Liu\n& Croft, 2006b; Kurland & Domshlak, 2008), and the diﬀerence s between the two (Liu &\nCroft, 2006b). These properties were utilized for automati cally deciding whether to employ\ncluster-based or document-based retrieval in response to a query (Liu & Croft, 2006b),\nand for ranking query-speciﬁc clusters (Kurland & Domshlak , 2008). The latter approach\n(Kurland & Domshlak, 2008) relies on rankings induced by clu sters’ models over the entire\ncorpus, in contrast to our approach that focuses on the context within the initially retrieved\nlist. However, our centrality-based methods from Table 2 ca n potentially be incorporated\nin this cluster-ranking framework (Kurland & Domshlak, 200 8).\nSome work on ranking query-speciﬁc clusters resembles ours in that it utilizes cluster-\ncentrality information (Kurland & Lee, 2006); in contrast t o our approach, centrality is\ninduced based on cluster-document similarities. We furthe r discuss this approach and com-\npare it to ours in Section 5.3.\n5. Evaluation\nWe next evaluate the eﬀectiveness of our cluster ranking approach in detecting query-speciﬁc\nclusters that contain a high percentage of relevant documen ts.\n375\nKurland & Krikon\n5.1 Language-Model Induction\nFor language model induction, we treat documents and querie s as term sequences. While\nthere are several possible approaches of representing clus ters as whole units (Voorhees,\n1985; Leuski, 2001; Liu & Croft, 2006b; Kurland & Domshlak, 2 008), our focus here is\non the underlying principles of our ranking framework. Ther efore, we adopt an approach\ncommonly used in work on cluster-based retrieval (Kurland & Lee, 2004; Liu & Croft, 2004;\nKurland & Lee, 2006; Liu & Croft, 2006a), and represent a clus ter by the term sequence\nthat results from concatenating its constituent documents . The order of concatenation has\nno eﬀect since we only deﬁne unigram language models that ass ume term independence.\nWe use pDir[µ ]\nx (·) to denote the Dirichlet-smoothed unigram language model i nduced\nfrom term sequence x (Zhai & Laﬀerty, 2001); µ is the smoothing parameter. To avoid\nlength bias and underﬂow issues when assigning language-mo del probabilities to long texts\n(Lavrenko et al., 2002; Kurland & Lee, 2005), as is the case for pd(c), we adopt the following\nmeasure (Laﬀerty & Zhai, 2001; Kurland & Lee, 2004, 2005, 200 6), which is used for all\nthe language-model-based estimates in the experiments to follow, unless otherwise speciﬁed\n(speciﬁcally, for relevance-model construction):\npy (x)\ndef\n= exp\n(\n−D\n(\npDir[0]\nx (·)\n⏐\n⏐\n⏐\n⏐\n⏐\n⏐ pDir[µ ]\ny (·)\n))\n;\nx and y are term sequences, and D is the Kullback-Leibler (KL) divergence. The estimate\nwas empirically demonstrated as eﬀective in settings where in long texts are assigned with\nlanguage-model probabilities (Kurland & Lee, 2004, 2005, 2 006).\nAlthough the estimate just described does not constitute a p robability distribution —\nas is the case for unigram language models — some previous wor k demonstrates the merits\nof using it as is without normalization (Kurland & Lee, 2005, 2006).\n5.2 Experimental Setup\nWe conducted experiments with the following TREC corpora:\ncorpus # of docs queries disk(s)\nAP 242,918 51-64, 66-150 1-3\nTREC8 528,155 401-450 4-5\nWSJ 173,252 151-200 1-2\nWT10G 1,692,096 451-550 WT10G\nSome of these data sets were used in previous work on ranking q uery-speciﬁc clusters (Liu\n& Croft, 2004; Kurland & Lee, 2006; Liu & Croft, 2008) with whi ch we compare our\nmethods. We used the titles of TREC topics for queries. We app lied tokenization and\nPorter stemming via the Lemur toolkit (www.lemurproject.o rg), which was also used for\nlanguage model induction.\nWe set Dinit, the list upon which clustering is performed, to the 50 highe st ranked\ndocuments by an initial ranking induced over the entire corpus using pDir[µ ]\nd (q) — i.e., a\nstandard language-model approach. To have an initial ranki ng of a reasonable quality, we\nset the smoothing parameter, µ , to a value that results in optimized MAP (calculated at\nthe standard 1000 cutoﬀ) performance. This practice also fa cilitates the comparison with\n376\nA Language Model Approach to Ranking Query-Specific Document Clusters\nsome previous work on cluster ranking (Kurland & Lee, 2006), which employs the same\napproach for creating an initial list of 50 documents to be cl ustered. The motivation for\nusing a relatively short initial list rises from previous observations regarding the eﬀectiveness\nof methods that utilize inter-document similarities among top-retrieved documents (Liu &\nCroft, 2004; Diaz, 2005; Kurland, 2006, 2009). The document s most highly ranked exhibit\nhigh query similarity, and hence, short retrieved lists cou ld be viewed as providing a more\n“concise” corpus context for the query than longer lists. Si milar considerations were echoed\nin work on pseudo-feedback-based query expansion, wherein top-retrieved documents are\nused for forming a new query model (Xu & Croft, 1996; Zhai & Laﬀ erty, 2001; Lavrenko &\nCroft, 2001; Tao & Zhai, 2006).\nTo produce the setC l(Dinit) of query-speciﬁc clusters, we use a simple nearest-neighbors-\nbased clustering approach that is known to produce (some) cl usters that contain a high\npercentage of relevant documents (Kurland, 2006; Liu & Crof t, 2006a). Given d ∈ D init\nwe deﬁne a cluster that contains d and the k − 1 documents di ∈ Dinit (di ̸= d) that yield\nthe highest language-model similarity pdi (d). (We break ties by document IDs.) The high\npercentages of relevant documents in an optimal cluster tha t were presented in Table 1\nare for these clusters. More generally, this clustering app roach was shown to be eﬀective\nfor cluster-based retrieval (Griﬃths, Luckhurst, & Willet t, 1986; Kurland & Lee, 2004;\nKurland, 2006; Liu & Croft, 2006b, 2006a; Tao et al., 2006), s peciﬁcally, with respect to\nusing hard clusters (Kurland, 2009).\nWe posed our cluster ranking methods as a means for increasin g precision at the very\ntop ranks of the returned document list. Thus, we evaluate a c luster ranking method by\nthe percentage of relevant documents in the highest ranked c luster. We use p@k to denote\nthe percentage of relevant documents in a cluster of size k (either 5 or 10), because it is the\nprecision of the top k documents that is obtained if the cluster’s ( k) constituent documents\nare positioned at the top ranks of the results. This cluster r anking evaluation approach\nwas also employed in previous work on ranking clusters (Kurland & Lee, 2006; Liu & Croft,\n2008) with which we compare our methods. We determine statistically signiﬁcant diﬀerences\nof p@k performance using Wilcoxon’s two-sided test at a conﬁ dence level of 95%.\nTo focus on the underlying principles of our approach and its potential eﬀectiveness,\nand more speciﬁcally, to compare the relative eﬀectiveness and contribution to the overall\nperformance of the diﬀerent information types utilized by o ur methods, we ﬁrst ameliorate\nfree-parameter-values eﬀects. To that end, we set the value s of free parameters incorpo-\nrated by our methods to optimize average (over all queries per corpus) p@k performance\nfor clusters of size k. (Optimization is based on a line search of the free-paramet er values\nranges.) We employ the same practice for all reference comparisons. That is, we inde-\npendently optimize performance with respect to free-param eter values for p@5 and p@10.\nThen, in Section 5.3.5 we analyze the eﬀect of free-paramete r values on the eﬀectiveness of\nour approach. In addition, in Section 5.3.6 we study the perf ormance of our approach when\nfree-parameter values are set using cross validation performed over queries. The value of λ ,\nthe interpolation parameter in the ClustRanker algorithm, is selected from {0, 0. 1, . . . , 1}.\nThe values of the (two) parameters controlling the graph-co nstruction methods (for induc-\ning the document and cluster biases) are chosen from previously suggested ranges (Kurland\n& Lee, 2005). (See Appendix A for further details on graph con struction.) The value of µ ,\nthe language model smoothing parameter, is set to 2000 follo wing previous recommenda-\n377\nKurland & Krikon\ntions (Zhai & Laﬀerty, 2001), except for estimating pd(q) where we use the value chosen for\ncreating Dinit so as to maintain consistency with the initial ranking.\nIt is important to point out that the computational overhead of our approach on top\nof the initial search is not signiﬁcant. Clustering of top-r etrieved documents (50 in our\ncase) can be performed quickly (Zamir & Etzioni, 1998); we no te that our framework is\nnot committed to a speciﬁc clustering approach. Furthermor e, computing PageRank scores\nover a graph of 50 documents (clusters) to induce document (c luster) centrality takes only\na few iterations of the Power method (Golub & Van Loan, 1996). Finally, we note that the\nnumber of documents in the corpus has no eﬀect on the eﬃciency of our approach, as our\nmethods are based on clustering the documents most highly ra nked by the initial search.\n5.3 Experimental Results\nIn what follows we present and analyze the performance numbe rs of our cluster ranking\napproach, and study the impact of various factors on its eﬀec tiveness. In Section 5.3.1 we\nstudy the eﬀectiveness of ClustRanker as a means for improvi ng precision at top ranks. To\nthat end, we use a comparison with the initial ranking upon wh ich clustering is performed,\nand with relevance models (Lavrenko & Croft, 2001). Then, in Section 5.3.2 we study the\nrelative performance eﬀect of the various cluster ranking criteria integrated by ClustRanker.\nWe compare the eﬀectiveness of ClustRanker with that of prev iously proposed methods for\ncluster ranking in Section 5.3.3. In Section 5.3.4 we compar e the performance of Clus-\ntRanker with that of document-based re-ranking approaches that utilize inter-documents\nsimilarities in various ways. In Section 5.3.5 we analyze th e performance sensitivity of\nClustRanker with respect to free-parameter values. Finall y, in Section 5.3.6 we analyze the\nperformance of ClustRanker, and contrast it with that of var ious reference comparisons,\nwhen free-parameter values are set using cross validation p erformed over queries.\n5.3.1 Comparison with Document-Based Retrieval\nThe ﬁrst question we are interested in is the eﬀectiveness (o r lack thereof) of ClustRanker\nin improving precision at the very top ranks. Recall that we u se ClustRanker to rank\nclusters of k (∈ { 5, 10}) documents from Dinit — the initially retrieved document list.\nAs described above, we evaluate ClustRanker’s eﬀectivenes s by the percentage of relevant\ndocuments in the cluster most highly ranked. This percentag e is the p@k attained if the\ncluster’s constituent documents are positioned at the highest ranks of the ﬁnal result list. In\nTable 3 we compare the performance of ClustRanker with that o f the initial ranking. Since\nthe initial ranking was created using a standard language-m odel-based document retrieval\nperformed over the corpus withpd(q) as a scoring function, and with the document language\nmodel smoothing parameter ( µ ) optimized for MAP, we also consider optimized baselines\nas reference comparisons: ranking all documents in the corp us by pd(q) where µ is set to\noptimize (independently) p@5 and p@10.\nAs we can see in Table 3, ClustRanker posts performance that i s substantially better\nthan that of the initial ranking in all relevant comparisons (corpus × evaluation mea-\nsure). For AP and WT10G the performance improvements are als o statistically signiﬁcant.\nFurthermore, ClustRanker almost always outperforms the op timized baselines, often to a\nsubstantial extent; in several cases, the improvements are also statistically signiﬁcant.\n378\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nopt. base. 46. 5 43 . 7 51. 2 46 . 4 56. 0 49. 4 34. 1 28 . 2\nClustRanker 52.7i 50.6i\no\n57.6 50 .6 56.0 51 .2 39. 8i\no 33. 9i\no\nTable 3: Comparison of ClustRanker with the initial document ranking and optimized base-\nlines. Boldface marks the best result in a column; ’i’ and ’o’ mark statistically\nsigniﬁcant diﬀerences with the initial ranking, and optimi zed baselines, respec-\ntively.\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nRel Model 50. 3i 48. 6i 54. 4 50 . 2 58. 4i 53. 2i 35. 7 29 . 9\nRel Model(Re-Rank) 51. 1i 48. 3i 53. 6 49 . 8 58. 8i 53. 4i 36. 3 30 . 1\nClustRanker 52. 7i 50. 6i 57.6 50 .6 56. 0 51 . 2 39. 8i 33. 9i\nTable 4: Comparison of ClustRanker with a relevance model (R M3) used to either rank\nthe entire corpus (Rel Model) or to re-rank the initial list ( Rel Model(Re-Rank)).\nBoldface marks the best result in a column; ’i’ marks statist ically signiﬁcant dif-\nference with the initial ranking.\nComparison with Pseudo-F eedback-Based Retrieval The ClustRanker algorithm\nhelps to identify relevant documents in Dinit by exploiting clustering information. Pseudo-\nfeedback-based query expansion approaches, on the other hand, deﬁne a query model based\non Dinit and use it for (re-)ranking the entire corpus (Buckley et al. , 1994; Xu & Croft,\n1996). To contrast the two paradigms, we use the relevance model RM3 (Lavrenko & Croft,\n2001; Abdul-Jaleel et al., 2004; Diaz & Metzler, 2006), whic h is a state-of-the-art pseudo-\nfeedback-based query expansion approach. We use RM3 for ran king the entire corpus as\nis standard, and refer to this implementation as Rel Model . Since ClustRanker can be\nthought of as a means to re-ranking the initial list Dinit, we also experiment with using RM3\nfor re-ranking only Dinit, rather than the entire corpus;Rel Model(Re-Rank) denotes this\nimplementation. We set the values of the free parameters of R el Model and Rel Model(Re-\nRank) so as to independently optimize p@5 and p@10 performan ce. (See Appendix B for\ndetails regarding the relevance model implementation.)\nWe can see in Table 4 that ClustRanker outperforms the releva nce models on AP,\nTREC8 and WT10G; for WSJ, the relevance models outperform Cl ustRanker. The per-\nformance diﬀerences between ClustRanker and the relevance models, however, are not sta-\ntistically signiﬁcant. Nevertheless, these results attes t to the overall eﬀectiveness of our\napproach in attaining high precision at top ranks. As we late r show, previous methods\nfor ranking clusters often yield performance that is only co mparable to that of the initial\nranking, and much inferior to that of the relevance model.\n379\nKurland & Krikon\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nClustCent 51. 7 48 . 6i 52. 4 49 . 4 54. 8 50 . 0 39. 8i 33. 0i\nClustQueryGen 39. 2i 38. 8i 39. 6i 40. 6i 44. 0i 37. 0i 30. 0 24 . 1\nClustCent ∧ ClustQueryGen 49. 7 48 . 0i 55. 2 50 . 4 52. 4 47 . 8 39. 6i 33. 1i\nDocCent 52. 9i 48. 8 52. 0 48 . 8 55. 6 50 . 6 31. 0 28 . 1\nDocQueryGen 43. 6 46 . 7 47. 6 43 . 2 55. 2 47 . 0 33. 5 27 . 0\nDocCent ∧ DocQueryGen 52. 7i 50.6i 54. 8 49 . 0 56.0 51. 2 37. 1 31 . 4\nClustCent ∧ DocCent 53.5i 48. 8i 54. 8 49 . 8 56.0 51 .4 39. 8i 33. 0i\nClustQueryGen ∧ DocQueryGen 43. 6 46 . 7 47. 6 43 . 2 55. 2 47 . 8 36. 5 29 . 1\nClustRanker 52. 7i 50.6i 57.6 50 .6 56.0 51. 2 39. 8i 33. 9i\nTable 5: Comparison of the cluster ranking methods from Table 2. Boldface marks the best\nresult in a column and ’i’ indicates a statistically signiﬁc ant diﬀerence with the\ninitial ranking.\n5.3.2 Deeper Inside ClustRanker\nWe now turn to analyze the performance of the various cluster ranking criteria (methods)\nthat ClustRanker integrates so as to study their relative contribution to its overall eﬀective-\nness. (Refer back to Table 2 for speciﬁcation of the diﬀerent methods.) The performance\nnumbers are presented in Table 5.\nWe are ﬁrst interested in the comparison of the two types of in formation utilized for\nranking, that is, centrality and query-similarity (genera tion). We can see in Table 5 that\nin almost all relevant comparisons (corpus × evaluation metric), using centrality infor-\nmation yields performance that is superior to that of using q uery-similarity (generation)\ninformation. (Compare ClustCent with ClustQueryGen, DocC ent with DocQueryGen, and\nClustCent ∧ DocCent with ClustQueryGen ∧ DocQueryGen.) Speciﬁcally, we see that\ncluster-query similarity (ClustQueryGen), which was the main ranking criterion in previous\nwork on cluster ranking, yields performance that is much wor se than that of cluster cen-\ntrality (ClustCent) — a cluster ranking criterion which is n ovel to this study. In addition,\nwe note that integrating centrality and query-similarity ( generation) information can often\nyield performance that is better than that of using each alon e, as is the case for DocCent\n∧ DocQueryGen with respect to DocCent and DocQueryGen.\nWe next turn to examine the relative eﬀectiveness of using the cluster as a whole versus\nusing its constituent documents. When using only query-sim ilarity (generation) informa-\ntion, we see that using the documents in the cluster is much mo re eﬀective than using the\ncluster as a whole. (Compare DocQueryGen with ClustQueryGe n.) This ﬁnding further\nattests to the merits of using documents as proxies for ranki ng clusters — the underlying\nidea of our approach. When using centrality information, the picture is split across corpora:\nfor AP and WSJ using the documents in the cluster yields bette r performance than using\nthe whole cluster, while the reverse holds for TREC8 and WT10 G. (Compare DocCent\nwith ClustCent.) Integrating whole-cluster-based and doc ument-based information results\nin performance that is for all corpora (much) better than tha t of using the less eﬀective of\nthe two, and sometimes even better than the more eﬀective of t he two.\n380\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nd ∈ Dinit 49. 5 47 . 6 54. 0 49 . 8 52. 8 49 . 6 39. 6i 33. 2i\nd ∈ c 52.7i 50.6i 57.6 50 .6 56.0 51 .2 39.8i 33. 9i\nTable 6: Performance numbers of ClustRanker when either all documents in Dinit serve as\nproxies for cluster c (denoted d ∈ D init), or only c’s constituent documents serve\nas its proxies, as in the original implementation (denoted d ∈ c). Boldface marks\nthe best result in a column; ’i’ marks statistically signiﬁc ant diﬀerences with the\ninitial ranking.\nIt is not a surprise, then, that the ClustRanker method, whic h integrates centrality\ninformation and query-similarity (generation) informati on that are induced from both the\ncluster as a whole and from its constituent documents, is in m ost relevant comparisons the\nmost eﬀective cluster ranking method among those presented in Table 5.\nDocuments as Proxies for Clusters The ﬁndings presented above demonstrated the\nmerits of using documents as proxies for clusters. We now tur n to study the eﬀect on per-\nformance of the documents selected as proxies. The derivation of ClustRanker was based on\ntruncating the summation in Equation 2 (Section 3) so as to al low only c’s constituent doc-\numents to serve as its proxies. We examine a variant of ClustRanker wherein all documents\nin the initial list Dinit can serve as c’s proxies:\nS core(c)\ndef\n= Cent(c)pc(q) + (1− λ )\n∑\ndi ∈Dinit\npdi (q)pdi (c)Cent(di).\nAs can be seen in Table 6, this variant (represented by the row labeled “ d ∈ D init”)\nposts performance that is almost always better than that of t he initial document ranking\nfrom which Dinit is derived. However, the performance is also consistently w orse than that\nof the original implementation of ClustRanker (represente d by the row labeled “ d ∈ c”)\nthat lets only c’s constituent documents to serve as its proxies. Furthermore, this variant of\nClustRanker posts less statistically signiﬁcant improvem ents over the initial ranking than\nthe original implementation. (The performance diﬀerences between the two variants of\nClustRanker, however, are not statistically signiﬁcant.) Thus, as was mentioned in Section\n3, using only the cluster’s constituent documents as its pro xies is not only computationally\nconvenient, but also yields performance improvements.\n5.3.3 Comparison with P ast Approaches for Ranking Clusters\nIn Table 7 we compare the performance of ClustRanker with tha t of previously proposed\nmethods for ranking clusters. In what follows we ﬁrst discus s these methods, and then\nanalyze the performance patterns.\nMost previous approaches to ranking (various types of) clus ters compare a cluster rep-\nresentation with that of the query (Jardine & van Rijsbergen , 1971; Croft, 1980; Kurland\n& Lee, 2004; Liu & Croft, 2004, 2006b). Speciﬁcally, in the la nguage model framework,\n381\nKurland & Krikon\nquery-speciﬁc clusters were ranked by the probability assi gned by their induced language\nmodels to the query (Liu & Croft, 2004, 2006b; Kurland & Lee, 2 006). Note that this is\nexactly the ClustQueryGen method in our setup, which ranks c by pc(q).\nThere has been some work on using the maximal (Leuski, 2001; S hanahan et al., 2003;\nLiu & Croft, 2008) and minimal (Leuski, 2001; Liu & Croft, 200 8) query-similarity values\nof the documents in a cluster for ranking it. We showed in Sect ion 3.3 that these ap-\nproaches can be explained in terms of our framework; speciﬁc ally, the score of cluster c is\nmaxdi ∈c pdi (q) and mindi ∈c pdi (q), respectively. However, these methods were originally pro-\nposed for ranking hard clusters. As the clusters we rank here are overlapping, these ranking\ncriteria are somewhat less appropriate as they result in man y ties of cluster scores. Still,\nwe use these methods as baselines and break ties arbitrarily as they were also used in some\nrecent work on ranking nearest-neighbors-based clusters ( Liu & Croft, 2008). 6 Following\nsome observations with regard to the merits of representing clusters by using the geomet-\nric mean of their constituent documents’ representations ( Liu & Croft, 2008; Seo & Croft,\n2010), we also consider the geometric mean of the query-simi larity values of documents in\nc for ranking it; that is, |c|\n√ ∏\ndi∈c pdi (q).7\nAn additional reference comparison that we consider, which was shown to yield eﬀective\ncluster ranking performance, is a recently proposed (bipartite-)graph-based approach (Kur-\nland & Lee, 2006). Documents in Dinit are vertices on one side, and clusters in C l(Dinit)\nare vertices on the other side; an edge connects document d with the δ clusters ci that\nyield the highest language-model similarity pci (d), which also serves as a weight function\nfor the edges. Then, Kleinberg’s (1997) HITS (hubs and authorities) algorithm is run on\nthe graph, and clusters are ranked by their induced authority values. It was shown that\nthe cluster with the highest authority value tends to contai n a high percentage of relevant\ndocuments (Kurland & Lee, 2006). For implementation, we fol low the details provided by\nKurland and Lee (2006); speciﬁcally, we choose the value of δ from {2, 4, 9, 19, 29, 39, 49} so\nas to optimize p@k performance for clusters of size k.\nTable 7 presents the comparison of ClustRanker with the refe rence comparisons just\ndescribed. The p@k ( k ∈ {5, 10}) reported for a cluster ranking method is the percentage\nof relevant documents in the highest ranked cluster, wherei n clusters contain k documents\neach.\nWe can see in Table 7 that ClustRanker outperforms all refere nce comparisons in al-\nmost all cases. Many of the these performance diﬀerences are also statistically signiﬁcant.\nFurthermore, ClustRanker is the only cluster ranking metho d in Table 7 that consistently\noutperforms the initial ranking. Moreover, ClustRanker po sts more statistically signiﬁcant\nimprovements over the initial ranking than the other cluste r ranking methods do.\n6. As noted above, previous work on cluster-based retrievalhas demonstrated the merits of using overlapping\nnearest-neighbors-based clusters with respect to using hard clusters (Kurland, 2009). Indeed, recent work\non cluster ranking has focused on ranking nearest-neighbor-based clusters as we do here (Kurland & Lee,\n2006; Liu & Croft, 2006b, 2006a, 2008; Seo & Croft, 2010).\n7. We note that in the original proposals (Liu & Croft, 2008; S eo & Croft, 2010) the geometric mean of\nlanguage models was used at the term level rather than at the query-assigned score level as we use here.\nT o maintain consistency with the other cluster-ranking methods explored, we use the geometric mean\nat the query-assigned score level; and, we hasten to point ou t that a geometric-mean-based language\nmodel (Liu & Croft, 2008; Seo & Croft, 2010) could be used instead of the standard language model for\nclusters in ClustRanker and in the reference comparisons so as to potentially improve performance.\n382\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nSClustQueryGen (c)\ndef\n= pc (q) 39. 2i 38. 8i 39. 6i 40. 6i 44. 0i 37. 0i 30. 0 24 . 1\nSM ax (c)\ndef\n= max di ∈Dinit pdi (q) 41. 8 40 . 3 38. 8i 41. 6 51. 2 46 . 6 33. 9 29 . 2\nSM in (c)\ndef\n= min di ∈Dinit pdi (q) 47. 0 46 . 7 46. 4 48 . 4 48. 4 47 . 8 31. 4 25 . 9\nSGeoM ean (c)\ndef\n= |c|\nqQ\ndi ∈c pdi (q) 44. 4 46 . 7i 50. 0 49 . 6 56. 0 50. 6 37. 4 31 . 8i\nSH IT S (c) 49. 5 47 . 2 50. 8 46 . 6 53. 6 49 . 0 26. 7i 23. 9i\nSClustRanker (c) 52. 7icM\ng 50. 6icM\n57. 6cM\nmh 50. 6cM\nm\n56. 0c\nm 51. 2c\n39. 8icM\nmh 33. 9icM\nmh\nTable 7: Comparison of ClustRanker with previously propose d methods for ranking clus-\nters. (’ S’ stands for the cluster-scoring function.) Boldface marks the best result\nin a column; ’i’ marks statistically signiﬁcant diﬀerence between a method and the\ninitial ranking; ’c’, ’M’, ’m’, ’g’, and ’h’ mark statistica lly signiﬁcant diﬀerences of\nClustRanker with the ClustQueryGen, Max, Min, GeoMean and H ITS methods,\nrespectively.\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nHITS 49. 5 47 . 2 50. 8 46 . 6 53. 6 49 . 0 26. 7i 23. 9i\nClustCent 51. 7 48 . 6i 52. 4 49 . 4 54. 8 50 . 0 39. 8ih 33. 0ih\nDocCent 52. 9ih 48.8 52. 0 48 . 8 55. 6 50 . 6 31. 0h 28. 1h\nClustCent ∧ DocCent 53.5ih 48.8i 54.8 49 .8 56.0 51 .4 39. 8ih 33. 0ih\nTable 8: Comparison of our centrality-solely approaches for ranking clusters with the HITS-\nbased method (Kurland & Lee, 2006). Boldface marks the best p erformance in a\ncolumn; ’i’ and ’h’ mark statistically signiﬁcant diﬀerences with the initial ranking\nand the HITS method, respectively.\nThe Comparison with the HITS-Based Approach The HITS-based method (Kur-\nland & Lee, 2006) utilizes cluster centrality information as induced over a cluster-document\ngraph. Our ClustRanker method, on the other hand, integrate s centrality information —\ninduced over document-solely and cluster-solely graphs — w ith query-similarity (genera-\ntion) information. In Table 8 we contrast the resultant perf ormance of using the diﬀerent\nnotions of centrality utilized by the two methods. We presen t the performance of our\ncentrality-solely-based methods ClustCent, DocCent, and ClustCent ∧ DocCent and of the\nHITS approach (Kurland & Lee, 2006).\nWe can see in Table 8 that all our centrality-solely-based ap proaches outperform the\nHITS-based method in all relevant comparisons. These resul ts attest to the eﬀective uti-\nlization of (a speciﬁc type of) centrality information by ou r framework.\n383\nKurland & Krikon\n5.3.4 Comparison with Utilizing Inter-Document Similarit ies to Directly\nRank Documents\nRanking clusters based on the presumed percentage of relevant documents that they contain,\nas in ClustRanker, is one approach of utilizing inter-docum ent similarities so as to improve\ndocument-ranking eﬀectiveness. Alternatively, inter-document similarities can be exploited\nso as to directly rank documents. For example, a re-ranking principle that was demonstrated\nto be eﬀective is rewarding documents that are initially highly ranked, and which are highly\nsimilar to many other documents in the initial list (Bali´ ns ki & Dani/suppress lowicz, 2005; Diaz,\n2005; Kurland & Lee, 2005). Speciﬁcally, using the DocCent c omponent of ClustRanker,\nthat is, the PageRank score of document d (Cent(d)) as induced over a document-similarity\ngraph, and scaling this value by pd(q) — which is the initial (query similarity) score of\nd — was shown to be an eﬀective re-ranking criterion (Kurland & Lee, 2005). We use\nPR+QuerySim to denote this method.\nAnother approach that we consider is ranking documents usin g clusters as a form of an\n“extended” document representation. (In language model te rms this translates to cluster-\nbased smoothing.) Speciﬁcally, we use the Interpolation algorithm (Kurland & Lee, 2004),\nwhich was shown to be highly eﬀective for re-ranking (Kurlan d, 2009). A document d\nis scored by λp d(q) + (1 − λ ) ∑\nc∈C l(Dinit) pc(q)pd(c); λ is a free parameter. That is, a\ndocument is rewarded by its “direct match” with the query ( pd(q)), which is the criterion\nused for creating the initial ranking, and by the “query matc h” of clusters ( pc(q)) with\nwhich it is strongly associated (as measured by pd(c)). In other words, the Interpolation\nmodel backs oﬀ from a document representation to a cluster-b ased representation. The\nInterpolation model could conceptually be viewed as a gener alization of methods that use\na single cluster (Liu & Croft, 2004; Tao et al., 2006) or a topi c model (Wei & Croft, 2006)\nfor smoothing a document language model. Furthermore, note that while Interpolation\nuses clusters as document proxies for ranking documents, ou r ClustRanker method uses\ndocuments as cluster proxies for ranking clusters.\nIn Table 9 we compare the performance of ClustRanker with that of the PR+QuerySim\nand Interpolation methods just described. The performance of Interpolation is indepen-\ndently optimized for p@5 and p@10 using clusters of 5 and 10 do cuments, respectively, as\nis the case for ClustRanker; the value of λ is chosen from {0, 0. 1, . . . , 0. 9}. The perfor-\nmance of PR+QuerySim is also independently optimized for p@ 5 and p@10, when setting\nthe graph out degree parameter ( δ ) to 4 and 9 respectively, and selecting the value of ν\nfrom {2, 4, 9, 19, 29, 39, 49}. (Setting the graph out-degree parameter to a value x amounts\nto having a document “transfer” centrality support to x other documents; hence, using the\nvalues of 4 and 9 amounts to considering “local neighborhood s” of 5 and 10 documents in\nthe similarity space, respectively; this is conceptually r eminiscent of using clusters of size 5\nand 10 respectively. Refer to Appendix A for further details .)\nWe can see in Table 9 that ClustRanker outperform PR+QuerySi m and Interpolation\nin most relevant comparisons. (Several of the performance d iﬀerences with the former\nare statistically signiﬁcant, while those with the latter a re not.) Speciﬁcally, the relative\nperformance improvements for WT10G are quite substantial. (We hasten to point out,\nhowever, that Interpolation posts more statistically sign iﬁcant performance improvements\nover the initial ranking than ClustRanker does.)\n384\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43. 2 50. 0 45. 6 53. 6 48. 4 33. 9 28. 0\nPR+QuerySim 49. 5 49. 5i 56. 0 51. 0i 57. 2i 50. 4 35. 9 30. 4\nInterpolation 51. 3i 50. 3i 55. 6i 49. 6i 56. 8 52. 4 36. 1 31. 8i\nClustRanker 52. 7i\np\n50. 6i 57. 6 50. 6 56. 0 51. 2 39. 8i\np\n33. 9i\np\nTable 9: Comparison with re-ranking methods that utilize in ter-document similarities to\ndirectly rank documents. Boldface marks the best result in a column. Statistically\nsigniﬁcant diﬀerences with the initial ranking and PR+QuerySim are marked with\n’i’ and ’p’, respectively. There are no statistically signi ﬁcant diﬀerences between\nClustRanker and Interpolation.\nAll in all, we see that ranking clusters as is done by ClustRanker can result in (document)\nre-ranking performance that is at least as eﬀective (and oft en more eﬀective) than that of\nmethods that utilize inter-document similarities to direc tly rank documents.\n5.3.5 Performance-Sensitivity Analysis\nWe next turn to analyze the eﬀect of varying the values of the f ree parameters that Clus-\ntRanker incorporates on its performance. The ﬁrst paramete r, λ , controls the reliance on\nthe cluster as a whole unit versus its constituent documents . (Refer back to Equation 4 in\nSection 3 for details.) Figure 1 depicts the p@5 performance of ClustRanker, with clusters\nof 5 documents, as a function of λ ; the p@10 performance patterns of ClustRanker with\nclusters of 10 documents are similar, and are omitted to avoi d cluttering the presentation.\nWe can see in Figure 1 that for AP, TREC8 and WT10G, all values o f λ result in\nperformance that is (much) better than that of the initial ra nking; for WSJ, λ ≤ 0. 4 results\nin performance superior to that of the initial ranking. Furt hermore, for AP, TREC8, and\nWT10G, λ = 0. 4, which strikes a good balance between using the cluster as a whole and\nusing its constituent documents, yields (near) optimal performance; for WSJ, smaller values\nof λ (speciﬁcally, λ = 0), which result in more weight put on the cluster’s constit uent\ndocuments, are more eﬀective. Thus, we witness again the imp ortance of using the cluster’s\nconstituent documents as proxies when ranking the cluster.\nThe graph-based method used by ClustRanker for inducing doc ument (and cluster)\ncentrality depends on two free parameters: δ (the number of nearest neighbors considered\nfor each element in the graph), and ν (PageRank’s damping factor); see Appendix A for\ndetails. As noted above, both the document graph and the clus ter graph are constructed\nusing the same values of these two parameters. Figures 2 and 3 depict the eﬀect of the\nvalues of δ and ν , respectively, on the p@5 performance of ClustRanker with c lusters of 5\ndocuments.\nWe can see in Figure 2 that all values of δ result in performance that is (often much)\nbetter than that of the initial ranking. In general, small va lues of δ yield the best per-\nformance. This ﬁnding is in accordance with those reported i n previous work on using\n385\nKurland & Krikon\nAP TREC8\n 40\n 42\n 44\n 46\n 48\n 50\n 52\n 54\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\np@5\nλ\nEffect of λ  on p@5, corpus=AP\nClustRanker\ninit. rank.\n 48\n 50\n 52\n 54\n 56\n 58\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\np@5\nλ\nEffect of λ  on p@5, corpus=TREC8\nClustRanker\ninit. rank.\nWSJ WT10G\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\np@5\nλ\nEffect of λ  on p@5, corpus=WSJ\nClustRanker\ninit. rank.\n 30\n 32\n 34\n 36\n 38\n 40\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1\np@5\nλ\nEffect of λ  on p@5, corpus=WT10G\nClustRanker\ninit. rank.\nFigure 1: Eﬀect of varying λ on the p@5 performance of ClustRanker.\nnearest-neighbor-based graphs for ranking documents in an initially retrieved list using\ndocument-solely graphs (Diaz, 2005; Kurland & Lee, 2005).\nFigure 3 shows that for almost every value ofν , the resultant performance of ClustRanker\ntranscends that of the initial ranking; in many cases, the im provement is quite substantial.\n5.3.6 Learning Free-P arameter V alues\nHeretofore, we studied the performance of ClustRanker, and analyzed the eﬀectiveness of\nthe information types that it utilizes, while ameliorating free-parameter values eﬀects. That\nis, we reported performance for parameter values that result in optimized average p@k over\nthe entire set of queries per corpus. We have applied the same practice for all reference\ncomparisons that we considered, which resulted in comparin g the potential eﬀectiveness\nof our approach with that of previously suggested ones. In ad dition, we studied in the\nprevious section the eﬀect of the values of free parameters i ncorporated by ClustRanker on\nits (average) performance.\nWe now turn to study the question of whether eﬀective values o f the free parameters\nof ClustRanker generalize across queries; that is, whether these values can be learned. To\nperform this study, we employ a leave-one-out cross validat ion procedure. For each query,\nthe free parameters of ClustRanker are set to values that yie ld optimal average p@k over\n386\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8\n 42\n 44\n 46\n 48\n 50\n 52\n 54\n 2  4  9  19  29  39  49\np@5\nδ\nEffect of δ  on p@5, corpus=AP\nClustRanker\ninit. rank.\n 48\n 50\n 52\n 54\n 56\n 58\n 2  4  9  19  29  39  49\np@5\nδ\nEffect of δ  on p@5, corpus=TREC8\nClustRanker\ninit. rank.\nWSJ WT10G\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 2  4  9  19  29  39  49\np@5\nδ\nEffect of δ  on p@5, corpus=WSJ\nClustRanker\ninit. rank.\n 32\n 34\n 36\n 38\n 40\n 42\n 2  4  9  19  29  39  49\np@5\nδ\nEffect of δ  on p@5, corpus=WT10G\nClustRanker\ninit. rank.\nFigure 2: Eﬀect of varying δ , one of the graph parameters (refer to Appendix A), on the\np@5 performance of ClustRanker.\nall other queries for the same corpus. Then, we report the resultant av erage p@k over all\nqueries per corpus. Thus, the reported p@k numbers are based on learning performed with\np@k as the optimization metric.\nWe contrast the performance of ClustRanker with that of the r eference comparisons\nused above. Speciﬁcally, the document-based ranking basel ines: (i) the initial ranking, (ii)\nthe relevance model used to rank the entire corpus (Rel Model ), (iii) the relevance model\nused to re-rank the initial list (Rel Model(Re-Rank)); and, the cluster ranking methods: (i)\nS coreClustQueryGen (c)\ndef\n= pc(q), (ii)S coreM ax (c)\ndef\n= max di ∈Dinit pdi (q), (iii) S coreM in (c)\ndef\n=\nmindi∈Dinit pdi (q), (iv) S coreGeoM ean (c)\ndef\n= |c|\n√ ∏\ndi∈c pdi (q), and (v) S coreHI T S (c). For the\nreference comparisons that incorporate free parameters — Rel Model, Rel Model(Re-Rank),\nand HITS— we employ leave-one-out cross validation so as to s et free-parameter values as\nwe do for ClustRanker. The performance numbers of all method s are presented in Table\n10.\nOur ﬁrst observation based on Table 10 is that ClustRanker ou tperforms the initial\nranking in most relevant comparisons; most of these improve ments are quite substantial.\n387\nKurland & Krikon\nAP TREC8\n 42\n 44\n 46\n 48\n 50\n 52\n 54\n 0.05 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9 0.95\np@5\nν\nEffect of ν  on p@5, corpus=AP\nClustRanker\ninit. rank.\n 48\n 50\n 52\n 54\n 56\n 58\n 0.05 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9 0.95\np@5\nν\nEffect of ν  on p@5, corpus=TREC8\nClustRanker\ninit. rank.\nWSJ WT10G\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 0.05 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9 0.95\np@5\nν\nEffect of ν  on p@5, corpus=WSJ\nClustRanker\ninit. rank.\n 32\n 34\n 36\n 38\n 40\n 42\n 0.05 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9 0.95\np@5\nν\nEffect of ν  on p@5, corpus=WT10G\nClustRanker\ninit. rank.\nFigure 3: Eﬀect of varying ν , one of the graph parameters (refer to Appendix A), on the\np@5 performance of ClustRanker.\nThe only exception is for the WSJ corpus, for which the releva nce models also do not\noutperform the initial ranking in terms of p@5. Thus, it seem s that query-variability issues,\nwhich aﬀect the ability to learn eﬀective free-parameter va lues, have quite an eﬀect on the\nperformance of methods for WSJ.\nWe can also see in Table 10 that except for WSJ, ClustRanker ou tperforms the previ-\nously proposed cluster ranking methods in almost all releva nt comparisons. Many of these\nperformance improvements are substantial and statistical ly signiﬁcant.\nAs can be seen in Table 10, the ClustRanker method also outper forms each of the\nrelevance models (Rel Model and Rel Model(Re-Rank)) in a maj ority of the relevant com-\nparisons (corpus × evaluation measure). While there is a single case for which C lustRanker\nis outperformed in a statistically signiﬁcantly manner by t he relevance models (p@10 for\nWSJ), for WT10G ClustRanker posts statistically signiﬁcan t improvements over the rele-\nvance models, with some of the performance diﬀerences being quite striking. Furthermore,\nwe observe that in contrast to ClustRanker, most previouslyproposed cluster ranking meth-\nods often post performance that is much worse — in many cases to a statistically signiﬁcant\ndegree — than that of the relevance models. Somewhat an excep tion is ranking clusters by\n388\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAP TREC8 WSJ WT10G\np@5 p@10 p@5 p@10 p@5 p@10 p@5 p@10\ninit. rank. 45. 7 43 . 2 50. 0 45 . 6 53. 6 48 . 4 33. 9 28 . 0\nRel Model 49. 9 48. 4i 50. 8 50. 2 52. 0 52. 6 30. 4i 29. 2\nRel Model(Re-Rank) 51. 1i 45. 8 50. 4 49 . 8 52. 0 52. 6 36. 3 27 . 8\nSClustQueryGen (c)\ndef\n= pc (q) 39. 2irρ 38. 8irρ 39. 6irρ 40. 6irρ 44. 0irρ 37. 0irρ 30. 0ρ 24. 1r\nSM ax (c)\ndef\n= max di ∈Dinit pdi (q) 41. 8rρ 40. 3rρ 38. 8irρ 41. 6rρ 51. 2rρ 46. 6rρ 33. 9 29 . 2\nSM in (c)\ndef\n= min di ∈Dinit pdi (q) 47. 0 46 . 7 46. 4 48 . 4rρ 48. 4rρ 47. 8rρ 31. 4 25 . 9\nSGeoM ean (c)\ndef\n= |c|\nqQ\ndi ∈c pdi (q) 44. 4rρ 46. 7i 50. 0 49 . 6 56. 0rρ 50. 6rρ 37. 4r 31. 8iρ\nSH IT S (c) 48. 5 47 . 2 50. 8 43 . 2rρ 53. 6 46 . 8ρ 25. 5iρ 23. 9r\nClustRanker 52. 3i\ncM g 48. 3ic\nM\n56. 8cM mh 49. 4cM 53. 2c 46. 2ρc 38. 6r\ncM m 31. 2ρ\ncm\nTable 10: Performance results when using leave-one-out cro ss validation to set free-\nparameter values. Boldface marks the best performance per c olumn. Statisti-\ncally signiﬁcant diﬀerences of a method with the initial ran king are marked with\n’i’. Statistically signiﬁcant diﬀerences of ClustRanker w ith the cluster ranking\nmethods, ClustQueryGen, Max, Min, GeoMean, and HITS are mar ked with ’c’,\n’M’, ’m’, ’g’, and ’h’, respectively. Statistically signiﬁ cant diﬀerences of a cluster\nranking method with Rel Model and Rel Model(Re-Rank) are mar ked with ’r’\nand ’ρ ’, respectively.\nthe geometric mean of their constituent documents query-si milarity values (GeoMean): for\nWT10G and WSJ the performance is better than that of the relev ance models; however,\nfor TREC8 and AP the performance is somewhat inferior to that of the relevance models.\nAll in all, the ﬁndings presented above attest that ClustRan ker, when learning free\nparameter values, is (i) a highly eﬀective method for obtain ing high precision at top ranks,\nand (ii) much more eﬀective than previously proposed method s for ranking clusters.\n6. Conclusions and F uture Work\nWe presented a novel language model approach to ranking query-speciﬁc clusters, that is,\nclusters created from documents highly ranked by some initial search performed in response\nto a query. The ranking of clusters is based on the presumed pe rcentage of relevant docu-\nments that they contain.\nOur cluster ranking model integrates information induced f rom the cluster as a whole\nunit with that induced from documents that are associated wi th the cluster. Two types\nof information are exploited by our approach: similarity to the query and centrality. The\nlatter reﬂects similarity to other central items in the refe rence set, may they be documents\nin the initial list, or clusters of these documents.\nEmpirical evaluation showed that using our approach result s in precision-at-top-ranks\nperformance that is substantially better than that of the initial ranking upon which cluster-\ning is employed. Furthermore, the performance often transc ends that of a state-of-the-art\npseudo-feedback-based query expansion method, namely, the relevance model. In addition,\nwe showed that our approach is substantially more eﬀective i n identifying clusters contain-\n389\nKurland & Krikon\ning a high percentage of relevant documents than previously proposed methods for ranking\nclusters.\nFor future work we intend to explore additional characteris tics of document clusters\nthat might attest to the percentage of relevant documents they contain; for example, cluster\ndensity as measured by inter-document-similarities withi n the cluster. Incorporating such\ncharacteristics in our framework in a principled way is an in teresting challenge.\nAcknowledgments\nWe thank the reviewers for their helpful comments. We also th ank Lillian Lee for helpful\ncomments on the work presented in this paper, and for discussions that led to ideas presented\nhere; speciﬁcally, the cluster-centrality induction method is a fruit of joint work with Lillian\nLee. This paper is based upon work supported in part by the Isr ael Science Foundation\nunder grant no. 557/09, by the National Science Foundation u nder grant no. IIS-0329064,\nby Google’s faculty research award, and by IBM’s SUR award. A ny opinions, ﬁndings and\nconclusions or recommendations expressed in this material are the authors’ and do not\nnecessarily reﬂect those of the sponsoring institutions.\nAppendix A. Centrality Induction\nWe brieﬂy describe a previously proposed graph-based appro ach for inducing document\ncentrality (Kurland & Lee, 2005), which we use for inducing document and cluster centrality.\nLet S (either Dinit, the initial list of documents, or C l(Dinit), the set of their clusters)\nbe a set of items, and G = (S, S × S) be the complete directed graph deﬁned over S. The\nweight w t(s1 → s2) of the edge s1 → s2 (s1, s 2 ∈ S) is deﬁned as\nw t(s1 → s2)\ndef\n=\n{\nps2 (s1) if s2 ∈ N bhd(s1; δ ),\n0 otherwise ,\nwhere N bhd(s1; δ ) is the set of δ items s′ ∈ S − {s1} that yield the highest ps′(s1). (Ties\nare broken by item ID.)\nWe use the PageRank approach (Brin & Page, 1998) to smooth theedge-weight function:\nw t[ν ](s1 → s2) = (1 − ν ) · 1\n|S| + ν · w t(s1 → s2)∑\ns′∈S w t(s1 → s′);\nν is a free parameter.\nThus, G with the edge-weight function w t[ν ] constitutes an ergodic Markov chain, for\nwhich a stationary distribution exists. We set Cent(s), the centrality value of s, to the\nstationary probability of “visiting” s.\nFollowing previous work (Kurland & Lee, 2005), the values of δ and ν are chosen from\n{2, 4, 9, 19, 29, 39, 49} and {0. 05, 0. 1, . . . , 0. 9, 0. 95}, respectively, so as to optimize the p@k\nperformance of a given algorithm for clusters of size k. We use the same parameter setting\nfor the document-graph ( S = Dinit) and for the cluster-graph ( S = C l(Dinit)), and there-\nfore inducing document and cluster centrality in any of our m ethods is based on two free\nparameters.\n390\nA Language Model Approach to Ranking Query-Specific Document Clusters\nAppendix B. Relevance Model\nTo estimate the standard relevance model, RM1, which was sho wn to yield better perfor-\nmance than that of the RM2 relevance model (Lavrenko & Croft, 2003), we employ the\nimplementation detailed by Lavrenko and Croft (2003). Let w denote a term in the vo-\ncabulary, {qi} be the set of query terms, and pJ M [α ]\nd (·) denote a Jelinek-Mercer smoothed\ndocument language model with smoothing parameter α (Zhai & Laﬀerty, 2001). RM1 is\nthen deﬁned by\npRM 1(w; α )\ndef\n=\n∑\nd∈Dinit\npJ M [α ]\nd (w)\n∏\ni pJ M [α ]\nd (qi)\n∑\ndj ∈Dinit\n∏\ni pJ M [α ]\ndj (qi)\n.\nIn practice, RM1 is clipped by setting pRM 1(w; α ) to 0 for all but the β terms with\nthe highest pRM 1(w; α ) to begin with (Connell et al., 2004; Diaz & Metzler, 2006); f ur-\nther normalization is performed to yield a probability dist ribution, which we denote by\n˜pRM 1(·; α, β ). To improve performance, RM1 is anchored to the original qu ery via inter-\npolation using a free parameter γ (Abdul-Jaleel et al., 2004; Diaz & Metzler, 2006). This\nresults in the RM3 model:\npRM 3(w; α, β, γ )\ndef\n= γp M LE\nq (w) + (1− γ )˜pRM 1(w; α, β );\np M LE\nq (w) is the maximum likelihood estimate of term w with respect to q. Documents in\nthe corpus are then ranked by the minus cross entropy −CE\n(\npRM 3(·; α, β, γ )\n⏐\n⏐\n⏐\n⏐\n⏐\n⏐ pDir[µ ]\nd (·)\n)\n.\nThe free-parameter values are chosen from the following ran ges to independently op-\ntimize p@5 and p@10 performance: α ∈ { 0, 0. 1, 0. 3, . . . , 0. 9}, β ∈ { 25, 50, 75, 100, 500,\n1000, 5000, ALL }, where “ ALL” stands for using all terms in the corpus (i.e., no clipping) ,\nand γ ∈ {0, 0. 1, 0. 2, . . . , 0. 9}; µ is set to 2000, as in our cluster-based algorithms, followin g\nprevious recommendations (Zhai & Laﬀerty, 2001).\nReferences\nAbdul-Jaleel, N., Allan, J., Croft, W. B., Diaz, F., Larkey, L., Li, X., Smucker, M. D., &\nWade, C. (2004). UMASS at TREC 2004 — novelty and hard. In Proceedings of the\nThirteenth Text Retrieval Conference (TREC-13) , pp. 715–725.\nAzzopardi, L., Girolami, M., & van Rijsbergen, K. (2004). Topic based language models for\nad hoc information retrieval. In Proceedings of International Conference on Neural\nNetworks and IEEE International Conference on Fuzzy System s, pp. 3281–3286.\nBali´ nski, J., & Dani/suppress lowicz, C. (2005). Re-ranking methodbased on inter-document dis-\ntances. Information Processing and Management , 41 (4), 759–775.\nBrin, S., & Page, L. (1998). The anatomy of a large-scale hype rtextual web search engine.\nIn Proceedings of the 7th International World Wide Web Confere nce, pp. 107–117.\nBuckley, C. (2004). Why current IR engines fail. In Proceedings of SIGIR , pp. 584–585.\nPoster.\n391\nKurland & Krikon\nBuckley, C., Salton, G., Allan, J., & Singhal, A. (1994). Aut omatic query expansion using\nSMART: TREC3. In Proceedings of the Third Text Retrieval Conference (TREC-3 ),\npp. 69–80.\nCollins-Thompson, K., Callan, J., Terra, E., & Clarke, C. L. (2004). The eﬀect of document\nretrieval quality on factoid question answering performance. In Proceedings of SIGIR ,\npp. 574–575. Poster.\nConnell, M., Feng, A., Kumaran, G., Raghavan, H., Shah, C., & Allan, J. (2004). UMass\nat TDT 2004. TDT2004 System Description.\nCrestani, F., & Wu, S. (2006). Testing the cluster hypothesi s in distributed information\nretrieval. Information Processing and Management , 42 (5), 1137–1150.\nCroft, W. B. (1980). A model of cluster searching based on cla ssiﬁcation. Information\nSystems, 5, 189–195.\nCroft, W. B., & Laﬀerty, J. (Eds.). (2003). Language Modeling for Information Retrieval .\nNo. 13 in Information Retrieval Book Series. Kluwer.\nDiaz, F. (2005). Regularizing ad hoc retrieval scores. In Proceedings of the Fourteenth\nInternational Conference on Information and Knowledge Man agement (CIKM) , pp.\n672–679.\nDiaz, F., & Metzler, D. (2006). Improving the estimation of r elevance models using large\nexternal corpora. In Proceedings of SIGIR , pp. 154–161.\nErkan, G. (2006a). Language model based document clusterin g using random walks. In\nProceedings of HLT/NAACL , pp. 479–486.\nErkan, G. (2006b). Using biased random walks for focused sum marization. In Proceedings\nof Document Understanding Conference (DUC) .\nErkan, G., & Radev, D. R. (2004). LexPageRank: Prestige in mu lti-document text summa-\nrization. In Proceedings of EMNLP , pp. 365–371. Poster.\nGeraci, F., Pellegrini, M., Maggini, M., & Sebastiani, F. (2 006). Cluster generation and\ncluster labeling for Web snippets: A fast and accurate hiera rchical solution. In Pro-\nceedings of the 13th international conference on string pro cessing and information\nretrieval (SPIRE) , pp. 25–37.\nGolub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). The Johns\nHopkins University Press.\nGriﬃths, A., Luckhurst, H. C., & Willett, P. (1986). Using interdocument similarity informa-\ntion in document retrieval systems. Journal of the American Society for Information\nScience (JASIS) , 37 (1), 3–11. Reprinted in Karen Sparck Jones and Peter Willett ,\neds., Readings in Information Retrieval , Morgan Kaufmann, pp. 365–373, 1997.\nHarman, D., & Buckley, C. (2004). The NRRC reliable information access (RIA) workshop.\nIn Proceedings of SIGIR , pp. 528–529. Poster.\nHearst, M. A., & Pedersen, J. O. (1996). Reexamining the clus ter hypothesis: Scat-\nter/Gather on retrieval results. In Proceedings of SIGIR , pp. 76–84.\n392\nA Language Model Approach to Ranking Query-Specific Document Clusters\nJardine, N., & van Rijsbergen, C. J. (1971). The use of hierar chic clustering in information\nretrieval. Information Storage and Retrieval , 7 (5), 217–240.\nKleinberg, J. (1997). Authoritative sources in a hyperlink ed environment. Tech. rep. Re-\nsearch Report RJ 10076, IBM.\nKurland, O. (2006). Inter-document similarities, language models, and ad hoc r etrieval.\nPh.D. thesis, Cornell University.\nKurland, O. (2009). Re-ranking search results using langua ge models of query-speciﬁc\nclusters. Journal of Information Retrieval , 12 (4), 437–460.\nKurland, O., & Domshlak, C. (2008). A rank-aggregation approach to searching for optimal\nquery-speciﬁc clusters. In Proceedings of SIGIR , pp. 547–554.\nKurland, O., & Lee, L. (2004). Corpus structure, language mo dels, and ad hoc information\nretrieval. In Proceedings of SIGIR , pp. 194–201.\nKurland, O., & Lee, L. (2005). PageRank without hyperlinks: Structural re-ranking using\nlinks induced by language models. In Proceedings of SIGIR , pp. 306–313.\nKurland, O., & Lee, L. (2006). Respect my authority! HITS wit hout hyperlinks utilizing\ncluster-based language models. In Proceedings of SIGIR , pp. 83–90.\nLaﬀerty, J. D., & Zhai, C. (2001). Document language models, query models, and risk\nminimization for information retrieval. In Proceedings of SIGIR , pp. 111–119.\nLavrenko, V. (2004). A Generative Theory of Relevance . Ph.D. thesis, University of Mas-\nsachusetts Amherst.\nLavrenko, V., Allan, J., DeGuzman, E., LaFlamme, D., Pollar d, V., & Thomas, S. (2002).\nRelevance models for topic detection and tracking. In Proceedings of the Human\nLanguage Technology Conference (HLT) , pp. 104–110.\nLavrenko, V., & Croft, W. B. (2001). Relevance-based langua ge models. In Proceedings of\nSIGIR, pp. 120–127.\nLavrenko, V., & Croft, W. B. (2003). Relevance models in info rmation retrieval. In Croft,\n& Laﬀerty (Croft & Laﬀerty, 2003), pp. 11–56.\nLeuski, A. (2001). Evaluating document clustering for inte ractive information retrieval.\nIn Proceedings of the Tenth International Conference on Infor mation and Knowledge\nManagement (CIKM) , pp. 33–40.\nLeuski, A., & Allan, J. (1998). Evaluating a visual navigation system for a digital library. In\nProceedings of the Second European conference on research a nd advanced technology\nfor digital libraries (ECDL) , pp. 535–554.\nLiu, X., & Croft, W. B. (2004). Cluster-based retrieval using language models. InProceedings\nof SIGIR , pp. 186–193.\nLiu, X., & Croft, W. B. (2006a). Experiments on retrieval of optimal clusters. Tech. rep. IR-\n478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts.\nLiu, X., & Croft, W. B. (2006b). Representing clusters for re trieval. In Proceedings of\nSIGIR, pp. 671–672. Poster.\n393\nKurland & Krikon\nLiu, X., & Croft, W. B. (2008). Evaluating text representati ons for retrieval of the best\ngroup of documents. In Proceedings of ECIR , pp. 454–462.\nMei, Q., Shen, X., & Zhai, C. (2007). Automatic labeling of mu ltinomial topic models. In\nProceedings of the 13th ACM SIGKDD international conferenc e, pp. 490–499.\nMihalcea, R. (2004). Graph-based ranking algorithms for se ntence extraction, applied to\ntext summarization. In The Companion Volume to the Proceedings of the 42nd Annual\nMeeting of the Association for Computational Linguistics , pp. 170–173.\nMihalcea, R., & Tarau, P. (2004). TextRank: Bringing order i nto texts. In Proceedings of\nEMNLP, pp. 404–411. Poster.\nOtterbacher, J., Erkan, G., & Radev, D. R. (2005). Using random walks for question-focused\nsentence retrieval. In Proceedings of Human Language Technology Conference and\nConference on Empirical Methods in Natural Language Proces sing (HLT/EMNLP) ,\npp. 915–922.\nPalmer, C. R., Pesenty, J., Veldes-Perez, R., Christel, M., Hauptmann, A. G., Ng, D., &\nWactlar, H. D. (2001). Demonstration of hierarchical docum ent clustering of digital\nlibrary retrieval results. In Proceedings of the 1st ACM/IEEE-CS joint conference on\ndigital libraries , p. 451.\nPonte, J. M., & Croft, W. B. (1998). A language modeling approach to information retrieval.\nIn Proceedings of SIGIR , pp. 275–281.\nPreece, S. E. (1973). Clustering as an output option. In Proceedings of the American Society\nfor Information Science , pp. 189–190.\nSeo, J., & Croft, W. B. (2010). Geometric representations fo r multiple documents. In\nProceedings of SIGIR , pp. 251–258.\nShanahan, J. G., Bennett, J., Evans, D. A., Hull, D. A., & Mont gomery, J. (2003). Clair-\nvoyance Corporation experiments in the TREC 2003. High accu racy retrieval from\ndocuments (HARD) track. In Proceedings of the Twelfth Text Retrieval Conference\n(TREC-12), pp. 152–160.\nSi, L., Jin, R., Callan, J., & Ogilvie, P. (2002). A language modeling framework for resource\nselection and results merging. In Proceedings of the 11th International Conference on\nInformation and Knowledge Management (CIKM) , pp. 391–397.\nTao, T., Wang, X., Mei, Q., & Zhai, C. (2006). Language model i nformation retrieval with\ndocument expansion. In Proceedings of HLT/NAACL , pp. 407–414.\nTao, T., & Zhai, C. (2006). Regularized esitmation of mixtur e models for robust pseudo-\nrelevance feedback. In Proceedings of SIGIR , pp. 162–169.\nTombros, A., Villa, R., & van Rijsbergen, C. (2002). The eﬀec tiveness of query-speciﬁc hi-\nerarchic clustering in information retrieval. Information Processing and Management ,\n38 (4), 559–582.\nTreeratpituk, P., & Callan, J. (2006). Automatically label ing hierarchical clusters. In\nProceedings of the sixth national conference on digital gov ernment research , pp. 167–\n176.\n394\nA Language Model Approach to Ranking Query-Specific Document Clusters\nvan Rijsbergen, C. J. (1979). Information Retrieval (second edition). Butterworths.\nVoorhees, E. M. (1985). The cluster hypothesis revisited. I n Proceedings of SIGIR , pp.\n188–196.\nVoorhees, E. M. (2002). Overview of the TREC 2002 question an swering track. In The\nEleventh Text Retrieval Conference TREC-11 , pp. 115–123.\nWei, X., & Croft, W. B. (2006). LDA-based document models for ad-hoc retrieval. In\nProceedings of SIGIR , pp. 178–185.\nWillett, P. (1985). Query speciﬁc automatic document class iﬁcation. International Forum\non Information and Documentation , 10 (2), 28–32.\nXu, J., & Croft, W. B. (1996). Query expansion using local and global document analysis.\nIn Proceedings of SIGIR , pp. 4–11.\nYang, L., Ji, D., Zhou, G., Nie, Y., & Xiao, G. (2006). Documen t re-ranking using cluster\nvalidation and label propagation. In Proceedings of CIKM , pp. 690–697.\nZamir, O., & Etzioni, O. (1998). Web document clustering: a f easibility demonstration. In\nProceedings of SIGIR , pp. 46–54.\nZhai, C., & Laﬀerty, J. D. (2001). A study of smoothing method s for language models\napplied to ad hoc information retrieval. In Proceedings of SIGIR , pp. 334–342.\n395",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}