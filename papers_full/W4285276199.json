{
    "title": "Team ÚFAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models",
    "url": "https://openalex.org/W4285276199",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2548065221",
            "name": "Sunit Bhattacharya",
            "affiliations": [
                "Charles University"
            ]
        },
        {
            "id": "https://openalex.org/A2316948279",
            "name": "Rishu Kumar",
            "affiliations": [
                "Charles University"
            ]
        },
        {
            "id": "https://openalex.org/A2057104564",
            "name": "Ondrej Bojar",
            "affiliations": [
                "Charles University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2141653444",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3169953327",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W3171355829",
        "https://openalex.org/W3154476213",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2168217710",
        "https://openalex.org/W2917049430",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3168575305",
        "https://openalex.org/W3082760180",
        "https://openalex.org/W4241626650",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2931720176",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W4285224078",
        "https://openalex.org/W2980708516"
    ],
    "abstract": "Eye-Tracking data is a very useful source of information to study cognition and especially language comprehension in humans. In this paper, we describe our systems for the CMCL 2022 shared task on predicting eye-tracking information. We describe our experiments withpretrained models like BERT and XLM and the different ways in which we used those representations to predict four eye-tracking features. Along with analysing the effect of using two different kinds of pretrained multilingual language models and different ways of pooling the token-level representations, we also explore how contextual information affects the performance of the systems. Finally, we also explore if factors like augmenting linguistic information affect the predictions. Our submissions achieved an average MAE of 5.72 and ranked 5th in the shared task. The average MAE showed further reduction to 5.25 in post task evaluation.",
    "full_text": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 130 - 135\nMay 26, 2022 ©2022 Association for Computational Linguistics\nTeam ÚFAL at CMCL 2022 Shared Task: Figuring out the correct recipe\nfor predicting Eye-Tracking features using Pretrained Language Models\nSunit Bhattacharya, Rishu Kumar and Ondˇrej Bojar\nCharles University\nFaculty Of Mathematics and Physics\nInsititute of Formal and Applied Linguistics\nbhattacharya,kumar,bojar@ufal.mff.cuni.cz\nAbstract\nEye-Tracking data is a very useful source of\ninformation to study cognition and especially\nlanguage comprehension in humans. In this\npaper, we describe our systems for the CMCL\n2022 shared task on predicting eye-tracking in-\nformation. We describe our experiments with\npretrained models like BERT and XLM and the\ndifferent ways in which we used those repre-\nsentations to predict four eye-tracking features.\nAlong with analysing the effect of using two dif-\nferent kinds of pretrained multilingual language\nmodels and different ways of pooling the token-\nlevel representations, we also explore how con-\ntextual information affects the performance of\nthe systems. Finally, we also explore if factors\nlike augmenting linguistic information affect\nthe predictions. Our submissions achieved an\naverage MAE of 5.72 and ranked 5th in the\nshared task. The average MAE showed further\nreduction to 5.25 in post task evaluation.\n1 Introduction and Motivation\nIn the last decade that has seen rapid developments\nin AI research, the emergence of the Transformer\narchitecture (Vaswani et al., 2017) marked a piv-\notal point in Natural Language Processing (NLP).\nFine-tuning pretrained language models to work on\nvarious downstream tasks has become a dominant\nmethod of obtaining state-of-the-art performance\nin different areas. Their capability to capture lin-\nguistic knowledge and learn powerful contextual\nword embeddings (Liu et al., 2019) have made the\ntransformer based models the work-horses in many\nNLP tasks. Pretrained models like the multilin-\ngual BERT (Devlin et al., 2019) and XLM (Con-\nneau et al., 2020) have also shown state-of-the-art\nperformance on cross-lingual understanding tasks\n(Wu and Dredze, 2019; Artetxe et al., 2019). In\nsome cases like machine translation, there are even\nclaims that deep learning systems reach transla-\ntion qualities that are comparable to professional\ntranslators (Popel et al., 2020).\nLanguage processing and its links with cognition\nis a very old research problem which has revealed\nhow cognitive data (eg. gaze, fMRI) can be used\nto investigate human cognition. Attempts at using\ncomputational methods for such studies (Mitchell\net al., 2008; Dehghani et al., 2017) have also shown\nencouraging results. However recently, there have\nbeen a number of works that have tried to incorpo-\nrate human cognitive data collected during reading\nfor improving the performance of NLP systems\n(Hollenstein et al., 2019). The CMCL 2022 Shared\nTask of multilingual and cross-lingual prediction of\nhuman reading behavior (Hollenstein et al., 2022)\nexplores how eye-gaze attributes can be algorithmi-\ncally predicted given reading data in multilingual\nsettings.\nInformed by the previous attempts at using pre-\ntrained multilingual language models to predict\nhuman reading behavior (Hollenstein et al., 2021)\nwe experiment with multilingual BERT and XLM\nbased models to test which fares better in this task.\nFor the experiments with the pretrained models, we\nuse the trained weights from Huggingface (Wolf\net al., 2020) and perform the rest of our experiments\nusing PyTorch1. Inspired by the psycholinguistic\nresearch on investigating context length during pro-\ncessing (Wochna and Juhasz, 2013), we experiment\nhow different contexts affect model performance.\nFinally, we merged the principles of the \"classi-\ncal\" approach of feature-based prediction with the\npretrained-language model based prediction for fur-\nther analysis. In the following sections, we present\nour results from a total of 48 different models.\n2 Task Description\nThe CMCL 2022 Shared Task of Multilingual and\nCross-lingual prediction of human reading behav-\nior frames the task of predicting eye-gaze attributes\nassociated with reading sentences as a regression\n1https://pytorch.org/\n130\ntask. The data for the task was comprised of eye\nmovements corresponding to reading sentences in\nsix languages (Chinese, Dutch, English, German,\nHindi, Russian). The training data for the task con-\ntained 1703 sentences while the development set\nand test set contained 104 and 324 sentences re-\nspectively. The data was presented in a way such\nthat for each word in a sentence there were four\nassociated eye-tracking features in the form of the\nmean and standard deviation scores of the Total\nReading Time (TRT) and First Fixation Duration\n(FFD). The features in the data were scaled in the\nrange between 0 and 100 to facilitate evaluation via\nthe mean absolute average (MAE).\n3 Experiments\nA total of 48 models of different configurations\nwere trained with the data provided for the shared\ntask. The different configurations used to construct\nthe models are based on intuition and literature\nsurvey.\nThee models were primarily categorized as\nSystem-1 (sys1) and System-2 (sys2) models. For\nsome word corresponding to a sentence in the\ndataset, System-1 models provided no additional\ncontext information. System-2 models on the other\nhand, contained the information of all the words in\nthe sentence that preceded the current word, provid-\ning additional context. This setting was inspired by\nworks (Khandelwal et al., 2018; Clark et al., 2019)\non how context is used by language models.\nAll systems under the System-1/2 labels were\nfurther trained as a BERT (bert) based system or a\nXLM (xlm) based system. BERT embeddings were\npreviously used by Choudhary et al. (2021) for\nthe eye-tracking feature prediction task in CMCL\n2021.\nCorresponding to each such language models\n(bert and xlm), the impact of different fine-tuning\nstrategies(Sun et al., 2019) on system performance\nwas studied. Hence, for one setting, only the con-\ntextualized word representation (CWR) was uti-\nlized by freezing the model weights and putting\na learnable regression layer on top of the model\noutput layer (classifier). Alternatively, the mod-\nels were fine-tuned with the regression layer on\ntop of them (whole). This setting is similar to the\none used by Li and Rudzicz (2021). However in\nour case, we experiment with a BERT and XLM\npretrained model.\nAdditionally, we also performed experiments\nwith pooling strategies for the layer representations\nby either using the final hidden representation of\nthe first sub-word encoding of the input (first) or\naggregating the representations of all sub-words\nusing mean-pooling (mean) or sum-pooling (sum).\nThe rationale behind using different pooling strate-\ngies was to have a sentence-level representation of\nthe input tokens. The impact of different pooling\nstrategies has previously been studied (Shao et al.,\n2019; Lee et al., 2019) for different problems. In\nthis paper, we analyze the effect of pooling feature-\nspace embeddings in the context of eye-tracking\nfeature prediction.\nFinally, for the experiments where we aug-\nmented additional lexical features (augmented) to\nthe neural features for regression, we used word\nlength and word-frequency as the additional infor-\nmation following Vickers et al. (2021).\nConstructing the experiments in this manner pro-\nvided us with models with a diverse set of proper-\nties and in turn provided insights into how well the\nmodel behaves when all other things stay the same,\nand only one aspect of learning is changed.\n4 Results\nThe results corresponding to the top 10 systems\nbased on the experiments described above are\nshown in Table 1.\nModel MAE\nbert_sys2_augmented_sum_classifier 5.251\nbert_sys2_unaugmented_first_classifier 5.267\nbert_sys2_augmented_mean_classifier 5.272\nbert_sys1_augmented_mean_classifier 5.279\nbert_sys2_augmented_first_classifier 5.295\nxlm_sys1_augmented_first_classifier 5.341\nxlm_sys2_augmented_first_whole 5.346\nbert_sys1_augmented_sum_classifier 5.353\nbert_sys2_augmented_sum_whole 5.367\nxlm_sys2_augmented_first_classifier 5.373\nTable 1: Top 10 best performing systems\nIt was observed that the maximum MAE scores\n(and the maximum variance of scores) for all the\nmodels was obtained for the attribute \"TRT_Avg\".\nThe attribute wise variances corresponding to the\ntest-data for all the models are shown in Table 2.\nSimilarly, the mean values of the attributes for all\nmodels are shown in Table 3.\nAn analysis of the models based on the different\nexperimental configurations are described in the\n131\nFFD_Avg FFD_Std TRT_Avg TRT_Std\n0.194 0.403 0.637 0.489\nTable 2: Attribute wise variance of scores for all models\nFFD_Avg FFD_Std TRT_Avg TRT_Std\n5.691 2.646 8.633 5.806\nTable 3: Attribute wise mean of scores for all models\nfollowing sections.\n4.1 System-1 vs System-2\nTable 4 shows the average model performance\nacross System-1 and System-2 configurations for\nboth BERT and XLM based models (based on the\naverage MAE values of the configurations). We see\nthat for the BERT based models, the average MAE\nfor System-1 is lower than that of System-2. But\nfor XLM-based models, the difference is almost\nnon-existent.\nModel Average MAE across models\nSys1_BERT 5.66\nSys1_XLM 5.70\nSys2_BERT 5.72\nSys2_XLM 5.69\nTable 4: System-1 vs System-2 performance across\nmodels\nHowever, it should be noted that 12 out of the\nfirst 20 best performing models were System-2\nmodels. Hence we posit that although the avail-\nability of the full sentence context is a factor for\nhaving more efficient systems, independently the\nfactor does not seem to boost the overall perfor-\nmance much.\n4.2 BERT vs XLM\nTable 5 shows that there is only a tiny difference\nin average MAE for all four attributes (FFD_ µ,\nFFD_σ, TRT_µ, TRT_σ) for all BERT vs XLM\nmodels . However, a brief look at Table 6 and\nTable 7 reveal that it was the XLM models that were\nresponsible for slightly decreased MAE scores for\n3 of the 4 attributes that were being predicted.\nWe also see that the amount of variance for XLM\nbased models was also smaller for 3 of the 4 at-\ntributes.\nModel Average MAE across models\nBERT 5.6920\nXLM 5.6960\nTable 5: BERT vs XLM performance across models\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nBERT 0.141 0.776 0.952 0.792\nXLM 0.236 0.045 0.349 0.204\nTable 6: Attribute wise variance of scores for all BERT\nand XLM based models\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nBERT 5.592 2.679 8.645 5.852\nXLM 5.789 2.612 8.622 5.760\nTable 7: Attribute wise mean of scores for all BERT and\nXLM based models\n4.3 Augmented vs Un-Augmented models\nFig. 1 shows that augmented models. i.e. models\nthat were fed information like word-frequency and\nword-length along with the neural representation\ninformation before being fed to the regression layer\nperformed better than models that used only con-\ntextual word embeddings resulting from pretrained\nlanguage models. Table 8 and Table 9 show the 5\nbest performing models of this category sorted by\ntheir MAE.\nModel MAE\nbert_sys2_unaugmented_first_classifier 5.267\nbert_sys2_unaugmented_mean_classifier 5.405\nxlm_sys1_unaugmented_mean_classifier 5.5\nxlm_sys2_unaugmented_mean_classifier 5.55\nxlm_sys1_unaugmented_mean_classifier 5.557\nTable 8: Performance of 5 best Un-Augmented models.\nModel MAE\nbert_sys2_augmented_sum_classifier 5.251\nbert_sys2_augmented_mean_classifier 5.272\nbert_sys1_augmented_mean_classifier 5.279\nbert_sys2_augmented_first_classifier 5.295\nxlm_sys1_augmented_first_classifier 5.341\nTable 9: Performance of 5 best Augmented models\nThe mean and variance of attributes across mod-\nels of these families presented in Table 10 & 11\nshow that augmented models show way less vari-\n132\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nAug 5.502 2.511 8.181 5.436\nUaug 5.88 2.78 9.086 6.176\nTable 10: Attribute wise mean of scores for all Aug-\nmented and Un-augmented models\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nAug 0.017 0.004 0.015 0.007\nUaug 0.292 0.749 0.823 0.678\nTable 11: Attribute wise variance of scores for all Aug-\nmented and Un-augmented models\nance in their predictions in comparison with neural-\nrepresentation only model families.\nFigure 1: Augmented vs Un-augmented model perfor-\nmance. The x-axis represents the 24 different models of\neach category. The y-axis shows the MAE correspond-\ning to each model.\n4.4 Nature of representation of input tokens\n(Pooling strategies)\nFig. 2 shows that using the first sub-word token or\nthe mean-pooled representation of the entire input\ngives lesser MAE scores than the sum-pooled rep-\nresentations. It was also observed that for System-2\nfamily of models, the mean-pooled representations\nwere associated with lesser MAE scores in com-\nparison to the first sub-word representation. The\nattribute wise mean in Table 15 and attribute wise\nvariance of model MAEs shown in Table 16 illus-\ntrates this point. Table 12,Table 13 and Table 14\nshow the 5 best performing models of this category\nsorted by their MAE.\n4.5 Fine-tuning\nFine-tuning on large pretrained language models\nhas become the standard way to conduct NLP re-\nModel MAE\nbert_sys2_unaugmented_first_classifier 5.267\nbert_sys2_augmented_first_classifier 5.295\nxlm_sys1_augmented_first_classifier 5.341\nxlm_sys2_augmented_first_whole 5.346\nxlm_sys2_augmented_first_classifier 5.373\nTable 12: Performance of 5 best first models\nModel MAE\nbert_sys2_augmented_mean_classifier 5.272\nbert_sys1_augmented_mean_classifier 5.279\nbert_sys2_augmented_mean_whole 5.375\nbert_sys2_unaugmented_mean_classifier 5.405\nxlm_sys1_augmented_mean_whole 5.413\nTable 13: Performance of 5 best Mean models\nModel MAE\nbert_sys2_augmented_sum_classifier 5.251\nbert_sys1_augmented_sum_classifier 5.353\nbert_sys2_augmented_sum_whole 5.367\nbert_sys1_augmented_sum_whole 5.402\nxlm_sys2_augmented_sum_classifier 5.456\nTable 14: Performance of 5 best Sum models\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nfirst 5.549 2.505 8.434 5.615\nMean 5.57 2.538 8.416 5.636\nSum 5.954 2.894 9.05 6.167\nTable 15: Attribute wise mean of scores for models with\ndifferent input token representations\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nfirst 0.036 0.004 0.118 0.054\nMean 0.047 0.005 0.118 0.048\nSum 0.383 1.082 1.374 1.139\nTable 16: Attribute wise variance of scores for models\nwith different input token representations\nsearch after the widespread adoption of the trans-\nformer architecture. And unsurprisingly, our exper-\niments reveal (Fig. 3) that fine-tuning of models\ngive smaller MAE scores than training only the\nregression layers. The stark difference in the vari-\nance for the predicted attributes between fine-tuned\nmodels and regression only models (as illustrated\nin Table 17-18) further demonstrates the advantage\nof fine-tuning.\n133\nFigure 2: Model performance based on the nature of\nrepresentation of input tokens.The x-axis represents the\n16 different models of each category. The y-axis shows\nthe MAE corresponding to each model.\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nAug 5.502 2.511 8.181 5.436\nUaug 5.88 2.78 9.086 6.176\nTable 17: Attribute wise variance of scores for fine-\ntuned models vs regression-layer only models\nModel FFD_µ FFD_σ TRT_µ TRT_σ\nAug 0.017 0.004 0.015 0.007\nUaug 0.292 0.749 0.823 0.678\nTable 18: Attribute wise mean of scores for fine-tuned\nmodels vs regression-layer only models\nFigure 3: Fine-tuning vs training only regression layer\nin the models. The x-axis represents the 24 different\nmodels of each category. The y-axis shows the MAE\ncorresponding to each model.\n5 Conclusion\nIn this paper, we have described our experiments\nwith different kinds of models that were trained\non the data provided for this shared-task. We have\nidentified five ways in which we can make better\nsystems to predict eye-tracking features based on\neye-tracking data from a multilingual corpus. First,\nthe experiments demonstrate that the inclusion of\ncontext (previous words occurring in the sentence)\nhelps the models to predict eye-tracking attributes\nbetter. This reaffirms previous observations made\nwith language models that more context is always\nhelpful. Second, we find that XLM based mod-\nels perform relatively better than the BERT based\nmodels. Third, our experiments show the advan-\ntages of augmenting additional linguistic features\n(word length and word frequency information in\nthis case) to the contextual word representations to\nmake better systems. This is in agreement with the\nfindings from eye-tracking prediction tasks from\nlast iterations of CMCL. Fourth, we see how dif-\nferent pooling methods applied on the input token\nrepresentations affect the final performance of the\nsystems. Finally, the experiments re-validate the ap-\nproach of fine-tuning pretrained language models\nfor specific tasks. Hence we conclude that contextu-\nalized word representations from language models\npretrained with many different languages, if care-\nfully augmented, engineered, and fine-tuned, can\npredict eye-tracking features quite successfully.\n6 Acknowledgement\nThis work has been funded from the grant 19-\n26934X (NEUREM3) of the Czech Science Foun-\ndation.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations. arXiv preprint\narXiv:1910.11856.\nShivani Choudhary, Kushagri Tandon, Raksha Agarwal,\nand Niladri Chatterjee. 2021. Mtl782_iitd at cmcl\n2021 shared task: Prediction of eye-tracking features\nusing bert embeddings and linguistic features. In\nProceedings of the Workshop on Cognitive Modeling\nand Computational Linguistics, pages 114–119.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. arXiv preprint\narXiv:1906.04341.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\n134\nciation for Computational Linguistics, pages 8440–\n8451.\nMorteza Dehghani, Reihane Boghrati, Kingson Man,\nJoe Hoover, Sarah I Gimbel, Ashish Vaswani, Ja-\nson D Zevin, Mary Helen Immordino-Yang, An-\ndrew S Gordon, Antonio Damasio, et al. 2017.\nDecoding the neural representation of story mean-\nings across languages. Human brain mapping,\n38(12):6096–6106.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nNora Hollenstein, Maria Barrett, Marius Troendle,\nFrancesco Bigiolli, Nicolas Langer, and Ce Zhang.\n2019. Advancing nlp with cognitive language pro-\ncessing signals. arXiv e-prints, pages arXiv–1904.\nNora Hollenstein, Emmanuele Chersoni, Cassandra Ja-\ncobs, Yohei Oseki, Laurent Prévott, and Enrico San-\ntus. 2022. Cmcl 2022 shared task on multilingual and\ncrosslingual prediction of human reading behavior.\nNora Hollenstein, Federico Pirovano, Ce Zhang, Lena\nJäger, and Lisa Beinborn. 2021. Multilingual lan-\nguage models predict human reading behavior. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 106–123, Online. Association for Computa-\ntional Linguistics.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away: How\nneural language models use context. arXiv preprint\narXiv:1805.04623.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. 2019.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Interna-\ntional Conference on Machine Learning, pages 3744–\n3753. PMLR.\nBai Li and Frank Rudzicz. 2021. Torontocl at cmcl\n2021 shared task: Roberta with multi-stage fine-\ntuning for eye-tracking prediction. arXiv preprint\narXiv:2104.07244.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual repre-\nsentations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1073–1094.\nTom M Mitchell, Svetlana V Shinkareva, Andrew Carl-\nson, Kai-Min Chang, Vicente L Malave, Robert A\nMason, and Marcel Adam Just. 2008. Predicting hu-\nman brain activity associated with the meanings of\nnouns. science, 320(5880):1191–1195.\nMartin Popel, Marketa Tomkova, Jakub Tomek, Łukasz\nKaiser, Jakob Uszkoreit, Ondˇrej Bojar, and Zdenˇek\nŽabokrtsk`y. 2020. Transforming machine transla-\ntion: a deep learning system reaches news translation\nquality comparable to human professionals. Nature\ncommunications, 11(1):1–15.\nTaihua Shao, Yupu Guo, Honghui Chen, and Zepeng\nHao. 2019. Transformer-based neural network for an-\nswer selection in question answering. IEEE Access,\n7:26146–26156.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification? In\nChina national conference on Chinese computational\nlinguistics, pages 194–206. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPeter Vickers, Rosa Wainwright, Harish Tayyar Mad-\nabushi, and Aline Villavicencio. 2021. CogNLP-\nSheffield at CMCL 2021 shared task: Blending cog-\nnitively inspired features with transformer-based lan-\nguage models for predicting eye tracking patterns. In\nProceedings of the Workshop on Cognitive Model-\ning and Computational Linguistics, pages 125–133,\nOnline. Association for Computational Linguistics.\nKacey L Wochna and Barbara J Juhasz. 2013. Con-\ntext length and reading novel words: An eye-\nmovement investigation. British Journal of Psychol-\nogy, 104(3):347–363.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 833–844, Hong\nKong, China. Association for Computational Linguis-\ntics.\n135"
}