{
    "title": "Benchmarking Large Language Models on Controllable Generation under Diversified Instructions",
    "url": "https://openalex.org/W4393160280",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2213365742",
            "name": "Yihan Chen",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A3035318654",
            "name": "Ben-Feng Xu",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2029296201",
            "name": "Quan Wang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2024876568",
            "name": "Yi Liu",
            "affiliations": [
                "China Machine Press"
            ]
        },
        {
            "id": "https://openalex.org/A2154925013",
            "name": "Zhendong Mao",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2029296201",
            "name": "Quan Wang",
            "affiliations": [
                "Beijing University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A2024876568",
            "name": "Yi Liu",
            "affiliations": [
                "China Machine Press"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6843733692",
        "https://openalex.org/W4285230133",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W6641983419",
        "https://openalex.org/W3088599783",
        "https://openalex.org/W4303649062",
        "https://openalex.org/W3083410900",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W4226375873",
        "https://openalex.org/W3085190015",
        "https://openalex.org/W3023710830",
        "https://openalex.org/W2108325777",
        "https://openalex.org/W3160250689",
        "https://openalex.org/W4321472132",
        "https://openalex.org/W4281765689",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W4221143575",
        "https://openalex.org/W2941599692",
        "https://openalex.org/W6800875267",
        "https://openalex.org/W3169017236",
        "https://openalex.org/W4306887345",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W2997195635",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4367369699",
        "https://openalex.org/W1966797434",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W3176618728",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4385572988",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4378473736",
        "https://openalex.org/W4366330700",
        "https://openalex.org/W4296604605",
        "https://openalex.org/W4221144473",
        "https://openalex.org/W4385570984",
        "https://openalex.org/W4376653782",
        "https://openalex.org/W4385573040",
        "https://openalex.org/W4386290290",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4377865076",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W4385000770",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W4386566508",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W4378499085"
    ],
    "abstract": "While large language models (LLMs) have exhibited impressive instruction-following capabilities, it is still unclear whether and to what extent they can respond to explicit constraints that might be entailed in various instructions. As a significant aspect of LLM alignment, it is thus important to formulate such a specialized set of instructions as well as investigate the resulting behavior of LLMs. To address this vacancy, we propose a new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs' responses to instructions with various constraints. We construct a large collection of constraints-attributed instructions as a test suite focused on both generalization and coverage. Specifically, we advocate an instruction diversification process to synthesize diverse forms of constraint expression and also deliberate the candidate task taxonomy with even finer-grained sub-categories. Finally, we automate the entire evaluation process to facilitate further developments. Different from existing studies on controllable text generation, CoDI-Eval extends the scope to the prevalent instruction-following paradigm for the first time. We provide extensive evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval, revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs. We believe this benchmark will facilitate research into improving the controllability of LLMs' responses to instructions. Our data and code are available at https://github.com/Xt-cyh/CoDI-Eval.",
    "full_text": "Benchmarking Large Language Models on Controllable Generation\nunder Diversified Instructions\nYihan Chen1, Benfeng Xu1, Quan Wang2, Yi Liu3, Zhendong Mao1*\n1University of Science and Technology of China\n2MOE Key Laboratory of Trustworthy Distributed Computing and Service,\nBeijing University of Posts and Telecommunications\n3State Key Laboratory of Communication Content Cognition, People’s Daily Online, Beijing, China\n{chenyihan, benfeng}@mail.ustc.edu.cn, wangquan@bupt.edu.cn, gavin1332@gmail.com, zdmao@ustc.edu.cn\nAbstract\nWhile large language models (LLMs) have exhibited im-\npressive instruction-following capabilities, it is still unclear\nwhether and to what extent they can respond to explicit con-\nstraints that might be entailed in various instructions. As a\nsignificant aspect of LLM alignment, it is thus important to\nformulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this\nvacancy, we propose a new benchmark CoDI-Eval to sys-\ntematically and comprehensively evaluate LLMs’ responses\nto instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite\nfocused on both generalization and coverage. Specifically,\nwe advocate an instruction diversification process to synthe-\nsize diverse forms of constraint expression and also deliber-\nate the candidate task taxonomy with even finer-grained sub-\ncategories. Finally, we automate the entire evaluation process\nto facilitate further developments. Different from existing\nstudies on controllable text generation, CoDI-Eval extends\nthe scope to the prevalent instruction-following paradigm for\nthe first time. We provide extensive evaluations of represen-\ntative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval, reveal-\ning their limitations in following instructions with specific\nconstraints and there is still a significant gap between open-\nsource and commercial closed-source LLMs. We believe this\nbenchmark will facilitate research into improving the control-\nlability of LLMs’ responses to instructions. Our data and code\nare available at https://github.com/Xt-cyh/CoDI-Eval.\n1 Introduction\nThe emergence and popularization of Large Language Mod-\nels (LLMs) have revolutionized the NLP field and the world.\nLLMs exhibit powerful capabilities in responding fluently\nto natural language instructions or various NLP tasks (Wei\net al. 2021; Chung et al. 2022). However, LLMs do not al-\nways respond accurately to instructions with certain con-\nstraints (Zhou et al. 2023; Qin et al. 2023), e.g., writing an\narticle summary with a specific length or drafting an email\nwith an expected sentiment. Therefore, it is crucial to evalu-\nate the responses of LLMs to these specific instructions.\nThe process of generating text while adhering to specific\nconstraints is commonly known as Controllable Text Gen-\n*corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nA warm breeze whispered through the \nrustling leaves of summer. (10)\nIs response right?\n[Task]: Let LLMs generate a text containing 10 words.\n[Instruction1]: Please give me a 10-words-long text: \n[Instruction2]: Produce a text consisting of ten words:\nLLM1\nThe moon shines silently with starry \nlight. (7)\nLLM2\nDiversify instructions\nInput\nFigure 1: An illustration of our proposed benchmark, which\nincludes diverse CTG instructions, can be used to evaluate\nwhether large language models can properly respond to the\ncontrol constraints specified in the instructions.\neration (CTG) (Zhang et al. 2022). While traditional CTG\nhas been extensively studied (Dathathri et al. 2019; Zhang\nand Song 2022), the formulation of control conditions is dis-\ncrete variables, thus not directly applicable under the new\ninstruction-following paradigm, as the latter entails natural\nlanguage instructions instead. Such discrepancy precludes\ndirectly applying traditional evaluation methods of control-\nlable text generation to LLMs or any related applications.\nMoreover, in real-world scenarios, the constraints in the\ninstructions are usually presented in free-form natural lan-\nguage, as illustrated in Figure 1. Therefore, LLMs are ex-\npected to respond accurately to instructions that contain dif-\nferent constraints expressed in various ways. In other words,\nthe instructions used for CTG evaluation need to cover as\nwide a range of natural language expressions as possible.\nThis requirement cannot be satisfied by simply converting\nthe limited constraints in traditional CTG tasks into natural\nlanguage instructions using fixed templates. The lack of in-\nstruction diversity will hinder evaluating the robustness and\ngeneralization of LLMs’ controllable text generation capa-\nbility as well as the alignment with actual user expectations.\nOne recent work, instuctCTG (Zhou et al. 2023) has imple-\nmented CTG using an instruction-based approach. Nonethe-\nless, they only employ fixed templates to transform lim-\nited discrete constrained conditions into natural language in-\nstruction, and the diversity of instructions is still very limited\nto evaluate LLMs’ capability under generalized settings.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17808\nAverage\nT opic\nMulti-aspect\nT oxicity avoidance Length\nKeyword\nSentiment\n20\n40\n60\n80\nGPT-4 (0613)\nChatGPT (0613)\nLLaMA-13B-chat\nVicuna-13B\nVicuna-7B\nChatGLM-6B\nFigure 2: Performance of typical LLMs on CoDI-Eval.\nTo address this gap and motivate further research to\nalign LLMs with human expectations better, we pro-\npose CoDI-Eval (Controllable Generation under Diversified\nInstructions), a new benchmark for systematically and com-\nprehensively evaluating the controllable generation capabil-\nities of LLMs. It can be utilized to accurately measure how\nwell an LLM is aligned with instructions that have specific\nconstraints, as shown in Figure 1.\nCoDI-Eval features in both coverage and generalization.\nFor coverage, we select five typical CTG tasks based on\nthe possible aspects of controllability, including Sentiment,\nTopic, Length, Keyword, and Toxicity Avoidance, we also\nfurther include a multi-aspect that simultaneously contains\ntwo aspects to test LLMs under more challenging and com-\nplex settings. For generalization, we maximize the diversity\nof instructions with a two-step process. We initially start\nfrom a small set of human-curated, high-quality seed in-\nstructions w.r.t. each constraint category. Then in step 1, we\nemploy an Expansion process to increase the number of in-\nstructions to construct the instructions pool. In step 2, we\nrandom sample instructions from the pool, and further em-\nploy a Diversification process to diversify them in a text\nrewritten manner. We repeat step 2 using Bootstrap until an\nexpected instruction scale is reached. Both steps are com-\npleted using a capable LLM with no human intervention.\nFor the evaluation of CoDI-Eval, we collect or construct\nautomated, easy-to-use, and reliable evaluation methods for\neach controllable generation task. For tasks that can not be\ndirectly evaluated, we resort to available open-source, spe-\ncialized models or external APIs, and demonstrate that these\nalternatives have qualified consistency with human evalua-\ntion. The evaluation metric for each CTG task is accuracy.\nWe rank the CTG capabilities of different LLMs using the\naverage accuracy across all CTG tasks.\nWe conduct extensive evaluations to verify the perfor-\nmance of mainstream LLMs (e.g., ChatGPT 1, LLaMA2-\nchat (Touvron et al. 2023b), Vicuna (Chiang et al. 2023)) on\nCoDI-Eval. The experimental results are simply depicted in\nFigure 2. Experiments reveal that top commercial LLMs like\n1https://platform.openai.com/docs/models/gpt-3-5\nGPT-4 (OpenAI 2023) and ChatGPT are capable of handling\nCTG tasks, but they still have shortcomings in certain areas,\nimplying there is substantial scope for enhancing their over-\nall CTG capabilities. The performance of the open-source\nLLMs we tested still lags behind that of their closed-source\ncounterparts. We believe CoDI-Eval will serve as an effec-\ntive benchmark to evaluate and compare various current and\nfuture LLMs in the specific task of controllable generation,\nas well as facilitate more related research progress. The main\ncontributions of this paper can be summarized as:\n• We propose A new benchmark for evaluating the CTG\ncapabilities of LLMs, which goes beyond traditional\nevaluation methods by incorporating diversified instruc-\ntions in natural language formats that allow us to better\nevaluate the generalized performance of LLMs.\n• We accompany the benchmark with automated and easy-\nto-use evaluation methods for further development.\n• We conduct zero-shot and few-shot evaluations on the\nproposed benchmark for a wide range of established\nLLMs, systematically validating and comparing their\nperformance on CTG for the first time.\n2 Related Works\nLarge Language Model LLMs are language models that\nhave been pre-trained on massive text data and contain a\nvast number of parameters (Zhao et al. 2023). To enhance\nor leverage the capabilities of LLMs, researchers have de-\nveloped various methods. One such approach is instruc-\ntion tuning (Wei et al. 2021), which means fine-tuning LM\nwith multi-task natural language instructions. Researchers\ncan also leverage the in-context learning (ICL) capability\nof LLMs by creating multiple demonstrations (Brown et al.\n2020). Currently, the evaluation benchmark of LLMs typi-\ncally involves a wide range of NLP tasks that test their ad-\nvanced abilities, including knowledge inference (Hendrycks\net al. 2020; Huang et al. 2023). Li et al. used verbalizer ma-\nnipulations to construct instructions for evaluating whether\nLLMs can comply with the requirements in the instruc-\ntions (Li et al. 2023), but it was limited to classification tasks\nand the instructions were not diverse enough.\nData Generation by LLMs With the support of prompt\nengineering, there is now a trend of using LLMs to generate\ndata. Self-Instruct (Wang et al. 2022) and Unnatural Instruc-\ntions (Honovich et al. 2022) rely on LLMs to provide in-\nstructions and responses to overcome the limitations of man-\nually written data, such as quantity and diversity shortages.\nTo obtain better outputs, LLAMA-GPT4 (Peng et al. 2023b)\ntook advantage of more powerful LLMs, such as GPT-4.\nExpertLLAMA (Xu et al. 2023a) and LongForm (K ¨oksal\net al. 2023) also proposed their ways to improve data quality.\nFurthermore, OpenRewriteEval (Shu et al. 2023) employs\nChain-of-Thought (CoT) (Wei et al. 2022) to generate in-\nstructions for a text rewriting benchmark.\nControllable Text Generation Current CTG tasks mainly\nfocus on two categories: hard constraints and soft con-\nstraints (Qin et al. 2022). Hard constraints are to limit\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17809\nApproach Contain\nmultiple tasks\nCover both soft and\nhard constraints\nUse natural\nlanguage instruction\nDiversify expressions\nsufficiently\nPPLM (Dathathri et al. 2019) ✓ ✗ ✗ ✗\nDExperts (Liu et al. 2021) ✓ ✗ ✗ ✗\nInstructCTG (Zhou et al. 2023) ✓ ✓ ✓ ✗\nCoDI-Eval (Ours) ✓ ✓ ✓ ✓\nTable 1: Comparison between our benchmark and previous studies.\nthe lexicon and syntax of the text, including controlling\ntext length (Takase and Okazaki 2019), and ensuring that\nthe generated text contains some keywords (Carlsson et al.\n2022). Soft constraints are designed to limit the semantics\nof the text, such as sentiment and topic (Gu et al. 2022; Lu\net al. 2022). In contrast to the previous approach of targeting\nonly one category, CoDI-Eval includes both categories and\nunifies them into the instruction-following paradigm.\nEvaluation of CTG In the past, there was no unified\nbenchmark in the CTG field, but some studies still made\ntheir attempts. PPLM designed several short prefixes as in-\nput for the CTG model, with the corresponding output being\na continuation of the prefix. In this study, a text classifier\nwas employed to label the model outputs, after which the\nratio of outputs meeting the requirements was calculated as\nthe accuracy of CTG. DExperts adopts a similar approach\nto RealToxicPrompt (Gehman et al. 2020), which constructs\nnumerous prompts that make it easier for language mod-\nels to generate toxic text. Specifically, they devised prompts\nthat promote the generations of positive, neutral, and neg-\native text to assess the model’s robustness to control senti-\nment across diverse input prompts. Other models either fol-\nlowed their proposed evaluation method or adopted a simi-\nlar approach (Yang and Klein 2021; Krause et al. 2021; Ke\net al. 2022). However, these methods can only be directly ap-\nplied to auto-regressive language models. InstructCTG and\nBound-Cap-LLM (Lu et al. 2023) do not have this problem,\nbut there is still room for improvement in terms of instruc-\ntion diversity. We compare our benchmark with the evalua-\ntion of previous works in Table 1.\n3 Benchmarking\n3.1 Preliminaries\nOur benchmark is primarily concerned with the problem of\ncontrollable text generation, where given an input X and a\nset of control conditions c, the objective is to generate the\noutput Y . It can be formally described as follows:\nP(Y |X, c) =\nnY\ni=1\nPθ(Yi|Y<i, X, c)\nWhere n is the length of Y , θ is the parameter of a language\nmodel, and Y<i is the part that has been generated. The first\ngenerated token is conditioned solely on the input and label.\nIn traditional CTG models (Liu et al. 2021), X is usually the\nprompt, an incomplete text, while Y is its continuation.\nFigure 3: Base CTG tasks and their corresponding control\nattributes we select. Note that the size of each task sector\ndoes not represent its proportion in the set.\nOur testing target is LLMs with instruction-following ca-\npability, which refers to a model’s proficiency to understand\nand follow given instructions. Thus, X will be transformed\ninto an instruction, and the label c will also be included in\nit. At this point, the above formulation can be expressed as\nY = LLM(X). Thereby, we construct an instruction set\nX that contains different control conditions, which is then\ninputted into a certain LLM. We test the response set Y\nand calculate the proportion of outputs that satisfy the corre-\nsponding control conditions, which will serve as the perfor-\nmance metric for the CTG capability of the LLMs.\n3.2 Design Principle\nIn order for our benchmark to cover multiple CTG tasks, we\nselect five typical CTG tasks as comprehensively as possible\nfrom two major categories: hard constraints and soft con-\nstraints. Besides, we also include a multi-aspect controllable\ngeneration task. These CTG tasks have been extensively re-\nsearched in previous studies.\nTo better evaluate the controllable generation capabilities\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17810\nDiversified \nInstructions\nInstructions\n(Task description and requirements)\nPlease comp up with 10 length-controlled text generation \ninstructions:\n1: [Text up to {number} words, help me to write:]\n2: [Generate a text which length is exactly {number}:]\n... ...\n7: [Generate a {number}-words-long text:]\n8: [Write sth between {number1} and {number2} words]\n9: [Craft a text, approximately {number} words long:]\n10: [Create a text with a length of at least {number} words:]\nSeed Instructions\nInstruction Pool\nA: [I kindly request that you generate a text consisting of precisely \n{number} words, no more, no less.]\n(Task description and requirements)\nPlease rewrite the length-controlled generation instructions:\nQ:'Provide me with a very short text, no longer than {number} \nwords:' rewrite this instruction, be concise\nA:[Write less than {number} words text:]\n... ... \nQ: 'Create a text with exactly {number} words :' rewrite this \ninstruction, be verbose\n\nLLM LLM\nStep1: Expansion Step2: Diversification\nPrompt Template Prompt Template\nFigure 4: The framework of constructing evaluation instruction sets. It consists of two steps: expansion and diversification.\nof LLMs, we need to diversify the expression of constraints\nin the evaluation instructions and ensure the instructions\nremain within the scope of the corresponding CTG task.\nWe find that text rewriting is able to maintain the meaning\nof instructions while diversifying their expressions. So text\nrewriting is a good way for us to diversify CTG instructions.\nSince diversifying instructions in a rewritten manner re-\nquires a certain amount of initial instructions, we first intro-\nduce an instruction expansion step. This step also requires\nsome initial instructions, so we manually write 20 typical\ninstructions for each CTG task. We call these manual in-\nstructions as seed instructions. We do not rely only on the\nexpansion step to construct instruction sets because it is in-\nsufficient to utilize this method to expand and diversify the\ninstructions of a fixed task, as exemplified by the fact that\na single emotional attribute word can be expressed through\nvarious means, such as adjectives and nouns. We will pro-\nvide a detailed explanation in the following sections.\n3.3 Tasks Overview\nBasic tasks in CoDI-Eval include sentiment, topic, key-\nword, length, and toxicity avoidance. We refer to the spe-\ncific constraint categories in each controllable generation\ntask as control attributes. The tasks and control attributes we\nselect are displayed in figure 3.\nTo evaluate whether LLMs can comprehend and gen-\nerate more fine-grained human emotions, our sentiment-\ncontrolled generation task includes a set of 9 control at-\ntributes. In addition to positive and negative sentiment, we\nutilize six basic emotions (Ekman 1992) as part of our con-\ntrol attributes, along with a neutral attribute that has no sen-\ntiment orientation. For the topic task, we selected 18 specific\ntopics from TweetTopic (Antypas et al. 2022) as the control\nattributes in the topic CTG tasks. To increase the challenge\nof our benchmark, we introduce a multi-aspect controlled\ngeneration task that pairs attributes from the sentiment and\ntopic CTG task as its control attributes.\nAs for hard constraints, in addition to precisely control-\nling the number of generated words, we introduce several\ntasks for length controllable generation, including generat-\ning text with at least, at most, or approximately a certain\nnumber of words, as well as generating text within a spec-\nified range of word counts. On top of the simple keyword\ninclusion task, we incorporate two additional tasks based on\nthe setup of InstructCTG: one task is excluding keywords,\nand the other involves selecting between two keywords, we\ncall this the complex keyword CTG task.\nFinally, regarding the toxicity avoidance task which\nis to avoid generating harmful or offensive content, we\nfollow ContrastivePrefixes (Qian et al. 2022) by select-\ning 203 prompts labeled as “challenge” from RealToxi-\ncPrompts (Gehman et al. 2020) with toxicity scores below\n0.5. They were used as inputs for the LLMs to generate con-\ntinuations of them.\n3.4 Constructing Diversified Instructions\nWe employ a two-step in-context learning prompting to gen-\nerate instructions with increased diversity and varying ex-\npressions, this is illustrated in Figure 4. We first manually\nwrite 20 different seed instructions for each CTG task, and\nwe do not add specific control attributes here but use spe-\ncial symbols as placeholders, such as “{sentiment}” used in\nthe sentiment CTG task. The reason for doing so is to estab-\nlish a one-to-one correspondence between instructions and\ntheir corresponding tasks and control attributes, ensuring the\nevaluability of the instructions in the benchmark.\nIn order to improve the quality of instructions generated\nby LLM and to ensure that they do not contain any control\ninformation beyond specific control attributes, we add a task\nrequirement description to the prompt for querying LLM.\nAll experiments are based on GPT-3.5-turbo (0301).\nInstruction Expansion For each CTG task and every step,\nwe sample 8 instructions in the seed instructions set for con-\nstructing the prompt. The ICL prompt for the query LLM\nwill be constructed in the following form:\nInew = LLM(D ⊕ i1 ⊕ ... ⊕ i8)\nWhere Inew is the response that contains 2 new instructions,\ni is the demo instruction, and D represents the task descrip-\ntion, accompanied by a request to generate 10 instructions.\nThis process is not used in toxicity avoidance tasks, because\nthe diversity of this task mainly stems from the variety of\ninput texts, whereas it is difficult to further diversify the de-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17811\nscription of continuation tasks. After this step, every task\nexcept toxicity avoidance will have 100 instructions.\nInstruction Diversification We design the instruction di-\nversification step with an instruction rewrite prompt and ini-\ntial the instruction pool with instructions generated in the\nfirst step. This prompt consists of a task description, 3 in-\ncontext examples, and an instruction that needs to be rewrit-\nten. This prompt can also be presented as:\nInew = LLM(D ⊕ {i1, r1} ⊕... ⊕ {i3, r3} ⊕i4)\nWhere Inew is the response that contains the new instruc-\ntion, {i, r} is the in-context demonstration, the text rewriting\ninstruction i consists of two parts: the source instruction and\nthe text rewriting method, and i4 represent the instruction to\nbe rewritten. At last, D represents the task description, ac-\ncompanied by a request to rewrite instructions. In this stage,\nBootstrap is utilized to randomly fetch the instructions to be\nrewritten from the instruction pool, and the rewritten instruc-\ntions will be added back to the instruction pool. However,\nthe in-context demonstration remains unchanged.\nThe selection of rewrite methods is done as follows: there\nis a 50% chance of selecting from six basic methods and\na 20% chance of choosing from a more complex set of 20\nrewrite methods obtained by querying GPT-4. These rewrite\nmethods are described by an adjective that represents the\nstyle of the rewritten sentence. Finally, the remaining 30%\nallows for the LLM to rewrite freely.\nFor the sentiment and topic control tasks, before rewrit-\ning, we randomly select one instruction from the instruction\npool, and then randomly select an attribute to fill it. For these\ntwo tasks, we add “part-of-speech conversion” to the basic\nrewriting method mentioned above, to prompt the LLM to\ntransform the part of speech of the attribute words. In the\nend, we filter out 1,000 instructions from the instruction pool\nto balance the number of each control attribute, and they\nwill become the final evaluation instruction set. As tomulti-\naspect control, due to the numerous categories involved, we\ndo not add generated instructions to the pool for the first half\nof the diversification process.\nFor length controlled text generation task, we also select\n1,000 instructions from the instruction pool with the number\nof each subtask balanced. At last, we use numbers or words\nthat represent numbers to fill out the instructions randomly.\nFor the keyword task, we generate 500 instructions for both\nthe simple and complex tasks. We randomly selected key-\nwords from the CommonGen dataset (Lin et al. 2020) to\nfill these instructions. Additional keywords used in com-\nplex tasks were generated by the LLM. We finally deal with\nthe toxicity avoidance task. We select 203 toxic prompts\nand combine them with 20 continuation prompts to create\n4,060 instructions, with each toxic prompt corresponding to\n20 text continuations. After the above steps, we obtain the\ntest instruction set for CoDI-Eval (Total 9060, Sentiment\n1000, Topic 1000, Multi-aspect 1000, Length 1000, Key-\nword 1000, Toxicity Avoidance 4060).\n3.5 Evaluation\nDue to the labor-intensive and costly nature of human evalu-\nation, we collect or construct methods to automatically eval-\nuate the accuracy (%) of each CTG task. The accuracy is de-\nfined as the ratio of an LLM’s responses to all instructions in\na CTG task that satisfy the corresponding control attributes.\nThe whole evaluation process is free and fast, only the toxi-\ncity avoidance evaluation takes a few hours.\nFor the sentiment and topic evaluation, we select corre-\nsponding text classifiers with high download rates on Hug-\ngingFace as evaluation models. However, the model for eval-\nuating topics is a multi-classifier that outputs scores between\n0 and 1 for each attribute. If the score for the target attribute\nis greater than 0.5, the input is considered to belong to that\ncategory. Toward multi-aspect control tasks, if both the sen-\ntiment and topic classifiers output the target attributes, the\ninput text is seen to meet the requirements.\nWe use a simple match to check whether the LLM re-\nsponses contain or do not contain the target keyword. Be-\nfore matching keywords, we perform the lemmatization of\nall the words in labels and generated text, then convert them\nto lowercase. For length control, we map every label to a\nclosed interval. If the text length is included in this interval,\nthe response is in accordance with the requirements.\nToxicity avoidance is more special, we use Perspective\nAPI2 to detect the toxicity of generated text. Given an LLM\nwill generate 20 continuations of a toxic prompt, if the tox-\nicity values of all 20 continuations do not exceed 0.5, the\nLLM is considered to have successfully completed the toxi-\ncity avoidance task on that toxic prompt. The final accuracy\nis defined as the proportion of toxicity avoidance tasks com-\npleted prompts to all prompts.\n4 Experiments\n4.1 Experimental Setup\nModels According to the access method, we classify the\nLLMs into two categories: (1) Open-source models which\nwe can access to all weights, such as LLaMA-7B (Tou-\nvron et al. 2023a), LLaMA2-7B/13B, and LLaMA2-\n7B/13B-chat which is fine-tuned on the upgraded version\nof LLaMA, ChatGLM/ChatGLM2-6B (Du et al. 2022),\nAlpaca-7B (Taori et al. 2023), Vicuna-7B/13B, GPT4ALL-\n13B-snoozy (Anand et al. 2023), RWKV-raven-14B (Peng\net al. 2023a) which are LLMs based on RNN, Baichuan-\n13B-chat3, and WizardLM-13B-V1.2 (Xu et al. 2023b), an\nLLM trained on LLaMA2; (2) Commercial models that\nwe can only access to their API service, include GPT-4\n(0613), GPT-4-turbo (gpt-4-1106-preview), and GPT-3.5-\nturbo (0613), which is commonly known as ChatGPT.\nWithin them, LLaMA and LLaMA2 are basic language\nmodels, the others are fine-tuned LLMs.\nInference and Decoding We primarily employ a zero-\nshot prompt to test the capability of LLMs to respond to the\nconstraints in instructions. Additionally, we also conduct ex-\nperiments under the few-shot setup. The zero-shot prompt is\ndisplayed in Figure 5, while the few-shot prompt is made\nup of adding 5 instruction-response demonstrations to the\n2https://perspectiveapi.com/\n3https://github.com/baichuan-inc/Baichuan-13B\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17812\nZero-shot Few-shot Comparison\nS T M L K TA Average Average ∆accuracy ∆s-BLEU\nGPT-4 (0613)* 91.6 93.5 70.2 73.8 86.2 93.6 84.82 - - -\nGPT-4-turbo* 88.3 88.6 65.2 70.8 83.3 94.09 81.72 - - -\nGPT-3.5-turbo (0613) 88 95.1 76.1 55 80.1 88.67 80.5 79.33 -1.17 0.0899\nLLaMA2-13B-chat 85.9 91.4 68.6 46.8 68.3 56.65 69.61 67.66 -1.95 0.0331\nWizardLM-13B-V1.2 84.8 93.3 67.6 48.9 70.1 51.72 69.4 69.51 0.11 0.0294\nLLaMA2-7B-chat 78.8 90 64.4 42.6 63.1 66.5 67.57 68.38 0.81 -0.0001\nChatGLM-6B 81.9 91.6 56.9 34.4 59.7 66.5 65.17 65.67 0.50 0.0254\nGPT4ALL-13B 85.1 93.3 68.2 38.9 60.1 38.92 64.09 60.9 -3.09 0.0395\nVicuna-13B 84.8 92.4 64.8 41.4 58.7 20.2 60.38 64.06 3.68 0.0741\nBaichuan-13B-chat 81.9 92.5 63.9 35.1 48.6 35.96 59.66 62.54 2.88 0.093\nVicuna-7B 80.1 87.3 60 36.3 65.1 21.67 58.41 61.45 3.04 0.1043\nChatGLM2-6B 85.3 88.3 49.5 37.2 34.8 50.74 57.64 61.75 4.11 0.0356\nAlpaca-7B 78.6 92.9 56.8 38.5 44.1 28.57 56.58 54.27 -2.31 0.0815\nRWKV-14B 79.7 82.3 49.8 30.2 40.7 14.78 49.58 39.61 -9.97 0.1296\nLLaMA2-13B 63.5 70.1 38.7 22.2 50.5 5.91 41.82 41.73 -0.09 0.1584\nLLaMA2-7B 59.1 59.4 32 24.3 41.2 9.85 37.64 38.83 1.19 0.0933\nLLaMA-7B 53.9 60.9 26.2 25.1 43.9 5.91 35.98 27.29 -8.69 0.0874\nTable 2: Model accuracy (%) (See Section 3.5) on each CTG task in CoDI-Eval. We use ‘S’, ‘T’, ‘M’, ‘L’, ‘K’, and ‘TA’ to\nrepresent Sentiment, Topic, Multi-aspect, Length, Keyword, and Toxicity Avoidance. The ‘Average’ is the average accuracy on\nzero-shot or few-shot settings. ∆s-BLEU is the average self-BLEU difference between the few-shot and the zero-shot. *Due to\nbudget constraints, experiments for GPT-4 and GPT-4-turbo were only performed on zero-shot settings.\n[System]: You are performing a test of controlled text \ngeneration. Generate text according to the following instruction \nand generate whatever you want, no other requirements:\n[Instruction]: Produce some text that expresses great happiness. \n[Response]: I jumped up and down with excitement as I read the \nemail - I had been accepted into my dream university!\nFigure 5: An example of the zero-shot prompt. The black\npart is the prompt while the green part is the output of LLM.\nzero-shot prompt. Our benchmark does not impose any re-\nstrictions on the decoding method of the models. However,\nfor the sake of experimental consistency, we simply use the\nnucleus sampling (Holtzman et al. 2019) and set the top-p\nparameter to 0.9, as well as the temperature to 1.0. To reduce\nthe generation time, we also limit the generation length (75\ntokens for toxicity avoidance; 300 tokens for other tasks).\n4.2 Results\nThe main results of these LLMs on CoDI-Eval are presented\nin Table 2. We report the accuracy (%) on the zero-shot set-\nting as the main score for every LLM.\nComparing the Performance of Different LLMs. Not\nsurprisingly, the top commercial LLMs achieved the highest\nscores on all CTG tasks, the open-source LLMs we tested\nexhibiting an accuracy gap of over 10%. As can be seen\nfrom Table 2, the fine-tuned LLMs perform better than the\nbase language model. Moreover, the more complex trained\nmodels (LLaMA2-chat, ChatGLM, etc.) also outperform the\nLLMs with the same amount of parameters that have only\nFigure 6: Reasons for the errors of GPT-3.5-turbo on multi-\naspect and complex keyword CTG tasks.\nundergone instruction tuning such as Vicuna and Alpaca.\nComparing LLMs’ Performance on Different Tasks.\nLLMs perform relatively well on sentiment and topic con-\ntrol tasks. However, once these two attributes are combined,\nthe difficulty of the task increases, and none of the LLMs\nachieve 80% accuracy. We use GPT-3.5-turbo as an exam-\nple to analyze the reasons why LLMs respond incorrectly\non multi-aspect controllable generation tasks. We show it in\nFigure 6. In the toxicity avoidance task, only LLMs that have\nexperienced alignment tuning such as RLHF, are able to per-\nform well on this task, especially GPT-4 and GPT-3.5-turbo\nwhich have undergone more refined alignment training.\nAs for the hard constraints part, The accuracy of LLMs\non keyword tasks is close to the average accuracy. We ana-\nlyze the cause of LLM’s error on the complex keyword CTG\ntask as we did for the multi-aspect task, see Figure 6. How-\never, in the seemingly simple length CTG task, even GPT-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17813\nAttribute Equal to Around Between\nAccuracy 6% 37% 58%\nAt least At most Zero-shot Few-shot\n98% 76% 55% 57.1%\nTable 3: Accuracy of GPT-3.5-turbo on each subtask of\nlength CTG. The last two columns show the average length\nCTG accuracy on zero-shot and few-shot setups.\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nAverage Rouge-L\n0\n20\n40\n60\n80\n100\n120\n140\n160Frequency\nUse step 1\nUse step 1 and 2\nFigure 7: Average Rouge-L scores distributions for the in-\nstructions. Instructions constructed by the diversification\nstep have low Rouge-L scores and more diversity.\n3.5-turbo’s accuracy is only 55%. This suggests that most\nLLMs have an insufficient perception of length. However,\nGPT-4 shows more strength in this task. We then calculate\nthe accuracy of GPT-3.5-turbo on each subtask of the length-\ncontrolled generation and find the accuracy is roughly posi-\ntively correlated with the range of target lengths (Table 3).\nFew-shot Setting The average accuracy in few-shot ex-\nperiments is presented in Table 2. Comparing the zero-shot\nand few-shot results, we can observe that the simple few-\nshot prompt does not necessarily improve the controllable\ngeneration of the LLMs. This is because of the high diver-\nsity and dispersed data distribution of our instructions. The\ndemonstrations in the few-shot prompt exhibit a low corre-\nlation with the target instruction. We also explore the text di-\nversity of the LLMs’ responses using the few-shot and zero-\nshot prompts by calculating the self-BLEU difference be-\ntween the few-shot and zero-shot settings. self-BLEU means\nthe average BLEU (Papineni et al. 2002) overlap between\nall generated texts of each LLM, lower self-BLEU indicates\nhigher diversity of generated text. As shown in Table 2, sim-\nple in-context learning may reduce generation diversity. So\nwe believe that simple in-context learning is not a good way\nto improve the model’s capability for CTG.\n5 Analysis and Discussion\nDiversity of Instructions To verify the validity of our “In-\nstruction Diversification” step, we conduct the following ex-\nperiments. We construct 1000 instructions only using the in-\nstruction expansion step (the first step) for each task that uses\ninstruction diversification, denoted as “Use step 1”. The fi-\nnal instruction set in CoDI-Eval is referred to as “Use step 1\nSentiment Topic Multi-aspect\nConsistency 94.5% 98.5% 89.5%\nTable 4: Consistency between our automated evaluation and\nhuman evaluation.\nand 2”. We calculate the average of Rouge-L (Lin and Och\n2004) scores for each instruction with all other instructions\non the same task, then plot them as histograms in Figure 7.\nSince a lower Rouge-L score indicates lower similarity, we\ncan see that the instructions undergoing the diversification\nstage exhibit greater diversity.\nQuality of Evaluations We conduct simple human judg-\nment to verify the reliability of the evaluation methods.\nSince the evaluation of length and keyword is based on rules,\nand the evaluation of text toxicity is based on the widely rec-\nognized Perspective API, we mainly verified the remaining\nthree tasks. For each task, we randomly sample 100 instruc-\ntions from the instruction set and collect a total of 200 cor-\nresponding responses of GPT-3.5-turbo under zero-shot and\nfew-shot settings. We then manually judge whether these re-\nsponses meet the requirements of the corresponding instruc-\ntions. We calculate the consistency between the automated\nevaluation results and the human evaluation results, which\nis shown in Table 4. The results show that automatic evalua-\ntion has a relatively high agreement with human evaluation.\nFurther Discussion LLMs perform the worst in the task\nof generating text under certain length constraints. We argue\nthat it is difficult for deep neural networks to fit the infor-\nmation of text length, in other words, LLMs do not have an\nexplicit mechanism or capability of “counting”. LLMs gen-\nerate text on a token-by-token basis, which poses a burden as\nthe number of tokens in the output may differ from the num-\nber of words in the output. These lead to the differences be-\ntween the Length-controllable text generation task and other\nCTG tasks. If a large number of length-controllable genera-\ntion instructions with non-diversified constraints are used to\nfine-tune LLMs, the accuracy of LLM generations may be\nimproved. However, it is possible that LLMs simply “mem-\norize” answers without truly comprehending the meaning\nof the length. As a result, the trained models may perform\npoorly on unseen instructions.\n6 Conclusion\nIn this paper, we introduce CoDI-Eval, a novel benchmark\nfor evaluating the controllable text generation capabilities of\nLLMs. Our benchmark comprises a set of evaluation instruc-\ntions involving multiple CTG tasks in a variety of natural\nlanguage expressions. Our results suggest that LLMs with\ninstruction tuning are able to perform certain CTG tasks,\nbut the accuracy of generations requires to be further im-\nproved, especially for some specific constraints. We also ob-\nserve a performance gap between open-source LLMs and\ntheir closed-source commercial counterparts, marking a po-\ntential direction for future works.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17814\nAcknowledgments\nWe thank all anonymous reviewers for their valuable and in-\nsightful comments. This work is supported by the National\nScience Fund for Excellent Young Scholars under Grant\n62222212 and the National Natural Science Foundation of\nChina under Grant U19A0527.\nReferences\nAnand, Y .; Nussbaum, Z.; Duderstadt, B.; Schmidt, B.; and\nMulyar, A. 2023. GPT4All: Training an Assistant-style\nChatbot with Large Scale Data Distillation from GPT-3.5-\nTurbo. https://github.com/nomic-ai/gpt4all.\nAntypas, D.; Ushio, A.; Camacho-Collados, J.; Silva, V .;\nNeves, L.; and Barbieri, F. 2022. Twitter Topic Classifica-\ntion. In Proceedings of the 29th International Conference\non Computational Linguistics, 3386–3400.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCarlsson, F.; ¨Ohman, J.; Liu, F.; Verlinden, S.; Nivre, J.; and\nSahlgren, M. 2022. Fine-grained controllable text genera-\ntion using non-residual prompting. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 6837–6857.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, Y .; Wang, X.; Dehghani, M.; Brahma, S.; Web-\nson, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowd-\nhery, A.; Castro-Ros, A.; Pellat, M.; Robinson, K.; Val-\nter, D.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V .; Huang,\nY .; Dai, A.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; De-\nvlin, J.; Roberts, A.; Zhou, D.; Le, Q. V .; and Wei, J.\n2022. Scaling Instruction-Finetuned Language Models.\narXiv:2210.11416.\nDathathri, S.; Madotto, A.; Lan, J.; Hung, J.; Frank, E.;\nMolino, P.; Yosinski, J.; and Liu, R. 2019. Plug and play\nlanguage models: A simple approach to controlled text gen-\neration. arXiv preprint arXiv:1912.02164.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335.\nEkman, P. 1992. An argument for basic emotions.Cognition\n& emotion, 6(3-4): 169–200.\nGehman, S.; Gururangan, S.; Sap, M.; Choi, Y .; and Smith,\nN. A. 2020. RealToxicityPrompts: Evaluating Neural Toxic\nDegeneration in Language Models. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020, 3356–\n3369. Online: Association for Computational Linguistics.\nGu, Y .; Feng, X.; Ma, S.; Zhang, L.; Gong, H.; and Qin, B.\n2022. A Distributional Lens for Multi-Aspect Controllable\nText Generation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, 1023–\n1043. Abu Dhabi, United Arab Emirates: Association for\nComputational Linguistics.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2020. Measuring Massive Mul-\ntitask Language Understanding. InInternational Conference\non Learning Representations.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2019. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations.\nHonovich, O.; Scialom, T.; Levy, O.; and Schick, T. 2022.\nUnnatural Instructions: Tuning Language Models with (Al-\nmost) No Human Labor. arXiv:2212.09689.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He, J. 2023.\nC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models. arXiv:2305.08322.\nKe, P.; Zhou, H.; Lin, Y .; Li, P.; Zhou, J.; Zhu, X.; and\nHuang, M. 2022. CTRLEval: An Unsupervised Reference-\nFree Metric for Evaluating Controlled Text Generation. In\nProceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n2306–2319. Dublin, Ireland: Association for Computational\nLinguistics.\nKrause, B.; Gotmare, A. D.; McCann, B.; Keskar, N. S.;\nJoty, S.; Socher, R.; and Rajani, N. F. 2021. GeDi: Gen-\nerative Discriminator Guided Sequence Generation. In\nFindings of the Association for Computational Linguistics:\nEMNLP 2021, 4929–4952. Punta Cana, Dominican Repub-\nlic: Association for Computational Linguistics.\nK¨oksal, A.; Schick, T.; Korhonen, A.; and Sch¨utze, H. 2023.\nLongForm: Optimizing Instruction Tuning for Long Text\nGeneration with Corpus Extraction. arXiv:2304.08460.\nLi, S.; Yan, J.; Wang, H.; Tang, Z.; Ren, X.; Srinivasan, V .;\nand Jin, H. 2023. Instruction-following Evaluation through\nVerbalizer Manipulation. arXiv:2307.10558.\nLin, B. Y .; Zhou, W.; Shen, M.; Zhou, P.; Bhagavatula, C.;\nChoi, Y .; and Ren, X. 2020. CommonGen: A Constrained\nText Generation Challenge for Generative Commonsense\nReasoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, 1823–1840.\nLin, C.-Y .; and Och, F. J. 2004. Automatic evaluation of ma-\nchine translation quality using longest common subsequence\nand skip-bigram statistics. In Proceedings of the 42nd An-\nnual Meeting of the Association for Computational Linguis-\ntics (ACL-04), 605–612.\nLiu, A.; Sap, M.; Lu, X.; Swayamdipta, S.; Bhagavatula, C.;\nSmith, N. A.; and Choi, Y . 2021. DExperts: Decoding-Time\nControlled Text Generation with Experts and Anti-Experts.\nIn Proceedings of the 59th Annual Meeting of the Associ-\nation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 6691–6706.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17815\nLu, A.; Zhang, H.; Zhang, Y .; Wang, X.; and Yang, D. 2023.\nBounding the Capabilities of Large Language Models in\nOpen Text Generation with Prompt Constraints. In Vla-\nchos, A.; and Augenstein, I., eds., Findings of the Asso-\nciation for Computational Linguistics: EACL 2023, 1982–\n2008. Dubrovnik, Croatia: Association for Computational\nLinguistics.\nLu, X.; Welleck, S.; Hessel, J.; Jiang, L.; Qin, L.; West, P.;\nAmmanabrolu, P.; and Choi, Y . 2022. QUARK: Controllable\nText Generation with Reinforced Unlearning. In Koyejo, S.;\nMohamed, S.; Agarwal, A.; Belgrave, D.; Cho, K.; and Oh,\nA., eds., Advances in Neural Information Processing Sys-\ntems, volume 35, 27591–27609. Curran Associates, Inc.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nPeng, B.; Alcaide, E.; Anthony, Q.; Albalak, A.; Arcad-\ninho, S.; Cao, H.; Cheng, X.; Chung, M.; Grella, M.; GV ,\nK. K.; He, X.; Hou, H.; Kazienko, P.; Kocon, J.; Kong,\nJ.; Koptyra, B.; Lau, H.; Mantri, K. S. I.; Mom, F.; Saito,\nA.; Tang, X.; Wang, B.; Wind, J. S.; Wozniak, S.; Zhang,\nR.; Zhang, Z.; Zhao, Q.; Zhou, P.; Zhu, J.; and Zhu, R.-J.\n2023a. RWKV: Reinventing RNNs for the Transformer Era.\narXiv:2305.13048.\nPeng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023b. In-\nstruction Tuning with GPT-4. arXiv:2304.03277.\nQian, J.; Dong, L.; Shen, Y .; Wei, F.; and Chen, W. 2022.\nControllable natural language generation with contrastive\nprefixes. arXiv preprint arXiv:2202.13257.\nQin, C.; Zhang, A.; Zhang, Z.; Chen, J.; Yasunaga, M.; and\nYang, D. 2023. Is ChatGPT a General-Purpose Natural Lan-\nguage Processing Task Solver? arXiv:2302.06476.\nQin, L.; Welleck, S.; Khashabi, D.; and Choi, Y . 2022. Cold\ndecoding: Energy-based constrained text generation with\nlangevin dynamics. Advances in Neural Information Pro-\ncessing Systems, 35: 9538–9551.\nShu, L.; Luo, L.; Hoskere, J.; Zhu, Y .; Liu, C.; Tong,\nS.; Chen, J.; and Meng, L. 2023. RewriteLM: An\nInstruction-Tuned Large Language Model for Text Rewrit-\ning. arXiv:2305.15685.\nTakase, S.; and Okazaki, N. 2019. Positional Encoding\nto Control Output Sequence Length. In Proceedings of\nNAACL-HLT, 3999–4004.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2022. Self-instruct: Align-\ning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nWei, J.; Bosma, M.; Zhao, V .; Guu, K.; Yu, A. W.; Lester, B.;\nDu, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned Language\nModels are Zero-Shot Learners. InInternational Conference\non Learning Representations.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nXu, B.; Yang, A.; Lin, J.; Wang, Q.; Zhou, C.; Zhang,\nY .; and Mao, Z. 2023a. ExpertPrompting: Instruct-\ning Large Language Models to be Distinguished Experts.\narXiv:2305.14688.\nXu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao,\nC.; and Jiang, D. 2023b. Wizardlm: Empowering large lan-\nguage models to follow complex instructions.arXiv preprint\narXiv:2304.12244.\nYang, K.; and Klein, D. 2021. FUDGE: Controlled Text\nGeneration With Future Discriminators. In Proceedings\nof the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, 3511–3535. Online: Association for\nComputational Linguistics.\nZhang, H.; and Song, D. 2022. DisCup: Discriminator Co-\noperative Unlikelihood Prompt-tuning for Controllable Text\nGeneration. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, 3392–\n3406.\nZhang, H.; Song, H.; Li, S.; Zhou, M.; and Song, D.\n2022. A survey of controllable text generation using\ntransformer-based pre-trained language models. arXiv\npreprint arXiv:2201.05337.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.;\nHou, Y .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al.\n2023. A survey of large language models. arXiv preprint\narXiv:2303.18223.\nZhou, W.; Jiang, Y . E.; Wilcox, E.; Cotterell, R.; and Sachan,\nM. 2023. Controlled text generation with natural language\ninstructions. arXiv preprint arXiv:2304.14293.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17816"
}