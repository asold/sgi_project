{
  "title": "Diabetic Retinopathy Image Classification Using Shift Window Transformer",
  "url": "https://openalex.org/W4386724829",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5079038963",
      "name": "Rasha Ali Dihin",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A5006461008",
      "name": "Waleed Al-Jawher",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019773053",
      "name": "Ebtesam N. AlShemmary",
      "affiliations": [
        "University of Kufa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2789816877",
    "https://openalex.org/W3158798816",
    "https://openalex.org/W2898395653",
    "https://openalex.org/W6789479594",
    "https://openalex.org/W3175311671",
    "https://openalex.org/W3202455182",
    "https://openalex.org/W2979927758",
    "https://openalex.org/W3166989437",
    "https://openalex.org/W3215327553",
    "https://openalex.org/W4378385037",
    "https://openalex.org/W3197929691",
    "https://openalex.org/W6810673825",
    "https://openalex.org/W2888400024",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W3205794738",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W6810814994",
    "https://openalex.org/W4283080861",
    "https://openalex.org/W4280490347",
    "https://openalex.org/W4282924992",
    "https://openalex.org/W4220815323",
    "https://openalex.org/W3103855452",
    "https://openalex.org/W3158333232",
    "https://openalex.org/W3199906704",
    "https://openalex.org/W4306686757",
    "https://openalex.org/W4378385319",
    "https://openalex.org/W4378385663",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4293433089",
    "https://openalex.org/W4312428231",
    "https://openalex.org/W4225912841",
    "https://openalex.org/W3121771238",
    "https://openalex.org/W4223589960"
  ],
  "abstract": "Diabetic retinopathy is one of the most dangerous complications for diabetic patients, leading to blindness if not diagnosed early. However, early diagnosis can control and prevent the disease from progressing to blindness. Transformers are considered state-of-the-art models in natural language processing that do not use convolutional layers. In transformers, means of multi-head attention mechanisms capture long-range contextual relations between pixels. For grading diabetic retinopathy, CNNs currently dominate deep learning solutions. However, the benefits of transformers, have led us to propose an appropriate transformer-based method to recognize diabetic retinopathy grades. A major objective of this research is to demonstrate that the pure attention mechanism can be used to determine diabetic retinopathy and that transformers can replace standard CNNs in identifying the degrees of diabetic retinopathy. In this study, a Swin Transformer-based technique for diagnosing diabetic retinopathy is presented by dividing fundus images into nonoverlapping batches, flattening them, and maintaining positional information using a linear and positional embedding procedure. Several multi-headed attention layers are fed into the resulting sequence to construct the final representation. In the classification step, the initial token sequence is passed into the SoftMax classification layer, which produces the recognition output. This work introduced the Swin transformer performance on the APTOS 2019 Kaggle for training and testing using fundus images of different resolutions and patches. The test accuracy, test loss, and test top 2 accuracies were 69.44%, 1.13, and 78.33%, respectively for 160*160 image size, patch size=2, and embedding dimension C=64. While the test accuracy was 68.85%, test loss: 1.12, and test top 2 accuracy: 79.96% when the patch size=4, and embedding dimension C=96. And when the size image is 224*224, patch size=2, and embedding dimension C=64, the test accuracy: 72.5%, test loss: 1.07, and test top 2 accuracy: 83.7%. When the patch size =4, embedding dimension C=96, the test accuracy was 74.51%, test loss: 1.02, and the test top 2 accuracy was 85.3%. The results showed that the Swin Transformer can achieve flexible memory savings. The proposed method highlights that an attention mechanism based on the Swin Transformer model is promising for the diabetic retinopathy grade recognition task.",
  "full_text": "International Journal of Innovative Computing 13(1-2) 23-29 \n23 \nDiabetic Retinopathy Image Classification Using \nShift Window Transformer\nRasha Ali Dihin \nDepartment of Computer Science \nUniversity of Kufa \nNajaf, Iraq \nEmail: rashaa.aljabry@uokufa.edu.iq \nWaleed A Mahmoud Al-Jawher \nDepartment of Electronic & Communication Eng. \n Uruk University \n Baghdad, Iraq \nEbtesam N AlShemmary \nIT Research and Development Center \n University of Kufa \nNajaf, Iraq \nSubmitted: 30/11/2022. Revised edition: 31/3/2023. Accepted: 31/3/2023. Published online: 13/9/2023 \nDOI: https://doi.org/10.11113/ijic.v13n1-2.415 \nAbstract‚ÄîDiabetic retinopathy is one of the most dangerous \ncomplications for diabetic patients, leading to blindness if not \ndiagnosed early. However, early diagnosis can control and prevent \nthe disease from progressing to blindness. Transformers are \nconsidered state-of-the-art models in natural language processing \nthat do not use convolutional layers. In transformers, means of \nmulti-head attention mechanisms capture long -range contextual \nrelations between pixels. For grading diabetic retinopathy, CNNs \ncurrently dominate deep learning solutions. However, the benefits \nof transformers, have led us to propose an appropriate \ntransformer-based method to recognize diabetic retinopathy \ngrades. A major objective of this research is to demonstrate that \nthe pure attention mecha nism can be used to determine diabetic \nretinopathy and that transformers can replace standard CNNs in \nidentifying the degrees of diabetic retinopathy. In this study, a \nSwin Transformer -based technique for diagnosing diabetic \nretinopathy is presented by div iding fundus images into \nnonoverlapping batches, flattening them, and maintaining \npositional information using a linear and positional embedding \nprocedure. Several multi-headed attention layers are fed into the \nresulting sequence to construct the final rep resentation. In the \nclassification step, the initial token sequence is passed into the \nSoftMax classification layer, which produces the recognition \noutput. This work introduced the Swin transformer performance \non the APTOS 2019 Kaggle for training and test ing using fundus \nimages of different resolutions and patches. The test accuracy, test \nloss, and test top 2 accuracies were 69.44%, 1.13, and 78.33%, \nrespectively for 160*160 image size, patch size=2, and embedding \ndimension C=64. While the test accuracy wa s 68.85%, test loss: \n1.12, and test top 2 accuracy: 79.96% when the patch size=4, and \nembedding dimension C=96. And when the size image is 224*224, \npatch size=2, and embedding dimension C=64, the test accuracy: \n72.5%, test loss: 1.07, and test top 2 accura cy: 83.7%. When the \npatch size =4, embedding dimension C=96, the test accuracy was \n74.51%, test loss: 1.02, and the test top 2 accuracy was 85.3%. The \nresults showed that the Swin Transformer can achieve flexible \nmemory savings. The proposed method highlig hts that an \nattention mechanism based on the Swin Transformer model is \npromising for the diabetic retinopathy grade recognition task. \nKeywords‚ÄîDiabetic retinopathy, Swin Transformers, Image \nclassification \nI. INTRODUCTION\nAs one of the leading causes of blindness worldwide, Diabetic \nRetinopathy (DR) has to be taken seriously. People with diabetes \nare expected to increase in number in the foreseeable future \nbecause of increased life expectancy, decadent lifestyles, and \nother causes [1]. Diabetic retinopathy is a frequent consequence \nand a leading cause of blindness in the general population. When \nblood sugar levels are adequately controlled, and therapy is \ngiven on time, many DR problems may be avoided. Medical \nprofessionals recommend that diabetic people be c hecked at \nleast twice a year since the condition progresses, and early \nindicators of sickness are challenging to detect [2,  3]. Diabetic \nMellitus, a condition of sugar metabolism, affects one in eleven \npeople worldwide, and by 2040, it is anticipated to affect one in \nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n24 \n \n10 people [4]. By 2045, this problem is expected to impact more \nthan half of the world's population, reaching 700 million [5, 6]. \nDiabetic retinopathy may progress through four stages: (i) \nnon-proliferative retinopathy's initial stage, microane urysms \n(MA), is linked to mild retinopathy. (ii) Non -proliferative \nretinopathy of moderate severity, as the condition progresses, \nthe retina's blood vessels may twist and expand, rendering them \nineffective in transporting blood. (iii) During severe non -\nproliferative retinopathy, the retina gets a signal to begin \ncreating new blood vessels when more blood vessels in the retina \nare blocked. (iv) Excessive glucose levels in the blood may \ncause diabetic retinopathy (PDR), a condition in which the \nretina's development characteristics are released and new blood \nvessels proliferate in the vitreous gel, flooding the eye, (Fig . 1, \nand Table 1) [7]. \n \n \nFig. 1.  Depicts a normal retina and the effects of diabetes on the retina \n \n \nTABLE I.  THE DR STAGES DEPENDING ON LESIONS \nCLASSIFICATION [7] \n \nDR ,Severity Level Lesions \nNo DR              No ,lesions. \nMild                DR Micro-aneurysms only. \n \nModerate       DR has more than MA and less than DR \nSevere              more than 20 intraretinal HM in each of 4 \nquadrants \nproliferative DR.       Proliferative DR One or more of the following: \nneovascularization, pre-retinal HM/ Vitreous \n \n \nAutomatic classification of DR plays a role in decision -\nmaking, ViT has gained great importance in the field of \ncomputer vision, however, studies include the use of computer \nvision transducers in the medical field. Vision transformer (ViT) \nwas proposed for  the first time to be used with machine \ntranslation tasks in natural language processing and had an \nadvanced performance in this field because it learns local and \nThe scalability of training with information is universal across \ndifferent layers, which is the opposite of CNN, which has very \nlimited sensory fields [8, 9]. Recently a pure transformer has \nbeen proposed called Shifted windows (Swin) Transformers. \nThis model consists of several blocks of Swin transformers, \nproviding a hierarchical representation of the input image which \nis then used in various computer vision tasks. The authors [10] \nachieved advanced performance in image classification, object \ndetection, and semantic segmentation [11]. The possibility of \napplying transformers in the field of view of the calculator such \nas image recognition demonstrates the success of ViT and Swin \nTransformer [8-12], the architecture of the swin that uses local \ncomputing via non -overlapping windows and thus achieves \nlinear complexity with image size O (M ‚àó N) instea d of the \ncomplexity found in ViT which is quadratic O (N2) and thus the \nswin can be more efficient [9]. Furthermore, they also linked \nblocks of pay attention to these patch merge blocks, which are \nused to combine adjacent patches to produce a hierarchical \nrepresentation to handle differences in the scale of visible \nentities [12]. The partitioning window in the second layer on the \nright is shifted by 2 image patches, resulting crossing in the \nboundary of the previous window [13].  \nThere are two important conc epts, hierarchical maps and \nwindow attention shifting, that the Swin Transformer introduces \nto solve problems for ViT. Where the name Swin Transformer \ncomes from \"Shifted window‚Äù. Fig. 2 shows the structure of the \nSwin Transformer. \n \nFig. 2.  The Architecture of the Swin Transformer \n \n \nArchitecture of Swin transformer consisting of 4 Stages: \n \nA. Patch Partitioning \n \nThe primary role of the Patch Partition is to convert the input \nimage into correction blocks, where each of these blocks \nconsists of four adjacent pixels. Where the image entered into \nPatch Partition, each (4√ó4) adjacent pixel is divided into a patch, \nwhere e ach (4 √ó4) and three channels of the color image are \nconverted into patches (1√ó1) that are flat and have (48) channel \nas shown in Fig. 3. Since the size of each patch is 4 √ó 4 so the \nnumber of pixels per patch will be 4 √ó 4 = 16 in a flat shape, so \nthat each of these pixels contains three values of R, G, and B, so \nit's 16 √ó 3 = 48 as show in Fig. 4. \n \n \nFig. 3.  Patch partition \n\nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n25 \n \n \nFig. 4.  Image from Patch partition \n \n \nThe input image passes through patch partition) which is \ndivided into ( 4√ó4) sized falls. This helps in creating debug \nsymbols,  where after patch partition the shape of the image is \nchanged to be whose shape is (W/4, H/4, √ó channel) = (W/4, \nH/4, √ó 3).  \n \nB. Linear Embedding  \n \nLinear Embedding performed after the patch partition step, \nwhich implement a linear transformation on the image channel. \nThis step is executed to convert the input tokens and the output \ntokens to vectors as well as. The feature in this layer looks like \na convolution layer, the number of channels in which the \nconvolution mapping is done is from 48 dimensions to 96 \ndimensions. On the application side, patch embed is used to \ncombine steps patch, partition and linear mmbedding where \nconvolution, kernel is used. \n \nC. Swin Transformer Block \n \nSwin Transformer is designed by replacing the multi -head \nMSA to a switched window -based module with a switched \nwindow-based module, this layer has been swapped but the rest \nof the layers remain without any replacement as shown in Fig . \n5. \n \n \nFig. 5.  Swin transformer blocks \n \nThe Swin transformer block contains two sub -modules, \nwhere the first module uses W1-MSA and in the second module \n‚ÄúSW-1MSA‚Äù is used. Each of these sub -layers contains a \nnormalization layer [14-16]. Each W-MSA layer is followed by \na 2-layer MLP with GELU nonlinearity between them wher e \nLN is applied before both MSA and MLP.Patch tokens pass \nthrough the linear embedding with size image (W/4, H/4, C) \nand are referred as ‚ÄúStage 1‚Äù. The number of 2 * 2 patch tokens \nis reduced and the shape of the tokens is (W/8, H/8, 2C) is \ndenoted as ‚ÄúSta ge 2‚Äù, (W/16, H/16, 4C) as ‚ÄúStage 3‚Äù, and \n(W/32, H/32, 8C) as ‚ÄúStage 4‚Äù respectively. Equations (1 -4) \nstate the mathematical expression of W-MSA and SW-MSA as \nfollows [17]: \n \nùëß‚àßùëô = ùëä ‚àíùëÄùëÜùê¥(ùêøùëÅ(ùëßùëô‚àí1))+ùëßùëô‚àí1 (1) \n  \nùëßùëô = ùëÄùêøùëÉ(ùêøùëÅ(ùëßùëô‚àí1))+ùëß‚àßùëô  (2) \n                         \nùëß‚àßùëô+1 = ùëÜùëä ‚àíùëÄùëÜùê¥(ùêøùëÅ(ùëßùëô))+ùëßùëô  (3) \n \n ùëßùëô+1 = ùëÄùêøùëÉ(ùêøùëÅ(ùëß‚àßùëô+1))+ùëß‚àßùëô+1  (4) \n              \nWhere (z ^ l) is the item in the current block, (zl ‚Äì 1) is the item \nin the previous block, (LN) is layer-norm, (MLP) is multi-layer \nperceptron, (W1-1MSA) is window self -attention, and (SW1 -\n1MSA) is shift window self-attention [15]. \n \nD. Patch Merging \n \nThe main function  of patch merging is to reduce both the \nheight and width of the image by sampling downwards to reduce \nthe accur acy. This is before the start of each phase and this \nfunction is equivalent to the down sampling process that is in \nCNN. In the patch merging stage, each (2 √ó2) of the adjacent \npixels is divided into a patch, and the pixels of the same colour \nare grouped to get 4 feature maps.  These four feature maps are \nlinked in the depth direction, the output from here being passed \nthrough a layer (LN) and layer (FC) is use d to linearly change \nthe depth direction of the feature map, whereby the feature map \ndepth is changed from C to C/2 as shown in Fig. 6. \n \n \nFig. 6.  Patch Merging \n \n \nII. RELATED WORK \n \nCao et al . proposed a Swin -Unet, where the Swin \nTransformer block used as a basic unit in building the model for \nsegmentation of medical images. A benchmarking comparison \n\nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n26 \n \nwas presented for the development of transformers in the field \nof medical image, where the (Swin-Unet) has a patch expansion \nlayer to map the decoder's feature maps and sho ws superior \nperformance in restoring fine details compared to binary down \nsampling [8]. \nHuang et al . suggested a SwinMR which is a new Swin -\nbased method used for rapid MRI reconstruction. The built \nmodel was consisting of an input unit (IM), a feature extraction \nunit (FEM), and an output unit (OM). When applying SwinMR, \nit achieved excellent results for high reconstruction. Quality \ncompared to other measurement methods under noise \ninterruption and on different data sets [17].  \nJiang et al. contemplated a new  approach called SwinBTS \nfor 3D medical image segmentation. This approach combines a \nTransformer, convolutional neural network, and decoder \narchitecture to determine the 3D brain tumor semantic \nsegmentation. Experimental results showed that the method \ngives better performance in segmenting MRI images of a brain \ntumor when compared with some state -of-the-art methods (for \nexample ‚ÄúResidual U -Net‚Äù, ‚ÄúAttention U -Net‚Äù, and \n‚ÄúTransBTS‚Äù) [18-21]. \nHao et al. Which is a Swin converter that is two-stream and \nused in the classification of remote sensing images, and TSTNet \nconsists of two vertical parts which are the original stream and \nthe second part is the edge stream, and through them the features \nare combined, and it has achieved good performance and also \ndesigned a driver unit called DESOM based on Sobel To extract \nthe features in the edge and thus give us a better rating [22]. \n \nIII. EXPERIMENTS SETTINGS  \n \nA. Datasets \n \nThe APTOS 2019 (Asia Pacific Teleophthalmology Society) \nKaggle benchmark dataset This rule contains images of the \nretina that were taken using fundus imaging, and the conditions \nthat were used in the imaging were very diverse, and this rule \nwas used in the challenge of detecting blindness This data has \nbeen manually classified by specialists int o 5 classes (0 to 4) \nwhere ‚Äú0‚Äù means no DR; ‚Äú1‚Äù means Mild 1; ‚Äú2‚Äù means  \n1Moderate; ‚Äú3‚Äù means  Severe 1; and ‚Äú4‚Äù means Proliferative 1 \nDR2) to indicate different severity levels of DR [23]. Table II \nshows the number of retinal images in the dataset to indicate the \nlevel1 of meverity, [24-30]. \n \nTABLE II.  DATASET SUMMARY OF APTOS DATASET \n \nSeverity level Number of images \nClass 00 (Normal1) 1805 \nClass 1(Mild1) 370 \nClass 22 (Moderate1) 999 \nClass 33 (1Severe) 193 \nClass 41 (Proliferative1) 295 \nTotal1  36621 \n \n \nB. Implementation details \n \nUsed Swin transformer to automatically recognize Diabetic \nRetinopathy progression level because the Swin transformer can \nachieve flexible memory savings. Where used GPU with 5.71 \nGB memory/12.68 GB Disk, we set the input image size as 160 \n√ó160 and 224 √ó 22 4. Here in the Table III below is the \ncombination of hyperparameters we are chosen after carefully \ntuning each of them across a wide range of values.  \nTable IV, Fig . 7 and Fig . 8 shows the loss and accuracy \nacross the training and validation process with input image size \n160 √ó160 when Patch size =2, C=64, Patch size =4, C=96 and \nTable V, Fig. 9 and Fig. 10 shows the loss and accuracy across \nthe training and validation process wi th input image size 224 \n√ó224 when Patch size =2, C=64, Patch size =4, C=96. \n \nTABLE III.  HYPER-PARAMETERS IN SWIN TRANSFORMER TRAINING \n \nHyper-parameter Value  \nBatch Size 16  \nLearning Rate 0.05  \nSize of shifting window 1  \nSize of attention window 2  \nEpoch 80  \nWeight decay 1e-3  \noptimizer Adam  \nPatch size 2 4 \nEmbedded dimension  64 96 \n# Param 987,381 567,445 \n \nTABLE IV.  CLASSIFICATION ACCURACY AND LOSS (IMAGE SIZE (160*160) \nEpoch \n \nPatch size =2, C=64 Patch size =4, C=96 \nTrain- \nAcc  \nTrain -\nloss \ntop-2-\nacc \nVal - \nAcc  \nVal-loss Val-\ntop-2-\nacc \nTrain- \nAcc  \nTrain -\nloss \ntop-2-\nacc \nVal - \nAcc  \nVal-loss Val-\ntop-2-\nacc \n1 0.4564 11.7449 0.6646 0.4125 8.2687 0.4669 0.4537 23.350 0.6786 0.6786 3.7794 0.7857 \n10 0.6612 1.1450 0.7935 0.5992 1.3052 0.7860 0.6914 1.0644 0.8272 0.6865 1.0510 0.8095 \n20 0.7059 1.0276 0.8312 0.6498 1.1441 0.7821 0.7381 0.9463 0.8690 0.7421 0.9478 0.8611 \n30 0.7323 0.9323 0.8716 0.6459 1.1282 0.8132 0.8961 0.8961 0.8832 0.6905 1.0901 0.7778 \n40 0.7488 0.9079 0.8781 0.6498 1.0653 0.7665 0.8100 0.8016 0.9114 0.6984 1.0232 0.8135 \n50 0.7714 0.8586 0.8924 0.6654 1.1918 0.7977 0.8395 0.7601 0.9290 0.7024 1.0530 0.8373 \n60 0.8208 0.7717 0.9150 0.6654 1.2200 0.7704 0.8395 0.7513 0.9317 0.7063 1.0252 0.8294 \n70 0.8499 0.7265 0.9380 0.6732 1.2898 0.7588 0.8642 0.7142 0.9484 0.6905 1.1238 0.8492 \n80 0.8803 0.6785 0.9458 0.6576 1.2626 0.7743 0.8849 0.6690 0.9563 0.6984 1.0851 0.7976 \n  \nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n27 \n \n \nFig. 7.  Training and validation over epoch for APTOS 2019 dataset, (a) \naccuracy, (b) loss, (epochs=80), image size (160√ó160), patch size =2, C=64 \n \nFig. 8.  Training and validation over epoch for APTOS 2019 dataset, (a) \naccuracy, (b) loss, (epochs=80), image size (160√ó160), patch size =4, C=96 \n \n \nTABLE V.  CLASSIFICATION ACCURACY AND LOSS (IMAGE SIZE (224*224) \n \nEpoch \n \nPatch size =2, C=64 Patch size =4, C=96 \nTrain- \nAcc  \nTrain -\nloss \ntop-2-\nacc \nVal - \nAcc  \nVal-\nloss \nVal-\ntop-2-\nacc \nTrain- \nAcc  \nTrain -\nloss \ntop-2-\nacc \nVal - \nAcc  \nVal-loss Val-top-\n2-acc \n1 0.5518 5.7511 0.6460 0.6187 4.4559 0.4280 0.4684 26.3868 0.6667 0.4757 7.6186 0.7431 \n10 0.6993 1.0575 0.8325 0.5759 1.2574 0.7549 0.6987 1.0220 0.8387 0.6389 1.0630 0.8507 \n20 0.7128 0.9727 0.8521 0.7043 1.0055 0.8210 0.7184 0.9704 0.8642 0.7604 0.9229 0.8681 \n30 0.7401 0.9212 0.8725 0.7198 0.9782 0.8171 0.7357 0.9529 0.8611 0.7604 0.9818 0.8229 \n40 0.7761 0.8509 0.9007 0.7082 1.0046 0.8132 0.7828 0.8561 0.8939 0.7222 1.0437 0.8438 \n50 0.7879 0.8458 0.8998 0.6887 1.0596 0.8560 0.8233 0.7796 0.9240 0.7674 0.9862 0.8576 \n60 0.8204 0.7826 0.9150 0.7043 1.0609 0.8171 0.8465 0.7415 0.8465 0.7326 0.9955 0.8438 \n70 0.8811 0.6802 0.9423 0.6965 1.1076 0.8288 0.8623 0.7167 0.9444 0.7361 1.1023 0.8299 \n80 0.8810 0.6801 0.9393 0.7043 1.0609 0.8289 0.8781 0.6865 0.9533 0.7500 1.0727 0.8750 \n \nFig. 9.  Training and validation over epoch for APTOS 2019 dataset, (a) \naccuracy, (b) loss, (epochs=80), image size (224√ó224), patch size =2, C=64 \n \nFig. 10.  Training and validation over epoch for APTOS 2019 dataset, (a) \naccuracy, (b) loss, (epochs=80), image size (224√ó224), patch size =4, C=96. \nIV. DISCUSSION \n \nWe tested  model on APTOS -2019 Blindness Detection \ndataset  whose data distribution between different classes is \nunbalanced, pre-training the model affects the performance of \nthe transformer -dependent model . In this work, we use two \ndifferent input size of image  (160*160) and (2248224) , patch \nsize is 2 and 4, and embedding dimension is 64  and 96. In the \ncase of selecting the patch size 4 and embedding dimension 96, \nthe model  give performance better than the patch size 2 and \nembedding dimension 64 for each input image sizes where the \nnumber of parameters in this case was 567,445 but in the second \ncase was 987,381. Other parameters can be changed, as well as \nthe change in the Swin transformer block to see its effect on the \nresults. \n \nV. CONCLUSION \n \nTransformer has made great technical advances in deep \nlearning and has achieved wide spread in the field of NLP and \nCV, because medical imaging is very similar to ‚ÄúCV‚Äù. Since  \nthe Swin transformer has great flexibility in modelling and its \ncomputational complexity is linear complexity proportional to \nthe size of the image. Thi s work used the Swin transform on \nAPTOS 2019 Kaggle. It was also used to classify the DR to 5 \nclass, The results demonstrated that the Swin transform \n\nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n28 \n \nperforms well in classifying DR with linear computational \ncomplexity as compared  to the ViT transformer's quadratic \ncomputational complexity. Test accuracy for size image 160 \n*160, patch size =2, embedding dimension C=64 was 69.44%, \nthe Test loss: 1.13, and Test top 2 accuracy: 78.33%, while \nwhen the patch size =4, embedding dimension C=96, the Test \nloss was 1.12, Test accuracy: 68.85% and Test top 2 accuracy: \n79.96%.  For the size image 224*224, patch size =2, embedding \ndimension C=64 the Test loss: 1.07, Test accuracy: 72.5% and \nTest top 2 accuracy: 83.7%, while when the patch size =4, \nembedding dimension C=96 the Test loss: 1.02, Test accuracy: \n74.51% and Test top 2 accuracy: 85.3%. From the results we \ncan conclude that it is possible to change other parameters, \nchanging the Swin transformer block or combining it with deep \nlearning algorithms that can give better results for automatically \nDiabetic Retinopathy recognition. Since the architecture of \ntransformers is still quite new together in the field of CV and It \nis possible in the future to discover other variants of the \nattention layers, and also plan to use the hypercomplex to detect \nthe mixing of symbols. \n \nACKNOWLEDGMENTS \n \nThe research behind this paper would not have been \npossible without the exceptional support of Prof. Ebtesam \nAlShemmary and Prof. Waleed AlJawher. Throughout \nresearching image processing journals and writing this paper, \nthey have inspired me with them enthusiasm, knowledge, and \nattention to detail.  \n \nREFERENCES \n \n[1]  Li, X., Pang, T., Xiong, B., Liu, W., Liang, P., & Wang, T. \n(2017, October). Convol utional neural networks based transfer \nlearning for diabetic retinopathy fundus image classification. \n2017 10th  International Congr ess on Image and Signal \nProcessing, Biomedical Engineeri ng and Informatics (CISP -\nBMEI) (pp. 1-11). IEEE. \n[2] Waheed, S. R., Suaib, N. M., Rahim, M. S. M., Adnan, M. M., \n& Salim, A. A. (2021, April). Deep learning algorithms-based \nobject detection and localization revisit ed. Journal of Physics: \nConference Series  (Vol. 1892, No. 1, p. 012001). IOP \nPublishing. \n[3] Salim, A. A., Ghoshal, S. K., Suan, L. P., Bidin, N., Hamzah, K., \nDuralim, M., & Bakhtiar, H. (2018). Liquid media regulated \ngrowth of cinnamon nanoparticles: Absorption and emission \ntraits. Malaysian Journal of Fundamental and Applied Sciences, \n14(3-1), 447-449. \n[4] Adnan, M. M., Rahim, M. S. M., Al -Jawaheri, K., Ali, M. H., \nWaheed, S. R., & Radie, A. H. (2020, September). A survey and \nanalysis on image annotation. 2020 3rd International \nConference on Engineering Technology and its Applications \n(IICETA) (pp. 203-208). IEEE. \n[5] Tsiknakis, N., Theodoropoulos, D., Manikis, G., Ktistakis, E., \nBoutsora, O., Berto, A., ... & Marias, K. (2021). Deep learning \nfor diabetic retinopathy detection and classification based on \nfundus images: A review.  Computers in Biology and Medicine, \n135, 104599. \n[6] Salim, A. A., Ghoshal, S. K., & Bakhtiar, H. (2022). Prominent \nabsorption and luminescence characteristics of novel silver -\ncinnamon core -shell nanoparticles prepared in ethanol using \nPLAL method. Radiation Physics and Chemistry, 190, 109794. \n[7] Jiang, H., Yang, K., Gao, M., Zhang, D., Ma, H., & Qian, W. \n(2019, July). An interpretable ensemble deep learning model for \ndiabetic retinopathy disease classification. 2019 41st Annual \nInternational Conference of the IEEE Engineering in Medicine \nand biology Society (EMBC) (pp. 2045-2048). IEEE. \n[8] Abbas, S. I., Hathot, S. F., Abbas, A. S., & Salim, A. A. (2021). \nInfluence of Cu doping on structure, morphology and optical \ncharacteristics of SnO 2 thin films prepared by chemical bath \ndeposition technique. Optical Materials, 117, 111212. \n[9] Tang, Y., Yang, D., Li, W., Roth, H. R., Landman, B., Xu, D., \n... & Hatamizadeh, A. (2022). Self -supervised pre -training of \nswin transformers for 3d medical image analysis. Proceedings of \nthe IEEE/CVF Conference on Computer Vi sion and Pattern \nRecognition (pp. 20730-20740). \n[10] Waheed, S. R., Sakran, A. A., Rahim, M. S. M., Suaib, N. M., \nNajjar, F. H., Kadhim, K. A., Salim A. A. & Adnan, M. M. \n(2023). Design a crime detection system based fog c omputing \nand IoT.  Malaysian Journal of Fundamental and Applied \nSciences, 19(3), 345-354.  \n[11] Nguyen, C., Asad, Z., Deng, R., & Huo, Y. (2022, April). \nEvaluating transformer-based semantic segmentation networks \nfor pathological image segmentation. Medical Imaging 2022: \nImage Processing (Vol. 12032, pp. 942-947). SPIE. \n[12] He, K., Gan, C., Li, Z., Rekik , I., Yin, Z., Ji, W., ... & Shen, D. \n(2022). Transformers in medical image analysis: A review.  \nIntelligent Medicine. \n[13] Salim, A. A., Bakhtiar, H., Bidin, N., & Ghoshal, S. K. (2018). \nUnique attributes of spherical cinnamon nanoparticles produced \nvia PLAL te chnique: Synergy between methanol media and \nablating laser wavelength. Optical Materials, 85, 100-105.  \n[14] Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. R., & \nXu, D. (2022, July). Swin unetr: Swin transformers for semantic \nsegmentation of brain tumo rs in mri images. Brainlesion: \nGlioma, Multiple Sclerosis, Stroke and Traumatic Brain \nInjuries: 7th International Workshop, BrainLes 2021, Held in \nConjunction with MICCAI 2021, Virtual Event, September 27, \n2021, Revised Selected Papers, Part I  (pp. 272 -284). Cham: \nSpringer International Publishing. \n[15] Aljewaw, O. B., Karim, M. K. A., Kamari, H. M., Zaid, M. H. \nM., Salim, A. A., & Mhareb, M. H. A. (2022). Physical and \nspectroscopic characteristics of lithium-aluminium-borate glass: \nEffects of varying Nd 2O3 doping contents.  Journal of Non -\nCrystalline Solids, 575, 121214. \n[16] Lin, A., Chen, B., Xu, J., Zhang, Z., Lu, G., & Zhang, D. (2022). \nDs-transunet: Dual swin transformer u -net for medical image \nsegmentation. IEEE Transactions on Instrumentation and \nMeasurement, 71, 1-15. \n[17] Huang, J., Fang, Y., Wu, Y., Wu, H., Gao, Z., Li, Y., ... & Yang, \nG. (2022). Swin transformer for fast MRI. Neurocomputing, 493, \n281-304. \n[18] Jiang, Y., Zhang, Y., Lin, X., Dong, J., Cheng, T., & Liang, J. \n(2022). SwinBTS: A method for 3 D multimodal brain tumor \nsegmentation using swin transformer.  Brain Sciences , 12(6), \n797. \n[19] Jiang, Y., Zhang, Y., Lin, X., Dong, J., Cheng, T., & Liang, J. \n(2022). SwinBTS: A method for 3D multimodal brain tumor \nsegmentation using swin transformer.  Brain Sci ences, 12(6), \n797. \n[20] Aldhuhaibat, M. J., Amana, M. S., Aboud, H., & Salim, A. A. \n(2022). Radiation attenuation capacity improvement of various \noxides via high density polyethylene composite reinforcement.  \nCeramics International, 48(17), 25011-25019. \n[21] Al-Jawher, W. A. M., & Awad, S. H. (2022). A proposed brain \ntumor detection algorithm using Multi wavelet Transform \n(MWT). Materials Today: Proceedings, 65, 2731-2737. \nRasha Ali Dihin et al. / IJIC Vol. 13 No. 1-2 (2023) 23-29 \n \n29 \n \n[22] Hao, S., Wu, B., Zhao, K., Ye, Y., & Wang, W. (2022). Two -\nstream swin transformer with different iable sobel operator for \nremote sensing image classification.  Remote Sensing , 14(6), \n1507. \n[23] Waheed, S. R., Rahim, M. S. M., Suaib, N. M., & Salim, A. A. \n(2023). CNN deep learning -based image to vector depiction.  \nMultimedia Tools and Applications, 1-20. \n[24] Bodapati, J. D., Naralasetti, V., Shareef, S. N., Hakak, S., Bilal, \nM., Maddikunta, P. K. R., & Jo, O. (2020). Blended multi-modal \ndeep convnet features for diabetic retinopathy severity \nprediction. Electronics, 9(6), 914. \n[25] Abbas, A. M., Abid, M. A., Abbas, K. N., Aziz, W. J., & Salim, \nA. A. (2021, April). Photocatalytic activity of Ag -ZnO \nnanocomposites integrated essential ginger oil fabricated by \ngreen synthesis method. Journal of Physics: Conference Series  \n(Vol. 1892, No. 1, p. 012005). IOP Publishing. \n[26] Hathot, S. F., Jubier, N. J., Hassani, R. H., & Salim, A. A. (2021). \nPhysical and elastic properties of TeO 2-Gd2O3 glasses: Role of \nzinc oxide contents variation. Optik, 247, 167941. \n[27] Salim, A. A., Bakhtiar, H., Shamsudin, M.  S., Aziz, M. S., \nJohari, A. R., & Ghoshal, S. K. (2022). Performance evaluation \nof rose bengal dye -decorated plasmonic gold nanoparticles -\ncoated fiber-optic humidity sensor: A mechanism for improved \nsensing. Sensors and Actuators A: Physical, 347, 113943. \n \n[28] Kadhim, K. A., Najjar, F. H., Waad, A. A., Al -Kharsan, I. H., \nKhudhair, Z. N., & Salim, A. A. (2023). Leukemia classification \nusing a convolutional neural network of AML Images.  \nMalaysian Journal of Fundamental and Applied Sciences, 19(3), \n306-312. \n[29] Waheed, S. R., Saadi, S. M., Rahim, M. S. M., Suaib, N. M., \nNajjar, F. H., Adnan, M. M., & Salim, A. A. (2023). Melanoma \nskin cancer classification based on CNN Deep learning \nalgorithms. Malaysian Journal of Fundamental and Applied \nSciences, 19(3), 299-305. \n[30] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, \nB. (2021). Swin transformer: Hierarchical vision transformer \nusing shifted windows. Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision  (pp. 10012 -\n10022). \n \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6200039386749268
    },
    {
      "name": "Diabetic retinopathy",
      "score": 0.5878673195838928
    },
    {
      "name": "Softmax function",
      "score": 0.57109534740448
    },
    {
      "name": "Artificial intelligence",
      "score": 0.566286027431488
    },
    {
      "name": "Transformer",
      "score": 0.5509178638458252
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47783470153808594
    },
    {
      "name": "Retinopathy",
      "score": 0.4776122272014618
    },
    {
      "name": "Deep learning",
      "score": 0.38569343090057373
    },
    {
      "name": "Medicine",
      "score": 0.355368435382843
    },
    {
      "name": "Diabetes mellitus",
      "score": 0.12881186604499817
    },
    {
      "name": "Engineering",
      "score": 0.09877020120620728
    },
    {
      "name": "Voltage",
      "score": 0.08026111125946045
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47229656",
      "name": "University of Kufa",
      "country": "IQ"
    }
  ],
  "cited_by": 11
}