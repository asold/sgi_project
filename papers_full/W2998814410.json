{
  "title": "Streaming Automatic Speech Recognition with the Transformer Model",
  "url": "https://openalex.org/W2998814410",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1735259561",
      "name": "Niko Moritz",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1988701769",
      "name": "Takaaki Hori",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2103126764",
      "name": "Jonathan Le",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1735259561",
      "name": "Niko Moritz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988701769",
      "name": "Takaaki Hori",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103126764",
      "name": "Jonathan Le",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6746345256",
    "https://openalex.org/W6747158283",
    "https://openalex.org/W2936123380",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W3007073761",
    "https://openalex.org/W2973166416",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W6739366949",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6638749077",
    "https://openalex.org/W3007227084",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W2750499125",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W6637516147",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2964272710",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W1710082047",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962742956",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Encoder-decoder based sequence-to-sequence models have demonstrated state-of-the-art results in end-to-end automatic speech recognition (ASR). Recently, the transformer architecture, which uses self-attention to model temporal context information, has been shown to achieve significantly lower word error rates (WERs) compared to recurrent neural network (RNN) based system architectures. Despite its success, the practical usage is limited to offline ASR tasks, since encoder-decoder architectures typically require an entire speech utterance as input. In this work, we propose a transformer based end-to-end ASR system for streaming ASR, where an output must be generated shortly after each spoken word. To achieve this, we apply time-restricted self-attention for the encoder and triggered attention for the encoder-decoder attention mechanism. Our proposed streaming transformer architecture achieves 2.8% and 7.2% WER for the \"clean\" and \"other\" test data of LibriSpeech, which to our knowledge is the best published streaming end-to-end ASR result for this task.",
  "full_text": "STREAMING AUTOMATIC SPEECH RECOGNITION WITH THE TRANSFORMER MODEL\nNiko Moritz, Takaaki Hori, Jonathan Le Roux\nMitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA\nABSTRACT\nEncoder-decoder based sequence-to-sequence models have demon-\nstrated state-of-the-art results in end-to-end automatic speech recog-\nnition (ASR). Recently, the transformer architecture, which uses\nself-attention to model temporal context information, has been\nshown to achieve signiÔ¨Åcantly lower word error rates (WERs) com-\npared to recurrent neural network (RNN) based system architectures.\nDespite its success, the practical usage is limited to ofÔ¨Çine ASR\ntasks, since encoder-decoder architectures typically require an entire\nspeech utterance as input. In this work, we propose a transformer\nbased end-to-end ASR system for streaming ASR, where an output\nmust be generated shortly after each spoken word. To achieve this,\nwe apply time-restricted self-attention for the encoder and triggered\nattention for the encoder-decoder attention mechanism. Our pro-\nposed streaming transformer architecture achieves 2.8% and 7.2%\nWER for the ‚Äúclean‚Äù and ‚Äúother‚Äù test data of LibriSpeech, which\nto our knowledge is the best published streaming end-to-end ASR\nresult for this task.\nIndex Terms‚Äî automatic speech recognition, streaming, end-to-\nend, transformer, triggered attention\n1. INTRODUCTION\nHybrid hidden Markov model (HMM) based automatic speech\nrecognition (ASR) systems have provided state-of-the-art results\nfor the last few decades [1, 2]. End-to-end ASR systems, which\napproach the speech-to-text conversion problem using a single\nsequence-to-sequence model, have recently demonstrated competi-\ntive performance [3]. The most popular and successful end-to-end\nASR approaches are based on connectionist temporal classiÔ¨Åcation\n(CTC) [4], recurrent neural network (RNN) transducer (RNN-T) [5],\nand attention-based encoder-decoder architectures [6]. RNN-T\nbased ASR systems achieve state-of-the-art ASR performance for\nstreaming/online applications and are successfully deployed in pro-\nduction systems [7, 8]. Attention-based encoder-decoder architec-\ntures, however, are the best performing end-to-end ASR systems [9],\nbut they cannot be easily applied in a streaming fashion, which pre-\nvents them from being used more widely in practice. To overcome\nthis limitation, different methods for streaming ASR with attention-\nbased systems haven been proposed such as the neural transducer\n(NT) [10], monotonic chunkwise attention (MoChA) [11], and\ntriggered attention (TA) [12]. The NT relies on traditional block\nprocessing with Ô¨Åxed window size and stride to produce incremen-\ntal attention model outputs. The MoChA approach uses an extra\nlayer to compute a selection probability that deÔ¨Ånes the length of\nthe output label sequence and provides an alignment to chunk the\nencoder state sequence prior to soft attention. The TA system re-\nquires that the attention-based encoder-decoder model is trained\njointly with a CTC objective function, which has also been shown\nto improve attention-based systems [13], and the CTC output is\nused to predict an alignment that triggers the attention decoding pro-\ncess [12]. A frame-synchronous one-pass decoding algorithm for\njoint CTC-attention scoring was proposed in [14] to further optimize\nand enhance ASR decoding using the TA concept.\nBesides the end-to-end ASR modeling approach, the underlying\nneural network architecture is of paramount importance as well to\nachieve good ASR performance. RNN-based architectures, such\nas the long short-term memory (LSTM) neural network, are of-\nten applied for end-to-end ASR systems. Bidirectional LSTMs\n(BLSTMs) achieve state-of-the-art results among such RNN-based\nsystems but are unsuitable for application in a streaming fashion,\nwhere unidirectional LSTMs or latency-controlled BLSTMs (LC-\nBLSTMs) must be applied instead [15]. The parallel time-delayed\nLSTM (PTDLSTM) architecture has been proposed to further re-\nduce the word error rate (WER) gap between unidirectional and\nbidirectional architectures and to improve the computational com-\nplexity compared to the LC-BLSTM [15]. Recently, the transformer\nmodel, which is an encoder-decoder type of architecture based on\nself-attention originally proposed for machine translation [16], has\nbeen applied to ASR with promising results and improved WERs\ncompared to RNN-based architectures [17].\nIn this work, we apply time-restricted self-attention to the encoder,\nand the TA concept to the encoder-decoder attention mechanism of\nthe transformer model to enable the application of online/streaming\nASR. The transformer model is jointly trained with a CTC objective\nto optimize training and decoding results as well as to enable the\nTA concept [3, 12]. For joint CTC-transformer decoding and scor-\ning, we employ the frame-synchronous one-pass decoding algorithm\nproposed in [14].\n2. STREAMING TRANSFORMER\nThe streaming architecture of the proposed transformer-based ASR\nsystem is shown in Fig. 1. The transformer is an encoder-decoder\ntype of architecture that uses two different attention layers: encoder-\ndecoder attention and self-attention. The encoder-decoder attention\ncan produce variable output lengths by using one or multiple query\nvectors, the decoder states, to control attention to a sequence of in-\nput values, the encoder state sequence. In self-attention (SA), the\nqueries, values, and keys are derived from the same input sequence,\nwhich results in an output sequence of the same length. Both at-\ntention types of the transformer model are based on the scaled dot-\nproduct attention mechanism,\nAttention(Q,K,V ) = Softmax\n(QKT\n‚àödk\n)\nV, (1)\nwhere Q ‚àà Rnq√ódq , K ‚àà Rnk√ódk , and V ‚àà Rnv√ódv are the\nqueries, keys, and values, where the d‚àódenote dimensions and the\nn‚àódenote sequence lengths, dq = dk, and nk = nv [16]. Instead\nof using a single attention head, multiple attention heads are used by\narXiv:2001.02674v5  [cs.SD]  30 Jun 2020\n‚ãØ\n‚ãØ\nCTC\n‚ãØ‚ãØ\nTriggered Attention \nDecoder\ny1 y2 y3 yl‚ãØ\nRNN-LM\n‚ãØ ‚ãØ\nInput features: X = (x1,‚Ä¶, xT)\nshallow fusion\nEncSA: Time-restricted self-attention\nEncCNN: 2-layer CNN\nEncoder\nJoint Decoding\nùíô1\nùê∏ ùíôùëõ‚Ä≤ùëô\nùê∏ùíô2\nùê∏ ùíô3\nùê∏ ùíô4\nùê∏ ùíô5\nùê∏ ùíôùúàùëô\nùê∏ ùíôùëÅ\nùê∏\nùíö1:ùëô‚àí1\nFig. 1. Joint CTC-TA decoding scheme for streaming ASR with a\ntransformer-based architecture.\neach layer of the transformer model with\nMHA( ÀÜQ, ÀÜK, ÀÜV) = Concat(Head1,..., Headdh )WH (2)\nand Headi = Attention( ÀÜQWQ\ni , ÀÜKWK\ni ,ÀÜVW V\ni ), (3)\nwhere ÀÜQ, ÀÜK, and ÀÜV are inputs to the multi-head attention (MHA)\nlayer, Headirepresents the output of thei-th attention head for a total\nnumber of dh heads, and WQ\ni ‚ààRdmodel√ódq , WK\ni ‚ààRdmodel√ódk ,\nWV\ni ‚ààRdmodel√ódv as well as WH ‚ààRdhdv√ódmodel are trainable\nweight matrices with typically dk = dv = dmodel/dh.\n2.1. Encoder: Time-restricted self-attention\nThe encoder of our transformer architecture consists of a two-layer\nCNN module E NCCNN and a stack of E self-attention layers\nENCSA:\nX0 = ENCCNN(X), (4)\nXE = ENCSA(X0), (5)\nwhere X = (x1,..., xT) denotes a sequence of acoustic input fea-\ntures, which are 80-dimensional log-mel spectral energies plus 3 ex-\ntra features for pitch information [18]. Both CNN layers of E NC-\nCNN use a stride of size 2, a kernel size of 3 √ó3, and a ReLU ac-\ntivation function. Thus, the striding reduces the frame rate of output\nsequence X0 by a factor of 4 compared to the feature frame rate of\nX. The E NCSA module of (5) consists of E layers, where the e-th\nlayer, for e= 1,...,E , is a composite of a multi-head self-attention\nlayer\nX‚Ä≤\ne = Xe‚àí1 + MHAe(Xe‚àí1,Xe‚àí1,Xe‚àí1), (6)\nand two feed-forward neural networks of inner dimension dÔ¨Ä and\nouter dimension dmodel that are separated by a ReLU activation\nfunction as follows:\nXe = X‚Ä≤\ne + FFe(X‚Ä≤\ne), (7)\nwith FFe(X‚Ä≤\ne) = ReLU(X‚Ä≤\neWÔ¨Ä\ne,1 + bÔ¨Ä\ne,1)WÔ¨Ä\ne,2 + bÔ¨Ä\ne,2, (8)\nwhere WÔ¨Ä\ne,1 ‚ààRdmodel√ódff , WÔ¨Ä\ne,2 ‚ààRdff √ódmodel , bÔ¨Ä\ne,1 ‚ààRdff , and\nbÔ¨Ä\ne,2 ‚ààRdmodel are trainable weight matrices and bias vectors.\nIn order to control the latency of the encoder architecture, the fu-\nture context of input sequence X0 is limited to a Ô¨Åxed size, which\nis referred to as restricted or time-restricted self-attention [16] and\nwas Ô¨Årst applied to hybrid HMM-based ASR systems [19]. We\ncan deÔ¨Åne a time-restricted self-attention encoder E NCSAtr, with\nn= 1,...,N , as\nxE\n1:n = ENCSAtr(x0\n1:n+Œµenc ), (9)\nwhere x0\n1:n+Œµenc = X0[1 : n+ Œµenc] = ( x0\n1,..., x0\nn+Œµenc ), and Œµenc\ndenotes the number of look-ahead frames used by the time-restricted\nself-attention mechanism.\n2.2. Decoder: Triggered attention\nThe encoder-decoder attention mechanism of the transformer model\nis using the TA concept [12, 14] to enable the decoder to oper-\nate in a streaming fashion. TA training requires an alignment\nbetween the encoder state sequence XE and the label sequence\nY = (y1,...,y L) to condition the attention mechanism of the de-\ncoder only on past encoder frames plus a Ô¨Åxed number of look-ahead\nframes Œµdec. This information is generated by forced alignment us-\ning an auxiliary CTC objective pctc(Y|XE) [4], which is jointly\ntrained with the decoder model, where the encoder neural network\nis shared [12, 13, 17].\nThe triggered attention objective function is deÔ¨Åned as\npta(Y|XE) =\nL‚àè\nl=1\np(yl|y1:l‚àí1,xE\n1:ŒΩl ) (10)\nwith ŒΩl = n‚Ä≤\nl + Œµdec, where n‚Ä≤\nl denotes the position of the Ô¨Årst\noccurrence of label yl in the CTC forced alignment sequence\n[12, 14], y1:l‚àí1 = ( y1,...,y l‚àí1), and xE\n1:ŒΩl = ( xE\n1 ,..., xE\nŒΩl ),\nwhich corresponds to the truncated encoder sequence. The term\np(yl|y1:l‚àí1,xE\n1:ŒΩl ) represents the transformer decoder model\np(yl|y1:l‚àí1,xE\n1:ŒΩl ) = DECTA(xE\n1:ŒΩl ,y1:l‚àí1), (11)\nwith\nz0\n1:l = EMBED (‚ü®sos‚ü©,y1,...,y l‚àí1), (12)\nzd\nl = zd‚àí1\nl + MHAself\nd (zd‚àí1\nl ,zd‚àí1\n1:l ,zd‚àí1\n1:l ), (13)\nz\nd\nl = zd\nl + MHAdec\nd (zd\nl,xE\n1:ŒΩl ,xE\n1:ŒΩl ), (14)\nzd\nl = z\nd\nl + FFd(z\nd\nl), (15)\nfor d = 1,...,D , where Ddenotes the number of decoder layers.\nFunction EMBED converts the input label sequence(‚ü®s‚ü©,y1,...,y l‚àí1)\ninto a sequence of trainable embedding vectorsz0\n1:l, where ‚ü®sos‚ü©de-\nnotes the start of sentence symbol. Function DECTA Ô¨Ånally predicts\nthe posterior probability of label yl by applying a fully-connected\nprojection layer to zD\nl and a softmax distribution over that output.\nThe CTC model and the triggered attention model of (10) are trained\njointly using the multi-objective loss function\nL= ‚àíŒ≥log pctc ‚àí(1 ‚àíŒ≥) logpta, (16)\nwhere hyperparameter Œ≥controls the weighting between the two ob-\njective functions pctc and pta.\n2.3. Positional encoding\nSinusoidal positional positional encodings (PE) are added to the se-\nquences X0 and Z0, which can be written as\nPE(pos,2i) = sin(pos/100002i/dmodel ), (17)\nPE(pos,2i+ 1) = cos(pos/100002i/dmodel ), (18)\nAlgorithm 1Joint CTC-triggered attention decoding\n1: procedure DECODE (XE, pctc, Œª, Œ±0, Œ±, Œ≤, K, P, Œ∏1, Œ∏2)\n2: ‚Ñì‚Üê(‚ü®sos‚ü©,)\n3: ‚Ñ¶ ‚Üê{‚Ñì}, ‚Ñ¶ta ‚Üê{‚Ñì}\n4: pnb(‚Ñì) ‚Üê0, pb(‚Ñì) ‚Üê1\n5: pta(‚Ñì) ‚Üê1\n6: for n= 1,...,N do\n7: ‚Ñ¶ctc,pnb,pb ‚Üê CTCP REFIX (pctc(n),‚Ñ¶,pnb,pb)\n8: for ‚Ñìin ‚Ñ¶ctc do ‚äøCompute CTC preÔ¨Åx scores\n9: pprfx(‚Ñì) ‚Üêpnb(‚Ñì) + pb(‚Ñì)\n10: ÀÜpprfx(‚Ñì) ‚Üêlog pprfx(‚Ñì) + Œ±0 log pLM(‚Ñì) + Œ≤|‚Ñì|\n11: ÀÜ‚Ñ¶ ‚ÜêPRUNE (‚Ñ¶ctc,ÀÜpprfx,K,Œ∏ 1)\n12: for ‚Ñìin ÀÜ‚Ñ¶ do ‚äøDelete old preÔ¨Åxes in ‚Ñ¶ta\n13: if ‚Ñìin ‚Ñ¶ta and DCOND (‚Ñì,ÀÜ‚Ñ¶,pctc) then\n14: delete ‚Ñìin ‚Ñ¶ta\n15: for ‚Ñìin ÀÜ‚Ñ¶ do ‚äøCompute transformer scores\n16: if ‚Ñìnot in‚Ñ¶ta and ACOND (‚Ñì,ÀÜ‚Ñ¶,pctc) then\n17: pta(‚Ñì) ‚ÜêDECTA(xE\n1:n+Œµdec ,‚Ñì)\n18: add ‚Ñìto ‚Ñ¶ta\n19: for ‚Ñìin ÀÜ‚Ñ¶ do ‚äøCompute joint scores\n20: ÀÜ‚Ñì‚Üê‚Ñìif ‚Ñìin ‚Ñ¶ta else ‚Ñì:‚àí1\n21: p‚ÜêŒªlog pprfx(‚Ñì) + (1‚àíŒª) logpta(ÀÜ‚Ñì)\n22: pjoint(‚Ñì) ‚Üêp+ Œ±log pLM(‚Ñì) + Œ≤|‚Ñì|\n23: ‚Ñ¶ ‚ÜêMAX(ÀÜ‚Ñ¶,pjoint,P)\n24: ÀÜ‚Ñ¶ ‚ÜêPRUNE (ÀÜ‚Ñ¶,ÀÜpprfx,P,Œ∏ 2)\n25: ‚Ñ¶ ‚Üê‚Ñ¶ + ÀÜ‚Ñ¶\n26: remove from ‚Ñ¶ta preÔ¨Åxes rejected due to pruning\n27: return MAX(ÀÜ‚Ñ¶,pjoint,1)\nwhere pos and iare the position and dimension indices [16].\n2.4. Joint CTC-triggered attention decoding\nAlgorithm 1 shows the frame-synchronous one-pass decoding pro-\ncedure for joint scoring of the CTC and transformer model outputs,\nwhich is similar to the decoding scheme described in [14]. The de-\ncoding algorithm is based on the frame-synchronous preÔ¨Åx beam\nsearch algorithm of [20], extending it by integrating the triggered at-\ntention decoder. The joint hypothesis set ‚Ñ¶ and the TA hypothesis\nset ‚Ñ¶ta are initialized in line 3 with the preÔ¨Åx sequence‚Ñì= (‚ü®sos‚ü©,),\nwhere the symbol ‚ü®sos‚ü©denotes the start of sentence label. The CTC\npreÔ¨Åx beam search algorithm of [20] maintains two separate proba-\nbilities for a preÔ¨Åx ending in blank pb and not ending in blank pnb,\nwhich are initialized in line 4. The initial TA scores pta are deÔ¨Åned\nin line 5.\nThe frame-by-frame processing of the CTC posterior probability se-\nquence pctc and the encoder state sequence XE is shown from line 5\nto 26, where pctc(n) denotes the CTC posterior probability distribu-\ntion at frame n. The function CTCP REFIX follows the CTC preÔ¨Åx\nbeam search algorithm described in [20], which extends the set of\npreÔ¨Åxes ‚Ñ¶ using the CTC posterior probabilities pctc of the current\ntime step n and returns the separate CTC preÔ¨Åx scores pb and pnb\nas well as the newly proposed set of preÔ¨Åxes ‚Ñ¶ctc. A local prun-\ning threshold of 0.0001 is used by CTCP REFIX to ignore labels of\nlower CTC probability. Note that no language model or any pruning\ntechnique is used by CTCP REFIX , they will be incorporated in the\nfollowing steps.\nThe preÔ¨Åx probabilities pprfx and scores ÀÜpprfx are computed in lines 9\nand 10, where pLM represents the language model (LM) probability\nand |‚Ñì|denotes the length of preÔ¨Åx sequence ‚Ñìwithout counting the\nstart of sentence label ‚ü®sos‚ü©. The function P RUNE prunes the set of\nCTC preÔ¨Åxes ‚Ñ¶ctc in line 11 in two ways: Ô¨Årst, the K most prob-\nable preÔ¨Åxes are selected based on ÀÜpprfx, then every preÔ¨Åx of score\nsmaller than max(ÀÜpprfx) ‚àíŒ∏1 is discarded, with Œ∏1 being the beam\nwidth. The remaining set of preÔ¨Åxes is stored in ÀÜ‚Ñ¶. From line 12\nto 14, preÔ¨Åxes are removed from the set ‚Ñ¶ta if they satisfy a delete\ncondition DC OND , and from line 15 to 18, TA scores are computed\nby function DECTA if an add condition ACOND returns ‚Äútrue‚Äù. The\ndelete and add conditions are used to delete ‚Äúold‚Äù TA scores com-\nputed at a non-optimal frame position and to delay the computation\nof TA scores, if a new CTC preÔ¨Åx appeared at a supposedly too early\ntime frame. The interested reader is referred to [14] for more details\non both conditions. Note that our ASR experiments indicated that\nboth conditions could be skipped without any WER degradation for\nthe LibriSpeech task, which uses word-piece output labels, whereas\ntheir usage improves WERs for tasks like WSJ [21] with character-\nlevel label outputs. Joint CTC-TA scores, computed from line 19 to\n22, are used to select the P most probable preÔ¨Åxes for further pro-\ncessing, which are stored in set ‚Ñ¶ as shown in line 23. In line 24,\nthe set of CTC preÔ¨Åxes ÀÜ‚Ñ¶ is further pruned to a maximum number\nof P preÔ¨Åxes with preÔ¨Åx scores within the beam width Œ∏2. Line 25\nadds the CTC preÔ¨Åx set ÀÜ‚Ñ¶ to the best jointly scored preÔ¨Åx set ‚Ñ¶,\nand line 26 removes preÔ¨Åxes from ‚Ñ¶ta that are no longer in ‚Ñ¶ for the\ncurrent and previous time steps. Finally, D ECODE returns the preÔ¨Åx\nsequence of highest joint probability pjoint as shown in line 27.\n3. EXPERIMENTS\n3.1. Dataset\nThe LibriSpeech data set, which is a speech corpus of read English\naudio books [22], is used to benchmark ASR systems presented in\nthis work. LibriSpeech is based on the open-source project LibriV ox\nand provides about 960 hours of training data, 10.7 hours of devel-\nopment data, and 10.5 hours of test data, whereby the development\nand test data sets are both split into approximately two halves named\n‚Äúclean‚Äù and ‚Äúother‚Äù. The separation into clean and other is based on\nthe quality of the recorded utterance, which was assessed using an\nASR system [22].\n3.2. Settings\nTwo transformer model sizes are used in this work: small and large.\nParameter settings of the small transformer model aredmodel = 256,\ndÔ¨Ä = 2048 , dh = 4 , E = 12 , and D = 6 , whereas the large\ntransformer model uses dmodel = 512 and dh = 8 instead. The\nAdam optimizer with Œ≤1 = 0.9, Œ≤2 = 0.98, œµ= 10‚àí9 and learning\nrate scheduling similar to [16] is applied for training using 25000\nwarmup steps. The initial learning rate is set to 10.0 and the number\nof training epochs amounts to 100 for the small model and to 120\nfor the large model setup [3, 23]. The set of label outputs consists of\n5000 subwords obtained by the SentencePiece method [24]. Weight\nfactor Œ≥, which is used to balance the CTC and transformer model\nobjectives during training, is set to 0.3. Layer normalization is ap-\nplied before and dropout with a rate of 10% after each MHA and FF\nlayer. In addition, label smoothing with a penalty of 0.1 is used [25].\nAn RNN-based language model (LM) is employed via shallow fu-\nsion. The RNN-LM consists of 4 LSTM layers with 2048 units each\ntrained using stochastic gradient descent and the ofÔ¨Åcial LM training\ntext data of LibriSpeech [22].\nThe LM weight, CTC weight, and beam size of the full-sequence\nbased joint CTC-attention decoding method are set to 0.7, 0.5, and\nTable 1. WERs [%] of the full-sequence based CTC-transformer\nmodel. Results are shown for joint CTC-attention decoding [13],\nCTC preÔ¨Åx beam search decoding only [20], and attention beam\nsearch decoding only [3]. In addition, results for including the RNN-\nLM, for using data augmentation [25] as well as for the large trans-\nformer setup are shown.\nCTC-attention dec. CTC beam search Att. beam search\nclean other clean other clean other\nSystem dev test dev test dev test dev test dev test dev test\nbaseline 4.7 4.9 13.0 12.9 6.1 6.1 15.7 15.9 6.0 7.8 14.5 14.9\n+RNN-LM 2.9 3.1 8.0 8.4 3.1 3.4 9.3 9.6 4.7 7.2 10.7 11.5\n+SpecAug. 2.4 2.8 6.4 6.7 2.9 3.2 7.6 7.9 4.2 5.2 8.3 8.6\n+large 2.4 2.7 6.0 6.1 2.5 2.8 6.9 7.0 4.1 5.0 7.9 8.0\n20 for the small transformer model and to 0.6, 0.4, and 30 for the\nlarge model setup. The parameter settings for CTC preÔ¨Åx beam\nsearch decoding [20] are LM weight Œ±0 = 0.7, pruning beam width\nŒ∏1 = 16.0, insertion bonus Œ≤ = 2.0, and pruning size K = 30. Pa-\nrameters for joint CTC-TA decoding are CTC weightŒª= 0.5, CTC\nLM weight Œ±0 = 0 .7, LM weight Œ± = 0 .5, pruning beam width\nŒ∏1 = 16.0, pruning beam width Œ∏2 = 6.0, insertion bonus Œ≤ = 2.0,\npruning size K = 300 , and pruning size P = 30 . All decoding\nhyperparameter settings are determined using the development data\nsets of LibriSpeech.\n3.3. Results\nTable 1 presents ASR results of our transformer-based baseline sys-\ntems, which are jointly trained with CTC to optimize training con-\nvergence and ASR accuracy [3, 13]. Results of different decoding\nmethods are shown with and without using the RNN-LM, SpecAug-\nment [25], and the large transformer model. Table 1 demonstrates\nthat joint CTC-attention decoding provides signiÔ¨Åcantly better ASR\nresults compared to CTC or attention decoding alone, whereas CTC\npreÔ¨Åx beam search decoding attains lower WERs compared to atten-\ntion beam search decoding, except for the dev-clean, dev-other, and\ntest-other conditions when no LM is used. For attention beam search\ndecoding, we normalize the log posterior probabilities of the trans-\nformer model and the RNN-LM scores when combining both using\nthe hypothesis lengths [17]. Still our attention results are worse com-\npared to the CTC results, which is unexpected but demonstrates that\njoint decoding stabilizes the transformer results.\nTable 2 shows WERs of the full-sequence and the time-restricted\nself-attention encoder architectures combined with the CTC preÔ¨Åx\nbeam search decoding method of [20] and our joint CTC-TA decod-\ning method of Section 2.4, which are both algorithms for streaming\nrecognition. Different encoder look-ahead settings are compared us-\ning Œµenc = 0 ,1,2,3, and ‚àû, where each consumed frame of the\nself-attention encoder corresponds to 40 ms of input due to the out-\nput frame rate of ENCCNN. Since such look-ahead is applied at ev-\nery encoder layer ( E = 12 ), the theoretical latency caused by the\ntime-restricted self-attention encoder amounts to E√óŒµenc √ó40 ms,\ni.e., to 0 ms ( Œµenc = 0 ), 480 ms ( Œµenc = 1 ), 960 ms ( Œµenc = 2 ),\nand 1440 ms (Œµenc = 3), respectively. The CTC preÔ¨Åx beam search\ndecoding results of Table 2 show that increasing Œµenc signiÔ¨Åcantly\nimproves the ASR accuracy, e.g., test-other WER drops from 9.4%\nto 7.0% when moving from 0 to ‚àû(full-sequence) encoder look-\nahead frames. The inÔ¨Çuence of different TA decoder settings are\ncompared in Table 2 as well, using Œµdec = 6 ,12, and 18 look-\nahead frames. Note that unlike the encoder, the total decoder delay\nTable 2. WERs [%] for different Œµenc settings of the time-restricted\nencoder using the CTC preÔ¨Åx beam search decoding method of [20]\nas well our proposed joint CTC-TA decoding method of Section 2.4\nwith different Œµdec conÔ¨Ågurations. SpecAugment [25], the RNN-\nLM, and the large transformer are applied for all systems.1\nCTC beam search TA:Œµdec= 6 TA:Œµdec= 12 TA:Œµdec= 18\nclean other clean other clean other clean other\nŒµenc dev test dev testdev test dev testdev test dev testdev test dev test\n0 3.3 3.7 9.4 9.4 3.2 3.3 8.4 8.6 3.0 3.4 8.4 8.5 2.9 3.2 8.1 8.0\n1 3.0 3.3 8.4 8.6 2.9 3.1 7.8 8.1 2.8 3.1 7.5 8.1 2.8 3.0 7.5 7.8\n2 2.9 3.1 8.0 8.2 2.8 2.9 7.4 7.8 2.7 2.9 7.2 7.6 2.7 2.9 7.3 7.4\n3 2.8 2.9 7.8 8.1 2.7 2.8 7.2 7.4 2.7 2.8 7.2 7.3 2.7 2.8 7.1 7.2\n‚àû 2.5 2.8 6.9 7.0 2.5 2.7 6.3 6.5 2.5 2.7 6.3 6.4 2.4 2.6 6.1 6.3\ndoes not grow with its depth, since each decoder layer is attend-\ning to the encoder output sequence XE. Thus, the TA decoder de-\nlay amounts to Œµdec √ó40 ms, i.e., to 240 ms ( Œµdec = 6 ), 480 ms\n(Œµdec = 12), and 720 ms ( Œµdec = 18), respectively. Results show\nthat joint CTC-TA decoding consistently improves WERs compared\nto CTC preÔ¨Åx beam search decoding, while for larger look-ahead\nvalues WERs are approaching the full-sequence CTC-attention de-\ncoding results, which can be noticed by comparing results of the\nŒµenc = ‚àû, Œµdec = 18 TA system setup with the full-sequence CTC-\nattention system of Table 1.\nThe best streaming ASR system of Table 2 achieves a WER of 2.8%\nand 7.2% for the test-clean and test-other conditions of LibriSpeech\nwith an overall processing delay of 30 ms (E NCCNN) + 1440 ms\n(ENCSA: Œµenc = 3) + 720 ms (DECTA: Œµdec = 18) = 2190 ms. For\nŒµenc = 1 and Œµdec = 18, the test-clean and test-other WERs amount\nto 3.0% and 7.8%, respectively, with a total delay of 1230 ms, which\nprovides a good trade-off between accuracy and latency. It should\nbe noted that a lattice-based CTC-TA decoding implementation can\noutput intermediate CTC preÔ¨Åx beam search results, which are up-\ndated after joint scoring with the TA decoder, and thus the perceived\nlatency of such an implementation will be on average smaller than its\ntheoretical latency and close to that of the encoder alone. However,\na thorough study of the user perceived latency remains to be done in\nfuture work.\n4. CONCLUSIONS\nIn this paper, a fully streaming end-to-end ASR system based on the\ntransformer architecture is proposed. Time-restricted self-attention\nis applied to control the latency of the encoder and the triggered\nattention (TA) concept to control the output latency of the decoder.\nFor streaming recognition and joint CTC-transformer model scor-\ning, a frame-synchronous one-pass decoding algorithm is applied,\nwhich demonstrated similar LibriSpeech ASR results compared\nto full-sequence based CTC-attention as the number of look-ahead\nframes is increased. Combined with the time-restricted self-attention\nencoder, our proposed TA-based streaming ASR system achieved\nWERs of 2.8% and 7.2% for the test-clean and test-other data\nsets of LibriSpeech, which to our knowledge is the best published\nLibriSpeech result of a fully streaming end-to-end ASR system.\n1Note that results shown here are updated compared to our ICASSP sub-\nmission.\n5. REFERENCES\n[1] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang, and S. Khudanpur, ‚ÄúPurely sequence-trained\nneural networks for ASR based on lattice-free MMI,‚Äù in Proc.\nISCA Interspeech, Sep. 2016, pp. 2751‚Äì2755.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly,\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainath, and\nB. Kingsbury, ‚ÄúDeep neural networks for acoustic modeling in\nspeech recognition: The shared views of four research groups,‚Äù\nIEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82‚Äì97,\n2012.\n[3] S. Karita, N. Yalta, S. Watanabe, M. Delcroix, A. Ogawa, and\nT. Nakatani, ‚ÄúImproving transformer-based end-to-end speech\nrecognition with connectionist temporal classiÔ¨Åcation and lan-\nguage model integration,‚Äù in Proc. ISCA Interspeech, Sep.\n2019, pp. 1408‚Äì1412.\n[4] A. Graves, S. Fern ¬¥andez, F. J. Gomez, and J. Schmidhuber,\n‚ÄúConnectionist temporal classiÔ¨Åcation: labelling unsegmented\nsequence data with recurrent neural networks,‚Äù in Proc. Inter-\nnational Conference on Machine Learning (ICML), vol. 148,\nJun. 2006, pp. 369‚Äì376.\n[5] A. Graves, ‚ÄúSequence transduction with recurrent neural net-\nworks,‚ÄùarXiv preprint arXiv:abs/1211.3711, 2012.\n[6] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine trans-\nlation by jointly learning to align and translate,‚ÄùarXiv preprint\narXiv:abs/1409.0473, 2014.\n[7] J. Schalkwyk, ‚ÄúAn all-neural on-device speech recognizer,‚Äù\nMar. 2019, url: https://ai.googleblog.com/2019/03/an-all-\nneural-on-device-speech.html.\n[8] J. Li, R. Zhao, H. Hu, and Y . Gong, ‚ÄúImproving RNN trans-\nducer modeling for end-to-end speech recognition,‚Äù arXiv\npreprint arXiv:abs/1909.12415, 2019.\n[9] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, ‚ÄúA comparison of sequence-to-sequence models for\nspeech recognition,‚Äù inProc. ISCA Interspeech, Sep. 2017, pp.\n939‚Äì943.\n[10] T. N. Sainath, C. Chiu, R. Prabhavalkar, A. Kannan,\nY . Wu, P. Nguyen, and Z. Chen, ‚ÄúImproving the perfor-\nmance of online neural transducer models,‚Äù arXiv preprint\narXiv:abs/1712.01807, 2017.\n[11] C. Chiu and C. Raffel, ‚ÄúMonotonic chunkwise attention,‚Äù in\nProc. International Conference on Learning Representations\n(ICLR), Apr. 2018.\n[12] N. Moritz, T. Hori, and J. Le Roux, ‚ÄúTriggered attention\nfor end-to-end speech recognition,‚Äù in Proc. IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), May 2019, pp. 5666‚Äì5670.\n[13] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi,\n‚ÄúHybrid CTC/attention architecture for end-to-end speech\nrecognition,‚Äù J. Sel. Topics Signal Processing, vol. 11, no. 8,\npp. 1240‚Äì1253, 2017.\n[14] N. Moritz, T. Hori, and J. Le Roux, ‚ÄúStreaming end-to-end\nspeech recognition with joint CTC-attention based models,‚Äù in\nProc. IEEE Workshop on Automatic Speech Recognition and\nUnderstanding (ASRU), Dec. 2019, pp. 936‚Äì943.\n[15] N. Moritz, T. Hori, and J. Le Roux, ‚ÄúUnidirectional neural net-\nwork architectures for end-to-end automatic speech recogni-\ntion,‚Äù in Proc. ISCA Interspeech, Sep. 2019, pp. 76‚Äì80.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all\nyou need,‚Äù in Proc. NIPS, Dec. 2017, pp. 6000‚Äì6010.\n[17] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, ‚ÄúA comparative study on\ntransformer vs RNN in speech applications,‚Äù in Proc. IEEE\nWorkshop on Automatic Speech Recognition and Understand-\ning (ASRU), Dec. 2019.\n[18] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, ‚ÄúAdvances in\njoint CTC-attention based end-to-end speech recognition with\na deep CNN encoder and RNN-LM,‚Äù in Proc. ISCA Inter-\nspeech, Aug. 2017, pp. 949‚Äì953.\n[19] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudan-\npur, ‚ÄúA time-restricted self-attention layer for ASR,‚Äù in Proc.\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2018, pp. 5874‚Äì5878.\n[20] A. L. Maas, A. Y . Hannun, D. Jurafsky, and A. Y . Ng, ‚ÄúFirst-\npass large vocabulary continuous speech recognition using bi-\ndirectional recurrent DNNs,‚Äù arXiv preprint arXiv:1408.2873,\n2014.\n[21] ‚ÄúCSR-II (WSJ1) complete,‚Äù vol. LDC94S13A. Philadelphia:\nLinguistic Data Consortium, 1994.\n[22] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib-\nrispeech: An ASR corpus based on public domain audio\nbooks,‚Äù in Proc. IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), Apr. 2015, pp. 5206‚Äì\n5210.\n[23] L. Dong, S. Xu, and B. Xu, ‚ÄúSpeech-transformer: A no-\nrecurrence sequence-to-sequence model for speech recogni-\ntion,‚Äù in Proc. IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2018, pp. 5884‚Äì\n5888.\n[24] T. Kudo and J. Richardson, ‚ÄúSentencePiece: A simple and\nlanguage independent subword tokenizer and detokenizer for\nneural text processing,‚Äù arXiv preprint arXiv:abs/1808.06226,\n2018.\n[25] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, ‚ÄúSpecAugment: A simple data augmenta-\ntion method for automatic speech recognition,‚Äù arXiv preprint\narXiv:abs/1904.08779, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8116744756698608
    },
    {
      "name": "Encoder",
      "score": 0.7988258600234985
    },
    {
      "name": "Transformer",
      "score": 0.7839275598526001
    },
    {
      "name": "Speech recognition",
      "score": 0.6675733923912048
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5701605081558228
    },
    {
      "name": "Decoding methods",
      "score": 0.5563889741897583
    },
    {
      "name": "Utterance",
      "score": 0.4853731393814087
    },
    {
      "name": "Language model",
      "score": 0.4580676853656769
    },
    {
      "name": "End-to-end principle",
      "score": 0.4426663815975189
    },
    {
      "name": "Word error rate",
      "score": 0.43938860297203064
    },
    {
      "name": "Artificial neural network",
      "score": 0.4385407567024231
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3609299659729004
    },
    {
      "name": "Algorithm",
      "score": 0.09314984083175659
    },
    {
      "name": "Engineering",
      "score": 0.07147136330604553
    },
    {
      "name": "Voltage",
      "score": 0.07089179754257202
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}