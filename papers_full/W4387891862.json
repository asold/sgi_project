{
    "title": "SigFormer: Signature Transformers for Deep Hedging",
    "url": "https://openalex.org/W4387891862",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5064825850",
            "name": "Anh Tong",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5086423509",
            "name": "Thanh Nguyen-Tang",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A5100720915",
            "name": "Dongeun Lee",
            "affiliations": [
                "East Texas A&M University"
            ]
        },
        {
            "id": "https://openalex.org/A5088089303",
            "name": "Toan M Tran",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5052985764",
            "name": "Jaesik Choi",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3204752081",
        "https://openalex.org/W4376853903",
        "https://openalex.org/W4321766651",
        "https://openalex.org/W3021131124",
        "https://openalex.org/W1890880943",
        "https://openalex.org/W4238411259",
        "https://openalex.org/W1876792444",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W3127936078",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W4382047356",
        "https://openalex.org/W3163425973",
        "https://openalex.org/W2744090841",
        "https://openalex.org/W3112488842",
        "https://openalex.org/W2158581396",
        "https://openalex.org/W3010733326",
        "https://openalex.org/W2027849119",
        "https://openalex.org/W2150733281",
        "https://openalex.org/W4382203079",
        "https://openalex.org/W4322769828",
        "https://openalex.org/W2263883519",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3093748013",
        "https://openalex.org/W4388063256",
        "https://openalex.org/W3121984249",
        "https://openalex.org/W2031753087",
        "https://openalex.org/W3122343475",
        "https://openalex.org/W3125017247",
        "https://openalex.org/W4283763321",
        "https://openalex.org/W2945798174",
        "https://openalex.org/W3123691155",
        "https://openalex.org/W2997014690",
        "https://openalex.org/W4285078420",
        "https://openalex.org/W2004095444",
        "https://openalex.org/W4383473327",
        "https://openalex.org/W2297500149",
        "https://openalex.org/W2963230864",
        "https://openalex.org/W3213873037",
        "https://openalex.org/W4297782957",
        "https://openalex.org/W4285818523",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3212890323",
        "https://openalex.org/W3169291081",
        "https://openalex.org/W4285268606",
        "https://openalex.org/W3127429496",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W2981150028",
        "https://openalex.org/W4286373020",
        "https://openalex.org/W4385763767",
        "https://openalex.org/W4320018887",
        "https://openalex.org/W3197219184",
        "https://openalex.org/W3186694531",
        "https://openalex.org/W3126136401",
        "https://openalex.org/W2137983211",
        "https://openalex.org/W2963611658",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Deep hedging is a promising direction in quantitative finance, incorporating\\nmodels and techniques from deep learning research. While giving excellent\\nhedging strategies, models inherently requires careful treatment in designing\\narchitectures for neural networks. To mitigate such difficulties, we introduce\\nSigFormer, a novel deep learning model that combines the power of path\\nsignatures and transformers to handle sequential data, particularly in cases\\nwith irregularities. Path signatures effectively capture complex data patterns,\\nwhile transformers provide superior sequential attention. Our proposed model is\\nempirically compared to existing methods on synthetic data, showcasing faster\\nlearning and enhanced robustness, especially in the presence of irregular\\nunderlying price data. Additionally, we validate our model performance through\\na real-world backtest on hedging the SP 500 index, demonstrating positive\\noutcomes.\\n",
    "full_text": "SigFormer: Signature Transformers for Deep Hedging\nAnh Tong\nanhtong@kaist.ac.kr\nKAIST\nSouth Korea\nThanh Nguyen-Tang\nnguyent@cs.jhu.edu\nJohns Hopkins University\nUSA\nDongeun Lee\ndongeun.lee@tamuc.edu\nTexas A&M University-Commerce\nUSA\nToan Tran\nv.toantm3@vinai.io\nVinAI Research\nVietnam\nJaesik Choi\njaesik.choi@kaist.ac.kr\nKAIST/INEEJI\nSouth Korea\nABSTRACT\nDeep hedging is a promising direction in quantitative finance, incor-\nporating models and techniques from deep learning research. While\ngiving excellent hedging strategies, models inherently requires care-\nful treatment in designing architectures for neural networks. To\nmitigate such difficulties, we introduce SigFormer, a novel deep\nlearning model that combines the power of path signatures and\ntransformers to handle sequential data, particularly in cases with\nirregularities. Path signatures effectively capture complex data pat-\nterns, while transformers provide superior sequential attention.\nOur proposed model is empirically compared to existing methods\non synthetic data, showcasing faster learning and enhanced ro-\nbustness, especially in the presence of irregular underlying price\ndata. Additionally, we validate our model performance through a\nreal-world backtest on hedging the S&P 500 index, demonstrating\npositive outcomes.\nACM Reference Format:\nAnh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, and Jaesik Choi.\n2023. SigFormer: Signature Transformers for Deep Hedging. In 4th ACM\nInternational Conference on AI in Finance (ICAIF â€™23), November 27â€“29, 2023,\nBrooklyn, NY, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3604237.3626841\n1 INTRODUCTION\nThe effective hedging of derivatives represents a crucial challenge\nin the field of mathematical finance. Over the years, various well-\nestablished approaches have been developed to derive tractable\nsolutions for these problems. Among the conventional methods,\nclassical risk management involves computing quantities known as\nâ€œgreeks. â€ Nevertheless, this approach heavily depends on somewhat\nunrealistic assumptions and settings, which may limit its applica-\nbility in certain scenarios.\nIn recent times, there has been a paradigm shift in addressing\nthis problem. Departing from the traditional techniques, BÃ¼hler\net al. [11] introduced a novel direction by harnessing the power of\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0240-2/23/11. . . $15.00\nhttps://doi.org/10.1145/3604237.3626841\ndeep learning models. This innovative approach has the remarkable\nability to handle a broader range of settings, even extending to high-\ndimensional cases, where conventional methods may fall short. By\nleveraging the capabilities of deep learning, the proposed frame-\nwork opens up new possibilities for enhancing hedging strategies\nand exploring more realistic and adaptable solutions.\nHedging a derivative involves making decisions to buy or sell\nthe underlying instrument at specific time points to achieve risk-\nneutrality. Typically, it requires making an assumption regarding\nunderlying price processes like the Hull-White model model [48]\nand the Heston models [21]. However, recent work [19] suggests\nthat fractional stochastic volatility (FSV) models offer a more suit-\nable choice, exhibiting properties that closely resemble those ob-\nserved in financial markets. Modeling such a process involves the\nuse of fractional Brownian models introduced in [36].\nDeep hedging [11, 23, 53] using recurrent neural networks (RNNs)\nmight encounter challenges when dealing with irregular sequences\nfrom FSV modes. Some evidence from [25] indicates that the RNN\napproach is less effective in this context. To handle the irregularity\npresent in data, we propose incorporating path signatures. Path\nsignatures [32, 33, 35] are mathematical transformations capable of\nextracting features that describe the curves of paths as a collection\nof terms, potentially infinite in number. By employing these path\nsignatures, we can effectively represent and capture the complexi-\nties of irregular data patterns [17].\nAdditionally, we further leverage the ability to extract important\nfeatures from signatures by incorporating transformers [46]. Unlike\nthe previous approaches like [11, 23] that directly use market infor-\nmation as input into models, our approach takes signature outputs\nas input for transformers. While signatures handle roughness in\ndata, transformers provide better sequential attention. Transform-\ners are preferable over RNNs in a vast number of machine learning\napplications due to the scalability in training and the capability to\nmodel long sequences. To the best of our knowledge, our work is\nthe first to explore the ability of transformers with a deep hedging\nframework.\nOur proposed model offers a distinct combination of signature\ncomputations and transformers. In contrast to a naive approach\nof directly applying transformers to signatures, we propose to in-\ncorporate multiple attention blocks designed to target selective\nand specialized terms of signature. Our novel design, driven by the\nfact that individual term in signatures possesses unique geometric\nproperties, leads to a strong ability to handle the irregularity in the\ndata, as we will show in our experiments.\narXiv:2310.13369v1  [cs.LG]  20 Oct 2023\nICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, and Jaesik Choi\nThe paper makes the following key contributions: (1) introducing\na novel deep learning model, named SigFormer, which carefully\nleverages the power of signatures and transformers in a princi-\npled manner, offering a novel and effective approach for sequential\ndata analysis; (2) conducting an extensive empirical comparison of\nour proposed model with existing methods on various synthetic\ndata settings. The results demonstrate that SigFormer exhibits\nfaster learning curves and enhanced robustness, particularly in\ncases where underlying price data exhibit greater irregularity; (3)\nproviding a backtest on real-world data for hedging the S&P 500\n(Standard & Poor 500) index, validating the performance of our\nmodel and showcasing positive outcomes.\n2 BACKGROUND\nThis section offers a brief review of signatures in rough path the-\nory and provides the background on deep hedging models and\ntransformers.\n2.1 Signatures\nHere, we follow the standard notion in [32]. Consider an extended\ntensor algebra\nğ‘‡((Rğ‘‘)):= {(ğ‘0,ğ‘1,...,ğ‘ ğ‘–,... )|ğ‘ğ‘– âˆˆ(Rğ‘‘)âŠ—ğ‘›}.\nHere, (Rğ‘‘)âŠ—ğ‘› = Rğ‘‘ âŠ—Â·Â·Â·âŠ— Rğ‘‘\n|            {z            }\nğ‘›times\n, and (Rğ‘‘)0 B R.\nDefinition 1 (Signature). Let ğ‘‹ : [0,ğ‘‡]â†’ Rğ‘‘ be a continuous\npath. The signature ofğ‘‹ over an interval [ğ‘ ,ğ‘¡]âŠ‚[ 0,ğ‘‡]is defined as\nan infinite series of tensors indexed by the signature order ğ‘› âˆˆN,\nSigğ‘ ,ğ‘¡(ğ‘‹)B\n\u0010\n1,Sig1\nğ‘ ,ğ‘¡(ğ‘‹),Sig2\nğ‘ ,ğ‘¡(ğ‘‹),..., Sigğ‘›\nğ‘ ,ğ‘¡(ğ‘‹),...\n\u0011\nâˆˆğ‘‡((Rğ‘‘)),\n(1)\nwhere\nSigğ‘›\nğ‘ ,ğ‘¡(ğ‘‹):=\nâˆ«\nğ‘ <ğ‘¢1<Â·Â·Â·<ğ‘¢ğ‘› <ğ‘¡\nğ‘‘ğ‘‹ğ‘¢1 âŠ—Â·Â·Â·âŠ— ğ‘‘ğ‘‹ğ‘¢ğ‘› âˆˆ(Rğ‘‘)âŠ—ğ‘›. (2)\nIn the lens of studying controlled differential equations, [32] em-\nphasizes that signatures are the key tools in modeling non-linear\nsystems with highly oscillatory signals. Many financial instruments\nare known to be one of such highly irregular time series. Intuitively,\nwe can understand Sigğ‘›(ğ‘‹)as the ğ‘›-th moment tensor of the infin-\nitesimal change given ğ‘›time points sampled uniformly on the path.\nSome references including [15] give a comprehensive introduction\nof signatures for machine learning.\nProposition 1 (Invariance to time parameterization).Let\nğ‘‹ : [0,ğ‘‡]â†’ Rğ‘‘ be a continuous path. For any function ğœ‘ : [0,ğ‘‡]â†’\n[0,ğ‘‡]that is continuously differentiable, increasing, and surjective,\nwe have\nSig(ğ‘‹)= Sig(ğ‘‹ âŠ™ğœ‘),\nwhere [ğ‘‹ âŠ™ğœ‘](Â·)= ğ‘‹(ğœ‘(Â·)).\nProposition 1 indicates that the signature is invariant under\nreparameterization (see [18, Proposition 7.10] for the proof). One\nmay refer to [44, Figure 1] for an intuitive example of this property.\nSuch an invariance is important in machine learning models to\ntackle symmetries in data, e.g.,ğ‘†ğ‘‚(3)invariance in computer vision.\nAs noted in [25], signature transforms share a resemblance with\nthe Fourier transform in the sense that a signal can be approximated\nwell given a finite basis. Consider a non-linear function of path\nğ‘‹ â†’ğ‘“(ğ‘‹), we can have a universal approximation ofğ‘“ via a linear\nfunction of the signatures which is ğ‘“(ğ‘‹)â‰ˆâŸ¨ ğ‘Š, Sig(ğ‘‹)âŸ©, where ğ‘Š\nis a linear weight. Such a property allows [1] calibrating financial\nmodels efficiently and provides the theory for optimal hedging [34].\nProposition 2 (Universal Nonlinearity).Let V1([0,ğ‘‡]; Rğ‘‘)\nbe the space of continuous paths from some interval [0,ğ‘‡]to Rğ‘‘. Sup-\npose KâˆˆV 1([0,ğ‘‡]; Rğ‘‘)is compact and ğ‘“ : Kâ†¦â†’ R is continuous.\nFor any ğœ€ > 0 there exists a truncation level ğ‘› âˆˆN and coefficients\nğ›¼ğ‘–(J)âˆˆ R such that for every ğ‘‹ âˆˆK, we have\n\f\f\f\f\f\f\nğ‘“(ğ‘‹)âˆ’\nğ‘›âˆ‘ï¸\nğ‘–=0\nâˆ‘ï¸\nJâˆˆ{1,...,ğ‘‘}ğ‘–\nğ›¼ğ‘–(J)Sigğ‘,ğ‘(ğ‘‹)\n\f\f\f\f\f\f\nâ‰¤ğœ€. (3)\nProving this proposition involves showing signatures span an\nalgebra and subsequently applying the Stone-Weierstrass theorem\n(refer to [28, Theorem 1] for a comprehensive proof).\nThe following proposition is helpful in handling stream data.\nProposition 3 (Chenâ€™s identity [35]). Let ğ‘¥,ğ‘¦ : [ğ‘,ğ‘]â†’ Rğ‘‘\nbe two continuous paths such that ğ‘¥(ğ‘)= ğ‘¦(ğ‘). The concatenation\nğ‘¥â˜…ğ‘¦yields the signature\nSig(ğ‘¥â˜…ğ‘¦)= Sig(ğ‘¥)âŠ— Sig(ğ‘¦). (4)\nWith the necessary background on signatures above, we will now\ndelve into their practical implementations. In practice, the truncated\nversion of the signature, denoted asSigğ‘ ,ğ‘¡(ğ‘‹)= (Sigğ‘›\nğ‘ ,ğ‘¡(ğ‘‹))ğ‘›=0,...,ğ‘,\nproves to be sufficiently expressive.\nSeveral libraries offer implementations for signature computa-\ntions, such asesig1, iisignature[43], signatory[26], andsignax2.\n2.2 Deep Hedging\nRecently, deep hedging models [11] emerged as a new paradigm for\npricing and hedging models. Several works, such as [9, 10, 12, 13]\nhave further extended or integrated deep hedging in their research.\nSettings. Let us consider a market with a time horizon denoted\nby ğ‘‡. Trading is exercised at dates 0 = ğ‘¡0 < ğ‘¡1 < Â·Â·Â· < ğ‘¡ğ‘› = ğ‘‡,\nand ğ¼ğ‘˜ represents the market information at each date ğ‘¡ğ‘˜. In this\nmarket, there are ğ‘‘hedging instruments, given by ğ‘† B (ğ‘†ğ‘˜)ğ‘˜=0,...,ğ‘›\nwhere ğ‘†ğ‘˜ âˆˆRğ‘‘. The liability of our derivative at ğ‘‡ is defined as ğ‘.\nA hedge strategy is denoted as ğ›¿ B (ğ›¿ğ‘˜)ğ‘˜=0,...,ğ‘›âˆ’1, with ğ›¿ğ‘˜ âˆˆRğ‘‘.\nDeep hedging, introduced by BÃ¼hler et al. [12], aims to find the\noptimal ğ›¿ that minimizes the following objective under a pricing\nmeasure Q of financial market:\ninf\nğ›¿\nEQ[ğœŒ(âˆ’ğ‘ +(ğ›¿Â·ğ‘†)ğ‘‡ âˆ’ğ¶ğ‘‡(ğ›¿))], (5)\nwhere ğœŒ : R â†¦â†’R represents a convex risk measure [24, 50], and\n(ğ›¿ Â·ğ‘†)ğ‘‡ B Ãğ‘›âˆ’1\nğ‘˜=0 ğ›¿ğ‘˜(ğ‘†ğ‘˜+1 âˆ’ğ‘†ğ‘˜)indicates the wealth at time ğ‘‡\nresulting from the chosen hedge strategy. The term ğ¶ğ‘‡(ğ›¿)denotes\nthe trading cost incurred by ğ›¿. For simplicity, we do not include the\ncost in this work and set ğ¶ğ‘‡(ğ›¿)= 0.\n1https://pypi.org/project/esig/\n2https://pypi.org/project/signax/\nSigFormer: Signature Transformers for Deep Hedging ICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA\nNeural network architecture. The neural network architecture,\nproposed by [11] to approximateğ›¿is denoted asğ›¿ğœƒ\nğ‘˜ B ğ¹ğœƒ\n\u0010\nğ¼ğ‘˜,ğ›¿ğœƒ\nğ‘˜âˆ’1\n\u0011\n.\nHere ğ¹ğœƒ is a feed-forward neural network known for its universal\napproximation properties [22]. Notably, in addition to incorporating\nmarket information ğ¼ğ‘˜, the model exhibits recurrent behaviors since\nit takes the previous hedge strategy ğ›¿ğœƒ\nğ‘˜âˆ’1 as an input.\nIn detail, the architecture of the model consists of an input layer\nwith 2ğ‘‘ nodes, two hidden layers, each comprising ğ‘‘+15 nodes,\nand an output layer with ğ‘‘ nodes. The activation function used is\nğœ(ğ‘¥)= max(ğ‘¥,0), commonly known as the ReLU (Rectified Linear\nUnit) activation function.\nThe use of this architecture allows the neural network to ef-\nfectively approximate the optimal hedging strategy ğ›¿, considering\nboth the historical hedging decisions and the evolving market in-\nformation. This makes the model capable of capturing complex\ndependencies and patterns.\n2.3 Attention and Transformer\nThe architecture of transformers [ 46] is grounded in three key\ncomponents: self-attention, multi-head attention, and feed-forward\nneural network. These essential elements are briefly presented here.\nSelf-attention. Self-attention is specified by three components: a\nquery, a key, and a value. Letğ‘‹ âˆˆRğ‘›Ã—ğ‘‘ğ‘¥ represent the input data,\nand let ğ‘Šğ‘ âˆˆRğ‘‘attn Ã—ğ‘‘ğ‘¥ , ğ‘Šğ‘˜ âˆˆRğ‘‘attn Ã—ğ‘‘ğ‘¥ , and ğ‘Šğ‘£ âˆˆRğ‘‘attn Ã—ğ‘‘ğ‘¥ denote\nthe linear projections for query, key, and value, respectively. We\ndefine ğ‘„ = ğ‘‹ğ‘ŠâŠ¤ğ‘ , ğ¾ = ğ‘‹ğ‘ŠâŠ¤\nğ‘˜ , and ğ‘‰ = ğ‘‹ğ‘ŠâŠ¤ğ‘£ . Then\nAttention(ğ‘„,ğ¾,ğ‘‰ )B softmax\n\u0012ğ‘„ğ¾âŠ¤\nâˆšğ‘‘ğ‘¥\n\u0013\nğ‘‰. (6)\nIntuitively, self-attention can be interpreted as an operation that\nencodes the process of determining the locations in a sequence that\nnecessitate attention. Let us consider a sequence ğ‘‹ represented\nby elements ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›. The attention mechanism is established\nthrough the creation of a query (ğ‘ğ‘¡ = ğ‘¥ğ‘¡ğ‘ŠâŠ¤ğ‘ ), which is subsequently\ncompared to other keys (ğ‘˜ğœ = ğ‘¥ğœğ‘ŠâŠ¤\nğ‘˜ ). The comparison is facilitated\nby a kernel function denoted as ğœ…(ğ‘ğ‘¡,ğ‘˜ğœ)= exp(ğ‘ğ‘¡ğ‘˜âŠ¤\nğœ )Ã\nğ‘  exp(ğ‘ğ‘¡ğ‘˜âŠ¤ğ‘  ), serving\nto quantify the similarity between queries and keys. Finally, the\nelement at position ğ‘¡ within the sequence is updated according\nto the aggregation Ãğ‘›\nğœ=1 ğœ…(ğ‘ğ‘¡,ğ‘˜ğœ)ğ‘£ğœ with ğ‘£ğœ = ğ‘¥ğœğ‘ŠâŠ¤ğ‘£ . The value\nof ğœ…(ğ‘ğ‘¡,ğ‘˜ğœ)determines the degree of attention the model directs\ntowards ğ‘£ğœ: a higher value of ğœ…(ğ‘ğ‘¡,ğ‘˜ğœ)implies a stronger focus by\nthe model on ğ‘£ğœ.\nMulti-head Attention. Multi-head attention is an operation that\nconcatenates various versions of attention and projects them into\nan appropriate space. It can be represented as\nMultiHead BConcat(head1,..., headâ„)ğ‘Šğ‘œ,\nwhere headğ‘– BAttention(ğ‘„ğ‘–,ğ¾ğ‘–,ğ‘Šğ‘–).\nHere,ğ‘Šğ‘œ denotes the weight of the output projection, andğ‘„ğ‘–,ğ¾ğ‘–,ğ‘‰ğ‘–\nrepresents the query, key, value associated with weightsğ‘Šğ‘–ğ‘,ğ‘Šğ‘–\nğ‘˜,ğ‘Šğ‘–ğ‘£,\nrespectively.\nThe multi-head attention mechanism facilitates the synthesis of\njoined representations and encodes richer information. It proves\nbeneficial in alleviating the sparsity inherent in the attention oper-\nation described in Eq. (6) which arises from the softmax function.\nFeed-forward network. The feed-forward network (FFN), consti-\ntuting the final component within transformers, consists of two\nlinear layers and a ReLU activation. Notably, practical implemen-\ntations often adopt the Gaussian Error Linear Unit (GELU) as the\ndefault choice. The FFN component can be expressed as follows:\nFFN(ğ‘¥)= GELU(ğ‘¥ğ‘ŠâŠ¤\n1 +ğ‘1)ğ‘ŠâŠ¤\n2 +ğ‘2. (7)\nHere, ğ‘Š1 and ğ‘Š2 represent weight matrices; ğ‘1 and ğ‘2 denote cor-\nresponding bias vectors.\nThe signification of FFN has been extensively examined in the\nliterature, wherein it is elucidated as a reservoir for information\nmemory that facilitates the emergence capacities in large trans-\nformers [20].\nIn addition to the above description of the essential components\nwithin the transformer, it is worth highlighting that other details,\nsuch as layer normalization and positional encoding, are elaborated\nin the original paper [46].\n3 RELATED WORK\nDeep hedging. The concept of deep hedging was first introduced\nin the literature by BÃ¼hler et al. [11]. Since its inception, numerous\nresearch efforts have been devoted to extending and enhancing the\ndeep hedging framework in various dimensions. For instance, BÃ¼h-\nler et al. [12] addressed the issue of drift removal, and recently ex-\nplored incorporating reinforcement learning techniques [13]. Other\nrelevant work, presented in Horvath et al. [23], Zhu and Diao [53],\nshares a similar goal aiming to enhance the deep hedging technique\nby using RNN architectures.\nIn our study, we propose a novel neural network architecture\nthat combines two essential methodologies, namely signatures and\ntransformers, to improve the deep hedging framework. In related\nwork, Limmer and Horvath[31] extended the optimization objective\nby considering data uncertainty through an adversarial approach.\nIt is worth noting that our proposed model directly employs sig-\nnatures for learning data representations, whereas Limmer and\nHorvath [31] incorporates signatures as a regularization compo-\nnent.\nWith these advancements, our paper contributes to the refine-\nment and enrichment of the deep hedging methodology by introduc-\ning a novel neural network architecture that integrates signatures\nand transformers. Moreover, we demonstrate the effectiveness of\nour proposed model in handling data uncertainty, which is also a\ncrucial aspect in hedging strategies.\nApplications of signatures. Signatures are a powerful mathemat-\nical tool utilized for modeling sequential data, as evidenced by\nseveral notable works [ 32, 33, 35]. From theoretical standpoint,\nsignatures hold a crucial role in the realm of rough path theory,\nestablishing the fundamental basis for stochastic partial differential\nequations [17].\nThe financial domain has witnessed an extensive array of ap-\nplications of signatures, exemplified by the work of [1], wherein\nsignatures find utility in pricing problems [34].\nMoreover, signatures have recently emerged as a subject of in-\nterest in the field of machine learning, particularly in the context\nICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, and Jaesik Choi\nof time series modeling. A comprehensive overview of these de-\nvelopments and surveys can be found in [15, 33]. Furthermore, re-\nsearchers have endeavored to integrate signature computation into\ndeep neural networks [25]. Additionally, novel machine learning\napproaches have been proposed for effectively modeling irregularly\nsampled time series [38, 39], further broadening the scope and im-\npact of signature-based techniques in this domain. Furthermore,\nthe application of signatures as a tool in rough path theory for the\nstudy of fractional Brownian motions has been explored [45].\nTransformers. Transformers [46], a recent advancement in deep\nneural network research, has gained widespread adoption in various\ndomains, particularly in natural language processing [8] and com-\nputer vision [16]. Additionally, there are several attempts to apply\nthis approach to time-series data [30, 47, 49, 52] with a primary fo-\ncus on addressing long-term prediction challenges. In contrast to the\nlong-term prediction task emphasized in the aforementioned works,\ndeep hedging employs an autoregressive approach to predict at each\nsingle step. It is worth noting that many of these transformer-based\napproaches have been employed in financial applications, leading\nto promising outcomes, as demonstrated by recent studies [2, 3, 29].\nNevertheless, it is important to highlight that certain variations\nof transformers have shown less than optimal performance when\nevaluated on financial datasets such as exchange indexes [51].\n4 SIGNATURE TRANSFORMERS\nThis section presents our main proposed model, Signature Trans-\nformer or SigFormer.\n4.1 SigFormer\nThis section focuses on the architecture of our main model, which\nwe call SIGnature transFORMER or SigFormer.\nModel specification. Our goal is to construct a hedging strategy\nğ›¿ğ‘˜ at time ğ‘¡ğ‘˜,ğ‘˜ = 0,...ğ‘› , which depends on the market information\nup to ğ‘¡ğ‘˜âˆ’1, namely ğ¼0,...,ğ¼ ğ‘˜âˆ’1. The primary aim of those hedging\nstrategies is to minimize the risk as defined in Eq. (5). In our pro-\nposed model, we formulate it as a sequence-to-sequence modeling\ntask. However, unlike recurrent approaches, we are interested in\nprocessing the entire input sequence (ğ¼0,...,ğ¼ ğ‘›âˆ’1)at once to pro-\nduce the predicted sequence (ğ›¿0,...,ğ›¿ ğ‘›âˆ’1). We further denote ğ‘‹ğ‘˜\nin place of the market information ğ¼ğ‘˜ in this section and write a\nsequence ğ‘‹0,...,ğ‘‹ ğ‘˜ as ğ‘‹0:ğ‘˜.\nIn essence, SigFormer is a transformer acting in the space of\ntensor algebra. We now describe its operations step by step. We\nfirst utilize the operator â„“ to lift the input sequence ğ‘‹0:ğ‘› while\npreserving the stream information [25]. The operator â„“ is defined\nas:\nâ„“ : ğ‘‹ â†¦â†’(â„“1(ğ‘‹),â„“2(ğ‘‹),...,â„“ ğ‘›(ğ‘‹)), where â„“ğ‘˜(ğ‘‹)B ğ‘‹0:ğ‘˜.\nNote that in practice, the sequence is padded with zeros in the\nbeginning.\nNext, by applying signature transformations to all the lifted\nsequences â„“ğ‘˜(ğ‘‹), for any ğ‘– = 1,...,ğ‘ , we obtain the ğ‘–-th level of\nthe signature as\nSigğ‘–(â„“(ğ‘‹))B\n\u0010\nSigğ‘–(â„“1(ğ‘‹)),..., Sigğ‘–(â„“ğ‘›(ğ‘‹))\n\u0011\nâˆˆ((ğ‘…ğ‘‘)âŠ—ğ‘–)ğ‘›. (8)\nThe stream version of signatures can be considered as a sequence\ncontaining ğ‘›time steps in the space of (Rğ‘‘)âŠ—ğ‘–.\nIntuitively, the lift function â„“ preserves the stream information\nof sequence, because Sig(ğ‘‹0:ğ‘›)is the summary up to time step ğ‘›\nbut does not represent sequential structures.\nAt every individual signature level Sigğ‘–(â„“(ğ‘‹)), we apply\nAttentionLayerğ‘–(Sigğ‘–(â„“(ğ‘‹))), (9)\nwhere AttentionLayerğ‘–(Â·)is a block containing all the components\nof the transformer described in Â§2.3. Since Sigğ‘–(â„“ğ‘˜(ğ‘‹))stays in\n(Rğ‘‘)âŠ—ğ‘– for any time step ğ‘˜, we need to flatten it into Rğ‘‘ğ‘–\nbefore\napplying projections to obtain queries, keys, values. To this end,\nwe have\nğ‘‹\nlift\nâˆ’âˆ’â†’â„“(ğ‘‹)âˆ’ â†’(Sigğ‘–(â„“(ğ‘‹)))ğ‘\nğ‘–=1 âˆ’ â†’(AttentionLayerğ‘–(Sigğ‘–(â„“(ğ‘‹))))ğ‘\nğ‘–=1.\nWe apply the attention layer several times. In the final layer, we\nconcatenate all transformed signatures and use a fully connected\nlayer to get the output. Let us denote the whole architecture asğ¹Sig.\nFigure 1 depicts an example of two-layer-attention SigFormer.\nRemark 1. The primary motivation for designing separate atten-\ntion layers for different signature levels is to equipSigFormer with\nflexibility in capturing the characteristic of sequence. That is, each\nsignature level exhibits distinct geometric properties. For instance,\nthe first level signature encodes changes over the interval, while\nthe second level represents the LÃ©vy area, which corresponds to\nthe area between the curve and the chord connecting its start and\nendpoints [39].\nWe postulate that the geometric properties from theğ‘–-th order\nsignature do not influence the decision of where to focus on the\nstream of another signature with different orders.\nTheoretical justification. It is worth noting that our construction\nof SigFormer possesses excellent approximation capabilities. Sig-\nFormer incorporates two main nonlinear transformations, namely\nğ´ B softmax(ğ‘„ğ¾âŠ¤/âˆšğ‘‘ğ‘¥)and a two-layer feed-forward network.\nIf we treat ğ´ as a fixed matrix, we can conjecture the universal\napproximation capabilities of SigFormer. This is attributed to the\nuniversal approximation theorem for neural networks [42] and the\nuniversal approximation theorem for signatures [33, Theorem 3.4]\n(see Proposition 2).\n4.2 Hedging with SigFormer\nOur hedge strategy uses ğ›¿Sig B (ğ›¿Sig\nğ‘˜ )ğ‘˜=0,...,ğ‘›âˆ’1 which relies on\nSigFormer to make decision. Given market informationğ‘‹, formally,\nğ›¿Sig\n0:ğ‘›âˆ’1 = ğ¹Sig (ğ‘‹0:ğ‘›âˆ’1)). (10)\nSimilar to [23], we train ğ¹Sig by using the quadratic loss\nmin\nğœƒ\nEQ\nh\n(ğ‘0 +(ğ›¿Sig Â·ğ‘†)ğ‘‡ âˆ’ğ‘)2\ni\n,\nwhere ğ‘0 = Eğ‘„[ğ‘]. In our experiments, the underlyingQ is a rough\nstochastic volatility model under European options.\nThe backbone neural network in BÃ¼hler et al. [11] w.r.t hedge\nstrategy ğ›¿rnn can be designed in this form\nğ›¿rnn\nğ‘˜ B ğ¹ğ‘˜(ğ‘‹0,...,ğ‘‹ ğ‘˜,ğ›¿rnn\nğ‘˜âˆ’1).\nSigFormer: Signature Transformers for Deep Hedging ICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA\nAttentionLayer\nSignature\ntransform\nInput paths\nConcat\n& Project Output\nAttentionLayer\nAttentionLayer\nAttentionLayer\nAttentionLayer\nAttentionLayer\nAttentionLayer\nAttentionLayer\nFigure 1: Overall architecture of SigFormer. Here we consider two layers of attentions. Signatures are truncated at ğ½-th\norder. Given that the input paths are one-dimensional, for the purpose of illustration, we represent Sig1(â„“(ğ‘‹)) âˆˆR as\n ,\nSig2(â„“(ğ‘‹))âˆˆ RâŠ—2 as\n , Sig3(â„“(ğ‘‹))âˆˆ RâŠ—3 as\n and so forth. Each layer layer in the plot, as in its original design [ 46], contains\nthree components: self-attention, multi-head attention, and feed-forward network.\nHowever, in the practical implementation, [11] resorts to the semi-\nrecurrence neural network, a Markovian style, defined as ğ›¿RNN\nğ‘˜ B\nğ¹ğ‘˜(ğ‘‹ğ‘˜,ğ›¿RNN\nğ‘˜âˆ’1 ). On the other hand, [23] presents an extension with\nhedge strategy ğ›¿ğ‘˜ = ğ¹ğ‘˜(ğ‘‹ğ‘˜,ğ›¿ğ‘˜âˆ’1,ğ»ğ‘˜âˆ’1). The primary distinction\nbetween the semi-recurrent model [11] and the recurrent model [23]\nlies in the hidden state, ğ»ğ‘˜âˆ’1, which tries to capture the dataâ€™s\ndynamics.\nIn contrast to using hidden states, our model makes a dynamic\nhedge at time ğ‘˜ using the signature of the entire trajectory up to\nğ‘‹ğ‘˜, denoted as Sig(â„“ğ‘˜(ğ‘‹))or Sig(ğ‘‹0:ğ‘˜). As a result, our model can\neffectively handle data with memories like non-Markovian paths.\nRemark 2. Compared to the recurrent architecture used in [23],\ntransformer architectures or self-attention mechanisms process the\nentire sequence as a whole rather than recursively. This allows\nparallel computation and avoids the long dependency issues of the\nrecurrent architecture. Transformers have proven to be more effec-\ntive in modeling long sequences where RNNs tend to fall short of.\nThe transformerâ€™s attention mechanism, for instance, in NLP tasks,\ndemonstrates improved performance with longer input lengths,\nrepresented by a larger number of tokens [8]. However, it is impor-\ntant to note that the computational complexity of transformers is\nquadratic with respect to the length of the sequences.\nThe design of SigFormer is closely related to the work by Kidger\net al. [25], which enables a flexible combination of neural network\ncomponents and signatures. In other words, [ 25] suggests that\nsignature computation can be integrated as a part of deep neural\nnetworks. The design choices involve using multi-layer percep-\ntrons (MLPs) or convolutional neural networks (CNNs) to either\nextract representations from signatures or to input signatures for\ncomputation.\nIn contrast, SigFormer follows a specific design, incorporating\na sophisticated transformer approach that offers two key advan-\ntages. First, transformers stand out as an attractive model for se-\nquential data compared to MLPs and CNNs. Second, while [ 25]\ntreats all terms Sig1(ğ‘‹),..., Sigğ‘(ğ‘‹)equally through concatena-\ntion, SigFormer treats these terms individually, recognizing that\nthey inherently represent different characteristics.\nNote that we do not discuss the settings with constrained trading\nor transaction costs in this paper. However, it can be easy to extend\nour models for such cases as well.\n5 EXPERIMENTAL RESULTS\nThis section presents empirical results comparing between the\nhedge strategy using SigFormer with the RNN approach from [23].\nSubsequently, we showcase a backtest conducted with real-world\ndata for hedging S&P500 index options.\nOur source code is available at https://github.com/anh-tong/\nsigformer, and it is implemented in JAX [7]. For computing signa-\ntures, we use signax 3. We observe that JAX offers faster running\ntimes in varies aspects such as simulating and solving stochastic\ndifferential equations, owing to its just-in-time (JIT) compilation\nfeatures.\n5.1 Rough Bergomi model\nConsider a rough Bergomi model (rBergomi) [4] as the underlying\npricing model Q. It is defined as\nğ‘‘ğ‘†ğ‘¡ = ğ‘†ğ‘¡\nâˆšï¸\nğ‘‰ğ‘¡(\nâˆšï¸ƒ\n1 âˆ’ğœŒ2ğ‘‘ğ‘Šğ‘¡ +ğœŒğ‘‘ğ‘ŠâŠ¥\nğ‘¡ ), (11)\nğ‘‰ğ‘¡ = ğœ‰exp\n\u0012\nğœ‚ğ‘Šğ»\nğ‘¡ âˆ’1\n2ğœ‚2ğ‘¡2ğ»\n\u0013\n. (12)\nHere, ğ» is the Hurst parameter which indicates how irregular or\nâ€œroughâ€ the instantaneous variance process is, and ğ‘Š,ğ‘Š âŠ¥are two\nindependent Brownian motions. In brief, rBergomi is specified by\nfour parameters: ğ»,ğœŒ,ğœ‚,ğœ‰ .\nPerfect hedge. The portfolio of perfect hedge consists of stock\nprice ğ‘†ğ‘¡ and its forward varianceÎ˜ğ‘¡\nğ‘‡ =\nâˆš\n2ğ»ğœ‚\nâˆ« ğ‘‡\n0 (ğ‘ âˆ’ğ‘Ÿ)ğ»âˆ’1\n2 ğ‘‘ğ‘Šğ‘Ÿ. Ac-\ncording to [23], the contingent claimğ‘ğ‘¡ = E(ğ‘”(ğ‘†ğ‘‡)|Fğ‘¡)should be in\na form ğ‘ğ‘¡ = ğ‘¢(ğ‘¡,ğ‘†[0,ğ‘¡],Î˜ğ‘¡\n[ğ‘¡,ğ‘‡])with Î˜ğ‘¡ğ‘  =\nâˆš\n2ğ»ğœ‚\nâˆ« ğ‘¡\n0 (ğ‘ âˆ’ğ‘Ÿ)ğ»âˆ’1\n2 ğ‘‘ğ‘Šğ‘Ÿ.\nThe perfect hedge is written as\nğ‘‘ğ‘ğ‘¡ = ğœ•ğ‘¥ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,Î˜ğ‘¡\n[ğ‘¡,ğ‘‡])ğ‘‘ğ‘†ğ‘¡+(ğ‘‡âˆ’ğ‘¡)1/2âˆ’ğ»âŸ¨ğœ•ğœ”ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,Î˜ğ‘¡\n[ğ‘¡,ğ‘‡]),ğ‘ğ‘¡âŸ©ğ‘‘Î˜ğ‘¡\nğ‘‡.\n(13)\nThe second term in this equation is called path-wise Gateaux de-\nrivative and is the result of functional ItÃ´ formula [23]. Note that\nin our implementation, we do not use the finite-difference method\nto compute these derivatives like in [23]. Instead, we leverage the\ncapacities of auto-differentiation in JAX, as detailed in Appendix A.\n3https://pypi.org/project/signax/\nICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, and Jaesik Choi\n15\n 10\n 5\n 0 5\n0\n500\n1000\n1500Frequency\nmodel\nModel hedge\nSigformer\nRNN\n10\n 5\n 0 5\n0\n500\n1000\n1500Frequency\nmodel\nModel hedge\nSigformer\nRNN\n5\n 0 5\n0\n500\n1000\n1500Frequency\nmodel\nModel hedge\nSigformer\nRNN\n5.0\n 2.5\n 0.0 2.5 5.0\n0\n500\n1000\n1500\n2000Frequency\nmodel\nModel hedge\nSigformer\nRNN\n(a)ğ»=0.1 (b)ğ»=0.2\n(c)ğ»=0.3 (d)ğ»=0.4\nFigure 2: Comparison of risk-adjusted PnL between three models (enhanced clarity when zoomed in).\n0 500 1000 1500 2000 2500 3000 3500 4000\nStep\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0Out-of-sample loss\nSigFormer\nRNN\n0 500 1000 1500 2000 2500 3000 3500 4000\nStep\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0Out-of-sample loss\nSigFormer\nRNN\n(a) ğ» = 0.1 (b) ğ» = 0.2\n0 500 1000 1500 2000 2500 3000 3500 4000\nStep\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0Out-of-sample loss\nSigFormer\nRNN\n0 500 1000 1500 2000 2500 3000 3500 4000\nStep\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0Out-of-sample loss\nSigFormer\nRNN\n(c) ğ» = 0.3 (d) ğ» = 0.4\nFigure 3: Comparing out-of-sample loss in various Hurst\nparameter settings. We plot the loss curves with error bars\ncomputed over 5 independent runs. Clearly, SigFormer con-\nverges at a faster rate and does not require many training\nsteps compared to the RNN approach.\nSimulation. Similar to [ 23], we adopt the approach proposed\nin [6, 37] to generate samples for rBergomi models, represented as\nğ‘†ğ‘¡. Additionally, we sample another instrument, known as the for-\nward variance process Î˜ğ‘‡fwd , which has a longer maturityğ‘‡fwd. The\nsampling process for Î˜ğ‘‡fwd follows ğ‘‘Î˜ğ‘¡\nğ‘‡fwd\n= ğ‘‘Î˜ğ‘¡\nğ‘‡fwd\nâˆš\n2ğ»ğœ‚(ğ‘‡fwd âˆ’\nğ‘¡)ğ»âˆ’1\n2 ğ‘‘ğ‘Šğ‘¡, where ğ‘¡ âˆˆ[0,ğ‘‡].\n5.2 Empirical results of hedge strategy under\nrBergomi\nIn the rough Bergomi model, we set ğœŒ = âˆ’0.7, ğœ‚ = âˆ’1.9, and ğœ‰ =\n0.2352, while varying the Hurst parameter as ğ» = 0.1,0.2,0.3,0.4.\nData Generation. In every training step, we generate 103 new\nsamples usingjax.random.fold_in(key, current_step)in JAX\ncode. This is considered as batch size for our training. For validation,\nwe use fixed 104 samples. And we use 104 out-of-sample for the\ntest dataset.\nModel Architecture. For training, we use SigFormer with a trun-\ncated signature order of 3. The attention mechanism consists of 12\nmulti-heads and a total of 5 attention layers. The recurrent neu-\nral network is constructed with 5 hidden layers containing 128\nunits each, using ReLU activation. All the model is trained with\nAdam method [27] with a learning rate 10âˆ’4. We select the market\ninformation ğ¼ğ‘˜ composed of two features: moneyness and volatility.\nFigure 2 depicts a comparison between the risk-adjusted profit\nand loss (PnL) of the model hedge, formulated using equation (13),\nand two other approaches: the RNN approach presented in [23] and\nour proposed approach - SigFormer. Notably, the upper tail of the\nPnL produced by SigFormer closely resembles a perfect hedge for\nthe cases ğ» = 0.1 and ğ» = 0.2, when the price is known; however,\nit appears to be more irregular, showing tendencies of jumps. This\ncan be attributed to the signatureâ€™s ability to effectively model such\nproperties. On the other hand, for ğ» = 0.3 and ğ» = 0.4, the results\nare comparable to those obtained with RNN models. Additionally,\nFigure 3 illustrates that SigFormer exhibits faster convergence on\nvalidation datasets that RNNs.\nSigFormer: Signature Transformers for Deep Hedging ICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA\nJan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\nğ» 0.071 0 .072 0 .051 0 .051 0 .064 0 .026 0 .069 0 .052 0 .092 0 .025 0 .067 0 .068\nğœŒ âˆ’0.856 âˆ’0.843 âˆ’0.758 âˆ’0.837 âˆ’0.851 âˆ’0.749 âˆ’0.841 âˆ’0.806 âˆ’0.837 âˆ’0.733 âˆ’0.822 âˆ’0.835\nğœ‚ 2.267 2 .284 2 .507 2 .235 2 .207 3 .136 2 .319 2 .207 2 .264 1 .953 1 .976 2 .195\nğœ‰ 0.0502 0.0502 0.3832 0.0502 0.2712 0.4712 0.0502 0.1722 0.0502 0.0682 0.2542 0.0502\nTable 1: Calibrated parameters of rough Bergomi model for the year of 2022.\nJan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nModel Hedge\nSigFormer\nRNN\nFigure 4: Wealth evolution.\n5.3 Backtest with real-world data\nThis section presents an empirical result comparing our proposed\nmodel against the existing work including [23]. We consider the\nrough Bergomi model for S&P 500 index and are interested in\nhedging S&P 500 with VIX index with maturity 1 month.\nWe conduct a backtest using the data in the whole year of 2022. In\ndetail, every month we calibrate the parameters of rough Bergomi\nmodels using [5]. Subsequently, we set the strike ğ¾ equal to the\ninitial price ğ‘†0 of that month.\nData. We collect the market quotes 4 of S&P 500 (SPX) in the\nyear of 2022. We also gather VIX index of this year as the second\ninstrument that proxies for the forward variance of rough Bergomi\nmodels.\nrBergomi parameter calibration. We adopt the approach of Bayer\nand Stemper [5] for calibrating the parameters ğ»,ğœ‚,ğœŒ and ğœ‰. In this\nmethod, a neural network ğœ‘nn is utilized to approximate implied\nvolatility surfaces under rBergomi. After training ğœ‘nn with 5 Ã—105\nsamples of rBergomi paths, we proceed with calibrating the model\nparameters using a Bayesian inference approach as described in [5,\nSection 5.2.1]. The entire implementation is carried out in JAX for\ndeep neural network training and Numpyro [41] is employed for\nthe Bayesian inference (see Table 1 and Figure 5).\nFigure 4 illustrates the wealth evolution of our hedge strategy\nbased on SigFormer, the model hedge in equation (13), alongside\nthe results from the RNN approach [23]. Remarkably, our model\nconsistently generates positive PnL outcomes, in line with our\nobservation regarding the upper tail of PnL distribution in Â§5.2 for\nsmall ğ». Intriguingly, from January to April, our model and the\nRNN approach exhibit opposite PnL trends. We hypothesize that the\n4Downloadable from www.optionsdx.com\nMoneyness\n-0.10 -0.07 -0.05 -0.02 0.00 0.03 0.05 0.08 0.10\nMaturity\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nImplied Volatility\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.9\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.01\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\n1.08\n1.09\n1.11\nmoneyness\n0.01\n0.02\n0.03\n0.04\n0.05\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\n0.14\n0.15\n0.16\n0.17\n0.18maturity\n0.1\n0.2\n0.3\n0.4\n(a) (b)\nFigure 5: Illustration of ğœ‘nn in deep calibration. (a) Implied\nvolatility surface produced by ğœ‘nn. (b) Relative error com-\npared to the true implied volatility.\nhidden states learned by the RNN may not adequately encapsulate\nthe information contained in the paths, while signatures are able to\nretain important characteristics of the path. During May and June,\nthey yield similar results. Furthermore, in June, the performance\nof both the SigFormer and RNN models was inferior to that of the\nhedge model. We posit that this poorer performance stems from\nthe unusually high magnitude of volatility (ğœ‰ = 0.4712, see Table 1)\nwhich poses difficulties for deep neural networks to process without\nadditional preprocessing.\n5.4 Attention map\nNext, we examine the attention maps in SigFormer.\nğ´ B softmax(ğ‘„ğ¾âŠ¤/\nâˆšï¸\nğ‘‘ğ‘¥).\nThese attention maps offers intriguing interpretability, revealing\nwhere the model allocates more attention while processing the\ninput. Additionally, SigFormer allows us to differentiate attention\nmaps between signature levels. In Figure 6, we present an example\nof an attention map generated for a given input. Remarkably, the\nattention map corresponding to Sig3(ğ‘‹), exhibits distinct charac-\nteristics that are closely connected to the input.\n5.5 Ablation Study\nIn this section, we conduct an ablation study to analyze the indi-\nvidual contributions of the signature computation and attention\noperator from the transformer in our proposed model. The study\ninvolves training models using each of these components separately\nto understand their impact on the overall performance.\nFirst, we create a simplified signature model with a linear output\ngiven by:\nğ‘“Signature(ğ‘‹)= âŸ¨Sig(ğ‘‹),ğ‘ŠâŸ©,\nICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Toan Tran, and Jaesik Choi\n0 10 20 30\nDay\n1.00\n1.05Moneyness\n0.2\n0.4\nVolatility\nFigure 6: Visualize attention maps. The Attention map acts\non Sig3(ğ‘‹)(bottom) paring with the input as moneyness and\nvolatility (top).\nwhere Sig(ğ‘‹)represents the signature computation of the input\ndata ğ‘‹, and ğ‘Š âˆˆğ‘‡((Rğ‘‘))is the weight matrix.\nFor the transformer model, we utilize an encoder-style trans-\nformer with a fully connected layer at the last stage. And Figure 7\nprovides clear evidence of our model outperforming the two base-\nlines.\nIn a separate ablation experiment, we explored the impact of\nvarying the number of truncated signature orders ( ğ‘€) and the\nnumber of attention blocks. Figure 8 indicates that deeper models\nconsistently outperform shallower ones. Additionally, the perfor-\nmance of signature order 3 is comparable to that of order 4, while\nthe latter is significantly slower. Hence, we find signature order 3\nto be a satisfactory choice for our model.\n0 500 1000 1500 2000\nStep\n0\n10\n20\n30\n40Out-of-sample loss\nSigFormer\nSignature\nVanilla Transformer\nFigure 7: Out-of-sample loss among three models: Sig-\nFormer, vanilla transformer [ 46], and signature with linear.\nWe consider ğ» = 0.1 in this experiment. The loss curves with\nerror bars are computed over 5 independent runs.\n0 500 1000 1500 2000\nStep\n0\n2\n4\n6\n8\n10Out-of-sample loss\ntruncated order\n2\n3\n4\n0 500 1000 1500 2000\nStep\n0\n2\n4\n6\n8\n10Out-of-sample loss\n# attention layer\n3\n5\n8\n12\n(a) (b)\nFigure 8: Out-of-sample loss when (a) varying signature order\nand (b) varying the number of attention layers.\n6 CONCLUSION AND DISCUSSION\nConclusion. We presented SigFormer, a novel deep hedging\nmodel that is carefully built on the transformer architecture from\nmachine learning and signature from rough path theory. As a result,\nwe showed via our extensive experiments that SigFormer exhibits\na strong advantage in handling irregularity, as compared to prior\ndeep hedging models. We hope our research will draw more at-\ntention from both the finance community and machine learning\ncommunity to the promising direction of exploring the advances\nin machine learning and rough path theory for addressing finance\nproblems.\nLimitations and Future Directions. Our current model addresses\ndeep hedging for a given portfolio and market state without adapt-\ning online to changes in our trading profiles and market conditions.\nThat is, SigFormer is required to retrain on every new portfo-\nlio profile and market state. As our future revenue, we envision\nnew adaptive models incorporating reinforcement learning (RL)\nfor modeling online hedging strategies [13] with SigFormer (e.g.,\npotentially via Decision Transformer [14]). In particular, we can\nemploy distributional RL [40] to estimate return distributions and\nthereby incorporate risk-adjusted returns conforming to human\ndecision.\nACKNOWLEDGMENTS\nWe gratefully acknowledge the support received for this work, in-\ncluding the Institute of Information & communications Technology\nPlanning & Evaluation (IITP) grant funded by Korean goverment\n(MSIT) under No. 2022-0-00984, which pertains to research in Artifi-\ncial Intelligence, Explainability, Personalization, Plug and Play, Uni-\nversal Explanation Platform. Additionally, we acknowledge support\nfrom the Artificial Intelligence Graduate School Program (KAIST)\nunder No. 2019-0-00075, and from the Development and Study of\nAI Technologies to Inexpensively Conform to Evolving Policy on\nEthics under No. 2022-0-00184.\nWe thank anonymous reviewers for their insightful feedback. We\nwould like to extend our appreciation to Enver Menadjiev, Artyom\nStitsyuk, and Hyukdong Kim for involvement in the early stage of\nthis project.\nREFERENCES\n[1] Imanol Perez Arribas, Cristopher Salvi, and Lukasz Szpruch. 2020. Sig-SDEs\nModel for Quantitative Finance. In Proceedings of the First ACM International\nConference on AI in Finance (ICAIF â€™20) . Association for Computing Machinery,\nNew York, NY, USA, 8 pages.\n[2] Alvaro Arroyo, Alvaro Cartea, Fernando Moreno-Pino, and Stefan Zohren. 2023.\nDeep Attentive Survival Analysis in Limit Order Books: Estimating Fill Probabil-\nities with Convolutional-Transformers. arXiv:2306.05479 [q-fin.ST]\n[3] Fazl Barez, Paul Bilokon, Arthur Gervais, and Nikita Lisitsyn. 2023. Exploring the\nAdvantages of Transformers for High-Frequency Trading. arXiv:2302.13850 [q-\nfin.ST]\n[4] Christian Bayer, Peter Friz, and Jim Gatheral. 2016. Pricing under rough volatility.\nQuantitative Finance 16, 6 (June 2016), 887â€“904.\n[5] Christian Bayer and Benjamin Stemper. 2018. Deep calibration of rough stochastic\nvolatility models. arXiv:1810.03399 [q-fin.PR]\n[6] Mikkel Bennedsen, Asger Lunde, and Mikko S. Pakkanen. 2017. Hybrid scheme\nfor Brownian semistationary processes. Finance and Stochastics 21, 4 (jun 2017),\n931â€“965. https://doi.org/10.1007/s00780-017-0335-5\n[7] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris\nLeary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of\nPython+NumPy programs. http://github.com/google/jax\nSigFormer: Signature Transformers for Deep Hedging ICAIF â€™23, November 27â€“29, 2023, Brooklyn, NY, USA\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[9] Hans Buehler and Blanka Horvath. 2022. Lecture Notes Learning to Trade I:\nStatistical Hedging. Lecture Notes Learning to Trade I: Statistical Hedging (June\n30, 2022) (2022).\n[10] Hans Buehler and Blanka Horvath. 2022. Lecture Notes Learning to Trade II:\nDeep Hedging. Lecture Notes Learning to Trade II: Deep Hedging (June 30, 2022)\n(2022).\n[11] Hans BÃ¼hler, Lukas Gonon, Josef Teichmann, and Ben Wood. 2018. Deep Hedg-\ning.\n[12] Hans BÃ¼hler, Phillip Murray, Mikko S. Pakkanen, and Ben Wood. 2022. Deep\nHedging: Learning to Remove the Drift under Trading Frictions with Minimal\nEquivalent Near-Martingale Measures. arXiv:2111.07844 [q-fin.CP]\n[13] Hans BÃ¼hler, Phillip Murray, and Ben Wood. 2023. Deep Bellman Hedging.\narXiv:2207.00932 [q-fin.CP]\n[14] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,\nPieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:\nReinforcement learning via sequence modeling. Advances in neural information\nprocessing systems 34 (2021), 15084â€“15097.\n[15] Ilya Chevyrev and Andrey Kormilitzin. 2016. A Primer on the Signature Method\nin Machine Learning.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[17] Peter K Friz and Martin Hairer. 2020. A Course on Rough Paths: With an Introduc-\ntion to Regularity Structures . Springer Nature.\n[18] Peter K Friz and Nicolas B Victoir. 2010. Multidimensional stochastic processes as\nrough paths: theory and applications . Vol. 120. Cambridge University Press.\n[19] Jim Gatheral, Thibault Jaisson, and Mathieu Rosenbaum. 2014. Volatility is rough.\narXiv:1410.3394 [q-fin.ST]\n[20] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer\nFeed-Forward Layers Are Key-Value Memories. arXiv:2012.14913 [cs.CL]\n[21] Steven L Heston. 1993. A closed-form solution for options with stochastic\nvolatility with applications to bond and currency options. The review of financial\nstudies 6, 2 (1993), 327â€“343.\n[22] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-\nforward networks are universal approximators. Neural networks 2, 5 (1989),\n359â€“366.\n[23] Blanka Horvath, Josef Teichmann, and Zan Zuric. 2021. Deep Hedging under\nRough Volatility.\n[24] AytaÃ§ Ilhan, Mattias Jonsson, and Ronnie Sircar. 2009. Optimal static-dynamic\nhedges for exotic options under convex risk measures. Stochastic Processes and\ntheir Applications 119, 10 (2009), 3608â€“3632.\n[25] Patrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, and Terry\nLyons. 2019. Deep Signature Transforms. In Advances in Neural Information\nProcessing Systems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc,\nE. Fox, and R. Garnett (Eds.). 3099â€“3109.\n[26] Patrick Kidger and Terry Lyons. 2021. Signatory: differentiable computations of\nthe signature and logsignature transforms, on both CPU and GPU. InInternational\nConference on Learning Representations .\n[27] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-\nmization. https://doi.org/10.48550/ARXIV.1412.6980\n[28] Franz J. Kiraly and Harald Oberhauser. 2019. Kernels for Sequentially Ordered\nData. Journal of Machine Learning Research 20, 31 (2019), 1â€“45.\n[29] Damian Kisiel and Denise Gorse. 2022. Portfolio Transformer for Attention-Based\nAsset Allocation. arXiv:2206.03246 [q-fin.PM]\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\nand Xifeng Yan. 2020. Enhancing the Locality and Breaking the Memory Bottle-\nneck of Transformer on Time Series Forecasting. arXiv:1907.00235 [cs.LG]\n[31] Yannick Limmer and Blanka Horvath. 2023. Robust Hedging GANs.\narXiv:2307.02310 [q-fin.CP]\n[32] Terry Lyons. 2014. Rough paths, Signatures and the modelling of functions on\nstreams. arXiv: Probability (2014).\n[33] Terry Lyons and Andrew D. McLeod. 2023. Signature Methods in Machine\nLearning. arXiv:2206.14674 [stat.ML]\n[34] Terry Lyons, Sina Nejad, and Imanol Perez Arribas. 2019. Nonparametric pricing\nand hedging of exotic derivatives.\n[35] Terry J. Lyons. 1998. Differential equations driven by rough signals. Revista\nMatemÃ¡tica Iberoamericana 14, 2 (1998), 215â€“310.\n[36] Benoit B Mandelbrot and John W Van Ness. 1968. Fractional Brownian motions,\nfractional noises and applications. SIAM review 10, 4 (1968), 422â€“437.\n[37] Ryan McCrickerd and Mikko S. Pakkanen. 2018. Turbocharging Monte Carlo\npricing for the rough Bergomi model. Quantitative Finance 18, 11 (apr 2018),\n1877â€“1886. https://doi.org/10.1080/14697688.2018.1459812\n[38] James Morrill, Adeline Fermanian, Patrick Kidger, and Terry Lyons. 2021. A\nGeneralised Signature Method for Multivariate Time Series Feature Extraction.\narXiv:2006.00873 [cs.LG]\n[39] James Morrill, Cristopher Salvi, Patrick Kidger, James Foster, and Terry\nLyons. 2021. Neural Rough Differential Equations for Long Time Series.\narXiv:2009.08295 [cs.LG]\n[40] Thanh Nguyen-Tang, Sunil Gupta, and Svetha Venkatesh. 2021. Distributional\nreinforcement learning via moment matching. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence , Vol. 35. 9144â€“9152.\n[41] Du Phan, Neeraj Pradhan, and Martin Jankowiak. 2019. Composable Effects for\nFlexible and Accelerated Probabilistic Programming in NumPyro. arXiv preprint\narXiv:1912.11554 (2019).\n[42] Allan Pinkus. 1999. Approximation theory of the MLP model in neural networks.\nActa Numerica 8 (1999), 143â€“195. https://doi.org/10.1017/S0962492900002919\n[43] Jeremy F. Reizenstein and Benjamin Graham. 2020. Algorithm 1004: The Iisig-\nnature Library: Efficient Calculation of Iterated-Integral Signatures and Log\nSignatures. ACM Trans. Math. Softw. 46, 1 (2020).\n[44] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang.\n2021. The Signature Kernel Is the Solution of a Goursat PDE. SIAM Journal on\nMathematics of Data Science (jan 2021), 873â€“899.\n[45] Anh Tong, Thanh Nguyen-Tang, Toan Tran, and Jaesik Choi. 2022. Learning\nFractional White Noises in Neural Stochastic Differential Equations. In Thirty-\nSixth Conference on Neural Information Processing Systems (NeurIPS) . https:\n//openreview.net/forum?id=lTZBRxm2q5\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeurIPS.\n[47] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and\nLiang Sun. 2023. Transformers in Time Series: A Survey. arXiv:2202.07125 [cs.LG]\n[48] Alan White and John Hull. 1993. One-Factor Interest-Rate Models and the Valua-\ntion of Interest-Rate Derivative Securities. Journal of Financial and Quantitative\nAnalysis 28 (06 1993), 235â€“254. https://doi.org/10.2307/2331288\n[49] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2022. Autoformer:\nDecomposition Transformers with Auto-Correlation for Long-Term Series Fore-\ncasting. arXiv:2106.13008 [cs.LG]\n[50] Mingxin Xu. 2006. Risk Measure Pricing and Hedging in Incomplete Markets.\nAnnals of Finance 2 (02 2006), 51â€“71. https://doi.org/10.1007/s10436-005-0023-x\n[51] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers\nEffective for Time Series Forecasting? Proceedings of the AAAI Conference on\nArtificial Intelligence.\n[52] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\nand Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long\nSequence Time-Series Forecasting. arXiv:2012.07436 [cs.LG]\n[53] Qinwen Zhu and Xundi Diao. 2023. From Stochastic to Rough Volatility: A New\nDeep Learning Perspective on Hedging. Fractal and Fractional 7, 3 (2023), 225.\nA COMPUTE PERFECT DELTA HEDGE OF\nRBERGOMI\nHere is an example code computing the derivativeğœ•ğ‘¥ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,Î˜ğ‘¡\n[ğ‘¡,ğ‘‡])\nand Gautaex derivative âŸ¨ğœ•ğœ”ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,Î˜ğ‘¡\n[ğ‘¡,ğ‘‡]),ğ‘ğ‘¡âŸ©.\n1 import jax\n2\n3 def price_fn (S_t , epsilon ):\n4 # make direction (T - t) ^ (H -1/2)\n5 a = ...\n6 Theta = Theta + a * epsilon\n7 # compute price given S and directional\n8 ...\n9\n10 price_der , path_der = jax . grad ( price_fn , (S_t , epsilon ))\nListing 1: Compute gradient\nNote that we use the approximation in the above code\nâŸ¨ğœ•ğœ”ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,Î˜ğ‘¡\n[ğ‘¡,ğ‘‡]),ğ‘ğ‘¡âŸ©â‰ˆ ğœ•ğœ–ğ‘¢(ğ‘¡,ğ‘†ğ‘¡,{Î˜ğ‘¡\nğ‘– +ğœ–ğ‘ğ‘¡\nğ‘–}ğ‘–âˆˆI)\n\f\fğœ–=0 . (14)\nHere, Iis a discretization scheme over [ğ‘¡,ğ‘‡].\nComparing to finite-difference methods, using auto-differential\nframework is more accurate, and not restricted to approximation\nerrors. However, it can be memory consuming and slower than\nfinite-different counterparts."
}