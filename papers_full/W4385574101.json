{
  "title": "Contrastive Demonstration Tuning for Pre-trained Language Models",
  "url": "https://openalex.org/W4385574101",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2960622019",
      "name": "Xiao-Zhuan Liang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2110418884",
      "name": "Siyuan Cheng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2324891497",
      "name": "Zhenru Zhang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2475478252",
      "name": "Chuanqi Tan",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W4287592659",
    "https://openalex.org/W3197503334",
    "https://openalex.org/W3203260618",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W4221021831",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W3207577858",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4387430414",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W3196437953",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W3205148446",
    "https://openalex.org/W4221160826",
    "https://openalex.org/W4221157571",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3114632476",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3201320181",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W4286905003",
    "https://openalex.org/W4285546627",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3201983976",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3196642073",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W4385970309",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4221157572",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3176489198"
  ],
  "abstract": "Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 799–811\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nContrastive Demonstration Tuning for Pre-trained Language Models\nXiaozhuan Liang1,2, Ningyu Zhang1,2∗, Siyuan Cheng1,2, Zhenru Zhang3,\nChuanqi Tan3, Huajun Chen1,2\n1Zhejiang University & AZFT Joint Lab for Knowledge Engine, China\n2Hangzhou Innovation Center, Zhejiang University, China\n3Alibaba Group, China\n{liangxiaozhuan,zhangningyu,sycheng,huajunsir}@zju.edu.cn\n{zhangzhenru.zzr,chuanqi.tcq}@alibaba-inc.com\nAbstract\nPretrained language models can be effectively\nstimulated by textual prompts or demonstra-\ntions, especially in low-data scenarios. Recent\nworks have focused on automatically searching\ndiscrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration\nare still limited. Concretely, the demonstra-\ntion examples are crucial for an excellent final\nperformance of prompt-tuning. In this paper,\nwe propose a novel pluggable, extensible, and\nefficient approach named contrastive demon-\nstration tuning, which is free of demonstration\nsampling. Furthermore, the proposed approach\ncan be: (i) Plugged into any previous prompt-\ntuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of cat-\negories. Experimental results on 16 datasets\nillustrate that our method integrated with pre-\nvious approaches LM-BFF and P-tuning can\nyield better performance1.\n1 Introduction\nPre-trained language models (PLMs) have been\napplied to widespread natural language understand-\ning and generation tasks, which are proven to ob-\ntain significant gains across benchmarks (Devlin\net al., 2019; Liu et al., 2019; Lewis et al., 2020a;\nDong et al., 2019; Bao et al., 2020; Zhang et al.,\n2022c; Xie et al., 2022a; Zhang et al., 2022a).\nOne paradigm of PLMs is the pre-train—fine-tune,\nwhich has become the de facto standard for natural\nlanguage processing (NLP), where task-specific ob-\njectives and additional parameters are leveraged in\nthe tuning procedure. Recently, the paradigm of the\nadaptation of PLMs has been shifting. A new fine-\ntuning methodology named prompt-tuning with a\nnatural language prompt and a few demonstra-\ntions has made waves in the NLP community by\n∗ Corresponding author\n1Code is available in https://github.com/zjunlp/\nPromptKG/tree/main/research/Demo-Tuning.\ndemonstrations\ninput \nvirtual demo \nRandom Similarity-based Demo-Tuning (ours)\nFigure 1: Comparison among current sampling strate-\ngies on demonstration-based learning. Compared to\nrandom and similarity-based sampling, demo-tuning\ncan obtain better demonstration distributions.\nproving astounding few-shot capabilities on myr-\niad language understanding tasks. Further studies\ntry to mitigate the labour-intensive prompt engi-\nneering with discrete prompt searching (Shin et al.,\n2020) or continuous prompt optimization (Liu et al.,\n2021d; Li and Liang, 2021; Hambardzumyan et al.,\n2021a; Zhong et al., 2021). However, few stud-\nies have focused on the demonstration, which is\nan indispensable component in prompt-oriented\nmethodologies.\nIn previous studies, demonstrations are sampled\nexamples in the training set. GPT-3’s naive “in-\ncontext learning” paradigm picks up to 32 ran-\ndomly sampled instances as demonstrations and\ndirectly concatenates them with the input sequence\n(Liu et al., 2021a; Min et al., 2022). Since infor-\nmative demonstrations are crucial for model perfor-\nmance, Gao et al. (2021a) develop a refined strat-\negy via sampling input pairs with similar examples,\nthereby providing the model with more discrimi-\nnative comparisons. However, it is still not guar-\nanteed to prioritize the most informative demon-\nstrations as (1) the similarity-based sampling may\nobtain degraded demonstrations in different classes\nbut have similar distances to the input; (2) the num-\nber of usable demonstrations is still bounded by\n799\nthe model’s maximum input length. For exam-\nple, as shown in Figure 1, the purple lines refer to\nthe random sampling while the blue lines indicate\nsimilarity-based sampling. Note that similarity-\nbased sampling may obtain examples very similar\nto the input sequence. However, those sampled\nexamples with different labels may tend to have a\nsimilar representation and thus confuse the discrim-\ninability of the model. Moreover, for datasets with\nmany classes, it is still non-trivial to concatenate all\nsampled demonstrations. Those above-mentioned\nchallenges hinder the applicability of demonstra-\ntion in prompt-tuning.\nTo address those issues, in this paper, we pro-\npose contrastive DEMOnstration Tuning (Demo-\ntuning) for pre-trained language models. Specif-\nically, we leverage learnable continuous embed-\ndings (e.g., one or two learnable tokens) as virtual\ndemonstrations to relax the maximum number of\ncategories. We concatenate those virtual demon-\nstrations to the input sequence; thus, our approach\ncan be extended to a wide variety of classification\ntasks with many categories. To optimize those\ncontinuous embeddings, we explore a simple con-\ntrastive framework without negative pairs (Grill\net al., 2020) since it is difficult to find an appropri-\nate negative pair in semantic space for NLP. In each\ntraining batch, we randomly sample a real example\nand regard the virtual and real examples as positive\npairs. With contrastive learning, we can obtain in-\nformative, optimized virtual demonstrations with\nmore discriminative comparisons.\nWe conduct extensive experiments on 16 NLP\ndatasets. Our contrastive demonstration tuning can\nyield better performance when integrated with pre-\nvious prompt-based methods (e.g., LM-BFF (Gao\net al., 2021a), P-tuning (Liu et al., 2021d)). More-\nover, our approach can be applied to datasets with\nmany categories and outperform baselines. Note\nthat our approach is model-agnostic and can be\nplugged into lots of prompt-based methods without\nthe effort to select suitable demonstrations. The\nmain contributions of this study are as follows:\n• We propose a pluggable, extensible, and effi-\ncient approach to contrastive demonstration\ntuning for pre-trained language models. To\nthe best of our knowledge, optimizing demon-\nstration is also a new branch of research that\nhas not been explored in language model\nprompting.\n• We propose virtual demonstration and lever-\nage contrastive learning to obtain informative\ndemonstrations and also relax the maximum\nnumber of categories in classification tasks.\n• A systematic evaluation of 16 NLP datasets\nshows that the proposed simple-yet-effective\napproach contributes towards improvements\nacross all these tasks.\n2 Related Work\n2.1 Prompt-tuning\nWith the prevalence of GPT-3 (Brown et al., 2020),\nprompting PLMs for few-shot learning has become\na new, popular learning paradigm in natural lan-\nguage processing (Schick and Schütze, 2021; Tam\net al., 2021; Liu et al., 2021b) and appealed to\nresearchers. Recently, prompt-tuning has been ap-\nplied to various NLP tasks, such as named entity\nrecognition (Cui et al., 2021; Chen et al., 2021b;\nZhou et al., 2021; Ma et al., 2022), entity typing\n(Ding et al., 2021), relation extraction (Han et al.,\n2021), event extraction (Hsu et al., 2021; Ye et al.,\n2021), sentiment analysis (Li et al., 2021), machine\ntranslation (Tan et al., 2021), and knowledge graph\ncompletion (Xie et al., 2022b). Schick and Schütze\n(2021, 2020) propose the PET, which reformulates\nthe NLP tasks as cloze-style questions and yields\nsatisfactory performance. Tam et al. (2021) fur-\nther propose a denser supervision object during\nfine-tuning to improve the PET.\nNote that handcrafting a best-performing prompt\nis like finding a needle in a haystack, which fa-\ncilitates the labor-intensive prompt engineering,\nThus, recent studies (Qin and Eisner, 2021; Ham-\nbardzumyan et al., 2021b; Ye et al., 2022; Chen\net al., 2021c) conducted in this field have been\nfocused on automatically searching the prompts.\nShin et al. (2020) propose AUTOPROMPT, which\nis a gradient-based method to acquire templates\nand label words for prompt-tuning. Wang et al.\n(2021) propose EFL, which reformulates the NLP\ntask as an entailment one and turns small LMs\ninto better few-shot learners. Additionally, Gao\net al. (2020) propose LM-BFF—better few-shot\nfine-tuning of language models, which utilizes a\ngeneration model to obtain templates and a refined\nstrategy for dynamically and selectively incorpo-\nrating demonstrations into each context. However,\nit is sub-optimal for the discrete prompt searching\ndue to the continuous nature of neural networks.\nTo overcome these limitations, Liu et al.\n(2021d,c) propose P-tuning to to automatically\n800\nsearch prompts in the continuous space. Li and\nLiang (2021) propose prefix-tuning, which opti-\nmizes a sequence of continuous task-specific vec-\ntors and keeps language model parameters frozen.\nLester et al. (2021a) leverage a mechanism to learn\n“soft prompts” to condition frozen language models.\nZhang et al. (2021) propose a differentiable prompt\nlearning method for few-shot NLP with optimized\nprompt templates as well as labels. Vu et al. (2021)\npropose SPoT, which learns a prompt on one or\nmore source tasks and then uses it to initialize the\nprompt for a target task to boost the performance\nacross many tasks. More related works including\nWARP (Hambardzumyan et al., 2021a) and OP-\nTIPROMPT (Zhong et al., 2021) also propose to\nleverage continuous templates, which is more ef-\nfective than discrete prompt search. To conclude,\nmost of the existing works try to obtain optimized\nprompts for widespread NLP tasks; however, few\nstudies have focused on the demonstration, which\nis an indispensable component in prompt-oriented\nlearning.\nOur work is orthogonal to previous prompt-\ntuning approaches, which are aimed at optimizing\nprompts. The major differences between virtual\ndemonstration and continuous prompts are that: 1)\nthey have a wholly different training strategy since\ncontinuous prompts are optimized via backpropaga-\ntion with a training set, while our approach utilizes\ncontrastive learning. 2) our approach requires no\nexternal architecture (e.g., LSTM in P-tuning), thus,\nmaking it efficient and pluggable to any prompt-\ntuning approaches. To date, Lee et al. (2021) is\nthe only approach that studies the demonstration\nand presents a simple demonstration-based learning\nmethod for named entity recognition. Apart from\nLee et al. (2021), our approach focus on general\nNLP classification tasks. Moreover, we propose\nvirtual demonstrations with contrastive learning\nstrategies, which can obtain better demonstrations\nand also relax the maximum number of categories\nin datasets.\n2.2 Contrastive Learning\nContrastive learning has been long considered ef-\nfective in learning meaningful representations. In\nthe early stage, Mikolov et al. (2013) propose to\nlearn word embeddings by regarding words nearby\na target word as a positive instance while others\nas negative. Logeswaran and Lee (2018); Chen\net al. (2021a) further generalize this approach to\nlearn sentence representations. Recently, Kim et al.\n(2021) propose a contrastive learning method that\nmakes use of a self-guidance mechanism. Yan et al.\n(2021) propose ConSERT, a contrastive framework\nfor self-supervised sentence representation transfer.\nGiorgi et al. (2021) propose DeCLUTR: Deep Con-\ntrastive Learning for Unsupervised Textual Rep-\nresentations. Gao et al. (2021b) leverage dropout\nas mimimal data augmentation and propose Sim-\nCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence em-\nbeddings.\nOn the other hand, contrastive learning has been\nalso appealed to the computer vision community\n(Jaiswal et al., 2020; Liu et al., 2020). Chen et al.\n(2020) propose SimCLR: a simple framework for\ncontrastive learning of visual representations with-\nout requiring specialized architectures or a mem-\nory bank. Chen and He (2021) observe that simple\nsiamese networks can learn meaningful representa-\ntions even using none of the negative sample pairs,\nlarge batches, and momentum encoders.\nOur work is related to Grill et al. (2020), a\nnon-contrastive self-supervised learning approach,\nwhich relies on two neural networks, referred to as\nonline and target networks, that interact and learn\nfrom each other. However, as opposed to this ap-\nproach, we utilize the encoder in the same state\nwhile Grill et al. (2020) leverage two networks in\nthe different states. Moreover, we focus on demon-\nstration optimization in prompt-tuning for NLP,\nincluding learning informative demonstrations and\nacquiring prompt temples and label tokens.\n3 Preliminaries\nIn this work, we focus on classification tasks\nin the few-shot setting, including text classifica-\ntion and natural language understanding, where\nthe input xin is either a sentence xin = x1 or a\npair of sentences xin = (x1,x2). Here, we let\nDtrain = {(xi,yi)}K×|Y|\ni denote the training set\nof a downstream task composed of only K train-\ning examples per class, where Yis label space\nof the task. Given a pre-trained language model\ncomprised of two stages: an encoder f(·) and a\nclassifier g(·) 2, we encode the input xin to a se-\nquence of hidden vectors {hk ∈ Rd}and take\n2In standard fine-tuning, the classifier is a set of randomly\ninitialized parameters Wo ∈ R|Y|×d with softmax function.\n801\n   terrible (label: negative) \n   great (label: postive)    Encoder\nLabel word \n[CLS] The movie has lots of fabulous music. It is  [MASK] . [SEP] The story is uninteresting. It is terrible. [SEP] Funny and ultimately sobering film. It is great. [SEP]\nInput Template Demonstration for label: terrible Demonstration for label: great \n(a) Prompt-tuning with demonstrations\n[CLS] A playful comedy. It is  [MASK] . [SEP] [ ] [ ] ... [ ] [ ] [ ] ... [ ]\n[CLS] A playful comedy. It is  [MASK] . [SEP] The story is uninteresting. It is terrible. [ ] [ ] ... [ ]\nDemonstration for label: terrible Virtual Demo \nVirtual Demo Virtual Demo \nMLM head     (label: negative) \n    (label: postive) \nGold label \nSupervised loss\nTraining contrastive instance\nEncoder\n    (label: negative) \n    (label: postive) \nGold label \nMLM head\n(b) Demonstration-tuning (ours)\nContrastive loss Supervised loss\n   terrible (label: negative) \n   great (label: postive)    \nLabel word \nTraining final loss\nFigure 2: An illustration of (a) prompt-tuning with demonstrations, and (b) our proposed contrastive demonstration\ntuning (demo-tuning). Note that we regard the input with virtual demonstration and a random sampled real\ndemonstrations as positive pairs for contrastive learning.\nthe hidden vector h[CLS] = f(xin) of [CLS] 3\nthrough classifier to obtain the probability distribu-\ntion p(y|x) =g(h[CLS]) over y∈Y.\nPrompt-based Fine-tuning Prompt-based fine-\ntuning (Schick and Schütze, 2021; Gao et al.,\n2021a) is an efficient work by designing cloze-style\ntemplate T and verbalizer M: Y→V mapping\ntask labels to individual words from vocabulary\nVof pre-trained language model to fill the gap\nbetween masked LM objective of pre-trained lan-\nguage model and downstream fine-tuning objec-\ntive.\nTemplate In prompt-based fine-tuning paradigm,\ntemplate T is mainly comprised of inputs xin and\na prompt P = [Pi]m\ni , where the prompt could be a\nseries of discrete tokens (Schick and Schütze, 2021)\nor continual pseudo tokens (Liu et al., 2021d). For\ninstance, in the sentiment analysis task (see Fig-\nure 2), a template with handcraft prompt may be:\nT(x) = [CLS]x1, It was[MASK].[SEP] where \"It\nwas ... .\" is prompt and [MASK] is target which cast\nclassification task as a language modeling task.\nVerbalizer A verbalizer Mdefines a mapping\nof label tokens from label space of a specific\ntask. In Figure 2a, the verbalizer maps \" nega-\ntive/postive\" to \" terrible/great\". In this way, we\ncould re-use the output weight Wv ∈Rd×|V| ref-\nered MLM head used in pre-training and model\nthe probability of predicting token M(y) ∈V as\n3For simplicity we will denote the hidden vector h[CLS] of\ncertain input xi through encoder using hi.\np(y|x) =g(h[MASK]) on hidden vector h[MASK].\nDemonstration Let Dc\ntrain be the subset of all ex-\namples of class c. We sample demonstrations dc =\n(x(c)\nin ,y(c)) ∈Dc\ntrain and convert it to T(x(c)\nin ,y(c))\nin which [MASK] is replaced by M(y(c)). We then\ncombine the original template T with templates\nabove in all classes to form T∗(xin), which will be\nused as a template during prompt-based tuning and\ninference (See Figure 2).\n4 Contrastive Demonstration Tuning\nIn this work, we focus on how to learn a com-\npact and differentiable virtual demonstration to\nserve as prompt augmentation instead of design-\ning specific sampling strategies for demonstration-\nbased learning. We propose a learning framework\nbased on a contrastive learning approach that can\nbe compatible with the current prompt-based learn-\ning paradigm. This section introduces the concepts\nof contrastive demonstration tuning (Demo-tuning)\nand provides details of this approach.\nVirtual Demonstration Let [D(c)\ni ]n\ni refer to the\nvirtual demonstration of the cth class where nis a\nhyper-parameter to set the length of virtual demon-\nstration, which is far less than the length of real\ndemonstration. For instance, given a template of\nbinary classification task (see Figure 2) as:\n˜T(x) =T(x) ⊕[D(1)] ⊕[D(2)] (1)\nwhere ⊕denotes concatenation of input sequences.\n[D(1)] and [D(2)] respectively denote the virtual\n802\ndemonstrations of two classes. Virtual demonstra-\ntions could be so flexible that can be integrated to\nwide variety of prompt learning approaches (Liu\net al., 2021d; Lester et al., 2021b).\nNext, we study how to obtain the optimal virtual\ndemonstrations, which are initialized as a series of\npseudo tokens at the start of fine-tuning. To ad-\ndress this challenging problem, we propose to use\ncontrastive learning, which aims to obtain effec-\ntive representation by pulling semantically close\nneighbors together. Intuitively, we believe the opti-\nmal virtual demonstrations may be analogous with\n“prototype” (Snell et al., 2017), the representative\nfor corresponding class, and we will discuss in §6.\nPositive Instances A key element of contrastive\nlearning is how to construct reasonable\n(\nxin,x+\nin\n)\npairs. Here, we design a new template ˜T+(x)\nbased on template ˜T(x) by randomly replacing one\nof virtual demonstrations [D(c)] with real demon-\nstration dc as shown in the Figure 2b:\n˜T+(x) =T(x) ⊕T(x(1)\nin ,y(1)) ⊕[D(2)] (2)\nwhere [D(1)] is replaced with a demonstrationd1 of\nclass “terrible”. Using this template, we could con-\nvert input xin to corresponding positive example\nx+\nin, i.e.,\n(\n˜T(xin),˜T+(xin)\n)\nis a positive training\ninstance. In this way, aligning virtual demonstra-\ntion [D(c)] with dc, the only difference between\nxin and x+\nin, and pulling representations (hin,h+\nin)\ncloser in semantic space could effectively alleviate\nthe problem that the existing of terrible or irrelevant\ndemonstration by previous sampling strategies.\nOptimization Similar to Chen et al. (2020),\nwe can randomly sample a minibatch of N ex-\namples from Dtrain to construct positive pairs\n{(xi,x+\ni )}N\ni=1 and take a cross-entropy objective\nwith in-batch negatives for (xi,x+\ni ):\nℓi = −log exp(sim(hi,h+\ni )/τ)∑N\nj=1 exp(sim(hi,h+\nj )/τ)\n(3)\nwhere τ denotes a temperature parameter and\nsim(hi,hj) is the cosine similarity hT\ni hj\n∥hi∥·∥hj∥. The\nnegative pairs are composed of two different exam-\nples with the same demonstration in a minibatch.\nIn this work, we also explore a simple contrastive\nframework without negative pairs 4 similar to re-\ncent non-contrastive self-supervised learning (Grill\n4This is the default contrastive learning method in all ex-\nperiments.\net al., 2020). Regarding the difficulty to find a ap-\npropriate negative pair in semantic space for NLP,\nspecially in few-shot setting, we only construct pos-\nitive pairs and define the following mean squared\nerror between hi and h+\ni with ℓ2-normalization,\nℓi = ∥hi −h+\ni ∥2\n2 = 2−2 · hT\ni h+\ni\n∥hi∥2 ·∥h+\ni ∥2\n(4)\nwhere hi and h+\ni are obtained through encoder f(·)\nin the same state different from Grill et al. (2020)\nwhich encodes xi and x+\ni through two networks\nin the different states (online network and target\nnetwork).\nWhen supervised examples Dtrain are available,\nthe pre-trained language model could be fine-tuned\nto minimize the joint objective comprised of cross-\nentropy and contrastive objective of Eq. (4). In\nthis way, during inference, we can concatenate the\ninput xin with trained virtual demonstrations in\ntemplate ˜T(x), which does not need to sample real\ndemonstrations. Besides, we provide empirical\nanalysis of negative sampling in §5.4.\n5 Experiments\n5.1 Datasets\nTo evaluate Demo-tuning, we conduct experiments\non 6 tasks from GLUE leaderboard (Wang et al.,\n2019) and 10 other popular classification tasks, in-\ncluding natural language inference (SNLI, MNLI,\nQNLI, RTE), sentiment classification (SST-2, SST-\n5, MR, CR, MPQA), paraphrase and similarity\n(MRPC, QQP) and sentence classification (DBpe-\ndia, Subj, TREC, Yahoo! Answers). The detailed\nstatistics are in Appendix A.\n5.2 Settings\nEvaluation During training, we follow the eval-\nuation protocol adopted in Gao et al. (2021a) and\nassume a development set Ddev for model selection\nand hyper-parameter tuning, where the size is same\nwith Dtrain, i.e., |Ddev|= |Dtrain|. For every exper-\niment, we measure average performance across 5\ndifferent randomly sampled Dtrain and Ddev splits\nusing a fixed set of seeds.\nHyperparameter Selection We implement our\nframework and reproduce P-tuning by ourselves\nusing PyTorch (Paszke et al., 2019) and Hugging-\nFace (Wolf et al., 2020). The main results of LM-\nBFF in Table 1 are from Gao et al. (2021a). We use\nRoBERTaLARGE (Liu et al., 2019) as pretrained\n803\nSST-2 SST-5 MR CR MPQA Subj TREC\n(acc) (acc) (acc) (acc) (acc) (acc) (acc)\n“GPT-3” in-context learning 84.8 (1.3) 30.6 (0.9) 80.5 (1.7) 87.4 (0.8) 63.8 (2.1) 53.6 (1.0) 26.2 (2.4)\nFine-tuning 81.4 (3.8) 43.9 (2.0) 76.9 (5.9) 75.8 (3.2) 72.0 (3.8) 90.8 (1.8) 88.8 (2.1)\nLM-BFF (w/ Demo) 92.6 (0.5) 50.6 (1.4) 86.6 (2.2) 90.2 (1.2) 87.0 (1.1) 92.3 (0.8) 87.5 (3.2)\nP-tuning (w/ Demo) 92.7 (1.4) 47.7 (3.3) 87.5 (1.3) 90.6 (1.4) 84.3 (0.8) 91.4 (1.7) 88.1 (2.7)\nDemo-tuning (LM-BFF) 93.2 (0.4) 50.1 (0.4) 87.9 (0.6) 91.5 (0.6) 85.9 (1.5) 92.3 (0.6) 90.1 (2.7)\nDemo-tuning (P-tuning) 92.7 (0.6) 48.7 (2.0) 86.4 (1.1) 91.4 (0.8) 86.0 (1.6) 92.0 (0.6) 90.7 (4.5)\nMNLI MNLI-mm SNLI QNLI RTE MRPC QQP\n(acc) (acc) (acc) (acc) (acc) (F1) (F1)\n“GPT-3” in-context learning 52.0 (0.7) 53.4 (0.6) 47.1 (0.6) 53.8 (0.4) 60.4 (1.4) 45.7 (6.0) 36.1 (5.2)\nFine-tuning 45.8 (6.4) 47.8 (6.8) 48.4 (4.8) 60.2 (6.5) 54.4 (3.9) 76.6 (2.5) 60.7 (4.3)\nLM-BFF (w/ Demo) 70.7 (1.3) 72.0 (1.2) 79.7 (1.5) 69.2 (1.9) 68.7 (2.3) 77.8 (2.0) 69.8 (1.8)\nP-tuning (w/ Demo) 71.0 (2.2) 70.8 (1.7) 78.7 (1.5) 68.2 (2.1) 70.8 (3.0) 75.0 (13.8) 66.6 (2.9)\nDemo-tuning (LM-BFF) 71.0 (2.0) 72.8 (1.5) 78.7 (1.9) 73.1 (1.8) 70.0 (3.4) 78.4 (2.3) 70.2 (1.7)\nDemo-tuning (P-tuning) 71.3 (1.3) 73.1 (1.9) 76.4 (1.7) 71.6 (3.0) 69.8 (4.6) 78.4 (4.4) 68.9 (2.9)\nTable 1: Comparison of performance of our approach with several baselines across 14 text classification tasks in\nfew-shot setting. We report mean (and standard deviation) results of 5 random seeds. LM-BFF (w/ Demo) and\nP-tuning (w/ Demo): prompt-tuning methods (LM-BFF and P-tuning) using demonstration in context with manual\ntemplate used in Gao et al. (2021a). Demo-tuning (LM-BFF) and Demo-tuning (P-tuning): Our proposed approach\nrespectively based on LM-BFF and P-tuning.\nlanguage model and set K = 16. For the length\nnof virtual demonstration per class, we select it\nfrom candidate set {1,2,3,5}.\n5.3 Main Results\nWe apply our method to two popular prompt-based\ntuning techniques, LM-BFF and P-tuning, and com-\npare them to a number of baselines, namely: (1)\nstandard fine-tuning in the few-shot setting; (2)\n\"GPT-3\" in-context learning: zero-shot prediction,\nwhich concatenates prompt (e.g., randomly sam-\npled demonstrations); (3) LM-BFF using demon-\nstration in context with a manual template. (4) P-\ntuning using demonstration in context with a man-\nual template, where we do not specifically search\nthe optimal length of continual prompt and fixed\nthe length mto 4 in all tasks.\nIn Table 1, we report the performance of the\nbaseline approaches and our two variants. First,\nin-context learning could achieve comparable or\neven higher performance to the standard fine-tuning\nmethod and prompt-tuning methods (LM-BFF and\nP-tuning); using demonstration in context bring\nconsistent improvement in a majority of tasks,\nwhich means that demonstration is worth being\nexploited.\nSecond, our approach based on two prompt-\nbased tuning techniques could consistently outper-\nform the vanilla methods. In detail, Demo-tuning\nDBpedia Yahoo!\nFine-tuning 98.2 (0.1) 66.4 (1.0)\nLM-BFF 98.1 (0.2) 66.2 (1.0)\nLM-BFF (w/ Demo) - -\nP-tuning 98.2 (0.2) 67.0 (0.8)\nDemo-tuning (LM-BFF) 98.3 (0.1) 67.9 (0.8)\nDemo-tuning (P-tuning) 98.3 (0.1) 68.4 (1.1)\nTable 2: Performance on multi-class sentence classifi-\ncation, DBpedia and Yahoo!. The size of label space\n|Y|are respectively 14 and 10. Due to sequence length\nlimitation in pretrained language model, LM-BFF with\ndemonstration-based learning can not be applied here.\nbased LM-BFF improves the average score by 0.75,\ncompared with LM-BFF with the demonstration in\nan input context. More importantly, Demo-tuning\nis flexible and orthogonal to most fine-tuning meth-\nods. Here, for evaluating the compatibility, we\ncombine Demo-tuning with P-tuning (Liu et al.,\n2021d), which could lead to a 1.0 average score\nimprovement in total. In this work, we do not\nspecially design template for P-tuning5. Although\ntemplates for P-tuning and prompt length are sub-\noptimal, we find that Demo-tuning with P-tuning\n5We simply construct template T (x) for P-tuning as\n[CLS]x1[PROMPT][MASK][SEP] in single-sentence tasks and\n[CLS]x1,[MASK]? x2[PROMPT][SEP] in sentence pair tasks,\nwhere [PROMPT] denotes continual prompt.\n804\nSST-2 TREC SNLI MRPC\nLM-BFF 92.7 84.8 77.2 74.5\nRandom 92.3 85.6 78.8 70.9\nFilter-based (RoBERTa) 92.7 83.4 79.5 76.6\nFilter-based (SBERT) 92.6 87.5 79.7 77.8\nVirtual Demo (w/ Mean) 90.9 85.9 75.3 66.4\nVirtual Demo (w/ CL)93.2 90.7 78.7 78.4\nTable 3: Impact of demonstration sampling strategies.\nRandom: uniform sampling from each class. Filter-\nbased: filtered sampling strategy proposed in Gao et al.\n(2021a) respectively based on RoBERTa and SBERT\n(Reimers and Gurevych, 2019). Virtual Demo (w/\nmean): averaing the representations of instances with\nthe same label as virtual demonstration.\nleads to consistent gains in a majority of tasks.\nThird, an advantage of our proposed virtual\ndemonstration is that it could be well applied for\nmulti-class sentence classification tasks. Table 2\ngives the results of Demo-tuning compared to stan-\ndard fine-tuning and prompt-based tuning. Due\nto the limitation of the model’s input length, in-\ncontext learning and LM-BFF with demonstration\ncould not be applied in this scenario. We notice\nthat while the performance of LM-BFF is worse\nthan fine-tuning, Demo-tuning based on LM-BFF\nimproves the score by 1.7 in Yahoo and achieves a\nbetter score compared to fine-tuning.\n5.4 Analysis of Virtual Demonstration\nThe selection of demonstration is crucial for\ndemonstration-based learning (e.g., in-context\nlearning and LM-BFF with demonstration). Next,\nwe compare and discuss our proposed virtual\ndemonstration with current approaches.\nDemonstration Sampling Table 3 provides the\nimpact of demonstration sampling strategies. Dur-\ning inference, our proposed virtual demonstration\nobtained by contrastive learning during training\ncould be an alternative to real demonstrations,\nwhich could be viewed as an implicit sampling\nstrategy. We compare our method with previous\nsampling strategies based on LM-BFF.\nWhile the performance of uniform demonstra-\ntion sampling from each class is better than the\nvanilla LM-BFF in TREC and SNLI, we notice that\non the MRPC task, this method causes severe accu-\nracy loss, which is up to 3.6. We think that random\nsampling is prone to generate irrelevant informa-\ntion in demonstrations. To address the above is-\nsue, Gao et al. (2021a) utilize RoBERTa or SBERT\nQQP SNLI Subj\nDifferent dataset for evaluation\n50\n60\n70\n80\n90Accuracy (%)\nLM-BFF\nDemo-T uning (w/ neg)\nDemo-T uning (w/o neg)\nFigure 3: Ablation study on virtual demonstration opti-\nmization w/ Vs. w/o negative sampling. Demo-tuning\n(w/ neg): using conventional contrastive learning with\nnegative samples to optimize virtual demonstration.\nDemo-tuning (w/o neg): Demo-tuning using our simpli-\nfied optimization method without negative samples.\n(Reimers and Gurevych, 2019) to select relevant\ndemonstrations to examples. The filter-based sam-\npling strategy could achieve consistent gains in the\nmajority of tasks, which yields the highest improve-\nment with 3.6 on the TREC task. We consider that\nthis KNN-style method, which concatenates exam-\nples and demonstrations that are semantically close\nto examples, could promote language models to\ndecipher meaningful patterns.\nVirtual demonstration, an alternative to the real\ndemonstration during inference, i.e., avoiding com-\nplex sampling steps, could achieve gains in most\ntasks. Besides our proposed method, We design a\nsimple strategy to construct virtual demonstrations\nvia averaging the representations of instances with\nthe same label. We notice that constructing virtual\ndemonstration with simple averaging of instances\ncauses poor performance in most tasks. However,\nour method with contrastive learning is more pre-\ndominant than previous approaches. The only ex-\nception is SNLI, which score only is comparable\nwith random sampling. We hypothesize that this\nis caused by some confusion issues, which may\nexist in filter-based strategy regarding semantically\ncloseness among contrastive demonstrations.\nOptimization w/ Vs. w/o Negative Samples Fig-\nure 3 gives the results of comparison between vir-\ntual demonstration optimization with negative sam-\npling and without negative sampling. We conduct\nexperiments with different optimization strategies\non 3 tasks. We find that optimizing the objective\n805\n1 2 3 5 10 15 20\nLength of Virtual Demonstration per class\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80Accuracy (%)\nQNLI\nDemo-T uning (w/o CL)\nDemo-T uning (w/ CL)\n1 2 3 5 10 15 20\nLength of Virtual Demonstration per class\n60\n65\n70\n75\n80\n85\n90Accuracy (%)\nMR\nDemo-T uning (w/o CL)\nDemo-T uning (w/ CL)\nFigure 4: Ablation study on length nof virtual demon-\nstration per class. Demo-tuning (w/o CL): Demo-tuning\nwithout contrastive learning (CL), i.e., virtual demon-\nstration will degrade into continual prompt.\nof Eq.3, i.e., conventional contrastive learning with\nnegative samples, causes dramatically performance\ndegradation, in which the average score is even\nlower than LM-BFF’s. We think there are two\npossible reasons: (1) In NLP tasks, finding a se-\nmantically reasonable negative pair is difficult, es-\npecially in the few-shot setting; (2) Negative pairs\nmay become example-demonstrations pairs with-\nout specific limitation, which will cause a certain\nconfusion to model. Moreover, our goal is to ob-\ntain optimal virtual demonstrations for downstream\ntasks. Using contrastive optimization without neg-\native sampling may be a more suitable solution.\nDemonstration Length Figure 4 shows the ab-\nlation study on length nof virtual demonstration\nper class. We compare Demo-tuning with its vari-\nant without contrastive learning in different set-\ntings about length n. It is noteworthy that without\ncontrastive learning, a virtual demonstration will\ndegrade into a continual prompt. We find that a\nrelatively shorter length (e.g., 2 or 3) could gain\nstable improvement of performance in QNLI and\nMR. Oppositely, a larger length (e.g., 20) may de-\ncrease the performance. We consider that as the\nlength of virtual demonstration increases, it will\nintroduce more parameters into the model, mak-\ning it challenging to learn from a small amount of\nannotated data. Demo-tuning could achieve con-\nsistent improvement in different lengths compared\nto its variant. Hence, we can conclude that virtual\ndemonstration optimized by simple contrastive\nframework plays a different role from continu-\nous prompt.\n6 Discussion\nWe will discuss several favorable properties of con-\ntrastive demonstration tuning and present some\nopen problems:\nPossible Supplement for Parameter-efficient\nFine-tuning. Previous studies (Liu et al., 2021d;\nLi and Liang, 2021) have demonstrated the\neffectiveness of prompt-tuning (e.g., P-tuning,\nPrefix-tuning) as an parameter-efficient fine-tuning\nmethodology for huge PLMs. Our approach can\nserve as a supplement or parameter-efficient fine-\ntuning via only tuning demonstration with PLM\nfixed. We leave this for future work.\nRelation to Prototype Learning. In §4, we note\nthat the optimal virtual demonstrations may be anal-\nogous with “prototype” (Snell et al., 2017), repre-\nsentative for corresponding class. Our approach\nmay have connections to prototype learning, and\nfurther empirical and theoretical analysis should be\nconducted.\nDemonstration as External Knowledge. Recall\nthat those concatenated demonstrations are simi-\nlar to previous studies such as RAG (Lewis et al.,\n2020b), REALM (Guu et al., 2020) which retrieve\nand concatenate relevant texts as external knowl-\nedge (Zhang et al., 2022b). We think that it is also\ninteresting to investigate novel knowledge injection\napproaches via demonstration.\nWe further discuss a few weaknesses of our\nmethod in its current form and look into some pos-\nsible avenues for future work. On the one hand,\nour work still suffers from biased/long-tailed label\ndistribution. Note that we obtain optimized vir-\ntual demonstration via contrastive learning; thus,\nthose virtual demonstrations of classes with many\nsamples may dominate the training stage. This lim-\nitation might be ameliorated with weighted sam-\npling strategies. On the other hand, our approach\ncannot directly handle structure prediction tasks.\nIntegrating demonstration with prefix-tuning-based\nmethods may help to mitigate such limitations.\n7 Conclusion and Future Work\nIn this work, we propose contrastive demonstra-\ntion tuning, a simple model-agnostic approach for\npre-trained language models, which improves state-\nof-the-art prompt-tuning performance without the\nnecessity of demonstration selection. In the fu-\nture, we plan to explore the following directions:\n1) studying the connection between virtual demon-\nstration and prototypes and theoretically analyzing\nthe optimal solution of demonstration for prompt-\ntuning. 2) applying our work to more NLP tasks\nand trying to adapt to natural language generation.\n806\n8 Limitations\nOur contrastive demonstration tuning has limita-\ntions. Firstly, our model leverages the pre-trained\nlanguage model; thus, it is necessary to cost GPU\nresources. Besides, in few-shot settings, the perfor-\nmance gains are still limited with virtual demon-\nstrations learned in only a few training instances. It\nis worth studying retrieving relevant context from\nthe internet as “demonstrations” to help efficient\nNLP.\nAcknowledgment\nWe want to express gratitude to the anonymous\nreviewers for their kind comments. This work\nwas supported by National Natural Science Foun-\ndation of China (No.62206246, 91846204 and\nU19B2027), Zhejiang Provincial Natural Science\nFoundation of China (No. LGG22F030011),\nNingbo Natural Science Foundation (2021J190),\nand Yongjiang Talent Introduction Programme\n(2021A-156-G).\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nfied language model pre-training. In Proceedings of\nthe 37th International Conference on Machine Learn-\ning, ICML 2020, 13-18 July 2020, Virtual Event ,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 642–652. PMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 1597–1607. PMLR.\nXiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin\nDeng, Ningyu Zhang, and Huajun Chen. 2021a. Dis-\nentangled contrastive learning for learning robust\ntextual representations. In Artificial Intelligence -\nFirst CAAI International Conference, CICAI 2021,\nHangzhou, China, June 5-6, 2021, Proceedings, Part\nII, volume 13070 of Lecture Notes in Computer Sci-\nence, pages 215–226. Springer.\nXiang Chen, Ningyu Zhang, Lei Li, Xin Xie, Shumin\nDeng, Chuanqi Tan, Fei Huang, Luo Si, and Hua-\njun Chen. 2021b. Lightner: A lightweight gener-\native framework with prompt-guided attention for\nlow-resource ner. arXiv preprint arXiv:2109.00720.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2021c. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. CoRR, abs/2104.07650.\nXinlei Chen and Kaiming He. 2021. Exploring simple\nsiamese representation learning. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 15750–15758.\nComputer Vision Foundation / IEEE.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.\n2021. Template-based named entity recognition us-\ning BART. In Findings of the Association for Com-\nputational Linguistics: ACL/IJCNLP 2021, Online\nEvent, August 1-6, 2021, volume ACL/IJCNLP 2021\nof Findings of ACL, pages 1835–1845. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nNing Ding, Yulin Chen, Xu Han, Guangwei Xu,\nPengjun Xie, Hai-Tao Zheng, Zhiyuan Liu, Juanzi\nLi, and Hong-Gee Kim. 2021. Prompt-learning for\nfine-grained entity typing. CoRR, abs/2108.10604.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042–13054.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. CoRR, abs/2012.15723.\n807\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3816–3830. Association for Computational Linguis-\ntics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 6894–\n6910. Association for Computational Linguistics.\nJohn M. Giorgi, Osvald Nitski, Bo Wang, and Gary D.\nBader. 2021. Declutr: Deep contrastive learning for\nunsupervised textual representations. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 879–895. Association\nfor Computational Linguistics.\nJean-Bastien Grill, Florian Strub, Florent Altché,\nCorentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Ávila Pires,\nZhaohan Guo, Mohammad Gheshlaghi Azar, Bilal\nPiot, Koray Kavukcuoglu, Rémi Munos, and Michal\nValko. 2020. Bootstrap your own latent - A new\napproach to self-supervised learning. In NeurIPS.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929–3938. PMLR.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021a. W ARP: word-level adversarial\nreprogramming. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 4921–4933. Association for Computa-\ntional Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021b. W ARP: word-level adversarial\nreprogramming. CoRR, abs/2101.00121.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. PTR: prompt tuning with rules\nfor text classification. CoRR, abs/2105.11259.\nI Hsu, Kuan-Hao Huang, Elizabeth Boschee, Scott\nMiller, Prem Natarajan, Kai-Wei Chang, Nanyun\nPeng, et al. 2021. Event extraction as natural lan-\nguage generation. arXiv preprint arXiv:2108.12724.\nAshish Jaiswal, Ashwin Ramesh Babu, Moham-\nmad Zaki Zadeh, Debapriya Banerjee, and Fillia\nMakedon. 2020. A survey on contrastive self-\nsupervised learning. CoRR, abs/2011.00362.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 2528–2540. Association for Computa-\ntional Linguistics.\nDong-Ho Lee, Mahak Agarwal, Akshen Kadakia, Jay\nPujara, and Xiang Ren. 2021. Good examples make\nA faster learner: Simple demonstration-based learn-\ning for low-resource NER. CoRR, abs/2110.08454.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021a.\nThe power of scale for parameter-efficient prompt\ntuning. CoRR, abs/2104.08691.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021b.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP (1), pages 3045–3059. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nChengxi Li, Feiyu Gao, Jiajun Bu, Lu Xu, Xiang\nChen, Yu Gu, Zirui Shao, Qi Zheng, Ningyu\nZhang, Yongpan Wang, et al. 2021. Sentiprompt:\nSentiment knowledge enhanced prompt-tuning for\naspect-based sentiment analysis. arXiv preprint\narXiv:2109.08306.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582–\n4597. Association for Computational Linguistics.\n808\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for gpt-3? CoRR,\nabs/2101.06804.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021c. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR, abs/2110.07602.\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang,\nLi Mian, Jing Zhang, and Jie Tang. 2020. Self-\nsupervised learning: Generative or contrastive.\nCoRR, abs/2006.08218.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021d. GPT\nunderstands, too. CoRR, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefficient framework for learning sentence representa-\ntions. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings. OpenReview.net.\nRuotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang\nLi, Qi Zhang, and Xuanjing Huang. 2022. Template-\nfree prompt tuning for few-shot NER. In Proceed-\nings of the 2022 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL\n2022, Seattle, WA, United States, July 10-15, 2022,\npages 5721–5732. Association for Computational\nLinguistics.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Advances in Neural Information Process-\ning Systems 26: 27th Annual Conference on Neural\nInformation Processing Systems 2013. Proceedings\nof a meeting held December 5-8, 2013, Lake Tahoe,\nNevada, United States, pages 3111–3119.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? CoRR,\nabs/2202.12837.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nCoRR, abs/2104.06599.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3980–3990.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. CoRR, abs/2009.07118.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n255–269. Association for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 4222–4235. Association for\nComputational Linguistics.\nJake Snell, Kevin Swersky, and Richard S. Zemel. 2017.\nPrototypical networks for few-shot learning. In NIPS,\npages 4077–4087.\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving\nand simplifying pattern exploiting training. CoRR,\nabs/2103.11955.\nZhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang\nLiu. 2021. MSP: multi-stage prompting for mak-\ning pre-trained language models better translators.\nCoRR, abs/2110.06609.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,\nand Daniel Cer. 2021. Spot: Better frozen model\nadaptation through soft prompt transfer. CoRR,\nabs/2110.07904.\n809\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,\nand Hao Ma. 2021. Entailment as few-shot learner.\nCoRR, abs/2104.14690.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nEMNLP (Demos), pages 38–45. Association for Com-\nputational Linguistics.\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui\nChen, Feiyu Xiong, Mosha Chen, and Huajun Chen.\n2022a. From discrimination to generation: Knowl-\nedge graph completion with generative transformer.\nCoRR, abs/2202.02113.\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui\nChen, Feiyu Xiong, Mosha Chen, and Huajun Chen.\n2022b. From discrimination to generation: Knowl-\nedge graph completion with generative transformer.\narXiv preprint arXiv:2202.02113.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. Consert: A con-\ntrastive framework for self-supervised sentence rep-\nresentation transfer. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021 , pages 5065–5075. Association for\nComputational Linguistics.\nHongbin Ye, Ningyu Zhang, Zhen Bi, Shumin Deng,\nChuanqi Tan, Hui Chen, Fei Huang, and Hua-\njun Chen. 2021. Learning to ask for data-\nefficient event argument extraction. arXiv preprint\narXiv:2110.00479.\nHongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen,\nHui Chen, Feiyu Xiong, Xi Chen, and Huajun Chen.\n2022. Ontology-enhanced prompt-tuning for few-\nshot learning. CoRR, abs/2201.11332.\nNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan\nCheng, Haosen Hong, Shumin Deng, Jiazhang Lian,\nQiang Zhang, and Huajun Chen. 2022a. Ontoprotein:\nProtein pretraining with gene ontology embedding.\nCoRR, abs/2201.11147.\nNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan\nCheng, Haosen Hong, Shumin Deng, Jiazhang Lian,\nQiang Zhang, and Huajun Chen. 2022b. Ontoprotein:\nProtein pretraining with gene ontology embedding.\narXiv preprint arXiv:2201.11147.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\nZhen Bi, Chuanqi Tan, Fei Huang, and Huajun\nChen. 2021. Differentiable prompt makes pre-trained\nlanguage models better few-shot learners. CoRR,\nabs/2108.13161.\nNingyu Zhang, Xin Xie, Xiang Chen, Shumin Deng,\nChuanqi Tan, Fei Huang, Xu Cheng, and Hua-\njun Chen. 2022c. Reasoning through memoriza-\ntion: Nearest neighbor knowledge graph embeddings.\nCoRR, abs/2201.05575.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 5017–5033. Association for Computa-\ntional Linguistics.\nXin Zhou, Ruotian Ma, Tao Gui, Yiding Tan, Qi Zhang,\nand Xuanjing Huang. 2021. Plug-tagger: A plug-\ngable sequence labeling framework using language\nmodels. CoRR, abs/2110.07331.\nA Datasets\nTable 4 provides the dataset evaluated in this work.\nDataset |Y| #Train #Test Type\nSST-2 2 6,920 872 sentiment\nSST-5 5 8,544 2,210 sentiment\nMR 2 8,662 2,000 sentiment\nCR 2 1,775 2,000 sentiment\nMPQA 2 8,606 2,000 opinion polarity\nSubj 2 8,000 2,000 subjectivity\nTREC 6 5,452 500 question cls.\nDBpedia 14 560,000 70,000 sentence cls.\nYahoo! Answers 10 1,400,000 60,000 sentence cls.\nMNLI 3 392,702 9,815 NLI\nSNLI 3 549,367 9,842 NLI\nQNLI 2 104,743 5,463 NLI\nRTE 2 2,490 277 NLI\nMRPC 2 3,668 408 paraphrase\nQQP 2 363,846 40,431 paraphrase\nTable 4: The datasets evaluated in this work. |Y|: the\nnumber of classes for classification tasks. Notes that we\nonly sample Dtrain and Ddev of K×|Y| examples from\nthe original training data set in our few-shot setting.\nB Template settings\nTable 5 and Table 6 provides manual templates and\nverbalizer similar with Gao et al. (2021a). We set\nthe template of demonstration same with example.\n810\nTemplate Tasks\n[CLS]x1, It was[MASK].[SEP] SST-2, SST-5, MR, CR, MPQA,\nDBpedia, Yahoo! Answers\n[CLS]x1, This is[MASK].[SEP]Subj\n[CLS][MASK]: x1[SEP] TREC\n[CLS]x1?[MASK],x2[SEP] MNLI, SNLI, QNLI, RTE\n[CLS]x1[MASK],x2[SEP] MRPC, QQP\nTable 5: Templates for all tasks evaluated in our work.\nTask Verbalizer\nSST-2 incorrect/correct\nSST-5 terrible/bad/okay/good/great\nMR terrible/great\nCR terrible/great\nMPQA terrible/great\nSubj subjective/objective\nTREC Description/Entity/Expression/\nHuman/Location/Number\nDBpedia company/institution/artist/athlete/\noffice/holder/transportation/building/\nplace/village/animal/plant/album/film/\nwritten/work\nYahoo! society/science/health/education/\ninternet/sports/business/entertainment/\nfamily/politics\nTable 6: Verbalizer for all tasks evaluated in our work.\n811",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.859015941619873
    },
    {
      "name": "Code (set theory)",
      "score": 0.5792167782783508
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5230634808540344
    },
    {
      "name": "Extensibility",
      "score": 0.5075010657310486
    },
    {
      "name": "Fine-tuning",
      "score": 0.506316065788269
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5025997161865234
    },
    {
      "name": "Language model",
      "score": 0.47806647419929504
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.47165384888648987
    },
    {
      "name": "Source code",
      "score": 0.4364665150642395
    },
    {
      "name": "Machine learning",
      "score": 0.42305225133895874
    },
    {
      "name": "Natural language processing",
      "score": 0.40267837047576904
    },
    {
      "name": "Programming language",
      "score": 0.2572126090526581
    },
    {
      "name": "Computer vision",
      "score": 0.09816926717758179
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ]
}