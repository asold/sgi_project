{
    "title": "Language Models as Knowledge Bases?",
    "url": "https://openalex.org/W2970476646",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5112562416",
            "name": "Fabio Petroni",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5079315903",
            "name": "Tim Rocktäschel",
            "affiliations": [
                "University College London",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5101404695",
            "name": "Sebastian Riedel",
            "affiliations": [
                "University College London",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5063377058",
            "name": "Patrick Lewis",
            "affiliations": [
                "University College London",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5024680544",
            "name": "Anton Bakhtin",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5011141204",
            "name": "Yuxiang Wu",
            "affiliations": [
                "University College London",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5055701014",
            "name": "Alexander Miller",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6603175991",
        "https://openalex.org/W2911435132",
        "https://openalex.org/W1854884267",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2250770256",
        "https://openalex.org/W1529533208",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2897513296",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2948380112",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W2756566873",
        "https://openalex.org/W3121854931",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2964915587",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963115613",
        "https://openalex.org/W2953345635",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2251803266",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W4288631803",
        "https://openalex.org/W4298422451"
    ],
    "abstract": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 2463–2473,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n2463\nLanguage Models as Knowledge Bases?\nFabio Petroni1 Tim Rockt¨aschel1,2 Patrick Lewis1,2 Anton Bakhtin1\nYuxiang Wu1,2 Alexander H. Miller1 Sebastian Riedel1,2\n1Facebook AI Research\n2University College London\n{fabiopetroni, rockt, plewis, yolo, yuxiangwu, ahm, sriedel}@fb.com\nAbstract\nRecent progress in pretraining language mod-\nels on large textual corpora led to a surge\nof improvements for downstream NLP tasks.\nWhilst learning linguistic knowledge, these\nmodels may also be storing relational knowl-\nedge present in the training data, and may\nbe able to answer queries structured as “ﬁll-\nin-the-blank” cloze statements. Language\nmodels have many advantages over structured\nknowledge bases: they require no schema en-\ngineering, allow practitioners to query about\nan open class of relations, are easy to extend to\nmore data, and require no human supervision\nto train. We present an in-depth analysis of the\nrelational knowledge already present (without\nﬁne-tuning) in a wide range of state-of-the-\nart pretrained language models. We ﬁnd that\n(i) without ﬁne-tuning, BERT contains rela-\ntional knowledge competitive with traditional\nNLP methods that have some access to ora-\ncle knowledge, (ii) BERT also does remark-\nably well on open-domain question answer-\ning against a supervised baseline, and (iii) cer-\ntain types of factual knowledge are learned\nmuch more readily than others by standard lan-\nguage model pretraining approaches. The sur-\nprisingly strong ability of these models to re-\ncall factual knowledge without any ﬁne-tuning\ndemonstrates their potential as unsupervised\nopen-domain QA systems. The code to re-\nproduce our analysis is available at https:\n//github.com/facebookresearch/LAMA.\n1 Introduction\nRecently, pretrained high-capacity language mod-\nels such as ELMo (Peters et al., 2018a) and BERT\n(Devlin et al., 2018a) have become increasingly\nimportant in NLP. They are optimised to either\npredict the next word in a sequence or some\nmasked word anywhere in a given sequence ( e.g.\n“Dante was born in [M ask] in the year 1265.”).\nThe parameters of these models appear to store\nMemory Query Answer\nSymbolic KB\nMemory Access\nNeural LM\nMemory Access\n(Dante, born-in, X)\n“Dante was born in[Mask].”\nDante\nFlorence\nborn-in\nFlorence\nFlorence\nKB\nLM\ne.g. ELMo/BERT\nFigure 1: Querying knowledge bases (KB) and lan-\nguage models (LM) for factual knowledge.\nvast amounts of linguistic knowledge (Peters et al.,\n2018b; Goldberg, 2019; Tenney et al., 2019) use-\nful for downstream tasks. This knowledge is\nusually accessed either by conditioning on latent\ncontext representations produced by the original\nmodel or by using the original model weights to\ninitialize a task-speciﬁc model which is then fur-\nther ﬁne-tuned. This type of knowledge transfer\nis crucial for current state-of-the-art results on a\nwide range of tasks.\nIn contrast, knowledge bases are e ﬀective so-\nlutions for accessing annotated gold-standard re-\nlational data by enabling queries such as (D ante,\nborn-in, X). However, in practice we often need\nto extract relational data from text or other modal-\nities to populate these knowledge bases. This\nrequires complex NLP pipelines involving entity\nextraction, coreference resolution, entity linking\nand relation extraction (Surdeanu and Ji, 2014)—\ncomponents that often need supervised data and\nﬁxed schemas. Moreover, errors can easily prop-\nagate and accumulate throughout the pipeline. In-\nstead, we could attempt to query neural language\nmodels for relational data by asking them to ﬁll in\nmasked tokens in sequences like “Dante was born\n2464\nin [Mask]”, as illustrated in Figure 1. In this set-\nting, language models come with various attractive\nproperties: they require no schema engineering,\ndo not need human annotations, and they support\nan open set of queries.\nGiven the above qualities of language models as\npotential representations of relational knowledge,\nwe are interested in the relational knowledge al-\nready present in pretrained oﬀ-the-shelf language\nmodels such as ELMo and BERT. How much re-\nlational knowledge do they store? How does this\ndiﬀer for di ﬀerent types of knowledge such as\nfacts about entities, common sense, and general\nquestion answering? How does their performance\nwithout ﬁne-tuning compare to symbolic knowl-\nedge bases automatically extracted from text?\nBeyond gathering a better general understand-\ning of these models, we believe that answers to\nthese questions can help us design better unsuper-\nvised knowledge representations that could trans-\nfer factual and commonsense knowledge reliably\nto downstream tasks such as commonsense (vi-\nsual) question answering (Zellers et al., 2018; Tal-\nmor et al., 2019) or reinforcement learning (Brana-\nvan et al., 2011; Chevalier-Boisvert et al., 2018;\nBahdanau et al., 2019; Luketina et al., 2019).\nFor the purpose of answering the above ques-\ntions we introduce the LAMA (LAnguage Model\nAnalysis) probe, consisting of a set of knowledge\nsources, each comprised of a set of facts. We\ndeﬁne that a pretrained language model knows a\nfact (subject, relation, object) such as (D ante,\nborn-in, Florence) if it can successfully predict\nmasked objects in cloze sentences such as “Dante\nwas born in ” expressing that fact. We test\nfor a variety of types of knowledge: relations be-\ntween entities stored in Wikidata, common sense\nrelations between concepts from ConceptNet, and\nknowledge necessary to answer natural language\nquestions in SQuAD. In the latter case we man-\nually map a subset of SQuAD questions to cloze\nsentences.\nOur investigation reveals that (i) the largest\nBERT model from Devlin et al. (2018b)\n(BERT-large) captures (accurate) relational\nknowledge comparable to that of a knowledge\nbase extracted with an o ﬀ-the-shelf relation\nextractor and an oracle-based entity linker from\na corpus known to express the relevant knowl-\nedge, (ii) factual knowledge can be recovered\nsurprisingly well from pretrained language mod-\nels, however, for some relations (particularly\nN-to-M relations) performance is very poor,\n(iii) BERT-large consistently outperforms other\nlanguage models in recovering factual and com-\nmonsense knowledge while at the same time\nbeing more robust to the phrasing of a query, and\n(iv) BERT-large achieves remarkable results for\nopen-domain QA, reaching 57.1% precision@10\ncompared to 63.5% of a knowledge base con-\nstructed using a task-speciﬁc supervised relation\nextraction system.\n2 Background\nIn this section we provide background on language\nmodels. Statistics for the models that we include\nin our investigation are summarized in Table 1.\n2.1 Unidirectional Language Models\nGiven an input sequence of tokens w =\n[w1, w2, . . . ,wN], unidirectional language models\ncommonly assign a probability p(w) to the se-\nquence by factorizing it as follows\np(w) =\n∏\nt\np(wt |wt−1, . . . ,w1). (1)\nA common way to estimate this probability is us-\ning neural language models (Mikolov and Zweig,\n2012; Melis et al., 2017; Bengio et al., 2003) with\np(wt |wt−1, . . . ,w1) = softmax(Wht + b) (2)\nwhere ht ∈Rk is the output vector of a neural net-\nwork at position t and W ∈R|V|× k is a learned\nparameter matrix that maps ht to unnormalized\nscores for every word in the vocabulary V. Var-\nious neural language models then mainly di ﬀer in\nhow they compute ht given the word history, e.g.,\nby using a multi-layer perceptron (Bengio et al.,\n2003; Mikolov and Zweig, 2012), convolutional\nlayers (Dauphin et al., 2017), recurrent neural net-\nworks (Zaremba et al., 2014; Merity et al., 2016;\nMelis et al., 2017) or self-attention mechanisms\n(Radford et al., 2018; Dai et al., 2019; Radford\net al., 2019).\nfairseq-fconv: Instead of commonly used recur-\nrent neural networks, Dauphin et al. (2017) use\nmultiple layers of gated convolutions. We use\nthe pretrained model in the fairseq 1 library in our\nstudy. It has been trained on the WikiText-103 cor-\npus introduced by Merity et al. (2016).\n1https://github.com/pytorch/fairseq\n2465\nModel Base Model #Parameters Training Corpus Corpus Size\nfairseq-fconv (Dauphin et al., 2017) ConvNet 324M WikiText-103 103M Words\nTransformer-XL (large) (Dai et al., 2019) Transformer 257M WikiText-103 103M Words\nELMo (original) (Peters et al., 2018a) BiLSTM 93.6M Google Billion Word 800M Words\nELMo 5.5B (Peters et al., 2018a) BiLSTM 93.6M Wikipedia (en) & WMT 2008-2012 5.5B Words\nBERT (base) (Devlin et al., 2018a) Transformer 110M Wikipedia (en) & BookCorpus 3.3B Words\nBERT (large) (Devlin et al., 2018a) Transformer 340M Wikipedia (en) & BookCorpus 3.3B Words\nTable 1: Language models considered in this study.\nTransformer-XL: Dai et al. (2019) introduce a\nlarge-scale language model based on the Trans-\nformer (Vaswani et al., 2017). Transformer-XL\ncan take into account a longer history by caching\nprevious outputs and by using relative instead of\nabsolute positional encoding. It achieves a test\nperplexity of 18.3 on the WikiText-103 corpus.\n2.2 Bidirectional “Language Models” 2\nSo far, we have looked at language models that\npredict the next word given a history of words.\nHowever, in many downstream applications we\nmostly care about having access to contextual rep-\nresentations of words, i.e., word representations\nthat are a function of the entire context of a unit\nof text such as a sentence or paragraph, and not\nonly conditioned on previous words. Formally,\ngiven an input sequence w = [w1, w2, . . . ,wN]\nand a position 1 ≤ i ≤ N, we want to esti-\nmate p(wi) = p(wi |w1, . . . ,wi−1, wi+1, . . . ,wN) us-\ning the left and right context of that word.\nELMo: To estimate this probability, Peters et al.\n(2018a) propose running a forward and backward\nLSTM (Hochreiter and Schmidhuber, 1997), re-\nsulting in − →h i and ← −h i which consequently are used\nto calculate a forward and backward language\nmodel log-likelihood. Their model, ELMo, uses\nmultiple layers of LSTMs and it has been pre-\ntrained on the Google Billion Word dataset. An-\nother version of the model, ELMo 5.5B, has been\ntrained on the English Wikipedia and monolingual\nnews crawl data from WMT 2008-2012.\nBERT: Instead of a standard language model ob-\njective, Devlin et al. (2018a) propose to sample\npositions in the input sequence randomly and to\nlearn to ﬁll the word at the masked position. To\nthis end, they employ a Transformer architecture\nand train it on the BookCorpus (Zhu et al., 2015)\nas well as a crawl of English Wikipedia. In addi-\n2Contextual representation models (Tenney et al., 2019)\nmight be a better name, but we keep calling them language\nmodels for simplicity.\ntion to this pseudo language model objective, they\nuse an auxiliary binary classiﬁcation objective to\npredict whether a particular sentence follows the\ngiven sequence of words.\n3 Related Work\nMany studies have investigated pretrained word\nrepresentations, sentence representations, and lan-\nguage models. Existing work focuses on un-\nderstanding linguistic and semantic properties of\nword representations or how well pretrained sen-\ntence representations and language models trans-\nfer linguistic knowledge to downstream tasks. In\ncontrast, our investigation seeks to answer to what\nextent pretrained language models store factual\nand commonsense knowledge by comparing them\nwith symbolic knowledge bases populated by tra-\nditional relation extraction approaches.\nBaroni et al. (2014) present a systematic com-\nparative analysis between neural word represen-\ntation methods and more traditional count-based\ndistributional semantic methods on lexical seman-\ntics tasks like semantic relatedness and concept\ncategorization. They ﬁnd that neural word rep-\nresentations outperform count-based distributional\nmethods on the majority of the considered tasks.\nHill et al. (2015) investigate to what degree word\nrepresentations capture semantic meaning as mea-\nsured by similarity between word pairs.\nMarvin and Linzen (2018) assess the gram-\nmaticality of pretrained language models. Their\ndataset consists of sentence pairs with a grammat-\nical and an ungrammatical sentence. While a good\nlanguage model should assign higher probability\nto the grammatical sentence, they ﬁnd that LSTMs\ndo not learn syntax well.\nAnother line of work investigates the ability of\npretrained sentence and language models to trans-\nfer knowledge to downstream natural language un-\nderstanding tasks (Wang et al., 2018). While such\nan analysis sheds light on the transfer-learning\n2466\nabilities of pretrained models for understanding\nshort pieces of text, it provides little insight into\nwhether these models can compete with traditional\napproaches to representing knowledge like sym-\nbolic knowledge bases.\nMore recently, McCoy et al. (2019) found that\nfor natural language inference, a model based on\nBERT learns to rely heavily on fallible syntac-\ntic heuristics instead of a deeper understanding of\nthe natural language input. Peters et al. (2018b)\nfound that lower layers in ELMo specialize on lo-\ncal syntactic relationships, while higher layers can\nlearn to model long-range relationships. Similarly,\nGoldberg (2019) found that BERT captures En-\nglish syntactic phenomena remarkably well. Ten-\nney et al. (2019) investigate to what extent lan-\nguage models encode sentence structure for diﬀer-\nent syntactic and semantic phenomena and found\nthat they excel for the former but only provide\nsmall improvements for tasks that fall into the lat-\nter category. While this provides insights into the\nlinguistic knowledge of language models, it does\nnot provide insights into their factual and com-\nmonsense knowledge.\nRadford et al. (2018) introduce a pretrained lan-\nguage model based on the Transformer which they\ntermed generative pretraining (GPTv1). The ﬁrst\nversion of GPT (Radford et al., 2018) has been\ntrained on the Book Corpus (Zhu et al., 2015) con-\ntaining 7000 books. The closest to our investiga-\ntion is the work by Radford et al. (2019) which\nintroduces GPTv2 and investigates how well their\nlanguage model does zero-shot transfer to a range\nof downstream tasks. They ﬁnd that GPTv2\nachieves an F1 of 55 for answering questions in\nCoQA (Reddy et al., 2018) and 4.1% accuracy on\nthe Natural Questions dataset (Kwiatkowski et al.,\n2019), in both cases without making use of anno-\ntated question-answer pairs or an information re-\ntrieval step. While these results are encouraging\nand hint at the ability of very large pretrained lan-\nguage models to memorize factual knowledge, the\nlarge GPTv2 model has not been made public and\nthe publicly available small version achieves less\nthan 1% on Natural Questions (5.3 times worse\nthan the large model). Thus, we decided to not\ninclude GPTv2 in our study. Similarly, we do not\ninclude GPTv1 in this study as it uses a limited\nlower-cased vocabulary, making it incompatible to\nthe way we assess the other language models.\n4 The LAMA Probe\nWe introduce the LAMA (LAnguage Model Anal-\nysis) probe to test the factual and commonsense\nknowledge in language models. It provides a set\nof knowledge sources which are composed of a\ncorpus of facts. Facts are either subject-relation-\nobject triples or question-answer pairs. Each fact\nis converted into a cloze statement which is used to\nquery the language model for a missing token. We\nevaluate each model based on how highly it ranks\nthe ground truth token against every other word\nin a ﬁxed candidate vocabulary. This is similar\nto ranking-based metrics from the knowledge base\ncompletion literature (Bordes et al., 2013; Nickel\net al., 2016). Our assumption is that models which\nrank ground truth tokens high for these cloze state-\nments have more factual knowledge. We discuss\neach step in detail next and provide considerations\non the probe below.\n4.1 Knowledge Sources\nTo assess the di ﬀerent language models in Sec-\ntion 2, we cover a variety of sources of factual\nand commonsense knowledge. For each source,\nwe describe the origin of fact triples (or question-\nanswer pairs), how we transform them into cloze\ntemplates, and to what extent aligned texts exist\nin Wikipedia that are known to express a partic-\nular fact. We use the latter information in super-\nvised baselines that extract knowledge representa-\ntions directly from the aligned text.\n4.1.1 Google-RE\nThe Google-RE corpus3 contains ∼60K facts man-\nually extracted from Wikipedia. It covers ﬁve re-\nlations but we consider only three of them, namely\n“place of birth”, “date of birth” and “place of\ndeath”. We exclude the other two because they\ncontain mainly multi-tokens objects that are not\nsupported in our evaluation. We manually deﬁne\na template for each considered relation, e.g., “[S]\nwas born in [O]” for “place of birth”. Each fact\nin the Google-RE dataset is, by design, manually\naligned to a short piece of Wikipedia text support-\ning it.\n4.1.2 T-REx\nThe T-REx knowledge source is a subset of\nWikidata triples. It is derived from the T-REx\n3https://code.google.com/archive/p/\nrelation-extraction-corpus/\n2467\ndataset (Elsahar et al., 2018) and is much larger\nthan Google-RE with a broader set of relations.\nWe consider 41 Wikidata relations and subsam-\nple at most 1000 facts per relation. As with the\nGoogle-RE corpus, we manually deﬁne a tem-\nplate for each relation (see Table 3 for some ex-\namples). In contrast to the Google-RE knowledge\nsource, T-REx facts were automatically aligned to\nWikipedia and hence this alignment can be noisy.\nHowever, Elsahar et al. (2018) report an accuracy\nof 97.8% for the alignment technique over a test\nset.\n4.1.3 ConceptNet\nConceptNet (Speer and Havasi, 2012) is a multi-\nlingual knowledge base, initially built on top of\nOpen Mind Common Sense (OMCS) sentences.\nOMCS represents commonsense relationships be-\ntween words and /or phrases. We consider facts\nfrom the English part of ConceptNet that have\nsingle-token objects covering 16 relations. For\nthese ConceptNet triples, we ﬁnd the OMCS sen-\ntence that contains both the subject and the object.\nWe then mask the object within the sentence and\nuse the sentence as template for querying language\nmodels. If there are several sentences for a triple,\nwe pick one at random. Note that for this knowl-\nedge source there is no explicit alignment of facts\nto Wikipedia sentences.\n4.1.4 SQuAD\nSQuAD (Rajpurkar et al., 2016) is a popular ques-\ntion answering dataset. We select a subset of 305\ncontext-insensitive questions from the SQuAD de-\nvelopment set with single token answers. We man-\nually create cloze-style questions from these ques-\ntions, e.g., rewriting “Who developed the theory of\nrelativity?” as “The theory of relativity was devel-\noped by ”. For each question and answer pair,\nwe know that the corresponding fact is expressed\nin Wikipedia since this is how SQuAD was cre-\nated.\n4.2 Models\nWe consider the following pretrained case-\nsensitive language models in our study (see Ta-\nble 1): fairseq-fconv ( Fs), Transformer-XL large\n(Txl), ELMo original ( Eb), ELMo 5.5B ( E5B),\nBERT-base (Bb) and BERT-large (Bl). We use the\nnatural way of generating tokens for each model\nby following the deﬁnition of the training objec-\ntive function.\nAssume we want to compute the generation for\nthe token at positiont. For unidirectional language\nmodels, we use the network output ( ht−1) just be-\nfore the token to produce the output layer soft-\nmax. For ELMo we consider the output just be-\nfore (− →h t−1) for the forward direction and just after\n(← −h t+1) for the backward direction. Following the\nloss deﬁnition in (Peters et al., 2018a), we average\nforward and backward probabilities from the cor-\nresponding softmax layers. For BERT, we mask\nthe token at position t, and we feed the output vec-\ntor corresponding to the masked token (ht) into the\nsoftmax layer. To allow a fair comparison, we let\nmodels generate over a uniﬁed vocabulary, which\nis the intersection of the vocabularies for all con-\nsidered models (∼21K case-sensitive tokens).\n4.3 Baselines\nTo compare language models to canonical ways\nof using oﬀ-the-shelf systems for extracting sym-\nbolic knowledge and answering questions, we\nconsider the following baselines.\nFreq: For a subject and relation pair, this baseline\nranks words based on how frequently they appear\nas objects for the given relation in the test data. It\nindicates the upper bound performance of a model\nthat always predicts the same objects for a partic-\nular relation.\nRE: For the relation-based knowledge sources, we\nconsider the pretrained Relation Extraction (RE)\nmodel of Sorokin and Gurevych (2017). This\nmodel was trained on a subcorpus of Wikipedia\nannotated with Wikidata relations. It extracts rela-\ntion triples from a given sentence using an LSTM-\nbased encoder and an attention mechanism. Based\non the alignment information from the knowledge\nsources, we provide the relation extractor with the\nsentences known to express the test facts. Using\nthese datasets, RE constructs a knowledge graph\nof triples. At test time, we query this graph by\nﬁnding the subject entity and then rank all ob-\njects in the correct relation based on the conﬁ-\ndence scores returned by RE. We consider two ver-\nsions of this procedure that di ﬀer in how the en-\ntity linking is implemented: REn makes use of a\nna¨ıve entity linking solution based on exact string\nmatching, while REo uses an oracle for entity link-\ning in addition to string matching. In other words,\nassume we query for the object o of a test subject-\nrelation fact ( s, r, o) expressed in a sentence x. If\nRE has extracted any triple (s′, r, o′) from that sen-\n2468\ntence x, s′ will be linked to s and o′ to o. In\npractice, this means RE can return the correct so-\nlution o if any relation instance of the right type\nwas extracted from x, regardless of whether it has\na wrong subject or object.\nDrQA: Chen et al. (2017) introduce DrQA, a pop-\nular system for open-domain question answering.\nDrQA predicts answers to natural language ques-\ntions using a two step pipeline. First, a TF /IDF\ninformation retrieval step is used to ﬁnd rele-\nvant articles from a large store of documents (e.g.\nWikipedia). On the retrieved top k articles, a neu-\nral reading comprehension model then extracts an-\nswers. To avoid giving the language models a\ncompetitive advantage, we constrain the predic-\ntions of DrQA to single-token answers.\n4.4 Metrics\nWe consider rank-based metrics and compute re-\nsults per relation along with mean values across all\nrelations. To account for multiple valid objects for\na subject-relation pair (i.e., for N-M relations), we\nfollow Bordes et al. (2013) and remove from the\ncandidates when ranking at test time all other valid\nobjects in the training data other than the one we\ntest. We use the mean precision at k ( P@k). For\na given fact, this value is 1 if the object is ranked\namong the top k results, and 0 otherwise.\n4.5 Considerations\nThere are several important design decisions we\nmade when creating the LAMA probe. Below\nwe give more detailed justiﬁcations for these de-\ncisions.\nManually Deﬁned Templates For each relation\nwe manually deﬁne a template that queries for the\nobject slot in that relation. One can expect that\nthe choice of templates has an impact on the re-\nsults, and this is indeed the case: for some rela-\ntions we ﬁnd both worse and better ways to query\nfor the same information (with respect to a given\nmodel) by using an alternate template. We argue\nthat this means we are measuring a lower bound\nfor what language models know. We make this\nargument by analogy with traditional knowledge\nbases: they only have a single way of querying\nknowledge for a speciﬁc relation, namely by us-\ning the relation id of that relation, and this way is\nused to measure their accuracy. For example, if\nthe relation ID is works-For and the user asks for\nis-working-for, the accuracy of the KG would\nbe 0.\nSingle Token We only consider single token ob-\njects as our prediction targets. The reason we in-\nclude this limitation is that multi-token decoding\nadds a number of additional tuneable parameters\n(beam size, candidate scoring weights, length nor-\nmalization, n-gram repetition penalties, etc.) that\nobscure the knowledge we are trying to measure.\nMoreover, well-calibrated multi-token generation\nis still an active research area, particularly for bidi-\nrectional models (see e.g. Welleck et al. (2019)).\nObject Slots We choose to only query object\nslots in triples, as opposed to subject or rela-\ntion slots. By including reverse relations (e.g.\ncontains and contained-by) we can also query\nsubject slots. We do not query relation slots for\ntwo reasons. First, surface form realisations of\nrelations will span several tokens, and as we dis-\ncussed above, this poses a technical challenge that\nis not in the scope of this work. Second, even if\nwe could easily predict multi-token phrases, rela-\ntions can generally be expressed with many dif-\nferent wordings, making it unclear what the gold\nstandard pattern for a relation should be, and how\nto measure accuracy in this context.\nIntersection of Vocabularies The models that\nwe considered are trained with di ﬀerent vocabu-\nlaries. For instance, ELMo uses a list of ∼800K\ntokens while BERT considers only ∼30K tokens.\nThe size of the vocabulary can inﬂuence the per-\nformance of a model for the LAMA probe. Specif-\nically, the larger the vocabulary the harder it would\nbe to rank the gold token at the top. For this rea-\nson we considered a common vocabulary of∼21K\ncase-sensitive tokens that are obtained from the\nintersection of the vocabularies for all considered\nmodels. To allow a fair comparison, we let every\nmodel rank only tokens in this joint vocabulary.\n5 Results\nWe summarize the main results in Table 2, which\nshows the mean precision at one (P@1) for the dif-\nferent models across the set of corpora considered.\nIn the remainder of this section, we discuss the re-\nsults for each corpus in detail.\nGoogle-RE We query the LMs using a standard\ncloze template for each relation. The base and\nlarge versions of BERT both outperform all other\nmodels by a substantial margin. Furthermore, they\n2469\nCorpus Relation Statistics Baselines KB LM\n#Facts #Rel Freq DrQA RE n REo Fs Txl Eb E5B Bb Bl\nGoogle-RE\nbirth-place 2937 1 4.6 - 3.5 13.8 4.4 2.7 5.5 7.5 14.9 16.1\nbirth-date 1825 1 1.9 - 0.0 1.9 0.3 1.1 0.1 0.1 1.5 1.4\ndeath-place 765 1 6.8 - 0.1 7.2 3.0 0.9 0.3 1.3 13.1 14.0\nTotal 5527 3 4.4 - 1.2 7.6 2.6 1.6 2.0 3.0 9.8 10.5\nT-REx\n1-1 937 2 1.78 - 0.6 10.0 17.0 36.5 10.1 13.1 68.0 74.5\nN-1 20006 23 23.85 - 5.4 33.8 6.1 18.0 3.6 6.5 32.4 34.2\nN-M 13096 16 21.95 - 7.7 36.7 12.0 16.5 5.7 7.4 24.7 24.3\nTotal 34039 41 22.03 - 6.1 33.8 8.9 18.3 4.7 7.1 31.1 32.3\nConceptNet Total 11458 16 4.8 - - - 3.6 5.7 6.1 6.2 15.6 19.2\nSQuAD Total 305 - - 37.5 - - 3.6 3.9 1.6 4.3 14.1 17.4\nTable 2: Mean precision at one (P@1) for a frequency baseline (Freq), DrQA, a relation extraction with na ¨ıve\nentity linking (REn), oracle entity linking (RE o), fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original\n(Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERT-large (Bl) across the set of evaluation corpora.\nobtain a 2 .2 and 2 .9 respective average accuracy\nimprovement over the oracle-based RE baseline.\nThis is particularly surprising given that with the\ngold-aligned Google-RE source we know for cer-\ntain that the oracle RE baseline has seen at least\none sentence expressing each test fact. Moreover,\nthe RE baseline was given substantial help through\nan entity linking oracle.\nIt is worth pointing out that while BERT-large\ndoes better, this does not mean it does so for the\nright reasons. Although the aligned Google-RE\nsentences are likely in its training set (as they\nare part of Wikipedia and BERT has been trained\non Wikipedia), it might not “understand” them\nto produce these results. Instead, it could have\nlearned associations of objects with subjects from\nco-occurrence patterns.\nT-REx The knowledge source derived from\nGoogle-RE contains relatively few facts and only\nthree relations. Hence, we perform experiments\non the larger set of facts and relations in T-REx.\nWe ﬁnd that results are generally consistent with\nGoogle-RE. Again, the performance of BERT in\nretrieving factual knowledge are close to the per-\nformance obtained by automatically building a\nknowledge base with an o ﬀ-the-shelf relation ex-\ntraction system and oracle-based entity linking.\nBroken down by relation type, the performance of\nBERT is very high for 1-to-1 relations (e.g., capi-\ntal of ) and low for N-to-M relations.\nNote that a downstream model could learn to\nmake use of knowledge in the output representa-\ntions of a language model even if the correct an-\nswer is not ranked ﬁrst but high enough (i.e. a hint\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n100 101 102\nmean P@k\nk\nFs\nTxl\nEb\nE5B\nBb\nBl\nFigure 2: Mean P@k curve for T-REx varying k. Base-\n10 log scale for X axis.\nabout the correct answer can be extracted from the\noutput representation). Figure 2 shows the mean\nP@k curves for the considered models. For BERT,\nthe correct object is ranked among the top ten in\naround 60% of the cases and among the top 100 in\n80% of the cases.\nTo further investigate why BERT achieves such\nstrong results, we compute the Pearson correlation\ncoeﬃcient between the P@1 and a set of metrics\nthat we report in Figure 3. We notice, for instance,\nthat the number of times an object is mentioned\nin the training data positively correlates with per-\nformance while the same is not true for the sub-\nject of a relation. Furthermore, the log probabil-\nity of a prediction is strongly positively correlated\nwith P@1. Thus, when BERT has a high conﬁ-\ndence in its prediction, it is often correct. Perfor-\nmance is also positively correlated with the cosine\nsimilarity between subject and object vectors, and\n2470\nSM OM LPFP SOCS ST SWP\nSM\nsubject\nmentions\nOM\nobject\nmentions\nLPFP\nlog probability\nfirst prediction\nSOCS\nsubject object\nvectors cosine\nsimilarity\nST\nsubject\ntokens\nSWP\nsubject\nword pieces\nP@1\n-0.0048\n-0.075 0.074\n0.12 -0.051 0.2\n-0.18 0.042 0.052 0.11\n-0.23 0.04 0.056 -0.21 0.52\n-0.05 0.2 0.42 0.31 0.12 0.035\n0.4\n0.2\n0.0\n0.2\n0.4\nFigure 3: Pearson correlation coe ﬃcient for the P@1\nof the BERT-large model on T-REx and a set of met-\nrics: SM and OM refer to the number of times a sub-\nject and an object are mentioned in the BERT training\ncorpus4respectively; LPFP is the log probability score\nassociated with the ﬁrst prediction; SOCS is the co-\nsine similarity between subject and object vectors (we\nuse spaCy5); ST and SWP are the number of tokens in\nthe subject with a standard tokenization and the BERT\nWordPiece tokenization respectively.\nslightly with the number of tokens in the subject.\nTable 3 shows randomly picked examples for\nthe generation of BERT-large for cloze template\nqueries. We ﬁnd that BERT-large generally pre-\ndicts objects of the correct type, even when the\npredicted object itself is not correct.\nTo understand how the performance of a pre-\ntrained language model varies with diﬀerent ways\nof querying for a particular fact, we analyze a\nmaximum of 100 random facts per relation for\nwhich we randomly select 10 aligned sentences in\nWikipedia from T-REx.6 In each of the sentences,\nwe mask the object of the fact, and ask the model\nto predict it. For several of our language models\nthis also tests their ability to memorize and recall\nsentences from the training data since as the mod-\nels have been trained on Wikipedia (see Table 1).\nFigure 4 shows the average distribution of the\nrank for ten queries per fact. The two BERT mod-\nels and ELMo 5.5B exhibit the lowest variabil-\nity while ranking the correct object close to the\ntop on average. Surprisingly, the performance of\nELMo original is not far from BERT, even though\nthis model did not see Wikipedia during train-\ning. Fairseq-fconv and Transformer-XL experi-\n5The original training corpus is not available, we created\nour version using the same sources.\n5https://spacy.io\n6We exclude all facts with less than 10 alignments.\nFs Txl Eb E5B Bb Bl\n0\n50\n100\n150\n200\n250\n300\n350rank\nFigure 4: Average rank distribution for 10 di ﬀerent\nmentions of 100 random facts per relation in T-REx.\nELMo 5.5B and both variants of BERT are least sen-\nsitive to the framing of the query but also are the most\nlikely to have seen the query sentence during training.\nence a higher variability in their predictions. Note\nthat BERT and ELMo 5.5B have been trained on\na larger portion of Wikipedia than fairseq-fconv\nand Transformer-XL and may have seen more sen-\ntences containing the test queries during training.\nConceptNet The results on the ConceptNet cor-\npus are in line with those reported for retriev-\ning factual knowledge in Google-RE and T-REx.\nThe BERT-large model consistently achieves the\nbest performance, and it is able to retrieve com-\nmonsense knowledge at a similar level to factual\nknowledge. The lower half of Table 3 shows gen-\nerations by BERT-large for randomly sampled ex-\namples. Some of the concepts generated by the\nlanguage models are surprisingly reasonable in ad-\ndition to being syntactically correct.\nSQuAD Next we evaluate our system on open-\ndomain cloze-style question answering and com-\npare against the supervised DrQA model. Table\n2 shows a performance gap between BERT-large\nand the DrQA open-domain QA system on our\ncloze SQuAD task. Again, note that the pretrained\nlanguage model is completely unsupervised, it is\nnot ﬁne-tuned, and it has no access to a ded-\nicated information retrieval system. Moreover,\nwhen comparing DrQA and BERT-large in terms\nof P@10, we ﬁnd that gap is remarkably small\n(57.1 for BERT-large and 63.5 for DrQA).\n6 Discussion and Conclusion\nWe presented a systematic analysis of the factual\nand commonsense knowledge in publicly avail-\nable pretrained language models as is and found\n2471\nRelation Query Answer GenerationT-Rex\nP19 Francesco Bartolomeo Conti was born in. Florence Rome [-1.8],Florence[-1.8], Naples[-1.9], Milan[-2.4], Bologna[-2.5]\nP20 Adolphe Adam died in. Paris Paris[-0.5], London[-3.5], Vienna[-3.6], Berlin[-3.8], Brussels[-4.0]\nP279 English bulldog is a subclass of. dog dogs [-0.3], breeds[-2.2],dog[-2.4], cattle[-4.3], sheep[-4.5]\nP37 The o ﬃcial language of Mauritius is. English English[-0.6], French[-0.9], Arabic[-6.2], Tamil[-6.7], Malayalam[-7.0]\nP413 Patrick Oboya plays inposition. midﬁelder centre [-2.0], center[-2.2],midﬁelder[-2.4], forward[-2.4], midﬁeld[-2.7]\nP138 Hamburg Airport is named after. Hamburg Hess [-7.0], Hermann[-7.1], Schmidt[-7.1],Hamburg[-7.5], Ludwig[-7.5]\nP364 The original language of Mon oncle Benjamin is. French French[-0.2], Breton[-3.3], English[-3.8], Dutch[-4.2], German[-4.9]\nP54 Dani Alves plays with. Barcelona Santos [-2.4], Porto[-2.5], Sporting[-3.1], Brazil[-3.3], Portugal[-3.7]\nP106 Paul Toungui is a by profession . politician lawyer[-1.1], journalist[-2.4], teacher[-2.7], doctor[-3.0], physician[-3.7]\nP527 Sodium sulﬁde consists of. sodium water [-1.2], sulfur[-1.7],sodium[-2.5], zinc[-2.8], salt[-2.9]\nP102 Gordon Scholes is a member of thepolitical party. Labor Labour[-1.3], Conservative[-1.6], Green[-2.4], Liberal[-2.9],Labor[-2.9]\nP530 Kenya maintains diplomatic relations with. Uganda India [-3.0],Uganda[-3.2], Tanzania[-3.5], China[-3.6], Pakistan[-3.6]\nP176 iPod Touch is produced by. Apple Apple[-1.6], Nokia[-1.7], Sony[-2.0], Samsung[-2.6], Intel[-3.1]\nP30 Bailey Peninsula is located in. Antarctica Antarctica[-1.4], Bermuda[-2.2], Newfoundland[-2.5], Alaska[-2.7], Canada[-3.1]\nP178 JDK is developed by. Oracle IBM [-2.0], Intel[-2.3], Microsoft[-2.5], HP[-3.4], Nokia[-3.5]\nP1412 Carl III used to communicate in. Swedish German [-1.6], Latin[-1.9], French[-2.4], English[-3.0], Spanish[-3.0]\nP17 Sunshine Coast, British Columbia is located in. Canada Canada[-1.2], Alberta[-2.8], Yukon[-2.9], Labrador[-3.4], Victoria[-3.4]\nP39 Pope Clement VII has the position of. pope cardinal [-2.4], Pope[-2.5],pope[-2.6], President[-3.1], Chancellor[-3.2]\nP264 Joe Cocker is represented by music label. Capitol EMI [-2.6], BMG[-2.6], Universal[-2.8],Capitol[-3.2], Columbia[-3.3]\nP276 London Jazz Festival is located in. London London[-0.3], Greenwich[-3.2], Chelsea[-4.0], Camden[-4.6], Stratford[-4.8]\nP127 Border TV is owned by. ITV Sky [-3.1],ITV[-3.3], Global[-3.4], Frontier[-4.1], Disney[-4.3]\nP103 The native language of Mammootty is. Malayalam Malayalam[-0.2], Tamil[-2.1], Telugu[-4.8], English[-5.2], Hindi[-5.6]\nP495 The Sharon Cuneta Show was created in. Philippines Manila[-3.2],Philippines[-3.6], February[-3.7], December[-3.8], Argentina[-4.0]\nConceptNet\nAtLocation You are likely to ﬁnd a overﬂow in a. drain sewer [-3.1], canal[-3.2], toilet[-3.3], stream[-3.6],drain[-3.6]\nCapableOf Ravens can. ﬂy ﬂy[-1.5], ﬁght[-1.8], kill[-2.2], die[-3.2], hunt[-3.4]\nCausesDesire Joke would make you want to. laugh cry [-1.7], die[-1.7],laugh[-2.0], vomit[-2.6], scream[-2.6]\nCauses Sometimes virus causes. infection disease [-1.2], cancer[-2.0],infection[-2.6], plague[-3.3], fever[-3.4]\nHasA Birds have . feathers wings [-1.8], nests[-3.1],feathers[-3.2], died[-3.7], eggs[-3.9]\nHasPrerequisite Typing requires. speed patience [-3.5], precision[-3.6], registration[-3.8], accuracy[-4.0],speed[-4.1]\nHasProperty Time is. ﬁnite short [-1.7], passing[-1.8], precious[-2.9], irrelevant[-3.2], gone[-4.0]\nMotivatedByGoal You would celebrate because you are. alive happy [-2.4], human[-3.3],alive[-3.3], young[-3.6], free[-3.9]\nReceivesAction Skills can be. taught acquired [-2.5], useful[-2.5], learned[-2.8], combined[-3.9], varied[-3.9]\nUsedFor A pond is for. ﬁsh swimming [-1.3], ﬁshing[-1.4], bathing[-2.0],ﬁsh[-2.8], recreation[-3.1]\nTable 3: Examples of generation for BERT-large. The last column reports the top ﬁve tokens generated together\nwith the associated log probability (in square brackets).\nthat BERT-large is able to recall such knowledge\nbetter than its competitors and at a level remark-\nably competitive with non-neural and supervised\nalternatives. Note that we did not compare the\nability of the corresponding architectures and ob-\njectives to capture knowledge in a given body of\ntext but rather focused on the knowledge present in\nthe weights of existing pretrained models that are\nbeing used as starting points for many researchers’\nwork. Understanding which aspects of data our\ncommonly-used models and learning algorithms\nare capturing is a crucial ﬁeld of research and this\npaper complements the many studies focused on\nthe learned linguistic properties of the data.\nWe found that it is non-trivial to extract a knowl-\nedge base from text that performs on par to di-\nrectly using pretrained BERT-large. This is de-\nspite providing our relation extraction baseline\nwith only data that is likely expressing target facts,\nthus reducing potential for false negatives, as well\nas using a generous entity-linking oracle. We\nsuspected BERT might have an advantage due to\nthe larger amount of data it has processed, so we\nadded Wikitext-103 as additional data to the re-\nlation extraction system and observed no signif-\nicant change in performance. This suggests that\nwhile relation extraction performance might be\ndiﬃcult to improve with more data, language mod-\nels trained on ever growing corpora might become\na viable alternative to traditional knowledge bases\nextracted from text in the future.\nIn addition to testing future pretrained language\nmodels using the LAMA probe, we are interested\nin quantifying the variance of recalling factual\nknowledge with respect to varying natural lan-\nguage templates. Moreover, assessing multi-token\nanswers remains an open challenge for our evalu-\nation setup.\nAcknowledgments\nWe would like to thank the reviewers for their\nthoughtful comments and eﬀorts towards improv-\ning our manuscript. In addition, we would like\nto acknowledge three frameworks that were used\nin our experiments: AllenNLP 7, Fairseq8 and the\nHugging Face PyTorch-Transformers9 library.\nReferences\nDzmitry Bahdanau, Felix Hill, Jan Leike, Edward\nHughes, Pushmeet Kohli, and Edward Grefenstette.\n2019. Learning to understand goal speciﬁcations by\n7https://github.com/allenai/allennlp\n8https://github.com/pytorch/fairseq\n9https://github.com/huggingface/\npytorch-transformers\n2472\nmodelling reward. In International Conference on\nLearning Representations (ICLR).\nMarco Baroni, Georgiana Dinu, and Germ ´an\nKruszewski. 2014. Don’t count, predict! A\nsystematic comparison of context-counting vs.\ncontext-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics, ACL 2014, June 22-27,\n2014, Baltimore, MD, USA, Volume 1: Long Papers,\npages 238–247.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nAntoine Bordes, Nicolas Usunier, Alberto Garc ´ıa-\nDur´an, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States. , pages 2787–\n2795.\nS. R. K. Branavan, David Silver, and Regina Barzi-\nlay. 2011. Learning to win by reading manuals in a\nmonte-carlo framework. In The 49th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Human Language Technologies, Proceedings\nof the Conference, 19-24 June, 2011, Portland, Ore-\ngon, USA, pages 268–277.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. CoRR, abs/1704.00051.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau,\nSalem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. 2018.\nBabyai: First steps towards grounded language\nlearning with a human in the loop. CoRR,\nabs/1810.08272.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, and Ruslan Salakhutdi-\nnov. 2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. CoRR,\nabs/1901.02860.\nYann N. Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling with\ngated convolutional networks. In Proceedings of the\n34th International Conference on Machine Learn-\ning, ICML 2017, Sydney, NSW, Australia, 6-11 Au-\ngust 2017, pages 933–941.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018a. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018b. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs] . ArXiv:\n1810.04805.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC-2018).\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\nRhinehart, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Matthew\nKelcey, Jacob Devlin, et al. 2019. Natural questions:\na benchmark for question answering research.\nJelena Luketina, Nantas Nardelli, Gregory Farquhar,\nJakob Foerster, Jacob Andreas, Edward Grefen-\nstette, Shimon Whiteson, and Tim Rockt ¨aschel.\n2019. A Survey of Reinforcement Learning In-\nformed by Natural Language. In Proceedings of\nthe Twenty-Eighth International Joint Conference\non Artiﬁcial Intelligence, IJCAI 2019, August 10-16\n2019, Macao, China.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 1192–1202.\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen.\n2019. Right for the wrong reasons: Diagnosing syn-\ntactic heuristics in natural language inference.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. CoRR, abs/1707.05589.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. CoRR, abs/1609.07843.\nTomas Mikolov and Geo ﬀrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn 2012 IEEE Spoken Language Technology Work-\nshop (SLT), Miami, FL, USA, December 2-5, 2012 ,\npages 234–239.\n2473\nMaximilian Nickel, Kevin Murphy, V olker Tresp, and\nEvgeniy Gabrilovich. 2016. A review of relational\nmachine learning for knowledge graphs. Proceed-\nings of the IEEE, 104(1):11–33.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , pages\n1499–1509.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000 + Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2392,\nAustin, Texas. Association for Computational Lin-\nguistics.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2018. Coqa: A conversational question answering\nchallenge. CoRR, abs/1808.07042.\nDaniil Sorokin and Iryna Gurevych. 2017. Context-\naware representations for knowledge base relation\nextraction. In Proceedings of the 2017 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2017, Copenhagen, Denmark,\nSeptember 9-11, 2017, pages 1784–1789.\nRobert Speer and Catherine Havasi. 2012. Represent-\ning general relational knowledge in conceptnet 5. In\nLREC, pages 3679–3686.\nMihai Surdeanu and Heng Ji. 2014. Overview of the\nEnglish Slot Filling Track at the TAC2014 Knowl-\nedge Base Population Evaluation. page 15.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4149–4158.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel R. Bowman.\n2018. GLUE: A multi-task benchmark and anal-\nysis platform for natural language understand-\ning. In Proceedings of the Workshop: Analyz-\ning and Interpreting Neural Networks for NLP ,\nBlackboxNLP@EMNLP 2018, Brussels, Belgium,\nNovember 1, 2018, pages 353–355.\nSean Welleck, Kiant ´e Brantley, Hal Daum ´e III, and\nKyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. arXiv preprint arXiv:1902.02192.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nCoRR, abs/1409.2329.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2018. From recognition to cognition: Visual\ncommonsense reasoning. CoRR, abs/1811.10830.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015 , pages 19–\n27."
}