{
    "title": "Pretrained Language Models for Dialogue Generation with Multiple Input Sources",
    "url": "https://openalex.org/W3093373170",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2106754310",
            "name": "Cao Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2389147550",
            "name": "Bi Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2057499807",
            "name": "Fang Meng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1960309444",
            "name": "Tao, Dacheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2742947407",
        "https://openalex.org/W2963475460",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2964352131",
        "https://openalex.org/W2742079690",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2133512280",
        "https://openalex.org/W2989429271",
        "https://openalex.org/W2914204778",
        "https://openalex.org/W2953039584",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3037026762",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2913443447",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2956901422",
        "https://openalex.org/W2922709902",
        "https://openalex.org/W2963854351"
    ],
    "abstract": "Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned on multiple sources. Previous work simply concatenates all input sources or averages information from different input sources. In this work, we study dialogue models with multiple input sources adapted from the pretrained language model GPT2. We explore various methods to fuse multiple separate attention information corresponding to different sources. Our experimental results show that proper fusion methods deliver higher relevance with dialogue history than simple fusion baselines.",
    "full_text": "Pretrained Language Models for Dialogue Generation\nwith Multiple Input Sources\nYu Cao 1∗, Wei Bi 2, Meng Fang 3, Dacheng Tao 1\n1 UBTECH Sydney AI Center, School of Computer Science, Faculty of Engineering,\nThe University of Sydney, Australia\n2 Tencent AI LAB, China, 3 Tencent Robotics X, China\nycao8647@uni.sydney.edu.au, victoriabi@tencent.com,\nmfang@tencent.com, dacheng.tao@sydney.edu.au\nAbstract\nLarge-scale pretrained language models have\nachieved outstanding performance on natu-\nral language understanding tasks. However,\nit is still under investigating how to apply\nthem to dialogue generation tasks, especially\nthose with responses conditioned on multiple\nsources. Previous work simply concatenates\nall input sources or averages information from\ndifferent input sources. In this work, we study\ndialogue models with multiple input sources\nadapted from the pretrained language model\nGPT2. We explore various methods to fuse\nmultiple separate attention information corre-\nsponding to different sources. Our experimen-\ntal results show that proper fusion methods\ndeliver higher relevance with dialogue history\nthan simple fusion baselines.\n1 Introduction\nLarge-scale pretrained language models (Devlin\net al., 2019; Radford et al., 2018, 2019) have\nachieved outstanding performance on various\nnatural language understanding tasks (Young\net al., 2018; Liu et al., 2019). Researchers\nhave then utilized them in dialogue generation\ntasks (Budzianowski and Vuli ´c, 2019; Edunov\net al., 2019; Zhang et al., 2019). Many of them sim-\nply concatenate the input dialogue history and the\noutput response in ﬁnetuning, since the pretrained\nlanguage model only accepts a single sequence as\ninput. However, dialogue generation tasks may in-\nvolve multiple input sources simultaneously. For\nexample, in personalized or knowledge-grounded\ndialogue generation (Li et al., 2016; Zhang et al.,\n2018; Dinan et al., 2018), a response is generated\nconditioned on both dialogue history and an aux-\niliary user proﬁle or knowledge article. Despite\n∗This work was done during Yu Cao’s internship in Ten-\ncent AI LAB, Shenzhen.\nsimple concatenation of all input sources, an impor-\ntant question arises on how we can better adapt a\nsingle-input pretrained language model to a multi-\ninput dialogue generation task.\nSome previous work forms an encoder-decoder\narchitecture with both encoder and decoder du-\nplicated from a pretrained language model (Golo-\nvanov et al., 2019; Zheng et al., 2019). Recently,\nBART (Lewis et al., 2019) even obtain a complete\npretrained model under this architecture directly.\nTaking personalized dialogue generation (Zhang\net al., 2018) as an example, we can treat persona in-\nformation, dialogue history and previous generated\ntokens as three different input sources. The for-\nmer two will be encoded ﬁrstly and then combined\nwith the last one in the decoder. In Golovanov\net al. 2019, the multi-head attention layer in the\ndecoder is copied three times for each input source\nand mean pooling is used to average results from\nmultiple attentions. This encoder-decoder adapta-\ntion is shown to outperform simple concatenation.\nHowever, when dialogue history gets longer, this\nmodel tends to use less information of each dia-\nlogue history utterance to predict the next token.\nZheng et al. 2019 add an extra weight predictor to\ncombine multiple attention information, but they\ndo not perform experiments using publicly released\npretrained models, nor on public datasets, making\ntheir results not directly comparable to other work.\nIn this work, we build our dialogue model on\nthe encoder-decoder architecture adapted from the\npretrained language model GPT2 (Radford et al.,\n2019). Our main contribution is to empirically\nstudy the attention fusion methods for multiple in-\nformation sources in each decoder layer. Three\nkinds of methods are explored in total. Our exper-\nimental results show performance improvements\non both automatic and human evaluations by us-\ning proper attention fusion methods, compared to\nbaselines using concatenation or mean pooling.\narXiv:2010.07576v1  [cs.CL]  15 Oct 2020\n2 Model\n2.1 The Encoder-Decoder Architecture\nFollowing the former work (Golovanov et al.,\n2019), we use the personalized dialogue genera-\ntion task on PersonaChat (Zhang et al., 2018) as\nan example in our study. The pretrained language\nmodel GPT2 and its parameters are duplicated to\nform an encoder-decoder architecture shown in Fig-\nure 1(a). We use GPT2 here due to its large-scale\npre-training corpus than other models and strong\nperformance in other generation tasks.\nWe have three separate inputs: personal proﬁle,\ndialogue history, and current reply (or previously\ngenerated response during the inference stage). Em-\nbeddings of the former two, which contain embed-\ndings of tokens, positions as well as token types,\nwill be successively put into the encoder, which\nis a GPT2 model with no attention mask to ﬁt the\nencoding procedure. The encoded representations,\ntogether with embeddings of current response to-\nkens will then be used as the input of a modiﬁed\nGPT2 decoder. Each decoder block will attend the\ncurrent state to the three sources using different\nattentions, then fuse their resulting information as\ninput for the next layer.\nInspired by multi-task learning (Zhang and Yang,\n2017), we further separate the original loss in lan-\nguage modeling into three parts corresponding to\nthree input sources respectively. By applying the\nsame linear prediction layer on the output of both\nencoder and decoder, three cross-entropy losses\nbetween predicted logits and corresponding truth\nsequences will be weighted by hyperparameters.\nL= αLpersona + βLhistory + γLpred (1)\nwith Adam optimizer (Kingma and Ba, 2014).\n2.2 Block Details in Decoder\nRecall that we have three input sources in the de-\ncoder, and thus some modiﬁcations are needed if\nthe decoder structure is inherited from GPT2. De-\ntails of each modiﬁed decoder block are shown in\nFigure 1(b), in which the most apparent change is\nthe additional two multi-head (MH) bidirectional\nattentions and the attention fusion module that\nfuses various attention outputs. The other parts\nremain the same as GPT2. In the following, we\nwill ﬁrst describe the MH Bi-attention. Attention\nfusion will be discussed in the next section.\nThe MH self-attention in Transformer (Vaswani\net al., 2017) handles a single input only. In order\nGPT2 Encoder GPT2 Decoder\nPersonality Dialog History\nLinear Layer\nCurrent Reply\nPersona LM loss History LM loss Prediction LM loss Full \nLoss\na +  b g+ =\n(a) The encoder-decoder architecture.\nEncoded \nPersonality\nEncoded Dialog \nHistory\nCurrent State/\nEmbedding\nMH Self-AttentionMH Bi-Attention MH Bi-Attention\nAttention Fusion\nLayer Normalization\nLayer Normalization\n+\n+\n×N\n. . . . . . . . .\nMLP\n(b) Details of each transformer block in decoder.\nFigure 1: Architecture of our proposed model.\nto make it accept two input sources, we regard\nthe current state Hc ∈RLc×d from the previous\nlayer (or embedding of reply in the ﬁrst layer) as\nquery and encoded state of auxiliary information\nHa ∈ RLa×d as key and value in the attention.\nHere Lc and La are corresponding lengths for these\ninput, and Ha can be encoded personality Hp or\ndialog history Hh. The output of each single head\nin MH Bi-attention can be obtained via\nA = softmax((HcWQ)(HaWK)T\n√\nd\n)(HaWV ),\n(2)\nwhere WQ, WK, WV are learnable matrices. In\nour model, different attentions own separate param-\neters instead of sharing. This differs from the previ-\nous work (Golovanov et al., 2019) which reuses the\nself-attention for bi-attention. Besides, the original\nGPT2 is a single-directional model using a triangu-\nlar matrix as the attention mask. Since the auxiliary\ninformation Ha is visible for the current reply at\nall time steps, no mask exists in MH bi-attention.\nIn total, three attention information Ac, Ap, and\nAh are obtained by attending the current state to\nitself, personality, and history respectively, all in\nthe same dimension RLc×d. They need to be fused\ninto one matrix Af ∈RLc×d so as to proceed to\nsubsequent decoding layers.\n2.3 Attention Fusion\nIn this section, we discuss various methods to fuse\nthe multiple attention information obtained above.\nThe simplest approach is to average three sources\nin all dimensions (Golovanov et al., 2019), which\ntreats all sources equally. However, in different\ndialogues, we may need to concentrate more on the\ndialogue history or the persona proﬁle in order to\ngenerate proper responses. Here we introduce the\nfollowing three kinds of methods to allow for more\nﬂexible information fusion from all input sources.\n• Static methods fuse different information using\nan identical fusion function with no training param-\neter. Except the average pooling (avg) which is\nregarded as a very simple fusion baseline, we also\ninclude Maximum (max), and Minimum (min) op-\neration for every dimension among all sources.\n• Weighting methods try to estimate the global\noptimal proportion of each source in a given do-\nmain by introducing extra learnable weights which\nare then ﬁxed in inference. Such methods can be:\n(i) source-level scalar weights (sw), which means\nthere are three trainable scalarswc,wp,wh for each\nsource in each layer and Af = (wcAc + wpAp +\nwhAh)/(wc + wp + wh).\n(ii) source-dimension level (dw), in which weights\nare learnable vectors wc,wp,wh ∈Rd. For each\nrow jof Af and weight vectors w, we perform the\nweighted combination via Af\nj = (wc\njAc\nj +wp\nj Ap\nj +\nwh\nj Ah\nj )/(wc\nj + wp\nj + wh\nj ).\n(iii) linear method ( linear) in which a linear net-\nwork is used to transform the concatenated atten-\ntion [Ac; Ap; Ah] into Af . Different from above\none, each dimension in the new feature space here\ncontains information from all dimensions of all\nsources to realize a better interaction.\n• Attention-based method fuses the information\nbased on a trainable modiﬁed transformer atten-\ntion (att). The attention fusion function changes\naccording to multiple input information as follows\nZ = softmax(\nsign(AcApT) ⊙(\n√\n|AcApT|\n√\nd\n)Ah,\n(3)\nwhere sign(·) is a function with value 1 when the\nelement is positive or -1 when negative; |·| for\nabsolute value; square root ensures that the value\nscale remains the same. This method utilizes matrix\nmultiplication to make fully interaction between\nall state values, obtaining the states conditioned\non all information sources dynamically. History\ninformation is selected as the “value” term to get\nmore dialog history involved in the obtained state.\n3 Experiment\nWe employ the PersonaChat (Zhang et al., 2018; Di-\nnan et al., 2020) dataset in our experiments which\nhas 164,356 utterances in 10,981 dialogues and\n1,155 personas. Each sample contains dialog his-\ntory with up to 15 utterances, a gold reply and a\npersona description with no more than 5 sentences.\nFour kinds of dialogue models using pretrained\nlanguage models as the initialization are compared:\n(i) TransferTransfo(Wolf et al., 2019), a single-\ninput OpenAI GPT using token type embedding to\ndistinguish different parts of a single concatenated\ninput (persona proﬁle, dialog history, and reply\nsuccessively). We also replace original GPT in this\nmethod with GPT2, denoted as TransferGPT2.\n(ii) MI-GPT (Golovanov et al., 2019) which uses\nthe OpenAI GPT in both encoder and decoder with\naverage pooling as the attention fusion method.\n(iii) Our architecture using GPT2 as the base model\nand average as fusion method (GPT2-avg), a very\nsimple baseline inherited from MI-GPT.\n(iv) Our model with each of the attention fusion\nmethods discussed in Sec 2.3, denoted as GPT2-X,\nand X is the corresponding fusion method.\nAll GPT2 models used here are small size (12\nlayers, hidden size is 768). Besides, Seq2seq model\nwith attention (Bahdanau et al., 2014) using 6-layer\nTransformer as the encoder and decoder is also\nincluded as an end-to-end single-input baseline.1\nThe following automatic metrics are considered\nin our evaluation: BLEU (Papineni et al., 2002),\nMETEOR (Lavie and Agarwal, 2007), NIST-4,\nwhich indicate the gram-level similarity between\nthe references and generated responses. Moreover,\nEntropy-4, corpus-level Distinct-2 and the average\nlength of replies are used to reﬂect the diversity\nof obtained text. In addition, human evaluation\nis also conducted on 200 dialog pairs in terms of\nﬂuency (range: 1 ∼3), relevance with dialogue\nhistory (h-rel, range: 1 ∼3) and consistency with\npersonality (p-consist, {0, 1}). More experiment\nconﬁgurations can be found in Appendix A.\n1Source code is available on: https://github.com/\ncaoyu-noob/Multi-GPT2\nModel BLEU METEOR NIST-4 Entropy-4 Dist-2 Avg.len ﬂuency h-rel p-consist\nHuman - - - 10.725 36.688 11.507 2.901 2.645 0.598\nSeq2seq 1.769 6.926 1.028 6.789 6.356 8.710 - - -\nTransferTransfo 2.054 7.672 1.183 8.429 17.917 7.824 2.748 2.348 0.542\nMI-GPT 3.151 8.112 1.264 8.054 13.264 9.026 2.809 2.150 0.628\nTransferGPT2 2.273 7.872 1.194 8.263 16.444 8.036 2.785 2.385 0.548\nGPT2-avg 3.211 8.149 1.291 7.904 13.612 8.932 2.838 2.149 0.648\nGPT2-max 3.344 8.156 1.206 8.175 14.104 8.750 - - -\nGPT2-min 3.774 8.661 1.388 8.099 14.925 9.209 - - -\nGPT2-sw 3.949 8.881 1.407 8.228 15.294 9.068 2.814 2.355 0.595\nGPT2-dw 3.714 8.694 1.385 8.096 14.647 9.095 - - -\nGPT2-linear 4.147 8.988 1.408 8.279 15.237 9.011 2.777 2.332 0.602\nGPT2-att 3.659 8.449 1.249 8.028 14.592 8.723 - - -\nTable 1: Dialogue generation performance comparison of different models on the test set of PersonaChat. Values\nfor BELU, METEOR and Dist-2 are in percentage. Human evaluation is only conducted on representative models.\n3.1 Results\nResults of different models on both automatic met-\nrics and human evaluations are shown in Table 1.\nWe ﬁrst analyze results on automatic metrics. It\ncan be observed that GPT2 is more powerful than\nOpenAI GPT under the same architecture. Multi-\ninput (MI) models that use the encoder-decoder\nframeworks generally outperform single-input (SI)\nmodels (TransferTransfo, TransferGPT2) which\nsimply concatenate all inputs. Although SI models\nshow higher diversity, their generated texts are gen-\nerally shorter. All attention fusion methods of our\nmodel make improvements compared to its base-\nline GPT2-avg. Among them, weighting methods\nhave higher scores than the other two kinds of fu-\nsion methods on most metrics. Compared with\nstatic methods, weighting methods are more ﬂexi-\nble to combine proper proportions of each source,\nthus it is no surprise that they can outperform static\nmethods. Meanwhile, though the attention-based\nmethod also allows for non-static attention fusion,\nit essentially poses dynamic weights on the history\nstate, and thus information of persona and reply is\nnot directly used in the ﬁnal fused representation\nand results in its failure It is also interesting to ﬁnd\nthat GTP2-dw shows no improvement compared\nto GPT2-sw, despite it extends the latter one using\ndifferent weights for each dimension.\nNow we discuss human evaluation results. Here,\nwe only conduct human evaluations on baselines\nand proposed models with the best automatic eval-\nuation results (i.e. weighting methods). Fluency\nscores of generated texts are very close to each\nother even compared to gold replies, which should\nbe largely beneﬁted from the pretrained model.\nHowever, h-rel scores (the relevance between dia-\nlog history and current responses) by models are\nsigniﬁcantly lower than those by a human. Note\nthat compared with SI models, MI models using the\naverage fusion (MI-GPT, GPT2-avg) show lower\nh-rel scores, though their persona consistency in-\ncreases much. This is also discussed in Golovanov\net al. 2019, and the reason is that SI model is sim-\nilar to a language model which stays tightly with\nhistory, while MI models take persona as a separate\ninput which is easier to reuse personalized word.\nHowever, our models with the weighting fusion\nmethods can not only improve the persona consis-\ntency compared to SI models, but also maintain\ncomparable best history relevance. The case study\nof generated replies is given in Appendix B.\n3.2 Inﬂuence of Attention Fusion\nIn this section, we further investigate how attention\nfusion affects the generation results, especially why\nusing the average fusion decreases the performance\non the relevance between dialog history and gener-\nated responses while the weighting fusion methods\ncan survive.\nWe group the 200 testing samples for human\nevaluation by their lengths of history, and then com-\npare the average results on h-rel scores of different\nmethods within each group. Results are shown in\nTable 2. We ﬁrst compare the weighting fusion\nmethods with the average fusion baseline. As can\nbe seen, all methods perform comparably when di-\nalogue history is short. With longer dialog history,\nmodels with weighting fusion methods perform\nHistory Win Tie Lose\nGPT2-weight\nvs.\nGPT2-avg\nL 53.2% 28.2% 18.6%\nM 37.0% 31.1% 31.9%\nS 29.3% 45.2% 25.5%\nGPT2-weight\nvs.\nTransferGPT2\nL 39.7% 35.5% 24.8%\nM 28.9% 37.1% 34.0%\nS 24.1% 43.7% 32.2%\nMI baselines\nvs.\nSI baselines\nL 17.7% 30.1% 52.2%\nM 22.2% 28.9% 48.9%\nS 18.9% 42.8% 38.3%\nTable 2: Percentage of generated replies by the up-\nper model better, equal or worse than the bottom one\non h-rel metric. Samples are grouped by dialog his-\ntory length (long (L) / short (S) / medium (M) history\nlength: > 9 utterances / ≤3 utterances / rest sam-\nples.). GPT2-weight: GPT2-sw and GPT2-linear, MI\nbaselines: GPT-MI and GPT2-avg, SI baselines: Trans-\nferTransfo and TransferGPT2.\nmuch better than GPT2-avg. The reason is that\nwhen dialogue history gets longer, the effect by\neach history token on current reply in bi-attention\nis averaged out by dialogue history length, mak-\ning the average fusion method harder to capture\nkey information from any history token to generate\nthe response. Next, we compare the GPT2 with\nweighting fusion methods to TransferGPT2 (the SI\nmodel with GPT2) and results indicate that they\ncan also outperform SI models when dialogue his-\ntory is long. Finally, we can see that SI models\nbeat the MI baselines with the average fusion un-\nder all conditions, proving the ineffectiveness of\nthe simple average between different information\nsources.\nFigure 2 further illustrates the estimated optimal\nweights of each attention information in every de-\ncoder layer in GPT2-sw. We observe that attention\nweights of different input sources are not equal and\nchange over different decoder layers, validating\nthat the use of average fusion is over-simpliﬁed.\nThe weights of diverse sources tend to be equiva-\nlent in high layers while they differ signiﬁcantly in\nlower layers because the history and persona infor-\nmation are already encoded and highly abstractive.\n4 Conclusion\nTo handle dialogue generation with multiple input\nsources, we adapt the pretrained language model\nGPT2 to an encoder-decoder architecture with\nmultiple independent attentions for different input\nsources in the decoder. We then investigate several\n1 2 3 4 5 6 7 8 9 10 11 12\nreply\npersona\nhistory\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nFigure 2: Visualization of normalized scalar attention\nweights on 3 different input sources for each layer in\nGPT2-sw decoder.\nattention fusion methods to obtain a preferable rep-\nresentation for dialogue generation. Experiments\nillustrate that weighting methods promote both auto\nmetrics and dialog history relevance scores anno-\ntated by human than baselines using average fusion,\nwhile they still maintain the persona consistency\nscores which outperform single-input models. And\nsuch architecture can be extended to other multi-\ninput dialogue generation tasks having different\ninformation source number.\n5 Acknowledgement\nThis work was supported by Australian Research\nCouncil Projects under grants FL-170100117, DP-\n180103424, and IC-190100031.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\ngpt-2–how can i help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. arXiv preprint arXiv:1907.05774.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, et al. 2020. The second conversational in-\ntelligence challenge (convai2). In The NeurIPS’18\nCompetition, pages 187–208. Springer.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019. Pre-trained language model representations\nfor language generation. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4052–4059.\nSergey Golovanov, Rauf Kurbanov, Sergey Nikolenko,\nKyryl Truskovskyi, Alexander Tselousov, and\nThomas Wolf. 2019. Large-scale transfer learning\nfor natural language generation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6053–6058.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An\nautomatic metric for mt evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the second workshop on statistical machine\ntranslation, pages 228–231.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016. A\npersona-based neural conversation model. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 994–1003.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487–4496.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nTom Young, Devamanyu Hazarika, Soujanya Poria,\nand Erik Cambria. 2018. Recent trends in deep\nlearning based natural language processing. ieee\nComputational intelligenCe magazine, 13(3):55–75.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2204–\n2213.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nYu Zhang and Qiang Yang. 2017. A survey on multi-\ntask learning. arXiv preprint arXiv:1707.08114.\nYinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, and\nMinlie Huang. 2019. A pre-training based personal-\nized dialogue generation model with persona-sparse\ndata. arXiv preprint arXiv:1911.04700.\nA Experiment Details\nWe use the ofﬁcial code for the implementation\nof TransferTransfo (Wolf et al., 2019) and GPT2-\nMI (Golovanov et al., 2019), following all default\nsettings to ﬁne-tune models. To implement our\nTransferGPT2, GPT2-avg, and all reﬁned attention\nfusion model, we utilize HuggingFace Transform-\ners library2 with the small-size GPT2 model which\nhas 12 layers and 768 dimensions in the hidden\nstate. It is noted that although both our encoder\nand decoder are initialized from GPT2 model, their\nparameters are not shared. Similarly, 3 different\nattention modules in each layer of the decoder (1\n2https://github.com/huggingface/transformers\nself-attention, 2 bi-attention) are also initialized by\nthe attention module of the corresponding layer in\noriginal GPT2 model but parameters are also not\nshared among them. The parameters of the addi-\ntional attention fusion module will be initialized\nby: 1) uniform initialization for source-weighting\nmethods, and 2) random initialization with normal\ndistribution for linear and attention-based meth-\nods. And the linear prediction layer has the shared\nweight with the embedding layer of the decoder.\nDuring ﬁne-tuning, we use Adam opti-\nmizer (Kingma and Ba, 2014) with an initial learn-\ning rate 5e-4 with 0.002 warmup proportion and\nthen a linear decay. The learning rate for the addi-\ntional attention fusion module is 5×current learn-\ning rate for other parts. We train it for 5 epochs\nusing mini-batch with size 256. And only the latest\n7 utterances in dialog history are remained to avoid\nexceeding maximum input length. All hyperparam-\neters are determined by manually tuning according\nto auto metrics BLEU, METEOR ,and NIST as\ncriteria.\nDuring inference, we use beam search with size\n3 for all test models. Length penalty (Wu et al.,\n2016) is added to ensure the diversity of generation.\nA single NVIDIA V100 GPU with CUDA10 is\nused to run experiments.\nB Case Study\nWe list dialogue generation results of Transfer-\nGPT2, GPT2-avg, GPT2-sw and GPT2-linear un-\nder some cases from PersonaChat dataset (Zhang\net al., 2018) in Table 3 and Table 4, containing sam-\nples with varied dialog history lengths. h-rel and\np-consist indicate the human evaluation scores for\ndialogue history relevance and personality consis-\ntency of generated replies respectively.\nIt can be found that our reﬁned attention fusion\nmodels generally show similar personality consis-\ntency with the baseline GPT2-avg model who uses\nthe same architecture but a simple average method\nto combine different information sources. When di-\nalog history is long, TransferGPT2 tends to directly\nrespond to the last history utterance using some\ngeneral replies, while GPT2-avg tends to directly\ncopy personal information as replies. GPT2-sw\nand GPT2-linear can properly make a response to\nthe last context as well as involve personal proﬁle.\nIn addition, we ﬁnd that when history length is not\nso long (length is 5 or 7), such difference will be\nreduced. But when dialog history is very short (less\nthan 5), all encoder-decoder models tend to gener-\nate universal replies or simply reuse personalities\nbecause the history information is too limited for\nthem to combine it with the given personal proﬁle.\nWhile the single-input TransferGPT2 is inclined to\nreuse personality descriptions because the whole\ninput sequence length is shorter and persona in-\nformation obtains more attention compared to the\ninput having a long history.\nitem text h-rel p-consist\nPersonality\ni have one cat.\ni am a kindergarten teacher.\ni can barely pay my bills every month.\ni share an apartment in seattle with two roommates.\ni just graduated from college two years ago.\nDialog history\n(length=13)\n...\nA: i also love to travel and take adventures. what are\nyour hobbies?\nB: i like to hang out with friends\nA: nice. where do you like to go?\nB: we like to do different things do you have pets?\nA: i have a dog. his name is max\nHuman that is cute i have a cat. 3 1\nTransferGPT2 that is a cute name. 3 0\nGPT2-avg i have two cats and a dog. 2 0.5\nGPT2-sw that is cool i have two cats. 3 0.5\nGPT2-linear that is a cute name i have a cat named maxy. 3 1\nPersonality\nmy favorite tv show is the ofﬁce.\ni do not drive because i live in new york.\ni like jazz music.\ni do not eat anything sweet.\nDialog history\n(length=9)\n...\nA: i need something to mellow me out after working all day\nB: that can be hard to ﬁnd up in new york.\nthings are fast and tight here\nA: do you have many insects? i do not care for that.\nB: not in the city, but yes upstate.\nA: what do you do for a living? i am in the science\nﬁeld engineer\nHuman i am a nutritionist , so no sweets for me. 3 1\nTransferGPT2 i am a stay at home mom. 3 0\nGPT2-avg i live in new york city. 2 1\nGPT2-sw i do not work. i just sit around and listen to jazz music. 3 1\nGPT2-linear that is cool. i work in a lab in new york city. 3 1\nTable 3: Some cases of generated dialogue replies by TrnasferGPT2, GPT2-avg, GPT2-sw and GPT2-linear.\nitem text h-rel p-consist\nPersonality\ni am a huge fantasy fan.\ni want to write fantasy books myself.\ni have two part time jobs.\ni like to binge watch the lord of the rings\nDialog history\n(length=7)\n...\nA: ah gosh. that is the worst!\nB: i want to just stay home and write.\nA: i would rather play softball than go to work\nB: i like to role play and play fantasy games. have you been\nto com icon?\nA: what is that? no i have never been\nHuman have you ever heard of the lord of the rings?. 2 1\nTransferGPT2 that is cool. i am a huge fantasy fan. 2 1\nGPT2-avg it is a fantasy game. 3 1\nGPT2-sw it is a fantasy world with lots of dragons. 3 1\nGPT2-linear i have not. i am more of a fantasy fan. 2 1\nPersonality\ni do not like exercise or physical activity.\ni am a homebody.\nmy favorite food is ice cream.\ni have a husband who i love very much.\ni love watching movies and tv.\nDialog history\n(length=5)\nA: hello ! tell me about yourself?\nB: i really like to stay at home at watch a movie with my\ndarling hubby. you?\nA: that sounds nice! i am a nurse who loves zoos and teddy\nbears.\nB: aww do you collect teddy bears? my sweet sweet husband\nbuys me them\nA: i do! i bring them to my patients at the childrens hospital.\nHuman so nice! do they also like ice cream? i love it! 3 1\nTransferGPT2 i do not have any kids, but i do have a husband. 1 1\nGPT2-avg that’s so sweet! i love to eat ice cream and ice cream. 3 1\nGPT2-sw that sounds like a lot of fun. i am a homebody myself. 3 1\nGPT2-linear that is nice. i am a homebody myself. 3 1\nPersonality\ni am going to auburn university next year as a freshman.\ni love shopping fashion and clothes.\ni want to be a doctor when i grow up.\ni am 17 years old and a senior in high school.\nDialog history\n(length=3)\nA: hi i am talking to text\nB: hello , interesting introduction you have there\nA: colors i can not see them well , my sis helps me out\nHuman that is unfortunate. what is something that you enjoy doing? 3 0\nTransferGPT2 i am a junior in high school, i want to be a doctor 3 0\nGPT2-avg what do you do for a living? 3 0\nGPT2-sw i am sorry to hear that. 1 1\nGPT2-linear what do you do for a living? 3 0\nTable 4: Some cases of generated dialogue replies by TrnasferGPT2, GPT2-avg, GPT2-sw and GPT2-linear."
}