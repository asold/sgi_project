{
    "title": "AttExplainer: Explain Transformer via Attention by Reinforcement Learning",
    "url": "https://openalex.org/W4285606254",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4284469528",
            "name": "Runliang Niu",
            "affiliations": [
                "Jilin University"
            ]
        },
        {
            "id": "https://openalex.org/A2890614283",
            "name": "Zhepei Wei",
            "affiliations": [
                "Jilin University"
            ]
        },
        {
            "id": "https://openalex.org/A1484673654",
            "name": "Yan Wang",
            "affiliations": [
                "Jilin University"
            ]
        },
        {
            "id": "https://openalex.org/A1981730961",
            "name": "Qi Wang",
            "affiliations": [
                "Jilin University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6777047548",
        "https://openalex.org/W6750667906",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2073231946",
        "https://openalex.org/W3112516115",
        "https://openalex.org/W2799194071",
        "https://openalex.org/W2783097478",
        "https://openalex.org/W2798139452",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W6803771590",
        "https://openalex.org/W2905526464",
        "https://openalex.org/W3018458867",
        "https://openalex.org/W2618851150",
        "https://openalex.org/W2344365922",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W2891575196",
        "https://openalex.org/W6863631769",
        "https://openalex.org/W3022708451",
        "https://openalex.org/W2766371743",
        "https://openalex.org/W2984775670",
        "https://openalex.org/W2988194011",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W6755811877",
        "https://openalex.org/W2766108848",
        "https://openalex.org/W2984936451",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2562979205",
        "https://openalex.org/W2963859254",
        "https://openalex.org/W2962816513",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2991265431",
        "https://openalex.org/W3034503989",
        "https://openalex.org/W2970863760",
        "https://openalex.org/W2963167310",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W3085380432",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2201581102",
        "https://openalex.org/W2963126845",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W2963834268",
        "https://openalex.org/W3034397670",
        "https://openalex.org/W2751448157",
        "https://openalex.org/W3087231533",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W3035736465",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W3101449015"
    ],
    "abstract": "Transformer and its variants, built based on attention mechanisms, have recently achieved remarkable performance in many NLP tasks. Most existing works on Transformer explanation tend to reveal and utilize the attention matrix with human subjective intuitions in a qualitative manner. However, the huge size of dimensions directly challenges these methods to quantitatively analyze the attention matrix. Therefore, in this paper, we propose a novel reinforcement learning (RL) based framework for Transformer explanation via attention matrix, namely AttExplainer. The RL agent learns to perform step-by-step masking operations by observing the change in attention matrices. We have adapted our method to two scenarios, perturbation-based model explanation and text adversarial attack. Experiments on three widely used text classification benchmarks validate the effectiveness of the proposed method compared to state-of-the-art baselines. Additional studies show that our method is highly transferable and consistent with human intuition. The code of this paper is available at https://github.com/niuzaisheng/AttExplainer .",
    "full_text": "ATTEXPLAINER : Explain Transformer via Attention by Reinforcement Learning\nRunliang Niu1 , Zhepei Wei1 , Yan Wang1,2∗ and Qi Wang1∗\n1School of Artificial Intelligence, Jilin University\n2Key Laboratory of Symbol Computation and Knowledge Engineering of Ministry of Education,\nCollege of Computer Science and Technology, Jilin University\n{niurl19,weizp19}@mails.jlu.edu.cn, wy6868@jlu.edu.cn, wangqi jlu@foxmail.com\nAbstract\nTransformer and its variants, built based on atten-\ntion mechanisms, have recently achieved remark-\nable performance in many NLP tasks. Most exist-\ning works on Transformer explanation tend to re-\nveal and utilize the attention matrix with human\nsubjective intuitions in a qualitative manner. How-\never, the huge size of dimensions directly chal-\nlenges these methods to quantitatively analyze the\nattention matrix. Therefore, in this paper, we pro-\npose a novel reinforcement learning (RL) based\nframework for Transformer explanation via atten-\ntion matrix, namely A TTEXPLAINER . The RL\nagent learns to perform step-by-step masking op-\nerations by observing the change in attention ma-\ntrices. We have adapted our method to two scenar-\nios, perturbation-based model explanation and text\nadversarial attack. Experiments on three widely\nused text classification benchmarks validate the ef-\nfectiveness of the proposed method compared to\nstate-of-the-art baselines. Additional studies show\nthat our method is highly transferable and consis-\ntent with human intuition. The code of this pa-\nper is available at https://github.com/niuzaisheng/\nAttExplainer.\n1 Introduction\nWith the development of deep learning methods in natural\nlanguage processing (NLP), model explanation has been in-\ncreasingly investigated to interpret the black-box model in an\nintelligible manner. As a recent representative neural network\narchitecture, Transformer [Vaswani and others, 2017] is lead-\ning the current NLP studies. For instance, the Transformer-\nbased BERT [Devlin and others, 2019 ] is believed to start a\ngroundbreaking era and has been proven effective in many\ndownstream tasks. Basically, it comprises multiple stacked\nlayers of self-attention and nonlinear feed-forward modules.\nAs the key to the explanation of the attention mechanism,\nthe attention matrix (also known as attention weight) is par-\nticularly investigated in previous research. It has been used\n∗ Corresponding authors\nInput tokens:\nI am feeling more confident \nthat we will be able to take \ncare of this baby\nTransformer\nFor Classification\nAttention Matrix\nImportant tokens :\nI am feeling more confident\nthat we will be able to take \ncare of this baby\nCan I know which \nposition is important \nvia attention matrix?\nPredicted label\nlove\nFigure 1: Demonstration of the sentence classification task with\nTransformer model. It is highly desired to find the key/important\ninput tokens via the attention matrix. Note that important tokens are\nmarked in red.\nto reveal the latent semantic relationships between input to-\nkens [Hewitt and Manning, 2019 ], or find some regular pat-\nterns in attention matrix images [Kovalevaet al., 2019].\nTypically, the attention matrix is regarded as an inher-\nent explanation for Transformer. Since [Jain and Wallace,\n2019] threw a question that the relationship between attention\nweights and model outputs is unclear, researchers have begun\nto investigate how to explain attention matrices and harvest\nthe potential information in them. Though widely discussed\nin previous works, none of them address the open problem\nas presented in Figure 1. In particular, there are two ma-\njor issues: (1) Most existing perturbation-based interpretation\nmethods use predict probabilities as the basis for interpreta-\ntion. It is still an open question whether some other inherent\ninformation of the model, such as attention matrix, can be\nemployed as a basis to explain the model’s predictions; (2)\nNot just at the level of analysis, it is still under-explored how\nto extract potential information from attention matrix for ben-\nefiting the downstream tasks, such as model explanation and\nmodel adversarial attack.\nOn the other hand, Perturbed Masking is an unsuper-\nvised method to measure the dependency degrees between\ninput tokens [Wu et al., 2020 ]. By successively masking\ntoken pairs in all pairwise combinations and sorting the de-\ngree of changes before and after the masking operations,\nthe probing method can draw an unlabeled dependency tree\nfrom the input sentence. In light of this, we argue that\nthe attention matrix could be explained by observing the\nchanges introduced by the masking operation. Specially,\nwe try to extend the possibility from pair to an unlimited\nnumber of token masking combinations. However, this will\nlead to an explosion of combination spaces. Some previ-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n724\nous methods used greedy search, such as beam search or ge-\nnetic algorithms to select crucial positions [Ren et al., 2019;\nLi et al., 2019 ]. But the time cost of searching is still high.\nNonetheless, there is still a lack of this kind of method and an\neffective model is needed.\nTo address the aforementioned issues, we introduce a rein-\nforcement learning (RL)-based method for perturbation sam-\npling and build a game environment for two downstream\ntasks: model explanation and text adversarial attack. Pertur-\nbation sampling is required in both tasks. Compared to other\ncombination search algorithms, the RL is more efficient to\nmodel the Markov Decision Process when searching in huge\naction space. Meanwhile, the attention matrix can be con-\nsidered as a kind of input observation for the RL agent for\nfurther efficiencies. Specifically, in each game step, a deep\nQ-network (DQN) based agent observes the attention matrix,\nthen selects a position to change the token’s visibility to the\nTransformer model by masking or unmasking the token. The\nmain contributions are summarized as follows:\n• We propose an RL-based model interpretation method\nnamed ATTEXPLAINER to explain the Transformer via at-\ntention matrix quantitatively by finding the key/important\ntokens in a sentence. Especially, attention matrices are em-\nployed to provide heuristic information to speed up the pro-\ncess.\n• We prove that the attention matrix does contain information\nabout token importance. Also, we demonstrate the strong\ntransferability of our method within different well-trained\nTransformer models and datasets.\n• We conduct extensive experiments on three public datasets,\ndemonstrating the interpretability of the attention matrix on\ntwo popular downstream tasks, namely, model explanation\nand model adversarial attack. In addition, we perform case\nstudies to visualize the step-by-step masking process for\nbetter understanding.\n2 Related Work\n2.1 Model Explanation\nTypically, model explanation aims to accurately deliver the\ntrue reasons behind the model’s decision/prediction by iden-\ntifying which parts of the input are important. The two dom-\ninant factions are gradient-based and perturbation-based. In\nthis article, we focus on perturbation-based explanation meth-\nods. As introduced in [Ribeiro et al., 2016 ], the goal of\nLIME is to use a smaller and simpler model to explain the\noriginal big model within a local scope of an input sample.\n[Lundberg and Lee, 2017 ] proposed the SHAP framework,\nwhich assigns each feature an important value for one partic-\nular prediction. The Shapley value of a feature is the average\nmarginal contribution of the feature in all features. Different\nmethods are proposed for the model explanation, but how to\nuniformly define model interpretation is still an open question\n[Feng and others, 2018].\n2.2 Attention Explanation\nAttention matrices, which are considered important inher-\nent information from Transformer models, have been exten-\nsively studied to interpret model predictions in NLP. For ex-\nample, some research works tried to study whether the at-\ntention matrix is interpretable by designing perturbation ex-\nperiments on model parameters [Jain and Wallace, 2019;\nSerrano and Smith, 2019; Wiegreffe and Pinter, 2019 ]. [Ko-\nvalevaet al., 2019] summarized several BERT’s self-attention\npatterns through manual observation. The model proposed\nin [Htut et al., 2019 ] compared the consistency of attention\nheads and human-labeled syntactic dependency trees. In[Ab-\nnar and Zuidema, 2020 ], the authors proposed a method to\ntrack the flow of information through self-attention layers to\nbetter explain the attention matrix. However, there is still\na lot of controversy about whether attention can indeed de-\nliver important information about the input tokens. [Chefer\net al., 2021] proposes a gradient-based method for interpret-\ning the attention matrix, which considers both the gradient of\nthe attention matrix and the relevancy score between tokens\nreflected by the attention matrix.\n2.3 Model Adversarial Attack\nAdversarial attack of text model is a fast-developing research\ndirection in the field of NLP. Normally, the attack process has\ntwo steps: finding the key positions to conduct the perturb op-\nerations, then generating the replacement text to these posi-\ntions. The adversarial attacker usually utilizes different types\nof information as its observation, such as gradient, probabil-\nity, prediction decision, or value of loss function [Papernot et\nal., 2016; Li et al., 2019]. Some automated model adversarial\nattack toolkits integrate existing methods [Wallace and oth-\ners, 2019; Zeng and others, 2021 ]. As far as we know, there\nis still no previous work that uses the attention matrix as the\nobservation.\n2.4 Reinforcement Learning (RL) in NLP\nThere have already been various NLP tasks that are solved by\nthe Reinforcement Learning (RL) techniques, such as ques-\ntion answering [Xiong et al., 2018], dialogue generation [Li\net al., 2016b ], and so on. In general, the RL agent model\ndoes not carry any language knowledge on itself. It is gen-\nerally used for auxiliary tasks, such as ensuring the output is\nproperly formatted [Zhong et al., 2017]. Some recent works\nfocused on using RL methods to improve the performance of\nNLP models. [Li et al., 2016a] proposed an RL-based method\nby removing the minimum number of words to change the\nmodel’s prediction. In [Xu et al., 2019], the authors improved\nthe robustness of the text classification model by using RL to\ngenerate semantically similar samples. Also, RL can be used\nto generate adversarial examples to improve the robustness of\nmachine translations [Zou et al., 2020].\n3 Framework\nIn this section, we introduce the technical details of the pro-\nposed ATTEXPLAINER framework. The overview is shown\nin Figure 2.\n3.1 Transformer Model\nIn our task setting, a Transformer classifier is used as a model\nbeing explained or a victim model for the adversarial attack.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n725\nReward Calculation & Game Status Determination\nTransformer\nDQN \nAgent\nAttention MatrixProbability\ni am felling more confident \nthat we will be able to take \ncare of this baby\nTransformer\nAction\nObserver\nProbability\n Attention Matrix\n•••\nInitial Stage Stage 1\nStep1\nTransformer\ni am felling more confident\nthat we will be able to take\ncare of this baby\nProbability Attention Matrix\nStage 4\ni am felling more confident\nthat we will be able to take \ncare of this baby\n•••\nStep1\nTransformer\ni am felling more confident\nthat we will be able to take\ncare of this baby\nProbability Attention Matrix\nStage 6\nEnd\nFigure 2: Overview of the explanation game process. ATTEXPLAINER identifys the key tokens by a step-by-step masking operation.\nGiven an input token sequence X = ( x1, ..., xn), where n\nis the length of the sequence. We use a well-trained BERT\nmodel as an instance of the Transformer, which is formal-\nized as a function T(·). ˆy is the model’s predicted label with\nˆy = T(X). p(ˆy|X) is the output probability for label ˆy.\nThe cross-entropy loss for classification is denoted asL(ˆy, y),\nwhere y is the golden label.\nSince BERT adopts dot-product style attention, in each\nlayer, the score of the i-th token xi in the attention matrix\nA ∈Rn×n is computed by a similarity function ϕ(Q, K) on\nthe input query Q ∈Rn×dim and key K ∈Rn×dim followed\nby an activation layer of softmax:\nAi,j = softmax(ϕ(Qj, Ki)) = exp(ϕ(Qj, Ki))P\nt exp(ϕ(Qj, Kt)) , (1)\nwhere the similarity function is ϕ(Q, K) = Q ×KT /\n√\ndim,\nand dim is the hidden dimension of Transformer. The atten-\ntion matrix that averaged over all layers is Att ∈R(n,n).\n3.2 RL Game Environment\nThe environment treats one sample’s analysis process as a\ngame round. There are two visibility for a single token to\nthe model: masked or unmasked. For a complete sequence,\nall tokens’ status can be combined into one boolean mask se-\nquence et = ( e(t,1), ..., e(t,n)), e(t,i) ∈ {T rue, F alse}, in\nwhich e(t,i) = T ruemeans token i is visible to the Trans-\nformer model at step t. Applying mask sequence e on token\nsequence X is formalized as X ⋆ e. So the input sequence to\nthe model at step t is Xt = X ⋆ et. In the initial step, e.g.,\nt = 0, all e(0,i) is set to 1.\nEnvironment. The environmental state s is composed of\nthe current token masking state et and attention matrix Attt.\nA flag bit d indicates whether the game is over or not.\nThe model interpretation aims to find an optimal combi-\nnation of token masks that maximizes the reduction of the\nmodel’s output probability on a given label. We denote this\nreduction in probability as ∆p = p(ˆy|X) −p(ˆy|X′). The\ngame ends when ∆p is reached to a certain threshold ε.\nThe adversarial attack aims to find an adversarial sample\nX′as quickly as possible to flip the output of the model. The\ngame ends when T(X) ̸= T(X′), which means the attack\naction is successful.\nAction. The agent action is denoted as an integer at, which\nindicates the next position index to inverse. The action of\nstep t will affect the input of the next step, e(t+1,at) is the re-\nverse of e(t,at). In this environment, the agent is encouraged\nto select actions that can obtain the largest overall rewards.\nIn order not to affect the text classification task-specific se-\nquence input format, it is not allowed to choose positions of\nspecial tokens, such as [CLS], [SEP], and [PAD].\nReward. The environment reward is made up of three as-\npects of the game: the model classification loss or label’s\nprobability, token mask rate mt = 1\nn\nP\nn et, and game sta-\ntus dt.\nIn the model interpretability task, we design the reward\nfunction as Eq.(2). In the adversarial attack task, the reward\nfunction is Eq.(3).\nrt = ∆p + α ×dt ×(1 −mt) −β , (2)\nrt = (Lt −L0) + α ×dt ×(1 −mt) −β , (3)\nwhere α is the final reward for game completion andβ is time\npunishment. The reward functions encourage a lower token\nmask rate and lower time cost.\n3.3 DQN Agent\nWe adopt a deep Q-learning network (DQN), denoted as\nQ(s, a), to predict the state-action value of expected return\nat each step and the goal is to maximize J= E[P∞\nt=0 γtrt],\nwhere γ is the discount factor.\nAs presented in Figure 3, we extract numerical distribu-\ntions and statistical features by performing histogram statis-\ntics along rows and columns from the attention matrix Att.\nSpecifically, We fix the bin size of the histogram to b. For an\ninput sentence with length n, we compute histograms along\nthe two dimensions separately, and the value is divided by n\nto normalize. Stitch them together with the mean and vari-\nance of the rows and columns, resulting in a statistical feature\nmatrix F in the shape of n ×(2b + 4). OutputLayer and\nHiddenLayer are two linear layers. Therefore, the structure\nof the value network is:\nQ(st) = OutputLayer([HiddenLayer (Ft), et]) . (4)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n726\n(a) Attention Matrix (b) Statistical Features \nFigure 3: An example of extracting statistical features. The oper-\nation indicated by the arrow converts a vector of variable length n\ninto a vector of fixed dimension b using numerical histogram statis-\ntics.\nThe ϵ-greedy policy is adopted to balance the exploration-\nexploitation, in which the action at = argmax Q(st) is ei-\nther determined with probability ϵ, or a random position is\nselected as action with probability 1 −ϵ. The agent chooses\none position to change its status in each game step.\nWe use prioritized experience replay (PER) [Schaul et al.,\n2016] to store transitions and sample DQN training batch.\nThe PER importance-sampling weights for one sampling ex-\nample is indicated by w. Therefore, the overall TD loss is:\nLQ(st, at, rt, st+1, at+1) =\nw ×[rt + γ ×(1 −dt) ×Q′(st+1, at) −Q(st, at)] . (5)\nThe detailed of DQN training progress is presented in Al-\ngorithm 1.\nAlgorithm 1DQN training progress\nInput: Transformer model T, input token sequence X.\nVariables: DQN eval net Q and target net Q′, agent replay\nbuffer, maximum game stepsM, value net parameters replace\nfrequency C.\n1: Initialize token sequence X0 = X, mask sequence e0.\n2: Compute ˆy0 = T(X0), get Att0, L0, d0, then s0 =\n[Att0, e0, d0].\n3: Set current step t = 1.\n4: while t < Mand dt−1 ̸= T ruedo\n5: The DQN agent gives an actionat by observing st−1.\n6: Update the environment stage et with at, then Xt =\nX ⋆ et.\n7: Compute ˆyt = T(Xt), get Attt, Lt. Set dt = T rue\nif the end condition has been reached. Compose new\nenvironment stage st = [ Attt, et, dt]. Compute the\naction reward rt.\n8: Store (st−1, at−1,rt,st,at) into replay buffer.\n9: Sample a mini-batch of transitions using PER from re-\nplay buffer. Perform one step training on Q.\n10: Reset target net with the parameters of A value net\nQ′←Q every C steps.\n11: end while\nOutput: The DQN value net Q.\nSettings Dataset # Labels # T\nokens # T\nrain # T\nest Model Accuracy\nSingle\nSentence\nEmotion 6 22 2000 2000 EM-1 0.935\nEM-2 0.937\nSST2 2 train 13\ne\nval 25 67349 872 SSTM 0.924\nNLI SNLI 3 26 549367 9842 SNLIM 0.905\nTable 1: Datasets statistics and the well-trained model performance.\n4 Experiment\nIn this section, we conduct extensive experiments to verify\nthe effectiveness of the proposed method in terms of two dif-\nferent tasks, namely, model explanation and adversarial at-\ntack. More precisely, we focus on investigating the following\nresearch questions (RQs):\n• RQ1: How does the proposed model perform in terms\nof model explanation and text adversarial attack tasks?\n• RQ2: Does the attention matrix necessarily deliver the\nkey information of input tokens?\n• RQ3: Are parameters of trained agents transferable\nwithin different Transformer models and datasets?\n4.1 Experimental Settings\nDatasets. We used two types of text classification settings:\nsingle sentence classification and Natural Language Inference\n(NLI). Specifically, single sentence classification datasets in-\nclude the Emotion dataset [Saravia et al., 2018] and the Stan-\nford Sentiment Treebank (SST2) dataset [Wang and others,\n2019]. The NLI dataset we used is the SNLI corpus [Bow-\nman and others, 2015]. The details of these datasets are pre-\nsented in Table 1. We utilize the fine-tuned model pretrained\nby Huggingface [Wolf et al., 2019] 1.\nBaselines. For model interpretation scenarios, we selected\nfive classical approaches for comparison: FeatureAblation,\nOcclusion [Zeiler and Fergus, 2014 ], KernelShap [Lund-\nberg and Lee, 2017 ], Shapley [Castro et al., 2009 ] and\nLIME [Ribeiro et al., 2016]. These methods are implemented\nby Captum toolkit [Kokhlikyan et al., 2020].\nFor textual adversarial attack scenarios, we selected the\nfollowing methods for comparison: PWWS [Ren et al.,\n2019], Genetic [Alzantot and others, 2018 ], SCPN [Iyyer et\nal., 2018], HotFlip [Ebrahimi et al., 2018], GAN [Zhao et al.,\n2018], DeepWordBug [Gao et al., 2018 ], TextBugger[Li et\nal., 2019], PSO [Zang et al., 2020], BERTAttacker[Li et al.,\n2020]. These attack methods are implemented by OpenAt-\ntack [Zeng and others, 2021] toolkit.\nEvaluation Metrics. We use 4 indicators to evaluate the\nmodel explanation methods: Fidelity, Model Query Times,\n∆p and Token Masked Rate. We use Fidelity to measure the\neffectiveness of the masking operation:\nF idelity= 1\nN\nNX\ni=1\nI(T(Xi) ̸= T(X′\ni)) . (6)\n1We use BertForSequenceClassification as the base model ar-\nchitecture. The names of the well-trained model are EM-1:\nAdapterHub/bert-base-uncased-pf-emotion, EM-2: nateraw/bert-\nbase-uncased-emotion, SSTM: textattack/bert-base-uncased-SST-2\nand SNLIM: textattack/bert-base-uncased-snl. These well-trained\nmodels have reached a high accuracy on the corresponding datasets,\nthe average evaluation accuracy is shown in Table 1.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n727\nDataset Method Fidelity↑\nModel\nQuery\nTimes\n↓ ∆ p ↑\nTok\nen\nMasked\nRate\n↓\nEmotion\nFeatureAblation 0.857 22.9 0.789 0.845\nOcclusion 0.859 20.9 0.795 0.935\nKernelShap 0.896 100 0.832 0.658\nShapley(5\ntimes/token) 0.916 111.3 0.849 0.737\nShapley(25\ntimes/token) 0.918 2212.4 0.853 0.802\nLIME 0.952 100 0.858 0.669\nAttExplainer (Ours) 0.918 6.9 0.836 0.287\nSST2\nFeatureAblation 0.533 26.6 0.501 0.863\nOcclusion 0.510 24.5 0.486 0.915\nKernelShap 0.728 100 0.672 0.602\nShapley(5\ntimes/token) 0.836 129.0 0.768 0.628\nShapley(25\ntimes/token) 0.883 2558.8 0.818 0.646\nLIME 0.794 100 0.739 0.619\nAttExplainer (Ours) 0.841 25.1 0.697 0.605\nSNLI\nFeatureAblation 0.737 28.6 0.666 0.683\nOcclusion 0.739 26.6 0.649 0.846\nKernelShap 0.900 100 0.792 0.602\nShapley(5\ntimes/token) 0.930 128.6 0.818 0.633\nShapley(25\ntimes/token) 0.967 2749.3 0.849 0.668\nLIME 0.921 100 0.809 0.591\nAttExplainer (Ours) 0.908 17.1 0.808 0.408\nTable 2: The model explanation experimental results. The ↑ arrow\nmeans higher is better. The best and second-best scores are marked\nin the column of Mask Token Rate. The sliding window of Occlu-\nsion is fixed at 3. The maximum number of samples for KernelShap\nand LIME is fixed at 100. Two records of the Shapley method are\nshown, they are the results of perturbing each token 5 times and 25\ntimes, respectively.\nWe adopt several commonly used metrics in model adver-\nsarial attack scenarios: Attack Success Rate, Victim Model\nQuery Times, and Word Modification Rate.\nImplementation Details. We trained our model on multi-\nple Titan RTX graphics cards. The max game step is limited\nto 100. Max size of replacing buffer for PER is 100,000. The\nAdam optimizer [Kingma and Ba, 2015 ] was used for train-\ning the DQN model at a fixed learning rate 10−4, training\nbatch size is 256. We set α = 10, β= 0.2, ϵ= 0.7, γ= 0.9.\nThe number of feature bins b is fixed at 32. The parameters\nof the DQN target net are replaced by the eval net every 100\nlearning steps. We use the special token [MASK] to replace\nthe original token in masking operation.\n4.2 Experimental Results\nModel Explanation Results (RQ1)\nThe performance of ATTEXPLAINER serving as a model ex-\nplainer is presented in Table 2. It can be observed that\nour method achieves surprisingly competitive performance in\nterms of Fidelity and ∆p with a much lower Token Masked\nRate. We argue that, given two model interpretation methods,\nif their performances in ∆p are comparatively close, while\none of them can find fewer key positions (i.e., with lower To-\nken Masked Rate) that determine the classification result, it\nmeans the corresponding method is more effective than the\nother one. For example, according to the experimental re-\nsults on the Emotion dataset, the LIME tends to explain that\n66.9% of tokens in a sentence are supporting the classifica-\ntion decision. In contrast, the results of the A TTEXPLAINER\nshow that we only need 28.7% of the tokens to be masked\nwhile keeping a comparable ∆p of 83.6%, indicating the su-\nperiority of the proposed method in finding more precise key\npositions. Meanwhile, we can also find that our proposed\nmethod spends less query time to achieve a better balance\nDataset Method Attack Succ.\nRate ↑ Victim\nModel\nQuery Times ↓ Word\nModi.\nRate ↓\nEmotion\nPWWS 0.750 132.80 0.114\nGenetic 0.834 1309.80 0.141\nSCPN 0.764 11.760 1.340\nHotFlip 0.675 49.44 0.305\nGAN 0.757 2.76 1.610\nDeepWordBug 0.670 21.50 0.379\nTe\nxtBugger 0.953 17.00 0.197\nPSO 0.740 88.00 0.126\nBERT\nAttacker 0.880 41.30 0.135\nAttExplainer (Ours) 0.968 4.86 0.155\nSST2\nPWWS 0.724 118.69 0.145\nGenetic 0.810 1431.9 0.164\nSCPN 0.666 11.7 1.430\nHotFlip 0.469 57.2 0.274\nGAN 0.429 2.43 1.300\nDeepWordBug 0.494 23.2 0.275\nTe\nxtBugger 0.804 34.0 0.293\nPSO 0.538 147.7 0.151\nBERT\nAttacker 0.881 62.96 0.190\nAttExplainer (Ours) 0.883 19.6 0.442\nSNLI\nPWWS 0.765 99.2 0.240\nGenetic 0.883 1109.0 0.283\nSCPN 0.473 11.5 1.114\nDeepWordBug 0.354 17.7 0.334\nTe\nxtBugger 0.729 44.4 0.311\nPSO 0.667 105.9 0.244\nBERT\nAttacker 0.859 45.3 0.219\nAttExplainer (Ours) 0.896 14.2 0.223\nTable 3: The model adversarial attack experimental results.\nbetween ∆p and Token Masked Rate, surpassing all the base-\nlines. Note that our approach does not perform optimally on\nthe SST2 dataset, which is also challenging for all baselines.\nThough the Shapley method achieves a higher ∆p metric, it\nis exponential in time cost, but the marginal benefits will di-\nminish when increasing query times. Overall, compared to\nexisting methods on all three datasets, the A TTEXPLAINER\ngenerally makes the searching both faster and more accurate,\nand we attribute this to the design of introducing the attention\nmatrix as heuristic information in our method.\nAdversarial Attack Results (RQ1)\nThe results of the model adversarial attack are shown in Ta-\nble 3. Overall, the proposed A TTEXPLAINER achieved the\nhighest attack success rate, relatively lower victim model\nquery times and word modification rate in three datasets com-\npared to baseline methods. More precisely, for the single\nsentence setting, our method surpasses the second runner-ups\nwith only 1.5% and 0.2% absolute increment in terms of at-\ntack success rate on both Emotion and SST2 detests, individ-\nually. In contrast, there is a comparatively significant perfor-\nmance boost of 13.1% between the existing state-of-the-art\nmethod (e.g. PWWS) and our method in the NLI setting, in-\ndicating the superiority of our proposed model in more com-\nplicated scenarios.\nInterpretability of Attention Matrix (RQ2)\nWe conduct extensive experiments to validate the effective-\nness of the proposed method in providing interpretable infor-\nmation based on the attention matrix. Specifically, we gen-\nerate a random matrix as environment observation to replace\nthe real attention matrix in each game step, while other set-\ntings are kept unchanged.\nAs can be observed from Figure 4, compared to the normal\ntraining setting (Emotion@1M), the agents represented by the\nremaining two curves fall into a poor solution that masks most\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n728\n(a)\nGame Step\nAttack Success Rate\nGame Step\nMask Token Num.\n(b)\nGame Step\nMask Token Rate\n(c)\nDone Step\nMask Token Num.\n(d)\nDone Step\nMask Token Rate\n(e)\nFigure 4: Impact of the introduction of the attention matrix as an observation on the adversarial attack task. • with att means the attention\nmatrix can be observed in both the training and evaluation phases. • eval w/o att means the attention matrix cannot be observed during\nevaluation phases. • train,eval w/o att means the attention matrix cannot be observed during both training and evaluation phases. From the\nperspective of attack success rate, shown in (a), there seems to be little difference between the final results of these three settings. But from\nthe perspective of masked token number and masked token rate, as shown in (b)(c)(d)(e), the agent will be more greedy to mask more tokens\nwithout attention’s heuristic information.\nTrain\nEv\nal Emotion\nattack for\nEM-1\nSST2\nattack for\nSSTM\nSNLI\nattack for\nSNLIM\nEmotion\nattack for\nEM-2\nEmotion @\n3M\nattack for EM-1\n0.969\n5.220\n0.187\n0.695\n32.19\n0.465\n0.794\n16.51\n0.194\n0.942\n9.830\n0.241\nSST2 @\n3M\nattack for SSTM\n0.953\n4.340\n0.183\n0.883\n19.50\n0.484\n0.969\n10.20\n0.209\n0.983\n6.930\n0.276\nSNLI @\n3M\nattack for SNLIM\n0.944\n6.460\n0.306\n0.880\n20.45\n0.506\n0.930\n12.450\n0.236\n0.977\n6.782\n0.289\nTable 4: Transferability study on the adversarial attack task. The\nvalues in each table cell are attack success rate, victim model query\ntimes, and token modification rate.\nof the tokens to let the decision flip. On one hand, this reveals\nthat there is potential token importance information in the at-\ntention matrix guiding the agent to choose a better gaming\nstrategy. On the other hand, it also highlights the necessity of\nintroducing the RL method due to the difficulty in finding a\nreasonable masking combination in the huge searching space\nwithout any textual information, other model-related obser-\nvation, or external knowledge.\nTransferability Study (RQ3)\nSince the decision-making process of the agent does not refer\nto the text information, we studied whether the agent’s pa-\nrameters can be transferred to other well-trained models. In\nthe scenario of the adversarial attack, the transferability result\nis shown in Table 4. Agents trained in three training settings\nwill be tested in four different evaluation settings. The table\nshows that, in a simpler dataset (Emotion), all settings can\nachieve well results. But the agent which is trained on a more\ndifficult dataset (SST2) has a better transferability to other\ndatasets. There is also a significant effect when migrating to\ndifferent well-trained models, from EM-1 to EM-2.\n4.3 Case Study\nIn this section, we give some practical examples for analy-\nsis. For the same input, we use two marking colors to in-\ndicate the different positions found by the explainer and at-\ntacker. The examples are shown in Table 5. In most cases,\nthe key positions found by the explainer and attacker over-\nDataset\nGolden\nlabel\nOriginal\nlabel\nAttacked\nLabel ∆p\nSentence\nEmotion\nanger anger joy 0.979\n[CLS] i\nam just feeling cranky\nand blue [SEP]\njoy joy anger 0.861\n[CLS] i\ncan have for a treat or\nif i am feeling\nfesti\nve [SEP]\nSST2\nneg. pos. neg. 0.765\n[CLS] this one is definitely one\nto skip, even for\nhorror mo\nvie fanatics. [SEP]\nneg. neg. pos. 0.875\n[CLS] it offers little be\nyond the momentary jo\nys\nof pretty and weightless intellectual\nentertain-\nment . [SEP]\nSNLI\nentail. entail. contr. 0.883\n[CLS] a man selling donuts to\na customer during\na world e\nxhibition event held in the city of ange-\nles [SEP]\na woman\ndrinks her coffee in a small\ncafe.\n[SEP]\nentail. entail. contr. 0.860\n[CLS] a\nyoung boy\nin a field of flowers\ncarrying\na ball [SEP] dog in pool [SEP]\nentail. entail. neutral 0.996\n[CLS] f\namilies waiting in line at an amusement\npark for their turn to\nride. [SEP] people are wait-\ning to see a movie.\n[SEP]\nTable 5: Some case study examples. Tokens inred color are masked\nby agent as an attacker. The highlighted tokens are identified by\nagent as an explainer.\nlap with each other. The explainer found more tokens than\nthe attacker. This coincides with the training aims of both.\nWe find that the explainer prefers to choose content words\n(e.g., nouns, verbs), while the attacker tends to choose func-\ntion words (e.g., prepositions, articles) to finish faster. Such\nresults inspire us to consider whether these pre-trained lan-\nguage models are indeed robust.\nTwo visualization examples are shown in Figure 5. At each\ntime step, we plotted a set of charts, each chart containing\nfour subplots: attention matrix, label prediction probability,\nheat-map of extracted features, and Q value distribution. The\nagent chooses the next mask position based on the maximum\nQ value.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n729\nExample 2 : i can have for a treat or if i am feeling festive.\nExample 1: i feel dirty and ashamed for saying that.\nTransformer\nAttention\nMatrix\nPrediction\nFeature\nMap\nFeature\nExtract\nAction Q value\nStep Mask Action\nMasked sequence\nDQN\nValue Net\nInitial status Initial features Step 1 featuresStep 1 status Step 2 status\nStep 1 actionInitial Pred. Step 1 Pred. Step 2 action\nDecision changed\nGame Done\nFigure 5: Visualization of the game process in adversarial attack scenario. The original image of BERT attention matrix in subplot is marked\nby 1⃝, output probability of label classification is marked by 2⃝, the numerical statistical features is marked by 3⃝, and Q value as the measure\nfor next action selection is marked by 4⃝.\n5 Conclusion\nIn this paper, we present a reinforcement learning-based\nmethod, named ATTEXPLAINER , for the Transformer expla-\nnation. Especially, a deep Q-network (DQN) based agent\nis designed to select the crucial position in a sentence step-\nby-step, with the benefit of observing attention matrices. By\ndesigning different reward functions, we have tested our ap-\nproach in two tasks: perturbation-based model explanation\nand text adversarial attack. In addition, we found that the pa-\nrameters of the DQN agent can be migrated within the same\nTransformer architecture. Through comparative experiments,\nwe believe that the attention matrix can provide heuristic in-\nformation for the agent to speed up the task. We also perform\nin-depth case studies to visualize the game processes intu-\nitively. In line with human’s intuition, we discover that the\nattention matrix indeed contains potential token importance\ninformation, which can be used as a basis for various appli-\ncations, such as model explanation and model attack. These\ncan be the direction of our future work.\nAcknowledgments\nThis work has been funded by the National Natural Sci-\nence Foundation of China (No.62072212), the Develop-\nment Project of Jilin Province of China (20200401083GX,\n20200403172SF).\nReferences\n[Abnar and Zuidema, 2020] Samira Abnar and Willem\nZuidema. Quantifying attention flow in transformers. In\nACL, 2020.\n[Alzantot and others, 2018] Moustafa Alzantot et al. Gener-\nating natural language adversarial examples. In EMNLP,\n2018.\n[Bowman and others, 2015] Samuel R. Bowman et al. A\nlarge annotated corpus for learning natural language in-\nference. In EMNLP, 2015.\n[Castro et al., 2009] Javier Castro, Daniel G ´omez, and Juan\nTejada. Polynomial calculation of the shapley value based\non sampling. Computers & Operations Research, 2009.\n[Chefer et al., 2021] Hila Chefer, Shir Gur, and Lior Wolf.\nTransformer interpretability beyond attention visualiza-\ntion. In CVPR, 2021.\n[Devlin and others, 2019] Jacob Devlin et al. BERT: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding. In NAACL-HLT, 2019.\n[Ebrahimi et al., 2018] Javid Ebrahimi, Anyi Rao, Daniel\nLowd, and Dejing Dou. HotFlip: White-box adversarial\nexamples for text classification. In ACL, 2018.\n[Feng and others, 2018] Shi Feng et al. Pathologies of neural\nmodels make interpretations difficult. In EMNLP, 2018.\n[Gao et al., 2018] Ji Gao, Jack Lanchantin, Mary Lou Soffa,\nand Yanjun Qi. Black-box generation of adversarial text\nsequences to evade deep learning classifiers. In2018 IEEE\nSecurity and Privacy Workshops (SPW), 2018.\n[Hewitt and Manning, 2019] John Hewitt and Christo-\npher D. Manning. A structural probe for finding syntax in\nword representations. In NAACL-HLT, 2019.\n[Htut et al., 2019] Phu Mon Htut, Jason Phang, Shikha Bor-\ndia, and Samuel R. Bowman. Do attention heads in\nbert track syntactic dependencies? arXiv preprint arXiv:\n1911.12246, 2019.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n730\n[Iyyer et al., 2018] Mohit Iyyer, John Wieting, Kevin Gim-\npel, and Luke Zettlemoyer. Adversarial example genera-\ntion with syntactically controlled paraphrase networks. In\nNAACL-HLT, 2018.\n[Jain and Wallace, 2019] Sarthak Jain and Byron C. Wallace.\nAttention is not Explanation. In NAACL-HLT, 2019.\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In ICLR,\n2015.\n[Kokhlikyan et al., 2020] Narine Kokhlikyan, Vivek\nMiglani, Miguel Martin, Edward Wang, Bilal Alsal-\nlakh, Jonathan Reynolds, Alexander Melnikov, Natalia\nKliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-\nRichardson. Captum: A unified and generic model\ninterpretability library for pytorch. arXiv preprint\narXiv:2009.07896, 2020.\n[Kovalevaet al., 2019] Olga Kovaleva, Alexey Romanov,\nAnna Rogers, and Anna Rumshisky. Revealing the dark\nsecrets of BERT. In EMNLP, 2019.\n[Li et al., 2016a] Jiwei Li, Will Monroe, and Dan Juraf-\nsky. Understanding neural networks through representa-\ntion erasure. arXiv preprint arXiv:1612.08220, 2016.\n[Li et al., 2016b] Jiwei Li, Will Monroe, Alan Ritter, Dan\nJurafsky, Michel Galley, and Jianfeng Gao. Deep rein-\nforcement learning for dialogue generation. In EMNLP,\n2016.\n[Li et al., 2019] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li,\nand Ting Wang. TextBugger: Generating adversarial text\nagainst real-world applications. In Proceedings 2019 Net-\nwork and Distributed System Security Symposium. Internet\nSociety, 2019.\n[Li et al., 2020] Linyang Li, Ruotian Ma, Qipeng Guo, Xi-\nangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adver-\nsarial attack against BERT using BERT. InEMNLP, 2020.\n[Lundberg and Lee, 2017] Scott M. Lundberg and Su-In\nLee. A unified approach to interpreting model predictions.\nIn NIPS, 2017.\n[Papernot et al., 2016] Nicolas Papernot, Patrick Mcdaniel,\nAnanthram Swami, and Richard E. Harang. Crafting ad-\nversarial input sequences for recurrent neural networks.\nMILCOM, 2016.\n[Ren et al., 2019] Shuhuai Ren, Yihe Deng, Kun He, and\nWanxiang Che. Generating natural language adversarial\nexamples through probability weighted word saliency. In\nACL, 2019.\n[Ribeiro et al., 2016] Marco T ´ulio Ribeiro, Sameer Singh,\nand Carlos Guestrin. ”why should I trust you?”: Explain-\ning the predictions of any classifier. In SIGKDD, 2016.\n[Saravia et al., 2018] Elvis Saravia, Hsien-Chi Toby Liu,\nYen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER:\nContextualized affect representations for emotion recogni-\ntion. In EMNLP, 2018.\n[Schaul et al., 2016] Tom Schaul, John Quan, Ioannis\nAntonoglou, and David Silver. Prioritized experience re-\nplay. In ICLR, 2016.\n[Serrano and Smith, 2019] Sofia Serrano and Noah A.\nSmith. Is attention interpretable? In ACL, 2019.\n[Vaswani and others, 2017] Ashish Vaswani et al. Attention\nis all you need. In NIPS, 2017.\n[Wallace and others, 2019] Eric Wallace et al. AllenNLP in-\nterpret: A framework for explaining predictions of NLP\nmodels. In EMNLP, 2019.\n[Wang and others, 2019] Alex Wang et al. GLUE: A multi-\ntask benchmark and analysis platform for natural language\nunderstanding. In ICLR, 2019.\n[Wiegreffe and Pinter, 2019] Sarah Wiegreffe and Yuval Pin-\nter. Attention is not not explanation. In EMNLP, 2019.\n[Wolf et al., 2019] Thomas Wolf, Lysandre Debut, Victor\nSanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Huggingface’s\ntransformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771, 2019.\n[Wu et al., 2020] Zhiyong Wu, Yun Chen, Ben Kao, and Qun\nLiu. Perturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In ACL, 2020.\n[Xiong et al., 2018] Caiming Xiong, Victor Zhong, and\nRichard Socher. DCN+: mixed objective and deep residual\ncoattention for question answering. In ICLR, 2018.\n[Xu et al., 2019] Jingjing Xu, Liang Zhao, Hanqi Yan,\nQi Zeng, Yun Liang, and Xu Sun. LexicalAT: Lexical-\nbased adversarial reinforcement training for robust senti-\nment classification. In EMNLP, 2019.\n[Zang et al., 2020] Yuan Zang, Fanchao Qi, Chenghao Yang,\nZhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun.\nWord-level textual adversarial attacking as combinatorial\noptimization. In ACL, 2020.\n[Zeiler and Fergus, 2014] Matthew D Zeiler and Rob Fergus.\nVisualizing and understanding convolutional networks. In\nECCV, 2014.\n[Zeng and others, 2021] Guoyang Zeng et al. OpenAttack:\nAn open-source textual adversarial attack toolkit. In ACL,\n2021.\n[Zhao et al., 2018] Zhengli Zhao, Dheeru Dua, and Sameer\nSingh. Generating natural adversarial examples. In ICLR,\n2018.\n[Zhong et al., 2017] Victor Zhong, Caiming Xiong, and\nRichard Socher. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.arXiv\npreprint arXiv:1709.00103, 2017.\n[Zou et al., 2020] Wei Zou, Shujian Huang, Jun Xie, Xinyu\nDai, and Jiajun Chen. A reinforced generation of adver-\nsarial examples for neural machine translation. In ACL,\n2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n731"
}