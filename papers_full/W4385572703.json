{
  "title": "Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models",
  "url": "https://openalex.org/W4385572703",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2984142359",
      "name": "Aniruddha Mahapatra",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2975684950",
      "name": "Sharmila Reddy Nangi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2501541109",
      "name": "Aparna Garimella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4319377359",
      "name": "Anandhavelu N",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034640977",
    "https://openalex.org/W103729224",
    "https://openalex.org/W3173156538",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2047477415",
    "https://openalex.org/W3101231927",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2857028992",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W2160987310",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2949759300",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2020278455",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2944420250",
    "https://openalex.org/W2950419928",
    "https://openalex.org/W2296283641"
  ],
  "abstract": "Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using unlabeled data from target domains can help improve the performances of these language models on the downstream tasks. However, using all of the available unlabeled data for pretraining can be time-intensive; also, it can be detrimental to the performance of the downstream tasks, if the unlabeled data is not aligned with the data distribution for the target tasks. Previous works employed external supervision in the form of ontologies for selecting appropriate data samples for pretraining, but external supervision can be quite hard to obtain in low-resource domains. In this paper, we introduce effective ways to select data from unlabeled corpora of target domains for language model pretraining to improve the performances in target entity extraction tasks. Our data selection strategies do not require any external supervision. We conduct extensive experiments for the task of named entity recognition (NER) on seven different domains and show that language models pretrained on target domain unlabeled data obtained using our data selection strategies achieve better performances compared to those using data selection strategies in previous works that use external supervision. We also show that these pretrained language models using our data selection strategies outperform those pretrained on all of the available unlabeled target domain data.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 942–951\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEntity Extraction in Low Resource Domains\nwith Selective Pre-training of Large Language Models\nAniruddha Mahapatra1+, Sharmila Reddy Nangi2+\nAparna Garimella3, Anandhavelu Natarajan3\n1Carnegie Mellon University, USA 2Stanford University, USA\n3Adobe Research, India\namahapat@andrew.cmu.edu1 srnangi@stanford.edu2\n{garimell, anandvn}@adobe.com3\nAbstract\nTransformer-based language models trained\non large natural language corpora have been\nvery useful in downstream entity extraction\ntasks. However, they often result in poor perfor-\nmances when applied to domains that are differ-\nent from those they are pretrained on. Contin-\nued pretraining using unlabeled data from tar-\nget domains can help improve the performances\nof these language models on the downstream\ntasks. However, using all of the available unla-\nbeled data for pretraining can be time-intensive;\nalso, it can be detrimental to the performance\nof the downstream tasks, if the unlabeled data\nis not aligned with the data distribution for the\ntarget tasks. Previous works employed exter-\nnal supervision in the form of ontologies for\nselecting appropriate data samples for pretrain-\ning, but external supervision can be quite hard\nto obtain in low-resource domains. In this pa-\nper, we introduce effective ways to select data\nfrom unlabeled corpora of target domains for\nlanguage model pretraining to improve the per-\nformances in target entity extraction tasks. Our\ndata selection strategies do not require any ex-\nternal supervision. We conduct extensive ex-\nperiments for the task of named entity recog-\nnition (NER) on seven different domains and\nshow that language models pretrained on tar-\nget domain unlabeled data obtained using our\ndata selection strategies achieve better perfor-\nmances compared to those using data selection\nstrategies in previous works that use external\nsupervision. We also show that these pretrained\nlanguage models using our data selection strate-\ngies outperform those pretrained on all of the\navailable unlabeled target domain data.\n1 Introduction\nNamed entity recognition (NER) (Lample et al.,\n2016; Nadeau and Sekine, 2007; Finkel and Man-\nning, 2009) is the task of extracting entities from\na given piece of text. NER is useful for several\n+Work done while authors were at Adobe Research.\nnatural language processing (NLP) applications\nsuch as information extraction (Chiticariu et al.,\n2013), retrieval (Banerjee et al., 2019), and lan-\nguage understanding. Over the years, various\napproaches including rule-based techniques (Far-\nmakiotou et al., 2000), unsupervised learning ap-\nproaches (Luo et al., 2019), feature-based extrac-\ntions (Li et al., 2020) have been explored for NER.\nThe recent deep learning-based methods are shown\nto result in state-of-the-art performances for vari-\nous domains (Chiu and Nichols, 2016; Peters et al.,\n2018; Yadav and Bethard, 2018).\nSpecifically, large pretrained language models\nsuch as BERT (Devlin et al., 2019) are widely\nused for NER, as they mark state-of-the-art perfor-\nmances (Jia et al., 2019). However, these models re-\nquire large amounts of annotated data which stands\nas a bottleneck to the training process, particularly\nwhen domains having few labeled data are involved.\nDirectly fine-tuning BERT model on small collec-\ntion of labeled data can yield sub-optimal results.\nTo address this issue, there have been works on\npretraining strategies (Liu et al., 2020; Gururangan\net al., 2020) to adapt BERT-like language models\nto target domains by further pretraining them on\nunlabeled target domain data prior to fine-tuning\non labeled data to improve performances in tasks\nlike NER. However, further pretraining language\nmodels on all of the available unlabeled data from\ntarget domains can be sometimes detrimental to the\ndownstream NER performances, if the entire unla-\nbeled data is not aligned with the entity distribution\nof the labeled data (Liu et al., 2020).\nSpecifically, Liu et al. (2020) used two types of\nunlabeled corpora belonging to the target domain,\nnamely task-level corpus, consisting of unlabeled\nsentences from the target domain that are highly\naligned with the distribution of the labeled down-\nstream NER dataset, and domain-level corpus,\nconsisting of a very large collection of unlabeled\nsentences from the target domain, excluding sen-\n942\ntences from the task-level corpus. Gururangan et al.\n(2020) showed the advantages of pretraining on\nboth domain-level and task-level corpora for im-\nproving NER performance. However, using the\ntask-level corpus only for further pre-training may\nbe inappropriate, as this is usually very small in\nsize. Additionally, pretraining on the entire domain-\nlevel corpus can be challenging, particularly in\nresource-constrained settings*, and can sometimes\nproduce sub-optimal results due to non-alignment\nand distribution mismatch. Liu et al. (2020) intro-\nduced a strategy to select samples from domain-\nlevel corpus to augment the task-level corpus using\nexternal supervision in the form of ontologies to\nimprove the downstream NER performance. While\nthis marks the importance of data selection for pre-\ntraining using ontology-based filtering, it is con-\nstrained by the availability of meta-data and entity\ninformation from Wikipedia, and thus, is not easily\nextensible to other low-resource domains†.\nIn this paper, we introduce new methods for data\nselection via (i) cosine similarity-based retrieval in\nembedding space of domain-level corpus generated\nusing pretrained and fine-tuned BERT (fine-tuned\non labeled target domain data), (ii) entity predic-\ntion on domain-level corpus using fine-tuned BERT.\nNote that both these strategies do not require any\nadditional supervision (e.g., in the form of ontolo-\ngies) or metadata, thus making them easily extensi-\nble to other low-resource domains. Similar to our\nmethod, Dai et al. (2019) proposes unsupervised\nmethods of computing similarities between differ-\nent corpora based on word-embedding and perplex-\nity scores of bert-base-cased. However, unlike our\nmethod of selecting sentences from the domain-\nlevel corpus based on similarity scores, Dai et al.\n(2019) uses the similarity scores to select the most\nappropriate source domain (out of multiple source\ndomains) for a particular target domain (in our set-\nting, both domain-level and task-level corpus be-\nlong to the same domain). We evaluate our method\non i2b2 (Uzuner et al., 2007), Atticus (Hendrycks\net al., 2021), and five other domains curated by (Liu\net al., 2020) with different amounts of selected sen-\ntences for pretraining. Experimental results show\nthat our proposed methods for data selection result\nin better performances in downstream NER tasks,\n*resource-constrained setting(s) refers to settings with\nlimited availability of compute (CPU or GPU) in terms of\ntime and magnitude.\n†low-resource domain(s) setting refers to domains with\nlimited or no availability of external metadata\ncompared to those proposed by previous works that\nrequire external supervision.\n2 Task Definitions\nFollowing the notions in Liu et al. (2020), let\nDC = {(xD\ni )}ND\ni=1, and TC = {(xT\ni )}NT\ni=1 denote\nthe domain-level and the task-level unlabeled cor-\npora having same domain as LC = {(xL\ni , yL\ni )}NL\ni=1,\nwhich denotes labeled corpus tagged with BIOES\nfor NER, where LC ⊆TC, and ND ≫NT ≥NL.\nGiven a large unlabeled DC and relatively small\nTC, our task involves selectively sampling sen-\ntences SC = {(xS\ni )}NS\ni=1 from DC, where SC ⊂\nDC and ND ≫NS, such that pretraining on TC\naugmented with SC increases the performance for\nNER, without having to pretrain on entire DC,\nwhich can be very time and resource intensive.\nFor a clearer understanding of DC, TC, and\nLC we present an example scenario involving\nthese 3 different corpus types: Let Xbe a small-\nscaled legal firm that has a limited collection of\ncustomer contracts. To ease their workflow, they\nwant to automate the process of extracting named\nentities (like contracting-party, contract-amount,\netc.) from contracts using NER models. How-\never, they don’t have permission to outsource all\nthe customer contracts to external annotators to\ncreate labeled data for NER due to legal restric-\ntions. Out of all the customer contracts, only\na very small number (without legal restrictions)\ncan be outsourced for annotations. They also\nhave a large collection of unlabeled SEC contracts\n(https://www.sec.gov/edgar.shtml) that are publicly\navailable (however, these SEC contracts are some-\nwhat different from customer contracts of X in\nterms of format, layout, etc.). In the context of\nour setting all the customer contracts to form TC,\nthe small subset of these contracts that can be used\nfor annotations form LC, and the SEC contracts\nconstitute DC.\n3 Methodology\nThis section describes the details of our stage-wise\nmethod of selecting the pretraining corpus ( SC)\nfrom the domain-level corpus ( DC) to pretrain\nBERT (Devlin et al., 2019), followed by fine-tuning\nit on a small labeled corpus (LC) for NER. Figure\n1 gives an overview of our data selection strate-\ngies for the selective pretraining and the following\nfine-tuning stages to train BERT for NER.\n943\n(a) \n(b) \nFigure 1: Our method of Selective pre-training and finetuning strategy. (a) describes the overall 3 stage approach.\n(b) describes all the 4 different strategies used in Stage 1 of (a) for selecting SC from domain-level corpus (DC). (i)\nCosine Similarity w/ Pretrained BERT, (ii) Cosine Similarity w/ task-level MLM BERT, (iii) Cosine Similarity w/\ntask-level MLM and NER BERT, and (iv) NER-Selected\n3.1 Data Selection Strategies\nContinual pretraining on DC with TC corpora\nprior to fine-tuning BERT for NER has shown sig-\nnificant improvement over just fine-tuned BERT\nw/o task-specific pretraining†† (Gururangan et al.,\n2020). However, when size of DC is extremely\nlarge, pretraining on entire corpus can be very\ntime-consuming and resource intensive (see Table\n5). Additionally, DC can contain noisy and un-\nrelated sentences compared to the TC, leading to\nreduced effectiveness of domain continual pretrain-\ning. Therefore, we devise unsupervised methods\nto select only the useful sentences from DC for\neffective pretraining, without requiring additional\ndomain metadata, such that it can even be applied\nto any real world low-resource domains.\nAharoni and Goldberg (2020) showed the use\nof pretrained BERT (Devlin et al., 2019) for ef-\nfectively clustering sentences consisting of diverse\ndomains. Drawn on this observation, we use the\nBERT model for selecting sentences from DC that\nare more closely associated to TC based on cosine\nsimilarity in sentence embedding space. Moreover,\nbased on results from Liu et al. (2020), sentences\n††in context of this work, w/o task-specific pretraining\nindicates BERT initialized with bert-base-cased parameters,\nwithout MLM on additional DC, TC or SC.\nin DC that contain more domain-specialized enti-\nties are shown to be more effective for pretraining\nthan randomly selected DC sentences. Based on\nthese observations, we propose four novel cosine-\nsimilarity and NER-Selected based unsupervised\nmethods for data selection from the domain-level\ncorpus for further pretraining (Stage I of figure 1):\n• Cosine Similarity w/ Pretrained BERT: We\nuse vanilla pretrained bert-base-cased model\nto obtain embeddings of sentences from TC\nand DC as the activations from the last layer\nof BERT. We compute the mean task-level\nembedding as the average of all task-level cor-\npus sentence embeddings. We then use it as\na query vector to obtain sentences with max-\nimal cosine similarity from the domain-level\ncorpus (DC).\n• Cosine Similarity w/ task-level MLM\nBERT: We first initialize BERT with bert-\nbase-cased parameters and then additionally\npretrain the BERT model using masked lan-\nguage modeling (details of MLM mentioned\nin Section 3.2 and 4.1) on the task-level corpus\n(TC). This model is referred to as B1. We use\nB1 to select sentences from a domain-level\ncorpus based on the previous method.\n944\n• Cosine Similarity w/ task-level MLM and\nNER BERT: We fine-tune B1 for the NER\ntask (on labeled corpus LC), termed B2. Us-\ning B1 and B2, we obtain task-level and\ndomain-level sentence embeddings. We per-\nform an element-wise concatenation of em-\nbeddings obtained from B1 with the ones ob-\ntained using B2, and then select sentences\nusing these embeddings like the previous\nmethod on the aggregated embeddings.\n• NER-Selected: Instead of using DBpedia On-\ntology to select sentences with plentiful enti-\nties as used in Liu et al. (2020), using B2, we\nobtain entity predictions for the sentences in\nthe domain-level corpus. Based on the labels\npredicted on the domain-level corpus as soft\nentity labels, we select the sentences with the\nmaximal number of predicted entities. Since\nB2 is trained on LC with domain-specialized\nentities, it will help in selecting sentences that\nlikely contain similar domain-specialized enti-\nties from DC, thus mitigating the problem of\ndistribution alignment between the NER task\ndata and data used for further pretraining.\n3.2 Domain Pretraining and Fine-tuning\nCombining selected sentences (SC) from domain-\nlevel corpus (DC) in Stage I with the task-level cor-\npus (TC), in 1:1 ratio, we pretrain the BERT model\n(initialized with bert-base-cased parameters) using\nmasked-language modeling (Stage II in figure 1) as\nspecified in Devlin et al. (2019). We mask out 15%\nof the tokens randomly in the selected sentences\nand then replace 80% of the masked tokens with\na special tag ([MASK ]), 10% with random, and\n10% with original tokens.\nFinally, we add a Conditional Random Field\n(CRF) layer on top of this BERT model and train it\nfor NER on labeled corpus LC (Stage III of figure\n1). More details are specified in Section 4.2.\n4 Experiments\n4.1 Experimental Setup\nFor all the experiments, we use BERT model\ninitialized with bert-base-cased (Devlin et al.,\n2019) parameters. The total number of trainable\nparameter in the BERT model used for experiments\nare 110M. In all experiments involving pretraining\nusing any corpus type, we pretrain BERT with\nMLM for 5 epochs with a batch size of 64. We\nthen fine-tune this pretrained BERT on NER\ntask by adding CRF layer on top for 200 epochs\n(stopping at early convergence on dev set) with\nlearning rate 2e −05, L2 regularization of 1e −08\nand AdamW betas (0.9, 0.999). The size of model\nused for NER task is 442.6 MB. All experiments\nwere performed on Tesla V100 GPU. We evaluate\nthe performance of all the methods with F1 Score\n(https://github.com/allanj/pytorch_neural_crf).\nEach experiment is conducted thrice with random\nseed and the average score is reported.\n4.2 Dataset Details\nWe evaluate our proposed method on seven di-\nverse domain datasets including contracts, medical\nrecords, AI, science, politics, music, and litera-\nture domains. The Atticus dataset comprises of\ncontracts, i2b2 (Uzuner et al., 2007) contains medi-\ncal records of patients, and the other five domain\ndatasets are taken from Liu et al. (2020) which\ncontain Wikipedia articles from each of the corre-\nsponding domains of AI, science, politics, music,\nand literature (for example, Science domain con-\ntains Wikipedia articles that fall under Science or\nsimilar categories).\n• i2b2 (Uzuner et al., 2007): We transform the\noriginal dataset of 37.1K labeled sentences\ninto 35.1K domain-level and 2K task-level\nsentences unlabeled sentences. We use la-\nbeled version of the same 2K task-level sen-\ntences as LC for the training of the NER task.\nWe use the original dev and test data provided\nby Uzuner et al. (2007).\n• CrossNER (Liu et al., 2020): We directly use\nthe five different domain datasets provided by\n(Liu et al., 2020), namely AI, Science, Lit-\nerature, Politics and Music. The dataset is\nalready split into task-level and domain-level\ncorpora, with separate train, test, and dev sets\nfor NER.\n• Atticus (Hendrycks et al., 2021): Atticus\nis a recently published Question-Answering\ndataset for contracts. We only use entity type\ncontracting-party from this dataset for NER.\nWe divide all the contracts into splits 70:10:20\nfor train, dev, and test sets. Out of the 28\ndifferent sub-categories of contracts, we se-\nlect 6 documents from three sub-categories\n(‘Strategic Alliance’, ‘Development’, ‘Distrib-\nutor’) as the labeled training set (LC) and all\n945\nDomain Unlabeled CorpusLabeled CorpusLc Entity CategoriesDomain-levelDc Train Dev Test\ni2b2 35.1K 2K 3.9K 35.5K\ndate, patient, doctor, medical-record, age, hospital, phone,\nidnum, username, street, city, state, zip, device, profession,\ncountry, organization, location-other, fax, email\nAI 111K 100 350 431 field, task, product, algorithm, researcher, metrics, university,\ncountry, person, organization, location, miscellaneous\nScience 2.08M 100 350 431\nscientist, person, university, organization, country, location, discipline,\nenzyme, protein, chemical compound, chemical element, event,\nastronomical object, academic journal, award, theory, miscellaneous\nPolitics 3.39M 200 450 450 politician, person, organization, political party, event,\nelection, country, location, miscellaneous\nMusic 4.46M 100 380 456 Music genre, song, band, album, Musical artist, Musical instrument,\naward, event, country, location, organization, person, miscellaneous\nLiterature 3.32M 100 400 416 book, writer, award, poem, event, magazine, person, location,\norganization, country, miscellaneous\nAtticus 38K 1.9K 8K 16.6K contracting-party\nTable 1: Dataset details of the different domains used for experimentation. Table describes the number of sentences\nin unlabeled domain-level corpus, train, test, and dev set of labeled corpus, and entity types of each domain.\nthe documents under these 3 sub-categories\nas task-level sentences ( TC) from the 70%\ntotal train split. The rest of the documents\nfrom the total train split (from the remaining\n25 sub-categories) that are not included in\nthe task-level corpus are considered from the\ndomain-level corpus (DC).\nNote that we modify the original i2b2 and At-\nticus datasets to our problem setting (using the\nmethod described above). More details on the num-\nber of domain-level DC, task-level TC, selected\nSC sentences and labeled train, test and dev splits\nalong with entity types are provided in Tables 1\nand 2. For fairness of comparison of our methods\nwith Entity-level data selection strategy (Section\n4.3) used in Liu et al. (2020), we use the same\nnumber of selected sentences (SC) for each of the\nLiu et al. (2020) datasets (as mentioned for respec-\ntive domains in Entity-level corpus of (Liu et al.,\n2020)) for experimentation. For i2b2 and Atticus\ndatasets, we take the number of sentences in SC to\nbe approximately 1\n2 that of DC.\n4.3 Baseline Methods\nWe compare the effectiveness of our data-selection\nand pre-training strategies against the following\nbaselines:\n• w/o task-specific pretraining ††: We use a\nbert-base-cased model (Devlin et al., 2019)\nand fine-tune it for the NER task on LC.\n• Task-level pertaining (TAPT): We initialize\nthe BERT model with bert-base-cased param-\neters and pretrain on the task-level corpus with\nMLM, then, fine-tune for the NER task (anal-\nogous to TAPT in Gururangan et al. (2020)).\n• Domain-level pretraining (DAPT): We first\ninitialize the BERT model with bert-base-\ncased parameters and pretrain on the domain-\nlevel corpus with MLM, followed by fine-\ntuning for the NER task (analogous to DAPT\nin Gururangan et al. (2020)).\n• Task w/ Domain-level pretraining (TAPT\n+ DAPT): We combine the domain-level and\ntask-level corpora in 1:1 ratio. We initialize\nBERT with bert-base-cased parameters and\npretrain on the combined corpus with MLM,\nthen, fine-tune for the NER task (analogous to\nTAPT + DAPT in Gururangan et al. (2020)).\n• Entity-level w/ Task-level pretraining: We\nuse the entity-level corpus, provided by Liu\net al. (2020), that comprises of sentences from\ndomain-level corpus having plentiful entities\nwhich are obtained by leveraging knowledge\nfrom DBpedia Ontology. We combine the\nentity-level and task-level corpora in 1:1 ratio\nfor pretraining (integrated corpus mentioned\nin Liu et al. (2020)). Since we use token-level\nMLM in all our methods, for fair comparison\nof the effectiveness of our selected sentences\nthat of Liu et al. (2020), we compare with\ntheir method which performs token-level pre-\ntraining with integrated corpus.\n• Perplexity Score (PPL) : Dai et al. (2019)\nuses language model perplexity score to select\nthe most appropriate source domain, from a\n946\ni2b2 AI Science Politics Music Litera. Atticus\nDomain-levelDc 35.1K (1x) 111K (1x) 2.08M (1x) 3.39M (1x) 4.46M (1x) 3.32M (1x) 38K (1x)\nTask-levelTc 2K (0.05x) 7.4K (0.06x)124K (0.05x)501K (0.14x)826K (0.18x)429K (0.12x)8.4K (0.22x)\nSelectedSc 16K (0.45x)38.4K (0.34x)396K (0.19x)692K (0.20x)1.19M (0.26x)1.01M (0.30x)20K (0.52x)\nTable 2: Number of sentences in different corpus types ( DC, TC and SC) for each domain. The number in the\nbrackets represents the size ratio between the corresponding corpus type and the domain-level corpus (DC).\nModels Corpus/ Selection Method i2b2 AI SciencePoliticsMusicLitera.Atticus\nBERT-based\nCosine-Similarity w/ pretrained BERT72.84 56.54 67.24 72.36 72.85 64.77 64.23\nCosine-Similarity w/ MLM tuned BERT74.22 56.19 67.22 72.44 72.95 64.49 64.53\nCosine-Similarity w/ MLM and NER tuned BERT73.15 56.42 67.13 72.65 72.39 64.74 66.03\nNER-Selected 74.15 56.92 67.54 72.69 73.55 64.29 63.51\nBaseline Methods\nBERT-based\nw/o task-specific pretraining 70.61 53.35 63.86 69.7 67.13 61.35 61.76\nTask-level pretraining (TAPT)72.24 55.08 66.02 70.43 70.61 62.33 63.41\nDomain-level pretraining (DAPT)73.25 55.38 66.27 72.23 71.35 63.2 63.22\nTask w/ Domain-level pretraining (TAPT+DAPT)72.49 55.52 66.56 72.62 71.58 64.57 63.74\nCrossNER Entity-level w/ Task-level pretraining- 56.44 67.15 72.43 72.42 64.36 -\nDai et al. (2019) modified Perplexity Score (PPL) 72.95 56.71 66.4 72.15 73.11 64.25 64.81\nTable 3: F1 score comparison of our and different baselines methods on all the domain datasets. Our model\noutperforms all the baselines methods across all domains. The F1 scores are averaged over 3 experimental runs.\nThe F1 scores for CrossNER (Liu et al., 2020) method on i2b2 and Atticus domains are absent as this method of\ndata selection requires external metadata (DBpedia Ontology) which is not present for both these domains.\nMethod Entity Types\nBook WriterAwardEventMagazinePersonLocationOrganizationCountryMisc.\nDomain-level pretraining (DAPT)69.2 80.49 85.7 62.78 71.55 21.03 49.55 62.61 67.31 36.71\nCosine-Similarity 71.61 81.09 86.62 65.06 75.47 21.34 50 62.86 68.29 36.74w/ MLM and NER tuned BERT\nTable 4: F1-scores for the different entity-types in the Literature domain. Scores are reported for 2 different methods,\n‘Domain-level pretraining (DAPT)’ and ‘Cosine Similarity w/ MLM and NER tuned BERT’. Scores are averaged\nover 3 experimental runs.\ncollection of different domains for a given tar-\nget domain. Specifically, they train a model\non target corpus and compute perplexity score\nof each source corpora using this model, and\nthen select the source that gives the lowest\nperplexity score. Instead, we modify this\nmethod for our scenario by first training a\nBERT model on the task-level corpus and us-\ning it to select sentences from domain-level\ncorpus having minimum perplexity ‡.\n5 Results and Analysis\n5.1 Performance v/s Data Selection Strategies\nFrom the results of our experiments in Table 3, we\nsee that pretraining using selected sentences us-\n‡Note that we do not use the ‘Target V ocabulary Covered’\n(TVC) or ‘Word Vector Variance’ (WVV) as in Dai et al.\n(2019). TVC would not make much meaning to calculate on\na sentence level (unlike for Dai et al. (2019) where they use\nTVC across entire domain corpora). Similarly, WVV is used\nto calculate word vector variance across pretranined word\nvectors on 2 different domains (and not across sentences) and\ncan’t be modified for our scenario of selecting sentences\ning our methods outperform all baselines for all of\nthe domain datasets. Figure 3 (b) shows qualita-\ntive examples of NER predictions from the Atti-\ncus test set, using our method: ‘Cosine-Similarity\nw/ MLM and NER tuned BERT’ and ‘w/o task-\nspecific pretraining’. Our method is able to predict\n‘contacting-party‘ much more accurately than the\n‘w/o task-specific pretraining’ model. According\nto Table 2, although the size of the selected cor-\npus SC is around half or less than half that of the\ndomain-level corpus, the pretraining using SC im-\nproves NER performance than pretraining on DC\nor DC and TC combined. Based on this it can be\nhypothesized that selection of sentences using co-\nsine similarity over BERT embeddings filter many\nnoisy sentences that might not be very related to the\ntask-domain corpus as only the top-most sentences\nare selected that lie close to mean task-level corpus\nembedding. Additionally, NER-Selected method\nremoves sentences that might not contain domain-\nspecific entities related to the particular NER task,\nincreasing the effectiveness of pretraining (See fig-\n947\n(a) Literature  \n(Cosine Similarity w/ MLM and NER Tuned BERT)\n(b) Music  \n(NER-Selected)\nFigure 2: Comparison of F1-scores for different amounts of selected corpus ( SC) (w/ task-level corpus ( TC)).\nScores are averaged over 3 experimental runs.\n(a) (b) \nFigure 3: Figure shows (a) examples of sentences selected and filtered out by the NER-Selected method for the\nScience domain, (b) qualitative examples of contracting-party entity predicted by our method ‘Cosine-Similarity w/\nMLM and NER tuned BERT’ v/s ‘w/o task-specific pretraining’ on two Atticus dataset samples. [Pink] indicates\nground-truth entity mentions, [Blue] indicates predictions by our method ‘Cosine-Similarity w/ MLM and NER\ntuned BERT’, [Yellow] indicates precitions by ‘w/o task-specific pretraining’ method.\nure 3 (a)). Furthermore, integrating the selected\ncorpus and task-level corpus is able to consistently\nboost the downstream NER performance compared\nto utilizing other corpus types although the size of\nthis corpus is still smaller than the domain-level\ncorpus. This is because it ensures that the pre-\ntraining corpus contains content that is explicitly\nrelated to the NER task in the target task-level cor-\npus while also making it relatively larger than the\ntask-level corpus itself. The results suggest that the\ncorpus content is essential for the effectiveness of\ncontinual pretraining. Surprisingly, this pretraining\nstrategy is still effective for the i2b2, Atticus, and\nAI domains even though the corpus sizes in these\ndomains are relatively small, which illustrates the\neffectiveness of our method in settings with less\navailability of unlabeled domain-level corpus.\n5.2 Performance v/s Pretraining Corpus Size\nSince such large domain-level corpora might not be\nalways available in real-world scenarios, we investi-\ngate the performance of pretraining on the different\nquantities of selected sentences (SC). From figure\n2, it is evident that as the number of selected sen-\ntences increases, the performance keeps improving\n(in this figure it is shown for both Literature and\nMusic domains). This implies that a larger corpus\nsize is better for pretraining. However, the slope of\nthe performance gain curve becomes less when we\nincrease the number of sentences for MLM beyond\n50% of SC. Our method of pretraining can also be\nused in resource-constrained settings, where it is\ninfeasible to pretrain with MLM on extremely large\ncorpora. Table 5 shows the performance with the\ntime required for MLM pretraining on different cor-\n948\nCorpus\nLiterature Music\n# sentences# F1 ScoreTime for MLM# sentences# F1 ScoreTime for MLM\n(hh:mm:ss) (hh:mm:ss)\nNone 0 61.35 0 0 67.13 0\nTC 429K 62.33 6:24:23 826K 70.61 11:10:30\nTC+SC(10%) 439K 62.77 6:33:26 838K 70.98 11:20:13\nTC+SC(25%) 682K 63.45 10:11:01 1.12M 71.29 15:12:55\nTC+SC(50%) 935K 63.97 13:57:40 1.42M 72.24 21:37:12\nTC+SC(100%) 1.44M 64.74 21:31:01 2.02M 73.55 30:41:35\nDC 3.32M 63.2 48:14:33 4.46M 71.35 62:14:29\nDC+TC 3.75M 64.57 56:02:13 5.29M 71.58 80:24:54\nTable 5: Comparison of performance (F1 score) and time (in hours:minutes:seconds) for Literature and Music\ndomains using different amounts and corpora ( TC, SC, and DC) of sentences used for MLM pretraining. We\nobserve that for both domains, TC + SC achieves the best performance, with almost half the number of sentences\nused and less than 2.5x the time required for pretraining compared to TC + DC, showing the efficiency of our\nmethod over naive pretraining on entire corpora. The F1 scores are reported over 3 experimental runs. Results for\nLiterature domain are calculated using ‘Cosine-Similarity w/ MLM and NER tuned BERT’ and Music domain using\n‘NER Selected’ methods. We show 2 different domains with different methods of sentence selection to demonstrate\nthe robustness for our different selection methods in terms of both time and F1 score improvement.\nFigure 4: Comparison of F1-scores for different amounts of training labeled corpus ( LC) on Literature domain.\nScores are reported for 2 different methods, ‘Domain-level pretraining (DAPT)’ and ‘Cosine Similarity w/ MLM\nand NER tuned BERT’. Scores are averaged over 3 experimental runs.\npus sizes and types for two different domains with\ntwo different methods of data selection. For the Lit-\nerature domain with ‘Cosine-Similarity w/ MLM\nand NER tuned BERT’, we see that we achieve\nan F1 of 64.67 when using TC with SC compared\nto pretraining using entire DC along with TC (F1\n64.57), even though it takes less than 2.5 times the\ntime required for MLM pretraining with almost\nhalf the number of total sentences. A similar trend\ncan be observed for the Music domain.\n5.3 Fine-grained Comparison\nIn this section, we explore the effectiveness of our\npretraining strategy using selected sentences on in-\ndividual entities belonging to the same domain over\n‘Domain-level pretraining (DAPT)’ method. Table\n4 shows the performance of the majority of the en-\ntities for the Literature domain using two methods\n‘Domain-level pretraining (DAPT)’ and ‘Cosine-\nSimilarity w/ MLM and NER tuned BERT’. We\nsee that the performance has increased for all entity\ntypes when pretraining on selected sentences using\nour method has been used. We observe that the\nperformance on the person entity type is compar-\natively lower than all other entity types for both\nmethods. It may be due to the hierarchical category\nstructure of this domain that causes the model to get\nconfused between person and writer entity types.\nIt can be also seen that the performance of domain-\nspecific entities (like ‘book’, ‘writer’, ‘event’ and\n‘magazine’) has improved much more than the im-\nprovement of the non domain-specific generic en-\n949\ntity types (like ‘person’, ‘location’, ‘organization’)\nwhen our method of ‘Cosine-Similarity w/ MLM\nand NER tuned BERT’ is used over ‘Domain-level\npretraining (DAPT)’ for pretraining.\n5.4 Performance v/s Labeled Corpus Size\nFrom figure 4, it can be inferred that the perfor-\nmance decreases drastically as the number of la-\nbeled samples in LC used for fine-tuning BERT-\nCRF is reduced. Specifically, the performance be-\ncomes extremely low for Literature when there\nare only 10 labeled samples (18.3 F1 for ‘Cosine-\nSimilarity w/ MLM tuned BERT’ and 15.45 F1\nfor ‘Domain-level pretraining (DAPT)’). Although\nthe performance of our method is always better\nthan ‘Domain-level pretraining (DAPT)’ when the\nnumber of labeled samples are extremely low (10),\nthe improvement from our method over ‘Domain-\nlevel pretraining (DAPT)’ is significant ( ∼18%\nincrease), and this margin of improvement gets\nsmaller as we increase the number of labeled sen-\ntences (∼2% at 100 labeled samples). This can\nbe explained by the fact that the BERT model is\nable to gain domain knowledge from the pretrain-\ning data and thus has the ability to better predict\nentity labels.\n6 Conclusion\nWe propose novel methods of data selection tech-\nniques, which do not require additional external\nsupervision, for pretraining BERT for NER. In ad-\ndition to being illustrated on 7 diverse domains, our\nmethod can be easily extended to NER for any new\ndomain with very scarce labeled data and plenty\nof domain unlabeled data. Experiments on multi-\nple domain datasets demonstrate that our selection\ntechniques are better than naive pretraining on the\nentire domain corpus and also achieve better perfor-\nmance compared to state-of-the-art data selection\nmethods using external knowledge bases. Since\nwe may not get domain labels for every corpora, in\nfuture we will try to extend our work with corpus\nconsisting of a mixture of different domains.\n7 Limitations\nOur method works well when both the task-level\nand domain-level corpus to belong to the same do-\nmain or in similar domain, i.e, if the task-level\ncorpus belongs to the science domain, then the\ndomain-level corpus should also belong to sci-\nence/similar domain. However, since clear domain\nlabels may not be present for all corpora in the wild,\none may have to choose an appropriate domain-\nlevel corpus corresponding to the task-level cor-\npus. The second limitation of this work is that\nwe assume the domain-level corpus consists only\nof a single domain, though, a real-world corpus\nmight consist of mixture of sentences belonging\nto different domains. In this work, we have not\nverified how effective our selection strategies will\nbe in selecting sentences for pretraining in such a\nscenario where domain-level corpus consists of a\nmixture of domains (with domains present other\nthan the domain label of task-level corpus). Ad-\nditionally, all our data selection strategies, except\nthe ‘Cosine-Similarity w/ pretrained BERT’ use\ndomain-specific models of data selection, hence\nthere can not be a unified model that can be ap-\nplied for data selection for any domain without\nadditional pretraining.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics.\nPartha Sarathy Banerjee, Baisakhi Chakraborty, Deepak\nTripathi, Hardik Gupta, and Sourabh S Kumar. 2019.\nA information retrieval based on question and an-\nswering and ner for unstructured information with-\nout using sql. Wireless Personal Communications,\n108(3):1909–1931.\nLaura Chiticariu, Yunyao Li, and Frederick Reiss. 2013.\nRule-based information extraction is dead! long live\nrule-based information extraction systems! In Pro-\nceedings of the 2013 conference on empirical meth-\nods in natural language processing, pages 827–832.\nJason P.C. Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional LSTM-CNNs. Trans-\nactions of the Association for Computational Linguis-\ntics, 4:357–370.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Ce-\ncile Paris. 2019. Using similarity measures to\nselect pretraining data for ner. arXiv preprint\narXiv:1904.00585.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n950\nDimitra Farmakiotou, Vangelis Karkaletsis, John Kout-\nsias, George Sigletos, Constantine D Spyropoulos,\nand Panagiotis Stamatopoulos. 2000. Rule-based\nnamed entity recognition for greek financial texts. In\nProceedings of the Workshop on Computational lex-\nicography and Multimedia Dictionaries (COMLEX\n2000), pages 75–78. Citeseer.\nJenny Rose Finkel and Christopher D Manning. 2009.\nNested named entity recognition. In Proceedings of\nthe 2009 conference on empirical methods in natural\nlanguage processing, pages 141–150.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer\nBall. 2021. Cuad: An expert-annotated nlp dataset\nfor legal contract review. NeurIPS.\nChen Jia, Xiaobo Liang, and Yue Zhang. 2019. Cross-\ndomain NER using cross-domain language modeling.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2464–\n2474, Florence, Italy. Association for Computational\nLinguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\narXiv preprint arXiv:1603.01360.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2020. A survey on deep learning for named entity\nrecognition. IEEE Transactions on Knowledge and\nData Engineering, pages 1–1.\nZihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei\nJi, Samuel Cahyawijaya, Andrea Madotto, and Pas-\ncale Fung. 2020. Crossner: Evaluating cross-domain\nnamed entity recognition.\nYing Luo, Hai Zhao, and Junlang Zhan. 2019. Named\nentity recognition only from word embeddings.\narXiv preprint arXiv:1909.00164.\nDavid Nadeau and Satoshi Sekine. 2007. A survey of\nnamed entity recognition and classification. Lingvis-\nticae Investigationes, 30(1):3–26.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nÖzlem Uzuner, Yuan Luo, and Peter Szolovits. 2007.\nEvaluating the state-of-the-art in automatic de-\nidentification. Journal of the American Medical In-\nformatics Association, 14(5):550–563.\nVikas Yadav and Steven Bethard. 2018. A survey on\nrecent advances in named entity recognition from\ndeep learning models. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 2145–2158, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\n951",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8367293477058411
    },
    {
      "name": "Labeled data",
      "score": 0.7389189600944519
    },
    {
      "name": "Transformer",
      "score": 0.659750759601593
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6152424812316895
    },
    {
      "name": "Data extraction",
      "score": 0.5689694881439209
    },
    {
      "name": "Language model",
      "score": 0.5635168552398682
    },
    {
      "name": "Task (project management)",
      "score": 0.5537176132202148
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5464197993278503
    },
    {
      "name": "Training set",
      "score": 0.5234046578407288
    },
    {
      "name": "Natural language processing",
      "score": 0.4910578727722168
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4810657501220703
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.45882320404052734
    },
    {
      "name": "Machine learning",
      "score": 0.43775519728660583
    },
    {
      "name": "Natural language",
      "score": 0.4216209053993225
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "MEDLINE",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 2
}