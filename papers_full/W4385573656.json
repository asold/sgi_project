{
  "title": "TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer",
  "url": "https://openalex.org/W4385573656",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3016539577",
      "name": "Jivnesh Sandhan",
      "affiliations": [
        "Indian Institute of Technology Kanpur"
      ]
    },
    {
      "id": "https://openalex.org/A5021926567",
      "name": "Rathin Singha",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A4366165745",
      "name": "Narein Rao",
      "affiliations": [
        "Indian Institute of Technology Kanpur"
      ]
    },
    {
      "id": "https://openalex.org/A2171370629",
      "name": "Suvendu Samanta",
      "affiliations": [
        "Indian Institute of Technology Kanpur"
      ]
    },
    {
      "id": "https://openalex.org/A1899303167",
      "name": "Laxmidhar Behera",
      "affiliations": [
        "Indian Institute of Technology Mandi",
        "Indian Institute of Technology Kanpur"
      ]
    },
    {
      "id": "https://openalex.org/A2152918269",
      "name": "Pawan Goyal",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034379414",
    "https://openalex.org/W2983180560",
    "https://openalex.org/W3153469086",
    "https://openalex.org/W2250511639",
    "https://openalex.org/W2789474247",
    "https://openalex.org/W2574685885",
    "https://openalex.org/W2166416461",
    "https://openalex.org/W3038008406",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W1513168562",
    "https://openalex.org/W2892239351",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297841269",
    "https://openalex.org/W4300058649",
    "https://openalex.org/W3025716198",
    "https://openalex.org/W3088327458",
    "https://openalex.org/W2538747984",
    "https://openalex.org/W2741444903",
    "https://openalex.org/W2890761057",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W3102906397",
    "https://openalex.org/W2963970605",
    "https://openalex.org/W3094003967"
  ],
  "abstract": "Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to generate the complete candidate solution space, over which various methods are applied to produce the most valid solution. However, these approaches fail while encountering out-of-vocabulary tokens. On the other hand, purely engineering methods for SWS have made use of recent advances in deep learning, but cannot make use of the latent word information on availability. To mitigate the shortcomings of both families of approaches, we propose Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST) consisting of (1) a module that encodes the character input along with latent-word information, which takes into account the sandhi phenomenon specific to SWS and is apt to work with partial or no candidate solutions, (2) a novel soft-masked attention to prioritize potential candidate words and (3) a novel path ranking algorithm to rectify the corrupted predictions. Experiments on the benchmark datasets for SWS show that TransLIST outperforms the current state-of-the-art system by an average 7.2 points absolute gain in terms of perfect match (PM) metric.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6902–6912\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTransLIST: A Transformer-Based Linguistically Informed Sanskrit\nTokenizer\nJivnesh Sandhan1, Rathin Singha2, Narein Rao1, Suvendu Samanta1,\nLaxmidhar Behera1,4 and Pawan Goyal3\n1IIT Kanpur, 2UCLA, 3IIT Kharagpur, 4IIT Mandi\njivnesh@iitk.ac.in,rsingha108@g.ucla.edu,\nnrao20@iitk.ac.in,pawang@cse.iitkgp.ac.in\nAbstract\nSanskrit Word Segmentation (SWS) is essen-\ntial in making digitized texts available and in\ndeploying downstream tasks. It is, however,\nnon-trivial because of the sandhi phenomenon\nthat modifies the characters at the word bound-\naries, and needs special treatment. Existing\nlexicon driven approaches for SWS make use\nof Sanskrit Heritage Reader, a lexicon-driven\nshallow parser, to generate the complete candi-\ndate solution space, over which various meth-\nods are applied to produce the most valid so-\nlution. However, these approaches fail while\nencountering out-of-vocabulary tokens. On the\nother hand, purely engineering methods for\nSWS have made use of recent advances in deep\nlearning, but cannot make use of the latent word\ninformation on availability.\nTo mitigate the shortcomings of both families\nof approaches, we propose Transformer based\nLinguistically Informed Sanskrit Tokenizer\n(TransLIST) consisting of (1) a module that\nencodes the character input along with latent-\nword information, which takes into account the\nsandhi phenomenon specific to SWS and is\napt to work with partial or no candidate solu-\ntions, (2) a novel soft-masked attention to prior-\nitize potential candidate words and (3) a novel\npath ranking algorithm to rectify the corrupted\npredictions. Experiments on the benchmark\ndatasets for SWS show that TransLIST outper-\nforms the current state-of-the-art system by an\naverage 7.2 points absolute gain in terms of\nperfect match (PM) metric.1\n1 Introduction\nSanskrit is considered as a cultural heritage and\nknowledge preserving language of ancient India.\nThe momentous development in digitization efforts\nhas made ancient manuscripts in Sanskrit readily\navailable for the public domain. However, the us-\nability of these digitized manuscripts is limited\n1The codebase and datasets are publicly available at:\nhttps://github.com/rsingha108/TransLIST\ndue to linguistic challenges posed by the language.\nSWS conventionally serves the most fundamen-\ntal prerequisite for text processing step to make\nthese digitized manuscripts accessible and to de-\nploy many downstream tasks such as text classifi-\ncation (Sandhan et al., 2019; Krishna et al., 2016b),\nmorphological tagging (Gupta et al., 2020; Krishna\net al., 2018), dependency parsing (Sandhan et al.,\n2021; Krishna et al., 2020a), automatic speech\nrecognition (Kumar et al., 2022) etc. SWS is not\nstraightforward due to the phenomenon of sandhi,\nwhich creates phonetic transformations at word\nboundaries. This not only obscures the word bound-\naries but also modifies the characters at juncture\npoint by deletion, insertion and substitution opera-\ntion. Figure 1 illustrates some of the syntactically\npossible splits due to the language-specific sandhi\nphenomenon for Sanskrit. This demonstrates the\nchallenges involved in identifying the location of\nthe split and the kind of transformation performed\nat word boundaries.\nśvetodhāvati\nśvā ita ūdhā avati\nśva ita ūdhā avati\nśvetaḥ dhāvati \nśva itaḥ dhāvati\nśveta ūdhā avati \nśva eta ūdhā avati\nInput chunk\nSet of candidate solutions\nCorrect segmentation\nFigure 1: An example to illustrate challenges posed by\nsandhi phenomenon for SWS task.\nThe recent surge in SWS datasets (Krishna et al.,\n2017; Krishnan et al., 2020) has led to various\nmethodologies to handle SWS. Existing lexicon-\ndriven approaches rely on a lexicon driven shal-\nlow parser, popularly known as Sanskrit Heritage\nReader (SHR) (Goyal and Huet, 2016a).2 This line\nof approaches (Krishna et al., 2016a, 2018, 2020b)\n2https://sanskrit.inria.fr/DICO/reader.fr.html\n6902\nformulate the task as finding the most accurate se-\nmantically and syntactically valid solution from the\ncandidate solutions generated by SHR. With the\nhelp of the significantly reduced exponential search\nspace provided by SHR and linguistically involved\nfeature engineering, these lexicon driven systems\n(Krishna et al., 2020b, 2018) report close to state-\nof-the-art performance for the SWS task. How-\never, these approaches rely on the completeness\nassumption of SHR, which is optimistic given that\nSHR does not use domain specific lexicons. These\nmodels are handicapped by the failure of this pre-\nliminary step. On the other hand, purely engineer-\ning based knowledge-lean data-centric approaches\n(Hellwig and Nehrdich, 2018; Reddy et al., 2018;\nAralikatte et al., 2018) perform surprisingly well\nwithout any explicit hand-crafted features and ex-\nternal linguistic resources. These purely engineer-\ning based approaches are known for their ease of\nscalability and deployment for training/inference.\nHowever, a drawback of these approaches is that\nthey are blind to latent word information available\nthrough external resources.\nThere are also lattice-structured approaches\n(Zhang and Yang, 2018; Gui et al., 2019; Li et al.,\n2020) (originally proposed for Chinese Named En-\ntity Recognition (NER), which incorporate lex-\nical information in character-level sequence la-\nbelling architecture). However, these approaches\ncannot be directly applied for SWS; since acquir-\ning word-level information is not trivial due to\nsandhi phenomenon. To overcome these shortcom-\nings, we proposeTransformer-based Linguistically\nInformed Tokenizer (TransLIST). TransLIST is a\nperfect blend of purely engineering and lexicon\ndriven approaches for the SWS task and provides\nthe following advantages: (1) Similar to purely\nengineering approaches, it facilitates ease of scal-\nability and deployment during training/inference.\n(2) Similar to lexicon driven approaches, it is capa-\nble of utilizing the candidate solutions generated by\nSHR, which further improves the performance. (3)\nContrary to lexicon driven approaches, TransLIST\nis robust and can function even when candidate\nsolution space is partly available or unavailable.\nOur key contributions are as follows: (a) We pro-\npose the linguistically informed tokenization mod-\nule (§ 2.1) which accommodates language-specific\nsandhi phenomenon and adds inductive bias for the\nSWS task. (b) We propose a novel soft-masked\nattention (§ 2.2) that helps to add inductive bias for\nprioritizing potential candidates keeping mutual in-\nteractions between all candidates intact. (c) We\npropose a novel path ranking algorithm (§ 2.3) to\nrectify the corrupted predictions. (d) We report an\naverage 7.2 points perfect match absolute gain (§ 3)\nover the current state-of-the-art system (Hellwig\nand Nehrdich, 2018).\nWe elucidate our findings by first describing\nTransLIST and its key components (§ 2), followed\nby the evaluation of TransLIST against strong base-\nlines on a test-bed of 2 benchmark datasets for the\nSWS task (§ 3). Finally, we investigate and delve\ndeeper into the capabilities of the proposed compo-\nnents and its corresponding modules (§ 4).\n2 Methodology\nIn this section, we will examine the key compo-\nnents of TransLIST which includes a linguisti-\ncally informed tokenization module that encodes\ncharacter input with latent-word information while\naccounting for SWS-specific sandhi phenomena\n(§ 2.1), a novel soft-masked attention to prioritise\npotential candidate words (§ 2.2) and a novel path\nranking algorithm to correct mispredictions (§ 2.3).\n2.1 Linguistically Informed Sanskrit\nTokenizer (LIST)\nLexicon driven approaches for SWS are brittle in\nrealistic scenarios and purely engineering based\napproaches do not consider the potentially use-\nful latent word information. We propose a win-\nwin/robust solution by formulating SWS as a\ncharacter-level sequence labelling integrated with\nlatent word information from the SHR as and when\navailable. TransLIST is illustrated with an example\n`svetodh¯avati in Figure 2. SHR employs a Finite\nState Transducer (FST) in the form of a lexical\njuncture system to obtain a compact representation\nof candidate solution space aligned with the input\nsequence. As shown in Figure 2(a), we receive\nthe candidate solution space from the SHR engine.\nHere, `svetah dh¯avati and `sveta ¯udh¯a avati are two\nsyntactically possible splits.3 It does not suggest\nthe final segmentation. The candidate space in-\ncludes words such as `sva, `svetah. and etah. whose\nboundaries are modified with respect to the in-\nput sequence due to sandhi phenomenon. SHR\ngives us mapping (head and tail position) of all\nthe candidate nodes with the input sequence. In\n3Only some of the solutions are shown for visualization.\n6903\nśvet odhāvat i \nśvetaḥ\nśveta\navati\ndhāvati\n(Subset of) candidate solutions from SHR\nūdhā\nśvet odhāvat i \nśv\nśve\nN-grams in absence of SHR\nśvet\nve tiat…\nvet … vat ati\n… vati\n(a)\nTransformer encoder with SMA module\n... śva śvetaḥ etaḥ dhāvatie\n... 1 1 3 63\n… 3 5 5 113\n...aḥ_e\nv\n2\n2\nv\nś\n1\n1\nś\nt\n4\n4\nt\no\n5\n5 (b)\nFigure 2: Illustration of TransLIST with a toy example “´svetodh¯avati”. Translation: “The white (horse) runs.” (a)\nLIST module: We use the candidate solutions (two possible candidate solutions are highlighted with , colors\nwhere the latter is the gold standard) from SHR if available; in the absence of SHR, we resort to using n-grams\n(n≤4). (b) TransLIST architecture: In span encoding, each node is represented by head and tail position index of\nits character in the input sequence. , , denote tokens, heads and tails, respectively. The SHR helps to include\nwords such as ´sva, ´svetah. and etah. whose boundaries are modified with respect to input sequence due to sandhi\nphenomenon. Finally, on the top of the Transformer encoder, classification head learns to predict gold standard\noutput shown by for the corresponding input character nodes only.\ncase such mapping is incorrect for some cases, we\nrectify it with the help of deterministic algorithm\nby matching candidate nodes with the input sen-\ntence and finding the closest match. In the absence\nof SHR, we propose to use all possible n-grams\n(n ≤4)4which helps to add inductive bias about\nneighboring candidates in the window size of 4.5\nWe feed the candidate words/n-grams to the Trans-\nformer encoder and the classification head learns to\npredict gold standard output for the corresponding\ninput character nodes only. The output vocabulary\nconsists of unigram characters (e.g., ´s, v), bigrams\nand tri-grams (e.g., a h. _). The output vocabulary\ncontains ‘_’ to represent spacing between words.\nConsequently, TransLIST is capable of using both\ncharacter-level modelling as well as latent word\ninformation as and when available. On the other\nhand, purely engineering approaches rely only on\ncharacter-level modelling and Lexicon driven ap-\nproaches rely only on word-level information from\nSHR to handle sandhi.\n2.2 Soft Masked Attention (SMA)\nTransformers (Vaswani et al., 2017) have been\nproven to be effective for capturing long-distance\n4We do not observe significant improvements forn >4.\n5Our probing analysis (Figure 4) suggests that char-char\nattention mostly focuses on immediate neighbors. Refer to § 4\nfor detailed ablations on LIST variants.\ndependencies in a sequence. The self-attention\nproperty of a Transformer facilitates effective in-\nteraction between character and available latent\nword information. There are two preliminary pre-\nrequisites for effective modelling of inductive bias\nfor tokenization: (1) Allow interactions between\nthe candidate words/characters within and amongst\nchunks. (2) Prioritize candidate words contain-\ning the input character for which a prediction is\nbeing made (e.g., in Figure 2(b), `sva and `svetah.\nare prioritized amongst the candidate words when\npredicting for the character `s).6 The vanilla self-\nattention (Vaswani et al., 2017) can address both\nthe requirements; however, it has to self-learn the\ninductive bias associated with prioritisation. It may\nnot be an effective solution in low-resourced set-\ntings. On the other hand, if we use hard-masked\nattention to address the second prerequisite, we\nlose mutual interactions between the candidates.\nHence, we propose a novel soft-masked attention\nwhich helps to address both the requirements effec-\ntively. To the best of our knowledge, there is no\nexisting soft-masked attention similar to ours. We\nformally discuss this below.\nSelf-attention maps a query and a set of key-\nvalue pairs to an output as discussed in Vaswani\net al. (2017). For an input x = ( x1,...,x n)\n6We find that failing to meet any one of the prerequisites\nleads to drop in performance (§ 4).\n6904\nwhere xi ∈ Rdx, self-attention gives an output\nz = ( z1,...,z n) where zi ∈Rdz . We presume\nthe standard formulation of vanilla self-attention\n(Vaswani et al., 2017) where dx is the dimension\nof input word representation and dz is the projec-\ntion dimension. Here, WQ,WK,WV ∈Rdx×dz\nare parameter matrices. For simplicity, we ignore\nmulti-head attention in equations 1, 2 and 3.\nzi =\nn∑\nj=1\nαij(xjWV ) (1)\nαij = exp (eij)∑n\nk=1 exp (eik) (2)\neij = (xiWQ)(xjWK)T\n√dz\n(3)\nIn soft-masked attention, we provide a prior\nabout interactions between candidate words and the\ninput characters using a span encoding (sij ∈Rdz )\n(Li et al., 2020). Intuitively, it helps inject inductive\nbias associated with prioritisation whilst maintain-\ning mutual interactions between the candidates.\nFormally, we modify Equation 2 to define soft\nmasked attention as:\nαSM\nij = Mij exp (eij)∑n\nk=1 Mik exp (eik) (4)\nwhere M ∈Rn×n, Mij ∈[0,1]. Mij is defined\nas:\nMij = (xiWQ)(sijWR)T\n√dz\n(5)\nWR ∈ Rdz×dz is a learnable parameter which\nprojects sij into a location-based key vector space.\nSummarily, the proposed SMA module helps to pri-\noritize potential candidate words with the help of\nseparation, inclusion and intersection information\nbetween nodes. Finally, we calculate the output z\nwith the help of the proposed SMA as follows:\nzi =\nn∑\nj=1\nαSM\nij (xjWV ) (6)\nNext, we discuss the span position encoding.\nSpan position encodingis one of the backbones\nof the proposed soft-masked module. It is utilized\nto capture the interactions between the candidate\nwords and the sequence of input characters. Each\nspan/node (which is a character/word and its cor-\nresponding position in the input sentence) is rep-\nresented by the head and tail which denote the\nposition index of the initial and final characters of\nthe token in the input sequence, as shown in Fig-\nure 2(b). The span of character is characterized by\nthe same head and tail position index. For example,\nhead[i] and tail[i] represent the head and tail index\nof span xi, respectively. The separation, inclusion\nand intersection information between nodes xi and\nxj can be captured by the four distance equations\n7-10.\nd(hh)\nij = head[i] −head[j] (7)\nd(ht)\nij = head[i] −tail[j] (8)\nd(th)\nij = tail[i] −head[j] (9)\nd(tt)\nij = tail[i] −tail[j] (10)\nThe final span encoding is a non-linear transforma-\ntion of these 4 distances:\nsij = ReLU(ws(pd(hh)\nij\n⊕pd(ht)\nij\n⊕pd(th)\nij\n⊕pd(tt)\nij\n))\n(11)\nwhere ws ∈Ris a learnable parameter, ⊕is a con-\ncatenation operation and pd ∈R\ndz\n4 is a sinusoidal\nposition encoding similar to Vaswani et al. (2017).\n2.3 Path Ranking for Corrupted Predictions\n(PRCP)\nOur error analysis (§ 4) suggests that sometimes the\nproposed system predicts words that are not part\nof the candidate solution space. These mistakes\ncan be rectified with the help of SHR’s candidate\nsolutions by appropriately substituting suitable can-\ndidates. We refer to the prediction corresponding\nto a chunk that does not fall in the candidate solu-\ntion space, as a corrupted predictionand define a\npath as the sequence of characters in a candidate\nsolution for a given input. We enumerate all the\npossible directed paths (In Figure 2(a), two possi-\nble candidate solutions are highlighted with ,\ncolors) corresponding to the input (with a corrupted\nprediction) and formulate the task as apath ranking\nproblem. While designing the path scoring func-\ntion (S), we consider the following criteria: (1)\nSelect a path consisting of semantically coherent\ncandidate words. We use an integrated judgment\nfrom two sources. First, we prefer a path having\na high log-likelihood (LL) score as per TransLIST\nto choose a semantically coherent path in line with\nthe contextual information of TransLIST. Second,\n6905\nwe reinforce the scoring function (S) by consid-\nering the perplexity score ( ρ) for the path from\nthe character-level language model. (2) To avoid\npaths consisting of over-generated segmentation\nprovided by SHR, we use a penalty proportional to\nthe number of words (|W|) present in the path to\nprefer paths with less number of words. This gives\nus the following path scoring function (S):\nS = LLTransLIST\nρCharLM ×|W|\nwhere\nLLTransLIST = log-likelihood by TransLIST\nρCharLM = Perplexity score by CharLM\n|W|= Number of words present in path\n3 Experiments\nData and Metrics: Currently, Digital Corpus\nof Sanskrit (Hellwig, 2010, DCS) has more than\n600,000 morphologically tagged text lines. It con-\nsists of digitized constructions composed in prose\nor poetry over a wide span of 3000 years. Sum-\nmarily, DCS is a perfect representation of var-\nious writing styles depending on time and do-\nmains. We use two available benchmark datasets\n(Krishna et al., 2017, SIGHUM) 7 and (Krishnan\net al., 2020, Hackathon) for SWS. Both datasets\nare subset of DCS (Hellwig, 2010). These datasets\nalso come with candidate solution space generated\nby SHR for SWS. We prefer Krishna et al. (2017,\nSIGHUM) over a relatively larger dataset (Hell-\nwig and Nehrdich, 2018) to obviate the time and\nefforts required for obtaining candidate solution\nspace. We obtain the ground truth segmentation so-\nlutions from DCS. We could not use DCS10k (Kr-\nishna et al., 2020b) due to partly missing gold\nstandard segmentation (inflections) for almost 50%\ndata points. SIGHUM consists of 97,000, 3,000\nand 4,200 sentences as train, dev, test set, respec-\ntively. Similarly, Hackathon consists of 90,000,\n10,332 and 9,963 sentences as train, dev and test\nset, respectively. We use the following word-level\nevaluation metrics: macro-averaged Precision (P),\nRecall (R), F1-score (F) and the percentage of sen-\ntences with perfect matching (PM).\n7https://zenodo.org/record/803508#\n.YRdZ43UzaXJ\nHyper-parameter settings: For the implemen-\ntation of TransLIST, we build on top of codebase\nby Li et al. (2020). We use the following hyper-\nparameters for the best configuration of TransLIST:\nnumber of epochs as 50 and a dropout rate of 0.3\nwith a learning rate of 0.001. We release our code-\nbase and datasets publicly under the Apache license\n2.0. All the artifacts used in this work are publicly\navailable for the research purpose. For all the sys-\ntems, we do not use any pretraining. All the input\nrepresentations are randomly initialized. We use\nGeForce RTX 2080, 11 GB GPU memory comput-\ning infrastructure for our experiments.\nBaselines: We consider two lexicon-driven ap-\nproaches where Krishna et al. (2016a, SupPCRW)\nformulate SWS as an iterative query expansion\nproblem and Krishna et al. (2018, Cliq-EBM) de-\nploy a structured prediction framework. Next, we\nevaluate four purely-engineering based approaches,\nnamely, Encoder-Decoder framework (Reddy et al.,\n2018, Seq2Seq), character-level sequence labelling\nsystem with combination of recurrent and convolu-\ntion element (Hellwig and Nehrdich, 2018, rcNN-\nSS), vanilla Transformer (Vaswani et al., 2017)\nand character-level Transformer with relative po-\nsition encoding (Yan et al., 2019, TENER). Fi-\nnally, we consider lattice-structured approaches\noriginally proposed for Chinese NER which in-\ncorporate lexical information in character-level se-\nquence labelling architecture. These approaches\nconsist of lattice-structured LSTM (Zhang and\nYang, 2018,Lattice-LSTM), graph neural network\n(GNN) based architecture (Gui et al., 2019,Lattice-\nGNN) and Transformer based architecture (Li et al.,\n2020, FLAT-Lattice). TransLIST:As per § 2.1,\nwe report two variants: (a) TransLISTngrams which\nmakes use of only n-grams, and (b) TransLIST\nwhich makes use of SHR candidate space.\nResults: Table 1 reports the results for the best\nperforming configurations of all the baselines on\nthe test set of benchmark datasets for the SWS\ntask.8 Except purely engineering based systems\n(Seq2seq, TENER, Transformer and rcNN-SS),\nall systems leverage linguistically refined candi-\ndate solution space generated by SHR. Among the\nlattice-structured systems, FLAT-Lattice demon-\nstrates competing performance against rcNN-SS.\n8We do not compare with recently proposed variant of\nClique-EBM (Krishna et al., 2020b) and seq2seq baseline\n(Aralikatte et al., 2018) due to unavailability of codebase.\nAlso, they do not report performance on these two datasets.\n6906\nSIGHUM Hackathon\nModel P R F PM P R F PM\nSeq2seq 73.44 73.04 73.24 29.20 72.31 72.15 72.23 20.21\nSupPCRW 76.30 79.47 77.85 38.64 - - - -\nTENER 90.03 89.20 89.61 61.24 89.38 87.33 88.35 49.92\nLattice-LSTM 94.36 93.83 94.09 76.99 91.47 89.19 90.31 65.76\nLattice-GNN 95.76 95.24 95.50 81.58 92.89 94.31 93.59 70.31\nTransformer 96.52 96.21 96.36 83.88 95.79 95.23 95.51 77.70\nFLAT-Lattice 96.75 96.70 96.72 85.65 96.44 95.43 95.93 77.94\nCliq-EBM 96.18 97.67 96.92 78.83 - - - -\nrcNN-SS 96.86 96.83 96.84 87.08 96.40 95.15 95.77 77.62\nTransLIST ngrams 96.97 96.77 96.87 86.52 96.68 95.74 96.21 79.28\nTransLIST 98.80 98.93 98.86 93.97 97.78 97.44 97.61 85.47\nTable 1: Performance evaluation between baselines in terms of P, R, F and PM metrics. The significance test\nbetween the best baselines, rcNN-ss, FLAT-lattice and TransLIST in terms of recall/perfect-match metrics:p< 0.05\n(as per t-test, for both the datasets). We do not report the performance of SupPCRW and Cliq-EBM on Hackathon\ndataset due to unavailability of codebase. On SIGHUM, we report numbers from their papers. The best baseline’s\nresults for the corresponding datasets are underlined. The overall best results per column are highlighted in bold.\nWe find that rcNN-SS and FLAT-Lattice perform\nthe best among all the baselines on SIGHUM and\nHackathon datasets, respectively.\nBoth the TransLIST variants outperforms all the\nbaselines in terms of all the evaluation metrics with\nTransLIST providing an average 1.8 points (F) and\n7.2 points (PM) absolute gain with respect to the\nbest baseline systems, rcNN-SS (on SIGHUM) and\nFLAT-Lattice (on Hackathon). Even when the SHR\ncandidate space is not available, the proposed sys-\ntem can use TransLISTngrams, which provides an av-\nerage 0.11 points (F) and 0.39 points (PM) absolute\ngain over the best baselines. TransLISTngrams gives\ncomparable performance to rcNN-SS on SIGHUM\ndataset, while on the Hackathon dataset, it performs\nsignificantly better than FLAT-Lattice (p <0.05\nas per t-test). The wide performance gap between\nTransLIST and TransLISTngrams demonstrates the\neffectiveness of using SHR candidate space, when\navailable. Summarily, we establish a new state-\nof-the-art results with the help of meticulously\nstitched LIST, SMA and PRCP modules. The\nknowledge of the candidate space by SHR gives an\nextra advantage to TransLIST. Otherwise, natural\nchoice is the proposed purely engineering variant\nTransLISTngrams when that is not available.\n4 Analysis\nIn this section, we investigate various questions\nto dive deeper into the proposed components and\ninvestigate the capabilities of various modules. We\nTransLIST\n- PRCP\n- SMA\n- LIST\n84 86 88 90 92 94\n(a)\nTransLIST\n- |W|\n-  ρ\n- LL\n91 92 93 94 (b)\nFigure 3: Ablations on (a) TransLIST (b) PRCP module\nin terms of PM (SIGHUM-test). Each ablation in (a)\nremoves a single module from TransLIST. For exam-\nple, “-SMA” removes SMA from TransLIST. For (b),\nablations are shown by removing a particular term from\npath scoring function (S).\nuse SIGHUM dataset for the analysis.\n(1) Ablation analysis: Here, we study the contri-\nbution of different modules towards the final per-\nformance of TransLIST. Figure 3(a) illustrates ab-\nlations in terms of PM when a specific module is\nremoved from TransLIST. For instance, ‘-LIST’\ncorresponds to character-level transformer encoder\nwith SMA and PRCP. Removal of any of the mod-\nules degrades the performance. Figure 3(a) shows\nthat LIST module is the most crucial for providing\ninductive bias of tokenization. Also, removal of\n‘PRCP’ module has a large impact on the perfor-\nmance. We observe that the PRCP module gets ac-\ntivated for 276 data points out of 4,200 data points\nin the test set. We then deep dive into the PRCP\npath scoring function in Figure 3(b), which con-\nsists of 3 terms, namely, penalty (|W|), perplexity\n6907\n(b) word-word(a) char-char (c) char-word\nFigure 4: SMA probing: Illustration of char-char, char-word and word-word interactions. The strength of the SMA\ndecreases in the following order: red, orange, green and blue. Char-char attention mostly focuses on characters\npresent in the vicinity of window size 1. Word-word interactions are able to capture whether a word is subword\nof another word or not. Finally, we find that quality of attention goes down for char-word as we move as per the\nfollowing order: in vocabulary gold words (pink), in vocabulary non-golds (black) and out-of-vocabulary words\n(red). Some of the attentions are invisible due to very low attention score.\nscore by CharLM (|ρ|) and log-likelihood (LL) by\nTransLIST, respectively. We remove a single term\nat a time from the path scoring function, and ob-\nserve each of the terms used in the scoring function\nplays a major role in the final performance.\n(2) Comparative analysis of potential LIST mod-\nule variants to add inductive bias for tokeniza-\ntion: We evaluate possible LIST variants which\ncan help inject inductive bias for tokenization via\nauxiliary (word) nodes illustrated in Figure 2(b):\n(a) sandhi rules: We use sandhi rules as a proxy to\nindicate potential modifications at specific position\nin the input sequence. For example, if input chunk\ncontains the character ‘o’ (Figure 1) then it can be\nsubstituted with two possibilities ¯o →a-¯u/ah. . We\nprovide this proxy information through auxiliary\nnodes. (b) Sanskrit vocab: We obtain a list of vo-\ncabulary words from DCS corpus (Hellwig, 2010)\nand add the words which can be mapped to the\ninput character sequence using a string matching\nalgorithm. (c) n-grams: This is TransLIST ngrams\n(d) SHR: We follow the exact settings as described\nin § 2.1 except that we do not use the PRCP compo-\nnent. In Table 2, we compare these with the purely\nengineering variant of TransLIST (Base system:\nonly character-level Transformer) where no induc-\ntive bias for tokenization is injected. Clearly, due\nto availability of enriched candidate space, SHR\nvariant outperforms all its peers. However, com-\npeting performance of n-gram variant is appealing\nbecause it completely obliviates the dependency\non SHR and remains unaffected in the absence of\nSHR’s candidate space.\nSystem P R F PM\nBase system 92.75 92.62 92.69 72.33\n+sandhi rules 93.53 93.70 93.62 75.71\n+Sanskrit V ocab 96.75 96.70 96.72 85.65\n+n-grams 96.97 96.77 96.87 86.52\n+SHR 97.79 97.45 97.62 88.47\nTable 2: The comparison (on SIGHUM-test set) in be-\ntween LIST variants. ‘+’ indicates system where the\ncorresponding variant is augmented with the base sys-\ntem. We do not activate PRCP for any of these systems.\n(3) Probing analysis on SMA:Here we analyze\nwhether SMA upholds the prerequisite for effective\nmodelling of inductive bias, i.e., prioritize candi-\ndate words which contain the input character for\nwhich the prediction is being made. Figure 4 il-\nlustrates three types of interactions, namely, char-\nchar, char-word and word-word. We use color\ncoding scheme to indicate the strength of atten-\n6908\ntion weight. The attention weight decreases in the\nfollowing order: Red, Orange, Green and Blue.\nChar-char attention mostly focuses on characters\npresent in the vicinity of window size 1. This local\ninformation is relevant to make decisions regard-\ning possible sandhi split. Word-word interactions\nare able to capture whether a word is subword of\nanother word or not. Finally, for char-word atten-\ntion, we find that quality of attention goes down\nas we move as per the following order: in vocab-\nulary gold words (pink), in vocabulary non-golds\n(black) and out-of-vocabulary (unseen in training\nbut recognized by SHR) gold words (red). While\nthe drop in attention from in-vocabulary gold to-\nkens to out-of-vocabulary gold tokens is expected,\nthe drop in attention from gold tokens to non-gold\ntokens is desired. Thus, this probing analysis sug-\ngests that SMA module helps to improve intra/inter\ninteractions between character/words and this sub-\nstantiates the need of SMA module in TransLIST.\n(4) How does TransLIST perform in a non-\ntrivial situation where multiplesandhi rules are\napplicable? In Table 3, we report the compar-\nison with rcNN-SS for a critical scenario of a\nsandhi phenomenon. Table 3 represents the possi-\nble sandhi rules that generate the surface character\n¯a. Following Goyal and Huet (2016b), the sandhi\nrewrite rules are formalized as u|v → f/x−−\n(Kaplan and Kay, 1994) where x,v,f ∈Σ , and\nu ∈Σ+. Σ is the collection of phonemes, Σ∗: a\nset of all possible strings over Σ, and Σ+ = Σ∗ −ϵ.\nFor example, the potential outputs for the input ¯a\ncan be ¯a, ¯a-¯a, ¯a-a, a-a and ah. . The correct rule can\nbe decided based on the context. These multiple\nrules pose a non-trivial challenge for a system to\nidentify the applicability of specific rule. There-\nfore, it is interesting to compare the TransLIST\nwith current state-of-the-art system to verify its\nability for semantic generalization. We observe\nthat TransLIST consistently outperforms rcNN-SS\nin terms of all metrics.9 Table 3 describes rules in\ndecreasing order of their frequency. Interestingly,\nwe notice large improvements over the current state-\nof-the-art system, especially for rare sandhi rules.\nThis observation confirms superior performance of\nTransLIST over the current state-of-the-art system.\n9Follwing Hellwig and Nehrdich (2018), we report\ncharacter-level F-score metric. P = |Sg∩Sp|\n|Sp| ; R = |Sg∩Sp|\n|Sg| ,\nF1 = 2PR\nP+R , (Sg) : Set of locations where the rule occurs in\ngold output, (Sp) : Set of locations where the rule is predicted.\nrcNN-SS TransLIST\nRules P R F P R F\n¯a 99.3 99.3 99.3 99.7 99.6 99.6\na-a 95.4 96.6 96.0 96.6 97.8 97.2\n¯a-a 88.4 83.1 86.5 90.5 83.8 87.0\n¯ah. 76.7 70.1 73.7 77.2 80.1 78.0\n¯a-¯a 50.1 42.1 45.7 80.0 40.9 53.3\nTable 3: The comparison (on SIGHUM-test set) in\nterms of P, R and F metrics between rcNN-SS and the\nTransLIST for ambiguous sandhi rules leading to the\nsame surface character ¯a. The proposed model consis-\ntently outperforms rcNN-SS in all the metrics.\n30 40 50 60 70 80 90 100 110\nSentence Length\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5F1-Score\nTransLIST\nFLAT-Lattice\nrcNN-SS\nLattice-GNN\nLattice-LSTM\nTENER\nFigure 5: F1-score against sentence length (no. of char-\nacters) over the SIGHUM dataset\n(5) How robust is the system when sentence\nlength is varied? In Figure 5, we analyze the per-\nformance of the baselines with different sentence\nlengths. We plot the F1-score against sentence\nlength. Clearly, while all the systems show supe-\nrior performance for shorter sentences, TransLIST\nis much more robust for longer sentences compared\nto other baselines. The lattice-structured baselines\ngive competing F1-scores over short sentences but\nrelatively sub-par performance over long sentences.\n(6) Illustration of PRCP with an example:Ta-\nble 4 illustrates an example that probes the ef-\nfectiveness of PRCP in TransLIST. We com-\npare TransLIST with rcNN-SS and observe that\nTransLIST also predicts words out of candidate\nsolution space when PRCP module is not acti-\nvated. However, the degree of such mistakes in\nTransLIST is comparatively less due to effective\nmodelling of inductive bias for tokenization using\nLIST and SMA modules. In Table 4, rcNN-SS\npredicts three words which are not part of candi-\ndate space, namely, v¯ambike, yaks. avapuh. and caka.\nThese are mistakes that can be rectified with the\nhelp of available candidate space. Interestingly,\nTransLIST commits only a single mistake in this\n6909\nSentence F-score\nInput sentence kimetad¯ı´se bahu´sobham¯ane v¯am. bike yaks.avapu´scak¯asti -\nTranslation: What is this body resembling a Yaksha that glows,\noh Ambika! You who lord over! You who shine!\nCorrect segmentation kim etat ¯ı´se bahu ´sobham¯ane v¯a ambike yaks.a vapuh. cak¯asti -\nSHR candidate space kim, etat, ¯ı´se, bahu, ´sobham¯ane, ´sobham, ¯ane, ´sobha, m ¯ane, -\nm¯a, v¯a, ambike, yaks.a, vapuh. , cak¯asti, ca, k ¯a, asti\nWord-word meaning: what, this, the one who lord, very much,\nthe one who shine, bright, mouth, I respect, never, or, Parvati,\na kind of celestial being, body, glows, and, who (female), is there (be).\nrcNN-SS kim etat ¯ı´se bahu ´sobham¯ane v¯ambike yaks.avapuh. caka asti 52.60\nTransLIST-PRCP kim etat ¯ı´se bahu ´sobham¯ane v¯a aambike yaks.a vapuh. cak¯asti 90.00\nTransLIST kim etat ¯ı´se bahu ´sobham¯ane v¯a ambike yaks.a vapuh. cak¯asti 100.00\nTable 4: An example to illustrate the effectiveness of PRCP module of TransLIST. Bold represents incorrect\nsegmentation for the input sequence.\ncategory by predicting out of solution space word\naambike. PRCP aids in mitigating such mistake by\nappropriately substituting suitable candidates.\n5 Related Work\nEarlier approaches on SWS focused on rule-based\nFinite State Transducer systems (Gérard, 2003;\nMittal, 2010). Natarajan and Charniak (2011) at-\ntempted to solve the SWS task for sentences with\none or two splits using the Bayesian approach. Re-\ncently, Goyal and Huet (2016a, SHR) proposed a\nlexicon driven shallow parser. This, along with the\nrecent upsurge in segmentation datasets (Krishna\net al., 2017; Hellwig and Nehrdich, 2018; Krishnan\net al., 2020) led to two categories of approaches,\nnamely, lexicon driven (Krishna et al., 2016a, 2018,\n2020b) and purely engineering (Hellwig, 2015;\nHellwig and Nehrdich, 2018; Aralikatte et al., 2018;\nReddy et al., 2018). These existing approaches for\nSWS are either brittle in realistic scenarios or do\nnot consider the potentially useful/available infor-\nmation. Thus, TransLIST bridges the shortcomings\nexhibited by each family and gives a win-win solu-\ntion that marks a new state-of-the-art results.\n6 Conclusion and Discussion\nIn this work, we focused on Sanskrit word segmen-\ntation task. To address the shortcomings of existing\npurely engineering and lexicon driven approaches,\nwe demonstrate the efficacy of TransLIST as a win-\nwin solution over drawbacks of the individual lines\nof approaches. TransLIST induces inductive bias\nfor tokenization in a character input sequence using\nthe LIST module, and prioritizes the relevant candi-\ndate words with the help of soft-masked attention\n(SMA module). Further, we propose a novel path\nranking algorithm to rectify corrupted predictions\nusing linguistic resources on availability (PRCP\nmodule). Our experiments showed that TransLIST\nprovides a significant boost with an average 7.2\npoints (PM) absolute gain compared to the best\nbaselines, rcNN-SS (SIGHUM) and FLAT-Lattice\n(Hackathon). We have also showcased fine-grained\nanalysis on TransLIST’s inner working. We plan to\nextend this work for morphological tagging in stan-\ndalone mode (Gupta et al., 2020) and multi-task\nsetting (Krishna et al., 2018) with the SWS task.\nLimitations\nThe preliminary requirement to extend TransLIST\nfor the languages which also exhibit sandhi phe-\nnomenon is lexicon-driven shallow parser similar\nto Sanskrit Heritage Reader (SHR). Otherwise, the\nnatural choice is the proposed purely engineering\nvariant TransLISTngram. It would be interesting\nto check if TransLIST and TransLISTngram can be\nused together.\nEthics Statement\nWe do not foresee any ethical concerns with the\nwork presented in this manuscript.\nAcknowledgements\nWe are grateful to Oliver Hellwig for providing\nthe DCS Corpus and Gerard Huet for providing\nthe Sanskrit Heritage Engine. We thank Sriram\nKrishnan, University of Hyderabad and Hackathon\norganizers10 for providing Hackathon dataset. We\n10https://sanskritpanini.github.io/\nindex.html\n6910\nthank Amrith Krishna, University of Cambridge for\nclarifying our queries related to SIGHUM dataset\nand evaluation metrics. We are grateful to Rishabh\nKumar, IIT Bombay for helping us with evaluation\nof Cliq-EBM baseline. We would like to thank the\nanonymous reviewers for their constructive feed-\nback towards improving this work. The work of\nthe first author is supported by the TCS Fellowship\nunder the Project TCS/EE/2011191P.\nReferences\nRahul Aralikatte, Neelamadhav Gantayat, Naveen Pan-\nwar, Anush Sankaran, and Senthil Mani. 2018. San-\nskrit sandhi splitting using seq2(seq)2. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4909–4914,\nBrussels, Belgium. Association for Computational\nLinguistics.\nHuet Gérard. 2003. Lexicon-directed segmentation and\ntagging of sanskrit. In XIIth World Sanskrit Confer-\nence, Helsinki, Finland, Aug, pages 307–325. Cite-\nseer.\nPawan Goyal and Gérard Huet. 2016a. Design and anal-\nysis of a lean interface for sanskrit corpus annotation.\nJournal of Language Modelling, 4:145.\nPawan Goyal and Gérard Huet. 2016b. Design and anal-\nysis of a lean interface for sanskrit corpus annotation.\nJournal of Language Modelling, 4:145.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jin-\nlan Fu, Zhongyu Wei, and Xuanjing Huang. 2019.\nA lexicon-based graph neural network for Chinese\nNER. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1040–1050, Hong Kong, China. Association for Com-\nputational Linguistics.\nAshim Gupta, Amrith Krishna, Pawan Goyal, and Oliver\nHellwig. 2020. Evaluating neural morphological tag-\ngers for Sanskrit. In Proceedings of the 17th SIG-\nMORPHON Workshop on Computational Research\nin Phonetics, Phonology, and Morphology , pages\n198–203, Online. Association for Computational Lin-\nguistics.\nOliver Hellwig. 2010. Dcs-the digital corpus of sanskrit.\nHeidelberg (2010-2021). URL http://www.sanskrit-\nlinguistics.org/dcs/index.php.\nOliver Hellwig. 2015. Using recurrent neural networks\nfor joint compound splitting and sandhi resolution\nin sanskrit. In 4th Biennial Workshop on Less-\nResourced Languages.\nOliver Hellwig and Sebastian Nehrdich. 2018. San-\nskrit word segmentation using character-level recur-\nrent and convolutional neural networks. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2754–2763,\nBrussels, Belgium. Association for Computational\nLinguistics.\nRonald M. Kaplan and Martin Kay. 1994. Regular\nmodels of phonological rule systems. Computational\nLinguistics, 20(3):331–378.\nAmrith Krishna, Ashim Gupta, Deepak Garasangi, Pa-\nvankumar Satuluri, and Pawan Goyal. 2020a. Keep it\nsurprisingly simple: A simple first order graph based\nparsing model for joint morphosyntactic parsing in\nSanskrit. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4791–4797, Online. Association for\nComputational Linguistics.\nAmrith Krishna, Bishal Santra, Sasi Prasanth Bandaru,\nGaurav Sahu, Vishnu Dutt Sharma, Pavankumar Sat-\nuluri, and Pawan Goyal. 2018. Free as in free word\norder: An energy based model for word segmentation\nand morphological tagging in sanskrit. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2550–2561,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAmrith Krishna, Bishal Santra, Ashim Gupta, Pavanku-\nmar Satuluri, and Pawan Goyal. 2020b. A graph\nbased framework for structured prediction tasks in\nsanskrit. Computational Linguistics, 46(4):1–63.\nAmrith Krishna, Bishal Santra, Pavankumar Satuluri,\nSasi Prasanth Bandaru, Bhumi Faldu, Yajuvendra\nSingh, and Pawan Goyal. 2016a. Word segmentation\nin Sanskrit using path constrained random walks. In\nProceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Technical\nPapers, pages 494–504, Osaka, Japan. The COLING\n2016 Organizing Committee.\nAmrith Krishna, Pavan Kumar Satuluri, and Pawan\nGoyal. 2017. A dataset for Sanskrit word segmenta-\ntion. In Proceedings of the Joint SIGHUM Workshop\non Computational Linguistics for Cultural Heritage,\nSocial Sciences, Humanities and Literature , pages\n105–114, Vancouver, Canada. Association for Com-\nputational Linguistics.\nAmrith Krishna, Pavankumar Satuluri, Shubham\nSharma, Apurv Kumar, and Pawan Goyal. 2016b.\nCompound type identification in Sanskrit: What roles\ndo the corpus and grammar play? In Proceedings of\nthe 6th Workshop on South and Southeast Asian Nat-\nural Language Processing (WSSANLP2016), pages\n1–10, Osaka, Japan. The COLING 2016 Organizing\nCommittee.\nSriram Krishnan, Amba Kulkarni, and Gérard Huet.\n2020. Validation and normalization of dcs corpus\nusing sanskrit heritage tools to build a tagged gold\ncorpus.\nRishabh Kumar, Devaraja Adiga, Rishav Ranjan, Am-\nrith Krishna, Ganesh Ramakrishnan, Pawan Goyal,\n6911\nand Preethi Jyothi. 2022. Linguistically informed\npost-processing for asr error correction in sanskrit.\nProc. Interspeech 2022, pages 2293–2297.\nXiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing\nHuang. 2020. FLAT: Chinese NER using flat-lattice\ntransformer. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6836–6842, Online. Association for Computa-\ntional Linguistics.\nVipul Mittal. 2010. Automatic Sanskrit segmentizer\nusing finite state transducers. In Proceedings of the\nACL 2010 Student Research Workshop , pages 85–\n90, Uppsala, Sweden. Association for Computational\nLinguistics.\nAbhiram Natarajan and Eugene Charniak. 2011. s3\n- statistical sandhi splitting. In Proceedings of 5th\nInternational Joint Conference on Natural Language\nProcessing, pages 301–308, Chiang Mai, Thailand.\nAsian Federation of Natural Language Processing.\nVikas Reddy, Amrith Krishna, Vishnu Sharma, Prateek\nGupta, Vineeth M R, and Pawan Goyal. 2018. Build-\ning a word segmenter for Sanskrit overnight. In Pro-\nceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018),\nMiyazaki, Japan. European Language Resources As-\nsociation (ELRA).\nJivnesh Sandhan, Amrith Krishna, Pawan Goyal, and\nLaxmidhar Behera. 2019. Revisiting the role of fea-\nture engineering for compound type identification in\nSanskrit. In Proceedings of the 6th International San-\nskrit Computational Linguistics Symposium, pages\n28–44, IIT Kharagpur, India. Association for Com-\nputational Linguistics.\nJivnesh Sandhan, Amrith Krishna, Ashim Gupta,\nLaxmidhar Behera, and Pawan Goyal. 2021. A lit-\ntle pretraining goes a long way: A case study on\ndependency parsing task for low-resource morpho-\nlogically rich languages. In Proceedings of the 16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Student Research\nWorkshop, pages 111–120, Online. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undefine-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu.\n2019. Tener: Adapting transformer encoder for\nnamed entity recognition.\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1554–1564,\nMelbourne, Australia. Association for Computational\nLinguistics.\nA Appendix\nAverage run times: Table 5 shows the average\ntraining time in hours and inference time in mil-\nliseconds for all competing baselines. We find\nthat pure engineering-based techniques (TENER,\nrcNN-SS) outperform lattice-structured architec-\ntures (Lattice-LSTM, Lattice-GNN, FLAT-Lattice)\nin terms of run time. When the inference times\nof TransLIST and TransLISTngrams are compared,\nTransLIST takes longer owing to the PRCP mod-\nule. It would be interesting to explore approaches\nto optimise the inference time of the PRCP module.\nSystem Train (Hours)Test (ms)\nTENER 4 H 7 ms\nLattice-LSTM 16 H 110 ms\nLattice-GNN 64 H 95 ms\nFLAT-Lattice 5 H 14 ms\nrcNN-SS 4 H 5 ms\nCliq-EBM 10.5 H 750 ms\nTransLISTngrams 8 H 14ms\nTransLIST 8 H 105 ms\nTable 5: Average training time (in hours) and inference\ntime (in milliseconds) for all the competing baselines.\n6912",
  "topic": "Sanskrit",
  "concepts": [
    {
      "name": "Sanskrit",
      "score": 0.8386414051055908
    },
    {
      "name": "Computer science",
      "score": 0.8284273147583008
    },
    {
      "name": "Lexicon",
      "score": 0.6969860792160034
    },
    {
      "name": "Transformer",
      "score": 0.6257287263870239
    },
    {
      "name": "Natural language processing",
      "score": 0.5712065696716309
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5694879293441772
    },
    {
      "name": "Vocabulary",
      "score": 0.5678347945213318
    },
    {
      "name": "Parsing",
      "score": 0.47551944851875305
    },
    {
      "name": "Word (group theory)",
      "score": 0.45764654874801636
    },
    {
      "name": "Speech recognition",
      "score": 0.35232168436050415
    },
    {
      "name": "Linguistics",
      "score": 0.14936822652816772
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I94234084",
      "name": "Indian Institute of Technology Kanpur",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I2799798094",
      "name": "UCLA Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I9579091",
      "name": "Indian Institute of Technology Mandi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I145894827",
      "name": "Indian Institute of Technology Kharagpur",
      "country": "IN"
    }
  ]
}