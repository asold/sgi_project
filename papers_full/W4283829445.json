{
  "title": "Learning Feature Recovery Transformer for Occluded Person Re-Identification",
  "url": "https://openalex.org/W4283829445",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A339959676",
      "name": "Xu Boqiang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2753274729",
      "name": "He Lingxiao",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2032091031",
      "name": "Liang Jian",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A750947612",
      "name": "Sun Zhenan",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963580223",
    "https://openalex.org/W2962923976",
    "https://openalex.org/W3093047836",
    "https://openalex.org/W3034727830",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2964304299",
    "https://openalex.org/W3035569526",
    "https://openalex.org/W2964201641",
    "https://openalex.org/W3034611771",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W6761583661",
    "https://openalex.org/W2931208564",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2962955197",
    "https://openalex.org/W2997987796",
    "https://openalex.org/W2799251491",
    "https://openalex.org/W2235916736",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W3183523885",
    "https://openalex.org/W3203532213",
    "https://openalex.org/W3217568419",
    "https://openalex.org/W2963078173",
    "https://openalex.org/W3194557739",
    "https://openalex.org/W3048070442",
    "https://openalex.org/W2948383821",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W3175823695",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2954137266",
    "https://openalex.org/W2987391422",
    "https://openalex.org/W6767098714",
    "https://openalex.org/W2964006613",
    "https://openalex.org/W2931882224",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W6760001035",
    "https://openalex.org/W2584637367",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W1991452654",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W6754663736",
    "https://openalex.org/W2798590501",
    "https://openalex.org/W3205065953",
    "https://openalex.org/W2998776895",
    "https://openalex.org/W4387969510",
    "https://openalex.org/W2795758732",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2100398441",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3068979841",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2916106175",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2990317318",
    "https://openalex.org/W4293177262",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2890159224"
  ],
  "abstract": "One major issue that challenges person re-identification (Re-ID) is the ubiquitous occlusion over the captured persons. There are two main challenges for the occluded person Re-ID problem, i.e. , the interference of noise during feature matching and the loss of pedestrian information brought by the occlusions. In this paper, we propose a new approach called Feature Recovery Transformer (FRT) to address the two challenges simultaneously, which mainly consists of visibility graph matching and feature recovery transformer. To reduce the interference of the noise during feature matching, we mainly focus on visible regions that appear in both images and develop a visibility graph to calculate the similarity. In terms of the second challenge, based on the developed graph similarity, for each query image, we propose a recovery transformer that exploits the feature sets of its k -nearest neighbors in the gallery to recover the complete features. Extensive experiments across different person Re-ID datasets, including occluded, partial and holistic datasets, demonstrate the effectiveness of FRT. Specifically, FRT significantly outperforms state-of-the-art results by at least 6.2% Rank- 1 accuracy and 7.2% mAP scores on the challenging Occluded-Duke dataset.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nLearning Feature Recovery Transformer for\nOccluded Person Re-identiÔ¨Åcation\nBoqiang Xu, Lingxiao He, Member, IEEE,Jian Liang, and Zhenan Sun, Senior Member, IEEE,\nAbstract‚ÄîOne major issue that challenges person re-\nidentiÔ¨Åcation (Re-ID) is the ubiquitous occlusion over the cap-\ntured persons. There are two main challenges for the occluded\nperson Re-ID problem, i.e., the interference of noise during\nfeature matching and the loss of pedestrian information brought\nby the occlusions. In this paper, we propose a new approach\ncalled Feature Recovery Transformer (FRT) to address the two\nchallenges simultaneously, which mainly consists of visibility\ngraph matching and feature recovery transformer. To reduce\nthe interference of the noise during feature matching, we mainly\nfocus on visible regions that appear in both images and develop\na visibility graph to calculate the similarity. In terms of the\nsecond challenge, based on the developed graph similarity, for\neach query image, we propose a recovery transformer that\nexploits the feature sets of its k-nearest neighbors in the gallery\nto recover the complete features. Extensive experiments across\ndifferent person Re-ID datasets, including occluded, partial and\nholistic datasets, demonstrate the effectiveness of FRT. Specif-\nically, FRT signiÔ¨Åcantly outperforms state-of-the-art results by\nat least 6.2% Rank-1 accuracy and 7.2% mAP scores on the\nchallenging Occluded-Duke dataset. The code is available at\nhttps://github.com/xbq1994/Feature-Recovery-Transformer.\nIndex Terms‚ÄîOccluded person re-identiÔ¨Åcation, Transformer,\nGraph, Occlusion recovery\nI. I NTRODUCTION\nP\nERSON re-identiÔ¨Åcation (Re-ID) [1]‚Äì[3] aims to retrieve\nthe same person from overlapping cameras, which is\nwidely used in security, video surveillance and smart city.\nRecently, considerable Re-ID methods have been proposed in\nthis Ô¨Åeld [4]‚Äì[7]. However, most of them rely on a strong\nassumption that the entire body of the pedestrian is available,\nhowever, this is not always the case in practice. As shown\nin Fig. 1(a), in realistic Re-ID systems, people are always\noccluded by some obstacles especially in crowded places such\nas malls, railway stations and airports. Thus, it is necessary to\nstudy the occluded person Re-ID problem [8].\nThere are two main challenges for the occluded person Re-\nID problem. Firstly, as shown in Fig. 1(a), occlusions always\nbring noise during feature extraction and feature matching.\nSecondly, as shown in Fig. 1(b), the pedestrian information\nin the occluded regions is always lost, making the extracted\nBoqiang Xu, Jian Liang and Zhenan Sun are with the Center for Research\non Intelligent Perception and Computing, National Laboratory of Pattern\nRecognition, Institute of Automation, Chinese Academy of Sciences, Beijing\n100190, China and also with University of Chinese Academy of Sciences,\nBeijing 100190 (email: boqiang.xu@cripac.ia.ac.cn; liangjian92@gmail.com;\nznsun@nlpr.ia.ac.cn).\nLingxiao He is with the AI Research of JD, Beijing 100020, China (email:\nhelingxiao3@jd.com)\nRecover\nShared Region \nFeatures\nChallenge ‚Ö†: Occlusions introduce noise \nduring feature extraction and matching.\nChallenge ‚Ö°: Pedestrian information is \nlost due to the occlusions.\nAfter slicing the images into several semantic parts, our model \nattempts to focus on the shared regions during feature matching.\nOur model attempts to exploit the pedestrian information in the k-\nnearest neighbors features for recovering the occluded query feature. \n(a)\n(c)\n(b)\n(d)\nQuery Gallery Query Gallery\nQuery Gallery\nQuery Feature\n: Features of the unoccluded parts : Features of the occluded parts\nHead\nTorso\nLeg\nHead\nTorso\nLeg\nHead\nTorso\nLeg\nHead\nTorso\nLeg\nNeighbors\nFig. 1. Two challenges in the occluded Re-ID problem and our solutions. The\nÔ¨Ågure is only for illustration, our method is processed in the feature level.\nfeatures not discriminative anymore. Recently, some occluded\nperson Re-ID methods have been proposed [9]‚Äì[12] for the\nÔ¨Årst challenge. They try to use key-points [9], [10] or proba-\nbility maps [11], [12] for feature alignment and increase the\nrobustness of the representations. A recent work [13] views\nkey-points as nodes to construct a graph for learning the\nhuman-topology information and has proved the effectiveness\nof the graph for solving the occluded person Re-ID problem.\nFor the second challenge, some researches [14]‚Äì[16] attempt to\npredict the occluded parts in the images with GANs. However,\noccluded regions generation is not so convincing, especially\nwhen the occlusion is serious. Therefore, the performance\ngains of these methods are very limited.\nIn this work, we propose a novel framework named Feature\nRecovery Transformer (FRT) to address the two challenges\nsimultaneously. SpeciÔ¨Åcally, the proposed framework mainly\nconsists of three phases, semantic feature extraction, visibility\ngraph matching, and occluded feature recovery. In the Ô¨Årst\nphase, we employ the pose information to extract semantic\nfeatures (i.e., global feature, head feature, torso feature and leg\nfeature) and calculate the visibility score for the corresponding\nregions. In the second feature matching phase, we construct\na directional graph with each node containing the semantic\nfeatures from the same part within a pair of images. The\nweights of edges are determined by the visibility score of the\nstarting node. As shown in Fig. 1(a), by promoting the Ô¨Çow\n0000‚Äì0000/00$00.00 ¬© 2021 IEEE\narXiv:2301.01879v1  [cs.CV]  5 Jan 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nof messages in the shared regions (which regions are visible\nin both images), the visibility graph pays more attention to\nthese areas during feature matching, which is not sensitive to\nthe occlusions. Based on the similarity computed by visibility\ngraph matching, we readily obtain its k-nearest neighbors\nin the gallery for each query. In the third occluded feature\nrecovery phase, as shown in Fig. 1(d), different from other\nmethods [14]‚Äì[16] which use GANs to predict the occluded\nparts, we propose a Feature Recovery Transformer (FRT) to\nexploit the pedestrian information in its k-nearest neighbors\nfeatures for occluded feature recovery. FRT considers the\nlocal information of each semantic feature in the k-nearest\nneighbors, including position, visibility score and similarity\nbetween the query. By this way, FRT is able to Ô¨Ålter out\nnoise in the k-nearest neighbors features and exploit valuable\ninformation to recover the occluded query feature. Finally, the\nrecovered query feature is used for person Re-ID. Extensive\nexperiments validate the consistent superiority of our FRT over\nprior state-of-the-art methods. SpeciÔ¨Åcally, FRT outperforms\nstate-of-the-art results by at least 6.2% Rank-1 accuracy and\n7.2% mAP scores on the challenging Occluded-Duke dataset\n[10].\nThe main contributions of this paper are summarized as\nfollows:\n‚Ä¢ We propose a Feature Recovery Transformer (FRT) to\nexploit the pedestrian information in the features of k-\nnearest neighbors for occluded feature recovery. Com-\npared to other methods [14]‚Äì[16] which use GANs to\npredict the occluded parts, our approach is more con-\nvincing and could bring much more improvements to the\noccluded person Re-ID performance.\n‚Ä¢ We propose a novel visibility graph to learn the human-\ntopology information among body parts, which is able\nto promote the information in the shared regions and\nsuppress the noisy message in the occluded parts.\n‚Ä¢ Extensive experiments on occluded, partial and holistic\nRe-ID datasets validate the effectiveness of our method\nfor solving occluded person Re-ID problem.\nII. R ELATED WORK\nOccluded and Partial Person Re-identiÔ¨Åcation. Occluded\n[10] and partial person re-identiÔ¨Åcation [17] aim to Ô¨Ånd the\nsame person, who is occluded or partially detected in the\nquery image, from dis-joint cameras. These two problems\nare usually studied as the same issue in research. There are\ntwo main challenges for the occluded person Re-ID problem.\nFirstly, occlusions always bring noise during feature extraction\nand feature matching. Secondly, the pedestrian information\nin the occluded regions is always lost, making the extracted\nfeatures not discriminative anymore. Recently, some methods\nhave been proposed for the Ô¨Årst challenge [10]‚Äì[13], [18]‚Äì\n[21]. Miao et al. [10] propose a feature alignment method\nbased on the semantic key-points. In addition, they design\na matching strategy to calculate the distance of represen-\ntations in an unoccluded region. He et al. [18] propose a\nreconstruction method for soft feature alignment and further\nintroduce foreground-background mask to avoid the inÔ¨Çuence\nof backgrounds in [11]. Sun et al. [12] propose a Visibility-\naware Part Model (VPM) to learn to perceive the visibility\nof regions through self-supervision. Wang et al. [13] uti-\nlize GCN, which considers different key-points as nodes,\nto embed the high-order information between various body\nparts. Zheng et al. [19] propose a Guided Feature Learning\nwith Knowledge Distillation (PGFL-KD) network to learn\naligned representations of different body parts. BeneÔ¨Åting from\nthe knowledge distillation and interaction-based learning, the\npose estimator could be discarded in testing. Chen et al.\n[20] propose an Occlusion Aware Mask Network (OAMN),\nwhich incorporates an attention-guided mask module to extract\nfeatures of body parts precisely regardless of the occlusion.\nYang et al. [21] propose to discretize pose information to the\nvisibility label of body parts for reducing the interference of\nnoisy pose information in the occluded Re-ID problem. Zhang\net al. [22] and Jia et al. [23] attempt to extract semantically\naligned features and eliminate occlusion noises for solving\nthe occluded Re-ID problem. Tan et al. [24] propose a Multi-\nHead Self-Attention Network (MHSA-Net) to prune noise and\ncapture key local information from images for occluded Re-\nID. Despite the promising results achieved in occlude Re-ID,\nall these methods ignore the lost pedestrian information in the\noccluded regions. To solve this problem, [14]‚Äì[16] attempt to\npredict the occluded parts in the images with GANs for the\nocclusion recovery. However, occluded regions generation is\nnot so convincing, especially when the occlusion is serious.\nTherefore, the performance gains of these methods [14]‚Äì[16]\nare very limited. Although Hou et al. [25] propose a Spatio-\nTemporal Completion network (STCnet) for recovering the\nappearance of the occluded parts with spatial and temporal\ninformation for video person reid, temporal information is\nunavailable in image-based occluded Re-ID. Different from\nprevious methods, our method attempts to Ô¨Ålter out noise in the\nk-nearest neighbors and fuse the query feature with valuable\ninformation in the k-nearest neighbors for occluded feature\nrecovery.\nTransformer. Vaswani et al. [26] propose the Transformer\nto dispense the recurrence and convolutions involved in the\nencoding step entirely. Transformer only relies on attention\nmechanisms to capture the global relations between input and\noutput for transduction problems such as machine translation\nand language modeling [27]‚Äì[29]. Some methods have tried\nto exploit Transformer in computer vision tasks such as image\nprocessing [30], object detection [31] , semantic segmentation\n[32], feature matching [33], etc. For example, Chen et al.\n[30] propose Image Processing Transformer (IPT) for utilizing\nlarge-scale pre-training and achieves the state-of-the-art per-\nformance on several image processing tasks like denoising,\nde-raining and super-resolution. Sarlin et al. [33] incorporate\nTransformer to establish pointwise correspondences between\na pair of images for feature matching. They utilize self-\n(intra-image) and cross- (inter-image) attention to simulate\nthe procedure that humans look back-and-forth at two images\nwhen matching them. Recently, Li et al. [34] try to solve the\noccluded Re-ID problem by the transformer encoder-decoder\narchitecture and propose a Part-Aware Transformer (PAT). PAT\nworks on precisely extracting features of visible body parts by\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nGlobal\nGlobal\nùëì0\nùëû\nùëì0\nùëî\nTorso\nTorso\nùëì2\nùëû\nùëì2\nùëî\nHead\nHead\nùëì1\nùëû\nùëì1\nùëî\nLeg\nLeg\nùëì3\nùëû\nùëì3\nùëî\nGlobal\nTorso\nHead\nLeg\nGlobal\nTorso\nHead\nLeg\nGlobal\nTorso\nHead\nLeg\nGlobal\nTorso\nHead\nLeg\nSimilarity\nScore\nRank \nList\n‚Ä¶\nK-nearest Neighbors\nFeature Recovery Transformer\nQuery\nRecovered\nQuery Feature Retrieve\n‚ë† Semantic Feature Extractor ‚ë¢ Feature Recovery Transformer‚ë°Visibility Graph Matching\nùë£0\nùë£1\nùë£2\nùë£3\nùë£0\nùë£1\nùë£2\nùë£3\nùëìùëû ùëìùëû\nùëìùëû\nùëìùëî ùëìùëî\nQuery\nGallery\nLE in Eq.(4) LG in Eq.(10)\nLT in Eq.(16)\nFig. 2. Overview of the proposed framework. It consists of semantic feature extractor E, visibility graph matching Gand feature recovery transformer T. In\nEwe utilize key-points to extract semantic features and calculate the visibility score {vi}3\ni=0 for each part. In G, we input the features extracted by Eand\nconsider the same semantic features within a pair of images as nodes of a graph to calculate the similarity scores. According to the feature matching by G, a\nrank list is produced for each query. In T, we input the query feature and features of its k-nearest neighbors for recovering the occluded query feature. The\nrecovered query feature is then utilized for retrieving.\na pixel context based transformer encoder and a part prototype\nbased transformer decoder. Different from [34], we utilize\nTransformer to exploit the pedestrian information in the k-\nnearest neighbors for recovering the occluded query features.\nGraph Convolutional Network. Graph Convolutional Net-\nworks (GCN) is Ô¨Årstly proposed in [35] to build the rela-\ntionship between graph nodes, and has been proved to be\neffective in many computer vision tasks [36]‚Äì[38]. Recently,\nRe-ID methods combined with GCN have also been explored\n[13], [39], [40]. Wang et al. [13] construct the graph based\non the visibility of key-points intra image, and take advantage\nof the afÔ¨Ånities between various key-points. Cheng et al. [39]\nformulate the structured distance into the graph Laplacian form\nto consider the relationships among training samples. Yan et\nal. [40] attempt to solve the person search by considering the\ncontext information with GCN.\nIII. O UR APPROACH\nThis section introduces our proposed framework, including\n1) Semantic feature extractor ( E) to extract the semantic\nfeatures with pose assistance and calculate visibility scores\nfor them; 2) Visibility graph matching ( G) to promote the\ninformation in the shared regions and learn the similarity;\n3) Feature recovery transformer ( T) to recover the occluded\nquery features with pedestrian information in the features of\nk-nearest neighbors. An overview of the proposed method is\nshown in Fig. 2.\nA. Semantic Feature Extractor\nThe semantic feature extractor (E) is demonstrated in Fig. 2.\nThe module E is inspired by two cues. Firstly, part-based\nmodels have been proved to be effective for person re-\nidentiÔ¨Åcation task as they can employ both global and Ô¨Åne-\ngrained local features [5]. Secondly, occlusions always cause\nspatial misalignment during feature matching. Therefore, accu-\nrate feature alignment is necessary for occluded Re-ID [11],\n[12], [18]. Following the ideas above, we use HR-Net [41]\npre-trained on the COCO dataset [42] for pose estimation.\nThe model predicts 12 key-points, including shoulders, elbows,\nwrists, hips, knees and ankles. We exploit the pose information\nto divide the person image into three parts: head part, torso\npart and leg part. Then, the local features of these three parts\ntogether with the global feature are extracted for alignment.\nAdditionally, we calculate the visibility score for each part as\nfollows:\nvi =\n‚àë\nsn‚ààRi sn\n|Ri| ,i = 0,1,2,3, (1)\nwhere v0,v1,v2,v3 are the visibility scores for the global,\nhead, torso and leg regions respectively. {Ri}3\ni=0 is the set\nof key-points in the region i and |¬∑|denotes the number of\nelements in the set. sn is the conÔ¨Ådence score calculated by\nthe pose estimator for the n-th key-point. The visibility score\nis calculated by the average conÔ¨Ådence scores for all the key-\npoints in the corresponding region. For example, we Ô¨Årstly use\npose estimator to detect 6 key-points in the torso region and\ncalculate their conÔ¨Ådence scores by the pose estimator. Then\nthe visibility score of the torso region is calculated by the\naverage conÔ¨Ådence scores for these 6 key-points. Furthermore,\nwe set a threshold Œ¥ to determine whether the region Ri is\ncompletely occluded. If the visibility score vi is smaller than\nthe Œ¥, we would regard the region Ri as fully occluded and\nset the feature of region Ri to zero.\nTraining Loss. To train the module E, we use cross-entropy\nloss LE\ncross and triplet LE\ntri for all the semantic features (i.e.\nglobal, head, torso and leg feature) as follows:\nLE\ncross = ‚àí\nN‚àë\nj=1\nlog\nexp(WE\nyj fj + bE\nyj )\n‚àëC\nk=1 exp(WE\nk fj + bE\nk)\n, (2)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nLE\ntri = |Œ∏E+ dfq\nj ,fP\nj\n‚àídfq\nj ,fN\nj\n|+, (3)\nLE= LE\ncross + LE\ntri, (4)\nwhere N is the number of images in a mini-batch, C is the\nnumber of classes and yj is the label for the feature fj. Wk\nand bk are the weights and bias of classiÔ¨Åer for the k-th class,\nrespectively. dfq\nj ,fP\nj\nand dfq\nj ,fN\nj\nare the distance between a\npositive pair (fq\nj ,fP\nj ) from the same identity and a negative\npair (fq\nj ,fN\nj ) from different identities, respectively. Œ∏E is a\nhyper-parameter to control the margin between the negative\nand positive pairs in feature space. Especially, we only utilize\nthe LEto monitor the feature of the part that is not completely\noccluded. The reason for this is that monitoring the feature\nof the completely occluded regions, which is manually set to\nzero, would confuse the classiÔ¨Åers.\nB. Visibility Graph Matching\nAlthough we have obtained the aligned pedestrian repre-\nsentations, occluded Re-ID is still challenging due to the\ninterference of the occlusions during feature matching. Thus, it\nis necessary to suppress the meaningless message of occluded\nparts and enhance the meaningful features of shared regions.\nWe resort to the graph convolutional network (GCN) [43]\nwhich is effective in message propagation and aggregation. As\nshown in Fig. 2, given two images q and g, their feature maps\n{fq\ni }3\ni=0and {fg\ni }3\ni=0 along with their corresponding visibility\nscores {vq\ni }3\ni=0 and {vg\ni }3\ni=0 could be extracted and calculated\nby the module Eabove. We aim to construct a graph which\ncould focus on the shared regions when matching features.\nVisibility Graph Building. In particular, considering a\ngraph G= {V,E}, which consists of 4 vertices Vand a set of\nedges Eas shown in Fig. 2, we assign corresponding features\n{fq\ni ,fg\ni }3\ni=0 to each node. We use A ‚ààR4√ó4 to denote the\nadjacent matrix. The adjacent matrix A is set as follows:\nAi,j =\n{\n1, i = j\nœïi + 1 (œïi ‚àí Œì)[1 ‚àí cosine(fq\ni , fg\ni )], otherwise (5)\nœïi = min{vq\ni ,vg\ni }, (6)\nwhere Ai,j indicates the information propagated from node i\nto node j, œïi denotes the shared visibility of i th part, Œì is\na margin and 1 (¬∑) is the indicator function. A high valued œïi\ndenotes that part i is a shared region while a small valued\nœïi means that at least one of i th parts of the image q and\ng is occluded. The Ô¨Årst term œïi in Eq. 5 indicates that the\nlower the shared visibility œïi is, the less information of node\ni is spread. The second term 1 (œïi ‚àíŒì)[1 ‚àícosine(fq\ni ,fg\ni )]\nindicates that when the shared visibility œïi is larger than Œì,\nthe greater the difference between fq\ni and fg\ni , the more their\nmessage will be propagated. This is designed to enhance the\ncomparisons of the shared regions.\nMessage Propagation. Following GCN [33], we denote\nm(l)\ni the message aggregated from all nodes to the i th node\nat layer l, which can be deÔ¨Åned as:\nm(l)\ni = œÉ( ÀÜAf(l)\ni W(l)\nm ), (7)\nPosition \n+ \nSimilarity score \n+ \nVisibility score\nEmbedding\nTransformer Layer √óMulti Step\n0 1 2 3 0 1 2\nQuery Feature K-Nearest Neighbors Features\nLegTorsoHeadGlobal TorsoHeadGlobal\nLeg\nTorso\nHead\nGlobal\nRecovered \nQuery  Feature\n‚Ä¶\n‚Ä¶\nùêøùë°ùëüùëñ\nùêøùëêùëüùëúùë†ùë†ùê∏\nùêπùëû ùêπùëò\n‚Ä¶\nSemantic \nFeature Extractor\nSemantic \nFeature Extractor\nQuery K-Nearest Neighbors\nFig. 3. Illustration of the proposed feature recovery transformer ( T). Fq, Fk\nare query feature and k-nearest neighbors features respectively.\nwhere ÀÜA is the normalized adjacency matrix, W(l)\nm is a\nlearnable parameter matrix, f(l)\ni represents an element from\n{fq(l)\ni ,fg(l)\ni }and œÉ is the ReLU activation function. Further-\nmore, we then use the residual message passing to update all\nthe nodes by:\nf(l+1)\ni = f(l)\ni + W(l)\nr [f(l)\ni ,m(l)\ni ], (8)\nwhere [¬∑,¬∑] denote concatenation and W(l)\nr is a learnable\nparameter matrix.\nFeature Matching. After message propagation, we obtain\nthe updated features fÀúq and fÀúg. Then, the cosine distance is\nused to calculate the similarity score as follows:\nsq,g = cosine(fÀúq,fÀúg). (9)\nAccording to the feature matching by G, a rank list is produced\nfor each query.\nTraining Loss. We use triplet and classiÔ¨Åcation losses to\nmonitor module Gas in Eq. 10. The deÔ¨Ånition of LG\ncross and\nLG\ntri is the same as Eq. 2 and Eq. 3 respectively.\nLG= LG\ncross + LG\ntri. (10)\nC. Feature Recovery Transformer\nEven so, learned features still suffer from loss of pedestrian\ninformation caused by the occlusions. To solve this problem,\nwe propose feature recovery transformer ( T) for recovering\nthe occluded features. In the gallery, there possesses lots of\npedestrian information, which hides the cues about complete\nfeatures recovery. Inspired by the success of Transformer\n[26] and local feature matching [33], we want to employ\nthe attention mechanism of Transformer to aggregate the\npedestrian information in the k-nearest neighbors features for\nrecovering the occluded query feature.\nAlthough both the Feature Recovery Transformer and re-\nranking strategy [44] utilize the k-nearest neighbors informa-\ntion, we emphasize that the motivation and implementation\nare different. Re-ranking [44] works on re-calculating the\ndistance in the k-nearest neighbors to re-rank the retrieval\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nresults. Feature Recovery Transformer focuses on recovering\nthe occluded query feature by Ô¨Åltering out noise in the k-\nnearest neighbors and fusing the query feature with valuable\ninformation in the k-nearest neighbors features.\nLocal Information Embedding. The feature recovery\ntransformer is illustrated in Fig. 3. We input the concatenation\nof query feature and its k-nearest neighbors features to the T.\nFor each part feature f, we embed its position, similarity score\nbetween the query and visibility score into a high-dimensional\nvector with a Multilayer Perceptron (MLP) as:\nf = f + MLP(p,cos,v ), (11)\nwhere p is the position for f, p = 0 ,1,2,3 stand for the\nglobal, head, torso and leg part respectively, cos is the cosine\ndistance between the pedestrian feature and query feature and\nv is the visibility score for f. This embedding enables T to\nconsider the local information of each part feature during the\nquery feature recovery.\nTransformer Layer. The transformer layer works on ag-\ngregating the information from k-nearest neighbors features\nto recover query feature. Inspired by Transformer [26], the\nkey, query and value can be calculated by:\nq(l) = W(l)\n1 fq(l) + b(l)\n1 ,\n[k\ns\n](l)\n=\n[W2\nW3\n](l)\nfk +\n[b2\nb3\n](l)\n,\n(12)\nwhere, q,k,s,f q,fk indicate query, key, value, query feature\nand k-nearest neighbors features respectively and lis the layer\nnumber. Each layer l has its own learnable projection matrix,\nshared for all part features fq and fk. Then, the message\npropagated to fq is computed as:\nmft(l)‚Üífq(l) =\n‚àë\nfk‚ààFk\nSoftmax(q(l)‚ä§k(l))s(l). (13)\nThe Ô¨Ånal projection is a linear projection:\nfq = Wfinal fq(L) + b, (14)\nwhere fq is the recovered query representation for better\nretrieval. It is worth noting that, the k-nearest neighbors\nfeatures are not updated during the whole process. Restricted\nby pages, we do not show more details of Transformer, please\nrefer to the paper [26], [33]. In addition, we employ multi-step\nmechanism as follows:\nfq(s) = TsTs‚àí1...T0(fq(0),fq(0)), (15)\nwhere s denotes conduct T for s times. The recovered query\nfeature fq(s) is then utilized for retrieval.\nTraining Loss. The T only recover the query features\nand would leave the k-nearest neighbors features unchanged.\nTherefore, we use the classiÔ¨Åer LE\ncross designed for Eto train\nthe T, and freeze LE\ncross during the whole training process\nof T to ensure that the gallery features and recovered query\nfeatures are in the same feature space. The training loss of the\nmodule T can be deÔ¨Åned as:\nLT = LE\ncross + LT\ntri, (16)\nwhere LE\ncross is the classiÔ¨Åer designed for the module Eand\nthe deÔ¨Ånition of LT\ntri is the same as Eq. 3.\nIV. E XPERIMENTS\nA. Setup\nDatasets. We evaluate our method on the following six\ndatasets and compare it with the state-of-the-art methods,\nthe details of the datasets are illustrated in Table I. 1) The\nOccluded-Duke dataset [10] is derived from DukeMTMC-\nreID [45] by Ô¨Åltering out some overlap pictures and leaving\noccluded images. It consists of 15,618 training images, 2,210\noccluded query images and 17,661 images in gallery. 2)\nThe Occluded-ReID dataset [8] contains 1000 occluded query\nimages and 1000 full-body gallery pictures. 3) The Partial-\nREID dataset [46] includes 600 images from 60 people in\ntest set, with Ô¨Åve full-body images and Ô¨Åve partial images\nfor each person. 4) Partial-iLIDS dataset [18] is selected from\niLIDS [47]. It contains 238 occluded images from 119 people\ncaptured in the airport. SpeciÔ¨Åcally, the Occluded-ReID [8],\nPartial-REID [46] and Partial-iLIDS dataset [18] only contain\ntest sets, following [11], [13], the Market-1501 [48] is used for\ntraining. 5) The Market-1501 [48] dataset consists of 32,688\nimages of 1,501 subjects captured by six cameras, and only\nfew of occluded or partial person images are included. 6)\nDukeMTMC-reID [45] is a holistic dataset which contains\n1,404 identities, 16,522 training images, 2,228 queries, and\n17,661 gallery images.\nTABLE I\nDATASETS DETAILS . WE EVALUATE OUR METHOD ON 6 PUBLIC\nDATASETS , INCLUDING 2 OCCLUDED , 2 PARTIAL AND 2 HOLISTIC ONES .\nOCCLUDED -REID, PARTIAL -REID AND PARTIAL -ILIDS DATASETS\nADOPT CROSS -DOMAIN SETTING .\nDataset Nums (ID/Image)\nTraining Query Gallery\nOccluded-Duke [10] 702/15,618 519/2210 1,110/17,661\nOccluded-ReID [8] - 200/1,000 200/1,000\nPartial-REID [46] - 60/300 60/300\nPartial-iLIDS [18] - 119/119 119/119\nMarket-1501 [48] 751/12,936 750/3,368 750/19,732\nDukeMTMC-reID [45] 702/16,522 702/2,228 1,110/17,661\nTraining Details. Our baseline is built based on the open-\nsource project ‚Äùfastreid‚Äù [56]. We resize all the training images\ninto 384 √ó128. We set the number of feature channels c to\n512 and batch size N to 64. Following the work of [57], the\nglobal average pooling (GAP) and fully connected layers are\nremoved from the original ResNet-50 [58] architecture and the\nstride of the last convolution layer is set to 1. The parameter\ns in Eq. 15 equals 3 and we input the 5-nearest neighbors\nfeatures to the T. We exploit one GCN layer in our method.\nEvaluation Metrics. We utilize mean average precision\n(mAP) and Cumulative Matching Characteristic (CMC) curves\nto evaluate the performance of various Re-ID models. All the\nexperiments are conducted in a single query setting.\nB. Comparison with State-of-the-art Methods\nResults on the Occluded Datasets. In Table II, we compare\nour method with the state-of-the-art Re-ID methods on the two\noccluded datasets, i.e., Occluded-Duke [10] and Occluded-\nReID [8]. Four kinds of methods are compared, they are\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nTABLE II\nPERFORMANCE (%) COMPARISONS WITH THE STATE -OF-THE -ART METHODS ON THE TWO OCCLUDED DATASETS , i.e.,OCCLUDED -DUKE [10] AND\nOCCLUDED -REID [8]. O UR METHOD ACHIEVES THE BEST PERFORMANCE ON THE OCCLUDED -DUKE DATASET . THE BEST PERFORMANCE IS\nHIGHLIGHTED IN BOLD .\nMethods Reference Occluded-Duke Occluded-REID Average\nRank-1 mAP Rank-1 mAP Rank-1 mAP\nPart-Aligned [6] ICCV 2017 28.8 20.2 - - - -\nPCB [5] ECCV 2018 42.6 33.7 41.3 38.9 42.0 36.3\nPart Bilinear [49] ECCV 2018 36.9 - - - - -\nFD-GAN [50] NIPS 2018 40.8 - - - - -\nDSR [18] CVPR 2018 40.8 30.4 72.8 62.8 56.8 46.6\nAd-Occluded [51] CVPR 2018 44.5 32.2 - - - -\nPGFA [10] ICCV 2019 51.4 37.3 - - - -\nHOReID [13] CVPR 2020 55.1 43.8 80.3 70.2 67.7 57.0\nPVPM [52] CVPR 2020 47.0 37.7 70.4 61.2 58.7 49.5\nPirt [53] ACM MM 2021 60.0 50.9 - - - -\nPGFA-KD [54] ACM MM 2021 63.0 54.1 80.7 70.3 71.9 62.2\nYang et al. [21] ICCV 2021 62.2 46.3 81.0 71.0 71.6 58.7\nOAMN [20] ICCV 2021 62.6 46.1 - - - -\nPAT [34] CVPR 2021 64.5 53.6 81.6 72.1 73.1 62.9\nFRT (ours) 70.7 61.3 80.4 71.0 75.6 66.2\nTABLE III\nPERFORMANCE (%) COMPARISONS WITH THE STATE -OF-THE -ART METHODS ON THE TWO PARTIAL DATASETS , i.e.,PARTIAL -REID [46] AND\nPARTIAL -ILIDS [18]. O UR METHOD ACHIEVES THE BEST PERFORMANCE ON THE PARTIAL -REID [46]. T HE BEST PERFORMANCE IS HIGHLIGHTED IN\nBOLD .\nMethods Reference Partial-REID Partial-iLIDS Average\nRank-1 Rank-3 Rank-1 Rank-3 Rank-1 Rank-3\nDSR [18] CVPR 2018 58.8 67.2 50.7 70.0 54.8 68.6\nAFPB [8] ICME 2018 78.5 - - - - -\nFPR [11] ICCV 2019 68.1 - 81.0 - 74.6 -\nPGFA [10] ICCV 2019 69.1 80.9 68.0 80.0 68.6 80.5\nVPM [12] CVPR 2019 65.5 74.8 67.7 81.9 66.6 78.4\nSTNReID [55] TMM 2020 66.7 80.3 54.6 71.3 60.7 75.8\nPVPM [52] CVPR 2020 78.3 87.7 - - - -\nHOReID [13] CVPR 2020 85.3 91.0 72.6 86.4 79.0 88.7\nPGFA-KD [54] ACM MM 2021 85.1 90.8 74.0 86.7 80.0 88.8\nOAMN [20] ICCV 2021 86.0 - 77.3 - 81.7 -\nPAT [34] CVPR 2021 88.0 92.3 76.5 88.2 82.2 90.3\nFRT (ours) 88.2 93.2 73.0 87.0 80.6 90.1\nholistic methods [5], [6], key-points based methods [49], [50],\npartial Re-ID methods [18] and occluded Re-ID methods\n[10], [13], [20], [21], [34], [51]‚Äì[54]. The result shows that\nFRT outperforms other methods on Occluded-Duke dataset\n[10], which demonstrates the effectiveness of our FRT in\ndealing with the occluded Re-ID problem. SpeciÔ¨Åcally, on the\nOccluded-Duke [10] dataset, FRT achieves the best result with\nRank-1 accuracy of 70.7% and mAP of 61.3%, which is at\nleast 6.2% and 7.7% higher than the corresponding metrics\nof other methods. On the Occluded-REID dataset [8], FRT\nachieves the competitive results to PAT [34] with80.4% Rank-\n1 accuracy and 71.0% mAP.\nResults on the Partial Datasets. In Table III, we compare\nour method with the state-of-the-art Re-ID methods on the two\npartial datasets, i.e.,Partial-REID [46] and Partial-iLIDS [18].\nAccompanied by occluded images, partial ones often occur due\nto outliers of camera views, imperfect detection, and so on. As\nwe can see, our method achieves the best results on the Partial-\nREID dataset [46], which outperforms other methods by at\nleast 0.2% Rank-1 accuracy and 0.9% Rank-3 accuracy. FRT\nalso achieves competitive results on the Partial-iLIDS dataset\n[18]. We think there are three reasons for the less performance\ngains on the three small-scale occluded and partial datasets,\ni.e., Occluded-ReID [8], Partial-REID [46] and Partial-iLIDS\n[18] than on the Occluded-Duke [10]: 1) The three small-\nscale occluded and partial datasets i.e. Occluded-ReID [8],\nPartial-REID [46] and Partial-iLIDS [18] adopt cross-domain\nsetting, which utilize Market-1501 as the training set and\ntest on the other domains. Domain bias in the cross-domain\nevaluation would have a negative impact on the performance\nof our model. 2) The gallery of the Occluded-Duke [10],\nOccluded-ReID [8], Partial-REID [46] and Partial-iLIDS [18]\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nTABLE IV\nPERFORMANCE (%) COMPARISONS WITH THE STATE -OF-THE -ART RE-ID METHODS ON HOLISTIC DATASETS i.e.MARKET -1501 [48] AND\nDUKE MTMC- REID [45]. O UR METHOD ACHIEVES BEST PERFORMANCE ON HOLISTIC RE-ID. T HE BEST PERFORMANCE IS HIGHLIGHTED IN BOLD .\nMethods Reference Market-1501 DukeMTMC-reID Average\nRank-1 mAP Rank-1 mAP Rank-1 mAP\nHolistic Methods PCB [5] ECCV 2018 92.3 77.4 81.8 66.1 87.1 71.8\nBOT [59] ICCV 2019 94.1 85.7 86.4 76.4 90.3 81.1\nKey-points Based Methods Part Bilinear [49] ECCV 2018 90.2 76.0 82.1 64.2 86.2 70.1\nFD-GAN [50] NIPS 2018 90.5 77.7 80.0 64.5 85.3 71.1\nPartial Re-ID Methods DSR [18] CVPR 2018 83.5 64.2 - - - -\nOccluded Re-ID Methods\nAd-Occluded [51] CVPR 2018 84.4 66.9 79.1 62.1 81.8 64.5\nFPR [11] ICCV 2019 95.4 86.5 88.6 76.4 92.0 81.5\nPGFA [10] ICCV 2019 91.2 76.8 82.6 65.5 86.9 71.2\nHOReID [13] CVPR 2020 91.0 85.3 86.4 72.6 88.7 79.0\nPirt [53] ACM MM 2021 94.1 86.3 88.9 77.6 91.5 82.0\nPGFA-KD [54] ACM MM 2021 95.3 87.2 89.6 79.5 92.5 83.4\nOAMN [20] ICCV 2021 93.2 79.8 86.3 72.6 89.8 76.2\nPAT [34] CVPR 2021 95.4 88.0 88.8 78.2 92.1 83.1\nFRT (ours) 95.5 88.1 90.5 81.7 93.0 84.9\ncontain 16,5,5,1 images for each identity respectively. There-\nfore, on Occluded-Duke [10], the feature recovery transformer\nis able to employ more pedestrian information in the gallery,\nresulting in better recovered query features and bigger Re-\nID performance improvements on Occluded-Duke than other\nthree small-scale occluded and partial datasets. 3) Occluded-\nDuke [10] is the largest dataset for studying the occluded Re-\nID problem, and the results on the Occluded-Duke are more\nreliable for occluded Re-ID performance evaluation.\nResults on Holistic Datasets. Although recent oc-\ncluded/partial person Re-ID methods have made progress\non occluded/partial datasets, their performances are always\nunsatisfying on the holistic datasets. In this part, we show\nthat our method can also achieve comparable state-of-the-art\nperformances on the holistic datasets Market-1501 [48] and\nDukeMTMC-reID [45]. The results are shown in Table IV. We\ncompare the proposed FRT with two holistic Re-ID methods\n[5], [59], two key-points based methods [49], [50], one partial\nRe-ID method [18] and eight occluded Re-ID methods [10],\n[11], [13], [20], [34], [51], [53], [54]. The result shows that\non the Market-1501 [48] our proposed FRT achieves the best\nresults with 95.5% Rank-1 accuracy and 88.1% mAP and on\nthe DukeMTMC-reID [45] FRT achieves the best results with\n90.5% Rank-1 accuracy and 81.7% mAP, which outperforms\nother methods by at least 0.9% and 2.2% respectively.\nC. Comparison with Post-Processing Techniques\nAs the visibility graph and feature recovery transformer\nwork in the feature matching stage, we additionally compare\nFRT with other state-of-the-art post-processing techniques, i.e.\nre-ranking [44] and average query expansion (AQE) [60] on\nOccluded-Duke dataset. The result is shown in Table V. From\nthe result we can see that FRT has the highest Rank-1 accuracy\nof 70.7%. We think the main reason for the higher Rank-1\naccuracy of FRT is thatTis able to Ô¨Ålter out the noisy message\nin the k-nearest neighbors, and employ valuable information\ninstead of weighted sum of all the features to recover the\nquery features. However, re-ranking achieves the highest mAP\nof 63.7%, which is 2.4% higher than the FRT. We think the\nreason for this is that re-ranking attempts to calculate the k-\nnearest neighbors for all the candidates in the rank list and\nrecalculate the Jaccard distance for re-ranking, which is better\nfor the mean accuracy. In addition, the result in the last row\nindicates that FRT and re-ranking are not conÔ¨Çicting, they\ncan be integrated for better performance. We visualize the\ncomparison of FRT, average query expansion (AQE) [60] and\nre-ranking [44] in Fig. 4.\nD. Further Analysis\nAblation Study of Proposed Modules. In this part, we\nanalyze our proposed semantic feature extractor ( E), visibility\ngraph matching ( G) and feature recovery transformer ( T) on\nOccluded-Duke dataset. The results are shown in Table VI.\nFirstly, in index-1, we can see that thanks to the feature\nalignment by extracting semantic features with key-points, E\nis able to achieve 56.5% Rank-1 accuracy and 48.6% mAP\non the Occluded-Duke dataset. Secondly, In index 2, afÔ¨Ånities\namong different body parts are considered and the information\nin the shared regions is promoted. This gives 3.4% and 3.2%\nimprovement to the Rank-1 accuracy and mAP respectively\nand demonstrates the effectiveness of G. Thirdly, in index 1\nand 3, we can see that T is able to give Rank-1 accuracy of\n13% and mAP of 11.2% improvement to the E. Finally, in\nindex 3 and 4, Ggives another 1.2% and 1.5% higher points\nto Rank-1 accuracy and mAP respectively to the T.\nAnalysis of Parameters. We evaluate the effects of parame-\nters s, kand Œ¥in Fig. 5. From Fig. 5(a), we can see that multi-\nstep mechanism is able to give about 1.5%, 1.9% higher points\nto Rank-1 accuracy and mAP respectively. The performance\nis at its best when s equals three. From Fig. 5(b) we can see\nthat the parameter k has a great impact on the performance.\nWhen we input the 5-nearest neighbors features to the feature\nrecovery transformer, FRT achieves the best result with Rank-1\naccuracy of 70.7% and mAP of 61.3%. From Fig. 5(c) we can\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nBaseline\nAverage \nQuery \nExpansion\nRe-ranking\nFeature \nRecovery \nTransformer \n(Ours)\nProbe\nImages Rank List Probe\nImages Rank List\nFig. 4. Visualization of the comparison of our proposed FRT and other state-of-the-art post-processing techniques, i.e. average query expansion [60] and\nre-ranking [44]. Green and red rectangles indicate correct and error retrieval results,respectively.\nTABLE V\nPERFORMANCE (%) COMPARISONS WITH THE STATE -OF-THE -ART\nPOST -PROCESSING TECHNIQUES ON OCCULUDED -DUKE DATASET . E\nINDICATES SEMANTIC FEATURE EXTRACTOR . OUR MODEL ACHIEVES THE\nBEST RESULTS AND COULD BE INTEGRATED WITH OTHER\nPOST -PROCESSING TECHNIQUES FOR BETTER RESULTS .\nMethods Rank-1 mAP\nPGFA [10] 51.4 37.3\nPGFA [10] + re-ranking 52.4 46.8\nHOReID [13] 55.1 43.8\nHOReID [13] + re-ranking 58.3 49.2\nPirt [53] 60.0 50.9\nPirt [53] + re-ranking 62.1 59.3\nE 56.5 48.6\nE+ AQE [60] 62.8 60.2\nE+ re-ranking [44] 64.6 63.7\nFRT (ours) 70.7 61.3\nFRT (ours) + re-ranking [44] 70.8 65.0\nÔ¨Ånd that when Œ¥equals 0.2, it is able to give about 1.5%, 1.7%\nTABLE VI\nABLATION STUDY OF THE PROPOSED MODULES ON OCCULUDED -DUKE\nDATASET. EIS THE SEMANTIC FEATURE EXTRACTOR , GIS THE VISIBILITY\nGRAPH MATCHING AND T IS THE FEATURE RECOVERY TRANSFORMER .\nTHE RESULTS VALIDATE THE EFFECTIVENESS OF THE THREE PROPOSED\nMODULES .\nIndex E G T Rank-1 mAP\n1 ‚àö √ó √ó 56.5 48.6\n2 ‚àö ‚àö √ó 59.9 51.4\n3 ‚àö √ó ‚àö 69.5 59.8\n4 ‚àö ‚àö ‚àö 70.7 61.3\nhigher points to Rank-1 accuracy and mAP respectively than\nŒ¥ equals 0. The reason is that the threshold Œ¥ can eliminate\nthe effectiveness of occlusion during the training.\nAnalysis of the Feature Recovery Transformer Time\nConsumption. We evaluate the time consumption of the\nfeature recovery transformer in Fig. 6. Fig. 6 shows the\ntime consumption on each image in inference. The result\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\n1 2 3 4 5\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\n1 2 3 4 5 6 7 8\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\nRank-1 Accuracy (%) mAP (%)\ns k\n(a) (b)\n35\n40\n45\n50\n55\n60\n65\n70\n75\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nRank-1 Accuracy (%) mAP (%)\nŒ¥\n(c)\nFig. 5. Analysis of parameters s, k and Œ¥ on the Occluded-Duke dataset.\ns indicates conduct T for s times, k indicates input the k-nearest neighbors\nfeatures to the T and Œ¥ is the threshold for the semantic feature extractor.\n0\n1\n2\n3\n4\n5\n6\n1 2 3 4 5 6 7 8\nOcclusion Recovery Transformer Re-rank\ns\nms\\image\nRe-rankingFeature Recovery Transformer\nFig. 6. Analysis of the time consumption of the feature recovery transformer.\nsindicates conduct T for stimes. The results show that our feature recovery\ntransformer has less time consumption.\nindicates that when s equals three, the time consumption\nis about 0.12 ms per image, which is nearly 1/40 of the\nRe-ranking costs. The results demonstrate that our feature\nrecovery transformer is able to achieve better performance\nwith less time consumption.\nEvaluation of the Feature Recovery Transformer. In\nTable VII, we evaluate the Re-ID performance of each part\nfeature and the Ô¨Ånal representation before and after being\nprocessed by feature recovery transformer on Occluded-Duke\ndataset. From the result we can Ô¨Ånd that before the fea-\nture recovery transformer, the leg part feature has the worst\nperformance with Rank-1 accuracy of 26.8% and mAP of\n18.9%, indicating that the lower part of most of the images\nis occluded. After being processed by the feature recovery\ntransformer, we can Ô¨Ånd that the Re-ID accuracy of the global\nfeature, head feature, torso feature and leg feature are all\nimproved. In particular, the Rank-1 accuracy and mAP of the\nTABLE VII\nEVALUATION OF THE FEATURE RECOVERY TRANSFORMER ON\nOCCLUDED -DUKE DATASET . THE RESULT DEMONSTRATES THAT\nFEATURES ARE RECOVERED AFTER BEING PROCESSED BY OUR FEATURE\nRECOVERY TRANSFORMER .\nFeature\nBefore\nFeature Recovery\nAfter\nFeature Recovery\nRank-1 mAP Rank-1 mAP\nGlobal 55.6 46.1 69.3 59.1\nHead 57.3 40.7 62.6 45.9\nTorso 49.2 35.5 67.4 49.6\nLeg 26.8 18.9 62.8 45.5\nConcat 56.5 48.6 70.7 61.3\n0\n10\n20\n30\n40\n50\n60\n70\n80\n3 5 7 9 11 13 15\nRank-1 Accuracy (%) mAP (%)\nImgs/ID\nFig. 7. Evaluation of the effects of the gallery size. The gallery size is changed\nfrom 3 images per ID to 15 images per ID.\nleg feature are improved from 26.8% to 62.8% and 18.9%\nto 45.5% respectively, indicating that the occluded leg feature\nhas been recovered by the feature recovery transformer. This\nexperiment proves that the feature recovery transformer is\nable to recover occluded features and improve the Re-ID\nperformance of both the global and local features.\nVisualization of the feature recovery transformer. We\nvisualize the recovery process of the feature recovery trans-\nformer in Fig. 8. We illustrate the top-5 nearest neighbors.\nGreen and red rectangles indicate correct and error retrieval\nresults,respectively. The numbers above the pictures indicate\nthe contribution of corresponding neighbors to the feature\nrecovery. In the Ô¨Årst example, the k-nearest neighbors contain\nan error retrieval with a similar appearance to the probe. The\nbiggest difference between the correct and error retrieval is the\nshoes. During the process the feature recovery, we Ô¨Ånd that\nthe T exploit little information from the error retrieval, and\nthus the recovered query feature is reliable for representing\nthe probe identity. In the second example, the probe image is\noccluded by a car, which leads to the two error retrievals in the\nk-nearest neighbors. Fortunately, the T is able to distinguish\nthe noise and exploit the message from the other 3 correct\nretrievals for the recovery of the occluded feature in the probe.\nThe result shows that the feature recovery transformer is able\nto Ô¨Ålter out the noisy information, even for some hard cases,\nin the k-nearest neighbors, and then exploit the valid message\nfor feature recovery.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nK-nearest Neighbors\n0.30 0.26 0.02 0.10 0.32\n0.23 0.22 0.26 0.05 0.24\nK-nearest Neighbors\nTop-1 Top-2 Top-3 Top-4 Top-5\nTop-1 Top-2 Top-3 Top-4 Top-5\nFig. 8. Visualization of the contributions of k-nearest neighbors when\nconducting the feature recovery transformer. We illustrate the top-5 nearest\nneighbors. Green and red rectangles indicate correct and error retrieval\nresults,respectively. The numbers above the pictures indicate the contribution\nof corresponding neighbors to the feature recovery. The result shows that the\nfeature recovery transformer is able to Ô¨Ålter out the noise in the k-nearest\nneighbors and exploit valid information for feature recovery.\nEvaluation of the Effects of the Gallery Size. Feature\nrecovery transformer utilizes the pedestrian information in\nthe gallery for features recovery. Therefor, we attempt to\nevaluate the effects of the gallery size on the feature recovery\ntransformer. We change the gallery size from 3 images per\nidentity to 15 images per identity, and the result is shown\nin Fig. 7. From the result we can see that the performance\nof the FRT improves with the increase of the gallery size.\nSpeciÔ¨Åcally, the Rank-1 accuracy and mAP are improved\nfrom 45.2%, 47.1% to 70.7%, 61.3% respectively when the\ngallery size increases from 3 images per ID to 15 images\nper ID. The reason is that when the gallery size increases, the\nfeature recovery transformer is able to employ more pedestrian\ninformation in the gallery for features recovery.\nV. C ONCLUSION\nIn this paper, we propose a novel framework called Feature\nRecovery Transformer (FRT) for the occluded person re-\nidentiÔ¨Åcation. Firstly, we employ key-points to extract seman-\ntic features for alignment and calculate the visibility scores\nfor them. Then, we consider the semantic features from\nsame part within a pair of images as nodes to construct a\ndirectional graph. We set the edge based on the visibility\nscore of starting nodes for promoting the propagation of\ninformation in the shared regions. In terms of the occluded\nfeature recovery, we propose a feature recovery transformer\nto exploit the pedestrian information in the features of its\nk-nearest neighbors. Finally, the recovered query feature is\nutilized for retrieving. Extensive experiments on occluded,\npartial and holistic datasets demonstrate that our proposed\nframework is able to recover the occluded features and achieve\nthe best Re-ID performance.\nACKNOWLEDGEMENT\nWe thank associate editor and anonymous reviewers for\nproviding valuable suggestions to improve this paper.\nREFERENCES\n[1] W.-H. Li, F.-T. Hong, and W.-S. Zheng, ‚ÄúLearning to learn relation for\nimportant people detection in still images,‚Äù in CVPR, 2019.\n[2] E. Ristani and C. Tomasi, ‚ÄúFeatures for multi-target multi-camera\ntracking and re-identiÔ¨Åcation,‚Äù in CVPR, 2018.\n[3] B. Xu, L. He, X. Liao, W. Liu, Z. Sun, and T. Mei, ‚ÄúBlack re-id:\nA head-shoulder descriptor for the challenging problem of person re-\nidentiÔ¨Åcation,‚Äù in ACMMM, 2020.\n[4] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, ‚ÄúStyle normalization\nand restitution for generalizable person re-identiÔ¨Åcation,‚Äù in CVPR,\n2020.\n[5] Y . Sun, L. Zheng, Y . Yang, Q. Tian, and S. Wang, ‚ÄúBeyond part models:\nPerson retrieval with reÔ¨Åned part pooling (and a strong convolutional\nbaseline),‚Äù in ECCV, 2018.\n[6] L. Zhao, X. Li, Y . Zhuang, and J. Wang, ‚ÄúDeeply-learned part-aligned\nrepresentations for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017.\n[7] J. Zhou, B. Su, and Y . Wu, ‚ÄúOnline joint multi-metric adaptation from\nfrequent sharing-subset mining for person re-identiÔ¨Åcation,‚Äù in CVPR,\n2020.\n[8] J. Zhuo, Z. Chen, J. Lai, and G. Wang, ‚ÄúOccluded person re-\nidentiÔ¨Åcation,‚Äù in ICME, 2018.\n[9] S. Gao, J. Wang, H. Lu, and Z. Liu, ‚ÄúPose-guided visible part matching\nfor occluded person reid,‚Äù in CVPR, 2020.\n[10] J. Miao, Y . Wu, P. Liu, Y . Ding, and Y . Yang, ‚ÄúPose-guided feature\nalignment for occluded person re-identiÔ¨Åcation,‚Äù in ICCV, 2019.\n[11] L. He, Y . Wang, W. Liu, H. Zhao, Z. Sun, and J. Feng, ‚ÄúForeground-\naware pyramid reconstruction for alignment-free occluded person re-\nidentiÔ¨Åcation,‚Äù in ICCV, 2019.\n[12] Y . Sun, Q. Xu, Y . Li, C. Zhang, Y . Li, S. Wang, and J. Sun, ‚ÄúPerceive\nwhere to focus: Learning visibility-aware part-level features for partial\nperson re-identiÔ¨Åcation,‚Äù in CVPR, 2019.\n[13] G. Wang, S. Yang, H. Liu, Z. Wang, Y . Yang, S. Wang, G. Yu, E. Zhou,\nand J. Sun, ‚ÄúHigh-order information matters: Learning relation and\ntopology for occluded person re-identiÔ¨Åcation,‚Äù in CVPR, 2020.\n[14] S. Iodice and K. Mikolajczyk, ‚ÄúPartial person re-identiÔ¨Åcation with\nalignment and hallucination,‚Äù in Asian Conference on Computer Vision,\n2018.\n[15] X. Jin, C. Lan, W. Zeng, G. Wei, and Z. Chen, ‚ÄúSemantics-aligned\nrepresentation learning for person re-identiÔ¨Åcation,‚Äù in AAAI, 2020.\n[16] Y . Zhou and L. Shao, ‚ÄúAware attentive multi-view inference for vehicle\nre-identiÔ¨Åcation,‚Äù in CVPR, 2018.\n[17] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, and S. Gong, ‚ÄúPartial\nperson re-identiÔ¨Åcation,‚Äù in ICCV, 2015.\n[18] L. He, J. Liang, H. Li, and Z. Sun, ‚ÄúDeep spatial feature reconstruction\nfor partial person re-identiÔ¨Åcation: Alignment-free approach,‚Äù in CVPR,\n2018.\n[19] K. Zheng, C. Lan, W. Zeng, J. Liu, Z. Zhang, and Z.-J. Zha, ‚ÄúPose-\nguided feature learning with knowledge distillation for occluded person\nre-identiÔ¨Åcation,‚Äù in ACM MM, 2021.\n[20] P. Chen, W. Liu, P. Dai, J. Liu, Q. Ye, M. Xu, Q. Chen, and R. Ji,\n‚ÄúOcclude them all: Occlusion-aware attention network for occluded\nperson re-id,‚Äù in ICCV, 2021.\n[21] J. Yang, J. Zhang, F. Yu, X. Jiang, M. Zhang, X. Sun, Y .-C. Chen,\nand W.-S. Zheng, ‚ÄúLearning to know where to see: A visibility-aware\napproach for occluded person re-identiÔ¨Åcation,‚Äù in ICCV, 2021.\n[22] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, ‚ÄúDensely semantically aligned\nperson re-identiÔ¨Åcation,‚Äù in CVPR, 2019.\n[23] M. Jia, X. Cheng, S. Lu, and J. Zhang, ‚ÄúLearning disentangled represen-\ntation implicitly via transformer for occluded person re-identiÔ¨Åcation,‚Äù\nIEEE Transactions on Multimedia, 2022.\n[24] H. Tan, X. Liu, B. Yin, and X. Li, ‚ÄúMhsa-net: Multihead self-attention\nnetwork for occluded person re-identiÔ¨Åcation,‚Äù IEEE Transactions on\nNeural Networks and Learning Systems, 2022.\n[25] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, ‚ÄúVrstc:\nOcclusion-free video person re-identiÔ¨Åcation,‚Äù in CVPR, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in NIPS, 2017.\n[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018.\n[28] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n‚ÄúBiobert: a pre-trained biomedical language representation model for\nbiomedical text mining,‚Äù Bioinformatics, vol. 36, no. 4, pp. 1234‚Äì1240,\n2020.\n[29] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,\nA. Kannan, R. J. Weiss, K. Rao, E. Goninaet al., ‚ÄúState-of-the-art speech\nrecognition with sequence-to-sequence models,‚Äù in ICASSP, 2018, pp.\n4774‚Äì4778.\n[30] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, ‚ÄúPre-trained image processing transformer,‚Äù arXiv\npreprint arXiv:2012.00364, 2020.\n[31] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in\nECCV, 2020.\n[32] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, ‚ÄúDual attention\nnetwork for scene segmentation,‚Äù in CVPR, 2019.\n[33] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, ‚ÄúSuperglue:\nLearning feature matching with graph neural networks,‚Äù in CVPR, 2020.\n[34] Y . Li, J. He, T. Zhang, X. Liu, Y . Zhang, and F. Wu, ‚ÄúDiverse part dis-\ncovery: Occluded person re-identiÔ¨Åcation with part-aware transformer,‚Äù\nin CVPR, 2021.\n[35] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,\n‚ÄúThe graph neural network model,‚Äù IEEE Transactions on Neural\nNetworks, vol. 20, no. 1, pp. 61‚Äì80, 2008.\n[36] J. Gao, T. Zhang, and C. Xu, ‚ÄúGraph convolutional tracking,‚Äù in CVPR,\n2019.\n[37] W. Wang, X. Lu, J. Shen, D. J. Crandall, and L. Shao, ‚ÄúZero-shot video\nobject segmentation via attentive graph neural networks,‚Äù inICCV, 2019.\n[38] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim, ‚ÄúGraph transformer\nnetworks,‚Äù in NIPS, 2019.\n[39] D. Cheng, Y . Gong, X. Chang, W. Shi, A. Hauptmann, and N. Zheng,\n‚ÄúDeep feature learning via structured graph laplacian embedding for\nperson re-identiÔ¨Åcation,‚Äù Pattern Recognition, vol. 82, pp. 94‚Äì104, 2018.\n[40] Y . Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, and X. Yang, ‚ÄúLearning\ncontext graph for person search,‚Äù in CVPR, 2019.\n[41] K. Sun, B. Xiao, D. Liu, and J. Wang, ‚ÄúDeep high-resolution represen-\ntation learning for human pose estimation,‚Äù in CVPR, 2019.\n[42] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll ¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\ncontext,‚Äù in ECCV, 2014.\n[43] F. Wu, A. H. Souza Jr, T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger,\n‚ÄúSimplifying graph convolutional networks,‚Äù in ICML, 2019.\n[44] Z. Zhong, L. Zheng, D. Cao, and S. Li, ‚ÄúRe-ranking person re-\nidentiÔ¨Åcation with k-reciprocal encoding,‚Äù in CVPR, 2017.\n[45] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, ‚ÄúPerformance\nmeasures and a data set for multi-target, multi-camera tracking,‚Äù in\nECCV, 2016.\n[46] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, and S. Gong, ‚ÄúPartial\nperson re-identiÔ¨Åcation,‚Äù in CVPR, 2015.\n[47] W.-S. Zheng, S. Gong, and T. Xiang, ‚ÄúPerson re-identiÔ¨Åcation by\nprobabilistic relative distance comparison,‚Äù in CVPR, 2011.\n[48] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, ‚ÄúScalable\nperson re-identiÔ¨Åcation: A benchmark,‚Äù in CVPR, 2015.\n[49] Y . Suh, J. Wang, S. Tang, T. Mei, and K. Mu Lee, ‚ÄúPart-aligned bilinear\nrepresentations for person re-identiÔ¨Åcation,‚Äù in ECCV, 2018.\n[50] Y . Ge, Z. Li, H. Zhao, G. Yin, S. Yi, X. Wang et al., ‚ÄúFd-gan: Pose-\nguided feature distilling gan for robust person re-identiÔ¨Åcation,‚Äù in NIPS,\n2018.\n[51] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, ‚ÄúAdversarially\noccluded samples for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018.\n[52] S. Gao, J. Wang, H. Lu, and Z. Liu, ‚ÄúPose-guided visible part matching\nfor occluded person reid,‚Äù in CVPR, 2020.\n[53] Z. Ma, Y . Zhao, and J. Li, ‚ÄúPose-guided inter-and intra-part relational\ntransformer for occluded person re-identiÔ¨Åcation,‚Äù in ACM MM, 2021.\n[54] K. Zheng, C. Lan, W. Zeng, J. Liu, Z. Zhang, and Z.-J. Zha, ‚ÄúPose-\nguided feature learning with knowledge distillation for occluded person\nre-identiÔ¨Åcation,‚Äù in ACM MM, 2021.\n[55] H. Luo, W. Jiang, X. Fan, and C. Zhang, ‚ÄúStnreid: Deep convolutional\nnetworks with pairwise spatial transformer networks for partial person\nre-identiÔ¨Åcation,‚Äù IEEE Transactions on Multimedia, vol. 22, no. 11, pp.\n2905‚Äì2913, 2020.\n[56] L. He, X. Liao, W. Liu, X. Liu, P. Cheng, and T. Mei, ‚ÄúFastreid: A\npytorch toolbox for general instance re-identiÔ¨Åcation,‚Äù arXiv, 2020.\n[57] G. Wang, Y . Yuan, X. Chen, J. Li, and X. Zhou, ‚ÄúLearning discriminative\nfeatures with multiple granularities for person re-identiÔ¨Åcation,‚Äù in\nACMMM, 2018.\n[58] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù 2016.\n[59] H. Luo, Y . Gu, X. Liao, S. Lai, and W. Jiang, ‚ÄúBag of tricks and a\nstrong baseline for deep person re-identiÔ¨Åcation,‚Äù in ICCV Workshops,\n2019.\n[60] O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman, ‚ÄúTotal recall:\nAutomatic query expansion with a generative feature model for object\nretrieval,‚Äù in ICCV, 2007.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7153940796852112
    },
    {
      "name": "Computer science",
      "score": 0.6890538930892944
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6319055557250977
    },
    {
      "name": "Feature extraction",
      "score": 0.531136691570282
    },
    {
      "name": "Transformer",
      "score": 0.5073282122612
    },
    {
      "name": "Feature matching",
      "score": 0.4768289625644684
    },
    {
      "name": "Graph",
      "score": 0.4475807845592499
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4436618685722351
    },
    {
      "name": "Computer vision",
      "score": 0.4363696575164795
    },
    {
      "name": "Feature learning",
      "score": 0.42038092017173767
    },
    {
      "name": "Theoretical computer science",
      "score": 0.09156611561775208
    },
    {
      "name": "Engineering",
      "score": 0.07161960005760193
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210103986",
      "name": "Jingdong (China)",
      "country": "CN"
    }
  ]
}