{
  "title": "Recurrent Spatial Transformer Networks",
  "url": "https://openalex.org/W2174479785",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4295178239",
      "name": "Sønderby, Søren Kaae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295178236",
      "name": "Sønderby, Casper Kaae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753449890",
      "name": "Maaløe, Lars",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2562191932",
      "name": "Winther, Ole",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1484210532",
    "https://openalex.org/W1850742715",
    "https://openalex.org/W1606347560",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2951527505"
  ],
  "abstract": "We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.",
  "full_text": "arXiv:1509.05329v1  [cs.CV]  17 Sep 2015\nRecurrent Spatial Transformer Networks\nSøren Kaae Sønderby1 SKAAESONDERBY @ GMAIL .COM\nCasper Kaae Sønderby1 CASPERKAAE @ GMAIL .COM\nLars Maaløe2 LARSMA @ DTU .DK\nOle Winther1,2 OLWI @ DTU .DK\n1 Bioinformatics Centre, Department of Biology, Universityof Copenhagen, Copenhagen, Denmark\n2 Department for Applied Mathematics and Computer Science, Technical University of Denmark, 2800 Lyngby, Denmark\nAbstract\nWe integrate the recently proposed spatial trans-\nformer network (SPN) (\nJaderberg & Simonyan,\n2015) into a recurrent neural network (RNN) to\nform an RNN-SPN model. We use the RNN-\nSPN to classify digits in cluttered MNIST se-\nquences. The proposed model achieves a single\ndigit error of 1.5% compared to 2.9% for a con-\nvolutional networks and 2.0% for convolutional\nnetworks with SPN layers. The SPN outputs a\nzoomed, rotated and skewed version of the input\nimage. We investigate different down-sampling\nfactors (ratio of pixel in input and output) for\nthe SPN and show that the RNN-SPN model is\nable to down-sample the input images without de-\nteriorating performance. The down-sampling in\nRNN-SPN can be thought of as adaptive down-\nsampling that minimizes the information loss in\nthe regions of interest. We attribute the superior\nperformance of the RNN-SPN to the fact that it\ncan attend to a sequence of regions of interest.\n1. Introduction\nAttention mechanisms have been used for machine\ntranslation (\nBahdanau et al., 2014), speech recognition\n(Chorowski et al., 2014) and image recognition (Ba et al.,\n2014; Gregor et al., 2015; Xu et al., 2015). The re-\ncently proposed spatial transformer network (SPN)\n(Jaderberg & Simonyan,2015) is a new method for incor-\nporating spatial attention in neural networks. SPN uses a\nlearned afﬁne transformation of the input and bilinear inter-\npolation to produce its output. This allows the SPN net-\nwork to zoom, rotate and skew the input. A SPN layer\ncan be used as any other layer in a feed-forward convolu-\ntional network\n1. The feed-forward SPN (FFN-SPN) is illus-\ntrated in Figure1 panel a) where a SPN combined with a\n1See e.g.https://goo.gl/1Ho1M6\nconvolutional network is used to predict a sequence of dig-\nits. Because the predictions are made withn independent\nsoftmax layers at the top of the network, the SPN layers\nmust ﬁnd the box containing the entire sequence. A better\nmodel would sequentially produce the targets by locating\nand zoom in on each individual element before each pre-\ndiction. In this paper we combine the SPN network with a\nrecurrent neural network (RNN) to create a recurrent SPN\n(RNN-SPN) that sequentially zooms in on each element.\nThis is illustrated in Figure\n1 panel b). In the SPN-RNN\na RNN network produces inputs for the SPN at each time-\nstep. Using these inputs the SPN produces a transformed\nversion of the input image which is then used for classi-\nﬁcation. By running the recursion for multiple steps and\nconditioning each transformation on the previous time-step\nthe RNN-SPN model can sequentially attend to the part of\nthe image containing each elements of interest and only use\nthe relevant information for classiﬁcation. Because these\nregions are generally small we experiment with forcing the\nRNN-SPN to down-sample the image which can thought of\nas adaptive down-sampling thus keeps the resolution of the\nregions of interest (nearly) constant.\n2. Related Work\nGregor et al. 2015introduced a differentiable attention\nmechanism based on an array of Gaussians and combined\nit with a RNN for both generative and discriminative\ntasks.\nBa et al. 2014and Sermanet et al.combined a non-\ndifferentiable attention mechanism with an RNN and used\nit for classiﬁcation. Their attention mechanism was trained\nusing reinforcement learning. Other related work include\n(\nXu et al.,2015) who applies visual attention in an encoder-\ndecoder structure.\n3. Spatial Transformer Network\nThe SPN network is implemented similarly to\n(\nJaderberg & Simonyan,2015). A SPN network takes an\nTransformer network\nFigure 1.SPN networks for predicting the sequence 723 from an image. a) A FFN-SPN network attend to the entire sequence (blue box).\nThe digits are classiﬁed by three separate softmax layers atthe top of the network. Because the network cannot zoom in on individual\ndigits in the sequence all digits are classiﬁed from the sameimage crop. b) A RNN-SPN where the transformation is predicted with an\nRNN. This allows the model to create a separate crop for each digit. Each crop (indicated with blue numbers) is then passedthrough the\nsame classiﬁcation network. The structure enables the model to zoom in on each individual digit.\nimage or a feature map from a convolutional network as\ninput. An afﬁne transformation and bilinear interpolation\nis then applied to the input to produce the output of the\nSPN. The afﬁne transformation allows zoom, rotation and\nskew of the input. The parameters of the transformation\nare predicted using a localization networkfloc:\nfloc(I) =Aθ =\n[\nθ1 θ2 θ3\nθ4 θ5 θ6\n]\n, (1)\nwhere I is the input to the SPN with shape[H × W × C]\n(height, width, channels) and matrixAθ speciﬁes the afﬁne\ntransformation. The afﬁne transformation is applied on a\nmesh gridG ∈ R h×w:\nG ={(y1, x 1), (y1, x 2), ... (y2, x 1), (y2, x 2),\n... (yw, x h−1), (yh, x w)}. (2)\nwhere w and h does not need to be equal toH and W . G\nis illustrated in Figure\n2 panel a). The grid is laid out such\nthat(y1, x 1) = (− 1, − 1) and (yh, x w) = (+1, +1) with\nthe points in-between spaced equally. The afﬁne transfor-\nmation is applied toG to produce an imageS which tells\nhow to select points fromI and map them back ontoG:\nSij = Aθ\n\n\nyi\nxj\n1\n\n (3)\nwhere we have augmented each point with a1. Since the\nmapped points inS does not correspond exactly to one\npixel inI bilinear interpolation is used to interpolate each\npoint inS. The sub-gradients for the bilinear interpola-\ntion are deﬁned and we can use standard backpropagation\nthrough the transformation to learn the parameters infloc.\nThe sampling process is illustrated in Figure\n2 where panel\na) illustratesG and panel b) illustratesS after we have ap-\nplied the transformation.\n3.1. Down-sampling\nWe can vary the number of sampled points by varyingh and\nw. Havingh and w less thanH and W will downsample\nthe input to the SPN. We specify the down sampling with\nd. d larger than 1 will downsample the input. The number\nof sampled points formI is:\nnpoints =\n( H\nd\n)\n·\n( W\nd\n)\n= H ·W\nd2 . (4)\nFor 2D images the sampled points decrease quadratically\nwithd.\n3.2. RNN-SPN\nIn the original FFN-SPN the localization network is a feed-\nforward convolutional neural network. We modify this\nmodel by letting an RNN predict the transformation ma-\ntrices such that\nc = fconv(I) (5)\nht = frnn\nloc(c, h t−1) (6)\nAθ = g(ht). (7)\nwhere fconv is a convolutional network takingI as input and\ncreating a feature mapc,frnn\nloc is an RNN, andg is a FFN.\nHere an afﬁne transformation is produced at each time-step\nfrom the hidden state of the RNN. Importantly the afﬁne\ntransformations are conditioned on the previous transfor-\nmations through the time dependency of the RNN.\n4. Experiments\nWe test the model on a dataset of sequences of MNIST dig-\nits cluttered with noise. The dataset was created by placing\n3 random MNIST digits on a canvas of size 100× 100 pix-\nels. The ﬁrst digits was placed by randomly sampling an\nTransformer network\nFigure 2.a) The sampling gridG of equally spaced sampling points. We sety1 and x1 to− 1 and yh,xw to+1. b) A recurrent SPN\nis able to zoom in on each element in the sequence. c) Bilineartransformation will interpolate the red cross by calculating a weighted\naverage of the four nearest pixels. The operation is differentiable.\ny position on the canvas. The x positions were randomly\nsampled subject to the entire sequence must ﬁt the canvas\nand the digits are non-overlapping. Subsequent digits are\nplaced by following a slope sampled from± 45◦. Finally\nthe images are cluttered by randomly placing 8 patches of\nsize9 × 9 pixels sampled from the original MNIST digits.\nFor the test, validation and training sets we sample from\nthe corresponding set in the original MNIST dataset. We\ncreate 60000 examples for training, 10000 for validation,\nand 10000 for testing\n2. Figure3 shows examples of the\ngenerated sequences.\nAs a baseline model we trained a FNN-SPN with the SPN\nlayer following immediately after the input. The classiﬁca-\ntion network had 4 layers of conv-maxpool-dropout layers\nfollowed by a fully connected layer with 400 units and ﬁ-\nnally a separate softmax layer for each position in the se-\nquence. The convolutional layers had 96 ﬁlters with size\n3 × 3 and rectiﬁed linear units were used for nonlinearity\nin both the convolutional and fully connected layers. For\ncomparison we further train a purely convolutional network\nsimilar to the classiﬁcation network used in the FFN-SPN.\nThe RNN-SPN use a gated recurrent unit (GRU)\n(\nChung et al.,2014) with 256 units. The GRU is run for\n3 time steps. At each time step the GRU unit usec as input.\nWe apply a linear layer to convertht intoAt\nθ . The RNN-\nSPN is followed by a classiﬁcation convolutional network\nsimilar to the network used in the FFN-SPN model, except\nthat the convolutional layers only have 32 ﬁlters.\nIn all experiments, the localization networks had 3 layers of\nmax-pooling convolutional layers. All convolutional layers\nhad 20 ﬁlters with size[3 × 3]. All models were trained\nwith RMSprop (\nTieleman & Hinton,2012) down-sampling\nfactors and dropout rates optimized on the validation set.\nA complete description of the models can be found in the\n2The script for generating the dataset is available along with\nthe rest of the code.\nTable 1.Per digit error rates on MNIST sequence dataset, d is the\ndown-sampling factor.\nCluttered MNIST Sequences\nModel Err. (%)\nRNN-SPN d=1 1.8\nRNN-SPN d=2 1.5\nRNN-SPN d=3 1.8\nRNN-SPN d=4 2.3\nFFN-SPN d=1 4.4\nFFN-SPN d=2 2.0\nFFN-SPN d=3 2.9\nFFN-SPN d=4 5.3\nConv. net. 2.9\nAppendix.\nThe models were implemented using Theano\n(\nBastien et al., 2012) and Lasagne ( Dieleman et al.,\n2015). The SPN has been merged into the Lasagne\nlibrary3. Code for models and dataset is released at\nhttps://goo.gl/RspkZy.\n5. Results\nTable1 reports the per digit error rates for the tested models.\nThe RNN-SPN models perform better than both convolu-\ntional networks (2.9%) and FFN-SPN networks (2.0%). In\nFigure\n3 we show where the model attend on three sample\nsequences from the test set. The last three columns show\nimage crops after the afﬁne transformation using a down-\nsampling factor of three. We found that increasing the\ndown-sampling factor above one encouraged the model to\nzoom. When the down-sampling factor is greater than one\nwe introduce an information bottleneck forcing the model\nto zoom in on each digit. The poor performance of the FFN-\n3Available here:http://goo.gl/kgSk0t\nTransformer network\nFigure 3.The left column shows three examples of the generated cluttered MNIST sequences. The next column shows where the model\nattend when classifying each digit. In the last 3 column we show the image crops that the RNN-SPN uses to classify each digit. The\ninput sequences are100 × 100 pixels. Each image crop is33 × 33 pixels because the model uses a down-sample factor of 3.\nSPN convolutional net for high down-sampling values is\nexplained by the effective decrease in resolution since the\nmodel needs to ﬁt all three digits in the image crop.\n6. Conclusion\nWe have shown that the SPN can be combined with an RNN\nto classify sequences. Combining RNN and SPN creates\na model that performs better than FFN-SPN for classify-\ning sequences. The RNN-SPN model is able to attend to\neach individual element in a sequence, something that the\nFFN-SPN network cannot do. The main advantage of the\nRNN-SPN model when compared to the DRAW network\n(\nGregor et al.,2015) is that the SPN attention is faster to\ntrain. Compared with the model of (Mnih et al.,2014) our\nmodel is end-to-end trainable with backpropagation. In this\nwork we have implemented a simple RNN-SPN model fu-\nture work include allowing multiple glimpses per digit and\nusing the current glimpse as input to the RNN network.\nReferences\nBa, Jimmy, Mnih, V olodymyr, and Kavukcuoglu, Koray.\nMultiple Object Recognition with Visual Attention. De-\ncember 2014.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.\nNeural Machine Translation by Jointly Learning to Align\nand Translate. September 2014.\nBastien, Fr´ ed´ eric, Lamblin, Pascal, Pascanu, Razvan,\nBergstra, James, Goodfellow, Ian, Bergeron, Arnaud,\nBouchard, Nicolas, Warde-Farley, David, and Bengio,\nYoshua. Theano: new features and speed improvements.\narXiv preprint arXiv:1211.5590, November 2012.\nChorowski, Jan, Bahdanau, Dzmitry, Cho, Kyunghyun, and\nBengio, Yoshua. End-to-end Continuous Speech Recog-\nnition using Attention-based Recurrent NN: First Re-\nsults. December 2014.\nChung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun,\nand Bengio, Yoshua. Empirical Evaluation of Gated Re-\ncurrent Neural Networks on Sequence Modeling.arXiv\npreprint arXiv:1412.3555, December 2014.\nDieleman, Sander, Schl¨ uter, Jan, Raffel, Colin, Ol-\nson, Eben, Sønderby, Søren Kaae, Nouri, Daniel,\nBattenberg, Eric, van den Oord, A¨ aron, and other\ncontributors. Lasagne: First release., August 2015. URL\nhttp://dx.doi.org/10.5281/zenodo.27878.\nGregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra,\nDaan. DRAW: A Recurrent Neural Network For Image\nGeneration.arXiv preprint arXiv:1502.04623, 2015.\nJaderberg, M and Simonyan, K. Spatial Transformer Net-\nworks.arXiv preprint arXiv: 1506.02025, 2015.\nMnih, V olodymyr, Heess, Nicolas, Graves, Alex, and\nKavukcuoglu, Koray. Recurrent Models of Visual At-\ntention. June 2014.\nSermanet, Pierre, Frome, Andrea, and Real, Esteban. At-\ntention for Fine-Grained Categorization.\nTieleman, T and Hinton, Geoffrey E.{Lecture 6.5—\nRmsProp: Divide the gradient by a running average of\nits recent magnitude}, 2012.\nXu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun,\nCourville, Aaron, Salakhutdinov, Ruslan, Zemel,\nRichard, and Bengio, Yoshua. Show, Attend and Tell:\nTransformer network\nNeural Image Caption Generation with Visual Attention.\nFebruary 2015.\nAppendix\n6.1. RNN-SPN\nLocalisation network\n1. maxpool(2,2)\n2. conv(20@(3,3))\n3. maxpool(2,2)\n4. conv(20@(3,3))\n5. maxpool(2,2)\n6. conv(20@(3,3))\n7. GRU(units=256)\n8. Denselayer(6, linear output)\nClassiﬁcation network\n1. SPATIAL TRANSFORMER LAYER (downsample = 1, 3,\n4)\n2. conv(32@(3,3))\n3. maxpool(2,2)\n4. dropout(p)\n5. conv(32@(3,3))\n6. maxpool(2,2)\n7. dropout(p)\n8. conv(32@(3,3))\n9. dropout(p)\n10. Dense(256)\n11. softmax\nIn table Table\n1 the model are reported at RNN SPN with\nan entry for each tested downsample factor.\n6.2. Baseline models\nFFN-SPN model\n1. SPATIAL TRANSFORMER LAYER (downsample = 2.0)\n2. conv(96@(3,3))\n3. maxpool(2,2)\n4. dropout(p)\n5. conv(96@(3,3))\n6. maxpool(2,2)\n7. dropout(p)\n8. conv(96@(3,3))\n9. dropout(p)\n10. Dense(400)\n11. 3× softmax\nLocalisation network\n1. maxpool(2,2)\n2. conv(20@(3,3))\n3. maxpool(2,2)\n4. conv(20@(3,3))\n5. maxpool(2,2)\n6. conv(20@(3,3))\n7. Denselayer(200)\n8. Denselayer(6, linear output)\nThis model is reported in Table\n1 as the SPN conv. network.\nFinally we also trained the classiﬁcation network from the\nFFN-SPN reported as conv. Net in Table\n1.",
  "topic": "Recurrent neural network",
  "concepts": [
    {
      "name": "Recurrent neural network",
      "score": 0.9067513346672058
    },
    {
      "name": "MNIST database",
      "score": 0.8625425100326538
    },
    {
      "name": "Computer science",
      "score": 0.6498181819915771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5415037870407104
    },
    {
      "name": "Transformer",
      "score": 0.5348533391952515
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47467875480651855
    },
    {
      "name": "Convolutional neural network",
      "score": 0.46625080704689026
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.43254226446151733
    },
    {
      "name": "Algorithm",
      "score": 0.3933201730251312
    },
    {
      "name": "Deep learning",
      "score": 0.3595240116119385
    },
    {
      "name": "Artificial neural network",
      "score": 0.2736661434173584
    },
    {
      "name": "Computer vision",
      "score": 0.1718316376209259
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}