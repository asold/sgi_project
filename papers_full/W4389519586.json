{
  "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
  "url": "https://openalex.org/W4389519586",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2886587608",
      "name": "Zexuan Zhong",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2960402259",
      "name": "Zhengxuan Wu",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2151390485",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2114426036",
      "name": "Christopher Potts",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2095803999",
      "name": "Danqi Chen",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W4385569933",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W3216037316",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4362598605",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4318142410",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4225644300",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4385571289",
    "https://openalex.org/W4385574113",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4313304293"
  ],
  "abstract": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15686‚Äì15702\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nMQUAKE: Assessing Knowledge Editing in\nLanguage Models via Multi-Hop Questions\nZexuan Zhong‚Ä†‚àó, Zhengxuan Wu‚Ä°‚àó,\nChristopher D. Manning‚Ä°, Christopher Potts‚Ä° and Danqi Chen‚Ä†\n‚Ä†Princeton University ‚Ä°Stanford University\n{zzhong,danqic}@cs.princeton.edu {wuzhengx,manning,cgpotts}@stanford.edu\nAbstract\nThe information stored in large language mod-\nels (LLMs) falls out of date quickly, and re-\ntraining from scratch is often not an option.\nThis has recently given rise to a range of tech-\nniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms\nare extremely limited, mainly validating the\nrecall of edited facts, but changing one fact\nshould cause rippling changes to the model‚Äôs\nrelated beliefs. If we edit the UK Prime Min-\nister to now be Rishi Sunak, then we should\nget a different answer to Who is married to\nthe British Prime Minister?In this work, we\npresent a benchmark, MQUAKE (Multi-hop\nQuestion Answering for Knowledge Editing),\ncomprising multi-hop questions that assess\nwhether edited models correctly answer ques-\ntions where the answer should change as an\nentailed consequence of edited facts. While we\nfind that current knowledge-editing approaches\ncan recall edited facts accurately, they fail catas-\ntrophically on the constructed multi-hop ques-\ntions. We thus propose a simple memory-based\napproach, MeLLo, which stores all edited facts\nexternally while prompting the language model\niteratively to generate answers that are consis-\ntent with the edited facts. While MQUAKE re-\nmains challenging, we show that MeLLo scales\nwell with LLMs (up to 175B) and outperforms\nprevious model editors by a large margin.1\n1 Introduction\nAs large language models (LLMs) are deployed\nwidely, the need to keep their knowledge correct\nand up-to-date without massive retraining costs\nbecomes increasingly important (Sinitsin et al.,\n2020). Prior work has proposed knowledge editing\nmethods to incrementally inject a set of new facts\ninto a language model (Zhu et al., 2021; De Cao\net al., 2021; Meng et al., 2022a,b; Mitchell et al.,\n*Equal contribution.\n1Our datasets and code are publicly available at https:\n//github.com/princeton-nlp/MQuAKE.\nNew Fact: The current British Prime Minister is Rishi Sunak.\nWho is the current British \nPrime Minister?\nWho is married to the \nBritish Prime Minister?\nRecall\nEdited Fact\nBoris Johnson Rishi Sunak\nCarrie Johnson Carrie Johnson\nModel \nBefore Edit\nModel \nAfter Edit\nOur\nQuestion\nWho is currently the head \nof the British government?\nRecall\nRelated Fact\nBoris Johnson Rishi Sunak\nFigure 1: An example of our benchmark MQUAKE .\nExisting knowledge-editing methods often perform well\nat answering paraphrased questions of the edited fact\nbut fail on multi-hop questions that are entailed conse-\nquences of the edited fact.\n2022a,b), but it is not yet clear whether these meth-\nods provide a viable solution of updating and main-\ntaining deployed LLMs.\nTo evaluate these methods, existing benchmarks\noften focus on measuring whether the edited model\ncan recall the newly injected facts and whether un-\nrelated knowledge remains unchanged post-editing.\nHowever, a vital unaddressed question is whether\nthe edited model can handle questions where the an-\nswer should change as an entailed consequence of\nedited facts. For example (see Figure 1), if we up-\ndate the British Prime Minister from Boris Johnson\nto Rishi Sunakwithin a model, the answer to Who\nis married to the British Prime Minister?should\nalso change as a consequence of this edit.\nTherefore, we propose MQUAKE (Multi-hop\nQuestion Answering for Knowledge Editing), a\nnew benchmark for a more complete evaluation\nof knowledge-editing methods. Each example in\nMQUAKE consists of a multi-hop question (in-\ncluding {2,3,4}-hop questions) which corresponds\nto a chain of facts. When we edit one or a few facts\nin a chain, the edited model needs to propagate\nthe change to entailed consequences of the edited\nfacts. MQUAKE includes a dataset MQUAKE- CF\n15686\nbased on counterfactual edits, and another dataset\nMQUAKE- T of temporal knowledge updates to\nevaluate model editors on real-world changes.\nWe evaluate state-of-the-art knowledge-editing\nmethods on MQUAKE , testing from editing facts\nmentioned in one question to editing facts men-\ntioned in a large set of questions. The latter setting\nevaluates approaches that are designed to handle\nmany edits, such as MEMIT (Meng et al., 2022b).\nSurprisingly, existing knowledge-editing methods\noften perform well on answering questions that are\nparaphrases of the edited fact but fail drastically\non questions where the answer should change as\na consequence of an edited fact. For example, a\nGPT-J model edited by ROME (Meng et al., 2022a)\ncan only answer 7.6% of multi-hop questions in\nMQUAKE- CF, even though it could answer43.4%\nof the questions before editing.\nTowards faithful knowledge editing, we propose\na simple but effective method, MeLLo, that sig-\nnificantly outperforms existing model editors even\nwith a large number of edits. Instead of updating\nmodel weights, MeLLo stores edits in an explicit\nmemory inspired by memory-based editing meth-\nods (Mitchell et al., 2022b) and prompts the lan-\nguage model iteratively to interact with the edited\nfacts. Specifically, it decomposes a multi-hop ques-\ntion into sub-questions successively, generates ten-\ntative answers, and checks whether it is consistent\nwith edited facts before returning the final answer.\nSuch a method does not require any additional train-\ning, and can be easily scaled up to large LMs such\nas GPT-3 (Brown et al., 2020; Ouyang et al., 2022)\nunlike methods requiring weight updates. We hope\nthat both our benchmark and proposed method\nprovide additional insights into building faithful\nknowledge-editing methods.\n2 Problem Definition\nThis section introduces our setting and argues that\na model edit is only truly successful if the edited\nmodel also returns new correct answers for ques-\ntions that change as a consequence of the edits.\n2.1 Querying Factual Knowledge in LLMs\nWe represent a fact as a triple (s, r, o), consist-\ning of a subject ( s), a relation ( r), and an object\n(o), and manually construct a natural language\nprompt template tr(‚ãÖ) for each type of relation\nr as a way of querying knowledge from a lan-\nguage model (Petroni et al., 2019). This template\ntakes a subject s as input and generates a ques-\ntion or a cloze-style statement tr(s). For instance,\ngiven the subject United Kingdomand the relation\nhead of government, we can form a cloze sentence\nThe Prime Minister of the United Kingdom is __.\nWe consider an autoregressive language model\nf‚à∂ X ‚Üí Y, which takes a piece of text x ‚ààX as in-\nput and predictsy ‚ààY, the continuation ofx. Given\na fact triple (s, r, o), we can query the language\nmodel to recall the fact by feeding the prompttr(s)\nas input and matching the output f(tr(s)) with\nthe object o. While prior work has studied how\nto prompt to extract more knowledge (Jiang et al.,\n2020; Shin et al., 2020; Zhong et al., 2021), we sim-\nply use manually-written templates, as enhancing\nknowledge retrieval is not our focus.\n2.2 Knowledge Editing\nThe knowledge stored in a language model can\nbe incorrect or become outdated over time. One\npossible solution is to edit the knowledge on the\nfly without retraining. A fact edit is defined as a\npair of fact triples that share the same subject and\nrelation e =((s, r, o),(s, r, o‚àó)), which represents\nthe associated object is updated from o to o‚àó. For\nsimplicity, we abbreviate the notation for an edit as\ne =(s, r, o‚Üí o‚àó) throughout the paper.\nGiven a collection of fact edits E ={e1, e2, . . .}\nand a language model f, knowledge editing in-\nvolves learning a function K ‚à∂ F √ó E ‚Üí F that\nyields an edited language model f‚àó ‚à∂ X ‚Üí Y,\nK(f, E) = f‚àó. For the methods we assess in Sec-\ntion 4, K modifies the weights of f in an attempt to\nincorporate E. Our proposed alternative, MeLLo,\nis much more lightweight: it keeps f frozen and\ninstead uses E as an external knowledge store to\nguide generation (Section 5).\nIn previous work (De Cao et al., 2021; Mitchell\net al., 2022a,c; Meng et al., 2022a,b), the evaluation\nfocuses on assessing whether the edited model re-\ncalls the updated knowledge and whether unrelated\nknowledge remains unchanged post-editing. To\nevaluate whether a ‚Äúsingle-hop‚Äù edit e =(s, r, o‚Üí\no‚àó) is successful with an edited model f‚àó(‚ãÖ), exist-\ning paradigms assess whether f‚àó(tr(s)) is equal\nto o‚àó (or assigns o‚àó a high probability). Addition-\nally, they check correctness by varying tr(s) while\nkeeping semantic equivalence (Meng et al., 2022b).\n2.3 Evaluation of Multi-hop Questions\nBy only evaluating single-hop questions, exist-\ning methods were tested for recalling edited facts.\n15687\nIt remains unknown whether an edited model\ncan handle a question where the answer should\nchange as an entailed consequence of an edited\nfact. We propose to evaluate edited models with\nmulti-hop questions by considering a chain of\nfacts C = ‚ü®(s1, r1, o1), . . . ,(sn, rn, on)‚ü©, where\nthe object of ith fact also serves as the subject\nof the next fact in the chain, i.e., oi = si+1. We\ndenote R = [r1, . . . , rn] as the relation set and\nS =[s1, . . . , sn] as the subject set. We then use C\nto construct a multi-hop question that asks about\nthe head entity s1, with the answer being the tail\nentity on. Similar to a single-hop question, we\ngenerate a question as tR(S). For example, with\na chain consisting of two facts (United Kingdom,\nhead of government, Boris Johnson), (Boris John-\nson, spouse, Carrie Johnson), one can write a 2-\nhop question Who is married to the British Prime\nMinister? Once one or more facts in the chain are\nedited, e.g., (United Kingdom, head of government,\nBoris Johnson‚Üí Rishi Sunak), the language model\nhas to leverage the updated knowledge to answer\nthe question, which we posit as a crucial indicator\nof a model faithfully updating the fact.\n3 MQ UAKE: Multi-hop Question\nAnswering for Knowledge Editing\nWe construct the benchmark MQUAKE (Multi-\nhop Question Answering for Knowledge Editing),\nwhich contains two datasets. The first, MQUAKE-\nCF, is designed as a diagnostic dataset for the evalu-\nation of knowledge editing methods on counterfac-\ntual edits. The second, MQUAKE- T, comprises\ntemporal-based knowledge updates and is intended\nto assess the effectiveness of knowledge editing\nmethods in updating outdated information with cur-\nrent, real facts. We first present the data construc-\ntion process for MQUAKE- CF and MQUAKE- T.\nThen, we present the data statistics and evaluation\nsettings, followed by evaluation metrics in the end.\n3.1 Data Construction of MQ UAKE- CF\nSampling chains of facts Our dataset is con-\nstructed based on Wikidata (Vrande Àáci¬¥c and\nKr√∂tzsch, 2014), a knowledge base consisting of\nfact triples associated with millions of entities. We\nfirst sample chains of facts from Wikidata. We man-\nually select 37 common relations and consider a\nsubgraph that solely comprises these relations and\nthe top 20% of common entities based on hyperlink\ncounts in Wikipedia articles. 2 We collect chains\nthat contain N = 2,3,4 triples from the Wikidata\nsubgraph. We also adopt heuristics rules to ensure\nthat the sampled fact triples are coherent and lead\nto natural questions (see Appendix A.1 for details).\nFiltering unrecallable facts As answering multi-\nhop questions requires the model to leverage each\nsingle-hop fact, we filter out any chain of facts\nwhich contain at least one fact that cannot be re-\ncalled by GPT-J (Wang and Komatsuzaki, 2021),\nwhich we will mainly evaluate on. To recall single-\nhop facts, we curate a question template for each\nrelation type following prior work (Petroni et al.,\n2019; Meng et al., 2022a), and query the model\nusing in-context learning with 8 demonstration ex-\namples (see Appendix A.2 for more details).\nGenerating multi-hop questions Given C =\n‚ü®(s1, r1, o1), . . . ,(sn, rn, on)‚ü©, we aim to write a\nset of questions Q about the head entity s1 with the\ngold answer a being the tail entity oN . We leverage\nChatGPT (gpt-3.5-turbo) to automatically gen-\nerate questions given a chain of facts C, because\n(1) this provides us more diverse question formats\nof good quality; (2) it is challenging to manually\nwrite question templates for all the different types.\nWe prompt ChatGPT to generate three questions\nfor each sampled chain of facts. We include the\nprompt we used and examples of generated multi-\nhop questions in Appendix A.3.\nSampling counterfactual edits So far, we\nhave collected ‚ü®Q, a,C‚ü© (questions, answer, fact\ntriples) for each instance in the dataset. Next,\nwe sample counterfactual edits E and col-\nlect the corresponding fact triples C‚àó and an-\nswer a‚àó. Given a chain of n factual triples C =\n‚ü®(s1, r1, o1), . . . ,(sn, rn, on)‚ü©, we randomly sam-\nple t ‚àà {1, . . . , N} counterfactual edits in C. For\na triple (s, r, o), we sample a counterfactual ob-\nject o‚àó from all possible objects that are related to\nrelation r. We replace (s, r, o) with (s, r, o‚àó) in\nthe chain and update other facts accordingly. We\nmake sure that, after injecting counterfactual ed-\nits, the new chain still exists so that we are able\nto find an updated answer a‚àó. We only keep the\nsampled edits if the corresponding updated answer\na‚àó is not identical to the original one a. We use the\nsame filtering process as Appendix A.2 to make\n2We focus on common entities as LMs can recall facts\nabout common entities more reliably (Mallen et al., 2023).\n15688\nE (W ALL-E, creator, Andrew Stanton‚ÜíJames Watt)\n(University of Glasgow, headquarters location,\nGlasgow‚ÜíBeijing)\nQ In which city is the headquarters of the employer of\nW ALL-E‚Äôs creator located?\nWhat is the location of the headquarters of the company\nthat employed the creator of W ALL-E?\nWhere is the headquarters of the company that employed\nthe creator of W ALL-E situated?\na Emeryville\na‚àó Beijing\nC (W ALL-E, creator, Andrew Stanton)\n(Andrew Stanton, employer, Pixar)\n(Pixar, headquarters location, Emeryville)\nC‚àó\n::::::::::(W ALL-E,:::::::creator,::::::James:::::Watt)\n(James Watt, employer, University of Glasgow)\n::::::::::(University::of:::::::::Glasgow,::::::::::::headquarters::::::::location,::::::::Beijing)\nTable 1: An instance in the MQUAKE- CF dataset,\nwhich consists of an edit set E, a set of three multi-hop\nquestions Q, the desirable answer pre- and post-editing\na, a‚àó, and the chain of facts pre- and post-editing C,C‚àó.\nThe edited facts are marked as ::(s,::r,:::o‚àó).\n#Edits 2-hop 3-hop 4-hop Total\nMQUAKE-CF\n1 2,454 855 446 3,755\n2 2,425 853 467 3,745\n3 - 827 455 1,282\n4 - - 436 436\nAll 4,879 2,535 1,804 9,218\nMQUAKE-T 1 (All) 1,421 445 2 1,868\nTable 2: Data statistics of MQUAKE.\nsure GPT-J can recall all unedited single-hop facts\nin the chains.\n3.2 Data Construction of MQ UAKE- T\nFollowing a similar procedure to building\nMQUAKE- CF, we construct the other segment:\nMQUAKE- T, focusing on temporal-based, real-\nworld fact updates. We take two dumps of Wiki-\ndata: 2021-04 and 2023-04, and obtain the differ-\nences between the two versions. We find that most\nchanges in Wikidata come from schema changes,\ni.e., (Encyclop√©die, instance of, encyclopedia ‚Üí\nwritten work) instead of actual fact updates. We\nthen manually select 6 relations where the changes\nare most likely to align with real fact changes, e.g.,\n(United Kingdom, head of government, Boris John-\nson ‚ÜíRishi Sunak). Similarly, we sample chains of\nfacts and filter out unrecallable facts using GPT-J.\nWhen we generate edits given a fact chain, instead\nof sampling artificial counterfactual facts, we re-\nquire that edits come from the diff set between the\ntwo versions of Wikidata. Note that different from\nMQUAKE- CF, each instance in MQUAKE- T re-\nlates to only one edit, because all the edits are about\nposition changes (e.g., head of state) and involving\ntwo in a question is not coherent. The goal of this\ndataset is to evaluate how successfully edited lan-\nguage models can answer questions which involve\nauthentic updates to real-world knowledge.\n3.3 Dataset Summary\nDataset format As shown in Table 1, each in-\nstance in the MQUAKE dataset is denoted by a tu-\nple d =‚ü®E,Q, a, a‚àó,C,C‚àó‚ü©, where E is a set of edits\nthat we want to inject into the language model, Q\nrepresents multi-hop questions we use to evaluate\nediting methods (we provide three multi-hop ques-\ntions), a and a‚àó denote the correct answer before\nand after edits, andC and C‚àó correspondingly repre-\nsent the factual triples associated with this question\nbefore and after editing. A desirable knowledge\nediting method will inject all the edits in E into the\nmodel, and enable the model to internally use the\nedits and answer the questions.\nData statistics Table 2 summarizes the statistics\nof the MQUAKE- CF and MQUAKE- T datasets.\nThe MQUAKE- CF dataset consists of more than\n9K N-hop questions (N ‚àà{2,3,4}), each of which\nassociates with one or more edits. 3 We regard\nit as a diagnostic dataset to study the ability of\nedited models leveraging newly injected knowl-\nedge through editing methods. The MQUAKE- T\ndataset includes 1.8K instances, each of them asso-\nciates with one real-world fact change.\nNumber of edited facts We consider two eval-\nuation scenarios: a) First, we perform knowl-\nedge editing on only one instance d, which is as-\nsociated with up to four edited facts. b) Then,\nwe split the dataset into groups of k instances\n(k ‚àà {1,100,1000,3000} on MQUAKE- CF and\nk ‚àà{1,100,500,1868}on MQUAKE- T), and con-\nsider all instances in a group at the same time\nand inject all the edited facts of these instances\ninto the model at once. This harder setting is par-\nticularly interesting for editing methods such as\nMEMIT, which can handle large numbers of edits\neffectively (Meng et al., 2022b).\n3Throughout the paper, our experiments on MQUAKE-\nCF are conducted on a randomly sampled subset of the full\ndataset which includes 3000 instances (1000 instances for each\nof {2,3,4}-hop questions) due to limited compute resources.\n15689\nBase Model Method Edit-wise Instance-wise Multi-hop Multi-hop (CoT)\nGPT-J\nBase ‚Äì 100.0 43.4 42.1\nFT 44.1 24.1 1.6 ‚Üì41.8 1.9‚Üì40.2\nMEND 72.8 59.6 9.2‚Üì34.2 11.5‚Üì30.6\nROME 90.8 86.7 7.6 ‚Üì35.8 18.1‚Üì24.0\nMEMIT 97.4 94.0 8.1‚Üì35.3 12.3‚Üì29.8\nVicuna-7B\nBase ‚Äì 61.0 30.0 36.6\nFT 20.2 7.8 0.7 ‚Üì29.3 0.2‚Üì36.4\nMEND 65.2 47.6 7.4 ‚Üì22.6 8.4‚Üì28.2\nROME 99.8 89.6 8.4 ‚Üì21.6 12.2‚Üì24.4\nMEMIT 96.6 84.0 7.6 ‚Üì22.4 9.0‚Üì27.6\nTable 3: Performance results on MQUAKE- CF (maximally 4 edits) for different knowledge editing methods\nusing two base models, GPT-J and Vicuna-7B. We consider edits associated with each instance independently.\nChain-of-thought (CoT) is included as a more advanced variant of prompt. Base denotes the model before editing.\nWe include breakdown multi-hop (CoT) performance on MQUAKE- CF for {2,3,4}-hop questions and for questions\nwith {1,2,3,4} edits in Appendix G.\n3.4 Evaluation Metrics\nWe use the following metrics to measure whether\nthe edits are made successfully. We include de-\ntailed formal definitions in Appendix B.\n‚Ä¢ Edit-wise success rate measures how many\nfacts can be successfully recalled from the\nedited language model.\n‚Ä¢ Instance-wise accuracy measures in how\nmany multi-hop instances, the model can re-\ncall all the individual single-hop facts. This is\na reference metric for multi-hop performance,\nas the model must encode each individual fact\nin order to answer the multi-hop question. We\nmeasure instance-wise accuracy both before\nand after editing the model.\n‚Ä¢ Multi-hop accuracy measures the accuracy\nof the original and edited language models\non multi-hop questions. In our datasets, there\nare three generated multi-hop questions for\neach instance. If any of the three questions is\ncorrectly answered by the model, we regard it\nas accurate. This is the main metric that we\nfocus on to study models‚Äô ability to use edited\nknowledge consistently.\n4 MQ UAKE Challenges Model Editors\n4.1 Experimental Setup\nLanguage models We use GPT-J (6B) and\nVicuna-7B (Chiang et al., 2023), which is a fine-\ntuned model based on LLaMA-7B (Touvron et al.,\n2023) as the baseline models to evaluate knowl-\nedge editing approaches. It is worth noting that\nexisting parameter-update methods require access\nto a white-box language model and are very com-\nputationally expensive to apply to large models.\nIn Section 5, we propose a lightweight approach,\nwhich can be applied to large black-box language\nmodels (Ouyang et al., 2022; Brown et al., 2020).\nKnowledge editing approaches We evaluate the\nfollowing state-of-the-art knowledge editing ap-\nproaches on our datasets (more details can be found\nin Appendix C).\n‚Ä¢ Fine-tuning (FT) simply performs gradient\ndescent on the edits to update model parame-\nters. We follow Zhu et al. (2021) and fine-tune\none layer in the model with a norm constraint\non weight changes.\n‚Ä¢ MEND (Mitchell et al., 2022a) trains a hyper-\nnetwork to produce weight updates by trans-\nforming the raw fine-tuning gradients given\nan edited fact.\n‚Ä¢ ROME (Meng et al., 2022a) first localizes\nthe factual knowledge at a certain layer in the\nTransformer architecture, and then updates the\nfeedforward network in that layer to insert the\nnew facts.\n‚Ä¢ MEMIT (Meng et al., 2022b) extends ROME\nto edit a large set of facts. It updates feedfor-\nward networks in a range of layers to encode\nall the facts.\nGiven an edited fact (s, r, o‚Üí o‚àó), we convert it\nto a cloze statement tr(s) and apply knowledge\nediting approaches with the objective of correctly\npredicting the edited object o‚àó given tr(s). We\n15690\nEdit- Instance- Multi- Multi-hop\nMethod wise wise hop (CoT)\nBase ‚Äì 100.0 34.3 46.8\nFT 19.5 19.0 0.0 ‚Üì34.3 0.2‚Üì46.6\nMEND 99.0 98.5 16.0‚Üì18.3 38.2‚Üì8.6\nROME 100.0 97.7 0.3 ‚Üì34.0 11.3‚Üì35.5\nMEMIT 100.0 98.9 0.3‚Üì34.0 4.8‚Üì42.0\nTable 4: Performance results on MQUAKE- T for differ-\nent knowledge editing methods using GPT-J as the base\nmodel.We consider edits associated with each instance\nindependently. Base denotes the model before editing.\ninclude the templates of cloze statement tr for each\nrelation type r in Appendix I.\nEvaluation metrics As discussed in Section 3.4,\nwe report edit-wise success rate, instance-wise ac-\ncuracy, and multi-hop accuracy in our evaluation.\nWe query the model with either manually-written\nprompt templates (for single-hop facts) or GPT-\ngenerated questions (for multi-hop fact chains). We\nadapt in-context learning and prompt the model\nwith demonstrations when calculating instance-\nwise and multi-hop accuracy, in order to encourage\nthe language model to recall and output knowledge\nin the desirable format (Brown et al., 2020).\nWe also consider chain-of-thought prompting\n(Wei et al., 2022) with in-context demonstrations\nto ensure the model‚Äôs reasoning ability is fully uti-\nlized. See Appendix D for detailed prompts that\nwe used to query language models. We denote the\nmulti-hop accuracy with chain-of-thought prompt-\ning as multi-hop (CoT).\n4.2 Results on MQ UAKE- CF\nTable 3 shows the results on MQUAKE- CF when\nconsidering each instance individually across dif-\nferent methods with GPT-J and Vicuna-7B as the\nediting base models. As shown, all of the editing\nmethods perform better than our fine-tuning base-\nline. In addition, they all gain traction on edit-wise\nevaluation, with MEMIT and ROME achieving\nhigher than 90% accuracy with GPT-J and Vicuna-\n7B. In other words, when injecting a small number\nof edits, these techniques successfully inject the ed-\nits into language models and have the edited model\nrecall them at inference time, which corroborates\nprevious findings (Zhu et al., 2021; De Cao et al.,\n2021; Meng et al., 2022a; Mitchell et al., 2022a,b).\nSubsequently, a low edit-wise success rate entails\na worse instance-wise accuracy (e.g., 59.6% for\nMEND vs. 94.0% for MEND), as instance-wise\ncorrectness relies on recalling every fact from the\nmodel correctly for multi-hop questions.\n0 500 1000 1500 2000 2500 3000\nNumber of Edited Instances\n0\n10\n20\n30\n40Multi-hop Acc. (%)\nFT\nMEND\nBase\nMEMIT\nROME\n(a) MQUAKE- CF\n0 250 500 750 1000 1250 1500 1750\nNumber of Edited Instances\n0\n10\n20\n30\n40Multi-hop Acc. (%)\nFT\nMEND\nBase\nMEMIT\nROME\n(b) MQUAKE- T\nFigure 2: Multi-hop performance (CoT) of GPT-J\nbefore and after editing on (a) MQUAKE- CF and\n(b) MQUAKE- T across four different knowledge-\nediting methods with k edited instances drawn for edit-\ning. k ‚àà {1, 100, 1000, 3000} on MQUAKE- CF. k ‚àà\n{1, 100, 500, 1868} on MQUAKE- T.\nSurprisingly, the performance of edited models\nfails catastrophically at answering multi-hop ques-\ntions. Even with the strongest baseline approach,\nMEMIT, multi-hop performance changes from\n43.4% ‚Üí 8.1% with GPT-J and 30.0% ‚Üí 7.6%\nwith Vicuna-7B. Our results lead to a surprising\nconclusion that, although these methods act faith-\nfully when evaluating with single-hop questions,\nall of them fail catastrophically at answering multi-\nhop questions that rely on the edited facts. More im-\nportantly, compared to the ability to answer multi-\nhop questions prior to edits, model performance\ndrops significantly as well. Our findings suggest\nthat current knowledge-editing techniques, instead\nof integrating new facts into the model as new inter-\nnal knowledge, are ratherhard codingthem into the\nmodel by updating weights locally. We hope these\nresults can act as a call to the community to rethink\nthe faithfulness of knowledge-editing methods and\n15691\nTentative answer\nMulti-hop questionWhat is the capital city of the country of  citizenship of Ivanka Trump‚Äôs spouse? \nSubquestionWho is Ivanka Trump‚Äôs spouse?Tentative answerIvanka Trump‚Äôs spouse is Jared KushnerRetrieved factDavid Cameron is married to Courtney LoveAnswerJared Kushner\nSubquestionWhat is the country of citizenship of Jared Kushner?The country of citizenship of Jared Kushner is United StatesJared Kushner is a citizen of CanadaCanada\n‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶\nFinal answerOttawa \nRetrieved factAnswer\nDavid Cameron is married to Courtney Love\nEdited Fact Memory\nThe capital of the US is Seattle\nThe CEO of Apple is Carlos Slim\n‚Ä¶\nOur Approach: MeLLo\nQuery the memory with a subquestion\nRetrieve an edited fact from the memory Tentative answers generated by the model Retrieved facts Edited facts stored in the memory\nJared Kushner is a citizen of Canada\nüòê not contradict\n‚ö† contradict!\nFigure 3: The illustration of our proposed method MeLLo. MeLLo decompose a multi-hop question into subques-\ntions iteratively. When a subquestion is generated, the base model generates a tentative answer to the subquestion.\nThen, the subquestion is used to retrieve a most relevant fact from the edited fact memory. The model checks if the\nretrieved fact contradicts the generated answer and updates the prediction accordingly. The concrete prompts used\nin MeLLo are shown in Appedix F.\nconduct deeper evaluations of edited models.\nOne hypothesis that these edited models cannot\nanswer our multi-hop questions faithfully is that\nour prompt is not effective enough. Recent works\nsuggest that providing explanations as Chain-of-\nthought (CoT) can greatly increase model perfor-\nmance even for models at the scale of 6B models\n(Wei et al., 2022). We further enhance our prompt\nwith explanations and reevaluate all methods. De-\ntails about our CoT prompt template can be found\nin Appendix D. As shown in Table 3, CoT helps\nslightly across all settings yet still fails catastroph-\nically at answering multi-hop questions. This fur-\nther suggests that current knowledge-editing meth-\nods fail to update knowledge faithfully.\n4.3 Results on MQ UAKE- T\nWe evaluate all methods on GPT-J with real-world\nknowledge edit on MQUAKE .4 The evaluation\nresults are shown in Table 4. We find that in\nthis setting, all methods except fine-tuning achieve\nnear-perfect performance in terms of edit-wise and\ninstance-wise accuracy. However, on multi-hop\nquestions, the performance drops significantly com-\npared to the base model before editing. We find\nthat MEND works surprisingly well with CoT on\nMQUAKE- T. We hypothesize that this may be\ndue to MEND being particularly effective in edit-\ning certain relations (e.g., head of state). On the\n4We exclude Vicuna-7B on MQUAKE- T as it is trained\nmore recently, and is likely to be contaminated with the new\nknowledge in our temporal questions.\nother hand, our results show that the edited model\nwith CoT can substantially boost multi-hop perfor-\nmance. This suggests that explicit knowledge recall\ngreatly helps the edited models answer multi-hop\nquestions, while these models struggle to utilize\nthe edited knowledge internally.\n4.4 Evaluation with Edits at Scale\nWe extend our evaluation and consider all the ed-\nits from a randomly split group of k instances\nat the same time ( k ‚àà {1,100,1000,3000} on\nMQUAKE- CF and k ‚àà {1,100,500,1868} on\nMQUAKE- T). The results are shown in Fig-\nure 2. We find that, on both MQUAKE- CF and\nMQUAKE- T, the multi-hop performance of all\nmethods further drops when injecting more edits\ninto language models at the same time.\n5 MeLLo: A Proposal for Editing Large\nLanguage Models\nIn Section 4, our evaluation results show that ex-\nisting knowledge-editing methods fail catastroph-\nically on multi-hop questions of MQUAKE . In\nthis section, we present a simple but effective alter-\nnative, MeLLo (Memory-based Editing for Large\nLanguage Models).\n5.1 Method\nFigure 3 illustrates how MeLLo answers multi-hop\nquestions. Inspired by memory-based knowledge-\nediting methods (Mitchell et al., 2022b), MeLLo\n15692\nkeeps the base language model frozen and main-\ntains all the edits in an explicit memory. During in-\nference, MeLLo (1) decomposes a multi-hop ques-\ntions into subquestions; (2) prompts the base lan-\nguage model to provide tentative answers to sub-\nquestions; and (3) self-checks whether the tentative\nanswers contradict any edited facts in the mem-\nory. MeLLo can be applied easily to LLMs such as\nGPT-3 (Ouyang et al., 2022; Brown et al., 2020).\nEdited fact memory MeLLo stores all the edited\nfacts explicitly in memory. Specifically, all edited\nfacts are first converted into sentence statements\nthrough manually-defined templates. Then, an off-\nthe-shelf retrieval model (we use the pretrained\nContriever model; Izacard et al. 2021) is used to\nembed all the edit statements and save them in a\nretrieval index. The index takes a query as input\nand returns an edited fact that is the most relevant\n(i.e., closest in the embedding space) to the query.\nStep-by-step generation and self-checking To\nanswer multi-hop questions with LLMs, we fol-\nlow previous works and first prompt the model to\ndecompose the multi-hop questions into multiple\nsimple subquestions (Press et al., 2022; Zhou et al.,\n2023a). For example, in Figure 3, the first subques-\ntion is Who is Ivanka Trump‚Äôs spouse?Second,\nthe model generates a tentative answer (e.g., Jared\nKushner) to the subquestion based on the (unedited)\nknowledge stored in the model. Third, to assess\nwhether the generated answer conflicts with any\nnew knowledge edits, the subquestion is used as a\nquery to retrieve a most relevant editing statement\nfrom the edited facts saved in memory. Fourth,\nthe model is prompted to self-check if the retrieved\nfact contradicts the generated answer. If it does, the\nmodel adjusts the intermediate answer to this sub-\nquestion using the retrieved statement. Note that it\nis possible that a subquestion does not relate to any\nedited fact in memory as the corresponding fact is\nnot edited; in this case, the model is prompted to\nkeep the generated answer as the retrieved edit does\nnot cause a contradiction. Finally, the model either\ngenerates the next subquestion of the multi-hop\nquestion or outputs the final answer.\n5.2 Evaluation Results\nWe apply MeLLo on GPT-J (Wang and Komat-\nsuzaki, 2021), Vicuna-7B (Chiang et al., 2023),\nand text-davinci-003 (Ouyang et al., 2022;\nBrown et al., 2020). Table 5 shows performance\nof MeLLo on MQUAKE- CF and MQUAKE- T.\nWe find that with the same base model (i.e.,\nGPT-J), MeLLo outperforms MEMIT and MEND\nsignificantly across all the settings while be-\ning more efficient and requiring no training.\nWhen incorporating MeLLo with a stronger LLM\n(text-davinci-003), MeLLo enlarges the perfor-\nmance gap substantially. This suggests that MeLLo\nworks particularly well on strong base language\nmodels which can easily follow the instructions in\nour prompts. Along with its simplicity and efficacy,\nwe think MeLLo can serve as a strong knowledge-\nediting baseline for future research. First, it does\nnot require access to white-box model weights, so\nit is very extensible without any adaptation. Sec-\nond, our base language model remains intact, avoid-\ning the pitfall of overfitting to editing facts or de-\nstroying existing capacities due to weight updates.\nThird, we store edits in an explicit memory com-\nponent instead of injecting facts into model pa-\nrameters, which provides greater controllability in\nremoving or adding knowledge on the fly.\nWe note that in order to answer multi-hop ques-\ntions correctly after editing, the retriever we use in\nMeLLo needs to retrieve all the associated edited\nfacts from the memory. In Appendix H, we investi-\ngate how retrieval accuracy affects the performance\nof MeLLo when using GPT-3 as the base model.\n6 Related Work\nKnowledge-editing methods Past work has in-\nvestigated different approaches in editing LLMs\nat scale by injecting new knowledge into static\nmodel artifacts (Zhu et al., 2021; Sotoudeh and\nThakur, 2019; Dai et al., 2022a; Hase et al., 2023;\nZhou et al., 2023b; Dong et al., 2022; Huang et al.,\n2023). Some of these approaches include locating\nand modifying model weights that are responsi-\nble for specific concepts (Meng et al., 2022a,b;\nDai et al., 2022b), and fast adaptation through a\nsmall auxiliary editing network (Mitchell et al.,\n2022a; De Cao et al., 2021). Recent work edits\nknowledge representations during decoding pro-\ncedures of LLMs (Hernandez et al., 2023). Our\nproposed approach MeLLo share a similar spirit\nwith SERAC (Mitchell et al., 2022b) where an ex-\nplicit memory component is used to maintain all\nthe edited facts. Different from SERAC, which\ntrains additional models to incorporate the memory,\nMeLLo directly uses the base model to self-check\nwhether the model generations need be adjusted.\nThis allows MeLLo to be easily applied to black-\n15693\nMQUAKE-CF MQUAKE-T\n# Edited instances 1 100 1000 3000 1 100 500 1868\nBase Model Method\nGPT-J MEMIT 12.3 9.8 8.1 1.8 4.8 1.0 0.2 0.0\nGPT-J MEND 11.5 9.1 4.3 3.5 38.2 17.4 12.7 4.6\nGPT-J MeLLo 20.3 12.5 10.4 9.8 85.9 45.7 33.8 30.7\nVicuna-7B MeLLo 20.3 11.9 11.0 10.2 84.4 56.3 52.6 51.3\nGPT-3 MeLLo 68.7 50.5 43.6 41.2 91.1 87.4 86.2 85.5\nTable 5: Performance results of MeLLo (ours) on MQUAKE- CF and MQUAKE- T with GPT-J, Vicuna-7B,\nor GPT-3 (text-davinci-003) as the base language model. We consider a batch of k instances as once ( k ‚àà\n{1,100,1000,3000} on MQUAKE- CF and k ‚àà{1,100,500,1868} on MQUAKE- T). We include the best results\nwith GPT-J from existing methods (MEMIT for MQUAKE- CF and MEND for MQUAKE- T) for comparison.\nbox LMs without any extra training.\nKnowledge-editing evaluation The evaluation\nmetrics for knowledge-editing techniques often in-\nvolve verifying the updated answers by querying\nthe edited facts or related facts (paraphrased or\nlogically-entailed facts), as well as verifying that ir-\nrelevant facts are not corrupted (Meng et al., 2022a;\nMitchell et al., 2022a; De Cao et al., 2021; Zhu\net al., 2021; Hase et al., 2023). More recent work\ntakes a step forward by evaluating LLMs‚Äô abilities\nto make inferences based on injected facts (Onoe\net al., 2023) (e.g., after learning iPhone is a smart-\nphone, the model should also know iPhone can\nbrowse the internet), or measuring the absence of\nunintended side effects of model edits (Hoelscher-\nObermaier et al., 2023). Complementary with\nexisting evaluation tools, MQUAKE particularly\nfocuses on assessing whether edited models can an-\nswer multi-hop questions where the answer should\nchange as an entailed consequence, showing that\nexisting approaches fail on those questions.\nPrompting methods for multi-hop QA Since\nthe debut of effective base models such as GPT-3,\nprompt-based methods combined with an optional\nretrieval module have become a popular approach\nin handling multi-step QA tasks (Press et al., 2022;\nYao et al., 2023; Khattab et al., 2022). Recent\nwork also seeks to combine external NLI mod-\nules to justify whether answers to prompt-based\nqueries are able to handle reasoning-based QA\nquestions (Mitchell et al., 2022c). Our method is\nsimilar but more generic since we rely on the LLM\nitself to perform NLI step-by-step before reaching\nthe final answer.\n7 Conclusion\nIn this work, we present a benchmark MQUAKE\nthat assesses knowledge editing methods for lan-\nguage models via multi-hop questions. We find\nthat although edited language models can effec-\ntively recall edited facts, they fail on multi-hop\nquestions that are entailed consequences of the ed-\nits. We propose a simple but effective alternative,\nMeLLo, which significantly outperforms existing\nknowledge editing methods. MeLLo does not re-\nquire any additional training and can be applied\nto large LMs such as GPT-3 (Brown et al., 2020).\nWe hope our work can facilitate future research on\ndeveloping faithful knowledge editing methods.\nLimitations\nThe limitations of our work are as follows.\n‚Ä¢ We mainly evaluate existing knowledge edit-\ning methods on GPT-J (Wang and Komat-\nsuzaki, 2021) and Vicuna (Chiang et al., 2023).\nThe efficacy of these methods on other LLMs\nremains less explored. Note that existing edit-\ning methods are very computationally expen-\nsive. We leave the evaluation on other models\nas future work.\n‚Ä¢ We demonstrate that MeLLo outperforms ex-\nisting knowledge editing methods on models\nwith > 6B parameters. As MeLLo relies on\nlanguage models for question decomposition\nand self-checking, future work may study how\nMeLLo works with smaller models such as\nGPT-2 (Radford et al., 2019).\n‚Ä¢ Our proposed memory-based approach,\nMeLLo, while being very effective on the\nMQUAKE benchmark, requires manually\n15694\ndefined prompts to drive language models\non new tasks. Although we believe MeLLo\nis easy to instantiate on different tasks, we\nacknowledge this limitation and leave the\nevaluation on other tasks as future work.\n‚Ä¢ The multi-hop questions in MQUAKE are\nautomatically generated by ChatGPT, rather\nthan being crafted by humans. Although\nMQUAKE- T already involves real knowl-\nedge changes, we posit that the use of\nhuman-authored questions could further align\nMQUAKE with the realistic applications of\nknowledge editing methods.\nAcknowledgements\nWe thank Dan Friedman, Tianyu Gao, Eric\nMitchell, Mengzhou Xia, Howard Yen, Jiayi Geng\nfor providing valuable feedback. This research is\npartially supported by an NSF CAREER award\n(IIS-2239290), a Sloan Research Fellowship, and\nMicrosoft Azure credits through the ‚ÄúAccelerate\nFoundation Models Academic Research‚Äù Initiative.\nZZ is supported by a JP Morgan Ph.D. Fellowship.\nCM is a CIFAR Fellow.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing GPT-4 with 90%* Chat-\nGPT quality.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022a. Knowledge neurons in\npretrained transformers. In Association for Computa-\ntional Linguistics (ACL).\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022b. Knowledge neurons in\npretrained transformers. In Association for Computa-\ntional Linguistics (ACL).\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifang Sui, and Lei Li. 2022. Calibrating factual\nknowledge in pretrained language models. In Find-\nings of Empirical Methods in Natural Language Pro-\ncessing (EMNLP).\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2023. Do language models have\nbeliefs? Methods for detecting, updating, and visu-\nalizing model beliefs. In European Chapter of the\nAssociation for Computational Linguistics (EACL).\nEvan Hernandez, Belinda Z Li, and Jacob Andreas.\n2023. Measuring and manipulating knowledge rep-\nresentations in language models. arXiv preprint\narXiv:2304.00740.\nJason Hoelscher-Obermaier, Julia Persson, Esben Kran,\nIoannis Konstas, and Fazl Barez. 2023. Detect-\ning edit failures in large language models: An\nimproved specificity benchmark. arXiv preprint\narXiv:2305.17553.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2023. Transformer-\npatcher: One mistake worth one neuron. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Towards unsupervised\ndense information retrieval with contrastive learn-\ning. Transactions on Machine Learning Research\n(TMLR).\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? In Transactions of the Association of\nComputational Linguistics (TACL).\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2023.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories. In Association for Computa-\ntional Linguistics (ACL).\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual asso-\nciations in GPT. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nediting memory in a transformer. In International\nConference on Learning Representations (ICLR).\n15695\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022a. Fast model\nediting at scale. In International Conference on\nLearning Representations (ICLR).\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022b. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning (ICML).\nEric Mitchell, Joseph J. Noh, Siyan Li, William S. Arm-\nstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn,\nand Christopher D. Manning. 2022c. Enhancing self-\nconsistency and performance of pretrained language\nmodels with NLI. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nYasumasa Onoe, Michael JQ Zhang, Shankar Padman-\nabhan, Greg Durrett, and Eunsol Choi. 2023. Can\nlms learn new entities from descriptions? Challenges\nin propagating injected knowledge. In Association\nfor Computational Linguistics (ACL).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E.\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul Francis Christiano, Jan Leike, and Ryan J.\nLowe. 2022. Training language models to follow\ninstructions with human feedback. In Advances in\nNeural Information Processing Systems (NeurIPS).\nFabio Petroni, Tim Rockt√§schel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with auto-\nmatically generated prompts. In Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4222‚Äì4235.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In International Conference on\nMachine Learning (ICML).\nMatthew Sotoudeh and Aditya V Thakur. 2019. Cor-\nrecting deep neural networks with small, generalizing\npatches. In Workshop on Safety and Robustness in\nDecision Making.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nDenny VrandeÀáci¬¥c and Markus Kr√∂tzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78‚Äì85.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2023a.\nLeast-to-most prompting enables complex reasoning\nin large language models. In International Confer-\nence on Learning Representations (ICLR).\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023b. Context-faithful prompt-\ning for large language models. arXiv preprint\narXiv:2303.11315.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2021. Modifying memories in transformer models.\nIn International Conference on Machine Learning\n(ICML).\n15696\nA Details of Dataset Construction\nA.1 Sampling Fact Chains from Wikidata\nWe collect chains of facts that containN ={2,3,4}\ntriples from Wikidata. We adopt heuristic rules to\nensure that the sampled fact triples are coherent\nand lead to natural questions. Specifically, we ap-\nply the following constraints when sampling fact\nchains from Wikidata. (1) The sampled chain does\nnot involve a circle; (2) The sampled chain does\nnot contain two triples that share the same relation\ntype; (3) The triples with the object being a country\ncan only appear in the last two hops of the chain;\n(4) The sampled chain contains up to three object\ntypes; (5) All triples with a person or location ob-\nject are consecutive in the chain; (6) The subject\nentity associated with the relation headquarters\nlocation (P159) must be a company or an organi-\nzation; (7) In all triples with the relation capital\n(P36), the subject has to be a country. To use these\nheuristic rules, we manually label the object types\nfor each relation we consider. For example, the\nrelation head of state(P35) corresponds to a person\nas the object.\nA.2 Filtering Unrecallable Facts with GPT-J\nWe filter out any chain of facts which contain at\nleast one fact that cannot be recalled by GPT-J. For\neach relation type, we manually define a question\ntemplate as well as 8 demonstration examples for\nin-context-learning. We use in-context-learning to\nensure the model can capture the answer format\nfrom the context. Table 6 shows an example of the\nprompt we use to recall facts of the relation devel-\noper (P178). We include the question templates of\nall relations on MQUAKE in Appendix I.\nA.3 Generating Questions using ChatGPT\nGiven a chain of facts, we prompt ChatGPT\n(gpt-3.5-turbo) to automatically generate multi-\nhop questions. The prompt we used is shown in\nTable 7.\nIn Table 8, we show some randomly selected\nexamples of the questions generated by ChatGPT\non MQUAKE- CF. We select 3 instances from\n2,3,4‚àíhop questions, each of which contains three\ngenerated questions. As shown, ChatGPT success-\nfully transforms the chain of triples into grammati-\ncally correct questions. Although these multi-hop\nquestions are synthetic, they are logically consis-\ntent with the flow of the triple chains. We believe\nthese generated questions are of sufficient qual-\nity for assessing the efficacy of knowledge-editing\nmethods.\nB Evaluation Metrics\nWe evaluate the editing results based on three evalu-\nation metrics: edit-wise success rate, instance-wise\naccuracy, and multi-hop accuracy. Suppose we\nhave edited a language model and obtain the edited\nmodel f‚àó(‚ãÖ).\nEdit-wise success rate measures how many\nedited facts can be recalled by the edited language\nmodel. Given an edit e =(s, r, o‚Üí o‚àó), the editing\nsuccess is defined as 1 [f‚àó(tr(s)) =o‚àó]. We take\nthe averaged value of the all edits in the dataset and\nrefer it as the edit-wise success rate metric.\nInstance-wise accuracy measures how many in-\nstances are there where all the associated facts\ncan be recalled by the language model (either\nthe original or edited one). Given an instance\nd =‚ü®E,Q, a, a‚àó,C,C‚àó‚ü©, the instance-wise accuracy\nbefore editingis defined as\n1\n‚é°‚é¢‚é¢‚é¢‚é¢‚é£\n‚ãÄ\n(s,r,o)‚ààC\n[f(tr(s)) =o]\n‚é§‚é•‚é•‚é•‚é•‚é¶\n,\nand the instance-wise accuracy post editingis de-\nfined as\n1\n‚é°‚é¢‚é¢‚é¢‚é¢‚é£\n‚ãÄ\n(s,r,o)‚ààC‚àó\n[f‚àó(tr(s)) =o‚àó]\n‚é§‚é•‚é•‚é•‚é•‚é¶\n.\nWe report the averaged instance-wise accuracy in\nour evaluation.\nMulti-hop accuracy measures the accuracy on\nmulti-hop questions. We regard an instance be-\ning predicted correctly if any of the multi-hop\nquestions are answered correctly by the language\nmodel. Given an instance d = ‚ü®E,Q, a, a‚àó,C,C‚àó‚ü©,\nthe multi-hop accuracy before editingis defined as\n1\n‚é°‚é¢‚é¢‚é¢‚é£\n‚ãÅ\nq‚ààQ\n[f(q) =a]\n‚é§‚é•‚é•‚é•‚é¶\n,\nand the multi-hop accuracy post editingis defined\nas\n1\n‚é°‚é¢‚é¢‚é¢‚é£\n‚ãÅ\nq‚ààQ\n[f‚àó(q) =a‚àó]\n‚é§‚é•‚é•‚é•‚é¶\n.\nWe report the averaged multi-hop accuracy in our\nevaluation.\n15697\n(In-context-learning examples)\nQ: Who is the developer of Telegram? A: Telegram FZ-LLC\nQ: Who is the developer of Microsoft Windows? A: Microsoft\nQ: Who is the developer of PlayStation 2? A: Sony Interactive Entertainment\nQ: Who is the developer of iTunes? A: Apple Inc.\nQ: Who is the developer of SR-71 Blackbird? A: Kelly Johnson\nQ: Who is the developer of Moblin? A: Linux Foundation\nQ: Who is the developer of Xbox 360? A: Microsoft\nQ: Who is the developer of Kinsey scale? A: Alfred Kinsey\n(Query during inference)\nQ: Who is the developer of SteamOS? A: Valve Corporation\nTable 6: An example of the prompt we used to recall single-hop factf with relation developer (P178) from language\nmodels. We use in-context-learning with 8 demonstration examples to ensure the model can output the answer in a\ndesirable format.\nSystem:\nYou are a powerful multi-hop question generator. Users will provide a chain of Wikidata triples,\nand you will help write questions to ask the tail entity from the head entity. You shouldn‚Äôt\ninclude bridge entities in generated questions. The questions should only include the head entity.\n[12 in-context demonstrations abbreviated]\nUser:\nGiven Wikidata triples (Daredevil/Bullseye: The Target, author, x1), (x1, country of citizenship,\nx2), (x2, continent, x3), write a question to ask x3. Don‚Äôt mention x1, x2, ... Write three\npossible questions in natural English.\nSystem:\n1. What continent is the country of citizenship of the author of Daredevil/Bullseye: The Target\nlocated in?\n2. From which continent does the author of Daredevil/Bullseye: The Target‚Äôs country of citizenship\nbelong?\n3. What continent is the author‚Äôs country of citizenship, who wrote Daredevil/Bullseye: The Target,\nsituated in?\nTable 7: An example of using ChatGPT (gpt-3.5-turbo) to generate questions from Wikidata triples. We manually\nwrite 12 demonstrations as the prompt when querying ChatGPT.\nC Implementation Details for Knowledge\nEditing Methods\nC.1 Fine-tuning\nOur fine-tuning baseline (FT) performs gradient\ndescent on the edits to update model parameters.\nWe fine-tune layer 21 of GPT-J and layer 31 of\nVicuna-7B. We follow Zhu et al. (2021) and use a\nnorm constraint on weight changes with a coeffi-\ncient 5 √ó 10‚àí5 in our implementation.\nC.2 MEND\nWe use the GPT-J MEND editor trained by Meng\net al. (2022a). For Vicuna-7B, we train our own\nMEND editor model on the Wikitext generation\nediting dataset (Mitchell et al., 2022a) with the\ndifferent hyperparameters. During inference, we\nset the learning rate scale to be 1.0.\nC.3 ROME\nFor GPT-J, we use the default hyperparameters of\nROME and the pre-computed covariance statistics\nreleased by Meng et al. (2022a). For Vicuna-7B,\nwe run ROME to update model weights at layer\n9 with the default hyperparameters. We compute\nthe covariance statistics for Vicuna-7B on Wikitext\nusing a sample size of 100,000.\nC.4 MEMIT\nFor GPT-J, we use the default hyperparameters of\nMEMIT and the pre-computed covariance statistics\nreleased by Meng et al. (2022b). For Vicuana-7B,\nwe update model weights at layers {5,6,7,8,9}\nwith the default hyperparameters. Similarly, we\ncompute the covariance statistics for Vicuna-7B on\nWikitext using a sample size of 100,000.\n15698\nExamples of 2-hop questions\nC (Jacques Necker, employer, University of Geneva) (University of Geneva, headquarters location, Geneva)\nQ What is the location of the headquarters of the employer of Jacques Necker?\nWhere is the employer of Jacques Necker headquartered?\nIn which city is the head office located for the company that employed Jacques Necker?\nC (Percival Lowell, educated at, Harvard University) (Harvard University, headquarters location, Cambridge)\nQ What is the location of the headquarters of the institution where Percival Lowell was educated?\nIn which city is the institution located where Percival Lowell received his education?\nWhere is the headquarters of the educational institution attended by Percival Lowell located?\nC (Gordon Moore, country of citizenship, United States of America) (United States of America, capital, Washington, D.C.)\nQ What is the capital of the country where Gordon Moore holds citizenship?\nWhich is the capital city of the country to which Gordon Moore belongs?\nIn which city is the seat of the government of the country where Gordon Moore is a citizen?\nExamples of 3-hop questions\nC (Kim Kardashian, spouse, Kanye West) (Kanye West, genre, hip hop music)\n(hip hop music, country of origin, United States of America)\nQ What is the country of origin of the genre associated with the spouse of Kim Kardashian?\nFrom which country does the genre of the partner of Kim Kardashian hail?\nWhich country is the genre of the partner of Kim Kardashian associated with originally from?\nC (Nicholas of Tolentino, religion or worldview, Catholic Church) (Catholic Church, founded by, Jesus Christ)\n(Jesus Christ, place of birth, Bethlehem)\nQ Where was the founder of Nicholas of Tolentino‚Äôs religion born?\nIn which city was the founder of the religion that Nicholas of Tolentino adhered to born?\nWhat is the birthplace of the founder of the religion that Nicholas of Tolentino followed?\nC (Boston, head of government, Marty Walsh) (Marty Walsh, educated at, Boston College)\n(Boston College, headquarters location, Chestnut Hill)\nQ In what city is the headquarters of the institution where the head of government of Boston was educated located?\nWhere is the location of the headquarters of the educational institution where the head of government of Boston\nreceived their education?\nWhat is the city where the headquarters of the institution where the head of government of Boston was educated\nat located?\nExamples of 4-hop questions\nC (Xbox Live, developer, Microsoft) (Microsoft, chief executive officer, Satya Nadella)\n(Satya Nadella, place of birth, Hyderabad) (Hyderabad, continent, Asia)\nQ Which continent is home to the birthplace of the CEO of Xbox Live developer?\nWhere was the CEO of the developer of Xbox Live born in which continent?\nIn what continent was the CEO of Xbox Live‚Äôs developer born?\nC (Winnie the Pooh, creator, A. A. Milne) (A. A. Milne, child, Christopher Robin Milne)\n(Christopher Robin Milne, country of citizenship, United Kingdom) (United Kingdom, official language, English)\nQ What is the official language of the country where the child of Winnie the Pooh‚Äôs creator holds citizenship?\nWhich language is officially spoken in the country where the child of the creator of Winnie the Pooh is a citizen?\nWhat is the officiated language of the country where the child of Winnie the Pooh‚Äôs creator is a citizen of?\nC (watchOS, developer, Apple Inc.) (Apple Inc., chief executive officer, Tim Cook)\n(Tim Cook, country of citizenship, United States of America) (United States of America, capital, Washington, D.C.)\nQ What is the capital of the country where the CEO of the developer of watchOS holds citizenship?\nIn which city does the CEO of the company that developed watchOS hold citizenship?\nWhich city is the capital of the home country of the CEO of the developer of watchOS?\nTable 8: Qualitative examples of the generated multi-hop questions on MQUAKE- CF. Given a chain of facts, we\nquery ChatGPT (gpt-3.5-turbo) to generate multi-hop questions with the prompt shown in Table 7.\nD Chain-of-thought Prompting for\nMulti-hop Questions\nWe use chain-of-thought (CoT) prompting (Wei\net al., 2022) to maximize model performance. Ta-\nble 9 shows one simplified example of our prompt\nwith CoT.\nE Extended Golden Labels for\nMQUAKE- T\nOur MQUAKE- T contains limited test cases. To\nbetter assess the model‚Äôs original performance on\nmulti-hop questions, we extend the possible golden\nlabels for each multi-hop question. Specifically,\nwe allow outdated answers given smaller language\n15699\nQuestion: What is the capital of the country where Plainfield Town Hall is located?\nThoughts: Plainfield Town Hall is located in the country of the United States of America. The\ncapital of United States is Washington, D.C.\nAnswer: Washington, D.C.\nQuestion: In which country is the company that created Nissan 200SX located?\nThoughts: Nissan 200SX was created by Nissan. Nissan is located in the country of Japan.\nAnswer: Japan\n[3 in-context demonstrations abbreviated]\nQuestion: Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts: The developer of Chevrolet Corvette (C4) is Chevrolet. Chevrolet is owned by General\nMotors.\nAnswer: Model Generated Answer Goes Here\nTable 9: The template of the prompt we used for asking multi-hop questions using chain-of-thoughts.\nPlease answer the following question faithfully using the knowledge you have from Wikipedia.\nProvide 10 possible answers to the question, using all the Wikipedia data you know. Rank them from\nthe most current to the most outdated.\nInput: What is the name of the current head of the United States of America government?\nOutput:\nJoe Biden\nDonald Trump\nBarack Obama\nGeorge W. Bush\nBill Clinton\nGeorge H. W. Bush\nRonald Reagan\nJimmy Carter\nGerald Ford\nRichard Nixon\nInput: What is the name of the current head of the New York City government?\nOutput: Model Generated Answer Goes Here\nTable 10: The template of the prompt we used for extending golden labels for MQUAKE- T. The prompt contains\none demonstration for better aligning model behaviors.\nmodels tend to be less calibrated. To extend the\ngolden labels, we use GPT-3 (text-davinci-003)\nto query outdated answers. See the prompt we used\nin Table 10.\nF Prompts used in MeLLo\nThe prompt we used in MeLLo is shown in Ta-\nble 11. We first prompt the language model to de-\ncompose subquestions. Then the language model\ngenerates a tentative question to the subquestion\n(marked in green text); then we use the generated\nsubquestion to retrieve the most relevant edited fact\n(marked in light blue text) and append it to the\nprompt. The model self-checks if the retrieved fact\ncontradicts the generated answer. The prompting\nprocedure goes iteratively until the model generates\nthe final answer.\nG Breakdown Results on MQ UAKE- CF\nTable 12 and Table 13 present the breakdown re-\nsults on MQUAKE- CF when using GPT-J as the\nbase model. We find that, in all editing methods\n(1) the performance on 2-hop questions is much\nhigher than 3-hop and 4-hop questions; (2) the\nperformance is worse when there are more edits\nasssociated with the edited instances.\nH Impact of Retrieval Performance\nIn MeLLo, in order to answer multi-hop questions\ncorrectly after editing, the retrieval model needs to\nretrieve all the associated edited facts (each ques-\ntion is associated with 1-4 edited facts) from the\nmemory. Here we investigate how retrieval accu-\nracy affects the performance of MeLLo when using\nGPT-3 as the base model. We compute the retrieval\naccuracy (i.e., how many instances where all the as-\n15700\n[4 in-context demonstrations abbreviated]\nQuestion: What is the capital city of the country of citizenship of Ivanka Trump‚Äôs spouse?\nSubquestion: Who is Ivanka Trump‚Äôs spouse?\nGenerated answer: Ivanka Trump‚Äôs spouse is Jared Kushner.\nRetrieved fact: David Cameron is married to Samantha Cameron.\nRetrieved fact does not contradict to generated answer, so the intermediate answer is: Jared\nKushner\nSubquestion: What is the country of citizenship of Jared Kushner?\nGenerated answer: The country of citizenship of Jared Kushner is United States.\nRetrieved fact: Jared Kushner is a citizen of Canada.\nRetrieved fact contradicts to generated answer, so the intermediate answer is: Canada\nSubquestion: What is the capital city of Canada?\nGenerated answer: The capital city of Canada is Ottawa.\nRetrieved fact: The capital city of United States is Seattle.\nRetrieved fact does not contradict to generated answer, so the intermediate answer is: Ottawa\nFinal answer: Ottawa\nTable 11: A step-by-step illustration of MeLLo solving one simplified example. Green parts are generated by the\nlanguage model, and blue parts are facts retrieved by the retriever.\n2-hop 3-hop 4-hop All\nBase 47.5 27.1 45.3 42.1\nFT 3.7 1.4 0.5 1.9\nMEND 13.9 11.3 9.5 11.5\nROME 33.8 9.1 11.4 18.1\nMEMIT 22.5 6.0 8.4 12.3\nTable 12: Breakdown multi-hop performance (CoT) on\nMQUAKE- CF for {2,3,4}-hop questions. We use GPT-\nJ as the base model in this experiment\n# Edits = 1 2 3 4 All\nBase 34.0 43.0 40.4 51.7 42.1\nFT 4.2 0.7 0.3 0.0 1.9\nMEND 16.0 11.0 7.3 4.4 11.5\nROME 23.8 20.9 9.0 2.6 18.1\nMEMIT 20.5 9.8 5.5 2.6 12.3\nTable 13: Breakdown multi-hop performance (CoT) on\nMQUAKE- CF for questions with {1,2,3,4} edits. We\nuse GPT-J as the base model in this experiment.\n# Instances(k =) 1 100 1000 3000\nRetrieval acc. 93.6 67.7 59.4 58.7\nMeLLo (on GPT-3) 68.7 50.5 43.6 41.2\nTable 14: How retrieval accuracy affects the multi-hop\nperformance of MeLLo on MQUAKE- CF. We consider\na group of k instances at the same time. Retrieval acc.:\nhow many instances where all the associated edited facts\nare correctly retrieved from the memory.\nsociated edited facts are correctly retrieved from the\nmemory) when applying MeLLo on MQUAKE-\nCF with GPT-3 by considering different numbers\nof edited instances at the same time. As Table 14\nshows, the performance of MeLLo decreases if the\nretrieval accuracy is lower (as a result of consider-\ning more instances at the same time). Among those\nquestions where all associated facts are success-\nfully retrieved from memory, MeLLo can answer\n73.1% of them correctly. This indicates that re-\ntrieval performance can significantly impact the\nmodel performance. When we consider more ir-\nrelevant knowledge edits in the memory, retrieval\ncan be more challenging, and we expect that using\nmore advanced retrieval techniques can improve\nthe performance.\nI Question/Cloze Statement Templates\nused in MQUAKE\nTable 15 shows the question templates and\nthe cloze-style statement templates we use in\nMQUAKE. We use the question templates to query\nsingle-hop facts when we use GPT-J for filtering\nand use the cloze-style statement templates to con-\nvert an edited fact to a natural language statement.\n15701\nRelationQuestion template Cloze-style statement template\nP30 Which continent is [S] located in? [S] is located in the continent of\nP36 What is the capital of [S]? The capital of [S] is\nP35 What is the name of the current head of state in [S]?The name of the current head of state in [S] is\nP6 What is the name of the current head of the [S] government?The name of the current head of the [S] government is\nP20 Which city did [S] die in? [S] died in the city of\nP26 Who is [S] married to? [S] is married to\nP140 Which religion is [S] affiliated with? [S] is affiliated with the religion of\nP1412 What language does [S] speak? [S] speaks the language of\nP19 Which city was [S] born in? [S] was born in the city of\nP69 Which university was [S] educated at? The univeristy where [S] was educated is\nP40 Who is [S]‚Äôs child? [S]‚Äôs child is\nP27 What is the country of citizenship of [S]? [S] is a citizen of\nP175 Who performed [S]? [S] was performed by\nP108 Who is the employer of [S]? [S] is employed by\nP112 Who founded [S]? [S] was founded by\nP50 Who is the author of [S]? The author of [S] is\nP170 Who was [S] created by? [S] was created by\nP407 Which language was [S] written in? [S] was written in the language of\nP37 What is the official language of [S]? The official language of [S] is\nP740 Where was [S] founded? [S] was founded in the city of\nP495 Which country was [S] created in? [S] was created in the country of\nP106 What kind of work does [S] do? [S] works in the field of\nP136 What type of music does [S] play? The type of music that [S] plays is\nP364 What is the original language of [S]? The original language of [S] is\nP937 Which city did [S] work in? [S] worked in the city of\nP800 What is [S] famous for? [S] is famous for\nP641 Which sport is [S] associated with? [S] is associated with the sport of\nP413 What position does [S] play? [S] plays the position of\nP286 Who is the head coach of [S]? The head coach of [S] is\nP159 Which city is the headquarter of [S] located in? The headquarters of [S] is located in the city of\nP178 Who is the developer of [S]? [S] was developed by\nP488 Who is the chairperson of [S]? The chairperson of [S] is\nP169 Who is the chief executive officer of [S]? The chief executive officer of [S] is\nP449 Who is the original broadcaster of [S]? The origianl broadcaster of [S] is\nP176 Which company is [S] produced by? The company that produced [S] is\nP1037 Who is the director of [S]? The director of [S] is\nP1308 Who is the [S]? The [S] is\nTable 15: Question templates and cloze-style statement templates that are used in MQUAKE . ‚Äú[S]‚Äù represents a\nplaceholder for the subject entity of the fact. We use the question templates to query single-hop facts when we use\nGPT-J for filtering and use the cloze-style statement templates to convert an edited fact to a statement.\n15702",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7164673805236816
    },
    {
      "name": "Recall",
      "score": 0.6174975633621216
    },
    {
      "name": "Language model",
      "score": 0.5455687642097473
    },
    {
      "name": "Question answering",
      "score": 0.49313420057296753
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.466087281703949
    },
    {
      "name": "Natural language processing",
      "score": 0.4296915531158447
    },
    {
      "name": "Hop (telecommunications)",
      "score": 0.41602373123168945
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40017077326774597
    },
    {
      "name": "Machine learning",
      "score": 0.24387416243553162
    },
    {
      "name": "Cognitive psychology",
      "score": 0.2316422462463379
    },
    {
      "name": "Psychology",
      "score": 0.15737798810005188
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}