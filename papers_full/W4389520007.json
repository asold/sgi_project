{
  "title": "Understanding HTML with Large Language Models",
  "url": "https://openalex.org/W4389520007",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2411057514",
      "name": "Izzeddin Gur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2558685655",
      "name": "Ofir Nachum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2205521520",
      "name": "Yingjie Miao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232518938",
      "name": "Mustafa Safdari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128221477",
      "name": "Austin Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1921468770",
      "name": "Aakanksha Chowdhery",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2490038776",
      "name": "Sharan Narang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1023462809",
      "name": "Noah Fiedel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2190457378",
      "name": "Aleksandra Faust",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3173325518",
    "https://openalex.org/W4225496097",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4286902133",
    "https://openalex.org/W1954441744",
    "https://openalex.org/W3164797848",
    "https://openalex.org/W3116323972",
    "https://openalex.org/W4283828996",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W3121976951",
    "https://openalex.org/W4225924362",
    "https://openalex.org/W4287016601",
    "https://openalex.org/W3182696977",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4382318868",
    "https://openalex.org/W4225784175",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W197294650",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2916245184"
  ],
  "abstract": "Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2803–2821\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUnderstanding HTML with Large Language Models\nIzzeddin Gur, Oﬁr Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang\nAakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust\nGoogle DeepMind\n{izzeddin,ofirnachum,yingjiemiao,msafdari,austinvhuang\nchowdhery,sharannarang,nfiedel,faust}@google.com\nAbstract\nLarge language models (LLMs) have shown\nexceptional performance on a variety of nat-\nural language tasks. Yet, their capabilities\nfor HTML understanding – i.e., parsing the\nraw HTML of a webpage, with applications\nto automation of web-based tasks, crawling,\nand browser-assisted retrieval – have not been\nfully explored. We contribute HTML under-\nstanding models (ﬁne-tuned LLMs) and an in-\ndepth analysis of their capabilities under three\ntasks: (i) Semantic Classiﬁcation of HTML el-\nements, (ii) Description Generationfor HTML\ninputs, and (iii) Autonomous Web Navigation\nof HTML pages. While previous work has\ndeveloped dedicated architectures and train-\ning procedures for HTML understanding, we\nshow that LLMs pretrained on standard natu-\nral language corpora transfer remarkably well\nto HTML understanding tasks. For instance,\nwhen ﬁne-tuned on data from the MiniWoB\nbenchmark, LLMs successfully complete 50%\nmore tasks using 192x less data compared\nto the previous best supervised model. We\ncreate and open-source a large-scale HTML\ndataset distilled and auto-labeled from Com-\nmonCrawl.1\n1 Introduction\nWeb crawling (Olston et al., 2010), form-\nﬁlling (Diaz et al., 2013; Gur et al., 2021), or infor-\nmation retrieving web agents (Nogueira and Cho,\n2016) are important for both automating and assist-\ning users in web-based tasks. These and similar\napplications rely on models that can search for spe-\nciﬁc content or controls on a web page as well as\nnavigate a website autonomously. Since a web page\nin its raw form is represented as an HTML-based\ntext sequence, the success of models for web-based\ntasks relies on their ability to understand HTML\nsemantics, structure, and embedded interactions.\n1See visualizations of the results at https://sites.\ngoogle.com/view/llm4html/home.\nThe predominant approach to web automation\nand HTML understanding is to train specialized\nmodels, i.e., gathering application-speciﬁc datasets\nand designing neural network (NN) architectures to\nleverage inductive biases of the HTML’s structure;\nsee, e.g., Liu et al. (2018); Toyama et al. (2021);\nGur et al. (2019, 2021); Humphreys et al. (2022).\nHowever, both dataset collection and neural archi-\ntecture design are expensive, time-consuming, and\nrequire highly-specialized, domain-speciﬁc knowl-\nedge.\nMeanwhile, in the natural language processing\n(NLP) literature, large language models (LLMs)\nhave emerged as a solution to the difﬁculties of\ndataset collection and specialized NN design (Ka-\nplan et al., 2020; Bommasani et al., 2021). A popu-\nlar paradigm in NLP is to take an off-the-shelf LLM\n– pretrained on a large text corpus via an unsuper-\nvised and task-agnostic learning objective – and\neither ﬁne-tune or prompt the LLM on a small task-\nspeciﬁc dataset. This paradigm has shown excep-\ntional performance on a variety of NLP tasks (Xue\net al., 2020; Brown et al., 2020; Austin et al., 2021).\nWhether LLMs can be applied to HTML under-\nstanding – especially given the much larger context\nand sequence lengths – remains an under-explored\nquestion.\nIn this paper, we investigate whether LLMs can\nbe applied to HTML understanding to produce\nbetter-performing, more sample-efﬁcient HTML\nunderstanding models and without the need for\ncustom NN architecture design. To that end, we\npresent a suite of three benchmarking tasks for\nHTML understanding that capture the essence of\nthese applications and require understanding both\nstructure and content. First, we devise Semantic\nClassiﬁcation as a task that requires a model to clas-\nsify a given HTML element into one of a set of cat-\negories, such as address, email, password etc., with\napplication to automated form-ﬁlling. Second, we\npresent Description Generation, a label-extraction\n2803\n<html>\n   <body>\n      <form class=\"login-form\">\n   <div>\n            <label class=\"form-label\" for=”uName”>\n               Enter Email Address\n            </label>\n      <label class=\"form-label\" for=”pass”>\n               Enter Password:\n            </label>\n   </div>\n         <div>\n <input type=\"email\" id=\"uName”>\n<input type=\"password\" id=\"pass\">\n<span class=\"hidden\">\n               Please enter your password.\n            </span>\n         </div>\n         <button type=\"submit\">Sign In</button>\n       </form>\n   </body>\n</html>\nFigure 1: HTML example page with a highlighted salient\nelement, an element of interest (dashed box). All canonical\ntasks evaluate a distinct interaction with this element, either\nby classifying it as one of a set of categories, generating a text\ndescription of its purpose, or applying an action as part of a\nsequential navigation of a multi-page website.\ntask where a model is given an HTML snippet\nand is asked to produce a natural language descrip-\ntion. For instance for an email ﬁeld, the description\nmight be “Please enter your email address.” Note\nthat in the majority of web pages, this connection\nbetween input elements and description content is\nonly implicit in the raw HTML code and inferring\nsuch links is a prerequisite for higher-level navi-\ngation objectives. The third task is Autonomous\nWeb Navigation (Shi et al., 2017). A model is pre-\nsented with an HTML page paired with a natural\nlanguage command and must apply appropriate ac-\ntions on a sequence of HTML pages to satisfy the\ncommand. See Figure 1 for a simpliﬁed example\nof these tasks.\nWith these benchmark tasks in hand, we eval-\nuate the transfer capabilities of a variety of pre-\ntrained LLMs (Table 1), varying in architecture\n(encoder-only, encoder-decoder, or decoder-only),\nmodel size (from 24.6M to 62B parameters), and\ntraining data corpora (both including and exclud-\ning pretraining NLP and HTML corpus). While\nprior work universally pre-parses the HTML as in-\nput to the model (Gur et al., 2021; Liu et al., 2018;\nNakano et al., 2021), ours uses raw, unprocessed\nHTML. Our results show that LLMs demonstrate\na remarkable level of HTML understanding across\nall tasks, with up to 192× more sample-efﬁciency\nthan models trained from scratch, and achieving a\nnew SoTA for supervised learning on the MiniWoB\nbenchmark suite (Shi et al., 2017). The encoder-\ndecoder architectures with bi-directional attention\nshow the best performance across the board even\nwhen their pretraining does not include HTML.\nThe broader objective of this research is to ad-\nvance the integration of LLMs with autonomous\nweb agents. It has only been in the last year that\nresearchers have begun to utilize LLMs outside of\nNLP and integrate them as core capabilities in au-\ntonomy ((Lu et al., 2021; Ahn et al., 2022)). In this\ncontext, LLMs are reasoning engines for sequential\ndecision making agents interacting with environ-\nments. We believe these contributions expand the\nscope of language models and connect their unique\ncapabilities with autonomous agents for the web.\nWe provide a new perspective on machine learn-\ning for HTML understanding and web automation,\nshowing that pretrained LLMs can achieve signiﬁ-\ncant performance on such tasks, reducing the need\nfor specialized architectures and training protocols.\nTo encourage further research in this direction, we\nopen sourced 2 model weights for agents used in\nthe WoB environment and our dataset for descrip-\ntion generation.\n2 Related Work\nHTML Understanding Autonomous web naviga-\ntion has been a popular application for neural net-\nwork models, and a variety of works propose simu-\nlated websites for training web-based agents, with\napplication to task fulﬁllment (Yao et al., 2022;\nGur et al., 2021; Burns et al., 2022; Mazumder and\nRiva, 2020; Shi et al., 2017; Liu et al., 2018) as\nwell as information retrieval or question-answering\n(Adolphs et al., 2021; Nogueira and Cho, 2016).\nSimulated websites provide an easy way to evalu-\nate models online, and for this reason we use the\nexisting MiniWoB benchmark (Shi et al., 2017) for\nour web navigation setting. However, it is still im-\nportant to have a mechanism for evaluating models\non a wide variety of real-world websites. This was\nthe key motivation for generating our own dataset\nfor the description generation task, which is dis-\ntilled and auto-labeled from CommonCrawl and is\na key contribution of our paper.\nAlongside these benchmarks, many works have\ndeveloped models for web navigation and related\nsubtasks (Pasupat et al., 2018; Bommasani et al.,\n2021; He et al., 2021; Gur et al., 2021; Humphreys\n2https://console.cloud.google.com/storage/\nbrowser/gresearch/webllm\n2804\net al., 2022; Liu et al., 2018; Jia et al., 2019). These\nworks often rely on specialized neural network ar-\nchitectures that leverage inductive biases of HTML\nstructure, or on preprocessing of HTML to make it\neasier to input to a model ((Li et al., 2021a,b)). In\ncontrast, our work takes a minimalist approach, pro-\nviding HTML in text form with minimal processing\nand using widely-adopted transformer networks.\nLLMs and HTML Works that explore the in-\ntersection of LLMs and HTML generally fall into\ntwo categories. The ﬁrst category uses LLMs to\nassist web navigation (Nakano et al., 2021; Yao\net al., 2022), and typically relies on a custom pre-\nprocessing to map the context and structure of a\nweb page to natural language, thus severely restrict-\ning what HTML pages the model can parse. The\nsecond category pretrains LLMs on a large corpora\nof HTML text (Aghajanyan et al., 2021). However,\nthese works typically restrict the model evaluation\nto standard NLP tasks, e.g., summarization and\nquestion/answering as opposed to tasks more rele-\nvant to HTML understanding and web automation.\nOur work can be thought of as the reverse: We\nkeep the pretraining of LLMs unchanged and focus\non the mechanisms for transferring the pretrained\nLLMs to HTML-relevant tasks.\n3 Canonical Tasks for HTML\nUnderstanding\nWe devise three canonical tasks to study HTML un-\nderstanding capabilities of LLM-based web agents.\nThese tasks require correctly interpreting both\nstructure and content to varying degrees to make\npredictions, with autonomous navigation being the\nmost challenging capability of the three.\nAutonomous Web Navigation. This task evalu-\nates how well a model navigates multi-page web-\nsites as a sequential decision-making problem (Shi\net al., 2017; Liu et al., 2018). At the beginning of\nan episode, the agent is given a natural language\ninstruction, e.g. Enter the username “lyda” and\nthe password “N22t” into the text ﬁelds and press\nlogin. The agent applies actions to a sequence of\nHTML pages, where each action is of the form\nfunction(selector, text). The function is\none of click or type, selectoris an integer pointer\nthat uniquely identiﬁes an element, and text is a\ntext to input if the type functionality is activated.\nAn episode terminates when either the page reaches\na terminal state (e.g., the ‘sign in’ button is clicked)\nor the maximum number of steps is reached.\nSemantic Classiﬁcation . Many HTML under-\nstanding applications require a model that can clas-\nsify HTML elements into standardized categories.\nFor example, in automated form-ﬁlling (Diaz et al.,\n2013; Gur et al., 2021), it is useful to identify a\n‘submit button’ across many websites (e.g., shop-\nping, ﬂight booking, utility application) with vari-\nous button representations (e.g., position, color, or\ntext). Thus, we formulate Semantic Classiﬁcation\nas classifying elements into role categories. Take\nthe example HTML in Figure 1 which includes\ntwo input elements and a submit button. Let’s\npick the ﬁrst input as an element of interest to be\nclassiﬁed by the system, also called a salient ele-\nment. The system should classify this element as\nusername, since it appears on a login page and it\nhas a label with Email Address which is typically\nassociated with the username in form-ﬁlling appli-\ncations. To solve this, the system can aggregate\ninformation from multiple sources in the page – the\nlabel that says Enter Email Address, the inputat-\ntributes (type=“email” and id=“uName”), or even\nthe ordering of other elements in the page such as\n‘password’ and ‘sign in’.\nDescription Generation . Motivated by appli-\ncations in accessibility-minded web browser con-\ntrol (Jorgensen and Binsted, 2005), we formulate\ndescription generation as an extractive problem\nwhere the goal is to locate the textual description\nof an element in the HTML and generate it as out-\nput. For instance, the description of the salient\nelement in Figure 1 is Enter Email Address; when\nrendered, this labelwill appear above the ‘email’\ninput ﬁeld. HTML provides a large amount of\nﬂexibility, and so in general a descriptive text that\nappears alongside a speciﬁc element when rendered\ncan be very far from that element when looking\nat the HTML plaintext. Thus, this task evaluates\na model’s ability to understand the structure of\nHTML as it would appear to a user, despite not\nhaving access to the rendered web page directly.\n4 Datasets\nEach of our canonical tasks requires a separate\ndataset, with the description generation task using\na newly contributed, auto-labelled dataset based on\nCommonCrawl.\nAutonomous Web Navigation. We use the 12K\ndemonstrations included in the publicly available\nMiniWoB benchmark (Shi et al., 2017), which\nencompass 62 website applications ranging from\n2805\nModel\nTask Dataset Size Input Architecture Output Task Output\nAutonomous Web Navigation MiniWoB Demos (Shi et al., 2017) 12K Page Enc-Dec Text DictionaryDec\nSemantic Classiﬁcation Annotated Shopping Webpages (Gur et al., 2021) 28K Snippet All Text Category\nDescription Generation CommonCrawl (new) 85K Snippet Enc-Dec Text TextDec\nTable 1: Task, dataset, and model summary. All models receive raw HTML.Autonomous Web Navigation receives the entire\nHTML, while the other tasks receive HTML snippets extracted given salient element.\nemail forwarding to social media interactions.\nEach demonstration is a sequence of (instruction,\nHTML, action) tuples. Every element in a Mini-\nWoB demonstration is accompanied by a reference\nnumber unique within its respective pages. This\nnumber can be used as an element selector, making\nthe action space uniﬁed across all tasks and time\nsteps. For instance, the action in Figure 1 would\nbe type(ref=5, \"username@email.com\"), where 5\nrefers to the index of the input when counted from\ntop-to-bottom. As model input, we concatenate\nthe natural language instruction and HTML into a\nsingle text input sequence. Similarly, we treat the\naction as a text sequence for the model to predict.\nSemantic Classiﬁcation . We use a dataset of\n28K labelled examples, containing 66 different cat-\negories, of the form (HTML, element, category),\npreviously used in the context of environment gen-\neration (Gur et al., 2021). The dataset consists of\nHTMLs from real-world shopping websites and\ncategories relevant to form-ﬁlling during payment\nand checkout on these websites.\nDescription Generation. For this task, we de-\nrive a dataset from CommonCrawl. 3 Common-\nCrawl does not include renderings or annotations\nthat would reveal what text in the HTML is as-\nsociated with which elements. Instead, we infer\ndescriptions of various elements by exploiting a\nspecial attribute in the HTML schema known as\nfor. As an example in Figure 1, the ﬁrst labelin\nthe HTML has a for attribute with value uName,\nwhich is the id of the element described by label;\nin this case, the id is that of the ﬁrst input in the\npage. This annotation does not affect the rendering\nof the page and is typically used for accessibil-\nity purposes. We utilize the information given by\nthese for attributes to create a large-scale dataset4\nto study description generation.\nSpeciﬁcally, we collected 100 WARC (from\nApril 2019) ﬁles from the CommonCrawl project\n3http://commoncrawl.org\n4https://console.cloud.google.com/storage/\nbrowser/gresearch/webllm/datasets/descgen\nand extracted all HTML labels that have a forat-\ntribute. Removing non-Unicode and alphanumeric\ntext in HTML labels results in a 400K example\ndatset. We balance the distribution of labels, effec-\ntively downsampling the dataset to 85K samples.\nEach example is represented as (HTML, element,\ndescription), where HTML is the HTML plain-\ntext of the page, element is the element whose\nidattribute matches that appearing in the label’s\nforattribute, and description is the text inside the\nlabel element (see example in Figure 1). More\ndetails of the dataset can be found in Appendix\nA.2.\n5 Pre-Processing\nIn treating HTML as token sequences, we minimize\nany HTML tree pre-processing prior to model input.\nWe thus provide HTML as raw text (i.e., sequences\nof text tokens) and only apply a snippet extraction\npre-processing for pages which are too large to ﬁt\ninto the typical LLMs context windows.\nSnippet Extraction. Real HTML pages can\ngrow extremely large, reaching thousands of el-\nements, far beyond the context window of the\nlargest LLM that we studied (1920 tokens in PaLM\n(Chowdhery et al., 2022)). LLMs typically truncate\nsuch long sequences, which can be detrimental to\nHTML understanding as HTMLs are not linearly\nstructured. We take an element-centric approach\nand extract HTML snippets (a small portion of\nHTML code) surrounding a salient element (Fig-\nure 5). A simple heuristic, which controls the tree’s\nwidth and depth, guides the process: Start with\na salient element and traverse its ancestors in the\nHTML tree until a stopping condition is satisﬁed.\nAs we traverse up, we estimate the height of the\ntree and the increased number of descendants of\nthe new root. We stop when either metric violates\na pre-deﬁned limit and take the resulting sub-tree\nas the snippet. We mark the salient element using a\nspecial attribute, calledtarget, to distinguish it from\nother elements. We perform the snippet extraction\n2806\nfor the semantic classiﬁcation and description gen-\neration datasets, and keep the full HTML pages in\nMiniWoB because these pages are typically much\nsmaller than real-world HTML.\nHTML un-Parsing. We provide the models\nwith the unparsed plaintext HTML in the form of\na sequence of tokens. This canonical representa-\ntion does not require speciﬁc model architectures\nsuch as hierarchical networks (Liu et al., 2018; Gur\net al., 2021) and can be fed into any LLM. We trans-\nform all datasets by converting every HTML page\nor snippet into a sequence. For MiniWoB, we ad-\nditionally concatenate (action history, instruction,\nHTML) tuples into a single sequence.\n6 Model Training\nWe study a variety of transformer-based\nLLMs (Vaswani et al., 2017) with different\nsizes and architectures for HTML understanding\ntasks (Table 1). In the rest of the text, we\npreﬁx models ﬁne-tuned for Autonomous Web\nNavigation, Description Generation, and Semantic\nClassiﬁcation with WebN-, WebD-, and WebC-,\nrespectively. For instance, WebD–T5-3B is the\nthree billion parameter T5 model (Raffel et al.,\n2020) ﬁne-tuned for the Description Generation\ntask. The rest of this section elaborates on training\ndetails.\nEncoder-Decoder and Decoder-only Models.\nWe train encoder-decoder models, i.e., T5 (Raf-\nfel et al., 2020), and decoder-only models, i.e.,\nLaMDA (Thoppilan et al., 2022) and PaLM\n(Chowdhery et al., 2022), with text input and text\noutput. Inputs are raw HTML pages or snippet\ntexts; similarly, outputs are categories, natural lan-\nguage descriptions, or actions represented as text.\nNamely, for Semantic Classiﬁcation we use the\ntextual representation of categories, similar to pre-\nvious classiﬁcation problems in NLP (Raffel et al.,\n2020). For Autonomous Web Navigation, actions\nare converted into text by ﬁrst converting them\ninto key and value pairs and then concatenating the\npairs.\nMany websites in MiniWoB require multiple\ninteractions, such as click-button-sequence or click-\ncheckboxes, where each interaction might cause a\nsubtle change in the website state. For instance, af-\nter clicking on a checkbox in the click-checkboxes\nwebsite, its value ﬂips from positive to negative\nor the other way around, which is not always re-\nﬂected in LLMs’ predictions and leads to action\nrepetitions. We solve this issue by augmenting tu-\nples in the dataset with a sequence of past actions,\n(action history, instruction, HTML, action), and\nallowing LLMs to learn from past experience.\nEncoder-only Models. We train encoder-only\nmodels, i.e., BERT (Devlin et al., 2018), with text\ninput and categorical output. We keep semantic\ncategories as discrete one-hot classes. To train\nencoder-only models, we add a new classiﬁcation\nlayer after the ﬁnal encoder layer to produce a dis-\ntribution over semantic categories. In addition to\nthe typical BERT models, we study MobileBERT\n(Sun et al., 2020), distilled from BERT-large with\ninverted bottlenecks, and Albert-XL (Lan et al.,\n2020), with parameter sharing and embedding split.\n7 Results\nWe now present the results of ﬁne-tuned LLMs\nfor HTML understanding. We compare the mod-\nels’ performance with the existing baselines where\npossible (autonomous web navigation) and against\nother LLM architectures and training regimes (all\ntasks). Sections 7.1, 7.2, and 7.3 evaluate task-\nspeciﬁc performance, while Section 7.4 assesses\nthe performance across all the tasks.\nMetrics: For autonomous web navigation we\nevaluate models’Success Rate, which is averaged\nover 100 episodes per task. For the other tasks,\nwe use Accuracy to measure exact match between\nprediction and ground truth. In the description gen-\neration task, we additionally provide evaluations us-\ning alternative ‘soft’ text evaluation metrics,BLEU\nand ROUGE-1, measuring the similarity between\npredicted and ground truth text.\n7.1 Autonomous Web Navigation Results\nFor Autonomous Web Navigation we ﬁne-tune two\nWebN- encoder-decoder architectures (WebN-T5-\nlarge and WebN-T5-3B) on 12k demonstrations\nfrom human-annotated real websites. We eval-\nuate the models on MiniWob (Liu et al., 2018)\nbenchmark, and compare with specialized archi-\ntectures trained using supervised learning (SL) on\n2.4 million human expert demonstrations CC-Net\n(SL) (Humphreys et al., 2022), and two RL models\nbootstrapped with SL, CC-Net (SL) (CC-Net (SL\n& RL) (Humphreys et al., 2022), and WGE (SL &\nRL) (Liu et al., 2018)). Additionally, we compare\nwith the decoder-only architecture (WebN-Lambda-\n1B) and perform an ablation study on the impact of\nincluding the action history in the input.\n2807\n2.4M\nDemos\n12K\nDemos\n(a) Baseline comparison.\nModel Name Success (%) Model Size\nT5-large 18.1 800M\nLaMDA-1B 15.6 1B\nT5-3B 11.1 3B\nWebN-T5-large 46.4 800M\nWebN-LaMDA-1B 48.8 1B\nWebN-T5-3B 51.8 3B\n(b) Pre-training effect.\nFigure 2: a) WebN–T5* performance compared to the pre-\nvious SOTA models on MiniWoB benchmark. WebN-T5-3B\nimproves the task success 16% while using 192 times less\ndata, compared to the best supervised learning (SL) model,\nCC-Net (SL). LLMs performance is only surpassed by works\nutilizing RL, requiring orders of magnitude more online expe-\nrience interaction with websites. b) LLMs with and without\npretraining on Autonomous Web Navigation task. Those with\npretraining (denoted by the ‘WebN-’ preﬁx) show a 2.5-4.5x\nperformance improvement.\nComparison to SoTA. Since previous works re-\nport success on only a subset of websites in Mini-\nWoB, we evaluate on 48 out of 62 websites that\nare common across all models. Table 8 in the\nAppendix reports ﬁne-grained results while Fig-\nure 2a presents results averaged over all websites.\nCompared to CC-Net (SL) which is trained on all\n2.4M demonstrations, WebN-T5-3B improves the\nsuccess 16% while only training on 12K publicly-\navailable demonstrations, yielding over 192x im-\nprovement in sample-efﬁciency. We ﬁnd that all\nchoices of LLMs outperform previous SL models.\nNotably, WebN-T5-3B signiﬁcantly improves on\nwebsites requiring multiple-action sequences such\nas click_checkboxes or websites requiring entering\ntext such as login_user (Table 8). We observe that\nthe performance of LLMs is only surpassed by pre-\nvious works utilizing RL, which require orders of\nmagnitude more online experience interaction. Ex-\ntending our ﬁne-tuned LLMs to an RL setting is a\npromising avenue for future work.\nAction history ablation. Across all LLMs we\nconsistently observe a decrease in success, on av-\nerage 6.4%, when past actions are excluded from\nthe inputs (Figure 2a). Action history helps with\nwebsites that require entering multiple texts, as\nwell as understanding minor changes that could\nModel Name Test (%) Dev (%) Model\nSize\nWebC-MobileBERT 78.1 77.7 24.6 M\nWebC-Albert-XL 83.5 83.1 58.9 M\nWebC-BERT-smallest 84.4 83.6 38.7 M\nWebC-BERT-small 84.4 85.2 52.8 M\nWebC-BERT-medium 85.2 84.5 67 M\nWebC-BERT-base 83.9 84.8 109.5 M\nWebC-BERT-large 84.1 85.8 335.2 M\nWebC-T5-base 86.8 89.9 250 M\nWebC-T5-large 87.0 89.3 800 M\nWebC-T5-3B 87.7 90.3 3 B\nWebC-LaMDA-1B (*) 87.4 87.1 1 B\nWebC-PaLM-8B (*) 86.6 89.9 8 B\nWebC-PaLM-62B (*) 88.7 90.5 62 B\nT5-large 76.4 75.2 800 M\nT5-3B 77.2 73.8 3 B\nPaLM-8B 73.3 70.1 8 B\nTable 2: LLMs performance on the Semantic Classiﬁcation\ntask. Fine-tuning off-the-shelf pretrained LLMs (model names\nwith preﬁx ‘Web’) helps LLMs transfer better compared to\ntraining the same architecture from scratch on the HTML\ndataset (model names without preﬁx ‘Web*’), improving the\naccuracy of PaLM-8B more than 12%. While WebC-PaLM-\n62B clearly performed better than all other models, we found\nWebC-T5-large to be competitive with much larger models\nsuch as WebC-LaMDA-1B or WebC-PaLM-8B. Models with\nan asterisk in their names utilize code in their training corpora.\nbe difﬁcult to detect (e.g. click_checkboxes and\nmulti_layout). multi_layout requires entering 3 dif-\nferent texts in the website where the layout is ran-\ndomized at each episode, yet, surprisingly, even the\n(relatively smaller) WebN-T5-large model without\naction history outperforms the CC-Net (SL) model;\nillustrating that incorporating action history is not\nthe only contributing factor for the better success.\n7.2 Semantic Classiﬁcation Task Results\nTo evaluate the Semantic Classiﬁcation task, we\ncompare the T5 encoder-decoder architecture’s\nthree size variants (WebC-T5-base, WebC-T5-\nlarge, and WebC-T5-3B) ﬁne-tuned on 22K real,\nhuman-labeled training websites. We compare with\na ﬁne-tuned encoder only architectures (WebC-\n*BERT*), three ﬁne-tuned decoder-only archi-\ntectures (WebC-LaMDA and PaLM), and both\nencoder-decoder and decoder-only models trained\non human labeled websites from scratch. Results\nare presented in Table-2, where we ﬁnd that all\nWebC-LLMs perform well and signiﬁcantly better\nthan the same architectures without pretraining.\nAccuracy per category. In Figure 4, we present\naccuracy distribution of the WebC-T5-3B model on\nthe development dataset. The ﬁne-tuned encoder-\ndecoder model performs strongly on a majority of\nthe categories (Figure 4), even on those with very\n2808\nNew Height Test (%) Dev (%)\ndescendants (%)\n25 3 87.7 90.3\n25 4 88.6 89.2\n50 3 88.4 90.0\n50 4 89.3 89.2\n300 5 87.8 88.8\n500 7 75.8 74.5\n(a)\nData Size\nAccuracy\n55\n60\n65\n70\n75\n80\n85\n500 1000 1500 2000\nWebC-PaLM WebC-T5-3B\nT5-3B (full data / no pretraining)\n(b)\nFigure 3: a) Effect of snippet extraction parameters on WebC-\nT5-3B. Increases above 50% in new descendants and height\nof 4. Large increases in both parameters lead to large snippets\nand decrease in accuracy. b) Accuracy over training data\nsize. Using only 1000 labeled examples (4.4% of all training\ndataset), WebC-T5-3B outperforms T5-3B (full data without\npretraining) which is trained on all available labeled data\n(approximately 30k examples), and outperforms WebC-PaLM-\n8B which is an order of magnitude larger.\nfew samples. For instance, the model is 100% accu-\nrate on password_new which has only 56 training\nexamples, because the class is unambiguous. On\nthe other hand, unsurprisingly, the performance\ndrops when the category is ambiguous, such as in\nthe email category which is frequently mistaken as\nusername.\nSnippet generation ablation. Two hyper-\nparameters govern snippet generation: percentage\nof new descendants and height of the new root.\nWhile small variations of both parameters do not\nchange the performance, increasing both degrades\nthe performance signiﬁcantly (Table 3a). With new\ndescendants up to 500% and height up to 7, the per-\nformance drops by more than 15%. Note that snip-\npet generation returns the full-page HTML when\nboth parameters increase indeﬁnitely.\nData size impact. When varying the ﬁne-tuning\ntraining data sizes (1, 5, 10, 20, or 50 samples per\nclass) in Figure 3b, WebC-T5-3B slightly outper-\nforms WebC-PaLM-8B which is an order of mag-\nnitude larger. Compared to T5-3B that is trained\non all available HTML data without pretraining,\nWebC-T5-3B achieves better performance while\nusing only 3.4% of labeled data (1000 samples),\nthus highlighting the beneﬁt of using standard off-\nthe-shelf pretrained LLMs for HTML understand-\ning.\n7.3 Description Generation Task Results\nFor Description Generation we split the Com-\nmonCrawl dataset based on URL top-level do-\nmains to test LLMs’ capabilities to generalize to\nunseen HTML. We ﬁne-tune encoder-decoder ar-\nchitectures (WebD–T5*) and decoder-only models\n(WebD–LaMDA*), with results presented in Ta-\nble 3. We also evaluate a strong heuristic baseline\nwhich simply ﬁnds the description closest to the\nsalient element in the HTML text (Closest Descrip-\ntion).\nAccuracy and Similarity Performance We\nshow results of our evaluations in Table 3. All mod-\nels achieve high scores across all metrics, achieving\n≈ 84% on the accuracy in terms of exact match and\na higher non-exact match score based on BLEU and\nROUGE-1 (≈ 91%). This difference indicates that\nthe models are capable of locating the descriptions,\nbut not always generating the exact output.\n7.4 HTML Understanding LLMs ’s\nPerformance Analysis Across Tasks\nWe now analyze our results in aggregate to derive\nour main conclusions.\n7.4.1 Pretraining Effect: Pretraining on\nLarge Text Corpora Matters\nFine-tuned pretrained LLMs outperform LLMs\ntrained on HTML-only data, improving the per-\nformance by more than 34.1% on the Autonomous\nWeb Navigation (Table 2b), and 10% to 12.7% on\nthe Semantic Classiﬁcation task (Table 2).\nSince Autonomous Web Navigationis the most\ndifﬁcult task, the improved performance is an en-\ncouraging evidence of the value of LLMs in HTML\nunderstanding tasks. Speciﬁcally, we observe that\nLLMs without pretraining are comparable to ﬁne-\ntuned pretrained models only on websites that re-\nquire simple text matching. In contrast, for web-\nsites such as click_checkboxes, text matching is\nharder and we ﬁnd that pretraining is key to good\nperformance. We also found that without pretrain-\ning, model outputs were frequently in an incorrect\nformat such as invalid dictionaries or invalid refs\nwith non-integer values. This suggests that the\nlarge corpora used for pretraining helps models to\nlearn general HTML structure.\n2809\nTest Dev\nModel Name Accuracy(%) BLEU ROUGE-1 Accuracy(%) BLEU ROUGE-1\nWebD-T5-large 83.2 90.2 90.5 84.3 91.7 91.5\nWebD-LaMDA-1B 83.3 87.5 90.2 84.3 88.6 91.2\nWebD-T5-3B 84 90.8 90.9 85.2 92.1 91.9\nClosest Description 57.4 24.4 59.2 60.8 23.9 62.1\nTable 3: Description generation accuracy of LLMs.\nCategories\nFigure 4: Accuracy per classiﬁcation category of the WebC-T5-3B model on the development dataset.\n7.4.2 Architecture Effect: T5-based Models\nPerform Best Across All Tasks\nEncoder-decoder T5 based models perform better\nacross all three tasks. On theAutonomous Web Nav-\nigation task, encoder-decoder (WebN-T5) architec-\ntures are better or comparable to WebN-LaMDA-\n1B (Figure 2a). On the Semantic Classiﬁcation, the\nsmallest encoder-decoder model (WebC-T5-base)\nperforms comparably to much larger decoder-only\nmodels (WebC-LaMDA-1B or WebC-PaLM-8B)\nand the largest encoder-only model (WebC-BERT-\nlarge) which has 85M more parameters (Table 2).\nWe also observe that decoder-only PaLM-8B per-\nforms worse than much-smaller encoder-decoder\nT5-large when trained only on HTML data. Finally,\non the Description Generation encoder-decoder\narchitecture has higher BLEU score.\nOne possible explanation for the strong perfor-\nmance of T5-based moels is the encoder-decoder\narchitecture of these models. Namely, T5 models\nutilize an encoder with a bidirectional attention\nmechanism, not present in the LaMDA and PaLM\ndecoders. The bidirectional attention mechanism\ncan process HTML pages from both ends, poten-\ntially overcoming the loss of information when\ntree-structured HTML pages are converted into a\nﬁxed linear text sequences.\n7.4.3 Model Size Effect: Size (Sub-linearly)\nMatters\nAcross the tasks it appears that the architecture\nplays an important role in the model performance.\nModel size and performance are also positively cor-\nrelated, although they reach diminishing returns.\nFor instance, the model performance is roughly\nO(log logn) with respect to model size on Seman-\ntic Classiﬁcation (Figure 6 in Appendix). On the\nAutonomous Web Navigation task, performance\ngrows slowly with the model size (Table 8), while\non the Description Generation it plateaus (Table 3).\n7.4.4 Error Analysis\nWe manually examined 50 errors of WebC-T5-3B\nmodel over the development set (Table 4) and as-\nsigned them into one of the 9 error types that we\ndevised. We found that 32% of the errors are due to\nlack of information in the HTML snippets, which is\nmainly the result of lost information during snippet\nextraction process. Annotation errors or email/user-\nname ambiguity make up 30% of the errors. These\ncan’t be improved without revising the annotated\ndata or adding extra information to resolve the am-\nbiguity. We also found that the model sometimes\npicks a more general category, or a nearby text mis-\nleads the model; the latter usually happens when\nthe HTML snippet is long where majority of the\nelements are noise.\n7.4.5 Few-Shot Prompting\nIn Table 5, we present few-shot prompting per-\nformance of a 540B PaLM model. We probe\nthe model using a prompt template <html> Role:\n<category> with 1 example per category and gen-\nerate categories using greedy-decoding. In our\n2810\nError Type Percentage of Examples\nNot enough information in the HTML snippet 30\nIncorrect annotation (ex: \"unknown_role\" instead of \"organization\") 12\nAnnotation tool translates user selection incorrectly 8\nEmail/Username ambiguity 10\nMore general category (ex: \"header\" instead of \"cart_header\") 8\nImmediate neighboring text misleads 8\nIncorrect date formatting (ex: \"mm\" instead of \"mmm\") 4\nNo information in the HTML snippet 2\nOthers 18\nTable 4: Types of errors over 50 manually examined examples. 32% of errors are due to lack of information in HTML snippets,\n30% of errors are related to annotations or can’t be improved due to ambiguity (email/username), and the remaining errors are\nincorrect predictions by the model.\nModel Name Test Dev\nPaLM-540B 64.2 60.3\n- w/o Example Cleaning 57.9 57.2\n- w/o Category Rewriting 52.1 50.7\n- w/o Dictionary Mapping 45.6 45.1\nTable 5: Few-shot prompting performance with different pre-\nand post-processing steps.\npreliminary experiments, we found that few-shot\nprompting achieves only 45.6 accuracy, much\nlower than a model ﬁne-tuned on the same data.\nWe found two common problems – the model is\nnot able to canonicalize predictions into categories\nand many of the examples are dropped due to con-\ntext length.\nWe developed post-processing methods to al-\nleviate the canonicalization problem and pre-\nprocessing methods to reduce lengths of examples.\nAdding a dictionary-based mapping on predictions\n– a manually curated paraphrase dictionary – im-\nproves the performance to 52.1. We also tried\nrewriting predictions by changing the order of to-\nkens around \"_\" such as name_ﬁrst to ﬁrst_name\nwhich further improved the performance to 57.9.\nFinally, we cleaned examples in the prompt by\nremoving certain elements such as \"svg\", \"path\",\n\"img\", and \"iframe\" and also removing class at-\ntribute from every element; this pre-processing step\ngives 64.2.\n8 Conclusion\nWe presented canonical tasks and ﬁne-tuned LLMs\nfor HTML understanding. The comprehensive eval-\nuations and analyses over a range of architectures,\ndataset sizes, and baselines yields practical ﬁndings\nand highlights current limitations of these mod-\nels. We ﬁnd that a) pretraining is critical for the\nperformance and can reduce labeled data require-\nments, improving sample efﬁciency up to 200x; b)\nmodel architecture is the second-most important\nfactor, and T5 models with bidirectional attention\nand encoder-decoder architecture perform the best\nacross the board; c) given a choice, model size\nshould be evaluated in the context of the model’s\ntraining and inference performance, as the model\nsize sub-linearly correlates with its performance.\nFinally, the proposed HTML understanding tasks\nhighlight the relatively short context window that\nlimits current LLMs, suggesting possibilities for\nfuture research that incorporate or eliminate this\nconstraint.\n9 Limitations\nOur experimental results are limited to relatively\nshort context windows. While HTML documents\ncan have 10s of thousands of tokens, LLMs that\nwe studied have only 2K context windows. We de-\nveloped heuristics to extract HTML snippets to ﬁt\ninto context windows which is a promising future\ndirection. We are also limited by the MiniWoB sim-\nulator for our web navigation experiments. While\nMiniWoB serves a wide variety of simulated web-\nsites, they still lack many of the complexities of\nreal websites.\nReferences\nLeonard Adolphs, Benjamin Boerschinger, Christian\nBuck, Michelle Chen Huebscher, Massimiliano Cia-\nramita, Lasse Espeholt, Thomas Hofmann, and Yan-\nnic Kilcher. 2021. Boosting search engines with in-\nteractive agents. arXiv preprint arXiv:2109.00527.\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\n2811\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-training\nand prompting of language models. arXiv preprint\narXiv:2107.06955.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, et al. 2022. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv\npreprint arXiv:2204.01691.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al.\n2021. Program synthesis with large language mod-\nels. arXiv preprint arXiv:2108.07732.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities\nand risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha\nKumar, Kate Saenko, and Bryan A Plummer. 2022.\nInteractive mobile app navigation with uncertain or\nunder-speciﬁed natural language commands. arXiv\npreprint arXiv:2202.02312.\nXingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji,\nDanyang Zhang, Ao Luo, Yuxuan Xiong, and Kai\nYu. 2021. WebSRC: A dataset for web-based struc-\ntural reading comprehension. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4173–4185, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nOscar Diaz, Itziar Otaduy, and Gorka Puente. 2013.\nUser-driven automation of web form ﬁlling. In In-\nternational Conference on Web Engineering , pages\n171–185. Springer.\nIzzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook\nChoi, Manoj Tiwari, Honglak Lee, and Aleksandra\nFaust. 2021. Environment generation for zero-shot\ncompositional reinforcement learning. Advances in\nNeural Information Processing Systems , 34:4157–\n4169.\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and\nDilek Hakkani-Tur. 2019. Learning to navigate the\nweb. In International Conference on Learning Rep-\nresentations.\nZecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying\nXu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner,\nRuby Lee, and Jindong Chen. 2021. Actionbert:\nLeveraging user actions for semantic understanding\nof user interfaces. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 35, pages\n5931–5938.\nPeter C Humphreys, David Raposo, Tobias Pohlen,\nGregory Thornton, Rachita Chhaparia, Alistair Mul-\ndal, Josh Abramson, Petko Georgiev, Adam San-\ntoro, and Timothy Lillicrap. 2022. A data-driven\napproach for learning to control computers. In In-\nternational Conference on Machine Learning, pages\n9466–9482. PMLR.\nSheng Jia, Jamie Ryan Kiros, and Jimmy Ba. 2019.\nDOM-q-NET: Grounded RL on structured language.\nIn International Conference on Learning Represen-\ntations.\nChuck Jorgensen and Kim Binsted. 2005. Web browser\ncontrol using emg based sub vocal speech recogni-\ntion. In Proceedings of the 38th Annual Hawaii In-\nternational Conference on System Sciences , pages\n294c–294c. IEEE.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nChenliang Li, Bin Bi, Ming Yan, Wei Wang, Song-\nfang Huang, Fei Huang, and Luo Si. 2021a. Struc-\nturallm: Structural pre-training for form understand-\ning. arXiv preprint arXiv:2105.11210.\nJunlong Li, Yiheng Xu, Lei Cui, and Furu Wei.\n2021b. Markuplm: Pre-training of text and markup\nlanguage for visually-rich document understanding.\narXiv preprint arXiv:2110.08518.\nZimeng Li, Bo Shao, Linjun Shou, Ming Gong, Gen\nLi, and Daxin Jiang. 2023. Wiert: Web information\nextraction via render tree. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 37(11):13166–\n13173.\n2812\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-\nlin Shi, and Percy Liang. 2018. Reinforcement learn-\ning on web interfaces using workﬂow-guided explo-\nration. arXiv preprint arXiv:1802.08802.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor\nMordatch. 2021. Pretrained transformers as\nuniversal computation engines. arXiv preprint\narXiv:2103.05247.\nSahisnu Mazumder and Oriana Riva. 2020. Flin: A\nﬂexible natural language interface for web naviga-\ntion. arXiv preprint arXiv:2010.12844.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nRodrigo Nogueira and Kyunghyun Cho. 2016. End-to-\nend goal-driven web navigation. Advances in neural\ninformation processing systems, 29.\nChristopher Olston, Marc Najork, et al. 2010. Web\ncrawling. Foundations and Trends® in Information\nRetrieval, 4(3):175–246.\nPanupong Pasupat, Tian-Shun Jiang, Evan Zheran Liu,\nKelvin Guu, and Percy Liang. 2018. Mapping nat-\nural language commands to web elements. arXiv\npreprint arXiv:1808.09132.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. 2023. Toolllm: Fa-\ncilitating large language models to master 16000+\nreal-world apis.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\nnandez, and Percy Liang. 2017. World of bits: An\nopen-domain platform for web-based agents. In In-\nternational Conference on Machine Learning, pages\n3135–3144. PMLR.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics.\nAssociation for Computational Linguistics.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, Boxi Cao, and Le Sun. 2023. Toolal-\npaca: Generalized tool learning for language models\nwith 3000 simulated cases.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kath-\nleen S. Meier-Hellstern, Meredith Ringel Morris,\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd H. Chi, and Quoc Le. 2022. Lamda: Language\nmodels for dialog applications. CoRR.\nDaniel Toyama, Philippe Hamel, Anita Gergely, Ghe-\norghe Comanici, Amelia Glaese, Zafarali Ahmed,\nTyler Jackson, Shibl Mourad, and Doina Precup.\n2021. Androidenv: a reinforcement learning plat-\nform for android. arXiv preprint arXiv:2105.13231.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable\nreal-world web interaction with grounded language\nagents. arXiv preprint arXiv:2207.01206.\nA Appendix\nA.1 Brief Background on HTML as\nSemi-Structured Text Data\nHTML is a markup language, used to organize\nweb page structure and content. Consider the ex-\nample HTML page in Figure 1. This web page\nincludes two adjacent input elements, one for e-\nmail and another for password, with their corre-\nsponding labels on a separate branch of the page.\nThese inputs and labels are one of many possi-\nble elements that serve as HTML building blocks.\n2813\nEach element has a set of attributes – key and value\npair – that describe the element’s content, such as\nstyle and human-readable text. When rendered in\na browser, these attributes will be responsible for\nhow the element is shown and where it is posi-\ntioned. In the example in Figure 1, the ﬁrst input\nhas three attributes, tag=\"input\", type=\"email\",\nand id=\"uName\", that identify the element as an\nemail input with an identiﬁer (“uName”) that can\nbe accessed programmatically.\nA.2 Dataset and Pre-Processing Details\nExamining the description distribution, we found\nthe original 400K dataset to be very skewed; only\n20 descriptions (such as Email and Password) were\ncovering 50% of the dataset. We sub-sampled the\ndataset so that each unique description has at most\n10 data points. We also found that for attributes\nare almost always deﬁned for HTML labels. This\ncould cause a model to overﬁt and just ﬁnd the\nlabelelement in the HTML and ignore everything\nelse. To avoid this sort of ‘cheating’ we replace the\ntags of HTML labels by randomly sampling from\n{div, span, a, label}. These tags are also fre-\nquently used to inject text in HTML but they are\nvery rarely used with forattributes. Finally, we re-\nmoved examples where there are only a single text\nin the HTML since models can trivially generate\ndescriptions by ﬁnding the only text in the HTML,\nwhich biases model weights and evaluation metrics.\nAfter this ﬁnal step, we have a total of85K labeled\nexamples.\nWe didn’t apply any special ﬁltering to keep\nonly text related information. We did minimal\npre-processing to ﬁlter some of the attributes from\nexcessively long inputs. For semantic classiﬁca-\ntion, we applied no ﬁltering on extracted snippets.\nFor description generation, we ﬁltered “class” and\n“style” attributes as we found them to increase the\nlength of the HTML documents signiﬁcantly. In\nMiniWoB, we applied no ﬁltering and used the\noriginal observation space provided by the environ-\nment.\nA.2.1 Snippet Generation\nIn Figure 5, we give a high-level overview of our\nsnippet generation procedure.\nA.3 Action Space for the Autonomous Agent\nWe set the number of tokens that our models can\ngenerate to 20. Our vocabulary consists of 32K\ntokens, which gives an initial estimate of 3200020\ncandidate generations at each step. While we don’t\nconstrain the vocabulary of our models during gen-\neration, it is important to note that output tokens\nin our dataset typically come from input HTML\ndocuments or user instructions (with formatting\ntokens such as \"{\", \":\", \"}\" and special \"click\",\n\"type\" tokens being exceptions). The models easily\nlearn these implicit constraints and assign much\nlower mass to tokens that are not in HTML docu-\nment or instruction. Additionally, they also quickly\nlearn the desired parsable format (\"<action type>,\n<target HTML element identiﬁer>, <instruction\nsubstring for typing actions>\") where the target\nHTML element speciﬁed in the action output is\nactually present in the input HTML (so it can be\nacted upon). We estimated the average statistics of\nthe public demonstrations that we used to train our\nmodels and found that there are 54 elements in a\ngiven HTML document and 35 instruction tokens,\non average. So our action space is approximately\n2 ∗ 54 ∗ 352 where there are 2 action types and 352\nnumber of substrings in an instruction.\nA.4 Sample Episodes from MiniWoB\nSee Table 6 for an example episode of web naviga-\ntion inferred by a ﬁne-tuned LLM.\nA.5 Detailed MiniWoB Results\nSee Table 7 for detailed performance of various\nmodels on MiniWob.\nA.6 Resource Requirements\nSee Table 8.\nA.7 Structure Dependence Ablation Study\nWe conducted an ablation study to examine the\nsensitivity of model performance to preserving\nstructural information. To do so, we evaluate the\nmodel’s performance on HTML input with critical\nstructure components removed. We kept the order\nof elements and their attributes ﬁxed while corrupt-\ning the nesting structure by removing closing tags.\nRemoving closing tags corresponds to a valid\ntraversal (BFS) and keeps the order of elements the\nsame as the text based input.\nAs a simple example:\n<div id=”form”><div><input id=”username”>\n</div></div>\nwould be converted into:\n<div id=”form”><div><input id=”username”>\n2814\nexpand\none level \nup\nsalient\nelements\nsnippet\ngeneration\n<html>\n   <body>\n      <form class=\"login-form\">\n         <div>\n            <label class=\"form-label\" for=”uName”>\n               Enter Email Address\n            </label>\n      <label class=\"form-label\" for=”pass”>\n               Enter Password:\n            </label>\n         </div>\n         <div>\n  <input type=\"email\" id=\"uName”>\n            <input type=\"password\" id=\"pass\">\n            <span class=\"hidden\">\n               Please enter your password.\n            </span>\n         </div>\n         <button type=\"submit\">Sign In</button>\n       </form>\n   </body>\n</html>\nHTML \n <input name=\"uName\">\n<input name=\"pass\">\n<button type=\"submit\">\n<input type=\"email\" id=\"uName”>\nif e xpand ab l e :\nexpand\n<div>\n  <input type=\"email\" id=\"uName”>\n  <input type=\"password\" id=\"pass\">\n  <span class=\"hidden\">\n     Please enter your password.\n  </span>\n</div>\notherwise\noutput\n<input type=\"email\"           \nid=\"uName” target>\nFigure 5: High-level overview of our pre-processing pipeline for generating snippets from a full HTML webpage. Given the\npage, we detect salient elements and for each one of them we extract snippets by recursively moving up in the HTML tree until a\nvalidation heuristic fails.\nFigure 6: Performance comparison w.r.t. increasing\nmodel size. As the model size increases, we observe\nan increase in overall accuracy with PaLM-62B model\nachieving the highest accuracy while being 7x larger\nthan PaLM-8B. We ﬁt two different curves using the\nfollowing functional forms: y = a ∗ loglog(n) +b and\ny = a∗log(n)+ b where n is the number of data points,\nand a and b are parameters to be learned. We found that\nthe average error was 3.75 and 4.78 for loglog and log\nﬁts, respectively.\nWe evaluated the trained WebN-T5-3B model\non the same set of synthetic websites from the\nMiniWoB benchmark with this aspect of structure\nremoved from the HTML pages. WebN-T5-3B\nachieves a 45.4% success rate, 6% lower than be-\nfore, suggesting that WebN-T5-3B is at least par-\ntially dependent on the DOM topology.\nA.8 Additional Related Works\nA.8.1 Task-speciﬁc Models\nAn alternative to LLMs is to adapt bespoke task-\nspeciﬁc architectures tailored towards processing\nof structured documents and HTML ((Li et al.,\n2021b,a)).\nStructuralLM ((Li et al., 2021a)) is an approach\nspeciﬁcally tailored for document understanding\n(i.e., combinations of images and text), and thus\nmakes several simplifying assumptions for its\nmodel that limit its applicability to HTML under-\nstanding (i.e., trees of elements with a richer struc-\nture and functionality). It is trained only on the\ntextual content of a document - the markup infor-\nmation is ignored. For example, any input ﬁeld or\ndropdown in a document would be missing from\nthe model inputs. All of the tasks we study re-\nquire knowledge of this information. For example,\nin autonomous navigation the model needs to in-\nteract with input elements (e.g. text, checkboxes,\ndropdowns) such as username and password in the\nlogin-user task in MiniWoB. Typically, a “type” ac-\ntion with a reference to an element and a text argu-\nment is generated by the model. Without knowing\nwhich input elements are available in the page, it\nis impossible to generate a reference to any input\nelement.\nWhile MarkupLM ((Li et al., 2021b)) is better\ntailored for understanding HTML pages, it has sim-\nilar drawbacks as StructuralLM in that it focuses\nsolely on text and structure of text while ignor-\ning everything else in the markup. To illustrate\nour point better, we used the open source imple-\nmentation of MarkupLM from the HuggingFace\nlibrary ((Wolf et al., 2019)) to process the sam-\nple HTML snippet in Figure-1. The MarkupLM\nignores all input elements, both username and pass-\nword, and generates <s>Email AddressEnter Pass-\nword:Please enter your password.</s> which is\nthe text input to the MarkupLM Transformer (Fig-\nure 7). Classifying this text as username or pass-\nword is not possible without the additional context\non which input element is the salient element (in\nthis context it is the username).\nMarkupLM is also evaluated on NLP-like tasks\nsuch as QA or entity classiﬁcation where under-\nstanding page content is paramount, whereas we\nfocus on HTML understanding tasks such as au-\ntonomous navigation where both content and the\npage’s layout structure need to be understood.\n2815\nWe perform a quantitative evaluation of Marku-\npLM on our tasks to understand how signiﬁcant\nthese limitations are. We ﬁne-tune the MarkupLM-\nbase model on the semantic classiﬁcation task, us-\ning the same setup as other WebC models but with\nthe suggested hyperparameters from ((Li et al.,\n2021b)). We use the MarkupLM implementa-\ntion from the HuggingFace library ((Wolf et al.,\n2019)). On development and test sets, MarkupLM-\nbase achieves 65% and 66% accuracy, respectively.\nThese results are more than 16% lower compared\nto similar size WebC-BERT-base results that we\nreport in our work. This suggests that although do-\nmain speciﬁc models may be suitable for process-\ning HTML for NLP tasks, the generality, ﬂexibility,\nand sample efﬁciency LLMs provide advantages\nfor autonomous navigation tasks.\nA.8.2 Web Information Extraction\nWebSRC (Chen et al., 2021) introduced a QA task\nfrom web documents. The authors manually cu-\nrated a small set of seed questions (460 questions)\nwhich are used to collect paraphrases and do data\naugmentation. Similar to DescriptionGeneration,\nthe problem is to ﬁnd the relevant text in a web doc-\nument. While DescriptionGeneration is focused\nmore on elements and fully automated, WebSRC\nis a QA task that requires manual annotation.\nWIERT (Li et al., 2023) studies information ex-\ntraction from web documents. They focus on uti-\nlizing simpliﬁed render trees (only text, tag, and\nstyle information are kept) to classify DOM nodes\ninto product categories. They traverse the render\ntree to generate a sequence of element tokens and\nencode the sequence with a pretrained language\nmodel; style nodes are independently encoded and\nconcatenated to the element encodings. The result-\ning encodings are used in a multi-objective setup to\ntrain the model. In contrast, our models are trained\nin a uniﬁed setup with a single objective, and using\noriginal HTML documents with no special ﬁltering\nrequired to extract informative elements.\nA.8.3 Tool Use\nMore recently, tool-using agents emerged as a way\nto augment LLMs’ capabilities with external tools.\nToolAlpaca (Tang et al., 2023) and TOOLLLM\n(Qin et al., 2023) are two examples that study multi-\nstep tool-use with iterated API calls. Actions as\nwell-deﬁned API interfaces can be useful to sim-\nplify and reduce the dimensionality of the interface\nbetween the model and environment. This in turn\nenables a degree of horizontal scaling and gener-\nalization as studied by these works. However, this\nﬂexibility is only achieved when the environment\nimplements an API interface. Where there are none\navailable, navigating web interfaces becomes cru-\ncial to unlock access to further information.\n2816\nfrom transformers import MarkupLMProcessor\nprocessor = MarkupLMProcessor.from_pretrained(f\"microsoft/markuplm-base\")\nsnippet = /quotesingle.Var/quotesingle.Var/quotesingle.Var<div><label class=\"form-label\" for=”uName”>Email Address\n</label><label class=\"form-label\" for=”pass”>Enter Password:\n</label></div><div><input type=\"email\" id=\"uName” target><input\ntype=\"password\" id=\"pass\"><span class=\"hidden\">Please enter your password.\n</span></div>/quotesingle.Var/quotesingle.Var/quotesingle.Var\nencoding = processor(snippet)\nprint(processor.batch_decode(encoding[\"input_ids\"]))\nFigure 7: Code snippet to reproduce MarkupLM preprocessing result. Encoding the example HTML code with the\nMarkupLM preprocessor will ignore all input elements.\n2817\nTable 6: A sample web page and corresponding episode using the T5-3B model. At each time step, previous\nactions, instruction, and HTML are concatenated into a single HTML text. Note that at the beginning of episode,\nthere is no past actions and we simply concatenate instruction and HTML. Action is generated as a sequence of\ntokens which is later parsed into a dictionary. Theref in the action points to an element that has aref attribute with\nthe same value. For instance, at the beginning of episode, ref: 6 corresponds to an input with ref=6. At the end of\nthe episode, the model clicks on the submit button and the episode terminates.\nWeb page\nHTML Text Action Text\n{action: click, ref: 6}\n{action: click, ref: 10}\n{action: click, ref: 12}\n{action: click, ref: 14}\n2818\n{action: click, ref: 16}\n{action: click, ref: 17}\n2819\nTable 7: Success rate comparison of various models in MiniWoB tasks. Baseline results are borrowed from (Humphreys et al.,\n2022). Note that these are normalized between 0 and 1.\nTASK Human CC-NetCC-NetWorldWorkﬂowLearningDOM-Q-NetWorkﬂowLearningAggregatedAggregatedWebN-T5-3BWebN-T5-3B(SL & RL)(SL) of guided to (RL) guided to SOTA SOTA(no history) bits explorationnavigate explorationnavigate(SL & RL)(Augmented)(SL & RL)(SL & RL)the web (Augmented)the web(RL) (Augmented)bisect-angle0.92 n/a n/a 0.97 0.29 0.8 n/a n/a n/a n/a n/a 0.8 0.8book-ﬂight 0.87 0 0 0.87 0 0 0 n/a n/a 0 1 0 1chase-circle0.82 n/a n/a 0.93 0.8 1 n/a n/a n/a n/a n/a 1 1choose-date-easy0.99 0.03 0.05 0.99 0.42 n/a n/a n/a n/a n/a n/a n/a n/achoose-date-medium0.98 0 0 0.99 0.26 n/a n/a n/a n/a n/a n/a n/a n/achoose-date0.97 0 0 0.97 0.12 0 0 n/a 1 0 n/a 1 1choose-list 0.98 0.26 0.14 0.99 0.19 0.25 0.16 0.26 n/a 0.16 0.26 0.26 0.26circle-center0.96 n/a n/a 0.97 0.36 0.98 n/a n/a n/a n/a n/a 0.98 0.98click-button-sequence0.94 1 1 1 0.47 0.22 0.99 n/a 1 1 n/a 1 1click-button0.98 1 0.96 1 0.78 0.62 1 1 1 1 1 1 1click-checkboxes-large0.87 0.22 0 0.71 0 n/a 0.68 n/a n/a 0.84 n/a 0.68 0.84click-checkboxes-soft0.73 0.54 0.43 0.95 0.04 n/a 0.51 n/a n/a 0.94 n/a 0.51 0.94click-checkboxes-transfer0.98 0.63 0.34 0.99 0.36 n/a 0.64 n/a n/a 0.64 n/a 0.64 0.64click-checkboxes0.97 0.96 0.84 0.98 0.32 0.48 0.98 n/a 1 1 n/a 1 1click-collapsible-20.97 0 0.01 0.98 0.17 0.11 0.65 n/a n/a 0.99 n/a 0.65 0.99click-collapsible0.99 0 0.01 1 0.81 0.98 1 1 n/a 1 1 1 1click-color 0.97 0.27 0.23 1 0.82 0.23 1 n/a n/a 1 n/a 1 1click-dialog-20.99 0.24 0.35 1 0.88 0.53 1 n/a n/a 1 n/a 1 1click-dialog 1 1 1 1 0.95 1 1 1 1 1 1 1 1click-link 0.99 1 0.96 0.99 0.59 0.31 1 1 1 1 1 1 1click-menu-20.98 n/a n/a 0.83 0.52 0.16 n/a n/a n/a n/a n/a 0.16 0.16click-menu0.97 0.37 0.38 0.94 0.22 0.13 n/a n/a n/a n/a n/a 0.13 0.13click-option0.99 0.87 0.78 0.99 0.21 0.28 1 n/a 1 1 n/a 1 1click-pie 0.98 0.51 0.14 0.97 0.15 0.15 0.32 1 n/a 0.32 1 1 1click-scroll-list0.91 0 0 0.6 0.01 0.07 n/a n/a n/a n/a n/a 0.07 0.07click-shades0.91 0 0 1 0.04 0.27 0.22 n/a n/a 0.99 n/a 0.27 0.99click-shape0.88 0.53 0.54 0.95 0.11 0.11 0.64 n/a n/a 0.64 n/a 0.64 0.64click-tab-2-easy0.99 n/a n/a 0.99 0.61 n/a n/a n/a n/a n/a n/a n/a n/aclick-tab-2-hard0.96 0.12 0.13 0.98 0.19 n/a n/a n/a n/a n/a n/a n/a n/aclick-tab-2-medium0.97 n/a n/a 0.99 0.54 n/a n/a n/a n/a n/a n/a n/a n/aclick-tab-2 0.97 0.18 0.09 0.98 0.27 0.08 0.64 n/a 1 0.98 n/a 1 1click-tab 0.99 0.74 1 1 0.95 0.97 0.55 1 1 1 1 1 1click-test-20.99 1 1 1 0.95 0.83 1 n/a 1 1 n/a 1 1click-test-transfer0.99 n/a n/a 1 0.94 n/a n/a n/a n/a n/a n/a n/a n/aclick-test 1 1 1 1 1 1 1 n/a 1 1 n/a 1 1click-widget0.83 1 0.97 1 0.56 0.34 0.93 n/a 1 0.93 n/a 1 1copy-paste-20.94 n/a n/a 0.63 0.01 0 n/a n/a n/a n/a n/a 0 0copy-paste 0.94 n/a n/a 0.79 0.04 0 n/a n/a n/a n/a n/a 0 0count-shape0.82 0.41 0.43 0.85 0.21 0.18 0.59 n/a n/a 0.76 n/a 0.59 0.76count-sides0.98 n/a n/a 1 0.74 0.3 n/a n/a n/a n/a n/a 0.3 0.3drag-box 0.99 n/a n/a 1 0.61 0.31 n/a n/a n/a n/a n/a 0.31 0.31drag-cube 0.99 n/a n/a 0.79 0.23 0.18 n/a n/a n/a n/a n/a 0.18 0.18drag-item 0.98 n/a n/a 1 0.61 n/a n/a n/a n/a n/a n/a n/a n/adrag-items-grid0.87 n/a n/a 0.98 0.05 0.01 n/a n/a n/a n/a n/a 0.01 0.01drag-items 0.93 n/a n/a 0.99 0.13 0.41 n/a n/a n/a n/a n/a 0.41 0.41drag-shapes0.96 n/a n/a 0.99 0.26 0.92 n/a n/a n/a n/a n/a 0.92 0.92drag-sort-numbers0.92 n/a n/a 0.97 0.11 0.66 n/a n/a n/a n/a n/a 0.66 0.66email-inbox-delete0.99 n/a n/a 1 0.22 n/a n/a n/a 1 n/a n/a 1 1email-inbox-forward-nl-turk0.88 0.33 0.09 1 0 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-forward-nl0.91 0.60 0.09 1 0 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-forward0.96 n/a n/a 1 0.01 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-important0.99 n/a n/a 1 0.3 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-nl-turk0.93 0.23 0.26 1 0.05 n/a 0.77 n/a n/a 0.93 n/a 0.77 0.93email-inbox-noscroll0.96 n/a n/a 1 0.13 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-reply0.91 n/a n/a 1 0 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox-star-reply0.95 n/a n/a 1 0.11 n/a n/a n/a n/a n/a n/a n/a n/aemail-inbox0.96 0.38 0.21 1 0.09 0.03 0.43 n/a 0.54 0.99 n/a 0.54 0.99enter-date 0.97 0 0 1 0.02 0.61 0 1 n/a 0.96 1 1 1enter-password0.96 0.97 0.92 1 0.02 0 0.99 1 1 1 1 1 1enter-text-20.91 n/a n/a 0.98 0.04 0 n/a n/a n/a n/a n/a 0 0enter-text-dynamic0.97 0.98 0.92 1 0.39 1 1 1 1 1 1 1 1enter-text 0.98 0.89 0.99 1 0.35 0 1 n/a 1 1 n/a 1 1enter-time 0.98 0 0.01 0.97 0.04 0.08 0.52 n/a n/a 0.9 n/a 0.52 0.9ﬁnd-midpoint0.94 n/a n/a 0.97 0.35 0.31 n/a n/a n/a n/a n/a 0.31 0.31ﬁnd-word 0.96 n/a n/a 0.88 0.05 0 n/a n/a n/a n/a n/a 0 0focus-text-20.99 1 1 1 0.96 0.83 1 n/a 1 1 n/a 1 1focus-text 1 1 1 1 0.99 0.95 1 n/a 1 1 n/a 1 1grid-coordinate0.87 0.49 0.42 1 0.66 0.26 1 n/a n/a 1 n/a 1 1guess-number0.99 0 0 1 0.21 0.2 0 n/a n/a 0 n/a 0.2 0.2highlight-text-20.97 n/a n/a 1 0.4 0.13 n/a n/a n/a n/a n/a 0.13 0.13highlight-text0.97 n/a n/a 1 0.51 0.9 n/a n/a n/a n/a n/a 0.9 0.9identify-shape0.98 0.88 0.89 1 0.68 0.36 0.9 n/a n/a 1 n/a 0.9 1login-user-popup0.94 0.72 0.40 1 0.02 n/a n/a n/a n/a n/a n/a n/a n/alogin-user 0.96 0.82 0.64 1 0 0 0.99 1 1 1 1 1 1moving-items0.18 n/a n/a 0.88 0.13 0.78 n/a n/a n/a n/a n/a 0.78 0.78multi-layouts0.95 0.83 0.48 1 0 n/a 0.99 n/a n/a 1 n/a 0.99 1multi-orderings0.96 0.88 0.64 1 0 n/a 0.05 n/a n/a 1 n/a 0.05 1navigate-tree0.98 0.91 0.99 0.99 0.32 0.2 0.99 1 1 0.99 1 1 1number-checkboxes0.96 n/a n/a 0.99 0 0.16 n/a n/a n/a n/a n/a 0.16 0.16read-table-20.95 n/a n/a 0.94 0 0 n/a n/a n/a n/a n/a 0 0read-table 0.97 n/a n/a 0.97 0.01 0 n/a n/a n/a n/a n/a 0 0resize-textarea0.94 n/a n/a 1 0.27 0.11 n/a n/a n/a n/a n/a 0.11 0.11right-angle 0.87 n/a n/a 0.98 0.26 0.38 n/a n/a n/a n/a n/a 0.38 0.38scroll-text-20.97 n/a n/a 1 0.88 0.96 n/a n/a n/a n/a n/a 0.96 0.96scroll-text 0.97 n/a n/a 0.96 0.04 0 n/a n/a n/a n/a n/a 0 0search-engine0.97 0.34 0.34 1 0.15 0 0.26 n/a 1 0.99 n/a 1 1simon-says0.62 n/a n/a 0 0.02 0.28 n/a n/a n/a n/a n/a 0.28 0.28simple-algebra0.86 n/a n/a 0.75 0.03 0.04 n/a n/a n/a n/a n/a 0.04 0.04simple-arithmetic0.96 n/a n/a 0.86 0.38 0.07 n/a n/a n/a n/a n/a 0.07 0.07social-media-all0.89 0 0 0.75 0 n/a 0.01 n/a n/a 0.01 1 0.01 1social-media-some0.91 0.02 0 0.85 0.01 n/a 0.01 n/a n/a 0.42 n/a 0.01 0.42social-media0.96 0.21 0.24 0.9 0.03 0.23 0.39 n/a 1 1 n/a 1 1terminal 0.88 n/a n/a -0.01 0 0 n/a n/a n/a n/a n/a 0 0text-editor 0.88 n/a n/a 0.98 0.11 0.01 n/a n/a n/a n/a n/a 0.01 0.01text-transform0.86 n/a n/a 0.6 0.19 0 n/a n/a n/a n/a n/a 0 0tic-tac-toe 0.71 0.48 0.40 0.83 0.32 0.34 0.37 n/a n/a 0.47 n/a 0.37 0.47unicode-test0.99 n/a n/a 1 0.86 n/a n/a n/a n/a n/a n/a n/a n/ause-autocomplete0.98 0.22 0.15 1 0.07 0 0.78 n/a n/a 0.98 n/a 0.78 0.98use-colorwheel-20.94 n/a n/a 0.95 0.38 1 n/a n/a n/a n/a n/a 1 1use-colorwheel0.9 n/a n/a 0.98 0.68 1 n/a n/a n/a n/a n/a 1 1use-slider-20.97 n/a n/a 0.95 0.03 0.15 n/a n/a n/a n/a n/a 0.15 0.15use-slider 0.98 n/a n/a 0.91 0.18 0.51 n/a n/a n/a n/a n/a 0.51 0.51use-spinner0.98 0.07 0.05 1 0.47 0.17 0.04 n/a n/a 0.04 n/a 0.17 0.17visual-addition0.97 n/a n/a 0.99 0.36 0.01 n/a n/a n/a n/a n/a 0.01 0.01\n2820\nTable 8: Resource requirements and running time of LLMs.\nModel NameModel SizeTPU versionBatch sizeInput sequence lengthExamples per sec (training)Examples per sec (inference)\nPaLM 62B TPU v4 8 1920 9.313 30.51\nPaLM 8B TPU v4 32 1920 64.4 184.3\nT5 3B TPU v4 128 512 163.8 734.5\nLaMDA 1B TPU v2 128 512 363.1 1416\n2821",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7347526550292969
    },
    {
      "name": "Natural language processing",
      "score": 0.4467617869377136
    },
    {
      "name": "Programming language",
      "score": 0.4093831181526184
    }
  ],
  "institutions": [],
  "cited_by": 24
}