{
    "title": "Protein stability prediction by fine-tuning a protein language model on a mega-scale dataset",
    "url": "https://openalex.org/W4400878092",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5005194141",
            "name": "Simon K. S. Chu",
            "affiliations": [
                "University of California, Davis"
            ]
        },
        {
            "id": "https://openalex.org/A5111360993",
            "name": "Kush Narang",
            "affiliations": [
                "University of California, Davis"
            ]
        },
        {
            "id": "https://openalex.org/A5076521371",
            "name": "Justin B. Siegel",
            "affiliations": [
                "University of California, Davis"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4291670558",
        "https://openalex.org/W3213779636",
        "https://openalex.org/W4390030732",
        "https://openalex.org/W3087371631",
        "https://openalex.org/W4386861232",
        "https://openalex.org/W2913078802",
        "https://openalex.org/W2025670719",
        "https://openalex.org/W2153457180",
        "https://openalex.org/W2538176089",
        "https://openalex.org/W2023490488",
        "https://openalex.org/W3163308783",
        "https://openalex.org/W2031553026",
        "https://openalex.org/W2912783011",
        "https://openalex.org/W2307896283",
        "https://openalex.org/W2149580316",
        "https://openalex.org/W2167775670",
        "https://openalex.org/W2896735041",
        "https://openalex.org/W2956094221",
        "https://openalex.org/W2121519777",
        "https://openalex.org/W2120949690",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W4386154677",
        "https://openalex.org/W4376612841",
        "https://openalex.org/W3120180752",
        "https://openalex.org/W2943495267",
        "https://openalex.org/W3088578860",
        "https://openalex.org/W2099258152",
        "https://openalex.org/W2795970199",
        "https://openalex.org/W3093556707",
        "https://openalex.org/W4384820649",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W1988395729",
        "https://openalex.org/W3037277178",
        "https://openalex.org/W1981063141",
        "https://openalex.org/W2058764769",
        "https://openalex.org/W2759860792",
        "https://openalex.org/W3044773187",
        "https://openalex.org/W2161160141",
        "https://openalex.org/W2111402254",
        "https://openalex.org/W4210840673",
        "https://openalex.org/W2010700017",
        "https://openalex.org/W2120139937",
        "https://openalex.org/W4379966447",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W3012078071",
        "https://openalex.org/W2031940323",
        "https://openalex.org/W1493541064",
        "https://openalex.org/W3000724807",
        "https://openalex.org/W2949389269",
        "https://openalex.org/W3205136345",
        "https://openalex.org/W2950629294",
        "https://openalex.org/W2735621019",
        "https://openalex.org/W2950954328",
        "https://openalex.org/W4281648132",
        "https://openalex.org/W4389472984",
        "https://openalex.org/W3216341763",
        "https://openalex.org/W4375858802",
        "https://openalex.org/W4399859005",
        "https://openalex.org/W3211728297",
        "https://openalex.org/W3146944767"
    ],
    "abstract": "Protein stability plays a crucial role in a variety of applications, such as food processing, therapeutics, and the identification of pathogenic mutations. Engineering campaigns commonly seek to improve protein stability, and there is a strong interest in streamlining these processes to enable rapid optimization of highly stabilized proteins with fewer iterations. In this work, we explore utilizing a mega-scale dataset to develop a protein language model optimized for stability prediction. ESM therm is trained on the folding stability of 528k natural and de novo sequences derived from 461 protein domains and can accommodate deletions, insertions, and multiple-point mutations. We show that a protein language model can be fine-tuned to predict folding stability. ESM therm performs reasonably on small protein domains and generalizes to sequences distal from the training set. Lastly, we discuss our model’s limitations compared to other state-of-the-art methods in generalizing to larger protein scaffolds. Our results highlight the need for large-scale stability measurements on a diverse dataset that mirrors the distribution of sequence lengths commonly observed in nature.",
    "full_text": "RESEA RCH ARTICL E\nProtein stability prediction by fine-tuning a\nprotein language model on a mega-scale\ndataset\nSimon K. S. Chu\nID\n1\n, Kush Narang\n2\n, Justin B. Siegel\nID\n3,4,5\n*\n1 Biophysics Graduate Program, University of California Davis, Davis, California, United States of America,\n2 College of Biological Sciences, Univers ity of Californi a Davis, Davis, Californi a, United States of America,\n3 Genom e Center, Univers ity of Californi a Davis, Davis, California, United States of America, 4 Department of\nChemis try, University of California Davis, Davis, Californi a, United States of America, 5 Department of\nBiochemi stry and Molecular Medicine, Univers ity of Californi a Davis, Davis, California, United States of\nAmerica\n* jbsiegel @ucdavis .edu\nAbstract\nProtein stability plays a crucial role in a variety of applications, such as food processing,\ntherapeutics, and the identification of pathogenic mutations. Engineering campaigns com-\nmonly seek to improve protein stability, and there is a strong interest in streamlining these\nprocesses to enable rapid optimizatio n of highly stabilized proteins with fewer iterations. In\nthis work, we explore utilizing a mega-scale dataset to develop a protein language model\noptimized for stability prediction. ESM\ntherm\nis trained on the folding stability of 528k natural\nand de novo sequences derived from 461 protein domains and can accommodate deletions,\ninsertions, and multiple-point mutations. We show that a protein language model can be\nfine-tuned to predict folding stability. ESM\ntherm\nperforms reasonably on small protein\ndomains and generalizes to sequences distal from the training set. Lastly, we discuss our\nmodel’s limitations compared to other state-of-the-art methods in generalizin g to larger pro-\ntein scaffolds. Our results highlight the need for large-scale stability measurements on a\ndiverse dataset that mirrors the distribution of sequence lengths commonly observed in\nnature.\nAuthor summary\nResearch in Professor Justin Siegel’s lab focuses on discovering and engineering enzyme\ncatalysis. His work follows a design-build-test cycle, integrating computational protein\nmodeling with wet-lab experiments. Key areas of his research include de novo enzyme\ndesign, enzyme therapeutics for celiac disease, and applications in food and renewable\nenergy. Additionally, his lab has developed the Design2Data program, a multi-year, multi-\ncampus effort to curate a high-quality dataset of enzymatic activity and stability for beta-\nglucosidase.\nUnder the supervision of Professor Justin Siegel, I am engaged in molecular modeling\nand machine learning in protein engineering. I have a background in molecular dynamics\nPLOS COMP UTATIONAL  BIOLOGY\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 1 / 13\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Chu SKS, Narang K, Siegel JB (2024)\nProtein stability prediction by fine-tuning a protein\nlanguage model on a mega-scale dataset. PLoS\nComput Biol 20(7): e1012248. https://do i.org/\n10.1371/ journal.pcbi.10 12248\nEditor: Piero Fariselli, Universita degli Studi di\nTorino, ITALY\nReceived: December 11, 2023\nAccepted: June 13, 2024\nPublished: July 22, 2024\nCopyright: © 2024 Chu et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The code and\nmaterials are maintaine d on https://githu b.com/\nSimonKitSa ngChu/Esm Therm. Mutant-le vel\npredictions from our model and benchmark\nevaluation of state-of -the-art are available under\nSupplement ary Materials.\nFunding: The author(s) received no specific\nfunding for this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nsimulations for protein and cell membrane permeability estimation and in using the\nRosetta molecular modeling suite for protein structure modeling and enzyme-substrate\ninteraction. Current research topics include the prediction of mutational effects on pro-\ntein functions and protein language models for functional prediction and protein design.\nIntroduction\nProtein stability is one of the foundations of protein engineering to design resilient proteins\nfor industrial processes and therapeutic manufacturing [1–3]. Beyond protein engineering,\ndestabilizing mutations are associated with pathogenicity, and stability predictors can help\nidentify pathogenic mutations across human proteome [4–7]. Molecular modeling methods,\nincluding Rosetta [8, 9], FoldX [10], and molecular dynamics simulations [11], have been\nshown to predict the impact of mutation on protein stability. More recently, the use of\nmachine learning models grounded in biophysical features and evolutionary statistics [12–20]\nhas offered an alternative approach to stability and function prediction without the need for\ncomputationally intensive molecular modeling simulations. Fueled by the latest advances in\ndeep learning, convolutional neural networks (CNNs) [21] and graph neural networks\n(GNNs) [22] are now being adopted to predict mutational impacts on stability by operating\ndirectly on the input protein structure [23, 24]. For example, RaSP is a CNN-based model\ntrained on top of Rosetta [25], while ELASPIC-2, another stability predictor, operates on both\nsequence embedding from ESM and structural embedding from GNN [26–28].\nDespite these advancements, the lack of a consistent and universal dataset remains an\nobstacle. While merging smaller datasets into a more comprehensive collection, such as\nProTherm [29], ProtaBank [30] and ThermoMutDB [31], is a feasible approach, combined\ndatasets often consist of closely related but distinct quantities accompanied by additional dis-\ncrepancies in experimental conditions. While deep mutagenesis scanning (DMS) offers pro-\nfound insights, these studies typically focus on a single protein target, limiting the broader\napplicability of the derived data and models subsequently trained on these datasets. In light of\nthese challenges, Tsuboyama et al. introduced a mega-scale thermostability dataset, encom-\npassing 776k short protein sequences derived from 479 small protein domains, all consistently\nevaluated using the same assay [32].\nUtilizing this dataset, we fine-tuned a protein language model (pLM), named ESM\ntherm\n,\nfrom ESM-2 [33] to act as an end-to-end stability predictor. We observe that ESM\ntherm\nper-\nforms comparably with state-of-the-art models and generalizes to small protein sequences\ndistal to those of the training set. We also demonstrate that training on an ensemble of protein\ndomains, instead of mutagenesis studies of a single domain, improves the performance of the\nfine-tuned protein language model for folding stability prediction. Lastly, we discuss the limi-\ntations of ESM\ntherm\nand compare it to other state-of-the-art methods in the ability to generalize\nto longer protein sequences.\nResults\nEvaluating model generalizabilit y on test-set-only domains\nProtein stability prediction can be assessed on different scales of generalizability. Although\nmachine learning algorithms are often trained and tested on different sets of non-overlapping\nsamples, the definition of overlap is ambiguous in protein sequences. For example, assigning\ntwo point mutants from the same WW domain, one to the training set and another to the test\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 2 / 13\nset, can assess the generalizability of the model to sequences sharing the same protein domain.\nHowever, it fails to evaluate the generalizability of the model to a domain different from those\nin the training set, such as an SH2 domain. To benchmark our model on both scales, our test\nset sequences consist of two parts. The first part is formed by protein domains also found in\ntraining set, whereas the second part consists of protein domains exclusively found in test set\nonly, denoted as test-set-only domains. We assess the model performance by Spearman’s R,\nand its capability to generalize to these test-set-only sequences by the highest sequence identity\nto any domains in the training set. Given that domains are classified according to the wildtype\ndefinitions by Tsuboyama et al. [32], it is possible for domains exclusive to the test set to still\nshare considerable sequence identity with those in the training set. This setup allows for an\nassessment of generalizability across varying degrees of sequence identity. The dataset-splitting\nscheme is illustrated in Fig 1 and further detailed in Methods and Materials.\nESM\ntherm\ngeneralizes reasonably well to 47 test-set-only protein domains, illustrated in\nFig 2. The Spearman’s R evaluated on individual domains ranges from 0.2 to 0.9, except for the\nuncharacterized bacterial protein yahO (PDB code: 2MA4) [34]. Among all test-set-only\ndomains, SH3-subunit of chicken alpha spectrin (PDB code: 6SCW) [35] has the highest\nsequence identity of 95.8% and scores a corresponding Spearman’s R of 0.88. Going down the\nladder to test-set-only domains in lower sequence identity, our model scores worse in Homo\nsapein J-domain protein HSJ1a (PDB code: 2LGW) [36] at 59% identity but still retains a\nSpearman’s R of 0.52.\nIn the 13 cases where no alignment with the training set sequences passes e-value < 10\n−3\n,\nESM\ntherm\nis capable of generalizing to both natural and de novo proteins. No training sequence\nFig 1. Dataset splitting scheme. Protein domains are first identified by their wildtype sequenc es and split into train-validatio n-test (green) and test-set-\nonly partition s (cyan). Mutants are then randoml y assigned to either training, validation and test sets or test set only according to their respective\nwildtype.\nhttps://doi.o rg/10.1371/j ournal.pcbi. 1012248.g001\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 3 / 13\ncan be aligned to Escherichia coli DNA-binding arginine repressor (PDB code: 1AOY) [37],\nand yet its Spearman’s R evaluated is 0.69. For de novo designs, we highlight two protein\ndomains from Baker Lab. αββα domain (HEEH_KT_rd6_0790) is a mini-protein from high-\nthroughput computational design with Rosetta [38], whereas the trRosetta-hallucinated struc-\nture (r11_233_TrROS_Hall) was sampled with iterative sequence refinement to improve the\nconfidence in the prediction of residue-residue distance map [39]. Spearman’s R on these\ndomains is 0.44 and 0.72, respectively.\nImproving stability prediction by learning all domains collectively\nPrior to the work by Tsuboyama et al. [32], DMS was often restricted to a single protein of\ninterest. In the case where the target of interest is not thoroughly mapped, site-saturated muta-\ngenesis studies from a homologous sequence(s) might provide insights into selecting the best\nmutation for the specific function of interest. However, direct cross-comparison between pro-\nteins is often complicated by the difference in measured quantities and experimental condi-\ntions between functional assays. This inconsistency makes it difficult to highlight the benefits\nof learning from multiple target proteins collectively in a systematic manner.\nThe mega-scale dataset addresses this difficulty by measuring folding stability across multi-\nple protein domains in a uniform experimental condition, and it helps us compare two\nFig 2. Spearman’s R on test-set-only protein domains. Natural protein domains are labeled in blue and de novo\ndomains are in orange. The x-axis is the highest sequence identity from the evaluated protein domain to those in the\ntraining set. In the case where no sequenc e alignment was found, 0% is assigned. The y-axis is the Spearman’s R\nevaluated on all sequences from the correspond ing domain. We highlighted some of the test-set-onl y AlphaFol d2\nmodels in cyan, and when possible, overlay them with the training-set protein domains of the highest sequence\nidentity in green.\nhttps://d oi.org/10.1371/j ournal.pc bi.1012248. g002\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 4 / 13\nparadigms, i.e. transfer learning from homologous sequences and learning from all domains\ncollectively. To contrast these approaches, we assess the generalizability of the model fine-\ntuned on these paradigms on test-set-only protein domains.\nExtrapolating to test-set-only domains clearly benefits from learning all domains collec-\ntively. Collective training improves Spearman’s R by 0.16 on average (p-value = 6x10\n-3\n), as\nillustrated in Fig (3). CdnL protein (PDB code: 2LQK) [40] cannot be aligned with any training\nsequence and instead was matched with its closest structural alignment (PDB code: 2BTT)\n[41] with Foldseek [42]. Collective training increased CdnL’s Spearman R from -0.25 to 0.65.\nSimilarly, amino-terminal domain of phase 434 repressor (PDB code: 1R69) [43] was matched\nby structural alignment to a redesigned protein G (PDB code: 1EM7) [44] with a TM-score of\n0.23, and gained 0.74 in Spearman’s R from -0.22 to 0.52. Looking into the domains with\nsequence alignment to the training set, WW domain from APBB3 (PDB code: 2YSC) shares\n47% identity with its training-set partner (PDB code: 1WR7) and yet still benefits from multi-\ndomain training with an improvement of 0.32. In contrast, uncharacterized yahO protein\nremains a difficult target. Compared to training on its closest training-set domain (PDB code:\n1IGV), learning on multiple domains only improves the correlation from -0.35 to -0.14. Over-\nall, these results highlight the benefits of a protein stability dataset on a diverse collection of\nprotein domains for generalization to previously understudied targets.\nAlthough the improvement brought by collective training highlights the benefits of a consis-\ntent large-scale dataset on folding stability, it is still unclear whether the improvement originates\nfrom the shared knowledge on folding stability across multiple domains or the sheer number of\nsamples. The discrepancy in dataset size is significant as an individual domain only constitutes\nup to 7k sequences, less than 2% of the training set on the collection of protein domains.\nIn addition to extrapolating to test-set-only protein domains, we conducted a similar com-\nparison on the impact of training on a collection of protein domains on interpolation on previ-\nously observed protein domains. Overall, performance on sequences from training-set\ndomains is marginally uplifted by learning from a multi-domain dataset. Illustrated in S2 Fig,\nlearning from an ensemble of protein domains weakly outperforms models trained on the\nsame domain by an average of 0.03 (p-value = 2x10\n-2\n). However, the margin is slim. 72% of the\ndomains have Spearman’s R only change by 0.1.\nFig 3. Compar ison between transfer learning from the closest protein domain in training set and trainin g on all domains collecti vely. (A)\nSchematic of the comparison . In the case of transfer learning, we match the test-set-o nly protein domain in cyan with the closest domain found in the\ntraining set in green. (B) Spearman’s R in the test set-only protein domains (x-axis) by learning from all domains collectively and (y-axis) by learning\nfrom the closest training-set domain alone. Samples (s) located under the diagonal line indicate better performance by learning collective ly. The closest\ntraining-set domains were identified primaril y by sequence alignment using MMseq s2, then by structure alignment using Foldseek , or discarded when\nno match was found in either case. The color bar indicates the highest sequence identity to any training -set domains and 0% was assigned when no\nsequence alignm ent was found. Statistica l significan ce is performed with Wilcoxon’s rank sum test (p-value = 6 x 10\n-3\n).\nhttps://doi.o rg/10.1371/j ournal.pcbi. 1012248.g003\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 5 / 13\nComparison with existing models on larger proteins\nAlthough natural proteins often span between 200 and 400 residues [45], ESM\ntherm\nis fine-\ntuned on sequences no longer than 72 residues in length. To explore its performance under\nthis limitation, we benchmarked our model on seven stability-related datasets on larger pro-\nteins and compared our results with state-of-the-art covering different methodologies in\nTables 1 and 2. These include Rosetta Cartesian ΔΔG for molecular modeling, MUPro for sup-\nport vector machine (SVM) on traditional sequence features, RaSP for structure-based CNN,\nELASPIC-2 which employs a machine-learning model based on both structure and sequence\nembedding, and unsupervised prediction from ESM-2 [46].\nWe observe comparable performance in predicting the thermostability of test-set-only pro-\ntein domains across all models except MUPro. Our pLM achieves a Spearman’s R of 0.65,\ncompared to 0.64 from RaSP and ELASPIC-2, and 0.61 from Rosetta molecular modeling.\nMUPro finishes last by scoring 0.31. Drawing an interesting parallel between datasets, Huang\net al. reported direct melting temperature measurements of beta-glucosidase active-site\nmutants (PDB code: 2JIE) manually selected based on biophysical knowledge [47, 48], while\nRomero et al. leveraged a log-enrichment value to gauge the stability for a similar beta-glucosi-\ndase (PDB code: 1GNX) in a site-saturated fashion [49]. The former closely resembles a\nsmaller-scale study guided by domain knowledge in contrast to the latter dataset that leverages\na parallelized assay. Despite an identical alpha-beta barrel scaffold and catalytic mechanism,\nand a shared sequence identity 48%, most models achieve Spearman’s R above 0.4 on the Bgl3\ndataset, and no method correlates with BglB dataset. This highlights the potential impact of\nsampling and assay through a comparative setting.\nTrained specifically on small protein domains, ESM\ntherm\ndoes not generalize to other data-\nsets on larger protein sequences. In a collection of direct [50] and indirect [51–53] stability\nTable 1. Compariso n of Spearman ’s R across methods on individu al DMS datasets. All evaluation is restricte d to point mutations, except our pLM on the mega-sc ale\ndataset. We also report unsupervised prediction from pretrained ESM-2 to contrast with supervi sed approaches . While the mega-sca le dataset from Tsuboyam a et al. covers\nmultiple protein domains [32], all other datasets studied only one target protein.\nDataset Length Supervis ed Prediction Unsupervis ed Prediction\nRosetta MUPro RaSP ELASPIC-2 ESM\ntherm\nESM-2 (35M) ESM-2 (3B)\nMega-scal e dataset 40–72 0.61 0.31 0.64 0.64 0.65 0.36 0.43\nBglB dataset 445 -0.01 -0.02 0.02 -0.11 -0.11 -0.12 -0.12\nBgl3 dataset 510 0.49 0.10 0.44 0.56 0.06 0.43 0.53\nAcetyltran sferase dataset 177 0.33 0.16 0.30 0.49 0.03 0.25 0.50\nLipase EstA dataset 212 0.48 0.06 0.41 0.47 0.04 0.26 0.28\nPTEN dataset 403 0.44 0.17 0.41 0.47 0.04 0.26 0.25\nMethyltran sferase dataset 245 0.48 0.21 0.40 0.58 0.03 0.42 0.46\nhttps://do i.org/10.1371/j ournal.pc bi.1012248. t001\nTable 2. Overview of benchm arked DMS datasets on protein stability.\nDataset name Protein name Protein length Measu red quantity No. of sequen ces\nMega-scal e dataset [32] multiple 40–72 cDNA display proteolysis 100,794 (test set)\nBglB dataset [47] Beta-glucos idase (BglB) 445 melting temperat ure (T\nm\n) 157\nBgl3 dataset [49] Beta-glucos idase (Bgl3) 501 catalytic susceptibil ity to heat shock 2,999\nAcetyltran sferase dataset [51] Gentamicin 3-N-acetylt ransferase 177 chemical stability 1,801\nLipase EstA dataset [50] Lipase EstA 212 melting temperat ure (T\n50\n) 2,172\nPTEN dataset [52] PTEN 403 protein abundan ce 5,083\nMethyltran sferase dataset [53] Thiopurine S-methylt ransferase 245 protein abundan ce 3,648\nhttps://do i.org/10.1371/j ournal.pc bi.1012248. t002\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 6 / 13\nmeasurements, state-of-the-art methods outperform our pLM convincingly. Cartesian ddG in\nRosetta achieves generalizability through molecular modeling with a correlation between 0.33\nand 0.48. Simultaneously, RaSP is built on top of Cartessian ddG and dramatically speeds up\nthe protocol with marginal correlation setbacks. Overall, ELASPIC-2 ranks highest with a\nSpearman’s R of 0.42–0.58 while our pLM correlates to none of these datasets.\nAnother intriguing observation is the performance of unsupervised predictions from pLM.\nWhile ESM-2 is less capable of predicting stability changes within the mega-scale dataset, it\nexcels in datasets where indirect stability measurements correlate with function. These include\nlog2-enrichment value which characterizes how catalytic activity reacts to heat shock in Bgl3\ndataset and the intracellular abundance of the protein in the acetyltransferase dataset. Con-\nversely, ESM-2 has a comparably weaker performance for proteolysis folding stability in the\nmega-scale dataset and chemical stability in the Lipase EstA dataset, where the assays measure\nstability directly. We also highlight the impact of fine-tuning by benchmarking the unsuper-\nvised prediction from 35M-parameter ESM-2 against our fine-tuned ESM\ntherm\nof the same\nmodel size. Supervised prediction improves the correlation from 0.36 to 0.65.\nDiscussion\nAlthough our model generalizes reasonably well to new small protein domains in the mega-\nscale thermostability dataset, it is substantially weaker on larger proteins. Studies have estab-\nlished a strong correlation between the parallelized assay and direct measurement of thermo-\nstability [54]. However, we cannot rule out that our language model is biased towards dataset-\nspecific details, including experiment conditions and sampling distribution of protein\nsequences. One hypothesis is that our pLM is biased toward shorter sequences, while geomet-\nric learning do not suffer from the same pitfall and already performs better in unsupervised\nprediction [55]. The protein domains on which we trained are limited to 40 to 72 amino acids\nin length, a stark contrast to the 177- to 501-residue-long sequences in our additional DMS\nbenchmark. This might suggest that fine-tuned pLM stability predictors would benefit from a\nlarge-scale folding stability dataset on longer sequences.\nWhile most methods can rank ΔΔG between mutants successfully, predicting ΔG is still\nchallenging. Our predictions often suffer from an offset and/or scale differently when com-\npared to the experimental ΔG of the test-set-only domains (S3 Fig) and other methods might\nshare the same problem. For example, Rosetta Cartesian ΔΔG follows a different energy unit\n(Rosetta Energy Unit), and it might not be suitable to be compared directly to kcal mol\n-1\n. How-\never, the misalignment can be easily resolved by a simple linear regression between model pre-\ndiction and experiment. Upon recalibration per protein domain, the root mean square error\nfrom our model improved from 1.34 to 0.83 and R\n2\nfrom -0.85 to 0.45, averaged across all test-\nset-only domains. For instance, our model scores a negative R\n2\non DNA-binding arginine\nrepressor before recalibration and improves to 0.47 after rescaling, while Spearman’s R\nremains the same at 0.69 regardless of any monotonic transformation (Fig 4).\nConclusion\nIn this work, we demonstrate that folding stability prediction is possible using a protein lan-\nguage model. Enabled by large-scale protein stability measurements, we fine-tuned ESM-2 on\nthe absolute folding energy of small protein domains. This approach generalizes successfully to\nprotein domains distal from the training set, showing the potential of transfer learning to\nreduce experimental burden. Furthermore, our result highlights the benefits of training collec-\ntively on all protein sequences instead of mutagenesis study on a single wildtype. Although its\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 7 / 13\nperformance on larger protein scaffolds is lagging behind state-of-the-art, a folding stability\ndataset of larger proteins might be vital to improving the generalizability of ESM\ntherm\n.\nMethods and materials\nProtein language model and fine-tuning protocol\nESM-2 is a transformer pLM pre-trained on masked-language-model (MLM) objective on\nUniRef50. We fine-tuned the model on whole-sequence regression task with a classification\nhead on the starting token. All parameters were trainable in fine-tuning, and used a local batch\nsize of 128 and a global batch size of 2048. We trained the model on A100 GPU at half preci-\nsion with a patience of 500 steps. We report all test-set-only evaluations on the checkpoint\nwith the best performance on the validation set.\nWe performed hyperparameter selection on model size (8, 35, 150, and 650 million parame-\nters) (S1 Table), and selected the 35-million-parameter model to balance prediction perfor-\nmance and compute speed. In addition, we performed an ablation study on pretraining.\nModel with pretraining has a superior advantage over that with random initialization (S1 Fig).\nDataset construction\nTsuboyama et al. measured the folding stability of 1.8M measurements derived from 542 pro-\ntein domains by cDNA display proteolysis [32]. We aggregated measurement(s) with the iden-\ntical protein sequence, regardless of their DNA sequence(s), into a single entry. In cases where\nthe DNA sequence was unique while sharing the same protein sequence, we evaluated the stan-\ndard deviation of ΔG and log K\n50\n. We removed measurements when the standard deviation of\nΔG was greater than 2 kcal mol\n-1\nor that of log K\n50\nwas greater than 0.5 and we kept only\ndomains with at least 100 measurements by protein sequence. This reduced the number of\nFig 4. Impact of recalibrat ion. (Left) Miscalibrat ion between prediction and true value on the stability of DNA-binding arginine represso r. (Right)\nRecovered agreement between prediction and stability measurem ent through linear rescaling on the same set of data.\nhttps://doi.o rg/10.1371/j ournal.pcbi. 1012248.g004\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 8 / 13\nentries from 851,552 protein sequences from their original criteria (K50_dG_Dataset1_Da ta-\nset2.csv) to 527,785 protein sequences and 258 natural and 203 de novo protein domains.\nUnder the hierarchical nature of this dataset, by which multiple domains are constituted\nand each domain holds a collection of multiple mutants, the definition of model generalizabil-\nity has two layers. The first is the ability of the model to generalize to mutants on training pro-\ntein domains, and the second is that on test-set-only domains. To evaluate the model on both\ntraining and test-set-only domains, we split our dataset into train, validation, and test sets by\ndomains as illustrated in Fig 1. 10% of all domains, defined by wildtype by the authors, are ran-\ndomly drawn and all of their mutants are assigned to the test set. Mutants are randomly\nassigned to train-validation-test sets in an 80–10-10 ratio for the remaining domains.\nSequence and structural alignment\nWe implemented sequence clustering and alignment through MMseqs2 [56]. For clustering,\nwe clustered the domain wild-type sequences using a similar strategy in constructing the Uni-\nclust database. We dropped prefiltering for all-to-all pairwise alignment. For Foldseek, we\nsearched for the structural identity based on AlphaFold structures from Tsuboyama et al [32].\nUnless otherwise specified, we used the default parameters in both MMseqs and Foldseek. The\nimplementation details of alignment can be found src/esmtherm/alignment in the GitHub\nrepository.\nMatching test-set-only domains\nWe fine-tuned ESM-2 (esm2_t12_35M_UR50 D) on each of the 416 protein domains in the\ntraining set as our independently learned models. We first matched each test-set-only domain\nto its closest partner in the training set by the highest sequence identity using MMseqs2. In the\ncase where no sequence alignment is identified, we matched test-set-only domain by the high-\nest structural identity by Foldseek. In the case that neither is identified, the test-set-only\ndomain was not compared. Pairwise comparisons of interpolation and extrapolation are per-\nformed in Wilcoxon’s rank sum test.\nBenchmark protein dataset selection\nGiven the intensive computing resource required to benchmark Rosetta, we limited ourselves\nto six DMS datasets on direct and indirect stability measurements from ProteinGym [57, 58],\nand another independent mutational dataset (BglB) from Huang et al. [47] to cover a range of\nassays. Nutschel et al. reported the thermostability (ΔT\n50\n) of Bacillus subtilis Lipase A [50],\nwhereas Dandage et al. reports chemical stability on Gentamicin 3-N-acetyltransferase [51].\nContrary to direct stability measurements, PTEN and Methyltransferase datasets correlate\nwith stability through enhancement or depreciation of intracellular abundance as an indirect\nindicator [52, 53]. The pair of Bglb and Bgl3 datasets was chosen for a comparative study on\nthe impact of sampling and measurement assays. Bgl3 from Romero et al. and BglB datasets\n[47, 49] share homologous beta-glucosidase sequences but differ in log enrichment value and\nmelting temperature (T\nm\n) as indirect and direct thermostability measurements.\nSupporting information\nS1 Fig. Ablation of pretraining measured in Spearman’s R. Each sample is a collection of\nmutants from a test-set-only domain. The x-axis is Spearman’s R of a test-set-only domain\nwith pretraining. The y-axis is that from randomly initialized model. The color bar on the\nright represents the closest sequence identity in the train and validation set domains. The\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 9 / 13\nstatistical assessment was performed using Wilcoxon’s rank sum test.\n(TIF)\nS2 Fig. Comparison between learning from the same protein domain only and training on\nall domains collectively. (A) Schematic of the comparison. (B) Spearman’s R on test mutants\nwhose protein domains are also present in the training set. The x-axis represents learning from\nall domains collectively and the y-axis is learning from the same protein domain alone.\nDomain(s) located under the diagonal line indicate better performance when learning collec-\ntively. Statistical significance is performed with Wilcoxon’s rank sum test.\n(TIF)\nS3 Fig. Offset in ΔG prediction on wildtype sequences. The x-axis is the ΔG prediction from\nESM\ntherm\nand the y-axis is the experimental ΔG label. The Spearman’s R across all wildtypes in\ntest-set-only protein domains is 0.39.\n(TIF)\nS1 Table. Performance evaluation on different model sizes on test set. Metrics are evaluated\non each individual domain, and then aggregated into mean and standard deviation over all\ndomains. All models have similar performance metrics with esm2_t12_35M_UR50D except\nesm2_t6_8M_UR50D on Spearman’s R (p-value < 5x10\n-2\n).\n(PDF)\nS1 Spreadsheet. Performance evaluation per protein domain.\n(CSV)\nS2 Spreadsheet. Model prediction per protein sequence.\n(CSV)\nAuthor Contributions\nConceptualization: Simon K. S. Chu, Justin B. Siegel.\nData curation: Simon K. S. Chu.\nFormal analysis: Simon K. S. Chu.\nInvestigation: Simon K. S. Chu.\nMethodology: Simon K. S. Chu.\nProject administration: Justin B. Siegel.\nSoftware: Simon K. S. Chu, Kush Narang.\nSupervision: Justin B. Siegel.\nValidation: Simon K. S. Chu, Kush Narang.\nVisualization: Simon K. S. Chu.\nWriting – original draft: Simon K. S. Chu, Justin B. Siegel.\nWriting – review & editing: Simon K. S. Chu, Kush Narang, Justin B. Siegel.\nReferences\n1. Lv Y., Zheng S., Golden zweig A., Liu F., Gao Y., Yang X., Kandale A., McGeary R.P., Williams S.J., Kobe\nB., Schemb ri M.A., Landsbe rg M.J., Wu B., Bru ¨ ck T.B., Sieber V., Bode ´ n M., Rao Z., Fleishman S.J.,\nSchenk G., Guddat L.W. Enhan cing the Thermal and Kinetic Stability of Ketol-Ac id Reductoi somerase, a\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 10 / 13\nCentral Catalys t of a Cell-Free Enzyme Cascade for the Manufactu re of Platform Chemical s. Applied\nBioscienc es.\n2. Rennison A., Winther J.R., Varrone C. Rational Protein Engineeri ng to Increase the Activity and Stability\nof IsPETase Using the PROSS Algorithm . Polymers, 13. https://do i.org/10.3390 /polym1322 3884\nPMID: 348331 82\n3. Hutchins on M., Ruffolo J.A., Haskins N., Iannotti M., Vozza G., Pham T., Mehzab een N., Shand ilya H.,\nRickert K., Croasdale -Wood R., Damschr oder M., Fu Y., Dippel A., Gray J.J., Kaplan G. (2023).\nEnhance ment of antibod y thermostab ility and affinity by computation al design in the absence of anti-\ngen. bioRxiv .\n4. Gerasima vicius L., Liu X., Marsh J.A. Identification of pathoge nic missens e mutations using protein\nstability predictors. Scientific Reports. 2020; 10. https://do i.org/10.10 38/s41598-0 20-72404- w PMID:\n32958805\n5. Cheng J., Novati G., Pan J., Bycroft C., Z\nˇ\nemgulytė A., Appleba um T., et al. Accurate proteo me-wide\nmissense variant effect prediction with AlphaMis sense. Science. 2023; 381. https://doi.or g/10.112 6/\nscience.adg7 492 PMID: 37733863\n6. Stein A., Fowler D.M., Hartman n-Petersen R., Lindorff-Lar sen K. Biophysica l and Mechanist ic Models\nfor Disease-C ausing Protein Variants. Trends in biochemical sciences , 2019; 44 7, 575–58 8. https://\ndoi.org/10.10 16/j.tibs.20 19.01.003 PMID: 307129 81\n7. Yue P., Li Z., Moult J. Loss of protein structure stability as a major causative factor in monogen ic dis-\nease. Journal of molecular biology, 2005; 353 2, 459–73. https://doi.or g/10.101 6/j.jmb.200 5.08.020\nPMID: 161690 11\n8. Kellogg EH, Leaver-Fay A, Baker D. Role of conformatio nal sampling in computing mutation- induced\nchanges in protein structure and stability. Proteins: Structur e, Function and Bioinformatics . 2011; 79\n(3):830–83 8. https://do i.org/10.1002 /prot.229 21 PMID: 21287615\n9. Park H, Bradley P, Greisen P, Liu Y, Mulligan VK, Kim DE, et al. Simultaneous Optimizati on of Biomo-\nlecular Energy Functions on Features from Small Molecules and Macrom olecules. Journal of Chemical\nTheory and Computation. 2016; 12(12):620 1–6212. https://doi.or g/10.102 1/acs.jctc.6 b00819 PMID:\n27766851\n10. Schymk owitz J, Borg J, Striche r F, Nys R, Rousseau F, Serrano L. The FoldX web server: An online\nforce field. Nucleic Acids Resea rch. 2005; 33(SUP PL. 2):382–388. https://doi.or g/10.109 3/nar/gki387\nPMID: 159804 94\n11. Wilson CJ, Chang M, Karttunen M, Choy WY. Keap1 cancer mutants: A large-sc ale molecular dynam-\nics study of protein stability. International Journal of Molecular Sciences. 2021; 22(10). https://doi.or g/\n10.3390/ ijms221054 08 PMID: 34065616\n12. Dehouck Y, Kwasigroch JM, Gilis D, Rooman M. PoPMu SiC 2.1: A web server for the estimat ion of pro-\ntein stability changes upon mutation and sequence optimality. BMC Bioinform atics. 2011; 12. https://\ndoi.org/10.11 86/1471-210 5-12-151 PMID: 21569468\n13. Cao H, Wang J, He L, Qi Y, Zhang JZ. DeepDDG: Predicting the Stabilit y Change of Protein Point Muta-\ntions Using Neural Networks. Journal of Chem ical Informatio n and Modeling. 2019; 59(4):1508 –1514.\nhttps://doi.or g/10.102 1/acs.jcim .8b00697 PMID: 307599 82\n14. Witvliet DK, Strokach A, Giraldo-Fo rero AF, Teyra J, Colak R, Kim PM. ELASP IC web-server :\nProteome- wide structure-ba sed prediction of mutation effects on protein stabilit y and binding affinity.\nBioinform atics. 2016; 32(10):1589 –1591. https://d oi.org/10.10 93/bioinfor matics/btw0 31 PMID:\n26801957\n15. Worth CL, Preissne r R, Blundell TL. SDM—A server for predicting effects of mutations on protein stabil-\nity and malfunc tion. Nucleic Acids Research. 2011; 39(SUP PL. 2). https://doi.or g/10.109 3/nar/gkr363\nPMID: 215931 28\n16. Masso M, Vaisman II. AUTO-MU TE 2.0: A portable framework with enhance d capabilities for predicti ng\nprotein functional consequen ces upon mutation. Advances in Bioinformatic s. 2014; 2014. https://doi.\norg/10.1155/ 2014/278385 PMID: 251972 72\n17. Strokach A., Corbi-Verg e C., Teyra J., Kim P.M. Predicting the Effect of Mutations on Protein Folding\nand Protein-P rotein Interactio ns. Methods in molecular biology, 2018; 1851, 1–17. https://doi.o rg/10.\n1007/978-1- 4939-8736- 8_1\n18. Strokach A., Corbi-Verg e C., Kim P.M. Predicting changes in protein stability caused by mutation using\nsequence- and structure- based methods in a CAGI5 blind challenge. Human Mutation, 40, 1414–1423.\nhttps://doi.or g/10.100 2/humu.23 852 PMID: 31243847\n19. Cheng J., Randall A., Baldi P. Prediction of protein stability changes for single-si te mutations using sup-\nport vector machines. Proteins : Structure, Function, and Bioinform atics, 2006; 62(4), 1125–1132.\nhttps://doi.or g/10.100 2/prot.20810 PMID: 16372356\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 11 / 13\n20. Huang L., Gromiha M.M., Ho S. iPTREE -STAB: interpretabl e decision tree based method for predicting\nprotein stability changes upon mutations. Bioinform atics, 23 10, 1292–3. https://do i.org/10.1093 /\nbioinforma tics/btm100 PMID: 17379687\n21. Lecun Y, Bottou E, Bengio Y, Haffner P. Gradient-Bas ed Learning Applied to Docume nt Recognition ;\n1998.\n22. Kipf TN, Welling M. Semi-Super vised Classificat ion with Graph Convolution al Networks. arxiv. 2016;.\n23. Wang S, Tang H, Shan P, Wu Z, Zuo L. ProS-GNN: Predic ting effects of mutations on protein stability\nusing graph neural networks . Computationa l Biology and Chemistry. 2023; 107. https://doi.or g/10.101 6/\nj.compbiol chem.2023.1 07952 PMID: 37643501\n24. Chu SKS, Siegel J. Predicting single-point mutational effect on protein stability; 2021.\n25. Blaabjerg LM, Kassem MM, Good LL, Jonsson N, Cagiada M, Johanss on KE, et al. Rapid protein stabil-\nity predicti on using deep learning represent ations. eLife. 2023; 12. https://doi.or g/10.755 4/eLife.825 93\nPMID: 371840 62\n26. Strokach A, Lu TY, Kim PM. ELASPIC2 (EL2): Combining Contex tualized Language Models and Graph\nNeural Networ ks to Predict Effects of Mutations. Journal of Molecular Biology . 2021; 433(11). https://\ndoi.org/10.10 16/j.jmb.20 21.166810 PMID: 334502 51\n27. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, et al. Biological structure and function emerge from\nscaling unsupervis ed learning to 250 million protein sequences . bioRxiv. 2019; 118(15):e2 016239118.\n28. Strokach A, Becerra D, Corbi-V erge C, Perez-Ri ba A, Kim PM. Fast and Flexible Protein Design Using\nDeep Graph Neural Networks. Cell System s. 2020; 11(4):402– 411. https://doi.or g/10.101 6/j.cels.2020 .\n08.016 PMID: 32971019\n29. Gromiha MM, An J, Kono H, Oobatak e M, Uedaira H, Prabakaran P, et al. ProTherm , version 2.0: ther-\nmodynam ic database for protei ns and mutants; 2000. 1. Available from: http://www.rtc .riken.go.jp/\nprotherm. html.\n30. Wang CY, Chang PM, Ary ML, Allen BD, Chica RA, Mayo SL, et al. ProtaBank : A repository for protein\ndesign and engineerin g data. Protein Science. 2018; 27(6):1113 –1124. https:// doi.org/10.10 02/pro.\n3406 PMID: 295753 58\n31. Xavier J.S., Nguyen T., Karmar kar M., Portelli S., Rezend e P.M., Velloso J.P., et al. ThermoMutD B: a\nthermodyn amic database for missense mutations. Nucleic Acids Researc h. 2020; 49, D475–D 479.\nhttps://doi.or g/10.109 3/nar/gkaa 925\n32. Tsuboyam a K, Dauparas J, Chen J, Laine E, Mohseni Behbaha ni Y, Weinstein JJ, et al. Mega-sc ale\nexperime ntal analysis of protein folding stability in biology and design. Nature. 2023; 620(7973) :434–\n444. https://do i.org/10.1038 /s41586- 023-06328- 6 PMID: 37468638\n33. Lin Z, Akin H, Rao R, Hie B, Zhu Z, Lu W, et al Evolutio nary-scale prediction of atomic-leve l protein\nstructure with a langua ge model. Science. 2023; 379:112 3–1130. https://doi. org/10.1126/s cience.\nade2574 PMID: 36927031\n34. Eletsky A, Michalsk a K, Houliston S, Zhang Q, Daily MD, Xu X, et al. Structur al and Functiona l Charac-\nterization of DUF1471 Domains of Salmonella Proteins SrfN, YdgH/Sss B, and YahO. PLoS ONE.\n2014; 9:e1017 87. https://doi.or g/10.137 1/journal.po ne.0101787 PMID: 25010333\n35. Grohe K, Patel S, Hebrank C, Medina S, Klein A, Rovo ´ P, et al. Protein Motional Details Revealed by\nComplem entary Structural Biology Techniques . Structure. 2020; 28(9):1024 –1034. https://doi.or g/10.\n1016/j.str .2020.06.00 1 PMID: 32579946\n36. Gao XC, Zhou CJ, Zhou ZR, Wu M, Cao CY, Hu HY. The C-termina l helices of heat shock protein 70\nare essential for J-domain bindin g and ATPase activation. Journal of Biological Chemistry . 2012; 287\n(8):6044–6 052. https:// doi.org/10.10 74/jbc.M111 .294728 PMID: 22219199\n37. Sunnerhag en M, Nilges M, Otting G, Carey J. Soluti on structure of the DNA-bindi ng domain and model\nfor the complex of multifunction al hexameri c arginine repressor with DNA; 1997. Availab le from: http://\nwww.natur e.com/ns mb.\n38. Chevalier A., Silva DA., Rocklin G. et al. Massivel y parallel de novo protei n design for targeted thera-\npeutics. Nature, 2017; 550, 74–79. https://doi.or g/10.1038/ nature23912 PMID: 2895386 7\n39. Anishche nko I, Chidyau siku TM, Ovchinni kov S, Pellock SJ, Baker D. De novo protein design by deep\nnetwork hallucinat ion. Nature. 2020; p. 547–552.\n40. Gallego-Ga rcı ´ a A, Mirassou Y, Elı ´ as-Arnanz M, Padman abhan S, Jime ´ nez MA. NMR structure note: N-\nterminal domain of Thermus thermoph ilus CdnL. Journal of Biomole cular NMR. 2012; 53(4):355– 363.\nhttps://doi.or g/10.100 7/s10858 -012-9648-z PMID: 22782235\n41. Musi V, Birdsall B, Fernandez -Ballester G, Guerrini R, Salvatori S, Serrano L, et al. New approac hes to\nhigh-throu ghput structure character ization of SH3 complexes : The example of Myosin-3 and Myosin-5\nSH3 domains from S. cerevisiae . Protein Science. 2006; 15:795–8 07. https://doi.or g/10.1110 /ps.\n051785506 PMID: 16600966\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 12 / 13\n42. van Kempen M, Kim SS, Tumescheit C, Mirdita M, Lee J, Gilchrist CLM, et al. Fast and accurate protein\nstructure search with Foldseek. Nature Biotechn ology. 2023. https:// doi.org/10.10 38/s4158 7-023-\n01773-0 PMID: 371569 16\n43. Mondrag bn A, Subbiah’ S, Almolt SC, Drottar’ M, Harrison SC. Structure of the Amino-termina l Doma in\nof Phage 434 Repress or at 2.0 A Resolution. J Mol Hiol (1989). 1989; 205:189 –200. https://doi.or g/10.\n1016/0022-2 836(89)90 375-6\n44. Strop P., Marinescu A., Mayo S.L. Structure of a protein G helix variant suggest s the importan ce of helix\npropensity and helix dipole interacti ons in protein design. Protein Science, 2000; 9. https://doi.or g/10.\n1110/ps.9 .7.1391 PMID: 10933505\n45. Nevers Y, Glover NM, Dessimoz C, Lecompte O. Protein length distribution is remarkab ly uniform\nacross the tree of life. Genom e Biology. 2023; 24(1). https:/ /doi.org/10.11 86/s1305 9-023-029 73-2\nPMID: 372916 71\n46. Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T. and Rives, A., 2021. Language models enable zero-\nshot prediction of the effects of mutations on protein function. Advances in Neural Informatio n Process-\ning Systems, 34 (2021): 29287-2930 3.\n47. Huang P, Chu SKS, Frizzo HN, Connolly MP, Caster RW, Siegel JB. Evaluati ng Protein Engineerin g\nThermostab ility Prediction Tools Using an Indepen dently Generated Datase t. ACS Omega . 2020; 5\n(12):6487– 6493. https:// doi.org/10.10 21/acsom ega.9b04105 PMID: 322588 84\n48. Isorna P, Polaina J, Latorre-Garc ı´ a L, Cañada FJ, Gonza ´ lez B, Sanz-Ap aricio J. Crystal Structur es of\nPaenibacillu s polymyxa β-Glucos idase B Complex es Reveal the Molecular Basis of Substrate Specific-\nity and Give New Insights into the Catalytic Machiner y of Family I Glycosida ses. Journal of Molecular\nBiology. 2007; 371(5):120 4–1218. https://doi. org/10.1016/j .jmb.2007 .05.082 PMID: 17585934\n49. Romero PA, Tran TM, Abate AR. Dissecting enzyme function with microfluid ic-based deep mutationa l\nscanning. Proceedings of the National Academy of Sciences. 2015; 112(23):71 59–7164. https://doi.\norg/10.1073/ pnas.142228 5112 PMID: 26040002\n50. Nutschel C, Fulton A, Zimmermann O, Schwanebe rg U, Jaeger KE, Gohlke H. Systemat ically Scrutiniz -\ning the Impact of Substitution Sites on Thermostab ility and Detergen t Toleranc e for Bacillus subtilis\nLipase A. Journal of Chemical Informatio n and Modelin g. 2020; 60:1568 –1584. https://doi.or g/10.102 1/\nacs.jcim.9 b00954 PMID: 319052 88\n51. Dandage R, Pandey R, Jayaraj G, Rai M, Berger D, Chakrabort y K. Differentia l strengths of molecular\ndetermin ants guide environm ent specific mutationa l fates. PLOS Genetics. 2018; 14:e1007419. https://\ndoi.org/10.13 71/journal.p gen.1007419 PMID: 29813059\n52. Matreyek KA, Stephan y JJ, Ahler E, Fowler DM. Integra ting thousand s of PTEN variant activity and\nabundanc e measuremen ts reveals variant subgroups and new dominan t negatives in cancers.\nGenome Medicine. 2021; 13:165. https://doi. org/10.1186/s 13073-021- 00984-x PMID: 34649609\n53. Matreyek KA, Starita LM, Stephany JJ, Martin B, Chiasson MA, Gray VE, et al. Multiplex assessmen t of\nprotein variant abunda nce by massivel y parallel sequencin g. Nature Genetics. 2018; 50(6):874– 882.\nhttps://doi.or g/10.103 8/s41588 -018-0122-z PMID: 29785012\n54. Rocklin GJ, Chidyausiku TM, Goreshnik I, Ford A, Houliston S, Lemak A, et al. Global analysis of pro-\ntein folding using massively parallel design, synthesis , and testing. Science. 2017; 357(6347) :168–175.\nhttps://doi.or g/10.112 6/science.aan 0693 PMID: 28706065\n55. Paul, S., Kollasch, A., Notin, P., Marks, D. Combining Structur e and Sequence for Superior Fitness Pre-\ndiction. NeurIPS 2023 Generative AI and Biology (GenBio) Workshop.\n56. Steinegger M, So ¨ ding J. MMseq s2 enables sensitiv e protein sequence searching for the analysis of\nmassive data sets. Nature Biotechn ology. 2017; 35(11):102 6–1028. https:// doi.org/10.10 38/nbt.39 88\nPMID: 290353 72\n57. Notin P, Dias M, Frazer J, Hurtado JM, Gomez AN, Marks D, et al. Trancepti on: protein fitness predic-\ntion with autoregress ive transforme rs and inferenc e-time retrieval. In: International Conferen ce on\nMachine Learning. PMLR; 2022. p. 16990–17017 .\n58. Notin P., Kollasch A.W., Ritter D., van Niekerk L., Paul S., Spinner H., et al. ProteinGym: Large-Scal e\nBenchmar ks for Protein Design and Fitness Prediction. bioRxiv, 2023. https://doi.or g/10.1101/ 2023.12.\n07.570727 PMID: 38106144\nPLOS COMP UTATIONAL  BIOLOGY\nFine-tuning Protein Language Model for Stability Predic tion\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1012248 July 22, 2024 13 / 13"
}