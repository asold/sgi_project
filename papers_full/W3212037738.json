{
  "title": "MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering",
  "url": "https://openalex.org/W3212037738",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2097441002",
      "name": "Junjie Wang",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A2165008106",
      "name": "Yatai Ji",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2095706704",
      "name": "Jiaqi Sun",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2309990480",
      "name": "Yujiu Yang",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2098370030",
      "name": "Tetsuya Sakai",
      "affiliations": [
        "Waseda University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963668159",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W2963717374",
    "https://openalex.org/W2892654592",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W2546696630",
    "https://openalex.org/W2981694290",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4243617875",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2597425697",
    "https://openalex.org/W2463565445",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963560969",
    "https://openalex.org/W2747623286",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964072591",
    "https://openalex.org/W2963383024",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2471094925",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2964138343",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963377948",
    "https://openalex.org/W2938082352",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2963954913",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2798895485",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3016211260"
  ],
  "abstract": "In Visual Question Answering (VQA), existing bilinear methods focus on the interaction between images and questions. As a result, the answers are either spliced into the questions or utilized as labels only for classification. On the other hand, trilinear models such as the CTI model efficiently utilize the inter-modality information between answers, questions, and images, while ignoring intra-modality information. Inspired by this observation, we propose a new trilinear interaction framework called MIRTT (Learning Multimodal Interaction Representations from Trilinear Transformers), incorporating the attention mechanisms for capturing inter-modality and intra-modality relationships. Moreover, we design a two-stage workflow where a bilinear model reduces the free-form, open-ended VQA problem into a multiple-choice VQA problem. Furthermore, to obtain accurate and generic multimodal representations, we pre-train MIRTT with masked language prediction. Our method achieves state-of-the-art performance on the Visual7W Telling task and VQA-1.0 Multiple Choice task and outperforms bilinear baselines on the VQA-2.0, TDIUC and GQA datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2280‚Äì2292\nNovember 7‚Äì11, 2021. ¬©2021 Association for Computational Linguistics\n2280\nMIRTT: Learning Multimodal Interaction Representations from Trilinear\nTransformers for Visual Question Answering\nJunjie Wang1‚àó, Yatai Ji 2‚àó, Jiaqi Sun 2, Yujiu Yang2‚Ä†, Tetsuya Sakai 1‚Ä†\n1Waseda University, Shinjuku, Tokyo, Japan\n2 Graduate School at Shenzhen, Tsinghua University, ShenZhen, GuangDong, China\nwjj1020181822@toki.waseda.jp\n{jyt21, sunjq20}@mails.tsinghua.edu.cn\nyang.yujiu@sz.tsinghua.edu.cn tetsuyasakai@acm.org\nAbstract\nIn Visual Question Answering (VQA), exist-\ning bilinear methods focus on the interaction\nbetween images and questions. As a result, the\nanswers are either spliced into the questions or\nutilized as labels only for classiÔ¨Åcation. On the\nother hand, trilinear models such as the CTI\nmodel of Do et al. (2019) efÔ¨Åciently utilize the\ninter-modality information between answers,\nquestions, and images, while ignoring intra-\nmodality information. Inspired by these ob-\nservations, we propose a new trilinear interac-\ntion framework called MIRTT (Learning Mul-\ntimodal Interaction Representations from Tri-\nlinear Transformers), incorporating the atten-\ntion mechanisms for capturing inter-modality\nand intra-modality relationships. Moreover,\nwe design a two-stage workÔ¨Çow where a bilin-\near model reduces the free-form, open-ended\nVQA problem into a multiple-choice VQA\nproblem. Furthermore, to obtain accurate and\ngeneric multimodal representations, we pre-\ntrain MIRTT with masked language prediction.\nOur method achieves state-of-the-art perfor-\nmance on the Visual7W Telling task and VQA-\n1.0 Multiple Choice task and outperforms bi-\nlinear baselines on the VQA-2.0, TDIUC and\nGQA datasets.\n1 Introduction\nOne key challenge for building robust artiÔ¨Åcial in-\ntelligence systems is to handle information that lies\nacross multimedia data. Visual Question Answer-\ning (VQA) (Wu et al., 2017) is a speciÔ¨Åc example\nof the challenge, where, given a natural language\nquestion about an accompanying image, the sys-\ntem is required to produce a correct answer. This\nis a typical multimodal problem since the intelli-\n‚àóThese authors contributed equally to this work and\nshould be considered co-Ô¨Årst authors.\n‚Ä†Corresponding author.\n[Candidate answer list]\nBlue; Yellow; Red; Gold \nGold\nIntelligence systems with\nbilinear methods\nRed\nIntelligence systems with\ntrilinear methods\n‚ùå\n‚úî\n[Question]\nWhat is the main color of the train?\nFigure 1: Visual question answering task1\ngence system needs to understand images and texts\nsimultaneously.\nFrom the perspective of a single modality, there\nhave been plenty of backbone methods for learning\nbetter representations of either language or vision.\nFor learning language representations, researchers\nhave developed several pre-trained models, such as\nGPT-2 (Radford et al., 2019), BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019). These ap-\nproaches can learn the universal language represen-\ntations on the large-scale corpus, which are beneÔ¨Å-\ncial for downstream tasks (Qiu et al., 2020). Con-\ncerning visual representations, He et al. (2016);\nRen et al. (2017); Simonyan and Zisserman (2015)\nhave been widely applied to extract image features.\nDespite the success of these single-modality works,\nlearning the relationships between different modal-\nities is still an unsolved problem.\nExisting VQA approaches focus on modeling\nthe relationship between visual and language fea-\ntures represented by bilinear models. For ex-\nample, through applying bilinear feature fusion\nmethods, the image and text representations are\nprojected into a uniformed higher-dimensional\nspace. Multimodal Compact Bilinear pooling\n(MCB) (Fukui et al., 2016) processes the vec-\ntors in Fast Fourier Transform (FFT) space. For\n1An example of image-question-answer pair from Vi-\nsual7W dataset (Zhu et al., 2016)\n2281\nbetter cross-modality information exchange, (Yu\net al., 2019) and (Tan and Bansal, 2019) utilize co-\nattention/cross-attention networks to capture the\nhigh-level fusion features. Generally, these bilinear\napproaches only consider how to learn the joint rep-\nresentations between the questions and the images\nwhile the answers are processed as labels, making\nVQA task a multi-class classiÔ¨Åcation task (Tan and\nBansal, 2019; Zhu et al., 2016; Lu et al., 2019; Li\net al., 2020).\nHowever, the answers contain semantic informa-\ntion, related to the question and the visual context.\nFor considering the answer information, trilinear\nmodels represented by Compact Trilinear Interac-\ntion (CTI) (Do et al., 2019), are designed to learn\nthe alignment relationships between the visual con-\ntext, the answers, and the questions. Unfortunately,\nthe trilinear interaction in CTI only considers the\ninter-modality relationships but ignores the intra-\nmodality information, leading to unsatisfactory in-\nference results.\nTo tackle the above problems in the context\nof VQA, we propose a new trilinear modali-\nties interaction framework called MIRTT (Learn-\ning Multimodal Interaction Representations from\nTrilinear Transformers). SpeciÔ¨Åcally, MIRTT can\nextract more reÔ¨Åned high-level feature informa-\ntion from the inter-modality and intra-modality\nrelationships by introducing interactive attention\nnetworks across three modalities and three self-\nattention networks within a single modality. In\ngeneral, MIRTT can accommodate requirements\nfor processing three different modal features and\nefÔ¨Åciently utilize the information from the answers.\nThe contributions of our work are as follows:\n‚Ä¢ By considering the inter-modality and intra-\nmodality relationships, we introduce a new\nend-to-end trilinear interaction model MIRTT,\nthat enhances each single modality representa-\ntion by proposed attention networks, resulting\nin better inference ability in VQA.\n‚Ä¢ We propose a two-stage workÔ¨Çow to sim-\nplify the harder Free-Form Opened-Ended\n(FFOE) VQA into simpler Multiple Choice\n(MC) VQA, which provides a method to solve\ndifÔ¨Åcult VQA tasks.\n‚Ä¢ Our proposed MIRTT achieves state-of-\nthe-art performance on Visual7W telling\ntask (Zhu et al., 2016) and VQA-1.0 for MC\nVQA and outperforms the bilinear methods on\nthe VQA-2.0 (Goyal et al., 2017), (KaÔ¨Çe and\nKanan, 2017a) and GQA (Hudson and Man-\nning, 2019) datasets for FFOE VQA. More-\nover, we take advantage of the pre-training\ntask on our model, improving multi-modality\nunderstanding.\n2 Related Work\nVisual question answering (VQA) task. Follow-\ning Antol et al. (2015) who deÔ¨Åned the VQA\ntask (i.e., obtaining answers from a given image-\nquestion pair), has received signiÔ¨Åcant attention\nfrom the entire artiÔ¨Åcial intelligence commu-\nnity (Wu et al., 2017). There are two major types\nof VQA tasks, Multiple Choice (MC) VQA and\nFree-Form Opened-Ended (FFOE) VQA (Do et al.,\n2019). In MC VQA (Zhu et al., 2016; KaÔ¨Çe and\nKanan, 2017b), the answer is chosen from a candi-\ndate answer list for a given image-question pair ac-\ncessible in both training and test scenarios. FFOE\nVQA is more complicated since the answers are\nonly available in the training phase, and there is no\ncandidate answer list for choosing answers. How-\never, FFOE VQA is the most common VQA task\nand almost all models are aimed at this problem.\nThe general solution is to extract the visual fea-\ntures and linguist features Ô¨Årst and then fuse them\nwith a multi-modality fusion model, followed by a\nclassiÔ¨Åer or a generator to obtain the answer (Wu\net al., 2017). Among them, exploring different\nfusion approaches is the mainstream research direc-\ntion. On the one hand, the interactive relationships\nbetween the query image and the question have\nbeen fully modeled, such as element-wise oper-\nations (Antol et al., 2015) and bilinear methods\n(Fukui et al., 2016; Kim et al., 2018; Ben-Younes\net al., 2017, 2019). On the other hand, some works\nhave improved the VQA performance by consid-\nering the answer information (Hu et al., 2018; Do\net al., 2019). For example, Jabri et al. (2016) com-\nbines the three input representations through a sim-\nple Multilayer Perceptron (MLP), and Wang et al.\n(2018) introduces a layered fusion operation by\nmerging the image-question bilinear embeddings\nand the image-answer bilinear embeddings in joint\nembedding space. In order to solve VQA in a tar-\ngeted manner, we make full use of the answer infor-\nmation and propose a two-stage workÔ¨Çow, which\nconverts FFOE VQA to MC VQA.\nAttention-based networks. Inspired by human‚Äôs\nnatural mechanism, Yang et al. (2016) introduce the\nattention mechanism to VQA and achieve success.\n2282\nFor bilinear feature fusion, some attention mecha-\nnisms have been proposed, such as co-attention (Lu\net al., 2016) and dual attention (Nam et al., 2017).\nIn terms of trilinear feature fusion, the attention\nmap for trilinear inputs is computed by PARAL-\nING decomposition (Do et al., 2019). However, the\noutput is only a joint vector for classiÔ¨Åcation. In\norder to enhance each single modality representa-\ntion by fusing the other modalities, we propose\nTrilinear Interaction Attention (TrI-Att). More-\nover, although self-attention can not fuse different\nmodalities, it can enhance the interaction within\neach modality (Yu et al., 2019). Therefore, we de-\nsign Self-Attention (Self-Att) unit for capturing the\nintra-modality information.\nMultimodal contextual representations. The\ntransformer-based models can achieve good per-\nformance in the vision-language tasks. These mod-\nels normally employ multi-layer transformers to\nlearn multimodal contextual representations. There\nare two basic types of their architectures: single-\nstream and two-stream. The single-stream models\nconcatenate image and language features Ô¨Årst, and\nthen they get the cross-modality representations\nwith a single multi-layer transformer, such as VL-\nBERT (Su et al., 2020) and UNITER (Chen et al.,\n2020). The two-stream models take advantage of\nself-attention transformers to encode language and\nimage features respectively, and then build joint\nrepresentations with cross-attention transformers,\nsuch as LXMERT (Tan and Bansal, 2019) and ViL-\nVERT (Lu et al., 2019). To better align vision-\nlanguage semantic, some works try to pre-train\ntransformer-based structures on a large corpus of\nimage-text pairs. The pre-training tasks usually\ninclude masked language prediction, RoI-feature\nregression, detected-label classiÔ¨Åcation and cross-\nmodality matching (Tan and Bansal, 2019). In this\npaper, our proposed trilinear transformers deal with\nthe three input embeddings different from former\ntransformer-based methods.\n3 MIRTT: Learning M ultimodal\nInteraction Representations from\nTrilinear Transformers\nAs shown in Figure 3, our model considers three\nmodality forms of input (e.g., images, questions\nand answers). The backbone of MIRTT is two\ntransformers with multiple layers, which are based\non TrI-Att and Self-Att mechanisms. Finally, in\nthe output layer, we adopt MLP for speciÔ¨Åc down-\nstream tasks.\n3.1 Single-modality Embedding Extraction\nImage embeddings. The image embeddings are\nextracted from a Faster R-CNN model (Anderson\net al., 2018), a regional visual feature extractor. In\nterms of speciÔ¨Åcation, for each object, it extracts\na vector with dv dimensions. Therefore, an im-\nage with v objects is represented as an embedding\nmatrix V ‚ààRv√ódv .\nQuestion and answer embeddings. We adopt\nBERT (Devlin et al., 2019) to Ô¨Åne-tune as our text\nextractor in the experiments. SpeciÔ¨Åcally, the text\nis converted to WordPiece embeddings Ô¨Årst (Wu\net al., 2016). Then through Ô¨Åne-tuning, each em-\nbedding will be projected intoRdq or Rda , for ques-\ntion and answer, respectively. Finally, the question\nwith a max length ofq is represented asQ ‚ààRq√ódq ,\nand the same for the answer that A ‚ààRa√óda .\n3.2 Trilinear Transformers\nImage Embedding\nQuestion Embedding\nAnswer Embedding\nM\nTrI-Att\nfQ\nfA\nfV\nFigure 2: Trilinear interaction attention\nTrI-Att for inter-modality representations. For\nbetter cross-modality information fusion, we de-\nsign TrI-Att to project single-modality embed-\nding into inter-modality enhanced space (Figure\n2). From section 3.1, let S = {V, Q, A}be the\nmultimodal input collection. Firstly, we introduce\nthe attention map M ‚ààRv√óq√óa, which is mainly\ncomputed by matrix multiplication and sum-based\ndimension reduction. The detailed calculation pro-\ncess is as follows:\nM = softmax\n(‚àë\ndv\n‚àë\ndq\n‚àë\nda V ‚äóQ ‚äóA\n‚àö\nd\n)\n(1)\nwhere softmax is a normalization operation of all\nelements in M, and d is the arithmetic mean of dv,\n2283\nImage Embedding\nQuestion Embedding\nAnswer Embedding\nImage \nExtractor\nText \nExtractor\nText \nExtractor\nSelf-Att\nùëÅùêø √ó\nFF FF\nImage representations\nSelf-Att\nFF FF\nQuestion representations\nSelf-Att\nFF FF\nAnswer representations\nSingle modality Transformer\nTrI-Att\nMultiple modalities Transformer\nTrilinear transformer\nFigure 3: Model architecture of MIRTT\ndq and da. Secondly, the fusion of initial single-\nmodality representation and the attention map f is\nconducted as follows:\nfV =\n‚àë\nq\n‚àë\na\nMV = TrI-AttV(V, Q, A) (2)\nhere, we take image representation V for example\n(questions and answers are as the same), and the\nfusion operation is similar to Eq. 1.\nWe further utilize multi-head attention (Vaswani\net al., 2017) to improve the robustness by intro-\nducing a linear mapping for each single-modality\nrepresentation. In general, the complete calculation\nof inter-modality fusion is as follows:\nfV =\nNh\n||\ni\nTrI-AttVi (\nV Wi\nV , QWi\nQ, AWi\nV\n)\n(3)\nwhere Wi\nV , Wi\nQ and Wi\nA are multi-head linear map-\npings, which are shared across the three forms of\nrepresentations. Nh is the number of heads. ||\nindicates the concatenation of all multi-heads. Sim-\nilarly, the fusion representations of questions and\nanswers are:\nfQ =\nNh\n||\ni\nTrI-AttQi (\nV Wi\nV , QWi\nQ, AWi\nV\n)\n(4)\nfA =\nNh\n||\ni\nTrI-AttAi (\nV Wi\nV , QWi\nQ, AWi\nV\n)\n(5)\nAfter that, a fully connected feed-forward net-\nwork with residual connection follows.\nSelf-Att for intra-modality representations. We\napply the encoder of Transformer (Vaswani et al.,\n2017) to capture the intra-modality relationships.\nWe deploy a multi-head self-attention mechanism,\nfollowed by a feed-forward network with the resid-\nual connection. With input feature X ‚ààRn√ód, the\nmulti-head self-attention is working as:\nNh\n||\ni\nSelf-AttM (X) =\nNh\n||\ni\nsoftmax\n(XX T\n‚àö\nd\n)\nXW i\nM\n(6)\nwhere Wi\nM ‚ààRd√ódh is the projection matrix for\na certain modality M in ith head. This structure\ncan enhance the long-distance dependency among\nthe multi-modality features, while weaken negative\nimpact on the result to a certain degree.\nTrilinear transformers stacks. In total, the tri-\nlinear transformer stacks NL layers, where each\nlayer efÔ¨Åciently combines two transformer mod-\nules. The multiple modalities transformer has a tri-\nlinear interaction attention module and a fully con-\nnected feed-forward (FF) network. And the single\nmodality transformer has three self-attention mod-\nules, following the same structure of the encoder in\nTransformer (Vaswani et al., 2017). Our essential\nmotivation is to take advantage of the answer in-\nformation, so a trilinear model is deployed Ô¨Årst to\nfuse the three modality information. However, this\nleads to the loss of information in each modality\nto some extent, so a single-modality transformer is\nfollowed to reinforce the information of each own.\nFor the MC VQA task, we put the pooled answer\nrepresentations of the Ô¨Ånal layer into a binary clas-\nsiÔ¨Åer. Pick the answer of the highest binary score\nas the right one.\n3.3 Two-stage WorkÔ¨Çow\nIn FFOE VQA, previous models usually do not\ntake the answer as input for keeping the same input\ndimensions in the training and test phases because\n2284\nBilinear \nMethods\nTrilinear \nMethods\n[Answer1] zebras \n[Answer2] zebra\n[Answer3] giraffe\n‚Ä¶\n[Answern] yes\nCandidate answers list\n[Question] \nWhat animal is this?\nStage 1: FFOE task\n54%\n29%\n12%\nStage 2: MC task\nImage \nEncoder\nText \nEncoder\nText \nEncoder\nTraining / test phase\n[Answer2] \nzebra\nBilinear result\nTrilinear result\n‚ùå\n‚úî\n‚Ä¶\nFigure 4: The overview of two-stage workÔ¨Çow2\nthe answer is not available in the test set. Knowl-\nedge distillation is a solution that trilinear methods\ncould run by teaching a bilinear model in the train-\ning phase, and the bilinear model is evaluated in\nthe test phase. However, the answer information is\nstill inaccessible in the test set.\nTherefore, as shown in Figure 4, we introduce a\ntwo-stage workÔ¨Çow to make full use of the dataset.\nOur proposal is a universal simpliÔ¨Åcation process\nfor the FFOE VQA task, which gives full play to\nthe advantages of the bilinear and trilinear models.\nIn the Ô¨Årst stage, we train a bilinear model for\nthe questions and images Ô¨Årst, and then the top four\ncandidate answers are provided for each question\nbased on the output logits. Since the bilinear model\nperforms very high accuracy on the training set,\nthe candidate answers basically contain the correct\nanswer. In the test phase, the trilinear model is\nfully dependent on the candidates from the bilinear\nmodel.\nIn stage two, the candidates are Ô¨Årst restruc-\ntured into several image-question-answer pairs by\nreuse the input image and question; therefore, the\nnumber of the pairs is equal to that of the candi-\ndates. Then the trilinear model utilizes the image-\nquestion-answer pairs to choose a conÔ¨Ådent answer\nunder the MC VQA task setting (illustrated in Sec-\ntion 3.2), where the answer-question and answer-\nimage alignment information is learned.\n4 Experiments\n4.1 The Pre-training Strategy\nIn the hope of initializing our model effectively, we\npre-train our model with the masked language mod-\neling task, which is in a way similar to BERT (De-\nvlin et al., 2019). Since our model is trilinear, the\npre-training data format is triple of the question,\nimage, and correct answer. We utilize Visual7W,\nVQA-2.0, and TDIUC datasets (the training set) to\nDataset Model Acc-MC\nVisual7W\nMCB 62.2\nCTI 72.3\nMIRTT (Ours) 74.4\nVQA-1.0 MC\nDual-MFA 70.0\nMCB 70.1\nMFH 73.4\nMIRTT (Ours) 77.0\nTable 1: Comparison with the state-of-the-art results\non Visual7W and VQA-1.0. Our pre-trained MIRTT\nmodel outperforms previous methods.\npre-train MIRTT. In detail, We mask the tokens of\nquestions and answers with a probability of 15%.\nIn these masked tokens, 80% of them are replaced\nby sign [MASK], 10% of them are kept, and the\nother 10% are replaced with random tokens.\n4.2 Datasets and Evaluation Metrics\n4.2.1 MC VQA Tasks\nDataset. Visual7W is a subset of Visual\nGenome (Krishna et al., 2017). For each question-\nimage pair, there are four candidate answers, where\nonly one choice is correct. There are two tasks for\nVisual7W: pointing and telling, and we conduct our\nmethod on telling task. VQA-1.0 MC (Antol et al.,\n2015) is similar to Visual7W, while there are 18\ncandidate answers for each question.\nMetrics. Each question only has one correct\nanswer. Accuracy (Acc-MC) is used to measure\nthe performance (Zhu et al., 2016; Antol et al.,\n2015). We evaluate our methods on ‚Äútest‚Äù split of\nVisual7W and ‚Äútest-std‚Äù split of VQA-1.0 MC.\n2An example image-question pair from VQA-2.0\ndataset (Goyal et al., 2017)\n2285\nDataset Text extractor Bilinear method Bilinear result Trilinear result Ensemble result\nVQA-2.0\nGRU BAN2 66.5 65.5 68.9\nGRU SAN 63.0 65.0 66.8\nBERT MLP 59.5 64.3 65.5\nBERT ViLBERT 69.2 3 68.0 70.3\nTDIUC\nGRU BAN2 85.5 87.4 87.6\nGRU SAN 82.3 86.0 85.6\nBERT MLP 80.0 84.5 83.6\nGQA BERT BAN2 55.0 52.8 55.7\nBERT SAN 54.8 52.4 55.9\nTable 2: The results from test sets of VQA-2.0 and TDIUC. Comparisons between different bilinear methods and\ntext encoders on stage one. The trilinear results are from MIRTT models on stage two.\n4.2.2 FFOE VQA Tasks\nDataset. VQA-2.0 is built from MSCOCO\ndataset (Lin et al., 2014). VQA-2.0 minimizes an-\nswer biases so that a language-only ‚Äúblind‚Äù model\ncan not guess the right answers. TDIUC is a large\nVQA deadset of real images, which has over 1.6M\nquestions of 12 categories. GQA consists of 22M\nquestions, and each image corresponds to a scene\ngraph. The questions focus on visual reasoning and\ncompositional question answering.\nMetrics. In VQA-2.0, each question has ten\nhuman-generated answers. To present the inter-\nhuman variability, we deÔ¨Åne the accuracy-based\nevaluation metric (ACC) as follows (Wu et al.,\n2017):\nACC = min{n\n3 , 1} (7)\nwhere n is the frequency of the answer given by\nthe model in the answer set of the corresponding\nimage-question pair. In TDIUC and GQA, there is\nonly one right answer for each question. Therefore,\nnormal accuracy is used. For details, we evaluate\nour methods on ‚Äútest-dev‚Äù split of VQA-2.0, ‚ÄúValid‚Äù\nsplit of TDIUC, and ‚Äútest-std‚Äù split of GQA.\n4.3 Implementation Details\nExcept for the referenced models and special in-\nstructions, we Ô¨Åne-tune BERT as our text extrac-\ntor for questions and answers. And we freeze the\nFaster R-CNN detector (Anderson et al., 2018)\nwithout Ô¨Åne-tuning as the image extractor. For\nimages, the maximum detected bounding box is\n3The result is not the same as in the cited paper. Regret-\ntably, after a lot of experiments, we still cannot reach the\naccuracy in the cited papers. Under the fair experimental envi-\nronment, the ensemble result outperforms the bilinear result.\nset to 50. For texts, the questions and answers are\ntrimmed to a sentence with a maximum length of\n12 tokens and 6 tokens, respectively.\nThe hyper-parameters of MIRTT follow the de-\nfault unless otherwise noted. The dimensions of\ninput images (dv), questions (dq) and answers (da)\nare 2048, 768 and 768. To simplify the calculation,\nwe reduce dv to 768 with a linear projection. For\nthe TrI-Att and Self-Att, the number of heads is 12,\nand the hidden dimension dh is 64.\nIn all experiments with a two-stage work-\nÔ¨Çow, we utilize six layers MIRTT with col-\nlection 2 (Table 3). Furthermore, our codes\nwill be made publicly available with instructions\nhttps://github.com/IIGROUP/MIRTT. More exper-\nimental settings can be found in the Appendix.\n4.4 MIRTT Performance on MC VQA\nAs shown in Table 1, we compare our methods\nwith previous methods on Visual7W telling task\nand VQA-1.0 multiple-choice task.\nMCB (Fukui et al., 2016): a method that consid-\ners FFT space to combine multimodal features.\nCTI (Do et al., 2019): a method that learns high-\nlevel associations between three inputs by using\nmultimodal-tensor-based decomposition.\nDual-MFA (Lu et al., 2018): a framework that\nfuses input embedding by selecting the free-form\nimage regions and detection boxes most related to\nthe input question.\nMFH (Yu et al., 2018): a framework that models\nboth the image attention and question attention\nsimultaneously.\nOur MIRTT with Ô¨Åne-tuning (Table 3) improves\nthe CTI ACC-MC by 2.1% and improves the MFH\nACC-MC by 3.6%.\n2286\n4.5 MIRTT Performance on FFOE VQA\nTo evaluate the effectiveness of the two-stage work-\nÔ¨Çow (Figure 4), we apply several bilinear methods\nas our backbones in stage one and set our MIRTT\nas trilinear methods in stage two. In detail, the\ncandidate answers lists are generated by baselines;\neach contains four answers.\nSAN (Yang et al., 2016): Stacked Attention Net-\nwork utilizes multiple attention layers by querying\nan image multiple times to infer the answer.\nBAN2 (Kim et al., 2018): Bilinear Attention\nNetwork fuses the question embeddings and image\nembeddings by utilizing co-attention.\nMLP: in this method, we use the Ô¨Årst output\ntoken embedding as the global representation of a\nquestion. Then, we sum up all object embeddings\nof an image after multiplying a learning weight\nfor each one. The global representations of the\nquestion and the image are then added and fed into\nan MLP layer for classiÔ¨Åcation.\nViLBERT (Lu et al., 2019) builds intra- and\ninter-relationship between vision and language\nbase on a pretrained transformer structure.\nEnsemble results : the predictions are calcu-\nlated by considering the outputs of stage one and\nstage two. The ensemble method normalizes the\ntwo results separately and adds them together. The\nÔ¨Ånal prediction is the candidate answer with the\nhighest probability.\nAs shown in Table 2, the trilinear results and en-\nsemble results outperform the bilinear results. Our\ntwo-stage workÔ¨Çow solves the problem that trilin-\near models are not able to be deployed in FFOE\nVQA. Furthermore, ensemble results show that bi-\nlinear models can utilize the answers after model-\ning the answer information by the trilinear models.\nIn particular, the GQA dataset is not introduced\nin pre-training data. Our two-stage workÔ¨Çow and\nMIRTT present better performances than the base-\nline methods, which shows the generalization capa-\nbility of our approaches.\n4.6 Ablation Studies\n4.6.1 The Components of MIRTT\nStacking layers and the size of pre-training\ndata. As shown in Table 3, MIRTT only needs\ntwo layers to signiÔ¨Åcantly outperform the others in\n‚ÄúRandom‚Äù based on accuracy.\nRandom: MIRTT is trained on Visual7W with-\nout pre-training.\nLayers Random Collection 1 Collection 2\n1 70.3 - -\n2 70.9 73.0 73.7\n4 70.4 73.3 74.2\n6 70.6 73.5 74.2\n8 70.3 73.5 74.4\nTable 3: The behaviors of the MIRTT with a different\nnumber of layers and different sizes of pre-training data\non the Visual7W dataset.\nCollection 1: MIRTT is pre-trained on the train\nsets of Visual7W and VQA-2.0.\nCollection 2: MIRTT is pre-trained on the train\nsets of Visual7W, VQA-2.0, and TDIUC.\nAfter pre-training, MIRTT outperforms the non-\npre-trained one in each layer from random and\ncollection 1. And as the number of layers increases,\nthe accuracy of MIRTT with collection 1 is im-\nproved. However, as the number of layers increases,\nthe capability of MIRTT with collection 1 seems\nto reach its limit at six layers, and growth hits a\nbottleneck.\nTherefore, we add one more dataset to pre-train\nMIRTT. Comparing with collection 1, MIRTT in\ncollection 2 can break the previously mentioned\nbottleneck and reaches the best score at the high-\nest layer with more pre-train data. Perhaps similar\nto ViT (Dosovitskiy et al., 2021), these attention-\nbased deep models are sensitive to dataset size.\nTherefore, the pre-trained MIRTT beneÔ¨Åts from a\nlarger number of parameters and more data, achiev-\ning an accuracy of 74.4%. Moreover, we conduct\nthe randomized Tukey HSD p-values and effect\nsizes based on one-way ANOV A (Sakai, 2018) to\nsupport statistical signiÔ¨Åcance of our results. De-\ntails are in the Appendix.\nAttention mechanisms. Since CTI does not con-\nsider the intra-modality information, we attempt\nto build some structures to enhance it. In the\nterm ‚ÄúCTI + Self-Att‚Äù, the original output of CTI\nis a joint representation, then make a fusion by\nadding text embeddings and the joint representa-\ntion. After that, we implement the Transformer‚Äôs\nencoders (Vaswani et al., 2017) with two layers.\nAs shown in Table 4, after adding self-attention to\nobtain Ô¨Åne-grained information within the modal-\nity, the CTI is improved by 0.5% compared to the\noriginal model.\nBERT‚àó: We Ô¨Åne-tune BERT on input questions\nand answers and fuse the extracted image embed-\n2287\n[Question]\nWhat is the main color of the train?\n[Answer list]\nBlue ‚ùå\nYellow ‚ùå\nRed ‚úî\nGold ‚ùå\n(a) (b) (c)\nFigure 5: The visualization of the attention map M from Eq. 1. The attention map is extracted from the last layer\nof our best model with the best result on Visual7W. (a) is an example image-question-answer pair from the test set\nof Visual7W (Zhu et al., 2016). The input image is attached with bounding boxes. (b) includes the related attention\nmaps for answers (‚ÄúRed‚Äù and ‚ÄúGold‚Äù). The details of each answer tokens are presented on (c).\nMethod Acc-MC\nCTI 72.3\nCTI + Self-Att 72.8\nBERT‚àó 65.4\nBERT + TrI-Att 70.5\nBERT + TrI-Att + Self-Att (MIRTT) 70.9\nTable 4: Ablation experiments for attention mecha-\nnism, evaluated on the test set of Visual7W.\ndings. In detail, we utilize the same operation\nof Bottom-Up and Top-Down (BUTD) (Anderson\net al., 2018) to fuse all representations.\nTo discuss two key components (‚ÄúTrI-Att‚Äù and\n‚ÄúSelf-Att‚Äù), we utilize two layers of MIRTT without\npre-training as our basic structure. By replacing the\nsimple fusion methods like adding, we enhance the\ninput embeddings by considering the inter-modality\ninformation in TrI-Att. 5.1% improves the accuracy\nas a result. Considering CTI can beneÔ¨Åt from self-\nattention mechanism, we implement the Self-Att in\nour trilinear transformers. From the relative 0.4%\nimprovement, our MIRTT can also learn the intra-\nmodality information like ‚ÄúCTI + Self-Att‚Äù.\nVisualization for TrI-Att. Figure 5 visualizes\nthe behavior of MIRTT by showing detailed at-\ntention values of TrI-Att. The detected objects are\npresented with their numerical labels. The spe-\ncial tokens in questions and answers are provided\nby BERT (Devlin et al., 2019). For the image-\nquestion-(answer ‚ÄúRed‚Äù) pair, the correlation of\nobject ‚Äú5‚Äù (the train) and token ‚ÄúRed‚Äù has a great\nattention value. Moreover, the model focuses on\nthe pair ‚Äú5‚Äù-‚Äútrain‚Äù-‚ÄúRed‚Äù, which is helpful in rea-\nsoning that the train in the image is red. In terms\nof the answer ‚ÄúGold‚Äù, the locomotive (object ‚Äú2‚Äù)\ngains more attention than the object ‚Äú2‚Äù in ‚ÄúRed‚Äù.\nTherefore, the answers could assist MIRTT in pre-\ndicting the correct choices.\n4.6.2 Cases for Two-stage WorkÔ¨Çow\nFigure 6 describes some examples with applying\nour two-stage workÔ¨Çow (Figure 4). In detail, the\ntext extractors are all GRU, and the trilinear meth-\nods are MIRTT. The results show that our trilin-\near method is able to retrieve the most proper an-\nswer by utilizing the abundant information of the\nanswers. Whether the problem requires stronger\nreasoning skills in (a), or the ability to Ô¨Ånd corre-\nspondences (images, questions, and answers) as in\n(b) and (c), MIRTT can handle it with a two-stage\nworkÔ¨Çow. Following different bilinear methods\nas backbones, the trilinear method might predict\ndifferent answers, such as (d).\n5 Conclusions\nWe introduced a trilinear interaction framework\ncalled MIRTT, which captures inter-modality and\nintra-modality information of images, questions,\nand answers. Our method is based on TrI-Att and\nSelf-Att mechanisms. The pre-trained model shows\nthe effectiveness among the baselines on several\ndatasets. Meanwhile, a two-stage workÔ¨Çow is intro-\nduced to apply the trilinear methods to FFOE VQA,\n2288\n(a) Q: What room are they \nlocated in?\noffice ‚ùå\nclassroom ‚úî\n(b) Q: Are people cooking?\nyes ‚ùå\nno ‚úî\n(d) Q: How many people are \nin this photo?\n6 ‚ùå\n5 ‚ùå\n(c) Q: What color is the \nglass covering the pilot?\nwhite ‚ùå\nblue ‚úî\n[Bilinear Answer]\n[TrilinearAnswer]\n[Images]\n[Questions]\n[Bilinear Answer]\n[TrilinearAnswer]\nliving room ‚ùå\nclassroom ‚úî\nyes ‚ùå\nno ‚úî\nwhite ‚ùå\nblue ‚úî\n5 ‚ùå\n4 ‚úî\nBilinear \nBackbone\nBAN2\nSAN\nFigure 6: A collection of image-question-answer pairs by random selection from VQA-2.0 (Goyal et al., 2017).\nComparisons of whether to use two-stage workÔ¨Çow and different bilinear methods at the stage one in the test phase.\nshowing improvements on VQA-2.0, TDIUC and\nGQA. We achieve state-of-the-art results on Vi-\nsual7W and VQA-1.0 MC. Generally, with rich ex-\nperimental comparisons and extensive discussion,\nwe demonstrate the value of the answer information\nand provide a solution for the VQA tasks.\nAcknowledgements\nThis research was supported by the Key Program of\nthe National Natural Science Foundation of China\nunder Grant No. U1903213. We would like to\nthank members of The Real Sakai Laboratory 4,\nWaseda University, for giving us suggestions. Jun-\njie Wang is especially grateful to our friend Yuxi-\nang Zhang for his support, advice, and encourage-\nment.\nReferences\nPeter Anderson, X. He, Chris Buehler, Damien Teney,\nMark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for im-\nage captioning and visual question answering. 2018\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6077‚Äì6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425‚Äì2433.\nHedi Ben-Younes, R√©mi Cadene, Matthieu Cord, and\nNicolas Thome. 2017. Mutan: Multimodal tucker\nfusion for visual question answering. In Proceed-\nings of the IEEE international conference on com-\nputer vision, pages 2612‚Äì2620.\n4http://sakailab.com/english/\nHedi Ben-Younes, Remi Cadene, Nicolas Thome, and\nMatthieu Cord. 2019. Block: Bilinear superdiago-\nnal fusion for visual question answering and visual\nrelationship detection. In Proceedings of the AAAI\nConference on ArtiÔ¨Åcial Intelligence , volume 33,\npages 8102‚Äì8109.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104‚Äì120. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT (1), pages 4171‚Äì4186. As-\nsociation for Computational Linguistics.\nTuong Do, Thanh-Toan Do, Huy Tran, Erman Tjiputra,\nand Quang D Tran. 2019. Compact trilinear interac-\ntion for visual question answering. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 392‚Äì401.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR. OpenReview.net.\nAkira Fukui, Dong Huk Park, Daylen Yang, Anna\nRohrbach, Trevor Darrell, and Marcus Rohrbach.\n2016. Multimodal compact bilinear pooling for vi-\nsual question answering and visual grounding. In\nEMNLP, pages 457‚Äì468. The Association for Com-\nputational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nv in vqa matter: Elevating the role of image under-\nstanding in visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904‚Äì6913.\n2289\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR, pages 770‚Äì778. IEEE Computer\nSociety.\nHexiang Hu, Wei-Lun Chao, and Fei Sha. 2018. Learn-\ning answer embeddings for visual question answer-\ning. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 5428‚Äì\n5436.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: a new dataset for compositional ques-\ntion answering over real-world images. CoRR,\nabs/1902.09506.\nAllan Jabri, Armand Joulin, and Laurens Van\nDer Maaten. 2016. Revisiting visual question an-\nswering baselines. In European conference on com-\nputer vision, pages 727‚Äì739. Springer.\nKushal KaÔ¨Çe and Christopher Kanan. 2017a. An analy-\nsis of visual question answering algorithms. In Pro-\nceedings of the IEEE International Conference on\nComputer Vision, pages 1965‚Äì1973.\nKushal KaÔ¨Çe and Christopher Kanan. 2017b. An analy-\nsis of visual question answering algorithms. In Pro-\nceedings of the IEEE International Conference on\nComputer Vision, pages 1965‚Äì1973.\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\n2018. Bilinear attention networks. In NeurIPS,\npages 1571‚Äì1581.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32‚Äì\n73.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard H. Hovy. 2017. RACE: large-scale read-\ning comprehension dataset from examinations. In\nEMNLP, pages 785‚Äì794. Association for Computa-\ntional Linguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121‚Äì137. Springer.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\nHe, and Piotr Doll√°r. 2017. Focal loss for dense ob-\nject detection. In Proceedings of the IEEE interna-\ntional conference on computer vision , pages 2980‚Äì\n2988.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740‚Äì755. Springer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In NeurIPS, pages 13‚Äì23.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\n2016. Hierarchical question-image co-attention for\nvisual question answering. In NIPS, pages 289‚Äì297.\nPan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang,\nand Xiaogang Wang. 2018. Co-attending free-form\nregions and detections with multi-modal multiplica-\ntive feature embedding for visual question answer-\ning. In Proceedings of the AAAI Conference on Arti-\nÔ¨Åcial Intelligence, volume 32.\nHyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim.\n2017. Dual attention networks for multimodal rea-\nsoning and matching. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 299‚Äì307.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, pages 1‚Äì26.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. 2017. Faster R-CNN: towards real-time ob-\nject detection with region proposal networks. IEEE\nTrans. Pattern Anal. Mach. Intell., 39(6):1137‚Äì1149.\nTetsuya Sakai. 2018. Laboratory experiments\nin information retrieval: Sample Sizes, Ef-\nfect Sizes, and Statistical Power . Springer.\nhttps://link.springer.com/book/10.\n1007/978-981-13-1199-4 .\nKaren Simonyan and Andrew Zisserman. 2015. Very\ndeep convolutional networks for large-scale image\nrecognition. In ICLR.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn ICLR. OpenReview.net.\nHao Tan and Mohit Bansal. 2019. LXMERT: learning\ncross-modality encoder representations from trans-\nformers. In EMNLP/IJCNLP (1), pages 5099‚Äì5110.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998‚Äì6008.\n2290\nZhe Wang, Xiaoyi Liu, Limin Wang, Yu Qiao, Xiao-\nhui Xie, and Charless Fowlkes. 2018. Structured\ntriplet learning with pos-tag guided attention for vi-\nsual question answering. In 2018 IEEE Winter Con-\nference on Applications of Computer Vision (WACV),\npages 1888‚Äì1896. IEEE.\nQi Wu, Damien Teney, Peng Wang, Chunhua Shen, An-\nthony Dick, and Anton van den Hengel. 2017. Vi-\nsual question answering: A survey of methods and\ndatasets. Computer Vision and Image Understand-\ning, 163:21‚Äì40.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nZichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,\nand Alexander J. Smola. 2016. Stacked attention\nnetworks for image question answering. In CVPR,\npages 21‚Äì29. IEEE Computer Society.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and\nQi Tian. 2019. Deep modular co-attention networks\nfor visual question answering. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6281‚Äì6290.\nZhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and\nDacheng Tao. 2018. Beyond bilinear: Generalized\nmultimodal factorized high-order pooling for visual\nquestion answering. IEEE transactions on neural\nnetworks and learning systems, 29(12):5947‚Äì5959.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n4995‚Äì5004.\n2291\nA Dataset details\nDataset Train Valid Test\nVisual7W 69.8k 28.0k 42.0k\nVQA-1.0 248.3k 121.5k 244.3k\nVQA-2.0 443.8k 214.4k 447.8k\nTDIUC 1115.3k 538.9k -\nGQA 15.4M 2.2M 4.2M\nTable 5: The sizes of datasets associated to our paper\nPre-training Dataset. Amount of data for pre-\ntraining datasets is shown in Table 5. Pre-training\ndataset collection 1 includes Visual7W (train/val)\nand VQA-2.0 (train/val). In VQA-2.0, there is a\nlist of human-generated answers to one question.\nWe treat the answer with the highest score as the\ncorrect answer. The size of the image-question-\nanswer pre-training collection is about 756k. In\ncollection 2, we add the train set of TDIUC to pre-\ntrain MIRTT. The size of pre-training tuples grows\nto 1.87M.\nB Implementation details\nThe whole details will be presented in our open-\nsource codes on Github. The Focal loss (Lin et al.,\n2017) is used for training the proposed models.\nPre-training. When we pre-train MIRTT, the\nbatch size is 128, and the initial learning rate is\n1e-4. We use the model of epoch 7 for later Ô¨Åne-\ntuning.\nVisual7W. Batch size is set to 32 for all models.\nFor random initialization, the learning rate is 1e-4,\nand the number of the epoch is 17. For Ô¨Åne-tuning,\nthe learning rate is 3e-5, and the number of the\nepoch is 11.\nVQA-1.0. Initial learning rate is set to 1e-4, and\nbatch size is 16. Since each question has 18 choices,\nthere are 288 samples in one batch.\nTwo-stage workÔ¨Çow. The settings of hyper-\nparameters of two-stage workÔ¨Çow are presented\nin Table 6. Stage one and stage two are separate,\nnot end-to-end structures. In stage one, the bilin-\near model is trained and we adopt a cross-entropy\nloss function to get the logits of the answers. Then,\nwe get the top four candidates based on the logits,\nwhich is the generated answer list. In stage two, we\nencode the answers and put those embeddings of\nthree modalities into MIRTT to get representations.\nWe put the pooled answer representations into a\nbinary classiÔ¨Åer and apply binary cross-entropy\nbased on labels generated from the FFOE dataset.\nThe number of candidate answers. To make our\nproposal a universal framework on both FFOE\nVQA and MC VQA tasks, we set the candidates to\nbe four Visual7W on VQA and RACE (Lai et al.,\n2017) on QA. We will do related explorations based\non this in the future. There are a few interesting\nproblems. For example, if the candidate answers\ndon‚Äôt include the correct answer, the trilinear model\nwon‚Äôt work for this question. However, this prob-\nlem is always possible unless the candidate list\nincludes all the answers, which is impossible. A\nlimited extension of the candidate list could help\nimprove the coverage of correct answers while con-\ntradicting our design‚Äôs universality.\nC P-value based on Randomized Tukey\nHSD tests\nTable 7, Table 8 and Table 9 show the statistical\nsigniÔ¨Åcance test results of the runs on Table 3. The\nname of runs are following the rules: name = D_L,\nwhere D ‚àà{Rand, Col1, Col2}is the name of the\nsize of pre-training data and L ‚àà{1, 2, 4, 6, 8}is\nthe number of layers to use in MIRTT. For example,\nCol2_2 stands for two layers MIRTT in collection\n2.\n2292\nStage one Stage two\nDataset Text extractors Bilinear method BS LR BS LR\nVQA-2.0\n(test-dev)\nGRU SAN 256 1.00E-03 64 1.00E-04\nGRU BAN2 256 1.00E-03 64 1.00E-04\nBERT MLP 256 1.00E-04 64 1.00E-04\nBERT ViLBERT 256 1.00E-04 64 1.00E-04\nTDIUC\n(valid)\nGRU BAN2 256 1.00E-03 64 1.00E-04\nGRU SAN 256 1.00E-03 64 1.00E-04\nBERT MLP 256 1.00E-04 64 1.00E-04\nGQA\n(test-dev,test-std)\nBERT BAN2 256 1.00E-04 64 1.00E-04\nBERT SAN 256 1.00E-04 64 1.00E-04\nTable 6: The settings of hyper-parameters of two-stage workÔ¨Çow.\nRand_1 Rand_2 Rand_4 Rand_6\nRand_2 p <0.001 (-0.835) - - -\nRand_4 p <0.001 (-0.472) p <0.001 (0.363) - -\nRand_6 p <0.001 (-0.480) p <0.001 (0.355) p = 0.706 (-0.008) -\nRand_8 p <0.001 (-0.556) p <0.001 (0.279) p <0.001 (-0.084) p <0.001 (-0.076)\nCol1_2 p <0.001 (-0.659) p <0.001 (0.176) p <0.001 (-0.187) p <0.001 (-0.179)\nCol1_4 p <0.001 (-0.793) p <0.001 (0.043) p <0.001 (-0.320) p <0.001 (-0.313)\nCol1_6 p <0.001 (-0.706) p <0.001 (0.128) p <0.001 (-0.234) p <0.001 (-0.227)\nCol1_8 p <0.001 (-0.651) p <0.001 (0.184) p <0.001 (-0.179) p <0.001 (-0.172)\nCol2_2 p <0.001 (-0.549) p <0.001 (0.286) p <0.001 (-0.076) p <0.001 (-0.069)\nCol2_4 p <0.001 (-0.363) p <0.001 (0.472) p <0.001 (0.109) p <0.001 (0.116)\nCol2_6 p <0.001 (-0.280) p <0.001 (0.555) p <0.001 (0.192) p <0.001 (0.200)\nCol2_8 p <0.001 (-1.701) p <0.001 (-0.866) p <0.001 (-1.230) p <0.001 (-1.222)\nTable 7: Statistical signiÔ¨Åcance calculated by Randomized Tukey HSD tests after 1,000 simulations. P-value and\neffect size. (Part 1)\nRand_8 Col1_2 Col1_4 Col1_6\nCol1_2 p <0.001 (-0.103) - - -\nCol1_4 p <0.001 (-0.236) p <0.001 (-0.134) - -\nCol1_6 p <0.001 (-0.150) p <0.001 (-0.048) p <0.001 (0.086) -\nCol1_8 p <0.001 (-0.095) p = 0.759 (0.007) p <0.001 (0.141) p <0.001 (0.055)\nCol2_2 p = 0.716 (0.008) p <0.001 (0.110) p <0.001 (0.244) p <0.001 (0.158)\nCol2_4 p <0.001 (0.193) p <0.001 (0.295) p <0.001 (0.429) p <0.001 (0.342)\nCol2_6 p <0.001 (0.276) p <0.001 (0.379) p <0.001 (0.513) p <0.001 (0.427)\nCol2_8 p <0.001 (-1.146) p <0.001 (-1.043) p <0.001 (-0.909) p <0.001 (-0.995)\nTable 8: Statistical signiÔ¨Åcance calculated by Randomized Tukey HSD tests after 1,000 simulations. P-value and\neffect size. (Part 2)\nCol2_2 Col2_4 Col2_6\nCol2_4 p <0.001 (0.185) - -\nCol2_6 p <0.001 (0.269) p <0.001 (0.084) -\nCol2_8 p <0.001 (-1.153) p <0.001 (-1.338) p <0.001 (-1.422)\nTable 9: Statistical signiÔ¨Åcance calculated by Randomized Tukey HSD tests after 1,000 simulations. P-value and\neffect size. (Part 3)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.738559901714325
    },
    {
      "name": "Question answering",
      "score": 0.7256353497505188
    },
    {
      "name": "Modality (human‚Äìcomputer interaction)",
      "score": 0.6660913228988647
    },
    {
      "name": "Bilinear interpolation",
      "score": 0.5915285348892212
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5881734490394592
    },
    {
      "name": "Transformer",
      "score": 0.5743052363395691
    },
    {
      "name": "Multimodal learning",
      "score": 0.5114204287528992
    },
    {
      "name": "Workflow",
      "score": 0.4498814046382904
    },
    {
      "name": "Task (project management)",
      "score": 0.43061262369155884
    },
    {
      "name": "Focus (optics)",
      "score": 0.4263514578342438
    },
    {
      "name": "Machine learning",
      "score": 0.38859695196151733
    },
    {
      "name": "Natural language processing",
      "score": 0.3771543800830841
    },
    {
      "name": "Computer vision",
      "score": 0.09160959720611572
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3131625388",
      "name": "University Town of Shenzhen",
      "country": "CN"
    }
  ]
}