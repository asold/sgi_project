{
    "title": "TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation",
    "url": "https://openalex.org/W3206636652",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2746830174",
            "name": "Ma, Haoyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2329424953",
            "name": "Chen Liangjian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2357847484",
            "name": "Kong Deying",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1062931711",
            "name": "Wang Zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352741798",
            "name": "Liu Xingwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101529693",
            "name": "Tang, Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2654983670",
            "name": "Yan, Xiangyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2328882986",
            "name": "Xie, Yusheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5031416720",
            "name": "Lin Shih‐Yao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2070434824",
            "name": "Xie Xiaohui",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2984313141",
        "https://openalex.org/W3176193798",
        "https://openalex.org/W2611932403",
        "https://openalex.org/W2901587941",
        "https://openalex.org/W2580084284",
        "https://openalex.org/W3197032408",
        "https://openalex.org/W2916798096",
        "https://openalex.org/W3034971010",
        "https://openalex.org/W3136525061",
        "https://openalex.org/W3009175371",
        "https://openalex.org/W2793814912",
        "https://openalex.org/W2963402313",
        "https://openalex.org/W3191850102",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2136391815",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2964221239",
        "https://openalex.org/W3010448990",
        "https://openalex.org/W2307770531",
        "https://openalex.org/W3142330916",
        "https://openalex.org/W3035464792",
        "https://openalex.org/W3206685025",
        "https://openalex.org/W3119780049",
        "https://openalex.org/W1968076120",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W3153807375",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2964304707",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W2984612350",
        "https://openalex.org/W2138496866",
        "https://openalex.org/W2973857456",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W3094830752",
        "https://openalex.org/W3009246422",
        "https://openalex.org/W3112160422",
        "https://openalex.org/W2516631066",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2118931255",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W3117707723",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2946127255",
        "https://openalex.org/W3021282624",
        "https://openalex.org/W2963488642",
        "https://openalex.org/W3184564979",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3009765082",
        "https://openalex.org/W2033819227",
        "https://openalex.org/W3158818292",
        "https://openalex.org/W2039477464",
        "https://openalex.org/W3087784721",
        "https://openalex.org/W3211316995",
        "https://openalex.org/W2101032778",
        "https://openalex.org/W2964093990",
        "https://openalex.org/W2964179555",
        "https://openalex.org/W2583585015",
        "https://openalex.org/W2974705243",
        "https://openalex.org/W3126761340"
    ],
    "abstract": "Estimating the 2D human poses in each view is typically the first step in calibrated multi-view 3D pose estimation. But the performance of 2D pose detectors suffers from challenging situations such as occlusions and oblique viewing angles. To address these challenges, previous works derive point-to-point correspondences between different views from epipolar geometry and utilize the correspondences to merge prediction heatmaps or feature representations. Instead of post-prediction merge/calibration, here we introduce a transformer framework for multi-view 3D pose estimation, aiming at directly improving individual 2D predictors by integrating information from different views. Inspired by previous multi-modal transformers, we design a unified transformer architecture, named TransFusion, to fuse cues from both current views and neighboring views. Moreover, we propose the concept of epipolar field to encode 3D positional information into the transformer model. The 3D position encoding guided by the epipolar field provides an efficient way of encoding correspondences between pixels of different views. Experiments on Human 3.6M and Ski-Pose show that our method is more efficient and has consistent improvements compared to other fusion methods. Specifically, we achieve 25.8 mm MPJPE on Human 3.6M with only 5M parameters on 256 x 256 resolution.",
    "full_text": "H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 1\nTransFusion: Cross-view Fusion with\nTransformer for 3D Human Pose Estimation\nHaoyu Ma1\nhaoyum3@uci.edu\nLiangjian Chen2\nclj@fb.com\nDeying Kong1\ndeyingk@uci.edu\nZhe Wang1\nzwang15@uci.edu\nXingwei Liu1\nxingweil@uci.edu\nHao Tang1\nhtang6@uci.edu\nXiangyi Y an1\nxiangyy4@uci.edu\nYusheng Xie3\nyushx@amazon.com\nShih-Y ao Lin4\nmike.lin@ieee.org\nXiaohui Xie1\nxhx@uci.edu\n1 University of California, Irvine\n2 Facebook\n(work done outside of Facebook)\n3 Amazon\n(work done outside of Amazon)\n4 R&D Center US Laboratory,\nSony Corporation of America\n(work done outside of Sony)\nAbstract\nEstimating the 2D human poses in each view is typically the ﬁrst step in calibrated\nmulti-view 3D pose estimation. But the performance of 2D pose detectors suffers from\nchallenging situations such as occlusions and oblique viewing angles. To address these\nchallenges, previous works derive point-to-point correspondences between different views\nfrom epipolar geometry and utilize the correspondences to merge prediction heatmaps or\nfeature representations. Instead of post-prediction merge/calibration, here we introduce a\ntransformer framework for multi-view 3D pose estimation, aiming at directly improving\nindividual 2D predictors by integrating information from different views. Inspired by\nprevious multi-modal transformers, we design a uniﬁed transformer architecture, named\nTransFusion, to fuse cues from both current views and neighboring views. Moreover,\nwe propose the concept of epipolar ﬁeld to encode 3D positional information into the\ntransformer model. The 3D position encoding guided by epipolar ﬁeld provides an efﬁ-\ncient way of encoding correspondences between pixels of different views. Experiments\non Human 3.6M and Ski-Pose show that our method is more efﬁcient and has consis-\ntent improvements compared to other fusion methods. Speciﬁcally, we achieve 25.8 mm\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2110.09554v3  [cs.CV]  9 Dec 2021\n2 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\nMPJPE on Human 3.6M with only 5M parameters on 256×256 resolution. Source code\nand trained model can be found at https://github.com/HowieMa/TransFusion-Pose.\n1 Introduction\nEstimating the 3D locations of human joints is a critical task for many AI applications such as\naugmented reality, virtual reality and medical diagnosis [9]. The estimation is often carried\nout in two common settings: One is estimating the 3D pose from monocular images [5,\n6, 14, 32, 43, 50, 59, 66], and the other is estimating 3D poses from multiple cameras\n[7, 19, 34, 37, 49]. The former is challenging due to the ambiguity of depth estimation\nwith only one view. The latter setting, the focus of this paper, usually obtains better 3D\npose estimation performance since the multi-view settings can help resolve depth ambiguity.\nMost multi-view works follow a two-step pipeline that ﬁrstly estimates 2D poses in each\nview and then recovers 3D pose from them. However, it is still difﬁcult to solve challenging\ncases such as occlusions in the ﬁrst step, and the estimated 3D poses are often inaccurate as\nit depends on the results from the ﬁrst step.\nResearchers have sought to introduce the 3D information in the ﬁrst step to improve\nthe 2D pose detector, because the challenging cases in one view are potentially easier to\nsolve in other views. Speciﬁcally, they usually fuse the features of the neighboring view\n(reference view) with epipolar constraints [19, 54, 60]. Although interpretable, fusing along\nthe epipolar line only does not fully utilize the semantic information of the reference view\nas the information off the epipolar line is discarded. For example, it is difﬁcult to associate\nthe ankle with the leg from the epipolar line in the reference view of Figure 1, which could\nbe an important cue as part of the structure information for pose estimation. On the other\nhand, fusing all locations of other views can address this drawback. In this paper, we propose\nthe Epipolar Field, a more general form of the epipolar line. It assigns probabilities to all\nlocations of the reference view and still keep the knowledge of epipolar constraints.\nRecently, attention mechanisms and the transformers [46] achieve great progress in com-\nputer vision areas [2, 11, 29, 42, 47, 51, 63, 64]. The self-attention module [46] can capture\nlong range dependencies and correspondences, which is difﬁcult for the convolutional layer.\nAlthough promising, there are only a few works [29] that apply it to the 3D pose estima-\ntion tasks. To the best of our knowledge, none of the previous works have exploited the\ntransformer architectures in the multi-view 3D pose estimation setting. Inspired by pre-\nvious multi-modal transformers [22, 39, 41], we propose the TransFusion, a lightweight\nframework that can utilize all pixels from both the current view itself and reference view\nsimultaneously. As an example in Figure 1, the attention layer actually relies on the whole\nleg to infer the location of the ankle. Moreover, we add the 3D geometry positional encoding\nbased on the epipolar ﬁeld to help the transformer explicitly capture the correspondence.\nOur main contributions are summarized as follows:\n• We are the ﬁrst to apply the transformer architecture to multi-view 3D human pose\nestimation. We propose the TransFusion, a uniﬁed architecture to fuse cues from\nmultiple views.\n• We propose the epipolar ﬁeld, a novel and more general form of epipolar line. It read-\nily integrates with the transformer through our proposed geometry positional encoding\nto encode the 3D relationships among different views.\n• Extensive experiments are conducted to demonstrate that our TransFusion outperforms\nprevious fusion methods on both Human 3.6M and SkiPose datasets, but requires sub-\nstantially fewer parameters.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 3\nFigure 1: Comparison of epipolar line and attention\nmodule. Given the query pixel (cyan dot) in view 1 (cur-\nrent). The attention map on the view 2 (reference) in-\ndicates that the prediction relies on the image clues pro-\nvided by the area of right shank, not just the corresponded\nright ankle. While previous methods based on epipolar\nline (yellow line) cannot capture this information.\nview 1 view 2\n2 Related Work\n2.1 Multi-view 3D Pose Estimation\nMulti-view 3D pose estimation usually follows a two-step process: (1) localize 2D joints\nwith a 2D pose estimator on each view, and (2) lift the 2D joints from multi-view images\nto the 3D position via triangulation. To improve the performance of 2D pose detector, re-\nsearchers typically resort to sophisticated architectures to capture both low-level and high-\nlevel representations [10, 33, 40, 52, 53] or use the structural information to model the spatial\nconstraints [8, 24, 25, 26, 44]. However, the occlusion cases are still challenging, as monoc-\nular images do not provide evidence for occlusion joints localization.\nAn alternative approach, more explainable, is to make the 2D pose detector 3D-aware,\ni.e., fusing the 2D feature heatmaps [7, 19, 34, 54, 60] from different views. Speciﬁcally, the\nCross-view Fusion [34] directly learns a ﬁxed attention weight to fuse all pairs of pixels given\na pair of views. However, the learnable weight requires the multi-camera setup unchanged\nduring the inference time, and the number of parameters is quadratic to the resolution of\ninput images. The epipolar transformer [19] applies the non-local module [47] to obtain the\nweights and only fuse pixels along the epipolar line in other views. Thus it is easy to learn\nand ﬂexible to use. However, sampling along the epipolar line discards off-epipolar line\ninformation and thus obtains limited information from the reference views. In the second\nstep, researchers use graphical model with the structure of human [34] to improve the quality\nof triangulation or directly learn 3D pose via differentiable triangulation [21]. Our work still\nfocuses on enhancing 2D pose by fully integrating information from different views.\n2.2 Transformer\nVision Transformer Recently, several studies demonstrated that the transformer architec-\ntures [46] plays a signiﬁcant role in a wide range of computer vision tasks, such as image\nclassiﬁcation [4, 11, 45], object detection [2, 64], and semantic segmentation [48, 55, 63].\nRecently, some studies also explored applying the transformer on human pose estimation\ntasks [28, 29, 31, 56, 62]. More speciﬁcally, for 2D pose estimation, TransPose [56] aims\nto explain the spatial dependencies of the predicted keypoints with transformers, PRTR [28]\nand TF-Pose [31] attempt to directly regress the joint coordinates by transformer decoders.\nWhile for the 3D pose estimation, METRO [29] ﬁrstly applies transformer to reconstruct 3D\nhuman pose and mesh from a single image, and PoseFormer [62] builds a spatial-temporal\ntransformers with the input of 2D joint sequences for 3D pose estimation in videos. How-\never, previous works have hardly exploited the transformer architectures on the multi-view\n3D pose estimation setting, which is however an important task in the pose estimation area.\nMulti-modal Transformer Transformers with multi-modality inputs such as images and\ntexts have also been fully exploited [22, 27, 39, 41, 57, 58, 65]. In general, these methods\ndirectly concatenate the embeddings from two sources together [22] and make the trans-\n4 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\nFlatten Features\nTransformer \nEncoder\nConv ⊕ ⊕\n⊕ ⊕\n2D sine Position Encoding Geometry Position Encoding\nHead\nHead\n Multi-Head \nAttention \nMLP \nAdd &Norm \nAdd &Norm \nConv\nTransformer Encoder\nN×\nFigure 2: Overview of TransFusion.\nformer itself to learn the correspondence between two modalities from millions of image-text\nparis [36]. Thus, these methods are quite expensive and inefﬁcient, and difﬁcult to apply on\nlimited datasets. Our method, however, directly provides the correspondence between two\ninputs and makes the transformer explicitly learn their relationships.\n3 Methods\n3.1 Overview\nFigure 2 is an overview of TransFusion. It takes two images from different views as input,\nand predicts the heatmaps of joints in each view. The framework consists of three modules:\na CNN backbone to extract low-level features; a transformer encoder to capture both corre-\nspondence between two views and long-range spatial correlations within single view images;\na head to predict the heatmaps of joints. Speciﬁcally, given images Ii ∈R3×HI ×WI in each\nview, where i ∈{1,2}denotes view 1 and view 2, the backbone F(·) ﬁrstly produces the\nlow-level features Xi = F(Ii) ∈Rd×H×W of each image. Here d is the number of channels.\nH and W are the height and width of the feature map, respectively. The featureXi is ﬂattened\ninto a sequence vector X\n′\ni ∈RL×d, where L = H ×W. Both 2D sine positional encoding E2D\nand 3D geometry positional encoding EGi are added onto X\n′\ni to make the transformer aware\nof position information. X\n′\n1 and X\n′\n2 are concatenated together to build a uniform embedding\nX = [X\n′\n1 +E2D +EG1,X\n′\n2 +E2D +EG2] ∈R2L×d. The embedding X then enters the standard\ntransformer encoder E(·). Finally, the output of the transformers ˜X = E(X) are split into ˜X1\nand ˜X2, which is embedding of each view, and a prediction head H(·) takes ˜Xi and predicts\nthe joint heatmaps ¯Hi = H( ˜Xi) ∈Rk×Hh×Wh for each view, where k is the number of joints.\n3.2 TransFusion\nTransformer Encoders The transform encoder E(·) consists of several layers of multi-\nhead self-attention. Let l = 2L for short, given the input sequenceX ∈Rl×d, the self-attention\nlayer ﬁrst uses linear projections to obtain a set of queries (Q ∈Rl×d), keys (K ∈Rl×d) and\nvalues (V ∈Rl×d) from X. The three linear projections are parameterized by three learnable\nmatrices Wq, Wk, Wv ∈Rd×d. Following [2], the position encodingE is added into the input\nX for computing the query and key. The scaled dot-product attention [46] between Q and K\nis adopted to compute the attention weights, and aggregate the values:\nA = softmax\n(QKT\n√\nd\n)\nV (1)\nFinally, a non-linear transformation (i.e., multi layer perceptron, and the skip connection) is\napplied on A to calculate the output ˜X. As X is low-level features of all views, given one\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 5\nquery pixel on the feature map, it can attend cues from the its own view and other views\nsimultaneously through the entire network.\nPositional encoding The attention layer would degenerate into a permutation-equivariant\narchitecture without any position information. Thus, the positional encoding is necessary to\nmake the transformer aware of position and order of input sequence. For each individual\nview, we follow the 2D sine positional encoding in the original transformers [11, 56], and\nwe denote it as E2D. However, it only encodes position information from its own view,\nwhile the position information in the 3D space and that from the reference views cannot be\nencoded. Thus, another positional encoding EGi (See Section 3.3) is required to encode the\n3D location information of each view i in the 3D space.\n3.3 Geometry Position Encoding (GPE)\nTo make the transformers 3D-aware, we introduce 3D camera information [1, 61] into the\npositional encoding and propose the Geometry Positional Encoding. Denote the world coor-\ndinate system as Oworld. The 3D location of view i ’s camera center in Oworld is denoted as\nCi, and the 3D location of the n-th pixel (n ∈{1,2,..., L}) of view i in Oworld is denoted as\nPn\ni . Pn\ni can be derived from the camera parameters of view i. As shown in Figure 3, the ray−−→CiPn\ni (gray line with arrow) indicates the direction of the pixel pi in the world. The unit vec-\ntor ˆ−−→CiPn\ni is its direction vector, and can encode the relative 3D location of each pixel. Thus,\nwe design GPE based on this unit vector, and we add one linear transformation to make it ﬁt\nthe input dimension d. The 3D geometry positional encoding for the n-th pixel in view i is\ndeﬁned as:\nEGn\ni = We\nˆ−−→CiPn\ni ∈Rd (2)\nWhere We ∈Rd×3 is a learnable transformation matrix. With EGi, the transformer can be\naware of the 3D location of each view.\nP1\nP2\nP\nC2C1\ne1 e2\nN\nP′ \n2\nl2\nl1\nFigure 3: Illustration of Geometry Positional Encoding. For each pixel, its geometry positional encoding is\ncalculated from the unit vector ˆun\ni of ray CiPi in the world coordinate Oworld.\n3.4 Epipolar Field\nAlthough GPE impose the 3D space information into transformers, it does not explicitly en-\ncode the relationship between two views. As a result, given a pixel in the current view, it is\nstill difﬁcult to attend the corresponding regions when performing global attention between\nfeatures of two views. We further impose the Epipolar Constraints [1] into GPE: Given one\npixel p1 in view 1, its correspondence pixel p2 in view 2 must be on the epipolar line l2\n(Figure 3). However, the epipolar line does not model the relationship with pixels outsidel2.\nInstead, pixels close to the line and pixels away from the line should be treated differently.\n6 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\nThus, we propose the Epipolar Field to model the relationship among all pixels in the refer-\nence view. In detail, given Pn\n1, the 3D location of n-th pixel pn\n1 in view 1, we calculate the\nnormal vector NPn\n1C1C2 of plane Pn\n1C1C2 by:\nNPn\n1C1C2 = ˆ−−−→C1C2 ×ˆ−−−→C1Pn\n1 (3)\nGiven Pm\n2 , the 3D location of m-th pixel pm\n2 in view 2, we use the angle θ between the\nnormal vector NPn\n1C1C2 and ray −−−→C2Pm\n2 to model the relationship between pn\n1 and pm\n2 , and use\nthe cosine of θ to calculate the correspondence score:\nS(pn\n1, pm\n2 ) =1−|cosθ |= 1−|NPn\n1C1C2 ·ˆ−−−→C2Pm\n2 | (4)\nThe absolute |·| is added to limit the score in [0,1]. With Eq. 4, if pm\n2 falls in the epipolar\nline l2, the score S(pn\n1, pm\n2 ) will be 1. Otherwise, the far pm\n2 is from l2, the closer the score\nwould be 0. We further add a soft factor γ to control the sharpness, thus the epipolar ﬁeld is\nS′(pn\n1, pm\n2 ) = (S(pn\n1, pm\n2 ))γ . Figure 4 gives a visualization of the epipolar ﬁeld. Comparing\nwith the epipolar line, the epipolar ﬁeld model relationships with all pixels in the reference\nview. We can also reduce it to the epipolar line with a very large γ. Thus, epipolar ﬁeld can\nbe considered as a more general form of the epipolar line.\nQuery Pixel Epipolar Line Epipolar Field ( )γ = 1 Epipolar Field ( )γ = 1000\nEpipolar Field ( )γ = 10\nFigure 4: Illustration of Epipolar Field. The epipolar ﬁeld can reﬂect the distance from the epipolar line to the\noff-line pixels. By adjusting the soft factor γ, it can also reduce to the standard epipolar line.\nWe then use the epipolar ﬁeld to guide the learning of We to help the EGn\ni encode cor-\nrespondence between two views. In detail, we let the dot product of EGn\n1 and EGm\n2 match\nS′(pn\n1, pm\n2 ) with the mean square error loss during the training process:\nLpos = 1\nL2\nL\n∑\nn\nL\n∑\nm\n(EGn\n1(EGm\n2 )T −S′(pn\n1, pm\n2 ))2 (5)\nTherefore, a high attention score will be achieved along the epipolar line when calculating the\ncross-view attention maps between X\n′\n1 +E2D +EG1 and X\n′\n2 +E2D +EG2, which makes the\ntransformer easy to attend corresponding regions. Moreover, with this soft design, semantic\ninformation from ofﬂine pixels are still kept, rather than discarded like [19].\n3.5 Implementation Details\nCNN backbone We follow [56] and apply a very shallow CNN architecture as the CNN\nbackbone F(·), which is the initial part of the ResNet-50 [18]. Speciﬁcally, the number of\nparameters of the shallow CNN is 1.4 M, which is just 5.5% of the original Simple Baseline\nwith ResNet-50 (25.6M). The output feature map has size H = HI/8, W = WI/8. Thus, the\nﬁne-grained local feature information can still be kept.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 7\nTransFusion Following [2, 56], we set the dimension of the feature embedding d to 256,\nthe number of heads to 8, the number of encoder layers N to 3. Due to the limitation of\nresource, we only consider the fusion of 2 neighborhood views, although out framework can\nbe easily extended to more than 2 views.\nPrediction head Given ˜Xi, we ﬁrst reshape it back to ˜X\n′\ni ∈Rd×H×W . The prediction head\nH(·) applies one deconvolution layer and one 1×1 convolution layer to predict the heatmap\nof keypoints. By default, the height and width of heatmapsHi are Hh = HI/4 andWh =WI/4.\nLoss function The groundtruth heatmap Hi ∈Rk×Hh×Wh of 2D keypoints is deﬁned as a\n2D a Gaussian centering around each keypoint [52]. We apply the Mean Square Error (MSE)\nloss to calculate the difference between the output heatmaps ¯Hi and Hi. By combining the\nEquation 5, we train the network end-to-end with loss function L = 1\nHW ∥¯Hi −Hi ∥2\nF +Lpos.\n4 Experiments\n4.1 Experimental Settings\nDataset We conduct extensive experiments on two public multi-view 3D human pose es-\ntimation datasets, Human 3.6M [3, 20] and Ski-Pose [12, 13, 15, 16, 17, 35, 38]. (1) The\nHuman 3.6M contains joint annotations of video frames captured by four calibrated cameras\nin a room. We adopt the same training and test split as in [19, 21, 34], where subjects 1, 5, 6,\n7, 8 are used for training, and 9, 11 are for testing. Note that 3D annotations of some scenes\nof the ’S9’ are damaged [21], we exclude these scenes from the evaluation as in [19, 21].\n(2) The Ski-Pose dataset aims to help analyze skiers’s giant slalom runs with 6 calibrated\ncameras. It provides six camera views as well as corresponding 3D pose. In detail, 8,481\nframes are used for training and 1,716 are used for testing. We resize all images to 256×256\nin all experiments.\nTraining As the training of transformers requires huge datasets [11, 46], while the scenes\nof multi-view pose datasets are quite limited, making it difﬁcult to train the transformer from\nscratch. By convention [19, 21], we use the MS-COCO [30] pretrained TransPose [56] to\ninitialize our network and ﬁne tune it on the multi-view human pose datasets. Following\nthe settings in [19], we apply Adam optimizer [23] and train the model for 20 epochs. The\nlearning rate is initialized with 0.001 and decays at 10-th and 15-th epoch with ratio 0.1.\nEvaluation metrics The performance of 2D pose estimation is evaluated by Joint Detec-\ntion Rate (JDR), which measures the percentage of the successfully detected keypoints. A\nkeypoints is detected if the distance between the predicted location and the ground truth is\nwithin a predeﬁned threshold. The threshold is set to half of the head size for human pose\nestimation. Given the estimated 2D joints of each view, following [19, 34], direct triangu-\nlation is used for estimating the 3D poses with respect to the global coordinates. The 3D\npose estimation accuracy is measured by Mean Per Joint Position Error (MPJPE) between\nthe groundtruth 3D pose and the estimated 3D pose.\n4.2 Results on Human 3.6M\nWe compare with two state-of-the-art methods, the crossview fusion [34] and the epipolar\ntransformers [19]. For fair comparison, we use the SimpleBaseline-ResNet50 pretrained on\nCOCO [53] as initialization and then ﬁnetuned with their ofﬁcial codes [19, 34].\n8 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\nQuantitative results The results of both 2D and 3D pose estimation are shown in Table\n1. We also shown the number of parameters of each model, the MACs (multiply-add op-\nerations). Besides, we also report the inference time to obtain the 3D pose from 4 views\non a single 2080Ti GPU of all multiview methods. For both 2D and 3D pose estimation,\nTransFusion consistently outperforms or achieves comparable performance with epipolar\ntransformers [19] and cross-view fusion [34]. Note that JDR is a relative loose metric, with a\nwider threshold which tolerates small errors, so the improvement on 2D is not very obvious.\nHowever, on the 3D metric, which directly computes the distance, our improvement is much\nmore signiﬁcant. Moreover, as in Table 2, our method can achieve signiﬁcant improvement\non sophisticated poses sequences such as \"Phone\" and \"Smoke\", which usually encounters\nheave occlusions for certain views. This result suggests that fusing features from the entire\nimages of other views, instead of just features along the epipolar line [19], can bring more\nbeneﬁts. Besides, comparing to the single view TransPose [56], our Transfusion can achieve\n4.7 mm gain on 3D. Thus, the improvement is not only from the TransPose architecture, but\nfrom the fusion with other views. Moreover, our method is lightweight and efﬁcient. It only\nrequires 2.1% (5M / 235M) of the parameters of cross-view fusion [34]. Beneﬁt from the\nparallel computing of transformers architectures, it further reduces the inference time, while\nthe operation of sampling along epipolar lines [19] is time-consuming.\nMethod Params MACs Inference Time (s)JDR (%)↑ MPJPE (mm)↓\nSingle view - Simple Baseline[53]34M 51.7G - 98.5 30.2\nSingle view - TransPose [56]5M 43.6G - 98.6 30.5\nCrossview Fusion [34] 235M 55.1G 0.048 99.4 27.8\nEpipolar Transformer [19]34M 51.7G 0.086 98.6 27.1\nTransFusion 5M 50.2G 0.032 99.4 25.8\nTable 1: 2D and 3D pose estimation accuracy comparison on Human3.6M. The metric of 2D pose is JDR (%), and\nthe metric of 3D pose is MPJPE (mm). All networks are pretrained on COCO [30] and then ﬁnetuned on Human\n3.6M [20]. All images are resized to 256 ×256.\nMethod Dir Disc Eat Greet Phone Pose Purch Sit SitD Smoke Photo Wait WalkD Walk WalkT\nCrossview Fusion[34]24.0 28.8 25.6 24.5 28.3 24.4 26.930.734.4 29.0 32.6 25.1 24.3 30.8 24.9Epipolar transformers [19]23.227.1 23.4 22.4 32.421.4 22.637.3 35.4 29.0 27.7 24.2 21.2 26.6 22.3TransFusion 24.426.4 23.4 21.1 25.223.2 24.7 33.829.8 26.4 26.8 24.223.2 26.1 23.3\nTable 2: The MPJPE of each pose sequence on Human 3.6M.\nVisualization of Attention maps Given the query pixel in one view, we further visualize\nthe attention maps on both views. We show our results in Figure 5. It is observed that on the\nview itself, typically the attention map is around the joints. If the query joint is occluded, it\nmay resort to joints on the other side of the symmetry [56]. On the neighboring view, the\nnetwork usually not just attends the corresponding keypoint, but attends the whole limbs,\nwhich cannot be located by the epipolar line. Previous methods based on epipoar line [19]\nactually miss this important clue.\nQualitative results We also present examples of predicted 2D keypoints on the image and\n3D pose in the space, and compare our methods with baseline methods [34]. As in Figure 6,\neven if the entire arms (green line) are occluded, our method still predicts the 2D keypoints\ncorrectly by fusing information from the reference view, and further gives a better 3D pose.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 9\nroot left shoulder right elbow right shoulder\nCurrent \nView\nReference \nView\nFigure 5: Visualization of attention maps on Human 3.6M test set. The cyan dots are groundtruth. Given the\nquery pixel, the ﬁrst rows are attention maps of current views, and the second rows are attention maps of reference\nviews. We also visualize the epipolar line (yellow) for comparison.\nGround Truth (3D)\nOurs (3D)Baseline (3D)\nBaseline (2D) Ours (2D) Ground Truth (2D)\nFigure 6: Visualization of predictions for Human 3.6M. Both skeletons of 2D keypoints on the image and 3D\npose in the space are presented.\n4.3 Ablation Studies\nGeometry Positional Encoding We conduct ablation studies on the GPE to verify its sig-\nniﬁcance. In detail, we consider 3 settings: 1) training without 3D geometry positional en-\ncoding, 2) applying a learnable 3D positional encoding, i.e., directly learn EGi from scratch\n3) training the 3D GPE without epipolar ﬁeld constraints Lpos. Table 3 presents the results.\nWithout the 3D location information, the performance of 1) and 2) are even worse than the\nsingle view TransPose, we hypothesize that the 2D sine PE makes the transformer easy to\nattend the same pixel location of all views, and the learned 3D PE is easy to overﬁt the train-\ning examples. Without Lpos, the error will also increase. Thus the guide from the epipolar\nﬁeld is favorable, as it imposes correspondence for cross-view attention.\nMethod 2D Pose / JDR (%)↑ 3D Pose / MPJPE (mm)↓\nTransFusion - without 3D positional encoding98.5 35.9\nTransFusion - learnable 3D positional encoding96.0 57.3\nTransFusion - GPE withoutLpos 99.3 26.8\nTransFusion 99.4 25.8\nTable 3: Ablation studies on different types of 3D positional encoding\n10 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\nSoft Factorγ We also try different values of the soft vector γ, results are shown in Figure\n7(a). With a small γ, the epipolar ﬁeld assign all locations with relative high probabilities,\nthe performance are slightly worse (1.3 mm drop). While with a hugeγ = 1000, the epipolar\nﬁeld reduces to the hard epipolar line, and the performance drops 2 .7 mm. Thus, we verify\nthe effectiveness of our epipolar ﬁeld compared with hard-coded epipolar line.\nTransformer architecture We study how performance scales with the size of the trans-\nformer. As in Figure 7(b), with the number of layersN increasing, the performance improves\nsigniﬁcantly, as the learning ability of transformer is more powerful with more parameters.\nBut when N > 3, it tends to saturate or degenerate. We hypothesize that the transformer is\neasy to overﬁt when the size is too huge. Meanwhile, as in Figure 7(c), with the number\nof heads increases, the performance also improves gradually, as more heads can help attend\ndifferent features [46]. In summary, our choice with N = 3 and 8 heas are reasonable.\n100\n101\n102\n103\nSoft Factor \n25.5\n26.0\n26.5\n27.0\n27.5\n28.0\n28.5\n29.0MPJPE / mm\n27.1\n26.4\n25.8\n28.5\n(a) Soft Factor\n1 2 3 4\nNumber of Encoder layers N\n95\n96\n97\n98\n99\n100JDR / %\n96.1\n97.8\n99.4 99.1\nJDR MPJPE\n25\n32\n39\n46\n53\n60\nMPJPE / mm\n53.9\n46.7\n25.8\n32.2 (b) Number of encoder layers\n1 4 8\nNumber of heads\n95\n96\n97\n98\n99\n100JDR / %\n95.5\n97.5\n99.4\nJDR MPJPE\n25\n40\n55\n70\n85\n100\nMPJPE / mm\n90.4\n46.0\n25.8 (c) Number of heads\nFigure 7: Ablation studies on the soft factorγ of the epipolar ﬁeld, the number of transformer encoder\nlayers N, and the number of transformer heads.\nMethod 2D Pose / JDR (%)↑ 3D Pose / MPJPE (mm)↓\nSingle view - Simple Baseline [53]94.5 39.6\nEpipolar Transformer [19] 94.9 34.2\nTransFusion 96.0 31.6\nTable 4: 2D and 3D pose estimation accuracy comparison on Ski-Pose.\n4.4 Results on Ski-Pose Dataset\nWe further apply TransFusion on the Ski-Pose dataset to verify its generalization ability.\nResults are presented in Table 4. In the settings with six cameras, the Crossview Fusion\nis too huge (537M) to train on the 2080Ti GPU. Similar to Human 3.6M, TransFusion still\noutperform or achieve comparable performance with other fusion methods, while it is much\nlightweight. Thus, our method is also effective in outdoor multi-view settings.\n5 Conclusion\nIn this paper, we apply the transformer to the multi-view 3D human pose estimation for the\nﬁrst time. Inspired by multi-modal transformers, we propose the TransFusion network, a\nlightweight architecture to integrate cues from both self views and reference views. Further-\nmore, we propose the epipolar ﬁeld, and apply it to the 3D positional encoding to encode\ncorrespondence between two views explicitly. Experimental results shows that our method\noutperform previous fusion methods but with a more light weighted network. In the future\nwe plan to apply our TransFusion to regress the 3D locations with multi-view inputs in an\nend-to-end way to further improve 3D predictions.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 11\nReferences\n[1] Alex M Andrew. Multiple view geometry in computer vision. Kybernetes, 2001.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Euro-\npean Conference on Computer Vision, pages 213–229. Springer, 2020.\n[3] Cristian Sminchisescu Catalin Ionescu, Fuxin Li. Latent structured models for human\npose estimation. In International Conference on Computer Vision, 2011.\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-\nscale vision transformer for image classiﬁcation. arXiv preprint arXiv:2103.14899 ,\n2021.\n[5] Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Hui Tang, Yufan Xue, Xiaohui Xie,\nYen-Yu Lin, and Wei Fan. Generating realistic training images based on tonality-\nalignment generative adversarial networks for hand pose estimation. arXiv preprint\narXiv:1811.09916, 2018.\n[6] Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Wei Fan, and Xiaohui Xie.\nDggan: Depth-image guided generative adversarial networks for disentangling rgb and\ndepth images in 3d hand pose estimation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 411–419, 2020.\n[7] Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, and Xiaohui Xie. Mvhm: A\nlarge-scale multi-view hand mesh benchmark for accurate 3d hand pose estimation. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 836–845, 2021.\n[8] Yifei Chen, Haoyu Ma, Deying Kong, Xiangyi Yan, Jianbao Wu, Wei Fan, and Xiaohui\nXie. Nonparametric structure regularization machine for 2d hand pose estimation. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 381–390, 2020.\n[9] Yifei Chen, Haoyu Ma, Jiangyuan Wang, Jianbao Wu, Xian Wu, and Xiaohui Xie. Pd-\nnet: Quantitative motor function evaluation for parkinson’s disease via automated hand\ngesture analysis. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining, pages 2683–2691, 2021.\n[10] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.\nCascaded pyramid network for multi-person pose estimation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 7103–7112, 2018.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[12] Benedikt Fasel, Jörg Spörri, Matthias Gilgien, Geo Bofﬁ, Julien Chardonnens, Erich\nMüller, and Kamiar Aminian. Three-dimensional body and centre of mass kinematics\nin alpine ski racing using differential gnss and inertial sensors. Remote Sensing, 8(8):\n671, 2016.\n12 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\n[13] Benedikt Fasel, Jörg Spörri, Julien Chardonnens, Josef Kröll, Erich Müller, and Kamiar\nAminian. Joint inertial sensor orientation drift reduction for highly dynamic move-\nments. IEEE journal of biomedical and health informatics, 22(1):77–86, 2017.\n[14] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, and\nJunsong Yuan. 3d hand shape and pose estimation from a single rgb image. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n10833–10842, 2019.\n[15] Matthias Gilgien, Jörg Spörri, Julien Chardonnens, Josef Kröll, and Erich Müller. De-\ntermination of external forces in alpine skiing using a differential global navigation\nsatellite system. Sensors, 13(8):9821–9835, 2013.\n[16] Matthias Gilgien, Jörg Spörri, Philippe Limpach, Alain Geiger, and Erich Müller. The\neffect of different global navigation satellite system methods on positioning accuracy\nin elite alpine skiing. Sensors, 14(10):18433–18453, 2014.\n[17] Matthias Gilgien, Jörg Spörri, Julien Chardonnens, Josef Kröll, Philippe Limpach, and\nErich Müller. Determination of the centre of mass kinematics in alpine skiing using\ndifferential global navigation satellite systems. Journal of sports sciences, 33(9):960–\n969, 2015.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[19] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Epipolar transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pages 7779–7788, 2020.\n[20] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m:\nLarge scale datasets and predictive methods for 3d human sensing in natural environ-\nments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7):1325–\n1339, jul 2014.\n[21] Karim Iskakov, Egor Burkov, Victor Lempitsky, and Yury Malkov. Learnable triangu-\nlation of human pose. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7718–7727, 2019.\n[22] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer\nwithout convolution or region supervision. arXiv preprint arXiv:2102.03334, 2021.\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[24] Deying Kong, Yifei Chen, Haoyu Ma, Xiangyi Yan, and Xiaohui Xie. Adaptive graph-\nical model network for 2d handpose estimation. arXiv preprint arXiv:1909.08205 ,\n2019.\n[25] Deying Kong, Haoyu Ma, Yifei Chen, and Xiaohui Xie. Rotation-invariant mixed\ngraphical model network for 2d hand pose estimation. InProceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages 1546–1555, 2020.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 13\n[26] Deying Kong, Haoyu Ma, and Xiaohui Xie. Sia-gcn: A spatial information aware\ngraph neural network with 2d convolutions for hand pose estimation. arXiv preprint\narXiv:2009.12473, 2020.\n[27] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A uni-\nversal encoder for vision and language by cross-modal pre-training. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, pages 11336–11344, 2020.\n[28] Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose\nrecognition with cascade transformers. arXiv preprint arXiv:2104.06976, 2021.\n[29] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh recon-\nstruction with transformers. arXiv preprint arXiv:2012.09760, 2020.\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra-\nmanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In European conference on computer vision, pages 740–755. Springer, 2014.\n[31] Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin\nWang. Tfpose: Direct human pose estimation with transformers. arXiv preprint\narXiv:2103.15320, 2021.\n[32] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad\nShaﬁei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect:\nReal-time 3d human pose estimation with a single rgb camera. ACM Transactions on\nGraphics (TOG), 36(4):1–14, 2017.\n[33] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human\npose estimation. In European conference on computer vision, pages 483–499. Springer,\n2016.\n[34] Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun Zeng. Cross\nview fusion for 3d human pose estimation. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 4342–4351, 2019.\n[35] Helge Rhodin, Jörg Spörri, Isinsu Katircioglu, Victor Constantin, Frédéric Meyer,\nErich Müller, Mathieu Salzmann, and Pascal Fua. Learning monocular 3d human pose\nestimation from multi-view images. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 8437–8446, 2018.\n[36] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual cap-\ntions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565, 2018.\n[37] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. Hand keypoint detection\nin single images using multiview bootstrapping. InProceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 1145–1153, 2017.\n[38] Jörg Spörri. Reasearch dedicated to sports injury prevention-the’sequence of preven-\ntion’on the example of alpine ski racing. Habilitation with Venia Docendi in Biome-\nchanics, 1(2):7, 2016.\n14 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\n[39] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.\nVl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint\narXiv:1908.08530, 2019.\n[40] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representa-\ntion learning for human pose estimation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 5693–5703, 2019.\n[41] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations\nfrom transformers. arXiv preprint arXiv:1908.07490, 2019.\n[42] Hao Tang, Xingwei Liu, Kun Han, Xiaohui Xie, Xuming Chen, Huang Qian, Yong\nLiu, Shanlin Sun, and Narisu Bai. Spatial context-aware self-attention model for multi-\norgan segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 939–949, 2021.\n[43] Denis Tome, Chris Russell, and Lourdes Agapito. Lifting from the deep: Convolutional\n3d pose estimation from a single image. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 2500–2509, 2017.\n[44] Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. Joint training of\na convolutional network and a graphical model for human pose estimation. Advances\nin neural information processing systems, 27:1799–1807, 2014.\n[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. arXiv preprint arXiv:2012.12877, 2020.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n[47] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 7794–7803, 2018.\n[48] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao\nShen, and Huaxia Xia. End-to-end video instance segmentation with transformers.\narXiv preprint arXiv:2011.14503, 2020.\n[49] Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin, and Charless Fowlkes. Ge-\nometric pose affordance: 3d human pose with scene constraints. arXiv preprint\narXiv:1905.07718, 2019.\n[50] Zhe Wang, Daeyun Shin, and Charless C Fowlkes. Predicting camera viewpoint im-\nproves cross-dataset generalization for 3d human pose estimation. In European Con-\nference on Computer Vision, pages 523–540. Springer, 2020.\n[51] Zhe Wang, Hao Chen, Xinyu Li, Chunhui Liu, Yuanjun Xiong, Joseph Tighe, and Char-\nless C Fowlkes. Sscap: Self-supervised co-occurrence action parsing for unsupervised\ntemporal action segmentation. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2022.\nH. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER 15\n[52] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional\npose machines. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 4724–4732, 2016.\n[53] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation\nand tracking. In Proceedings of the European conference on computer vision (ECCV),\npages 466–481, 2018.\n[54] Rongchang Xie, Chunyu Wang, and Yizhou Wang. Metafuse: A pre-trained fusion\nmodel for human pose estimation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13686–13695, 2020.\n[55] Xiangyi Yan, Hao Tang, Shanlin Sun, Haoyu Ma, Deying Kong, and Xiaohui Xie.\nAfter-unet: Axial fusion transformer unet for medical image segmentation. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision ,\n2022.\n[56] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Transpose: Towards explainable\nhuman pose estimation by transformer. arXiv preprint arXiv:2012.14214, 2020.\n[57] Chenyu You, Nuo Chen, and Yuexian Zou. MRD-Net: Multi-Modal Residual Knowl-\nedge Distillation for Spoken Question Answering. In IJCAI, 2021.\n[58] Chenyu You, Nuo Chen, and Yuexian Zou. Self-supervised contrastive cross-\nmodality representation learning for spoken question answering. arXiv preprint\narXiv:2109.03381, 2021.\n[59] Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen Zheng. End-to-end hand\nmesh recovery from a monocular rgb image. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2354–2364, 2019.\n[60] Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, and Wenjun Zeng. Adafuse:\nAdaptive multiview fusion for accurate human pose estimation in the wild. Interna-\ntional Journal of Computer Vision, 129(3):703–718, 2021.\n[61] Yunhan Zhao, Shu Kong, and Charless Fowlkes. Camera pose matters: Improving\ndepth prediction by mitigating pose distribution bias. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 15759–15768, 2021.\n[62] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming\nDing. 3d human pose estimation with spatial and temporal transformers.arXiv preprint\narXiv:2103.10455, 2021.\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,\nYanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with transformers. arXiv preprint\narXiv:2012.15840, 2020.\n[64] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. De-\nformable detr: Deformable transformers for end-to-end object detection.arXiv preprint\narXiv:2010.04159, 2020.\n16 H. MA ET AL.: TRANSFUSION: CROSS-VIEW FUSION WITH TRANSFORMER\n[65] Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou,\nMinghui Qiu, and Ling Shao. Kaleido-bert: Vision-language pre-training on fashion\ndomain. arXiv preprint arXiv:2103.16110, 2021.\n[66] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and\nThomas Brox. Freihand: A dataset for markerless capture of hand pose and shape from\nsingle rgb images. In Proceedings of the IEEE International Conference on Computer\nVision, pages 813–822, 2019."
}