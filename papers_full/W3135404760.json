{
    "title": "Generative Adversarial Transformers",
    "url": "https://openalex.org/W3135404760",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225821087",
            "name": "Hudson, Drew A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3183588634",
            "name": "Zitnick, C. Lawrence",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W967544008",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2990690382",
        "https://openalex.org/W3107537120",
        "https://openalex.org/W2963767194",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W3129603602",
        "https://openalex.org/W3166513219",
        "https://openalex.org/W3092354667",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2976023236",
        "https://openalex.org/W1710476689",
        "https://openalex.org/W3042183427",
        "https://openalex.org/W2805516822",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W2963448850",
        "https://openalex.org/W2164239909",
        "https://openalex.org/W2963470893",
        "https://openalex.org/W2412320034",
        "https://openalex.org/W3110991353",
        "https://openalex.org/W2971044234",
        "https://openalex.org/W2735601643",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963951231",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964213104",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W3016697633",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W3035570181",
        "https://openalex.org/W2911448865",
        "https://openalex.org/W2937274663",
        "https://openalex.org/W3037784242",
        "https://openalex.org/W2013894622",
        "https://openalex.org/W2087303720",
        "https://openalex.org/W1898014694",
        "https://openalex.org/W3111551570",
        "https://openalex.org/W3008721991",
        "https://openalex.org/W2963184176",
        "https://openalex.org/W2030066581",
        "https://openalex.org/W2963717490",
        "https://openalex.org/W2994971263",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W3105675572",
        "https://openalex.org/W2902617128",
        "https://openalex.org/W2156406284",
        "https://openalex.org/W2964091144",
        "https://openalex.org/W2150593711",
        "https://openalex.org/W3148140980",
        "https://openalex.org/W581314551",
        "https://openalex.org/W2785961484",
        "https://openalex.org/W2060294957",
        "https://openalex.org/W2963684088",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W2190177721",
        "https://openalex.org/W2893749619",
        "https://openalex.org/W3094502228"
    ],
    "abstract": "We introduce the GANformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linear efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at https://github.com/dorarad/gansformer.",
    "full_text": "Generative Adversarial Transformers\nDrew A. Hudson§\nDepartment of Computer Science\nStanford University\ndorarad@cs.stanford.edu\nC. Lawrence Zitnick\nFacebook AI Research\nFacebook, Inc.\nzitnick@fb.com\nAbstract\nWe introduce the GANformer, a novel and efﬁ-\ncient type of transformer, and explore it for the\ntask of visual generative modeling. The network\nemploys a bipartite structure that enables long-\nrange interactions across the image, while main-\ntaining computation of linear efﬁciency, that can\nreadily scale to high-resolution synthesis. It itera-\ntively propagates information from a set of latent\nvariables to the evolving visual features and vice\nversa, to support the reﬁnement of each in light of\nthe other, and encourage the emergence of compo-\nsitional representations for objects and scenes. In\ncontrast to the classic transformer architecture, it\nutilizes multiplicative integration that allows ﬂexi-\nble region-based modulation, and can thus be seen\nas a multi-latent generalization of the successful\nStyleGAN network. We demonstrate the model’s\nstrength and robustness through a careful eval-\nuation over a range of datasets, from simulated\nmulti-object environments to rich real-world in-\ndoor and outdoor scenes, showing it attains state-\nof-the-art results in terms of image quality and\ndiversity, while enjoying fast learning and better\ndata-efﬁciency. Further qualitative and quantita-\ntive experiments offer an insight into the model’s\ninner workings, revealing improved interpretabil-\nity and stronger disentanglement, and illustrate\nthe beneﬁts and efﬁcacy of our approach. An im-\nplementation of the model is available at https:\n//github.com/dorarad/gansformer.\n1. Introduction\nThe cognitive science literature speaks of two reciprocal\nmechanisms that underlie human perception: the bottom-up\nprocessing, proceeding from the retina up to the cortex, as\nlocal elements and salient stimuli hierarchically group to-\ngether to form the whole [27], and the top-down processing,\nwhere surrounding global context, selective attention and\nprior knowledge inform the interpretation of the particular\n[32]. While their respective roles and dynamics are being\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nFigure 1.Sample images generated by the GANformer, along with\na visualization of the model attention maps.\nactively studied, researchers agree that it is the interplay\nbetween these two complementary processes that enables\nthe formation of our rich internal representations, allowing\nus to perceive the world around in its fullest and create vivid\nimageries in our mind’s eye [13, 17, 39, 52].\nNevertheless, the very mainstay and foundation of computer\nvision over the last decade – the Convolutional Neural Net-\nwork, surprisingly, does not reﬂect this bidirectional nature\nthat so characterizes the human visual system, and rather\ndisplays a one-way feed-forward progression from raw sen-\nsory signals to higher representations. Unfortunately, the\nlocal receptive ﬁeld and rigid computation of CNNs reduce\ntheir ability to model long-range dependencies or develop\nholistic understanding of global shapes and structures that\ngoes beyond the brittle reliance on texture [26], and in the\ngenerative domain especially, they are linked to considerable\noptimization and stability issues [70] due to their fundamen-\ntal difﬁculty in coordinating between ﬁne details across the\ngenerated scene. These concerns, along with the inevitable\ncomparison to cognitive visual processes, beg the question\nof whether convolution alone provides a complete solution,\nor some key ingredients are still missing.\n§I wish to thank Christopher D. Manning for the fruitful dis-\ncussions and constructive feedback in developing the bipartite\ntransformer, especially when explored within the language repre-\nsentation area, as well as for the kind ﬁnancial support that allowed\nthis work to happen.\narXiv:2103.01209v4  [cs.CV]  29 Mar 2022\nGenerative Adversarial Transformers\nFigure 2.Bipartite Attention. We introduce the GANformer network, that leverages a bipartite structure to support long-range interactions\nwhile evading the quadratic complexity standard transformers suffer from. We present two novel attention operations over the bipartite\ngraph: simplex and duplex, the former permits communication in one direction, in the generative context – from the latents to the image\nfeatures, while the latter enables both top-down and bottom-up connections between these two dual representations.\nMeanwhile, the NLP community has witnessed a major rev-\nolution with the advent of the Transformer network [ 65],\na highly-adaptive architecture centered around relational\nattention and dynamic interaction. In response, several at-\ntempts have been made to integrate the transformer into\ncomputer vision models, but so far they have met only lim-\nited success due to scalabillity limitations stemming from\nits quadratic mode of operation.\nMotivated to address these shortcomings and unlock the\nfull potential of this promising network for the ﬁeld of\ncomputer vision, we introduce the Generative Adversarial\nTransformer, or GANformer for short, a simple yet effective\ngeneralization of the vanilla transformer, explored here for\nthe task of visual synthesis. The model utilizes a bipartite\nstructure for computing soft attention, that iteratively aggre-\ngates and disseminates information between the generated\nimage features and a compact set of latent variables that\nfunctions as a bottleneck, to enable bidirectional interaction\nbetween these dual representations. This design achieves a\nfavorable balance, being capable of ﬂexibly modeling global\nphenomena and long-range interactions on the one hand,\nwhile featuring an efﬁcient setup that still scales linearly\nwith the input size on the other. As such, the GANformer\ncan sidestep the computational costs and applicability con-\nstraints incurred by prior works, caused by the dense and\npotentially excessive pairwise connectivity of the standard\ntransformer [5, 70], and successfully advance the generative\nmodeling of compositional images and scenes.\nWe study the model’s quantitative and qualitative behavior\nthrough a series of experiments, where it achieves state-\nof-the-art performance for a wide selection of datasets, of\nboth simulated as well as real-world kinds, obtaining par-\nticularly impressive gains in generating highly-structured\nmulti-object scenes. As indicated by our analysis, the GAN-\nformer requires less training steps and fewer samples than\ncompeting approaches to successfully synthesize images of\nhigh quality and diversity. Further evaluation provides ro-\nbust evidence for the network’s enhanced transparency and\ncompositionality, while ablation studies empirically validate\nthe value and effectiveness of our approach. We then present\nvisualizations of the model’s produced attention maps, to\nshed more light upon its internal representations and synthe-\nsis process. All in all, as we will see through the rest of the\npaper, by bringing the renowned GANs and Transformer\narchitectures together under one roof, we can integrate their\ncomplementary strengths, to create a strong, compositional\nand efﬁcient network for visual generative modeling.\n2. Related Work\nGenerative Adversarial Networks (GANs) [28], originally\nintroduced in 2014, have made remarkable progress over the\npast years, with signiﬁcant advances in training stability and\ndramatic improvements in image quality and diversity. that\nturned them to be nowadays one of the leading paradigms\nin visual synthesis [ 5, 44, 58]. In turn, GANs have been\nwidely adopted for a rich variety of tasks, including image-\nto-image translation [40, 72], super-resolution [47], style\ntransfer [12], and representation learning [ 18], to name a\nfew. But while generated images for faces, single objects or\nnatural scenery have reached astonishing ﬁdelity, becoming\nnearly indistinguishable from real samples, the uncondi-\ntional synthesis of more structured or compositional scenes\nis still lagging behind, suffering from inferior coherence, re-\nduced geometric consistency and, at times, a lack of global\ncoordination [9, 43, 70]. As of now, faithful generation of\nstructured scenes is thus yet to be reached.\nConcurrently, the last years saw impressive progress in\nthe ﬁeld of NLP, driven by the innovative architecture\ncalled Transformer [ 65], which has attained substantial\ngains within the language domain and consequently sparked\nconsiderable interest across the deep learning community\n[16, 65]. In response, several attempts have been made to\nincorporate self-attention constructions into vision models,\nGenerative Adversarial Transformers\nmost commonly for image recognition, but also in segmenta-\ntion [25], detection [8], and synthesis [70]. From structural\nperspective, these can be roughly divided into two streams:\nthose that apply local attention operations, failing to cap-\nture global interactions [14, 37, 56, 57, 71], and others that\nborrow the original transformer structure as-is and perform\nattention globally across the entire image, resulting in pro-\nhibitive computation due to the quadratic complexity, which\nfundamentally hinders its applicability to low-resolution lay-\ners only [3, 5, 19, 24, 41, 66, 70]. Few other works proposed\nsparse, discrete or approximated variations of self-attention,\neither within the adversarial or autoregressive contexts, but\nthey still fall short of reducing memory footprint and com-\nputational costs to a sufﬁcient degree [11, 24, 36, 38, 62].\nCompared to these prior works, the GANformer stands out\nas it manages to avoid the high costs ensued by self at-\ntention, employing instead bipartite attentionbetween the\nimage features and a small collection of latent variables. Its\ndesign ﬁts naturally with the generative objective of trans-\nforming source latents into an output image, facilitating\nlong-range interaction without sacriﬁcing computational ef-\nﬁciency. Rather, the network maintains a scalable linear\ncomputation across all layers, realizing the transformer’s\nfull potential. In doing so, we seek to take a step forward in\ntackling the challenging task of scene generation. Intuitively,\nand as is later corroborated by our ﬁndings, allocating mul-\ntiple latents to interact through attention with the generated\nimage serves as a structural prior of a bottleneck that pro-\nmotes the formation of compact and compositional scene\nrepresentations, as the different latents may specialize to\ncertain objects or semantic regions of interest. Indeed, as\ndemonstrated in section 4, the Generative Adversarial Trans-\nformer achieves state-of-the-art performance in synthesizing\nvaried real-world indoor and outdoor scenes, while showing\nindications for semantic disentanglement along the way.\nIn designing our model, we draw inspiration from multiple\nlines of research on generative modeling, compositionality\nand scene understanding, including techniques for scene de-\ncomposition, object discovery and representation learning.\nSeveral variational approaches [7, 22, 23, 31] perform itera-\ntive inference to encode scenes into multiple slots, but are\nmostly applied in the contexts of synthetic and oftentimes\nfairly rudimentary 2D settings. Works such as Capsule net-\nworks [29, 61] leverage ideas from psychology about Gestalt\nprinciples [34, 63], perceptual grouping [6] or analysis-by-\nsynthesis [4], and like us, introduce ways to piece together\nvisual elements to discover compound entities and, in the\ncases of Set Transformers [48] or A2-Nets [10], group lo-\ncal information into global aggregators, which proves use-\nful for a broad spectrum of tasks, spanning unsupervised\nsegmentation [30, 50], clustering [48], image recognition\n[2], NLP [59] and viewpoint generalization [46]. However,\nour work stands out incorporating new ways to integrate\nFigure 3.Model overview. Left : The GANformer layer is com-\nposed of a bipartite attention operation to propagate information\nfrom the latents to the image grid, followed by convolution and\nupsampling. These are stacked multiple times starting from an\ninitial 4×4 grid and up to producing a ﬁnal high-resolution im-\nage. Right: The latents and image features attend to each other\nto capture the scene structure. The GANformer’s compositional\nlatent space contrasts with the StyleGAN’s monolithic one (where\na single latent modulates the whole scene uniformly).\ninformation across the network through novel forms of at-\ntention: (Simplex and Duplex), that iteratively update and\nreﬁne the assignments between image features and latents,\nand is the ﬁrst to explore these techniques in the context of\nhigh-resolution generative modeling.\nMost related to our work are certain GAN models for con-\nditional and unconditional visual synthesis: A few meth-\nods [21, 33, 54, 64] utilize multiple replicas of a generator\nto produce a set of image layers, that are then combined\nthrough alpha-composition. As a result, these models make\nquite strong assumptions about the independence between\nthe components depicted by each layer. In contrast, our\nmodel generates one uniﬁed image through a cooperative\nprocess, coordinating between the different latents through\nthe use of soft attention. Other works, such as SPADE\n[55, 73], employ region-based feature modulation for the\ntask of layout-to-image translation, but, contrary to us, use\nﬁxed segmentation maps and static class embeddings to\ncontrol the visual features. Of particular relevance is the\nprominent StyleGAN model [44, 45], which utilizes a sin-\ngle global style vector to consistently modulate the features\nof each layer. The GANformer generalizes this design, as\nmultiple style vectors impact different regions in the image\nconcurrently, allowing for spatially ﬁner control over the\ngeneration process. Finally, while StyleGAN broadcasts\ninformation in one direction from the single global latent to\nthe local image features, our model propagates information\nboth from latents to features and vice versa, enabling top-\ndown and bottom-up reasoning to occur simultaneously1.\n1Note however that our model certainly does not claim to serve\nGenerative Adversarial Transformers\nFigure 4.Attention maps. Sample images generated by the GANformer for the CLEVR, LSUN-Bedrooms and Cityscapes datasets,\nand a visualization of the produced attention maps, from lower (top row) and upper (bottom row) layers. The colors correspond to the\ndifferent latents that attend to each region.\n3. The Generative Adversarial Transformer\nThe Generative Adversarial Transformer (GANformer) is\na type of Generative Adversarial Network, which involves\na generator network (G) that maps random samples from\nthe latent space to the output space (e.g. an image), and a\ndiscriminator network (D) which seeks to discern between\nreal and fake samples [ 28]. The two networks compete\nwith each other through a minimax game until reaching an\nequilibrium. Typically, each of these networks consists of\nmultiple layers of convolution, but in the GANformer case,\nwe instead construct them using a novel architecture, called\nBipartite Transformer, formally deﬁned below.\nThe section is structured as follows: we ﬁrst present a for-\nmulation of the Bipartite Transformer, a domain-agnostic\ngeneralization of the Transformer2 (section 3.1). Then, we\nprovide an overview of how the transformer is incorporated\ninto the generative adversarial framework (section 3.2). We\nconclude by discussing the merits and distinctive properties\nof the GANformer, that set it apart from the traditional GAN\nand transformer networks (section 3.3).\n3.1. The Bipartite Transformer\nThe standard transformer network is composed of alternat-\ning multi-head self-attention and feed-forward layers. We\nrefer to each pair of self-attention and feed-forward oper-\nations as a transformer layer, such that a transformer is\nconsidered to be a stack of several such layers. The Self-\nAttention layer considers all pairwise relations among the\ninput elements, updating each one by attending to all the oth-\ners. The Bipartite Transformer generalizes this formulation,\nfeaturing instead a bipartite graph between two groups of\nvariables – in the GAN case, latents and image features. In\nas a biologically-accurate reﬂection of cognitive top-down process-\ning. Rather, this analogy plays as a conceptual source of inspiration\nthat aided us through the idea development.\n2By transformer, we precisely mean a multi-layer bidirectional\ntransformer encoder, as described in [16], which interleaves self-\nattention and feed-forward layers.\nthe following, we consider two forms of attention that could\nbe computed over the bipartite graph – Simplex attention\nand Duplex attention, depending on the direction in which\ninformation propagates3 – either in one way only, from the\nlatents to the image, or both in top-down and bottom-up\nways. While for clarity purposes, we present the technique\nhere in its one-head version, in practice we make use of a\nmulti-head variant, in accordance with prior work [65].\n3.1.1. S IMPLEX ATTENTION\nWe begin by introducing the simplex attention, which dis-\ntributes information in a single direction over the bipartite\ntransformer graph. Formally, let Xn×d denote an input set\nof n vectors of dimension d (where, for the image case,\nn= W×H), and Ym×d denote a set of maggregator vari-\nables (the latents, in the generative case). We can then\ncompute attention over the derived bipartite graph between\nthese two groups of elements. Speciﬁcally, we deﬁne:\nAttention(Q,K,V ) =softmax\n(QKT\n√\nd\n)\nV\na(X,Y ) =Attention(q(X),k(Y),v(Y))\nWhere q(·),k(·),v(·) are functions that respectively map\nelements into queries, keys, and values, all maintaining\ndimensionality d. We also provide the mappings with po-\nsitional encodings, to reﬂect the distinct spatial position of\neach element e.g. in the image (see section 3.2 for details).\nNote that this bipratite attentionis a generalization of self\nattention, where Y = X.\nWe can then integrate the attended information with the\ninput elements X, but whereas the standard transformer\nimplements an additive update rule of the form:\nua(X,Y ) =LayerNorm(X+ a(X,Y ))\nwe instead use the retrieved information to control both the\nscale as well as the bias of the elements in X, in line with\n3In computer networks, simplex refers to single direction com-\nmunication, while duplex refers to communication in both ways.\nGenerative Adversarial Transformers\nFigure 5.Upper-layer attention maps produced by the GANformer model during synthesis, for the LSUN-Bedrooms dataset.\nthe practice promoted by the StyleGAN model [44]. As our\nexperiments indicate, such multiplicative integration enables\nsigniﬁcant gains in the model’s performance. Formally:\nus(X,Y ) =γ(a(X,Y )) ⊙ω(X) +β(a(X,Y ))\nWhere γ(·),β(·) are mappings that compute multiplicative\nand additive factors (scale and bias), both maintaining a\ndimension of d, and ω(X) = X−µ(X)\nσ(X) normalizes the fea-\ntures of X4. By normalizing X (the image features), and\nthen letting Y (the latents) control X’s statistical tenden-\ncies, we essentially enable information propagation from Y\nto X, intuitively, allowing the latents to control the visual\ngeneration of spatial attended regions within the image, so\nas to guide the synthesis of objects and entities.\n3.1.2. D UPLEX ATTENTION\nWe can go further and consider the variables Y to posses a\nkey-value structure of their own [53]: Y = (Km×d,V m×d),\nwhere the values V store the content of the Y variables as\nbefore (i.e. the randomly sampled latent vectors) while the\nkeys K track the centroids of the attention-based assignment\nbetween Xand Y, which can be computed byK = a(Y,X)\n– namely, the weighted averages over X elements, using\nthe attention distribution derived by comparing them to Y\nelements. Intuitively, each centroid tracks the region in\nthe image X that interacts with the respective latent in Y.\nConsequently, we can deﬁne a new update rule:\nud(X,Y ) =γ(A(Q,K,V )) ⊙ω(X) +β(A(Q,K,V ))\nThis update compounds together two attention operations:\nﬁrst (1) computing attention assignments between Xand Y,\nby K = a(Y,X), and then (2) reﬁning the soft assignments\nby considering their centroids, through A(Q,K,V ), where\nQ= q(X), which computes attention between the elements\nXand their centoroids K. This is analogous to the Expecta-\ntion–Maximization or k-means algorithms, [49, 50], where\nwe iteratively reﬁne the assignments of elements X to clus-\nters Y based on their distance to their respective centroids\nK = a(Y,X). As is empirically shown later, this works\nmore effectively than the update us deﬁned above.\nFinally, to support bidirectional interaction between X and\nY (the image and the latents), we can chain two reciprocal\n4The statistics are computed either with respect to other ele-\nments in Xfor instance normalization, or among element channels\nin the case of layer normalization, which performs better.\nsimplex attentions from X to Y and from Y to X, ob-\ntaining the duplex attention, which alternates computing\nY := ua(Y,X) and X := ud(X,Y ), such that each repre-\nsentation is reﬁned in light of the other, integrating together\nbottom-up and top-down interactions.\n3.1.3. O VERALL ARCHITECTURE STRUCTURE\nVision-speciﬁc adaptations. In the classic NLP trans-\nformer, each self-attention layer is followed by a feed-\nforward layer that processes each element independently,\nwhich can also be deemed a 1 ×1 convolution. Since our\ncase pertains to images, we use instead a kernel size of\nk = 3after each attention operation. We further apply a\nLeaky ReLU nonlinearity after each convolution [51] and\nthen upsample or downsmaple the features X, as part of\nthe generator and discriminator respectively. To account for\nthe features location within the image, we use a sinusoidal\npositional encoding [65] along the horizontal and vertical\ndimensions for the visual features X, and trained positional\nembeddings for the set of latent variables Y.\nModel structure & information ﬂow. Overall, the bipar-\ntite transformer is composed of a stack that alternates at-\ntention (simplex or duplex), convolution, and up- or down-\nsampling layers (see ﬁgure 3), starting from an initial 4 ×4\ngrid up to the desirable resolution for the generator, or pro-\ngressing inversely for the distriminator. Conceptually, this\nstructure fosters an interesting communication ﬂow: rather\nthan densely modeling interactions among all the pairs of\npixels in the image, it supports adaptive long-range inter-\naction between far away regions in a moderated manner,\npassing through a compact and global latent bottleneck,\nthat selectively gathers information from the entire input\nand distributes it back to the relevant regions. Intuitively, it\ncan be viewed as analogous to the top-down and bottom-up\nnotions discussed in section 1, as information is propagated\nin the two directions, both from the local pixel to the global\nhigh-level representation and vice versa.\nComputational efﬁciency. We note that both the simplex\nand the duplex attention operations enjoy a bilinear efﬁ-\nciency of O(mn) thanks to the network’s bipartite structure\nthat considers all element pairs from X and Y. Since, as\nwe see below, we maintain Y to be of a fairly small size,\nchoosing min the range of 8–32, this compares favorably\nto the prohibitive O(n2) complexity of self attention, which\nimpedes its applicability to high-resolution images.\nGenerative Adversarial Transformers\nFigure 6.Sample images and attention maps of lower and upper GANformer layers, for the CLEVR, LSUN-Bedrooms, FFHQ and\nCityscapes datasets. The colors in the attention maps correspond to the assignment between the image regions and the latent variables that\ncontrol them. For the CLEVR dataset, we can see multiple attention maps produced by different layers of the model, revealing how the\nrole of the latent variables changes at different stages of the generation – while they correspond to an instance segmentationas the layout\nof the scene is being formed in the early low-resolution layers, they behave similarly to a surface normalin the upper high-resolution\nlayers of the generator. We see similar progression from a coarser to ﬁner pattern of attention for the FFHQ dataset.\nGenerative Adversarial Transformers\n3.2. The Generator and Discriminator Networks\nOur generator and discriminator networks follow the gen-\neral design of prior work [44, 45], with the key difference\nof incorporating the novel bipartite attention layers instead\nof the single-latent modulation that characterizes earlier\nmodels: Commonly, a generator network consists of a multi-\nlayer CNN that receives a randomly sampled vector zand\ntransforms it into an image. The popular StyleGAN ap-\nproach departs from this design and, instead, introduces a\nfeed-forward mapping network that outputs an intermediate\nvector w, which in turn interacts directly with each convolu-\ntion through the synthesis network, globally modulating the\nfeature maps’ statistics at every layer.\nEffectively, this approach attainslayer-wise decomposition\nof visual properties, allowing StyleGAN to control global\naspects of the picture such as the pose, lighting conditions\nor color scheme, in a coherent manner over the entire im-\nage. But while StyleGAN successfully disentangles global\nattributes, it is more limited in its ability to perform spatial\ndecomposition, as it provides no direct means to control the\nstyle of localized regions within the generated image.\nThe bipartite transformer offers a solution to accomplish\nthis objective. Instead of modulating the style of all features\nglobally, we use instead our new attention layer to perform\nadaptive region-wise modulation. As shown in ﬁgure 3\n(right), we split the latent vector zinto kcomponents, z=\n[z1,....,z k] and, as in StyleGAN, pass each of them through\na shared mapping network, obtaining a corresponding set of\nintermediate latent variables Y = [y1,...,y k]. Then, during\nsynthesis, after each CNN layer of the generator, we let\nthe feature map X and latents Y play the roles of the two\nelement groups, mediating their interaction through our new\nattention layer – either simplex or duplex.\nThis setting thus allows for a ﬂexible and dynamic style\nmodulation at the level of the region . Since soft atten-\ntion tends to group elements based on their proximity and\ncontent similarity, we see how the transformer architec-\nture naturally ﬁts into the generative task and proves useful\nin the visual domain, allowing the model to exercise ﬁner\ncontrol in modulating local semantic regions. As we see in\nsection 4, this capability turns out to be especially useful in\nmodeling highly-structured scenes.\nAs to the loss function, optimization and training conﬁgura-\ntions, we adopt the settings and techniques used by Style-\nGAN2 [45], including in particular style mixing, stochastic\nvariation, exponential moving average for weights, and a\nnon-saturating logistic loss with lazy R1 regularization5.\n5In the prior version of the paper and in earlier stages of the\nmodel development, we explored incorporating the bipartite atten-\ntion to both the generator and the discriminator, in order to allow\nboth components make use of long-range interactions. However,\nin ablation experiments we observed that applying attention to the\n3.3. Summary\nTo recapitulate the discussion above, the GANformer suc-\ncessfully uniﬁes the GAN and Transformer architectures for\nthe task of scene generation. Compared to traditional GANs\nand transformers, it introduces multiple key innovations:\n• Compositional Latent Space with multiple variables\nthat coordinate through attention to produce the image\ncooperatively, in a manner that matches the inherent\ncompositionality of natural scenes.\n• Bipartite Structure that balances between expressive-\nness and efﬁciency, modeling long-range dependencies\nwhile maintaining linear computational costs.\n• Bidirectional Interaction between the latents and the\nvisual features, which allows the reﬁnement and inter-\npretation of each in light of the other.\n• Multiplicative Integration rule to impact the features’\nvisual style more ﬂexibly, akin to StyleGAN but in\ncontrast to the classic transformer network.\nAs we see in the following section, the combination of these\ndesign choices yields a strong architecture that demonstrates\nhigh efﬁciency, improved latent space disentanglement, and\nenhanced transparency of the generative process.\n4. Experiments\nWe investigate the GANformer through a suite of experi-\nments that study its quantitative performance and qualitative\nbehavior. As we will see below, the GANformer achieves\nstate-of-the-art results, successfully producing high-quality\nimages for a varied assortment of datasets: FFHQ for human\nfaces [44], the CLEVR dataset for multi-object scenes [42],\nand the LSUN-Bedrooms [69] and Cityscapes [15] datasets\nfor challenging indoor and outdoor scenes. Notably, it even\nattains state-of-the-art FID scores for the challenging and\nhighly-structured COCO dataset.\nFurther analysis we conduct in sections 4.1, 4.2 and 4.3 pro-\nvides evidence for multiple favorable properties the GAN-\nformer posses, including better data-efﬁciency, enhanced\ntransparency, and stronger disentanglement than prior ap-\nproaches. Section 4.4 then quantitatively assesses the net-\nwork’s semantic coverage of the natural image distribution\nfor the CLEVR dataset, while ablation and variation studies\nat section 4.5 empirically validate the necessity of each of\nthe model’s design choices. Taken altogether, our evaluation\noffers solid evidence for the GANformer’s effectiveness and\nefﬁcacy in modeling compsitional images and scenes.\ngenerator only allows for stronger results, and so we have updated\nthe paper accordingly.\nGenerative Adversarial Transformers\nTable 1.Comparison between the GANformer and competing methods for image synthesis.We evaluate the models along commonly\nused metrics of FID, Precision and Recall scores. FID is well-received as a reliable indication of image ﬁdelity and diversity, while\nPrecision and Recall measure the similarity between the generated and natural distributions. Metrics are computed over 50k samples.\nCLEVR LSUN-Bedrooms\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ FID ↓ IS ↑ Precision ↑ Recall ↑\nGAN 25.02 2.17 21.77 16.76 12.16 2.66 52.17 13.63\nk-GAN 28.29 2.21 22.93 18.43 69.90 2.41 28.71 3.45\nSAGAN 26.04 2.17 30.09 15.16 14.06 2.70 54.82 7.26\nStyleGAN2 16.05 2.15 28.41 23.22 11.53 2.79 51.69 19.42\nGANformers 10.26 2.46 38.47 37.76 8.56 2.69 55.52 22.89\nGANformerd 9.17 2.36 47.55 66.63 6.51 2.67 57.41 29.71\nFFHQ Cityscapes\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ FID ↓ IS ↑ Precision ↑ Recall ↑\nGAN 13.18 4.30 67.15 17.64 11.57 1.63 61.09 15.30\nk-GAN 61.14 4.00 50.51 0.49 51.08 1.66 18.80 1.73\nSAGAN 16.21 4.26 64.84 12.26 12.81 1.68 43.48 7.97\nStyleGAN2 9.24 4.33 68.61 25.45 8.35 1.70 59.35 27.82\nGANformers 8.12 4.46 68.94 10.14 14.23 1.67 64.12 2.03\nGANformerd 7.42 4.41 68.77 5.76 5.76 1.69 48.06 33.65\nWe compare our network with several approaches, includ-\ning both baselines and leading models for image synthesis:\n(1) A baseline GAN [28] that follows the typical convolu-\ntional architecture6; (2) StyleGAN2 [ 45], where a single\nglobal latent interacts with the evolving image by modulat-\ning its global style; (3) SAGAN [70], which performs self\nattention across all feature pairs in low-resolution layers of\nthe generator and the discriminator; and (4) k-GAN [ 64]\nthat produces kseparated images, which are then blended\nthrough alpha-composition.\nTo evaluate all models under comparable training conditions,\nmodel size, and optimization scheme, we implement them\nall within our public codebase, which extends the ofﬁcial\nStyleGAN repository. All models have been trained with\nimages of 256 ×256 resolution and for the same number\nof training steps, roughly spanning a week on 2 NVIDIA\nV100 GPUs per model (or equivalently 3-4 days using 4\nGPUs). For the GANformer, we select k – the number\nof latent variables, from the range of 8–32. Note that in-\ncreasing the value of k does not translate to an increased\noverall latent dimension, and we rather keep it equal across\nmodels. See section A for further implementation details,\nhyperparameter settings and training conﬁgurations.\nAs shown in table 1, our model matches or outperforms\nprior work, achieving substantial gains in terms of FID\nscore, which correlates with image quality and diversity\n[35], as well as other commonly used metrics such as Preci-\nsion and Recall (P&R)7. As could be expected, we obtain\n6In the baseline GAN, we input the noise through the network’s\nstem instead of through weight modulation.\n7Note that while the StyleGAN paper [45] reports lower FID\nscores for FFHQ and LSUN-Bedrooms, they are obtained by train-\ning for 5-7 times longer than our experiments (speciﬁcally, they\ntrain for up to 17.5 million steps, producing 70M samples and\ndemanding over 90 GPU-days). To comply with a reasonable\ncompute budget, we equally reduced the training duration for all\nmodels in our evaluation, maintaining the same number of steps.\nthe least gains for the FFHQ human faces dataset, where\nnaturally there is relatively lower diversity in image layout.\nOn the ﬂip side, most notable are the signiﬁcant improve-\nments in performance for CLEVR, where our approach suc-\ncessfully lowers FID scores from 16.05 to 9.17, as well as\nLSUN-Bedrooms, where the GANformer nearly halves the\nFID score from 11.53 to 6.51, being trained for equal num-\nber of steps. These ﬁndings suggest that the GANformer\nis particularly adept at modeling scenes of high composi-\ntionality (CLEVR) or layout diversity (LSUN-Bedrooms).\nComparing between the Simplex and Duplex Attentions fur-\nther reveals the strong beneﬁts of integrating the reciprocal\nbottom-up and top-down processes together.\n4.1. Data and Learning Efﬁciency\nWe examine the learning curves of our and competing mod-\nels (ﬁgure 7, (3)) and inspect samples of generated images\nat different stages of the training (ﬁgure 12). These results\nboth indicate that our model learns signiﬁcantly faster than\ncompeting approaches. In the case of CLEVR, it produces\nhigh-quality images in approximately 3-times less training\nsteps than the second-best approach. To further explore the\nGANformer’s learning aptitude, we perform experiments\nwhere we reduce the size of the dataset each model (and\nspeciﬁcally, its discriminator) is exposed to during training\nto varying degrees (ﬁgure 7, (4)). These results similarly val-\nidate the model’s superior data-efﬁciency, especially where\nas few as 1k images are provided for training.\n4.2. Transparency & Compositionality\nTo gain more insight into the model’s internal representa-\ntion and its underlying generative process, we visualize the\nattention distributions produced by the GANformer as it\nsynthesizes new images. Recall that at each layer of the\ngenerator, it casts attention between the klatent variables\nand the evolving spatial features of the generated image.\nGenerative Adversarial Transformers\n10\n40\n70\n100\n130\n160\n190FID Score\nStep\nAttention First Layer\n3 4 5\n6 7 8\n1250k\n10\n20\n30\n40\n50\n60\n70FID Score\nStep\nAttention Last Layer\n4 5 6 7 8\n1250k\n10\n20\n30\n40\n50\n60\n70\n80\n90FID Score\nStep\nCLEVR\nGAN\nk-GAN\nSAGAN\nStyleGAN2\nSimplex (Ours)\nDuplex (Ours)\n1250k\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n100 1000 10000 100000\nFID Score\nDataset size (Logaritmic)\nData Efficiency\nGAN\nStyleGAN2\nSimplex (Ours)\nDuplex (Ours)\nFigure 7.From left to right: (1-2) Learning Performance as a function of the earliest and latest layers that the bipartite attention is\napplied to. The more layers attention is used through, the better the model’s performance gets and the faster it learns, conﬁrming the\neffectiveness of our approach. (3) Learning Curves for the GANformer vs. competing approaches, demonstrating its fast learning. (4):\nData-Efﬁciency for CLEVR: performance as a function of the training set size.\nTable 2.Chi-Square Statistics for CLEVR generated scenes,\nbased on 1k samples. Images were processed by a pre-trained\nobject detector, identifying objects and semantic attributes, to com-\npute the properties’ distribution across the generated scenes.\nGAN StyleGAN GANformers GANformerd\nObject Area 0.038 0.035 0.045 0.068\nObject Number 2.378 1.622 2.142 2.825\nCo-occurrence 13.532 9.177 9.506 13.020\nShape 1.334 0.643 1.856 2.815\nSize 0.256 0.066 0.393 0.427\nMaterial 0.108 0.322 1.573 2.887\nColor 1.011 1.402 1.519 3.189\nClass 6.435 4.571 5.315 16.742\nAs illustrated by ﬁgures 4 and 6, the latent variables tend to\nattend to coherent visual regions in terms of proximity and\ncontent similarity. Figure 6 provides additional attention\nmaps computed by the model in various layers, showing how\nit behaves distinctively in different stages of the generation\nprocess. The visualizations imply that the latents carry a\nsemantic sense, capturing objects, visual entities or other\nconstituent components of the synthesized scenes. These\nﬁndings can thereby attest to an enhanced compositionality\nthat our model acquires through its multi-latent structure.\nWhereas prior work uses a single monolithic latent vector\nto account for the whole scene and modulate features at a\nglobal scale only, our design lets the GANformer exercise\nﬁner control that impacts features at the object granularity,\nwhile leveraging the use of attention to make its internal\nrepresentations more structured and transparent.\nTo quantify the compositionality exhibited by the model, we\nuse a pre-trained detector [67] to produce segmentations for\na set of generated scenes, in order to measure the correlation\nbetween the attention cast by the latents with various seman-\ntic classes. Figure 8shows the classes that have the highest\ncorrelation with respect to the latent variables, indicating\nthat different latents indeed coherently attend to semantic\nconcepts such as windows, pillows, sidewalks or cars, as\nwell as background regions like carpets, ceiling, and walls.\nThis illustrates how the multiple latents are effectively used\nto semantically decompose the scene generation task.\nTable 3.Disentanglement metrics (DCI and modularity) ,\nwhich asses the Disentanglement, Completeness Informativeness,\nand Modularity of the latent representations, effectively measuring\ntheir correspondance to visual attributes in the out images, com-\nputed over 1k CLEVR samples. The GANformer achieves the\nstrongest results compared to competing approaches.\nGAN StyleGAN GANformers GANformerd\nDisentanglement 0.126 0.208 0.556 0.768\nModularity 0.631 0.703 0.891 0.952\nCompleteness 0.071 0.124 0.195 0.270\nInformativeness 0.583 0.685 0.899 0.972\nInformativeness’ 0.434 0.332 0.848 0.963\n4.3. Disentanglement\nWe consider the DCI and Modularity metrics commonly\nused in the disentanglement literature [ 20, 60] to provide\nmore evidence for the beneﬁcial impact our architecture has\non the model’s internal representation. These metrics asses\nthe Disentanglement, Completeness, Informativeness and\nModularity of a given representation, essentially evaluating\nthe degree to which there is a 1-to-1 correspondence be-\ntween latent factors and global image attributes. To obtain\nthe attributes, we consider the area size of each semantic\nclass (e.g. cubes, spheres, ﬂoor), predicted by a pre-trained\nsegmentor, and use them as the output response features\nfor measuring the latent space disentanglement, computed\nover 1k images. We follow the protocol proposed by Wu\net al. [68] and present the results in table 3. This analysis\nconﬁrms that the GANformer’s latent representations enjoy\nhigher disentanglement compared to competing approaches.\n4.4. Image Diversity\nA major advantage of compositional representations is that\nthey can support combinatorial generalization – a key foun-\ndation of human intelligence [ 1]. Inspired by this obser-\nvation, we measure this property in the context of visual\nsynthesis of multi-object scenes. We use a pre-trained ob-\nject detector on generated CLEVR scenes, to extract the\nobjects and properties within each sample. We then com-\npute Chi-Square statistics on the sample set to determine the\nGenerative Adversarial Transformers\ndegree to which each model manages to cover the natural\nuniform distribution of CLEVR images. Table 2 summa-\nrizes the results, where we can see that our model obtains\nbetter scores across almost all the semantic properties of the\nscenes distribution. These metrics complement the common\nFID and PR scores as they emphasize structure over texture,\nor semantics over perceptual appearance, focusing on object\nexistence, arrangement and local properties, and thereby\nsubstantiating further the model’s compositionality.\n4.5. Ablation Studies\nTo validate the usefulness of bipartite attention, we conduct\nablation studies, where we vary the index of the earliest and\nlatest layers of the generator network to which attention is\nincorporated. As indicated by ﬁgure 7 (1-2), the earlier (or\nlower resolution) attention begins being applied, the better\nthe model’s performance and the faster it learns. The same\ngoes for the latest layer to apply attention to – as attention\ncan especially contribute in high-resolutions, which beneﬁt\nthe most from long-range interactions. These studies pro-\nvide a validation for the effectiveness of our approach in\nenhancing generative scene modeling.\n5. Conclusion\nWe have introduced the GANformer, a novel and efﬁcient bi-\npartite transformer that combines top-down and bottom-up\ninteractions, and explored it for the task of generative model-\ning, achieving strong quantitative and qualitative results that\nattest to the model robustness and efﬁcacy. The GANformer\nﬁts within the general philosophy that aims to incorporate\nstronger inductive biases into neural networks to encourage\ndesirable properties such as transparency, data-efﬁciency\nand compositionality – properties which are at the core of\nhuman intelligence, serving as the basis for our capacity to\nplan, reason, learn, and imagine. While our work focuses\non visual synthesis, we note that the bipartite transformer\nis a general-purpose model, and expect it may be found\nuseful for other tasks in both vision and language. Overall,\nwe hope that our work will help progressing further in our\ncollective search to bridge the gap between the intelligence\nof humans and machines.\n6. Acknowledgments\nWe are grateful to Stanford HAI for the generous compu-\ntational resources provided through Amazon AWS cloud\ncredits. I also wish to thank Christopher D. Manning for the\nfruitful discussions and constructive feedback in developing\nthe bipartite transformer, especially when we explored it for\nlanguage representation, as well as for the kind ﬁnancial\nsupport he provided that allowed this work to happen.\nReferences\n[1] Peter W Battaglia, Jessica B Hamrick, Victor Bapst,\nAlvaro Sanchez-Gonzalez, Vinicius Zambaldi, Ma-\nteusz Malinowski, Andrea Tacchetti, David Raposo,\nAdam Santoro, Ryan Faulkner, et al. Relational induc-\ntive biases, deep learning, and graph networks. arXiv\npreprint arXiv:1806.01261, 2018.\n[2] Irwan Bello. Lambdanetworks: Modeling long-\nrange interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[3] Irwan Bello, Barret Zoph, Quoc Le, Ashish Vaswani,\nand Jonathon Shlens. Attention augmented convolu-\ntional networks. In 2019 IEEE/CVF International Con-\nference on Computer Vision, ICCV 2019, Seoul, Korea\n(South), October 27 - November 2, 2019 , pp. 3285–\n3294. IEEE, 2019. doi: 10.1109/ICCV .2019.00338.\n[4] Irving Biederman. Recognition-by-components: a\ntheory of human image understanding. Psychological\nreview, 94(2):115, 1987.\n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan.\nLarge scale GAN training for high ﬁdelity natural im-\nage synthesis. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[6] Joseph L Brooks. Traditional and new principles of\nperceptual grouping. 2015.\n[7] Christopher P Burgess, Loic Matthey, Nicholas Wat-\nters, Rishabh Kabra, Irina Higgins, Matt Botvinick,\nand Alexander Lerchner. MONet: Unsupervised scene\ndecomposition and representation. arXiv preprint\narXiv:1901.11390, 2019.\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In European Conference on Computer Vision,\npp. 213–229. Springer, 2020.\n[9] Arantxa Casanova, Michal Drozdzal, and Adriana\nRomero-Soriano. Generating unseen complex scenes:\nare we there yet? arXiv preprint arXiv:2012.04027,\n2020.\n[10] Yunpeng Chen, Yannis Kalantidis, Jianshu Li,\nShuicheng Yan, and Jiashi Feng. A2-nets: Double\nattention networks. In Samy Bengio, Hanna M. Wal-\nlach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-\nBianchi, and Roman Garnett (eds.), Advances in Neu-\nral Information Processing Systems 31: Annual Con-\nference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Montr´eal,\nCanada, pp. 350–359, 2018.\nGenerative Adversarial Transformers\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[12] Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo\nHa, Sunghun Kim, and Jaegul Choo. Stargan: Uni-\nﬁed generative adversarial networks for multi-domain\nimage-to-image translation. In 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018 ,\npp. 8789–8797. IEEE Computer Society, 2018. doi:\n10.1109/CVPR.2018.00916.\n[13] Charles E Connor, Howard E Egeth, and Steven Yantis.\nVisual attention: bottom-up versus top-down. Current\nbiology, 14(19):R850–R852, 2004.\n[14] Jean-Baptiste Cordonnier, Andreas Loukas, and Mar-\ntin Jaggi. On the relationship between self-attention\nand convolutional layers. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020.\n[15] Marius Cordts, Mohamed Omran, Sebastian Ramos,\nTimo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The\ncityscapes dataset for semantic urban scene under-\nstanding. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016, pp. 3213–3223. IEEE\nComputer Society, 2016. doi: 10.1109/CVPR.2016.\n350.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pp. 4171–4186, Minneapolis,\nMinnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423.\n[17] Nadine Dijkstra, Peter Zeidman, Sasha Ondobaka,\nMarcel AJ van Gerven, and K Friston. Distinct top-\ndown and bottom-up brain connectivity during visual\nperception and imagery. Scientiﬁc reports, 7(1):1–9,\n2017.\n[18] Jeff Donahue, Philipp Kr ¨ahenb¨uhl, and Trevor Dar-\nrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929,\n2020.\n[20] Cian Eastwood and Christopher K. I. Williams. A\nframework for the quantitative evaluation of disentan-\ngled representations. In 6th International Conference\non Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track\nProceedings. OpenReview.net, 2018.\n[21] S´ebastien Ehrhardt, Oliver Groth, Aron Monszpart,\nMartin Engelcke, Ingmar Posner, Niloy J. Mitra, and\nAndrea Vedaldi. RELATE: physically plausible multi-\nobject scene synthesis using structured latent spaces.\nIn Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-\nsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020.\n[22] Martin Engelcke, Adam R. Kosiorek, Oiwi Parker\nJones, and Ingmar Posner. GENESIS: generative\nscene inference and sampling with object-centric latent\nrepresentations. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net, 2020.\n[23] S. M. Ali Eslami, Nicolas Heess, Theophane Weber,\nYuval Tassa, David Szepesvari, Koray Kavukcuoglu,\nand Geoffrey E. Hinton. Attend, infer, repeat:\nFast scene understanding with generative models.\nIn Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pp. 3225–3233, 2016.\n[24] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer.\nTaming transformers for high-resolution image synthe-\nsis. arXiv preprint arXiv:2012.09841, 2020.\n[25] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao,\nZhiwei Fang, and Hanqing Lu. Dual attention network\nfor scene segmentation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019 , pp. 3146–\n3154. Computer Vision Foundation / IEEE, 2019. doi:\n10.1109/CVPR.2019.00326.\n[26] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,\nMatthias Bethge, Felix A. Wichmann, and Wieland\nBrendel. Imagenet-trained cnns are biased towards\ntexture; increasing shape bias improves accuracy and\nGenerative Adversarial Transformers\nrobustness. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net, 2019.\n[27] James J Gibson. A theory of direct visual perception.\nVision and Mind: selected readings in the philosophy\nof perception, pp. 77–90, 2002.\n[28] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnetworks. arXiv preprint arXiv:1406.2661, 2014.\n[29] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun\nSodhani, Sergey Levine, Yoshua Bengio, and Bernhard\nSch¨olkopf. Recurrent independent mechanisms. arXiv\npreprint arXiv:1909.10893, 2019.\n[30] Klaus Greff, Sjoerd van Steenkiste, and J ¨urgen\nSchmidhuber. Neural expectation maximization. In\nIsabelle Guyon, Ulrike von Luxburg, Samy Ben-\ngio, Hanna M. Wallach, Rob Fergus, S. V . N. Vish-\nwanathan, and Roman Garnett (eds.), Advances in\nNeural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n6691–6701, 2017.\n[31] Klaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra,\nNick Watters, Christopher Burgess, Daniel Zoran, Loic\nMatthey, Matthew Botvinick, and Alexander Lerchner.\nMulti-object representation learning with iterative vari-\national inference. In Kamalika Chaudhuri and Ruslan\nSalakhutdinov (eds.), Proceedings of the 36th Interna-\ntional Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA , vol-\nume 97 of Proceedings of Machine Learning Research,\npp. 2424–2433. PMLR, 2019.\n[32] Richard Langton Gregory. The intelligent eye. 1970.\n[33] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image\nprocessing using multi-code GAN prior. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pp. 3009–3018. IEEE, 2020. doi:\n10.1109/CVPR42600.2020.00308.\n[34] David Walter Hamlyn. The psychology of perception:\nA philosophical examination of Gestalt theory and\nderivative theories of perception, volume 13. Rout-\nledge, 2017.\n[35] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local\nnash equilibrium. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fer-\ngus, S. V . N. Vishwanathan, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pp. 6626–6637, 2017.\n[36] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. Axial attention in multidimensional\ntransformers. arXiv preprint arXiv:1912.12180, 2019.\n[37] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin.\nLocal relation networks for image recognition. In2019\nIEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2019, Seoul, Korea (South), October 27 -\nNovember 2, 2019, pp. 3463–3472. IEEE, 2019. doi:\n10.1109/ICCV .2019.00356.\n[38] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-\ncross attention for semantic segmentation. In 2019\nIEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2019, Seoul, Korea (South), October 27\n- November 2, 2019, pp. 603–612. IEEE, 2019. doi:\n10.1109/ICCV .2019.00069.\n[39] Monika Intait ˙e, Valdas Noreika, Alvydas ˇSoli¯unas,\nand Christine M Falter. Interaction of bottom-up and\ntop-down processes in the perception of ambiguous\nﬁgures. Vision Research, 89:24–31, 2013.\n[40] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and\nAlexei A. Efros. Image-to-image translation with con-\nditional adversarial networks. In 2017 IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017 ,\npp. 5967–5976. IEEE Computer Society, 2017. doi:\n10.1109/CVPR.2017.632.\n[41] Yifan Jiang, Shiyu Chang, and Zhangyang Wang.\nTransGAN: Two transformers can make one strong\ngan. arXiv preprint arXiv:2102.07074, 2021.\n[42] Justin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.\nGirshick. CLEVR: A diagnostic dataset for composi-\ntional language and elementary visual reasoning. In\n2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017 , pp. 1988–1997. IEEE Computer\nSociety, 2017. doi: 10.1109/CVPR.2017.215.\n[43] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Im-\nage generation from scene graphs. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pp. 1219–1228, 2018.\nGenerative Adversarial Transformers\n[44] Tero Karras, Samuli Laine, and Timo Aila. A style-\nbased generator architecture for generative adversarial\nnetworks. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pp. 4401–4410. Computer\nVision Foundation / IEEE, 2019. doi: 10.1109/CVPR.\n2019.00453.\n[45] Tero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. Analyzing\nand improving the image quality of stylegan. In\n2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pp. 8107–8116. IEEE, 2020. doi:\n10.1109/CVPR42600.2020.00813.\n[46] Adam R. Kosiorek, Sara Sabour, Yee Whye Teh, and\nGeoffrey E. Hinton. Stacked capsule autoencoders. In\nHanna M. Wallach, Hugo Larochelle, Alina Beygelz-\nimer, Florence d’Alch´e-Buc, Emily B. Fox, and Ro-\nman Garnett (eds.), Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada , pp.\n15486–15496, 2019.\n[47] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose\nCaballero, Andrew Cunningham, Alejandro Acosta,\nAndrew P. Aitken, Alykhan Tejani, Johannes Totz, Ze-\nhan Wang, and Wenzhe Shi. Photo-realistic single\nimage super-resolution using a generative adversarial\nnetwork. In 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2017, Honolulu,\nHI, USA, July 21-26, 2017, pp. 105–114. IEEE Com-\nputer Society, 2017. doi: 10.1109/CVPR.2017.19.\n[48] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. Set trans-\nformer: A framework for attention-based permutation-\ninvariant neural networks. In Kamalika Chaudhuri\nand Ruslan Salakhutdinov (eds.), Proceedings of the\n36th International Conference on Machine Learning,\nICML 2019, 9-15 June 2019, Long Beach, California,\nUSA, volume 97 of Proceedings of Machine Learning\nResearch, pp. 3744–3753. PMLR, 2019.\n[49] Stuart Lloyd. Least squares quantization in pcm. IEEE\ntransactions on information theory , 28(2):129–137,\n1982.\n[50] Francesco Locatello, Dirk Weissenborn, Thomas Un-\nterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf.\nObject-centric learning with slot attention. arXiv\npreprint arXiv:2006.15055, 2020.\n[51] Andrew L Maas, Awni Y Hannun, and Andrew Y\nNg. Rectiﬁer nonlinearities improve neural network\nacoustic models. In Proc. icml, volume 30, pp. 3.\nCiteseer, 2013.\n[52] Andrea Mechelli, Cathy J Price, Karl J Friston, and\nAlumit Ishai. Where bottom-up meets top-down: neu-\nronal interactions during perception and imagery.Cere-\nbral cortex, 14(11):1256–1265, 2004.\n[53] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason We-\nston. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pp. 1400–1409, Austin, Texas, November\n2016. Association for Computational Linguistics. doi:\n10.18653/v1/D16-1147.\n[54] Thu Nguyen-Phuoc, Christian Richardt, Long Mai,\nYong-Liang Yang, and Niloy Mitra. Blockgan: Learn-\ning 3d object-aware scene representations from un-\nlabelled images. arXiv preprint arXiv:2002.08988 ,\n2020.\n[55] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and\nJun-Yan Zhu. Semantic image synthesis with spatially-\nadaptive normalization. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019 , pp. 2337–\n2346. Computer Vision Foundation / IEEE, 2019. doi:\n10.1109/CVPR.2019.00244.\n[56] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit,\nLukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In Jennifer G. Dy\nand Andreas Krause (eds.), Proceedings of the 35th\nInternational Conference on Machine Learning, ICML\n2018, Stockholmsm ¨assan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pp. 4052–4061. PMLR, 2018.\n[57] Niki Parmar, Prajit Ramachandran, Ashish Vaswani,\nIrwan Bello, Anselm Levskaya, and Jon Shlens. Stand-\nalone self-attention in vision models. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Flo-\nrence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett\n(eds.), Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pp. 68–80, 2019.\n[58] Alec Radford, Luke Metz, and Soumith Chintala. Un-\nsupervised representation learning with deep convo-\nlutional generative adversarial networks. In Yoshua\nBengio and Yann LeCun (eds.), 4th International Con-\nference on Learning Representations, ICLR 2016, San\nGenerative Adversarial Transformers\nJuan, Puerto Rico, May 2-4, 2016, Conference Track\nProceedings, 2016.\n[59] Anirudh Ravula, Chris Alberti, Joshua Ainslie,\nLi Yang, Philip Minh Pham, Qifan Wang, Santiago\nOntanon, Sumit Kumar Sanghai, Vaclav Cvicek, and\nZach Fisher. Etc: Encoding long and structured inputs\nin transformers. 2020.\n[60] Karl Ridgeway and Michael C Mozer. Learning deep\ndisentangled embeddings with the f-statistic loss. Ad-\nvances in neural information processing systems, 31,\n2018.\n[61] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hin-\nton. Dynamic routing between capsules. In Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\nWallach, Rob Fergus, S. V . N. Vishwanathan, and Ro-\nman Garnett (eds.), Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pp. 3856–3866, 2017.\n[62] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli,\nXuhui Jia, and Ching-Hui Chen. Global self-attention\nnetworks for image recognition. arXiv preprint\narXiv:2010.03019, 2020.\n[63] Barry Smith. Foundations of gestalt theory. 1988.\n[64] Sjoerd van Steenkiste, Karol Kurach, J¨urgen Schmid-\nhuber, and Sylvain Gelly. Investigating object compo-\nsitionality in generative adversarial networks. Neural\nNetworks, 130:309–325, 2020.\n[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn Isabelle Guyon, Ulrike von Luxburg, Samy Ben-\ngio, Hanna M. Wallach, Rob Fergus, S. V . N. Vish-\nwanathan, and Roman Garnett (eds.), Advances in\nNeural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n5998–6008, 2017.\n[66] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta,\nand Kaiming He. Non-local neural networks. In 2018\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018 , pp. 7794–7803. IEEE Computer\nSociety, 2018. doi: 10.1109/CVPR.2018.00813.\n[67] Yuxin Wu, Alexander Kirillov, Francisco\nMassa, Wan-Yen Lo, and Ross Girshick.\nDetectron2. https://github.com/\nfacebookresearch/detectron2, 2019.\n[68] Zongze Wu, Dani Lischinski, and Eli Shecht-\nman. StyleSpace analysis: Disentangled controls\nfor StyleGAN image generation. arXiv preprint\narXiv:2011.12799, 2020.\n[69] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song,\nThomas Funkhouser, and Jianxiong Xiao. Lsun: Con-\nstruction of a large-scale image dataset using deep\nlearning with humans in the loop. arXiv preprint\narXiv:1506.03365, 2015.\n[70] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas,\nand Augustus Odena. Self-attention generative adver-\nsarial networks. In Kamalika Chaudhuri and Ruslan\nSalakhutdinov (eds.), Proceedings of the 36th Interna-\ntional Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA , vol-\nume 97 of Proceedings of Machine Learning Research,\npp. 7354–7363. PMLR, 2019.\n[71] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Ex-\nploring self-attention for image recognition. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA, June\n13-19, 2020 , pp. 10073–10082. IEEE, 2020. doi:\n10.1109/CVPR42600.2020.01009.\n[72] Jun-Yan Zhu, Taesung Park, Phillip Isola, and\nAlexei A. Efros. Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks. In\nIEEE International Conference on Computer Vision,\nICCV 2017, Venice, Italy, October 22-29, 2017 , pp.\n2242–2251. IEEE Computer Society, 2017. doi:\n10.1109/ICCV .2017.244.\n[73] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter\nWonka. SEAN: image synthesis with semantic region-\nadaptive normalization. In 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp.\n5103–5112. IEEE, 2020. doi: 10.1109/CVPR42600.\n2020.00515.\nGenerative Adversarial Transformers\nSupplementary Material\nIn the following, we provide additional experiments and\nvisualizations for the GANformer model. First, we present\nin ﬁgures 12 and 9 a comparison of sample images produced\nby the GANformer and a set of baseline models, over the\ncourse of the training and after convergence respectively.\nSection A speciﬁes the implementation details, optimization\nscheme and training conﬁguration of the model. In section\nB and ﬁgure 8, we evaluate the spatial compositionality of\nthe GANformer’s attention mechanism, shedding light upon\nthe roles of the different latent variables.\nA. Implementation and Training Details\nTo evaluate all models under comparable conditions of train-\ning conﬁguration, model size, and optimization details, we\nimplement them all within the TensorFlow codebase intro-\nduced by the StyleGAN authors [44]. See table 4 for partic-\nular settings of the GANformer and table 5 for comparison\nof model sizes.\nIn terms of the loss function, optimization and training con-\nﬁguration, we adopt the settings and techniques used in the\nStyleGAN2 model [45], including in particular style mix-\ning, Xavier Initialization, stochastic variation, exponential\nmoving average for weights, and a non-saturating logistic\nloss with lazy a R1 regularization. We use Adam optimizer\nwith batch size of 32 (4 ×8 using gradient accumulation),\nequalized learning rate of 0.001, β1 = 0.0 and β2 = 0.99\nas well as leaky ReLU activations with α = 0.2, bilinear\nﬁltering in all up/downsampling layers and minibatch stan-\ndard deviation layer at the end of the discriminator. The\nmapping layer of the generator consists of 8 layers, and\nResNet connections are used throughout the model, for the\nmapping network, synthesis network and discriminator.\nWe train all models on images of 256 ×256 resolution,\npadded as necessary. The CLEVR dataset consists of 100k\nimages, the FFHQ has 70k images, Cityscapes has overall\nabout 25k images and LSUN-Bedrooms has 3M images.\nThe images in the Cityscapes and FFHQ datasets are mirror-\naugmented to increase the effective training set size. All\nmodels have been trained for the same number of training\nsteps, roughly spanning a week on 2 NVIDIA V100 GPUs\nper model.\nB. Spatial Compositionality\nTo quantify the compositionality level exhibited by the\nmodel, we employ a pre-trained segmentor to produce\nsemantic segmentations for the synthesized scenes, and\nuse them to measure the correlation between the atten-\ntion cast by the latent variables and the various semantic\nclasses. We derive the correlation by computing the maxi-\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nwall floorceiling bed pillowwindowcurtainlamp\npainting\nIOU\nSegment Class\nBedroom Correlation\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nroad sky\nground\nvegetation\ncar bus\nsidewalkbuildingfence\nIOU\nSegment Class\nCityscapes Correlation\nFigure 8.Spatial compositionality. Correlation between attention\nmaps and semantic segments, computed over 1k samples. Results\nare presented for the LSUN-Bedrooms and Cityscapes.\nmum intersection-over-union between a class segment and\nthe attention segments produced by the model in the dif-\nferent layers. The mean of these scores is then taken over\na set of 1k images. Results presented in ﬁgure 8 for the\nLSUN-Bedrooms and Cityscapes datasets, showing seman-\ntic classes which have high correlation with the model atten-\ntion, indicating it decomposes the image into semantically-\nmeaningful segments of objects and entities.\nTable 4.Hyperparameter choices. The latents number (each vari-\nable is multidimensional) is chosen based on performance among\n{8,16,32,64}. The overall latent dimension is chosen among\n{128,256,512}and is then used both for the GANformer and the\nbaseline models. The R1 regularization factor γis chosen among\n{1,10,20,40,80,100}.\nFFHQ CLEVR Cityscapes Bedroom\n# Latent var 8 16 16 16\nLatent var dim 16 32 32 32\nLatent overall dim 128 512 512 512\nR1 reg weight (γ) 10 40 20 100\nTable 5.Model size for the GANformer and competing ap-\nproaches, computed given 16 latent variables and an overall latent\ndimension of 512. All models are comparable in size.\n# G Params # D Params\nGAN 34M 29M\nStyleGAN2 35M 29M\nk-GAN 34M 29M\nSAGAN 38M 29M\nGANformers 36M 29M\nGANformerd 36M 29M\nGenerative Adversarial Transformers\nGAN\nStyleGAN2\nk-GAN\nFigure 9.State-of-the-art comparison. A comparison between models’ sample images for the CLEVR, LSUN-Bedrooms and Cityscapes\ndatasets. All models have been trained for the same number of steps, which ranges between 5k to 15k kimg training samples. Note that the\noriginal StyleGAN2 model has been trained by its authors for up to 70k kimg samples, which is expected to take over 90 GPU-days for a\nsingle model. See next pages for comparison with further models. These images show that given the same training length the GANformer\nmodel’s sample images enjoy higher quality and diversity compared to prior works, demonstrating the efﬁcacy of our approach.\nGenerative Adversarial Transformers\nSAGAN\nVQGAN\nGANformers\nFigure 10.A comparison of models’ sample images for the CLEVR, LSUN-Bedrooms and Cityscapes datasets. See ﬁgure 9 for further\ndescription.\nGenerative Adversarial Transformers\nGANformerd\nFigure 11.A comparison between models’ sample images for the CLEVR, LSUN-Bedrooms and Cityscapes datasets. See ﬁgure 9 for\nfurther description.\nGenerative Adversarial Transformers\nGAN\nStyleGAN\nk-GAN\nFigure 12.State-of-the-art comparison over training. A comparison between models’ sample images for the CLEVR, LSUN-Bedrooms\nand Cityscapes datasets, generated at different stages throughout the training. Sample images from different points in training are based\non the same sampled latent vectors, thereby showing how the image evolves during the training. For CLEVR and Cityscapes, we present\nresults after training to generate 100k, 200k, 500k, 1m, and 2m samples. For the Bedroom case, we present results after 500k, 1m, 2m, 5m\nand 10m generated samples during training. These results show how the GANformer, especially when using duplex attention, manages to\nlearn a lot faster than competing approaches, generating impressive images early in the training.\nGenerative Adversarial Transformers\nSAGAN\nVQGAN\nGANformers\nFigure 13.A comparison of models’ sample images for the CLEVR, LSUN-Bedrooms and Cityscapes datasets throughout the training.\nSee ﬁgure 12 for further description.\nGenerative Adversarial Transformers\nGANformerd\nFigure 14.A comparison of models’ sample images for the CLEVR, LSUN-Bedrooms and Cityscapes datasets throughout the training.\nSee ﬁgure 12 for further description."
}