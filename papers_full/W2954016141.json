{
  "title": "Collaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning",
  "url": "https://openalex.org/W2954016141",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287527105",
      "name": "Balachandar, Niranjan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Dieter, Justin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4327630537",
      "name": "Ramachandran, Govardana Sachithanandam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2209553695",
    "https://openalex.org/W2617547828",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2519058043"
  ],
  "abstract": "There are many AI tasks involving multiple interacting agents where agents should learn to cooperate and collaborate to effectively perform the task. Here we develop and evaluate various multi-agent protocols to train agents to collaborate with teammates in grid soccer. We train and evaluate our multi-agent methods against a team operating with a smart hand-coded policy. As a baseline, we train agents concurrently and independently, with no communication. Our collaborative protocols were parameter sharing, coordinated learning with communication, and counterfactual policy gradients. Against the hand-coded team, the team trained with parameter sharing and the team trained with coordinated learning performed the best, scoring on 89.5% and 94.5% of episodes respectively when playing against the hand-coded team. Against the parameter sharing team, with adversarial training the coordinated learning team scored on 75% of the episodes, indicating it is the most adaptable of our methods. The insights gained from our work can be applied to other domains where multi-agent collaboration could be beneficial.",
  "full_text": "Collaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement\nLearning\nNiranjan Balachandar, Justin Dieter, Govardana Sachithanandam Ramachandran\n*all authors contributed equally\nAbstract\nThere are many AI tasks involving multiple inter-\nacting agents where agents should learn to coop-\nerate and collaborate to effectively perform the\ntask. Here we develop and evaluate various multi-\nagent protocols to train agents to collaborate with\nteammates in grid soccer. We train and evaluate\nour multi-agent methods against a team operating\nwith a smart hand-coded policy. As a baseline,\nwe train agents concurrently and independently,\nwith no communication. Our collaborative proto-\ncols were parameter sharing, coordinated learn-\ning with communication, and counterfactual pol-\nicy gradients. Against the hand-coded team, the\nteam trained with parameter sharing and the team\ntrained with coordinated learning performed the\nbest, scoring on 89.5% and 94.5% of episodes\nrespectively when playing against the hand-coded\nteam. Against the parameter sharing team, with\nadversarial training the coordinated learning team\nscored on 75% of the episodes, indicating it is\nthe most adaptable of our methods. The insights\ngained from our work can be applied to other do-\nmains where multi-agent collaboration could be\nbeneﬁcial.\n1. Introduction\nThere are numerous tasks where collaboration is beneﬁcial:\ndriving, multi-player games, etc. In the future, many of the\ncars on the road will be autonomous (Heineke et al., 2017),\nso it would be useful for these cars to learn to collaborate\nto avoid accidents and improve efﬁciency. In multi-player\ngames, it would be beneﬁcial for agents to operate under a\ncoordinated protocol that leverages uniﬁed strategies with\nspecialization of responsibilities. A good example of such a\ngame is soccer, where players have a common team strategy\nwhile performing individual roles.\nWhile learning to collaborate for these tasks seems intuitive\nto humans, for a given task, it is unclear how to most ef-\nfectively balance AI collaboration with specialization. We\nmight consider treating the entire multi-agent system as a\nsingle agent and train this agent with a centralized controller.\nHowever, for a centralized controller the state and action\nspaces rise exponentially with the number of sub-agents, so\nthis would not be logistically practical or computationally\ntractable when the state-space is large or there are many\nagents.\nThus, decentralized methods that are capable of allowing\ncollaboration and communication between agents are de-\nsirable for these situations. Ideally, the policies learned by\nthe agents via decentralized methods will be adaptable to\nvariations in the policies of collaborating agents, general-\nizable, and capable of achieving similar performance to a\nthat of a centralized controller. Here we explore and eval-\nuate various multi-agent reinforcement learning methods\nin application to grid soccer, a multi-player soccer game\n(Iravanian), our objective is to train our multi-agent rein-\nforcement learning teams to defeat a team operating with an\nintelligent handed-coded strategy.\n2. Background/Related Work\nA number of methods for multi-agent control have been\ndeveloped in literature.\n2.1. Centralized\nThis approach involves a centralized controller that maps\nstates of all the agents to a joint action (an action for each\nagent) (Gupta et al., 2017). This is a Multi-agent Partially\nobservable Markov decision process (MPOMDP) policy.\nAdvantages: This scheme allows tighter coordination be-\ntween agents and the dynamics are stationary.\nDisadvantages: The main disadvantage of the centralized\napproach is the exponential increase in the state and action\nspaces with increase in number of agents. Manipulations can\nbe performed to reduce the joint action space size to n|A|\n(Moradi, 2016) for a discrete action space. For example, we\ncould reduce the action space by factoring action probability\nas P(⃗ a) = ∏\niP(ai) where ai are the individual actions of\nagents, which reduces the action space from |A|n to n|A|.\narXiv:1907.00327v1  [cs.MA]  30 Jun 2019\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nHowever, when there are many collaborating agents, this\nmethod may still be impractical. Thus for our paper, we will\nfocus on decentralized methods for multi-agent control.\n2.2. Concurrent Learning\nIn concurrent learning, each agent is independent and learns\nits own unique policy (Gupta et al., 2017). In this scheme\neach agent has an independent observation and indepen-\ndently executes actions based on its observation. Here the\nreward is shared between all the agents. Agents should\nlearn their individual roles while being evaluated with team\nperformance, so this method should effectively decompose\ncoordination in a distributed way.\nAdvantages: The agents learn heterogeneous policies and\nthe agents take speciﬁc roles, for example, in the soccer\nsimulator, an agent could learn to be defender, another can\nlearn to be the striker. Another advantage is the there is no\ncommunication between agents, which may be useful in a\nsystem where there are bandwidth constraints.\nDisadvantages: Since the agents do not share their experi-\nence with each other, the training of unique policies does not\nscale with a large number of agents. This adds to the sample\ncomplexity of the scheme. Another disadvantage is that\nsince all agents are learning policies independently, from\neach agent’s point of view the dynamics are not stationary.\n2.3. State sharing\nIn this scheme, neighboring agents share their observation\nand action space with each other (Chernova & Veloso, 2008).\nHere the neighboring agents’ actions and observation are\nstacked as the state space of each agent. This reduces the\nproblem to distributed constraint optimization (DCOP).\n2.4. Counterfactual Policy Gradients\nCounterfactual policy gradients is an actor critic method that\nleverages the utility of a centralized value approximation\nnetwork as with parameter sharing but also gives a method\nfor credit assignment to ensure that agents are properly\ngiven credit for individual actions rather than the actions\nof the group and allows for specialization of individual\nagents (Foerster et al., 2017) . The agents are represented\nby decentralized policy networks.\nThe counterfactual algorithm uses TD learning to update\nthe centralized critic and gradients of the policy network\nto perform updates. The policy gradients use a specialized\nadvantage function that analyzes the contributions of each\nagents individual actions to the overall received reward.\n3. Approach\n3.1. Grid Soccer Simulator\nGrid Soccer Simulator is a discrete grid simulation of multi-\nplayer soccer (Iravanian). We built a python API to interact\nwith this grid soccer simulator. The two teams have 3 play-\ners each that play on a grid of size 10 ×18, as shown in\nFigure 1. Goals are indicated by the bolded lines on the\nright and left sides of the grids, and at any given timestep,\none of the 6 players has the ball.\nIn order to score, a player must physically move to the goal.\nIt is possible (but obviously not advisable) for a player to\nscore in its own goal, thus rewarding the opposing team.\nAt each timestep, players can hold (stay still), move to\none of the 8 adjacent grid positions, or pass to a teammate.\nchoosing to move to an invalid position causes a player to\nremain at the same position. After each goal, a player from\nthe team that did not score starts with the ball. Players can\nsteal (cause a turnover of the ball) by running into a player\nof the opposing team who has the ball and is not holding or\nstanding in between teammates of the opposing team that\nare passing to each other.\nThe grid soccer game comes with a team that operates with\nan intelligent hand-coded strategy that is based on real-\nworld soccer strategies. For example, when the opposing\nteam has the ball and is close to the hand-coded team’s goal,\nthe players on the hand-coded team will go towards the\nhand-coded team’s goal to play defense.\n3.2. States\nWe intend to leverage the ability of Convolutional Neu-\nral Networks to analyze images, so at each timestep, we\nrepresent the state of each agent as a 4-channel image\nS ∈RH×W×4 where H and W are the height and width of\nthe grid respectively as illustrated in Figure 2. Each pixel\nof Sis a boolean indicating the presence of a certain object.\nThe ﬁrst channel indicates the agent location:\nSi,j,1 =\n{\n1 location of agent is (i,j)\n0 otherwise .\nThe second channel indicates the teammates’ locations:\nSi,j,2 =\n{\n1 location of any teammate is (i,j)\n0 otherwise .\nThe third channel indicates the opposing team members’\nlocations:\nSi,j,3 =\n{\n1 location of any opposing team member is (i,j)\n0 otherwise .\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nFigure 1.Sample rendering of grid soccer game state. Two of our AI teams are playing against each other.\nFigure 2.illustration of state representation as boolean 4-channel\nimages\nThe fourth channel indicates the ball’s location:\nSi,j,4 =\n{\n1 location of ball is (i,j)\n0 otherwise .\n3.3. Actions\nThere are a total of n+ 8 actions, where nis the number of\nplayers per team (n= 3 for our experiments). The possible\nactions for an agent currently located at (i,j) are listed in\nTable 1.\n3.4. Rewards\nWe constructed a handcrafted reward function to penalize\ngetting scored on, turnovers, and stalling to incentive steal-\ning the ball and scoring quickly. At each timestep, all agents\nin the team receive a reward, which depends on the result\nof the timestep. The possible results of a timestep and the\ncorresponding rewards are summarized in Table 2.\nAction Index Action\n0 stay at (i,j)\n1 move to (i+1,j)\n2 move to (i,j-1)\n3 move to (i-1,j)\n4 move to (i,j+1)\n5 move to (i+1,j+1)\n6 move to (i+1,j-1)\n7 move to (i-1,j-1)\n8 move to (i-1,j+1)\n8 + kfor k= 1 to n−1 pass to teammate k\nTable 1.possible actions for each agent if current agent location is\n(i, j) and each team has n players\nTimestep Result Reward\nagent own-goal -100\nteam own-goal -75\nagent scored goal 50\nteam scored goal 50\nopponent scored goal -50\nopponent own goal 10\nagent turns ball over -10\nteam turns ball over -10\nagent steals ball 10\nteam steals ball 10\nagent illegal movement -3\nagent successful pass -1\nagent hold -1\nagent legal movement -2\nTable 2.Rewards for each possible timestep result\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\n3.5. Deep Q-Learning\nBecause the state space is very large (around (2n)HW+1 =\n6181 for n = 3 players per team, tabular Q-learning is\nimpractical, so we approximate the Q-value with a Convo-\nlutional Neural Network (CNN). Our CNN takes as input\nan agent state ( H ×W ×4) image and outputs a vector\nof size |A|of Q-values for each action for the input state.\nOur CNN has 3 convolutional layers followed by 2 fully\nconnected layers, and the architecture is summarized in Fig-\nure 3. Each layer except the ﬁnal fully connected layer has\nReLU activation. For stability, we include a target network\nas included (Mnih et al., 2015). We update the target net-\nwork weights update after every iteration in which a goal\nis scored. We optimize the following objective function for\ngradient descent:\nL(θ) =\n(\nr+ γmax\na′\nˆQ(s′,a′,θ−) −ˆQ(s,a,θ )\n)2\nwhere θare the network weights and θ−are the target net-\nwork weights, and ˆQis the feed-forward output of the CNN.\nDuring training, we perform an epsilon-greedy exploration\nwith ϵdecayed over time and gradient descent with Adam\n(Kingma & Ba, 2014).\n3.6. Concurrent Learning\nOur baseline approach is concurrent learning with no com-\nmunication. Here each agent learns its own set Q-network\nweights. Since this approach has no explicit communica-\ntion or collaboration protocols, we expect this approach to\nperform the poorest.\n3.7. Parameter Sharing\nIn the parameter sharing scheme, the agents are homoge-\nneous. This allows model to be trained with experiences of\nall agents, which makes the algorithm data efﬁcient. Here\nthe learning is centralized while the control is decentral-\nized by sharing learned parameters of the model between\nthe agents. The agents still have different behavior since\neach agent receive different observations. This means a\nsingle policy is learned during train time and each agent\nexecutes the policy during inference with their independent\nobservation.\nAdvantage: The experience of all agents are used by the\nlearner, hence making them sample efﬁcient. Since the\nmodel is learned by a centralized learner and shared across\nall agents, they are computationally efﬁcient. Computational\nresources such as GPU can be allocated to the centralized\nlearner and each agent should use only light computational\nresources since inference is not computationally intensive.\nDisadvantage: It requires a centralized leaner and all of the\nagent’s observation, reward and action needs to be transmit-\nted to the leaner. This might be a limitation for an appli-\ncation that has limited bandwidth. The model lacks tighter\ncoordination between agents as there is no possibility of\ncommunication. Here the model learns co-ordination im-\nplicitly by learning protocols an individual agent should\nfollow based solely on its state.\nHere we use Q-Learning to update our value function ap-\nproximations. State, action, reward and the next sate from\nagents are transmitted to the centralized learner. For each\ntime step of each agent, the centralized learner computes\nthe Q-learning gradient to update the Q network. All agents\nshare the same Q-network weights.\n3.8. Coordinated Learning with Communication\nThe main limitation of parameter sharing is that they lack\ntight co-ordination between agents. We address this by deﬁn-\ning a communication mechanism between agents. Many\napproaches in the past that use communication for tighter\nco-ordination, formulate the problem as Distributed Con-\nstraint optimization (DCOP), which involves usage of the\nwhole system or at least a smaller subsystem of agents. This\nuses signiﬁcant communication bandwidth and causes la-\ntency in action selection for each agent. This becomes more\nintractable as the number of agents in the system increases.\nWe formulate a new form of communication that uses a very\nsmall amount of bandwidth where the action selection is\ndecentralized and does not involve computation for a group\nof agents. We achieved this by deﬁning a joint action with\ncommunication a∗:\na∗= (ai,ag)\nhere ai is the action taken by an individual agent. ag is one\nof a discrete number of communications that is broadcasted\nto other agents. The communications from neighboring\nagents are appended to an agents own observation xit, to\nform the state space\nsit = xit,ajt∀j,j ̸= i\nIn our DQN, we allow agents to receive this communication\nby having the following revised agent state representation.\nInstead of the original 4 channels, there will now be3+ |Ag|\nchannels. The agent location channel, opponents location\nchannel, and ball location channel remain. But now instead\nof a teammates locations channel, there will be |Ag|chan-\nnels, where channel ag will have a 1 at pixel (i,j) if there is\na teammate at (i,j) that broadcasted communication action\nag at the most recent timestep. Now our Q-network will\noutput to joint action space instead of action space. Our\nimplementation of coordinated learning also includes param-\neter sharing (each agent has the same Q-network weights)\nfor greater sample efﬁciency.\nOne of the limitations of this approach is that the action\nspace increases to |ai|×|ag|. Hence discretion is required\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nFigure 3.Q-value approximation network architecture\nin choosing the number of available group actions. In our\nexperiments on grid soccer, we achieve this by deﬁning a\njoint action:\na∗= arg max\nat\nQ(st,at)\nAnother limitation is that since each agent is learning to\ncommunicate the group actions and since the neighboring\nagents’ group actions are used as part of this agent’s state\nspace, the dynamics of the model are not stationary. To\nmitigate this issue, we use minibatches and a replay buffer\nfor the model updates. For each time step for each agent,\nwe capture experience et = (st,at,rt,st+1) and we update\na replay buffer R= e1,e2,...en. At each epoch, we sample\na minibatch from the replay buffer R. Use of minibatch and\nreplay buffer improves stability.\nThough coordinated learning with communication performs\nbest, it requires a lot of epochs to train. One of the main\nreasons for this is because, each agents receives the same\nreward. We address this by using appropriate credit as-\nsignment. The approach we explored is difference reward\n(Colby et al., 2015), which is deﬁned as\nDi(z) = G(z) −G(zi + ci)\nHere G(z) is the global reward of uniﬁed state z of the\nsystem, zi is the system state without agent i, and ci is the\ncounterfactual term. Since it is non-trivial to estimateG(zi+\nci), we formulated a simple strategy for credit assignment.\nWe assign a reward for each agent based on the following:\nRi = R.Qi(st,at)( 1\nN\nN∑\ni=1\nQi(sit,ait))\nHere N is number of agents in the system, Ris global re-\nward signal, and Qi(sit,ait) is the state action value for\nagent i at timestep t. The intuition behind this is that if\nthe agent directly contributed to the reward, the correspond-\ning Qi would be greater than for an agent which did not\ncontribute to overall performance of the system.\n3.9. Counterfactual Policy Gradients\nThe implementation of the counterfactual actor critic policy\ngradient method works as follows:\n3.9.1. C ENTRALIZED CRITIC\nThe centralized critic is a function approximator\nQ: S×A →R\nwhere A is the joint action space of all agents. The usual\nrepresentation of value approximation with a neural network\nthat outputs a vector of size |A|will not work as |A|will\ngrow exponentially with the number of agents and will\nbecome infeasible to represent as the output of a neural\nnetwork. Instead, the centralized critic can output a vector\nof size |A|to provide the Qvalues of a single agent whereA\nis the action space of a single agent. As an input, the network\ntakes in the other agents actions as well as a one-hot vector\nrepresenting which agent Q-values are being produced for.\nThis network structure is demonstrated in Figure 4.\nThe details of the actual model implemented are as follows.\nThe states were represented as a 3 dimensional tensor, with\nthe ﬁrst two dimensions the dimensions of the grid followed\nby one channel each for every player on the counterfactual\nteam, a single channel for every player on the opposing\nteam, and a channel for the location of the ball. This was\nfed into a convolutional layer with 32 output channels, a\n3×3 kernel, and a stride of1. This was then fed into another\nconvolutional layer with 64 output channels, a 4 ×4 kernel,\nand a stride of 2. This is then ﬂattened and a one hot rep-\nresentation of u−a as well as a one hot representation of a\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nFigure 4.Structure of the centralized critic for the counterfactual\nalgorithm. ut denotes the joint action at timestep t and u−a\nt\ndenotes the joint action excluding the action of agent a (Foerster\net al., 2017)\nis appended before being processed by two fully connected\nlayers with ReLu activation and respective sizes of 128 and\n64. Finally a linear layer is added at the end to output a vec-\ntor of size |A|. The network is trained using the SARSA(λ)\nalgorithm which gives a combination of SARSA style re-\nturns and Monte Carlo returns (Foerster et al., 2017). We\nused λ= 0.8 to be empirically effective but the model could\npotentially beneﬁt from more ﬁne tuning of this parameter.\n3.9.2. P OLICY NETWORK\nTo represent the stochastic policies of the actors, a policy\nnetwork with a softmax output of dimension |A|is used. To\nallow the agents to share what they have learned parameters\nare shared but a one-hot representation of which agent is\nusing the network is passed as an input to enable special-\nization of agents. The exact structure of the neural network\nis as follows. The state is represented as a 3 dimensional\ntensor with a channel for the current agent’s locations, a\nchannel for the ball’s location, a channel for the location\nof teammates, and a channel for the location of opposing\nplayers. This is fed into a convolutional layer with 32 out-\nput channels, a 3 ×3 kernel, and a stride of 1. This was\nthen fed into another convolutional layer with 64 output\nchannels, a 4 ×4 kernel, and a stride of 2. This is then ﬂat-\ntened and a one hot representation of ais appended before\nbeing processed by two fully connected layers with ReLu\nactivation and respective sizes of 128 and 64. A softmax\nlayer is added at the end to give an output dimension of |A|.\nAdditionally a decaying randomness factor of ϵ is added\nsuch that π(a|s) = (1 −ϵ)π(a|s) + ϵ to ensure that the\nagent is exploring sufﬁciently. ϵis decayed from 0.5 to 0.05\nover 300000 timesteps.\n3.9.3. A CTOR TRAINING AND COUNTERFACTUAL\nADVANTAGES\nThe unique representation of the centralized critic allows\nfor a very useful advantage function to be computed.\nAa(s,u) = Q(s,u) −\n∑\nu′a\nπa(u′a|τa)Q(s,(u−a,u′a))\nThis counterfactual advantage provides a metric of how\nmuch better/worse an agents actions will be compared to\nif it had chosen a different action with the actions of other\nagents held ﬁxed. This provides a method of credit assign-\nment to ensure that agents are only rewarded for how their\nactions contribute to the success of the team verses getting\nrewarded/penalized for results they did not contribute to.\nThis advantage is then fed into the typical policy gradient\nupdate rule with advantages:\n∆θ= α\n∑\na\n∇θlogπa(ua|τa)Aa(s,u)\nwhere θis the parameters of the policy network. αis the\nlearning rate, empirically chosen to be 0.001 for our ex-\nperiments. Also, all the theoretical guarantees of conver-\ngence with normal actor critic methods hold for this actor\ncritic method with counterfactual advantages (Foerster et al.,\n2017).\n4. Experiment Results\nThe current code for all our simulators, APIs, and learn-\ning algorithms is included in https://github.com/\njdietz31/CS234-MultiagentProject. We eval-\nuate the performance of our algorithms by measuring the\n“goal ratio”, which is the proportion of goals scored by the\nalgorithm’s team of the last 200 goals. So the worst possible\ngoal ratio an algorithm can achieve is 0, and the best possi-\nble goal ratio an algorithm can achieve is 1. We train each\nof our methods for hundreds of thousands of iterations. For\neach of our models, we use a learning rate of 0.001, initial\nepsilon of 0.5, ﬁnal epsilon of 0.05, and perform linear ep-\nsilon decay over the course of training. We vary the decay\nrate to account for differing sample efﬁciencies. For coordi-\nnated learning, we set the minibatch size to1,000 and replay\nbuffer size to 50,000. Hyperparameters for Counterfactual\nare speciﬁed in the methods section.\nWe then trained our two best models- parameter sharing\nvs coordinated learning against each other to further de-\nlineate performance. The results of this adversarial train-\ning are shown in Figure 7. Videos of the parameter shar-\ning team playing the coordinated learning team are in-\ncluded here: https://drive.google.com/open?\nid=1jntEENSbnW_oyoFZJuCobBHybO_jdpUy.\nThe results in Figure 5 and Figure 6 shows the training per-\nformance and ﬁnal performance of all of our different multi-\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nFigure 5.Performance of multiagent models throughout training in terms of ratio of goals scored against the hand-coded team.\nFigure 6.Performance of multiagent models in the last 1000 iterations of training.\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nFigure 7.Goal ratio of the coordinated learning team when training adversarially against parameter sharing after both are pretrained\nagainst the hand-coded team.\nagent models in terms of the ratio of goals scored against\nthe hand-coded agent. The Concurrent model achieves de-\ncent performance, but we observe that one of the agents\ntends to learn offense and defense, while the other two learn\nvery little, so there is almost no teamwork. Counterfactual\nlearns quickly but converges to a local optima where it only\nplays offense and no defense. Parameter Sharing learns to\nbe almost perfect but is slightly unstable in its performance\nthroughout training. The coordinated model learns the slow-\nest but is stable and converges to near perfect performance.\nAlso, when allowed to train adversarially against other mod-\nels after being pretrained against the hand-coded team, coor-\ndinated learning performs the best as demonstrated in Figure\n7.\n5. Conclusion\nOur results show that the model with communication is\nable to perform the best against both the hand coded agent\nand against our other reinforcement learning agents. It is a\nsimple enough model that it is able to learn very effectively\nand its communication model allows it to learn methods of\ncooperating and team strategy that simpler models cannot\ncompete with. The counterfactual policy gradient method\nis the most versatile and complex model and with sufﬁcient\nhyper-parameter tuning it should be able to surpass other\nmodels. The complexity of the model makes it promising to\nbe able to learn difﬁcult tasks but also makes it very unstable\nand not very robust with respect to performing well with\na variety of hyper-parameters. Future work can expand on\nmaking this counterfactual method more robust and less\nunstable. Also, our experiments only worked with 3 agents\nand real world multiagent systems can be composed of many\nmore agents. Future work should investigate into how these\nmodels can perform in more complicated scenarios with\nmany more agents.\nReferences\nChernova, Sonia and Veloso, Manuela. Teaching multi-\nrobot coordination using demonstration of communica-\ntion and state sharing. In Proceedings of the 7th Inter-\nnational Joint Conference on Autonomous Agents and\nMultiagent Systems - Volume 3, AAMAS ’08, pp. 1183–\n1186, Richland, SC, 2008. International Foundation for\nAutonomous Agents and Multiagent Systems. ISBN\n978-0-9817381-2-3. URL http://dl.acm.org/\ncitation.cfm?id=1402821.1402826.\nColby, Mitchell K., Curran, William, and Tumer, Ka-\ngan. Approximating difference evaluations with lo-\ncal information. In Proceedings of the 2015 Interna-\ntional Conference on Autonomous Agents and Multia-\ngent Systems, AAMAS ’15, pp. 1659–1660, Richland,\nSC, 2015. International Foundation for Autonomous\nAgents and Multiagent Systems. ISBN 978-1-4503-3413-\n6. URL http://dl.acm.org/citation.cfm?\nid=2772879.2773372.\nFoerster, Jakob N., Farquhar, Gregory, Afouras, Triantafyl-\nlos, Nardelli, Nantas, and Whiteson, Shimon. Counterfac-\ntual multi-agent policy gradients. CoRR, abs/1705.08926,\n2017.\nGupta, Jayesh K., Egorov, Maxim, and Kochenderfer,\nMykel J. Cooperative multi-agent control using deep\nCollaboration of AI Agents via Cooperative Multi-Agent Deep Reinforcement Learning\nreinforcement learning. In Adaptive Learning Agents\nWorkshop, 2017. doi: 10.1007/978-3-319-71682-4 5.\nHeineke, Kersten, Kampshoff, Philipp, Mkrtchyan,\nArmen, and Shao, Emily. Self-driving car tech-\nnology: When will the robots hit the road?\nhttps://www.mckinsey.com/industries/\nautomotive-and-assembly/our-insights/\nself-driving-car-technology-when-\nwill-the-robots-hit-the-road , May 2017.\nIravanian, Sina. Grid-soccer simulator and\nagents. https://github.com/sinairv/\nGridSoccerSimulator.\nKingma, Diederik P. and Ba, Jimmy. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014.\nMnih, V olodymyr, Kavukcuoglu, Koray, Silver, David, et al.\nHuman-level control through deep reinforcement learn-\ning. Nature, 518(7540):529–533, February 2015. ISSN\n00280836. URL http://dx.doi.org/10.1038/\nnature14236.\nMoradi, Milad. A centralized reinforcement learning\nmethod for multi-agent job scheduling in grid. CoRR,\nabs/1609.03157, 2016. URL http://arxiv.org/\nabs/1609.03157.",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7818906307220459
    },
    {
      "name": "Computer science",
      "score": 0.6834601163864136
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.6795987486839294
    },
    {
      "name": "Task (project management)",
      "score": 0.6686440706253052
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5533016324043274
    },
    {
      "name": "Adversarial system",
      "score": 0.5250570774078369
    },
    {
      "name": "Machine learning",
      "score": 0.4005821943283081
    },
    {
      "name": "Human–computer interaction",
      "score": 0.36231696605682373
    },
    {
      "name": "Engineering",
      "score": 0.13766783475875854
    },
    {
      "name": "Psychology",
      "score": 0.07893401384353638
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": []
}