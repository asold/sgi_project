{
  "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
  "url": "https://openalex.org/W4206650881",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2033460667",
      "name": "Ashok Kumar Durairaj",
      "affiliations": [
        "Bharathidasan University"
      ]
    },
    {
      "id": "https://openalex.org/A3127295749",
      "name": "Anandan Chinnalagu",
      "affiliations": [
        "Bharathidasan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2934853022",
    "https://openalex.org/W3166340303",
    "https://openalex.org/W3184182155",
    "https://openalex.org/W3202819097",
    "https://openalex.org/W3046978282",
    "https://openalex.org/W3093495200",
    "https://openalex.org/W3197482624",
    "https://openalex.org/W3096786051",
    "https://openalex.org/W3191794863",
    "https://openalex.org/W3172969687",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W3139172628",
    "https://openalex.org/W3193514658",
    "https://openalex.org/W3096187181",
    "https://openalex.org/W6719819555",
    "https://openalex.org/W3115080939",
    "https://openalex.org/W3202851885",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We've proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.",
  "full_text": "(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n474 | P a g e  \nwww.ijacsa.thesai.org \nTransformer based Contextual Model for Sentiment \nAnalysis of Customer Reviews: A Fine-tuned BERT \nA Sequence Learning BERT Model for Sentiment Analysis \nAshok Kumar Durairaj1, Anandan Chinnalagu2 \nDepartment of Computer Science, Govt. Arts College \n(Affiliated to Bharathidasan University Tiruchirappalli) \nKulithali, Karur, India \n \n \nAbstract—The Bidirectional Encoder Representations from \nTransformers (BERT) is a state -of-the-art language model used \nfor multiple natural language processing tasks and sequential \nmodeling applications. The accuracy of predictions from context -\nbased sentiment and analysis of customer review data from \nvarious social media platforms are challenging and time -\nconsuming tasks due to the high volumes of unstructured data. In \nrecent years,  more research has been conducted based on the \nrecurrent neural network algorithm, Long Short -Term Memory \n(LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, \nneutral, and traditional text classification algorithms. This paper \npresents our experimental r esearch work to overcome these \nknown challenges of the sentiment analysis models, such as its \nperformance, accuracy, and context -based predictions. We’ve \nproposed a fine -tuned BERT model to predict customer \nsentiments through the utilization of customer re views from \nTwitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we \ncompared the results of the proposed model with our custom \nLinear Support Vector Machine (LSVM), fastText, BiLSTM and \nhybrid fastText -BiLSTM models, as well as presented a \ncomparative an alysis dashboard report. This experiment result \nshows that the proposed model performs better than other \nmodels with respect to various performance measures. \nKeywords—Transformers model; BERT; sequential model; \ndeep learning; RNN; LSVM; LSTM; BiLSTM; fastText \nI. INTRODUCTION \nThe RNN, LSTM, gated recurrent neural network (GRNN) \nand BiLSTM models are some of the various sequence models \n[1] for NLP tasks, language modeling, and machine \ntranslation. The Google research team introduced (Year 201 7) \nthe first Transformer and the first transduction model that \nreplaces the recurrent layer with attention. The first \nTransformer model was used for language translation (English \nto France and English to German) tasks and the results showed \nthat it outperf ormed all other neural model architecture with \nconvolutional or recurrent layers [2].  Fig. 1 shows the \nTransformer architecture and the build blocks of BERT \nmodel. In this Transformer architecture, input sequence \n                 ) mapped for symbol  representation of \nencoder to the continuous sequence representation of decoder \n                  ) = z the z decoder generates the  \n                 ) sequence symbol of an element output at a \ntime. \nModel generates the symbols based on the outpu t of \npreviously generated symbols as an additional input, it uses \nthe auto-regressive method at each step for next generation of \nsymbols [2]. The overall Transformer architecture is based on \nself-attention with fully connected encoder and decode. \nBERT is a  pre-trained, an open -sourced (Year 2018) [3] \nand transformer -based language model from Google. It’s \ndesigned to pre -train bidirectional (left and right) text \nrepresentations from unlabeled text [4]. The BERT BASE and \nBERTLARGE are two original models. The base model consists \nof 12 Encoders and bidirectional self -attention, while the large \nmodel consists of 24 Encoders and 16 bidirectional heads. \nBERT model is pre -trained on 800 million words from \nBooksCorpus and English Wikipedia’s unlabeled text of 2.5 \nbillion words. As BERT is pre -trained with large unlabeled \ntext datasets, this model can be easily fine -tuned for small \ndatasets that are specific to an NLP task like sentiment \nprediction on customer or employees’ reviews and question -\nanswer system for chatbot applications. \nFig. 2 shows the neural network architecture of BERT’s \ndeep bidirectional and OpenAI GPT’s unidirectional (Left -to-\nRight) contextual models [1], in which the unidirectional \nmodel generates a representation for each word based on other \nwords in the same sentence. The BERT bidirectional model \nrepresents both the previous and next context in a sentence. \nHowever, the context free Word2vec and Glove models \ngenerate a word representation based on each word in the \nvocabulary. \nThere are many organiz ations that rely on reviews to \nimprove customer experience and increase revenue of their \nproducts and services. Positive sentiment s are one of the key \nfactors for the success of several  online businesses. However, \ndetermining the context of the review, pol arity, and sentiment \nin textual content of customer reviews remain a challenge. The \ncustom and hybrid deep learning models (LSTM, BiLSTM, \nfastText and fastText -BiLSTM) perform higher in textual \ndatasets compare to traditional models [ Naive Bays (NB), \nLogistic Regression (LR), Decision Tree (DT), Random \nForest (RF), and Support Vector Machines (SVM)]. \n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n475 | P a g e  \nwww.ijacsa.thesai.org \n \nFig. 1. Multi-layer Transformer Architecture. \n \nFig. 2. BERT and OpenAI GPT Neural Network Architecture. \nEvaluating Transformer based models and comparing their \nresults with other state-of-the-art LTSM models are important \nfor identifying the best model to utilize for Sentiment Analysis \napplications. Several researchers have used traditional models \nfor sentiment analysis on datasets of customer reviews and \ndiscussed abou t their experience and issues occurred from it. \nThese issues include time consumption of tasks such as data \npre-processing, preparation for testing and training the \ndatasets, as well as problems in performance and accuracy. \nWe identified the gap in leverag ing the pre -trained BERT \nmodel for datasets of customer reviews and found that the pre -\ntrained model solves the problems and issues that traditional \nmodels have. \nThe pre-trained BERT model’s performance, accuracy and \napproach motivated the authors to exper iment with this model \nfor customer sentiment analysis.  The main objectives and \ncontributions are as follows; \n Overcome known challenges of SA model \nperformance, accuracy and context-based prediction. \n Train BERT -base-cased model on Twitter, IMDB \nMovie Reviews, Yelp, and Amazon customer reviews \ndatasets to improve the accuracy and performance of \nthe model. \n Evaluate the custom deep learning sequential model of \nBiLSTM, hybrid fastText -BiLSTM model and linear \nmodels of LSVM, fastText models using the same \ndatasets. \n Compare the results of the BERT model with the \nresults of the deep learning sequential and linear \nmodel. \n Customize the data pre -processing steps for hybrid and \nlinear model training. \n Fine-tune the hyperparame ters for fastText -BiLSTM \nmodels. \nThis paper presented with several recent BERT and SA \nrelated research papers reviews and major contributions from \nvarious researchers in Section II . T he literature reviews of \nBERT, fastText, BiLSTM, and LSVM models are presented in \nSection III. The experimental setti ng and model evaluation \nresults are discussed in the Section IV. In Section  V, \nconcluded this paper with model results, findings of this \nresearch work and future work. \nII. RELATED WORK \nRecently, many researchers evaluated BERT model for \nmany NLP tasks. In this  section, presented most recent \nresearch papers on SA and pre-trained BERT models. \nIn 2021, [5] evaluated RNN+LSTM and BERT model for \nsentiment analysis to detect cyberbullying based on Twitters \nSpanish language dataset. The evaluation results show the \naccuracy and performance of the BERT model outperformed \nRNN+LSTM by 20%. Based on evaluation the bert -base-\nmultilingual-uncased and Bert -large-uncased models show \nmore accuracy.  However, BERT model requires higher \nconfiguration of the computational environmen t. A challenge \nresearchers face is finding the investment on a grand -scale \ninfrastructure to train the models.  There are many BERT -\nbased SA experimental research, case studies and review \npapers presented for Arabic aspect -based [6], Italian Twitter \nSA [7] and Bangla-English Machine Translation [8]. \nIn 2021, [9] present a large -scale open source pre -trained \nBERTweet model for English Tweets. BERTweet used for \nPart of speech (POS), recognition of Named entity and text \nclassifications. Experimental result show s that it outperforms \nXLM-Rbase and RoBERTabse models, all these models are \nhaving a same architecture of BERT -base. There are several \nmodels available as open -sourced, whereas other models are \ninaccessible to be customized for commercial use. \nIn 2020, [10 ] presented a research article about a question \nanswering (QA) system based on LSTM, multilingual BERT \nand BERT+vnKG models. They had used crafted Vietnam \ntourism QA dataset for this experiment and evaluated three \nmodels with the same data. The experimenta l result shows the \nproposed model outperformed other model in terms of time \nand the accuracy. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n476 | P a g e  \nwww.ijacsa.thesai.org \nIn 2021, [11] conducted an Aspect -based SA study on \nconsumers product reviews data. They have proposed two \nBERT models for aspect extraction, sentiment classifica tion \nusing parallel and hierarchical aggregation methods based on \nhierarchical transformer model [12]. The following Fig.  3 and \nFig. 4 show the parallel and hierarchical models.  Study result \nshows that applying their proposed model approach improved \nthe mo del performance and eliminate the need of further \nmodel training. \n \nFig. 3. Parallel Agrregation Model. \n \nFig. 4. Hierarchical Aggregation Model. \nIn 2021, [13] presented a research paper on attentions and \nhidden representation of learned pre -trained BERT models for \naspect-based SA (ABSA) on reviews datasets. This research \nstudy found that BERT uses the aspect representation \ndedicated to semantics of the domain and self -attention head \nencode the context of the words . In 2021, [14] conduct a \nexperiment based on Aspect -level SA (ABSA) using ALM -\nBERT model on consumer datasets, ALM -BERT model show \nthe better performance than the other models. \nIn 2021, [15] presented BERT -based SA models research \npaper, in this research analyzed Github, Jira web portal \ncomments and Stack O verflow posting datasets related to \nSoftware Engineering.  comments. Authors used vanilla \nBERT, fine -tuned ensemble BERT and compressed \nDistilBERT models  [16] and based on F1 score measure the \nensemble and compressed BERT models improved \nperformance by 6 -12% over other model.  Another research \nstudy conduted by [17] on GitHub dataset and the exprimental \nresults show 94% accuracy of emotion classification. \nIn 2021, [18] published an article of SA models \ncomparative analysis od machine and deep learnings models , \nSA research work carried out by using Naive Bayes (NB), \nSupport Vector Machine (SVM), LSTM and BERT -based \nuncased model on consumer reviews and rating dataset from \nAmazon, Flipkart e-commerce platforms. Binary classification \nof consumer sentiment is cate gorized as positive and negative. \nThis research study results show that deep learning BERT \nuncased model improved performance, and higher accuracy of \nsentiment predictions compared [19][20] to other machine \nlearning models. \nIII. PROPOSED MODEL AND METHODOLOGY \nProposed sentiment analysis frameworks and models’ \narchitectures present with data pre -processing steps in the \nfollowing section. \nA. Data Pre-processing \nWe’ve developed python scripts for data pre -processing \nsteps, customized the scripts and steps sequences ba sed on \ndataset. Our custom script consists of data cleansing, reduction \nof unwanted data, data transformation and validations steps. \n \nFig. 5. Data Pre-processing Steps. \nThe above Fig. 5 shows data pre-processing steps. For this \nexperiment, Twitter, IMDB Movie Reviews, Yelp, Amazon \ncustomer reviews datasets are used for training and testing the \nmodels. To improve the data quality, model performance and \naccuracy, performed the following pre -processing tasks: \nconverted text to lowercasing text and removed stop wor ds, \nconverted negation words to normal word, removed white \nspaces, special characters, punctuations , stripping recurring \nheaders from the text and stemming. \nB. Proposed BERT Model \nWe propose fine-tuned BERT-base-cased model for \nsentiment prediction on custome r reviews datasets . The \nadvantage of BERT  model is that it was pre-trained on large \ncorpus of raw  texts data. These models are used for Masked \nLanguage Modeling (MLM)  and Next Sentence Prediction \n(NSP) and these pre -trained models can be fine -tuned for \ndownstream applications [2]. BERT -base fine -tuned SA \nmodel framework is represented in Fig. 6. This framework \nconsists of the following four components, input data pre -\nprocessing, Tokenization of input text for model, BERT -base-\ncased model and classification layer. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n477 | P a g e  \nwww.ijacsa.thesai.org \n \nFig. 6. BERT-base-cased Sentiment Analysis Framework. \nThe input layer generated tokens for given pre -processed \ntext, the following Fig. 7 shows the embedding layers of \ntokanization of sentences. The Token embeddings is the sum \nof segement and position emb eddings. These embeedings are \nlearned specific token from WordPiece token vocabulary [4]. \nTo train the model on customer review dataset,  we follow \nthe pre -train procedure with Adam optimzer from Hugging \nFace. following Fig. 8 shows the  training history of \nBERTmodel on our datset, the model training result show the \nclose to 100% accuracy after 10 epochs. We’ve evaluated the \npre-trained model and the evalution result show 95%  accuracy \non our test dataset. \nFine-tuned the hyperparameters for higher accuracy and  \nperformance for customer reviews datasets. The Softmax \nfunction applied in the output layer to get the predicted \nprobabilities of sentiment values. Here are the questions for \nprobabilities and Softmax. \n            \n         \n                        (1) \n \nFig. 7. Input Representation of BERT Model. \n \nFig. 8. Model Training History. \nSoftmax performs the transform on                  \nnumbers. \n           )  \n    \n∑     \n \n   \n              (2) \nC. FastText, BiLSTM and FastText-BiLSTM Models \nThe Fig. 9 shows the neural networks SA framework used \nfor this experimental research. These models are based on \nfastText and LSTM architectures. \nFastText introduce by Facebook AI research (FAIR) lab \nfor text representation and classi fication, it’s a lightweight \nmethod and work s on generic hardware  with multicore CPU, \nthe fastText models show faster performance, accuracy during \nmodel training and evaluation [21]. A new extension of the \ncontinuous skipgram and Continuous Bag of Words (C BOW) \nmodel like word2vec  word embedding is introduce d by \nfastText, where each word is represented as a bag of character \nn-grams. fastText model is Pre -Trained on Wikipedia dataset \n(294 languages)  [22]. To compare with BERT model result, \nwe evaluate customer review dataset using fastText model. \nAuthors build a two novel multilayer sequence processing \ncustom models using BiLSTM and hybrid fastText -BiLSTM. \nit consists of two LSTM units and multilayers, one unit taking \nthe input in a forward direction and other  unit taking the input \nin a backward direction [23]. In this experiment, our main goal \nis to compare the BERT model result with authors The Hybrid \nfastText-BiLSTM and custom multilayer BiLSTM. Models \nconsist of 196 memory units, 128 Embedding Layer, 5 Dens e \nLayer and Softmax activation function at output layer. The \nLSTM unit consists of input, output and forget gates and these \nthree gates are the activation of sigmoid function; the sigmoid \noutput value is between 0 and 1 , when gates are blocked the \nvalue is 0 and gates allow the input to pass through when the \nvalue is 1 . The following are the equations of sigmoid and \ninput, output and forget gates. \n     )  \n \n                   (3) \n                    )             (4) \n                    )            (5) \n                    )             (6) \n  represents sigmoid function, \n   represents input at current timestamp. \n      represents LSTM block output of previous state at \ntimestamp t-1. \n  ,           are representing weight of input, forget and \noutput gates. \n  ,    and    are representing bias for input, output, forget \ngates. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n478 | P a g e  \nwww.ijacsa.thesai.org \n \nFig. 9. FastText, BiLSTM and FastText-BiLSTM Models. \nD. LSVM Model \nLinear SVM (LSVM) is a most frequently used supervised \nlearning algorithm for  classification problems. SVM has two \ntypes of classifiers Linear [24] and Non -Linear. In Linear \nClassifier separates the data in a liner order  and a data point \nconsidered as a p-dimensional vector, but the best hyperplane \nconsider one which maximize the margin. The text can be \ncategorized linearly using the SVM’s linear kernel and LSVM \n[25] works well with lot of features and also less parameters to \noptimize for training the model . We used linear k ernel for this \nexperiment. Fig.  10 shows the LSVM architecture and SA \nframework used for this experiment. \n \nFig. 10. Linear SVM (LSVM) Model. \nHere is the equation for linear kernel function      ), \ngiven by the inner product <     , and  c represent the \noptional constant. \n     )                    (7) \nIV. EXPERIMENTAL SETTING AND RESULTS \nThere are four models and two environments used for this \nexperiment. The data source, dataset, environment settings, \nfine-tuned hyperparameters and experimental results are \npresented in the following sub-sections. \nA. Data Source and Dataset \nThere are multiple data sources are available online for \ncustomer reviews data, we have obtained publicly available \nTwitter, IMDB Movie Reviews, Yelp and Amazon customer \nreviews datasets from  Kaggle.com. Total 778631 customer \nreviews finalized after preprocessing the raw dataset and split \nthe dataset 545041 (70%) for training and 233590 (30%) for \ntesting the models. The data pre -processing steps are \nexplained in the proposed model Section III.  Models have \ntrained and tested after fine-tuning the hyper parameters. \nB. Experimental Environments \nTraining and evaluation of the BERT model is used \nthrough the Google Colaboratory cloud environment, while a \nstandard server is used for training and evaluati ng the \nfastText, LSVM and SA -BLSTM models. Table I lists the \ndetails and components used for this experiment. \nTABLE I. EXPERIMENTAL ENVIRONMENTS \nEnvironment #1 Description \nModel BERT-base-cased \nTransformer Hugging face \nServer Configuration Google Colab Cloud \nProgramming Language & Tool PyTorch, Python 3.8.8 \nJupyter Notebook 6.3.0 \nEnvironment #2 Description \nModel fastText, LSVM and SA-BLSTM \nServer Configuration Windows 64-bit Operating System with \nIntel core i7 processor, 16 GB Memory. \nWord Embedding  fastText Library and Keras Encoder  \nProgramming Language & Tool Python 3.8.8 \nJupyter Notebook 6.3.0 \nLibraries and Frameworks fastText, Pandas, Numpy, Seaborn, \nMatplotlib, Nltk, Scikit-learn, Keras \nC. Parameters Fine-tuning for Models \nTable II lists the values of parameters set for these models. \nTABLE II. PARAMETERS SETTINGS \nModel Parameters Value \nBERT \nMode \nTransformer  \nBatch Size  \nToken Length  \nEpoch  \nLearning Rate \nOptimizer  \nLoss  \nBert-base-cased \nHugging Face  \n16 \n160 \n10 \n0.0002 \nAdam \nSoftmax \nLSVM \nUnigram,  \nBigram \nTrigram \nKernel \n \n \n \nLinear \nfastText \nUnigram  \nBigram \nTrigram \nEpoch  \nLearning Rate \nLoss  \n \n \n \n10 \n0.01 \nSoftmax \nBLSTM \nEpoch=10 \nLearning Rate \nLoss=softmax \n10 \n0.01 \nSoftmax \nfastText-BLSTM \nEpoch \nLearning Rate \nLoss \n10 \n0.01 \nSoftmax \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n479 | P a g e  \nwww.ijacsa.thesai.org \nD. Performance Measures \nThe models evaluated are based on accuracy, recall, \nprecision and F1 score while the performance measures are \ncalculated based on the True Positive (TP), True Negative \n(TN), False Positive (FP) and False Negative (TN) matrix.  \nThe following Table I II lists the value s of performance \nmeasures. \nTABLE III. EVALUATION METRIC \nModels TP FP TN FN \nBERT 102913 11089 107371 12217 \nLSVM 95772 18687 105116 14015 \nfastText 107451 11680 98108 16351 \nBiLSTM 88764 25695 91100 28031 \nfastText-BiLSTM 104117 9885 106036 13552 \nThe following are the  equations to calculate the \nperformance measures: \n          \n  \n                  (8) \n       \n  \n                   (9) \n            \n                \n                          (10) \n                \n                \nThe following Table IV and Table V show the \nperformance measures and sentiment score of the models. \nTABLE IV. MODELS PERFORMANCE MEASURES \nModel Accuracy % Recall Precision F1 \nBERT 90% 0.91 0.90 0.90 \nLSVM 86 % 0.87 0.84 0.85 \nfastText 88% 0.87 0.89 0.88 \nBiLSTM 77% 0.75 0.77 0.76 \nfastText-BiLSTM 90% 0.87 0.91 0.89 \nTABLE V. SENTIMENT SCORE \nModels Positive  Negative  \nBERT 49.28% 50.71% \nLSVM 46.99% 53.00% \nfastText 52.99% 47.00% \nBiLSTM 50.00% 50.00% \nfastText-BiLSTM 50.37% 49.62% \nThe results of the accuracy are compared with other \nresearchers’ model results for the same datasets. The result \nshowed BERT and fastText models’ accuracy are 90% \ncompared to other models. Table VI shows the comparison of \nmodel accuracy . Our experimental results performance \nmeasures show that fine-tuned Pre -trained BERT model \nscores highest F1 score of 0.90 and hybrid fastText -BiLSTM \nmodel scores F1 score of 0.89 and other models scored less \nthan 0.85 . BERT show s the best result as a state -of-the-art \nmodel with respect  to per formance, accuracy, training and \ntesting on customer reviews datasets. \nTABLE VI. COMPARE MODELS ACCURACY SCORE \nEt.al Dataset Model Accuracy % \nAshok and \nAnandan \nTwitter \nCustomer \nReview \nfastText \nUnigram \nBigram \nTrigram \n88.23 \n90.55 \n90.71 \nLSVM \nUnigram \nBigram \nTrigram \n87.74  \n89.96 \n90.11 \nSA-BLSTM 77.00 \nfastText-BLSTM 90.00 \nBERT 90.00 \nGeetika \nGautam \nTwitter \nCustomer \nReview \nSVM \nMax Entropy \n Naïve Bayes Semantic \nAnalysis (WordNet) \n85.50 \n83.80 \n88.20 \n89.90 \nSeyed-Ali \nBahrainaian \nTwitter Data \non Smart \nphones \nHybrid Approach- \nUnigram, Naive Bayes, \nMaxEnt, SVM \n89.78 \nNeethu M.S  \nTwitter data \non \nelectronic \nproducts \nNaïve Bayes \nSVM \nMax Entropy \nEnsemble \n89.50 \n90.00 \n90.00 \n90.00 \nDhiraj \nGurkhe Twitter Data \nUnigram \nBigram \nUnigram-Bigram \n81.20 \n15.00 \n67.50 \nV. CONCLUSION AND FUTURE WORK \nThe proposed BERT model outperforms in terms of \naccuracy and model performance compare to other models. \nThe results of the fastText model showed low accuracy when \nunigram and bigram methods were used for training the \nmodel. The overall model tra ining and data preparation tasks \ntook less time for BERT model in comparison to others. This \nexperiment reveals that the BERT model required more \ncomputational resources to train compared with other \ntraditional models. The fastText model performed well wit h a \nstandard server environment with minimal computational \nresources compare to other models. The fine -tuned BERT \nmodel simplifies the sentiment an alysis tasks on large \ndatasets. \nA proposal for future research can be made to build \ntransformer models for va rious domains, and framework for a \ncontinuous model trained to streamline data processing \nmethods. In the future, more work will be done to conduct \npre-training a BERT -base model on datasets of customer \nreviews for sentiment analysis and detecting emotions. \nREFERENCES \n[1] Connor Holmes, Daniel Mawhirter, Yuxiong He, Feng Yan, Bo Wu, \n“GRNN: Low -Latency and Scalable RNN Inference on GPUs”, In \nFourteenth Eurosys Conference (Eurosys ’19) \nhttps://doi.org/10.1145/3302424.3303949, March, 2019. \n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 12, No. 11, 2021 \n480 | P a g e  \nwww.ijacsa.thesai.org \n[2] Ashish Vaswani, Noam Sha zeer, Niki Parmar, Jakon Uszkoreit, Llion \nJones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, “Attention is \nall you need”. 31st Conferencence on Neural Information Processing \nSystems (NIPS), Long Beach, CA, USA, December, 2017. \n[3] Jacob Devlin and Ming -Wei Chnag, Research Scientists Google AI \nLanaguage, “https://ai.googleblog.com/2018/11/open -sourcing-bert-\nstate-of-art-pre.html”, November 2, 2018. Accesed on 10/30/2021. \n[4] Jacob Devlin, Ming -Wei Chang, Kenton Lee, Kristina Toutanova, \n“BERT:Pre-training of Dee p Birirectional Transformers for Lanaguage \nUnderstanding” Google AI Lanaguage, arXiv:181 0.04805v2 [cs.CL] 24 \nMay, 2019. \n[5] Diego A. Andrade -Segarra1, Gabriel A. Leon -Paredes ´2, \"Deep \nLearning-based Natural Language Processing Methods Comparison for \nPresumptive Detection of Cyberbullying in Social Networks\", (IJACSA) \nInternational Journal of Advanced Computer Science and \nApplications,Vol. 12, No. 5, May, 2021. \n[6] Mohammed M.Abdelgwad, Taysir Hassan A Soliman, Ahmed I.Tabloba \nand Mohamed Fawzy Farghaly, “Arabic aspect based sentiment analysis \nusing BERT”, arXiv:2107.13290v2 [cs.CL] 28 September, 2021. \n[7] Marco Pota, Mirko Ventura, Rosario Catelli, Massimo Esposito, “An \nEffective BERT-BasedPipleline for Twitter Sentiment Analysis: A Case \nStudy in Italina”, Sensors, MDPI Article, December 2020. \n[8] M. A. H. Akhand, Arna Roy, Argha Chandra Dhar and Md Abdus \nSamad Kamal, \" Recent Progress, Emerging Techniques, and Future \nResearch Prospects of Bangla Machine Translation: A Systematic \nReview\", (IJACSA) International Journal of A dvanced Computer \nScience and Applications, Vol. 12, No. September, 2021. \n[9] Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen, “BERTweet: A \npre-trained lanaguage model for English Tweets”. Proceeding of the \n2020 EMNLP, pages 9-14. November 16-20, 2020. \n[10] Truong H. V Phan and Phuc Do,\"BERT+vnKG: Using Deep Learning \nand Knowledge Graph to Improve Vietnamese Question Answering \nSystem\", (IJACSA) International Journal of Advanced Computer \nScience and Applications, Vol. 11, No. 7, July, 2020. \n[11] Akbar Karimi, Leonardo Rossi,  Andrea Prati, “Improving BERT \nPerformance for Aspect -Based Sentiment Analysis”, Department of \nEngineering and Architecture, University of Parma. Italy. March, 2021. \n[12] Luca Bacco, Andrea Cimino, Felice Dell’Orletta, and Mario Merone, \n\"Explainable Sentiment A nalysis: A Hierarchical Transformer -Based \nExtractive Summarization Approach\", MDPI, Electronics 2021, 10, \n2195. https://doi.org/10.3390/electronics10182195. September, 2021. \n[13] Hu Xu, Lei Shu, Philip S. Yu, Bing Liu, “Understanding Pre -trained \nBERT for Aspect-based Sentiment Analysis”, Department of Computer \nScience, University of Illinois at Chicago, IL, USA. Proceedings of the \n28th International Conference on computational Linguistics, Barcelona, \nSpain, December, 2020. \n[14] Guangyao Pang, Keda Lu, Xiaoying Zhu, J ie He, Zhiyi Mo, Zizhen \nPeng, and Baoxing Pu, \"Aspect-Level Sentiment Analysis Approach via \nBERT and Aspect Feature Location Model\", Hindawi, Wireless \nCommunications and Mobile Computing, Volume 2021, Article ID \n5534615, 13 pages. https://doi.org/10.1155/2021/5534615. 2021. \n[15] Himanshu Batra, Narinder Singh Punn, Sanjay Kumar Sonbhadra, and \nSonali Agarwal, \"BERT -Based Sentiment Analysis: A Software \nEngineering Perspective\", Indian Institute of Information Technology \nAllahabad, India, arXiv:2106.02581v3 [cs.CV], Jun, 2021. \n[16] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, \nHassan Sajjad, Preslav Nakov, Deming Chen, Marianne Winslett, \n\"Compressing Large -Scale Transformer -Based Models: A Case Study \non BERT\", https://doi.org/10.1162/tacl a 00413.  Transactions of the \nAssociation for Computational Linguistics, vol. 9, pp. 1061 –1080, Jun, \n2021. \n[17] Mrityunjay Singh, Amit Kumar Jakhar, Shivam Pandey, “Sentiment \nanalysis on the impact of coronavirus in social life using BERT model”, \nSocial Network Analysis and M ining, Licence to Springer -Verlag \nGmBH Austria, part os Springer Nature. February, 2021. \n[18] M.P.Geetha and D.Karthika Renuka, “Improving the performance of \naspect based sentiment analysis using fine -tuned Bert base Uncased \nmodel”, International Journal of Int elligent Networks 2 (2021) 64 –69. \nJun, 2021. \n[19] Fatima-ezzahra Lagrari and Youssfi Elkettani, “Customized BERT with \nConvolution Model: A New Heuristic Enabled Encoder for Twitter \nSentiment Analysis”, (IJACSA) International Journal of Advanced \nComputer Science and Applications, Vol. 11, No. 10, October, 2020. \n[20] Majdi Baseiso and Saleh Alzahrani, “An Empirical Analysis of BERT \nEmbedding for Autometed Eassay Scoring”, (IJACSA) International \nJournal of Advanced Computer Science and Applications, Vol. 11, No. \n10, October, 2020. \n[21] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of tricks for \nefficient text classification”, Facebook AI Research arXiv:1607.01759v3 \n[cs.CL], August, 2016. \n[22] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov, \n“Enriching Word Vectorrs with Subword Information”, Facebook AI \nResearch, arXiv:1607.04606v2 [cs.CL] 19 Jun, 2017. \n[23] Karthik Gopalakrishnan and Fathi M. Salem, “Sentiment Analaysis \nUsing Simplified Long Short -term Memory Recurrent Neural \nNetworks”, Depatment of Electri cal and Computer Engineering, \nMichigan State University USA. arXiv:2005.03993v1[cs.CL]. May, \n2020. \n[24] Surbhi Arora, “SVM: Difference between Linear and Non -Linear \nModels”, 4 February, 2020. \n[25] Alexandre Kowalczyk,  “Support Vector Machine Succinctly”, \nSyncfusion Inc, https://www.syncfusion.com/succinctly-free-\nebooks/confirmation/support-vector-machines-succinctly October, 2017. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.911689281463623
    },
    {
      "name": "Sentiment analysis",
      "score": 0.7435287833213806
    },
    {
      "name": "Encoder",
      "score": 0.7101010680198669
    },
    {
      "name": "Transformer",
      "score": 0.6381410360336304
    },
    {
      "name": "Language model",
      "score": 0.6064841151237488
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5754780173301697
    },
    {
      "name": "Machine learning",
      "score": 0.4970541298389435
    },
    {
      "name": "Artificial neural network",
      "score": 0.46040594577789307
    },
    {
      "name": "Support vector machine",
      "score": 0.45535942912101746
    },
    {
      "name": "Dashboard",
      "score": 0.45532894134521484
    },
    {
      "name": "Recurrent neural network",
      "score": 0.455030232667923
    },
    {
      "name": "Context (archaeology)",
      "score": 0.45192453265190125
    },
    {
      "name": "Long short term memory",
      "score": 0.4330301284790039
    },
    {
      "name": "Social media",
      "score": 0.410493403673172
    },
    {
      "name": "Natural language processing",
      "score": 0.3747752904891968
    },
    {
      "name": "Data mining",
      "score": 0.34322500228881836
    },
    {
      "name": "Data science",
      "score": 0.22160392999649048
    },
    {
      "name": "World Wide Web",
      "score": 0.13162970542907715
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4076070",
      "name": "Bharathidasan University",
      "country": "IN"
    }
  ]
}