{
    "title": "Probing Simile Knowledge from Pre-trained Language Models",
    "url": "https://openalex.org/W4225106362",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100686045",
            "name": "Weijie Chen",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A5052417334",
            "name": "Yongzhu Chang",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5018408488",
            "name": "Rongsheng Zhang",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5023907515",
            "name": "Jiashu Pu",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5039428264",
            "name": "Guandan Chen",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5100350656",
            "name": "Le Zhang",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5013796186",
            "name": "Yadong Xi",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5101436487",
            "name": "Yijiang Chen",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A5115781256",
            "name": "Chang Su",
            "affiliations": [
                "Xiamen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2915774325",
        "https://openalex.org/W2951080837",
        "https://openalex.org/W2997188245",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W1969075342",
        "https://openalex.org/W1843033958",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4211084392",
        "https://openalex.org/W3154903254",
        "https://openalex.org/W3103252405",
        "https://openalex.org/W2941884113",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3156159991",
        "https://openalex.org/W3173441389",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W4297795751",
        "https://openalex.org/W4295531287",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W1563685000",
        "https://openalex.org/W4385574138",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2190970377",
        "https://openalex.org/W2604703085",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3031914912",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2002467630",
        "https://openalex.org/W3171213613",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2010474349",
        "https://openalex.org/W4221151670",
        "https://openalex.org/W112197792",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3003238303",
        "https://openalex.org/W2521924953",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3168921656",
        "https://openalex.org/W2767248689",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W3111699881",
        "https://openalex.org/W4238388933",
        "https://openalex.org/W2470742932",
        "https://openalex.org/W2509990731",
        "https://openalex.org/W2946725933",
        "https://openalex.org/W2115792525",
        "https://openalex.org/W3173841754"
    ],
    "abstract": "Weijie Chen, Yongzhu Chang, Rongsheng Zhang, Jiashu Pu, Guandan Chen, Le Zhang, Yadong Xi, Yijiang Chen, Chang Su. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5875 - 5887\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nProbing Simile Knowledge from Pre-trained Language Models\nWeijie Chen1∗, Yongzhu Chang2*, Rongsheng Zhang2*, Jiashu Pu2\nGuandan Chen2, Le Zhang2, Yadong Xi2, Yijiang Chen1 and Chang Su1†\n1 School of Informatics, Xiamen University, Xiamen, China\n2 Fuxi AI Lab, NetEase Inc., Hangzhou, China\nchenwj@stu.xmu.edu.cn, {changyongzhu,zhangrongsheng}@corp.netease.com\nAbstract\nSimile interpretation (SI) and simile genera-\ntion (SG) are challenging tasks for NLP be-\ncause models require adequate world knowl-\nedge to produce predictions. Previous works\nhave employed many hand-crafted resources\nto bring knowledge-related into models, which\nis time-consuming and labor-intensive. In\nrecent years, pre-trained language models\n(PLMs) based approaches have become the de-\nfacto standard in NLP since they learn generic\nknowledge from a large corpus. The knowl-\nedge embedded in PLMs may be useful for\nSI and SG tasks. Nevertheless, there are few\nworks to explore it. In this paper, we probe\nsimile knowledge from PLMs to solve the SI\nand SG tasks in the uniﬁed framework of sim-\nile triple completion for the ﬁrst time. The\nbackbone of our framework is to construct\nmasked sentences with manual patterns and\nthen predict the candidate words in the masked\nposition. In this framework, we adopt a sec-\nondary training process (Adjective-Noun mask\nTraining) with the masked language model\n(MLM) loss to enhance the prediction diver-\nsity of candidate words in the masked position.\nMoreover, pattern ensemble (PE) and pattern\nsearch (PS) are applied to improve the quality\nof predicted words. Finally, automatic and hu-\nman evaluations demonstrate the effectiveness\nof our framework in both SI and SG tasks.\n1 Introduction\nThe simile, which is a special type of metaphor,\nis deﬁned as a ﬁgurative expression in which two\nfundamentally different things are explicitly com-\npared, usually using “like” or “as” (Israel et al.,\n2004; Zeng et al., 2020). It is widely used in litera-\nture because it can inspire the reader’s imagination\n(Paul, 1970) by giving a vivid and unexpected anal-\nogy between two objects with similar attributes.\n∗Equal contribution. Work is done during Weijie’s intern-\nship at NetEase Inc..\n†Corresponding author. E-mail: suchang@xmu.edu.cn.\n(a) Simile Interpretation\n( love, ? , rose ) beautiful, thorny, wizen, …\nSimile Triple Completion (STC)\n(b) Simile Generation\n( love, beautiful, ? )        rose, swan, spring, …\nFigure 1: In the form of triple, the tasks of Simile In-\nterpretation and Simile Generation can be uniﬁed into\nSimile Triple Completion.\nA simile sentence usually contains three key ele-\nments: the tenor, the attribute and the vehicle,1\nwhich can be deﬁned in the form of a triple (tenor,\nattribute, vehicle) (Song et al., 2021). For example,\nthe simile sentence “Love is as thorny as rose” can\nbe extracted as the triple (love, thorny, rose), where\nthe tenor is “love”, the vehicle is “rose”, and the\nattribute is “thorny”. Note that a simile triple can\nproduce different simile sentences with different\ntemplates. For the example triple above, the simile\nsentences can be also constructed as “love is thorny\nlike rose\" with the pattern “tenor is attribute like\nvehicle\".\nThe study of simile is beneﬁt to many down-\nstream tasks, like sentiment analysis (Rentoumi\net al., 2012), question answering (Zheng et al.,\n2020), writing polishment (Zhang et al., 2021) and\ncreative writing (Gero and Chilton, 2019). Simile\ninterpretation (SI) (Qadir et al., 2016; Su et al.,\n2016) and simile generation(SG) (Yu and Wan,\n2019) are the two important tasks in the study of\nsimile (Tong et al., 2021). The SI task is to ﬁnd\nsuitable attributes as a mediator between the tenor\n1Tenor: the logical subject of the comparison, usually a\nnoun phrase. Attribute: what things being compared have in\ncommon, usually an adjective. Vehicle: the logical object of\nthe comparison, usually a noun phrase.\n5875\nand vehicle. Likewise, the SG task is to select a\nproper vehicle for the tenor with the given attri-\nbution. And these two tasks can be uniﬁed into\nthe form of simile triple completion(STC) (Song\net al., 2021) as shown in Figure 1.\nPrevious works on the SI and SG tasks relied on\na limited training corpus or labor-intensive knowl-\nedge base, which leads to an upper limit on the\ndiversity of results. (Song et al., 2021) collected\nsentences containing comparator words from a Chi-\nnese essays corpus and manually annotated them to\nobtain the simile triple. Some works (Stowe et al.,\n2021; Gero and Chilton, 2019; Veale et al., 2016)\nrelied on a knowledge base such as ConceptNet2,\nFrameNet3, which are scarce to other languages\nbecause it is time-consuming and labor-intensive\nto construct such a knowledge base. It is notable\nthat pre-trained language models (PLMs) (Devlin\net al., 2019; Radford et al., 2019) have made signif-\nicant progress recently in many NLP tasks since it\nlearns generic knowledge such as grammar, com-\nmon sense from a large corpus (Davison et al.,\n2019; Liu et al., 2021a,b). Considering the suf-\nﬁcient existence of simile in the large corpus, it’s\nreasonable to assume that PLMs are equipped with\nrich knowledge of similes during the pre-training\nstage. However, few works have explored directly\nprobing the knowledge of simile from the PLMs.\nIn this paper, we propose a uniﬁed framework to\nsolve the SI and SG tasks by mining the knowledge\nin PLMs, which does not require ﬁne-labeled train-\ning data or knowledge graphs. The backbone of\nour method is to construct masked sentences with\nmanual patterns from an incomplete simile triple,\nand then use language models with MLM heads\nto predict the masked words over the task-speciﬁc\nvocabulary. We take theKwords with the highest\nprobability as the result words. However, there are\nproblems with this crude approach. Firstly, the pre-\ndicted words should be creative and surprised for\nthe simile sentence. On the contrary, the PLMs tend\nto predict common words (e.g., good, bad) with a\nhigher probability. To address this issue, we in-\ntroduce a secondary pre-training stage - Adjective-\nNoun mask Training (ANT), where only the noun\nor adjective contained in the amod dependencies\n(Nivre et al., 2017) could be masked in the MLM\ntraining process and the number of words masked\ntimes are limited. Secondly, the words predicted\n2https://conceptnet.io/\n3https://framenet.icsi.berkeley.edu/fndrupal/\nby MLM have a preference for different patterns.\nFor this reason, we employ a pattern ensemble to\nobtain high-quality and robust results. Finally, we\nalso introduce a prompt-search method to improve\nthe quality of the simile component predictions.\nOur main contributions are as follows:\n• We propose a uniﬁed framework to solve both\nthe simile interpretation (SI) and simile gener-\nation (SG) tasks based on pre-trained models.\nTo the best of our knowledge, it is the ﬁrst\nwork to introduce pre-trained language mod-\nels to unify these tasks.\n• We propose a secondary pre-training stage\nthat effectively improves the prediction diver-\nsity. Further, we employ the pattern-ensemble\nand pattern-search approaches to obtain better\nresults.\n• We compare our models on both automated\nmetrics and manual measures, and the results\nshow that our approach outperforms the base-\nlines in terms of diversity and correctness.\n2 Related Work\n2.1 Simile Interpretation and Generation\nSimile interpretation and simile generation are the\ntwo main directions of the simile study (Yu and\nWan, 2019). The SI task (Shutova, 2010; Su et al.,\n2017) aims at ﬁnding a suitable attribute when\ngiven the tenor and vehicle, while the SG task\n(Yu and Wan, 2019) is to ﬁnd a proper vehicle\nwhen given the tenor and its attribute. For sim-\nile interpretation, some works (Zheng et al., 2020;\nBar et al., 2018; Xiao et al., 2016; Gagliano et al.,\n2016; Qadir et al., 2016) applied word vectors\nto decide which attribute words can ﬁt into the\ntenor and vehicle domains and some other works\n(Gero and Chilton, 2019; Stowe et al., 2021) intro-\nduced knowledge base (Baker et al., 1998; Speer\net al., 2017) to help ﬁnd intermediate attributes.\nFor simile generation, some works focused on\nconstructing limited training corpus to ﬁnetune a\nsequence-to-sequence model (Lewis et al., 2020)\nby pattern-based (Zhang et al., 2021; Bollegala and\nShutova, 2013) or knowledge-based approaches\n(Chakrabarty et al., 2020, 2021; Stowe et al., 2021).\nThere are also some works (Abe et al., 2006;\nHervás et al., 2007; Zheng et al., 2020) that focused\nmore on the relationships between concepts (i.e.,\n5876\nBookCorpus\nIncomplete\ntriple\nOptimal multi-pattern \ncombination\nPattern Search\nMasked LM\nLM_ANT\nAdjective-Noun \nMask Training\nTraining Dataset\n…Beautiful\nRomantic\n…\n…\nTask V ocab\nT V\nA The T is as A as V .\n...\n4 Classes Multi-Patterns\nT AV A T V\n... ... ...\nPattern \nEnsemble\nFigure 2: The uniﬁed framework for STC. The incomplete triple will be transfer to masked sentences by a multi-\npattern combination, which is searched from four classes well-designed patterns. And LM_ANT (obtained by\nusing unlabeled corpus to perform Adjective-Noun Mask Training) predicts the missing simile element over the\ntask-speciﬁc vocabulary based on the masked sentences.\ntenor and vehicle) and attribute. However, our pa-\nper carries out the task of simile interpretation and\ngeneration uniformly in the form of simile triples.\nAnd instead of extracting the simile triples from the\nlimited corpus using designed templates or a hand-\ncrafted knowledge base, we probe simile-related\nknowledge from PLMs.\n2.2 Explore knowledge from PLMs\nPre-trained language models such as Bert and\nGPT (Devlin et al., 2019; Radford et al., 2019)\nare trained on the large-scale unlabeled corpus.\nMany recent works (Manning et al., 2020; Ettinger,\n2020; Petroni et al., 2019; Shin et al., 2020; Ha-\nviv et al., 2021; Jiang et al., 2020; Zhong et al.,\n2021; Wang et al., 2022a,b; Li and Liang, 2021)\nfocused on exploring the rich knowledge embed-\nded in these PLMs. Manning et al. (2020) and\nEttinger (2020) learned the syntactic and semantic\nknowledge from PLMs. Among these works, one\nbranch of works(Petroni et al., 2019; Shin et al.,\n2020; Haviv et al., 2021; Jiang et al., 2020) de-\nsigned discrete patterns to explore the common\nsense and world knowledge embedded in PLMs. In\naddition, some works (Zhong et al., 2021; Li and\nLiang, 2021) probed knowledge by searching the\nbest-performing continuous patterns in the space\nof embedding. Inspired by the above works, in\nthis paper, we probe the knowledge of simile in\nthese pre-trained models and further apply pattern\nensemble and pattern search to improve the results.\n3 Backbone\n3.1 Simile Triple Completion\nAs shown in Figure 1, the simile triple complete\nconsists of two tasks: simile interpretation (SI)\nand simile generation (SG). Each simile sentence\ncan be abstracted into the form of a triple. There-\nfore, we deﬁne a triple: (T,A,V), where T, V\nare mainly nouns or noun phrases and represent\nthe tenor and vehicle in the simile sentence, re-\nspectively. Ais the attribute in simile sentences,\nwhich is an adjective. If the Ais None in the triple,\ni.e. (T,None, V), we deﬁne it as the simile in-\nterpretation task. Similarly, if the Vis None, i.e.\n(T,A,None), this will be the task of simile gener-\nation.\n3.2 Masked Language Model\nThe masked language model (MLM) (Devlin et al.,\n2019; Taylor, 1953) randomly masks the words in\nthe input sentence and feeds the masked sentence\ninto the pre-trained models to make predictions\nby other visible words. For example, given a sen-\ntence s= [w1,w2,...,w i,...,w m], where the wi\nmeans the i-th word in the sentence. We can ran-\ndomly mask sand feed the masked sequence ˜sinto\nthe PLMs e.g. BERT (Devlin et al., 2019) to obtain\nthe masked words by Equation:\n˜s= fmask(s,i,v ) (1)\nP = fθ(˜s) (2)\n5877\nwhere the v means the V ocabulary for pre-\ntrained models, and the idenotes the position of\nthe masked word in Equation 1. The θ is the pa-\nrameters of PLMs in Equation 2. We can select the\nword corresponding to the maximum probability in\nP as the output of the model.\n3.3 Probe Simile Knowledge with MLM\nTo probe the simile knowledge in pre-trained\nmasked language models, the intuitive solution is:\n(1) Construct a sentence that contains the simile\ntriple in Section 3.1 with the given pattern. (2)\nMask the attribute Aor vehicle Vin this simile sen-\ntence. (3) Predict the words in the masked position\nwith MLM. For example, when given a pattern The\nT is as Aas V, the input sentence of MLM is The\nT is as [MASK] as Vfor the SI task while The T\nis as Vas [MASK] for the SG task.\nTo formulate this problem, we deﬁne the pattern\nfunction as p(τ), where τ ∈{SG,SI }. The pre-\ntrained MLM is denoted as Mand the predicted\ndistribution Q over vocabulary V can be formu-\nlated as:\nQ(w|p(τ)) = exp(M(w|p(τ)))∑\nw′∈V exp(M(w′|p(τ))) (3)\n4 Method\nIn this section, we will introduce our proposed\nmethod of probing simile knowledge from pre-\ntrained models. Our method ﬁrst introduces a sec-\nondary pre-training stage - Adjective-Noun mask\nTraining (ANT) based on pre-trained language\nmodels to acquire diverse lexical-speciﬁc words.\nThen two modules of pattern ensemble and pattern\nsearch are used to obtain the high-quality predic-\ntions. The framework of our method is shown in\nFigure 2 in detail4.\n4.1 Adjective-Noun Mask Training (ANT)\nFor the MLM task, pre-trained models prefer to\noutput high-frequency words as candidate words\nsince the objective of the training is to minimize\nthe cross-entropy loss (Gehrmann et al., 2019).\nHowever, the components of simile triples are usu-\nally nouns or adjectives and the simile sentences\nare appealing due to their creativity and unexpect-\nedness. Therefore, to predict more diverse and\n4we released our code at https://github.com/nairoj/Probing-\nSimile-from-PLM.\nspeciﬁc words of simile component, we introduce\na secondary pre-training stage - Adjective-Noun\nmask Training (ANT) that ﬁne-tune the pre-trained\nmodel with specially designed datasets. First, we\nutilize trankit (Nguyen et al., 2021) to construct\nthe training set by selecting sentences from Book-\nCorpus (Zhu et al., 2015) that contains amod5 de-\npendencies (Nivre et al., 2017). Second, we mask\na word at the end of amod relation, instead of ran-\ndomly masking, and all words are masked no more\nthan 5 times. Finally, the pre-trained model is ﬁne-\ntuned on the constructed dataset with MLM loss.\nIn this way, the pre-trained model will avoid the\nbias to high-frequency words and have a higher\nprobability of generating diverse and novel words.\n4.2 Pattern Ensemble (PE)\nSince words predicted by MLM have a preference\nfor different patterns and only using one pattern\nis insufﬁcient, we apply the pattern ensemble to\nobtain better performance where different types of\npatterns are designed as shown in Table 1. Speciﬁ-\ncally, the class I describes the relationship between\nthe three-element T, Vand A. However, the simi-\nles tend to highlight an obvious attribute between\ntenor and vehicle (Israel et al., 2004). We further\ndesign the class II and class III to ﬁnd the attribute\ncorresponding to the tenor and vehicle, respectively.\nFinally, the attributes of simile sentences are some-\ntimes omitted and thus the class IV is designed to\ndeal with this case. Additionally, we also design\nthree patterns for each class to obtain high-quality\nand robust results.\nThe output distributionQPE of pattern ensemble\ncan be formulated as\nQPE(w|P) = 1\n|P|\n∑\np(τ)∈P\nlog(Q(w|p(τ))) (4)\nwhere P is the set of patterns p(τ) for speciﬁc\ntask τ. Note that though we design four classes\nof patterns in Table 1, some classes of patterns are\nnot required for the SI or SG task. Speciﬁcally,\nThe patterns of Class IV are not used for the SI\ntask because the attribute Ais missed in Class IV .\nLikewise, the patterns of Class III are not used for\nthe SG task due to the lack of vehicle V.\n5An adjectival modiﬁer of a noun (or pronoun) is any ad-\njectival phrase that serves to modify the noun (or pronoun).\nThe relation applies whether the meaning of the noun is modi-\nﬁed in a compositional way (e.g., large house) or an idiomatic\nway (hot dogs).\n5878\nClass Relationship Pattern\nI\nT V\nA\nThe {tenor} is as {attribute} as {vehicle}. p1\n{vehicle} is very {attribute}, so as {tenor}. p2\n{tenor} is like {vehicle}, because they are both {attribute}. p3\nII\nV A\nThe {attribute} {vehicle}. p4\n{vehicle} is very {attribute}. p5\n{vehicle} is {attribute}. p6\nIII\nT A\nThe {attribute} {tenor}. p7\n{tenor} is very {attribute}. p8\n{tenor} is {attribute}. p9\nIV\nT V\n{tenor} is similar to {vehicle}. p10\n{tenor} is like {vehicle}. p11\n{tenor} and {vehicle} are alike. p12\nTable 1: All patterns and corresponding classes. Class I models the relationship between three elements, and other\nclasses model relationships between two elements. Every pattern is denoted as the right side symbol pi.\n4.3 Pattern Search (PS)\nThe prediction of pattern ensemble in Section 4.2\nis averaged by adding up the output distributions\nof all the patterns. Conversely, the hand-designed\npatterns are heuristic, which may lead to subopti-\nmal results. Therefore, it is worth studying how\nthese patterns can be combined to obtain better per-\nformance. To solve this problem, we introduce an\napproach of pattern search (PS) to ﬁnd the best com-\nbination of different patterns. Speciﬁcally, given\na simile dataset DPS, we calculate Equation 4 on\nDPS by iterating all subsets of the patterns. Finally,\nwe select the optimal subset pbest as the input of\nMLM to predict simile components.\n5 Experiments\n5.1 Dataset\nDataset for ANT:We constructed our train set of\nANT from BookCorpus. We ﬁrst extracted the sen-\ntences with length less than 64 and then masked\nnouns or adjectives in them based on amod depen-\ndencies (Nivre et al., 2017). Meanwhile, we limited\nthe frequency of masked words to less than 5. Fi-\nnally, we got 98k sentences as the dataset of ANT,\nwhich contains 68k noun-masked sentences and\n30k adjective-masked sentences.\nDataset for PE and PS: We evaluate our\nmethod on the dataset proposed in (Roncero and\nde Almeida, 2015). As the samples in Table 2,\nthere are multiple attributes for each ( T, V) pair.\nFor example, the pair of (anger, ﬁre) has the at-\ntributes of dangerous, hot, and red. In addition, we\nfollowed the previous work (Xiao et al., 2016) to\nﬁlter the dataset by reversing simile triples with\nattribute frequencies greater than 4. Eventually, we\nobtain the train set with 533 samples and the test\nset with 145 samples. Notice that the train set is the\nTriple Frequency\n(Anger, Dangerous, Fire) 8\n(Anger, Hot, Fire) 8\n(Anger, Red, Fire) 5\n(Love, Beautiful, Rainbow) 10\n(Love, Beautiful, Melody) 2\n(Love, Beautiful, Rose) 9\nTable 2: Some samples from the dataset. Frequency\nrepresents the number of annotators who consider the\nattribute is suitable for the Tenor-Vehicle pair.\nDPS in Section 4.3 used for the pattern search and\nthe test set is used for evaluating all the approaches\nin this paper.\n5.2 Implementation Details\nDetails for ANT: In adjective-nouns mask training,\nwe utilized Adam as our optimizer and the learning\nrate is 5e-5. The batch size is set to 32 and the max\nsequence length is set to 64, respectively. Further,\nwe utilize the Bert-Large6 with 340M parameters\nas the basic model to perform adjective-nouns mask\ntraining and the number of training epoch is 3.\nVehicle Filtering: For simile generation, we ﬁl-\nter the predicted vehicles that are similar to the\ntensor by calculating the semantic similarity with\nGlove embedding. For instance, given the sentence\n“The child is as tall as [MASK]\", we will ﬁlter out\nthe word “father\" as its vehicle due to not meeting\nthe simile deﬁnition7. To solve this problem, we\ncompute the similarity score of the tenor and vehi-\ncle and ﬁlter the predicted vehicle whose score is\nabove the threshold 0.488.\n6https://huggingface.co/Bert-large-uncased\n7Using something with similar characteristics to give an\nanalogy to another thing\n8The threshold is the maximum similarity score of tenor\nand vehicle in the train set\n5879\n72.4\n67.1\n48.5 46.1\n35.4\n25.0\n1.3\n21.1\n29.0\n9.2 5.3 1.3\ngood big strong old real great\n0\n20\n40\n60\n80\n100\nPercentage (%)\nBert\nANT\nTop 15\n84.2\n76.3\n69.7 67.1\n55.3\n42.1\n5.3\n29.0 32.9\n17.1\n11.8\n1.3\ngood big strong old real great\n0\n20\n40\n60\n80\n100\nPercentage (%)\nBert\nANT\nTop 25\nFigure 3: Percentage of samples whose top K predicted words contain a given common word. The horizontal\ncoordinates are some common adjectives.\n5.3 Evaluating the effectiveness of ANT\nIn this section, we will demonstrate that ANT could\nimprove the diversity of predicted words for both\nthe SI and SG tasks. We compare the predicted\nresults of MLM (i.e., Bert) before and after ANT,\nwhich use the patterns “The Tis as [MASK] as V\"\nfor the SI task and “The T is as Aas [MASK]\" for\nthe SG task.\nMetric: We evaluate the diversity of the MLM\npredictions by calculating the proportion of unique\nwords in the predicted TopKresults on the test set.\nIt can be formulated as\np@K = Num(Unique_words)\nK∗N (5)\nwhere the Num(Unique_words) means the\nnumber of unique words, and the N represents size\nof the test set.\nResult: To illustrate the effectiveness of ANT,\nWe evaluate the results on the test set based on\nEquation 5. As shown in Table 3, the diversity of\npredicted words signiﬁcantly improves after ANT\nfor different p@K, speciﬁcally about 100% im-\nprovement for the SI task and about 50% for the SG\ntask. Additionally, Figure 3 plots the percentage\nof samples on the test set, where a given common\nword (e.g., good, big, strong) appears in the list\nof the top k = 15,25 predicted words. We can\nobserve that the frequency of common words de-\ncreases signiﬁcantly after ANT. For example, the\nfrequency of the common word good decreases\nfrom 72.37% to 1.32% when k= 15.\nMethod p@5 p@10 p@15 p@25\nBert 0.263 0.216 0.189 0.163SI ANT 0.492 0.412 0.382 0.312\nBert 0.232 0.201 0.182 0.158SG ANT 0.370 0.299 0.256 0.216\nTable 3: The results of diversity on both the SI and SG\ntasks. The method Bert and ANT separately represent\nthe results before and after the Adjective-Noun mask\ntraining.\n5.4 Evaluating the effectiveness of PE and PS\n5.4.1 Baselines\nWe compare the proposed approaches with the fol-\nlowing baseline:\n(1) Meta4meaning (Xiao et al., 2016): It uses\nthe trained LSA vector representation according to\nthe degree of abstraction and salience imbalance\nto select appropriate attributes. (2) GEM (Bar\net al., 2018): A method calculates the cosine simi-\nlarity and normalized PMI between each attribute\nand tensor/vehicle based on Glove representing to\nobtain the best attribute with ranking. (3) Bert (De-\nvlin et al., 2019): Directly use pre-trained MLM\nto predict the simile component with a single pat-\ntern as Section 3.3. In this paper, we utilize the\nbert-large-uncased as the basic pre-trained MLM.\n(4) ConScore (Zheng et al., 2020): A connecting\nscore is proposed to select an attribute word Afor\nT and V.\nOur proposed approaches are denoted as:\n(1) ANT: Perform Adjective-Noun mask Train-\ning based on a pre-trained MLM with the datasets\nmentioned in Section 5.1. (2) ANT+PE: Based on\nANT, the output distribution over vocabulary is pre-\n5880\nTask Method MRR R@5 R@10 R@15 R@25 R@50\nMeta4meaning N/A 0.221 0.303 0.339 0.397 0.454\nGEM 0.312 0.198 0.254 0.278 0.405 0.562\nConScore 0.078 0.076 0.138 0.172 0.269 0.386\nBert 0.266 0.338 0.428 0.448 0.538 0.641\nANT 0.245 0.310 0.407 0.455 0.510 0.614\nANT+PE 0.241 0.331 0.400 0.448 0.552 0.628\nSI\nANT+PS+PE 0.270 0.379 0.490 0.524 0.579 0.655\nConScore 0.036 0.055 0.09 0.103 0.145 0.200\nBert 0.064 0.076 0.124 0.159 0.207 0.283\nANT 0.049 0.069 0.117 0.145 0.186 0.303\nANT+PE 0.036 0.034 0.083 0.097 0.131 0.172\nSG\nANT+PS+PE 0.095 0.124 0.145 0.159 0.214 0.290\nTable 4: Automatic evaluation for SI and SG tasks. The best results are in bold, and the second best results are\nunderlined.\nTask Method Top5 Top10 Top15\nSI\nConScore 0.192 † 0.169† 0.172†\nBert 0.411 † 0.364† 0.326†\nANT 0.471 0.396 † 0.365†\nANT+PE 0.494 0.469 0.456\nANT+PS+PE 0.496 0.433† 0.398†\nSG\nConScore 0.780 † 0.690† 0.673†\nBert 0.597 † 0.667† 0.629†\nANT 0.867 † 0.868† 0.808†\nANT+PE 0.887 † 0.805† 0.751†\nANT+PS+PE 1.123 1.052 0.973\nTable 5: The average score of human evaluation for\nSTC. The best results are in bold, and the second best\nresults are underlined. †denotes signiﬁcant difference\nwith the best result (t-test, p-value<0.05).\ndicted by average on all the corresponding patterns\nin Table 1. (3) ANT+PS+PE: Based on ANT, ﬁrst\nthe pattern search is to decide which patterns in\nTable 1 are applied, and then the pattern ensemble\nis used over these selected patterns.\n5.4.2 Metrics\nWe use both automatic evaluation and human eval-\nuation to compare our approaches with baselines.\nAutomatic Evaluation:\n(1) Mean Reciprocal Rank (MRR): average on\nthe reciprocal of the ranking ri of label words in\nthe predicted candidates, denoted as\nMRR = 1\nN\nN∑\ni=1\n1\nri\n(6)\n(2) R@K: the percentage of the label words ap-\npear in the top Kpredictions. Note that, following\nprevious works (Xiao et al., 2016; Bar et al., 2018),\nwe consider a predicted word as the correct an-\nswer if it is a synonym of label word n in WordNet\n(Miller, 1992). It can be formulated as\ncor(w) =\n{\n1 w∈Synonyms(Li)\n0 w /∈Synonyms(Li) (7)\nR@K = 1\nN\nN∑\ni=1\n∑\nw∈Ki cor(w)\nK (8)\nwhere Ki denotes the list of predicted words, Li\ndenotes the list of label words and Synonyms(Li)\nrepresents the synonyms of a word.\nHuman Evaluation: To further prove our ap-\nproaches are better than baselines, human evalu-\nation is used to evaluate the quality of predicted\nsimile triples from three levels (0, 1, 2). 0 - The\ntriple is unacceptable. 1 - The triple is acceptable.\n2 - The triple is acceptable and creative. Given a\nsimile triple, annotators need to score it according\nto their subjective judgment and each triple is an-\nnotated by three annotators independently. We use\nthe average score of three annotators as the quality\nof a simile triple.\n5.4.3 Results\nAutomatic and Human Evaluation:The results\nof both automatic and human evaluation are shown\nin Table 4 and Table 5. The agreement between\nannotators is measured using Fleiss’s kappaκ(Ran-\ndolph, 2005). The κvalue is 0.68 (substantial agree-\nment) for the SI task and 0.48 (moderate agree-\nment) for the SG task.\nFrom the results, we can conclude\n5881\nTask Subset of Patterns MRR R@5 R@10 R@15 R@25\nSI\n{p1,p5} 0.100 0.126 0.184 0.233 0.281\n{p1,p2,p3,p4,p5,p9} 0.095 0.107 0.171 0.203 0.268\n{p1,p2,p3,p4,p5,p7,p8} 0.095 0.099 0.163 0.206 0.274\n{p1,p4,p5} 0.094 0.094 0.163 0.203 0.261\nSG\n{p3,p4} 0.056 0.068 0.105 0.135 0.159\n{p1,p4} 0.056 0.071 0.092 0.120 0.154\n{p1,p3,p4} 0.052 0.06 0.105 0.128 0.163\n{p1,p2,p4} 0.052 0.058 0.096 0.116 0.137\nTable 6: The top 4 best performing pattern subsets for SI and SG tasks (see Table 1 for which class the pattern pi\nbelongs to). The best results are in bold. More results of pattern search are shown in the Appendix A\n.\n(1) For both SI and SG tasks, our proposed ap-\nproaches (i.e., ANT, ANT+PE, ANT+PS+PE)\nsigniﬁcantly outperform the baselines on both\nautomatic and human evaluations. It proves\nthat our methods not only enhance the diver-\nsity of predicted simile components in Section\n5.3 but also their quality.\n(2) Pre-trained MLM-based methods (i.e., Bert,\nANT, ANT+PE and ANT+PS+PE) perform\nbetter than the traditional methods (i.e., GEM,\nMeta4meaning, ConScore). It shows the po-\ntential of pre-trained models in probing simile\nknowledge.\n(3) Compared ANT with Bert, we found that\nthough ANT improves the diversity of pre-\ndicted words in Table 3, the average scores\non automatic and human evaluations decrease\nbecause the simile knowledge is not involved\nin the ANT training process. However, our\nproposed PE and PS compensate for the per-\nformance.\n(4) The scores of automatic evaluation metrics on\nthe SI task are remarkably higher than the SG\ntask. Yet, the scores of human evaluation met-\nrics are signiﬁcantly lower than on the SG task.\nWe conjecture that this may be because the\nlist of candidate words of attribute predicted\nby SI are smaller than that of the vehicle for\nthe SG task. For example, given the SI sample\n“(Cloud, None, Cotton)”, the attribute words\nare almost restricted to the physical proper-\nties of the vehicle, such as “Soft”, while the\nchoices of vehicle words are more varied and\nunexpected given the SG sample “(Cloud, soft,\nNone)” such as “cotton, silk, towel\".\nDiscussion for PS:Compared ANT+PS+PE to\nANT+PE, it can be included that pattern search\nbrings a great improvement to the results on both\nautomatic and human evaluations. To have a deeper\ninsight into PS, the pattern subsets with high per-\nformance are listed in Table 6. For the SI task,\nthe optimal multi-pattern combination is {p1,p5},\nwhich support the hypothesis proposed by (Ortony,\n1979) considers that the highlighted attribute of a\nsimile triple is more salient in the vehicle domain\ndespite it is commonly shared by both tenor and\nvehicle domains. Speciﬁcally, pattern p1 belonging\nto the Class I, models the relationship of all three\nsimile components while the pattern p5 belonging\nto Class II requires the candidate words to be the\nsalient attribute of the vehicle. Similarly, for SG\ntask, optimal multi-pattern combination is{p3,p4},\nwhich is also a combination of the Class I pattern\nand the Class II pattern.\n6 Conclusion and Future work\nIn this paper, from the perspective of simile triple\ncompletion, we propose a uniﬁed framework to\nsolve the SI and SG tasks by probing the knowl-\nedge of the pre-trained masked language model.\nThe backbone of our method is to construct masked\nsentences with manual patterns from an incomplete\nsimile triple, and then use language models with\nMLM heads to predict the masked words. More-\nover, a secondary pre-training stage (the adjective-\nnoun mask training) is applied to improve the di-\nversity of predicted words. Pattern ensemble (PE)\nand pattern search (PS) are further used to improve\nthe quality of predicted words. Finally, automatic\nand human evaluations demonstrate the effective-\nness of our framework in both SI and SG tasks.\nIn future work, we will continue to study how\n5882\nto mine broader or complex knowledge from pre-\ntrained models, such as metaphor, common sense\nand we expect more researchers to perform related\nresearch.\nAcknowledgements\nThis work is supported by the Key Research and\nDevelopment Program of Zhejiang Province (No.\n2022C01011) and National Natural Science Foun-\ndation of China (Project 61075058). We would\nlike to thank the anonymous reviewers for their\nexcellent feedback.\nReferences\nKeiga Abe, Kayo Sakamoto, and Masanori Nakagawa.\n2006. A computational model of the metaphor gen-\neration process. In Proceedings of the Annual Meet-\ning of the Cognitive Science Society, volume 28.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In COLING\n1998 Volume 1: The 17th International Conference\non Computational Linguistics.\nKﬁr Bar, Nachum Dershowitz, and Lena Dankin. 2018.\nMetaphor interpretation using word embeddings. In-\nternational Journal of Computational Linguistics\nand Applications.\nDanushka Bollegala and Ekaterina Shutova. 2013.\nMetaphor interpretation using paraphrases extracted\nfrom the web. PloS one, 8(9):e74304.\nTuhin Chakrabarty, Smaranda Muresan, and Nanyun\nPeng. 2020. Generating similes effortlessly like a\npro: A style transfer approach for simile generation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6455–6469, Online. Association for Computa-\ntional Linguistics.\nTuhin Chakrabarty, Xurui Zhang, Smaranda Muresan,\nand Nanyun Peng. 2021. MERMAID: Metaphor\ngeneration with symbolism and discriminative de-\ncoding. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4250–4261, Online. Association for\nComputational Linguistics.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nAndrea Gagliano, Emily Paul, Kyle Booten, and\nMarti A. Hearst. 2016. Intersecting word vectors to\ntake ﬁgurative language to new heights. In Proceed-\nings of the Fifth Workshop on Computational Lin-\nguistics for Literature, pages 20–31, San Diego, Cal-\nifornia, USA. Association for Computational Lin-\nguistics.\nSebastian Gehrmann, Hendrik Strobelt, and Alexander\nRush. 2019. GLTR: Statistical detection and visu-\nalization of generated text. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n111–116, Florence, Italy. Association for Computa-\ntional Linguistics.\nKaty Ilonka Gero and Lydia B. Chilton. 2019.\nMetaphoria: An algorithmic companion for\nmetaphor creation. In Proceedings of the 2019\nCHI Conference on Human Factors in Computing\nSystems, CHI 2019, Glasgow, Scotland, UK, May\n04-09, 2019, page 296. ACM.\nAdi Haviv, Jonathan Berant, and Amir Globerson.\n2021. BERTese: Learning to speak to BERT. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3618–3623, Online.\nAssociation for Computational Linguistics.\nRaquel Hervás, Rui Ponte Costa, Hugo Costa, Pablo\nGervás, and Francisco C. Pereira. 2007. Enrichment\nof automatically generated texts using metaphor. In\nMICAI 2007: Advances in Artiﬁcial Intelligence, 6th\nMexican International Conference on Artiﬁcial In-\ntelligence, Aguascalientes, Mexico, November 4-10,\n2007, Proceedings, volume 4827 ofLecture Notes in\nComputer Science, pages 944–954. Springer.\nMichael Israel, Jennifer Riddle Harding, and Vera To-\nbin. 2004. On simile. Language, culture, and mind,\n100.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\n5883\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nArXiv preprint, abs/2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. ArXiv preprint, abs/2103.10385.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proc. Natl. Acad. Sci.\nUSA, 117(48):30046–30054.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New\nYork, February 23-26, 1992.\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran\nBen Veyseh, and Thien Huu Nguyen. 2021. Trankit:\nA light-weight transformer-based toolkit for multi-\nlingual natural language processing. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 80–90, Online. Associa-\ntion for Computational Linguistics.\nJoakim Nivre, Daniel Zeman, Filip Ginter, and Francis\nTyers. 2017. Universal Dependencies. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nTutorial Abstracts, Valencia, Spain. Association for\nComputational Linguistics.\nAndrew Ortony. 1979. Beyond literal similarity. Psy-\nchological review, 86(3):161.\nAnthony M. Paul. 1970. Figurative language. Philoso-\nphy & Rhetoric, 3(4):225–248.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAshequl Qadir, Ellen Riloff, and Marilyn A. Walker.\n2016. Automatically inferring implicit properties in\nsimiles. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1223–1232, San Diego, California.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nJustus J Randolph. 2005. Free-marginal multirater\nkappa (multirater k [free]): An alternative to ﬂeiss’\nﬁxed-marginal multirater kappa. Online submission.\nVassiliki Rentoumi, George A. V ouros, Vangelis\nKarkaletsis, and Amalia Moser. 2012. Investigat-\ning metaphorical language in sentiment analysis: A\nsense-to-sentiment perspective. ACM Trans. Speech\nLang. Process., 9(3):6:1–6:31.\nCarlos Roncero and Roberto G de Almeida. 2015.\nSemantic properties, aptness, familiarity, conven-\ntionality, and interpretive diversity scores for 84\nmetaphors and similes. Behavior research methods,\n47(3):800–812.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nEkaterina Shutova. 2010. Automatic metaphor inter-\npretation as a paraphrasing task. In Human Lan-\nguage Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics , pages 1029–1037,\nLos Angeles, California. Association for Computa-\ntional Linguistics.\nWei Song, Jingjin Guo, Ruiji Fu, Ting Liu, and Lizhen\nLiu. 2021. A knowledge graph embedding approach\nfor metaphor processing. IEEE ACM Trans. Audio\nSpeech Lang. Process., 29:406–420.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, Febru-\nary 4-9, 2017, San Francisco, California, USA ,\npages 4444–4451. AAAI Press.\nKevin Stowe, Tuhin Chakrabarty, Nanyun Peng,\nSmaranda Muresan, and Iryna Gurevych. 2021.\nMetaphor generation with conceptual mappings. In\n5884\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n6724–6736, Online. Association for Computational\nLinguistics.\nChang Su, Shuman Huang, and Yijiang Chen. 2017.\nAutomatic detection and interpretation of nominal\nmetaphor based on the theory of meaning. Neuro-\ncomputing, 219:300–311.\nChang Su, Jia Tian, and Yijiang Chen. 2016. Latent\nsemantic similarity based interpretation of chinese\nmetaphors. Eng. Appl. Artif. Intell., 48:188–203.\nWilson L. Taylor. 1953. “cloze procedure”: A new tool\nfor measuring readability. Journalism & Mass Com-\nmunication Quarterly, 30:415 – 433.\nXiaoyu Tong, Ekaterina Shutova, and Martha Lewis.\n2021. Recent advances in neural metaphor process-\ning: A linguistic, cognitive and social perspective.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 4673–4686, Online. Association for Compu-\ntational Linguistics.\nTony Veale, Ekaterina Shutova, and Beata Beigman\nKlebanov. 2016. Metaphor: A Computational Per-\nspective. Synthesis Lectures on Human Language\nTechnologies. Morgan & Claypool Publishers.\nJiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng,\nZhixu Li, Jianfeng Qu, and Jie Zhou. 2022a. Clid-\nsum: A benchmark dataset for cross-lingual dia-\nlogue summarization. ArXiv, abs/2202.05599.\nJiaan Wang, Beiqi Zou, Zhixu Li, Jianfeng Qu, Peng-\npeng Zhao, An Liu, and Lei Zhao. 2022b. Incor-\nporating commonsense knowledge into story end-\ning generation via heterogeneous graph networks.\nArXiv, abs/2201.12538.\nPing Xiao, Khalid Al-Najjar, Mark Granroth-Wilding,\nKathleen Agres, and Hannu Toivonen. 2016.\nMeta4meaning: Automatic metaphor interpretation\nusing corpus-derived word associations. In Pro-\nceedings of the Seventh International Conference\non Computational Creativity, ICCC 2016, UPMC,\nParis, France, June 27 - July 1, 2016 , pages 230–\n237. Sony CSL Paris, France.\nZhiwei Yu and Xiaojun Wan. 2019. How to avoid sen-\ntences spelling boring? towards a neural approach\nto unsupervised metaphor generation. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 861–871, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nJiali Zeng, Linfeng Song, Jinsong Su, Jun Xie, Wei\nSong, and Jiebo Luo. 2020. Neural simile recog-\nnition with cyclic multitask learning and local at-\ntention. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 9515–9522. AAAI Press.\nJiayi Zhang, Zhi Cui, Xiaoqiang Xia, Yalong Guo, Yan-\nran Li, Chen Wei, and Jianwei Cui. 2021. Writing\npolishment with simile: Task, dataset and A neural\napproach. In Thirty-Fifth AAAI Conference on Arti-\nﬁcial Intelligence, AAAI 2021, Thirty-Third Confer-\nence on Innovative Applications of Artiﬁcial Intelli-\ngence, IAAI 2021, The Eleventh Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2021, Virtual Event, February 2-9, 2021 , pages\n14383–14392. AAAI Press.\nDanning Zheng, Ruihua Song, Tianran Hu, Hao Fu,\nand Jin Zhou. 2020. \"love is as complex as\nmath\": Metaphor generation system for social chat-\nbot. ArXiv preprint, abs/2001.00733.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033, Online. Association for\nComputational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\nA More results of Pattern Search\nThe more results of Pattern Search are shown in\nTable 7.\nB More Prediction\nSome results are shown in Table 8 and Table 9.\n5885\nTask Subset of Patterns MRR R@5 R@10 R@15 R@25 R@50\n{p1,p5} 0.100 0.126 0.184 0.233 0.281 0.375\n{p1,p2,p3,p4,p5,p9} 0.095 0.107 0.171 0.203 0.268 0.377\n{p1,p2,p3,p4,p5,p7,p8} 0.095 0.099 0.163 0.206 0.274 0.373\n{p1,p4,p5} 0.094 0.094 0.163 0.203 0.261 0.366\n{p1,p2,p4,p5,p9} 0.094 0.111 0.165 0.214 0.265 0.373\n{p1,p4,p5,p6,p8} 0.093 0.109 0.171 0.212 0.283 0.368\n{p1,p4,p5,p6,p7,p8,p9} 0.093 0.090 0.152 0.205 0.263 0.338\n{p1,p2,p4,p5} 0.093 0.113 0.167 0.210 0.280 0.371\n{p1,p2,p4,p6,p8} 0.093 0.111 0.178 0.218 0.272 0.371\n{p1,p2,p4,p5,p6,p8} 0.093 0.105 0.173 0.216 0.283 0.370\n{p1,p2,p4,p5,p6,p7,p8,p9} 0.093 0.096 0.156 0.210 0.261 0.347\n{p1,p3,p4,p5,p8,p9} 0.093 0.098 0.159 0.223 0.265 0.368\n{p1,p5,p6,p7} 0.092 0.101 0.163 0.203 0.274 0.366\n{p1,p2,p5,p9} 0.092 0.099 0.171 0.225 0.285 0.362\n{p1,p2,p4,p5,p8} 0.092 0.105 0.173 0.218 0.280 0.360\n{p1,p2,p4,p5,p8,p9} 0.092 0.099 0.158 0.216 0.274 0.360\n{p1,p2,p4,p5,p6,p8,p9} 0.092 0.094 0.159 0.210 0.270 0.355\n{p1,p3,p5} 0.092 0.105 0.169 0.216 0.280 0.381\n{p1,p3,p4,p5,p8} 0.092 0.096 0.173 0.220 0.276 0.368\n{p1,p2,p3,p5,p9} 0.092 0.107 0.165 0.208 0.281 0.371\n{p1,p5,p6} 0.091 0.116 0.180 0.220 0.283 0.385\n{p1,p5,p6,p8} 0.091 0.111 0.174 0.229 0.278 0.364\n{p1,p4,p5,p8} 0.091 0.103 0.176 0.216 0.291 0.366\nSI\n{p1,p4,p5,p8,p9} 0.091 0.099 0.165 0.205 0.270 0.358\n{p3,p4} 0.056 0.068 0.105 0.135 0.159 0.223\n{p1,p4} 0.056 0.071 0.092 0.120 0.154 0.225\n{p1,p3,p4} 0.052 0.060 0.105 0.128 0.163 0.218\n{p1,p2,p4} 0.052 0.058 0.096 0.116 0.137 0.197\n{p1,p4,p5} 0.052 0.064 0.094 0.114 0.137 0.203\n{p3,p4,p11} 0.050 0.058 0.079 0.099 0.141 0.186\n{p1,p4,p6} 0.049 0.051 0.086 0.105 0.131 0.197\n{p3,p4,p5} 0.048 0.058 0.096 0.114 0.144 0.208\n{p3,p4,p6} 0.048 0.051 0.094 0.109 0.135 0.199\n{p1,p3,p4,p5} 0.048 0.049 0.092 0.120 0.148 0.208\n{p1,p3,p4,p6} 0.048 0.054 0.090 0.111 0.137 0.214\n{p1,p3,p4,p11} 0.048 0.062 0.088 0.105 0.128 0.188\n{p2,p3,p4} 0.047 0.062 0.090 0.105 0.133 0.197\n{p1,p2,p4,p6} 0.047 0.051 0.084 0.113 0.146 0.184\n{p1,p2,p4,p5} 0.047 0.054 0.083 0.113 0.141 0.188\n{p1,p2,p3,p4} 0.046 0.058 0.088 0.109 0.133 0.206\n{p1,p2,p3,p4,p5} 0.046 0.054 0.083 0.096 0.131 0.188\n{p4,p11} 0.046 0.053 0.081 0.099 0.122 0.171\n{p1,p3,p4,p5,p12} 0.046 0.058 0.079 0.094 0.114 0.171\n{p1,p4,p11} 0.045 0.053 0.084 0.101 0.139 0.208\n{p1,p2,p3,p4,p11} 0.045 0.060 0.084 0.099 0.118 0.169\n{p1,p3,p4,p5,p6} 0.045 0.047 0.083 0.116 0.137 0.184\n{p1,p4,p5,p6} 0.045 0.049 0.079 0.101 0.133 0.189\nSG\n{p1,p4,p5,p11} 0.045 0.045 0.077 0.096 0.133 0.186\nTable 7: The top 25 best performing pattern subsets for SI and SG tasks, sorted according to MRR. See Table 1\nfor which class the pattern pi belongs to.\n5886\nTriple Score\n(anger, burning, ﬁre) 2.00\n(cities, humid, jungles) 2.00\n(clouds, ﬂuffy, cotton) 2.00\n(deserts, hot, ovens) 2.00\n(exams, tough, hurdles) 2.00\n(families, powerful, fortresses) 2.00\n(ﬁngerprints, accurate, portraits) 2.00\n(highways, crooked, snakes) 2.00\n(love, pure, ﬂower) 2.00\n(anger, blazing, ﬁre) 1.67\n(love, romantic, melody) 1.67\n(money, valuable, oxygen) 1.67\n(obligations, binding, shackles) 1.67\n(teachers, creative, sculptors) 1.67\n(time, important, money) 1.67\n(tv, addicted, drug) 1.67\n(wisdom, inﬁnite, ocean) 1.67\n(desks, messy, junkyards) 1.33\n(eyelids, close, curtains) 1.33\n(god, benevolent, parent) 1.33\n(music, soothing, medicine) 1.33\n(skating, relaxing, ﬂying) 1.33\n(friendship, lovely, rainbow) 1.00\n(life, challenging, journey) 1.00\n(love, sweet, ﬂower) 1.00\n(love, fragile, rose) 1.00\n(pets, annoying, kids) 1.00\n(television, attractive, candy) 1.00\n(women, quiet, cats) 1.00\n(trust, secure, glue) 0.67\n(tv, harmful, drug) 0.67\n(tree trunks, weak, straws) 0.67\n(trees, sturdy, umbrellas) 0.67\n(winter, long, death) 0.33\n(tongues, spicy, ﬁre) 0.33\n(typewriters, obsolete, dinosaurs) 0.00\n(time, quick, snail) 0.00\n(trees, long, umbrellas) 0.00\n(tv, ineffective, drug) 0.00\n(tv, unreliable, drug) 0.00\nTable 8: Some results of simile interpretation. Score is\nthe average score of human evaluation.\nTriple Score\n(clouds, white, cream) 2.00\n(friendship, colorful, jewelry) 2.00\n(love, colorful, coral) 2.00\n(love, shiny, pearl) 2.00\n(skating, relaxing, noon) 2.00\n(tv, addictive, drug) 2.00\n(dreams, clear, crystal) 1.67\n(friendship, colorful, sunrise) 1.67\n(love, addictive, coke) 1.67\n(love, colorful, sunrise) 1.67\n(music, cure, lullaby) 1.67\n(clouds, white, pearl) 1.33\n(dreams, clear, glass) 1.33\n(exams, challenging, boxing) 1.33\n(friendship, colorful, pottery) 1.33\n(knowledge, important, faith) 1.33\n(love, addictive, alcohol) 1.33\n(love, colorful, lavender) 1.33\n(music, cure, art) 1.33\n(clouds, white, dove) 1.00\n(desks, messy, nightmare) 1.00\n(desks, messy, storage) 1.00\n(highways, long, march) 1.00\n(knowledge, important, time) 1.00\n(love, addictive, poison) 1.00\n(love, colorful, perfume) 1.00\n(love, colorful, silk) 1.00\n(music, cure, time) 1.00\n(skating, relaxing, outdoors) 1.00\n(typewriters, ancient, legend) 1.00\n(cities, crowded, blast) 0.67\n(knowledge, important, intuition) 0.67\n(love, colorful, neon) 0.67\n(clouds, white, bone) 0.33\n(friendship, colorful, lightning) 0.33\n(love, addictive, spice) 0.33\n(cities, crowded, hell) 0.00\n(clouds, white, steel) 0.00\n(dreams, clear, stone) 0.00\n(exams, challenging, robotics) 0.00\nTable 9: Some results of simile generation. Score is\nthe average score of human evaluation.\n5887"
}