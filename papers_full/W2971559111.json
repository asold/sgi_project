{
  "title": "R-Transformer Network Based on Position and Self-Attention Mechanism for Aspect-Level Sentiment Classification",
  "url": "https://openalex.org/W2971559111",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2112382699",
      "name": "Ziyu Zhou",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2323876393",
      "name": "Fang Ai Liu",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097481743",
      "name": "Qianqian Wang",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2112382699",
      "name": "Ziyu Zhou",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2323876393",
      "name": "Fang Ai Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097481743",
      "name": "Qianqian Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963909901",
    "https://openalex.org/W2739983396",
    "https://openalex.org/W4236758004",
    "https://openalex.org/W3105174597",
    "https://openalex.org/W6753114942",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6639118987",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2963168371",
    "https://openalex.org/W2964164368",
    "https://openalex.org/W2963240575",
    "https://openalex.org/W6765571568",
    "https://openalex.org/W6676723433",
    "https://openalex.org/W2252057809",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2772515592",
    "https://openalex.org/W2962808042",
    "https://openalex.org/W1930677882",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6638318767",
    "https://openalex.org/W2060099436",
    "https://openalex.org/W2252128625",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2109872889",
    "https://openalex.org/W2757541972",
    "https://openalex.org/W4205184193",
    "https://openalex.org/W6693505360",
    "https://openalex.org/W2910164082",
    "https://openalex.org/W6639301165",
    "https://openalex.org/W2251792193",
    "https://openalex.org/W2251124635",
    "https://openalex.org/W6725876726",
    "https://openalex.org/W6727807531",
    "https://openalex.org/W2113125055",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2814589985",
    "https://openalex.org/W1879966306",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W2529550020",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2956480774",
    "https://openalex.org/W2514722822",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Aspect-level sentiment classification (ASC) is a research hotspot in natural language processing, which aims to infer the sentiment polarity of a particular aspect in an opinion sentence. There are three main influence factors in the aspect-level sentiment classification: the semantic information of the context; the interaction information of the context and aspect; the position information between the aspect and the context. Some researchers have proposed way to solve aspect-level sentiment classification. However, previous work mainly used the average vector of the aspect to calculate the attention score of the context, which introduced the influence of noise words. Moreover, these attention-based approaches simply used relative positions to calculate positional information for contextual and aspect terms and did not provided better semantic information. Based on these above questions, in this paper, we propose the PSRTN model. Firstly, obtaining the position-aware influence propagate between words and aspects by Gaussian kernel and generating the influence vector for each context word. Secondly, capturing global and local information of the context by the R-Transformer, and using the self-attention mechanism to obtain the keywords in the aspect. Finally, context representation of a particular aspect is generated for classification. In order to evaluate the validity of the model, we conduct experiments on SemEval2014 and Twitter. The results show that the accuracy of the PSRTN model can reach 83.8%, 80.9%, and 75.1% on three data sets, respectively.",
  "full_text": "Received August 7, 2019, accepted August 21, 2019, date of publication September 2, 2019, date of current version September 19, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2938854\nR-Transformer Network Based on Position and\nSelf-Attention Mechanism for Aspect-Level\nSentiment Classification\nZIYU ZHOU\n , FANG’AI LIU, AND QIANQIAN WANG\nSchool of Information Science and Engineering, Shandong Normal University, Jinan 250014, China\nCorresponding author: Fang’ai Liu (lfa@sdnu.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61772321, in part by the Natural\nScience Foundation of Shandong Province under Grant ZR2016FP07, in part by the Innovation Foundation of Science and Technology\nDevelopment Center of Ministry of Education and New H3C Group under Grant 2017A15047, and in part by the CERNET Innovation\nProject under Grant NGII20170508.\nABSTRACT Aspect-level sentiment classiﬁcation (ASC) is a research hotspot in natural language pro-\ncessing, which aims to infer the sentiment polarity of a particular aspect in an opinion sentence. There\nare three main inﬂuence factors in the aspect-level sentiment classiﬁcation: the semantic information of the\ncontext; the interaction information of the context and aspect; the position information between the aspect and\nthe context. Some researchers have proposed way to solve aspect-level sentiment classiﬁcation. However,\nprevious work mainly used the average vector of the aspect to calculate the attention score of the context,\nwhich introduced the inﬂuence of noise words. Moreover, these attention-based approaches simply used\nrelative positions to calculate positional information for contextual and aspect terms and did not provided\nbetter semantic information. Based on these above questions, in this paper, we propose the PSRTN model.\nFirstly, obtaining the position-aware inﬂuence propagate between words and aspects by Gaussian kernel\nand generating the inﬂuence vector for each context word. Secondly, capturing global and local information\nof the context by the R-Transformer, and using the self-attention mechanism to obtain the keywords in the\naspect. Finally, context representation of a particular aspect is generated for classiﬁcation. In order to evaluate\nthe validity of the model, we conduct experiments on SemEval2014 and Twitter. The results show that the\naccuracy of the PSRTN model can reach 83.8%, 80.9%, and 75.1% on three data sets, respectively.\nINDEX TERMS Aspect-level sentiment classiﬁcation, Gaussian kernel, R-transformer, self-attention\nmechanism.\nI. INTRODUCTION\nWhen people experience products and services, they often\nmake reviews. As the amount of review data continues to\ngrow, people must process it before using. The results of data\nprocessing are valuable information for potential customers\nand suppliers. Sentiment analysis is an important task in\nthe ﬁeld of natural language processing (NLP) [1]. Aspect-\nlevel sentiment classiﬁcation (ASC) is a ﬁne-grain task in\nsentiment analysis, which aims to identify the polarity of\nsentiment (positive, neutral, negative) of different aspects\nin the context. Therefore, ASC provides a more complete\nsentiment expression. For example, given a sentence like\n‘‘The pizza at this restaurant is very good, but the take-out\nThe associate editor coordinating the review of this article and approving\nit for publication was Junchi Yan.\npizza is very slow.’’ The sentiment polarity of the aspect\nterm ‘‘pizza’’ is positive, while the sentiment polarity of the\naspect ‘‘take-out pizza’’ is opposite to the sentiment polarity\nof ‘‘pizza.’’ The standard sentiment classiﬁcation is to assign\nan emotional label to each sentence. Since the ASC needs to\nconsider different aspects of the sentence compared with the\nstandard sentiment classiﬁcation, it is difﬁcult to classify the\naspect-level sentiment.\nThere are two ways to solve this task: traditional machine\nlearning methods and deep learning techniques. The tradi-\ntional classiﬁcation methods mainly train classiﬁers such as\nsupport vector machine (SVM) by designing a set of words,\nemotional lexicon [2], [3]. The quality of feature engineering\nis the key to traditional classiﬁcation methods. The main\nreason that deep learning technology can effectively solve\nASC is that deep learning can automatically learn effective\n127754 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 7, 2019\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nfeatures from high-dimensional data [4]–[6]. As a power-\nful sequence modeling technique, recurrent neural network\n(RNN) is widely used in NLP tasks [7], [8]. Some RNN-based\nmethod has been used for ASC [9]. But RNN treats each\ncontext word equally. Inspired by the human visual attention\nmechanism, the attention mechanism\nbased on neural network has been well applied in many\nresearches, such as image generation, machine translation,\nnatural language reasoning and so on [10]–[12]. Recently,\nsome attention-based RNN methods have been used for\nASC. Chen et al. [9] and Tang et al. [13] used the average\naspect vector to learn the attention weight of context words.\nMa et al. averaged the hidden vectors of aspect and con-\ntext respectively, and utilized the bidirectional attention to\nlearn weights [7]. When inferring the sentiment polarity of\none aspect of a sentence, other aspects and irrelevant con-\ntext words become noise. Modeling the relationship between\naspects and context and contextual semantic information\nbecomes the focus of ASC.\nWhen we calculate the relationship between aspects and\ncontext, the previous method mainly used the average pool\nof aspect hidden vectors as the ﬁnal representation of the\naspect to learn weights. However, if the aspect is a phrase,\nthe average aspect vector may result in information loss\nand introduce noise words. For example, the term ‘‘mobile\nphone’’ in the aspect ‘‘one mobile phone’’ is more important,\nwhile ‘‘one’’ becomes a noise word. Each word in the aspect\npays different attention to the context. If we use the keyword\n‘‘mobile phone’’ in the aspect to calculate the attention score\nof the context, the accuracy of the sentiment classiﬁcation\nwill be improved. Ma et al. proposed the IAN model, which\nused the average vector of aspect terms to calculate the repre-\nsentation of context, so the accuracy of model classiﬁcation\nwas low [14]. Each word in aspect terms has a different\nimpact on classiﬁcation, so it is important to calculate the\nweight of each word in aspect terms. Therefore, we use the\nself-attention mechanism to calculate the weight of each word\nin the aspect terms and generate the feature representation of\nthe aspect terms.\nOn the other hand, some previous work has clariﬁed\nthe position information between aspect terms and context,\nwhich will improve the classiﬁcation accuracy [15]. There-\nfore, we use the Gaussian kernel function to calculate the\npositional relationship between the aspect terms and context.\nSimilar to some of the previous work, we use the positional\nrelationship to limit the aspect to pay more attention to neigh-\nboring words.\nIn addition, a lot of previous work is based on RNN to\nget the semantic information of the context. Since RNN is\ndifﬁcult to compute in parallel and cannot maintain long-\nterm dependencies, the semantic information in the mod-\neling context shows weaknesses. In this paper, we use the\nR-transformer proposed by Wang et al. to model the con-\ntext [16]. It can obtain both long-term dependency informa-\ntion and local semantic information.\nBased on the above ideas, we propose a based-on posi-\ntion and self-attention mechanism R-Transformer network\n(PSRTN). PSRTN considers three inﬂuencing factors for\nASC. Our model uses Gaussian kernel function to calcu-\nlate the positional relationship between aspect and context.\nIn order to avoid noise words and better use of the keywords\nin aspect, PSRTN uses the self-attention mechanism to cal-\nculate the weight of each word in the aspect. In order to\nbetter obtain the semantic information of the context, we use\nR-Transformer to obtain global information dependence and\nlocal information dependence. We evaluate our model on\nSemEval2014 and Twitter dataset. The experimental results\nshow that our model has a certain improvement on ASC.\nThe main contributions of our model are summarized as\nfollows:\n1) We consider three inﬂuencing factors for ASC: the\nkeywords in the aspect term, the positional relation-\nship between the aspect term and the context, and the\nsemantic information of the context.\n2) In order to overcome the shortcomings of RNN, we use\nR-Transformer to obtain the semantic information of\nthe context.\n3) In order to get an aspect-speciﬁc context representa-\ntion, we use a self-attention mechanism to calculate the\nweight of each word in the aspect term.\n4) We conduct experiments on the SemEval2014 and\nTwitter datasets. The experimental results show that our\nmodel is 0.7%, 1%, and 0.4% higher than the baseline\nmodel on the three data sets.\nThe main structure of our paper is as follows: In the second\npart, we review the related work of ASC. The third part\nshows the details of the PSRTN model. In the fourth part,\nexperiments were conducted on three public data sets. The\nlast part, summarizing our work and shortcomings.\nII. RELATED WORK\nThe purpose of ASC was to determine the sentiment polarity\nof a particular aspect in a sentence. Recently, the ASC had\nbeen paid more and more attention by researchers. Many\nmethods only used contextual features, but did not consider\ndifferent aspects of the sentence. Jiang et al. found in the\nstudy that neglecting aspects would lead to 40% sentiment\nclassiﬁcation errors [17]. Now, we had a lot of ways to solve\nthe sentiment classiﬁcation problem. The machine learning\nmethod usually labeled the text, then extracted the features\nof the training data to construct a classiﬁer such as SVM,\nand ﬁnally classiﬁed the unlabeled data by the classiﬁer [18].\nThe quality of the feature determines the performance of\nthe classiﬁer. In recent years, neural networks had achieved\nremarkable results in the ﬁeld of sentiment classiﬁcation [22].\nMany neural network structures had been proposed, includ-\ning Convolutional Neural Networks [19], Recurrent Neural\nNetworks [20], and Recursive Neural Networks [21]. Neural\nnetwork-based methods could automatically learn semantic\nrepresentations without complex feature engineering.\nVOLUME 7, 2019 127755\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nA. ASC BASED ON RNN\nRecurrent neural networks (RNN) are capable of performing\non various tasks in NLP. Long Short-Term Memory (LSTM)\nand Gated Recurrent Unit (GRU) are derived from RNN.\nDong et al. proposed the AdaRNN model, which used an\nadaptive multi-combination layer in the Recurrent neural\nnetwork, so that the sentiment of words could be propa-\ngated to the target according to the context and syntactic\nstructure [23]. Nguyen and Shirai proposed a newmodel,\nPhraseRNN, based on RNN and AdaRNN, which enriched\nthe representation of the target by using the dependency from\nthe sentence and the syntactic information of the composition\ntree [24]. Tang et al. proposed TD-LSTM, that divided the\ncontext into two parts from the target location. The context\nbefore the target is the left half, followed by the right half [25].\nTD-LSTM used two long and short-term memory networks\nto model the left and right parts, and ﬁnally combined the\ntwo parts for classiﬁcation. The structure of TC-LSTM was\nsimilar to TD-LSTM, which connected the average vector of\nthe aspect to the word embedding as part of the input, thus\nmaking the model more concerned with the aspect informa-\ntion. Zhang et al. proposed two gated neural networks for\ntarget sentiment analysis, one for obtaining grammatical and\nsemantic information and the other for modeling the relation-\nship between left and right contexts of a given target [26].\nXue et al. proposed a model based on convolutional neural\nnetworks and gating mechanisms (GTRU). GTRU used two\nconvolutional layers to model aspects and sentiment informa-\ntion separately and controlled sentiment ﬂow based on given\naspect information [27].\nB. ASC BASED ON ATTENTION MECHANISM\nNeural network based on attention mechanism had been suc-\ncessfully applied in machine translation and image genera-\ntion. Recently, many works had applied attention mechanisms\nto the aspect sentiment classiﬁcation. Wang et al. proposed\nan attention-based LSTM for aspect-level sentiment classiﬁ-\ncation [28]. At ﬁrst, he mapped the aspect to an embedding\nvector and then computed the relationship between the aspect\nand the context. Inspired by the deep memory network [29],\nTang et al. designed MemNet, which used a deep memory\nnetwork with multiple compute layers to capture the impor-\ntance of each context word [13]. Chen et al. used a multi-\nlayered attention mechanism model to capture long-distance\nsentiment features, and then nonlinearly combined different\nattentional features to obtain important information [9]. The\ndifference between Tang et al.[13] and Chen et al.[9] was\nthat they synthesized the features of a word sequence between\nthe attention module and the input module by introducing\na memory module. The previous approach was to model\ntheir context by generating target-speciﬁc representations,\nbut none of these methods neglected to model the target\nseparately. Ma et al. proposed an interactive attention network\nand interactive learned attention in target and context through\ntwo LSTM networks [14]. Finally, the combination of aspect\nrepresentation and context representation was used as input\nto the classiﬁer.\nThe previous method had achieved some success in the\nASC, but they all regard the aspect as a whole. Gu et al.\nproposed a position-aware bidirectional attention network.\nAlthough they realized that location information was helpful\nfor ASC, they simply introduced location information by rel-\native distance [30]. Song et al. proposed an attention encoder\nnetwork (AEN), which used multiple self-attention to obtain\ninteraction information between each word and context in the\naspect [31]. Although AEN overcame the inability of LSTM\nto process in parallel and maintained long-term dependency\non sentences, it discarded recurrent structures that caused\nperformance degradation and neglected local semantic\ninformation.\nIII. THE PROPOSED MODELS: PSRTN\nIn this section, we describe our propose model, PSRTN, in\ndetail. PSRTN can effectively infer the sentiment polarity\nof the aspect. In Section 3.1, we describe the generation\nprocess of position vectors. Section 3.2, we use Bi-GRU and\nR-Transformer to obtain aspect and context semantic\ninformation, respectively. The aspect-speciﬁc context rep-\nresentation is taken in Section 3.3. Finally, we show the\noverall structure of the PSRTN in Section 3.4. Next, we will\nintroduce each part one by one.\nA. POSITION VECTORS\nThrough our previous analysis, more accurate location infor-\nmation is very effective for ASC. Inspired by the method of\nmarked context position in [32] and [33], we propagate the\ninﬂuence of aspect to other locations through the Gaussian\nkernel and calculate the positional relationship between the\naspect term and the context.\nKernel(µ) =exp\n(−µ2\n2γ2\n)\n(1)\nwhere µ represents the distance between the aspect terms\nand the words in the sentence, and γ represents the range of\npropagation. Obviously, the greater the distance, the smaller\nthe position perception inﬂuence. In other words, the farther\nthe distance is, the smaller the impact of words on the aspect.\nThe value of γ varies from word to word, so we will set γ to\na constant value.\nTo use µ, we use a normal distribution to represent it as a\nvector. We use matrix P to represent the inﬂuence of distance\nµ on each dimension. The effect of the ith dimension of\ndistance µis calculated as:\nP(i,µ) ∼N(Kernel(µ),σ′) (2)\nwhere N is a normal distribution and σ′represents a standard\ndeviation. Each column in matrix P represents an inﬂuence\nvector for a particular distance. For a clearer representa-\ntion, we use pj ∈Rdp in matrix P as vector representation\nof between each word and aspect in a sentence. dp is the\n127756 VOLUME 7, 2019\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nFIGURE 1. The generation process of a position-aware vector.\ndimension of the position-aware inﬂuence vector. Figure 1\nshows the generation process of a position-aware vector.\nB. WORD REPRESENTATION\nWe deﬁne a sentence S with n words S =[w1,..., wn] and an\naspect term M contains m words M =[a1,a2,..., am]. Our\ngoal is to determine the sentiment polarity of the aspect term\nM for sentence S. We use the pre-trained embedded matrix\nGloVe and the pre-trained language model BERT to map\neach word in the aspect term and context to word embedding.\nThe embedded vector of context and aspect terms can be\nexpressed as: Ec =[ec\n1,..., ec\nn] and Et =[et\n1,..., et\nm]. The\nposition inﬂuence matrix P and the word embedding matrix\nEc are connected to obtain an embedding matrix Rc.\nRc =pj ⊕ec\ni (3)\n1) CONTEXT REPRESENTATION WITH R-TRANSFORMER\nBecause RNN cannot capture long-term dependencies and\ncannot perform parallel calculations on sequences. Recently,\nmany sequence learning models abandon the recurrent struc-\nture and only rely on the attention mechanism [31]. Although\nthe multi-attention mechanism allows each position in the\nsequence to be connected to any other positions. Therefore,\ninformation can ﬂow to other position without any inter-\nmediate loss. However, there are problems with multi-head\nattention mechanism. In order to solve the loss of position\nsequence information, the multi-head attention mechanism\nintroduces position information. Dai et al also pointed out\nthat the impact of this position information was limited and\nwe need to design effective position embedding or intro-\nduce them into the learning process in different ways [34].\nIn addition, wang et al proposed that multi-head atten-\ntion mechanism could captured long-term dependencies, but\nit ignored the important local structures in natural lan-\nguage [16]. Inspired by [16], we use R-Transformer to get the\nhidden vector of the context. R-Transformer takes advantage\nof RNN and multi-head self-attention while avoiding their\nshortcomings.\nThe R-Transformer consists of three parts: the lowest layer\nis a local RNN, which is used to capture local information.\nFIGURE 2. Local RNN obtains local semantic information.\nThe middle layer is a multi-head self-attention layer. The top\nlayer is a nonlinear feature conversion layer.\nThe local RNN organizes the original long sequence into a\nnumber of short sequences that include local information and\nthat are independently and identically processed by a shared\nRNN. Speciﬁcally, the R-Transformer builds a local window\nof size M for each target position, which contains M con-\nsecutive locations and ends at the target position. Therefore,\nlocal RNNs only focus on local short-term dependencies.\nFigure 2 shows a schematic of the local RNN. Speciﬁcally,\nthe position of a local short sequence of length M is given\nM =[xt−M−1,xt−M−2,..., xt ]. These short sequences are\nprocessed by the local RNN and output M hidden vectors,\nand then the last hidden vector represent as a feature of the\nlocal sequence.\nht =LocalRNN(xt−M−1,xt−M−2,..., xt ) (4)\nTherefore, the local RNN slides each window one by one,\nand then connects the representation of each local region as\nthe local hidden vector representation of the entire sentence.\nh1,h2,..., hN =LocalRNN(x1,x2,..., xN ) (5)\nThen, the representation of the local hidden vector is input\nto the multi-head self-attention module to capture the long-\nterm dependency information of the sentence [35]. Given the\ncurrent representation h1,h2,..., ht , enter it into the multi-\nhead self-attention. The new representation ut is calculated as\nfollows:\nut =MultiHeadAttention(h1,h2,..., ht )\n=Concatenation(head1(ht ),head2(ht ),...,\nheadk (ht ))W o (6)\nwhere headk (ht ) is the result of the kth attention head and\nW o is a linearization mapping matrix. Each attention head\nis a weighted sum of a set of vectors, which is calculated\nVOLUME 7, 2019 127757\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nFIGURE 3. R-Transformer overall structure.\nas follows:\n{α1,α2,...,α n}= soft max(QKT\n√dk\n)V\nheadi(ht ) =\nn∑\nj=1\nαjvj (7)\nAmong them, Q, K, and V are the matrix of the query, key,\nand value, respectively. The vector q, ki, vi in the matrix is\nmapped by the mapping matrix. The mathematical formula is\nas follows:\nq,ki,vi =W qht ,W k hi,W vhi (8)\nwhere W q, W k , W v are mapping matrices and different\nattention heads have different mapping matrices. Finally, the\nfeedforward network is used to nonlinearly transform the\nfeatures, and the residuals and layernorm are used to process\nthe connection between the layers. The entire R-Transformer\nis shown in Figure 3.\nFeedForward(mt ) =max(0,ut W1 +b1)W2 +b2 (9)\nAccording to [16], we set the R-Transformer to 3 layers.\nWe enter the contextual embedded matrix Rc =[rc\n1,..., rc\nn]\ninto the R-Transformer to get the hidden vector. The calcula-\ntion process of the ith layer is as follows:\nwi\n1,wi\n2,..., wi\nn =LocalRNN(rci\n1 ,rci\n2 ,..., rci\nn )\nwi′\n1,wi′\n2,..., wi′\nn =LayerNorm(wi\n1 +rci\n1 ,..., wi\nn +rci\nn )\nui\n1,ui\n2,..., ui\nn =MultiHeadAttention(wi′\n1,wi′\n2,..., wi′\nn)\nui′\n1,ui′\n2,..., ui′\nn =LayerNorm(ui\n1 +wi′\n1,..., ui\nn +wi′\nn)\nvi\n1,vi\n2,..., vi\nn =FeedForward(ui′\n1,ui′\n2,..., ui′\nn)\nri+1\n1 ,ri+1\n2 ,..., ri+1\nn =LayerNorm(vi\n1 +ui′\n1,..., vi\nn +ui′\nn)\n(10)\nwhere n represents the length of the sentence S and i rep-\nresents ith layer. The hidden vector of the ﬁnal context is\nobtained through the three-layer R-Transformer as follows:\nHn\nr =[h1\nr ,h2\nr ,..., hn\nr ] (11)\n2) ASPECT TERM REPRESENTATION WITH BI-GRU\nAmong the various NLP tasks, Bi-GRU has achieved remark-\nable results. It models the context in both directions for\nbetter semantic information. The forward GRU obtains the\nsemantic information of the given context from left to right,\nand the reverse GRU obtains the semantic information of\nthe given context from right to left. Finally, the two seman-\ntic information is linked and use as a representation of the\nentire context. Since the aspect contains fewer words, we use\nBi-GRU to obtain hidden vectors for aspect.\nHm\nt =[h1\nt ,h2\nt ,..., hm\nt ] (12)\nC. ATTENTION MECHANISM\nThe self-attention mechanism has been successfully applied\nin various tasks. Self-attention is the correlation calculation\nof every unit and all units in a sequence. It can directly calcu-\nlate dependencies without considering the distance between\nwords, and learn the internal structure of a sentence. We use\nthe self-attention mechanism to capture the keywords in the\naspect and generate aspect representations Vt .\nγ(hi\nt ) =tanh(wahi\nt +ba)\nαi = exp(γ(hi\nt ))\nm∑\nj=1\nexp(γ(hj\nt ))\nVt =\nm∑\ni=1\nαihi\nt (13)\nIn order to get words that affect the polarity of aspect.\nCalculating the weight of aspect represent Vt for the context.\nFinally, get an aspect-speciﬁc context representation Vs.\nβi = exp(γ(hi\nr ,Vt ))\nn∑\nj=1\nexp(γ(hj\nr ,Vt ))\nVs =\nn∑\ni=1\nβihn\nr (14)\nOther words in the aspect may also provide some informa-\ntion. In order to get more complete aspect-speciﬁc context\ninformation, we average the hidden vector of the aspect.\nMavg =\nm∑\ni=1\nhi\nt /m (15)\nThen, the aspect-speciﬁc context representation Vs and the\nmean Mavg of the aspect hidden vector are concatenated.\nThe ﬁnal representation Z input the SoftMax layer after the\nconnection.\nZ =[Mavg,Vs]\nx =tanh(wr Z +br )\ny =soft max(wsx +bs) (16)\n127758 VOLUME 7, 2019\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nFIGURE 4. PSRTN overall architecture. The PSRTN consists of three parts: The attention\nmechanism module, the word representation module, and the position vector module. These\nthree parts are represented by dashed lines of different colors.\nD. MODEL ARCHITECTURE\nThe overall architecture of the PSRTN is shown in Figure 4.\nIt consists of three modules. The underlying module is a\nposition vectors module that uses the Gaussian kernel to\nobtain the position vector of the context and then joins it to\nthe word embedding.\nThe intermediate module is a word representation module,\nand the embedding of aspects and contexts is input into\nBi-GRU and R-Transformer respectively to obtain hidden\nvectors. R-Transformer can obtain both global and local\ndependencies.\nThe top module is the attention module, which uses a self-\nattention mechanism to capture the keywords in the aspect\nand then generates an aspect-speciﬁc context representation.\nE. MODEL TRAINING\nOur goal is to optimize all parameters in order to minimize\nthe loss function as much as possible. We use cross entropy\nand L2 regularization as loss function. The formula is as\nfollows:\nL =−\n∑\nd∈D\nC∑\nc=1\nyc(d) ·log(gc(d)) +λL2(θ) (17)\nwhere D is the data set, d is one of the samples, C is the num-\nber of categories, and yc(d) is the true sentiment distribution.\nThe output of log( gc(d)) is a vector that represents the prob-\nability of sentiment polarity. λis a regularization coefﬁcient\nand θ contains all the parameters. To avoid overﬁtting, the\ndropout strategy is used.\nIV. EXPERIMENTS\nIn this section, we describe the experimental setup and design\nseveral sets of experiments to demonstrate the validity of\nour model. First, we compare our model with some baseline\nmodels to prove the validity of our model. Then, we design\nseveral sets of ablation experiments to prove the validity\nof our module. Finally, we visualize the data sets in our\nexperiments.\nA. EXPERIMENT SETTINGS\nWe conduct experiment on Laptop and Restaurant and\nTwitter1. SemEval2014 2 includes both Laptop and Restau-\nrant. These data sets have three sentiment polarities: positive,\nneutral, and negative. Each review contains aspect terms\nand corresponding sentiment polarity. The number of each\npolarity is shown in table 1. We also count the number of\nwords in aspect in table 2. From table 2, we can see that more\nthan half of the data sets have multiple words in the aspect\nterms.\nIn our experiments, we use GloVe [36] and pre-trained\nlanguage model word representation Bert [37] to initial-\nize aspects and context word embedding. Glove sets the\nembedding dimension of each word to 300 and Bert to 768.\nThe number of GRU hidden units is set to 300, and the\n1http://goo.gl/5Enpu7\n2http://alt.qcri.org/semeval2014/\nVOLUME 7, 2019 127759\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nTABLE 1. Statistics of experimental data sets.\nTABLE 2. Statistics of the number of words in aspect on three data sets.\nR-Transformer is set to 128. Words outside the vocabulary\nare initialized with a uniform distribution U(-0.1,0.1), and\nthe weight matrix is set to zero. The propagation range γ\nis set to 23 and the standard deviation is set to 0.1. In the\nexperiment, the size of the batch is set to 25, the weight of\nthe L2 regularization is set to 10 −5 and the initial learning\nrate of AdaDelta is set to 0.05.\nB. COMPARATIV METHODS\nTo evaluate the effectiveness of our model, we compare our\nmodel with the following baseline model:\nLSTM ignores the importance of aspects. It gets the hidden\nvector of each word in the sentence and takes the last hidden\nvector as the ﬁnal representation of the sentence and then\ninputs the SoftMax layer [28].\nTD-LSTM divides the entire context into the left half of\nthe target context and the right half of the target context, and\nthen models the left and right parts by two LSTM networks.\nFinally, the two parts are connected for classiﬁcation [25].\nTC-LSTM The difference between TC-LSTM and\nTD-LSTM is that it integrates the relationship between target\nwords and context to enhance the importance of aspect\nrepresentation [25].\nAE-LSTM maps aspects to word embedding and then uses\nit as part of the training [28].\nATAE-LSTM is designed based on AE-LSTM, which\nemphasizes the importance of aspect by adding aspect\nembedding in the embedding of each word [28].\nMemNet uses a deep memory network to model sentences\nand captures the association between context words and\naspect terms [13].\nIAN uses two LSTM networks to model aspect terms and\nwords respectively, and then generates sentence representa-\ntions and aspect representations through interactive attention.\nFinally, two representations are connected for prediction [14].\nPBAN adds relative position information to each word\nembedding, and then uses Bi-GRU to model the interrelation-\nship between words and aspect terms [30].\nAOA-LSTMﬁrst models aspect terms and context through\nBi-LSTM, and then uses an interaction module to obtain\nattention from aspect terms to context and from context to\naspect terms to generate sentence representation for classiﬁ-\ncation [38].\nAEN avoids recurrent structures and models the rela-\ntionship between aspect and context by multi head\nself-attention [31].\nC. EXPERIMENT RESULTS\nWe use the accuracy to evaluate the validity of the model.\nThe experimental results of the PSRTN model and some\nbaseline models are shown in Table 3. To prevent the effects\nof different word representations, we perform experiments on\nGloVe and BERT, respectively. It can be clearly seen that the\nexperimental results of our model are superior to all other\nmodels based on GloVe and BERT.\nTABLE 3. Performance comparisons between different methods and the\nbest results in bold.\nFrom the experimental results, we can see that the\nLSTM network performs the worst in all LSTM-based\nmodels because LSTM does not consider aspect. Since\nTC-LSTM considers the relationship between target and con-\ntext, the accuracy rate is increased by 0.7% and 0.2%, 0.6%\ncompared with TD-LSTM. AE-LSTM embeds aspects into\nthe LSTM model for training, so AE-LSTM is more accurate\nthan TD-LSTM and TC-LSTM. Because ATAE-LSTM, IAN,\nand MemNet use attention mechanisms to model the relation-\nship between aspects and contexts, they are more accurate\nthan TD-LSTM and TC-LSTM. IAN is the ﬁrst to use atten-\ntion to model aspects. Comparing with ATAE-LSTM, IAN\n127760 VOLUME 7, 2019\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nimproves 0.9% and 3.4% on Restaurant and Laptop, respec-\ntively. PBAN introduces relative position information, so the\naccuracy has been signiﬁcantly improved, which indicates\nthat position has an important role in ASC. AOA-LSTM and\nPBAN have a similar result.\nAmong all GloVe-based models, PSRTN-GloVe has the\nhighest accuracy. Comparing with AOA-LSTM, PSRTN-\nGloVe increases by 2.2% and 4.4% on the restaurant and lap-\ntop, respectively. The main reason is that PSRTN-GloVe uses\nlocation information. Since PSRTN-GloVe obtains global and\nlocal information of the context, it is increased by 2.3% and\n4.8% compared with PBAN. In addition, comparing with the\nAEN model, the PSRTN-Bert model increases by 0.7%, 1%,\nand 0.4% on Restaurant, Laptop, and Twitter. Although AEN\ncan capture long-term dependency information, it discards\nthe recurrent structure, which is the main reason why AEN\nis less accurate than PSRTN.\nTABLE 4. Effect of position on model accuracy.\nFrom Table 4, we can see that the PSRTN w/o\np does not use any position information, so the model\nperforms the worst. The comparison of PSRTN w/o p\nand PSRTN +Absolute shows that position information is\nvery helpful for the classiﬁcation results. Comparing with\nPSRTN+Absolute, PSRTN +Gaussian increases by 0.6%,\n0.5%, and 0.4% on three data sets. Therefore, Gaussian kernel\nfunction is more suitable for our task.\nTABLE 5. Impact of averaging pooling and self-attention on aspect.\nTable 5 shows that PSRTN has higher classiﬁcation accu-\nracy by using aspect keywords than average aspect vector.\nComparing with average pooling, self-attention increases by\n0.9%, 1.1%, and 0.5%. In fact, the aspect does not have any\nsentiment information. Aspects that contain multiple words\nwill weaken the sentiment features during the processing of\nthe model and introduce noise. Therefore, the use of the self-\nattention mechanism can extract key words, thereby extract-\ning sentiment words for the aspect.\nFrom the experimental results in Table 6, we can see that\nR-Transformer is more suitable for our model. Using the\nBi-GRU module to obtain hidden vectors of context,\nthe model has the lowest accuracy because it cannot\nmaintain long-term dependencies and does not get better\nTABLE 6. Effect of PSRTN with Bi-GRU, multi-head attention and\nR-Transformer module on accuracy.\nsemantic information. Comparing to multi-head attention,\nR-Transformer increases by 0.9%, 0.8%, and 0.3% on Restau-\nrant, Laptop, and Twitter, respectively. Since R-Transformer\nnot only obtain long-term dependency information of context,\nbut also obtain local dependency information, it achieves\nbetter results than multi-head attention.\nFIGURE 5. The impact of propagation rangeγ on accuracy.\nD. ANALYZE THE PROPAGATION RANGEγ\nIn the formula (1), γ controls the propagation range of the\naspect. The position propagation inﬂuence decreases as γ\nincreases. In Figure 5, we use accuracy to measure the impact\nVOLUME 7, 2019 127761\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nof γ on three public data sets. The value of γ is set between\n5 and 35. As can be seen from the Figure5(a), the growth rate\nis more obvious between 15 and 25.\nIn Figure 5(b), we magnify the effect of γ on the accuracy\nbetween 20 and 25. We can see that the accuracy of γ is\nbasically stable between 20 and 25. When γ =21, γ =23,\nγ =23, the accuracy of the three data sets reach the highest.\nWe suspect that the length of the text mainly is between\n20 and 25 in the three data sets. For shorter or longer texts,\nthe effect of words on the aspect decreases as the distance\nincreases.\nFIGURE 6. The effect of the number of iterations.\nE. THE EFFECT OF NUMBER OF ITERATIONS\nON THE MODEL\nFigure 6 shows the effect of the number of iterations of the\nthree data sets on the model. The loss is relatively large at\nthe beginning. As the number of iterations increases, the loss\ngradually decreases. After 30 iterations of the Restaurant,\nthe model basically reached a convergence state, while Lap-\ntop and Twitter reached convergence at 18 and 49 respec-\ntively.Twitter has a slower convergence time than Restaurant\nand Laptop.\nFIGURE 7. Attention weights are visualized on two sentences by PSRTN.\nF. CASE STUDY\nIn order to better understand the PSRTN model, we use\nPSRTN model to predict the sentiment polarity of the review\n‘‘Boot time is super-fast, around anywhere from 35 seconds\nto 1 minute, but quite unreasonably priced’’ for ‘‘Boot time’’\nand ‘‘Priced’’. Figure 7 visualizes the attention of the word,\nthe deeper the color, the greater the weight of attention.\nThe PSRTN model accurately infers the sentiment polarity\nof aspects in a given text. The sentiment polarity of ‘‘Boot\ntime’’ is positive and the sentiment polarity of ‘‘priced’’ is\nnegative. As can be seen from Figure 7, the model considers\nthe key words in the aspect. For example, the aspect ‘‘Boot\ntime’’ calculates the weight of two words by the self-attention\nmechanism, and the word ‘‘time’’ should give more attention\nin aspect. PSRTN model also considers position relation-\nship. For example, when inferring the sentiment polarity of\n‘‘Boot time’’, the word ‘‘super-fast’’ with a close distance\nis signiﬁcant, while the words ‘‘seconds’’, ‘‘but’’, ‘‘quite’’\nwith a long distance are less weighted. Similarly, ‘‘priced’’\nis also affected by the word ‘‘unreasonably’’. Even if there\nare multiple aspects in a given text, the PSRTN model can\nﬁnd relevant sentiment words based on a given aspect term.\nFrom the second example, ‘‘The menu is limited but almost\nall of the dishes are excellent,’’ we can also see that the\nPSRTN model calculates key words in aspect terms and focus\non words that are close to aspect terms. Through the above\nanalysis, the PSRTN model can correctly judge the sentiment\npolarity of the aspect.\nV. CONCLUSION AND FUTURE WORK\nIn this paper, we present a PSRTN model for aspect-level sen-\ntiment classiﬁcation. PSRTN effectively exploits the relation-\nship between aspect and context and gets better contextual\nsemantic information. The PSRTN considers the inﬂuence\nof position information, which obtains positional informa-\ntion between aspects and contexts through a Gaussian kernel\nfunction and generates a position vector. PSRTN uses the\nself-attention mechanism to obtain key words in the aspect,\n127762 VOLUME 7, 2019\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nso that the model can better extract sentiment words for\nkeywords. PSRTN also utilizes R-Transformer to obtain long-\nterm dependency information and local dependency infor-\nmation. Finally, we conduct experiments on SemEval2014\nand Twitter, and our results show that our model exceed all\nbaseline models.\nWe will further study how to improve the performance of\nthe PSRTN model and apply it to other NLP tasks. On the\none hand, there may be a more efﬁcient way to connect\nposition vectors and word embedding. On the other hand,\nin addition to position features, studying other features is\na problem worthy of study for improving the aspect-level\nsentiment classiﬁcation.\nREFERENCES\n[1] B. Pang and L. Lee, ‘‘Opinion mining and sentiment analysis,’’ Found.\nTrends Inf. Retr., vol. 2, nos. 1–2, pp. 1–135, 2008.\n[2] Z. Zhang and M. Lan, ‘‘ECNU: Extracting effective features from mul-\ntiple sequential sentences for target-dependent sentiment analysis in\nreviews,’’ in Proc. 9th Int. Workshop Semantic Eval. (SemEval), 2015,\npp. 736–741.\n[3] V . Perez-Rosas, C. Banea, and R. Mihalcea, ‘‘Learning sentiment lexicons\nin Spanish,’’ in Proc. LREC, vol. 12, 2012, p. 73.\n[4] H. Liang, X. Sun, Y . Sun, and Y . Gao, ‘‘Text feature extraction based on\ndeep learning: A review,’’ EURASIP J. Wireless Commun. Netw., 2017,\nvol. 2017, no. 1, p. 211, 2017.\n[5] S. Zhou, Q. Chen, and X. Wang, ‘‘Active deep learning method for\nsemi-supervised sentiment classiﬁcation,’’ Neurocomputing, vol. 120,\npp. 536–546, Nov. 2013.\n[6] Y . LeCun, Y . Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[7] K. S. Tai, R. Socher, and C. D. Manning, ‘‘Improved semantic rep-\nresentations from tree-structured long short-term memory networks,’’\nin Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics , 2015,\npp. 1556–1566.\n[8] Q. Qian, B. Tian, M. Huang, Y . Liu, X. Zhu, and X. Zhu, ‘‘Learning\ntag embeddings and tag-speciﬁc composition functions in recursive neural\nnetwork,’’ in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics, 2015,\npp. 1365–1374.\n[9] P. Chen, Z. Sun, L. Bing, and W. Yang, ‘‘Recurrent attention network on\nmemory for aspect sentiment analysis,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2017, pp. 452–461.\n[10] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,\n‘‘DRAW: A recurrent neural network for image generation,’’ in Proc. 32nd\nInt. Conf. Mach. Learn., 2015, pp. 1462–1471.\n[11] M.-T. Luong, H. Pham, and C. D. Manning, ‘‘Effective approaches to\nattention-based neural machine translation,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., 2015, pp. 1412–1421.\n[12] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kočiský, and\nP. Blunsom, ‘‘Reasoning about entailment with neural attention,’’\n2015, arXiv:1509.06664. [Online]. Available: https://arxiv.org/abs/\n1509.06664\n[13] D. Tang, B. Qin, and T. Liu, ‘‘Aspect level sentiment classiﬁcation with\ndeep memory network,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess., 2016, pp. 214–224.\n[14] D. Ma, S. Li, X. Zhang, and H. Wang, ‘‘Interactive attention networks for\naspect-level sentiment classiﬁcation,’’ in Proc. 26th Int. Joint Conf. Artif.\nIntell., 2017, pp. 4068–4074.\n[15] X. Li, L. Bing, W. Lam, and B. Shi, ‘‘Transformation networks for target-\noriented sentiment classiﬁcation,’’ 2018, arXiv:1805.01086. [Online].\nAvailable: https://arxiv.org/abs/1805.01086\n[16] Z. Wang, Y . Ma, Z. Liu, and J. Tang, ‘‘R-transformer: Recur-\nrent neural network enhanced transformer,’’ 2019, arXiv:1907.05572.\n[Online]. Available: https://arxiv.org/abs/1907.05572\n[17] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao, ‘‘Target-dependent Twitter\nsentiment classiﬁcation,’’ in Proc. 49th Annu. Meeting Assoc. Comput.\nLinguistics, Hum. Lang., vol. 1, 2011, pp. 151–160.\n[18] S. Kiritchenko, X. Zhu, C. Cherry, and S. M. Mohammad, ‘‘NRC-Canada-\n2014: Detecting aspects and sentiment in customer reviews,’’ in Proc. 8th\nInt. Workshop Semantic Eval. (SemEval), 2014, pp. 437–442.\n[19] Y . Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’ in\nProc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), 2014,\npp. 1746–1751.\n[20] S. Lai, L. Xu, K. Liu, and J. Zhao, ‘‘Recurrent convolutional neural\nnetworks for text classiﬁcation,’’ in Proc. 29th AAAI Conf. Artif. Intell.,\nvol. 333, 2015, pp. 2267–2273.\n[21] X. Zhu, P. Sobihani, and H. Guo, ‘‘Long short-term memory over recursive\nstructures,’’ in Proc. Int. Conf. Mach. Learn., 2015, pp. 1604–1612.\n[22] F. Abid, M. Alam, M. Yasir, and C. Li, ‘‘Sentiment analysis through\nrecurrent variants latterly on convolutional neural network of Twitter,’’\nFuture Gener. Comput. Syst., vol. 95, pp. 292–308, Jun. 2019.\n[23] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, and K. Xu, ‘‘Adaptive recur-\nsive neural network for target-dependent Twitter sentiment classiﬁcation,’’\nin Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics, vol. 2, 2014,\npp. 49–54.\n[24] T. H. Nguyen and K. Shirai, ‘‘PhraseRNN: Phrase recursive neural network\nfor aspect-based sentiment analysis,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2015, pp. 2509–2514.\n[25] D. Tang, B. Qin, X. Feng, and T. Liu, ‘‘Effective LSTMs for target-\ndependent sentiment classiﬁcation,’’ in Proc. 26th Int. Conf. Comput.\nLinguistics (COLING), 2016, pp. 3298–3307.\n[26] M. Zhang, Y . Zhang, and D.-T. V o, ‘‘Gated neural networks for tar-\ngeted sentiment analysis,’’ in Proc. 13th AAAI Conf. Artif. Intell., 2016,\npp. 3087–3093.\n[27] W. Xue and T. Li, ‘‘Aspect based sentiment analysis with gated\nconvolutional networks,’’ 2018, arXiv:1805.07043. [Online]. Available:\nhttps://arxiv.org/abs/1805.07043\n[28] Y . Wang, M. Huang, L. Zhao, and X. Zhu, ‘‘Attention-based LSTM for\naspect-level sentiment classiﬁcation,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP), 2016, pp. 606–615.\n[29] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, ‘‘End-to-end memory\nnetworks,’’ in Proc. Annu. Conf. Neural Inf. Process. Syst., Montreal, QC,\nCanada, Dec. 2015, pp. 2440–2448.\n[30] S. Gu, L. Zhang, Y . Hou, and Y . Song, ‘‘A position-aware bidirectional\nattention network for aspect-level sentiment analysis,’’ in Proc. 27th Int.\nConf. Comput. Linguistics, 2018, pp. 774–784.\n[31] Y . Song, J. Wang, T. Jiang, Z. Liu, and Y . Rao, ‘‘Attentional encoder\nnetwork for targeted sentiment classiﬁcation,’’ 2019, arXiv:1902.09314.\n[Online]. Available: https://arxiv.org/abs/1902.09314\n[32] B. Liu, X. An, and J. X. Huang, ‘‘Using term location information to\nenhance probabilistic information retrieval,’’ in Proc. 38th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retr., 2015, pp. 883–886.\n[33] Q. Chen, Q. Hu, J. X. Huang, L. He, and W. An, ‘‘Enhancing recurrent\nneural networks with positional attention for question answering,’’ in Proc.\n40th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., 2017, pp. 993–996.\n[34] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n‘‘Transformer-XL: Attentive language models beyond a ﬁxed-length\ncontext,’’ 2019, arXiv:1901.02860. [Online]. Available: https://arxiv.org/\nabs/1901.02860\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[36] J. Pennington, R. Socher, and C. D. Manning, ‘‘GloVe: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’\n2018, arXiv:1810.04805. [Online]. Available: https://arxiv.org/abs/\n1810.04805\n[38] B. Huang, Y . Ou, and K. M. Carley, ‘‘Aspect level sentiment classiﬁcation\nwith attention-over-attention neural networks,’’ in Proc. Int. Conf. Social\nComput., Behav.-Cultural Modeling Predict. Behav. Represent. Modeling\nSimulation. Cham, Switzerland: Springer, 2018, pp. 197–206.\nVOLUME 7, 2019 127763\nZ. Zhouet al.: R-Transformer Network Based on Position and Self-Attention Mechanism for ASC\nZIYU ZHOU is currently pursuing the master’s\ndegree with the School of Information Science and\nEngineering, Shandong Normal University, China.\nHer research interests include natural language\nprocessing and data mining.\nFANG’AI LIU received the Ph.D. degree in\ncomputer science from China Science College,\nin 2002. He is currently a Professor and a Doc-\ntoral Student Guide Architect with the School of\nInformation Science and Engineering, Shandong\nNormal University, China. His research interests\ninclude the Internet, parallel processing, recom-\nmendation systems, prediction systems, and data\nmining. He is a CCF Senior Member.\nQIANQIAN WANG is currently pursuing the\nPh.D. degree with the School of Information\nScience and Engineering, Shandong Normal Uni-\nversity, China. Her research interests include pre-\ndiction systems, recommendation systems, and\ndata mining.\n127764 VOLUME 7, 2019",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8078078031539917
    },
    {
      "name": "Sentiment analysis",
      "score": 0.65206378698349
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5934956073760986
    },
    {
      "name": "Sentence",
      "score": 0.5655269622802734
    },
    {
      "name": "Support vector machine",
      "score": 0.49726036190986633
    },
    {
      "name": "Context model",
      "score": 0.449665904045105
    },
    {
      "name": "Natural language processing",
      "score": 0.4202384948730469
    },
    {
      "name": "Machine learning",
      "score": 0.3318588137626648
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28006308",
      "name": "Shandong Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 19
}