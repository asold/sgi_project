{
  "title": "Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer",
  "url": "https://openalex.org/W3137239140",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5032734433",
      "name": "Xiaojie Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050163233",
      "name": "Yueming Jin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040473879",
      "name": "Yonghao Long",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5090516040",
      "name": "Qi Dou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032708386",
      "name": "Phengâ€Ann Heng",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979797179",
    "https://openalex.org/W2025241052",
    "https://openalex.org/W2979536508",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2308279915",
    "https://openalex.org/W2980110287",
    "https://openalex.org/W2547734011",
    "https://openalex.org/W2580456502",
    "https://openalex.org/W2510642588",
    "https://openalex.org/W2921303908",
    "https://openalex.org/W2777273430",
    "https://openalex.org/W2963853051",
    "https://openalex.org/W2538822334",
    "https://openalex.org/W2599653512",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2884036902",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2986382673",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3092562667",
    "https://openalex.org/W2980225217",
    "https://openalex.org/W2550143307",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3150630416",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2266464013",
    "https://openalex.org/W1572533718",
    "https://openalex.org/W2064767749",
    "https://openalex.org/W3091935435",
    "https://openalex.org/W3095500987"
  ],
  "abstract": "Real-time surgical phase recognition is a fundamental task in modern operating rooms. Previous works tackle this task relying on architectures arranged in spatio-temporal order, however, the supportive benefits of intermediate spatial features are not considered. In this paper, we introduce, for the first time in surgical workflow analysis, Transformer to reconsider the ignored complementary effects of spatial and temporal features for accurate surgical phase recognition. Our hybrid embedding aggregation Transformer fuses cleverly designed spatial and temporal embeddings by allowing for active queries based on spatial information from temporal embedding sequences. More importantly, our framework processes the hybrid embeddings in parallel to achieve a high inference speed. Our method is thoroughly validated on two large surgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and outperforms the state-of-the-art approaches at a processing speed of 91 fps.",
  "full_text": "Trans-SVNet: Accurate Phase Recognition from\nSurgical Videos via Hybrid Embedding\nAggregation Transformer\nXiaojie Gao1, Yueming Jin1, Yonghao Long1, Qi Dou1,2\f, and\nPheng-Ann Heng1,2\n1 Department of Computer Science and Engineering,\nThe Chinese University of Hong Kong, Hong Kong, China\n2 T Stone Robotics Institute, CUHK, Hong Kong, China\nqdou@cse.cuhk.edu.hk\nAbstract. Real-time surgical phase recognition is a fundamental task\nin modern operating rooms. Previous works tackle this task relying on\narchitectures arranged in spatio-temporal order, however, the supportive\nbeneï¬ts of intermediate spatial features are not considered. In this paper,\nwe introduce, for the ï¬rst time in surgical workï¬‚ow analysis, Transformer\nto reconsider the ignored complementary eï¬€ects of spatial and temporal\nfeatures for accurate surgical phase recognition. Our hybrid embedding\naggregation Transformer fuses cleverly designed spatial and temporal\nembeddings by allowing for active queries based on spatial information\nfrom temporal embedding sequences. More importantly, our framework\nprocesses the hybrid embeddings in parallel to achieve a high inference\nspeed. Our method is thoroughly validated on two large surgical video\ndatasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and outper-\nforms the state-of-the-art approaches at a processing speed of 91 fps.\nKeywords: Surgical Phase Recognition Â· Transformer Â· Hybrid Embed-\nding Aggregation Â· Endoscopic Videos.\n1 Introduction\nWith the developments of intelligent context-aware systems (CAS), the safety\nand quality of modern operating rooms have signiï¬cantly been improved [19].\nOne underlying task of CAS is surgical phase recognition, which facilitates\nsurgery monitoring [2], surgical protocol extraction [33], and decision support [21].\nHowever, purely vision-based recognition is quite tricky due to similar inter-class\nappearance and scene blur of recorded videos [13,21]. Essentially, online recogni-\ntion is even more challenging because future information is not allowed to assist\ncurrent decision-making [30]. Moreover, processing high-dimensional video data\nis still time-consuming, given the real-time application requirement.\nTemporal information has been veriï¬ed as a vital clue for various surgical\nvideo analysis tasks, such as robotic gesture recognition [7,8], surgical instru-\nment segmentation [12,32]. Initial methods for surgical workï¬‚ow recognition,\narXiv:2103.09712v2  [cs.CV]  12 Jul 2021\n2 X. Gao et al.\n(a) (b)\nTemporal feature\nextractor\nDecision \nnetwork\nSpatial feature\nextractor\nTemporal feature\nextractor\nDecision \nnetwork\nSpatial feature\nextractor\nFig. 1.(a) Previous methods extract spatio-temporal features successively for surgical\nphase recognition; (b) We propose to reuse extracted spatial features together with\ntemporal features to achieve more accurate recognition.\nutilized statistical models, such as conditional random ï¬eld [23,3] and hidden\nMarkov models (HMMs) [22,5,26]. Nevertheless, temporal relations among sur-\ngical frames are highly complicated, and these methods show limited represen-\ntation capacities with pre-deï¬ned dependencies [13]. Therefore, long short-term\nmemory (LSTM) [11] network was combined with ResNet [10] in SV-RCNet [13]\nto model spatio-temporal dependences of video frames in an end-to-end fashion.\nYi et al. [30] suggested an Online Hard Frame Mapper (OHFM) based on ResNet\nand LSTM to focus on the pre-detected rigid frames. Gao et al. [8] devised a tree\nsearch algorithm to consider future information from LSTM for surgical gesture\nrecognition. With additional tool presence labels, multi-task learning methods\nare proposed to boost phase recognition performance. Twinanda [27] replaced\nthe HMM of EndoNet [26] with LSTM to enhance its power of modeling tem-\nporal relations. MTRCNet-CL [14], the best multi-task framework, employed\na correlation loss to strengthen the synergy of tool and phase predictions. To\novercome limited temporal memories of LSTMs, Convolutional Neural Networks\n(CNN) are leveraged to extract temporal features. Funke et al. [7] used 3D CNN\nto learn spatial and temporal features jointly for surgical gesture recognition.\nZhang et al. [31] devised a Temporal Convolutional Networks (TCN) [18,17]\nbridged with a self-attention module for oï¬„ine surgical video analysis. Czem-\npiel et al. [4] designed an online multi-stage TCN [6] called TeCNO to explore\nlong-term temporal relations in pre-computed spatial features. TMRNet [15], a\nconcurrent work, integrated multi-scale LSTM outputs via non-local operations.\nHowever, these methods process spatial and temporal features successively, as\nshown in Fig. 1 (a), which leads to losses of critical visual attributes.\nTransformer [28] allows concurrently relating entries inside a sequence at dif-\nferent positions rather than in recurrent computing styles, which facilitates the\npreservation of essential features in overlong sequences. Therefore, it can enable\nthe discovery of long-term clues for accurate phase recognition in surgical videos\nwhose average duration spans minutes or hours. Moreover, thanks to its parallel\ncomputing fashion, high speed in both training and inference stages is realized.\nBesides strong capacity in sequence learning, Transformer also demonstrates out-\nstanding ability in visual feature representation [9,16]. Recently, Transformer was\nemployed to fuse multi-view elements in point clouds and illustrated excellent\noutcomes [29], which implies its potential to promote the synergy of spatial and\ntemporal features in surgical videos.\nAccurate Surgical Phase Recognition via Transformer 3\nResNet\nğ‘¥1\nğ‘¥2\nâ‹¯\nâ‹¯\nğ‘¥ğ‘¡âˆ’1\nğ‘¥ğ‘¡\nâ‹¯ â‹¯\nğ‘™1 ğ‘™ğ‘¡\nSpatial embedding\nâ‹¯ â‹¯\nğ‘”1 ğ‘”ğ‘¡\nTemporal embedding\nğ‘¥ğ‘‡\nğ‘¥ğ‘‡âˆ’1\nğ‘™ğ‘‡\nğ‘”ğ‘‡\nGround truth\nâ„’ğ¶\nMulti-head\nAttention \nFeed Forward\nAdd & Norm\nğ‘ğ‘¡ ğ‘¦ğ‘¡\nTCN\nğ‘”1\nğ‘™â€²1 ğ‘™â€²ğ‘‡\nğ‘”ğ‘‡\nğ‘™ğ‘¡\nLinear\nLinear\nâ‹¯\nâ‹¯\nğ‘”ğ‘¡ âˆ’ğ‘›+1\nà·¤ğ‘”ğ‘¡ âˆ’ğ‘›+1 à·¤ğ‘”ğ‘¡à·¤ğ‘”ğ‘¡âˆ’1\nLinear\nFeed Forward\nSoftmax\nğ‘”ğ‘¡ğ‘”ğ‘¡âˆ’1 â‹¯â‹¯\nTrans layer\nğ‘”ğ‘‡ğ‘”ğ‘¡+1\n(a) Embedding model (b) Aggregation model\nAdd & Norm\nTrans layer\n(c) Trans layer\nğ‘ ğ‘ 1:ğ‘›\nğ‘™ğ‘‡ğ‘™ğ‘¡+1 â‹¯ğ‘™ğ‘¡âˆ’1â‹¯\náˆšğ‘™ğ‘¡\nTrans(ğ‘, ğ‘ 1:ğ‘›)\nQuery\nKey\nFig. 2.Overview of our proposed Trans-SVNet for surgical phase recognition. (a) Ex-\ntracted spatial embeddings enable the generation of temporal embeddings, (b) and are\nfused with the temporal information for reï¬ned phase predictions in our aggregation\nmodel, with the architecture of (c) Transformer layer presented in detail.\nIn this paper, we propose a novel method, named Trans-SVNet, for accu-\nrate phase recognition from surgical videos via Hybrid Embedding Aggregation\nTransformer. As shown in Fig. 1 (b), we reconsider the spatial features as one\nof our hybrid embeddings to supply missing appearance details during temporal\nfeature extracting. Speciï¬cally, we employ ResNet and TCN to generate spatial\nand temporal embeddings, respectively, where representations with the same se-\nmantic labels cluster in the embedding space. Then, we introduce Transformer,\nfor the ï¬rst time, to aggregate the hybrid embeddings for accurate surgical phase\nrecognition by using spatial embeddings to attend supporting information from\ntemporal embedding sequences. More importantly, our framework is parameter-\neï¬ƒcient and shows extraordinary potential for real-time applications. We exten-\nsively evaluate our Trans-SVNet on two large public 3 surgical video datasets.\nOur approach outperforms all the compared methods and achieves a real-time\nprocessing speed of 91 fps.\n2 Method\nFig. 2 presents an overview of our proposed Trans-SVNet, composed of embed-\nding and aggregation models. Our embedding model ï¬rst represents surgical\nvideo frames with spatial embeddings l and temporal embeddings g. The aggre-\ngation model fuses the hybrid embeddings by querying l from g to explore their\nsynergy for accurate phase recognition.\n2.1 Transformer Layer\nRather than only employed for temporal feature extraction, spatial features are\nreused to discover necessary information for phase recognition via our introduced\n3 http://camma.u-strasbg.fr/datasets\n4 X. Gao et al.\nTransformer. As depicted in Fig. 2 (c), a Transformer layer, composed of a multi-\nhead attention layer and a feed-forward layer, fuses a query q with a temporal\nsequence s1:n = [s1, . . . , snâˆ’1, sn]. Each head computes the attention of q with\ns1:n as key and value:\nAttn(q, s1:n) = softmax(Wqq(Wks1:n)T\nâˆšdk\n)Wvs1:n, (1)\nwhere W are linear mapping matrices and dk is the dimension of q after linear\ntransformation. The outputs of all heads are concatenated and projected to\nenable the residual connection [10] with q followed by a layer normalization [1].\nSince each attention head owns diï¬€erent learnable parameters, they concentrate\non respective features of interest and jointly represent crucial features. We ï¬nd\nit necessary to utilize multiple heads rather than a single head to produce a\nmuch faster convergence speed. The feed-forward layer is made up of two fully\nconnected layers connected with a ReLU activation. The residual connection\nand layer normalization are applied in a similar way as the multi-head attention\nlayer. Finally, the output of the Transformer layer is denoted as Trans( q, s1:n),\nwhich contains synthesized information of q and s1:n.\n2.2 Video Embedding Extraction\nGiven the discrete and sequential nature of video frames, we suggest two kinds of\nembeddings to represent their spatial and temporal information, which extends\nthe spirit of word embeddings [20] to surgical video analysis. Let xt âˆˆRHÃ—WÃ—C\nand yt âˆˆRN denote the t-th frame of a surgical video with T frames in total and\nthe corresponding one-hot phase label, respectively. We ï¬rst employ a very deep\nResNet50 [10] to extract discriminative spatial embeddings, which is realized by\ntraining a frame-wise classiï¬er using the cross-entropy loss. Note that we only\nutilize phase labels because additional annotations like tool presence labels are\nnot widely available, and single-task methods are more practical in real-world\napplications. Then, outputs of the average pooling layer of ResNet50 are made\nas our spatial embeddings, i.e., lt âˆˆR2048, and high-dimensional video data are\nconverted into low-dimensional embeddings.\nTo save memory and time, temporal embeddings are directly extracted from\nthe spatial embeddings generated by the trained and ï¬xed ResNet50. We ï¬rst\nadjust the dimension of lt with a 1Ã—1 convolutional layer and generate lâ€²\nt âˆˆR32.\nThen, we exploit TCN to process the embedding sequence of a whole video\nwithout touching future information as illustrated in Fig. 2 (a). For easy com-\nparison, we employ TeCNO [4], a two-stage TCN model, to generate temporal\nembeddings using lâ€²\n1:T . Owing to multi-layer convolutions and dilated kernels, its\ntemporal receptive ï¬eld is increased to several minutes. Since lt is not updated,\nspatial embeddings of a whole video could be processed in a single forward com-\nputation, and the network converges quickly. Moreover, the outputs of the last\nstage of the TeCNO are used as our temporal embedding gt âˆˆRN .\nAccurate Surgical Phase Recognition via Transformer 5\n2.3 Hybrid Embedding Aggregation\nOur aggregation model, consisting of two Transformer layers, aims to output the\nreï¬ned prediction pt of frame xt by fusing the pre-computed hybrid video em-\nbeddings only available at time step t. The intuition is that a ï¬xed-size represen-\ntation encoded with spatio-temporal details is insuï¬ƒcient to express all critical\nfeatures in both spatial and temporal dimensions, thus information loss is in-\nevitably caused. Hence, we propose to look for supportive information based on\na spatial embedding lt from an n-length temporal embedding sequence gtâˆ’n+1:t\n(see Section 3 for ablation study), which allows for the rediscovery of missing\nyet crucial details during temporal feature extraction. In other words, our ag-\ngregation model learns a function R2048 Ã—RnÃ—N â†’RN .\nBefore synthesizing the two kinds of embeddings, they ï¬rst conduct internal\naggregation, respectively. On the one hand, dimension reduction is executed for\nthe temporal embedding lt to generate Ëœlt âˆˆRN by\nËœlt = tanh(Wllt), (2)\nwhere Wl âˆˆRNÃ—2048 is a parameter matrix. On the other hand, the temporal\nembedding sequence gtâˆ’n+1:t is processed by one of our Transformer layer to\ncapture self-attention and an intermediate sequence Ëœ gtâˆ’n+1:t âˆˆRnÃ—N is pro-\nduced.4 Speciï¬cally, each entry in [gtâˆ’n+1, . . . , gtâˆ’1, gt] attends all entries of the\nsequence, which is denoted as\nËœgi = Trans(gi, gtâˆ’n+1:t), i = t âˆ’n + 1, . . . , t. (3)\nGiven self-aggregated embeddings Ëœl and Ëœg, we employ the other Transformer\nlayer to enable Ëœlt to query pivotal information from Ëœgtâˆ’n+1:t as key and value\nwhile fuse with the puriï¬ed temporal features through residual additions (red\narrow in Fig. 2 (b)). Next, the output of the second Transformer layer is activated\nwith the Softmax function to predict phase probability:\npt = Softmax(Trans(Ëœlt, Ëœgtâˆ’n+1:t)). (4)\nAlthough the fused embeddings have a dimension of N, they still contain rich\ninformation for further processing. Lastly, our aggregation model is trained using\nthe cross-entropy loss:\nLC = âˆ’\nTâˆ‘\nt=1\nyt log(pt). (5)\n3 Experiments\nDatasets. We extensively evaluate our Trans-SVNet on two challenging surgical\nvideo datasets of cholecystectomy procedures recorded at 25 fps, i.e., Cholec80 [26]\n4 Zero padding is applied if necessary.\n6 X. Gao et al.\nTable 1. Phase recognition results (%) of diï¬€erent methods on the Cholec80 and\nM2CAI16 datasets. The best results are marked in bold. Note that the * denotes\nmethods based on multi-task learning that requires extra tool labels.\nMethod Cholec80 M2CAI16 #paramAccuracy Precision Recall JaccardAccuracy Precision Recall Jaccard\nEndoNet* [26] 81.7Â±4.2 73.7Â±16.1 79.6Â±7.9 58.3MEndoNet+LSTM* [27]88.6Â±9.6 84.4Â±7.9 84.7Â±7.9 68.8MMTRCNet-CL* [14]89.2Â±7.6 86.9Â±4.3 88.0Â±6.9 29.0M\nPhaseNet [24,26]78.8Â±4.7 71.3Â±15.6 76.6Â±16.6 79.5Â±12.1 64.1Â±10.3 58.3MSV-RCNet [13] 85.3Â±7.3 80.7Â±7.0 83.5Â±7.5 81.7Â±8.1 81.0Â±8.3 81.6Â±7.2 65.4Â±8.9 28.8MOHFM [30] 87.3Â±5.7 67.0Â±13.3 85.2Â±7.5 68.8Â±10.5 47.1MTeCNO [4] 88.6Â±7.8 86.5Â±7.0 87.6Â±6.7 75.1Â±6.9 86.1Â±10.0 85.7Â±7.7 88.9Â±4.5 74.4Â±7.2 24.7MTrans-SVNet (ours)90.3Â±7.1 90.7Â±5.0 88.8Â±7.4 79.3Â±6.6 87.2Â±9.3 88.0Â±6.7 87.5Â±5.5 74.7Â±7.7 24.7M\nand M2CAI16 Challenge dataset [25]. Cholec80 includes 80 laparoscopic videos\nwith 7 deï¬ned phases annotated by experienced surgeons. Its frame resolution\nis either 1920Ã—1080 or 854Ã—480. This dataset also provides tool presence labels\nto allow for multi-task learning. We follow the same evaluation procedure of\nprevious works [26,13,30] by separating the dataset into the ï¬rst 40 videos for\ntraining and the rest for testing. The M2CAI16 dataset consists of 41 videos that\nare segmented into 8 phases by expert physicians. Each frame has a resolution\nof 1920Ã—1080. It is divided into 27 videos for training and 14 videos for testing,\nfollowing the split of [24,13,30]. All videos are subsampled to 1 fps following\nprevious works [26,13], and frames are resized into 250 Ã—250.\nEvaluation Metrics.We employ four frequently-used metrics in surgical phase\nrecognition for comprehensive comparisons. These measurements are accuracy\n(AC), precision (PR), recall (RE), and Jaccard index (JA), which are also utilized\nin [13,30]. The AC is calculated at the video level, deï¬ned as the percentage\nof frames correctly recognized in the entire video. Since the video classes are\nimbalanced, the PR, RE, and JA are ï¬rst computed towards each phase and\nthen averaged over all the phases. We also count the number of parameters to\nindicate the training and inference speed to a certain degree.\nImplementation Details.Our embedding and aggregation models are trained\none after the other on PyTorch using an NVIDIA GeForce RTX 2080 Ti GPU.\nWe initialize the parameters of the ResNet from a pre-trained model on the\nImageNet [10]. It employs an SGD optimizer with a momentum of 0.9 and a\nlearning rate of 5e-4 except for its fully connected layers with 5e-5. Its batch size\nis set to 100, and data augmentation is applied, including 224 Ã—224 cropping,\nrandom mirroring, and color jittering. We re-implement TeCNO [4] based on\ntheir released code with only phase labels and directly make outputs of its second\nstage as our temporal embeddings. We report the re-implemented results of\nTeCNO, and this well-trained model directly generates our temporal embeddings\nwithout further tuning. Our aggregation model is trained by Adam optimizer\nwith a learning rate of 1e-3 and utilizes a batch size identical to the length of each\nvideo. The number of attention heads is empirically set to 8, and the temporal\nAccurate Surgical Phase Recognition via Transformer 7\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nCholec80 M2CAI16\nResNet\nTeCNO\nOurs\nGT\nP0\nFig. 3.Color-coded ribbon illustration for two complete surgical videos. The time axes\nare scaled for better visualization.\nsequence length n is 30. N is set to the dimension of the one-hot phase label.\nOur code is released at: https://github.com/xjgaocs/Trans-SVNet.\nComparison with State-of-the-arts. Table 1 presents comparisons of our\nTrans-SVNet with seven existing methods without a post-processing strategy.\nUsing extra tool presence annotations of the Cholec80 dataset, multi-task learn-\ning methods [26,27,14] generally achieve high performances, and MTRCNet-CL\nbeats all single-task models except ours. As for methods using only phase labels,\nPhaseNet is far behind all other models due to its shallower network. Thus the\nmuch deeper ResNet50 becomes a standard visual feature extractor since SV-\nRCNet [13]. As a multi-step learning framework like OHFM, our approach gains\na signiï¬cant improvement by 6%-12% in JA with a much simpler training pro-\ncedure. Compared to the state-of-the-art TeCNO with the same backbones, our\nTrans-SVNet gains a boost by 4% in PR and JA on the larger Cholec80 dataset\nwith a negligible increase in parameters (âˆ¼30k). In a word, our Trans-SVNet out-\nperforms all the seven compared methods, especially on the enormous Cholec80\ndataset. Our method is observed to achieve a more remarkable improvement\non the Cholec80 dataset than the M2CAI16 dataset. The underlying reason is\nthat the M2CAI16 dataset is smaller and contains less challenging videos. The\nrobustness of our method yields a better advantage on the more complicated\nCholec80 dataset. Thanks to the designed low-dimensional video embeddings,\nour model generates predictions at 91 fps with one GPU, which vastly exceeds\nthe video recording speed.\nQualitative Comparison. In Fig. 3, we show the color-coded ribbon of two\ncomplete laparoscopic videos from the two datasets. Due to the lack of temporal\nrelations, ResNet suï¬€ers from noisy patterns and generates frequently jumped\npredictions. TeCNO achieves smoother results by relating long-term temporal\ninformation in spatial embeddings generated by ResNet. However, its predic-\ntions for P2 in both videos still need to be improved. We also visualize some of\n8 X. Gao et al.\nTable 2.Ablative testing results (%) for increasing length of our temporal embedding\nsequence on the Cholec80 dataset.\nLength (n) Accuracy Precision Recall Jaccard\n0 82.1Â±7.8 78.0Â±6.4 78.5Â±10.8 61.7Â±11.310 89.9Â±7.2 89.6Â±5.2 88.4Â±7.9 78.4Â±6.620 90.2Â±7.1 90.2Â±5.1 88.8Â±7.7 79.1Â±6.630 90.3Â±7.190.7Â±5.0 88.8Â±7.4 79.3Â±6.640 90.3Â±7.0 90.8Â±4.9 88.5Â±7.2 79.0Â±6.8\nTable 3.Phase recognition results (%) of diï¬€erent architectures and their P-values in\nJA towards our proposed method on the Cholec80 dataset.\nArchitecture Accuracy Precision Recall Jaccard P-values\nPureNet ResNet 82.1Â±7.8 78.0Â±6.4 78.5Â±10.8 61.7Â±11.3 2e-8TeCNO 88.6Â±7.8 86.5Â±7.0 87.6Â±6.7 75.1Â±6.9 2e-7ResNetcatTeCNO87.9Â±7.5 86.6Â±5.9 85.3Â±8.2 73.0Â±7.8 2e-8\nTransformer\nQueryKey Accuracy Precision Recall Jaccard P-valueslt ltâˆ’n+1:t 81.9Â±9.2 78.0Â±12.5 78.3Â±12.8 60.8Â±12.4 2e-8gt gtâˆ’n+1:t 89.1Â±7.8 87.6Â±6.3 87.7Â±6.9 76.2Â±6.6 4e-7gt ltâˆ’n+1:t 89.2Â±7.5 87.7Â±6.7 87.7Â±7.0 76.1Â±7.0 3e-7lt gtâˆ’n+1:t 90.3Â±7.1 90.7Â±5.0 88.8Â±7.4 79.3Â±6.6\nthe misclassiï¬ed frames of TeCNO and ï¬nd they are negatively inï¬‚uenced by\nexcessive reï¬‚ection, where bright but trivial parts might dominate the extracted\nspatial features, making it easy to miss pivotal information. Aggregating em-\nbeddings from ResNet and TeCNO elegantly, our Trans-SVNet contributes to\nmore consistent and robust predictions of surgical phases, which highlights its\npromotion towards the synergy between the hybrid embeddings.\nAblation Study.We ï¬rst analyze the eï¬€ect of diï¬€erent lengthn of our temporal\nembedding sequence on the Cholec80 dataset, and the results are reported in\nTable 2. It is observed that our design of temporal sequence is undoubtedly\nnecessary to gain a notable boost relative to not using temporal embeddings,\ni.e., n = 0. We also notice that gradually increasing the temporal sequence\nlength produces improvements towards all metrics, and our approach behaves\nalmost equally well with the length n âˆˆ[20, 40]. The boost tends to be slower\nbecause adding n by one increases the temporal sequence span by one second\n(only for n > 0) and over-long sequences bring too much noise. Therefore, we\nchoose n = 30 as the length of our temporal embedding sequence.\nTable 3 lists the results of diï¬€erent network structures, i.e., letting embed-\ndings from ResNet and TeCNO be query or key in every possible combination,\nto identify which one makes the best use of information. We ï¬rst show baseline\nmethods without Transformer denoted as PureNet. ResNet cat TeCNO employs\na superï¬cial linear layer to process concatenated lt and gt, whose performance\nunsurprisingly falls between ResNet and TeCNO. As for Transformer-based net-\nworks, there are no advancements to use spatial embedding lt to query ltâˆ’n+1:t.\nThe reason is that spatial embeddings cannot indicate their orders in videos and\nbring ambiguity in the aggregation stage. Better performances are achieved than\nPureNet by letting TeCNO embeddings g with sequential information be either\nquery or key, which justiï¬es Transformer rediscovers necessary details neglected\nAccurate Surgical Phase Recognition via Transformer 9\nby temporal extractors. Our Trans-SVNet uses lt to query gtâˆ’n+1:t and gen-\nerates the best outcomes with a clear margin, which conï¬rms the eï¬€ectiveness\nof our proposed architecture. We also calculate P-values in JA using Wilcoxon\nsigned-rank test for compared settings towards our Trans-SVNet. It is found\nthat P-values are substantially less than 0.05 in all cases, which indicates that\nour model learns a formerly non-existent but eï¬€ective policy.\n4 Conclusion\nWe propose a novel framework to fuse diï¬€erent embeddings based on Trans-\nformer for accurate real-time surgical phase recognition. Our novel aggregation\nstyle allows the retrieval of missing but critical information with rarely additional\ncost. Extensive experimental results demonstrate that our method consistently\noutperforms the state-of-the-art models while maintains a breakneck processing\nspeed. The excellent performance and parameter eï¬ƒciency of our method justify\nits promising applications in real operating rooms.\nAcknowledgements. This work was supported by Hong Kong RGC TRS\nProject T42-409/18-R, and National Natural Science Foundation of China with\nProject No. U1813204.\nReferences\n1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016) 4\n2. Bricon-Souf, N., Newman, C.R.: Context awareness in health care: A review. In-\nternational Journal of Medical Informatics 76(1), 2â€“12 (2007) 1\n3. Charri` ere, K., Quellec, G., Lamard, M., Martiano, D., Cazuguel, G., Coatrieux,\nG., Cochener, B.: Real-time analysis of cataract surgery videos using statistical\nmodels. Multimedia Tools and Applications 76(21), 22473â€“22491 (2017) 2\n4. Czempiel, T., Paschali, M., Keicher, M., Simson, W., Feussner, H., Kim, S.T.,\nNavab, N.: Tecno: Surgical phase recognition with multi-stage temporal convolu-\ntional networks. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention (2020) 2, 4, 6\n5. Dergachyova, O., Bouget, D., HuaulmÂ´ e, A., Morandi, X., Jannin, P.: Automatic\ndata-driven real-time segmentation and recognition of surgical workï¬‚ow. Interna-\ntional Journal of Computer Assisted Radiology and Surgery (2016) 2\n6. Farha, Y.A., Gall, J.: MS-TCN: Multi-stage temporal convolutional network for\naction segmentation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 3575â€“3584 (2019) 2\n7. Funke, I., Bodenstedt, S., Oehme, F., von Bechtolsheim, F., Weitz, J., Speidel,\nS.: Using 3D convolutional neural networks to learn spatiotemporal features for\nautomatic surgical gesture recognition in video. In: International Conference on\nMedical Image Computing and Computer-Assisted Intervention (2019) 1, 2\n8. Gao, X., Jin, Y., Dou, Q., Heng, P.A.: Automatic gesture recognition in robot-\nassisted surgery with reinforcement learning and tree search. In: IEEE International\nConference on Robotics and Automation. pp. 8440â€“8446. IEEE (2020) 1, 2\n10 X. Gao et al.\n9. Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu,\nC., Xu, Y., et al.: A survey on visual transformer. arXiv preprint arXiv:2012.12556\n(2020) 2\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\npp. 770â€“778 (2016) 2, 4, 6\n11. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation\n9(8), 1735â€“1780 (1997) 2\n12. Jin, Y., Cheng, K., Dou, Q., Heng, P.A.: Incorporating temporal prior from mo-\ntion ï¬‚ow for instrument segmentation in minimally invasive surgery video. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention (2019) 1\n13. Jin, Y., Dou, Q., Chen, H., Yu, L., Qin, J., Fu, C.W., Heng, P.A.: SV-RCNet:\nworkï¬‚ow recognition from surgical videos using recurrent convolutional network.\nIEEE Transactions on Medical Imaging 37(5), 1114â€“1126 (2018) 1, 2, 6, 7\n14. Jin, Y., Li, H., Dou, Q., Chen, H., Qin, J., Fu, C.W., Heng, P.A.: Multi-task\nrecurrent convolutional network with correlation loss for surgical video analysis.\nMedical Image Analysis 59, 101572 (2020) 2, 6, 7\n15. Jin, Y., Long, Y., Chen, C., Zhao, Z., Dou, Q., Heng, P.A.: Temporal memory\nrelation network for workï¬‚ow recognition from surgical video. IEEE Transactions\non Medical Imaging (2021) 2\n16. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin Vision: A Survey. arXiv preprint arXiv:2101.01169 (2021) 2\n17. Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional\nnetworks for action segmentation and detection. In: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition. pp. 156â€“165 (2017) 2\n18. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A\nuniï¬ed approach to action segmentation. In: European Conference on Computer\nVision. pp. 47â€“54. Springer (2016) 2\n19. Maier-Hein, L., Vedula, S.S., Speidel, S., Navab, N., Kikinis, R., Park, A., Eisen-\nmann, M., Feussner, H., Forestier, G., Giannarou, S., et al.: Surgical data science\nfor next-generation interventions. Nature Biomedical Engineering (2017) 1\n20. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed represen-\ntations of words and phrases and their compositionality. In: Advances in Neural\nInformation Processing Systems (2013) 4\n21. Padoy, N.: Machine and deep learning for workï¬‚ow recognition during surgery.\nMinimally Invasive Therapy & Allied Technologies 28(2), 82â€“90 (2019) 1\n22. Padoy, N., Blum, T., Feussner, H., Berger, M.O., Navab, N.: On-line recognition\nof surgical activity for monitoring in the operating room. In: Proceedings of the\nAAAI Conference on Artiï¬cial Intelligence. pp. 1718â€“1724 (2008) 2\n23. Quellec, G., Lamard, M., Cochener, B., Cazuguel, G.: Real-time segmentation and\nrecognition of surgical tasks in cataract surgery videos. IEEE Transactions on\nMedical Imaging 33(12), 2352â€“2360 (2014) 2\n24. Twinanda, A.P., Mutter, D., Marescaux, J., de Mathelin, M., Padoy, N.: Single-\nand multi-task architectures for surgical workï¬‚ow challenge at M2CAI 2016. arXiv\npreprint arXiv:1610.08844 (2016) 6\n25. Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy,\nN.: MICCAI modeling and monitoring of computer assisted interventions challenge.\nhttp://camma.u-strasbg.fr/m2cai2016/ 6\nAccurate Surgical Phase Recognition via Transformer 11\n26. Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy,\nN.: Endonet: a deep architecture for recognition tasks on laparoscopic videos. IEEE\nTransactions on Medical Imaging 36(1), 86â€“97 (2017) 2, 5, 6, 7\n27. Twinanda, A.P.: Vision-based approaches for surgical activity recognition using\nlaparoscopic and RBGD videos. Ph.D. thesis, Strasbourg (2017) 2, 6, 7\n28. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems. pp. 5998â€“6008 (2017) 2\n29. Wang, Y., Solomon, J.M.: Deep closest point: Learning representations for point\ncloud registration. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 3523â€“3532 (2019) 2\n30. Yi, F., Jiang, T.: Hard frame detection and online mapping for surgical phase recog-\nnition. In: International Conference on Medical Image Computing and Computer-\nAssisted Intervention (2019) 1, 2, 6\n31. Zhang, J., Nie, Y., Lyu, Y., Li, H., Chang, J., Yang, X., Zhang, J.J.: Symmetric\ndilated convolution for surgical gesture recognition. In: International Conference\non Medical Image Computing and Computer-Assisted Intervention (2020) 2\n32. Zhao, Z., Jin, Y., Gao, X., Dou, Q., Heng, P.A.: Learning motion ï¬‚ows for semi-\nsupervised instrument segmentation from robotic surgical video. In: International\nConference on Medical Image Computing and Computer-Assisted Intervention. pp.\n679â€“689. Springer (2020) 1\n33. Zisimopoulos, O., Flouty, E., Luengo, I., Giataganas, P., Nehme, J., Chow, A.,\nStoyanov, D.: DeepPhase: surgical phase recognition in CATARACTS videos. In:\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention (2018) 1\nSupplementary Materials\nCholec80 MI2CAI16\nResNet\nTeCNO\nOurs\nGT\nP1 P2 P3 P4 P5 P6 P7P0\nFig. S1.Additional results for qualitative comparisons on the two datasets.\nResNet TeCNO Ours\nFig. S2. Confusion matrices visualized by color brightness on the Cholec80 dataset.\nSupplementary Materials 13\nP1 P2 P3 P4 P5 P6 P7\nVideo 1 Video 2\nğ‘™ğ‘¡ ğ‘ğ‘ğ‘¡ ğ‘”ğ‘¡\nTrans(ğ‘™ğ‘¡,ğ‘™ğ‘¡âˆ’ğ‘›+1:ğ‘¡)\nGT\nTrans(ğ‘”ğ‘¡,ğ‘”ğ‘¡âˆ’ğ‘›+1:ğ‘¡)\nTrans(ğ‘”ğ‘¡,ğ‘™ğ‘¡âˆ’ğ‘›+1:ğ‘¡)\nTrans(ğ‘™ğ‘¡,ğ‘”ğ‘¡âˆ’ğ‘›+1:ğ‘¡)\nFig. S3.Visual results for ablation settings on the Cholec80 dataset.",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.8439779281616211
    },
    {
      "name": "Transformer",
      "score": 0.7562870979309082
    },
    {
      "name": "Computer science",
      "score": 0.7446775436401367
    },
    {
      "name": "Workflow",
      "score": 0.6863435506820679
    },
    {
      "name": "Inference",
      "score": 0.6835620403289795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5158478617668152
    },
    {
      "name": "Speedup",
      "score": 0.4626643657684326
    },
    {
      "name": "Task (project management)",
      "score": 0.4457095265388489
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3910527527332306
    },
    {
      "name": "Machine learning",
      "score": 0.33991771936416626
    },
    {
      "name": "Voltage",
      "score": 0.10558676719665527
    },
    {
      "name": "Engineering",
      "score": 0.09189188480377197
    },
    {
      "name": "Database",
      "score": 0.08986625075340271
    },
    {
      "name": "Parallel computing",
      "score": 0.07302257418632507
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "topic": "Embedding"
}