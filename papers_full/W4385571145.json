{
  "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
  "url": "https://openalex.org/W4385571145",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2779539080",
      "name": "Ilias Chalkidis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2912731862",
      "name": "Nicolas Garneau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224025738",
      "name": "Catalina Goanta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1909656138",
      "name": "Daniel Katz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100615786",
      "name": "Anders Søgaard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2573648081",
    "https://openalex.org/W3136888420",
    "https://openalex.org/W3201977280",
    "https://openalex.org/W4386566638",
    "https://openalex.org/W4389519438",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3176964086",
    "https://openalex.org/W4291960966",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3198651167",
    "https://openalex.org/W3176443840",
    "https://openalex.org/W4283810944",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W4282813826",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3196304422",
    "https://openalex.org/W3032232719",
    "https://openalex.org/W4285291532",
    "https://openalex.org/W3204112174",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3035390927"
  ],
  "abstract": "In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models' size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing performance. Based on these findings, we can conclude that both dimensions are important for those seeking the development of domain-specific PLMs.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15513–15535\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLeXFiles and LegalLAMA: Facilitating English Multinational\nLegal Language Model Development\nIlias Chalkidis∗ Nicolas Garneau∗ Anders Søgaard\nDepartment of Computer Science, University of Copenhagen, Denmark\nC˘at˘alina Goant, ˘a\nUtrecht University School of Law,\nNetherlands\nDaniel Martin Katz\nIllinois Tech – Chicago Kent College of Law,\nIL, United States\nAbstract\nIn this work, we conduct a detailed analy-\nsis on the performance of legal-oriented pre-\ntrained language models (PLMs). We exam-\nine the interplay between their original ob-\njective, acquired knowledge, and legal lan-\nguage understanding capacities which we de-\nﬁne as the upstream, probing, and downstream\nperformance, respectively. We consider not\nonly the models’ size but also the pre-training\ncorpora used as important dimensions in our\nstudy. To this end, we release a multinational\nEnglish legal corpus (L eXFiles) and a legal\nknowledge probing benchmark (LegalLAMA)\nto facilitate training and detailed analysis of\nlegal-oriented PLMs. We release two new le-\ngal PLMs trained on L eXFiles and evaluate\nthem alongside others on L egalLAMA and\nLexGLUE. We ﬁnd that probing performance\nstrongly correlates with upstream performance\nin related legal topics. On the other hand,\ndownstream performance is mainly driven by\nthe model’s size and prior legal knowledge\nwhich can be estimated by upstream and prob-\ning performance. Based on these ﬁndings,\nwe can conclude that both dimensions are im-\nportant for those seeking the development of\ndomain-speciﬁc PLMs.\n1 Introduction\nFollowing closely the advances in the development\nof NLP technologies, the legal NLP literature is\nﬂourishing with the release of many new resources,\nincluding large legal corpora (Henderson* et al.,\n2022), datasets (Chalkidis et al., 2021a; Koreeda\nand Manning, 2021; Zheng et al., 2021; Chalkidis\net al., 2022a; Habernal et al., 2022), and pre-trained\nlegal-oriented language models (PLMs) (Chalkidis\net al., 2020; Zheng et al., 2021; Xiao et al., 2021).\nBenchmark suites (Chalkidis et al., 2022a; Hwang\net al., 2022; Niklaus et al., 2023) to evaluate the\nperformance of PLMs in a more systematic way\n∗ Equal contribution.\nhave been also developed, showcasing the superi-\nority of legal-oriented PLMs over generic ones on\ndownstream legal NLP tasks.\nDespite this impressive progress, there is still\nnot a thorough study on (a) how PLMs trained\nunder diﬀerent settings (pre-training corpora, size\nof the model) perform across di ﬀerent legal sub-\ncorpora, and (b) what sort of knowledge such mod-\nels have acquired from pre-training, and (c) how\nimportant is domain (legal) speciﬁcity vs general\n(cross-domain) legal knowledge. Furthermore, of-\nten times, legal NLP relies on datasets without\ndrawing clear lines and comparisons between the\nvarious legal systems they may reﬂect. A legal sys-\ntem may be deﬁned as a set of rules adopted and\nenforced at a given governance level, which may be\nnational, regional or international (Friedman and\nHayden, 2017), e.g., UK, EU, US, CoE, etc.\nWe deﬁne the upstream evaluation as the task\nPLMs are explicitly designed to do: Masked Lan-\nguage Modelling (MLM) (Devlin et al., 2019). We\nthen probe for speciﬁc legal concepts that are legal-\nsystem speciﬁc, in a similar fashion as Petroni et al.\n(2019) did using the “LAnguage Models Analy-\nsis” (LAMA) framework. Finally, we assess the\nPLMs performance in LexGLUE (Chalkidis et al.,\n2022a) downstream tasks. More importantly, we\nexplore how the aforementioned factors (upstream,\nand probing performance) interplay and relate to\ndownstream performance. Our contributions are:\n(a) We release LeXFiles, a new diverse English\nlegal corpus including 11 sub-corpora that\ncover legislation and case law from 6 primar-\nily English-speaking legal systems (EU, CoE,\nCanada, US, UK, India). The corpus comprises\napprox. 6 million documents which sum up to\napprox. 19 billion tokens.\n(b) We release 2 new legal-oriented PLMs, dubbed\nLexLMs, warm-started from the RoBERTa (Liu\net al., 2019) models, and further pre-trained on\nthe LeXFiles for 1M additional steps.\n15513\nSub-Corpus (Source) # Documents # Tokens/ Percentage (%) Sampling Smoothing (%)\nEU Legislation 93.7K 233.7M (01.2%) 05.0%\nEU Case Law 29.8K 178.5M (00.9%) 04.3%\nUK Legislation 52.5K 143.6M (00.7%) 03.9%\nUK Case Law 47K 368.4M (01.9%) 06.2%\nCanadian Legislation 6K 33.5M (00.2%) 01.9%\nCanadian Case Law 11.3K 33.1M (00.2%) 01.8%\nU.S. Legislation 518 1.4B (07.4%) 12.3%\nU.S. Case Law 4.6M 11.4B (59.2%) 34.7%\nU.S. Contracts 622K 5.3B (27.3%) 23.6%\nECtHR Case Law 12.5K 78.5M (00.4%) 02.9%\nIndian Case Law 34.8K 111.6M (00.6%) 03.4%\nTotal 5.8M 18.8B (100%) 100%\nTable 1: Core statistics of the newly introduced L eXFiles corpus. In the last column, we present the sampling\nsmoothing percentages used to train our LexLM models (Section 4.1).\n(c) We release LegalLAMA, a diverse probing\nbenchmark suite comprising 8 sub-tasks that\naims to assess the acquaintance of legal knowl-\nedge that PLMs acquired in pre-training.\n(d) We evaluate 7 PLMs on both LeXFiles and\nLegalLAMA, analyzing their performance out\nof the box per LeXFiles sub-corpus and Legal-\nLAMA tasks. We also ﬁne-tune and evaluate\nthese models in selected LexGLUE tasks, and\nexamine the interplay between MLM, probing,\nand downstream performance.\n2 LeXFiles Corpus\nThe LeXFiles is a new diverse English multina-\ntional legal corpus that we created including 11\ndistinct sub-corpora (Table 1) that cover legislation\nand case law from 6 primarily English-speaking\nlegal systems (EU, CoE, Canada, US, UK, India).\nThe corpus contains approx. 19 billion tokens. In\ncomparison, the Pile of Law corpus released by\nHenderson* et al. (2022) comprises 32 billion in\ntotal, where the majority (26 /30) of sub-corpora\ncome from the United States of America (USA),\nhence the corpus as a whole is biased towards the\nUS legal system in general, and the federal or state\njurisdiction in particular, to a signiﬁcant extent.\nThe LeXFiles’s sub-corpora are:\n(a) EU Legislation. We release 93.7K EU laws\n(regulations, decisions, directives) published\nin EUR-Lex, the website of the EU Publica-\ntion Oﬃce.1\n(b) EU Case Law. We release 29.8K EU court\ndecisions, mainly issued from the Court of\n1https://eur-lex.europa.eu/\nJustice (CJEU), published in EUR-Lex.1\n(c) UK Legislation. We release 52.5 UK laws pub-\nlished in UK.LEGISLATION.GOV .UK, the\noﬃcial website of the UK National Archives.2\n(d) UK Case Law. We release 47K UK court deci-\nsions published in the British and Irish Legal\nInformation Institute (BAILII) database.3\n(e) US Legislation. We re-distribute 518 US state\nstatutes (legislation) originally published by\nHenderson* et al. (2022).\n(f) US Case Law. We release 4.6M US decisions\n(opinions) published by Court Listener, 4 a\nweb database hosted by the Free Law Project.5\n(g) US Contracts. We release 622K US contracts\n(agreements) obtained from US Securities and\nExchange Commission (SEC) ﬁlings, which\nare publicly available from the SEC-EDGAR6\ndatabase.\n(h) Canadian Legislation. We release 6K Cana-\ndian laws (acts, regulations) published in the\noﬃcial legislation portal of Canada.7\n(i) Canadian Case Law. We re-distribute 13.5K\nCanadian decisions (opinions) originally pub-\nlished by Henderson* et al. (2022).\n(j) ECtHR Case Law. We release 12.5K decisions\nruled by the European Court of Human rights\n2https://www.legislation.gov.uk/\n3https://www.bailii.org/\n4https://www.courtlistener.com/\n5We release decisions published from 1965 on-wards (cf.\npost Civil Rights Act), as a hard threshold for cases that possi-\nbly rely on out-dated and discriminatory law standards. The\nrest of the sub-corpora include more recent documents.\n6https://www.sec.gov/edgar\n7https://laws-lois.justice.gc.ca/eng/\n15514\n(ECtHR) published in HUDOC,8 the database\nof ECtHR.\n(k) Indian Case Law. We include 34.8K Indian\nSupreme Court cases originally published by\nMalik et al. (2021).\nThe LeXFiles is pre-split into training and test\nsubsets to provide a fair ground for comparing the\nperformance of PLMs that have not been trained in\nthe training set. We use the training subset of the\nLeXFiles corpus to train 2 new transformer-based\nlanguages models, dubbed LexLMs (Section 4.1),\nand evaluate their MLM performance across many\nother already available PLMs (Section 4.2).\n3 L egalLAMA Benchmark\nLAnguage Model Analysis (LAMA) (Petroni et al.,\n2019) is a probing task that is designed to assess\nspeciﬁc capabilities of PLMs. The general frame-\nwork of LAMA is to let PLMs predict a target to-\nken behind a [MASK] given its context, e.g., “Paris\nis the capital of [MASK]”, where the answer is\n‘France’. LegalLAMA is a new probing bench-\nmark suite inspired by this framework. It includes\n8 sub-tasks that aim to assess the acquaintance of\nlegal knowledge that PLMs acquired in the pre-\ntraining phase in a zero-shot fashion. Such tasks\ncannot be resolved by laypersons or even law pro-\nfessionals that are not experts in the speciﬁc ﬁelds\nof law in many cases. 9 The acquaintance of le-\ngal knowledge can be interpreted as some form of\nprimitive understanding of the law, for speciﬁc as-\npects in very controlled (limited) settings -limited\nlegal concepts under a speciﬁc jurisdiction-. As\nSahlgren and Carlsson (2021) mentioned:\n“Rather than asking whether a language model\nunderstands or not, we should askto what extent,\nand in which way, a model understands.”\nWe further extend the LAMA framework by al-\nlowing PLMs to predict multi-token targets. Take\nfor example the “Drug Traﬃcking” oﬀence under\nthe “Drug-Related” crimes of the US legislation.\nUsing the RoBERTa tokenizer, this term is split into\ntwo tokens, that is “Drug” and “Traﬃcking”. We\nreplace thus the “drug traﬃcking” phrase with two\n[MASK] tokens, and then ask the model to predict\nthese tokens simultaneously.\n8https://hudoc.echr.coe.int/eng\n9In Appendix A, we present a discussion on the Legal-\nLAMA tasks’ level of diﬃculty.\n…was arrested and charged under Alabama law with\n[MASK] [MASK], regarding paraphernalia…\nTop 5 predictions\nmarijuana\naggravated\nattempted\ncocaine\ndrug\ndistribution\nmarijuana\ncocaine\ntrafficking\npossession\nRR 1.00 0.50\nMean\n0.75\n1.\n2.\n3.\n4.\n5.\nFigure 1: Example from the ‘Terminology (US)’ sub-\ntask. Multi-token LAMA where “drug tra ﬃcking” has\nbeen replaced with two[MASK] tokens. Given the rank-\nings of each predicted token, we compute the recipro-\ncal rank (RR) and obtain a mean reciprocal rank (MRR)\nover the [MASK] tokens.\nWe evaluate the overall performance of PLMs\nusing the macro-averaged Mean Reciprocal Rank\n(MRR) (V oorhees and Tice, 2000) over the set of\nlabels (not the entire vocabulary). 10 In the case\nof multi-token targets, we average the MRR over\nthe predicted tokens. 11 Note that LegalLAMA\nexamples come from the test subset of the related\nLexFiles sub-corpora in order to have a fair compar-\nison between models trained or not on the LexFiles\ntraining sets. We provide a concrete example in\nFigure 1, and describe the tasks in detail:\nECHR Articles (CoE). In this task, we have\nparagraphs from the court assessment section of\nECtHR decisions. We extract those paragraphs\nfrom the newly introduced ECHR corpus presented\nin Section 2. The paragraphs include references to\nECHR articles, e.g., “Article [MASK] of the Con-\nvention”, where [MASK] is the article number. For\nexample, “The applicant complained under Article\n[2] of the Convention that the prison authorities\nhad failed to protect her son’s right to life by tak-\ning the necessary measures. ”Given a paragraph,\nwhere the article number is masked, the model has\nto predict the associated article number given the\ncontext. The dataset is composed of 5,072 test in-\nstances containing on average 69 tokens and 13\nunique article numbers to predict.\n10We decided to report only MRR results in the main paper\nfor the sake of clarity. Moreover, MRR avoids penalizing for\nnear-identical outcomes. Detailed results including Precision\nat 1 (P@1) are available in Appendix C.\n11A stricter evaluation would be to consider a multi-token\nprediction valid only if all the sub-tokens are properly pre-\ndicted by the PLM. We decided to average the MRR to con-\nsider minor variations and errors.\n15515\nContractual Section Titles (US). In this task,\nwe have sections from US contracts reusing the\ndataset of Tuggener et al. (2020). Contractual sec-\ntions are usually numbered and titled, e.g., \"10.\n[Arbitration]. Any controversy, dispute or claim\ndirectly or indirectly arising out of or relating to\nthis Agreement [...]\". The section titles reﬂect the\ncontent (subject matter) of the section, and are com-\nmonly re-used. Given a section, where the section\ntitle is masked, the model has to predict the asso-\nciated title given the context. The dataset is com-\nposed of 1,527 test instances containing on average\n85 tokens and 20 unique section titles to predict.\nContract Types (US). In this task, we have intro-\nductory paragraphs from US contracts. We extract\nthose paragraphs from the newly introduced corpus\nof US contracts, presented in Section 2. Introduc-\ntory paragraphs usually start with the contract title\nrevealing the contract type, e.g., \"Service Agree-\nment\", and follow with the names of the involved\nparties, and their roles in this agreement. For exam-\nple, “This [Purchase] Agreement is entered into\nthis 23rd day of January 2020 by and between A\n(the \"Purchaser\") and B (the \"Seller\"). ”. Given an\nintroductory paragraph, where the contract type is\nmasked, the model has to predict the associated\ntype given the context. The task is composed of\n1,089 test instances containing on average 150 to-\nkens and 15 unique types of contracts to predict.\nCrime Charges (US). In this task, we have para-\ngraphs from US court judgments (opinions). We\nextract those paragraphs from the US case law cor-\npus, presented in Section 2. We select a list of\ncriminal oﬀenses (e.g., “Sexual Assault”), catego-\nrized into 11 major categories (e.g., Sex-related)\nfrom the FindLaw website. 12 We ﬁlter out para-\ngraphs that refer the speciﬁed criminal charges ver-\nbatim. For example, “A person commits the crime\nof [burglary] in the ﬁrst degree when he or she en-\nters or remains unlawfully in a building with the\nintent to commit a crime against a person or prop-\nerty therein” Given a paragraph, where a criminal\ncharge is masked, the model has to predict the asso-\nciated criminal charge given the context. The task\nis composed of 4,518 test instances containing on\naverage 118 tokens and 59 charges to predict.\nLegal Terminology (US). In this task, we have\nparagraphs from US court judgments (opinions).\n12https://www.findlaw.com/criminal/\ncriminal-charges.html\nWe extract those paragraphs from the US case law\ncorpus, presented in Section 2. We select a sub-\nset of legal terms per legal topic (e.g., ﬁnance law,\nproperty law, family law) using the legal vocab-\nularies provided by the Legal Information Insti-\ntute (LII) of the Cornell Law School. 13 We ﬁlter\nout paragraphs that use the speciﬁed legal terms.\nFor example, “The [marital privilege]against self-\nincrimination is [...] grounded upon the theory that\njust as one may not be convicted by his own com-\npelled testimony, so may he not be convicted by the\ntestimony of his spouse. ”Given a paragraph, where\na legal term is masked, the model has to predict\nthe associated legal term given the context. The\ntask is composed of 5,829 test instances containing\non average 308 tokens and 92 legal terms from 7\ntopics to predict.\nLegal Terminology (EU). In this task, we have\nparagraphs from CJEU judgments (opinions). We\nextract those paragraphs from the newly introduced\nEU case law corpus, presented in Section 2. We\nselect a subset of legal terms based on the sub-\nject matters provided by the database of the courts\n(CURIA).14 We ﬁlter out paragraphs that use the\nspeciﬁed legal terms. For example, “The guiding\nprinciple at the basis of EU [data protection]law\nis that of a self-determined decision of an individ-\nual who is capable of making choices about the\nuse and processing of his or her data. ” Given a\nparagraph, where a legal term is masked, the model\nhas to predict the associated legal term given the\ncontext. The task is composed of 2,127 test in-\nstances containing on average 164 tokens and 42\nlegal terms from 23 topics to predict.\nLegal Terminology (CoE). In this task, we have\nparagraphs from ECtHR decisions. We extract\nthose paragraphs from the newly introduced ECHR\ncorpus presented in Section 2. We select a subset\nof legal terms (legal issues) based on the keywords\nprovided by the database of the courts (HUDOC).15\nWe ﬁlter out paragraphs that use the speciﬁed le-\ngal terms. For example, “The applicants alleged\nthat their relatives’ [right to life]was violated in\nthat they were deliberately killed by village guards. ”\nGiven a paragraph, where a legal term is masked,\nthe model has to predict the associated legal term\ngiven the context. The task is composed of 6,803\n13https://www.law.cornell.edu/\n14https://curia.europa.eu/\n15https://www.echr.coe.int/Documents/HUDOC_\nKeywords_ENG.pdf\n15516\nModel (Source) # Params # Vocab # Acc. Tokens Pre-training Corpora\nRoBERTa (Liu et al., 2019) 124/355M 50K 2T (160GB) Generic Corpora\nLegalBERT (Chalkidis et al., 2020) 110M 32K 43B (12GB) Legal Corpora\nCaseLawBERT (Zheng et al., 2021) 110M 32K 43B (37GB) US Case Law\nPoL-BERT (Henderson* et al., 2022) 340M 32K 130B (256GB) US Legal Corpora\nLexLM (ours) 124/355M 50K 2T+256B (175GB) Legal Corpora\nTable 2: Key speciﬁcations of the examined models. We report the number of parameters, the size of vocabulary,\nthe number of accumulated training tokens, and the nature of pre-trainig corpora.\ntest instances containing on average 97 tokens and\n250 legal terms from 15 articles to predict.\nCriminal Code Sections (Canada). In this task,\nwe have paragraphs from the Criminal Court of\nCanada’s decisions containing Section Numbers of\nthe Criminal Code of Canada (CCC)16. For exam-\nple, “Section [680] of the Criminal Code provides\nthat a bail review is to be conducted by a panel\nof this court where directed by the Chief Justice. ”\nGiven a paragraph, where a criminal code’s section\nis masked, the model has to predict the associated\nsection number, paragraph, and sub-paragraph (if\nany) given the context. The task is composed of\n321 test instances containing on average 72 tokens\nand 144 diﬀerent section numbers to predict.\nIn Appendix D, we present the full list of vocabu-\nlary (masked terms) grouped in categories (clusters)\n-when applicable- per LegalLAMA sub-task.\n4 Experiments\n4.1 Pre-trained Language Models\nWe consider 7 large language models to assess their\nperformance with respect to the upstream (MLM),\nprobing, and downstream evaluation:\nRoBERTa (Base /Large) are the original\nRoBERTa models (Liu et al., 2019) trained for 64k\nsteps with very large batches on generic corpora;\nthus do not have any clear legal prior (knowledge).\nLegalBERT (Base) is a legal-oriented BERT\nmodel (Devlin et al., 2019) released by Chalkidis\net al. (2020) trained for 1M steps on legal corpora\nfrom EU, UK, CoE, and USA.\nCaseLawBERT (Base) is another legal-oriented\nBERT released by Zheng et al. (2021). CaseLaw-\nBERT (which we will refer to as CL-BERT hence-\nforth) is trained from scratch for 2M steps on the\nHarvard Law case corpus, which comprises 3.4M\nlegal decisions from US federal and state courts.\n16https://laws-lois.justice.gc.ca/eng/acts/\nc-46/index.html\nPoL-BERT (Large) is a legal-oriented RoBERTa\nmodel released by Henderson* et al. (2022) trained\nfrom scratch for 2M steps on the Pile of Law, a\ncorpus consisting of approx. 256GB of English,\nmainly US, language legal and administrative text.\nLexLM (Base /Large) are our newly released\nRoBERTa models. We follow a series of best-\npractices in language model development:\n(a) We warm-start (initialize) our models from\nthe original RoBERTa checkpoints (base or\nlarge) of Liu et al. (2019).\n(b) We train a new tokenizer of 50k BPEs, but we\nreuse the original embeddings for all lexically\noverlapping tokens (Pfeiﬀer et al., 2021).\n(c) We continue pre-training our models on the\ndiverse LeXFiles (Section 2) corpus for ad-\nditional 1M steps with batches of 512 sam-\nples, and a 20/30% masking rate (Wettig et al.,\n2023), for base/large models, respectively.\n(d) We use a sentence sampler with exponential\nsmoothing of the sub-corpora sampling rate\nfollowing Conneau et al. (2019) since there\nis a disparate proportion of tokens across sub-\ncorpora (Table 1) and we aim to preserve per-\ncorpus capacity (avoid overﬁtting).\n(e) We consider mixed cased models, similar to\nall recently developed large PLMs.\nAdditional details on LexLM models pre-training\ncan be found in Appendix B.\n4.2 Upstream Evaluation\nIn Table 3, we present the upstream (MLM) per-\nformance for all PLMs across the LeXFiles sub-\ncorpora. The performance is measured in terms\nof accuracy, i.e. Precision@1 of the masked to-\nken to be predicted. The accuracy is thus averaged\nover all the masked tokens for each task. We also\nprovide the average across all tasks, per model.\nWe observe that results vary across models trained\nin very di ﬀerent settings (model’s capacity, pre-\n15517\nSub-Corpus RoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nEU Legislation 72.0 75.1 83.1 61.4 73.3 78.7 81.8\nEU Case Law 72.7 76.5 81.4 63.0 68.5 79.8 82.9\nUK Legislation 71.3 75.1 86.2 65.1 72.8 84.1 87.3\nUK Case Law 68.9 73.2 72.3 61.2 62.4 73.2 76.9\nCAN Legislation 75.5 78.9 80.6 66.4 73.3 82.9 85.2\nCAN Case Law 62.8 66.0 73.8 64.1 66.0 76.7 80.3\nUS Case Law 68.2 72.5 71.6 64.4 63.8 71.7 74.8\nUS Legislation 74.5 78.1 79.7 65.3 77.0 80.5 83.5\nUS Contracts 67.5 70.9 89.1 69.5 76.9 85.1 87.8\nECtHR Case Law 72.0 75.7 83.3 61.9 66.3 80.1 83.3\nIndian Case Law 65.6 70.0 65.2 56.3 58.3 73.3 76.2\nAverage 70.1 73.8 78.7 63.5 68.9 78.7 81.8\nModel Rank 5 4 2 7 6 2 1\nTable 3: Upstream evaluation measured in terms of accuracy (Precision@1) on the Masked Language Modelling\n(MLM) task across all LeXFiles sub-corpora.\ntraining corpora), while the results also vary across\nlegal sub-corpora.\nWe want to remind the reader that the upstream\nevaluation oﬀers a rough idea of a model’s capabili-\nties since it relies on random masked sub-words, in\nwhich case many of those can be generic and thus\nhighly predictable (e.g. preposition “of”). This phe-\nnomenon further motivates the construction of the\nLegalLAMA benchmark, in which case only “legal\nknowledge sensitive” words have been masked.\nType of Documents. In terms of di ﬀerences\nacross sub-corpora, we observe that the perfor-\nmance on legislation is better compared to case\nlaw in 3/4 legal systems, where we have both (EU,\nUK, US, Canada), with US contractual language\nbeing the most predictable for the models which\nhave been trained on it (LexLMs, LegalBERT).\nComparison of PLMs. Overall, the large\nLexLM model outperforms the rest, being 3% more\naccurate on average compared to the 2nd best mod-\nels (base versions of LexLM, and LegalBERT).\nSuch results are expected since LexLMs have been\ntrained in a diverse corpus, similarly to Legal-\nBERT, compared to CL-BERT, and PoL-BERT,\nwhich have been trained on US corpora. Over-\nspecialization harms the two US-centric models in\na great extend since they are outperformed even\nfrom the generic RoBERTa models.\nWe also observe that LegalBERT outperforms\nthe similarly-sized LexLM in speciﬁc sub-corpora\n(Both EU, UK legislation, ECtHR case law, and US\nContracts) that were included in its training. We\nhypothesize that these results are related to the pre-\ntraining data diversity, since LexLMs have been\ntrained in a more diverse corpus including many\nmore documents from diﬀerent legal systems with\na sampling smoothing to preserve capacity per sub-\ncorpus. The larger LexLM model has the capacity\nto cover all sub-corpora to a greater detail.\nIn general, larger models pre-trained on the same\ncorpora (RoBERTas, LexLMs) perform better com-\npared to smaller ones, but in-domain pre-training is\na much more important factor for upstream perfor-\nmance, e.g., LegalBERT outperforms RoBERTa-L.\n4.3 Probing Evaluation\nIn Table 4, we present the results across all exam-\nined PLMs onLegalLAMA. We analyze the results\nfrom two core perspectives: the prior knowledge\nand the probing task.\nPrior Knowledge. The pre-training corpus has\na signiﬁcant impact on the probing performance.\nRoBERTa models, having little to no legal prior,\nwere expected to achieve worst performance on all\nprobing tasks. Surprisingly, CL-BERT and PoL-\nBERT achieve on-par or sometimes worst perfor-\nmance than RoBERTa (Base & Large) in most tasks.\nBeing trained on the “Harvard Law Case” corpus\n(CL-BERT) and the Pile ofLaw (PoL-BERT), we\nwould have expected better performance than a\nmodel without legal prior. Their pre-training cor-\npora might be lacking diversity, which might cause\ntheir poor performance even on Legal-US probing\n15518\nStatistics Models\nTask #T #L #T /L RoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nECHR Articles 69 13 1.0 39.8 41.3 91.1 37.5 35.2 91.4 94.3\nContract Sections85 20 1.3 23.6 44.5 80.2 29.2 64.8 88.2 87.3\nContract Types 150 15 1.1 43.4 47.8 82.2 54.9 49.7 84.0 86.1\nCrime Charges (US)118 59 2.1 56.3 62.4 51.5 62.6 43.5 63.0 68.1\nTerminology (US)92 7 2.9 47.1 54.2 60.5 66.7 44.6 66.4 67.5\nTerminology (EU)164 42 3.0 38.0 45.3 63.2 38.6 36.9 63.1 70.4\nTerminology (CoE)97 250 1.2 45.4 53.1 77.3 49.7 32.8 81.3 86.8\nCC Sections 72 144 2.0 15.8 19.7 21.9 18.4 19.9 50.6 68.8\nAverage 33.1 41.3 54.8 38.0 36.8 70.8 77.4\nModel Rank 7 4 3 5 6 2 1\nTable 4: The 8 LegalLAMA tasks’ statistics regarding the average number of tokens in the input (#T), the number\nof labels to predict from (#L), and the average number of tokens per label (#T/L) along with the Mean Reciprocal\nRank results of the 7 examined PLMs.\ntasks. LegalBERT (Base), being trained on UK,\nEU and USA data illustrates important improve-\nment over models without legal prior (RoBERTa)\nor having only US legal prior (CaseLaw and PoL-\nBERT). LexLM models, being trained on the new\nLeXFiles dataset, show performance improvement\nover LegalBERT across all tasks, especially on the\ntask of predicting Section Numbers of the Crim-\ninal Code of Canada. Regarding the size of the\nmodel, we are able to compare the cased versions\nof RoBERTa Base/Large and LexLM Base/Large.\nAs expected, the larger versions oﬀer better perfor-\nmance than the smaller ones on every task.\n1 2 3 4 5 6 7\n0.30\n0.40\n0.50\n0.60\n0.70\nNumber of tokens to predict\nPerformance\nMRR\nFigure 2: Models performance on L egalLAMA’s test\nset with respect to the label complexity. Labels with\nmore than three tokens are much harder to predict.\nProbing Tasks. We characterize the diﬃculty of\nthe tasks by their semantic level, the output space\n(the number of labels to predict from), and the label\ncomplexity (how many tokens per label). We ex-\npose the tasks’ diﬀerent characteristics in Table 4.\nGiven the best-performing model (LexLM-L), we\ncan see that Crime Charges and Legal Terminology\n(US and EU) are the hardest tasks to solve. Look-\ning at Table 4, we can see that these three tasks are\ncharacterized by a higher label complexity ( >2).\nWe further demonstrate the label complexity im-\npact in Figure 2. The output space does not seem\nto have a correlation with the models’ performance,\nsince the selected Legal Terminology Topic Clus-\nters (US) has only 7 possible labels, whereas the\nCriminal Code Section (Canada) has 144 possible\nlabels. Finally, Crime Charges, being the hard-\nest task to solve, has on average 118 tokens as\ninput and 59 possible labels with moderate com-\nplexity, similar to the Terminology tasks (EU and\nCoE). This suggests that the diﬃculty of the task is\nnot only driven by the labels’ complexity but may\nrather lie in the lack of contextualization. Take for\nexample the following sentence:\n“This case involves perhaps the ﬁrst prosecu-\ntion under New York’s new[computer crime]\nstatute, Penal Law article 156, which went into\neﬀect on November 1, 1986, just days before\nthe incidents charged herein.”\nThe only contextual hint the PLMs have to predict\nthe correct tokens ([computer crime]) is the utter-\nance “Penal Law article 156, which went into eﬀect\non November 1, 1986”. This is the opposite task of\npredicting article numbers given a context, which\nis much more di ﬃcult than predicting the actual\ncontext because the output space is larger.17\n4.4 Downstream Evaluation\nFor downstream evaluation, we conduct experi-\nments for 6 legal classiﬁcation tasks, 5 part of\nLexGLUE (Chalkidis et al., 2022a), covering US\ncontracts, US, EU, and ECHR law.\nECtHR (Task B) (Chalkidis et al., 2021b) is a\nmulti-label topic classiﬁcation task, where given\n17The actual tokens predicted by the best-performing exam-\nined PLM were “sexual” and “abuse”.\n15519\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nTask µF1 mF1 µF1 mF1 µF1 mF1 µF1 mF1 µF1 mF1 µF1 mF1 µF1 mF1\nECtHR 61.2 40.5 74.2 51.5 59.1 37.2 53.6 29.1 69.1 46.9 63.2 41.8 76.7 57.9\nLEDGAR 80.5 62.6 83.6 71.5 81.2 64.7 80.9 64.0 83.3 71.4 82.5 66.8 84.7 72.8\nCNLI 66.8 48.6 68.0 63.5 70.2 65.6 69.0 64.6 68.3 64.1 61.6 42.9 69.7 64.5\nSCOTUS 65.0 36.0 68.9 41.4 60.9 31.2 62.9 33.8 66.3 39.5 66.9 37.7 71.1 43.9\nCaseHOLD 72.7 72.7 75.6 75.6 76.1 76.1 77.6 77.6 73.7 73.7 74.8 74.8 78.5 78.5\nEURLEX 33.4 06.1 62.7 27.1 27.7 04.0 27.0 04.7 60.5 25.4 34.2 06.9 63.1 28.0\nAverage 58.4 22.5 71.5 48.6 55.0 17.1 53.9 18.7 69.5 46.4 59.0 24.3 73.3 51.0\nUpstream 5 4 2 7 6 2 1\nProbing 7 4 3 5 6 2 1\nDownstream 5 2 6 7 3 4 1\nTable 5: Test Results for all models across all downstream tasks after ﬁne-tuning for a single epoch.\nthe facts of an ECtHR case, the model has to predict\nthe alleged violated ECHR article among 10 such\narticles (e.g., “Art 3. - Prohibition of Torture”, “Art.\n6 - Right to Fair Trial”).\nLEDGAR (Tuggener et al., 2020) is a single-label\nmulti-class topic classiﬁcation task, where given\na contractual paragraph, the model has to predict\none of the correct topic among 100 topics (e.g.,\n“Limitation of Liability”, “Arbitration”).\nContractNLI (Koreeda and Manning, 2021) is a\ncontract-based Natural Language Inference (NLI)\ntask, where given an Non-Disclosure Agreement\n(NDA) and one out 17 templated hypotheses (e.g.,\n“The Party may share some Conﬁdential Informa-\ntion with some third-parties.”), the model has to\npredict if the hypothesis is (entailed, contradicted,\nor is neutral) to the terms of the NDA.\nSCOTUS (Chalkidis et al., 2022a) is a single-label\nmulti-class topic classiﬁcation task, where given\na Supreme Court of US (SCOTUS) opinion, the\nmodel has to predict the relevant area among 14\nissue areas (e.g., “Civil Rights”, “Judicial Power”).\nCaseHOLD (Zheng et al., 2021) is a multiple\nchoice QA classiﬁcation task, where given a para-\ngraph from a US legal opinion where a legal rule\n(holding) is masked, the model has to predict the\napplicable rule among 5 alternatives (the correct\none and 2 irrelevant presented in other cases).\nEURLEX (Chalkidis et al., 2021a) is a multi-label\ntopic classiﬁcation task, where given an EU law,\nthe model has to predict the correct EUROVOC\nconcept among hundred concepts (e.g., “Environ-\nmental Policy”, “International Trade”).\nWe ﬁne-tune all examined PLMs (Section 4.1)\n1 2 3 4 5\n75\n80\nTraining Epochs\nPerformance (µF1)\nLexLM-L\nRoBERTa-L\nFigure 3: Development Results of RoBERTa and\nLexLM large on ECtHR across 5 training epochs.\nfor a single epoch with a learning rate of 1e − 5\nleading to a small number of updates. We are\ninterested to examine how fast each model con-\nvergence based on its prior knowledge; in other\nwords, what can a model learn in a single pass over\ntraining data? Finetuning models for many epochs\nover large datasets will eventually lead to a full\nre-parameterization of the models, in which case\nthe importance of prior knowledge will diminish\ncompromise the goal of our study (Figure 3).18\nFor all tasks, we use standard N-way classiﬁers\nwith a classiﬁcation head (Devlin et al., 2019). For\nECtHR, and SCOTUS, involving long documents,\nwe warm-start Longformer (Beltagy et al., 2020)\nmodels from each PLM’s parameters to encode up\nto 2048 tokens. We evaluate classiﬁcation perfor-\nmance with micro-F1 (µF1) and macro-F1 (mF1)\nacross tasks following Chalkidis et al. (2022a).\nResults In Table 5, we present the test results\nacross all tasks /datasets. We analyze the results\nfrom two perspectives: model’s capacity (size), and\nprior legal knowledge abducted via pre-training.\n18In most tasks, models fully converge after approx. 5\nepochs with improved performance, and the relative di ﬀer-\nences between generic and legal-oriented models are dimin-\nished (Chalkidis et al., 2022a).\n15520\nModel’s capacity (size) strongly correlates with\nthe overall downstream performance. Across all\ntasks, there are 2 /6 exceptions (CNLI and Case-\nHOLD) where LegalBERT outperforms larger\nPLMs. Both tasks are using sentence pairs, a setup\nused in BERT’s pre-training, but not in RoBERTa,\nwhich may bring LegalBERT, a BERT-based model,\nin a better initial condition co-considering the min-\nimal updates steps, compared to all large models\nfollowing the RoBERTa pre-training setup, which\ndo no use pairs of sentences or optimized based on\na sentence-level objective (NSP).\nLegal Knowledge also plays an important role fol-\nlowing the model’s capacity (size). We observe that\nLexLM-B trained in the diverse LeXFiles corpus\noutperforms the equally-sized RoBERTa-B model\nin 5/6 tasks, while LegalBERT and CL-BERT out-\nperform it only in 3 out of 6 tasks. In this case,\nthe results are mixed, i.e., acquaintance of legal\nknowledge as expressed by upstream (Section 4.2)\nand probing (Section 4.3) performance does not\ncorrelate with downstream performance.\nIn the case of large-sized models, LexLM-L out-\nperform RoBERTa-L across all tasks, while PoL-\nBERT trained on the US-biased Pile ofLaw cor-\npus is outperformed by RoBERTa-L in 5 out of 6\ntasks. Given the results with respect to upstream\nand probing performance, RoBERTa-L has a better\nlegal prior; so in these regards, acquaintance of\nlegal knowledge fully correlates with downstream\nperformance in the large models’ regime.\n5 Release of Resources\nWe release our code base to assure reproducibility\nand let others extend our study by experimenting\nwith other PLMs, or develop new ones. 19 The\nnew LexLM models (Section 4.1), the LeXFiles\ncorpus 20 (Section 2), and the LegalLAMA bench-\nmark 21 (Section 4.3) are available on Hugging\nFace Hub (Lhoest et al., 2021).22\n6 Conclusions and Future Work\nIn this work, we introduced a multinational En-\nglish legal corpus ( LeXFiles) and a legal knowl-\nedge probing benchmark (LegalLAMA) to facili-\ntate training and detailed analysis of legal-oriented\n19https://github.com/coastalcph/lexlms\n20https://huggingface.co/datasets/lexlms/lex_\nfiles\n21https://huggingface.co/datasets/lexlms/\nlegal_lama\n22https://huggingface.co/lexlms\nPLMs. We also released two new legal PLMs and\nevaluate them alongside others on LegalLAMA\nand LexGLUE. Based on our analysis (Section 4),\nwe make the following general observations:\n(a) The use of diverse legal corpora leads to better\noverall upstream performance (Section 4.2).\n(b) We ﬁnd that probing performance strongly\ncorrelates with upstream performance in re-\nlated legal topics (Section 4.3).\n(c) For both upstream, and probing performance,\nthe selection of pre-training corpora has a\nmuch larger e ﬀect compared to model’s ca-\npacity (Sections 4.2-4.3). Nonetheless, larger\nmodels pre-trained on similar corpora have\nbetter overall performance.\n(d) Downstream performance is mainly driven by\nthe model’s capacity and prior legal knowl-\nedge which can be estimated by upstream and\nprobing performance (Section 4.4).\nIn future work, we plan to further analyze the\nlearning dynamics of legal language models by\ncomparing their representations with representa-\ntions derived from legal knowledge bases. Given\nthe availability of the new resources, the develop-\nment of instruction-following (Wei et al., 2021)\nﬁne-tuned legal-oriented GPT-like (Ouyang et al.,\n2022) models is also an anticipated direction.\nLimitations\nDiversity of Corpora While the newly intro-\nduced LeXFiles corpus is signiﬁcantly more di-\nverse compared to the Pile ofLaw corpus of Hen-\nderson* et al. (2022), it is still an English-only\ncorpus covering only 6 legal systems (EU, UK,\nCoE, US, India, Canada). Despite, the fact that\nwe can train better models (LexLMs) and evaluate\nthese models across these corpora, in future work,\nwe should extend our analysis to cover even more\nlanguages and legal systems, and a higher granu-\nlarity in the labeling of legal ﬁelds within these\nsystems. Not only will this help support the inclu-\nsion of other legal traditions but also adding more\nlinguistic and cultural diversity will help us better\nunderstand the robustness of existing methods.\nSimilarly, the newly introduced LegalLAMA\nbenchmark consists of 8 sub-tasks targeting EU,\nECHR, US, and Canadian jurisdictions in a very\ncontrolled setting; where examples were automat-\nically extracted. While on this benchmark, legal-\noriented PLMs has demonstrated a signiﬁcant de-\ngree of “understanding\" of legal language and legal\n15521\ntopics, this benchmark should be further expanded\nwith more sub-tasks to evaluate the acquaintance\nof legal knowledge across more legal systems and\ntopics, and possibly cleansed from both very easy\nand unsolvable examples.\nModel Considerations In this work, we con-\nsider encoder-only (BERT-like) models up to ap-\nprox. 350M parameters, while recent work on the\ndevelopment of Large Language Models (LLMs)\n(Kaplan et al., 2020; Brown et al., 2020; Hoﬀmann\net al., 2022; Chowdhery et al., 2022) is mainly tar-\ngeting billion-parameter-sized models (10-100Bs\nof parameters) that usually follow a decoder-only,\ne.g., GPT (Radford and Narasimhan, 2018), or\nencoder-decoder, e.g., T5 (Raﬀel et al., 2020), ar-\nchitecture. Moreover, new paradigms of training\nPLMs have been introduced, such as instruction-\nbased ﬁnetuning (Wei et al., 2021), and alignment\nvia Reinforcement Learning from Human Feed-\nback (RLHF) (Stiennon et al., 2020; Ouyang et al.,\n2022). Latest GPT models (Ouyang et al., 2022)\nhave recently shown signiﬁcant zero-shot progress\non law-related tasks such as bar examination ques-\ntion answering (Katz et al., 2023). Thus, future\nwork should follow the most recent advances by\npre-training much larger auto-regressive GTP-like\nmodels that seem to lead to emergent zero-shot and\nfew-shot capabilities.\nEvaluation Considerations In Section 3, we\npresent how we account for and evaluate multi-\ntoken expressions (terms) on the LegalLAMA\nbenchmark; we are open to ideas on how we should\npossibly improve the current approach to provide a\nfairer and more robust evaluation framework across\nall models. Similarly, in Section 4.4, we ﬁne-tune\nall examined PLMs for a single epoch to avoid ex-\ntreme over-reparameterization and better estimate\nhow model’s knowledge aﬀects convergence and\nperformance. Nonetheless, there are possibly bet-\nter approaches to control for these aspects, e.g.,\nAdapter-based (Rücklé et al., 2021) ﬁnetuning, or\nother approaches, such as LoRA (Hu et al., 2022).\nBeyond Performance While we consider a\nmulti-facet analysis, we do not cover other inter-\nesting dimensions that should also be explored,\nespecially since law is a very sensitive application\ndomain; for instance trustworthiness-related top-\nics, such as model interpretability (Chalkidis et al.,\n2021b; Malik et al., 2021), and fairness (Chalkidis\net al., 2022b). Future work can build from the\nresults reported herein to explore these important\ntopics.\nEthics Statement\nThe scope of this work is to examine the perfor-\nmance of legal-oriented PLMs from a multi-facet\nperspective and broaden the discussion to help prac-\ntitioners build assisting technology for legal profes-\nsionals and laypersons. We believe that this is an\nimportant application ﬁeld, where research should\nbe conducted (Tsarapatsanis and Aletras, 2021) to\nimprove legal services and democratize law, while\nalso highlighting (informing the audience on) the\nvarious multi-aspect shortcomings seeking a re-\nsponsible and ethical (fair) deployment of legal-\noriented technologies.\nIn this direction, we introduce new resources\ncovering various legal systems to build new mod-\nels that better represent law and better assess their\ncapabilities. All newly developed and published re-\nsources are based on publicly available data, most\nof them scattered on several web portals.\nAcknowledgments\nThis work was partly funded by the In-\nnovation Fund Denmark (IFD, https:\n//innovationsfonden.dk/en) and the Fonds\nde recherche du Québec – Nature et technolo-\ngies (FRQNT, https://frq.gouv.qc.ca/\nnature-et-technologies/).\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Je ﬀrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nIlias Chalkidis, Manos Fergadiotis, and Ion Androut-\nsopoulos. 2021a. MultiEURLEX - a multi-lingual\nand multi-label legal document classiﬁcation dataset\n15522\nfor zero-shot cross-lingual transfer. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 6974–6996,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online.\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat-\nsanis, Nikolaos Aletras, Ion Androutsopoulos, and\nProdromos Malakasiotis. 2021b. Paragraph-level\nrationale extraction through regularization: A case\nstudy on European court of human rights cases. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 226–241, Online. Association for Computa-\ntional Linguistics.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nBommarito, Ion Androutsopoulos, Daniel Katz, and\nNikolaos Aletras. 2022a. LexGLUE: A benchmark\ndataset for legal language understanding in English.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4310–4330, Dublin, Ireland.\nAssociation for Computational Linguistics.\nIlias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia\nTomada, Sebastian Schwemer, and Anders Søgaard.\n2022b. FairLex: A multilingual benchmark for eval-\nuating fairness in legal text processing. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 4389–4406, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeﬀ Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLawrence M. Friedman and Grant M. Hayden. 2017.\n1What Is a Legal System? In American Law: An\nIntroduction. Oxford University Press.\nIvan Habernal, Daniel Faber, Nicola Recchia,\nSebastian Bretthauer, Iryna Gurevych, Indra\nSpiecker genannt Döhmann, and Christoph Bur-\nchard. 2022. Mining Legal Arguments in Court\nDecisions. arXiv preprint.\nPeter Henderson*, Mark S. Krass*, Lucia Zheng, Neel\nGuha, Christopher D. Manning, Dan Jurafsky, and\nDaniel E. Ho. 2022. Pile of law: Learning respon-\nsible data ﬁltering from the law and a 256gb open-\nsource legal dataset.\nJordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nWonseok Hwang, Dongjun Lee, Kyoungyeon Cho,\nHanuhl Lee, and Minjoon Seo. 2022. A multi-task\nbenchmark for korean legal language understand-\ning and judgement prediction. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Je ﬀrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2023. Gpt-4 passes the\nbar exam.\n15523\nYuta Koreeda and Christopher Manning. 2021. Con-\ntractNLI: A dataset for document-level natural lan-\nguage inference for contracts. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1907–1919, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cistac,\nThibault Goehringer, Victor Mustar, François Lagu-\nnas, Alexander M. Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nVijit Malik, Rishabh Sanjay, Shubham Kumar Nigam,\nKripabandhu Ghosh, Shouvik Kumar Guha, Arnab\nBhattacharya, and Ashutosh Modi. 2021. ILDC\nfor CJPE: Indian legal documents corpus for court\njudgment prediction and explanation. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 4046–4062,\nOnline. Association for Computational Linguistics.\nJoel Niklaus, Veton Matoshi, Pooja Rani, Andrea\nGalassi, Matthias Stürmer, and Ilias Chalkidis. 2023.\nLextreme: A multi-lingual and multi-task bench-\nmark for the legal domain.\nLong Ouyang, Jeﬀ Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions\nwith human feedback.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJonas Pfeiﬀer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. UNKs everywhere: Adapting mul-\ntilingual language models to new scripts. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10186–\n10203, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nColin Ra ﬀel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAndreas Rücklé, Gregor Geigle, Max Glockner,\nTilman Beck, Jonas Pfeiﬀer, Nils Reimers, and Iryna\nGurevych. 2021. AdapterDrop: On the e ﬃciency\nof adapters in transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7930–7946, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMagnus Sahlgren and Fredrik Carlsson. 2021. The sin-\ngleton fallacy: Why current critiques of language\nmodels miss the point. Frontiers in Artiﬁcial Intel-\nligence, 4.\nNisan Stiennon, Long Ouyang, Je ﬀrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nDimitrios Tsarapatsanis and Nikolaos Aletras. 2021.\nOn the ethical limits of natural language processing\non legal text. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3590–3599, Online. Association for Computational\nLinguistics.\nDon Tuggener, Pius von Däniken, Thomas Peetz, and\nMark Cieliebak. 2020. LEDGAR: A large-scale\nmulti-label corpus for text classiﬁcation of legal pro-\nvisions in contracts. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference ,\npages 1235–1241, Marseille, France. European Lan-\nguage Resources Association.\nEllen V oorhees and D Tice. 2000. The trec-8 question\nanswering track evaluation. 3. The TREC-8 Ques-\ntion Answering Track Evaluation.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned\nlanguage models are zero-shot learners. CoRR,\nabs/2109.01652.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2023. Should you mask 15% in\nmasked language modeling? In Proceedings of\n15524\nthe 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics , pages\n2985–3000, Dubrovnik, Croatia. Association for\nComputational Linguistics.\nChaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu,\nand Maosong Sun. 2021. Lawformer: A pre-trained\nlanguage model for chinese legal long documents.\nCoRR, abs/2105.03887.\nLucia Zheng, Neel Guha, Brandon R. Anderson, Pe-\nter Henderson, and Daniel E. Ho. 2021. When does\npretraining help? assessing self-supervised learning\nfor law and the casehold dataset. In Proceedings\nof the 18th International Conference on Artiﬁcial In-\ntelligence and Law. Association for Computing Ma-\nchinery.\nA LegalLAMA Discussion\nThe LegalLAMA tasks cannot be resolved by\nlaypersons or even law professionals that are not\nexperts in the speciﬁc ﬁelds of law in many cases.\nAnother consideration that often goes unspeciﬁed\nis that expertise is legal system-speciﬁc (e.g. US\nlaw diﬀers widely from EU law), as do the distinc-\ntions between the academic and the practical knowl-\nedge of law (including potential sub-distinctions\nbetween diﬀerent types of legal practitioners, e.g.\nlitigation experts, contract drafting experts, due dil-\nligence experts, etc.). Lastly, it is also important to\nnote that legal systems can be clustered according\nto similarities or diﬀerences. Speciﬁcally:\n• For task ‘ECHR Articles’, both laypersons\nand lawyers who are not experts in human\nrights law (particularly ECHR) would perform\nat random chance level, since they lack knowl-\nedge of the ECHR in an article level. Provid-\ning the titles of the articles (Table 6), we can\nexpect improved performance in case of rich\ncontext. Generally, the same can be said for\nthe related task ‘Legal Terminology (CoE)’.\nLegal terminology is very particular to indi-\nvidual legal systems, and predicting the place\nof legal concepts within the ECHR would re-\nquire a very high level of specialization.\n• For task ‘Contractual Section Titles (US)’,\nstructural knowledge of US contracts would\nbe necessary for the performance of this task\nwith a high degree of accuracy. This is due to\nthe fact that contracts often have some struc-\ntural similarities, but also particular charac-\nteristics depending on the type of contract\n(e.g. employment, sale, credit). Laypersons\nwould perform this task at random chance\nlevel. Practicing lawyers with contract draft-\ning expertise would potentially have the high-\nest performance in this task. Non-US lawyers\nwith no contract drafting expertise would per-\nform slightly higher than random chance level.\nThe same considerations apply to the task\n‘Contract Types (US)’.\n• For tasks ‘Crime Charges (US)’and ‘Crim-\ninal Code Sections (Canada)’, both layper-\nsons and lawyers who are not experts in crim-\ninal law (particularly US law and Canadian\nlaw) would perform at random chance level,\nsince the legal concepts are very speciﬁc (e.g.\nmanslaughter). Improved performance could\nbe seen in cases where the masked terms are\nspeciﬁcally deﬁned.\n• For tasks ‘Legal Terminology (US)’and ‘Le-\ngal Terminology (EU)’, the same discussion\nas above is applicable. Legal terminology is\nsystem-speciﬁc. There may be similar terms,\nbut in the absence of knowledge relating to\nhow such similarities may be interpreted, a\nnon-expert lawyer would not perform such a\ntask with a very high accuracy level.\nA.1 ECtHR Articles\nWe hereby provide details on the 13 ECtHR arti-\ncles;\nECHR ArticleDescription (Title)\nArticle 2 Right to life\nArticle 3 Prohibition of torture\nArticle 5 Right to liberty and security\nArticle 6 Right to a fair trial\nArticle 7 No punishment without law\nArticle 8 Right to respect for private and family life\nArticle 9 Freedom of thought, conscience and religion\nArticle 10 Freedom of expression\nArticle 11 Freedom of assembly and association\nArticle 13 Right to an eﬀective remedy\nArticle 14 Prohibition of discrimination\nArticle 34 Individual applications\nArticle 35 Admissibility criteria\nTable 6: ECHR Articles\nB LexLM Pre-training Details\nFor the newly released, LexLM models (LexLMs),\nwe followed a series of best-practices in language\nmodel development literature:\n(a) We warm-start (initialize) our models from\nthe original RoBERTa checkpoints (base or\nlarge) of Liu et al. (2019). Model recycling\n15525\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nTask P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nECHR Articles 0.26 0.40 0.27 0.41 0.86 0.91 0.23 0.38 0.20 0.35 0.86 0.91 0.91 0.94\nContract Sections 0.20 0.40 0.53 0.66 0.77 0.85 0.24 0.40 0.51 0.65 0.78 0.86 0.78 0.86\nContract Types 0.32 0.48 0.34 0.50 0.80 0.87 0.42 0.55 0.37 0.50 0.82 0.89 0.85 0.91\nCrime Charges (US)0.46 0.58 0.54 0.65 0.44 0.56 0.51 0.63 0.33 0.45 0.56 0.67 0.61 0.71\nTerminology (US) 0.41 0.51 0.49 0.58 0.52 0.63 0.58 0.69 0.37 0.49 0.64 0.74 0.70 0.79\nTerminology (EU) 0.34 0.47 0.40 0.53 0.51 0.64 0.25 0.39 0.25 0.38 0.60 0.72 0.67 0.77\nTerminology (CoE)0.43 0.54 0.51 0.60 0.69 0.78 0.36 0.49 0.30 0.41 0.78 0.86 0.86 0.91\nCC Sections 0.36 0.45 0.40 0.50 0.53 0.59 0.45 0.54 0.46 0.53 0.77 0.83 0.86 0.90\nAverage 0.33 0.47 0.41 0.54 0.61 0.71 0.34 0.49 0.32 0.46 0.71 0.80 0.77 0.85\nModel Rank 6 4 3 5 7 2 1\nTable 7: P@1 and MRR results of the 7 examined PLMs on the 8 LegalLAMA tasks.\nis a standard process followed by many (Wei\net al., 2021; Ouyang et al., 2022) to beneﬁt\nfrom starting from an available “well-trained”\nPLM, instead from scratch (random).\n(b) We train a new tokenizer of 50k BPEs based\non the training subsets of LeXFiles to bet-\nter cover legal language across all covered\nlegal systems. Although, we reuse the orig-\ninal RoBERTa embeddings for all lexically\noverlapping tokens (Pfeiﬀer et al., 2021), i.e.,\nwe warm-start word embeddings for tokens\nthat already exist in the original RoBERTa\nvocabulary, and use random ones for the rest.\n(c) We continue pre-training our models on the\ndiverse LeXFiles (Section 2) corpus for addi-\ntional 1M steps with batches of 512 samples.\nWe do initial warm-up steps for the ﬁrst 5% of\nthe total training steps with a linearly increas-\ning learning rate up to 1e−4, and then follow\na cosine decay scheduling, following recent\ntrends. For half of the warm-up phase (2.5%),\nthe Transformer encoder is frozen, and only\nthe embeddings, shared between input and\noutput (MLM), are updated. We also use an\nincreased 20/30% masking rate, where also\n100% of the predictions are based on masked\ntokens, compared to Devlin et al. (2019)23 for\nbase/large models respectively, based on the\nﬁndings of Wettig et al. (2023).\n(d) For both training the tokenizer and the LexLM\nmodels, we use a sentence sampler with ex-\nponential smoothing of the sub-corpora sam-\npling rate following Conneau et al. (2019) and\n23Devlin et al. –and many other follow-up work– used a\n15% masking ratio, and a recipe of 80/10/10% of predictions\nmade across masked/randomly-replaced/original tokens.\nRaﬀel et al. (2020), since there is a disparate\nproportion of tokens across sub-corpora (Ta-\nble 1) and we aim to preserve per-corpus ca-\npacity, i.e., avoid overﬁtting to the majority\n(approx. 94% of the total number or tokens)\nUS-origin texts.\n(e) We consider mixed cased models, similar to\nall recently developed large PLMs (Liu et al.,\n2019; Raﬀel et al., 2020; Brown et al., 2020).\nWe make LexLM models (base/large) publicly\navailable alongside all intermediate checkpoints\nevery 50k training steps on Hugging Face Hub.24\nC Detailed Legal-LAMA results per\ntasks\nTable 7 contains the same results as in Table 4 with\nthe addition of Precision@1 scores (P@1). The rea-\nson why we decided to only present MRR results in\nthe main paper is that the diﬀerence between MRR\nand P@1 does not change the ranking of the mod-\nels, and P@1 does not account for minor variations\nin predictions.\nFor each task, we display detailed results per\npredicted terms for each model. Table 8 contains\nresults on the 13 article numbers from the ECHR\ntask. Table 9 contains results on the 20 clause types\nfrom the Contract Section task. Table 10 contains\nresults on the 16 types of contracts from the Con-\ntract Section task. Table 11 contains results on the\n11 topics from the Crime Charges (US) task. Each\ntopic contains multiple labels. Table 12 contains\nresults on the 7 topics from the Terminology (US)\ntask. Each topic contains multiple labels. Table 13\n24https://huggingface.co/lexlms\n15526\ncontains results on the 23 topics from the Terminol-\nogy (EU) task. Each topic contains multiple labels.\nTable 14 contains results on the 12 articles from\nthe Terminology (CoE) task. Each article contains\nmultiple labels. Table 15 contains results on the 43\nsections from the Criminal Code Sections (Canada)\ntask.\nD LegalLAMA Tasks’ Vocabulary\nIn Tables 8, 9, 10, 13, and 15 we present the labels’\nlist for the ‘ECHR Articles’, ‘Contract Sections’,\n‘Contract Types’, ‘Terminology (EU)’ and ’Crim-\ninal Code Sections (Canada)’ sub-tasks and the\nlabel-wise performance. In Tables 16, 17, and 18,\nwe present the labels’ list for the ‘Terminology\n(CoE)’, ‘Crimes Charges (US)’, and ‘Terminology\n(US)’ sub-tasks grouped in clusters.\n15527\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nECHR ArticleP@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nArt. 2 0.87 0.91 0.63 0.76 0.87 0.92 0.27 0.45 0.29 0.51 0.86 0.91 0.91 0.94\nArt. 3 0.23 0.56 0.35 0.59 0.93 0.96 0.44 0.62 0.32 0.54 0.93 0.96 0.96 0.97\nArt. 5 0.35 0.56 0.39 0.58 0.83 0.89 0.32 0.44 0.20 0.41 0.79 0.86 0.88 0.92\nArt. 6 0.27 0.40 0.26 0.38 0.93 0.96 0.28 0.43 0.18 0.36 0.93 0.96 0.94 0.96\nArt. 7 0.15 0.38 0.30 0.53 0.53 0.72 0.15 0.36 0.29 0.49 0.62 0.75 0.74 0.83\nArt. 8 0.16 0.28 0.18 0.36 0.89 0.93 0.17 0.32 0.13 0.30 0.89 0.94 0.91 0.95\nArt. 9 0.33 0.46 0.32 0.46 0.83 0.89 0.27 0.45 0.27 0.45 0.85 0.92 0.95 0.97\nArt. 10 0.23 0.34 0.24 0.37 0.84 0.90 0.27 0.43 0.21 0.33 0.87 0.91 0.90 0.93\nArt. 11 0.25 0.33 0.27 0.36 0.94 0.96 0.30 0.44 0.23 0.34 0.91 0.94 0.97 0.99\nArt. 13 0.28 0.36 0.32 0.40 0.89 0.94 0.27 0.36 0.26 0.39 0.90 0.94 0.92 0.95\nArt. 14 0.14 0.24 0.15 0.26 0.85 0.91 0.14 0.27 0.07 0.19 0.88 0.92 0.90 0.94\nArt. 34 0.09 0.20 0.08 0.19 0.90 0.93 0.08 0.17 0.06 0.15 0.90 0.94 0.93 0.96\nArt. 35 0.05 0.13 0.06 0.17 0.90 0.94 0.05 0.13 0.05 0.13 0.88 0.93 0.92 0.95\nAverage 0.26 0.40 0.27 0.41 0.86 0.91 0.23 0.38 0.20 0.35 0.86 0.91 0.91 0.94\nTable 8: P@1 and MRR results of the 7 examined PLMs on the 13 article numbers from the ECHR task.\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nClause Type P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nArbitration 0.44 0.65 0.97 0.98 1.00 1.00 0.83 0.91 1.00 1.00 1.00 1.00 1.00 1.00\nAssignments 0.05 0.15 0.34 0.49 0.85 0.89 0.01 0.12 0.40 0.58 0.90 0.94 0.94 0.96\nConﬁdentiality 0.14 0.34 0.73 0.84 0.99 0.99 0.14 0.34 0.67 0.77 0.99 0.99 0.99 0.99\nCosts 0.00 0.22 0.56 0.66 0.78 0.89 0.22 0.38 0.33 0.54 0.56 0.78 0.67 0.80\nDeﬁnitions 1.00 1.00 0.99 0.99 0.78 0.84 0.27 0.53 0.75 0.85 0.78 0.85 0.81 0.87\nDisclosures 0.56 0.70 0.37 0.50 0.80 0.89 0.02 0.16 0.01 0.23 0.65 0.80 0.59 0.77\nEmployment 0.42 0.69 1.00 1.00 0.92 0.96 0.50 0.67 0.65 0.80 0.85 0.92 1.00 1.00\nEnforceability 0.00 0.17 0.26 0.37 0.42 0.64 0.00 0.06 0.25 0.42 0.33 0.54 0.16 0.39\nFees 0.12 0.50 0.52 0.70 0.43 0.62 0.39 0.54 0.38 0.60 0.48 0.67 0.51 0.69\nIndemniﬁcation 0.41 0.59 0.70 0.80 0.92 0.96 0.10 0.34 0.98 0.98 0.96 0.98 0.97 0.98\nLaw 0.00 0.40 0.21 0.57 0.37 0.58 0.87 0.92 0.00 0.16 0.79 0.87 0.78 0.86\nParticipations 0.04 0.20 0.45 0.66 0.82 0.90 0.52 0.67 0.38 0.59 0.80 0.87 0.82 0.89\nRemedies 0.05 0.25 0.16 0.34 0.92 0.96 0.11 0.37 0.52 0.71 0.98 0.99 0.99 0.99\nRepresentations0.01 0.30 0.43 0.62 0.77 0.85 0.17 0.46 0.46 0.64 0.86 0.91 0.80 0.87\nSeverability 0.02 0.17 0.34 0.58 0.99 0.99 0.00 0.16 0.97 0.98 0.98 0.99 0.98 0.99\nSolvency 0.09 0.22 0.38 0.52 0.94 0.97 0.00 0.06 0.11 0.26 0.97 0.99 0.97 0.99\nTaxes 0.29 0.59 0.86 0.90 0.99 0.99 0.24 0.48 0.56 0.68 0.99 0.99 0.99 0.99\nTermination 0.31 0.56 0.60 0.77 0.75 0.85 0.22 0.45 0.84 0.91 0.80 0.89 0.76 0.86\nWaivers 0.12 0.22 0.59 0.67 0.79 0.87 0.00 0.07 0.57 0.74 0.94 0.95 0.84 0.89\nWarranties 0.00 0.14 0.05 0.26 0.08 0.39 0.14 0.33 0.27 0.53 0.05 0.36 0.10 0.41\nAverage 0.20 40.2 0.53 0.66 0.77 0.85 0.24 0.40 0.51 0.65 0.78 0.86 0.78 0.86\nTable 9: P@1 and MRR results of the 7 examined PLMs on the 20 clause types from the Contract Section task.\n15528\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nContract TypeP@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nAward 0.62 0.67 0.62 0.70 1.00 1.00 0.54 0.60 0.62 0.70 1.00 1.00 1.00 1.00\nConsulting 0.03 0.17 0.10 0.23 0.94 0.97 0.08 0.29 0.07 0.17 0.81 0.87 0.90 0.93\nCredit 0.57 0.72 0.37 0.53 0.97 0.98 0.80 0.88 0.55 0.77 0.90 0.95 0.95 0.98\nEmployment 0.40 0.54 0.30 0.44 0.88 0.94 0.63 0.73 0.56 0.72 0.99 0.99 0.96 0.98\nIndemnity 0.08 0.34 0.00 0.16 0.62 0.71 0.00 0.15 0.00 0.11 1.00 1.00 1.00 1.00\nLetter 0.22 0.33 0.24 0.34 0.96 0.98 0.76 0.87 0.18 0.27 0.77 0.88 0.93 0.97\nLicense 0.40 0.62 0.20 0.42 0.63 0.76 0.49 0.70 0.31 0.44 0.69 0.79 0.86 0.91\nLoan 0.51 0.67 0.72 0.84 0.90 0.93 0.72 0.83 0.95 0.97 0.90 0.94 0.87 0.93\nPurchase 0.70 0.83 0.59 0.68 0.70 0.83 0.52 0.68 0.93 0.96 0.89 0.92 0.93 0.94\nSecurity 0.35 0.56 0.70 0.80 0.95 0.97 0.59 0.75 0.35 0.59 0.97 0.99 0.97 0.99\nSeparation 0.12 0.26 0.16 0.28 0.66 0.77 0.15 0.38 0.07 0.21 0.73 0.86 0.71 0.82\nServices 0.24 0.45 0.29 0.48 0.52 0.67 0.05 0.19 0.38 0.54 0.52 0.69 0.52 0.69\nSettlement 0.49 0.63 0.49 0.71 0.70 0.80 0.88 0.93 0.58 0.72 0.53 0.74 0.65 0.80\nSupply 0.09 0.24 0.35 0.51 0.61 0.73 0.09 0.19 0.04 0.14 0.70 0.77 0.65 0.74\nV oting 0.00 0.13 0.03 0.33 1.00 1.00 0.00 0.10 0.00 0.13 0.83 0.91 0.90 0.95\nAverage 0.32 0.48 0.34 0.50 0.80 0.87 0.42 0.55 0.37 0.50 0.82 0.89 0.85 0.91\nTable 10: P@1 and MRR results of the 7 examined PLMs on the 16 types of contracts from the Contract Types\ntask.\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nCrime Charges P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nChildren 0.69 0.78 0.73 0.82 0.47 0.61 0.67 0.78 0.45 0.60 0.73 0.82 0.77 0.85\nComputer 0.36 0.51 0.46 0.62 0.32 0.41 0.42 0.53 0.29 0.40 0.44 0.56 0.51 0.64\nCourt-related 0.55 0.66 0.57 0.69 0.53 0.65 0.61 0.73 0.44 0.58 0.63 0.74 0.67 0.78\nDrug-related 0.40 0.53 0.48 0.60 0.31 0.44 0.35 0.50 0.26 0.38 0.42 0.55 0.46 0.60\nWrongful Life Taking0.50 0.64 0.59 0.72 0.59 0.71 0.58 0.72 0.31 0.47 0.61 0.74 0.63 0.76\nMens Rea 0.56 0.64 0.62 0.69 0.55 0.65 0.68 0.76 0.47 0.59 0.69 0.77 0.75 0.82\nMonetary 0.40 0.51 0.48 0.59 0.52 0.63 0.50 0.63 0.30 0.44 0.53 0.65 0.61 0.72\nPattern of Behavior 0.37 0.50 0.48 0.59 0.41 0.50 0.44 0.57 0.26 0.37 0.52 0.62 0.57 0.68\nProperty 0.25 0.34 0.36 0.43 0.26 0.36 0.32 0.41 0.14 0.22 0.40 0.46 0.42 0.48\nSex-related 0.55 0.65 0.59 0.70 0.47 0.59 0.54 0.66 0.36 0.48 0.60 0.70 0.66 0.75\nViolent 0.46 0.61 0.57 0.70 0.45 0.59 0.54 0.69 0.29 0.45 0.58 0.72 0.65 0.77\nAverage 0.46 0.58 0.54 0.65 0.44 0.56 0.51 0.63 0.33 0.45 0.56 0.67 0.61 0.71\nTable 11: Results on the ‘Crime Charges (US)’ LegalLAMA tasks. Results are clustered in Crime Topics.\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nTopic P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nBusiness law 0.29 0.38 0.37 0.45 0.48 0.59 0.59 0.70 0.35 0.46 0.59 0.71 0.69 0.79\nCriminal law 0.39 0.49 0.46 0.54 0.48 0.58 0.54 0.65 0.32 0.45 0.64 0.73 0.67 0.76\nEmployment law 0.47 0.60 0.58 0.68 0.47 0.60 0.54 0.67 0.41 0.54 0.55 0.67 0.65 0.76\nFamily law 0.52 0.61 0.59 0.67 0.49 0.62 0.66 0.77 0.40 0.52 0.75 0.84 0.82 0.88\nImmigration 0.48 0.57 0.54 0.62 0.58 0.67 0.55 0.65 0.38 0.48 0.65 0.74 0.72 0.80\nLandlord-tenant law0.37 0.46 0.44 0.52 0.64 0.73 0.69 0.77 0.42 0.52 0.75 0.82 0.80 0.86\nBankruptcy 0.37 0.49 0.43 0.55 0.48 0.59 0.49 0.62 0.34 0.47 0.53 0.66 0.59 0.71\nAverage 0.41 0.51 0.49 0.58 0.52 0.63 0.58 0.69 0.37 0.49 0.64 0.74 0.70 0.79\nTable 12: Results on the ‘Terminology (US)’ LegalLAMA task. Results are clustered in Law Topics.\n15529\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nTopic P@1 MRRP@1 MRRP@1 MRRP@1 MRRP@1 MRRP@1 MRRP@1 MRR\nAccession 0.32 0.45 0.57 0.68 0.93 0.95 0.46 0.55 0.87 0.90 0.80 0.88 0.80 0.89\nAdministrative cooperation 0.15 0.33 0.23 0.40 0.53 0.69 0.12 0.27 0.19 0.32 0.65 0.79 0.82 0.89\nApproximation of laws 0.46 0.54 0.54 0.58 0.36 0.47 0.18 0.32 0.08 0.23 0.67 0.73 0.72 0.79\nArea of freedom, security and justice0.14 0.27 0.13 0.28 0.11 0.24 0.14 0.28 0.11 0.25 0.13 0.27 0.19 0.34\nCitizenship of the union 0.40 0.60 0.47 0.64 0.26 0.45 0.12 0.30 0.31 0.47 0.50 0.70 0.53 0.72\nCompetition 0.50 0.68 0.75 0.80 0.84 0.90 0.52 0.62 0.52 0.62 0.88 0.89 0.88 0.89\nConsumer protection 0.40 0.57 0.50 0.62 0.45 0.58 0.28 0.42 0.20 0.37 0.25 0.42 0.40 0.54\nData protection 0.47 0.63 0.61 0.73 0.64 0.75 0.17 0.28 0.20 0.35 0.66 0.76 0.73 0.82\nExternal relations 0.30 0.45 0.40 0.61 0.38 0.55 0.19 0.29 0.09 0.22 0.40 0.61 0.55 0.68\nFree movement of capital 0.42 0.45 0.42 0.45 0.18 0.38 0.11 0.26 0.08 0.22 0.33 0.53 0.33 0.59\nFree movement of goods 0.25 0.37 0.25 0.35 0.32 0.48 0.21 0.34 0.18 0.31 0.62 0.74 0.38 0.58\nFreedom of establishment 0.22 0.34 0.42 0.50 0.64 0.75 0.33 0.43 0.29 0.40 0.81 0.88 0.94 0.95\nFreedom of movement for workers0.22 0.34 0.35 0.41 0.19 0.35 0.12 0.23 0.11 0.22 0.43 0.56 0.38 0.55\nFreedom to provide services 0.07 0.20 0.04 0.23 0.23 0.40 0.10 0.24 0.15 0.29 0.39 0.58 0.54 0.67\nFundamental rights 0.60 0.73 0.69 0.81 0.89 0.93 0.26 0.37 0.22 0.36 0.84 0.90 0.83 0.89\nInternal market 0.00 0.24 0.20 0.40 0.94 0.96 0.26 0.36 0.40 0.55 0.40 0.62 0.70 0.77\nNon-contractual liability 0.09 0.19 0.09 0.20 0.19 0.35 0.19 0.40 0.10 0.23 0.30 0.49 0.55 0.70\nNon-discrimination 0.00 0.24 0.00 0.25 0.50 0.68 0.29 0.48 0.10 0.26 0.67 0.83 0.33 0.67\nPrivileges and immunities 0.17 0.27 0.12 0.24 0.63 0.77 0.25 0.36 0.20 0.35 0.81 0.88 0.81 0.87\nProcedural provisions 0.53 0.66 0.63 0.75 0.68 0.80 0.61 0.73 0.42 0.56 0.71 0.82 0.75 0.84\nPublic health 0.62 0.80 0.50 0.72 0.68 0.79 0.38 0.58 0.28 0.48 0.54 0.75 0.92 0.96\nSafeguard measures 0.50 0.52 0.50 0.58 0.64 0.76 0.31 0.39 0.42 0.52 0.75 0.88 1.00 1.00\nSocial policy 0.75 0.78 0.75 0.81 0.42 0.54 0.22 0.37 0.15 0.32 0.75 0.83 1.00 1.00\nAverage 0.34 0.47 0.40 0.53 0.51 0.64 0.25 0.39 0.25 0.38 0.60 0.72 0.67 0.77\nTable 13: Results on the ‘Terminology (EU)’ LegalLAMA task. Results are clustered in Law Topics.\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nArticle P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\nArt. 2 0.46 0.57 0.52 0.63 0.72 0.82 0.37 0.51 0.36 0.47 0.80 0.87 0.90 0.94\nArt. 3 0.51 0.61 0.58 0.69 0.80 0.87 0.40 0.54 0.34 0.45 0.83 0.90 0.89 0.93\nArt. 5 0.39 0.51 0.46 0.57 0.56 0.69 0.36 0.48 0.25 0.38 0.63 0.75 0.74 0.83\nArt. 6 0.42 0.55 0.49 0.62 0.68 0.77 0.43 0.55 0.36 0.49 0.77 0.85 0.82 0.89\nArt. 7 0.71 0.78 0.82 0.86 0.89 0.93 0.36 0.59 0.44 0.52 0.88 0.93 0.91 0.94\nArt. 8 0.35 0.47 0.45 0.56 0.62 0.71 0.29 0.41 0.26 0.36 0.73 0.82 0.84 0.90\nArt. 9 0.49 0.57 0.56 0.64 0.67 0.76 0.43 0.53 0.33 0.44 0.79 0.86 0.85 0.91\nArt. 10 0.30 0.43 0.41 0.52 0.57 0.69 0.25 0.37 0.20 0.31 0.73 0.82 0.84 0.90\nArt. 11 0.32 0.44 0.42 0.52 0.66 0.75 0.29 0.40 0.23 0.34 0.74 0.84 0.87 0.92\nArt. 13 0.44 0.61 0.55 0.69 0.78 0.86 0.38 0.56 0.27 0.45 0.86 0.90 0.91 0.94\nArt. 14 0.72 0.80 0.79 0.85 0.80 0.86 0.69 0.78 0.52 0.63 0.84 0.89 0.91 0.94\nArt. 35 0.14 0.21 0.18 0.24 0.61 0.71 0.14 0.26 0.09 0.18 0.78 0.85 0.89 0.93\nAverage 0.43 0.54 0.51 0.61 0.69 0.79 0.36 0.49 0.30 0.41 0.78 0.86 0.86 0.91\nTable 14: Results on the ‘Terminology (CoE)’ LegalLAMA task. Results are clustered by Article.\n15530\nRoBERTa-B RoBERTa-L LegalBERT CL-BERT PoL-BERT LexLM-B LexLM-L\nSection P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR\n16 0.00 0.08 0.00 0.04 0.00 0.04 0.00 0.10 0.00 0.08 0.50 0.62 1.00 1.00\n21 0.23 0.41 0.37 0.47 0.46 0.56 0.43 0.56 0.44 0.55 0.94 0.96 0.97 0.99\n85 0.46 0.51 0.31 0.42 0.30 0.41 0.29 0.37 0.30 0.39 0.40 0.52 0.57 0.69\n86 0.38 0.53 0.38 0.50 0.50 0.62 0.50 0.54 0.50 0.53 0.50 0.71 0.50 0.66\n87 0.75 0.78 0.50 0.62 0.75 0.79 0.50 0.65 0.75 0.82 0.75 0.83 0.75 0.80\n88.23 0.25 0.34 0.33 0.38 0.33 0.38 0.33 0.39 0.33 0.42 0.33 0.40 0.33 0.38\n95 0.48 0.54 0.52 0.56 0.52 0.55 0.46 0.52 0.45 0.49 0.79 0.84 0.80 0.85\n122 0.17 0.19 0.11 0.15 0.17 0.18 0.12 0.15 0.12 0.16 0.50 0.67 0.83 0.86\n145 0.25 0.38 0.25 0.40 0.44 0.51 0.38 0.50 0.50 0.55 0.62 0.71 0.88 0.90\n151 0.59 0.61 0.89 0.91 0.62 0.64 0.04 0.34 0.02 0.32 0.91 0.92 0.91 0.92\n152 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.50 0.75 1.00 1.00 1.00 1.00\n163 0.50 0.52 0.50 0.51 0.50 0.51 0.50 0.51 0.50 0.52 0.50 0.75 1.00 1.00\n163.1 0.33 0.40 0.44 0.57 0.67 0.68 0.33 0.51 0.33 0.46 1.00 1.00 1.00 1.00\n231 0.25 0.29 0.38 0.54 0.62 0.65 0.44 0.51 0.56 0.59 0.94 0.94 1.00 1.00\n249 0.40 0.45 0.33 0.41 0.60 0.68 0.66 0.74 0.53 0.66 0.87 0.91 0.88 0.90\n254 0.50 0.61 0.65 0.73 0.50 0.58 0.40 0.52 0.50 0.59 0.75 0.85 0.85 0.92\n264 0.67 0.67 0.50 0.56 0.50 0.59 0.42 0.51 0.25 0.38 0.92 0.96 1.00 1.00\n267.12 0.33 0.53 0.33 0.44 0.67 0.77 0.75 0.84 0.75 0.80 1.00 1.00 1.00 1.00\n267.5 0.67 0.78 0.75 0.85 0.83 0.90 0.67 0.76 0.50 0.62 1.00 1.00 1.00 1.00\n267.8 0.47 0.54 0.56 0.62 0.66 0.69 0.56 0.63 0.60 0.66 0.83 0.87 0.83 0.88\n268 0.45 0.54 0.25 0.41 0.35 0.44 0.35 0.44 0.40 0.49 0.50 0.65 0.75 0.86\n279 0.83 0.86 0.92 0.92 0.75 0.81 0.83 0.88 0.83 0.86 1.00 1.00 0.92 0.96\n380 0.24 0.35 0.24 0.36 0.39 0.47 0.47 0.53 0.35 0.48 0.78 0.80 0.71 0.73\n462.37 0.40 0.49 0.40 0.52 0.65 0.69 0.67 0.70 0.65 0.69 0.78 0.80 0.81 0.87\n465 0.50 0.63 0.75 0.76 0.50 0.63 0.38 0.54 0.75 0.75 1.00 1.00 1.00 1.00\n467.1 0.29 0.41 0.57 0.75 0.67 0.76 0.33 0.64 0.58 0.70 1.00 1.00 1.00 1.00\n495 0.32 0.40 0.32 0.46 0.60 0.66 0.56 0.61 0.60 0.65 0.77 0.87 0.87 0.92\n530 0.00 0.01 0.00 0.01 0.00 0.01 0.00 0.02 0.00 0.03 1.00 1.00 1.00 1.00\n591 0.13 0.31 0.25 0.36 0.61 0.71 0.52 0.58 0.51 0.60 0.87 0.93 0.87 0.92\n601 0.58 0.62 0.58 0.64 0.86 0.89 0.79 0.81 0.29 0.49 0.86 0.93 0.86 0.93\n650 0.64 0.70 0.72 0.77 0.78 0.80 0.69 0.74 0.75 0.76 0.97 0.99 0.97 0.99\n672.73 0.25 0.30 0.25 0.32 0.33 0.34 0.33 0.34 0.33 0.38 0.67 0.71 1.00 1.00\n672.78 0.27 0.34 0.34 0.43 0.42 0.46 0.50 0.55 0.42 0.48 0.83 0.92 1.00 1.00\n676 0.14 0.29 0.14 0.27 0.50 0.62 0.57 0.66 0.36 0.55 0.93 0.94 1.00 1.00\n683 0.11 0.26 0.18 0.30 0.48 0.52 0.52 0.57 0.48 0.54 0.81 0.88 0.90 0.94\n684 0.35 0.43 0.60 0.72 0.25 0.51 0.25 0.34 0.25 0.27 1.00 1.00 1.00 1.00\n686 0.21 0.28 0.28 0.36 0.57 0.65 0.65 0.68 0.43 0.55 0.68 0.79 0.94 0.96\n687 0.20 0.30 0.30 0.49 0.62 0.64 0.38 0.51 0.50 0.53 0.88 0.94 0.75 0.83\n715.1 0.12 0.25 0.12 0.22 0.33 0.50 0.33 0.45 0.50 0.56 1.00 1.00 1.00 1.00\n718.1 0.17 0.26 0.08 0.24 0.67 0.67 0.67 0.67 0.33 0.48 1.00 1.00 1.00 1.00\n718.2 0.20 0.30 0.17 0.31 0.52 0.59 0.52 0.57 0.59 0.64 0.76 0.85 0.87 0.92\n784 0.20 0.29 0.30 0.49 0.50 0.52 0.38 0.46 0.50 0.52 1.00 1.00 1.00 1.00\n839 0.17 0.25 0.07 0.20 0.33 0.37 0.33 0.36 0.33 0.35 1.00 1.00 1.00 1.00\nAverage 0.36 0.45 0.40 0.50 0.53 0.59 0.45 0.54 0.46 0.53 0.77 0.83 0.86 0.90\nTable 15: Results on the ‘Criminal Code Sections (Canada)’ L egalLAMA task. We kept only the sections with\nmore than one example.\n15531\nECHR Article Masked Terms\nArt. 2 ‘accessibility’ , ‘eﬀective investigation’ , ‘expulsion’ , ‘extradition’ , ‘foreseeability’ , ‘positive obligations’ ,\n‘prescribed by law’ , ‘right to life’ , ‘safeguards against abuse’ , ‘use of force’\nArt. 3 ‘eﬀective investigation’ , ‘expulsion’ , ‘extradition’ , ‘inhuman punishment’ , ‘inhuman treatment’ , ‘positive\nobligations’ , ‘prohibition of torture’ , ‘torture’\nArt. 5 ‘competent court’ , ‘deprivation of liberty’ , ‘drug addicts’ , ‘educational supervision’ , ‘expulsion’ , ‘extradition’\n, ‘guarantees to appear for trial’ , ‘lawful arrest or detention’ , ‘lawful order of a court’ , ‘length of pre-trial\ndetention’ , ‘minors’ , ‘order release’ , ‘persons of unsound mind’ , ‘procedure prescribed by law’ , ‘reasonable\nsuspicion’ , ‘release pending trial’ , ‘review by a court’ , ‘right to liberty and security’ , ‘security of person’ ,\n‘speediness of review’ , ‘take proceedings’ , ‘trial within a reasonable time’\nArt. 6 ‘charged with a criminal oﬀence’ , ‘disciplinary proceedings’ , ‘enforcement proceedings’ , ‘equality of arms’\n, ‘examination of witnesses’ , ‘exclusion of public’ , ‘expulsion’ , ‘extradition’ , ‘fair hearing’ , ‘free legal\nassistance’ , ‘impartial tribunal’ , ‘independent tribunal’ , ‘insuﬃcient means’ , ‘legal aid’ , ‘national security’ ,\n‘necessary in a democratic society’ , ‘oral hearing’ , ‘presumption of innocence’ , ‘protection of public order’ ,\n‘proved guilty according to law’ , ‘public hearing’ , ‘public judgment’ , ‘reasonable time’ , ‘right to a fair trial’ ,\n‘rights of defence’ , ‘same conditions’ , ‘tribunal established by law’\nArt. 7 ‘criminal oﬀence’ , ‘heavier penalty’ , ‘retroactivity’\nArt. 8 ‘accessibility’ , ‘economic well-being of the country’ , ‘expulsion’ , ‘extradition’ , ‘foreseeability’ , ‘interference’\n, ‘national security’ , ‘necessary in a democratic society’ , ‘positive obligations’ , ‘prevention of crime’ ,\n‘prevention of disorder’ , ‘protection of health’ , ‘protection of morals’ , ‘protection of the rights and freedoms of\nothers’ , ‘public authority’ , ‘public safety’ , ‘respect for correspondence’ , ‘respect for family life’ , ‘respect for\nhome’ , ‘respect for private life’ , ‘right to respect for private and family life’ , ‘safeguards against abuse’\nArt. 9 ‘foreseeability’ , ‘freedom of conscience’ , ‘freedom of religion’ , ‘freedom of thought’ , ‘interference’ ,\n‘necessary in a democratic society’ , ‘observance’ , ‘positive obligations’ , ‘practice’ , ‘prescribed by law’ ,\n‘protection of health’ , ‘protection of public order’ , ‘protection of the rights and freedoms of others’ , ‘public\nsafety’ , ‘safeguards against abuse’ , ‘teaching’ , ‘worship’\nArt. 10 ‘duties and responsibilities’ , ‘foreseeability’ , ‘freedom of expression’ , ‘freedom to hold opinions’ , ‘freedom\nto impart information’ , ‘freedom to receive information’ , ‘interference’ , ‘national security’ , ‘necessary in a\ndemocratic society’ , ‘positive obligations’ , ‘prescribed by law’ , ‘prevention of crime’ , ‘prevention of disorder’\n, ‘protection of health’ , ‘protection of morals’ , ‘protection of the reputation of others’ , ‘protection of the rights\nof others’ , ‘public safety’ , ‘safeguards against abuse’ , ‘territorial integrity’\nArt. 11 ‘accessibility’ , ‘foreseeability’ , ‘form and join trade unions’ , ‘freedom of assembly and association’ , ‘freedom\nof association’ , ‘freedom of peaceful assembly’ , ‘interference’ , ‘national security’ , ‘necessary in a democratic\nsociety’ , ‘positive obligations’ , ‘prescribed by law’ , ‘prevention of crime’ , ‘prevention of disorder’ , ‘protection\nof health’ , ‘public safety’\nArt. 13 ‘eﬀective remedy’ , ‘national authority’ , ‘right to an eﬀective remedy’\nArt. 14 ‘discrimination’ , ‘language’ , ‘national minority’ , ‘national origin’ , ‘objective and reasonable justiﬁcation’ ,\n‘prohibition of discrimination’ , ‘property’ , ‘race’ , ‘religion’ , ‘sex’ , ‘social origin’\nArt. 35 ‘continuing situation’ , ‘eﬀective domestic remedy’ , ‘exhaustion of domestic remedies’ , ‘ﬁnal domestic decision’\n, ‘manifestly ill-founded’ , ‘no signiﬁcant disadvantage’ , ‘relevant new information’\nArt. P1-1 ‘accessibility’ , ‘deprivation of property’ , ‘foreseeability’ , ‘general interest’ , ‘general principles of international\nlaw’ , ‘interference’ , ‘peaceful enjoyment of possessions’ , ‘positive obligations’ , ‘possessions’ , ‘prescribed by\nlaw’ , ‘protection of property’ , ‘secure the payment of taxes’\nTable 16: Masked Terms used in the ‘Terminology (CoE)’ LegalLAMA task.\n15532\nCrime Area Masked Terms\nChildren ‘child abandonment’ , ‘child abuse’\nComputer ‘computer crime’ , ‘cyberbullying’ , ‘identity theft’\nCourt-related ‘criminal contempt of court’ , ‘perjury’ , ‘probation violation’\nDrug-related ‘drug distribution’ , ‘drug manufacturing’ , ‘drug possession’ , ‘drug traﬃcking’ , ‘medical marijuana’ , ‘minor\nin possession’ , ‘public intoxication’\nLife Taking ‘homicide’ , ‘manslaughter’ , ‘murder’\nMens Rea ‘accessory’ , ‘aiding and abetting’ , ‘attempt’ , ‘conspiracy’ , ‘hate crime’\nMonetary ‘bribery’ , ‘embezzlement’ , ‘extortion’ , ‘forgery’ , ‘insurance fraud’ , ‘money laundering’ , ‘pyramid schemes’\n, ‘racketeering’ , ‘securities fraud’ , ‘shoplifting’ , ‘tax evasion’ , ‘telemarketing fraud’ , ‘theft’ , ‘white collar\ncrime’ , ‘wire fraud’\nBehavior ‘disorderly conduct’ , ‘disturbing the peace’ , ‘harassment’ , ‘stalking’\nProperty ‘arson’ , ‘vandalism’\nSex-related ‘child pornography’ , ‘indecent exposure’ , ‘prostitution’ , ‘rape’ , ‘sexual assault’ , ‘solicitation’ , ‘statutory\nrape’\nViolence ‘aggravated assault’ , ‘battery’ , ‘burglary’ , ‘domestic violence’ , ‘kidnapping’ , ‘robbery’\nTable 17: Masked Terms used in the ‘Crime Charges (US)’ LegalLAMA task grouped by crime areas.\nLegal Topic Masked Terms\nBusiness Law ‘adhesion contract’ , ‘implied warranty’ , ‘limited liability’ , ‘parol evidence’ , ‘quantum meruit’ , ‘reliance\ndamages’ , ‘self-dealing’ , ‘severability clause’ , ‘speciﬁc performance’ , ‘statute of frauds’ , ‘substantial\nperformance’ , ‘tender oﬀer’ , ‘third-party beneﬁciary’ , ‘unconscionability’\nCriminal Law\nand Procedure\n‘accessory before the fact’ , ‘accomplice’ , ‘aggravated assault’ , ‘allocution’ , ‘arson’ , ‘defense of others’ ,\n‘inchoate’ , ‘merger doctrine’ , ‘mitigating circumstances’ , ‘money laundering’ , ‘stop and frisk’\nEmployment\nLaw\n‘bargaining unit’ , ‘boycott’ , ‘casual labor’ , ‘industrial safety’ , ‘minimum wage’ , ‘workplace safety’ , ‘wrongful\ntermination’\nFamily Law ‘consent divorce’ , ‘emancipation of minors’ , ‘marital privilege’ , ‘marital property’ , ‘marital settlement\nagreement’ , ‘separate property’ , ‘separation agreement’ , ‘shared custody’ , ‘sole custody’ , ‘spousal privilege’ ,\n‘spousal support’ , ‘visitation’ , ‘wage attachment’\nImmigration ‘alienage’ , ‘asylum seeker’ , ‘asylum’ , ‘childhood arrivals’ , ‘citizenship’ , ‘deferred action’ , ‘deportation’ ,\n‘geneva conventions’ , ‘naturalization’ , ‘nonresident’ , ‘refugee’ , ‘resettlement’ , ‘visa’\nLandlord-\nTenant Law\n‘abandonment’ , ‘commercial reasonability’ , ‘constructive eviction’ , ‘eviction’ , ‘habitability’ , ‘privity’ , ‘quiet\nenjoyment’ , ‘reasonableness’ , ‘self-help eviction’ , ‘sole discretion’ , ‘tenancy at suﬀerance’ , ‘tenancy at will’\nMoney And\nFinancial\nProblems\n‘bankruptcy discharge’ , ‘bond’ , ‘consumer credit’ , ‘kiting’ , ‘malfeasance’ , ‘mortgage’ , ‘nonrecourse’ , ‘ponzi\nscheme’ , ‘securities fraud’ , ‘self-dealing’ , ‘senior lien’ , ‘stock dividend’ , ‘straw man’ , ‘swindle’ , ‘tontine’ ,\n‘variable annuity’\nTable 18: Masked Terms used in the ‘Terminology (US)’ LegalLAMA task grouped by legal topics.\n15533\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLeft blank.\n□\u0013 A2. Did you discuss any potential risks of your work?\nLeft blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSections 2-3\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 2\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15534\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n15535",
  "topic": "Multinational corporation",
  "concepts": [
    {
      "name": "Multinational corporation",
      "score": 0.7149235606193542
    },
    {
      "name": "Upstream (networking)",
      "score": 0.7084091901779175
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6314675807952881
    },
    {
      "name": "Computer science",
      "score": 0.5976830720901489
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.5148600935935974
    },
    {
      "name": "Best practice",
      "score": 0.4633539021015167
    },
    {
      "name": "Natural language processing",
      "score": 0.4358541667461395
    },
    {
      "name": "Work (physics)",
      "score": 0.43584853410720825
    },
    {
      "name": "Knowledge management",
      "score": 0.38489317893981934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37102800607681274
    },
    {
      "name": "Engineering",
      "score": 0.18072175979614258
    },
    {
      "name": "Political science",
      "score": 0.13742002844810486
    },
    {
      "name": "Operations management",
      "score": 0.1192575991153717
    },
    {
      "name": "Law",
      "score": 0.09018203616142273
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}