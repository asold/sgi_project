{
  "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
  "url": "https://openalex.org/W2767321762",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2229690813",
      "name": "Yang Zhilin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352997497",
      "name": "Dai, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177323215",
      "name": "Salakhutdinov, Ruslan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578411",
      "name": "Cohen, William W.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1593114658",
    "https://openalex.org/W2408607167",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2396566817",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W1604264128",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2605246398",
    "https://openalex.org/W1884859883",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2313180542",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W1993750641",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1850742715",
    "https://openalex.org/W2951244767",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2411934291",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2132339004"
  ],
  "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",
  "full_text": "Published as a conference paper at ICLR 2018\nBREAKING THE SOFTMAX BOTTLENECK :\nA HIGH -RANK RNN L ANGUAGE MODEL\nZhilin Yang∗, Zihang Dai∗, Ruslan Salakhutdinov, William W. Cohen\nSchool of Computer Science\nCarnegie Mellon University\n{zhiliny,dzihang,rsalakhu,wcohen}@cs.cmu.edu\nABSTRACT\nWe formulate language modeling as a matrix factorization problem, and show\nthat the expressiveness of Softmax-based models (including the majority of neu-\nral language models) is limited by a Softmax bottleneck. Given that natural lan-\nguage is highly context-dependent, this further implies that in practice Softmax\nwith distributed word embeddings does not have enough capacity to model nat-\nural language. We propose a simple and effective method to address this issue,\nand improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to\n47.69 and 40.68 respectively. The proposed method also excels on the large-scale\n1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.1\n1 I NTRODUCTION\nAs a fundamental task in natural language processing, statistical language modeling has gone\nthrough signiﬁcant development from traditional Ngram language models to neural language mod-\nels in the last decade (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2010). Despite\nthe huge variety of models, as a density estimation problem, language modeling mostly relies on a\nuniversal auto-regressive factorization of the joint probability and then models each conditional fac-\ntor using different approaches. Speciﬁcally, given a corpus of tokens X = (X1,...,X T), the joint\nprobability P(X) factorizes as P(X) = ∏\ntP(Xt |X<t) = ∏\ntP(Xt |Ct),where Ct = X<t is\nreferred to as the context of the conditional probability hereafter.\nBased on the factorization, recurrent neural networks (RNN) based language models achieve state-\nof-the-art results on various benchmarks (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017).\nA standard approach is to use a recurrent network to encode the context into a ﬁxed size vector,\nwhich is then multiplied by the word embeddings (Inan et al., 2016; Press & Wolf, 2017) using dot\nproduct to obtain the logits. The logits are consumed by the Softmax function to give a categorical\nprobability distribution over the next token. In spite of the expressiveness of RNNs as universal\napproximators (Schäfer & Zimmermann, 2006), an unclear question is whether the combination\nof dot product and Softmax is capable of modeling the conditional probability, which can vary\ndramatically with the change of the context.\nIn this work, we study the expressiveness of the aforementioned Softmax-based recurrent language\nmodels from a perspective of matrix factorization. We show that learning a Softmax-based recurrent\nlanguage model with the standard formulation is essentially equivalent to solving a matrix factoriza-\ntion problem. More importantly, due to the fact that natural language is highly context-dependent,\nthe matrix to be factorized can be high-rank. This further implies that standard Softmax-based lan-\nguage models with distributed (output) word embeddings do not have enough capacity to model\nnatural language. We call this the Softmax bottleneck.\nWe propose a simple and effective method to address the Softmax bottleneck. Speciﬁcally, we\nintroduce discrete latent variables into a recurrent language model, and formulate the next-token\nprobability distribution as a Mixture of Softmaxes (MoS). Mixture of Softmaxes is more expressive\nthan Softmax and other surrogates considered in prior work. Moreover, we show that MoS learns\n∗Equal contribution. Ordering determined by dice rolling.\n1Code is available at https://github.com/zihangdai/mos.\n1\narXiv:1711.03953v4  [cs.CL]  2 Mar 2018\nPublished as a conference paper at ICLR 2018\nmatrices that have much larger normalized singular values and thus much higher rank than Softmax\nand other baselines on real-world datasets.\nWe evaluate our proposed approach on standard language modeling benchmarks. MoS substantially\nimproves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of\nperplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further\napply MoS to a dialog dataset and show improved performance over Softmax and other baselines.\nOur contribution is two-fold. First, we identify the Softmax bottleneck by formulating language\nmodeling as a matrix factorization problem. Second, we propose a simple and effective method that\nsubstantially improves over the current state-of-the-art results.\n2 L ANGUAGE MODELING AS MATRIX FACTORIZATION\nAs discussed in Section 1, with the autoregressive factorization, language modeling can be reduced\nto modeling the conditional distribution of the next tokenxgiven the contextc. Though one might ar-\ngue that a natural language allows an inﬁnite number of contexts due to its compositionality (Pinker,\n1994), we proceed with our analysis by considering a ﬁnite set of possible contexts. The unbound-\nedness of natural language does not affect our conclusions, which will be discussed later.\nWe consider a natural language as a ﬁnite set of pairs of a context and its conditional next-token\ndistribution2 L = {(c1,P∗(X|c1)),··· ,(cN,P∗(X|cN))}, where N is the number of possible\ncontexts. We assume P∗ > 0 everywhere to account for errors and ﬂexibility in natural language.\nLet {x1,x2,··· ,xM}denote a set of M possible tokens in the language L. The objective of a\nlanguage model is to learn a model distribution Pθ(X|C) parameterized by θto match the true data\ndistribution P∗(X|C).\nIn this work, we study the expressiveness of the parametric model class Pθ(X|C). In other words,\nwe are asking the following question: given a natural language L, does there exist a parameter θ\nsuch that Pθ(X|c) = P∗(X|c) for all cin L?\nWe start by looking at a Softmax-based model class since it is widely used.\n2.1 S OFTMAX\nThe majority of parametric language models use a Softmax function operating on a context vector\n(or hidden state) hc and a word embedding wx to deﬁne the conditional distribution Pθ(x|c). More\nspeciﬁcally, the model distribution is usually written as\nPθ(x|c) = exp h⊤\nc wx∑\nx′ exp h⊤c wx′\n(1)\nwhere hc is a function of c, and wx is a function of x. Both functions are parameterized by θ. Both\nthe context vector hc and the word embedding wx have the same dimension d. The dot product\nh⊤\nc wx is called a logit.\nTo help discuss the expressiveness of Softmax, we deﬁne three matrices:\nHθ =\n\n\nh⊤\nc1\nh⊤\nc2\n···\nh⊤\ncN\n\n; Wθ =\n\n\nw⊤\nx1\nw⊤\nx2\n···\nw⊤\nxM\n\n; A =\n\n\nlog P∗(x1|c1), log P∗(x2|c1) ··· log P∗(xM|c1)\nlog P∗(x1|c2), log P∗(x2|c2) ··· log P∗(xM|c2)\n... ... ... ...\nlog P∗(x1|cN), log P∗(x2|cN) ··· log P∗(xM|cN)\n\n\nwhere Hθ ∈RN×d, Wθ ∈RM×d, A ∈RN×M, and the rows of Hθ, Wθ, and A correspond to\ncontext vectors, word embeddings, and log probabilities of the true data distribution respectively.\nWe use the subscriptθbecause (Hθ,Wθ) is effectively a function indexed by the parameterθ, from\nthe joint function family U. Concretely, Hθ is implemented as deep neural networks, such as a\nrecurrent network, while Wθ is instantiated as an embedding lookup.\nWe further specify a set of matrices formed by applying row-wise shift to A\nF(A) = {A + ΛJN,M|Λ is diagonal and Λ ∈RN×N},\n2We use capital letters for variables and small letters for constants.\n2\nPublished as a conference paper at ICLR 2018\nwhere JN,M is an all-ones matrix with size N ×M. Essentially, the row-wise shift operation adds\nan arbitrary real number to each row of A. Thus, F(A) is an inﬁnite set. Notably, the set F(A) has\ntwo important properties (see Appendix A for the proof), which are key to our analysis.\nProperty 1. For any matrix A′, A′ ∈F(A) if and only if Softmax (A′) = P∗. In other words,\nF(A) deﬁnes the set of all possible logits that correspond to the true data distribution.\nProperty 2. For any A1 ̸= A2 ∈F(A), |rank(A1) −rank(A2)|≤ 1. In other words, all matrices\nin F(A) have similar ranks, with the maximum rank difference being 1.\nBased on the Property 1 of F(A), we immediately have the following Lemma.\nLemma 1. Given a model parameter θ, HθW⊤\nθ ∈F(A) if and only if Pθ(X|c) = P∗(X|c) for all\ncin L.\nNow the expressiveness question becomes: does there exist a parameterθand A′∈F(A) such that\nHθW⊤\nθ = A′.\nThis is essentially a matrix factorization problem. We want the model to learn matricesHθ and Wθ\nthat are able to factorize some matrix A′∈F(A). First, note that for a valid factorization to exist,\nthe rank of HθW⊤\nθ has to be at least as large as the rank of A′. Further, since Hθ ∈RN×d and\nWθ ∈RM×d, the rank of HθW⊤\nθ is strictly upper bounded by the embedding size d. As a result,\nif d≥rank(A′), a universal approximator can theoretically recover A′. However, if d< rank(A′),\nno matter how expressive the function family Uis, no (Hθ,Wθ) can even theoretically recover A′.\nWe summarize the reasoning above as follows (see Appendix A for the proof).\nProposition 1. Given that the function family Uis a universal approximator, there exists a param-\neter θsuch that Pθ(X|c) = P∗(X|c) for all cin Lif and only if d≥minA′∈F(A) rank(A′).\nCombining Proposition 1 with the Property 2 of F(A), we are now able to state the Softmax Bottle-\nneck problem formally.\nCorollary 1. (Softmax Bottleneck)If d <rank(A) −1, for any function family Uand any model\nparameter θ, there exists a context cin Lsuch that Pθ(X|c) ̸= P∗(X|c).\nThe above corollary indicates that when the dimension dis too small, Softmax does not have the\ncapacity to express the true data distribution. Clearly, this conclusion is not restricted to a ﬁnite\nlanguage L. When Lis inﬁnite, one can always take a ﬁnite subset and the Softmax bottleneck still\nexists. Next, we discuss why the Softmax bottleneck is an issue by presenting our hypothesis that\nA is high-rank for natural language.\n2.2 H YPOTHESIS : N ATURAL LANGUAGE IS HIGH -RANK\nWe hypothesize that for a natural language L, the log probability matrix A is a high-rank matrix. It\nis difﬁcult (if possible) to rigorously prove this hypothesis since we do not have access to the true\ndata distribution of a natural language. However, it is suggested by the following intuitive reasoning\nand empirical observations:\n• Natural language is highly context-dependent (Mikolov & Zweig, 2012). For example, the token\n“north” is likely to be followed by “korea” or “korean” in a news article on international politics,\nwhich however is unlikely in a textbook on U.S. domestic history. We hypothesize that such\nsubtle context dependency should result in a high-rank matrix A.\n• If A is low-rank, it means humans only need a limited number (e.g. a few hundred) of bases,\nand all semantic meanings can be created by (potentially) negating and (weighted) averaging\nthese bases. However, it is hard to ﬁnd a natural concept in linguistics and cognitive science that\ncorresponds to such bases, which questions the existence of such bases. For example, semantic\nmeanings might not be those bases since a few hundred meanings may not be enough to cover\neveryday meanings, not to mention niche meanings in specialized domains.\n• Empirically, our high-rank language model outperforms conventional low-rank language models\non several benchmarks, as shown in Section 3. We also provide evidences in Section 3.3 to\nsupport our hypothesis that learning a high-rank language model is important.\n3\nPublished as a conference paper at ICLR 2018\nGiven the hypothesis that natural language is high-rank, it is clear that the Softmax bottleneck limits\nthe expressiveness of the models. In practice, the embedding dimension dis usually set at the scale\nof 102, while the rank of A can possibly be as high as M (at the scale of 105), which is orders of\nmagnitude larger than d. Softmax is effectively learning a low-rank approximation to A, and our\nexperiments suggest that such approximation loses the ability to model context dependency, both\nqualitatively and quantitatively (Cf. Section 3).\n2.3 E ASY FIXES ?\nIdentifying the Softmax bottleneck immediately suggests some possible “easy ﬁxes”. First, as con-\nsidered by a lot of prior work, one can employ a non-parametric model, namely an Ngram model\n(Kneser & Ney, 1995). Ngram models are not constrained by any parametric forms so it can univer-\nsally approximate any natural language, given enough parameters. Second, it is possible to increase\nthe dimension d(e.g., to match M) so that the model can express a high-rank matrix A.\nHowever, these two methods increase the number of parameters dramatically, compared to using\na low-dimensional Softmax. More speciﬁcally, an Ngram needs (N ×M) parameters in order to\nexpress A, where Nis potentially unbounded. Similarly, a high-dimensional Softmax requires(M×\nM) parameters for the word embeddings. Increasing the number of model parameters easily leads\nto overﬁtting. In past work, Kneser & Ney (1995) used back-off to alleviate overﬁtting. Moreover,\nas deep learning models were tuned by extensive hyper-parameter search, increasing the dimension\ndbeyond several hundred is not helpful3 (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017).\nClearly there is a tradeoff between expressiveness and generalization on language modeling. Naively\nincreasing the expressiveness hurts generalization. Below, we introduce an alternative approach that\nincreases the expressiveness without exploding the parametric space.\n2.4 M IXTURE OF SOFTMAXES : A H IGH -RANK LANGUAGE MODEL\nWe propose a high-rank language model called Mixture of Softmaxes (MoS) to alleviate the Softmax\nbottleneck issue. MoS formulates the conditional distribution as\nPθ(x|c) =\nK∑\nk=1\nπc,k\nexp h⊤\nc,kwx\n∑\nx′ exp h⊤\nc,kwx′\n; s.t.\nK∑\nk=1\nπc,k = 1\nwhere πc,k is the prior or mixture weight of the k-th component, and hc,k is the k-th context vec-\ntor associated with context c. In other words, MoS computes K Softmax distributions and uses a\nweighted average of them as the next-token probability distribution. Similar to prior work on re-\ncurrent language modeling (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017), we ﬁrst\napply a stack of recurrent layers on top of X to obtain a sequence of hidden states (g1,··· ,gT).\nThe prior and the context vector for context ct are parameterized as πct,k =\nexp w⊤\nπ,kgt\n∑K\nk′=1 exp w⊤\nπ,k′gt\nand\nhct,k = tanh(Wh,kgt) where wπ,k and Wh,k are model parameters.\nOur method is simple and easy to implement, and has the following advantages:\n• Improved expressiveness(compared to Softmax). MoS is theoretically more (or at least equally)\nexpressive compared to Softmax given the same dimension d. This can be seen by the fact that\nMoS with K = 1 is reduced to Softmax. More importantly, MoS effectively approximatesA by\nˆAMoS = log\nK∑\nk=1\nΠkexp(Hθ,kW⊤\nθ )\nwhere Πk is an (N ×N) diagonal matrix with elements being the prior πc,k. Because ˆAMoS is\na nonlinear function (log_sum_exp) of the context vectors and the word embeddings, ˆAMoS can\nbe arbitrarily high-rank. As a result, MoS does not suffer from the rank limitation, compared to\nSoftmax.\n3This is also conﬁrmed by our preliminary experiments.\n4\nPublished as a conference paper at ICLR 2018\n• Improved generalization (compared to Ngram). Ngram models and high-dimensional Softmax\n(Cf. Section 2.3) improve the expressiveness but do not generalize well. In contrast, MoS does\nnot have a generalization issue due to the following reasons. First, MoS deﬁnes the following\ngenerative process: a discrete latent variable k is ﬁrst sampled from {1,··· ,K}, and then the\nnext token is sampled based on the k-th Softmax component. By doing so we introduce an\ninductive bias that the next token is generated based on a latent discrete decision (e.g., a topic),\nwhich is often safe in language modeling (Blei et al., 2003). Second, since ˆAMoS is deﬁned by\na nonlinear function and not restricted by the rank bottleneck, in practice it is possible to reduce\nd to compensate for the increase of model parameters brought by the mixture structure. As a\nresult, MoS has a similar model size compared to Softmax and thus is not prone to overﬁtting.\n2.5 M IXTURE OF CONTEXTS : A L OW-RANK BASELINE\nAnother possible approach is to directly mix the context vectors (or logits) before taking the Soft-\nmax, rather than mixing the probabilities afterwards as in MoS. Speciﬁcally, the conditional distri-\nbution is parameterized as\nPθ(x|c) =\nexp\n(∑K\nk=1 πc,khc,k\n)⊤\nwx\n∑\nx′ exp\n(∑K\nk=1 πc,khc,k\n)⊤\nwx′\n=\nexp\n(∑K\nk=1 πc,kh⊤\nc,kwx\n)\n∑\nx′ exp\n(∑K\nk=1 πc,kh⊤\nc,kwx′\n), (2)\nwhere hc,k and πc,k share the same parameterization as in MoS. Despite its superﬁcial similarity to\nMoS, this model, which we refer to as mixture of contexts (MoC), actually suffers from the same\nrank limitation problem as Softmax. This can be easily seen by deﬁning h′c = ∑K\nk=1 πc,khc,k,\nwhich turns the MoC parameterization (2) into Pθ(x|c) = exp h′⊤\nc wx∑\nx′ exp h′⊤\nc wx′\n. Note that this is equiv-\nalent to the Softmax parameterization (1). Thus, performing mixture in the feature space can only\nmake the function family Umore expressive, but does not change the fact that the rank of HθW⊤\nθ\nis upper bounded by the embedding dimension d. In our experiments, we implement MoC as a\nbaseline and compare it experimentally to MoS.\n3 E XPERIMENTS\n3.1 M AIN RESULTS\nWe conduct a series of experiments with the following settings:\n• Following previous work (Krause et al., 2017; Merity et al., 2017; Melis et al., 2017), we eval-\nuate the proposed MoS model on two widely used language modeling datasets, namely Penn\nTreebank (PTB) (Mikolov et al., 2010) and WikiText-2 (WT2) (Merity et al., 2016) based on per-\nplexity. For fair comparison, we closely follow the regularization and optimization techniques\nintroduced by Merity et al. (2017). We heuristically and manually search hyper-parameters for\nMoS based on the validation performance while limiting the model size (see Appendix B.1 for\nour hyper-parameters).\n• To investigate whether the effectiveness of MoS can be extended to even larger datasets, we\nconduct an additional language modeling experiment on the 1B Word dataset (Chelba et al.,\n2013). Speciﬁcally, we lower-case the text and choose the top 100K tokens as the vocabulary. A\nstandard neural language model with 2 layers of LSTMs followed by a Softmax output layer is\nused as the baseline. Again, the network size of MoS is adjusted to ensure a comparable number\nof parameters. Notably, dropout was not used, since we found it not helpful to either model (see\nAppendix B.2 for more details).\n• To show that the MoS is a generic structure that can be used to model other context-dependent\ndistributions, we additionally conduct experiments in the dialog domain. We use the Switch-\nboard dataset (Godfrey & Holliman, 1997) preprocessed by Zhao et al. (2017) 4 to train a\nSeq2Seq (Sutskever et al., 2014) model with MoS added to the decoder RNN. Then, a Seq2Seq\nmodel using Softmax and another one augmented by MoC with comparable parameter sizes\n4https://github.com/snakeztc/NeuralDialog-CVAE/tree/master/data\n5\nPublished as a conference paper at ICLR 2018\nModel #Param Validation Test\nMikolov & Zweig (2012) – RNN-LDA + KN-5 + cache 9M‡ - 92.0\nZaremba et al. (2014) – LSTM 20M 86.2 82.7\nGal & Ghahramani (2016) – Variational LSTM (MC) 20M - 78.6\nKim et al. (2016) – CharCNN 19M - 78.9\nMerity et al. (2016) – Pointer Sentinel-LSTM 21M 72.4 70.9\nGrave et al. (2016) – LSTM + continuous cache pointer† - - 72.1\nInan et al. (2016) – Tied Variational LSTM + augmented loss 24M 75.7 73.2\nZilly et al. (2016) – Variational RHN 23M 67.9 65.4\nZoph & Le (2016) – NAS Cell 25M - 64.0\nMelis et al. (2017) – 2-layer skip connection LSTM 24M 60.9 58.3\nMerity et al. (2017) – AWD-LSTM w/o ﬁnetune 24M 60.7 58.8\nMerity et al. (2017) – AWD-LSTM 24M 60.0 57.3\nOurs – AWD-LSTM-MoS w/o ﬁnetune 22M 58.08 55.97\nOurs – AWD-LSTM-MoS 22M 56.54 54.44\nMerity et al. (2017) – AWD-LSTM + continuous cache pointer† 24M 53.9 52.8\nKrause et al. (2017) – AWD-LSTM + dynamic evaluation† 24M 51.6 51.1\nOurs – AWD-LSTM-MoS + dynamic evaluation† 22M 48.33 47.69\nTable 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained\nfrom Merity et al. (2017) and Krause et al. (2017). † indicates using dynamic evaluation.\nModel #Param Validation Test\nInan et al. (2016) – Variational LSTM + augmented loss 28M 91.5 87.0\nGrave et al. (2016) – LSTM + continuous cache pointer† - - 68.9\nMelis et al. (2017) – 2-layer skip connection LSTM 24M 69.1 65.9\nMerity et al. (2017) – AWD-LSTM w/o ﬁnetune 33M 69.1 66.0\nMerity et al. (2017) – AWD-LSTM 33M 68.6 65.8\nOurs – AWD-LSTM-MoS w/o ﬁnetune 35M 66.01 63.33\nOurs – AWD-LSTM-MoS 35M 63.88 61.45\nMerity et al. (2017) – AWD-LSTM + continuous cache pointer † 33M 53.8 52.0\nKrause et al. (2017) – AWD-LSTM + dynamic evaluation† 33M 46.4 44.3\nOurs – AWD-LSTM-MoS + dynamical evaluation† 35M 42.41 40.68\nTable 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and\nKrause et al. (2017). † indicates using dynamic evaluation.\nare used as baselines. For evaluation, we include both the perplexity and the precision/recall\nof Smoothed Sentence-level BLEU, as suggested by Zhao et al. (2017). When generating re-\nsponses, we use beam search with beam size 10, restrict the maximum length to 30, and retain\nthe top-5 responses.\nThe language modeling results on PTB and WT2 are presented in Table 1 and Table 2 respectively.\nWith a comparable number of parameters, MoS outperforms all baselines with or without dynamic\nevaluation, and substantially improves over the current state of the art, by up to 3.6 points in per-\nplexity.\nModel #Param Train Validation Test\nSoftmax 119M 41.47 43.86 42.77\nMoS 113M 36.39 38.01 37.10\nTable 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.\nThe improvement on the large-scale dataset is even more signiﬁcant. As shown in Table 3, MoS\noutperforms Softmax by over 5.6 points in perplexity. It suggests the effectiveness of MoS is not\nlimited to small datasets where many regularization techniques are used. Note that with limited\ncomputational resources, we didn’t tune the hyper-parameters for MoS.\n6\nPublished as a conference paper at ICLR 2018\nPerplexity BLEU-1 BLEU-2 BLEU-3 BLEU-4\nModel prec recall prec recall prec recall prec recall\nSeq2Seq-Softmax 34.657 0.249 0.188 0.193 0.151 0.168 0.133 0.141 0.111\nSeq2Seq-MoC 33.291 0.259 0.198 0.202 0.159 0.176 0.140 0.148 0.117\nSeq2Seq-MoS 32.727 0.272 0.206 0.213 0.166 0.185 0.146 0.157 0.123\nTable 4: Evaluation scores on Switchboard.\nFurther, the experimental results on Switchboard are summarized in Table 45. Clearly, on all metrics,\nMoS outperforms MoC and Softmax, showing its general effectiveness.\n3.2 A BLATION STUDY\nTo further verify the improvement shown above does come from the MoS structure rather than\nadding another hidden layer or ﬁnding a particular set of hyper-parameters, we conduct an ablation\nstudy on both PTB and WT2. Firstly, we compare MoS with an MoC architecture with the same\nnumber of layers, hidden sizes, and embedding sizes, which thus has the same number of parame-\nters. In addition, we adopt the hyper-parameters used to obtain the best MoS model (denoted as MoS\nhyper-parameters), and train a baseline AWD-LSTM. To avoid distractive factors and save compu-\ntational resources, all ablative experiments excluded the use of ﬁnetuing and dynamic evaluation.\nThe results are shown in Table 5. Compared to the vanilla AWD-LSTM, though being more expres-\nsive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another\nhidden layer or employing a mixture structure in the feature space does not guarantee a better per-\nformance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the\nperformance, which rules out hyper-parameters as the main source of improvement.\nPTB WT2\nModel Validation Test Validation Test\nAWD-LSTM-MoS 58.08 55.97 66.01 63.33\nAWD-LSTM-MoC 59.82 57.55 68.76 65.98\nAWD-LSTM (Merity et al. (2017) hyper-parameters) 61.49 58.95 68.73 65.40\nAWD-LSTM (MoS hyper-parameters) 78.86 74.86 72.73 69.18\nTable 5: Ablation study on Penn Treebank and WikiText-2 without ﬁnetuning or dynamical evaluation.\n3.3 V ERIFY THE ROLE OF RANK\nWhile the study above veriﬁes that MoS is the key to achieving the state-of-the-art performance, it\nis still not clear whether the superiority of MoS comes from its potential high rank, as suggested by\nour theoretical analysis in Section 2. In the sequel, we take steps to verify this hypothesis.\n• Firstly, we verify that MoS does induce a high-rank log-probability matrix empirically, while\nMoC and Softmax fail. On the validation or test set of PTB with tokens X = {X1,...,X T}, we\ncompute the log probabilities{log P(Xi |X<i) ∈RM}T\nt=1 for each token using all three models.\nThen, for each model, we stack all T log-probability vectors into a T ×M matrix, resulting in\nˆAMoS, ˆAMoC and ˆASoftmax. Theoretically, the number of non-zero singular values of a matrix is\nequal to its rank. However, performing singular value decomposition of real valued matrices using\nnumerical approaches often encounter roundoff errors. Hence, we adopt the expected roundoff\nerror suggested by Press (2007) when estimating the ranks of ˆAMoS, ˆAMoC and ˆASoftmax.\nThe estimated ranks are shown in Table 6. As predicted by our theoretical analysis, the matrix\nranks induced by Softmax and MoC are both limited by the corresponding embedding sizes. By\ncontrast, the matrix rank obtained from MoS does not suffer from this constraint, almost reaching\nfull rank ( M = 10000 ). In appendix C.1, we give additional evidences for the higher rank of\nMoS.\n5The numbers are not directly comparable to Zhao et al. (2017) since their Seq2Seq implementation and\nevaluation scripts are not publicly available.\n7\nPublished as a conference paper at ICLR 2018\nModel Validation Test\nSoftmax 400 400\nMoC 280 280\nMoS 9981 9981\nTable 6: Rank comparison on PTB. To ensure com-\nparable model sizes, the embedding sizes of Softmax,\nMoC and MoS are 400, 280, 280 respectively. The\nvocabulary size, i.e., M, is 10,000 for all models.\n#Softmax Rank Perplexity\n3 6467 58.62\n5 8930 57.36\n10 9973 56.33\n15 9981 55.97\n20 9981 56.17\nTable 7: Empirical rank and test perplexity on\nPTB with different number of Softmaxes.\n• Secondly, we show that, before reaching full rank, increasing the number of mixture components\nin MoS also increases the rank of the log-probability matrix, which in turn leads to improved\nperformance (lower perplexity). Speciﬁcally, on PTB, with other hyper-parameters ﬁxed as used\nin section 3.1, we vary the number of mixtures used in MoS and compare the corresponding em-\npirical rank and test perplexity without ﬁnetuning. Table 7 summarizes the results. This clear\npositive correlation between rank and performance strongly supports the our theoretical analysis\nin section 2. Moreover, note that after reaching almost full rank (i.e., using 15 mixture compo-\nnents), further increasing the number of components degrades the performance due to overﬁtting\n(as we inspected the training and test perplexities).\n• In addition, as performance improvement can often come from better regularization, we investi-\ngate whether MoS has a better, though unexpected, regularization effect compared to Softmax.\nWe consider the 1B word dataset where overﬁtting is unlikely and no explicit regularization tech-\nnique (e.g., dropout) is employed. As we can see from the left part of Table 3, MoS and Softmax\nachieve a similar generalization gap, i.e., the performance gap between the test set and the train-\ning set. It suggests both models have similar regularization effects. Meanwhile, MoS has a lower\ntraining perplexity compared to Softmax, indicating that the improvement of MoS results from\nimproved expressiveness.\n• The last evidence we provide is based on an inverse experiment. Empirically, we ﬁnd that when\nSoftmax does not suffer from a rank limitation, e.g., in character-level language modeling, using\nMoS will not improve the performance. Due to lack of space, we refer readers to Appendix C.2\nfor details.\n3.4 A DDITIONAL ANALYSIS\nMoS computational time The expressiveness of MoS does come with a computational cost—\ncomputing a K-times larger Softmax. To give readers a concrete idea of the inﬂuence on training\ntime, we perform detailed analysis in Appendix C.3. As we will see, computational wall time of\nMoS is actually sub-linear w.r.t. the number of Softmaxes K. In most settings, we observe a two to\nthree times slowdown when using MoS with up to 15 mixture components.\nQualitative analysis Finally, we conduct a case study on PTB to see how MoS improves the\nnext-token prediction in detail. Due to lack of space, we refer readers to Appendix C.4 for details.\nThe key insight from the case study is that MoS is better at making context-dependent predictions.\nSpeciﬁcally, given the same immediate preceding word, MoS will produce distinct next-step predic-\ntion based on long-term context in history. By contrast, the baseline often yields similar next-step\nprediction, independent of the long-term context.\n4 R ELATED WORK\nIn language modeling, Hutchinson et al. (2011; 2012) have previously considered the problem from\na matrix rank perspective. However, their focus was to improve the generalization of Ngram lan-\nguage models via a sparse plus low-rank approximation. By contrast, as neural language models\nalready generalize well, we focus on a high-rank neural language model that improves expressive-\nness without sacriﬁcing generalization. Neubig & Dyer (2016) proposed to mix Ngram and neural\nlanguage models to unify and beneﬁt from both. However, this mixture might not generalize well\nsince an Ngram model, which has poor generalization, is included. Moreover, the fact that the\n8\nPublished as a conference paper at ICLR 2018\ntwo components are separately trained can limit its expressiveness. Levy & Goldberg (2014) also\nconsidered the matrix factorization perspective, but in the context of learning word embeddings.\nIn a general sense, Mixture of Softmaxes proposed in this work can be seen as a particular instan-\ntiation of the long-existing idea called Mixture of Experts (MoE) (Jacobs et al., 1991). However,\nthere are two core differences. Firstly, MoE has usually been instantiated as mixture of Gaussians\nto model data in continuous domains (Jacobs et al., 1991; Graves, 2013; Bazzani et al., 2016). More\nimportantly, the motivation of using the mixture structure is distinct. For Gaussian mixture models,\nthe mixture structure is employed to allow for a parameterized multi-modal distribution. By con-\ntrast, Softmax by itself can parameterize a multi-modal distribution, and MoS is introduced to break\nthe Softmax bottleneck as discussed in Section 2.\nThere has been previous work (Eigen et al., 2013; Shazeer et al., 2017) proposing architectures that\ncan be categorized as instantiations of MoC, since the mixture structure is employed in the feature\nspace.6 The target of Eigen et al. (2013) is to create a more expressive feed-forward layer through\nthe mixture structure. In comparison, Shazeer et al. (2017) focuses on a sparse gating mechanism\nalso on the feature level, which enables efﬁcient conditional computation and allows the training of\na very large neural architecture. In addition to having different motivations from our work, all these\nMoC variants suffer from the same rank limitation problem as discussed in Section 2.\nFinally, several previous works have tried to introduce latent variables into sequence model-\ning (Bayer & Osendorfer, 2014; Gregor et al., 2015; Chung et al., 2015; Gan et al., 2015; Frac-\ncaro et al., 2016; Chung et al., 2016). Except for (Chung et al., 2016), these structures all deﬁne\na continuous latent variable for each step of the RNN computation, and rely on the SGVB estima-\ntor (Kingma & Welling, 2013) to optimize a variational lower bound of the log-likelihood. Since\nexact integration is infeasible, these models cannot estimate the likelihood (perplexity) exactly at test\ntime. Moreover, for discrete data, the variational lower bound is usually too loose to yield a com-\npetitive approximation compared to standard auto-regressive models. As an exception, Chung et al.\n(2016) utilizes Bernoulli latent variables to model the hierarchical structure in language, where the\nBernoulli sampling is replaced by a thresholding operation at test time to give perplexity estimation.\n5 C ONCLUSIONS\nUnder the matrix factorization framework, the expressiveness of Softmax-based language models is\nlimited by the dimension of the word embeddings, which is termed as the Softmax bottleneck. Our\nproposed MoS model improves the expressiveness over Softmax, and at the same time avoids over-\nﬁtting compared to non-parametric models and naively increasing the word embedding dimensions.\nOur method improves the current state-of-the-art results on standard benchmarks by a large margin,\nwhich in turn justiﬁes our theoretical reasoning: it is important to have a high-rank model for natural\nlanguage.\nACKNOWLEDGMENTS\nThis work was supported by the DARPA award D17AP00001, the Google focused award, and the\nNvidia NV AIL award.\n6Although Shazeer et al. (2017) name their architecture as MoE, it is not a standard MoE (Jacobs et al.,\n1991) and should be classiﬁed as MoC under our terminology.\n9\nPublished as a conference paper at ICLR 2018\nREFERENCES\nJustin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv preprint\narXiv:1411.7610, 2014.\nLoris Bazzani, Hugo Larochelle, and Lorenzo Torresani. Recurrent mixture density network for\nspatiotemporal visual attention. arXiv preprint arXiv:1603.08199, 2016.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022, 2003.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-\ngio. A recurrent latent variable model for sequential data. In Advances in neural information\nprocessing systems, pp. 2980–2988, 2015.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704, 2016.\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\nMarco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models\nwith stochastic layers. In Advances in Neural Information Processing Systems , pp. 2199–2207,\n2016.\nYarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent\nneural networks. In Advances in neural information processing systems, pp. 1019–1027, 2016.\nZhe Gan, Chunyuan Li, Ricardo Henao, David E Carlson, and Lawrence Carin. Deep temporal\nsigmoid belief networks for sequence modeling. In Advances in Neural Information Processing\nSystems, pp. 2467–2475, 2015.\nJohn J Godfrey and Edward Holliman. Switchboard-1 release 2. Linguistic Data Consortium,\nPhiladelphia, 1997.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. arXiv preprint arXiv:1612.04426, 2016.\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\nKarol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A\nrecurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.\nBrian Hutchinson, Mari Ostendorf, and Maryam Fazel. Low rank language models for small training\nsets. IEEE Signal Processing Letters, 18(9):489–492, 2011.\nBrian Hutchinson, Mari Ostendorf, and Maryam Fazel. A sparse plus low rank maximum entropy\nlanguage model. In INTERSPEECH, pp. 1676–1679, 2012.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\nlocal experts. Neural computation, 3(1):79–87, 1991.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language\nmodels. In AAAI, pp. 2741–2749, 2016.\n10\nPublished as a conference paper at ICLR 2018\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nReinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In\nAcoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on,\nvolume 1, pp. 181–184. IEEE, 1995.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural\nsequence models. arXiv preprint arXiv:1709.07432, 2017.\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Ad-\nvances in neural information processing systems, pp. 2177–2185, 2014.\nMatt Mahoney. Large text compression benchmark, 2011.\nGábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589, 2017.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182, 2017.\nTomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.\nSLT, 12:234–239, 2012.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Interspeech, volume 2, pp. 3, 2010.\nTomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cer-\nnocky. Subword language modeling with neural networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf), 2012.\nAndriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling.\nIn Proceedings of the 24th international conference on Machine learning , pp. 641–648. ACM,\n2007.\nGraham Neubig and Chris Dyer. Generalizing and hybridizing count-based and neural language\nmodels. arXiv preprint arXiv:1606.00499, 2016.\nSteven Pinker. The language instinct, 1994.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. InEACL, 2017.\nWilliam H Press. Numerical recipes 3rd edition: The art of scientiﬁc computing . Cambridge uni-\nversity press, 2007.\nAnton Maximilian Schäfer and Hans Georg Zimmermann. Recurrent neural networks are universal\napproximators. In International Conference on Artiﬁcial Neural Networks, pp. 632–640. Springer,\n2006.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538, 2017.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pp. 3104–3112, 2014.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural\nnetworks using dropconnect. In Proceedings of the 30th international conference on machine\nlearning (ICML-13), pp. 1058–1066, 2013.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\n11\nPublished as a conference paper at ICLR 2018\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural di-\nalog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960, 2017.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent\nhighway networks. arXiv preprint arXiv:1607.03474, 2016.\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n12\nPublished as a conference paper at ICLR 2018\nA P ROOFS\nProof of Property 1\nProof. For any A′∈F(A), let PA′(X|C) denote the distribution deﬁned by applying Softmax on\nthe logits given by A′. Consider row icolumn j, by deﬁnition any entry in A′can be expressed as\nA′\nij = Aij + Λii. It follows\nPA′(xj|ci) = exp A′\nij∑\nkexp A′\nik\n= exp(Aij + Λii)∑\nkexp(Aik + Λii) = exp Aij∑\nkexp Aik\n= P∗(xj|ci)\nFor any A′′∈{A′′|Softmax(A′′) = P∗}, for any iand j, we have\nPA′′(xj|ci) = PA(xj|ci)\nIt follows that for any i, j, and k,\nPA′′(xj|ci)\nPA′′(xk|ci) = exp A′′\nij\nexp A′′\nik\n= exp Aij\nexp Aik\n= PA(xj|ci)\nPA(xk|ci)\nAs a result,\nA′′\nij −Aij = A′′\nik −Aik\nThis means each row in A′′can be obtained by adding a real number to the corresponding row in\nA. Therefore, there exists a diagonal matrix Λ ∈RN×N such that\nA′′= A + ΛJN,M\nIt follows that A′′∈F(A).\nProof of Property 2\nProof. For any A1 and A2 in F(A), by deﬁnition we have A1 = A + Λ1JN,M, and A2 =\nA + Λ2JN,M where Λ1 and Λ2 are two diagonal matrices. It can be rewritten as\nA1 = A2 + (Λ1 −Λ2)JN,M\nLet S be a maximum set of linearly independent rows in A2. Let eN be an all-ones vector with\ndimension N. The i-th row vector a1,i in A1 can be written as\na1,i = a2,i + (Λ1,ii −Λ2,ii)eN\nBecause a2,i is a linear combination of vectors in S, a1,i is a linear combination of vectors in\nS∪{eN}. It follows that\nrank(A1) ≤rank(A2) + 1\nSimilarly, we can derive\nrank(A2) ≤rank(A1) + 1\nTherefore,\n|rank(A1) −rank(A2)|≤ 1\nProof of Proposition 1\nProof. If there exists a parameter θsuch that Pθ(X|c) = P∗(X|c) for all cin L, by Lemma 1, we\nhave HθW⊤\nθ ∈F(A). As a result, there exists a matrix A′ ∈F(A) such that HθW⊤\nθ = A′.\nBecause Hθ and Wθ are of dimensions (N ×d) and (M ×d) respectively, we have\nd≥rank(A′) ≥ min\nA′′∈F(A)\nrank(A′′)\nIf d ≥minA′′∈F(A) rank(A′′), there exist matrices A′ ∈F(A), H′ ∈RN×d and W′ ∈RM×d,\nsuch that A′can be factorized as A′= H′W′⊤. Because Uis a universal approximator, there exists\nθsuch that Hθ = H′and Wθ = W′. By Lemma 1, Pθ(X|c) = P∗(X|c) for all cin L.\n13\nPublished as a conference paper at ICLR 2018\nB E XPERIMENT SETTING AND HYPER -PARAMETERS\nB.1 PTB AND WT2\nThe hyper-parameters used for MoS in language modeling experiment is summarized below.\nHyper-parameter PTB WT2\nLearning rate 20 15\nBatch size 12 15\nEmbedding size 280 300\nRNN hidden sizes [960, 960, 620] [1150,1150,650]\nNumber of mixture components 15 15\nWord-level V-dropout 0.10 0.10\nEmbedding V-dropout 0.55 0.40\nHidden state V-dropout 0.20 0.225\nRecurrent weight dropout (Wan et al., 2013) 0.50 0.50\nContext vector V-dropout 0.30 0.30\nTable 8: Hyper-parameters used for MoS. V-dropout abbreviates variational dropout (Gal & Ghahramani,\n2016). See (Merity et al., 2017) for more detailed descriptions.\nThe hyper-parameters used for dynamic evaluation of MoS is summarized below.\nHyper-parameter PTB WT2\nBatch size 100 100\nlearning rate (η) 0.002 0.002\nϵ 0.001 0.002\nλ 0.075 0.02\nTable 9: Hyper-parameters used for dynamic evaluation of MoS. See (Krause et al., 2017) for more detailed\ndescriptions.\nB.2 1B W ORD DATASET\nFor training, we use all of the 100 training shards. For validation, we use two shards from the\nheldout set, namely [heldout-00, heldout-10]. For test, we use another three shards from\nthe heldout set, namely [heldout-20, heldout-30, heldout-40].\nThe hyper-parameters are listed below.\nHyper-parameter Softmax MoS-7\nLearning rate 20 20\nBatch size 60 60\nBPTT langth 35 35\nEmbedding size 1024 900\nRNN hidden sizes [1024, 1024] [1024,1024]\nDropout rate 0 0\nTable 10: Hyper-parameters used for Softmax and MoS in experiment on 1B word dataset.\nC A DDITIONAL EXPERIMENTS\nC.1 H IGHER EMPIRICAL RANK OF MOS COMPARED TO MOC AND SOFTMAX\nIn section 3, we compute the rank of different models based on the non-zero singular values of the\nempirical log-likelihood matrix. Since there can be roundoff mistakes, a less error-prone approach\nis to directly study the distribution of singular values. Speciﬁcally, if more singular values have\nrelatively larger magnitude, the rank of the matrix tends to be higher. Motivated from this intuition,\n14\nPublished as a conference paper at ICLR 2018\n10 10\n 10 8\n 10 6\n 10 4\n 10 2\n 100\nNormalized singular value in log scale\n0%\n20%\n40%\n60%\n80%\n100%Cumulative percentage\nSoftmax\nMoC\nMoS\nFigure 1: Cumulative percentage of normalized singulars given a value in [0,1].\nwe visualize the distribution of the singular values. To account for the different magnitudes of\nsingular values from different models, we ﬁrst normalize all singular values to [0,1]. Then, we plot\nthe cumulative percentage of normalized singular values, i.e., percentage of normalized singular\nvalues below a threshold, in Figure 1. As we can see, most of the singular values of Softmax and\nMoC concentrate on an area with very low values. In comparison, the concentration area of the MoS\nsingular values is not only several orders larger, but also spans a much wider region. Intuitively, MoS\nutilizes the corresponding singular vectors to capture a larger and more diverse set of contexts.\nModel Validation Test\nSoftmax 4.869 4.763\nMoC 4.955 4.864\nMoS 5.400 5.284\nTable 11: Empirical expected pairwise KLD on PTB.\nWhat’s more, another indicator of high rank is that the model can precisely capture the nuance of dif-\nference contexts. If a model can better capture the distinctions among contexts, we expect the next-\nstep conditional distributions to be less similar to each on average. Based on this intuition, we use the\nexpected pairwise Kullback–Leibler divergence (KLD), i.e., Ec,c′∼C[KLD(P(X |c)∥P(X |c′))]\nwhere Cdenotes all possible contexts, as another metric to evaluate the ranks of the three models\n(MoS, MoC and Softmax). Practically, we sample c,c′from validation or test data of PTB to get\nthe empirical estimations for the three models, which are shown in the right half of Table 11. As we\nexpected, MoS achieves higher expected pairwise KLD, indicating its superiority in covering more\ncontexts of the next-token distribution.\nC.2 A N INVERSE EXPERIMENT ON CHARACTER -LEVEL LANGUAGE MODELING\nModel #Param Train Validation Test\nSoftmax (hid1024, emb1024) 8.42M 1.35 1.41 1.49\nMoS-7 (hid910, emb510) 8.45M 1.35 1.40 1.49\nMoS-7 (hid750, emb750) 8.45M 1.38 1.42 1.50\nMoS-10 (hid860, emb452) 8.43M 1.35 1.41 1.49\nMoS-10 (hid683, emb683) 8.43M 1.38 1.42 1.50\nTable 12: BPC comparison on text8. For MoS, “-n” indicates using nmixtures. “hid” and “emb” denote the\nhidden size and embedding size respectively.\n15\nPublished as a conference paper at ICLR 2018\nHere, we detail the inverse experiment, which shows that when Softmax does not suffer from a\nrank limitation, using MoS will not improve the performance. Notice that character-level language\nmodeling (CharLM) is exactly such a problem, because the rank of the log-likelihood matrix is\nupper bounded by the vocabulary size, and CharLM usually has a very limited vocabulary (tens of\ncharacters). In this case, with the embedding size being hundreds in practice, Softmax is no longer a\nbottleneck in this task. Hence, we expect MoS to yield similar performance to Softmax on CharLM.\nWe conduct experiments of CharLM using the text8 dataset (Mahoney, 2011), which consists of\n100M characters including only alphabetical characters and spaces derived from Wikipedia. We\nfollow Mikolov et al. (2012) and use the ﬁrst 90M characters for training, the next 5M for validation\nand the ﬁnal 5M for testing. The standard evaluation metric bit-per-character (BPC) is employed.\nWe employ a 1-layer 1024-unit LSTM followed by Softmax as the baseline. For MoS, we consider\n7 or 10 mixtures and reduce the hidden and/or embedding size to match the baseline capacity. When\ndecreasing the hidden and/or embedding size, we either keep both the same, or make the hidden\nsize relatively larger. The results are summarized in Table 12. Clearly, the Softmax and MoS\nobtain the same BPC on the test set and comparable BPC on the validation set, which well match\nour hypothesis. Since the only difference in word-level language modeling is the existence of the\nSoftmax bottleneck, the distinct behavior of MoS again supports our hypothesis that it is solving the\nSoftmax bottleneck problem.\nC.3 M OS COMPUTATIONAL TIME\nModel PTB/bs PTB/best-1 WT2/bs WT2/best-1 WT2/best-3 1B/bs 1B/best-1 1B/best-3\nSoftmax 1x 1x 1x 1x 1x 1x 1x 1x\nMoS-5 1.2x – 1.3x – – – – –\nMoS-7 – – – – – 3.8x 5.7x 2.1x\nMoS-10 1.6x – 1.9x – – – – –\nMoS-15 1.9x 2.8x 2.5x 6.4x 2.9x – – –\nTable 13: Training time slowdown compared to Softmax. MoS-Kmeans using Kmixture components. “bs”\nindicates Softmax and MoS use the same batch sizes on one GPU. “best-1” and “best-3” refer to the settings\nwhere Softmax and MoS obtain their own best perplexity, with 1 and 3 GPUs respectively.\nWe evaluate the additional computational cost introduced by MoS. We consider two sets of con-\ntrolled experiments. In the ﬁrst set, we compare the training time of MoS and Softmax using the\nsame batch sizes. In the second set, we compare the training time of two methods using the hyper-\nparameter settings that achieve the best performance for each model (i.e., the settings in Tables 1, 2,\nand 3). In both sets, we control two models to have comparable model sizes.\nThe results on the three datasets are shown in Table 13. Thanks to the efﬁciency of matrix multi-\nplication on GPU, the computational wall time of MoS is actually sub-linear w.r.t. the number of\nSoftmaxes K. In most settings, we observe a two to three times slowdown when using MoS. Specif-\nically, the “bs” setting measures the computational cost introduced by MoS given enough memory,\nwhich is 1.9x, 2.5x, and 3.8x slowdown on PTB, WT2, and 1B respectively. The “best-1” setting\nis usually slower compared to “bs”, because a single batch does not ﬁt into the memory of a single\nGPU using MoS, in which case we have to split one batch into multiple small ones, resulting in\nfurther slowdown. In this sense, the gap between “best-1” and “bs” measures the computational\ncost introduced due to the increase of memory consumed by MoS. The “best-3” alleviates this is-\nsue by using three GPUs, which allows larger-batch training for MoS. In this case, we reduce the\ncomputational cost to 2.9x on WT2 and 2.1x on 1B with our best performing model.\nNote that the computational cost is closely related to the batch size, which is interleaved with opti-\nmization. Though how batch sizes affect optimization remains an open question and might be task\ndependent, we believe the “best-1” and “best-3” settings well reﬂect the actual computational cost\nbrought by MoS on language modeling tasks.\n16\nPublished as a conference paper at ICLR 2018\nC.4 Q UALITATIVE ANALYSIS\nSince MoC shows a stronger performance than Softmax on PTB, the qualitative study focuses on\nthe comparison between MoC and MoS. Concretely, given the same context (previous tokens), we\nsearch for prediction steps where MoS achieves lower negative log loss than MoC by a margin. We\nshow some representative cases in Table 14 with the following observations:\n• Comparing the ﬁrst two cases, given the same preceding word “N”, MoS ﬂexibly adjusts its top\npredictions based on the different topic quantities being discussed in the context. In comparison,\nMoC emits quite similar top choices regardless of the context, suggesting its inferiority in make\ncontext-dependent predictions.\n• In the 3rd case, the context is about international politics, where country/region names are likely\nto appear. MoS captures this nuance well, and yields top choices that can be used to complete a\ncountry name given the immediate preceding word “south”. Similarly, in the 4th case, MoS is\nable to include “ual”, a core entity of discussion in the context, in its top predictions. In contrast,\nMoC gives rather generic predictions irrieselevant to the context in both cases.\n• For the 5th and the 6th example, we see MoS is able to exploit less common words accurately\naccording to the context, while MoC fails to yield such choices. This well matches our analysis\nthat MoS has the capacity of modeling context-dependent language.\n17\nPublished as a conference paper at ICLR 2018\n#1 Context managed properly and with a long-term outlook these can become investment-grade quality prop-\nerties <eos> canadian <unk> production totaled N metric tons in the week ended oct. N up N N\nfrom the preceding week ’s total of N __?__\nMoS top-5 million 0.38 tons 0.24 billion 0.09 barrels 0.06 ounces 0.04\nMoC top-5 billion 0.39 million 0.36 trillion 0.05 <eos> 0.04 N 0.03\nReference canadian <unk> production totaled N metric tons in the week ended oct. N up N N from the\npreceding week ’s total of N tons statistics canada a federal agency said <eos>\n#2 Context the thriving <unk> street area offers <unk> of about $ N a square foot as do <unk> locations\nalong lower ﬁfth avenue <eos> by contrast <unk> in the best retail locations in boston san fran-\ncisco and chicago rarely top $ N __?__\nMoS top-5 <eos> 0.36 a 0.13 to 0.07 for 0.07 and 0.06\nMoC top-5 million 0.39 billion 0.36 <eos> 0.05 to 0.04 of 0.03\nReference by contrast <unk> in the best retail locations in boston san francisco and chicago rarely top $ N\na square foot <eos>\n#3 Context as other <unk> governments particularly poland and the soviet union have recently discovered\ninitial steps to open up society can create a momentum for radical change that becomes difﬁcult\nif not impossible to control <eos> as the days go by the south __?__\nMoS top-5 africa 0.15 african 0.15 <eos> 0.14 korea 0.08 korean 0.05\nMoC top-5 <eos> 0.38 and 0.08 of 0.06 or 0.05 <unk> 0.04\nReference as the days go by the south african government will be ever more hard pressed to justify the\ncontinued <unk> of mr. <unk> as well as the continued banning of the anc and enforcement of\nthe state of emergency <eos>\n#4 Context shares of ual the parent of united airlines were extremely active all day friday reacting to news\nand rumors about the proposed $ N billion buy-out of the airline by an <unk> group <eos>\nwall street ’s takeover-stock speculators or risk arbitragers had placed unusually large bets that a\ntakeover would succeed and __?__\nMoS top-5 the 0.14 that 0.07 ual 0.07 <unk> 0.03 it 0.02\nMoC top-5 the 0.10 <unk> 0.06 that 0.05 in 0.02 it 0.02\nReference wall street ’s takeover-stock speculators or risk arbitragers had placed unusually large bets that a\ntakeover would succeed and ual stock would rise <eos>\n#5 Context the government is watching closely to see if their presence in the <unk> leads to increased <unk>\nprotests and violence if it does pretoria will use this as a reason to keep mr. <unk> behind bars\n<eos> pretoria has n’t forgotten why they were all sentenced to life <unk> in the ﬁrst place for\nsabotage and __?__\nMoS top-5 <unk> 0.47 violence 0.11 conspiracy 0.03 incest 0.03 civil 0.03\nMoC top-5 <unk> 0.41 the 0.03 a 0.02 other 0.02 in 0.01\nReference pretoria has n’t forgotten why they were all sentenced to life <unk> in the ﬁrst place for sabotage\nand conspiracy to <unk> the government <eos>\n#6 Context china ’s <unk> <unk> program has achieved some successes in <unk> runaway economic growth\nand stabilizing prices but has failed to eliminate serious defects in state planning and an <unk>\ndrain on state budgets <eos> the ofﬁcial china daily said retail prices of <unk> foods have n’t\nrisen since last december but acknowledged that huge government __?__\nMoS top-5 subsidies 0.15 spending 0.08 ofﬁcials 0.04 costs 0.04 <unk> 0.03\nMoC top-5 ofﬁcials 0.04 ﬁgures 0.03 efforts 0.03 <unk> 0.03 costs 0.03\nReference the ofﬁcial china daily said retail prices of <unk> foods have n’t risen since last december but ac-\nknowledged that huge government subsidies were a main factor in keeping prices down <eos>\nTable 14: Compaison of next-token prediction on Penn Treebank test data. N stands for a number as the result\nof preprocessing (Mikolov et al., 2010). The context shown only includes the previous sentence and the current\nsentence the prediction step resides in.\n18",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.95360267162323
    },
    {
      "name": "Treebank",
      "score": 0.8987019062042236
    },
    {
      "name": "Softmax function",
      "score": 0.8572921752929688
    },
    {
      "name": "Computer science",
      "score": 0.810494601726532
    },
    {
      "name": "Language model",
      "score": 0.7191897034645081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6076433658599854
    },
    {
      "name": "Natural language processing",
      "score": 0.5873667597770691
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5323688387870789
    },
    {
      "name": "Bottleneck",
      "score": 0.5272960662841797
    },
    {
      "name": "Natural language",
      "score": 0.51556396484375
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5110105276107788
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.4541638493537903
    },
    {
      "name": "Word (group theory)",
      "score": 0.4226105213165283
    },
    {
      "name": "Artificial neural network",
      "score": 0.2221258580684662
    },
    {
      "name": "Linguistics",
      "score": 0.1456969976425171
    },
    {
      "name": "Mathematics",
      "score": 0.0998140275478363
    },
    {
      "name": "Dependency (UML)",
      "score": 0.07036957144737244
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311269955",
      "name": "Apple (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}