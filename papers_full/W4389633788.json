{
  "title": "MediBioDeBERTa: Biomedical Language Model With Continuous Learning and Intermediate Fine-Tuning",
  "url": "https://openalex.org/W4389633788",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5060956962",
      "name": "Eunhui Kim",
      "affiliations": [
        "Korea Institute of Science & Technology Information"
      ]
    },
    {
      "id": "https://openalex.org/A5028471037",
      "name": "Yuna Jeong",
      "affiliations": [
        "Korea Institute of Science & Technology Information"
      ]
    },
    {
      "id": "https://openalex.org/A5014066631",
      "name": "Myung-Seok Choi",
      "affiliations": [
        "Korea Institute of Science & Technology Information"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W6779068807",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4285219308",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6850287543",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6857904750",
    "https://openalex.org/W6847032321",
    "https://openalex.org/W6922433603",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W6803557570",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4389524502",
    "https://openalex.org/W4388584897"
  ],
  "abstract": "The emergence of large language models (LLMs) has marked a significant milestone in the evolution of natural language processing. With the expanded use of LLMs in multiple fields, the development of domain-specific pre-trained language models (PLMs) has become a natural progression and requirement. Developing domain-specific PLMs requires careful design, considering not only differences in training methods but also various factors such as the type of training data and hyperparameters. This paper proposes MediBioDeBERTa, a specialized language model (LM) for biomedical applications. First, we present several practical analyses and methods for improving the performance of LMs in specialized domains. As the initial step, we developed SciDeBERTa v2, an LM specialized in the scientific domain. In the SciERC dataset evaluation, SciDeBERTa v2 achieves the state-of-the-art model performance in the named entity recognition (NER) task. We then provide an in-depth analysis of the datasets and training methods used in the biomedical field. Based on these analyses, MediBioDeBERTa, was continually trained on SciDeBERTa v2 to specialize in the biomedical domain. Utilizing the biomedical language understanding and reasoning benchmark (BLURB), we analyzed factors that degrade task performance and proposed additional improvement methods based on intermediate fine-tuning. The results demonstrate improved performance in three categories: named entity recognition (NER), semantic similarity (SS), and question-answering (QnA), as well as in the ChemProt relation extraction (RE) task on BLURB, compared with existing state-of-the-art LMs.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nMediBioDeBERTa: Biomedical Language\nModel with Continuous Learning and\nIntermediate Fine-Tuning\nEUNHUI KIM1(Member, IEEE), YUNA JEONG2, and MYUNG-SEOK CHOI3\n1Korea Institute of Science and Technology Information, 245 Daehak-ro, 34131, Korea (e-mail: ehkim@kisti.re.kr)\n2Korea Institute of Science and Technology Information, 245 Daehak-ro, 34131, Korea (e-mail: jeongyuna@kisti.re.kr)\n3Korea Institute of Science and Technology Information, 245 Daehak-ro, 34131, Korea (e-mail: mschoi@kisti.re.kr)\nCorresponding author: Yuna Jeong\nThis research was supported by the Korea Institute of Science and Technology Information (KISTI) (KISTI: K-23-L04-C05,\nNTIS:1711198593) and a government-wide R&D fund project for Infectious Disease Research (GFID), Republic of Korea (KISTI:\nN-23-NT-CU04, HG23C1624) and helped by the National Supercomputing Center with their resources and technical supports\nABSTRACT The emergence of large language models (LLMs) has marked a signiﬁcant milestone in\nthe evolution of natural language processing. With the expanded use of LLMs in multiple ﬁelds, the\ndevelopment of domain-speciﬁc pre-trained language models (PLMs) has become a natural progression and\nrequirement. Developing domain-speciﬁc PLMs requires careful design, considering not only differences\nin training methods but also various factors such as the type of training data and hyperparameters. This\npaper proposes MediBioDeBERTa, a specialized language model (LM) for biomedical applications. First,\nwe present several practical analyses and methods for improving the performance of LMs in specialized\ndomains. As the initial step, we developed SciDeBERTa v2, an LM specialized in the scientiﬁc domain.\nIn the SciERC dataset evaluation, SciDeBERTa v2 achieves the state-of-the-art model performance in the\nnamed entity recognition (NER) task. We then provide an in-depth analysis of the datasets and training\nmethods used in the biomedical ﬁeld. Based on these analyses, MediBioDeBERTa, was continually trained\non SciDeBERTa v2 to specialize in the biomedical domain. Utilizing the biomedical language understanding\nand reasoning benchmark (BLURB), we analyzed factors that degrade task performance and proposed\nadditional improvement methods based on intermediate ﬁne-tuning. The results demonstrate improved\nperformance in three categories: named entity recognition (NER), semantic similarity (SS), and question-\nanswering (QnA), as well as in the ChemProt relation extraction (RE) task on BLURB, compared with\nexisting state-of-the-art LMs.\nINDEX TERMS Language model, ﬁne-tuning, domain-speciﬁc modeling, natural language processing.\nI. INTRODUCTION\nThe emergence of large language models (LLMs) marked\na signiﬁcant milestone in the evolution of natural language\nprocessing (NLP) [1], [2], [3]. LLMs have been utilized to\nbuild a universal knowledge of a language by pre-training\non large amounts of data and then being ﬁne-tuned for spe-\nciﬁc downstream tasks. Typical transformer encoder-based\nlanguage models (LMs) have been pre-trained to acquire\ngeneral knowledge and contextual characteristics from large\ncorpora, such as Wikipedia, news articles, and books [2], [4],\n[5], [6], [7]. This knowledge transfer to downstream tasks is\nthe cornerstone of the successful application of a pre-trained\nlanguage model (PLM) to many problems. The advantage of\ntransfer learning has been robustly validated in many NLP\napplications.\nHowever, it is premised on the assumption that the lan-\nguage features of the downstream task are similar to those\nused in pretraining. Language features vary in terms of\nlanguage type, style, terminology, etc.; the larger the gap\nbetween the pretraining information and the downstream\nproblem, the more difﬁcult it is to take advantage of them [8],\n[9], [10], [11], [12]. This culminated in the development of\ndomain-speciﬁc PLMs that relied on domain-speciﬁc data\nfor pretraining. This trend also manifests in generative LMs\nemploying transformer decoders. OpenAI’s GPT model [13],\nas a representative example, utilizes in-context learning to\nacquire general task knowledge through prompts. This allows\nit to demonstrate proﬁciency across a wide range of tasks\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nwithout the need for ﬁne-tuning. However, its performance\non specialized domains for which it hasn’t been trained is\ncomparatively less effective. Even when ﬁne-tuned, it cannot\nsurpass the performance of domain-speciﬁc pre-trained mod-\nels [14].\nRegarding the development of PLMs for speciﬁc domains,\ntwo primary approaches have been identiﬁed. One is continu-\nous learning, which involves learning specialized datasets in\naddition to PLMs that have already learned general knowl-\nedge, and the other involves learning models from scratch\nusing only specialized data [8], [9], [10], [11], [12], [15]. In\nthe ﬁrst method, only a relatively small specialized dataset\nis utilized for training, considering that general knowledge\nhas already been acquired. This approach is particularly\nsuited to scenarios in which computational resources are\nlimited or specialized datasets are rare. Conversely, the sec-\nond approach requires training models entirely from scratch,\nusing only domain-speciﬁc data. This technique requires a\nsigniﬁcantly larger dataset and an extended training period,\nincreasing computing resource requirements. However, the\nbeneﬁt of the latter strategy lies in its potential to yield\nhighly domain-specialized models that often surpass the per-\nformance of the former, which is reﬁned using a continuous\nlearning approach [9], [11].\nAs Large Language Models (LLMs) have found increased\nuse across multiple ﬁelds, the development of domain-\nspeciﬁc Pre-trained Language Models (PLMs) has emerged\nas both a natural progression and a necessity.\nThe main contribution point of this paper can be summa-\nrized as follows:\n• The proposed PLM, SciDeBERTa v2, which is trained\nfrom scratch on the S2ORC dataset [9], encompasses\nfull text and outperforms the previous version, SciDe-\nBERTa [12]. SciDeBERTa v2 achieves the state-of-the-\nart model on the NER task in the SciERC dataset [16].\n• This study provides guideline for the development a bio-\nmedical domain-speciﬁc LM using the BLURB bench-\nmark, which consists of 6 categories and 13 tasks.\nThe process consists of pre-training, domain-speciﬁc\npre-training, inter-mediate ﬁne-tuning according to the\ntask,and ﬁne-tuning.\n• The proposed model, MediBioDeBERTa-IFT, surpassed\nthe existing state-of-the-art models of the same size in\nthree kinds of categories (NER by 0.28%, SS by 0.2%,\nQnA by 1.29%) and one task (ChemProt RE by 14.96%)\nin BLRUB.\nThe remainder of the paper is structured as follows: In\nSection II we present a brief review of previous studies.\nSection III outlines the procedure of pretraining to develop\nan optimized PLM for a specialized domain. Section IV de-\ntails further improvement techniques. We show experimental\nresults and discussion in Section V, and ﬁnally, we conclude\nin Section VI.\nII. RELATED WORK\nTransformer-based LMs have recently been used to improve\nlanguage comprehension and generation. The representative\ntransformer-based LM, BERT [2], outperformed humans in\nthe 2019 GLUE benchmark [17]. RoBERTa [4] optimized\nBERT by including as many documents as possible in a batch\nby excluding sentence-relationship matching. DeBERTa [7],\nan extension of RoBERTa, introduces disentangled attention,\nenhanced masked decoder, and accounts for the relative posi-\ntions of tokens, achieving superior results in the SuperGLUE\nbenchmark [18], despite its comparatively smaller size.\nSpecialized domain adaptation further enhances language\ncomprehension. SciBERT [8] and S2ORC-SciBERT [9],\ntrained from scratch using science and technology datasets\nexemplify this. SciDeBERTa [12], a DeBERTa extension\ntailored for computer science through continuous learning,\ndemonstrates leading performance in speciﬁc tasks. In the\nbiomedical ﬁeld, BioBERT [10] leverages transfer learning\nfrom BERT pre-trained in the general domain, while Pub-\nMedBERT [11], trained from scratch using biomedical liter-\nature, excels in the BLURB benchmark. BioLinkBERT [19],\nwith its focus on linked documents, currently leads in the\nBLURB leaderboard [11].\nThe emergence of generative LMs like BioGPT\n(1.5B) [15], and MedPalm (540B) [20] in the biomedical\ndomain has shown promising results, particularly in the Pub-\nMedQA task. A comprehensive survey of medical domain-\nspeciﬁc LLMs [3] analyzed the overall process of the LLMs\nas pre-training, medical-domain ﬁne-tuning, and prompting.\nThe prompting process is the case when LLMs(over 10B) are\nused with in-context-learning characteristics.\nOur study leverages DeBERTa-v2 (100M), in conjunc-\ntion with the SentencePiece tokenizer [21]. Our approach\nadvances beyond the continuous learning methodology of\nthe SciDeBERTa [12] to yield an optimized LM within the\nbiomedical domain. This study’s methodology reveals that\na biomedical LM based on DeBERTa, trained progressively,\nmirrors the specialized trajectory typical in biomedical edu-\ncation, resulting in optimal performance.\nIII. PRETRAINING PROCEDURE OF MEDIBIODEBERTA\nDomain-speciﬁc PLM development requires a careful design\nthat considers the above differences in training methods and\nvarious factors such as the type of training data and hyper-\nparameters. This study aims to provide guidelines for the\ndevelopment of domain-speciﬁc PLMs. We analyze factors\ncritical for optimizing PLMs to speciﬁc domains and assess\nthe impact of each factor on performance. We speciﬁcally\ndemonstrate the development process of MediBioDeBERTa,\na biomedical domain-speciﬁc PLM.\nThe development process of MediBioDeBERTa, as illus-\ntrated in Fig. 1, encapsulates the following three key steps:\n• Choosing DeBERTa as the base large language model\n(LLM), balancing computational cost and performance.\n• Designing a domain-speciﬁc pretraining strategy by an-\nalyzing PLM performance changes based on pretraining\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nFIGURE 1: Summary of the MediBioDeBERTa training\nprocess.\nmethods and datasets.\n• Optimizing performance for each task, leveraging re-\nsults from the domain-speciﬁc PLM.\nA survey in medical domain-speciﬁc LLMs [22] supports\nour process, which includes pre-training, medical-domain\nﬁne-tuning, and prompting.\nA. BASE MODEL AND PRETRAINING MECHANISM\nPretraining approaches typically fall into two categories.\nThe ﬁrst involves training a randomly initialized model\nfrom scratch. The second method is to continue training a\nPLM that has already been pre-trained with other knowl-\nedge. The latter is often considered when developing a\nspecialized LM because of the difﬁculty in securing data.\nIn terms of performance, a PLM trained from scratch is\ngenerally expected to yield better performance due to its high\ndomain-speciﬁc knowledge. Analyzing representative results\nin the biomedical ﬁeld, PubMedBERT [11], which pretrains\nBERT from scratch, exhibited better overall performance\nthan BioBERT [10], which pretrains BERT continuously.\nHowever, BioLinkBERT [19], which also pretrains BERT\ncontinuously, demonstrated that enhanced performance can\nbe achieved by reﬁning the pretraining algorithm.\nOur study compares PLMs across different pretraining\nmethodologies to establish an optimized process for biomed-\nical PLMs. The comparative analysis, detailed in Section V,\nreveals that scratch-based training using domain-speciﬁc data\ntypically outperforms continuous learning, as shown by Pub-\nMedBERT and BioBERT. Nonetheless, the most effective\napproach was training from scratch with general science data,\nfollowed by continuous learning in the biomedical domain\n(see Fig. 1).\nIn developing MediBioDeBERTa, we initially evaluated\nexisting LMs, selecting DeBERTa as the foundation due to\nits balance of performance and computational efﬁciency. Our\nanalysis, detailed in Table 5 in Section V, indicated that\nSciDeBERTa was less suitable as a base model. Thus, we\nreﬁned SciDeBERTa’s pretraining process to create SciDe-\nBERTa v2, which, when further trained with a biomedical\nsubset from the S2ORC dataset, led to the development of\nMediBioDeBERTa, a PLM tailored for the biomedical ﬁeld.\nOur ﬁndings suggest that choosing a domain-speciﬁc base\nmodel enhances performance compared to a general domain\nmodel. MediBioDeBERTa, for instance, beneﬁts from a grad-\nuated training approach, mirroring the progressive specializa-\ntion typical in biomedical studies. For an in-depth exploration\nof these experiments, see Section V-C.\nB. ANALYSIS OF SCHOLARLY CORRELATION IN\nDATASET\nIt is crucial to perform analysis while constructing a dataset\ncorresponding to a speciﬁc domain. In the experiment, we\nconstructed a biomedical domain dataset by selectively ex-\ntracting papers included in the ‘biology’ and ‘medicine’\ncategories from the S2ORC dataset [9] based on human\nintuition. However, the experimental results showed that the\nbiomedical knowledge from this set was insufﬁcient to solve\nthe BLURB tasks. Therefore, we performed data analysis to\nconstruct an optimally reﬁned dataset.\nOur study utilized the S2ORC dataset, which consists of\n81.1M scientiﬁc papers accompanied by detailed metadata\ntags. This dataset encompasses 19 science and technology\ndisciplines, with each paper often classiﬁed under multiple\ndisciplines. In our analysis, we aimed to identify disciplines\nthat frequently overlap with the ’medicine’ category.\nThe results of the correlation analysis across the 19 cat-\negories are shown in Fig. 2. We deﬁned the disciplinary\ncorrelation Cij between irow and jcolumn as:\nCij = Nij\nNi\n= Ni\n⋂Nj⋃\nj Nij\n(1)\nwhere Cij quantiﬁes the proportion of papers in discipline\nithat are associated with discipline j. Here, Ni denotes the\ntotal count of papers in discipline i, and Nij represents the\nnumber of papers that span both disciplines i and j within\nthe S2ORC dataset’s ’mag ﬁeld of study’. Nii,for i == j,\nrepresents the proportion of research conducted within a\nsingle discipline, as depicted on the diagonal of Fig. 2.\nThe mathematical notation Cij also indicates the degree of\ninterdisciplinary collaboration research between the ﬁeld of\ndiscipline in region iand the ﬁeld of discipline in region j.\nIn summary, the categories most frequently associated\nwith ‘medicine’ were ‘biology,’ ‘chemistry,’ and ‘psychol-\nogy.’ The correlations between these four ﬁelds are shown\nin Fig. 3. We therefore extracted the subset of data related\nto these four disciplines (a combined volume of 52 GB\npost-deduplication) as the dataset for continuous learning,\nas illustrated in Fig. 1. That is, we developed MediBioDe-\nBERTa from SciDeBERTa v.2 through continual learning on\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nFIGURE 2: Confusion matrix of scholarly correlation for the 19 categories in the Semantic Scholar Open Research\nCorpus(S2ORC) dataset.\nFIGURE 3: Confusion matrix of scholarly correlation for\nmedicine-related categories in S2ORC.\nan extracted subset of data, i.e., a Medicine-related dataset.\nFig. 4 demonstrates the distribution of the 19 categories in\nthe S2ORC dataset and the medicine-related, mathematics-\nrelated, and other categories classiﬁed in our study. When\ndividing the three types of major academic disciplines in\nFig. 4, the ﬁelds of Medicine and Mathematics, which have\nFIGURE 4: Category distribution of the S2ORC dataset and\nclassiﬁcation results.\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\na high proportion of interdisciplinary research, were ex-\ntracted. The proportion of each major discipline in the total\nacademic ﬁelds was calculated using the union operation\nfor the interdisciplinary research areas. Mathematically, the\nmajor academic discipline Bi is deﬁned as Bi = ⋃\nj Cij,\nrepresenting the aggregate interdisciplinary research areas\nrelated to discipline i.\nIV. INTERMEDIATE FINE-TUNING FOR TASK\nKNOWLEDGE TRANSFER\nPretrained language models (PLMs), which learn general\nlanguage representations, are typically ﬁne-tuned with task-\nspeciﬁc datasets to solve the downstream tasks. A key chal-\nlenge in transfer learning is aligning the domain of the pre-\ntraining dataset with the target task. Previous studies have fo-\ncused primarily on linguistic domains. This section discusses\ntransfer learning improvements, focusing on enhancing the\ntask knowledge of PLMs through intermediate ﬁne-tuning\n(IFT).\nA. TASK KNOWLEDGE IN PLMS\nPLMs acquire task and linguistic knowledge during pre-\ntraining, which subsequently impacts the performance of\ndownstream tasks. Therefore, it’s essential to account for\nthe task knowledge of the base model during this phase.\nDeBERTa [7] embraces the RoBERTa [4] structure but ex-\ncludes the next sentence prediction (NSP). Additionally, it\nreﬁnes the masked language model (MLM) mechanism to\nconsider the relative positions between tokens. Such a mod-\niﬁcation improves performance in tasks emphasizing token-\nto-token relations, closely mirroring MLM tasks. However,\nit has demonstrated limited efﬁcacy for tasks that emphasize\nsentence-to-sentence or document-to-document correlations.\nDeBERTa suggests that for sequence pair tasks in the GLUE\nbenchmark [17], better performance is achievable through\nIFT, particularly in tasks like multi-genre natural language\ninference (MNLI) for similarity prediction (STS-B), para-\nphrase identiﬁcation (MRPC), and NLI (RTE).\nB. INTERMEDIATE FINE-TUNING\nIntermediate ﬁne-tuning (IFT) has proven effective in en-\nhancing downstream task performance, as demonstrated by\nDeBERTa [7]. Our study concentrates on intermediate tasks\nthat leverage the task knowledge embedded in the base PLM.\nInitially, we evaluated task knowledge at the sentence level\nwithin the PLM. Table 1 compares the performance of each\nPLM on BIOSSES, the sentence similarity (SS) task of\nthe BLURB [11]. Both BERT[2] and LinkBERT[19], pre-\ntrained with NSP and its improved method, exhibited higher\nperformance than DeBERTa. This difference is attributed to\nthe varied task knowledge related to sentence relationship\nidentiﬁcation developed during pretraining. DeBERTa, when\nﬁne-tuned with MNLI, achieved comparable performance to\nBERT. Consequently, we employed IFT to augment the task\nknowledge of our model, MediBioDeBERTa.\nTABLE 1: Performance of the BIOSSES task (micro F1).\nBERT LinkBERT DeBERTa DeBERTaMNLI\n83.21 87.19 76.14 83.15\n† Our experimental results are average values of ﬁve runs.\nBIOSSES, a semantic sentence similarity estimation task\nin the biomedical domain, and MNLI, an intermediate task\nfrom the general domain, demonstrate that IFT using general\ndomain datasets can be effectively enhance performance in\nspecialized domains. This approach is particularly useful in\nﬁelds where acquiring domain-speciﬁc datasets is challeng-\ning. Table 2 summarizes the tasks and datasets for the IFT\nused in the experiments. Experimental results are presented\nin Section V-E.\nC. INTERMEDIATE FINE-TUNING BY MULTITASK AND\nMULTI-FORMAT\nDifferent perspectives of a task can reveal various aspects.\nEmploying different metrics for the same dataset eluci-\ndates different classiﬁcations and similarities. According to\nSciRepEval [23], integrating varied task types during the\ntraining process enhances LM performance. Additionally,\ncombining multitask learning with multi-format learning fur-\nther improves results. In other words, enhancing sentence\ncomprehension through tasks like NER, RE, and co-reference\nresolution (Coref), as well as incorporating regression tasks\nthat use different metrics, such as report references fre-\nquency, can lead to performance gains. We attempted to\napply these principles in our IFT approach. Table 2 out-\nlines the target tasks for performance enhancement via IFT,\nalong with their respective datasets and evaluation metrics.\nFor the hallmarks of cancer (HoC) task involving document\nclassiﬁcation, the IFT was conducted using both regression\nwith Kendall’s τ metric and classiﬁcation with the macro\nF1 metric. For the PubMedQA task, which involves QA,\nIFT combined proximity and search tasks. For the detailed\nexperimental results, please refer to Section V-E and Table 7.\nV. EXPERIMENT AND RESULTS\nWe utilized the DeBERTa-v2 (12-layer base model) [7] with\n128K SentencePiece [21] tokens. Unlike DeBERTa-v1 which\nadopts RoBERTa’s byte pair encoding (BPE), it employs\nSentencePiece due to memory constraints associated with\nbyte-level tokenization. In this experiment, SentencePiece\nwas used to train both SciDeBERTa v2 and MediBioDe-\nBERTa. It took approximately 40 days to train SciDeBERTa\nv2 from scratch using a 256GB S2ORC scholar dataset on\nan A100 2-node connected by 40GB 8 NVLinks. Approx-\nimately, 67 hours were required to train MediBioDeBERTa\nusing a 52GB medibio dataset based on SciDeBERTa v2 in a\ncontinuous-learning manner using an A100 3 node connected\nby 80GB of 8 NVLinks for 10K steps. See Section III for\ndetails on the dataset selection.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nTABLE 2: Summary of the tasks and dataset for IFT.\nTask Dataset Name Format† Train Used/Total Dev Used/Total Eval Metric\nBIOSSES Clinical semantic textual similarity CLF 2392/2392 730/730 Pearson\nBIOSSES MNLI semantic textual similarity CLF 5750/5750 1501/1501 Pearson\nHOC\ncitation count RGN 175K/175K 26K/26K kendall’sτpublication year 198K/198K 19K/19K\nmesh descriptors CLF 600K/2069K 40K/258K Macro F1ﬁeld of studies 500K/541K 40K/67K\nPubMedQA\ncitation prediction PRX 600K/676K 50K/143K MAP\nsearch\nSRCH\n453K/453K 75K/75K\nnDGCsame author detection 67K/67K 8.9K/8.9K\nhighly inﬂuential citation 58K/58K 7K/7K\n†Format abbreviation: classiﬁcation (CLF), regression (RGN), proximity (PRX), adhoc search (SRCH)\nTABLE 3: Hyperparameters for pretraining of SciDeBERTa\nv2 and MediBioDeBERTa.\nHyperparameter Assignment\nSciDeBERTa v.2 MediBioDeBERTa\nmax training steps 500K 10K\nwarmup steps 50K 1K\nbatch size 8,192 49,152\nlearning rate 0.0001 0.0005\noptimizer AdamW AdamW\nweight decay 0.01 0.01\nlearning rate decay linear linear\nTABLE 4: Comparison of the test performances (F1-score)\nof SciDeBERTa and SciDeBERTa v2.\nModel SciERC\nNER JRE Coref\nSciDeBERTa [12] 71.1 ±0.6 46.0 ±0.8 57.4 ±0.6\nSciDeBERTa v2 72.4 ±0.4 47.4 ±1.2 56.9 ±0.8\n†Our experimental results are average values of ﬁve runs.\nA. HYPERPARAMETERS OF SCIDEBERTA V2 AND\nMEDIBIODEBERTA\nThe hyperparameters used for pretraining follow the training\nconditions of DeBERTa [7]. Table 3 provides further details.\nFor MediBioDeBERTa, the training batch size per device\nwas 4,096; 3 nodes were used, and the accumulated updates\nwere performed 4 times. Thus, the total batch size is 49,152\n(4,096 ×3 ×4). The warmup was performed 1,000 times,\nwhich was 10% of the total steps.\nB. PERFORMANCE COMPARISON OF SCIDEBERTA\nAND SCIDEBERTA V2 IN SCIERC DATASET\nSciDeBERTa [12] is a model trained through continual learn-\ning on the S2ORC abstract dataset based on the DeBERTa ar-\nchitecture. In contrast, SciDeBERTa v2 is a domain-speciﬁc\nknowledge model trained from scratch on the S2ORC full\ndataset. We evaluated and compared the performances of\nSciDeBERTa [12] and SciDeBERTa v2 on the SciERC\ndataset. As shown in Table 4, SciDeBERTa v2 outperforms\nSciDeBERTa on the SciERC NER and JRE tasks.\nC. PERFORMANCE COMPARISON OF\nDOMAIN-SPECIFIC PLMS BASED ON TRAINING\nMETHODS AND DATASETS\nIn this section, we compared the model performance accord-\ning to the pretraining algorithm and corpus type of medicine-\nrelated data, as detailed in Section III-B and illustrated in\nFig. 4. We examined the inﬂuence of the pretraining algo-\nrithm, the base model, and the type of training data, given\ntheir pivotal roles in the development of domain-speciﬁc\nLMs. Table 5 summarizes each model’s conﬁguration and av-\nerage performance on the BLURB benchmark. The detailed\nresults for each task are presented in Table 6. We selected the\n#1 model as the MediBioDeBERTa from the experimental\nresults.\nThe performance results of models #2 and #3 align with\nthose of the existing PubMedBERT [11] and BioBERT [10],\nindicating that training specialized domain data from scratch\nyields better results than ﬁne-tuning on top of a general\ndomain model. The most favorable outcome was observed\nin model #1, in which the base LM for continuous learning\nwas also specialized in the language domain. Based on these\nﬁndings, we utilized MediBioDeBERTa with SciDeBERTa\nv2 as the base model for continuous learning.\nIn models #3 and #4, we observed variations in perfor-\nmance depending on the type of corpus. When dealing with\npapers predominantly used in specialized ﬁelds, it is crucial\nto decide whether to rely solely on relatively simpler ab-\nstracts or to integrate comprehensive full-text encompassing\nvarious formats and contents. Previous study [11] showed\nthat full-text data yield better performance when trained\nsufﬁciently to acquire complex knowledge.\nHowever, our empirical observations did not corroborate a\nsigniﬁcant enhancement in performance. Moreover, adjusting\nthe corpus type to abstracts and pursuing additional continu-\nous learning diminished performance. This suggests that the\nefﬁcacy of the study [11] is contingent on the adequacy of the\ntraining data volume.\nConclusively, the comparative experiments presented in\nTable 5 substantiate that biomedical language models man-\nifest optimal performance when trained in a graduated man-\nner, similar to the progressive specialization in biological or\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nTABLE 5: PLM conﬁguration and its average performance of the BLURB benchmark.\nPretraining Algorithm Base model Corpus Type Training Steps BLURB Avg.\n#1 continual learning SciDeBERTa v2 abstract 90,000 78.03\n#2 continual learning DeBERTabasev3 abstract 100,000 76.65\n#3 from scratch DeBERTa-v2 abstract 125,000 77.38\n#4 from scratch DeBERTa-v2 fulltext 125,000 76.46\n#5 continual learning #4 abstract 50,000 76.38\n†Learning data is the medicine-related subset of S2ORC dataset described in III-B.\n†Our experimental results are average values of ﬁve runs.\nTABLE 6: Comparison of the test performances of the BLURB benchmark for PLM for Table 5.\nTask Dataset #1 #2 #3 #4 #5\nNER\nBC5-chem 93.04 93.11 92.39 91.79 92.05\nBC5-disease 85.13 82.18 84.51 84.21 84.68\nNCBI-disease 89.08 89.19 89.05 88.78 88.23\nBC2GM 83.94 84.10 83.27 83.20 83.46\nJNLPBA 80.23 80.13 79.76 79.80 79.79\nPICO EBM PICO 73.73 74.12 73.59 73.67 73.75\nRE\nChem Prot 77.80 74.27 75.53 74.02 74.95\nDDI 80.47 80.75 79.35 79.81 80.07\nGAD 82.23 81.79 78.32 80.17 79.93\nSS BIOSSES 57.68 66.56 62.78 48.39 42.52\nDC HOC 61.09 68.74 70.28 69.13 66.93\nQnA PubmedQA 56.84 49.84 52.00 59.36 58.12\nBioASQ 93.14 71.71 85.14 81.71 85.57\nBLURB Avg. 78.03 76.65 77.38 76.46 76.38\n†Our experimental results are average values of ﬁve runs.\nTABLE 7: Comparison of the test performances of MediBioDeBERTa with other models in the BLURB benchmark.\nTask Dataset BioLinkBERT SciDeBERTa v2 MediBio MediBio\n(metric) (base) (full-FS) DeBERTa DeBERTa-IFT\nNER(F1)\nBC5-chem 93.38 92.75 93.04\nBC5-disease 85.45 84.27 85.13\nNCBI-disease 88.12 89.89 89.08\nBC2GM 84.39 83.97 83.94\nJNLPBA 78.78 66.2 80.23\nNER Avg. 86.02 83.42 86.28\nPICO(Macro F1) EBM PICO 74.2 73.69 73.73\nRE(Micro F1)\nChem Prot 78.1 76.86 93.04\nDDI 81.12 78.75 80.47\nGAD 82.51 80.2 82.23\nRE Avg. 80.81 78.60 80.17\nSS(Micro F1) BIOSSES 92.5 59.54 57.68 92.7\nDC(Micro F1) HOC 84.73 61.42 61.09 71.49\nQnA(Accuracy) PubmedQA 58.32 51.99 56.84 59.33\nBioASQ 91.57 67.86 93.14\nQnA Avg. 74.95 59.93 74.99 76.24\nBLURB Avg. 82.61 74.41 78.03 81.72\n†Our experimental results are average values of ﬁve runs.\nmedical studies built upon a broad science education.\nD. PERFORMANCE COMPARISON OF\nMEDIBIODEBERTA IN THE BLURB BENCHMARK\nAs described in Table 7, the MediBioDeBERTa achieved the\nbest performance in three tasks, named entity recognition\n(NER), sentence similarity (SS), and question & answering\n(Q&A), with average scores of 86.28%, 92.7%, and 76.32%,\nrespectively. However, BioLinkBERT, which accommodates\nextensive document cross-references, outperformed the pa-\ntient intervention comparison outcomes (PICO), relation\nextraction (RE), and document classiﬁcation (DC) tasks.\nMediBioDeBERTa, encompassing not only ‘medicine’ and\n‘biology,’ but also ‘chemistry’ category articles, showed a\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\nsigniﬁcant improvement of 14.94% over BioLinkBERT [19]\nin the ChemProt RE task, with an F1 score of 93.04%.\nFurthermore, the IFT of MediBioDeBERTa enhanced the\nperformance by 35.02%, 10.4%, and 1.49% in the BIOSSES,\nHOC, and PubmedQA tasks, respectively.\nE. EXPERIMENTS FOR IFT OF MEDIBIODEBERTA\nWe demonstrated the results of the IFT to improve the\nperformance of sequence pair tasks. We aimed to transfer\ntask knowledge through the IFT and tested both general\nand biomedical domain datasets to investigate dependencies\nbased on the domain of the dataset used for the IFT. We used\nthe Semantic Textual Similarity Benchmark (STSB) task of\nGLUE benchmark and ClinicalSTS datasets as the general\nand biomedical domain datasets, respectively. ClinicalSTS\nused both the ClinicalSTS2018 [24] and ClinicalSTS2019\ndatasets [25]. The IFT using both datasets improved the per-\nformance of the BIOSSES task, which is a sentence similarity\ntask of the BLURB leaderboard, as shown in Table 2. An\ninteresting observation from the experimental results was that\nusing a general domain dataset led to better performance\nthan using a biomedical domain dataset. This suggests that\nusing a target domain dataset for task knowledge transfer\nthrough the IFT is not always necessary. This is predicted\nbecause domain knowledge has already been sufﬁciently\nlearned during pretraining.\nAs suggested in SciREpEval models[23], we utilized both\nregression and classiﬁcation formats for the IFT of the HOC\ntask, which is a document classiﬁcation task. The regression\nformat consisted of the citation count and year of publication\nand the classiﬁcation format included mesh descriptors and\nﬁelds of study as described in Table 2.\nComparing the HOC performance of model #1 in Table 6\nbefore applying IFT and MediBioDeBERTa in Table 7 after\napplying IFT, the performance increased by 10.4%. Sim-\nilarly, four tasks in two different formats, prediction, and\nsearch, were employed to enhance the performance of Pub-\nMedQA. The prediction format utilized a citation prediction\ndataset and the search format involved searching for the same\nauthor and high-inﬂuence citations. This approach led to a\nperformance improvement of 1.49%, as shown in Table 7.\nVI. CONCLUSION\nThis study ﬁrst presented SciDeBERTa v2, an LM trained\nfrom scratch on a scientiﬁc domain-speciﬁc S2ORC dataset\nusing DeBERTa. SciDeBERTa v2 achieved superior perfor-\nmance compared to its predecessor, SciDeBERTa. To adapt\nthe model to the bio-medical domain, we extracted biomed-\nical data from S2ORC using correlation analysis and trained\nMediBioDeBERTa. Applying IFT enabled us to improve\ndomain-speciﬁc task performance simply and effectively.\nOur model, MediBioDeBERTa, outperformed the state-of-\nthe-art models in categories such as NER, SS, and Q&A and\nthe ChemProt RE task, ranking 11th in the BLURB leader-\nboard with an average score of 81.72. Recently, generative\nlanguage models like BioGPT [15] have been employed to\ndirectly pose questions and evaluate responses. In contrast,\nthis study leverages an MLM-based NLU model, evaluat-\ning the QA task performance as a sequence classiﬁcation\ntask. Nevertheless, MediBioDeBERTa remains valuable for\nmedical information processing applications, such as NER\nand sequence classiﬁcation-based QA. These applications\nare crucial for extracting features in paragraphs through\nlanguage understanding. Future work will involve scaling\nup MediBioDeBERTa to a 24-layer model based on the\nrecent deberta v3 architecture [26] and integrating special-\nized biomedical news data. We anticipate improvements in\napplications related to infectious diseases.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT (1), 2019.\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong et al., “A survey of large language models,” arXiv\npreprint arXiv:2303.18223, 2023.\n[4] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[5] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Al-\nbert: A lite bert for self-supervised learning of language representations,”\narXiv preprint arXiv:1909.11942, 2019.\n[6] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy,\n“Spanbert: Improving pre-training by representing and predicting spans,”\nTransactions of the Association for Computational Linguistics, vol. 8, pp.\n64–77, 2020.\n[7] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-\nenhanced bert with disentangled attention,” in International\nConference on Learning Representations, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=XPZIaotutsD\n[8] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language model\nfor scientiﬁc text,” in Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), 2019,\npp. 3615–3620.\n[9] K. Lo, L. L. Wang, M. Neumann, R. Kinney, and D. S. Weld, “S2orc:\nThe semantic scholar open research corpus,” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, 2020,\npp. 4969–4983.\n[10] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:\na pre-trained biomedical language representation model for biomedical\ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[11] Y . Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann,\nJ. Gao, and H. Poon, “Domain-speciﬁc language model pretraining for\nbiomedical natural language processing,” ACM Transactions on Comput-\ning for Healthcare (HEALTH), vol. 3, no. 1, pp. 1–23, 2021.\n[12] Y . Jeong and E. Kim, “Scideberta: Learning deberta for science technology\ndocuments and ﬁne-tuning information extraction tasks,” IEEE Access,\nvol. 10, pp. 60 805–60 813, 2022.\n[13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing sys-\ntems, vol. 33, pp. 1877–1901, 2020.\n[14] E. Lehman, E. Hernandez, D. Mahajan, J. Wulff, M. J. Smith, Z. Ziegler,\nD. Nadler, P. Szolovits, A. Johnson, and E. Alsentzer, “Do we still need\nclinical language models?” Proceedings of Machine Learning Research,\nConference on Health, Inference, and Learning, vol. 209, pp. 578–597,\n2023.\n[15] R. Luo, L. Sun, Y . Xia, T. Qin, S. Zhang, H. Poon, and T.-Y . Liu,\n“Biogpt: generative pre-trained transformer for biomedical text generation\nand mining,” Brieﬁngs in Bioinformatics, vol. 23, no. 6, 2022.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEunhui Kimet al.: MediBioDeBERTa\n[16] Y . Luan, L. He, M. Ostendorf, and H. Hajishirzi, “Multi-task identiﬁcation\nof entities, relations, and coreference for scientiﬁc knowledge graph con-\nstruction,” in Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, 2018, pp. 3219–3232.\n[17] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural language\nunderstanding,” in International Conference on Learning Representations,\n2018.\n[18] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, “Superglue:\nLearning feature matching with graph neural networks,” in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 4938–4947.\n[19] M. Yasunaga, J. Leskovec, and P. Liang, “Linkbert: Pretraining language\nmodels with document links,” in ICML 2022 2nd AI for Science Work-\nshop, 2022.\n[20] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,\nA. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language models\nencode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–180,\n2023.\n[21] T. Kudo and J. Richardson, “Sentencepiece: A simple and language in-\ndependent subword tokenizer and detokenizer for neural text processing,”\nin Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, 2018, pp. 66–71.\n[22] H. Zhou, B. Gu, X. Zou, Y . Li, S. S. Chen, P. Zhou, J. Liu, Y . Hua,\nC. Mao, X. Wu, Z. Li, and F. Liu, “A survey of large language mod-\nels in medicine:progress, application, and challenge,” arXiv preprint\narXiv:2311.05112v1, 2023.\n[23] A. Singh, M. D’Arcy, A. Cohan, D. Downey, and S. Feldman, “Scirepeval:\nA multi-format benchmark for scientiﬁc document representations,” arXiv\npreprint arXiv:2211.13308, 2022.\n[24] Y . Wang, N. Afzal, S. Liu, M. Rastegar-Mojarad, L. Wang, F. Shen, S. Fu,\nand H. Liu, “Overview of the biocreative/ohnlp challenge 2018 task 2: clin-\nical semantic textual similarity,” Proceedings of the BioCreative/OHNLP\nChallenge, vol. 2018, 2018.\n[25] Y . Wang, S. Fu, F. Shen, S. Henry, O. Uzuner, H. Liu et al., “The 2019\nn2c2/ohnlp track on clinical semantic textual similarity: overview,” JMIR\nmedical informatics, vol. 8, no. 11, p. e23375, 2020.\n[26] P. He, J. Gao, and W. Chen, “Deberta v3: Improving deberta using\nelectra-style pre-training with gradient-disentangled embedding sharing,”\nInternational Conference on Learning Representations(ICLR), 2023.\nEUNHUI KIM received the M.S. and Ph.D. de-\ngrees in electrical engineering from the Korea\nAdvanced Institute of Science and Technology\n(KAIST), Korea, in 2009 and 2015, respectively,\nand received the B.S. degree in information com-\nmunication engineering from Chungnam National\nUniversity, Korea, in 2000. From 2000 to 2007,\nshe was a researcher with Samsung Electronics in\nSeoul, Korea. From 2015 to 2018, she was a post-\ndoctoral researcher at KAIST, Korea. In 2018, she\nwas an invited professor with the National Center of Excellence in Software,\nat Chungnam National University, Korea. Since 2019, she has worked in the\nData AI Center of the Korea Institute of Science and Technology Information\nas a senior researcher. Dr. Kim’s research interests include machine learning,\nrecommendation systems, lightweight deep neural network modeling in\nvision and language processing, and language modeling and its applications.\nYUNA JEONG received a B.S. degree in com-\nputer engineering at Korea Polytechnic University\n(2012) and a Ph.D. degrees in computer engi-\nneering at Sungkyunkwan University (2019). She\nis a senior researcher in the Open XR Platform\nResearch Center at the Korea Institute of Science\nand Technology Information (KISTI). Her main\nresearch interests include computer graphics, deep\nlearning, and natural language processing.\nMYUNG-SEOK CHOI received his B.S., M.S.,\nand Ph.D. degrees from the Department of Com-\nputer Science at the Korea Advanced Institute of\nScience and Technology (KAIST), Korea, in 1996,\n1998, and 2005, respectively. Since joining the\nKorea Institute of Science and Technology Infor-\nmation (KISTI) in 2005, he has held various roles\nand is currently the director of the AI Data Re-\nsearch Center at KISTI. His research interests are\nin the ﬁelds of machine learning, natural language\nprocessing, and open science.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3341612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8585956692695618
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7405508756637573
    },
    {
      "name": "Language model",
      "score": 0.6394045352935791
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6260507106781006
    },
    {
      "name": "Task (project management)",
      "score": 0.6182187795639038
    },
    {
      "name": "Biomedical text mining",
      "score": 0.5862642526626587
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5780362486839294
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5550216436386108
    },
    {
      "name": "Relationship extraction",
      "score": 0.512388288974762
    },
    {
      "name": "Natural language processing",
      "score": 0.4751795828342438
    },
    {
      "name": "Field (mathematics)",
      "score": 0.46152061223983765
    },
    {
      "name": "Performance improvement",
      "score": 0.4268404245376587
    },
    {
      "name": "Relation (database)",
      "score": 0.41123440861701965
    },
    {
      "name": "Machine learning",
      "score": 0.3225451707839966
    },
    {
      "name": "Information extraction",
      "score": 0.2426934540271759
    },
    {
      "name": "Data mining",
      "score": 0.12568089365959167
    },
    {
      "name": "Text mining",
      "score": 0.09800884127616882
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I878022262",
      "name": "Korea Institute of Science & Technology Information",
      "country": "KR"
    }
  ],
  "cited_by": 5
}