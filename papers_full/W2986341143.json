{
  "title": "Deep Bidirectional Transformers for Relation Extraction without Supervision",
  "url": "https://openalex.org/W2986341143",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2023441051",
      "name": "Yannis Papanikolaou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973492580",
      "name": "Ian Roberts",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1889740267",
      "name": "Andrea Pierleoni",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2404161646",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2138516811",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963021258",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2513378248",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2963454301",
    "https://openalex.org/W2891417293",
    "https://openalex.org/W2152135319",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W2760600531",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2149713870",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2155454737",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2884668708",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2052217781",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W174427690",
    "https://openalex.org/W2251135946",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2127978399",
    "https://openalex.org/W1493490255"
  ],
  "abstract": "We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a knowledge base. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting data set is employed to fine tune a pre-trained BERT model in order to perform relation extraction. Empirical evaluation on four data sets from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning.",
  "full_text": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 67–75\nHong Kong, China, November 3, 2019.c⃝2019 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n67\nDeep Bidirectional Transformers for Relation Extraction\nwithout Supervision\nYannis Papanikolaou, Ian Roberts, Andrea Pierleoni\nHealx, Cambridge, UK\n{yannis.papanikolaou, ian.roberts, andrea.pierleoni}@healx.io\nAbstract\nWe present a novel framework to deal with\nrelation extraction tasks in cases where there\nis complete lack of supervision, either in the\nform of gold annotations, or relations from\na knowledge base. Our approach leverages\nsyntactic parsing and pre-trained word em-\nbeddings to extract few but precise relations,\nwhich are then used to annotate a larger cor-\npus, in a manner identical to distant supervi-\nsion. The resulting data set is employed to\nﬁne tune a pre-trained BERT model in order\nto perform relation extraction. Empirical eval-\nuation on four data sets from the biomedical\ndomain shows that our method signiﬁcantly\noutperforms two simple baselines for unsuper-\nvised relation extraction and, even if not using\nany supervision at all, achieves slightly worse\nresults than the state-of-the-art in three out of\nfour data sets. Importantly, we show that it is\npossible to successfully ﬁne tune a large pre-\ntrained language model with noisy data, as op-\nposed to previous works that rely on gold data\nfor ﬁne tuning.\n1 Introduction\nThe last years have seen a number of important\nadvances in the ﬁeld of Relation Extraction (RE),\nmainly based on deep learning models (Zeng et al.,\n2014, 2015; Lin et al., 2016; Zeng et al., 2016; Wu\net al., 2017; Verga et al., 2018). These advances\nhave led to signiﬁcant improvements in bench-\nmark tasks for RE. The above cases assume the\nexistence of some form of supervision either man-\nually annotated or distantly supervised data (Mintz\net al., 2009), where relations from a knowledge\nbase are used in order to automatically annotate\ndata, which then can be used as a noisy train-\ning set. For most real-world cases manually la-\nbeled data is either limited or completely missing,\nso typically one resorts to distant supervision to\ntackle a RE task.\nVerb Relation Similarity\napply use treat 0.40\ninvestigate administer treat 0.51\nhave manage treat 0.60\nevaluate improve treat 0.41\nbe eradicate treat 0.55\ndevelop cause cause 0.81\ninduce exacerbate cause 0.58\nknow contribute cause 0.41\nresult lead cause 0.57\nrelate induce cause 0.47\nTable 1: Examples of verb mappings for compound-\ndisease relation. Each verb (can be n-gram as well)\nis mapped to its closest class ( cause, treat) with pre-\ntrained word embeddings.\nThere exist cases though, where even the dis-\ntant supervision approach cannot be followed due\nto the lack of a knowledge base. This is often the\ncase in domains like the Web or the biomedical\nliterature, where entities of interest might be re-\nlated with other entities and no available supervi-\nsion signal exists.\nIn this work, we propose an approach to deal\nwith such a scenario, from a purely unsupervised\napproach, that is without providing any manual\nannotation or any supervision whatsoever. Our\ngoal is to provide a framework that enables a pre-\ntrained language model to be self-ﬁne tuned1 on a\nset of predeﬁned relation types, in situations with-\nout any existing training data and without the pos-\nsibility or budget for human supervision.\nOur method proceeds as follows:\n•The data are ﬁrst parsed syntactically, ex-\ntracting relations of the form subject-verb-\nobject. The resulting verbs are embedded in a\n1We employ the term self-ﬁne tuned to denote that the\nmodel creates its own data set, without any supervision.\n68\nvector space along with the relation types that\nwe are interested to learn and each is mapped\nto their most similar relation type. Table 1\nshows an example of this mapping process.\nThis process is entirely automatic, we only\nprovide the set of relation types that we are\ninterested in and a threshold below which a\nverb is mapped to a Null class.\n•Subsequently, we use these extracted re-\nlations identically to a distant supervision\nsignal to annotate automatically all co-\noccurrences of entities on a large corpus.\n•The resulting data set is used to ﬁne tune\na Deep Bidirectional Transformer (BERT)\nmodel (Devlin et al., 2018).\nImportantly, the ﬁrst step ensures that the result-\ning relations will have high precision (although\nat the expense of low recall), since they largely\nexclude the possibility of the two entities co-\noccurring randomly in the sentence, through the\nsubject-verb-object association. In other words,\nwe end up with a small, but high quality set of re-\nlations, which can then be used in a way identical\nto distant supervision.\nThe main contribution of this work is the in-\ntroduction of a novel framework to deal with RE\ntasks without any supervision, either manually an-\nnotated data or known relations. Our approach\nis empirically evaluated on four data sets. A\nsecondary implication of our work involves how\nwe employ a pre-trained language model such as\nBERT: unlike previous approaches that employ a\nsmall gold data set, we show that it is possible to\ninstead use a large noisy data set to successfully\nﬁne tune such a model.\nThe rest of the paper is organized as follows:\nwe describe the related work in Section 2, subse-\nquently describing our method in Section 3 and\npresenting the empirical evaluation results in Sec-\ntion 4.\n2 Related work\nDealing with relation extraction in the absence\nof training data is not a novel task: for more\nthan a decade, researchers have employed suc-\ncessfully techniques to tackle the lack of super-\nvision, mainly by resorting to distant supervision\n(Mintz et al., 2009; Riedel et al., 2010). This ap-\nproach assumes the existence of a knowledge base,\nwhich contains already known relations between\nspeciﬁc entities. These relations are then used to\nautomatically annotate texts containing these en-\ntity pairs. Although this approach leads to noisy\nlabelling, it is cheap and has the ability to leverage\na vast amount of training data. A great body of\nwork has built upon this approach aiming to alle-\nviate the noise in annotations, using formulations\nsuch as multi-label multi-instance learning (Sur-\ndeanu et al., 2012; Zeng et al., 2015), employ-\ning generative models to reduce wrong labelling\n(Takamatsu et al., 2012), developing different loss\nfunctions for relation extraction (dos Santos et al.,\n2015; Wang et al., 2016) or using side information\nto constraint predicted relations (Vashishth et al.,\n2018).\nMore recently, a number of other interesting\napproaches have been presented aiming to deal\nwith the lack of training data, with direct appli-\ncation to RE: data programming (Ratner et al.,\n2016) provides a framework that allows domain\nexperts to write labelling functions which are then\ndenoised through a generative model. Levy et al.\n(2017) have formulated the relation extraction task\nas a reading comprehension problem by associat-\ning one or more natural language questions with\neach relation. This approach enables generaliza-\ntion to unseen relations in a zero-shot setting.\nOur work is different from the aforementioned\napproaches, in that it does not rely on the existence\nof any form of supervision. We build a model that\nis driven by the data, discovering a small set of\nprecise relations, using them to annotate a larger\ncorpus and being self-ﬁne tuned to extract new re-\nlationships.\nTo train the RE classiﬁer, we employ BERT,\na recently proposed deep language model that\nachieved state-of-the-art results across a variety\nof tasks. BERT, similarly to the works of Rad-\nford et al. (2018a) and Radford et al. (2018b),\nbuilds upon the idea of pre-training a deep lan-\nguage model on massive amounts of data and then\napplies it (by ﬁne tuning) to solve a diverse set of\ntasks. The building block of BERT is the Tran-\nformer model (Vaswani et al., 2017), a neural net-\nwork cell that uses a multi-head, self-attention\nmechanism.\nThe ﬁrst step of our approach is highly remi-\nniscent of approaches from the open Information\nExtraction (openIE) literature (Banko et al., 2007).\nIndeed, similar to openIE approaches, we also use\n69\nsyntactic parsing to extract relations. Neverthe-\nless, unlike openIE we are interested in a) speciﬁc\ntypes of entities which we assume that have been\npreviously extracted with Named Entity Recogni-\ntion (NER) and b) in speciﬁc, predeﬁned types of\nrelations between entities. We use syntactic pars-\ning only as a means to extract a few precise rela-\ntions and then follow an approach similar to dis-\ntant supervision to train a neural relation extrac-\ntion classiﬁer. It should be noted though, that as a\npotential extension of this work we could employ\nmore sophisticated techniques instead of syntactic\nparsing, similar to the latest openIE works (Yahya\net al., 2014)\n3 Method and Implementation Details\nWe present here the details of our method. First,\nwe describe how we create our training set which\nresults from a purely unsupervised procedure dur-\ning which the only human intervention is to de-\nﬁne the relation types of interest, e.g., ’treat’ or\n’associate’. Subsequently, we describe BERT, the\nmodel that we use in our approach.\n3.1 Training Set Creation\nOur method assumes that the corpus is split in\nsentences2, which are then passed through a NER\nmodel and a syntactic parser. We use the spaCy\nlibrary3 for the above steps.\nGiven a pair of two entities A and B, we ﬁnd\ntheir shortest dependency path and if one or more\nverbs V are in that path we assume that A is re-\nlated to B with V . The next step involves map-\nping the verbs to a set of predeﬁned relation types,\nas shown in Table 1. To do so, we embed both\nrelation types and verbs to a continuous, lower-\ndimensional space with a pre-trained skip gram\nmodel (Mikolov et al., 2013), and map each verb\nto its closest relation type, if the cosine similarity\nof the two vectors is greater than a threshold (in\ninitial small scale experiments using a validation\nset, we have found that a threshold = 0.4 works\nwell). Otherwise, the verb is not considered to rep-\nresent a relation. In our experiments we used the\npre-trained BioASQ word vectors 4, since our re-\n2We can easily extend to cross-sentence relations, since\nthe Transformer models which are the basis of BERT do not\nsuffer from the problems encountered in LSTMs or CNNs for\nlonger sequences, thanks to their self-attention mechanism.\n3https://spacy.io/\n4http://bioasq.lip6.fr/tools/\nBioASQword2vec/\nlation extraction tasks come from the biomedical\ndomain.\nIt is important to note that in the above proce-\ndure the only human involvement is deﬁning the\nset of relation types that we are interested in. In\nthat sense, this approach is neither domain or scale\ndependent: any set of relations can be used (com-\ning from any domain) and likewise we can con-\nsider any number of relation types.\nThe above procedure results in a small but rel-\natively precise set of relations which can then be\nused in a way similar to distant supervision, to an-\nnotate all of our corpus. Nevertheless, there are a\nnumber of caveats to be taken into consideration:\n•As expected, there will be errors in the re-\nlations that come from the syntactic parsing\nand verbs mapping procedure.\n•Our distant supervision-like approach comes\nalso with inherent noise: we end up with a\ntraining set that has a lot of false negative and\nalso a few false positive errors.\n•The resulting training set will be largely im-\nbalanced, since the way that we extract rela-\ntions sacriﬁces recall for precision.\nTo deal with the above noise, we employ BERT\nas a relation extraction classiﬁer. Furthermore, we\nuse a balanced bagging approach to deal with class\nimbalance. Both approaches are described in de-\ntail in the following section.\n3.2 Deep Bidirectional Transformers\nBERT is a deep learning network that focus in\nlearning general language representations which\ncan then be used in downstream tasks. Much like\nthe work of Radford et al. (2018a) and Radford\net al. (2018b), the general idea is to leverage the\nexpressive power of a deep Transformer architec-\nture that is pre-trained on a massive corpus on a\nlanguage modelling task. Indeed, BERT comes\nin two ﬂavors of 12 and 24 layers and 110M and\n340M parameters respectively and is pre-trained\non a concatenation of the English Wikipedia and\nthe Book Corpus (Zhu et al., 2015). The result-\ning language model can then be ﬁne tuned across\na variety of different NLP tasks.\nThe main novelty of BERT is its ability to\npre-train bidirectional representations by using a\nmasked language model as a training objective.\nThe idea behind the masked language model is to\n70\nrandomly mask some of the word tokens from the\ninput, the objective being to predict what that word\nactually is, based on its context. The model is si-\nmultaneously trained on a second objective in or-\nder to model sentence relationships, that is, given\ntwo sentences senta and sentb predict if sentb is\nthe next sentence after senta.\nBERT has achieved state-of-the-art across\neleven NLP tasks using the same pre-trained\nmodel and only with some simple ﬁne tuning. This\nmakes it particularly attractive for our use case,\nwhere we need a strong language model that will\nbe able to learn from noisy patterns.\nIn order to further deal with the challenges\nmentioned in the previous section, in our exper-\niments we ﬁne tuned BERT for up to 5 epochs,\nsince in early experiments we noticed that the\nmodel started overﬁtting to noise and validation\nloss started increasing after that point.\n3.3 Balanced Bagging\nIn order to deal with class imbalance we employed\nbalanced bagging (Tao et al., 2006), an ensembling\ntechnique where each component model is trained\non a sub-sample of the data, such that the neg-\native examples are roughly equal to the positive\nones. To train each model of the ensemble, we\nsub-sample only the negative class so as to end up\nwith a balanced set of positives and negatives.\nThis sub-sampling of the negative class is im-\nportant not only in order to alleviate the data im-\nbalance, but also because the negative class will\ncontain more noise than the positive by deﬁnition\nof our approach. In other words, since we consider\nas positives only a small set of relations coming\nfrom syntax parsing and verb mapping, it is more\nlikely that a negative is in reality a positive sample\nrather than the opposite.\n4 Experiments\nIn this section we ﬁrst describe the data sets used\nin experiments and the experimental setup and\nthen present the results of our experiments.\n4.1 Data Sets and Setup\nWe evaluate our method on four data sets coming\nfrom the biomedical domain, expressing disease-\ndrug and disease-gene relations. Three of them\nare well known benchmark data sets for relation\nextraction: The Biocreative chemical-disease re-\nlations (CDR) data set (Li et al., 2016), the Ge-\nnetic Association Database (GAD) data set (Bravo\net al., 2015) and the EU-ADR data set (Van Mul-\nligen et al., 2012). Additionally, we present a pro-\nprietary manually curated data set, Healx CD, ex-\npressing therapeutic drug-disease relations. We\nconsider only sentence-level relations, so we split\nCDR instances into sentences (the rest of the data\nsets are already at sentence-level). Statistics for\nthe data sets are provided in Table 2. We should\nnote that for our approach we map each verb to the\nrespective relation class that is depicted in Table 2\nin parentheses.\nAs stated, we are mainly interested to under-\nstand how our proposed method performs under\ncomplete lack of training signal, so we compare\nit with two simple baselines for unsupervised re-\nlation extraction. The ﬁrst, assumes that a sen-\ntence co-occurrence of two entities signals a posi-\ntive relation, while the second is equivalent to the\nﬁrst two steps of our method, syntactic parsing fol-\nlowed by verb mapping to the relation types of in-\nterest. In other words, if two entities are connected\nin the shortest dependency path through a verb that\nis mapped to a class, they are considered to be re-\nlated with that class.\nAdditionally, we would like to understand how\nour method performs against supervised methods,\nso for the ﬁrst three data sets we compare it with a\nBERT model trained on the respective gold data,\nreporting also the current state-of-the-art, while\nfor the Healx CD data set since there are no man-\nual annotations, we compare our method against\na distant supervision approach, retrieving ground\ntruth relations from our internal knowledge base.\nAcross all experiments and for all methods we\nuse the same BERT model, BioBERT (Lee et al.,\n2019), which is a BERT model initialized with\nthe model from Devlin et al. (2018) and then pre-\ntrained on PubMed, and thus more relevant to our\ntasks. That model is ﬁne tuned on relation extrac-\ntion classiﬁcation using the code provided by the\nBioBERT authors, either on the gold or the dis-\ntantly supervised or our approach’s training set.\nWe ﬁne tune for up to 5 epochs with a learning\nrate of 0.00005 and a batch size of 128, keeping\nthe model that achieves the best loss on the respec-\ntive validation set.\nFinally, for the distant supervision as well as for\nour method, we use the previously mentioned bal-\nanced bagging approach, ﬁne tuning an ensemble\nof ten models for each relation.\n71\nData set Relation (class) # Train (pos) # Dev (pos) # Test (pos)\nAnnotated\nCDR Drug-Disease (cause) 3,596(1,453) 3,875(1,548) 3,805(1,482)\nGAD Disease-Gene (cause) 5,330(1,834) - -\nEUADR Disease-Gene (cause) 355(243) - -\nHealx CD Drug-Disease (treat) 564(325) - -\nDist.Sup.\n250k Drug-Disease (treat) 250k(35k) - -\nfull Drug-Disease (treat) 8m(1.1m) - -\nOur approach\n250k Drug-Disease (treat, cause) 250k(70k 10k) - -\nfull Drug-Disease (treat, cause) 8m(2.2m 325k) - -\n250k Disease-Gene (cause) 250k(62k) - -\nfull Disease-Gene (cause) 9.1m(2.2m) - -\nTable 2: Data sets used in our experiments. ’Our approach’ stands for the procedure described in Section 3.1.\nThe Drug-Disease relation for our approach yields two positive classes, treat and cause, therefore we report\naccordingly positives from each class in parentheses.\n4.2 Results\nTable 3 shows the results for the four data sets,\nreporting the average over ﬁve runs. For the\nGAD and EU-ADR data sets, we use the train and\ntest splits provided by Lee et al. (2019). Also,\nfor CDR, since the state-of-the-art results (Verga\net al., 2018) are given at the abstract level, we re-\nrun their proposed algorithm on our transformed\nsentence-level CDR data set, reporting results for a\nsingle model, without additional data (Verga et al.\n(2018) reports also results when adding weakly la-\nbeled data).\nLet us ﬁrst focus on the two unsupervised base-\nlines. The ﬁrst, dubbed ’co-occurrences’, achieves\na perfect recall since it considers all entity pairs\nco-occurrences as expressing a relation, but is\nclearly sub-optimal with regards to precision. The\nopposite behaviour is observed for the second\nbaseline (syntactic parsing with verb mapping)\nsince that one focuses in extracting high-precision\nrelations, sacriﬁcing recall: only entity pairs with\na verb in between that is mapped to a relation\nare considered positives. Notably, this baseline\nachieves the highest precision in two out of four\ndata sets, even compared to the supervised meth-\nods.\nOur method proves signiﬁcantly better com-\npared to the other two unsupervised baselines, out-\nperforming them by a large margin in all cases\napart for EUADR. In that case our method is\nslightly worse than the co-occurrences baseline,\nsince EUADR contains a big percentage of posi-\ntives. Speciﬁcally, it is interesting to observe the\nimprovement over the second baseline, which acts\nas a training signal for our method. Thanks to the\npredictive power and the robustness of BERT, our\nmethod manages to learn useful patterns from a\nnoisy data set and actually improve substantially\nupon its training signal.\nAn additional advantage of our method com-\npared to the two other unsupervised baselines and\nsimilar approaches in general, is that it outputs a\nprobability. Unlike the other methods, this proba-\nbility allows us to tune our method for better pre-\ncision or recall, depending on the application.\nWe then focus on comparing our proposed ap-\nproach against the same BERT model ﬁne tuned\non supervised data, either manually annotated for\nthe ﬁrst three data sets, or distantly annotated for\nthe fourth. For the ﬁrst three data sets, we also\nreport the current state-of-the-art results. Interest-\ningly, even if our method is completely unsuper-\nvised, it is competitive with the state-of-the-art of\nfully supervised methods in three out of four cases,\nbeing inferior to them from 3.7 to 14.1 F1 points.\nOn average, our method is worse by 7.5 F1 points\nagainst the best supervised model (either BERT or\ncurrent state-of-the-art).\nThese results are particularly important, if we\ntake into account that they come from a proce-\ndure that is fully unsupervised and which entails\nsubstantial noise from its sub-steps: the syntac-\ntic parsing may come with errors and mapping\nthe verbs to relevant relation types is a process\n72\nData set Method Precision Recall F1\nCDR\nUnsupervised Co-occurrences 30.9 100.0 47.2\nsyntactic parsing+verb mapping 84.0 8.5 15.4\nOur method on BERT (250k) 49.4 76.3 60.4\nOur method on BERT (full) 50.1 81.3 62.2\nSupervised SOTA (Verga et al., 2018) 64.2 68.5 66.3\nGold Data on BERT 61.1 80.3 70.4\nGAD\nUnsupervised Co-occurrences 34.4 100.0 51.2\nsyntactic parsing+verb mapping 71.9 9.9 17.4\nOur method on BERT (250k) 53.1 82.8 64.6\nOur method on BERT (full) 56.9 90.1 69.8\nSupervised SOTA (Bhasuran and Natarajan, 2018) 79.2 89.2 83.9\nGold Data on BERT 76.4 87.7 81.7\nEUADR\nUnsupervised Co-occurrences 68.5 100.0 81.3\nsyntactic parsing+verb mapping 70.1 6.9 12.1\nOur method on BERT (250k) 71.7 79.4 75.5\nOur method on BERT (full) 75.5 87.9 81.2\nSupervised SOTA (Bhasuran and Natarajan, 2018) 76.4 98.0 85.3\nGold Data on BERT 78.0 93.9 85.2\nHealx CD\nUnsupervised Co-occurrences 57.6 100.0 73.0\nsyntactic parsing+verb mapping 91.0 17.9 29.9\nOur method on BERT (250k) 73.4 85.1 79.0\nOur method on BERT (full) 74.4 90.0 81.4\nSupervised Distant Supervision on BERT (250k) 83.3 83.1 83.4\nDistant Supervision on BERT (full) 87.1 83.2 85.1\nTable 3: Results on relation classiﬁcation. State-of-the-art results were obtained from the corresponding papers.\nWe averaged over ﬁve runs and report the evaluation metrics for a0.5 probability threshold.\nlargely subject to the quality of the embeddings.\nEven worse, the relations obtained from the pre-\nvious steps are used to automatically annotate all\nco-occurrences in a distant supervision-like fash-\nion, which leads to even more noise.\nWhat we show empirically here is that despite\nall that noise coming from the above unsupervised\nprocedure, we manage to successfully ﬁne tune a\ndeep learning model so as to achieve comparable\nperformance to a fully supervised model. BERT\nis the main factor driving this robustness to noise\nand it can be mainly attributed to the fact that it\nconsists of a very deep language model (112M pa-\nrameters) and that it is pre-trained generatively on\na massive corpus (3.3B words). The signiﬁcance\nof these results is further ampliﬁed if we consider\nhow scarce are labeled data for tasks such as rela-\ntion extraction.\n4.3 Qualitative Analysis\nAlthough we showed empirically that our pro-\nposed approach is consistently capable to achieve\nresults comparable to the SOTA, we would like to\nfurther focus on what are the weak points of the\nsyntax parsing method and of our approach com-\npared to a fully supervised approach.\nTo this end we inspected manually examples of\npredictions of the three aforementioned methods\non the CDR data set, focusing on failures of our\nmethod and the syntactic parsing method which\nacts as training signal of our approach. Table 4\nshows some characteristic cases:\n•In the ﬁrst sentence, the syntactic pars-\ning+verb mapping baseline (SP+VM) fails\nsince the verb ( developed) is not associated\nwith cause. Conversely our method, BERT\n73\nSentence class BERT+gold BERT+SP+VM SP+VM\nA patient with renal disease developed\ncoombs-positive DISEASE while\nreceiving COMPOUND therapy. cause 0.98 0.69 Null (developed)\nFive cases of DISEASE during treatment\nof loiasis with COMPOUND. cause 0.97 0.95 Null\nCOMPOUND induced bradycardia in a\npatient with DISEASE. Null 0.04 0.99 cause (induced)\nNeuroleptic drugs such as haloperidol,\nwhich block COMPOUND receptors,\nalso cause DISEASE in rodents. Null 0.92 0.99 treat (block)\nThe results provide new insight\ninto the potential role of ectopic\nhilar granule cells in the COMPOUND\nmodel of DISEASE. cause 0.89 0.05 Null (provide)\nTable 4: Examples of predictions from the three methods on the CDR data set. SP+VM stands for the syntactic\nparsing+verb mapping baseline, while BERT+SP+VM stands for our method. BERT+gold is a BERT model\ntrained on the gold CDR training set. For SP+VM we also provide the phrase verb in parentheses.\nwith SP+VM manages to model correctly the\nsentence and extract the relation.\n•SP+VM fails in the second example for the\nsame reason, although the sentence is rela-\ntively simple.\n•The third sentence represents also an inter-\nesting case, with SP+VM being ”tricked” by\nthe verb induced. Our method also fails here,\nfailing to attend correctly to the DISEASE\nmasked entity.\n•The fourth example represents a similar case,\nboth BERT-based models are being tricked\nby the language. The SP+VM baseline is er-\nroneously associating the verbblock to the re-\nlation treat instead of cause.\n•The ﬁfth sentence resembles the ﬁrst two:\nSP+VM fails to extract the relation for the\nsame reason (verb in between). Our method\nfails too in that case, perhaps due to the rela-\ntively uncommon way that the causal relation\nis expressed ( COMPOUND model of DIS-\nEASE.\nWhile further inspecting the results, we also no-\nticed a steady tendency of SP+VM to be able to\ncapture relations in simpler (from a syntax per-\nspective) and shorter sentences, while failing in\nthe opposite case.\nOverall, we observe, as expected, that the\nSP+VM method is largely dependent on the sim-\nplicity of the expressed relation. Our method is\nclearly dependent on the quality of the syntax\nparsing, but manages up to a point to overcome\nlow quality training data. To conclude, we can\nsafely assume that our method would further ben-\neﬁt by replacing the SP+VM method with a more\nsophisticated unsupervised approach as the train-\ning signal, a future direction that we intend to take.\n5 Conclusions\nThis work has introduced a novel framework\nto deal with relation extraction tasks in settings\nwhere there is complete lack of supervision. Our\nmethod employs syntactic parsing and word em-\nbeddings to extract a small set of precise relations\nwhich are then used to annotate a larger corpus,\nin the same way as distant supervision. With that\ndata, we ﬁne tune a pre-trained BERT model to\nperform relation extraction.\nWe have empirically evaluated our method\nagainst two unsupervised baselines, a BERT\nmodel trained with gold or distantly supervised\ndata and the current state-of-the-art. The results\nshowed that our approach is signiﬁcantly better\nthan the unsupervised baselines, ranking slightly\nworse than the state-of-the-art in three out of four\ncases.\nApart from presenting a novel perspective on\nhow to train a relation extraction model in the ab-\n74\nsence of supervision, our work also shows empir-\nically that it is possible to successfully ﬁne tune\na deep pre-trained language model with substan-\ntially noisy data.\nWe are interested in extending this paradigm to\nother areas of natural language processing tasks or\nadjusting our framework for more complex rela-\ntion extraction tasks, as well as using more sophis-\nticated unsupervised methods as training signal.\nAcknowledgments\nWe would like to thank Saatviga Sudhahar, for\nher insightful comments that greatly helped in im-\nproving this paper.\nReferences\nMichele Banko, Michael J Cafarella, Stephen Soder-\nland, Matthew Broadhead, and Oren Etzioni. 2007.\nOpen information extraction from the web. In IJ-\nCAI, pages 2670–2676.\nBalu Bhasuran and Jeyakumar Natarajan. 2018. Au-\ntomatic extraction of gene-disease associations from\nliterature using joint ensemble learning. PloS one,\n13(7):e0200699.\n`Alex Bravo, Janet Pi ˜nero, N ´uria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015. Ex-\ntraction of relations between genes and diseases\nfrom text and large-scale data analysis: implica-\ntions for translational research. BMC bioinformat-\nics, 16(1):55.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint arXiv:1901.08746.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extrac-\ntion via reading comprehension. arXiv preprint\narXiv:1706.04115.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nYankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,\nand Maosong Sun. 2016. Neural relation extraction\nwith selective attention over instances. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 2124–2133.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP: Vol-\nume 2-Volume 2, pages 1003–1011. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018a. Improving language under-\nstanding by generative pre-training. Technical re-\nport, Technical report, OpenAi.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018b. Lan-\nguage models are unsupervised multitask learners.\nTechnical report, Technical report, OpenAi.\nAlexander J Ratner, Christopher M De Sa, Sen Wu,\nDaniel Selsam, and Christopher R´e. 2016. Data pro-\ngramming: Creating large training sets, quickly. In\nAdvances in neural information processing systems,\npages 3567–3575.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions with-\nout labeled text. In Joint European Conference\non Machine Learning and Knowledge Discovery in\nDatabases, pages 148–163. Springer.\nCicero dos Santos, Bing Xiang, and Bowen Zhou.\n2015. Classifying relations by ranking with con-\nvolutional neural networks. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), volume 1, pages 626–634.\nMihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,\nand Christopher D Manning. 2012. Multi-instance\nmulti-label learning for relation extraction. In Pro-\nceedings of the 2012 joint conference on empirical\nmethods in natural language processing and compu-\ntational natural language learning, pages 455–465.\nAssociation for Computational Linguistics.\nShingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.\n2012. Reducing wrong labels in distant supervi-\nsion for relation extraction. In Proceedings of the\n50th Annual Meeting of the Association for Compu-\ntational Linguistics: Long Papers-Volume 1 , pages\n721–729. Association for Computational Linguis-\ntics.\n75\nDacheng Tao, Xiaoou Tang, Xuelong Li, and Xindong\nWu. 2006. Asymmetric bagging and random sub-\nspace for support vector machines-based relevance\nfeedback in image retrieval. IEEE Transactions on\nPattern Analysis & Machine Intelligence, (7):1088–\n1099.\nErik M Van Mulligen, Annie Fourrier-Reglat, David\nGurwitz, Mariam Molokhia, Ainhoa Nieto, Gian-\nluca Triﬁro, Jan A Kors, and Laura I Furlong. 2012.\nThe eu-adr corpus: annotated drugs, diseases, tar-\ngets, and their relationships. Journal of biomedical\ninformatics, 45(5):879–884.\nShikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,\nChiranjib Bhattacharyya, and Partha Talukdar. 2018.\nReside: Improving distantly-supervised neural rela-\ntion extraction using side information. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1257–1266.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPatrick Verga, Emma Strubell, and Andrew McCallum.\n2018. Simultaneously self-attending to all mentions\nfor full-abstract biological relation extraction. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), volume 1, pages 872–884.\nLinlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan\nLiu. 2016. Relation classiﬁcation via multi-level at-\ntention cnns. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1298–1307.\nYi Wu, David Bamman, and Stuart Russell. 2017. Ad-\nversarial training for relation extraction. InProceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 1778–1783.\nMohamed Yahya, Steven Whang, Rahul Gupta, and\nAlon Halevy. 2014. Renoun: Fact extraction for\nnominal attributes. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 325–335.\nDaojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.\n2015. Distant supervision for relation extraction via\npiecewise convolutional neural networks. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1753–\n1762.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nJun Zhao, et al. 2014. Relation classiﬁcation via\nconvolutional deep neural network.\nWenyuan Zeng, Yankai Lin, Zhiyuan Liu, and\nMaosong Sun. 2016. Incorporating relation paths\nin neural relation extraction. arXiv preprint\narXiv:1609.07479.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19–27.",
  "topic": "Relationship extraction",
  "concepts": [
    {
      "name": "Relationship extraction",
      "score": 0.8569939136505127
    },
    {
      "name": "Computer science",
      "score": 0.84471595287323
    },
    {
      "name": "Transformer",
      "score": 0.7473281025886536
    },
    {
      "name": "Parsing",
      "score": 0.636001467704773
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5869883298873901
    },
    {
      "name": "Natural language processing",
      "score": 0.5730242729187012
    },
    {
      "name": "Relation (database)",
      "score": 0.5661499500274658
    },
    {
      "name": "Labeled data",
      "score": 0.5011637210845947
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.49468109011650085
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4519875943660736
    },
    {
      "name": "Word (group theory)",
      "score": 0.42572927474975586
    },
    {
      "name": "Feature extraction",
      "score": 0.42531338334083557
    },
    {
      "name": "Training set",
      "score": 0.42248091101646423
    },
    {
      "name": "Information extraction",
      "score": 0.369894802570343
    },
    {
      "name": "Data mining",
      "score": 0.3517584800720215
    },
    {
      "name": "Machine learning",
      "score": 0.3330785632133484
    },
    {
      "name": "Programming language",
      "score": 0.07294559478759766
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 27
}