{
  "title": "Universal Transformers",
  "url": "https://openalex.org/W2866343820",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2273124145",
      "name": "Dehghani, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293495268",
      "name": "Gouws, Stephan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3025185919",
      "name": "Vinyals, Oriol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227555713",
      "name": "Uszkoreit, Jakob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286867883",
      "name": "Kaiser, Łukasz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963187627",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2786167576",
    "https://openalex.org/W2964246695",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2964091467"
  ],
  "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
  "full_text": "Published as a conference paper at ICLR 2019\nUNIVERSAL TRANSFORMERS\nMostafa Dehghani∗† Stephan Gouws∗ Oriol Vinyals\nUniversity of Amsterdam DeepMind DeepMind\ndehghani@uva.nl sgouws@google.com vinyals@google.com\nJakob Uszkoreit Łukasz Kaiser\nGoogle Brain Google Brain\nusz@google.com lukaszkaiser@google.com\nABSTRACT\nRecurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for sequence\nmodeling tasks. However, their inherently sequential computation makes them\nslow to train. Feed-forward and convolutional architectures have recently been\nshown to achieve superior results on some sequence modeling tasks such as machine\ntranslation, with the added advantage that they concurrently process all inputs in\nthe sequence, leading to easy parallelization and faster training times. Despite these\nsuccesses, however, popular feed-forward sequence models like the Transformer\nfail to generalize in many simple tasks that recurrent models handle with ease, e.g.\ncopying strings or even simple logical inference when the string or formula lengths\nexceed those observed at training time. We propose the Universal Transformer\n(UT), a parallel-in-time self-attentive recurrent sequence model which can be\ncast as a generalization of the Transformer model and which addresses these\nissues. UTs combine the parallelizability and global receptive ﬁeld of feed-forward\nsequence models like the Transformer with the recurrent inductive bias of RNNs.\nWe also add a dynamic per-position halting mechanism and ﬁnd that it improves\naccuracy on several tasks. In contrast to the standard Transformer, under certain\nassumptions UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and language\nunderstanding tasks, including the challenging LAMBADA language modeling\ntask where UTs achieve a new state of the art, and machine translation where UTs\nachieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n1 I NTRODUCTION\nConvolutional and fully-attentional feed-forward architectures like the Transformer have recently\nemerged as viable alternatives to recurrent neural networks (RNNs) for a range of sequence\nmodeling tasks, notably machine translation (Gehring et al., 2017; Vaswani et al., 2017). These\nparallel-in-time architectures address a signiﬁcant shortcoming of RNNs, namely their inherently\nsequential computation which prevents parallelization across elements of the input sequence, whilst\nstill addressing the vanishing gradients problem as the sequence length gets longer (Hochreiter et al.,\n2003). The Transformer model in particular relies entirely on a self-attention mechanism (Parikh et al.,\n2016; Lin et al., 2017) to compute a series of context-informed vector-space representations of the\nsymbols in its input and output, which are then used to predict distributions over subsequent symbols as\nthe model predicts the output sequence symbol-by-symbol. Not only is this mechanism straightforward\nto parallelize, but as each symbol’s representation is also directly informed by all other symbols’\nrepresentations, this results in an effectively global receptive ﬁeld across the whole sequence. This\nstands in contrast to e.g. convolutional architectures which typically only have a limited receptive ﬁeld.\nNotably, however, the Transformer with its ﬁxed stack of distinct layers foregoes RNNs’ inductive bias\ntowards learning iterative or recursive transformations. Our experiments indicate that this inductive\n∗Equal contribution, alphabetically by last name.\n†Work performed while at Google Brain.\n1\narXiv:1807.03819v3  [cs.CL]  5 Mar 2019\nPublished as a conference paper at ICLR 2019\nh1\nt\nh2\nt\nhm\nt\nPer Position States\nTime\n…\nSelf-Attention\nSelf-Attention\nSelf-Attention\nTransition Function\nTransition Function\nTransition Function\n… …\nParameters are tied across positions and time steps\nh1\nt+1\nh2\nt+1\nhm\nt+1\n…\nSelf-Attention\nSelf-Attention\nSelf-Attention\nTransition Function\nTransition Function\nTransition Function\nh1\nt+2\nh2\nt+2\nhm\nt+2\n…… …\nFigure 1: The Universal Transformer repeatedly reﬁnes a series of vector representations for each\nposition of the sequence in parallel, by combining information from different positions using\nself-attention (see Eqn 2) and applying a recurrent transition function (see Eqn 4) across all time\nsteps 1 ≤t ≤T. We show this process over two recurrent time-steps. Arrows denote dependencies\nbetween operations. Initially, h0 is initialized with the embedding for each symbol in the sequence.\nht\ni represents the representation for input symbol1 ≤i ≤m at recurrent time-step t. With dynamic\nhalting, T is dynamically determined for each position (Section 2.2).\nbias may be crucial for several algorithmic and language understanding tasks of varying complexity:\nin contrast to models such as the Neural Turing Machine (Graves et al., 2014), the Neural GPU (Kaiser\n& Sutskever, 2016) or Stack RNNs (Joulin & Mikolov, 2015), the Transformer does not generalize\nwell to input lengths not encountered during training.\nIn this paper, we introduce theUniversal Transformer (UT), a parallel-in-time recurrent self-attentive\nsequence model which can be cast as a generalization of the Transformer model, yielding increased\ntheoretical capabilities and improved results on a wide range of challenging sequence-to-sequence\ntasks. UTs combine the parallelizability and global receptive ﬁeld of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs, which seems to be better suited to\na range of algorithmic and natural language understanding sequence-to-sequence problems. As the\nname implies, and in contrast to the standard Transformer, under certain assumptions UTs can be\nshown to be Turing-complete (or “computationally universal”, as shown in Section 4).\nIn each recurrent step, the Universal Transformer iteratively reﬁnes its representations for all symbols\nin the sequence in parallel using a self-attention mechanism (Parikh et al., 2016; Lin et al., 2017),\nfollowed by a transformation (shared across all positions and time-steps) consisting of a depth-wise\nseparable convolution (Chollet, 2016; Kaiser et al., 2017) or a position-wise fully-connected layer\n(see Fig 1). We also add a dynamic per-position halting mechanism (Graves, 2016), allowing the model\nto choose the required number of reﬁnement stepsfor each symboldynamically, and show for the ﬁrst\ntime that such a conditional computation mechanism can in fact improve accuracy on several smaller,\nstructured algorithmic and linguistic inference tasks (although it marginally degraded results on MT).\nOur strong experimental results show that UTs outperform Transformers and LSTMs across a wide\nrange of tasks. The added recurrence yields improved results in machine translation where UTs\noutperform the standard Transformer. In experiments on several algorithmic tasks and the bAbI\nlanguage understanding task, UTs also consistently and signiﬁcantly improve over LSTMs and the\nstandard Transformer. Furthermore, on the challenging LAMBADA text understanding data set UTs\nwith dynamic halting achieve a new state of the art.\n2 M ODEL DESCRIPTION\n2.1 T HE UNIVERSAL TRANSFORMER\nThe Universal Transformer (UT; see Fig. 2) is based on the popular encoder-decoder architecture\ncommonly used in most neural sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014;\nVaswani et al., 2017). Both the encoder and decoder of the UT operate by applying a recurrent neural\nnetwork to the representations of each of the positions of the input and output sequence, respectively.\nHowever, in contrast to most applications of recurrent neural networks to sequential data, the UT does\nnot recur over positions in the sequence, but over consecutive revisions of the vector representations of\neach position (i.e., over “depth”). In other words, the UT is not computationally bound by the number\nof symbols in the sequence, but only by the number of revisions made to each symbol’s representation.\n2\nPublished as a conference paper at ICLR 2019\nIn each recurrent time-step, the representation of every position is concurrently (in parallel) revised\nin two sub-steps: ﬁrst, using a self-attention mechanism to exchange information across all positions\nin the sequence, thereby generating a vector representation for each position that is informed by the\nrepresentations of all other positions at the previous time-step. Then, by applying a transition function\n(shared across position and time) to the outputs of the self-attention mechanism, independently at\neach position. As the recurrent transition function can be applied any number of times, this implies\nthat UTs can have variable depth (number of per-symbol processing steps). Crucially, this is in contrast\nto most popular neural sequence models, including the Transformer (Vaswani et al., 2017) or deep\nRNNs, which have constant depth as a result of applying a ﬁxed stack of layers. We now describe\nthe encoder and decoder in more detail.\nENCODER : Given an input sequence of lengthm, we start with a matrix whose rows are initialized\nas the d-dimensional embeddings of the symbols at each position of the sequenceH0 ∈Rm×d. The\nUT then iteratively computes representationsHt at step t for all m positions in parallel by applying\nthe multi-headed dot-product self-attention mechanism from Vaswani et al. (2017), followed by a\nrecurrent transition function. We also add residual connections around each of these function blocks\nand apply dropout and layer normalization (Srivastava et al., 2014; Ba et al., 2016) (see Fig. 2 for a\nsimpliﬁed diagram, and Fig. 4 in the Appendix A for the complete model.).\nMore speciﬁcally, we use the scaled dot-product attention which combines queriesQ, keys K and\nvalues V as follows\nATTENTION (Q,K,V )= SOFTMAX\n(QKT\n√\nd\n)\nV, (1)\nwhere d is the number of columns of Q, K and V . We use the multi-head version with k heads, as\nintroduced in (Vaswani et al., 2017),\nMULTI HEAD SELFATTENTION (Ht)= CONCAT (head1,...,headk)WO (2)\nwhere headi =ATTENTION (HtWQ\ni ,HtWK\ni ,HtWV\ni ) (3)\nand we map the stateHt to queries, keys and values with afﬁne projections using learned parameter\nmatrices WQ ∈Rd×d/k, WK ∈Rd×d/k, WV ∈Rd×d/k and WO ∈Rd×d.\nAt step t, the UT then computes revised representationsHt ∈Rm×d for all m input positions as follows\nHt =LAYER NORM (At+TRANSITION (At)) (4)\nwhere At =LAYER NORM ((Ht−1+Pt)+MULTI HEAD SELFATTENTION (Ht−1+Pt)), (5)\nwhere LAYER NORM () is deﬁned in Ba et al. (2016), and TRANSITION () and Pt are discussed below.\nDepending on the task, we use one of two different transition functions: either a separable\nconvolution (Chollet, 2016) or a fully-connected neural network that consists of a single rectiﬁed-linear\nactivation function between two afﬁne transformations, applied position-wise, i.e. individually to\neach row ofAt.\nPt ∈Rm×d above are ﬁxed, constant, two-dimensional (position, time) coordinate embeddings,\nobtained by computing the sinusoidal position embedding vectors as deﬁned in (Vaswani et al., 2017)\nfor the positions1≤i≤m and the time-step1≤t≤T separately for each vector-dimension1≤j ≤d,\nand summing:\nPt\ni,2j =sin(i/100002j/d)+sin(t/100002j/d) (6)\nPt\ni,2j+1 =cos(i/100002j/d)+cos(t/100002j/d). (7)\n3\nPublished as a conference paper at ICLR 2019\nInput Sequence\nEmbed Input Symbols\nMultihead Self-Attention\nTransition Function Multihead Attention\nTarget Sequence (right-shifted by one)\nEmbed Target Symbols\nMultihead Self-Attention\nTransition Function\nSoftmax\nOutput Probabilities\nAfter T steps\nAfter T steps\nFor T steps\nFor T steps\nRecurrent\nEncoder\nBlock\nRecurrent\nDecoder\nBlock\nFigure 2: The recurrent blocks of the Universal Transformer encoder and decoder. This diagram omits\nposition and time-step encodings as well as dropout, residual connections and layer normalization.\nA complete version can be found in Appendix A. The Universal Transformer with dynamic halting\ndetermines the number of stepsT for each position individually using ACT (Graves, 2016).\nAfter T steps (each updating all positions of the input sequence in parallel), the ﬁnal output of the\nUniversal Transformer encoder is a matrix ofd-dimensional vector representationsHT ∈Rm×d for\nthe m symbols of the input sequence.\nDECODER : The decoder shares the same basic recurrent structure of the encoder. However, after\nthe self-attention function, the decoder additionally also attends to the ﬁnal encoder representation\nHT of each position in the input sequence using the same multihead dot-product attention function\nfrom Equation 2, but with queriesQ obtained from projecting the decoder representations, and keys\nand values (K and V ) obtained from projecting the encoder representations (this process is akin to\nstandard attention (Bahdanau et al., 2014)).\nLike the Transformer model, the UT is autoregressive (Graves, 2013). Trained using teacher-forcing, at\ngeneration time it produces its output one symbol at a time, with the decoder consuming the previously\nproduced output positions. During training, the decoder input is the target output, shifted to the right\nby one position. The decoder self-attention distributions are further masked so that the model can\nonly attend to positions to the left of any predicted symbol. Finally, the per-symbol target distributions\nare obtained by applying an afﬁne transformation O ∈Rd×V from the ﬁnal decoder state to the\noutput vocabulary sizeV , followed by a softmax which yields an(m×V )-dimensional output matrix\nnormalized over its rows:\np\n(\nypos|y[1:pos−1],HT )\n=SOFTMAX (OHT )1 (8)\nTo generate from the model, the encoder is run once for the conditioning input sequence. Then the\ndecoder is run repeatedly, consuming all already-generated symbols, while generating one additional\ndistribution over the vocabulary for the symbol at the next output position per iteration. We then\ntypically sample or select the highest probability symbol as the next symbol.\n2.2 D YNAMIC HALTING\nIn sequence processing systems, certain symbols (e.g. some words or phonemes) are usually more\nambiguous than others. It is therefore reasonable to allocate more processing resources to these\nmore ambiguous symbols. Adaptive Computation Time (ACT) (Graves, 2016) is a mechanism for\ndynamically modulating the number of computational steps needed to process each input symbol\n1Note that T here denotes time-stepT and not the transpose operation.\n4\nPublished as a conference paper at ICLR 2019\nModel 10K examples 1K examples\ntrain single train joint train single train joint\nPrevious best results:\nQRNet (Seo et al., 2016) 0.3 (0/20) - - -\nSparse DNC (Rae et al., 2016) - 2.9 (1/20) - -\nGA+MAGE Dhingra et al. (2017) - - 8.7 (5/20) -\nMemN2N Sukhbaatar et al. (2015) - - - 12.4 (11/20)\nOur Results:\nTransformer (Vaswani et al., 2017) 15.2 (10/20) 22.1 (12/20) 21.8 (5/20) 26.8 (14/20)\nUniversal Transformer (this work) 0.23 (0/20) 0.47 (0/20) 5.31 (5/20) 8.50 (8/20)\nUT w/ dynamic halting (this work) 0.21 (0/20) 0.29 (0/20) 4.55 (3/20) 7.78 (5/20)\nTable 1: Average error and number of failed tasks (> 5% error) out of 20 (in parentheses; lower is\nbetter in both cases) on the bAbI dataset under the different training/evaluation setups. We indicate\nstate-of-the-art where available for each, or ‘-’ otherwise.\n(called the “ponder time”) in standard recurrent neural networks based on a scalarhalting probability\npredicted by the model at each step.\nInspired by the interpretation of Universal Transformers as applying self-attentive RNNs in parallel\nto all positions in the sequence, we also add a dynamic ACT halting mechanism to each position (i.e. to\neach per-symbol self-attentive RNN; see Appendix C for more details). Once the per-symbol recurrent\nblock halts, its state is simply copied to the next step until all blocks halt, or we reach a maximum number\nof steps. The ﬁnal output of the encoder is then the ﬁnal layer of representations produced in this way.\n3 E XPERIMENTS AND ANALYSIS\nWe evaluated the Universal Transformer on a range of algorithmic and language understanding tasks,\nas well as on machine translation. We describe these tasks and datasets in more detail in Appendix D.\n3.1 BABI QUESTION -ANSWERING\nThe bAbi question answering dataset (Weston et al., 2015) consists of 20 different tasks, where the\ngoal is to answer a question given a number of English sentences that encode potentially multiple\nsupporting facts. The goal is to measure various forms of language understanding by requiring a\ncertain type of reasoning over the linguistic facts presented in each story. A standard Transformer\ndoes not achieve good results on this task2. However, we have designed a model based on the Universal\nTransformer which achieves state-of-the-art results on this task.\nTo encode the input, similar to Henaff et al. (2016), we ﬁrst encode each fact in the story by applying a\nlearned multiplicative positional mask to each word’s embedding, and summing up all embeddings. We\nembed the question in the same way, and then feed the (Universal) Transformer with these embeddings\nof the facts and questions.\nAs originally proposed, models can either be trained on each task separately (“train single”) or jointly\non all tasks (“train joint”). Table 1 summarizes our results. We conducted 10 runs with different\ninitializations and picked the best model based on performance on the validation set, similar to previous\nwork. Both the UT and UT with dynamic halting achieve state-of-the-art results on all tasks in terms\nof average error and number of failed tasks3, in both the 10K and 1K training regime (see Appendix E\nfor breakdown by task).\nTo understand the working of the model better, we analyzed both the attention distributions and the\naverage ACT ponder times for this task (see Appendix F for details). First, we observe that the attention\ndistributions start out very uniform, but get progressively sharper in later steps around the correct\nsupporting facts that are required to answer each question, which is indeed very similar to how humans\nwould solve the task. Second, with dynamic halting we observe that the average ponder time (i.e. depth\n2We experimented with different hyper-parameters and different network sizes, but it always overﬁts.\n3Deﬁned as >5% error.\n5\nPublished as a conference paper at ICLR 2019\nFigure 3: Ponder time of UT with dynamic halting for encoding facts in a story and question in a bAbI\ntask requiring three supporting facts.\nof the per-symbol recurrent processing chain) over all positions in all samples in the test data for tasks\nrequiring three supporting facts is higher (3.8±2.2) than for tasks requiring only two (3.1±1.1), which\nis in turn higher than for tasks requiring only one supporting fact (2.3±0.8). This indicates that the\nmodel adjusts the number of processing steps with the number of supporting facts required to answer\nthe questions. Finally, we observe that the histogram of ponder times at different positions is more\nuniform in tasks requiring only one supporting fact compared to two and three, and likewise for tasks\nrequiring two compared to three. Especially for tasks requiring three supporting facts, many positions\nhalt at step 1 or 2 already and only a few get transformed for more steps (see for example Fig 3). This\nis particularly interesting as the length of stories is indeed much higher in this setting, with more\nirrelevant facts which the model seems to successfully learn to ignore in this way.\nSimilar to dynamic memory networks (Kumar et al., 2016), there is an iterative attention process in\nUTs that allows the model to condition its attention over memory on the result of previous iterations.\nAppendix F presents some examples illustrating that there is a notion of temporal states in UT, where\nthe model updates its states (memory) in each step based on the output of previous steps, and this chain\nof updates can also be viewed as steps in a multi-hop reasoning process.\n3.2 S UBJECT -VERB AGREEMENT\nNext, we consider the task of predicting number-agreement between subjects and verbs in English\nsentences (Linzen et al., 2016). This task acts as a proxy for measuring the ability of a model to\ncapture hierarchical (dependency) structure in natural language sentences. We use the dataset provided\nby (Linzen et al., 2016) and follow their experimental protocol of solving the task using a language mod-\neling training setup, i.e. a next word prediction objective, followed by calculating the ranking accuracy\nof the target verb at test time. We evaluated our model on subsets of the test data with different task dif-\nﬁculty, measured in terms ofagreement attractors– the number of intervening nouns with the opposite\nnumber from the subject (meant to confuse the model). For example, given the sentenceThe keys to the\ncabinet4, the objective during training is to predict the verbare (plural). At test time, we then evaluate\nthe ranking accuracy of the agreement attractors: i.e. the goal is to rankare higher than is in this case.\nOur results are summarized in Table 2. The best LSTM with attention from the literature achieves\n99.18% on this task (Yogatama et al., 2018), outperforming a vanilla Transformer (Tran et al., 2018).\nUTs signiﬁcantly outperform standard Transformers, and achieve anaverage result comparable to the\ncurrent state of the art (99.2%). However, we see that UTs (and particularly with dynamic halting) per-\nform progressively better than all other models as the number of attractors increases (see the last row,∆).\n3.3 LAMBADA L ANGUAGE MODELING\nThe LAMBADA task (Paperno et al., 2016) is a language modeling task consisting of predicting a\nmissing target word given a broader context of 4-5 preceding sentences. The dataset was speciﬁcally\ndesigned so that humans are able to accurately predict the target word when shown the full context,\nbut not when only shown the target sentence in which it appears. It therefore goes beyond language\n4Cabinet (singular) is an agreement attractor in this case.\n6\nPublished as a conference paper at ICLR 2019\nModel Number of attractors\n0 1 2 3 4 5 Total\nPrevious best results (Yogatama et al., 2018):\nBest Stack-RNN 0.994 0.979 0.965 0.935 0.916 0.880 0.992\nBest LSTM 0.993 0.972 0.950 0.922 0.900 0.842 0.991\nBest Attention 0.994 0.977 0.959 0.929 0.907 0.842 0.992\nOur results:\nTransformer 0.973 0.941 0.932 0.917 0.901 0.883 0.962\nUniversal Transformer 0.993 0.971 0.969 0.940 0.921 0.892 0.992\nUT w/ ACT 0.994 0.969 0.967 0.944 0.932 0.907 0.992\n∆ (UT w/ ACT - Best) 0 -0.008 0.002 0.009 0.016 0.027 -\nTable 2: Accuracy on the subject-verb agreement number prediction task (higher is better).\nModel LM Perplexity & (Accuracy) RC Accuracy\ncontrol dev test control dev test\nNeural Cache (Grave et al., 2016) 129 139 - - - -\nDhingra et al. Dhingra et al. (2018) - - - - - 0.5569\nTransformer 142 (0.19) 5122 (0.0) 7321 (0.0) 0.4102 0.4401 0.3988\nLSTM 138 (0.23) 4966 (0.0) 5174 (0.0) 0.1103 0.2316 0.2007\nUT base, 6 steps (ﬁxed) 131 (0.32) 279 (0.18) 319 (0.17) 0.4801 0.5422 0.5216\nUT w/ dynamic halting 130 (0.32) 134 (0.22) 142 (0.19) 0.4603 0.5831 0.5625\nUT base, 8 steps (ﬁxed) 129(0.32) 192 (0.21) 202 (0.18) - - -\nUT base, 9 steps (ﬁxed) 129(0.33) 214 (0.21) 239 (0.17) - - -\nTable 3: LAMBADA language modeling (LM) perplexity (lower better) with accuracy in parentheses\n(higher better), and Reading Comprehension (RC) accuracy results (higher better). ‘-’ indicates no\nreported results in that setting.\nmodeling, and tests the ability of a model to incorporate broader discourse and longer term context\nwhen predicting the target word.\nThe task is evaluated in two settings: as language modeling (the standard setup) and as reading\ncomprehension. In the former (more challenging) case, a model is simply trained for next-word\nprediction on the training data, and evaluated on the target words at test time (i.e. the model is trained\nto predict all words, not speciﬁcally challenging target words). In the latter setting, introduced by Chu\net al. Chu et al. (2017), the target sentence (minus the last word) is used as query for selecting the target\nword from the context sentences. Note that the target word appears in the context 81% of the time,\nmaking this setup much simpler. However the task is impossible in the remaining 19% of the cases.\nThe results are shown in Table 3. Universal Transformer achieves state-of-the-art results in both\nthe language modeling and reading comprehension setup, outperforming both LSTMs and vanilla\nTransformers. Note that the control set was constructed similar to the LAMBADA development and\ntest sets, but without ﬁltering them in any way, so achieving good results on this set shows a model’s\nstrength in standard language modeling.\nOur best ﬁxed UT results used 6 steps. However, the average number of steps that the best UT with\ndynamic halting took on the test data over all positions and examples was8.2±2.1. In order to see if\nthe dynamic model did better simply because it took more steps, we trained two ﬁxed UT models with\n8 and 9 steps respectively (see last two rows). Interestingly, these two models achieve better results\ncompared to the model with 6 steps, butdo not outperform the UT with dynamic halting. This leads\nus to believe that dynamic halting may act as a useful regularizer for the model via incentivizing a\nsmaller numbers of steps for some of the input symbols, while allowing more computation for others.\n3.4 A LGORITHMIC TASKS\nWe trained UTs on three algorithmic tasks, namely Copy, Reverse, and (integer) Addition, all on\nstrings composed of decimal symbols (‘0’-‘9’). In all the experiments, we train the models on\nsequences of length 40 and evaluated on sequences of length 400 (Kaiser & Sutskever, 2016). We\n7\nPublished as a conference paper at ICLR 2019\nModel Copy Reverse Addition\nchar-acc seq-acc char-acc seq-acc char-acc seq-acc\nLSTM 0.45 0.09 0.66 0.11 0.08 0.0\nTransformer 0.53 0.03 0.13 0.06 0.07 0.0\nUniversal Transformer 0.91 0.35 0.96 0.46 0.34 0.02\nNeural GPU∗ 1.0 1.0 1.0 1.0 1.0 1.0\nTable 4: Accuracy (higher better) on the algorithmic tasks.∗Note that the Neural GPU was trained with\na special curriculum to obtain the perfect result, while other models are trained without any curriculum.\nCopy Double Reverse\nModel char-acc seq-acc char-acc seq-acc char-acc seq-acc\nLSTM 0.78 0.11 0.51 0.047 0.91 0.32\nTransformer 0.98 0.63 0.94 0.55 0.81 0.26\nUniversal Transformer 1.0 1.0 1.0 1.0 1.0 1.0\nTable 5: Character-level (char-acc) and sequence-level accuracy (seq-acc) results on the Memorization\nLTE tasks, with maximum length of 55.\nProgram Control Addition\nModel char-acc seq-acc char-acc seq-acc char-acc seq-acc\nLSTM 0.53 0.12 0.68 0.21 0.83 0.11\nTransformer 0.71 0.29 0.93 0.66 1.0 1.0\nUniversal Transformer 0.89 0.63 1.0 1.0 1.0 1.0\nTable 6: Character-level (char-acc) and sequence-level accuracy (seq-acc) results on the Program\nEvaluation LTE tasks with maximum nesting of 2 and length of 5.\ntrain UTs using positions starting with randomized offsets to further encourage the model to learn\nposition-relative transformations. Results are shown in Table 4. The UT outperforms both LSTM and\nvanilla Transformer by a wide margin on all three tasks. The Neural GPU reports perfect results on this\ntask (Kaiser & Sutskever, 2016), however we note that this result required a special curriculum-based\ntraining protocol which was not used for other models.\n3.5 L EARNING TO EXECUTE (LTE)\nAs another class of sequence-to-sequence learning problems, we also evaluate UTs on tasks\nindicating the ability of a model to learn to execute computer programs, as proposed in (Zaremba &\nSutskever, 2015). These tasks include program evaluation tasks (program, control, and addition), and\nmemorization tasks (copy, double, and reverse).\nWe use the mix-strategy discussed in (Zaremba & Sutskever, 2015) to generate the datasets.\nUnlike (Zaremba & Sutskever, 2015), we do not use any curriculum learning strategy during training\nand we make no use of target sequences at test time. Tables 5 and 6 present the performance of an\nLSTM model, Transformer, and Universal Transformer on the program evaluation and memorization\ntasks, respectively. UT achieves perfect scores in all the memorization tasks and also outperforms\nboth LSTMs and Transformers in all program evaluation tasks by a wide margin.\n3.6 M ACHINE TRANSLATION\nWe trained a UT on the WMT 2014 English-German translation task using the same setup as reported in\n(Vaswani et al., 2017) in order to evaluate its performance on a large-scale sequence-to-sequence task.\nResults are summarized in Table 7. The UT with a fully-connected recurrent transition function (instead\nof separable convolution) and without ACT improves by 0.9 BLEU over a Transformer and 0.5 BLEU\nover a Weighted Transformer with approximately the same number of parameters (Ahmed et al., 2017).\n8\nPublished as a conference paper at ICLR 2019\nModel BLEU\nUniversal Transformersmall 26.8\nTransformer base (Vaswani et al., 2017) 28.0\nWeighted Transformerbase (Ahmed et al., 2017) 28.4\nUniversal Transformerbase 28.9\nTable 7: Machine translation results on the WMT14 En-De translation task trained on 8xP100 GPUs\nin comparable training setups. Allbase results have the same number of parameters.\n4 D ISCUSSION\nWhen running for a ﬁxed number of steps, the Universal Transformer is equivalent to a multi-layer\nTransformer with tied parameters across all its layers. This is partly similar to the Recursive\nTransformer, which ties the weights of its self-attention layers across depth (Gulcehre et al., 2018)5.\nHowever, as the per-symbol recurrent transition functions can be applied any number of times, another\nand possibly more informative way of characterizing the UT is as a block of parallel RNNs (one for\neach symbol, with shared parameters) evolving per-symbol hidden states concurrently, generated at\neach step by attending to the sequence of hidden states at the previous step. In this way, it is related\nto architectures such as the Neural GPU (Kaiser & Sutskever, 2016) and the Neural Turing Machine\n(Graves et al., 2014). UTs thereby retain the attractive computational efﬁciency of the original feed-\nforward Transformer model, but with the added recurrent inductive bias of RNNs. Furthermore, using\na dynamic halting mechanism, UTs can choose the number of processing steps based on the input data.\nThe connection between the Universal Transformer and other sequence models is apparent from\nthe architecture: if we limited the recurrent steps to one, it would be a Transformer. But it is more\ninteresting to consider the relationship between the Universal Transformer and RNNs and other\nnetworks where recurrence happens over the time dimension. Superﬁcially these models may seem\nclosely related since they are recurrent as well. But there is a crucial difference: time-recurrent models\nlike RNNs cannot access memory in the recurrent steps. This makes them computationally more similar\nto automata, since the only memory available in the recurrent part is a ﬁxed-size state vector. UTs on\nthe other hand can attend to the whole previous layer, allowing it to access memory in the recurrent step.\nGiven sufﬁcient memory the Universal Transformer is computationally universal – i.e. it belongs to\nthe class of models that can be used to simulate any Turing machine, thereby addressing a shortcoming\nof the standard Transformer model6. In addition to being theoretically appealing, our results show\nthat this added expressivity also leads to improved accuracy on several challenging sequence modeling\ntasks. This closes the gap between practical sequence models competitive on large-scale tasks such\nas machine translation, and computationally universal models such as the Neural Turing Machine\nor the Neural GPU (Graves et al., 2014; Kaiser & Sutskever, 2016), which can be trained using gradient\ndescent to perform algorithmic tasks.\nTo show this, we can reduce a Neural GPU to a Universal Transformer. Ignoring the decoder and pa-\nrameterizing the self-attention module, i.e. self-attention with the residual connection, to be the identity\nfunction, we assume the transition function to be a convolution. If we now set the total number of\nrecurrent steps T to be equal to the input length, we obtain exactly a Neural GPU. Note that the last step\nis where the Universal Transformer crucially differs from the vanilla Transformer whose depth cannot\nscale dynamically with the size of the input. A similar relationship exists between the Universal Trans-\nformer and the Neural Turing Machine, whose single read/write operations per step can be expressed by\nthe global, parallel representation revisions of the Universal Transformer. In contrast to these models,\nhowever, which only perform well on algorithmic tasks, the Universal Transformer also achieves\ncompetitive results on realistic natural language tasks such as LAMBADA and machine translation.\nAnother related model architecture is that of end-to-end Memory Networks (Sukhbaatar et al.,\n2015). In contrast to end-to-end memory networks, however, the Universal Transformer uses\nmemory corresponding to states aligned to individual positions of its inputs or outputs. Furthermore,\nthe Universal Transformer follows the encoder-decoder conﬁguration and achieves competitive\nperformance in large-scale sequence-to-sequence tasks.\n5Note that in UT both the self-attention and transition weights are tied across layers.\n6Appendix B illustrates how UT is computationally more powerful than the standard Transformer.\n9\nPublished as a conference paper at ICLR 2019\n5 C ONCLUSION\nThis paper introduces the Universal Transformer, a generalization of the Transformer model that\nextends its theoretical capabilities and produces state-of-the-art results on a wide range of challenging\nsequence modeling tasks, such as language understanding but also a variety of algorithmic tasks,\nthereby addressing a key shortcoming of the standard Transformer. The Universal Transformer\ncombines the following key properties into one model:\nWeight sharing: Following intuitions behind weight sharing found in CNNs and RNNs, we extend the\nTransformer with a simple form of weight sharing that strikes an effective balance between inductive\nbias and model expressivity, which we show extensively on both small and large-scale experiments.\nConditional computation: In our goal to build a computationally universal machine, we equipped the\nUniversal Transformer with the ability to halt or continue computation through a recently introduced\nmechanism, which shows stronger results compared to the ﬁxed-depth Universal Transformer.\nWe are enthusiastic about the recent developments on parallel-in-time sequence models. By adding\ncomputational capacity and recurrence in processing depth, we hope that further improvements beyond\nthe basic Universal Transformer presented here will help us build learning algorithms that are both\nmore powerful, data efﬁcient, and generalize beyond the current state-of-the-art.\nThe code used to train and evaluate Universal Transformers is available at https:\n//github.com/tensorflow/tensor2tensor (Vaswani et al., 2018).\nAcknowledgements We are grateful to Ashish Vaswani, Douglas Eck, and David Dohan for their\nfruitful comments and inspiration.\nREFERENCES\nKarim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer network for machine translation.\narXiv preprint arXiv:1711.02132, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.arXiv preprint arXiv:1607.06450,\n2016. URL http://arxiv.org/abs/1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align\nand translate. CoRR, abs/1409.0473, 2014. URL http://arxiv.org/abs/1409.0473.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\nLearning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR,\nabs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.\nFrancois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint\narXiv:1610.02357, 2016.\nZewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading\ncomprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers, volume 2, pp. 52–57, 2017.\nBhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory\nfor recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017.\nBhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for\nreasoning over multiple mentions using coreference.arXiv preprint arXiv:1804.05922, 2018.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence\nto sequence learning. CoRR, abs/1705.03122, 2017. URL http://arxiv.org/abs/1705.03122.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache.\narXiv preprint arXiv:1612.04426, 2016.\nAlex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL\nhttp://arxiv.org/abs/1308.0850.\nAlex Graves. Adaptive computation time for recurrent neural networks.arXiv preprint arXiv:1603.08983, 2016.\n10\nPublished as a conference paper at ICLR 2019\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL\nhttp://arxiv.org/abs/1410.5401.\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter\nBattaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks.arXiv preprint\narXiv:1805.09786, 2018.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with\nrecurrent entity networks. arXiv preprint arXiv:1612.03969, 2016.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in recurrent nets: the\ndifﬁculty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003.\nA. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. InAdvances in\nNeural Information Processing Systems, (NIPS), 2015.\nŁukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning\nRepresentations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228.\nŁukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine\ntranslation. CoRR, abs/1706.03059, 2017. URL http://arxiv.org/abs/1706.03059.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain\nPaulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing.\nIn International Conference on Machine Learning, pp. 1378–1387, 2016.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.\nA structured self-attentive sentence embedding.arXiv preprint arXiv:1703.03130, 2017.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-sensitive\ndependencies. Transactions of the Association of Computational Linguistics, 4(1):521–535, 2016.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring a\nbroad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1, pp. 1525–1534, 2016.\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable atten-\ntion model. In Empirical Methods in Natural Language Processing , 2016. URL https:\n//arxiv.org/pdf/1606.01933.pdf.\nJack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves,\nand Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. InAdvances\nin Neural Information Processing Systems, pp. 3621–3629, 2016.\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. Query-reduction networks for question\nanswering. arXiv preprint arXiv:1606.04582, 2016.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:\na simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):\n1929–1958, 2014.\nSainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.\nIn C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in\nNeural Information Processing Systems 28 , pp. 2440–2448. Curran Associates, Inc., 2015. URL\nhttp://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural net-\nworks. In Advances in Neural Information Processing Systems , pp. 3104–3112, 2014. URL\nhttp://arxiv.org/abs/1409.3215.\nKe Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling hierarchical\nstructure. In Proceedings of NAACL’18, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. CoRR, 2017. URL http://arxiv.org/abs/1706.03762.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit.\nTensor2tensor for neural machine translation.CoRR, abs/1803.07416, 2018.\n11\nPublished as a conference paper at ICLR 2019\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and\nTomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint\narXiv:1502.05698, 2015.\nDani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom.\nMemory architectures in recurrent neural network language models. InInternational Conference on Learning\nRepresentations, 2018. URL https://openreview.net/forum?id=SkFqf0lAZ.\nWojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2015. URL\nhttp://arxiv.org/abs/1410.4615.\n12\nPublished as a conference paper at ICLR 2019\nAPPENDIX A D ETAILED SCHEMA OF THE UNIVERSAL TRANSFORMER\nInput Sequence\nEmbed Input Symbols\nPosition embedding\nTimestep embedding\nMultihead Self-Attention\nTransition Function\nDropout\nLayer Normalization\nDropout\nLayer Normalization\n+\n+\n+\n+\nMultihead Attention\nTarget Sequence (right-shifted by one)\nEmbed Target Symbols\nPosition embedding\nTimestep embedding\nMultihead Self-Attention\nTransition Function\nDropout\nLayer Normalization\nDropout\nLayer Normalization\n+\n+\n+\n+\nDropout\nLayer Normalization\n+\nSoftmax\nOutput Probabilities\nAfter T steps\nAfter T steps\nFor T steps\nFor T steps\nRecurrent\nEncoder\nBlock\nRecurrent\nDecoder\nBlock\nFigure 4: The Universal Transformer with position and step embeddings as well as dropout and layer\nnormalization.\nAPPENDIX B O N THE COMPUTATIONAL POWER OF UT VS TRANSFORMER\nWith respect to their computational power, the key difference between the Transformer and the Universal\nTransformer lies in the number of sequential steps of computation (i.e. in depth). While a standard Transformer\nexecutes a total number of operations that scales with the input size, the number of sequential operations is\nconstant, independent of the input size and determined solely by the number of layers. Assuming ﬁnite precision,\nthis property implies that the standard Transformer cannot be computationally universal. When choosing a number\nof steps as a function of the input length, however, the Universal Transformer does not suffer from this limitation.\nNote that this holds independently of whether or not adaptive computation time is employed but does assume\na non-constant, even if possibly deterministic, number of steps. Varying the number of steps dynamically after\ntraining is enabled by sharing weights across sequential computation steps in the Universal Transformer.\nAn intuitive example are functions whose execution requires the\nsequential processing of each input element. In this case, for any\ngiven choice of depth T, one can construct an input sequence of\nlength N > Tthat cannot be processed correctly by a standard\nTransformer. With an appropriate, input-length dependent choice of\nsequential steps, however, a Universal Transformer, RNNs or Neural\nGPUs can execute such a function.\n13\nPublished as a conference paper at ICLR 2019\nAPPENDIX C UT WITH DYNAMIC HALTING\nWe implement the dynamic halting based on ACT (Graves, 2016) as follows in TensorFlow. In each step of the UT\nwith dynamic halting, we are given the halting probabilities, remainders, number of updates up to that point, and\nthe previous state (all initialized as zeros), as well as a scalar threshold between 0 and 1 (a hyper-parameter). We\nthen compute the new state for each position and calculate the new per-position halting probabilities based on the\nstate for each position. The UT then decides to halt for some positions that crossed the threshold, and updates the\nstate of other positions until the model halts for all positions or reaches a predeﬁned maximum number of steps:\n1 # While −l o o p s t o p s when t h i s p r e d i c a t e i s FALSE\n2 # i . e . a l l ( ( p r o b a b i l i t y < t h r e s h o l d ) & ( c o u n t e r < max_steps ) ) a r e f a l s e\n3 d e f s h o u l d _ c o n t i n u e ( u0 , u1 , h a l t i n g _ p r o b a b i l i t y , u2 , n _ u p d a t e s , u3 ) :\n4 r e t u r n t f . r e d u c e _ a n y (\n5 t f . l o g i c a l _ a n d (\n6 t f . l e s s ( h a l t i n g _ p r o b a b i l i t y , t h r e s h o l d ) ,\n7 t f . l e s s ( n _ u p d a t e s , max_steps ) ) )\n8 # Do w h i l e l o o p i t e r a t i o n s u n t i l p r e d i c a t e above i s f a l s e\n9 ( _ , _ , _ , r e m a i n d e r , n _ u p d a t e s , n e w _ s t a t e ) = t f . w h i l e _ l o o p (\n10 s h o u l d _ c o n t i n u e , u t _ w i t h _ d y n a m i c _ h a l t i n g , ( s t a t e ,\n11 s t e p , h a l t i n g _ p r o b a b i l i t y , r e m a i n d e r s , n _ u p d a t e s , p r e v i o u s _ s t a t e ) )\nListing 1: UT with dynamic halting.\nThe following shows the computations in each step:\n1 d e f u t _ w i t h _ d y n a m i c _ h a l t i n g ( s t a t e , s t e p , h a l t i n g _ p r o b a b i l i t y ,\n2 r e m a i n d e r s , n _ u p d a t e s , p r e v i o u s _ s t a t e ) :\n3 # C a l c u l a t e t h e p r o b a b i l i t i e s b a s e d on t h e s t a t e\n4 p = common_layers . d e n s e ( s t a t e , 1 , a c t i v a t i o n = t f . nn . sigmoid ,\n5 u s e _ b i a s = True )\n6 # Mask f o r i n p u t s which have n o t h a l t e d y e t\n7 s t i l l _ r u n n i n g = t f . c a s t (\n8 t f . l e s s ( h a l t i n g _ p r o b a b i l i t y , 1 . 0 ) , t f . f l o a t 3 2 )\n9 # Mask o f i n p u t s which h a l t e d a t t h i s s t e p\n10 n e w _ h a l t e d = t f . c a s t (\n11 t f . g r e a t e r ( h a l t i n g _ p r o b a b i l i t y + p ∗ s t i l l _ r u n n i n g , t h r e s h o l d ) ,\n12 t f . f l o a t 3 2 ) ∗ s t i l l _ r u n n i n g\n13 # Mask o f i n p u t s which haven ’ t h a l t e d , and d i d n ’ t h a l t t h i s s t e p\n14 s t i l l _ r u n n i n g = t f . c a s t (\n15 t f . l e s s _ e q u a l ( h a l t i n g _ p r o b a b i l i t y + p∗ s t i l l _ r u n n i n g ,\n16 t h r e s h o l d ) , t f . f l o a t 3 2 ) ∗ s t i l l _ r u n n i n g\n17 # Add t h e h a l t i n g p r o b a b i l i t y f o r t h i s s t e p t o t h e h a l t i n g\n18 # p r o b a b i l i t i e s f o r t h o s e i n p u t s which haven ’ t h a l t e d y e t\n19 h a l t i n g _ p r o b a b i l i t y += p ∗ s t i l l _ r u n n i n g\n20 # Compute r e m a i n d e r s f o r t h e i n p u t s which h a l t e d a t t h i s s t e p\n21 r e m a i n d e r s += n e w _ h a l t e d∗ ( 1 − h a l t i n g _ p r o b a b i l i t y )\n22 # Add t h e r e m a i n d e r s t o t h o s e i n p u t s which h a l t e d a t t h i s s t e p\n23 h a l t i n g _ p r o b a b i l i t y += n e w _ h a l t e d∗ r e m a i n d e r s\n24 # I n c r e m e n t n _ u p d a t e s f o r a l l i n p u t s which a r e s t i l l r u n n i n g\n25 n _ u p d a t e s += s t i l l _ r u n n i n g + n e w _ h a l t e d\n26 # Compute t h e w e i g h t t o be a p p l i e d t o t h e new s t a t e and o u t p u t :\n27 # 0 when t h e i n p u t has a l r e a d y h a l t e d ,\n28 # p when t h e i n p u t hasn ’ t h a l t e d yet ,\n29 # t h e r e m a i n d e r s when i t h a l t e d t h i s s t e p .\n30 u p d a t e _ w e i g h t s = t f . expand_dims ( p ∗ s t i l l _ r u n n i n g +\n31 n e w _ h a l t e d∗ r e m a i n d e r s ,−1)\n32 # Apply t r a n s f o r m a t i o n t o t h e s t a t e\n33 t r a n s f o r m e d _ s t a t e = t r a n s i t i o n _ f u n c t i o n ( s e l f _ a t t e n t i o n ( s t a t e ) )\n34 # I n t e r p o l a t e t r a n s f o r m e d and p r e v i o u s s t a t e s f o r non −h a l t e d i n p u t s\n35 n e w _ s t a t e = ( ( t r a n s f o r m e d _ s t a t e∗ u p d a t e _ w e i g h t s ) +\n36 ( p r e v i o u s _ s t a t e∗ ( 1 − u p d a t e _ w e i g h t s ) ) )\n37 s t e p += 1\n38 r e t u r n ( t r a n s f o r m e d _ s t a t e , s t e p , h a l t i n g _ p r o b a b i l i t y ,\n39 r e m a i n d e r s , n _ u p d a t e s , n e w _ s t a t e )\nListing 2: Computations in each step of the UT with dynamic halting.\n14\nPublished as a conference paper at ICLR 2019\nAPPENDIX D D ESCRIPTION OF SOME OF THE TASKS /DATASETS\nHere, we provide some additional details on the bAbI, subject-verb agreement, LAMBADA language modeling,\nand learning to execute (LTE) tasks.\nD.1 BABI QUESTION -ANSWERING\nThe bAbi question answering dataset (Weston et al., 2015) consists of 20 different synthetic tasks7. The aim\nis that each task tests a unique aspect of language understanding and reasoning, including the ability of: reasoning\nfrom supporting facts in a story, answering true/false type questions, counting, understanding negation and\nindeﬁnite knowledge, understanding coreferences, time reasoning, positional and size reasoning, path-ﬁnding, and\nunderstanding motivations (to see examples for each of these tasks, please refer to Table 1 in (Weston et al., 2015)).\nThere are two versions of the dataset, one with 1k training examples and the other with 10k examples. It is\nimportant for a model to be data-efﬁcient to achieve good results using only the 1k training examples. Moreover,\nthe original idea is that a single model should be evaluated across all the tasks (not tuning per task), which is\nthe train joint setup in Table 1, and the tables presented in Appendix E.\nD.2 S UBJECT -VERB AGREEMENT\nSubject-verb agreement is the task of predicting number agreement between subject and verb in English sentences.\nSucceeding in this task is a strong indicator that a model can learn to approximate syntactic structure and therefore\nit was proposed by Linzen et al. (2016) as proxy for assessing the ability of different models to capture hierarchical\nstructure in natural language.\nTwo experimental setups were proposed by Linzen et al. (2016) for training a model on this task: 1) training\nwith a language modeling objective, i.e., next word prediction, and 2) as binary classiﬁcation, i.e. predicting\nthe number of the verb given the sentence. In this paper, we use the language modeling objective, meaning that\nwe provide the model with an implicit supervision and evaluate based on the ranking accuracy of the correct\nform of the verb compared to the incorrect form of the verb.\nIn this task, in order to have different levels of difﬁculty, “agreement attractors” are used, i.e. one or more\nintervening nouns with the opposite number from the subject with the goal of confusing the model. In this case,\nthe model needs to correctly identify the head of the syntactic subject that corresponds to a given verb and ignore\nthe intervening attractors in order to predict the correct form of that verb. Here are some examples for this task\nin which subjects and the corresponding verbs are in boldface and agreement attractors are underlined:\nNo attractor: The boy smiles.\nOne attractor: The number of men is not clear.\nTwo attractors: The ratio of men to women is not clear.\nThree attractors: The ratio of men to women and children is not clear.\nD.3 LAMBADA L ANGUAGE MODELING\nThe LAMBADA task (Paperno et al., 2016) is a broad context language modeling task. In this task, given a\nnarrative passage, the goal is to predict the last word (target word) of the last sentence (target sentence) in the\npassage. These passages are speciﬁcally selected in a way that human subjects are easily able to guess their last\nword if they are exposed to a long passage, but not if they only see the target sentence preceding the target word8.\nHere is a sample from the dataset:\nContext:\n“Yes, I thought I was going to lose the baby.”\n“I was scared too,” he stated, sincerity flooding his eyes.\n“You were?” “Yes, of course. Why do you even ask?”\n“This baby wasn’t exactly planned for.”\nTarget sentence:\n“Do you honestly think that I would want you to have a ________?”\nTarget word:\nmiscarriage\nThe LAMBADA task consists in predicting the target word given the whole passage (i.e., the context plus the\ntarget sentence). A “control set” is also provided which was constructed by randomly sampling passages of the\nsame shape and size as the ones used to build LAMBADA, but without ﬁltering them in any way. The control\n7https://research.fb.com/downloads/babi\n8http://clic.cimec.unitn.it/lambada/appendix_onefile.pdf\n15\nPublished as a conference paper at ICLR 2019\nset is used to evaluate the models at standard language modeling before testing on the LAMBADA task, and\ntherefore to ensure that low performance on the latter cannot be attributed simply to poor language modeling.\nThe task is evaluated in two settings: aslanguage modeling (the standard setup) and asreading comprehension.\nIn the former (more challenging) case, a model is simply trained for the next word prediction on the training\ndata, and evaluated on the target words at test time (i.e. the model is trained to predict all words, not speciﬁcally\nchallenging target words). In this paper, we report the results of the Universal Transformer in both setups.\nD.4 L EARNING TO EXECUTE (LTE)\nLTE is a set of tasks indicating the ability of a model to learn to execute computer programs and was proposed\nby Zaremba & Sutskever (2015). These tasks include two subsets: 1) program evaluation tasks (program,\ncontrol, and addition) that are designed to assess the ability of models for understanding numerical operations,\nif-statements, variable assignments, the compositionality of operations, and more, as well as 2) memorization\ntasks (copy, double, and reverse).\nThe difﬁculty of the program evaluation tasks is parameterized by theirlength and nesting. The length parameter\nis the number of digits in the integers that appear in the programs (so the integers are chosen uniformly from\n[1, length]), and the nesting parameter is the number of times we are allowed to combine the operations with\neach other. Higher values of nesting yield programs with deeper parse trees. For instance, here is a program that\nis generated with length = 4 and nesting = 3.\nInput:\nj=8584\nfor x in range(8):\nj+=920\nb=(1500+j)\nprint((b+7567))\nTarget:\n25011\n16\nPublished as a conference paper at ICLR 2019\nAPPENDIX E BABI DETAILED RESULTS\nBest seed run for each task (out of 10 runs)\nTask id 10K 1K\ntrain single train joint train single train joint\n1 0.0 0.0 0.0 0.0\n2 0.0 0.0 0.0 0.5\n3 0.4 1.2 3.7 5.4\n4 0.0 0.0 0.0 0.0\n5 0.0 0.0 0.0 0.5\n6 0.0 0.0 0.0 0.5\n7 0.0 0.0 0.0 3.2\n8 0.0 0.0 0.0 1.6\n9 0.0 0.0 0.0 0.2\n10 0.0 0.0 0.0 0.4\n11 0.0 0.0 0.0 0.1\n12 0.0 0.0 0.0 0.0\n13 0.0 0.0 0.0 0.6\n14 0.0 0.0 0.0 3.8\n15 0.0 0.0 0.0 5.9\n16 0.4 1.2 5.8 15.4\n17 0.6 0.2 32.0 42.9\n18 0.0 0.0 0.0 4.1\n19 2.8 3.1 47.1 68.2\n20 0.0 0.0 2.4 2.4\navg err 0.21 0.29 4.55 7.78\nfailed 0 0 3 5\nAverage (±var) over all seeds (for 10 runs)\nTask id 10K 1K\ntrain single train joint train single train joint\n1 0.0 ±0.0 0.0 ±0.0 0.2 ±0.3 0.1 ±0.2\n2 0.2 ±0.4 1.7 ±2.6 3.2 ±4.1 4.3 ±11.6\n3 1.8 ±1.8 4.6 ±7.3 9.1 ±12.7 14.3 ±18.1\n4 0.1 ±0.1 0.2 ±0.1 0.3 ±0.3 0.4 ±0.6\n5 0.2 ±0.3 0.8 ±0.5 1.1 ±1.3 4.3 ±5.6\n6 0.1 ±0.2 0.1 ±0.2 1.2 ±2.1 0.8 ±0.4\n7 0.3 ±0.5 1.1 ±1.5 0.0 ±0.0 4.1 ±2.9\n8 0.3 ±0.2 0.5 ±1.1 0.1 ±0.2 3.9 ±4.2\n9 0.0 ±0.0 0.0 ±0.0 0.1 ±0.1 0.3 ±0.3\n10 0.1 ±0.2 0.5 ±0.4 0.7 ±0.8 1.3 ±1.6\n11 0.0 ±0.0 0.1 ±0.1 0.4 ±0.8 0.3 ±0.9\n12 0.2 ±0.1 0.4 ±0.4 0.6 ±0.9 0.3 ±0.4\n13 0.2 ±0.5 0.3 ±0.4 0.8 ±0.9 1.1 ±0.9\n14 1.8 ±2.6 1.3 ±1.6 0.1 ±0.2 4.7 ±5.2\n15 2.1 ±3.4 1.6 ±2.8 0.3 ±0.5 10.3 ±8.6\n16 1.9 ±2.2 0.9 ±1.3 9.1 ±8.1 34.1 ±22.8\n17 1.6 ±0.8 1.4 ±3.4 43.7 ±18.6 51.1 ±12.9\n18 0.3 ±0.4 0.7 ±1.4 2.3 ±3.6 12.8 ±9.0\n19 3.4 ±4.0 6.1 ±7.3 50.2 ±8.4 73.1 ±23.9\n20 0.0 ±0.0 0.0 ±0.0 3.2 ±2.5 2.6 ±2.8\navg 0.73 ±0.89 1.12 ±1.62 6.34 ±3.32 11.21 ±6.65\n17\nPublished as a conference paper at ICLR 2019\nAPPENDIX F BABI ATTENTION VISUALIZATION\nWe present a visualization of the attention distributions on bAbI tasks for a couple of examples. The visualization\nof attention weights is over different time steps based on different heads over all the facts in the story and a\nquestion. Different color bars on the left side indicate attention weights based on different heads (4 heads in total).\nAn example from tasks 1: (requiring one supportive fact to solve)\nStory:\nJohn travelled to the hallway.\nMary journeyed to the bathroom.\nDaniel went back to the bathroom.\nJohn moved to the bedroom\nQuestion:\nWhere is Mary?\nModel’s output:\nbathroom\n(a) Step 1\n(b) Step 2\n(c) Step 3\n(d) Step 4\nFigure 5: Visualization of the attention distributions, when encoding the question:“Where is Mary?”.\n18\nPublished as a conference paper at ICLR 2019\nAn example from tasks 2: (requiring two supportive facts to solve)\nStory:\nSandra journeyed to the hallway.\nMary went to the bathroom.\nMary took the apple there.\nMary dropped the apple.\nQuestion:\nWhere is the apple?\nModel’s output:\nbathroom\n(a) Step 1\n(b) Step 2\n(c) Step 3\n(d) Step 4\nFigure 6: Visualization of the attention distributions, when encoding the question: “Where is the\napple?”.\n19\nPublished as a conference paper at ICLR 2019\nAn example from tasks 2: (requiring two supportive facts to solve)\nStory:\nJohn went to the hallway.\nJohn went back to the bathroom.\nJohn grabbed the milk there.\nSandra went back to the office.\nSandra journeyed to the kitchen.\nSandra got the apple there.\nSandra dropped the apple there.\nJohn dropped the milk.\nQuestion:\nWhere is the milk?\nModel’s output:\nbathroom\n(a) Step 1\n(b) Step 2\n(c) Step 3\n(d) Step 4\nFigure 7: Visualization of the attention distributions, when encoding the question:“Where is the milk?”.\n20\nPublished as a conference paper at ICLR 2019\nAn example from tasks 3: (requiring three supportive facts to solve)\nStory:\nMary got the milk.\nJohn moved to the bedroom.\nDaniel journeyed to the office.\nJohn grabbed the apple there.\nJohn got the football.\nJohn journeyed to the garden.\nMary left the milk.\nJohn left the football.\nDaniel moved to the garden.\nDaniel grabbed the football.\nMary moved to the hallway.\nMary went to the kitchen.\nJohn put down the apple there.\nJohn picked up the apple.\nSandra moved to the hallway.\nDaniel left the football there.\nDaniel took the football.\nJohn travelled to the kitchen.\nDaniel dropped the football.\nJohn dropped the apple.\nJohn grabbed the apple.\nJohn went to the office.\nSandra went back to the bedroom.\nSandra took the milk.\nJohn journeyed to the bathroom.\nJohn travelled to the office.\nSandra left the milk.\nMary went to the bedroom.\nMary moved to the office.\nJohn travelled to the hallway.\nSandra moved to the garden.\nMary moved to the kitchen.\nDaniel took the football.\nMary journeyed to the bedroom.\nMary grabbed the milk there.\nMary discarded the milk.\nJohn went to the garden.\nJohn discarded the apple there.\nQuestion:\nWhere was the apple before the bathroom?\nModel’s output:\noffice\n21\nPublished as a conference paper at ICLR 2019\n(e) Step 1\n(f) Step 2\n22\nPublished as a conference paper at ICLR 2019\n(g) Step 3\n(h) Step 4\nFigure 7: Visualization of the attention distributions, when encoding the question:“Where was the\napple before the bathroom?”.\n23",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7550162076950073
    },
    {
      "name": "Transformer",
      "score": 0.6691729426383972
    },
    {
      "name": "Recurrent neural network",
      "score": 0.598163902759552
    },
    {
      "name": "Machine translation",
      "score": 0.537956714630127
    },
    {
      "name": "Inductive bias",
      "score": 0.46671196818351746
    },
    {
      "name": "Inference",
      "score": 0.44765153527259827
    },
    {
      "name": "Computation",
      "score": 0.4158893823623657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41451412439346313
    },
    {
      "name": "Algorithm",
      "score": 0.4123302698135376
    },
    {
      "name": "Artificial neural network",
      "score": 0.2700434923171997
    },
    {
      "name": "Multi-task learning",
      "score": 0.17010116577148438
    },
    {
      "name": "Voltage",
      "score": 0.1521686315536499
    },
    {
      "name": "Task (project management)",
      "score": 0.1489792764186859
    },
    {
      "name": "Engineering",
      "score": 0.08735132217407227
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}