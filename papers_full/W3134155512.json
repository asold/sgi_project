{
  "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models",
  "url": "https://openalex.org/W3134155512",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2728774655",
      "name": "Inoue, Go",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4229305985",
      "name": "Alhafni, Bashar",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Baimukan, Nurpeiis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4229305987",
      "name": "Bouamor, Houda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3155073428",
      "name": "Habash, Nizar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2964573145",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W2250278916",
    "https://openalex.org/W3210486066",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2250783208",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963555443",
    "https://openalex.org/W2251635021",
    "https://openalex.org/W3099299360",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2250816155",
    "https://openalex.org/W2900704097",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2945102115",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2812315852",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W2394969460",
    "https://openalex.org/W3032746405",
    "https://openalex.org/W2805974756",
    "https://openalex.org/W3012069291",
    "https://openalex.org/W3025939269",
    "https://openalex.org/W3119989665",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W2806962830",
    "https://openalex.org/W3119566336",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W1977195462",
    "https://openalex.org/W3120253119",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W1728883788",
    "https://openalex.org/W3035261420",
    "https://openalex.org/W2560280095",
    "https://openalex.org/W2109704865",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2970513828"
  ],
  "abstract": "In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.",
  "full_text": "The Interplay of Variant, Size, and Task Type\nin Arabic Pre-trained Language Models\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda Bouamor,† Nizar Habash\nComputational Approaches to Modeling Language (CAMeL) Lab\nNew York University Abu Dhabi\n†Carnegie Mellon University in Qatar\n{go.inoue,alhafni,nurpeiis,nizar.habash}@nyu.edu\nhbouamor@qatar.cmu.edu\nAbstract\nIn this paper, we explore the effects of lan-\nguage variants, data sizes, and ﬁne-tuning task\ntypes in Arabic pre-trained language models.\nTo do so, we build three pre-trained language\nmodels across three variants of Arabic: Mod-\nern Standard Arabic (MSA), dialectal Arabic,\nand classical Arabic, in addition to a fourth\nlanguage model which is pre-trained on a mix\nof the three. We also examine the importance\nof pre-training data size by building additional\nmodels that are pre-trained on a scaled-down\nset of the MSA variant. We compare our dif-\nferent models to each other, as well as to eight\npublicly available models by ﬁne-tuning them\non ﬁve NLP tasks spanning 12 datasets. Our\nresults suggest that the variant proximity of\npre-training data to ﬁne-tuning data is more\nimportant than the pre-training data size. We\nexploit this insight in deﬁning an optimized\nsystem selection model for the studied tasks.\n1 Introduction\nPre-trained language models such as BERT (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019b)\nhave shown signiﬁcant success in a wide range of\nnatural language processing (NLP) tasks in various\nlanguages. Arabic has beneﬁted from extensive\nefforts in building dedicated pre-trained language\nmodels, achieving state-of-the-art results in a num-\nber of NLP tasks, across both Modern Standard\nArabic (MSA) and Dialectal Arabic (DA) (Antoun\net al., 2020; Abdul-Mageed et al., 2020a).\nHowever, it is hard to compare these models to\nunderstand what contributes to their performances\nbecause of their different design decisions and hy-\nperparameters, such as data size, language variant,\ntokenization, vocabulary size, number of training\nsteps, and so forth. Practically, one may empiri-\ncally choose the best performing pre-trained model\nby ﬁne-tuning it on a particular task; however, it is\nstill unclear why a particular model is performing\nbetter than another and what design choices are\ncontributing to its performance.\nTo answer this question, we pre-trained various\nlanguage models as part of a controlled experiment\nwhere we vary pre-training data sizes and language\nvariants while keeping other hyperparameters con-\nstant throughout pre-training. We started by scaling\ndown MSA pre-training data size to measure its im-\npact on performance in ﬁne-tuning tasks. We then\npre-trained three different variants of Arabic: MSA,\nDA, and classical Arabic (CA), as well as a mix of\nthese three variants.\nWe evaluate our models along with eight other re-\ncent Arabic pre-trained models across ﬁve different\ntasks covering all the language variants we study,\nnamely, named entity recognition (NER), part-of-\nspeech (POS) tagging, sentiment analysis, dialect\nidentiﬁcation, and poetry classiﬁcation, spanning\n12 datasets.\nOur contributions can be summarized as follows:\n• We create and release eight Arabic pre-trained\nmodels, which we name CAMeLBERT, with\ndifferent design decisions, including one\n(CAMeLBERT-Mix) that is trained on the\nlargest dataset to date.1\n• We investigate the interplay of data size,\nlanguage variant, and ﬁne-tuning task type\nthrough controlled experimentation. Our\nresults show that variant proximity of pre-\ntraining data and task data is more important\nthan pre-training data size.\n• We exploit this insight in deﬁning an opti-\nmized system selection model.\n1Our pre-trained models are available at https://\nhuggingface.co/CAMeL-Lab, and the ﬁne-tuning code\nand models are available at https://github.com/\nCAMeL-Lab/CAMeLBERT.\narXiv:2103.06678v2  [cs.CL]  4 Sep 2021\nRef Model Variants Size #Word Tokens Vocab #Steps\nBERT (Devlin et al., 2019) - - 3.3B WP 30k 1M\nX1 mBERT (Devlin et al., 2019) MSA - - WP 120k -\nX2 AraBERTv0.1 (Antoun et al., 2020) MSA 24GB - SP 60k 1.25M\nX3 AraBERTv0.2 (Antoun et al., 2020) MSA 77GB 8.6B WP 60k 3M\nX4 ArabicBERT (Safaya et al., 2020) MSA 95GB 8.2B WP 32k 4M\nX5 Multi-dialect-Arabic-BERT MSA/DA - - WP 32k -\n(Talafha et al., 2020)\nX6 GigaBERTv4 (Lan et al., 2020) MSA - 10.4B WP 50k 1.48M\nX7 MARBERT (Abdul-Mageed et al., 2020a) MSA/DA 128GB 15.6B WP 100K 17M\nX8 ARBERT (Abdul-Mageed et al., 2020a) MSA 61GB 6.5B WP 100K 8M\nCAMeLBERT-MSA MSA 107GB 12.6B WP 30k 1M\nCAMeLBERT-DA DA 54GB 5.8B WP 30k 1M\nCAMeLBERT-CA CA 6GB 847M WP 30k 1M\nCAMeLBERT-Mix MSA/DA/CA 167GB 17.3B WP 30k 1M\nTable 1: Conﬁgurations of existing models and CAMeLBERT models. Ref is a model identiﬁer used in Table 5.\nWP is WordPiece and SP is SentencePiece.\n2 Related Work\nThere have been several research efforts on Ara-\nbic pre-trained models achieving state-of-the-art\nresults in a number of Arabic NLP tasks. One\nof the earliest efforts includes AraBERT (Antoun\net al., 2020), where they pre-trained a monolingual\nBERT model using 24GB of Arabic text in the\nnews domain. Safaya et al. (2020) pre-trained Ara-\nbicBERT using 95GB of text mainly from the Ara-\nbic portion of the OSCAR corpus. Based on Ara-\nbicBERT, Talafha et al. (2020) further pre-trained\ntheir model using 10 million tweets, which in-\ncluded dialectal data. Lan et al. (2020) released sev-\neral English-Arabic bilingual models dubbed Gi-\ngaBERTs, where they studied the effectiveness of\ncross-lingual transfer learning and code-switched\npre-training using Wikipedia, Gigaword, and the\nOSCAR corpus. Most recently, Abdul-Mageed\net al. (2020a) developed two models, ARBERT and\nMARBERT, pre-trained on a large collection of\ndatasets in MSA and DA. They reported new state-\nof-the-art results on the majority of the datasets in\ntheir ﬁne-tuning benchmark.\nMoreover, there have been various studies ex-\nplaining why pre-trained language models perform\nwell on downstream tasks either in monolingual\n(Hewitt and Manning, 2019; Jawahar et al., 2019;\nLiu et al., 2019a; Tenney et al., 2019a,b) or multi-\nlingual settings (Wu and Dredze, 2019; Chi et al.,\n2020; Kulmizev et al., 2020; Vuli ´c et al., 2020).\nMost of these efforts leveraged probing techniques\nto explore the linguistic knowledge that is captured\nby pre-trained language models such as morphosyn-\ntactic and semantic knowledge. More recently,\nthere have been additional efforts investigating the\neffects of pre-training data size and tokenization\non the performance of pre-trained language mod-\nels. Zhang et al. (2020) showed that pre-training\nRoBERTa requires 10M to 100M words to learn\nrepresentations that reliably encode most syntactic\nand semantic features. However, a much larger\nquantity of data is needed for the model to per-\nform well on typical downstream NLU tasks. Rust\net al. (2020) empirically compared multilingual\npre-trained language models to their monolingual\ncounterparts on a set of nine typologically diverse\nlanguages. They showed that while the pre-training\ndata size is an important factor, the designated tok-\nenizer of each monolingual model plays an equally\nimportant role in the downstream performance.\nIn this work, we primarily focus on understand-\ning the behavior of pre-trained models against vari-\nables such as data sizes and language variants. We\ncompare against eight existing models. We ﬁnd\nthat AraBERTv02 (X3) is the best on average and\nit wins or ties for a win in six out of 12 subtasks.\nOur CAMeLBERT-Star model is second overall\non average, and it wins or ties for a win in four\nout of 12 subtasks. Interestingly, these systems are\ncomplementary in their performance and between\nthe two, they win or tie for a win in nine out of 12\nsubtasks.\n3 Pre-training CAMeLBERT\nWe describe the datasets and the procedure we use\nto pre-train our models. We use the original imple-\nmentation released by Google for pre-training.2\n3.1 Data\nMSA Training Data For MSA, we use the Ara-\nbic Gigaword Fifth Edition (Parker et al., 2011),\nAbu El-Khair Corpus (El-Khair, 2016), OSIAN\ncorpus (Zeroual et al., 2019), Arabic Wikipedia,3\nand the unshufﬂed version of the Arabic OSCAR\ncorpus (Ortiz Su´arez et al., 2020).\nDA Training Data For DA, we collect a range\nof dialectal corpora: LDC97T19-CALLHOME\nTranscripts (Gadalla et al., 1997); LDC2002T38-\nCALLHOME Supplement Transcripts (Linguistic\nData Consortium, 2002); LDC2005S08-Babylon\nLevantine Arabic Transcripts (BBN Technologies,\n2005); LDC2005S14-CTS Levantine Arabic Tran-\nscripts (Maamouri et al., 2005); LDC2006T07-\nLevantine Arabic Transcripts (Maamouri et al.,\n2006); LDC2006T15-Gulf Arabic Transcripts (Ap-\npen Pty Ltd, 2006a); LDC2006T16-Iraqi Arabic\nTranscripts (Appen Pty Ltd, 2006b); LDC2007T01-\nLevantine Arabic Transcripts (Appen Pty Ltd,\n2007); LDC2007T04-Fisher Levantine Arabic\nTranscripts (Maamouri et al., 2007); Arabic Online\nCommentary Dataset (AOC) (Zaidan and Callison-\nBurch, 2011); LDC2012T09-English/Arabic Paral-\nlel text (Raytheon BBN Technologies et al., 2012);\nArabic Multi Dialect Text Corpora (Almeman\nand Lee, 2013); A Multidialectal Parallel Corpus\nof Arabic (Bouamor et al., 2014); Multi-Dialect,\nMulti-Genre Corpus of Informal Written Arabic\n(Cotterell and Callison-Burch, 2014); YouDACC\n(Salama et al., 2014); PADIC (Meftouh et al.,\n2015); Curras (Jarrar et al., 2016); WERd (Ali et al.,\n2017); LDC2017T07-BOLT Egyptian SMS (Chen\net al., 2017); Shami (Abu Kwaik et al., 2018);\nSUAR (Al-Twairesh et al., 2018); Arap-Tweet (Za-\nghouani and Charﬁ, 2018); Gumar (Khalifa et al.,\n2018); MADAR (Bouamor et al., 2018); Habibi (El-\nHaj, 2020); NADI (Abdul-Mageed et al., 2020b);\nand QADI (Abdelali et al., 2020).\nCA Training Data For CA, we use the OpenITI\ncorpus (v1.2) (Nigst et al., 2020).\n2https://github.com/google-research/\nbert\n3https://archive.org/details/\narwiki-20190201\n3.2 Pre-processing\nAfter extracting the raw text from each corpus, we\napply the following pre-processing. We ﬁrst re-\nmove invalid characters and normalize white spaces\nusing the utilities provided by the original BERT\nimplementation. We also remove lines without any\nArabic characters. We then remove diacritics and\nkashida using CAMeL Tools (Obeid et al., 2020).\nFinally, we split each line into sentences with a\nheuristic-based sentence segmenter.\n3.3 Preparing Data for BERT Pre-training\nWe follow the original English BERT model’s hy-\nperparameters for pre-training. We train a Word-\nPiece (Schuster and Nakajima, 2012) tokenizer on\nthe entire dataset (167 GB text) with a vocabulary\nsize of 30,000 using Hugging Face’s tokenizers.4\nWe do not lowercase letters nor strip accents. We\nuse whole word masking and a duplicate factor\nof 10. We set maximum predictions per sequence\nto 20 for the datasets with a maximum sequence\nlength of 128 tokens and 80 for the datasets with a\nmaximum sequence length of 512 tokens.\n3.4 Pre-training Procedure\nWe use a Google Cloud TPU (v3-8) for model pre-\ntraining. We use a learning rate of 1e-4 with a\nwarmup over the ﬁrst 10,000 steps. We pre-trained\nour models with a batch size of 1,024 sequences\nwith a maximum sequence length of 128 tokens\nfor the ﬁrst 900,000 steps. We then continued\npre-training with a batch size of 256 sequences\nwith a maximum sequence length of 512 tokens\nfor another 100,000 steps. In total, we pre-trained\nour models for one million steps. Pre-training one\nmodel took approximately 4.5 days.\n4 Fine-tuning Tasks\nWe evaluate our pre-trained language models on\nﬁve NLP tasks: NER, POS tagging, sentiment anal-\nysis, dialect identiﬁcation, and poetry classiﬁcation.\nSpeciﬁcally, we ﬁne-tune and evaluate the models\nusing 12 datasets (corresponding to 12 subtasks).\nWe used Hugging Face’s transformers (Wolf et al.,\n2020) to ﬁne-tune our CAMeLBERT models.5 The\nﬁne-tuning was done by adding a fully connected\nlinear layer to the last hidden state.\n4https://github.com/huggingface/\ntokenizers\n5We used transformers v3.1.0 along with PyTorch v1.5.1\nTask Dataset/Subtask #Label #Train #Test Unit Variant %MSA\nNER ANERcorp (Benajiba et al., 2007) 9 125,102 25,008 Token MSA 83.6\nPOS\nPATB (MSA) (Maamouri et al., 2004) 32 503,015 63,172 Token MSA 85.1\nARZTB (EGY) (Maamouri et al., 2012) 33 133,751 20,464 Token DA 21.5\nGumar (GLF) (Khalifa et al., 2018) 35 162,031 20,100 Token DA 30.0\nSA\nASTD (Nabil et al., 2015) 3 23,327 663 Sent MSA 56.9\nArSAS (Elmadany et al., 2018) 3 23,327 3,705 Sent MSA 60.5\nSemEval (Rosenthal et al., 2017) 3 23,327 6,100 Sent MSA 77.7\nDID\nMADAR-26 (Salameh et al., 2018) 26 41,600 5,200 Sent DA 14.1\nMADAR-6 (Salameh et al., 2018) 6 54,000 12,000 Sent DA 17.2\nMADAR-Twitter-5 (Bouamor et al., 2019) 21 39,836 9,116 Grp MSA 92.3\nNADI (Abdul-Mageed et al., 2020b) 21 21,000 5,000 Sent DA 38.3\nPoetry APCD (Yousef et al., 2019) 23 1,391,541 173,963 Sent CA 71.3\nTable 2: Statistics of our ﬁne-tuning datasets. Unit refers to a unit we use to calculate the number of examples\nin Train and Test. For MADAR-Twitter-5, we use a group of ﬁve tweets as a unit (Grp). A language variant is\ndetermined based on dataset design and the estimated proportion of MSA sentences in the dataset.\nTasks and Variants We selected the ﬁne-tuning\ndatasets and subtasks to represent multiple variants\nof Arabic by design. For some of the datasets, the\nvariant is readily known. However, other datasets\ncontain a lot of social media text where the domi-\nnant variant of Arabic is unknown. Therefore, we\nestimate the proportion of MSA sentences in each\ndataset by identifying whether the text is MSA\nor DA using the Corpus 6 dialect identiﬁcation\nmodel in Salameh et al. (2018) as implemented\nin CAMeL Tools (Obeid et al., 2020). This tech-\nnique does not model CA. Of course, none of the\ndatasets was purely MSA or DA; however, based\non known dataset variants, we observe that having\nabout 40% or fewer MSA labels strongly suggests\nthat the dataset is dialectal (or a strong dialectal\nmix).\nTable 2 presents the number of labels, size, unit,\nvariant, and MSA percentage for the datasets used\nin the subtasks.\n4.1 Named Entity Recognition\nDataset We ﬁne-tuned our models on the pub-\nlicly available Arabic NER Dataset ANERcorp\n(∼150K words) (Benajiba et al., 2007) which is in\nMSA and we followed the splits deﬁned by Obeid\net al. (2020). We also kept the same IOB (inside,\noutside, beginning) tagging format deﬁned in the\ndataset covering four classes: Location (LOC), Mis-\ncellaneous (MISC), Organization (ORG), and Per-\nson (PERS).\nExperimental Setup During ﬁne-tuning, we\nused the representation of the ﬁrst sub-token as\nan input to the linear layer. All models were ﬁne-\ntuned on a single GPU for 3 epochs with a learning\nrate of 5e-5, batch size of 32, and a maximum se-\nquence length of 512. Since ANERcorp does not\nhave a dev set, we used the last checkpoint after\nthe ﬁne-tuning is done to report results on the test\nset using the micro F1 score.\n4.2 Part-of-Speech Tagging\nDataset We ﬁne-tuned our models on three dif-\nferent POS tagging datasets: (1) the Penn Arabic\nTreebank (PATB) (Maamouri et al., 2004) which is\nin MSA and includes 32 POS tags; (2) the Egyp-\ntian Arabic Treebank (ARZATB) (Maamouri et al.,\n2012) which is in Egyptian (EGY) and includes\n33 POS tags; and (3) the GUMAR corpus (Khalifa\net al., 2018) which is in Gulf (GLF) and includes\n35 POS tags.\nExperimental Setup Similar to NER, we used\nthe representation of the ﬁrst sub-token as an input\nto the linear layer. Our models were ﬁne-tuned on\na single GPU for 10 epochs with a learning rate of\n5e-5, batch size of 32, and a maximum sequence\nlength of 512. We used the same hyperparameters\nfor the ﬁne-tuning across the three POS tagging\ndatasets. After the ﬁne-tuning, we used the best\ncheckpoints based on the dev sets to report results\non the test sets using the accuracy score.\n4.3 Sentiment Analysis\nDataset We used a combination of sentiment\nanalysis datasets to ﬁne-tune our models. The\ndatasets are: (1) the Arabic Speech-Act and Senti-\nment Corpus of Tweets (ArSAS) (Elmadany et al.,\n2018); (2) the Arabic Sentiment Tweets Dataset\n(ASTD) (Nabil et al., 2015); (3) SemEval-2017 task\n4-A benchmark dataset (Rosenthal et al., 2017);\nand (4) the Multi-Topic Corpus for Target-based\nSentiment Analysis in Arabic Levantine Tweets\n(ArSenTD-Lev) (Baly et al., 2019). We combined\nand preprocessed the datasets in a similar way to\nwhat was done by Abu Farha and Magdy (2019)\nand Obeid et al. (2020). That is, we removed dia-\ncritics, URLs, and Twitter usernames from all the\ntweets.\nExperimental Setup Our models were ﬁne-\ntuned on ArSenTD-Lev and the train splits from\nSemEval, ASTD, and ArSAS (23,327 tweets) on\na single GPU for 3 epochs with a learning rate of\n3e-5, batch size of 32, and a maximum sequence\nlength of 128. After the ﬁne-tuning, we used the\nbest checkpoint based on a single dev set from\nSemEval, ASTD, and ArSAS to report results on\nthe test sets. We used the FPN\n1 score which was\ndeﬁned in the SemEval-2017 task 4-A; FPN\n1 is\nthe macro F1 score over the positive and negative\nclasses only while neglecting the neutral class.\n4.4 Dialect Identiﬁcation\nDataset We ﬁne-tuned our models on four dif-\nferent dialect identiﬁcation datasets: (1) MADAR\nCorpus 26 which includes 26 labels; (2) MADAR\nCorpus 6 which includes six labels; (3) MADAR\nTwitter Corpus (Bouamor et al., 2018; Salameh\net al., 2018; Bouamor et al., 2019) which includes\n21 labels; and (4) NADI Country-level (Abdul-\nMageed et al., 2020b) which includes 21 labels.\nThe datasets were preprocessed by removing dia-\ncritics, URLs, and Twitter usernames while main-\ntaining the same train, dev, and test splits for each\ndataset. Moreover, we collated the tweets belong-\ning to a particular user in the MADAR Twitter Cor-\npus in groups of 5 before feeding them to the model.\nWe refer to this preprocessed version as MADAR-\nTwitter-5 to avoid confusion with the publicly avail-\nable original MADAR Twitter Corpus.\nExperimental Setup Our models were ﬁne-\ntuned for 10 epochs with a learning rate of 3e-5,\nbatch size of 32, and a maximum sequence length\nof 128. After the ﬁne-tuning, we used the best\ncheckpoints based on the dev sets to report results\non the test sets using the macroF1 score. Moreover,\nfor the MADAR-Twitter-5 evaluation, we took a\nvoting approach. That is, each user in the dev and\ntest sets is assigned to the most frequent predicted\ncountry label. In case of a tie, we always pick the\nmost frequent predicted country label based on the\ntraining set.\n4.5 Poetry Meter Classiﬁcation\nDataset We used the Arabic Poem Comprehen-\nsive Dataset (APCD) (Yousef et al., 2019), which\nis mostly in CA, to ﬁne-tune our models to identify\nthe meters of Arabic poems. The dataset contains\naround 1.8M poems and covers 23 meters. We\npreprocessed the dataset by removing diacritics\nfrom the poems and separated the halves of each\nverse by using the [SEP] token. We applied an\n80/10/10 random split to create train, dev, and test\nsets respectively.\nExperimental Setup We ﬁne-tuned our models\non a single GPU for 3 epochs with a learning rate\nof 3e-5, batch size of 32, and a maximum sequence\nlength of 128. After the ﬁne-tuning, we used the\nbest checkpoint based on the dev set to report re-\nsults on the test set using the macro F1 score.\n5 Evaluation Results and Discussion\nWe ﬁrst present an experiment where we investigate\nthe effect of pre-training data size. We then report\non CAMeLBERT models pre-trained on MSA, DA,\nand CA data, in addition to a model that is pre-\ntrained on a mixture of these variants. We then\nprovide a comparison against publicly available\nmodels.\n5.1 Models with Different Data Sizes\nTo investigate the effect of pre-training data size\non ﬁne-tuning tasks, we pre-train MSA models\nin a controlled setting where we scale down the\nMSA pre-training size by a factor of two while\nkeeping all other hyperparameters constant. We\npre-train four CAMeLBERT models on MSA data\nas follows: MSA-1/2 (54GB, 6.3B words), MSA-\n1/4 (27GB, 3.1B words), MSA-1/8 (14GB, 1.5B\nwords), and MSA-1/16 (6GB, 636M words). In\nTable 3, we show the results on our ﬁne-tuning\nsubtasks.\nWe observe that the full MSA model and the\nMSA-1/4 model are on average the highest per-\nforming systems, even though the MSA-1/4 model\nwas pre-trained on a quarter of the full MSA data.\nThe MSA-1/4 model wins or ties for a win in seven\nTask Subtask Variant\n%Performance\nMSA MSA-1/2 MSA-1/4 MSA-1/8 MSA-1/16 Max-Min(107GB) (53GB) (27GB) (14GB) (6GB)\nNER ANERcorp MSA 82.4 82.0 82.1 82.6 80.8 1.9\nPOS\nPATB (MSA) MSA 98.3 98.2 98.3 98.2 98.2 0.1\nARZTB (EGY) DA 93.6 93.6 93.7 93.6 93.6 0.2\nGumar (GLF) DA 97.9 97.9 97.9 97.9 97.9 0.1\nSA\nASTD MSA 76.9 76.0 76.8 76.7 75.3 1.6\nArSAS MSA 93.0 92.6 92.5 92.5 92.3 0.8\nSemEval MSA 72.1 70.7 72.8 71.6 71.2 2.0\nDID\nMADAR-26 DA 62.6 62.0 62.8 62.0 62.2 0.8\nMADAR-6 DA 91.9 91.8 92.2 92.1 92.0 0.4\nMADAR-Twitter-5 MSA 77.6 78.5 77.3 77.7 76.2 2.3\nNADI DA 24.9 24.6 24.6 24.9 23.8 1.1\nPoetry APCD CA 79.7 79.9 80.0 79.7 79.8 0.3\nVariant-wise-average\nMSA 83.4 83.0 83.3 83.2 82.3 1.1\nDA 74.2 74.0 74.3 74.1 73.9 0.4\nCA 79.7 79.9 80.0 79.7 79.8 0.3\nMacro-average 79.2 79.0 79.2 79.1 78.6 0.6\nTable 3: Performance of CAMeLBERT models trained on MSA datasets with different sizes. We use the F1\nscore as an evaluation metric for all tasks, except for POS tagging where we use accuracy. Max-Min refers to the\ndifference in performance among the models for each dataset. Variant-wise-average refers to average over a group\nof tasks in the same language variant. The best results among the models are in bold.\nout of 12 subtasks, and it is also the best model on\naverage in the DA and CA subtasks.\nWe also observe that different subtasks have dif-\nferent patterns. For some subtasks, plateauing in\nperformance happens rather early. For instance,\nthe performance on Gumar (GLF) does not change\neven if we increase the size. Similarly, the differ-\nence in performance on PATB (MSA) is very small.\nFor other subtasks, the improvement is not consis-\ntent with the size, as seen in SemEval. When we\ncalculate the correlation between the performance\nand the pre-training data size, we note that ArSAS\nhas a strong positive correlation of 0.96, however,\nMADAR-6 has a negative correlation of -0.62. In\nfact, the average of the correlation of each of the 12\nexperiments is 0.25, which is not a strong pattern\ncorrelating size with performance.\nThese observations suggest that the size of pre-\ntraining data has limited and inconsistent effect on\nthe ﬁne-tuning performance. This is consistent with\nMicheli et al. (2020), where they concluded that\npre-training data size does not show a strong mono-\ntonic relationship with ﬁne-tuning performance in\ntheir controlled experiments on French corpora.\n5.2 Models with Different Language Variants\nNext, we explore the relationship between language\nvariants in pre-training and ﬁne-tuning datasets.\n5.2.1 MSA, DA, and CA\nTask Type Difference We compare the behavior\nof three models pre-trained on MSA, DA, and CA\ndata. From Table 4, we observe that the difference\nin performance (Max-Min) among CAMeLBERT’s\nMSA, DA, and CA models is 4.6% on average,\nranging from 0.2% to 14.5%. To study trends by\ntask type, we compute the average performance\ndifference across the subtasks for each task. NER is\nthe most sensitive to the pre-trained model variant\n(14.5%), followed by sentiment analysis (8.2%),\ndialect identiﬁcation (3.8%), poetry classiﬁcation\n(1.3%), and POS tagging (0.7%). This indicates\nthe importance of optimal pairing of pre-trained\nmodels and ﬁne-tuning tasks.\nOn average the CAMeLBERT-MSA model per-\nforms best, and is the winner in 10 out of 12\nsubtasks. The following are the two exceptions:\n(a) the CAMeLBERT-DA model performs best in\nthe highly dialectal MADAR-6 subtask; and (b)\nthe CAMeLBERT-CA model outperforms other\nmodels in the poetry classiﬁcation task, which is\nTask Dataset Variant %Performance %OOV\nStar Mix MSA DA CA Max-Min Mix MSA DA CA\nNER ANERcorp MSA 82.4 80.8 82.4 74.1 67.9 14.5 0.2 0.2 1.4 4.2\nPOS\nPATB (MSA) MSA 98.3 98.1 98.3 97.7 97.8 0.6 0.2 0.2 0.9 3.0\nARZTB (EGY) DA 93.6 93.6 93.6 92.7 92.3 1.4 0.6 0.8 1.0 7.3\nGumar (GLF) DA 98.1 98.1 97.9 97.9 97.7 0.2 0.2 0.8 0.3 5.4\nSA\nASTD MSA 76.9 76.3 76.9 74.6 69.4 7.5 0.9 1.1 1.2 5.3\nArSAS MSA 93.0 92.7 93.0 91.8 89.4 3.6 1.3 1.5 1.8 7.4\nSemEval MSA 72.1 69.0 72.1 68.4 58.5 13.6 1.9 2.1 2.4 6.6\nDID\nMADAR-26 DA 62.9 62.9 62.6 61.8 61.9 0.8 0.4 0.8 0.8 7.5\nMADAR-6 DA 92.5 92.5 91.9 92.2 91.5 0.7 0.1 1.1 0.2 8.1\nMADAR-Twitter-5 MSA 77.6 75.7 77.6 74.2 71.4 6.2 2.4 2.6 3.0 6.7\nNADI DA 24.7 24.7 24.9 20.1 17.3 7.6 1.6 2.0 2.4 8.1\nPoetry APCD CA 80.9 79.8 79.7 79.6 80.9 1.3 0.4 1.1 2.7 0.9\nVariant-wise-average\nMSA 83.4 82.1 83.4 80.1 75.7 7.6 1.2 1.3 1.8 5.5\nDA 74.4 74.4 74.2 72.9 72.1 2.1 0.6 1.1 0.9 7.3\nCA 80.9 79.8 79.7 79.6 80.9 1.3 0.4 1.1 2.7 0.9\nMacro-average 79.4 78.7 79.2 77.1 74.7 4.6 0.9 1.2 1.5 5.9\nTable 4: Performance of CAMeLBERT models trained on MSA, DA, CA, and their Mix data. Star refers to a way\nof choosing CAMeLBERT models based on the language variant of the ﬁne-tuning dataset. We use theF1 score as\nan evaluation metric for all tasks, except for POS tagging where we use accuracy. Max-Min refers to the difference\nin performance among CAMeLBERT’s MSA, DA, and CA models only. The best results among CAMeLBERT’s\nMSA, DA, and CA models are underlined. The best results among CAMeLBERT’s MSA, DA, CA, Mix, and Star\nare in bold. The OOV rate for each dataset is calculated based on the data used for pre-training each model. We\nunderline the lowest OOV value per dataset.\nin classical Arabic. These two exceptions sug-\ngest that performance in ﬁne-tuning tasks may be\nassociated with the variant proximity of the pre-\ntraining data to ﬁne-tuning data; although we also\nacknowledge that CAMeLBERT-MSA’s data is\ntwo times the size of CAMeLBERT-DA’s, and 18\ntimes the size of CAMeLBERT-CA’s, which may\ngive CAMeLBERT-MSA an advantage.\nOOV Effect To further investigate the effect of\nvariant proximity on performance, we compute\nthe word out-of-vocabulary (OOV) rate of all ﬁne-\ntuning test sets against the pre-training data, as\na way to estimate their similarity. 6 Note that\nCAMeLBERT-Mix, where we concatenate MSA,\nDA, and CA pre-training data, has the lowest OOV\nrate by design. In Table 4, we show the OOV rates\nfor each dataset.\nIn all the cases, we obtain the best performance\nwhere the model has the lowest OOV rate. To\nbetter understand the relationship between ﬁne-\ntuning performance and OOV rates, we assessed\nthe correlation between model performance and\n6We use a simple token as a unit, where we segment text\nwith white space and punctuation.\nOOV rates for each dataset. We found a strong\nnegative correlation of -0.82 on average. Inter-\nestingly, the CAMeLBERT-CA model which was\npre-trained only on 6 GB of data outperforms other\nmodels that are pre-trained on signiﬁcantly larger\ndata in the poetry classiﬁcation task. It is also worth\nmentioning that the CAMeLBERT-CA model has\nthe lowest OOV rate on the poetry dataset (0.9%),\nwhile having access to approximately 18 times less\ndata compared to the CAMeLBERT-MSA model\n(6GB vs 107GB). This again suggests that the vari-\nant proximity of pre-training data to ﬁne-tuning\ndata is more important than the size of pre-training\ndata.\n5.2.2 Mix of MSA, DA, and CA\nTo further study the interplay of language variants\nand pre-training data size, we pre-trained a model\n(CAMeLBERT-Mix) on the concatenation of the\nMSA, DA, and CA datasets. This is the largest\ndataset used to pre-train an Arabic language model\nto date. As shown in Table 4, the CAMeLBERT-\nMix model improves over other models in three\ncases, all of which are dialectal, suggesting that the\nTask Dataset Variant %Performance\nStar Mix MSA DA CA X1 X2 X3 X4 X5 X6 X7 X8\nNER ANERcorp MSA 82.4 80.8 82.4 74.1 67.9 76.7 82.8 82.0 80.3 77.3 82.0 79.3 83.6\nPOS\nPATB (MSA) MSA 98.3 98.1 98.3 97.7 97.8 97.9 98.2 98.3 98.3 98.1 98.4 98.0 98.4\nARZTB (EGY) DA 93.6 93.6 93.6 92.7 92.3 92.0 93.0 94.1 93.1 93.3 93.6 93.5 93.6\nGumar (GLF) DA 98.1 98.1 97.9 97.9 97.7 97.4 97.8 98.1 97.8 97.7 98.0 97.9 97.9\nSA\nASTD MSA 76.9 76.3 76.9 74.6 69.4 64.5 74.2 78.1 73.5 74.2 74.3 77.0 74.9\nArSAS MSA 93.0 92.7 93.0 91.8 89.4 88.4 91.5 93.3 92.3 91.3 92.2 92.9 91.9\nSemEval MSA 72.1 69.0 72.1 68.4 58.5 57.5 69.5 72.7 69.5 70.7 70.0 70.4 70.2\nDID\nMADAR-26 DA 62.9 62.9 62.6 61.8 61.9 60.4 61.9 62.2 58.4 59.8 59.1 61.2 60.7\nMADAR-6 DA 92.5 92.5 91.9 92.2 91.5 90.8 91.9 92.3 90.8 91.5 91.4 92.1 91.4\nMADAR-Twitter-5 MSA 77.6 75.7 77.6 74.2 71.4 71.8 79.0 79.0 74.7 77.7 77.6 78.6 76.5\nNADI DA 24.7 24.7 24.9 20.1 17.3 16.7 21.1 24.5 24.0 25.0 21.3 27.0 24.6\nPoetry APCD CA 80.9 79.8 79.7 79.6 80.9 78.8 79.6 79.9 78.8 79.1 79.1 79.0 78.4\nVariant-wise-average\nMSA 83.4 82.1 83.4 80.1 75.7 76.1 82.5 83.9 81.4 81.6 82.4 82.7 82.6\nDA 74.4 74.4 74.2 72.9 72.1 71.5 73.1 74.2 72.8 73.5 72.7 74.3 73.6\nCA 80.9 79.8 79.7 79.6 80.9 78.8 79.6 79.9 78.8 79.1 79.1 79.0 78.4\nMacro-average 79.4 78.7 79.2 77.1 74.7 74.4 78.4 79.5 77.6 78.0 78.1 78.9 78.5\nTable 5: Performance of CAMeLBERT models and other existing models. We use the F1 score as an evaluation\nmetric for all tasks, except for POS tagging where we use accuracy. Star refers to a way of choosing CAMeL-\nBERT models based on the language variant of the ﬁne-tuning dataset. X1,···, X8 corresponds to the models in\nTable 1. The best results among the models are in bold.\nCAMeLBERT-Mix model does better in some di-\nalectal context. However, we do not see an increase\nin performance in other cases when compared with\nthe best performing model, although the size of the\npre-training data and the variety of the data are in-\ncreased. This suggests that having a wide language\nvariety in pre-training data can be beneﬁcial for DA\nsubtasks, whereas variant proximity of pre-training\ndata to ﬁne-tuning data is important MSA and CA\nsubtasks.\n5.2.3 Selecting an Optimal Model\nTaking these insights into consideration, one can-\nnot help but consider the exciting possibility of\na system-selection ensembling approach that can\nhelp users make decisions with reasonable expecta-\ntions using what they know of their speciﬁc tasks.\nWe outline here such a setup: the user has access to\nthree versions of the models: CAMeLBERT’s CA,\nMSA, and Mix. If the task data is known a priori\nto be CA, then we select the CAMeLBERT-CA\nmodel; if the task data is known to be MSA, we\nselect the CAMeLBERT-MSA model; otherwise,\nwe use the CAMeLBERT-Mix model (for dialects,\ni.e.). We report on this model in Table 4 and 5 as\nCAMeLBERT-Star.\nIt is noteworthy that this model is not the same as\noracularly selecting the best performer among our\nfour models (MSA, DA, CA, and Mix). In fact, it\nis lower in performance than such oracular system\nas the CAMeLBERT-MSA model performs better\nthan CAMeLBERT-Mix model in NADI. We do not\nclaim here that this is a foolproof method; however,\nit is an interesting candidate for common wisdom\nof the kind we are hoping to develop through this\neffort.\n5.3 Comparison with Existing Models\nTable 5 compares our work with other existing mod-\nels. We do not use models that require morpho-\nlogical pre-tokenization to allow direct compari-\nson, and also because existing tokenization systems\nare mostly focused on MSA or EGY (Pasha et al.,\n2014; Abdelali et al., 2016; Obeid et al., 2020).\nWe are aware that design decisions such as vo-\ncabulary size and number of training steps are not\nthe same across these eight existing pre-trained\nmodels, which might be a contributing factor to\ntheir varying performances. We plan to investigate\nthe effects of such decisions in future work.\nTask Performance Complementarity The best\nmodel on average is AraBERTv02 (X3); it wins or\nties for a win in six out of 12 subtasks (four MSA\nand two DA). Our CAMeLBERT-Star is second\noverall on average, and it wins or ties for a win\nin four out of 12 subtasks (three DA, one CA).\nInterestingly, the two systems are complementary\nin their performance and between the two they win\nor tie for a win in nine out of 12 subtasks. The\nthree remaining subtasks are won by MARBERT\n(X7) (NADI, DA), ARBERT ( X8) (ANERcorp,\nMSA; PATB, MSA), and GigaBERT (X6) (PATB,\nMSA). In practice, such complementarity can be\nexploited by system developers to achieve higher\noverall performance.\nSize and Performance Considering the data size\nand performance of the other pre-trained models\n(X1 to X8), we observe a similar trend to our\nCAMeLBERT models. AraBERTv02 (X3) is the\nbest on average, with only 77GB of pre-training\ndata. AraBERTv01 ( X2) is the smallest (24GB);\nhowever, on average it outperforms other models\npre-trained on much larger datasets, such as and\nArabicBERT (X4, 95GB) and multi-dialectal Ara-\nbic BERT ( X5, 95GB with 10M tweets). This\nconﬁrms that pre-training data size may not be an\nimportant factor to ﬁne-tuning performance, as we\nshowed in Section 5.1.\nVariant Proximity and Performance When we\nexamine the proximity in terms of language vari-\nants of the pre-training data and the ﬁne-tuning\ndata across the eight existing pre-trained models,\nwe observe the following. First, the monolingual\nMSA models (X2, X3, X4, X8) are better perform-\ners than the mixed models ( X5, X7) on average\n(78.5% and 78.4%, respectively). 7 Second, the\nmonolingual MSA models perform better than the\nmixed models in MSA subtasks on average (82.6%\nand 82.1%, respectively), while the mixed models\nperform better than the MSA models in DA sub-\ntasks on average (73.5% and 73.9%, respectively).8\nThis result is consistent with our analysis of the\nCAMeLBERT-Mix and the CAMeLBERT-MSA\nmodels in Section 5.2, where we found that the\nCAMeLBERT-Mix model is the best choice for DA\nsubtasks, whereas the CAMeLBERT-MSA model\nis the best in MSA subtasks.\nOn MARBERT and ARBERT In another study\nthat compared models pre-trained on MSA alone\nor a mix of MSA and DA data, Abdul-Mageed et al.\n(2020a) reported that MARBERT (X7, pre-trained\n7The average over macro-average performances.\n8The average over variant-wise-average performances.\non MSA-DA mix) is more powerful than AR-\nBERT (X8, pre-trained on MSA). In our study, we\ndo replicate their speciﬁc relative performance in\nterms of macro-average in our experiments (78.9%\nfor MARBERT and 78.5% for ARBERT). It is not\nclear why MARBERT and ARBERT do not ex-\nhibit similar trends as observed in the analysis of\nour own CAMeLBERT models and other existing\nmodels. This may be attributed to numerous fac-\ntors such as the degree of MSA-DA mixture, genre,\nand the pre-training procedure details. It is also\nworth noting that the data used to pre-train our\nCAMeLBERT-MSA model is a subset of the data\nused to pre-train our CAMeLBERT-Mix model,\nwhereas the pre-training data for MARBERT and\nARBERT are derived from different data sources.\n6 Conclusion and Future Work\nIn this paper, we investigated the interplay of size,\nlanguage variant, and ﬁne-tuning task type in Ara-\nbic pre-trained language models using carefully\ncontrolled experiments on a number of Arabic NLP\ntasks. Our results show that pre-training data and\nsubtask data variant proximity is more important\nthan pre-training data size. We conﬁrm these re-\nsults on existing models. We exploit this insight\nin deﬁning an optimized system selection model\nfor the studied tasks. We make all of our created\nmodels and ﬁne-tuning code publicly available.\nIn future work, we plan to explore other design\ndecisions that may contribute to the ﬁne-tuning per-\nformance, including vocabulary size, tokenization\ntechniques, and additional data mixtures. We also\nplan to utilize CAMeLBERT models in a number of\nother Arabic NLP tasks, and integrate them in the\nopen-source toolkit, CAMeL Tools (Obeid et al.,\n2020).\nAcknowledgment\nThis research was supported with Cloud TPUs from\nGoogle’s TensorFlow Research Cloud (TFRC).\nThis work was also carried out on the High Per-\nformance Computing resources at New York Uni-\nversity Abu Dhabi. The ﬁrst and second authors\nwere supported by the New York University Abu\nDhabi Global PhD Student Fellowship program.\nWe thank Salam Khalifa, and Ossama Obeid for\nhelpful discussions. We also thank the anonymous\nreviewers for their valuable comments.\nReferences\nAhmed Abdelali, Kareem Darwish, Nadir Durrani, and\nHamdy Mubarak. 2016. Farasa: A fast and furious\nsegmenter for Arabic. In Proceedings of the Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL), pages\n11–16, San Diego, California.\nAhmed Abdelali, Hamdy Mubarak, Younes Samih,\nSabit Hassan, and Kareem Darwish. 2020. Arabic\ndialect identiﬁcation in the wild.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2020a. ARBERT\n& MARBERT: Deep bidirectional transformers for\nArabic.\nMuhammad Abdul-Mageed, Chiyu Zhang, Houda\nBouamor, and Nizar Habash. 2020b. NADI 2020:\nThe First Nuanced Arabic Dialect Identiﬁcation\nShared Task. In Proceedings of the Fifth Arabic\nNatural Language Processing Workshop (WANLP\n2020), Barcelona, Spain.\nIbrahim Abu Farha and Walid Magdy. 2019. Mazajak:\nAn online Arabic sentiment analyser. In Proceed-\nings of the Fourth Arabic Natural Language Process-\ning Workshop, pages 192–198, Florence, Italy. Asso-\nciation for Computational Linguistics.\nKathrein Abu Kwaik, Motaz Saad, Stergios Chatzikyr-\niakidis, and Simon Dobnik. 2018. Shami: A cor-\npus of Levantine Arabic dialects. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018) , Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nNora Al-Twairesh, Rawan Al-Matham, Nora Madi,\nNada Almugren, Al-Hanouf Al-Aljmi, Shahad Al-\nshalan, Raghad Alshalan, Naﬂa Alrumayyan, Shams\nAl-Manea, Sumayah Bawazeer, Nourah Al-Mutlaq,\nNada Almanea, Waad Bin Huwaymil, Dalal Alqu-\nsair, Reem Alotaibi, Suha Al-Senaydi, and Abeer\nAlfutamani. 2018. SUAR: Towards building a cor-\npus for the Saudi dialect. InProceedings of the Inter-\nnational Conference on Arabic Computational Lin-\nguistics (ACLing).\nAhmed Ali, Preslav Nakov, Peter Bell, and Steve\nRenals. 2017. WERD: using social text spelling\nvariants for evaluating dialectal speech recogni-\ntion. In 2017 IEEE Automatic Speech Recogni-\ntion and Understanding Workshop, ASRU 2017, Ok-\ninawa, Japan, December 16-20, 2017 , pages 141–\n148. IEEE.\nKhalid Almeman and Mark Lee. 2013. Automatic\nbuilding of Arabic multi dialect text corpora by boot-\nstrapping dialect words. In Proceedings of the In-\nternational Conference on Communications, Signal\nProcessing, and their Applications (ICCSPA), pages\n1–6.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nAppen Pty Ltd. 2006a. Gulf Arabic Conversational\nTelephone Speech, Transcripts LDC2006T15.\nAppen Pty Ltd. 2006b. Iraqi Arabic Conversational\nTelephone Speech, Transcripts LDC2006T16.\nAppen Pty Ltd. 2007. Levantine Arabic Conversational\nTelephone Speech, Transcripts LDC2007T01.\nRamy Baly, Alaa Khaddaj, Hazem Hajj, Wassim El-\nHajj, and Khaled Bashir Shaban. 2019. ArSentD-\nLEV: A multi-topic corpus for target-based senti-\nment analysis in Arabic Levantine tweets.\nBBN Technologies. 2005. BBN/AUB DARPA\nBabylon Levantine Arabic Speech and Transcripts\nLDC2005S08.\nYassine Benajiba, Paolo Rosso, and Jos ´e Miguel\nBened´ı Ruiz. 2007. ANERsys: An Arabic Named\nEntity Recognition System Based on Maximum En-\ntropy. In Computational Linguistics and Intelligent\nText Processing, pages 143–153, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.\nHouda Bouamor, Nizar Habash, and Kemal Oﬂazer.\n2014. A multidialectal parallel corpus of Arabic. In\nProceedings of the Language Resources and Evalu-\nation Conference (LREC), Reykjavik, Iceland.\nHouda Bouamor, Nizar Habash, Mohammad Salameh,\nWajdi Zaghouani, Owen Rambow, Dana Abdul-\nrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,\nAlexander Erdmann, and Kemal Oﬂazer. 2018. The\nMADAR Arabic Dialect Corpus and Lexicon. In\nProceedings of the Language Resources and Eval-\nuation Conference (LREC), Miyazaki, Japan.\nHouda Bouamor, Sabit Hassan, and Nizar Habash.\n2019. The MADAR shared task on Arabic ﬁne-\ngrained dialect identiﬁcation. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop, pages 199–207, Florence, Italy. Association\nfor Computational Linguistics.\nSong Chen, Dana Fore, Stephanie Strassel, Haejoong\nLee, and Jonathan Wright. 2017. BOLT Egyptian\nArabic SMS/Chat and Transliteration LDC2017T07.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical rela-\ntions in multilingual BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5564–5577, Online. As-\nsociation for Computational Linguistics.\nRyan Cotterell and Chris Callison-Burch. 2014. A\nMulti-Dialect, Multi-Genre Corpus of Informal\nWritten Arabic. In Proceedings of the Language Re-\nsources and Evaluation Conference (LREC) , pages\n241–245, Reykjavik, Iceland.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMahmoud El-Haj. 2020. Habibi - a multi dialect multi\nnational Arabic song lyrics corpus. In Proceedings\nof The 12th Language Resources and Evaluation\nConference, pages 1318–1326, Marseille, France.\nEuropean Language Resources Association.\nIbrahim Abu El-Khair. 2016. 1.5 billion words Arabic\ncorpus. CoRR, abs/1611.04033.\nAbdelRahim Elmadany, Hamdy Mubarak, and Walid\nMagdy. 2018. ArSAS: An Arabic speech-act and\nsentiment corpus of tweets. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018) , Paris,\nFrance. European Language Resources Association\n(ELRA).\nHassan Gadalla, Hanaa Kilany, Howaida Arram,\nAshraf Yacoub, Alaa El-Habashi, Amr Shalaby,\nKrisjanis Karins, Everett Rowson, Robert MacIn-\ntyre, Paul Kingsbury, David Graff, and Cynthia\nMcLemore. 1997. CALLHOME Egyptian Arabic\ntranscripts LDC97T19. Web Download. Philadel-\nphia: Linguistic Data Consortium.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMustafa Jarrar, Nizar Habash, Faeq Alrimawi, Diyam\nAkra, and Nasser Zalmout. 2016. Curras: an anno-\ntated corpus for the Palestinian Arabic dialect. Lan-\nguage Resources and Evaluation, pages 1–31.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nSalam Khalifa, Nizar Habash, Fadhl Eryani, Ossama\nObeid, Dana Abdulrahim, and Meera Al Kaabi.\n2018. A morphologically annotated corpus of Emi-\nrati Arabic. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nArtur Kulmizev, Vinit Ravishankar, Mostafa Abdou,\nand Joakim Nivre. 2020. Do neural language mod-\nels show preferences for syntactic formalisms? In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4077–\n4091, Online. Association for Computational Lin-\nguistics.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn empirical study of pre-trained transformers for\nArabic information extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4727–4734,\nOnline. Association for Computational Linguistics.\nLinguistic Data Consortium. 2002. CALL-\nHOME Egyptian Arabic Transcripts Supplement\nLDC2002T38.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nMohamed Maamouri, Ann Bies, Tim Buckwalter, and\nWigdan Mekki. 2004. The Penn Arabic Treebank:\nBuilding a Large-Scale Annotated Arabic Corpus.\nIn Proceedings of the International Conference on\nArabic Language Resources and Tools , pages 102–\n109, Cairo, Egypt.\nMohamed Maamouri, Ann Bies, Seth Kulick, Dalila\nTabessi, and Sondos Krouna. 2012. Egyptian Arabic\nTreebank DF Parts 1-8 V2.0 - LDC catalog num-\nbers LDC2012E93, LDC2012E98, LDC2012E89,\nLDC2012E99, LDC2012E107, LDC2012E125,\nLDC2013E12, LDC2013E21.\nMohamed Maamouri, Tim Buckwalter, David Graff,\nand Hubert Jin. 2006. Levantine Arabic QT Train-\ning Data Set 5, Transcripts LDC2006T07.\nMohamed Maamouri, Tim Buckwalter, David Graff,\nand Hubert Jin. 2007. Fisher Levantine Ara-\nbic Conversational Telephone Speech, Transcripts\nLDC2007T04.\nMohamed Maamouri, Tim Buckwalter, and Hubert Jin.\n2005. Levantine Arabic QT Training Data Set 4\n(Speech + Transcripts) LDC2005S14.\nKarima Meftouh, Salima Harrat, Salma Jamoussi,\nMourad Abbas, and Kamel Smaili. 2015. Machine\ntranslation experiments on PADIC: A parallel Ara-\nbic dialect corpus. In Proceedings of the Paciﬁc Asia\nConference on Language, Information and Compu-\ntation.\nVincent Micheli, Martin d’Hoffschmidt, and Franc ¸ois\nFleuret. 2020. On the importance of pre-training\ndata volume for compact language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7853–7858, Online. Association for Computa-\ntional Linguistics.\nMahmoud Nabil, Mohamed Aly, and Amir Atiya. 2015.\nASTD: Arabic sentiment tweets dataset. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2515–\n2519, Lisbon, Portugal. Association for Computa-\ntional Linguistics.\nLorenz Nigst, Maxim Romanov, Sarah Bowen Sa-\nvant, Masoumeh Seydi, and Peter Verkinderen. 2020.\nOpenITI: a Machine-Readable Corpus of Islamicate\nTexts.\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima\nTaji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl\nEryani, Alexander Erdmann, and Nizar Habash.\n2020. CAMeL tools: An open source python toolkit\nfor Arabic natural language processing. In Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 7022–7032, Marseille, France.\nEuropean Language Resources Association.\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1703–1714, Online. Association for Computational\nLinguistics.\nRobert Parker, David Graff, Ke Chen, Junbo Kong, and\nKazuaki Maeda. 2011. Arabic Gigaword Fifth Edi-\ntion. LDC catalog number No. LDC2011T11, ISBN\n1-58563-595-2.\nArfath Pasha, Mohamed Al-Badrashiny, Mona Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan Roth.\n2014. MADAMIRA: A fast, comprehensive tool for\nmorphological analysis and disambiguation of Ara-\nbic. In Proceedings of the Language Resources and\nEvaluation Conference (LREC) , pages 1094–1101,\nReykjavik, Iceland.\nRaytheon BBN Technologies, Linguistic Data Con-\nsortium, and Sakhr Software. 2012. Arabic-\nDialect/English Parallel Text LDC2012T09.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemeval-2017 task 4: Sentiment analysis in twitter.\nIn Proceedings of the International Workshop on Se-\nmantic Evaluation (SemEval), pages 501–516, Van-\ncouver, Canada.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2020. How good is your\ntokenizer? on the monolingual performance of mul-\ntilingual language models.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nAhmed Salama, Houda Bouamor, Behrang Mohit, and\nKemal Oﬂazer. 2014. YouDACC: the Youtube Di-\nalectal Arabic Comment Corpus. In Proceedings of\nthe Language Resources and Evaluation Conference\n(LREC), pages 1246–1251, Reykjavik, Iceland.\nMohammad Salameh, Houda Bouamor, and Nizar\nHabash. 2018. Fine-grained Arabic dialect identi-\nﬁcation. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1332–1344, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean voice search. In International Confer-\nence on Acoustics, Speech and Signal Processing ,\npages 5149–5152.\nBashar Talafha, Mohammad Ali, Muhy Eddin Za’ter,\nHaitham Seelawi, Ibraheem Tuffaha, Mostafa Samir,\nWael Farhan, and Hussein Al-Natsheh. 2020. Multi-\ndialect Arabic BERT for country-level dialect iden-\ntiﬁcation. In Proceedings of the Fifth Arabic Natu-\nral Language Processing Workshop, pages 111–118,\nBarcelona, Spain (Online). Association for Compu-\ntational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaˇs, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nWaleed A. Yousef, Omar M. Ibrahime, Taha M. Mad-\nbouly, and Moustafa A. Mahmoud. 2019. Learning\nmeters of Arabic and English poems with recurrent\nneural networks: a step forward for language under-\nstanding and synthesis.\nWajdi Zaghouani and Anis Charﬁ. 2018. ArapTweet:\nA Large Multi-Dialect Twitter Corpus for Gender,\nAge and Language Variety Identiﬁcation. In Pro-\nceedings of the Language Resources and Evaluation\nConference (LREC), Miyazaki, Japan.\nOmar F Zaidan and Chris Callison-Burch. 2011. The\nArabic Online Commentary Dataset: an Annotated\nDataset of Informal Arabic With High Dialectal Con-\ntent. In Proceedings of the Conference of the Asso-\nciation for Computational Linguistics (ACL) , pages\n37–41.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. OSIAN: Open source inter-\nnational Arabic news corpus - preparation and inte-\ngration into the CLARIN-infrastructure. In Proceed-\nings of the Fourth Arabic Natural Language Process-\ning Workshop, pages 175–182, Florence, Italy. Asso-\nciation for Computational Linguistics.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R. Bowman. 2020. When do you need bil-\nlions of words of pretraining data?",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7948063015937805
    },
    {
      "name": "Arabic",
      "score": 0.715206503868103
    },
    {
      "name": "Task (project management)",
      "score": 0.7016571760177612
    },
    {
      "name": "Natural language processing",
      "score": 0.6836418509483337
    },
    {
      "name": "Modern Standard Arabic",
      "score": 0.667086660861969
    },
    {
      "name": "Language model",
      "score": 0.6637121438980103
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6228684782981873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.613592267036438
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6045489311218262
    },
    {
      "name": "Training set",
      "score": 0.5409960746765137
    },
    {
      "name": "Exploit",
      "score": 0.4612312316894531
    },
    {
      "name": "Linguistics",
      "score": 0.23956111073493958
    },
    {
      "name": "Engineering",
      "score": 0.06319907307624817
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}