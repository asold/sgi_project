{
  "title": "Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features",
  "url": "https://openalex.org/W3202780679",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2899013443",
      "name": "Bruce W. Lee",
      "affiliations": [
        "University of Wisconsin–Madison",
        "Consumer Healthcare Products Association"
      ]
    },
    {
      "id": "https://openalex.org/A3203933341",
      "name": "Yoo-Sung Jang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2057791628",
      "name": "Jason Lee",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3160254559",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W3135668378",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2036367260",
    "https://openalex.org/W3134409176",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1528370852",
    "https://openalex.org/W4297747459",
    "https://openalex.org/W2295056205",
    "https://openalex.org/W4287186318",
    "https://openalex.org/W1618905105",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2004841825",
    "https://openalex.org/W2963156201",
    "https://openalex.org/W2952215077",
    "https://openalex.org/W610032745",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1567923459",
    "https://openalex.org/W2171575620",
    "https://openalex.org/W2165599843",
    "https://openalex.org/W1670346950",
    "https://openalex.org/W3102085674",
    "https://openalex.org/W2807383112",
    "https://openalex.org/W3037554857",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2007231077",
    "https://openalex.org/W4246857541",
    "https://openalex.org/W2112787834",
    "https://openalex.org/W1950891412",
    "https://openalex.org/W168564468",
    "https://openalex.org/W2025075790",
    "https://openalex.org/W2008056655",
    "https://openalex.org/W2105702020",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2305305422",
    "https://openalex.org/W4239584046",
    "https://openalex.org/W3119820556",
    "https://openalex.org/W2915337082",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W178255566",
    "https://openalex.org/W2087388117",
    "https://openalex.org/W4254113264",
    "https://openalex.org/W2512700785",
    "https://openalex.org/W1979532929",
    "https://openalex.org/W2115968643",
    "https://openalex.org/W3047152792",
    "https://openalex.org/W2806183494",
    "https://openalex.org/W2153635508",
    "https://openalex.org/W2898140483",
    "https://openalex.org/W3120899979",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2399511071",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2334398681",
    "https://openalex.org/W2134770395",
    "https://openalex.org/W1971020201",
    "https://openalex.org/W1746111881",
    "https://openalex.org/W33677238",
    "https://openalex.org/W1991750682",
    "https://openalex.org/W2135898104",
    "https://openalex.org/W3097785473",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962753301",
    "https://openalex.org/W2118585731",
    "https://openalex.org/W2117823388",
    "https://openalex.org/W1998504424",
    "https://openalex.org/W3108146978",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W3099585677",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2760797156",
    "https://openalex.org/W2531727313",
    "https://openalex.org/W2153081451",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2055438657",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W3120421331",
    "https://openalex.org/W2611873983",
    "https://openalex.org/W2006751573",
    "https://openalex.org/W1997161938",
    "https://openalex.org/W190511925",
    "https://openalex.org/W2104924975",
    "https://openalex.org/W2152052831",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2019416425",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4245439273",
    "https://openalex.org/W3126930515",
    "https://openalex.org/W2080961804",
    "https://openalex.org/W2964941017",
    "https://openalex.org/W1760231297",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2398973842",
    "https://openalex.org/W2024546487",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1808099759",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1481128830",
    "https://openalex.org/W2062585132",
    "https://openalex.org/W2133944470",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2972960423",
    "https://openalex.org/W136568931",
    "https://openalex.org/W2514166524",
    "https://openalex.org/W2807564732"
  ],
  "abstract": "We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA.",
  "full_text": "Pushing on Text Readability Assessment:\nA Transformer Meets Handcrafted Linguistic Features\nBruce W. Lee1,3\nUniv. of Pennsylvania1\nPA, USA\nbrucelws@seas.upenn.edu\nYoo Sung Jang2,3\nUniv. of Wisconsin-Madison2\nWI, USA\nyjang43@wisc.edu\nJason Hyung-Jong Lee3\nLXPER AI3\nSeoul, South Korea\njasonlee@lxper.com\nAbstract\nWe report two essential improvements in read-\nability assessment: 1. three novel features in\nadvanced semantics and 2. the timely evidence\nthat traditional ML models (e.g. Random For-\nest, using handcrafted features) can combine\nwith transformers (e.g. RoBERTa) to augment\nmodel performance. First, we explore suitable\ntransformers and traditional ML models. Then,\nwe extract 255 handcrafted linguistic features\nusing self-developed extraction software. Fi-\nnally, we assemble those to create several hy-\nbrid models, achieving state-of-the-art (SOTA)\naccuracy on popular datasets in readability as-\nsessment. The use of handcrafted features help\nmodel performance on smaller datasets. Nota-\nbly, our RoBERTA-RF-T1 hybrid achieves the\nnear-perfect classification accuracy of 99%, a\n20.3% increase from the previous SOTA.\n1 Introduction\nThe long quest for advancing readability assess-\nment (RA) mostly centered on handcrafting the\nlinguistic features that affect readability (Pitler and\nNenkova, 2008). RA is a time-honored branch of\nnatural language processing (NLP) that quantifies\nthe difficulty with which a reader understands a text\n(Feng et al., 2010). Being one of the oldest system-\natic approaches to linguistics (Collins-Thompson,\n2014), RA developed various linguistic features.\nThese range from simple measures like the aver-\nage count of syllables to those as sophisticated as\nsemantic complexity (Buchanan et al., 2001).\nPerhaps due to the abundance of dependable lin-\nguistic features, an overwhelming majority of RA\nsystems are Support Vector Machines (SVM) with\nhandcrafted features (Hansen et al., 2021). Such\ntraditional machine learning (ML) methods were\nlinguistically explainable, expandable, and most\nimportantly, competent against the modern neural\nmodels. As a fragmentary example, Filighera et al.\n(2019) reports that a large ensemble of 6 BiLSTMs\nwith BERT (Devlin et al., 2019), ELMo (Peters\net al., 2018), Word2Vec (Mikolov et al., 2013),\nand GloVe (Pennington et al., 2014) embeddings\nshowed only ∼1% accuracy improvement from a\nsingle SVM model developed by Xia et al. (2016).\nEven though deep neural networks have achieved\nstate-of-the-art (SOTA) performance in almost all\nsemantic tasks where sufficient data were available\n(Collobert et al., 2011; Zhang et al., 2015), neural\nmodels started showing promising results in RA\nonly quite recently (Martinc et al., 2021). A known\nchallenge for the researchers in RA is the lack of\nlarge public datasets – with the unique exception of\nWeeBit (Vajjala and Meurers, 2012). Technically\nspeaking, even WeeBit is not entirely public since\nit has to be directly obtained from the authors.\nMartinc et al. (2021) raised the SOTA classi-\nfication accuracy on the popular WeeBit dataset\n(Vajjala and Meurers, 2012) by about 4% using\nBERT. This was the first solid proof that neural\nmodels with auto-generated features can show sig-\nnificant improvement compared to traditional ML\nwith handcrafted features. However, neural models,\nor transformers (which is the interest of this paper),\nstill show not much better performance than tradi-\ntional ML on smaller datasets like OneStopEnglish\n(Vajjala and Luˇci´c, 2018), despite the complexity.\nFrom our observations, the reported low perfor-\nmances of transformers on small RA datasets can\nbe accounted for two reasons. 1. Only BERT was\napplied to RA, and there could be other transform-\ners that perform better, even on small datasets. 2.\nIf a transformer shows weak performance on small\ndatasets, there must be some additional measures\ndone to supply the final model (e.g. ensemble) with\nmore linguistic information, but such a study is rare\nin RA. Hence, we tackle the abovementioned issues\nin this paper. In particular, we 1. perform a wide\nsearch on transformers, traditional ML models, and\nhandcrafted features & 2. develop a hybrid archi-\ntecture for SOTA and robustness on small datasets.\nHowever, before we move on to hybrid mod-\nels, we begin by supplementing an underexplored\nlinguistic branch of handcrafted features. Accord-\ning to survey research on RA (Collins-Thompson,\n2014), the study on advanced semantics is scarce.\nWe lack a model to capture how deeper semantic\nstructures affect readability. We attempt to solve\nthis issue by viewing a text as a collection of latent\ntopics and calculating the probability distribution.\nThen, we move on to combine traditional ML\n(w handcrafted features1) and transformers. Such\na hybrid system is only reported by Deutsch\net al. (2020), concluding, “(hybrid models) did not\nachieve SOTA performance.” But we obtain con-\ntrary results. Through a large study on the optimal\ncombination, we obtain SOTA results on WeeBit\nand OneStopEnglish. Also, our BERT-GB-T1 hy-\nbrid beats the (previous) SOTA accuracy with only\n30% of the full dataset, in section 4.7.\nOur main objectives are creating advanced se-\nmantic features and hybrid models. But our contri-\nbutions to academia are not limited to the above-\nmentioned two. We make the following additions:\n1. We numerically represent certain linguistic prop-\nerties pertaining to advanced semantics.\n2. We develop a large-scale, openly available 255\nfeatures extraction Python toolkit (which is highly\nscarce2 in RA). We name the software LingFeat3.\n3. We conduct wide searches and parametrizations\non transformers4 and traditional ML5 for RA use.\n4. We develop hybrid models for SOTA and robu-\nstness on small datasets. Notably, RoBERTa-RF-T1\nachieves 99% accuracy on OneStopEnglish, 20.3%\nhigher than the previous SOTA (table 5).\n2 Advanced Semantics\n2.1 Overview\nA text is a communication between author and\nreader, and its readability is affected by the reader\nhaving shared world/domain knowledge. Accord-\ning to Collins-Thompson (2014), the features re-\nsulting from topic modeling may characterize the\ndeeper semantic structures of a text. These deeper\nrepresentations accumulate and appear to us in the\nform of perceivable properties like sentiment and\n1We use “handcrafted features” and “linguistic features”\ninterchangeably throughout this paper.\n2An exception is Dr. Vajjala’s Java toolkit, available at\nbitbucket.org/nishkalavallabhi/complexity-features.\n3github.com/brucewlee/lingfeat\n4github.com/yjang43/pushingonreadability_transformers\n5github.com/brucewlee/pushingonreadability_traditional_ML\ngenre. But advanced semantics aims to capture the\ndeeper representation itself.\nAmong the four branches of linguistic properties\n(in RA) identified by Collins-Thompson (2014),\nadvanced semantics remain unexplored. Lexico-\nsemantic (Lu, 2011; Malvern and Richards, 2012),\nsyntactic (Heilman et al., 2007; Petersen and Os-\ntendorf, 2009), and discourse-based (McNamara\net al., 2010) properties had several notable works\nbut little dealt with advanced semantics as the given\ndefinition. The existing examples in higher-level\nsemantics focus on word-level complexity (Collins-\nThompson and Callan, 2004; Crossley et al., 2008;\nLandauer et al., 2011; Nam et al., 2017).\nSuch a phenomenon is complex. The lack of in-\nvestigation on advanced semantics could be due to\nits low correlation with readability. This is plausi-\nble because RA studies often test their features on a\nhuman-labeled dataset, potentially biased towards\neasily recognizable surface-level features (Evans,\n2006). Such biases could cause low performance.\nFurther, it must be noted that: 1. world knowl-\nedge might not always directly indicate difficulty,\nand 2. there can be other existing substitute features\nthat capture similar properties on a word level.\nS1) Kindness is good.\nS2) Christmas is good.\nS3) I return with the stipulation to dismiss Smith’s\ncase; the same being duly executed by me.\nS2 above seems to require more world knowl-\nedge than S1. However, “Christmas”, as a familiar\nentity, seems to have no apparent contribution to in-\ncreased difficulty. If any, similar properties can be\ncaptured by word frequency/familiarity measures\n(lexico-semantics) in a large representative corpus\n(Leroy and Kauchak, 2013). Also, it seems that S3\nis the most difficult, and this can be easily deduced\nusing entity counts (discourse). Entities mostly in-\ntroduce conceptual information (Feng et al., 2010).\nOur key objective in studying advanced seman-\ntics is to identify features that add orthogonal in-\nformation. In other words, we hope to see a perfor-\nmance increase in our overall RA model rather than\nspecific features’ high correlations with readability.\nGiven the considerations, we draw two guide-\nlines: 1. develop passage-level features since most\nword-level attributes are captured by existing fea-\ntures, and 2. value the orthogonal addition of infor-\nmation, not individual feature’s high correlation.\nFigure 1: Graphical representation. Semantic Richness,\nClarity, and Noise. abbrev: abbreviation.\n2.2 Approach\nTopics convey text meaning on a global level (Holt-\ngraves, 1999). In order to capture the deeper struc-\nture of meaning (advanced semantics), we hypothe-\nsized that calculating the distribution of document-\ntopic probabilities from latent dirichlet allocation\n(LDA) (Blei et al., 2003) could be helpful.\nMoreover, domain/world knowledge can be ac-\ncounted for in LDA-resulting measures since LDA\ncan be trained on various data. As explored in Qum-\nsiyeh and Ng (2011), it may seem sensible to use\nthe count of discovered topics as the measure of\nrequired knowledge. However, such measures can\nbe extremely sensitive to passage length. Along\nwith the count of discovered topics, we develop\nthree others that consider how these topics are dis-\ntributed: semantic richness, clarity, and noise.\nFig. 1 depicts the steps: 1. obtain output from a\ntrained LDA model, 2. ignore topic ID and create a\nsorted probabilities list, and 3. calculate semantic\nrichness, clarity, and noise. We model “how” the\ntopics are distributed, not “what” the topics are.\n2.3 Semantic Richness\nTraditionally, semantic richness is quantified ac-\ncording to word usage (Pexman et al., 2008). In\na high-dimensional model of semantic space (Li\net al., 2000), co-occurring words clustered as se-\nmantic neighbors, quantifying semantic richness.\nAs such, the previous models of semantic rich-\nness were often studied for word-level complexity\nand made no explicit connection with readability\non a global level. Also, they were often subject-\ndependent (Buchanan et al., 2001). As concluded\nby Pexman et al. (2008), semantic richness is de-\nfined in several ways. We propose a novel variation.\nWe apply the similar co-occurrence concept but\non the passage level using LDA. Here, semantic\nrichness is the measure of how “largely” populated\nthe topics are. In fig. 1, we approximately define\nrichness as the product total of SPL, which mea-\nsures the count of discovered topics (n) and topic\nprobability (p). Additionally, we multiply index\n(i) to reward longer n so that the overall richness\nincreases faster with more topics. See eqn. 1.\nSemantic Richness =\nnX\ni=1\npi · i (1)\n2.4 Semantic Clarity\nSemantic clarity is critical in understanding text\n(Peabody and Schaefer, 2016). Likewise, complex\nmeaning structures lead to comprehension diffi-\nculty (Pires et al., 2017). Some existing studies\nquantify semantic complexity (or clarity) through\nvarious measures, but most on the fine line between\nlexical and semantic properties (Collins-Thompson,\n2014). They rarely deal with the latent meaning\nrepresentations or the clarity of the main topic.\nFor semantic clarity, we quantify how the prob-\nability distribution (fig. 1) is focused (skewed) to-\nwards the largest discovered topic. In other words,\nwe hope to see how easily identifiable the main\ntopic is. We wanted to adopt the standard skewness\nequation from statistics, but we developed an alter-\nnative (eqn. 2) because the standard equation failed\nto capture the anticipated trends in appendix A.\nSemantic Clarity = 1\nn ·\nnX\ni=1\n(max(p) − pi) (2)\n2.5 Semantic Noise\nSemantic noise is the measure of the less-important\ntopics (those with low probability), also the “tailed-\nness” of sorted probability lists (fig. 1). A sorted\nprobability list that resembles a (right-halved) lep-\ntokurtic curve would have higher semantic noise.\nIn comparison, a (right-halved) platykurtic curve of\nsimilar length would have low semantic noise. We\nadopt the kurtosis equation under Fisher definition\n(Kokoska and Zwillinger, 2000). See eqn. 3.\nSemantic Noise = n ·\nPn\ni=1(pi − ¯p)4\n(Pn\ni=1(pi − ¯p)2)2 (3)\n3 Covered Features\nWe study 255 linguistic features. For the already\nexisting features, we add variations to widen cov-\nerage. The full list of features, feature codes, and\ndefinition are provided in appendix B. Also, we\nclassify features into 14 subgroups. External depe-\nndencies (e.g. parser) are reported in appendix D.\n3.1 Advanced Semantic Features (AdSem)\nHere, we follow the methods provided in section 2.\n1∼3) Wikipedia (WoKF), WeeBit (WBKF),\n& OneStop Knowledge Features (OSKF). Each\nsubgroup name represents the respective training\ndata. We train Online LDA (Hoffman et al., 2010)\nwith the 20210301 dump6 from English Wikipedia\nfor WoKF. The others are trained on two popular\ncorpora in RA: WeeBit and OneStopEnglish.\nFor each training set, four variations of 50, 100,\n150, 200 topics models are trained. Four features –\nsemantic richness, clarity, noise, and the total count\nof discovered topics – are extracted per model.\n3.2 Discourse-Based Features (Disco)\nA text is more than a series of random sentences. It\nindicates a higher-level structure of dependencies.\n4) Entity Density Features (EnDF) . Concep-\ntual information is often introduced by entities.\nHence, the count of entities affects the working\nmemory burden (Feng et al., 2009). We bring\nentity-related features from Feng et al. (2010).\n5) Entity Grid Features (EnGF)Coherent texts\nare easy to comprehend. Thus, we measure coher-\nence through entity grid, using the 16 transition pat-\ntern ratios approach by Pitler and Nenkova (2008)\nas features. Also, we adopt local coherence scores\n(Guinaudeau and Strube, 2013), using the code im-\nplemented by Palma and Atkinson (2018).\n3.3 Syntactic Features (Synta)\nSyntactic complexity is associated with longer pro-\ncessing times (Gibson, 1998). Such syntactic prop-\nerties also affect the overall complexity of a text\n(Hale, 2016), an important indicator of readability.\n6) Phrasal Features (PhrF). Ratios involving\nclauses correlate with learners’ abilities to read (Lu,\n2010). We implement several variations, including\nthe counts of noun, verb, and adverb phrases.\n7) Tree Structure Features (TrSF) . We deal\nwith the structural shape of parsed trees, inspired by\nthe work on average parse tree height by Schwarm\n6dumps.wikimedia.org/enwiki\nand Ostendorf (2005). On a constituency parser\n(appendix D) output, NLTK (Loper and Bird, 2002)\nis used for the final calculation of features.\n8) Part-of-Speech Features (POSF) . Several\nstudies report the effectiveness of using POS counts\nas features (Tonelli et al., 2012; Lee and Lee,\n2020a). We count based on Universal POS tags7.\n3.4 Lexico-Semantic Features (LxSem)\nPerhaps the most explored, lexico-semantics cap-\nture the attributes associated with the difficulty or\nunfamiliarity of words (Collins-Thompson, 2014).\n9) Variation Ratio Features (VarF)Lu (2011)\nreports noun, verb, adjective, and adverb variations,\nwhich represent the proportion of the respective cat-\negory’s words to total. We implement the features\nwith variants from Vajjala and Meurers (2012).\n10) Type Token Ratio Features (TTRF). TTR\nhas been widely used as a measure of lexical rich-\nness in language acquisition studies (Malvern and\nRichards, 2012). We bring five variations of TTR\nfrom Vajjala and Meurers (2012). For MTLD (Mc-\nCarthy and Jarvis, 2010), we default TTR to 0.72.\n11) Psycholinguistic Features (PsyF) As ex-\nplored in Vajjala and Meurers (2016), we imple-\nment various Age-of-Acquisition features from Ku-\nperman study database Kuperman et al. (2012).\n12) Word Familiarity Features (WorF)Word\nfrequency in a large representative corpus often rep-\nresents lexical difficulty (Collins-Thompson, 2014)\ndue to unfamiliarity. We use SubtlexUS database\n(Brysbaert and New, 2009) to measure familiarity.\n3.5 Shallow Traditional Features (ShaTr)\nClassic readability formulas (e.g. Flesch-Kincaid\nGrade) (Kincaid et al., 1975) or shallow measures\noften do not represent a specific linguistic branch.\n13) Shallow Features (ShaF) These features\ncapture surface-level difficulty. Our measures in-\nclude the average count of tokens and syllables.\n14) Traditional Formulas (TraF). For Flesh-\nKincaid Grade Level, Automated Readability, and\nGunning Fog, we follow the “new” formulas in\nKincaid et al. (1975). We follow Si and Callan\n(2001) for Smog Index (Mc Laughlin, 1969). And\nwe follow Eltorai et al. (2015) for Linsear Write.\n7universaldependencies.org/u/pos\nFigure 2: Hybrid model. AdSem, Disco, LxSem, Synta, and ShaTr show handcrafted features’ linguistic branches.\n4 Hybrid Model\n4.1 Overview\nAs shown in section 3, myriad linguistic properties\naffect readability. Despite the continual effort at\nhandcrafting features, they lack coverage. Deutsch\net al. (2020) hint neural models can better model\nthe linguistic properties for RA task. But the perfor-\nmance/flexibility of neural models could improve.\nIn our hybrid model, we take a simple approach\nof joining the soft label predictions of a neural\nmodel (e.g. BERT) with handcrafted features and\nwrapping it with a non-neural model (e.g. SVM).\nIn fig. 2, the non-neural model (i.e. secondary\npredictor) learns 1. predictions/outputs of the neu-\nral model and 2. handcrafted features. The addition\nof handcrafted features supplements what neural\nmodels (i.e. initial predictor) might miss, reinforc-\ning performance on the secondary prediction.\n4.2 Finding Best Combination\nOur hybrid architecture (fig. 2) is simple; Deutsch\net al. (2020) explored a similar concept but did\nnot achieve SOTA. But the benefits (section 4.1)\nfrom its simplicity are critical for RA, which has a\nlacking number/size of public datasets, wide edu-\ncational use, and diverse handcrafted features. We\nobtain SOTA with a wider search on combinations.\n4.2.1 Datasets and Evaluation Setups\nWeeBit. Perhaps the most widely-used, WeeBit is\noften considered the gold standard in RA. It was\nfirst created as an expansion of the famous Weekly\nReader corpus (Feng et al., 2009). To avoid classi-\nfication bias, we downsample classes to equalize\nthe number of items (passages) in each class to 625.\nIt is common practice to downsample WeeBit.\nProperties WeeBit OneStopEng Cambridge\nTarget AudienceGeneral L2 L2\nCovered Age 7∼16 Adult A2 ∼C2 (CEFR)\nCurriculum-Based?No No Yes\nClass-Balanced?No Yes No\n# of Classes 5 3 5\n# of Items/Class625 189 60\n# of Tokens/Item217 693 512\nAccessibility Author Public Public\nTable 1: Statistics for datasets.\nOneStopEnglish. OneStopEnglish is an aligned\npassage corpus developed for RA and simplifica-\ntion studies. A passage is paraphrased into three\nreadability classes. OneStopEnglish is designed to\nbe a balanced dataset. No downsampling is needed.\nCambridge. Cambridge (Xia et al., 2016) cate-\ngorizes articles based on Cambridge English Exam\nlevels (KET, PET, FCE, CAE, CPE). These five ex-\nams are targeted at learners at A2–C2 levels of the\nCommon European Framework of Reference (Xia\net al., 2016). We downsample to 60 items/class.\nFor evaluation, we calculate accuracy, weighted\nF1 score, precision, recall, and quadratic weighted\nkappa (QWK). The use of QWK is inspired by\nChen et al. (2016); Palma et al. (2019). We use strat-\nified k-fold (k=5, train=0.8, val=0.1, test=0.1) and\naverage the results for reliability. We use SciKit-\nlearn (Pedregosa et al., 2011) for metrics.\n4.2.2 Search on Neural Model\nExtending from the existing use of BERT on RA\n(Deutsch et al., 2020; Martinc et al., 2021), we ex-\nplore RoBERTa, (Liu et al., 2019), BART (Lewis\net al., 2020), and XLNet (Yang et al., 2019). We\nuse base models for all (details in appendix D).\nFor each of the four models (table 2), we perform\ngrid searches on WeeBit validation sets to identify\nCorpus BERT RoBERTa BART XLNet\nWeeBit\nAccuracy0.893 0.900 0.889 0.881\nF1 0.893 0.900 0.889 0.880\nPrecision0.896 0.902 0.892 0.881\nRecall 0.896 0.902 0.892 0.881\nQWK 0.966 0.970 0.963 0.966\nOneStopE\nAccuracy0.801 0.965 0.968 0.804\nF1 0.793 0.965 0.968 0.794\nPrecision0.815 0.968 0.970 0.810\nRecall 0.814 0.968 0.970 0.810\nQWK 0.840 0.942 0.952 0.845\nCambridge\nAccuracy0.573 0.680 0.620 0.573\nF1 0.517 0.658 0.598 0.554\nPrecision0.528 0.693 0.643 0.591\nRecall 0.525 0.693 0.643 0.591\nQWK 0.809 0.881 0.835 0.832\nTable 2: Best performances, neural models.\nthe well-performing hyperparameters based on 5-\nfold mean accuracy. Once identified, we used the\nsame configuration for all the other corpora and\nperformed no corpus-specific tweaking. We search\nthe learning rates of [1e-5, 2e-5, 4e-5, 1e-4] and\nthe batch sizes of [8, 16, 32]. The input sequence\nlengths are all set at 512, and we used Adam op-\ntimizer. Last, we fine-tuned the model for three\nepochs. Full hyperparameters are in appendix F.\nIn table 2, RoBERTa & BART beat BERT & XL-\nNet on most metrics. Martinc et al. (2021) reports\nthat transformers are weak on parallel datasets (On-\neStopEnglish) due to their reliance on semantic\ninformation. However, RoBERTa & BART show\ngreat performances on OneStopEnglish as well.\nSuch a phenomenon likely derives from numer-\nous aspects of the architecture. We carefully posit\nthat the varying pretraining steps could be a reason.\nBERT uses two objectives, masked language\nmodel (MLM) and next sentence prediction (NSP).\nThe latter was included to capture the relation be-\ntween sentences for natural language inference.\nThus, sentence/segment-level input is used. Like-\nwise, XLNet adopts a similar idea, limiting input to\nsentence/segment-level. But RoBERTa disproved\nthe efficiency of NSP, adopting document-level in-\nputs. Similarly, BART, via random shuffling of\nsentences and in-filling scheme, does not limit it-\nself to a sentence/segment size input. As in section\n3, “readability” is possibly a global-level represen-\ntation (accumulated across the whole document).\nThus, the performance differences could stem from\nthe pretraining input size; sentence/segment-level\ninput likely loses the global-level information.\nCorpus SVM RandomF XGBoost LogR\nWeeBit\nAccuracy0.679 0.638 0.638 0.622\nF1 0.672 0.626 0.627 0.615\nPrecision0.696 0.645 0.656 0.676\nRecall 0.679 0.638 0.638 0.622\nQWK 0.716 0.703 0.692 0.640\nOneStopE\nAccuracy0.737 0.709 0.719 0.778\nF1 0.730 0.706 0.701 0.770\nPrecision0.751 0.726 0.734 0.778\nRecall 0.737 0.709 0.719 0.778\nQWK 0.400 0.434 0.363 0.486\nCambridge\nAccuracy0.627 0.673 0.685 0.680\nF1 0.613 0.663 0.681 0.657\nPrecision0.660 0.696 0.701 0.694\nRecall 0.627 0.673 0.674 0.680\nQWK 0.857 0.880 0.852 0.855\nTable 3: Best performances, non-neural models.\nSubgr Model\nLogR SVM\nEnDF0.442 0.374\nShaF 0.404 0.409\nTrSF 0.396 0.360\nPOSF0.394 0.513\nWorF0.391 0.387\nPsyF0.378 0.437\nWoKF0.367 0.369\n(a) WeeBit\nSubgr Model\nLogR SVM\nTraF 0.513 0.620\nPsyF0.437 0.696\nPhrF 0.429 0.608\nVarF0.409 0.626\nTrSF 0.391 0.614\nWorF0.387 0.637\nOSKF0.359 0.605\n(b) OneStopEnglish\nSubgr Model\nLogR SVM\nTraF 0.640 0.593\nWorF0.613 0.593\nShaF 0.600 0.587\nVarF0.600 0.533\nPsyF0.593 0.620\nPOSF0.553 0.407\nWoKF0.540 0.433\n(c) Cambridge\nTable 4: Top 7 Feature Subgroups.\n4.2.3 Search on Non-Neural Model\nWe explored SVM, Random Forest (RandomF),\nGradient Boosting (XGBoost) (Chen and Guestrin,\n2016), and Logistic Regression (LogR). With the\nexception of XGBoost, the chosen models are fre-\nquently used in RA but rarely go through adequate\nhyperparameter optimization steps (Ma et al., 2012;\nYaneva et al., 2017; Mohammadi and Khasteh,\n2020). We perform a randomized search to first\nidentify the sensible range of hyperparameters to\nsearch. Then, we apply grid search to specify the\noptimal values. The parameters are in appendix F.\nIn table 3, we report the performances of the\nparameter-optimized models trained with all 255\nhandcrafted features. Compared to transformers,\nthese non-neural models show lower accuracy in\ngeneral. Even on the smallest Cambridge dataset,\nnon-neural models do not necessarily show higher\nperformances than transformers. But it is important\nto note that they managed to show fairly good, “ex-\npectable” performances on a much smaller dataset.\n4.2.4 Search on Handcrafted Features\nWe start by ranking performances of the feature\nsubgroups. In table 4, we report the top 7 (upper\nhalf) by accuracy on WeeBit. The result is obtained\nCorpus\nModel\nBaselines, Previous Studies BERT RoBERTa BART XLNet\nXia-16 Fili-19 Mar-21hybrid∆ ∆ hybrid∆ ∆ hybrid∆ ∆ hybrid∆ ∆\nSVM LSTM BERT HANGB-T1 BERT GBRF-T1 RBRT RFRF-T1 BART RFRF-P3 XLNet RF\nWeeBit\nAccuracy0.803 0.8130.8570.7520.895 0.002 0.2570.902 0.002 0.2640.9050.016 0.2670.892 0.011 0.254\nF1 - - 0.8660.7530.895 0.002 0.2680.902 0.002 0.2760.9050.016 0.2790.892 0.012 0.266\nPrecision - - 0.8570.7520.897 0.001 0.2410.903 0.001 0.2580.9050.013 0.2600.893 0.012 0.248\nRecall - - 0.8580.7520.897 0.001 0.2590.903 0.001 0.2650.9040.012 0.2660.892 0.011 0.254\nQWK - - 0.9530.8860.969 0.001 0.2770.971 0.001 0.2680.9680.005 0.2650.966 0.000 0.263\nOneStopE\nAccuracy - - 0.674 0.7870.982 0.181 0.2630.9900.025 0.2810.971 0.003 0.2620.848 0.044 0.139\nF1 - - 0.740 0.7980.982 0.189 0.2810.9950.030 0.2890.971 0.003 0.2650.848 0.050 0.142\nPrecision - - 0.674 0.7870.983 0.168 0.2490.9950.027 0.2690.972 0.002 0.2460.852 0.042 0.126\nRecall - - 0.677 0.7890.982 0.168 0.2630.9960.028 0.2870.971 0.001 0.2620.848 0.038 0.139\nQWK - - 0.708 0.8250.973 0.133 0.6100.9960.054 0.5620.952 0.000 0.5180.855 0.010 0.369\nCambridge\nAccuracy0.786∗∗ - - - 0.687 0.114 0.0020.7630.083 0.0900.727 0.107 0.0540.687 0.114 0.014\nF1 - - - - 0.682 0.165 0.0010.7520.094 0.0890.727 0.129 0.0640.676 0.122 0.013\nPrecision - - - - 0.732 0.204 0.0310.7920.099 0.0960.760 0.117 0.0640.710 0.119 0.014\nRecall - - - - 0.687 0.162 0.0130.7530.060 0.0800.727 0.084 0.0540.687 0.096 0.014\nQWK - - - - 0.873 0.064 0.0210.9190.038 0.0390.889 0.054 0.0090.888 0.056 0.008\n** Xia-16 (Cambridge) uses semi-supervised learning (self-training) on a larger corpus to increase performance.\nTable 5: Best performances, hybrid models.\nSetFeatures LogR\nT1 AdSem+Disco+Synta+LxSem+ShaTr 0.622\nP3 ShaTr+EnDF+TrSF+POSF+WorF+PsyF+TraF+VarF0.647\n* Note: 5 letters (e.g. AdSem) mean linguistic branch. 4 letters\n(e.g. PhrF) mean subgroup. We report accuracy on WeeBit.\nTable 6: Best feature sets.\nafter training the respective model using the spec-\nified feature subgroup. Importantly, the advanced\nsemantic features show good performance in all\nmeasures. WorF and PsyF, features calculated from\nexternal databases, rank in the top 7 for all corpora,\nhinting they are strong measures of readability.\nMoving on, we constructed several types of fea-\nture combinations with varying aims. These incl-\nude: 1. T-type to thoroughly capture linguistic\nproperties and 2. P-type to collect features by per-\nformance. We tested the variations on LogR and\nSVM to determine the optimal. Two sets (table 6)\nperformed well. Appendix G reports all tested vari-\nations. We highlight that both advanced semantics\nand discourse added distinct (orthogonal) informa-\ntion, which was evidenced by performance change.\n4.3 Assembling Hybrid Model\nBased on the exploration so far, we assemble our\nhybrid model. We perform a brute-force grid search\non four neural models (table 2), four non-neural\nmodels (table 3), and 14 feature sets (table 24).\nTo interweave the model, we followed the steps\nof 1: obtain soft labels (probabilities that a text\nbelongs to the respective readability class) from a\nneural model by softmax layer, 2: append the soft\nlabels to handcrafted features (create a dataframe),\n3. train non-neural model on the dataframe. As\nin fig 2, the neural models performed a sort of re-\nprediction to the data used for training to match the\ndataframe dimensions in training and test stages.\nTable 5 reports the best performing combination\nper respective neural model. Under “hybrid” col-\numn are code names (e.g. GB-T1 under BERT =\nXGBoost trained with handcrafted feature set T1\nand BERT outputs). Under “∆” column, we report\ndifferences with the respective single model per-\nformance. We also include SOTA baseline results\nXia-16 → Xia et al. (2016), Fili-19 → Filighera\net al. (2019), Mar-21 → Martinc et al. (2021).\n4.4 Hybrid Model Results\nIn table 5, our hybrid models achieve SOTA per-\nformances on WeeBit (BART-RF-T1) and OneSt-\nopEnglish (RoBERTa-RF-T1). With the exception\nof Xia et al. (2016) which uses extra data to in-\ncrease accuracy, we also achieve SOTA on Cam-\nbridge: 76.3% accuracy on a small dataset of only\n60 items/class. Among the hybrids, RoBERTa-RF-\nT1 showed consistently high performance on all\nmetrics. But all hybrid models beat previous SOTA\nresults by a large margin. Notably, we achieve the\nnear-perfect accuracy of 99% on OneStopEnglish,\na massive 20.3% increase from the previous SOTA\n(Martinc et al., 2021) by HAN (Meng et al., 2020).\nBoth neural and non-neural models benefit from\nthe hybrid architecture. This is explicitly shown\nin BERT-GB-T1 performance on OneStopEnglish,\nachieving 98.2% accuracy. This is an 18.1% in-\ncrease from BERT and a 26.3% increase from\nXGBoost. However, BART did not benefit much\nfrom the hybrid architecture on WeeBit and On-\neStopEnglish, meaning that hybrid architectures do\nnot augment model performance at all conditions.\nAlong similar lines, the hybrid architecture per-\nformance on the larger WeeBit dataset showed only\na small improvement from the transformer-only\nresult. On the other hand, the hybrid architecture\nperformance on the smaller Cambridge dataset was\nconsistently better than the transformer-only per-\nformance. The hybrid shows ∼10% improvement\nin accuracy on average for Cambridge. On the\nsmallest dataset (Cambridge), the hybrid architec-\nture benefited more from a non-neural, handcrafted\nfeatures-based model like RF (Random Forest) and\nGB (XGBoost). On the largest dataset (WeeBit),\nthe hybrid benefited more from a transformer.\nOur explanation is that the handcrafted features\ndo not add much, at the data size of WeeBit. But the\nhandcrafted features could be a great help where\ndata is insufficient like they did for the Cambridge\ndataset. OneStopEnglish, being the medium-sized\nparallel dataset, could have hit the sweet spot for\nthe hybrid architecture. But it must be noted that\nthe data size is not the only determining factor as\nto which model (neural or non-neural) the hybrid\narchitecture benefits more from. It must also be\nquestioned if the max performance (∵ label noise\ninduced by subjectivity) (Frénay et al., 2014) is\nalready achieved on WeeBit (Deutsch et al., 2020).\nAlso, it seems that the hybrid architecture ben-\nefits when each model (neural and non-neural) al-\nready shows considerably good performance. This\nis plausible as the neural model outputs are consid-\nered features for the non-neural model. Including\nmore “fairly” well-performing features only cre-\nates extra distractions. The hybrid architecture’s\nlimit is that it gets a model from “good” to “great,”\nnot “fair” to “good.” But determining the definition\nof “fair” performance is a difficult feat as it likely\ndepends on the dataset and a researcher’s intuition\nfrom the empirical experience of the model. Hence,\nthe hybrid architecture’s limit is that one must test\nseveral combinations to pick the effective one.\n4.5 Why Not Directly Append Features?\nRegarding the model architecture, we examined\nappending the handcrafted features to transformer\nembeddings without the use of a secondary predic-\nFigure 3: Performance Change, WeeBit Data Size\ntor like SVM. But an existing example of Read-\nNet (Meng et al., 2020) hints that such a model\nis not robust to small datasets. ReadNet reports\n52.8% accuracy on Cambridge, worse than any of\nour tested models (table 2, 3, 5). Besides, Read-\nNet claims to have achieved 91.7% accuracy on\nWeeBit, without reports on downsampling. Many\nstudies, like Deutsch et al. (2020), report that the\nmodel accuracy can increase∼4% on the full, class-\nimbalanced WeeBit. Hence, ReadNet is not directly\ncomparable. We omitted ReadNet from table 5.\n4.6 Why Was Our BERT Better?\nNoticeable in table 2 and table 5 is that our BERT\nimplementation performed much better on WeeBit\nthan what was reported. The dataset preparation\nmethods and overall evaluation settings are the\nsame or very similar across ours (accuracy: 89.3%),\nDeutsch et al. (2020)’s (accuracy: 83.9%), and Mar-\ntinc et al. (2021)’s (accuracy: 85.7%). We believe\nthat the differences stem from the hyperparameters.\nNotably, Deutsch et al. (2020) uses 128 input se-\nquence length. This is ineffective as the downsam-\npled WeeBit has 2374 articles of over 128 tokens\nbut only 275 articles of over 512 tokens (which was\nour input sequence length). Hence, we can reason-\nably think that much semantic information was lost\nin Deutsch et al. (2020)’s implementation. Martinc\net al. (2021) uses 512 input sequence length but\nlacks a report on other possibly critical hyperpa-\nrameters, and we cannot compare in detail.\n4.7 Data Size Effect\nIn table 5, our hybrid architecture generally did\nnot contribute much to the classification on WeeBit.\nBut we argue that it has much to do with data size.\nTo model how data size affects the accuracies of\n1. hybrid model, 2. transformer, and 3. traditional\nML, we conducted an additional experiment using\nthe same test data (10% of WeeBit) explained in\nsection 4.2.1. However, we random sampled the\ntrain data (80% of WeeBit) into the smaller sizes of\nfrom 50 to 750, with 50 passages increase each set.\nWe sampled with equal class weights, meaning that\na 250 passages train set has 50 from each readabil-\nity class. We trained BERT-GB-T1 (table 5) on the\nsampled data and evaluated on the same test data\nthroughout. We also recorded BERT and XGBoost\n(with T1 features) performances in fig. 3.\nIn fig. 3, the hybrid model performs consistently\nbetter than transformer (+0.01 ∼ 0.05) at all sizes.\nBut the difference gap gets smaller as the train data\nsize increases. The hybrid model does help the\nefficiency of learning RA linguistic properties.\nContrary to the conventional beliefs, the trans-\nformer (BERT) performed better than our expec-\ntations, even on smaller data sizes. BERT always\noutperformed XGBoost. The traditional ML per-\nformance was arguably more consistent but never\nbetter than a transfomer’s.\nBERT-GB-T1’s trend line seemed like the mix-\nture of GB-T1’s and BERT’s. Notably, BERT-GB-\nT1 achieves 85.8% accuracy on WeeBit using only\n750 passages, 30% of the original train data. For\ncomparison, 85.7% was the past SOTA (table 5).\n5 Cross Domain Evaluation\n99% accuracy on OneStopEnglish (table 5) shows\nthat our model is capable of almost perfectly captur-\ning the linguistic properties relating to readability\non certain datasets. This is a positive and abnor-\nmally quick improvement, considering that the re-\nported RA accuracies have never exceeded 90% on\npopular datasets (Vajjala and Meurers, 2012; Xu\net al., 2015; Xia et al., 2016; Vajjala and Lu ˇci´c,\n2018) until 2021. Since the reported in-domain\naccuracies in RA had much room for improvement,\nwe were not at the stage to be seriously concerned\nabout cross-domain evaluation (Štajner and Nisioi,\n2018) in this paper.\nIt would be very favorable to run an extra cross-\ndomain evaluation (which we believe to be a next-\nlevel topic). But realistically, performing a cross-\ndomain evaluation requires a thorough study on at\nleast two datasets, which is potentially out of scope\nin this research. The readability classes/levels are\nlabeled by a few human experts, making the stan-\ndards vary among datasets. To make two datasets\nsuitable for cross-domain evaluation, much effort\nis needed to connect the two, such as the class map-\nping used in Xia et al. (2016). However, it should\nbe noted for future researchers that the notion of do-\nmain overfitting is indeed a common problem faced\nin RA, which often uses one dataset for train/test/-\nvalidation. Without a new methodology to connect\nseveral datasets or a new large public dataset for\nRA, it will forever be challenging to develop a RA\nmodel for general use (Vajjala, 2021).\n6 Conclusion\nWe have reported the four contributions mentioned\nin section 1. We checked that the new advanced\nsemantic features add orthogonal information to the\nmodel. Further, we created hybrid models (table\n5) that achieved SOTA results. RoBERTA-RF-T1\nachieved 99% accuracy on OneStopEnglish, and\nBERT-GB-T1 beat the previous SOTA on WeeBit\nusing only 30% of the original train data.\n7 Acknowledgements\nWe wish to thank Dr. Inhwan Lee, Dongjun Lee,\nSangjo Park, Donghyun Lee, and Eunsoo Shin.\nPartly funded by the 4th Industrial Revolution R&D\nProgram, Min. of SMEs & Startups, Rep. of Korea.\nReferences\nSandra Aluisio, Lucia Specia, Caroline Gasperin, and\nCarolina Scarton. 2010. Readability assessment for\ntext simplification. In Proceedings of the NAACL\nHLT 2010 Fifth Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 1–9.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. the Journal of\nmachine Learning research, 3:993–1022.\nOV Blinova, Tarasov NA, Modina VV , and IS Blekanov.\n2020. Modeling lemma frequency bands for lexical\ncomplexity assessment of russian texts1. In Com-\nputational Linguistics and Intellectual Technologies:\nProceedings of the International Conference “Di-\nalogue 2020”, Moscow, June 17–20, 2020 , pages\n76–92.\nLeo Breiman. 2001. Random forests. Machine learning,\n45(1):5–32.\nMarc Brysbaert and Boris New. 2009. Moving beyond\nkuˇcera and francis: A critical evaluation of current\nword frequency norms and the introduction of a new\nand improved word frequency measure for american\nenglish. Behavior research methods, 41(4):977–990.\nLori Buchanan, Chris Westbury, and Curt Burgess. 2001.\nCharacterizing semantic space: Neighborhood effects\nin word recognition. Psychonomic Bulletin & Review,\n8(3):531–544.\nChih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:\na library for support vector machines. ACM transac-\ntions on intelligent systems and technology (TIST) ,\n2(3):1–27.\nJing Chen, James H Fife, Isaac I Bejar, and André A\nRupp. 2016. Building e-rater® scoring models using\nmachine learning methods. ETS Research Report\nSeries, 2016(1):1–12.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794.\nAlina Maria Ciobanu, Liviu P Dinu, and Flaviu Pepelea.\n2015. Readability assessment of translated texts. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing , pages\n97–103.\nKevyn Collins-Thompson. 2014. Computational as-\nsessment of text readability: A survey of current and\nfuture research. ITL-International Journal of Applied\nLinguistics, 165(2):97–135.\nKevyn Collins-Thompson and James P Callan. 2004. A\nlanguage modeling approach to predicting reading\ndifficulty. In Proceedings of the human language\ntechnology conference of the North American chapter\nof the association for computational linguistics: HLT-\nNAACL 2004, pages 193–200.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nJournal of Machine Learning Research, 12(76):2493–\n2537.\nScott A Crossley, Jerry Greenfield, and Danielle S Mc-\nNamara. 2008. Assessing text readability using cog-\nnitively based indices. Tesol Quarterly, 42(3):475–\n493.\nPatcharanut Daowadung and Yaw-Huei Chen. 2011. Us-\ning word segmentation and svm to assess readability\nof thai text for primary school students. In 2011\nEighth International Joint Conference on Computer\nScience and Software Engineering (JCSSE) , pages\n170–174. IEEE.\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien.\n2014. Saga: A fast incremental gradient method with\nsupport for non-strongly convex composite objec-\ntives. arXiv preprint arXiv:1407.0202.\nTovly Deutsch, Masoud Jasbi, and Stuart Shieber. 2020.\nLinguistic features for readability assessment. In\nProceedings of the Fifteenth Workshop on Innovative\nUse of NLP for Building Educational Applications,\npages 1–17, Seattle, W A, USA Online. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAdam EM Eltorai, Syed S Naqvi, Soha Ghanian,\nCraig P Eberson, Arnold-Peter C Weiss, Christo-\npher T Born, and Alan H Daniels. 2015. Readability\nof invasive procedure consent forms. Clinical and\ntranslational science, 8(6):830–833.\nVyvyan Evans. 2006. Cognitive linguistics. Edinburgh\nUniversity Press.\nJohan Falkenjack, Katarina Heimann Mühlenbock, and\nArne Jönsson. 2013. Features indicating readability\nin swedish text. In Proceedings of the 19th Nordic\nConference of Computational Linguistics (NODAL-\nIDA 2013), pages 27–40.\nRong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-\nRui Wang, and Chih-Jen Lin. 2008. Liblinear: A\nlibrary for large linear classification. the Journal of\nmachine Learning research, 9:1871–1874.\nLijun Feng, Noémie Elhadad, and Matt Huenerfauth.\n2009. Cognitively motivated features for readability\nassessment. In Proceedings of the 12th Conference\nof the European Chapter of the ACL (EACL 2009),\npages 229–237.\nLijun Feng, Martin Jansche, Matt Huenerfauth, and\nNoémie Elhadad. 2010. A comparison of features for\nautomatic readability assessment.\nAnna Filighera, Tim Steuer, and Christoph Rensing.\n2019. Automatic text difficulty estimation using em-\nbeddings and neural networks. In European Con-\nference on Technology Enhanced Learning , pages\n335–348. Springer.\nPaul R Fitzsimmons, BD Michael, Joane L Hulley, and\nG Orville Scott. 2010. A readability assessment of\nonline parkinson’s disease information. The jour-\nnal of the Royal College of Physicians of Edinburgh,\n40(4):292–296.\nThomas François. 2014. An analysis of a french as a\nforeign language corpus for readability assessment.\nIn Proceedings of the third workshop on NLP for\ncomputer-assisted language learning, pages 13–32.\nBenoît Frénay, Ata Kabán, et al. 2014. A comprehen-\nsive introduction to label noise. In ESANN. Citeseer.\nEdward Gibson. 1998. Linguistic complexity: Locality\nof syntactic dependencies. Cognition, 68(1):1–76.\nCamille Guinaudeau and Michael Strube. 2013. Graph-\nbased local coherence modeling. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 93–103.\nJohn Hale. 2016. Information-theoretical complex-\nity metrics. Language and Linguistics Compass ,\n10(9):397–412.\nHieronymus Hansen, Adam Widera, Johannes Ponge,\nand Bernd Hellingrath. 2021. Machine learning for\nreadability assessment and text simplification in crisis\ncommunication: A systematic review. In Proceed-\nings of the 54th Hawaii International Conference on\nSystem Sciences, page 2265.\nMarti A. Hearst. 1998. Support vector machines. IEEE\nIntelligent Systems, 13(4):18–28.\nMichael Heilman, Kevyn Collins-Thompson, Jamie\nCallan, and Maxine Eskenazi. 2007. Combining lexi-\ncal and grammatical features to improve readability\nmeasures for first and second language texts. In Hu-\nman Language Technologies 2007: The Conference\nof the North American Chapter of the Association for\nComputational Linguistics; Proceedings of the Main\nConference, pages 460–467, Rochester, New York.\nAssociation for Computational Linguistics.\nMatthew Hoffman, Francis R Bach, and David M Blei.\n2010. Online learning for latent dirichlet allocation.\nIn advances in neural information processing sys-\ntems, pages 856–864. Citeseer.\nThomas Holtgraves. 1999. Comprehending indirect\nreplies: When and how are their conveyed mean-\nings activated? Journal of Memory and Language,\n41(4):519–540.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nJ. Kincaid, R. P. Fishburne, R. L. Rogers, and B. S.\nChissom. 1975. Derivation of new readability formu-\nlas (automated readability index, fog count and flesch\nreading ease formula) for navy enlisted personnel.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nStephen Kokoska and Daniel Zwillinger. 2000. CRC\nstandard probability and statistics tables and formu-\nlae. Crc Press.\nVictor Kuperman, Hans Stadthagen-Gonzalez, and\nMarc Brysbaert. 2012. Age-of-acquisition ratings\nfor 30,000 english words. Behavior research meth-\nods, 44(4):978–990.\nThomas K Landauer, Kirill Kireyev, and Charles Panac-\ncione. 2011. Word maturity: A new metric for word\nknowledge. Scientific Studies of Reading, 15(1):92–\n108.\nBruce W. Lee and Jason Lee. 2020a. LXPER index\n2.0: Improving text readability assessment model\nfor L2 English students in Korea. In Proceedings of\nthe 6th Workshop on Natural Language Processing\nTechniques for Educational Applications, pages 20–\n24, Suzhou, China. Association for Computational\nLinguistics.\nBruce W Lee and Jason Hyung-Jong Lee. 2020b. Lxper\nindex: a curriculum-specific text readability assess-\nment model for efl students in korea. arXiv preprint\narXiv:2008.01564.\nGondy Leroy and David Kauchak. 2013. The effect of\nword familiarity on actual and perceived text diffi-\nculty. Journal of the American Medical Informatics\nAssociation, 21(e1):e169–e172.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPing Li, Curt Burgess, and Kevin Lund. 2000. The\nacquisition of word meaning through global lexical\nco-occurrences. In Proceedings of the thirtieth an-\nnual child language research forum, pages 166–178.\nCiteseer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nEdward Loper and Steven Bird. 2002. Nltk: The natural\nlanguage toolkit. arXiv preprint cs/0205028.\nXiaofei Lu. 2010. Automatic analysis of syntactic com-\nplexity in second language writing. International\njournal of corpus linguistics, 15(4):474–496.\nXiaofei Lu. 2011. A corpus-based evaluation of syntac-\ntic complexity measures as indices of college-level\nesl writers’ language development. TESOL quarterly,\n45(1):36–62.\nYi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.\nRanking-based readability assessment for early pri-\nmary children’s literature. In Proceedings of the 2012\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 548–552.\nDavid Malvern and Brian Richards. 2012. Measures of\nlexical richness. The encyclopedia of applied linguis-\ntics.\nMatej Martinc, Senja Pollak, and Marko Robnik-\nŠikonja. 2021. Supervised and Unsupervised Neural\nApproaches to Text Readability. Computational Lin-\nguistics, pages 1–39.\nG Harry Mc Laughlin. 1969. Smog grading-a new read-\nability formula. Journal of reading, 12(8):639–646.\nPhilip M McCarthy and Scott Jarvis. 2010. Mtld, vocd-\nd, and hd-d: A validation study of sophisticated ap-\nproaches to lexical diversity assessment. Behavior\nresearch methods, 42(2):381–392.\nDanielle S McNamara, Max M Louwerse, Philip M\nMcCarthy, and Arthur C Graesser. 2010. Coh-metrix:\nCapturing linguistic features of cohesion. Discourse\nProcesses, 47(4):292–330.\nChangping Meng, Muhao Chen, Jie Mao, and Jennifer\nNeville. 2020. Readnet: A hierarchical transformer\nframework for web article readability analysis. In Ad-\nvances in Information Retrieval, pages 33–49, Cham.\nSpringer International Publishing.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nHamid Mohammadi and Seyed Hossein Khasteh. 2020.\nA machine learning approach to persian text readabil-\nity assessment using a crowdsourced dataset. In 2020\n28th Iranian Conference on Electrical Engineering\n(ICEE), pages 1–7. IEEE.\nSungJin Nam, Gwen Frishkoff, and Kevyn Collins-\nThompson. 2017. Predicting short-and long-term\nvocabulary learning via semantic features of partial\nword knowledge. International Educational Data\nMining Society.\nDiego Palma and John Atkinson. 2018. Coherence-\nbased automatic essay assessment. IEEE Intelligent\nSystems, 33(5):26–36.\nDiego Palma, Christian Soto, Mónica Veliz, Bernardo\nRiffo, and Antonio Gutiérrez. 2019. A data-driven\nmethodology to assess text complexity based on syn-\ntactic and semantic measurements. In International\nConference on Human Interaction and Emerging\nTechnologies, pages 509–515. Springer.\nMary Anne Peabody and Charles E Schaefer. 2016. To-\nwards semantic clarity in play therapy. International\nJournal of Play Therapy, 25(4):197.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. 2011. Scikit-learn: Machine\nlearning in python. the Journal of machine Learning\nresearch, 12:2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nS. E. Petersen and Mari Ostendorf. 2009. A machine\nlearning approach to reading level assessment. Com-\nput. Speech Lang., 23:89–106.\nPenny M Pexman, Ian S Hargreaves, Paul D Siakaluk,\nGlen E Bodner, and Jamie Pope. 2008. There are\nmany ways to be rich: Effects of three measures of\nsemantic richness on visual word recognition. Psy-\nchonomic Bulletin & Review, 15(1):161–167.\nCarla Pires, Afonso Cavaco, and Marina Vigário. 2017.\nTowards the definition of linguistic metrics for eval-\nuating text readability. Journal of Quantitative Lin-\nguistics, 24(4):319–349.\nEmily Pitler and Ani Nenkova. 2008. Revisiting read-\nability: A unified framework for predicting text qual-\nity. In Proceedings of the 2008 Conference on Empir-\nical Methods in Natural Language Processing, pages\n186–195, Honolulu, Hawaii. Association for Compu-\ntational Linguistics.\nJohn C. Platt. 1999. Probabilistic outputs for support\nvector machines and comparisons to regularized like-\nlihood methods. In ADVANCES IN LARGE MARGIN\nCLASSIFIERS, pages 61–74. MIT Press.\nRani Qumsiyeh and Yiu-Kai Ng. 2011. Readaid: a\nrobust and fully-automated readability assessment\ntool. In 2011 IEEE 23rd International Conference\non Tools with Artificial Intelligence, pages 539–546.\nIEEE.\nRadim ˇReh˚ uˇrek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45–50, Val-\nletta, Malta. ELRA.\nMark Schmidt, Nicolas Le Roux, and Francis Bach.\n2017. Minimizing finite sums with the stochastic av-\nerage gradient. Mathematical Programming, 162(1-\n2):83–112.\nSarah E. Schwarm and Mari Ostendorf. 2005. Reading\nlevel assessment using support vector machines and\nstatistical language models. In Proceedings of the\n43rd Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’05, page 523–530, USA.\nAssociation for Computational Linguistics.\nLuo Si and Jamie Callan. 2001. A statistical model\nfor scientific readability. In Proceedings of the tenth\ninternational conference on Information and knowl-\nedge management, pages 574–576.\nSanja Štajner and Sergiu Nisioi. 2018. A detailed eval-\nuation of neural sequence-to-sequence models for\nin-domain and cross-domain text simplification. In\nProceedings of the eleventh international conference\non language resources and evaluation (LREC 2018).\nKumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-\nada. 2010. Sorting texts by readability. Computa-\ntional Linguistics, 36(2):203–227.\nSara Tonelli, Ke M Tran, and Emanuele Pianta. 2012.\nMaking readability indices readable. In Proceedings\nof the First Workshop on Predicting and Improving\nText Readability for target reader populations, pages\n40–48.\nSowmya Vajjala. 2021. Trends, limitations and open\nchallenges in automatic readability assessment re-\nsearch. arXiv preprint arXiv:2105.00973.\nSowmya Vajjala and Ivana Luˇci´c. 2018. Onestopenglish\ncorpus: A new corpus for automatic readability as-\nsessment and text simplification. In Proceedings of\nthe thirteenth workshop on innovative use of NLP for\nbuilding educational applications, pages 297–304.\nSowmya Vajjala and Detmar Meurers. 2012. On improv-\ning the accuracy of readability classification using\ninsights from second language acquisition. In Pro-\nceedings of the Seventh Workshop on Building Ed-\nucational Applications Using NLP, pages 163–173,\nMontréal, Canada. Association for Computational\nLinguistics.\nSowmya Vajjala and Detmar Meurers. 2013. On the\napplicability of readability models to web texts. In\nProceedings of the Second Workshop on Predicting\nand Improving Text Readability for Target Reader\nPopulations, pages 59–68.\nSowmya Vajjala and Detmar Meurers. 2016.\nReadability-based sentence ranking for evaluating\ntext simplification. arXiv preprint arXiv:1603.06009.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMenglin Xia, Ekaterina Kochmar, and Ted Briscoe.\n2016. Text readability assessment for second lan-\nguage learners. In Proceedings of the 11th Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 12–22, San Diego, CA. Associa-\ntion for Computational Linguistics.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015. Problems in current text simplification re-\nsearch: New data can help. Transactions of the Asso-\nciation for Computational Linguistics, 3:283–297.\nVictoria Yaneva, Constantin Or˘asan, Richard Evans, and\nOmid Rohanian. 2017. Combining multiple corpora\nfor readability assessment for people with cognitive\ndisabilities. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in Neural Informa-\ntion Processing Systems, 32:5753–5763.\nShuyu Zhang, Xuanyu Zhou, Huifeng Pan, and Junyi\nJia. 2019. Cryptocurrency, confirmatory bias and\nnews readability–evidence from the largest chinese\ncryptocurrency exchange. Accounting & Finance ,\n58(5):1445–1468.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nYu Zhang, Houquan Zhou, and Zhenghua Li. 2020. Fast\nand accurate neural CRF constituency parsing. In\nProceedings of IJCAI, pages 4046–4053.\nCiyou Zhu, Richard Byrd, Jorge Nocedal, and Jose Luis\nMorales. 2011. L-bfgs-b. Retrieved Feb, 18:2012.\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2009. A multi-dimensional model for assessing the\nquality of answers in social Q&A sites. Ph.D. thesis.\nA Trend, Advanced Semantic Features\nSorted Probability ListR. out C. out N. out\n9, 0.5, 0.5 Low 115High 56.7H-M 30.0\n6, 2, 1, 0.5, 0.3, 0.2 L-M 177H-M 43.3High 48.1\n4, 4, 1, 1 Mid 190 L-M 15.0L-M 18.5\n4, 2, 1, 1, 0.6, 0.4 H-M 204Mid 25.0 Mid 35.3\n2.5, 1.5, 1.5, 1.5, 1.5, 1.5High 325Low 8.34Low 13.3\nTable 7: Trends. Richness, Clarity, Noise. All numbers\n×10 for conciseness. L-M: Low-Mid. H-M: High-Mid.\nIn table 7, we name each list as 1 ∼ 5 from top to\nbottom. “out” refers to raw output from equations.\nSee what the sorted probabilities list is in fig. 1.\nSemantic Richness. List 4 and list 5 have the\nsame lengths. However, list 5 contains more mean-\ningful topics ( ↑ p) throughout the list, resulting\nin higher overall semantic richness. As such, se-\nmantic richness rewards long probability lists (↑ n)\nwith more meaningful (↑ p) topics. Similarly, list 3\n(↓ n,↑ p) has higher richness than list 2 (↑ n,↓ p).\nSemantic Clarity. List 3 and list 4 have the\nsame max(p) and two other same elements ( 1).\nHowever, the second element in list 3 is the same\nas the first element, resulting in increased difficulty\nin identifying the main topic (max(p)). Likewise,\nsemantic clarity rewards the deviation between the\nmax(p) and the other elements & short probability\nlists (↓ n). Hence, list 1 has the highest clarity.\nSemantic Noise. List 2 and list 4 have the same\nlengths of 6 elements. However, list 2 contains\nmore extraneous topics (↓ p), resulting in higher\nsemantic noise. As such, semantic noise rewards\nlonger lists (↑ n) with more extraneous elements\n(↓ p). As a result, list 5 has the least semantic noise.\nB Features, Codes, and Definitions\nidxCode Definition\n1 WRich05_SRichness, 50 topics extracted from Wikipedia Dump\n2 WClar05_SClarity, 50 topics extracted from Wikipedia Dump\n3 WNois05_SNoise, 50 topics extracted from Wikipedia Dump\n4 WTopc05_S# of topics, 50 topics extracted from Wikipedia Dump\n5 WRich10_SRichness, 100 topics extracted from Wikipedia Dump\n6 WClar10_SClarity, 100 topics extracted from Wikipedia Dump\n7 WNois10_SNoise, 100 topics extracted from Wikipedia Dump\n8 WTopc10_S# of topics, 100 topics extracted from Wikipedia Dump\n9 WRich15_SRichness, 150 topics extracted from Wikipedia Dump\n10 WClar15_SClarity, 150 topics extracted from Wikipedia Dump\n11 WNois15_SNoise, 150 topics extracted from Wikipedia Dump\n12 WTopc15_S# of topics, 150 topics extracted from Wikipedia Dump\n13 WRich20_SRichness, 200 topics extracted from Wikipedia Dump\n14 WClar20_SClarity, 200 topics extracted from Wikipedia Dump\n15 WNois20_SNoise, 200 topics extracted from Wikipedia Dump\n16 WTopc20_S# of topics, 200 topics extracted from Wikipedia Dump\nTable 8: Wikipedia Knowledge Features (WoKF).\nidxCode Definition\n17 BRich05_SRichness, 50 topics extracted from WeeBit Corpus\n18 BClar05_SClarity, 50 topics extracted from WeeBit Corpus\n19 BNois05_SNoise, 50 topics extracted from WeeBit Corpus\n20 BTopc05_S# of topics, 50 topics extracted from WeeBit Corpus\n21 BRich10_SRichness, 100 topics extracted from WeeBit Corpus\n22 BClar10_SClarity, 100 topics extracted from WeeBit Corpus\n23 BNois10_SNoise, 100 topics extracted from WeeBit Corpus\n24 BTopc10_S# of topics, 100 topics extracted from WeeBit Corpus\n25 BRich15_SRichness, 150 topics extracted from WeeBit Corpus\n26 BClar15_SClarity, 150 topics extracted from WeeBit Corpus\n27 BNois15_SNoise, 150 topics extracted from WeeBit Corpus\n28 BTopc15_S# of topics, 150 topics extracted from WeeBit Corpus\n29 BRich20_SRichness, 200 topics extracted from WeeBit Corpus\n30 BClar20_SClarity, 200 topics extracted from WeeBit Corpus\n31 BNois20_SNoise, 200 topics extracted from WeeBit Corpus\n32 BTopc20_S# of topics, 200 topics extracted from WeeBit Corpus\nTable 9: WeeBit Knowledge Features (WBKF).\nidxCode Definition\n33 ORich05_SRichness, 50 topics extracted from OneStop Corpus\n34 OClar05_SClarity, 50 topics extracted from OneStop Corpus\n35 ONois05_SNoise, 50 topics extracted from OneStop Corpus\n36 OTopc05_S# of topics, 50 topics extracted from OneStop Corpus\n37 ORich10_SRichness, 100 topics extracted from OneStop Corpus\n38 OClar10_SClarity, 100 topics extracted from OneStop Corpus\n39 ONois10_SNoise, 100 topics extracted from OneStop Corpus\n40 OTopc10_S# of topics, 100 topics extracted from OneStop Corpus\n41 ORich15_SRichness, 150 topics extracted from OneStop Corpus\n42 OClar15_SClarity, 150 topics extracted from OneStop Corpus\n43 ONois15_SNoise, 150 topics extracted from OneStop Corpus\n44 OTopc15_S# of topics, 150 topics extracted from OneStop Corpus\n45 ORich20_SRichness, 200 topics extracted from OneStop Corpus\n46 OClar20_SClarity, 200 topics extracted from OneStop Corpus\n47 ONois20_SNoise, 200 topics extracted from OneStop Corpus\n48 OTopc20_S# of topics, 200 topics extracted from OneStop Corpus\nTable 10: OneStop Knowledge Features (OSKF).\nidxCode Definition\n49 to_EntiM_Ctotal number of Entities Mentions\n50 as_EntiM_Caverage number of Entities Mentions per sentence\n51 at_EntiM_Caverage number of Entities Mentions per token (word)\n52 to_UEnti_Ctotal number of unique Entities\n53 as_UEnti_Caverage number of unique Entities per sentence\n54 at_UEnti_Caverage number of unique Entities per token (word)\nTable 11: Entity Density Features (EnDF).\nidxCode Definition\n55 ra_SSToT_Cratio of SS transitions:total, count from Entity Grid\n56 ra_SOToT_Cratio of SO transitions:total, count from Entity Grid\n57 ra_SXToT_Cratio of SX transitions:total, count from Entity Grid\n58 ra_SNToT_Cratio of SN transitions:total, count from Entity Grid\n59 ra_OSToT_Cratio of OS transitions:total, count from Entity Grid\n60 ra_OOToT_Cratio of OO transitions:total, count from Entity Grid\n61 ra_OXToT_Cratio of OX transitions:total, count from Entity Grid\n62 ra_ONToT_Cratio of ON transitions:total, count from Entity Grid\n63 ra_XSToT_Cratio of XS transitions:total, count from Entity Grid\n64 ra_XOToT_Cratio of XO transitions:total, count from Entity Grid\n65 ra_XXToT_Cratio of XX transitions:total, count from Entity Grid\n66 ra_XNToT_Cratio of XN transitions:total, count from Entity Grid\n67 ra_NSToT_Cratio of NS transitions:total, count from Entity Grid\n68 ra_NOToT_Cratio of NO transitions:total, count from Entity Grid\n69 ra_NXToT_Cratio of NX transitions:total, count from Entity Grid\n70 ra_NNToT_Cratio of NN transitions:total, count from Entity Grid\nTable 12: Entity Grid Features (EnDF) Part 1.\nidxCode Definition\n71 LoCohPA_SLocal Coherence for PA score from Entity Grid\n72 LoCohPW_SLocal Coherence for PW score from Entity Grid\n73 LoCohPU_SLocal Coherence for PU score from Entity Grid\n74 LoCoDPA_SLocal Coherence dist. for PA score from Entity Grid\n75 LoCoDPW_SLocal Coherence dist. for PW score from Entity Grid\n76 LoCoDPU_SLocal Coherence dist. for PU score from Entity Grid\nTable 13: Entity Grid Features (EnDF) Part 2.\nidx Code Definition\n77 to_NoPhr_Ctotal count of Noun phrases\n78 as_NoPhr_Caverage count of Noun phrases per sentence\n79 at_NoPhr_Caverage count of Noun phrases per token\n80 ra_NoVeP_Cratio of Noun phrases : Verb phrases count\n81 ra_NoSuP_Cratio of Noun phrases : Subordinate clauses count\n82 ra_NoPrP_Cratio of Noun phrases : Prep phrases count\n83 ra_NoAjP_Cratio of Noun phrases : Adj phrases count\n84 ra_NoAvP_Cratio of Noun phrases : Adv phrases count\n85 to_VePhr_Ctotal count of Verb phrases\n86 as_VePhr_Caverage count of Verb phrases per sentence\n87 at_VePhr_Caverage count of Verb phrases per token\n88 ra_VeNoP_Cratio of Verb phrases : Noun phrases count\n89 ra_VeSuP_Cratio of Verb phrases : Subordinate clauses count\n90 ra_VePrP_Cratio of Verb phrases : Prep phrases count\n91 ra_VeAjP_Cratio of Verb phrases : Adj phrases count\n92 ra_VeAvP_Cratio of Verb phrases : Adv phrases count\n93 to_SuPhr_Ctotal count of Subordinate clauses\n94 as_SuPhr_Caverage count of Subordinate clauses per sentence\n95 at_SuPhr_Caverage count of Subordinate clauses per token\n96 ra_SuNoP_Cratio of Subordinate clauses : Noun phrases count\n97 ra_SuVeP_Cratio of Subordinate clauses : Verb phrases count\n98 ra_SuPrP_Cratio of Subordinate clauses : Prep phrases count\n99 ra_SuAjP_Cratio of Subordinate clauses : Adj phrases count\n100ra_SuAvP_Cratio of Subordinate clauses : Adv phrases count\n101to_PrPhr_Ctotal count of prepositional phrases\n102as_PrPhr_Caverage count of prepositional phrases per sentence\n103at_PrPhr_Caverage count of prepositional phrases per token\n104ra_PrNoP_Cratio of Prep phrases : Noun phrases count\n105ra_PrVeP_Cratio of Prep phrases : Verb phrases count\n106ra_PrSuP_Cratio of Prep phrases : Subordinate clauses count\n107ra_PrAjP_Cratio of Prep phrases : Adj phrases count\n108ra_PrAvP_Cratio of Prep phrases : Adv phrases count\n109to_AjPhr_Ctotal count of Adjective phrases\n110as_AjPhr_Caverage count of Adjective phrases per sentence\n111at_AjPhr_Caverage count of Adjective phrases per token\n112ra_AjNoP_Cratio of Adj phrases : Noun phrases count\n113ra_AjVeP_Cratio of Adj phrases : Verb phrases count\n114ra_AjSuP_Cratio of Adj phrases : Subordinate clauses count\n115ra_AjPrP_Cratio of Adj phrases : Prep phrases count\n116ra_AjAvP_Cratio of Adj phrases : Adv phrases count\n117to_AvPhr_Ctotal count of Adverb phrases\n118as_AvPhr_Caverage count of Adverb phrases per sentence\n119at_AvPhr_Caverage count of Adverb phrases per token\n120ra_AvNoP_Cratio of Adv phrases : Noun phrases count\n121ra_AvVeP_Cratio of Adv phrases : Verb phrases count\n122ra_AvSuP_Cratio of Adv phrases : Subordinate clauses count\n123ra_AvPrP_Cratio of Adv phrases : Prep phrases count\n124ra_AvAjP_Cratio of Adv phrases : Adj phrases count\nTable 14: Phrasal Features (PhrF)\nidxCode Definition\n125to_TreeH_Ctotal parsed Tree Height of all sentences\n126as_TreeH_Caverage parsed Tree Height per sentence\n127at_TreeH_Caverage parsed Tree Height per token\n128to_FTree_Ctotal length of Flattened parsed Trees\n129as_FTree_Caverage length of Flattened parsed Trees per sentence\n130at_FTree_Caverage length of Flattened parsed Trees per token\nTable 15: Tree Structural Features (TrSF)\nidxCode Definition\n131to_NoTag_Ctotal count of Noun tags\n132as_NoTag_Caverage count of Noun tags per sentence\n133at_NoTag_Caverage count of Noun tags per token\n134ra_NoAjT_Cratio of Noun : Adjective count\n135ra_NoVeT_Cratio of Noun : Verb count\n136ra_NoAvT_Cratio of Noun : Adverb count\n137ra_NoSuT_Cratio of Noun : Subordinating Conj. count\n138ra_NoCoT_Cratio of Noun : Coordinating Conj. count\n139to_VeTag_Ctotal count of Verb tags\n140as_VeTag_Caverage count of Verb tags per sentence\n141at_VeTag_Caverage count of Verb tags per token\n142ra_VeAjT_Cratio of Verb : Adjective count\n143ra_VeNoT_Cratio of Verb : Noun count\n144ra_VeAvT_Cratio of Verb : Adverb count\n145ra_VeSuT_Cratio of Verb : Subordinating Conj. count\n146ra_VeCoT_Cratio of Verb : Coordinating Conj. count\n147to_AjTag_Ctotal count of Adjective tags\n148as_AjTag_Caverage count of Adjective tags per sentence\n149at_AjTag_Caverage count of Adjective tags per token\n150ra_AjNoT_Cratio of Adjective : Noun count\n151ra_AjVeT_Cratio of Adjective : Verb count\n152ra_AjAvT_Cratio of Adjective : Adverb count\n153ra_AjSuT_Cratio of Adjective : Subordinating Conj. count\n154ra_AjCoT_Cratio of Adjective : Coordinating Conj. count\n155to_AvTag_Ctotal count of Adverb tags\n156as_AvTag_Caverage count of Adverb tags per sentence\n157at_AvTag_Caverage count of Adverb tags per token\n158ra_AvAjT_Cratio of Adverb : Adjective count\n159ra_AvNoT_Cratio of Adverb : Noun count\n160ra_AvVeT_Cratio of Adverb : Verb count\n161ra_AvSuT_Cratio of Adverb : Subordinating Conj. count\n162ra_AvCoT_Cratio of Adverb : Coordinating Conj. count\n163to_SuTag_Ctotal count of Subordinating Conj. tags\n164as_SuTag_Caverage count of Subordinating Conj. per sentence\n165at_SuTag_Caverage count of Subordinating Conj. per token\n166ra_SuAjT_Cratio of Subordinating Conj. : Adjective count\n167ra_SuNoT_Cratio of Subordinating Conj. : Noun count\n168ra_SuVeT_Cratio of Subordinating Conj. : Verb count\n169ra_SuAvT_Cratio of Subordinating Conj. : Adverb count\n170ra_SuCoT_Cratio, Subordinating Conj. : Coordinating Conj. count\n171to_CoTag_Ctotal count of Coordinating Conj. tags\n172as_CoTag_Caverage count of Coordinating Conj. per sentence\n173at_CoTag_Caverage count of Coordinating Conj. per token\n174ra_CoAjT_Cratio of Coordinating Conj. : Adjective count\n175ra_CoNoT_Cratio of Coordinating Conj. : Noun count\n176ra_CoVeT_Cratio of Coordinating Conj. : Verb count\n177ra_CoAvT_Cratio of Coordinating Conj. : Adverb count\n178ra_CoSuT_Cratio, Coordinating Conj. : Subordinating Conj. count\n179to_ContW_Ctotal count of Content words\n180as_ContW_Caverage count of Content words per sentence\n181at_ContW_Caverage count of Content words per token\n182to_FuncW_Ctotal count of Function words\n183as_FuncW_Caverage count of Function words per sentence\n184at_FuncW_Caverage count of Function words per token\n185ra_CoFuW_Cratio of Content words to Function words\nTable 16: Part-of-Speech Features (POSF)\nidxCode Definition\n186SimpNoV_Sunique Nouns/total Nouns #Noun Variation\n187SquaNoV_S(unique Nouns**2)/total Nouns #Squared Noun Variation\n188CorrNoV_Sunique Nouns/sqrt(2*total Nouns) #Corrected Noun Var\n189SimpVeV_Sunique Verbs/total Verbs #Verb Variation\n190SquaVeV_S(unique Verbs**2)/total Verbs #Squared Verb Variation\n191CorrVeV_Sunique Verbs/sqrt(2*total Verbs) #Corrected Verb Var\n192SimpAjV_Sunique Adjectives/total Adjectives #Adjective Var\n193SquaAjV_S(unique Adj**2)/total Adj #Squared Adj Variation\n194CorrAjV_Sunique Adj/sqrt(2*total Adj) #Corrected Adj Var\n195SimpAvV_Sunique Adverbs/total Adverbs #Adverb Variation\n196SquaAvV_S(unique Adv**2)/total Adv #Squared Adv Variation\n197CorrAvV_Sunique Adv/sqrt(2*total Adv) #Corrected Adv Var\nTable 17: Variation Ratio Features (VarF)\nidxCode Definition\n198SimpTTR_Sunique tokens/total tokens #TTR\n199CorrTTR_Sunique/sqrt(2*total) #Corrected TTR\n200BiLoTTR_Slog(unique)/log(total) #Bi-Logarithmic TTR\n201UberTTR_S(log(unique))2/log(total/unique) #Uber\n202MTLDTTR_S#Measure of Textual Lexical Diversity (TTR, 0.72)\nTable 18: Type Token Ratio Features (TTRF)\nidxCode Definition\n203to_AAKuW_Ctotal AoA (Age of Acquisition) of words, Kuperman\n204as_AAKuW_Caverage AoA of words per sentence, Kuperman\n205at_AAKuW_Caverage AoA of words per token, Kuperman\n206to_AAKuL_Ctotal AoA of lemmas, Kuperman\n207as_AAKuL_Caverage AoA of lemmas per sentence, Kuperman\n208at_AAKuL_Caverage AoA of lemmas per token, Kuperman\n209to_AABiL_Ctotal AoA of lemmas, Bird norm\n210as_AABiL_Caverage AoA of lemmas, Bird norm per sent\n211at_AABiL_Caverage AoA of lemmas, Bird norm per token\n212to_AABrL_Ctotal AoA of lemmas, Bristol norm\n213as_AABrL_Caverage AoA of lemmas, Bristol norm per sent\n214at_AABrL_Caverage AoA of lemmas, Bristol norm per token\n215to_AACoL_Ctotal AoA of lemmas, Cortese and Khanna norm\n216as_AACoL_Caverage AoA of lem, Cortese and K norm per sent\n217at_AACoL_Caverage AoA of lem, Cortese and K norm per token\nTable 19: Psychollinguistic Features (PsyF)\nidxCode Definition\n218to_SbFrQ_Ctotal SubtlexUS FREQcount value\n219as_SbFrQ_Caverage SubtlexUS FREQcount value per sentence\n220at_SbFrQ_Caverage SubtlexUS FREQcount value per token\n221to_SbCDC_Ctotal SubtlexUS CDcount value\n222as_SbCDC_Caverage SubtlexUS CDcount value per sent\n223at_SbCDC_Caverage SubtlexUS CDcount value per token\n224to_SbFrL_Ctotal SubtlexUS FREQlow value\n225as_SbFrL_Caverage SubtlexUS FREQlow value per sent\n226at_SbFrL_Caverage SubtlexUS FREQlow value per token\n227to_SbCDL_Ctotal SubtlexUS CDlow value\n228as_SbCDL_Caverage SubtlexUS CDlow value per sent\n229at_SbCDL_Caverage SubtlexUS CDlow value per token\n230to_SbSBW_Ctotal SubtlexUS SUBTLWF value\n231as_SbSBW_Caverage SubtlexUS SUBTLWF value per sent\n232at_SbSBW_Caverage SubtlexUS SUBTLWF value per token\n233to_SbL1W_Ctotal SubtlexUS Lg10WF value\n234as_SbL1W_Caverage SubtlexUS Lg10WF value per sent\n235at_SbL1W_Caverage SubtlexUS Lg10WF value per token\n236to_SbSBC_Ctotal SubtlexUS SUBTLCD value\n237as_SbSBC_Caverage SubtlexUS SUBTLCD value per sent\n238at_SbSBC_Caverage SubtlexUS SUBTLCD value per token\n239to_SbL1C_Ctotal SubtlexUS Lg10CD value\n240as_SbL1C_Caverage SubtlexUS Lg10CD value per sent\n241at_SbL1C_Caverage SubtlexUS Lg10CD value per token\nTable 20: Word Familiarity Features (WorF)\nidxCode Definition\n242TokSenM_Stotal count of tokens x total count of sentence\n243TokSenS_Ssqrt(total count of tokens x total count of sentence)\n244TokSenL_Slog(total count of tokens)/log(total count of sent)\n245as_Token_Caverage count of tokens per sentence\n246as_Sylla_Caverage count of syllables per sentence\n247at_Sylla_Caverage count of syllables per token\n248as_Chara_Caverage count of characters per sentence\n249at_Chara_Caverage count of characters per token\nTable 21: Shallow Features (ShaF)\nidxCode Definition\n250SmogInd_SSmog Index\n251ColeLia_SColeman Liau Readability Score\n252Gunning_SGunning Fog Count Score (New, US Navy Report)\n253AutoRea_SAutomated Readability Idx (New, US Navy Report)\n254FleschG_SFlesch Kincaid Grade Level (New, US Navy Report)\n255LinseaW_SLinsear Write Formula Score\nTable 22: Shallow Features (ShaF)\nC Rules Behind Feature Codes\nIn table 8 ∼22, “Code” columns show feature\ncodes. The related linguistic features appear with\nquite a number of variations across academia,\nwithout a naming convention (Zhu et al., 2009;\nFitzsimmons et al., 2010; Tanaka-Ishii et al., 2010;\nDaowadung and Chen, 2011; Vajjala and Meur-\ners, 2013; Ciobanu et al., 2015; Zhang et al., 2019;\nBlinova et al., 2020; Lee and Lee, 2020b). For\nconsistency, we set ourselves a few naming rules.\n1. Feature codes consist of 8 letters/numerals, with\n1 or 2 underscores depending on feature types.\n2. All features classify into either count-based or\nscore-based, following popular convention.\n• Count-based\n– define: final calculation uses simple counts\n(i.e. total, avg per sent, avg per token, ratio)\n– format: xx_xxxxx_C. First two letters are\n“to” (total), “as” (avg per sent), “at” (avg per\ntoken), “ra” (ratio). Five letters in the mid-\ndle explain what the feature is. Last letter\nalways “C.” Two underscores in between.\n• Score-based\n– define: require additional calculation (e.g.\nlog, square), or famous features with pre-\ndefined names (e.g. Flesch-Kincaid, TTR).\n– format: xxxxxxx_S. Seven letters are all\ndedicated to explaining what the feature is.\nLast letter always “S.” One underscore.\n3. For the “explanation” part of each feature code,\ncapital letters denote new words. The small\nletters that follow are from the same word. (e.g.\n1: Coleman Liau → ColeLia, 2: AoA (Age of\nAcquisition) Kuperman of words → AAKuW)\nD Scientific Artifacts\nWe use Online LDA implemented by Gensim v4.0\n( ˇReh˚ uˇrek and Sojka, 2010). For most general tasks,\nincluding sentence/entity recognition, POS tagging,\nand dependency parsing, we use spaCy v3.08 (Hon-\nnibal et al., 2020) with en_core_web_sm pretrained\nmodel. For constituency parsing, we use CRF\nparser (Zhang et al., 2020) in SuPar v1.0 9.\nD.1 Transformers\nFor transformers, we use the following models\nfrom HuggingFace transformers v4.5.0 (Wolf et al.,\n2020).\n1. bert-base-uncased\n2. roberta-base\n3. bart-base\n4. xlnet-base-cased\nD.2 Non-Neural Models\nFor non-neural models, we use the following mod-\nels from from SciKit-Learn v0.24.1.\n1. support vector classifiers (svm.SVC) (Hearst,\n1998; Platt, 1999; Chang and Lin, 2011)\n2. random forest classifiers (ensemble.RandomF\norestClassifier) (Breiman, 2001)\n3. logistic regression (linear_model.LogisticRegr\nession)\nFor gradient boosting, we use the following from\nXGBoost v1.4.0 (Chen and Guestrin, 2016).\n4. gradient boosting (XGBclassifier)\nE Preprocessing\nOur preprocessing steps are inspired by Martinc\net al. (2021) and several other existing RA research.\nThese steps are used only during the extraction of\nhandcrafted features for increased accuracy.\n1. remove all special characters\n2. remove words less than 3 characters\n3. lowercase all\n4. tokenize\n5. remove NLTK default stopwords\nF Full Hyperparameters\nF.1 Machine Learning Models\nWe perform grid search on the hyperparameters (ta-\nble 3) after performing a large randomized search\nto identify the sensible range of hyperparameters to\ntune. In particular, logistic regression solver hyper-\nparameter search include libfgs (Zhu et al., 2011),\nliblinear (Fan et al., 2008), SAG (Schmidt et al.,\n2017), and SAGA (Defazio et al., 2014).\nIn table 3(a), C is the regularization parameter,\nG is the kernel coefficient (gamma), and K is the\n8github.com/explosion/spaCy\n9github.com/yzhangcs/parser\nModel Hyperparameter\nSVM\nC G K\n1 scale rbf\n5 auto linear\n10 poly\n50 sigmoid\n(a) SVM, Best Params\nModel Hyperparameter\nRF\nnEst MDep Mfea\n600 20 auto\n700 60 sqrt\n800 100 log2\n900 None None\n(b) RandomF, Best Params\nModel Hyperparameter\nXGBoost\neta G MDep\n1e-2 0 3\n5e-21e-2 6\n1e-1 1e-1 9\n2e-1 1 12\n(c) XGBoost, Best Params\nModel Hyperparameter\nLR\nC Pen Solver\n1e-1 l1 lbfgs\n5e-1 l2 l.linear\n1 elastic newton\n10 none saga\n(d) LogR, Best Params\nTable 23: Hyperparameters, non-neural models.\nkernel. In table 3(b), nEst is the number of trees,\nMDep is the max depth of a tree, and Mfea is the\nnumber of features considered. In table 3(c), eta is\nthe learning rate, G is the minimum loss reduction\nneed to make a further partition on the leaf node\n(gamma), and MDep is the max depth of a tree.\nIn table 4(d), C is the inverse of the regularization\nstrength, Pen is the norm used in penalization, and\nSolver is the algorithm used in optimization. The\nother parameters best performed at default.\nF.2 Transformers\nWe use AdamW (optimizer) (Kingma and Ba,\n2014), linear (scheduler), 10% (warmup steps), 8\n(batch size), 3 (epoch) for all tested transformers.\nWe use the learning rate of 2e-5 for BERT and 3e-5\nfor the other three transformers.\nG Feature Combinations\nSetFeatures LogR SVM\nT1 AdSem + Disco + Synta + LxSem + ShaTr0.622 0.679\nT2 Disco + Synta + LxSem + ShaTr 0.528 0.546\nT3 AdSem + Synta + LxSem + ShaTr 0.591 0.582\nH1AdSem + Disco 0.463 0.513\nL1 Synta + LxSem 0.499 0.561\nL2 Set L1 - PhrF 0.539 0.577\nL3 Set L1 - VarF 0.529 0.551\nL4 Set L1 - POSF 0.449 0.551\nE1 AdSem + PsyF + WorF + TraF 0.489 0.473\nE2 AdSem+PsyF+WorF 0.490 0.479\nE3 PsyF + WorF 0.464 0.459\nP1 EnDF + ShaF + TrSF + POSF + WorF + PsyF + TraF0.608 0.633\nP2 Set P1 + TraF 0.629 0.638\nP3 Set P2 + VarF 0.647 0.674\n* Note: 5 letters (e.g. AdSem) mean linguistic branch. 4 letters\n(e.g. PhrF) mean subgroup. We report accuracy on WeeBit.\nTable 24: Defining feature sets.\nThe five types of feature sets have varying aims:\n1. T-type thoroughly captures linguistic properties,\n2. H-type captures the high-level properties, 3. L-\ntype captures the low, surface-level properties, 4.\nE-type uses features calculated from external data\n(out-of-model info, i.e. Age-of-Acquisition), and\n5. P-type collects features by performance. Both\nadvanced semantic and discourse features add dis-\ntinctive information. This can be evidenced by\nthe performance decreases (T1 → T2 and T1 →\nT3). We checked that all measures of F1, preci-\nsion, recall, and QWK followed the same trend.\nSimilar method was used in Feng et al. (2009);\nAluisio et al. (2010); Vajjala and Meurers (2012);\nFalkenjack et al. (2013); François (2014) to check\nif a feature added orthogonal information. More\nlinguistic branches generally indicated better per-\nformance. We use SciKit-learn (Pedregosa et al.,\n2011) for metrics.\nH Transformers Training Time\nAll numbers are in seconds. We report in the or-\nder of (BERT, RoBERTa, XLNet, BART). These\nare the average training times for each fold, with\n80% of the full dataset used to train. We used an\nNVIDIA Tesla V100 GPU.\n1. WeeBit (1546, 1485, 3617, 1202)\n2. OneStopEnglish (451, 373, 977, 396)\n3. Cambridge (215, 122, 393, 239)\nI LingFeat\nThroughout our paper, we mention LingFeat as one\nof our contributions to academia. This is because a\nlarge-scale handcrafted features extraction toolkit\nis scarce in RA, despite its reliance on the features.\nLingFeat is a Python research package for vari-\nous handcrafted linguistic features. More specifi-\ncally, LingFeat is an NLP feature extraction soft-\nware, which currently extracts 255 linguistic fea-\ntures from English string input. The package is\navailable on both PyPI and GitHub.\nDue to the wide number of supported features,\nwe had to define subgroups (section 3) for features.\nHence, features are not accessible individually. In-\nstead, one has to call the subgroups to obtain the\ndictionary of the corresponding features. The cor-\nresponding code is applicable to LingFeat v.1.0.\n1 \"\"\"\n2 Import\n3\n4 this is the only import you need\n5 \"\"\"\n6 from lingfeat import extractor\n7\n8\n9 \"\"\"\n10 Pass text\n11\n12 here, text must be in string type\n13 \"\"\"\n14 text = \"...\"\n15 LingFeat = extractor.pass_text(text)\n16\n17\n18 \"\"\"\n19 Preprocess text\n20\n21 options (all boolean):\n22 - short (def. False): include short words\n23 - see_token (def. False): return token list\n24 - see_sent_token (def. False): return sent\n25\n26 output:\n27 - n_token\n28 - n_sent\n29 - token_list (optional)\n30 - sent_token_list (optional)\n31 \"\"\"\n32 LingFeat.preprocess()\n33 # or\n34 # print(LingFeat.preprocess())\n35\n36\n37 \"\"\"\n38 Extract features\n39\n40 each method returns a dictionary of\n41 the corresponding features\n42 \"\"\"\n43 # Advanced Semantic (AdSem) Features\n44 WoKF=LingFeat.WoKF_() #Wiki Know. Features\n45 WBKF=LingFeat.WBKF_() #WB Knowledge Features\n46 OSKF=LingFeat.OSKF_() #OSE Knowledge Features\n47\n48 # Discourse (Disco) Features\n49 EnDF=LingFeat.EnDF_() #Entity Dens. Features\n50 EnGF=LingFeat.EnGF_() #Entity Grid Features\n51\n52 # Syntactic (Synta) Features\n53 PhrF=LingFeat.PhrF_() #Phrasal Features\n54 TrSF=LingFeat.TrSF_() #(Parse) Tree Features\n55 POSF=LingFeat.POSF_() #POS Features\n56\n57 # Lexico Semantic (LxSem) Features\n58 TTRF=LingFeat.TTRF_() #TTR Features\n59 VarF=LingFeat.VarF_() #Variational Features\n60 PsyF=LingFeat.PsyF_() #Psycholing Difficulty\n61 WoLF=LingFeat.WorF_() #Word Familiarity\n62\n63 # Shallow Traditional (ShTra) Features\n64 ShaF=LingFeat.ShaF_() #Shallow Features\n65 TraF=LingFeat.TraF_() #Traditional Formulas",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.9190349578857422
    },
    {
      "name": "Transformer",
      "score": 0.8237615823745728
    },
    {
      "name": "Computer science",
      "score": 0.7700706720352173
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5871864557266235
    },
    {
      "name": "Natural language processing",
      "score": 0.5752268433570862
    },
    {
      "name": "Random forest",
      "score": 0.5041114091873169
    },
    {
      "name": "Machine learning",
      "score": 0.34917551279067993
    },
    {
      "name": "Programming language",
      "score": 0.11841264367103577
    },
    {
      "name": "Engineering",
      "score": 0.09151279926300049
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}