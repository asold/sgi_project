{
  "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
  "url": "https://openalex.org/W4327737695",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2559942581",
      "name": "Evans Kotei",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2732057995",
      "name": "Ramkumar Thirunavukarasu",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2022479123",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4309548228",
    "https://openalex.org/W4312846227",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2891631795",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2998957378",
    "https://openalex.org/W2982213159",
    "https://openalex.org/W2982021527",
    "https://openalex.org/W3013601031",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W4290875442",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2014079492",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2594990650",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W1999554638",
    "https://openalex.org/W6797132756",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W7051469422",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W3164896303",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3017961061",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3175898847",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3185909895",
    "https://openalex.org/W3118043957",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4311430112",
    "https://openalex.org/W2806758205",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W3193158708",
    "https://openalex.org/W6784416658",
    "https://openalex.org/W3037063616",
    "https://openalex.org/W3007007518",
    "https://openalex.org/W6775774072",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W6772299980",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3175870271",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3092448486",
    "https://openalex.org/W6788210547",
    "https://openalex.org/W6745682157",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W2970368801",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3157876196",
    "https://openalex.org/W3098469895",
    "https://openalex.org/W6781275321",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4313550290",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3128090102",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W3016187590"
  ],
  "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8329981565475464
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6787575483322144
    },
    {
      "name": "Transformer",
      "score": 0.6771350502967834
    },
    {
      "name": "Transfer of learning",
      "score": 0.6609372496604919
    },
    {
      "name": "Language model",
      "score": 0.607001781463623
    },
    {
      "name": "Overfitting",
      "score": 0.5714921951293945
    },
    {
      "name": "Machine learning",
      "score": 0.5698449015617371
    },
    {
      "name": "Inference",
      "score": 0.5521376729011536
    },
    {
      "name": "Domain adaptation",
      "score": 0.5294138193130493
    },
    {
      "name": "Deep learning",
      "score": 0.470439612865448
    },
    {
      "name": "Natural language processing",
      "score": 0.41584813594818115
    },
    {
      "name": "Artificial neural network",
      "score": 0.21313634514808655
    },
    {
      "name": "Classifier (UML)",
      "score": 0.12289455533027649
    },
    {
      "name": "Engineering",
      "score": 0.07924306392669678
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I876193797",
      "name": "Vellore Institute of Technology University",
      "country": "IN"
    }
  ]
}