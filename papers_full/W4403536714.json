{
  "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
  "url": "https://openalex.org/W4403536714",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2742283555",
      "name": "Cheng Baijun",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2351550429",
      "name": "Zhang, Cen",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A246320800",
      "name": "Wang Kailong",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2043239742",
      "name": "Shi Ling",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2222093681",
      "name": "Wang Haoyu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2230355597",
      "name": "Guo Yao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1989094844",
      "name": "Li Ding",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2224189978",
      "name": "Chen Xiangqun",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2987375469",
    "https://openalex.org/W2885030880",
    "https://openalex.org/W4390690139",
    "https://openalex.org/W2297774820",
    "https://openalex.org/W4384155508",
    "https://openalex.org/W1816718056",
    "https://openalex.org/W2785618324",
    "https://openalex.org/W4311166013",
    "https://openalex.org/W2898326323",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4386764545",
    "https://openalex.org/W3089472875",
    "https://openalex.org/W4396242417",
    "https://openalex.org/W6857873949",
    "https://openalex.org/W4394769342",
    "https://openalex.org/W4391946054",
    "https://openalex.org/W4390528935",
    "https://openalex.org/W4391579642",
    "https://openalex.org/W4378591002",
    "https://openalex.org/W4297677548",
    "https://openalex.org/W4402410044",
    "https://openalex.org/W4312090841",
    "https://openalex.org/W6997171492",
    "https://openalex.org/W4390529272",
    "https://openalex.org/W4398794941",
    "https://openalex.org/W4391833847",
    "https://openalex.org/W4399567237",
    "https://openalex.org/W4400582517",
    "https://openalex.org/W4380353722",
    "https://openalex.org/W4384816574",
    "https://openalex.org/W4403223051",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W4398230183",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4391833078",
    "https://openalex.org/W3048197573",
    "https://openalex.org/W6860041859",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W6856800273",
    "https://openalex.org/W4284671049",
    "https://openalex.org/W4385187234",
    "https://openalex.org/W2118221555",
    "https://openalex.org/W1993736952",
    "https://openalex.org/W2964938167",
    "https://openalex.org/W3081770884",
    "https://openalex.org/W2547862110",
    "https://openalex.org/W2799226481",
    "https://openalex.org/W2997088857",
    "https://openalex.org/W2891057055",
    "https://openalex.org/W4391416727",
    "https://openalex.org/W4289038676",
    "https://openalex.org/W4392617597",
    "https://openalex.org/W4387635545"
  ],
  "abstract": "In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios. To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.",
  "full_text": "Semantic-Enhanced Indirect Call Analysis with Large Language\nModels\nBaijun Cheng\nPeking University\nBeijing, China\nprophecheng@stu.pku.edu.cn\nCen Zhangâˆ—\nNanyang Technological University\nSingapore, Singapore\ncen001@e.ntu.edu.sg\nKailong Wangâˆ—\nHuazhong University of Science and\nTechnology\nWuhan, China\nwangkl@hust.edu.cn\nLing Shi\nNanyang Technological University\nSingapore, Singapore\nling.shi@ntu.edu.sg\nYang Liu\nNanyang Technological University\nSingapore, Singapore\nyangliu@ntu.edu.sg\nHaoyu Wang\nHuazhong University of Science and\nTechnology\nWuhan, China\nhaoyuwang@hust.edu.cn\nYao Guoâˆ—\nPeking University\nBeijing, China\nyaoguo@pku.edu.cn\nDing Li\nPeking University\nBeijing, China\nding_li@pku.edu.cn\nXiangqun Chen\nPeking University\nBeijing, China\ncherry@sei.pku.edu.cn\nABSTRACT\nIn contemporary software development, the widespread use of\nindirect calls to achieve dynamic features poses challenges in con-\nstructing precise control flow graphs (CFGs), which further impacts\nthe performance of downstream static analysis tasks. To tackle this\nissue, various types of indirect call analyzers have been proposed.\nHowever, they do not fully leverage the semantic information of\nthe program, limiting their effectiveness in real-world scenarios.\nTo address these issues, this paper proposes Semantic-Enhanced\nAnalysis (SEA), a new approach to enhance the effectiveness of\nindirect call analysis. Our fundamental insight is that for common\nprogramming practices, indirect calls often exhibit semantic similar-\nity with their invoked targets. This semantic alignment serves as a\nsupportive mechanism for static analysis techniques in filtering out\nfalse targets. Notably, contemporary large language models (LLMs)\nare trained on extensive code corpora, encompassing tasks such as\ncode summarization, making them well-suited for semantic analysis.\nSpecifically, SEA leverages LLMs to generate natural language sum-\nmaries of both indirect calls and target functions from multiple per-\nspectives. Through further analysis of these summaries, SEA can de-\ntermine their suitability as caller-callee pairs. Experimental results\ndemonstrate that SEA can significantly enhance existing static anal-\nysis methods by producing more precise target sets for indirect calls.\n1 INTRODUCTION\nIndirect calls (icalls hereafter) have been widely used in system\nand application software to enable dynamic behavior, simplify code\nstructure, and improve code maintainability. They allow the target\nof a function call to be determined at runtime, providing better\nflexibility and modularity. As a result, they have become ubiquitous\nin large-scale software systems. For example, the Linux kernel 5.1.0\ncontains roughly 58K icalls, while Firefox has 37K icalls [1].\nâˆ— Corresponding authors\nThe use of icalls, with their runtime-dynamic nature, creates\nsignificant challenges for precise code analysis tools, such as those\nrequiring an in-depth understanding of Interprocedural Control\nFlow Graph (ICFG) properties. Typical examples of these scenarios\nare code navigation [2] and code security analysis [3â€“5]. The former,\nfunctionalities such as \"Go to Definition\" and \"Find References\",\nassist developers in locating identifier definitions and detecting\ncallees. However, icalls complicate these tasks, thereby reducing\nthe effectiveness of existing tools. The latter involves security anal-\nysis like taint analysis which is an ICFG-based method to identify\nwhether a vulnerability is present or can be exploited, where an\naccurate icall resolution can provide significant benefits. Therefore,\nhaving a precise and effective icall analysis is critical for compre-\nhending program behavior and enabling enhanced code analysis\ncapabilities.\nResearchers have proposed to conduct icall analysis through\npointer analysis [6â€“8] and type analysis [9â€“12]. However, due to\nthe inherent complexities of icalls (compared to regular function\ncalls), it is hard to achieve high effectiveness using only interpro-\ncedural data flow or parameter types. In contrast, we argue that\nhigh-level, e.g., natural language, textual semantics in code are also\nhighly valuable for icall analysis . For example, in code repositories\nadhering to programming conventions, caller and callee functions\n(including icalls) often exhibit textual semantic similarities, which\ncould be used to improve icall analysis results.\nIn recent years, the progress in AI [13, 14] and Natural Language\nProcessing (NLP) [15â€“17] has opened new opportunities to aug-\nment traditional icall analysis by extracting semantic data from the\ncode. Various software engineering applications are already making\nuse of LLMs to better program analysis tasks [18â€“25]. By incorpo-\nrating LLMs, it is possible to establish natural language describable\nattributes, which are typically challenging to formalize. This can\npotentially offer a superior understanding of codeâ€™s semantics and\ncontext, which is comparable or even exceeding the grasp of human\nexperts.\narXiv:2408.04344v3  [cs.SE]  30 Oct 2024\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nAn intuitive perspective for utilizing LLMs for enhancing icall\nanalysis is to boost code semantics interpretation, so we can better\ndetermine caller-callee relationships. However, directly applying\nLLMs to this task presents two key challenges. Challenge #1: The\nfirst challenge involves determining the key factors in or effective\napproaches to using LLMs to enhanced icall analysis. It is neces-\nsary to refine the range of information extraction so it focuses\non relevant functions. Despite the various entities defined in the\nrepositories, such as functions, structures, and unions, not all code\npertains to icalls. Challenge #2: The second challenge is how to\neffectively apply these approaches in complex, real-world projects.\nThe inherent intricacy of practical projects complicates the icall\nanalysis. Due to the largeness of code bases in practical projects,\nadaptive and meticulous methodologies are required to effectively\ngather the necessary information. Further, as task complexity in-\ncreases, relying solely on a single prompt might not be sufficient\nsince the context surrounding caller-callee interactions may involve\nmultiple entities. Therefore, it is essential to develop strategies that\ncan efficiently extract and use pertinent semantic information, mit-\nigating the limitations of LLMs.\nOur Work. In this work, we present SEA (Semantic-Enhanced\nAnalysis), a novel semantic-aware static analysis framework for\nicall analysis. Our approach aims to address the limitations of ex-\nisting techniques by constructing semantic information within the\ncontext of callers and their target callees. With these semantic\ninformation, SEA can enable effective filtering of false targets, ul-\ntimately leading to more accurate and efficient icall analysis. For\ninstance, a caller module->create_conf(cycle) suggests that the\ncorresponding callees are specifically designed to create configura-\ntions for particular modules. By leveraging this semantic similarity,\nSEA can effectively prune a significant portion of false targets.\nTo tackle Challenge #1, considering the characteristics of the\nexisting ICFG-based analysis and the textual nature of code se-\nmantics, we proposed a composite approach to utilize the code\nsemantics for enhancement. Specifically, SEA first conducts tradi-\ntional analyses (e.g., type and pointer analyses), whose output is a\nset of candidate caller-callees pairs. It then apply code semantics\nanalysis using LLM to further filter the candidates, modeling the\nproblem as a binary classification task. This semantic-aware ap-\nproach enables SEA to identify and filter out irrelevant functions,\nnarrowing down the analysis scope to pertinent candidates.\nTo tackle Challenge #2, we propose a two-step approach: se-\nmantic summary and caller-callee matching. First, we introduce\na dedicated context database that extracts and collects necessary\ncontext information of callers and callees within the given project,\ninspired by observations of real-world programs and prior related\nwork [1, 26â€“28] revealing semantic similarity between callers and\ncallees. Our method employs an LLM-based extractor to compre-\nhend the local and global contexts of the caller and callee, summa-\nrizing their task-specific characteristics and information, allowing\nSEA to gain a deeper understanding of their purpose and behav-\nior within the broader context of the codebase. Once the caller\nand callee summaries are generated, SEA utilizes LLMs to match\nthem based on their semantic compatibility. Only when the callerâ€™s\nsummary indicates that it can invoke the callee according to the\nextracted semantics, SEA will consider them a valid match. This\nsemantic-driven matching process significantly reduces false posi-\ntives and improves the accuracy of icall resolution.\nWe evaluate the effectiveness of SEA on 31 projects from oss-\nfuzz [29]. The experimental results demonstrate that, in an environ-\nment without compilation options, SEA can improve the F1 score\nof the static analysis tool FLTA [10] by up to 24% in the best-case\nscenario. Additionally, in cases where only FLTA can perform the\nanalysis, SEA improves its F1 score from 38% to 67%. Compared to\ntwo more advanced tools MLTA [1] and Kelp [30], SEA also demon-\nstrate notable advantages in terms of flexibility and robustness. SEA\nis not restricted to analyzing callers that fit specific patterns, such\nas requiring the caller to be a simple function pointer or related to\na non-escaped struct. The principles underlying SEA are applicable\nto a wide range of indirect calls.\nContributions. Our contributions are summarized as follows:\nâ€¢We propose SEA, the first semantic-enhanced approach for indi-\nrect call analysis with the help of LLMs. Compare to conventional\ntechniques [1, 7, 30], our approach can fully leverage the textual\nsemantics in code to better determine icall relationships.\nâ€¢We implement SEA and conduct experiments by integrating\nSEA with existing conventional static analysis methods. Results\ndemonstrate that SEA can accurately filter out invalid caller-\ncallee pairs, and improve existing static analysis significantly.\nâ€¢We curate an extensive benchmark dataset sourced from real-\nworld programs, leveraging fuzzing techniques [29] in its con-\nstruction. To facilitate the future research on this topic, we release\nour artifacts at [31].\n2 PRELIMINARIES\n2.1 Background\nExisting Work on Indirect Call Analysis Currently, scalable\nstatic analysis methods include FLTA [10], MLTA [1], and Kelp [30].\nTheir relationships are depicted in Figure 1, highlighting their differ-\nent application scenarios. FLTA is based on simple function signa-\nture matching, making it suitable for analyzing all icalls. MLTA [1]\nproposes a hierarchical analysis approach based on struct type\nlevels to enhance the precision of FLTA, which is effective only\nwhen function pointers are encapsulated within structs and the\ncorresponding structs do not exhibit escape behavior. Kelp [ 30]\nintroduces a regional pointer analysis technique tailored for sce-\nnarios characterized by simple data flow patterns. When icalls do\nnot satisfy the specified patterns, MLTA and Kelp would fall back\nto other analysis algorithms (MLTA falls back to FLTA, while Kelp\nfalls back to either MLTA or FLTA). An intuitive solution is to aug-\nment the analytical rules to encompass as many cases as possible.\nHowever, such an approach entails the precision-recall balance\ndilemma, wherein the endeavor to prune a greater number of false\ntargets may inadvertently leads to the pruning of a portion of true\ntargets. During our preliminary study on the benchmarks, among\nall successfully parsed and analyzed icalls, we observed that Kelp\nfigured out 10.4% of cases, while MLTA covered 46.3% of cases.\nFLTA handled the remaining 43.3% of cases. This leaves significant\nroom for further optimization.\nUnderutilization of Code Semantics. Existing static analysis\nmethods lack comprehension of code semantics. Typically, within\ncode repositories adhering to programming standards, function\nSemantic-Enhanced Indirect Call Analysis with Large Language Models ASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States\nFLTAMLTAKelp\nFigure 1: Relationship between traditional methods.\npointers and target functions often exhibit semantic similarities\nsuperficially. Thanh et al. [26] proved that semantic features are\nhelpful in call graph pruning. However, existing static analysis\nmethods capture this aspect inadequately. We observe that in many\ncases not covered by Kelp, there exists a strong semantic similarity\nbetween icalls and their invoked targets. Leveraging these features,\nwe can further refine the target sets.\nCapabilities of LLMs. Recently, significant advancements have\nbeen made in software analysis by applying LLMs. Previous stud-\nies [18, 19, 23, 25, 32] have demonstrated the superiority of LLMs\nover traditional program analysis methods in summarizing path\nconditions, seed variation, source-sink analysis, summarizing loop\ninvariants, and extract critical attributes. In our scenario, we find\nthat LLMs could play a significant role in code semantic analysis.\nThis is attributed to LLMs being trained on extensive code corpora,\nencompassing tasks such as code translation, code summarization,\ndocument generation, etc. Such training endows LLMs with strong\ncapabilities in comprehending code semantics.\nDespite offering new perspectives for software analysis, LLMs\nface several challenges [33â€“39]. Firstly, their limited context capac-\nity means that we cannot input large chunks of code for analysis\nat once. Consequently, it may be impossible to provide all relevant\ncontext for both the caller and callee in a single input to LLMs.\nSecondly, LLMs are prone to generating incorrect responses, a\nphenomenon known as hallucination [40â€“42]. Lastly, their perfor-\nmance deteriorates as task complexity increases [43], and even the\nchain-of-thought strategy may become ineffective [44].\n2.2 Motivating Example\nFigure 2 shows the general process of traditional static analysis\nmethods on an icall example from the bind9 [45] project. The caller\nhas two potential callees: the true target isc_log_error_call-\nback, and the false target towire_compare. The analysis falls out-\nside the scope of Kelp [30] as it does not meet the definition of a\nsimple icall according to the toolâ€™s criteria. Furthermore, despite\nbeing related to a struct, the challenge of type-escape [1] renders it\nunanalyzable by another SOTA approach MLTA [1]. Consequently,\nFLTA [10] stands as the sole capable analyzer in this scenario. The\nanalysis outcome of FLTA marks both callees as true targets, con-\nsidering the casting relationship between void* and other pointer\ntypes.\nReviewing the case discussed above, we observe that current\nstatic analysis methods overlook the intrinsic semantic information\nof the program. This semantic information is essential for enhancing\nthe accuracy of the analysis results. For instance, by comparing\nthe context of the true callee and the caller, we can see that their\nfunctions are related to reporting errors, whereas the false callee,\nbased on textual analysis, is a comparison function that is unlikely\nto be invoked by the caller.\n3 METHODOLOGY\nFigure 3 presents the overview of SEA, which is inspired by a call\ngraph pruning tool Autopruner [ 26]. It begins by utilizing tradi-\ntional static analysis (Â§ 3.1) to ascertain a superset of the target set\nfor each icall. Subsequently, it harnesses the semantic comprehen-\nsion capabilities of a language model to refine this superset through\nthe binary classification of caller-callee pairs.\nDuring semantic analysis, SEA initially preprocesses the source\ncode of a project, constructing a context database that includes\ncrucial context information such as TypeAliasMap (stage 2, Â§ 3.2).\nSubsequently, SEA queries corresponding context from the context\ndatabase based on both caller and callee, and inputs it to LLM,\nprompting it to analyze the purposes of the caller and callee and\ngenerate natural language summaries (stage 3, Â§ 3.3). Finally, SEA\nconstructs the ultimate matching prompt based on the summaries\nof the caller and callee, which is then analyzed by LLM to determine\nif they form a caller-callee pair. Based on the analysis by LLM, SEA\nrefines the initial target set into the final target set (stage 4, Â§ 3.4).\n3.1 Traditional Analysis\nThe first step of SEA involves identifying an initial target func-\ntion set for each icall from all functions of a project. This target\nset theoretically encompasses numerous false positives and neces-\nsitates further refinement. Currently, FLTA [10], MLTA [1], and\nkelp [30] can achieve this objective. However, the evaluation of\nthese three static analysis methods is predominantly conducted\nwithin the LLVM IR context, with no current implementation at the\nsource code level. At the source code level, our implementations\nlargely follow the analysis algorithms used at the IR level. The\nadditional components that need to be implemented include (1)\nthe identification of address-taken functions and (2) the strategy\nof type matching.\nIdentification of Address-Taken Functions. A presumption\nin icall analysis is that the target of an icall must be an address-\ntaken function, meaning a function whose address is assigned to a\npointer variable. In LLVM IR, identifying address-taken functions is\nstraightforward as users can easily determine if a functionâ€™s address\nis taken by utilizing LLVMâ€™s API [46]. However, this process is not\nas straightforward at the source code level, especially in the absence\nof compilation options. We can solely rely on analyzing the abstract\nsyntax trees (ASTs) to identify the set of address-taken functions\nwithin a project. The traversal rule is defined as follows:\nâ€¢For each identifier node id, if id is not the first child of a call\nexpression, also id is not a global or local variable name, we add\nid to address-taken function name set.\nStrategy of Type Matching. Before type matching, we must iden-\ntify all types of call arguments and function parameters. This step\ncould be done by traversing ASTs. However, during this process, we\nmay encounter parsing errors. For instance, the presence of macros\nsuch as DNS__DB_FLARG in variable definition statements likedns_-\nrdataset_t *rdataset DNS__FLARG can cause parsing errors,\npreventing us from identifying the type of the variable rdataset.\nIn such cases, we mark these variables with unresolved parsing\nerrors as unknown types. Consequently, our type-matching algo-\nrithm differs from that of K. Lu et al. [ 1]. Besides, our approach\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nstaticvoidisclog_error_callback(  dns_rdatacallbacks_t *callbacks,  constchar*fmt, ...) { // â€¦isc_log_vwrite(â€¦);}\nstaticinttowire_compare(constvoid*av,  constvoid*bv) { // â€¦return a->key â€“b->key;}\nstaticisc_result_t gettoken(  isc_lex_t *lex, unsignedintoptions, â€¦){ // â€¦(*callbacks->error)(â€¦)      } context of caller\ncontext of true callee\ncontext of false callee\nis simple-icall?related to sturct?is type-escape?\nKelpFLTAMLTA\nyes\nno\nno\nno\nyesyes\nTraditional Analysis\nAnalysis1: The caller is not simple icall, hence not covered by kelpAnalysis2: The caller is related to type-escaped struct, therefore out of the scope of MLTA.Analysis3: Only FLTA is available, hence the result is determined by type match.\nAnalysis Resultcalleeisclog_error_callbackType-MatchYestrue calleeYescalleetowire_compareType-MatchYestrue calleeYes\nUnusedSemantic InformationFrom the text of caller of callees, we can see that both caller and function isclog_error_callbackseems to log error message. towire_compare seems to be used as comparsion method.\nFigure 2: Example indirect-call and target functions.\nicall: (*callback)->error(â€¦)target funcs:     isclog_error_callbackchain_compare\nfile:lib/dns/adb.c text: isc_result_tdns_acl_create(isc_mem_t *mctx\nSource Codefile: lib/dns/acl.c text: isc_result_tdns_acl_create(isc_mem_t *mctx, intn, dns_acl_t **target) { â€¦\nicall: (*callback)->error(â€¦)target funcs:     isclog_error_callbackchain_compare\nInitial Targets For Icallsicall: (*callback)->error(â€¦)target funcs:     isclog_error_callbackchain_compare\nicall: (*callback)->error(â€¦)target funcs:     isclog_error_callbackchain_compare\nicall: (*callback)->error(â€¦)target funcs:     isclog_error_callbackchain_compare\nFinal Targets For Icallsicall: (*callback)->error(â€¦)target funcs:     isclog_error_callback\nPreprocess2\nASTs\nContext DatabaseSemantic SummaryCaller\nCallee\nQuery Context\nCaller ContextSource FunctionType Definitions\nCallee ContextCallee FunctionAddr-Taken Sites\nLLM\nCaller Summary\nCallee Summary\nCaller-CalleeMatchMatchingPrompt\nQuestion: Given information above, could caller match callee?\nLLM\n01\nThe calleris used for â€¦The function pointer is defined for â€¦\nThe calleewill â€¦It is assigned to function pointers which will â€¦\nCaller Summary\nCallee Summaryâ€¦\nâ€¦\nLocal:Global:\nLocal:Global:\nFilter\n3 4\n1Parse\nTraverse\nTraditional AnalysisType AnalysisPointer Analysis\nTypeAliasMapStructInfoMapâ€¦\nResult\nBuildPrompt\nFigure 3: Overview of SEA.\nalso considers factors such as variable parameters, leading us to\nemploy a more conservative matching strategy. Specifically, our\ntype-matching approach differs from MLTA in the following ways:\nâ€¢Our approach does not rely on matching function signatures.\nInstead, it compares the types of each parameter individually. This\ncomparison is conducted under the condition that the number\nof parameters aligns, accounting for variadic parameters when\nnecessary.\nâ€¢Parameters marked as unknown types are treated conservatively,\nbeing assumed to match with all types.\nâ€¢We establish that void* and char* can be matched with any\npointer type, as observed in benchmarks where similar type\nconversions exist.\nThis conservative strategy may introduce more false targets but\nalso preserves a greater number of true targets.\n3.2 Preprocess\n1 // type definition example\n2 typedef void (* sdp_free_func_t )( void *);\n3\n4 // struct example\n5 typedef struct {\n6 ngx_int_t (* preconfiguration )( ngx_conf_t *cf );\n7 ngx_int_t (* postconfiguration )( ngx_conf_t *cf );\n8\n9 void *(* create_main_conf )( ngx_conf_t *cf );\n10 char *(* init_main_conf )( ngx_conf_t *cf , void * conf );\n11\n12 void *(* create_srv_conf )( ngx_conf_t *cf );\n13 char *(* merge_srv_conf )( ngx_conf_t *cf , void *prev , void * conf );\n14\n15 void *(* create_loc_conf )( ngx_conf_t *cf );\n16 char *(* merge_loc_conf )( ngx_conf_t *cf , void *prev , void * conf );\n17 } ngx_http_module_t ;\n18\n19 // global declaration example\n20 static ngx_http_module_t ngx_http_log_module_ctx = {\n21 NULL , /* preconfiguration */\n22 ngx_http_log_init , /* postconfiguration */\n23\n24 ngx_http_log_create_main_conf , /* create main configuration */\n25 NULL , /* init main configuration */\n26\n27 NULL , /* create server configuration */\n28 NULL , /* merge server configuration */\n29\n30 ngx_http_log_create_loc_conf , /* create location configuration */\n31 ngx_http_log_merge_loc_conf /* merge location configuration */\n32 };\nFigure 4: Example of global context.\nThe primary objective of this step is to construct a context data-\nbase for subsequent analysis, akin to the context retrieval methods\nSemantic-Enhanced Indirect Call Analysis with Large Language Models ASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States\nemployed in IDECoder [27] and Cocomic [28]. However, our work\ndistinguishes itself by concentrating on obtaining context related to\nthe caller and callee for caller-callee pair matching. In contrast, IDE-\nCoder and Cocomic focus on extracting other textual information\nthat the current code depends on to facilitate code completion.\nTo identify the optimal context for input into an LLM for analysis,\nwe reference other works. Autopruner [26], in particular, demon-\nstrated the critical role of both caller and callee functions in the\nsemantic analysis of call graphs. The text of these functions is typ-\nically located in the local code segments of the caller and callee,\nwhich we refer to as the local context. Additionally, beyond the\nlocal context, there are code segments semantically related to the\ncaller and callee, which we refer to as the global context. Compared\nto the local context, we observe that the global context provides a\nmore high-level description of the caller and other functions of the\nsame type. Through manual analysis of several projects, we identify\nthree types of global context that are beneficial for understanding\nthe semantics of caller and callee functions: type definitions, global\nvariable declarations, and struct definitions. We illustrate this with\nFigure 4, in which the type definition statement defines the sdp_-\nfree_func_t function type, indicating that this class of function\npointers is related to freeing memory. Where in struct ngx_http_-\nmodule_t, eight different function pointers are defined, each cor-\nresponding to a handler in the nginx http module, which needs to\nperform different functions. In the subsequent global variable dec-\nlaration, an nginx module ngx_http_log_module_ctx is defined,\nwhere functions such as ngx_http_log_create_main_conf and\nngx_http_log_init are assigned to corresponding fields. When\nthe callerâ€™s function pointer or the calleeâ€™s address-taken site in-\nvolves the above type, struct, or global variable, incorporating their\ntext into the prompt can better facilitate the LLMâ€™s understanding\nof its semantics.\nIn addition, the calleeâ€™s address-taken site primarily originates\nfrom three sources: assignment expressions, initializers of declara-\ntions, and call arguments. Therefore, we abstract the language we\nprocess into the form illustrated in Figure 5, where a program is\nviewed as a collection of entities. Each entity can be a global vari-\nable declaration, function, type, or struct definition. Declarations\ncan specifically refer to global variables, local variables, fields, or\nparameter declarations. Regarding statements, we focus on call, as-\nsignment, and declaration statements, as these may indicate address-\ntaken sites of a function. When traversing the AST, we use seven\nmaps to store relevant information: TypeAliasMap, StructIn-\nfoMap, GlobalVarMap, FunctionMap, FuncNameToCallExprsMap,\nFuncNameToDeclarationsMap, and FuncNameToAssignmentsMap.\nThe FunctionMap stores basic information about a function, in-\ncluding details about its parameters and local variables. The\nTypeAliasMap, StructInfoMap, and GlobalVariableMap store\ninformation related to type aliases, defined structures, and de-\nclared global variables, respectively. Furthermore, the FuncName-\nToCallExprsMap, FuncNameToDeclarationsMap, and FuncName-\nToAssignmentsMap store basic information about each functionâ€™s\ncall expression, variable declaration, and assignment statement\naddress-taken sites, respectively. The corresponding rules can be\ndescribed as follows:\nProgram P := E+ CallExpression C := func(Expr1, . . . ,Exprn)\nEntity E := D | F | S | T FunctionDef F := f(D1, . . . ,Dn){Stmtâˆ—; }\nStructDef S := struct{Dâˆ—; } Declaration D := Type Name(= Expr)+\nStatement Stmt :=D | A | C Assignment A := Expr1 = Expr2\nTypeDef T := typedef srcType dstType;\nFigure 5: Abstraction of language.\nCaller Local ContextThe icall is refered in function gettoken:[definition of gettoken]Caller Global ContextThe variable related to icall is: [declarator of callbacks]Also, it refers to struct dns_rdatacallbacks:[definition of dns_rdatacallbacks ]Callee Local ContextThe code of target function is:[definition of isclog_error_callback]Callee Global Contextisclog_error_callback is assigned to a dns_rdatacallbacks variable in function dns_rdatacallbacks_init:[declarator of dns_rdatacallbacks_init][assignment_expression][definition of dns_rdatacallbacks ]\n1. The `gettoken` function is a utility function â€¦2. The icall formats an error message â€¦\nCaller Local Summary\nCaller Global SummaryThe purpose of the function pointer is to handle error reporting during the process of loading a DNS master zone fileCallee Local SummaryThe isclog_error_callback function  log an error message with the ISC log writer â€¦Callee Global SummaryThe function pointer assigned by isclog_error_callback  allowing the caller to handle or log these errors appropriately\nFigure 6: Example of context for caller and callee.\nâ€¢For each TypeDef statement, srcType represents the existing\ntype name, which may be a function type, anddstType represents\nthe new type name. We store the information of srcType and\ndstType to the TypeAliasMap.\nâ€¢For each StructDef, which contains a series of field declara-\ntions, some of these fields may be function pointers. We store the\nstruct name along with the corresponding field information in\nthe StructInfoMap.\nâ€¢For each Declaration D, if it declares a global variable, we add\nvariable name and D to GlobalVarMap.\nâ€¢For each FunctionDef statement, we parse its declarator to ob-\ntain the type and name of each parameter and analyze the func-\ntion body to retrieve information about local variables. We then\nadd this information to the FunctionMap.\nâ€¢For each CallExpression C , if its ğ‘–-th argument expression\nğ¸ğ‘¥ğ‘ğ‘Ÿğ‘– references an address-taken function func, we add C and\nthe argument index ğ‘–to FuncNameToCallExprsMap[func].\nâ€¢For each Assignment A, if the right-hand side expression ğ¸ğ‘¥ğ‘ğ‘Ÿ2\nreferences an address-taken function func, we add A to Func-\nNameToAssignmentsMap[func].\nâ€¢For each global or local variable Declaration D, if it includes\nan initializer that references an address-taken function func, we\nadd D to FuncNameToDeclarationsMap[func].\n3.3 Semantic Summary\nIn the previous section (Â§ 3.2), we discuss the global and local\ncontexts that are useful for caller-callee matching and the context\ndatabase used to store them. Subsequently, the primary objective of\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nthis step is to extract the corresponding local and global contexts\nfor a given caller and callee from the context database and input\nthem into the LLM for analysis. We follow a four-step process\nto obtain the callerâ€™s and calleeâ€™s local and global contexts. The\nLLM then summarizes these contexts, generating natural language\ndescriptions of their purposes, which we refer to as the caller local\nsummary, caller global summary, callee local summary, and callee\nglobal summary, respectively. It is important to note that if no\nglobal context is collected, we skip the corresponding step. Finally,\nwe input all the summaries generated by the LLM into the LLM\nagain to perform the caller-callee matching. As the example shown\nin Figure 6, SEA successfully extracts the contexts of the caller and\ncallee, generates the corresponding natural language summaries,\nand further combines these four summaries through the LLM to\nsuccessfully identify the caller-callee pair.\nQuery Context. The process of extracting the local context for\nthe caller and callee is straightforward: we simply retrieve the\ncorresponding function text from the FunctionMap. Extracting the\ncallerâ€™s global context is slightly more complex. We need to parse the\ncallerâ€™s corresponding AST node and analyze its function pointers.\nThe rules for this analysis can be broadly described as follows:\nâ€¢We extract the corresponding variable declaration from theGlob-\nalVarMap if the function pointer is a global variable, or from\nFunctionMap if it is a local variable, and add it to the global\ncontext.\nâ€¢If the corresponding function pointer is a struct field, we extract\nthe corresponding struct text from the StructInfoMap and add\nit to the global context.\nâ€¢If the function pointer variableâ€™s declaration involves a type alias,\nwe extract the corresponding type declaration text from the Ty-\npeAliasMap and add it to the global context.\nIn comparison to extracting the callerâ€™s global context, extracting\nthe calleeâ€™s global context is even more complex. While extract-\ning the callerâ€™s global context only requires analyzing the callerâ€™s\nfunction pointers, extracting the calleeâ€™s global context involves\nanalyzing its corresponding address-taken sites, which may be\ninitializers, assignments, or call arguments. Additionally, a callee\nmay contain multiple address-taken sites. For the callee function\nfunc, we first extract the corresponding global context for each\naddress-taken site according to the following rules:\nâ€¢For assignment site in FuncNameToAssignmentsMap[func] and\nFuncNameToDeclarationsMap[func] (initializer and assign-\nment share similar process), we parse the left expression of the\nassignment statement, which is the function pointer being as-\nsigned, according to the method described in the previous step\nof extracting caller global context. We then combine the corre-\nsponding global context with the assignment site text to form\nthe global context for the current assignment site.\nâ€¢For each call expression in FuncNameToCallExpr[func], we ob-\ntain its call-chain with respect to func, represented as [(ğ‘ğ‘ğ‘™ğ‘™1,\nğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡1), ..., (ğ‘ğ‘ğ‘™ğ‘™ğ‘›, ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ğ‘›)]. Here, ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ğ‘– is the target of\nğ‘ğ‘ğ‘™ğ‘™ğ‘– , and func is used as an argument in one of the calls. The\ncall chainâ€™s endpoint is either another icall or a point wherefunc\nis assigned to a variable or directly invoked in ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ğ‘›. If it is\nultimately assigned to a variable, we analyze the corresponding\nassignment or initializer following the previous steps to obtain\nthe global context. We then combine this with the call-chain text\n(where each (ğ‘ğ‘ğ‘™ğ‘™ğ‘– , ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ğ‘– ) consists of the text of the correspond-\ning call statement and the function declarator text) to form the\nglobal context for the address-taken site. If it is directly invoked,\nwe combine the call statement with the call-chain text to form\nthe final global context.\nContext Summary. A critical aspect of this process is prompt\ndesign. To date, various prompt strategies [47] have been proposed\nto enhance the performance of LLMs. Among these, task decompo-\nsition [18, 43] and chain-of-thought [44] have proven useful in han-\ndling complex tasks or when dealing with lengthy input sequences.\nConsidering that combining the local and global contexts of the\ncaller and callee may exceed the context length, and given that these\ncontexts describe their specific functionalities and the roles these\nfunction pointers are meant to fulfill, with semantic differences be-\ntween them. We adopt the task decomposition strategy. Specifically,\nwe decompose the caller-callee matching task into two steps: se-\nmantic summarization and matching. Our intuition is that semantic\nsummarization can distill the critical information from the callerâ€™s\nand calleeâ€™s contexts into a concise natural language summary. This\napproach not only reduces the likelihood of excessively long inputs\nbut also minimizes the inclusion of irrelevant information during\nthe matching process. Semantic summarization consists of four\nsteps, and each step uses a prompt to process the callerâ€™s or calleeâ€™s\nglobal or local context and generate the corresponding natural lan-\nguage summary. Each prompt consists of two parts: [context]\nand [instruction]. The [context] is the relevant code snippet\nretrieved from the context database, while the [instruction] is\na sentence that guides the LLM in analyzing the context.\nThe local context for the indirect-call is listed as follows:\n[local context]\nAnalyze the functionality of the indirect call and response with a\nconcise summary.\nNote that a callee function may have multiple address-taken\nsites, and each site may be assigned to variables with different\nsemantics, resulting in different summaries. Therefore, when\nmultiple address-taken sites are present, we use a single prompt\nto combine the summaries of all address-taken sites, as follows:\nThe summaries of each address-taken site for the target function\nare:\n[summary1],\n...\n[summaryn]\nPlease consolidate those summaries.\n3.4 Caller-Callee Match\nAfter obtaining the summaries for the caller and callee, we in-\nstruct the LLM to determine if there is a calling relationship between\nthem using the following prompt:\nSemantic-Enhanced Indirect Call Analysis with Large Language Models ASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States\nTable 1: Basic information of benchmarks. IN denotes the\nnumber of icalls, and VN denotes the number of valid icalls.\nSLoc denotes the number of lines of code\nProject IN VN SLoc Project IN VN SLoc\nbind9 46 45 388k bluez 4 4 450k\ncairo 41 41 242k cyclonedds 47 47 286k\ndovecot 60 39 486k fwupd 31 28 299k\ngdbm 18 18 18k gdk-pixbuf 3 3 88k\nhdf5 94 89 1365k igraph 19 6 275k\nkrb5 18 7 409k libdwarf 202 17 154k\nlibjpeg-turbo 462 209 142k libpg_query 11 11 520k\nlibsndfile 8 8 67k libssh 15 15 96k\nlibrabbitmq 6 6 13k lua 7 2 33k\nlxc 7 7 74k md4c 39 6 23k\nmdbtools 2 2 18k nginx 23 23 629k\nopensips 8 8 603k oniguruma 268 18 95k\npjsip 43 22 809k postfix 4 3 223k\nrtpproxy 161 8 136k selinux 21 21 518k\nsudo 96 95 282k tmux 11 11 73k\nvlc 1065 31 2560k\nThe subsequent text provides the summary of the caller and callee:\n# 1.summary of caller\n## 1.1.[CallerLocalSummary]\n## 1.2.[CallerGlobalSummary]\n# 2.summary of callee\n## 2.1.[CalleeLocalSummary]\n## 2.2.[CalleeGlobalSummary]\nAssess if the caller could invoke callee based on the semantic infor-\nmation given above. Answer with â€˜yesâ€™ or â€˜noâ€™.\nIn this step, the LLM determines whether a real calling rela-\ntionship exists between the caller and each potential callee. After\nobtaining the LLMâ€™s analysis results, we use them to refine the\noutcomes of traditional static analysis methods.\n4 EVALUATION\n4.1 Experimental Settings\nBenchmark Selection We select test programs from the oss-\nfuzz projects [29] and follow their build scripts to download, com-\npile, and dynamically run them with a fuzz testing tool to generate\nground-truth data. During compilation, we implement a custom\nLLVM pass to instrument the programs, enabling the dumping of\ninformation related to the ground truth when indirect calls are\nexecuted. We used AFL++ [48] as the fuzz testing tool. Due to poor\nversion management of many projects under oss-fuzz, some projects\nfailed to compile successfully and are excluded from our benchmark.\nAdditionally, we find that some programs either produced invalid\nresults due to parsing errors or achieved 100% F1 scores using only\nFLTA, leading us to exclude these projects as well. Ultimately, we\ncollected 31 projects for our benchmark. Detailed information is\nlisted in Table 1, which includes the number of icalls presented in\nthe ground-truth dataset. However, many icalls are either wrapped\nin macros or sufferred from other parsing errors, resulting in the\nabsence of valid analysis results. The valid icall number (VN) in-\ndicates the icalls that can be correctly analyzed. The fundamental\nsolution to these issues is to preprocess the project with a compiler\n1 // parsing error that leads MLTA fail\n2 const cairo_font_face_backend_t _cairo_ft_font_face_backend = {\n3 CAIRO_FONT_TYPE_FT ,\n4 #if CAIRO_HAS_FC_FONT\n5 _cairo_ft_font_face_create_for_toy ,\n6 # else\n7 NULL ,\n8 # endif\n9 _cairo_ft_font_face_destroy ,\n10 _cairo_ft_font_face_scaled_font_create ,\n11 _cairo_ft_font_face_get_implementation\n12 };\n13\n14 // parsing error that leads Kelp fail\n15 herr_t\n16 H5G_loc_find ( const H5G_loc_t *loc , const char *name ,\n17 H5G_loc_t * obj_loc ) {\n18 // macro leads to parsing error\n19 FUNC_ENTER_NOAPI ( FAIL )\n20 // ...\n21 if ( H5G_traverse (loc , name , H5G_TARGET_NORMAL , H5G__loc_find_cb ,\n22 & udata ) < 0)\n23 // ...\n24 }\nFigure 7: Parsing errors introduced by conditional compila-\ntion and macro.\nTable 2: Binary classification result for SEA with qwen at\ntemperature 0.5. baseline refers to treating all call edges ana-\nlyzed by the baseline approach as true targets.\nApproach FLTA MLTA KelpA P R F A P R F A P R F\nbaseline17.3 17.3 100.027.3 26.4 26.4 100.038.2 31.4 31.4 100.043.4\nSEA 76.0 41.2 98.9 55.366.5 41.1 98.9 55.364.8 44.2 99.4 57.9\nbefore analyzing the preprocessed code. However, this conflicts\nwith the preset condition of not using compilation options.\nLLM selection Our experiment requires a substantial token ex-\npenditure, rendering the use of SOTA close-source models [49â€“51]\ncostly. Consequently, we primarily focused on leveraging open-\nsource LLMs. Regarding model selection, we focused on instruction-\ntuned [52] models. Based on the intuition that larger models tend\nto perform better, we chose Qwen1.5-72B-Chat [53]1 and LLaMA3-\n70B-Instruct [54] (qwen and llama3 hereafter) for experiments.\nBaseline Approach We consider three static analysis meth-\nodsâ€”FLTA [10], MLTA [ 1], and Kelp [ 30]â€”as baselines. At the\nLLVM IR level, MLTA optimizes FLTAâ€™s results in some cases, and\nKelp further optimizes MLTAâ€™s results. From FLTA to Kelp, precision\nimproves without introducing false negatives. Therefore, we only\nneed to compare with Kelp. However, at the source code level, the\nsituation is more complex. Language features such as conditional\ncompilation and macros can cause parsing errors in the code before\npreprocessing, leading to incorrect fact propagation and resulting\nin false negatives. Figure 7 illustrates one example, where the pres-\nence of the conditional macroCAIRO_HAS_FC_FONT in the initializer\nof the _cairo_ft_font_face_backend variable causes errors in\nAST analysis. This results in the function addresses like _cairo_-\nft_font_face_create_for_toy being incorrectly associated with\nother fields in thecairo_font_face_backend_t structure, leading\nto false negatives in MLTA. Similarly, in the H5G_loc_find func-\ntion, the macro FUNC_NOAPI(FAIL) causes parsing errors in the\nAST beyond the function body, preventing the correct analysis of\nthe H5G_traverse call. Consequently, H5G__loc_find_cb is not\npropagated to its corresponding simple function pointer, resulting\nin false negatives in Kelp. The fundamental solution to these issues\nremains to preprocess the project with a compiler before analyzing\nthe preprocessed code.\n1At the time of our experiment, Qwen1.5-110B had not yet been released. Additionally,\nthe deployment of the 110B large model was beyond our capabilities.\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nSEA Setup We assumed that the caller and all its callees reside\nwithin the same scope. We define this scope as being in the same\ndirectory and its subdirectories as the callerâ€™s source file.\nEvaluation Metrics Three evaluated metrics, i.e., precision (P),\nrecall (R), and F1 (F) are defined as following. For each icall, ğ¶ğ‘ğ‘Ÿ is\nthe callee set predicted by SEA for a given icall, ğ¶ğ‘”ğ‘¡ represents the\ncorresponding callee set in the ground truth.\nğ‘ƒ = |ğ¶ğ‘ğ‘Ÿ âˆ©ğ¶ğ‘”ğ‘¡ |\n|ğ¶ğ‘ğ‘Ÿ | ğ‘… = |ğ¶ğ‘ğ‘Ÿ âˆ©ğ¶ğ‘”ğ‘¡ |\n|ğ¶ğ‘”ğ‘¡ | ğ¹ = 2 Â·ğ‘ƒ Â·ğ‘…\nğ‘ƒ+ğ‘… (1)\nResearch Questions (RQs) Our key RQs are as following:\nâ€¢RQ1: Is SEA effective in enhancing the performance of current\nstatic analysis?\nâ€¢RQ2: To what extent do the global and local context of caller and\ncallee contribute to SEA?\n4.2 Effectiveness of SEA (RQ1)\nWe evaluate the effectiveness of SEA when combined with FLTA,\nMLTA, and Kelp. The combinations work as follows: when com-\nbined with FLTA and MLTA, SEA is used to refine the callee set\nproduced by these analyzers. For Kelp, cases with simple icalls,\nwhich already achieve 100% accuracy, SEA directly use the results\nfrom Kelp. When Kelp falls back to FLTA or MLTA, SEA refines their\nresults. We conduct experiments across five temperature settings:\n0, 0.25, 0.5, 0.75, and 1 with qwen and llama3. In our experiments,\nwe find that SEA demonstrates optimal performance when using\nthe qwen model at a temperature setting of 0.5. Consequently, we\nfirst present SEAâ€™s binary classification performance under this\nsetting, where it predicts each call edge produced by the baseline as\neither a true or false edge. The average results across 31 projects are\nshown in Table 2. Besides P, R, F, accuracy (A, correctly predicted\ncall relations / total predicted call relations) is also presented. We\ncan observe that, compared to treating all call edges predicted by\nthe baselines as true, SEA achieves a significant improvement in\nF1 score at the cost of approximately 1% recall. Specifically, SEA\nenhances F1 by 14.5% for Kelp, 17.1% for MLTA, and 28% for FLTA.\nThe diminishing performance improvement from FLTA to Kelp\nsuggests that these baseline methods also prune some false targets,\nthereby reducing SEAâ€™s potential for enhancement. Furthermore,\nSEAâ€™s precision, at approximately 41%, is partly influenced by the\ndataset. The presence of false negatives in the dataset significantly\nimpacts the binary classification results.\nTo further understand the performance of SEA, we conduct a\nfull evaluation and an exclusive evaluation. For the full evalua-\ntion, the results represent the average icall analysis effectiveness\nacross 31 projects. For the exclusive evaluation, we first categorize\nall icalls in the benchmark into three scenarios: FLTA-exclusive,\nMLTA-exclusive, and Kelp-exclusive. FLTA-exclusive denotes icalls\nanalyzable only by FLTA, MLTA-exclusive denotes icalls refined\nby MLTA but not Kelp, and Kelp-exclusive denotes icalls refined\nby Kelp. Since Kelp already achieves 100% accuracy, evaluating\nSEAâ€™s performance improvement on Kelp-exclusive cases is unnec-\nessary. Therefore, we focus on its effectiveness in FLTA-exclusive\nand MLTA-exclusive scenarios. In the ground truth, these categories\ncomprise 343, 373, and 82 cases, respectively. After filtering out\ninvalid cases due to parsing errors where FLTAâ€™s analysis results\ndo not include labeled true targets, we have 294 FLTA-exclusive\nand 359 MLTA-exclusive cases. The exclusive evaluation results\nrepresent the average performance of SEA on all FLTA-exclusive\nand MLTA-exclusive cases.\nThe full and exclusive evaluation results are presented in\nTable 3. It is observed that the recall results of the baselines are\nless than 100%, which is attributed to parsing errors in the code.\nThrough the analysis of the full evaluation results, we observe\nthat when combined with FLTA, SEA demonstrates a significant\nperformance gain. When using qwen as the main LLM, the F1 score\nincreases from 34.8% to between 58.5% and 59.4%, with a maximum\nrecall loss of only 1.5%. Although there is a slight decline in\nperformance when using llama3 as the main LLM, we still observe\nan improvement in the F1 score of at least 21%. However, when\ncombined with MLTA, the gain in SEAâ€™s F1 score decreases to\n7.9%-8.1% using qwen and 6.5%-7.8% using llama3. When combined\nwith Kelp, the gain is only around 6%.\nThrough the analysis of the exclusive evaluation results, we can\nobserve a significant improvement in SEAâ€™s performance in the\nFLTA-exclusive cases. When using qwen, the F1 score increases\nfrom 38.2% to 66.4%-67.2%, while with llama3, the F1 score still rises\nto 62.6%-63.7%. However, in the MLTA-exclusive cases, we observe\nthat SEA brings almost no improvement, indicating SEA struggles\nto further optimize cases already refined by MLTA. We believe the\nunderlying reason is that, in a certain sense, MLTA functions as a\nheuristic-based semantic analysis approach.\nWe illustrate this with Figure 4. In the global, the function\nngx_http_log_create_main_conf is assigned to the create_-\nmain_conf field of ngx_http_log_module_ctx. In this exam-\nple, MLTA successfully filters out a batch of false positives\nthrough the confinement between field create_main_conf of\nstruct ngx_http_log_module_ctx and function ngx_http_log_-\ncreate_main_conf, while SEA eliminates the same false positives\nthrough the semantic similarity between them. Therefore, SEA does\nnot demonstrate a noticeable advantage in the MLTA-cases.\nRegarding the FLTA-exclusive cases, We further categorizing the\noutcomes on these cases into three types: performance decrease,\nno change in performance, and performance improvement. Con-\nsidering the potential inconsistency in LLM analysis results and\nqwenâ€™s superior performance compared to llama3, we focused on\nqwenâ€™s analysis. We find that during five times analysis, two cases\nexperience a decrease in the F1 score, 61 cases show no change,\nand 206 cases consistently show an improvement in the F1 score.\nThe reason SEA can enhance FLTAâ€™s performance is that it lever-\nages semantic information to filter out semantically irrelevant false\npositives. The motivating example in Figure 2 illustrates this well,\nwhere false targets are easily filtered out due to their complete\nfunctional dissimilarity with the call. SEA captures this semantic\ndifference and filters out the false positive. In addition, we also\nobserve for a FLTA-exclusive case within the cairo project, the\nbaseline approach identifies 208 callee targets, SEA successfully\nfilter out the sole true target from this set. Due to page limitations,\nmore case studies can be found at our repository [31].\nFor the 61 cases with no change in performance, we find that\nin 41 cases, FLTA alone achieves 100% F1, indicating no room for\nSemantic-Enhanced Indirect Call Analysis with Large Language Models ASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States\nTable 3: icall analysis results for different setups: FLTA, MLTA, and Kelp denote the full evaluation, while FLTA-exclusive and\nMLTA-exclusive denote exclusive evaluations.\nModel Temperature FLTA MLTA Kelp FLTA-exclusive MLTA-exclusive\nP R F P R F P R F P R F P R F\nqwen\n0 48.7 96.9 59.0 52.0 96.5 61.3 54.9 96.6 63.7 59.3 97.5 66.4 52.9 98.0 61.6\n0.25 48.8 97.3 59.1 51.9 96.8 61.4 54.8 96.9 63.7 59.4 97.7 66.4 52.9 98.0 61.6\n0.5 49.1 97.3 59.4 51.9 96.9 61.4 55.0 97.0 63.9 60.6 97.2 67.2 53.3 97.8 62.0\n0.75 49.3 97.2 59.4 52.2 96.8 61.5 54.9 96.9 63.7 60.5 97.1 66.9 52.9 98.0 61.6\n1 48.5 96.4 58.6 51.4 96.0 60.6 55.0 96.7 63.6 60.3 97.4 66.7 53.2 97.4 61.7\nllama3\n0 46.5 95.9 56.6 51.4 95.5 60.5 55.0 96.2 63.6 57.4 96.9 63.7 52.9 98.0 61.6\n0.25 47.0 96.6 57.1 51.6 96.2 60.8 55.0 96.2 63.6 56.8 96.9 63.4 53.0 97.7 61.7\n0.5 45.9 96.2 56.2 51.6 95.8 60.6 55.3 96.4 63.8 57.2 97.2 63.9 52.9 98.0 61.6\n0.75 45.4 96.0 55.8 50.6 95.6 59.9 54.3 96.3 63.1 55.8 96.6 62.6 52.9 97.9 61.7\n1 46.3 96.9 56.6 52.1 96.4 61.2 55.1 96.5 63.7 56.8 97.6 63.6 52.8 97.5 61.5\nbaseline (traditional analysis)26.1 97.9 34.8 45.1 97.4 53.4 49.2 97.4 57.5 31.4 99.4 38.2 52.9 98.0 61.6\ncaller contextintlxc_set_config_item_locked(structlxc_conf *conf,  constchar*key,constchar*v) {â€¦config = lxc_get_config(key);ret = config->set(key, v, conf, NULL);â€¦.}\ncaller summaryLocal: The caller function responsible for setting a configuration item for a Linux Containers (LXC).The caller itself invoke the specific configuration setting logic â€¦â€¦\nAnalysis ResultThe callersuggests a generic configuration setting function.set_config_cgroup2_controller seems to setting cgroup2 controller configurations in LXC container.Based on the provided information, we cannot conclusively determine caller can invoke callee, more information is needed.\ncallee contextstaticintset_config_cgroup2_controller( constchar*key, constchar*value,structlxc_conf *lxc_conf, void*data) { â€¦}\ncallee summaryThe set_config_cgroup2_controller function is likely used to set the configuration for a cgroup2 controller in a container\nFigure 8: Example of removing local context.\nfurther improvement. Regarding the remaining 21 cases, our man-\nual analysis reveals that the callees filtered by FLTA had strong\nsemantic similarities to the callers and were very likely to be in-\nvoked. However, due to the complexity of the call relationships of\nthe programs and considering the possibility of false negatives in\nthe ground truth, we cannot be entirely certain about the presence\nor extent of false positives. The failed cases on the one hand may\nbe related to the hallucination issue [ 41] of LLMs. On the other\nhand, they can be attributed to ambiguous semantic information\nbetween the caller and the callee. For example, in a specific case\nin vlc project, the callerâ€™s context indicates that it is used to test\nthe moduleâ€™s suitability, while the calleeâ€™s context suggests it func-\ntions as an activate function, which is not related to the module.\nConsequently, the semantic analysis fails. Nevertheless, we can still\nconsider SEA to be quite effective in enhancing traditional methods.\nANSWER: SEA substantially improves the effectiveness of\ntraditional indirect call methods, increasing the F1 score by at\nleast 20% when combined with FLTA. SEA achieves a F1 score\nincrease of up to nearly 30% in the FLTA-exclusive cases.\nThis improvement stems from its effective use of the LLMâ€™s\nsemantic understanding to distinguish true/false callees.\n4.3 Ablation Study (RQ2)\nTo further understand the contribution of local and global contexts\nto SEA, we analyzed this through three control groups: wo-local\nâ€“ using only the global context of the caller and callee; wo-global\nâ€“ using only the local context of the caller and callee; and wo-all\nâ€“ not using any local and global context, matching directly based\non the caller statement and callee name.\nThe ablation study mostly follows the evaluation setup outlined\nin RQ1 (Â§ 4.2) but only evaluates on the best temperature to save\nlocal summaryngx_http_log_create_main_conffunction creates and initializes a main configuration structure for HTTP logging, allocating memory for it, initializing an array for log formats,adding a default \"combined\" format, and creating an array for log operations.\nglobal summaryngx_http_log_create_main_confis to create the main configuration for the HTTP log module. It is part of the ngx_http_module_tstruct instance ngx_http_log_module_ctx, which defines the lifecycle callbacks for the module configuration within the nginxweb server.\nFigure 9: Similarities and differences of local/global contexts.\nevaluation cost. For qwen, the optimal value is 0.5. For llama3, the\noptimal temperature varies depending on the baseline combination.\nNotably, when combined with Kelp and analyzed exclusively in\nthe FLTA cases, the best performance is observed at 0.5. Therefore,\nwe select this temperature setting for the ablation study. Taking\nthese findings into account, we conduct the ablation study using\nboth qwen and llama3 at temperature of 0.5.\nThe experiment results are depicted in Table 4. From this, we\nobserved that for qwen, removing both local and global contexts\nsignificantly degrades SEAâ€™s performance. However, for llama3, its\nrecall is much higher compared to qwen. Upon analyzing the results,\nwe find that this difference is due to llama3â€™s tendency to consis-\ntently answer yes when provided with insufficient information,\nwhile qwen exhibits more randomness in its responses. In addition,\nthe impact of eliminating global context is more significant than\nthat of deleting local context, the performance degradation caused\nby removing the local context is less noticeable. Upon contrasting\nthis performance degradation with the results presented in Table 3,\nwe observe a consistent and significant drop in performance across\nall tested temperature values. This finding effectively mitigate the\npotential influence of randomness.\nThese findings indicate that both global and local contexts are\nessential for icall analysis. However, the comparison between the\noriginal and the wo-global group suggests that there is some overlap\nbetween global and local contexts. To further illustrate this issue, we\nanalyze two specific cases. The first example, illustrated in Figure 8,\nhighlights that in certain cases, the local context can effectively\nsupplement the global context, ensuring a more accurate interpre-\ntation. Here, the callerâ€™s local context indicates its responsibility\nfor setting configurations for a Linux container. When this local\ncontext is omitted during matching, the LLM interprets the func-\ntion as a generic configuration setting function and subsequently\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nTable 4: Ablation Study Under Temperature of 0.5.\nmodel group FLTA MLTA Kelp\nP R F P R F P R F\nqwen\norigin 49.1 97.3 59.4 51.9 96.9 61.4 55.0 97.0 63.9\nwo-local47.7 95.5 57.5 51.3 95.1 60.1 54.1 95.4 62.6\nwo-global44.7 87.3 53.4 48.4 86.9 56.1 52.2 88.8 59.8\nwo-all 38.8 58.8 41.9 42.2 58.4 44.2 46.9 68.4 50.5\nllama3\norigin 45.9 96.2 56.2 51.6 95.8 60.6 55.3 96.4 63.8\nwo-local43.7 97.7 53.8 49.5 97.3 58.6 53.4 97.3 62.1\nwo-global43.7 89.1 52.6 49.4 88.7 57.1 52.5 90.2 60.3\nwo-all 33.4 95.9 43.1 46.9 95.5 55.4 51.1 96.3 59.7\nindicates that more information is needed. The second example,\nillustrated in Figure 9, presents the local and global summaries for\nthe â€œngx_http_log_create_main_conf â€ function shown in Figure 4.\nThe yellow-highlighted parts indicate similarities between the local\nand global summaries, while the green-highlighted parts denote\ndifferences. Both summaries describe the function as creating the\nmain configuration for the HTTP log module. However, the lo-\ncal summary provides a more detailed description of its specific\nfunctionality, whereas the global summary offers a higher-level\ndescription of the functionâ€™s role within the lifecycle of an Nginx\nmodule. Because both the local and global summaries capture the\ncore functionality of the function, the removal of either context in\nthis case does not significantly impact SEAâ€™s performance.\nANSWER: Both local/global contexts contribute to SEAâ€™s\nperformance, as they describe the caller and callee from\ndifferent angles. However, there is a certain degree of overlap\nin the information they provide. This indicates that when\nencountering parsing errors leading to partial information\nloss, SEA still has the potential to make correct analyses.\n5 DISCUSSIONS\nContext Sensitivity In this work, we follow previous studies to\nperform context-insensitive indirect call analysis. This approach\ncan enhance tasks such as code navigation and improve bug detec-\ntion and other inter-procedural analyses, similar to prior studies.\nHowever, unlike code navigation, a context-sensitive and more pre-\ncise call analysis can further reduce false positives in static bug anal-\nysis. Therefore, an interesting future direction for inter-procedural\nbug detection would be to incorporate the function call chains\ninvolved in current indirect calls to perform a context-sensitive\nindirect call analysis.\nTransferrability among Programming Languages Currently\nwe only evaluated SEA in C language, in theory, our method can be\nadapted to other languages such as C++/Java/Python. However, the\ndiverse features of various languages may introduce new challenges\nto the application of SEA. For example, features like higher-order\nfunctions, reflection, and dynamic typing in Python can complicate\nthe collection of global context, requiring adjustments to our con-\ntext collection strategy. Therefore, adapting it to each languageâ€™s\nunique features still require careful consideration.\n6 THREATS TO VALIDITY\nInternal Validity To measure the effectiveness of SEA, we fol-\nlowed previous works [26, 55, 56] by using dynamic execution to\ngenerate ground truth. However, due to inherent limitation of dy-\nnamic testing, the generated ground truth inevitably includes false\nnegatives, and a more comprehensive large-scale manual labeling re-\nquires substantial effort. Conclusively, the biased dataset may nega-\ntively impact our experimental results. To mitigate this concern, we\nsupplemented our evaluation with manual analysis for certain cases.\nExternal Validity External influences mainly arise from the\nfollowing aspects: LLM selection, dataset, and compilation envi-\nronment. For LLM selection, we choose the open-source models\nLlama3 and Qwen1.5 for our experiments. However, as LLMs con-\ntinue to evolve, both the open-source community and commercial\nplatforms regularly update or release new models, which typically\ndemonstrate enhanced capabilities. Consequently, some of the anal-\nysis results in this paper may need to be updated in the future.\nRegarding the dataset, we select 31 projects from OSS-Fuzz as our\nbenchmark. However, since these are relatively popular projects,\nthey are likely to have been included in the training data of the\nLLMs. Therefore, it is unclear whether our experimental results\ncan be generalized to the newest projects, especially those that are\nunlikely to appear in the training data. For the compilation environ-\nment, we conduct our experiments exclusively in a no-compilation\nenvironment. The results demonstrate that SEA is highly effective\nfor certain source code-level tasks. However, it remains unclear\nhow much improvement our tool offers over traditional tools when\nextended to environments with compilation options.\n7 RELATED WORK\n7.1 Static Call Graph Analysis\nType Analysis For icall Type analysis [ 11, 57] considers all\naddress-taken functions sharing identical function signatures\nwith indirect calls as potential callees. Due to its straightforward\nprinciples, it can be efficiently applied in control flow integrity\nschemes [9, 10, 12, 58], enabling scalability to millions of lines of\ncode within minutes. Subsequent works [1, 10, 59], while matching\nfunction signatures, concurrently consider the structure hierarchy\nof function pointers, leading to a substantial decrease in false\npositives. Nevertheless, type analysis-based approaches continue\nto yield many false positives [30, 60].\nPointer Analysis For icalls Pointer analysis [ 7, 61] treat\nfunction pointers as ordinary address-taken variables and aim to\nidentify all address-taken functions aliased with them. Depending\non the analysis dimensions, pointer analysis can be categorized\ninto field-sensitive/insensitive, flow-sensitive/insensitive, and\ncontext-sensitive/insensitive, as well as inclusion-based and\nunification-based methods. Generally, algorithms that consider\nmore sensitive analyses tend to yield more accurate results but\nalso incur higher performance overhead, leading to the â€œpointer\ntrapâ€ problem [62]. To mitigate performance overhead, on-demand\npointer analysis approaches [ 61â€“64] have been proposed and\nhave shown promising results. Nevertheless, scaling up existing\nprecise demand-driven pointer analysis to hundreds of thousands\nof lines of code remains challenging. In a recent study, conducted\nby Y. Cai et al. [30], it was revealed that within the Linux kernel\n(version 5.15), 34.5% of indirect calls are made via simple function\npointers, while 23.9% of address-taken functions are exclusively\ninvoked through simple indirect calls. Consequently, they devised\na regional def-use analysis approach named Kelp, which effectively\nSemantic-Enhanced Indirect Call Analysis with Large Language Models ASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States\ndiminishes erroneous callees by 54.2%, while incurring only a\nnegligible additional time cost of 8.5%.\nDeep Learning-Enhanced Call Graph Analysis In addition to\nthese two lines of methodologies, Zhu et al. [56] proposed a distinc-\ntive approach to icall analysis through similarity matching with a\ndeep learning model, introducing a novel perspective and providing\nan alternative avenue for enhancing the precision of icall analyses.\nIn addition to icall analysis, another noteworthy area is call graph\nanalysis in object-oriented languages such as Java. To reduce false\npositives produced by static analysis tools, Akshay et al. [55] pro-\nposed a method based on structural features. They extracted struc-\ntural feature vectors from the call graph and then used a trained\nclassifier to determine whether a caller-callee pair exists. Thanh et\nal. [26] later extended this approach by incorporating the semantic\nfeatures of the caller and callee functionsâ€™ code into the structural\nfeatures, thereby improving performance. Although proven effec-\ntive, these methods require labeled datasets for model training.\nHowever, acquiring a substantial dataset labeled with caller-callee\npairs is exceedingly challenging for our task. Fortunately, the emer-\ngence of LLMs presents new opportunities to address this challenge.\n7.2 LLM-assisted Program Analysis\nThe emergence of LLMs has presented numerous new opportunities\nfor program analysis. Over the past year, LLMs have made notable\nadvancements in assisting fuzzing [23â€“25], static vulnerability de-\ntection [19, 21], and bug report analysis [ 18]. This is attributed\nto the powerful code understanding capabilities of LLMs, which\nreduce the dependency on heuristic rules in code analysis tools,\nthereby enhancing their performance. Additionally, LLMs have\nbeen leveraged to augment various stages of the software develop-\nment lifecycle. For Instance, LLMs have enhanced automated code\ngeneration [27] and refactoring tasks [65], streamlining software\nmaintenance efforts. Overall, integrating LLMs into program anal-\nysis methodologies holds great potential for improving software\nsystemsâ€™ reliability, security, and efficiency.\n8 CONCLUSION\nIn this paper, we proposed SEA, a method that optimizes tradi-\ntional static indirect call analysis through semantic analysis. By\nleveraging LLMs to understand the icall context, SEA effectively\nfilters out semantically irrelevant caller-callee pairs from the static\nanalysis results. Our experiments show that SEA refines the target\nset produced by static analysis methods through its code semantic\nunderstanding, which indicates a promising direction for semantic-\nenhanced program analysis.\nREFERENCES\n[1] Kangjie Lu and Hong Hu. Where does it go? refining indirect-call targets with\nmulti-layer type analysis. In Proceedings of the 2019 ACM SIGSAC Conference on\nComputer and Communications Security , pages 1867â€“1881, 2019.\n[2] SourceGraph. https://sourcegraph.com/.\n[3] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen.\nSysevr: A framework for using deep learning to detect software vulnerabilities.\nIEEE Transactions on Dependable and Secure Computing , 19(4):2244â€“2258, 2021.\n[4] Baijun Cheng, Kailong Wang, Cuiyun Gao, Xiapu Luo, Yulei Sui, Li Li, Yao Guo,\nXiangqun Chen, and Haoyu Wang. The vulnerability is in the details: Locating\nfine-grained information of vulnerable code identified by graph-based detectors.\narXiv preprint arXiv:2401.02737 , 2024.\n[5] Baijun Cheng, Shengming Zhao, Kailong Wang, Meizhen Wang, Guangdong Bai,\nRuitao Feng, Yao Guo, Lei Ma, and Haoyu Wang. Beyond fidelity: Explaining\nvulnerability localization of learning-based detectors. ACM Trans. Softw. Eng.\nMethodol., 33(5), jun 2024.\n[6] Konrad Weiss and Christian Banse. A language-independent analysis platform\nfor source code. arXiv preprint arXiv:2203.08424 , 2022.\n[7] Yulei Sui and Jingling Xue. Svf: interprocedural static value-flow analysis in llvm.\nIn Proceedings of the 25th international conference on compiler construction , pages\n265â€“266, 2016.\n[8] Kai Cheng, Yaowen Zheng, Tao Liu, Le Guan, Peng Liu, Hong Li, Hongsong Zhu,\nKejiang Ye, and Limin Sun. Detecting vulnerabilities in linux-based embedded\nfirmware with sse-based on-demand alias analysis. In Proceedings of the 32nd\nACM SIGSOFT International Symposium on Software Testing and Analysis , pages\n360â€“372, 2023.\n[9] Caroline Tice, Tom Roeder, Peter Collingbourne, Stephen Checkoway, Ãšlfar\nErlingsson, Luis Lozano, and Geoff Pike. Enforcing {Forward-Edge}{Control-\nFlow}integrity in {GCC}& {LLVM}. In 23rd USENIX security symposium\n(USENIX security 14) , pages 941â€“955, 2014.\n[10] Jinku Li, Xiaomeng Tong, Fengwei Zhang, and Jianfeng Ma. Fine-cfi: fine-grained\ncontrol-flow integrity for operating system kernels. IEEE Transactions on Infor-\nmation Forensics and Security , 13(6):1535â€“1550, 2018.\n[11] Markus Bauer, Ilya Grishchenko, and Christian Rossow. Typro: Forward cfi for\nc-style indirect function calls using type propagation. In Proceedings of the 38th\nAnnual Computer Security Applications Conference , pages 346â€“360, 2022.\n[12] Reza Mirzazade Farkhani, Saman Jafari, Sajjad Arshad, William Robertson, En-\ngin Kirda, and Hamed Okhravi. On the effectiveness of type-based control\nflow integrity. In Proceedings of the 34th Annual Computer Security Applications\nConference, pages 28â€“39, 2018.\n[13] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao\nChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation\nof large language models.ACM Transactions on Intelligent Systems and Technology ,\n15(3):1â€“45, 2024.\n[14] Jiahao SHEN, Ke JIANG, and Xiaoyang TAN. Boundary data augmentation for\noffline reinforcement learning. ZTE Communications , 21(3):29, 2023.\n[15] Salvatore Claudio Fanni, Maria Febi, Gayane Aghakhanyan, and Emanuele Neri.\nNatural language processing. In Introduction to Artificial Intelligence , pages 87â€“99.\nSpringer, 2023.\n[16] Felix Stahlberg. Neural machine translation: A review. Journal of Artificial\nIntelligence Research, 69:343â€“418, 2020.\n[17] Daiyi LI, Yaofeng TU, Xiangsheng ZHOU, Yangming ZHANG, and Zongmin\nMA. End-to-end chinese entity recognition based on bert-bilstm-att-crf. ZTE\nCommunications, 20(S1):27, 2022.\n[18] Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian. Enhancing static analysis for\npractical bug detection: An llm-integrated approach. Proc. ACM Program. Lang. ,\n8(OOPSLA1), apr 2024.\n[19] Chong Wang, Jianan Liu, Xin Peng, Yang Liu, and Yiling Lou. Boosting static\nresource leak detection via llm-based resource-oriented intention inference.arXiv\npreprint arXiv:2311.04448, 2023.\n[20] Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng\nWang, Zhi Li, and Limin Sun. Harnessing the power of llm to support binary\ntaint analysis. arXiv preprint arXiv:2310.08275 , 2023.\n[21] Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei\nXie, and Yang Liu. Gptscan: Detecting logic vulnerabilities in smart contracts\nby combining gpt with program analysis. In Proceedings of the IEEE/ACM 46th\nInternational Conference on Software Engineering , ICSE â€™24, New York, NY, USA,\n2024. Association for Computing Machinery.\n[22] Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie, and Xi-\nangyu Zhang. When dataflow analysis meets large language models, 2024.\n[23] Chenyuan Yang, Zijie Zhao, and Lingming Zhang. Kernelgpt: Enhanced kernel\nfuzzing via large language models. arXiv preprint arXiv:2401.00563 , 2023.\n[24] Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing\nYang, and Lingming Zhang. Large language models are edge-case generators:\nCrafting unusual programs for fuzzing deep learning libraries. In Proceedings of\nthe 46th IEEE/ACM International Conference on Software Engineering , ICSE â€™24,\nNew York, NY, USA, 2024. Association for Computing Machinery.\n[25] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming\nZhang. Large language models are zero-shot fuzzers: Fuzzing deep-learning\nlibraries via large language models. In Proceedings of the 32nd ACM SIGSOFT\ninternational symposium on software testing and analysis , pages 423â€“435, 2023.\n[26] Thanh Le-Cong, Hong Jin Kang, Truong Giang Nguyen, Stefanus Agus Haryono,\nDavid Lo, Xuan-Bach D Le, and Quyet Thang Huynh. Autopruner: transformer-\nbased call graph pruning. In Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering ,\npages 520â€“532, 2022.\n[27] Yichen Li, Yun Peng, Yintong Huo, and Michael R Lyu. Enhancing llm-based\ncoding tools through native integration of ide-derived static context. arXiv\npreprint arXiv:2402.03630, 2024.\n[28] Yangruibo Ding, Zijian Wang, Wasi U Ahmad, Murali Krishna Ramanathan,\nRamesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Cocomic: Code\ncompletion by jointly modeling in-file and cross-file context. In Proceedings of\nASE 2024, 27 October - 1 November, 2024, Sacramento, California, United States Trovato and Tobin, et al.\nthe 2024 Joint International Conference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024) , pages 3433â€“3445, 2024.\n[29] Oliver Chang, Jonathan Metzman, Max Moroz, Martin Barbella, and Abhishek\nArya. Oss-fuzz: Continuous fuzzing for open source software. URL: https://github.\ncom/google/ossfuzz, 2016.\n[30] Yuandao Cai, Yibo Jin, and Charles Zhang. Unleashing the power of type-based\ncall graph construction by using regional pointer information. In 33nd USENIX\nSecurity Symposium (USENIX Security 24) , 2024.\n[31] artifact. https://github.com/for-just-we/CodeAnalyzer.\n[32] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. Can\nlarge language models reason about program invariants? In International Confer-\nence on Machine Learning , pages 27496â€“27520. PMLR, 2023.\n[33] Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang\nLiu, Guoai Xu, Guosheng Xu, and Haoyu Wang. Digger: Detecting Copyright\nContent Mis-usage in Large Language Model Training, 2024.\n[34] Yuxi Li, Yi Liu, Yuekang Li, Ling Shi, Gelei Deng, Shengquan Chen, and Kailong\nWang. Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipula-\ntion, 2024.\n[35] Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, and Yang Liu.\nPandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning. In\nNDSS-AISCC, 2024.\n[36] Guanyu Wang, Yuekang Li, Yi Liu, Gelei Deng, Tianlin Li, Guosheng Xu, Yang Liu,\nHaoyu Wang, and Kailong Wang. MeTMaP: Metamorphic Testing for Detecting\nFalse Vector Matching Problems in LLM Augmented Generation. InFORGE, page\n12â€“23, 2024.\n[37] Yuxi Li, Yi Liu, Gelei Deng, Ying Zhang, Wenjia Song, Ling Shi, Kailong Wang,\nYuekang Li, Yang Liu, and Haoyu Wang. Glitch Tokens in Large Language Models:\nCategorization Taxonomy and Effective Detection. In FSE, 2024.\n[38] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu\nWang, Yan Zheng, and Yang Liu. Prompt Injection attack against LLM-integrated\nApplications, 2023.\n[39] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu\nWang, Tianwei Zhang, and Yang Liu. MasterKey: Automated Jailbreak Across\nMultiple Large Language Model Chatbots. In NDSS, 2024.\n[40] Ningke Li, Yuekang Li, Yi Liu, Ling Shi, Kailong Wang, and Haoyu Wang.\nDrowzee: Metamorphic Testing for Fact-conflicting Hallucination Detection\nin Large Language Models. In OOPSLA (To Appear) , 2024.\n[41] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in\nnatural language generation. ACM Comput. Surv. , 55(12), mar 2023.\n[42] Xiao Yu, Lei Liu, Xing Hu, Jacky Wai Keung, Jin Liu, and Xin Xia. Fight fire with\nfire: How much can we trust chatgpt on source code-related tasks?arXiv preprint\narXiv:2405.12641, 2024.\n[43] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. Decomposed prompting: A modular approach\nfor solving complex tasks. In The Eleventh International Conference on Learning\nRepresentations, 2023.\n[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems ,\n35:24824â€“24837, 2022.\n[45] bind9. https://gitlab.isc.org/isc-projects/bind9.\n[46] The LLVM Compiler Infrastructure. https://llvm.org/.\n[47] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\nand Aman Chadha. A systematic survey of prompt engineering in large language\nmodels: Techniques and applications. arXiv preprint arXiv:2402.07927 , 2024.\n[48] Andrea Fioraldi, Dominik Maier, Heiko EiÃŸfeldt, and Marc Heuse. AFL++: Com-\nbining incremental steps of fuzzing research. In 14th USENIX Workshop on\nOffensive Technologies (WOOT 20) . USENIX Association, August 2020.\n[49] Chatgpt: Optimizing language models for dialogue. https://chatgpt.com/.\n[50] Anthropic (2023). 2023. Claude 2. https://www.anthropic.com/news/claude-2.\n[51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023.\n[52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Train-\ning language models to follow instructions with human feedback. Advances in\nneural information processing systems , 35:27730â€“27744, 2022.\n[53] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji\nLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,\nXingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng\nWang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\nZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report.\narXiv preprint arXiv:2309.16609 , 2023.\n[54] AI@Meta. Llama 3 model card. 2024.\n[55] Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg. Strik-\ning a balance: pruning false-positives from static call graphs. In Proceedings of\nthe 44th International Conference on Software Engineering , pages 2043â€“2055, 2022.\n[56] Wenyu Zhu, Zhiyao Feng, Zihan Zhang, Jianjun Chen, Zhijian Ou, Min Yang,\nand Chao Zhang. Callee: Recovering call graphs for binaries with transfer and\ncontrastive learning. In 2023 IEEE Symposium on Security and Privacy (SP) , pages\n2357â€“2374. IEEE, 2023.\n[57] Darren C Atkinson. Accurate call graph extraction of programs with function\npointers using type signatures. In 11th Asia-Pacific Software Engineering Confer-\nence, pages 326â€“335. IEEE, 2004.\n[58] Ben Niu and Gang Tan. Modular control-flow integrity. In Proceedings of the 35th\nACM SIGPLAN Conference on Programming Language Design and Implementation ,\npages 577â€“587, 2014.\n[59] Kangjie Lu, Aditya Pakki, and Qiushi Wu. Detecting {Missing-Check}bugs via\nsemantic-and {Context-Aware}criticalness and constraints inferences. In 28th\nUSENIX Security Symposium (USENIX Security 19) , pages 1769â€“1786, 2019.\n[60] Seyedhamed Ghavamnia, Tapti Palit, Shachee Mishra, and Michalis Polychronakis.\nTemporal system call specialization for attack surface reduction. In 29th USENIX\nSecurity Symposium (USENIX Security 20) , pages 1749â€“1766, 2020.\n[61] Yulei Sui and Jingling Xue. On-demand strong update analysis via value-flow\nrefinement. In Proceedings of the 2016 24th ACM SIGSOFT international symposium\non foundations of software engineering , pages 460â€“473, 2016.\n[62] Qingkai Shi, Xiao Xiao, Rongxin Wu, Jinguo Zhou, Gang Fan, and Charles Zhang.\nPinpoint: Fast and precise sparse value flow analysis for million lines of code.\nIn Proceedings of the 39th ACM SIGPLAN Conference on Programming Language\nDesign and Implementation , pages 693â€“706, 2018.\n[63] Manu Sridharan, Denis Gopan, Lexin Shan, and Rastislav BodÃ­k. Demand-driven\npoints-to analysis for java. ACM SIGPLAN Notices , 40(10):59â€“76, 2005.\n[64] Yulei Sui and Jingling Xue. Value-flow-based demand-driven pointer analysis for\nc and c++. IEEE Transactions on Software Engineering , 46(8):812â€“835, 2018.\n[65] Dorin Pomian, Abhiram Bellur, Malinda Dilhara, Zarina Kurbatova, Egor Bogo-\nmolov, Timofey Bryksin, and Danny Dig. Together we go further: Llms and ide\nstatic analysis for extract method refactoring. arXiv preprint arXiv:2401.15298 ,\n2024.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7840286493301392
    },
    {
      "name": "Natural language processing",
      "score": 0.5007989406585693
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3508254885673523
    }
  ]
}