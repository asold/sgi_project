{
  "title": "CALYPSO: LLMs as Dungeon Master's Assistants",
  "url": "https://openalex.org/W4387394580",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2608345035",
      "name": "Andrew Zhu",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2122539845",
      "name": "Lara, Martin",
      "affiliations": [
        "University of Maryland, Baltimore County"
      ]
    },
    {
      "id": "https://openalex.org/A2113910852",
      "name": "Andrew Head",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A4207984947",
      "name": "Chris Callison-Burch",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205401527",
    "https://openalex.org/W3093524609",
    "https://openalex.org/W3003404702",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3165482393",
    "https://openalex.org/W4225012671",
    "https://openalex.org/W3180374035",
    "https://openalex.org/W4308759793",
    "https://openalex.org/W4363651284",
    "https://openalex.org/W4306759153",
    "https://openalex.org/W4300121755",
    "https://openalex.org/W2804808697",
    "https://openalex.org/W2941016074",
    "https://openalex.org/W4206538586",
    "https://openalex.org/W3035259249",
    "https://openalex.org/W6689457204",
    "https://openalex.org/W2536781342",
    "https://openalex.org/W1818553245",
    "https://openalex.org/W3161612798",
    "https://openalex.org/W4312091000",
    "https://openalex.org/W4367860615",
    "https://openalex.org/W3103662468",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385570693",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W47529128",
    "https://openalex.org/W4362655976",
    "https://openalex.org/W4390709437",
    "https://openalex.org/W2236262502",
    "https://openalex.org/W4385570947",
    "https://openalex.org/W4306679410",
    "https://openalex.org/W2938704169"
  ],
  "abstract": "The role of a Dungeon Master, or DM, in the game Dungeons &amp; Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&amp;D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to CALYPSO, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see CALYPSO as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.",
  "full_text": "CALYPSO : LLMs as Dungeon Masters’ Assistants\nAndrew Zhu1, Lara Martin2*, Andrew Head1, Chris Callison-Burch1\n1University of Pennsylvania\n2University of Maryland, Baltimore County\n{andrz, head, ccb}@seas.upenn.edu, laramar@umbc.edu\nAbstract\nThe role of a Dungeon Master, or DM, in the game Dun-\ngeons & Dragons is to perform multiple tasks simultaneously.\nThe DM must digest information about the game setting and\nmonsters, synthesize scenes to present to other players, and\nrespond to the players’ interactions with the scene. Doing\nall of these tasks while maintaining consistency within the\nnarrative and story world is no small feat of human cogni-\ntion, making the task tiring and unapproachable to new play-\ners. Large language models (LLMs) like GPT-3 and ChatGPT\nhave shown remarkable abilities to generate coherent natural\nlanguage text. In this paper, we conduct a formative evalua-\ntion with DMs to establish the use cases of LLMs in D&D\nand tabletop gaming generally. We introduce C ALYPSO , a\nsystem of LLM-powered interfaces that support DMs with in-\nformation and inspiration specific to their own scenario. CA-\nLYPSO distills game context into bite-sized prose and helps\nbrainstorm ideas without distracting the DM from the game.\nWhen given access to CALYPSO , DMs reported that it gener-\nated high-fidelity text suitable for direct presentation to play-\ners, and low-fidelity ideas that the DM could develop further\nwhile maintaining their creative agency. We see C ALYPSO\nas exemplifying a paradigm of AI-augmented tools that pro-\nvide synchronous creative assistance within established game\nworlds, and tabletop gaming more broadly.\nIntroduction\nDungeons & Dragons (D&D) (Gygax and Arneson 1974)\nis a tabletop role-playing game (TTRPG)—a collaborative\nstorytelling game where a group of players each create and\nplay as their own character, exploring a world created by\nand challenges set by another player known as the Dungeon\nMaster (DM). It is the DM’s role to play the non-player char-\nacters and monsters, and to write the overarching plot of the\ngame.\nAs a co-creative storytelling game, Dungeons & Dragons\npresents multiple unique challenges for AI systems aiming\nto interact with it intelligently. Over the course of a game,\nwhich is played out across multiple sessions spanning a\nlong duration of time (often multiple months to years), the\nDM and the other players work together to produce a narra-\ntive grounded in commonsense reasoning and thematic con-\n*Work done while at the University of Pennsylvania.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: After rolling a random encounter (red), DMs can\nuse LLMs with C ALYPSO to help generate an encounter\nscene and digest information about monsters. CALYPSO can\npresent monster information concisely (green) and brain-\nstorm conversationally (purple) to help build a compelling\nnarrative to present to players (purple).\nsistency (Ammanabrolu et al. 2020; Bergstr ¨om 2011). As\nthe group plays for longer, the players define more of the\nworld and ad-hoc rules for interacting with it (van Velsen,\nWilliams, and Verhulsdonck 2009). In order to make in-\ncharacter decisions, each individual player must maintain a\npersonal understanding of the game world which they build\nfrom the game history (Martin, Sood, and Riedl 2018) while\nkeeping track of what information other players and their\ncharacters know (Zhou et al. 2023).\nBy using an AI co-DM tool, human DMs can devote more\nmental energy to cognitively demanding tasks of being a\nDM, such as improvising dialog of NPCs (non-player char-\nacters) or repairing the script of their planned campaign. Fur-\nthermore, an AI co-DM would drastically reduce the barrier\nof entry into DMing. Therefore, an AI co-DM tool would be\ninvaluable to the D&D community.\nAn effective AI co-DM tool should not only produce\ncoherent and compelling natural language output for a\nDM to effectively use for inspiration but also account for\nan immense amount of background context and require-\nments for internal consistency—both within D&D rules and\nwithin a given scenario or campaign. Large language models\n(LLMs), such as GPT-3 (Brown et al. 2020) and ChatGPT\n(OpenAI 2022), have shown impressive abilities to generate\nProceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE2023)\n380\ncoherent text. Some (Callison-Burch et al. 2022; Zhu et al.\n2023) have even applied LLMs to the problem of D&D di-\nalog and narrative by finetuning the models with structured\ninformation. Whereas these works used structured informa-\ntion scraped from user data to fine-tune a single model, we\nuse existing data in D&D source books to improve genera-\ntion using zero-shot prompting with multiple models.\nIn this paper, we present a study in which we created a\nLLM-augmented tool to assist DMs in playing D&D. We\nemployed the following methods:\n1. We interviewed DMs to understand how they digest\ngame information and learn design motivations for AI as-\nsistants in the domain.\n2. We created a gameplay setting that allowed us to study\nD&D gameplay on a larger scale than other recent works\nand invited 71 players to participate.\n3. We created a system of three LLM-powered interfaces,\nwhich we call C ALYPSO (Collaborative Assistant for\nLore and Yielding Plot Synthesis Objectives), that DMs\nand players could use as they played D&D, and studied\nthe ways in which DMs and players incorporated them\ninto their creative process over four months using estab-\nlished HCI methods.\nWe show that language models are capable “co-DMs” –\nnot a player in the same way that the human players and\nDM are, but still a synchronous agent that acts as a guide for\nthe human DM. We provide insights into how TTRPG play-\ners actually want to use these tools and present validated so-\nlutions that can extend beyond the D&D domain. Our study\nshows that a system designed with these motivations in mind\nsaw consistent prolonged usage among a community of cre-\native writers.\nBackground and Related Work\nDungeons & Dragons in the Time of COVID\nTraditionally, Dungeons & Dragons is played in person.\nPlayers use physical character sheets and monster stats ref-\nerenced from books containing hundreds of prewritten “stat\nblocks” (as pictured in Figure 2a) (Perkins et al. 2014). DMs\nhave the option to create a world of their own to play in (also\nsometimes called “homebrewing” a setting) or to set their\ngame in a professionally written “module”: a book contain-\ning a detailed outline of an adventure, including the setting,\nnon-player characters, predesigned challenges and monster\nencounters, and lore. Previous works have explored meth-\nods of how to present information in these existing settings\nmore clearly to DMs, such as through a computer-generated\nadventure flowchart (Acharya, Mateas, and Wardrip-Fruin\n2021) or recommender systems for relevant entities in a\nscene (Perez, Eisemann, and Bidarra 2021).\nSince the beginning of the COVID-19 pandemic, there has\nbeen a shift towards playing D&D online (Yuan et al. 2021).\nRather than using physical character sheets and reference\nbooks while playing in person, a large number of groups\ninstead play virtually using tools like D&D Beyond (Wiz-\nards of the Coast, LLC 2017) for virtual character sheets\nand reference books, Discord for messaging, virtual table-\ntops like Foundry (Foundry Gaming, LLC 2019) to simulate\nmaps, and game state trackers like Avrae (Zhu and Wizards\nof the Coast, LLC 2016) to track character and monster stats.\nFor inspiration and immersion, DMs also use online tools\nlike dScryb (dScryb Inc. 2020), which provides prewritten\ntext, Tabletop Audio (Roven 2014), which provides sound-\nboards and soundscapes, and random tables published in\nD&D source books (Crawford, Perkins, and Wyatt 2014),\nwhich provide a prewritten set of options, for specific sce-\nnarios (e.g. encountering a dragon).\nLarge Language Models and D&D\nLarge language models (LLMs) are a recent development in\nthe area of Natural Language Processing that have demon-\nstrated emergent capabilities of understanding users’ input\nand replying directly in the user’s language (c.f. a machine\nlanguage). A neural architecture based on the Transformer\n(Vaswani et al. 2017), they are capable of learning user-\ndefined tasks with no additional training (“few-shot” or “in-\ncontext” learning) and referencing concepts defined in their\nlarge training corpus (Brown et al. 2020).\nAlthough there has been some work looking at playing\nDungeons & Dragons using earlier neural language mod-\nels (Louis and Sutton 2018; Martin, Sood, and Riedl 2018;\nRameshkumar and Bailey 2020), the introduction of LLMs\nhas created a renewed interest in researching tabletop gam-\ning. Callison-Burch et al. (2022) frame D&D as a dialogue\nchallenge and examine whether LLMs are capable of pre-\ndicting a player’s next utterance based on the conversational\nhistory, finding that local game-specific state context is im-\nportant for grounded narrative generation. Newman and Liu\n(2022) use LLMs to generate novel material (namely spells)\nthat is consistent with the style and rules of the game. Zhou\net al. (2023) create a system that models the intents of D&D\nplayers using LLMs to inform a surrogate Theory of Mind.\nZhu et al. (2023) instrument a game state tracker to provide\nconcrete actor stats and combat state, finding that LLMs are\ncapable of producing interesting roleplay in combat scenar-\nios and predicting the action a player will take. They high-\nlight the importance of player and DM agency in LLM-\ngenerated texts, proposing that LLMs are better suited for\nassistant-style use cases. Kelly, Mateas, and Wardrip-Fruin\n(2023) present a preliminary work using LLMs to identify\nplayer questions from live transcriptions of gameplay and\nsuggest in-character responses.\nSantiago et al. (2023) have proposed multiple scenarios\nwhere LLMs and other generative AI models may be used\nto assist DMs, and discuss the ways AI may be used. In\nthis workshop paper, they hypothesize the potential for AI to\nhelp inspire and take cognitive burden off the DM and pro-\nvide brainstorming inspiration, but also weaknesses where\nAI may fall back onto overused tropes or underrepresent mi-\nnority groups. In this work, we explore and expand upon\nmany of these hypotheses through interviews with DMs. We\ncreate a system where DMs can fluently incorporate a LLM\ninto their creative process and run a broad study on its use\nand failure cases.\nLLMs have been explored as a writing assistant in other\n381\nmodalities as well, using various methods to assist in col-\nlaboratively building a narrative. These works have exam-\nined the use of conversational agents (Coenen et al. 2021;\nIppolito et al. 2022), writing in established settings (Akoury\net al. 2020), and other human-in-the-loop methods (Chung\net al. 2022; Roemmele and Gordon 2015; Samuel, Mateas,\nand Wardrip-Fruin 2016; Calderwood et al. 2020; Yang et al.\n2022; Kreminski et al. 2022). There has also been work\nproposing LLMs for multimodal co-creative frameworks\n(Lin, Agarwal, and Riedl 2022). Overall, these techniques\ndiffer from D&D and other TTRPGs in that they primarily\nfocus on a single writer/creator interacting with the system,\nrather than the multi-player experience in TTRPGs where all\nplayers directly interact with the story.\nTo our knowledge, our work is the first to examine con-\ncrete implementations of multiple unique interaction modal-\nities in and outside of combat scenarios and the ways D&D\nplayers interact with language models on this scale.\nDesign Motivation\nTo better understand the friction DMs face in looking up\nreference material midgame, we conducted interviews and\nran workshop sessions with seven DMs (referred to as D1-\n7 below) from a wide range of backgrounds before creating\nour system. Participants ranged from 1 to 39 years of ex-\nperience playing D&D (various editions). In these sessions,\nwe asked DMs how they approached improvising encoun-\nters – i.e., to run random encounters that are generated on\nthe fly (usually by rolling on an encounter table). In random\nencounters, DMs do not have time to research the monster’s\nstats and lore beforehand and think of backstories as to why\nthe monster ended up in a particular setting. From these in-\nterviews, we identify several ways how an AI system could\nbe helpful to DMs:\nInspiration. As proposed by Santiago et al. (2023), we\nfind that DMs desired the ability to use a language model\nto generate the first draft of an encounter, which they could\nthen build on top of with their own ideas (D1-3). Different\nDMs envisioned giving the system varying amounts of con-\ntrol over the narrative. D3 expressed that they would want a\nsystem to write a scene that they would then vet and choose\nwhether to present it verbatim to their players, edit it to their\nliking, or use as inspiration to overcome writer’s block. D1\nand D2 envisioned using the system’s generation verbatim to\npresent an initial scene to players while they either read the\ncomplete text of the monster description (D2) or to reduce\ncognitive load (D1).\nStrategic Copilot. One DM mentioned that managing\nboth narrative gameplay and tracking monster stats and\nmechanics overwhelmed their short-term memory, and ex-\npressed interest in a system that could aid them in making\nstrategic decisions and acting as a high-level copilot. They\nexpressed that the large amount of low-level management\nwas a barrier to them running more D&D, and that they\nwanted to “feel more like an orchestra conductor over some-\none who’s both putting down the train tracks AND fueling\nthe train” (D4).\nAnother DM said that DMs often fail to take into ac-\ncount monsters’ unique abilities and stats when running en-\ncounters, making simplifications to manage a large num-\nber of monsters. For example, a monster with very high\nintelligence and low dexterity attempting to move sneakily\n“should know not to move and make a bunch of noise” (D6).\nThematic Commonsense. We asked DMs what parts of\nmonsters’ game statistics they found to be the most impor-\ntant for their understanding of how to use a monster in their\ngame, and found that multiple DMs used a concept of “base-\nline” monsters to gain a broad understanding of a monster\nwhen they first encounter it. The idea of the baseline mon-\nster was not to find a specific monster to compare another\nto, but to determine which parts of an individual monster’s\ngame statistics to focus on, and which parts to use prior the-\nmatic commonsense to fill in.\nIn this context, we define thematic commonsense as the\nDM’s intuitive understanding of D&D as a game with me-\ndieval fantasy themes, and how they might draw inspira-\ntion from other works of fantasy literature. For example, a\nDM might intuitively understand that a dragon is a kind of\nwinged reptile with a fire breath based on their consumption\nof other fantasy works, reason that all dragons are capable\nof flight, and focus on a particular dragon’s unique abilities\nrather than flight speed (D7). Although D&D reference ma-\nterial does not include an explicit description of the dragon’s\nfire breath, the DM might base their narration on depictions\nof fire breath from other authors.\nWe find this similar to the idea of a genus-differentia def-\ninition (Parry and Hacker 1991), in that DMs use their gen-\neral background understanding of fantasy settings to define\ntheir personal genus and supplement prior knowledge by\nskimming monster reference books for differentia. This sug-\ngests that co-DM systems should focus on helping DMs ex-\ntract these differentiae, and that they also require the same\nextensive background knowledge as the user. For the D&D\ndomain, we believe that LLMs such as GPT-3 (Brown et al.\n2020) have included sufficient information on the game and\nthe game books themselves in their training corpus so as to\nestablish such a background knowledge. However, we are in-\nterested in methods for establishing this thematic common-\nsense knowledge for works not included in models’ training\ndata in future work.\nSimple Language. Multiple DMs emphasized that they\nwould like a co-DM system to present monster information\nin plain language, rather than the elaborate prose found in\ngame reference manuals (D3-6). As a work of fantasy litera-\nture, D&D publications (including reference manuals) often\nuse heavy figurative language and obscure words. For exam-\nple, the first paragraph of an owlbear’s description reads:\nAn owlbear’s screech echoes through dark valleys\nand benighted forests, piercing the quiet night to an-\nnounce the death of its prey. Feathers cover the thick,\nshaggy coat of its bearlike body, and the limpid pupils\nof its great round eyes stare furiously from its owlish\nhead (Crawford, Mearls, and Perkins 2018, pg. 147).\nThis style of description continues for seven additional\n382\nparagraphs. On average, across all D&D monsters published\non D&D Beyond, a monster’s description and list of abil-\nities contains 374 words (min: 0, max: 2,307). DMs often\nuse multiple monsters together in the same encounter, com-\npounding the amount of information they must hold in their\nmind.\nMonster descriptions often include descriptions of the\nmonster, its abilities, and lore. Some DMs’ preferred method\nof referencing monster lore while running the game was to\nskim the full monster entry, and the complex and long prose\noften led to DMs feeling overwhelmed (D4, D5). Other DMs\nwanted a short and salient mechanical (i.e. focusing on mon-\nster’s game abilities and actions) description, rather than a\nnarrative (lore and history-focused) one (D3, D6).\nOverall, the complexity of monster descriptions led DMs\nto forget parts of monsters’ lore or abilities during game-\nplay (D5) or use overly broad simplifications that did not\ncapture an individual monster’s uniqueness (D6). While of-\nfline resources exist to help DMs run monsters (e.g. Amman\n(2019)), they cannot account for the environment or generate\na unique scenario for each encounter with the same monster.\nWe believe that LLMs’ capability to summarize and gener-\nate unique material is particularly applicable to these chal-\nlenges.\nImplementation\nIn this section, we describe the three interfaces we developed\nto provide DMs with the sorts of support they desired. These\ninterfaces were designed with ”in the wild” deployment in\nmind:\n1. Encounter Understanding: a zero-shot method to gener-\nate a concise setup of an encounter, using GPT-3.\n2. Focused Brainstorming: a conversational method for\nDMs to ask additional questions about an encounter or\nrefine an encounter summary, using ChatGPT.\n3. Open-Domain Chat Baseline: a conversational interface\nwithout the focus of an encounter, using ChatGPT.\nOur implementation differs from other efforts to develop\nAI-powered co-creative agents in two ways. First, compared\nto models where the AI acts as the writer, AI-generated con-\ntent is not necessarily directly exposed to the audience. CA-\nLYPSO only presents ideas to a human DM, who has final\nsay over what is presented to the players. Second, compared\nto co-writing assistants where the writer has plentiful time to\niterate, the time between idea and presentation is very short.\nSince the DM uses C ALYPSO in the midst of running a real\ngame, CALYPSO should be frictionless to adopt and should\nnot slow down the game.\nEncounter Understanding\nThe first interface we provided to DMs was a button to use a\nlarge language model to distill down game statistics and lore\navailable in published monster stat blocks. To accomplish\nthis, we prompted GPT-3 (Brown et al. 2020) (specifically,\nthe text-davinci-003 model) with the text of the chosen en-\ncounter, the description of the setting the encounter was tak-\ning place in, and the game statistics and lore of each monster\ninvolved in the encounter. The full prompts are available in\nthe appendix.\nWe began by presenting the LLM with the task to sum-\nmarize monsters’ abilities and lore and the environment. We\ncollected feedback from DMs after generating the extracted\ninformation by allowing them to select a positive or nega-\ntive feedback button, and optionally leave comments in an\nin-app modal. This interaction is illustrated in Figure 2.\nSummarization. At first, we prompted GPT-3 to “summa-\nrize the following D&D setting and monsters for a DM’s\nnotes without mentioning game stats,” then pasted verbatim\nthe text description of the setting and monster information.\nFor decoding, we used a temperature of 0.9, top-p of 0.95,\nand frequency and presence penalties of 1. Based on feed-\nback from DMs (discussed below), we later changed to a\nmore abstract “understanding” task described below.\nAbstractive Understanding. In the understanding task,\nwe prompted GPT-3 with the more abstract task to help\nthe DM “understand” the encounter, along with explicit in-\nstructions to focus on the unique aspects of each creature,\nuse information from mythology and common sense, and to\nmention how multiple creatures interact with each other. Af-\nter these instructions, we included the same information as\nthe Summarization task above. Finally, if a monster had no\nwritten description, we included instructions in place of the\nmonster’s description telling C ALYPSO to provide the DM\ninformation from mythology and common sense. For de-\ncoding, we used a temperature of 0.8, top-p of 0.95, and a\nfrequency penalty of 0.5.\nFocused Brainstorming\nTo handle cases where a single round of information extrac-\ntion was not sufficient or a DM had additional focused ques-\ntions or ideas they wanted assistance elaborating, we also\nprovided an interface to open a private thread for focused\nbrainstorming. Available at any time after an encounter was\nrandomly chosen, we provided the same encounter informa-\ntion as in theEncounter Understanding interface as an initial\nprompt to ChatGPT (i.e., gpt-3.5-turbo) (OpenAI 2022). If\nthe DM had used the Encounter Understanding interface to\ngenerate an information block, we also provided it as context\n(Figure 4). The full prompts are available in the appendix.\nFor decoding, we used a temperature of 1, top-p of 0.95, and\na frequency penalty of 0.3.\nOpen-Domain Chat Baseline\nFinally, we made a baseline open-domain chat interface\navailable to all players, without the focus of an encounter.\nAs this interface was available at any time and open-ended,\nit helped provide a baseline for how DMs would use AI chat-\nbots generally. To access the interface, users were able to\nrun a bot command, which would start a new thread. We\nprompted ChatGPT to take on the persona of a fantasy crea-\nture knowledgeable about D&D, and generated replies to\nevery message sent in a thread opened in this manner. For\ndecoding, we used a temperature of 1, top-p of 0.95, and\na frequency penalty of 0.3. Unlike the private threads cre-\nated by the Focused Brainstorming interface, open-domain\n383\n(a) An example of the kinds of information\nDMs reference in an encounter. Digesting the\nmyriad sources of information and using them\nto build a compelling narrative is a challeng-\ning task.\n(b) After rolling an\nencounter, the interface\nDMs used to interact with\nCALYPSO .\n (c) An example of CALYPSO ’s summary of the encounter.\nFigure 2: Using the Encounter Understanding interface to generate a distilled summary of an encounter.\nconversation threads were public and allowed other users to\njoin.\nExperimental Setup\nBy deploying CALYPSO in the wild, we sought to learn how\nreal DMs would adopt the new technology (if at all) and the\nemergent use cases that would arise.\nWe set up a special “play-by-post living world” game,\nwhich we describe below, and invited 71 players and DMs\n(referred to as P1-71) to participate by posting on D&D re-\ncruitment forums. While preserving the core foundations of\nD&D, our setup allowed us to conduct a large-scale study\nwith a greater number of play sessions than studying indi-\nvidual games of D&D.\nIn this section, we describe our methodology for setting\nup this large-scale D&D game.\nD&D Game Setup\nAll gameplay occurred on our Discord server. We used\nAvrae, a D&D Discord bot with over five million users, to\nfacilitate gameplay. Avrae is commonly used to run D&D\ngames in this fashion, so the large-scale game was famil-\niar to players and DMs (Zhu et al. 2023). All participants\nwere asked to review the server’s research statement and to\nprovide their informed consent before participating. Partic-\nipants were compensated with free access to all published\nD&D game materials (worth $981.35). We explain the core\ndifferences between a traditional game of D&D and our\nsetup here:\nPlay-by-Post. While most commonly D&D is played in\nperson or using a virtual teleconference, a large number of\nplayers also play in a text-only mode known as “play-by-\npost”. In play-by-post games, rather than acting out char-\nacters using voices and body movements, players narrate\ntheir characters’ actions and speech in a textual format. This\ntext-based modality allowed us to monitor a large number of\nplay sessions and allowed players to interface with language\nmodels without having to add an additional step to transcribe\nverbal play into text.\nLiving World. Our setup takes aspects from playing both\nprewritten modules and homebrew worlds. Traditionally,\ngroups are comprised of 1 DM and 3-6 players playing in\ndifferent worlds created by the DM, who play in regularly\nscheduled 3-4 hour play sessions (most commonly, once a\nweek). To allow for a larger scale study, in our setting, all 71\nplayers exist in the same would, which we created. To emu-\nlate traditional play sessions, players form groups of 3-6 (on\naverage) to partake in self-contained quests in the setting,\nalways returning to a central hub after each quest. Within\nthe hub, players are free to interact with each other, allow-\ning room for storytelling and character development through\nroleplay without a DM. Outside the hub, we created a di-\nverse set of environments that players could explore, each\nwith a short description and image.\nMode of Play\nIn the C ALYPSO study, the primary mode of play was a\nrandom encounter system: players explore the established\nworld and the DM is given a set of monsters to use (ran-\ndomly selected from a table of encounters). The DM must\nthen create an encounter involving those monsters for their\nplayers to play in. By giving DMs access to the interfaces de-\nscribed above, we study how language models can be used\nwith focused local knowledge to help a DM understand com-\nplex monster lore, interactions, and how to incorporate new\nencounters into an existing setting. Figure 1 shows the flow\nof how an encounter is randomly chosen and how a DM\nmight use CALYPSO to help build a compelling narrative.\n384\nResults and Discussion\nIn this section, we compile the ways that DMs adopted CA-\nLYPSO ’s three interfaces into their games. We highlight the\nsuccesses and failures of LLMs, and describe its emergent\nuse cases. We refer to the DMs who used C ALYPSO as P1-\nP71 in this section.\nEncounter Understanding\nSummarization. DMs interacted with the summarization\nmodel in 37 encounters, indicating that the summary helped\nthem understand the monsters and setting in 13 encounters\nand did not help in 7 encounters.\nWe found that DMs appreciated the summary for distill-\ning long information from stat blocks (P10) or highlighting\nan aspect of the monster to focus on (P8). However, when\nprompted with the task to summarize provided information,\nGPT-3 would focus too much on numeric game stats (de-\nspite the prompt stating not to) and summarize the environ-\nment and monsters separately, often focusing too much on\nthe environment (P3). For example, for an encounter involv-\ning 12 Blink Dogs, the model’s output simply paraphrased\nthe information written in the setting and stat block rather\nthan suggesting more interesting interactions:\nCALYPSO : A warm and calm autumnal forest with a\nriver running through it, the players can travel through\nthis area towards the distant mountain range. The\ncreatures they may encounter here are blink dogs;\nthey have an armor class of 13, 22 Hit Points, a speed\nof 40 ft., as well as skills in Perception and Stealth.\nThey also possess limited intelligence and understand\nSylvan though cannot speak it.\nAlthough the information presented by C ALYPSO is fac-\ntually correct, DMs argued that simply rephrasing the infor-\nmation in the monster’s reference material “doesn’t really\nhelp in running an encounter” (P8) and generally wanted\n“more information about [the monsters] and why they might\ninteract with [the players]” (P11). DMs wanted C ALYPSO\nto generate details that were entirely omitted from the refer-\nence material, using the written reference as only a basis to\ngain an understanding of the monster.\nAnother common case where C ALYPSO did not present\na helpful summary was when a monster did not have a\npublished prose description, only numeric game stats. Most\ncommonly, such creatures were an animal found in real life\n(e.g. a brown bear; P3) or relied entirely on a DM’s thematic\ncommonsense to run (e.g. a snow golem; P3). In these cases,\nthe model would only use minimal external knowledge (e.g.\nnoting that giant boars usually had “thick, protective skin”)\nrather than using thematic commonsense to make up a more\ninteresting narrative (discussed in Design Motivation). We\nfound that in order for the LLM to use external knowledge\nabout monsters rather than only what was written in game\nstats, we had to prompt it with explicit instructions to do so.\nWith these findings in mind, we remodeled the summariza-\ntion task to a more abstract “understanding” task, in which\nwe provided the model explicit instructions to use thematic\ncommonsense.\nSummarization\nAbstractive\n0% 25% 50% 75%\nHelpful Not Helpful No Feedback\nFigure 3: DMs found theAbstractive Understanding method\nof distilling monster information more consistently helpful\nthan the Summarization method.\nAbstractive Understanding. Overall, DMs much pre-\nferred this more abstract task: over interactions in 114 en-\ncounters, DMs indicated that the summary helped them un-\nderstand the monsters and setting in 55 encounters and did\nnot help in 2 encounters. Figure 3 shows the breakdown of\nfeedback for each interface.\nPrimarily, DMs using this model were satisfied with its\nability to describe monsters based on real-life animals (P8),\ncreature behavior (P3), interactions with players (P3), how\nto roleplay creatures (P12), and general performance (P3,\nP8, P11-13). The most common undesired behavior DMs\nnoticed was the model hallucinating, or making up, facts\nabout the monsters, such as giving giant spiders telepathy\n(P15), an insect monster a poison ability (P8), or a frost sala-\nmander wings (P14). Given the same example encounter as\nabove, the model’s output includes much more useful infor-\nmation about appearance and monster behavior:\nCALYPSO : The blink dogs are canine creatures, about\nthe size of a large dog. Their fur is a mottled grey and\nwhite colour, and their eyes flash yellow when they\nuse their supernatural ability to blink in and out of ex-\nistence. They are surprisingly fast despite their size,\nand can easily keep up with most horses. In terms of\nbehavior, the blink dogs are curious but shy creatures\nwho prefer to avoid contact with other creatures un-\nless absolutely necessary. If threatened, they will use\ntheir blinking power to confuse and disorientate their\nopponents before attacking with a pack mentality. The\nblink dogs of this forest seem particularly adept at\nworking together as a unit and can prove a formidable\nfoe if provoked.\nAs the Summarization and Abstractive Understanding\ntasks differ only in prompting methods, we demonstrate\nhow sensitive prompting approaches can be. It is possible\nthat GPT-3 is overfit on the word “summarize”, and even\nsubtle changes in prompt design encourage original and\ndomain-specific generation. With these alterations, we found\nthat LLMs are capable of synthesizing creative ideas about\nD&D monsters using thematic commonsense contained in\nits training priors.\nWe also found that if we repeated a certain phrase mul-\ntiple times in the prompt, e.g. “you may use information\nfrom mythology and common sense” (which could occur if\nan encounter involved multiple monsters with no published\nlore), the model would often include the phrase “mythology\n385\nDungeon Master\nCalypso\nFigure 4: Using the Focused Brainstorming interface to ask\nspecific questions about an encounter. C ALYPSO suggests\nreasons why the players might encounter the monsters and\nhow they might act.\nand common sense” verbatim in its output. This is likely\na case of degenerative output (Holtzman et al. 2020). To\nprevent this, each time a set phrase would have been tem-\nplated in, we instead randomly chose 2-4 words from the set\n{”folklore”, “common sense”, “mythology”, “culture”} and\ninserted them in a random order (e.g. “use information from\ncommon sense, mythology, and folklore”).\nEffect of Hallucinations. We find that not all hallucina-\ntions are undesired. In many cases, the model suggests mon-\nster behaviors or appearances that are not explicitly written\nout in monster descriptions, such as the appearance of the\nblink dogs’ fur in the example above. More drastic devia-\ntions, such as the model suggesting giving a creature wings,\nwere however undesired.\nDMs often take creative liberty to synthesize sensical\ninformation that isn’t included in the source material. As\nshown above, they expect their tools to do the same when\nnecessary – while the Summarization interface was more\nconservative in ensuring it did not hallucinate any details,\nthe Abstractive Understanding interface was more well-\nreceived even with minor hallucinations. Since the DM acts\nas a curator of the model’s output, the DM can choose which\nof the generations to accept.\nFocused Brainstorming\nIn total, DMs used the focused brainstorming model in 71\nencounters, comprising a total of 162 rounds of conversa-\ntion. DMs used the brainstorming model in a number of\ndiverse way, which we qualitatively coded and tabulate in\nTable 1. Here, we discuss these use cases and some failure\ncases.\nGeneral and Specific Descriptions. The most common\nway DMs used the interface was to ask it for a high level\ndescription of a given encounter and specific descriptions\nof points in the encounter. Since our prompt included infor-\nmation on the setting and involved monsters, the model was\nable to reference the information in its description. Addition-\nally, the conversational nature of the language model added\nto its context, so DMs could reference earlier ideas without\nhaving to repeat them. This allowed DMs to ask C ALYPSO\nto simply “describe this scene” or “describe X” without hav-\ning to specify additional details (P3, P8-10, P12, P16-20).\nAfter presenting an idea to their players and seeing what\npart of the encounter players interacted with, the DM was\nable to ask follow-up questions to describe in detail specific\nelements the players interacted with. For example, when\nrunning an encounter involving a ship’s figurehead that was\nwashed ashore, P3 first asked for a description of the figure-\nhead. Then, when the players investigated it further, P3 fol-\nlowed up by asking for “a description about its construction,\nspecifically how it was carved, and perhaps what D&D race\ncrafted it.” This allowed DMs to elaborate on specific parts\nof an encounter when it became relevant, rather than present-\ning a large amount of information up front. However, DMs\nfound that the model struggled sometimes to describe com-\nbat, and suggested that including more information about the\ncombat state (similar to Zhu et al. (2023)) or map placement\ninformation could help generate more specific descriptions\n(P3, P9). Some DMs used these descriptions verbatim (P3,\nP8, P17), while others picked out particularly vivid phrases\nto use in a description of their own (P3, P8, P10, P12, P20).\nOthers disagreed with the model’s description and wrote\ntheir own instead (P13, P16, P18, P19).\nStrategy. Another common use case for DMs was to ask\nthe model for monsters’ “motives, tactics, and who they\nmight prioritize [in a fight]” (P8-9, P12-13, P19, P23). As\ndiscussed in Design Motivation, coming up with and stick-\ning to strategies for each monster can be overwhelming, and\noften DMs use simplifications to manage their mental load.\nThis use case allowed DMs to create more engaging fights\nwith clearer paths to resolutions by describing a creature’s\nmotive and specific tactics the creature would use. For ex-\nample, when a DM asked how a pack of ten wolves might\napproach a camping party, the model suggested to have the\nwolves “circle around the camp, hiding behind trees and\nbushes [...] and wait until a member of the party is alone and\nvulnerable before striking, hoping to separate and weaken\nthe group” (P8). Similar to the interactions with descrip-\ntions, these DMs did not always use the strategy presented\nby the model; sometimes they picked and chose interest-\ning suggestions, while other times they chose a different ap-\nproach.\nMaking Decisions. Some DMs used the model to get an\nopinion on two options they had already written or thought\n386\nUse Case Description Example\nGeneral Descriptions Asking the model to generate a high-level\ndescription of a scene and encounter.\n“Describe this encounter from the player’s perspec-\ntive. ”(P8)\nSpecific Descriptions Asking specific questions about parts of\nthe encounter, often in response to player\nactions.\n“Describe drinking games that the satyrs are taking\npart in that are so dangerous someone could get hurt\ndoing them. ”(P17)\nStrategy Using the model to understand monster\nmotives and get suggestions for their tac-\ntics.\n“Why would a Displacer Beast Kitten leave the\nsafety of its den if it believes an intruder is nearby?”\n(P12)\nMaking Decisions Using the model to decide how the DM\nshould run a given encounter.\n“Should a diplomatic solution be possible for this en-\ncounter?” (P14)\nList of Ideas Generating a list of multiple ideas to build\noff of individually.\n“give me encounter ideas” (P10)\n“...make up more [magic items] to make this en-\ncounter more interesting. ”(P19)\nTable 1: A list of common ways DMs used the Focused Brainstorming interface.\nof (P3, P8-9, P12-14, P18, P23). For example, when players\nencountered a ravine whose bottom was enshrouded in mist,\none DM asked whether the mist should hide a very long or\nshort drop. The model would sometimes simply give feed-\nback on both of the options without choosing one (“Both\noptions have their merits depending on the tone and style of\nyour game... ”; P3) and sometimes give a more straightfor-\nward answer (“...would that revenant have a vengeance to-\nwards the party member?” / “Yes, absolutely... ”; P12). DMs\ndid not ask the model to come to a conclusive decision, sug-\ngesting that the model providing its “opinion” helped inspire\nthe DM, without relying on it to run the encounter.\nList of Ideas. In this use case, the DM simply asks the\nmodel for a list of ideas; for example, a list of magic items\nsea-dwelling humanoids might have (P10). We believe that\nthe reasoning for this use case is the same reason that makes\nrandom tables (as discussed in Background and Related\nWork) a popular method of inspiration – however, compared\nto prewritten random tables, LLMs have the powerful ca-\npability of generating unique “random table” entries cus-\ntomized for specific contexts.\nFailure Cases. The most common failure case was when\nDMs tried to invoke other tools (such as a dice rolling or\nspell search bot) available in the brainstorming chat. As the\nmodel responded to every message in the thread, it would\nalso respond to the other tool’s invocation and reply with a\ngeneric error message or try to infer the other tool’s output\n(e.g. “!check stealth” / “Abominable Yeti stealth check: 18”,\nhallucinating a result while ignoring the output of an actual\ndice roller). In some cases, the DM attempted to upload an\nimage, which the model was unable to view. Finally, as dis-\ncussed above, the model sometimes hallucinated facts about\ncreatures and rules. We believe multimodality (allowing the\nmodel to view images) and allowing the model to use tools\n(e.g. to retrieve rules text, spell descriptions, or search mon-\nsters) to be an interesting direction to explore in future work.\nWe also find that certain artifacts of the model’s training\nprocess influences its output. For example, the model would\nsometimes refuse to suggest (fantasy) races, likely due to\nefforts to reduce the potential for real-world racial bias. In\nanother case, the model insists that it is incapable of playing\nD&D, likely due to efforts to prevent the model from making\nclaims of abilities it does not possess. Although generally\ninfrequent, these artifacts suggest that domain-specific fine-\ntuning may improve models’ performance.\nOpen-Domain Chat Baseline\nParticipants chatted with C ALYPSO in 51 unique threads,\ncomprising a total of 2,295 rounds of conversation. Com-\npared to conversations with the AI in the Focused Brain-\nstorming interface, conversations lasted much longer (aver-\naging 45.0 rounds per interaction vs. the brainstorming in-\nterface’s 2.3). Without the time pressure of an active game\nthat the DM is responsible for, participants spent more time\nplaying with the model and refining its responses to gener-\nate high-level quest ideas (P3, P8, P12, P16), character and\nlocation names (P3, P9, P19, P22), role-play specific charac-\nters from other games (P3, P9, P12, P16), and write fanfic-\ntion about events happening between their characters in the\ngame (P3, P8, P9, P16, P21), among other non-D&D uses.\nHowever, during a game of D&D, DMs did not have\nthe time luxury to iterate on responses for hours. Without\nCALYPSO ’s management of the game, DMs would have to\nspend many turns of conversation copying and pasting in-\nformation to provide it to the LLM, taking attention away\nfrom the game and making the baseline implementation un-\nsuitable for real-world adoption.\nWe believe this highlights the difference between syn-\nchronous and asynchronous systems and the importance\nof removing friction from AI-augmented user interfaces –\nwhile the human user may have the capability to supply a\nLLM with additional information, the burden should be on\nthe synchronous system, not the user.\nConclusions\nIn this paper, we present C ALYPSO , a system of three\nLLM-powered interfaces that DMs could use to assist them\n387\nin preparing and running focused monster encounters in\nan established setting, and a large-scale study of how 71\nD&D players incorporated C ALYPSO into their gameplay.\nThrough interviews with DMs, we established common\nthemes and desires for AI-augmented DM tools, and used\nthese motivations and iterative design to guide our develop-\nment. In conclusion, we found that:\n1. LLMs are capable brainstorming partners. DMs used\nCALYPSO to generate both low-fidelity ideas that they\ncould grow using their own creative expression, and\nguided it to generate high-fidelity descriptions they could\npresent to other players with only minor edits.\n2. LLMs present thematic commonsense when prompted\nto. Having been trained on a large corpus containing\nD&D texts and discussions, works of fantasy literature,\nand descriptions of real-world creatures, C ALYPSO was\nable to fill in gaps in the D&D literature by probing into\nthematically relevant common sense knowledge. How-\never, we found that to access this trove of information,\nthe LLM had to be explicitly prompted to do so.\n3. LLMs assist, rather than replace, human DMs. CALYPSO\nwas designed to aid a human DM while maintaining their\ncreative agency. We find that human DMs use AI co-DMs\nto understand complex rules text, brainstorm interactions\nbetween non-player characters or monsters, and present\nDMs with suggestions that the DM can weave into a story\nto present to players without taking away from the pace\nof the game. Human creativity is an integral part of sto-\nrytelling games like D&D, and it is important for future\nAI tools to always maintain the human’s creative agency.\nLLM Prompts\nIn this appendix, we provide the prompts used in the C A-\nLYPSO system. Generally, we make use of Markdown-style\nheaders to divide sections of the prompt. For chat-based\nmodels, we annotate each message with the corresponding\nrole (system, assistant, or user, as exposed in the ChatGPT\nAPI).\nEncounter Understanding\nSummarization\nSummarize the following D&D setting and\nmonsters for a Dungeon Master’s notes\nwithout mentioning game stats.\nSetting\n=======\n<Setting description inserted here.>\nCreatures\n=========\n<Name>\n------\n<Statistics and lore inserted here. If the\nencounter involves multiple creatures,\nrepeat for each creature.>\nSummary\n=======\nAbstractive Understanding\nYour name is Calypso, and your job is to\nhelp the Dungeon Master with an encounter.\nYour task is to help the DM understand the\nsetting and creatures as a group, focusing\nmainly on appearance and how they act.\nEspecially focus on what makes each creature\nstand out.\nAvoid mentioning game stats.\nYou may use information from common sense,\nmythology, and culture.\nIf there are multiple creatures, conclude by\nmentioning how they interact.\nEncounter: <Encounter inserted here.>\nThe rest of the prompt follows as in the Summarization\nprompt above, beginning with the setting. If a monster did\nnot have published lore, we inserted the string “Calypso,\nplease provide the DM with information about the (mon-\nster name) using information from (folklore, common sense,\nmythology, and culture)” (see Results and Discussion: En-\ncounter Understanding) in place of lore.\nFocused Brainstorming\nSYSTEM: You are a creative D&D player and DM\nnamed Calypso.\nAvoid mentioning game stats. You may use\ninformation from common sense, mythology,\nand culture.\nUSER: I’m running this D&D encounter: <\nEncounter inserted here.>\n<Setting and creatures inserted here, in the\nsame format as Abstractive Understanding.>\nYour job is to help brainstorm some ideas\nfor the encounter.\nIf the DM used the Encounter Understanding interface be-\nfore starting a brainstorming thread, we add an additional\nmessage to the prompt:\nUSER: Here’s what I have so far:\n<Summary generated by Encounter\nUnderstanding inserted here.>\nThis allows the DM to reference ideas proposed by C A-\nLYPSO in its summary without having to repeat the entire\nmessage, aiding continuity.\nAcknowledgments\nThank you to the Northern Lights Province Discord server\nfor playing with us and being so enthusiastic about AI and\nD&D! Thank you to the NLP server staff - friends and play-\ners who helped us write rules, settings, game mechanics, and\nmanage so many players: Ryan Crowley, Nicki Dulmage-\nBekker, @ephesia, @lyra.kat, and Joseph Keen. Finally,\nthank you to D&D Beyond for providing us with access to\nmonster information and game materials.\nThis material is based upon work supported by the Na-\ntional Science Foundation under Grant #2030859 to the\nComputing Research Association for the CIFellows Project.\n388\nReferences\nAcharya, D.; Mateas, M.; and Wardrip-Fruin, N. 2021. Inter-\nviews Towards Designing Support Tools for TTRPG Game\nMasters. In Mitchell, A.; and V osmeer, M., eds.,Interactive\nStorytelling, Lecture Notes in Computer Science, 283–287.\nCham: Springer International Publishing. ISBN 978-3-030-\n92300-6.\nAkoury, N.; Wang, S.; Whiting, J.; Hood, S.; Peng, N.; and\nIyyer, M. 2020. STORIUM: A Dataset and Evaluation Plat-\nform for Machine-in-the-Loop Story Generation. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 6470–6484. On-\nline: Association for Computational Linguistics.\nAmman, K. 2019. The Monsters Know What They’re Doing.\nNew York, NY: Gallery Books. ISBN 9781982122669.\nAmmanabrolu, P.; Cheung, W.; Tu, D.; Broniec, W.; and\nRiedl, M. 2020. Bringing Stories Alive: Generating Inter-\nactive Fiction Worlds. Proceedings of the AAAI Conference\non Artificial Intelligence and Interactive Digital Entertain-\nment, 16(1): 3–9.\nBergstr¨om, K. 2011. Framing Storytelling with Games.\nIn Si, M.; Thue, D.; Andr ´e, E.; Lester, J. C.; Tanenbaum,\nT. J.; and Zammitto, V ., eds., Interactive Storytelling, Lec-\nture Notes in Computer Science, 170–181. Berlin, Heidel-\nberg: Springer. ISBN 978-3-642-25289-1.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nCalderwood, A.; Qiu, V .; Gero, K. I.; and Chilton, L. B.\n2020. How Novelists Use Generative Language Models:\nAn Exploratory User Study. In International Conference on\nIntelligent User Interfaces (IUI) Workshops. Cagliari, Italy:\nACM.\nCallison-Burch, C.; Singh Tomar, G.; Martin, L. J.; Ippolito,\nD.; Bailis, S.; and Reitter, D. 2022. Dungeons and Dragons\nas a Dialogue Challenge for Artificial Intelligence. In Con-\nference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 9379–9393. Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics.\nChung, J. J. Y .; Kim, W.; Yoo, K. M.; Lee, H.; Adar, E.; and\nChang, M. 2022. TaleBrush: Sketching Stories with Gener-\native Pretrained Language Models. In CHI Conference on\nHuman Factors in Computing Systems, 1–19. New Orleans\nLA USA: ACM. ISBN 978-1-4503-9157-3.\nCoenen, A.; Davis, L.; Ippolito, D.; Reif, E.; and Yuan,\nA. 2021. Wordcraft: a Human-AI Collaborative Editor for\nStory Writing. In First Workshop on Bridging Human-\nComputer Interaction and Natural Language Processing at\nEACL 2021. Association for Computational Linguistics.\nCrawford, J.; Mearls, M.; and Perkins, C. 2018. D&D Basic\nRules. Renton, W A: Wizards of the Coast.\nCrawford, J.; Perkins, C.; and Wyatt, J. 2014.Dungeon Mas-\nter’s Guide. Renton, W A: Wizards of the Coast.\ndScryb Inc. 2020. dScryb. https://dscryb.com/. Accessed:\n2023-08-15.\nFoundry Gaming, LLC. 2019. Foundry Virtual Tabletop.\nhttps://foundryvtt.com/. Accessed: 2023-08-15.\nGygax, G.; and Arneson, D. 1974. Dungeons & Dragons.\nLake Geneva, W A: TSR, Inc.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2020. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations.\nIppolito, D.; Yuan, A.; Coenen, A.; and Burnam, S. 2022.\nCreative Writing with an AI-Powered Writing Assistant:\nPerspectives from Professional Writers. ArXiv:2211.05030\n[cs].\nKelly, J.; Mateas, M.; and Wardrip-Fruin, N. 2023. Towards\nComputational Support with Language Models for TTRPG\nGame Masters. In Proceedings of the 18th International\nConference on the Foundations of Digital Games, FDG ’23,\n1–4. New York, NY , USA: Association for Computing Ma-\nchinery. ISBN 978-1-4503-9855-8.\nKreminski, M.; Dickinson, M.; Wardrip-Fruin, N.; and\nMateas, M. 2022. Loose Ends: A Mixed-Initiative Creative\nInterface for Playful Storytelling. Proceedings of the AAAI\nConference on Artificial Intelligence and Interactive Digital\nEntertainment, 18(1): 120–128. Number: 1.\nLin, Z.; Agarwal, R.; and Riedl, M. 2022. Creative Wand: A\nSystem to Study Effects of Communications in Co-Creative\nSettings. AAAI Conference on Artificial Intelligence and In-\nteractive Digital Entertainment (AIIDE), 18(1): 45–52.\nLouis, A.; and Sutton, C. 2018. Deep Dungeons and Drag-\nons: Learning Character-Action Interactions from Role-\nPlaying Game Transcripts. InConference of the North Amer-\nican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies (NAACL-HLT), volume\nV olume 2 (Short Papers), 708–713. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\nMartin, L. J.; Sood, S.; and Riedl, M. O. 2018. Dungeons\nand DQNs: Toward Reinforcement Learning Agents that\nPlay Tabletop Roleplaying Games. In Wu, H.-Y .; Si, M.;\nand Jhala, A., eds., Joint Workshop on Intelligent Narra-\ntive Technologies and Workshop on Intelligent Cinematog-\nraphy and Editing (INT-WICED). Edmonton, AB, Canada:\nhttp://ceur-ws.org.\nNewman, P.; and Liu, Y . 2022. Generating Descriptive\nand Rules-Adhering Spells for Dungeons & Dragons Fifth\nEdition. In Proceedings of the 9th Workshop on Games\nand Natural Language Processing within the 13th Language\nResources and Evaluation Conference , 54–60. Marseille,\nFrance: European Language Resources Association.\nOpenAI. 2022. Introducing ChatGPT. https://openai.com/\nblog/chatgpt. Accessed: 2023-08-15.\nParry, W. T.; and Hacker, E. A. 1991. Aristotelian logic.\nAlbany, NY: State University of New York Press. ISBN\n9780791406892.\n389\nPerez, M. R. B.; Eisemann, E.; and Bidarra, R. 2021. A\nSynset-Based Recommender Method for Mixed-Initiative\nNarrative World Creation. In Mitchell, A.; and V osmeer,\nM., eds., Interactive Storytelling, Lecture Notes in Com-\nputer Science, 13–28. Cham: Springer International Publish-\ning. ISBN 978-3-030-92300-6.\nPerkins, C.; Crawford, J.; Sims, C.; Thompson, R.; Lee, P.;\nMearls, M.; Schwalb, R. J.; Sernett, M.; Townshend, S.; and\nWyatt, J. 2014. Monster Manual. Renton, W A: Wizards of\nthe Coast.\nRameshkumar, R.; and Bailey, P. 2020. Storytelling with Di-\nalogue: A Critical Role Dungeons and Dragons Dataset. In\nAnnual Meeting of the Association for Computational Lin-\nguistics (ACL), 5121–5134. Online: Association for Compu-\ntational Linguistics.\nRoemmele, M.; and Gordon, A. S. 2015. Creative Help: A\nStory Writing Assistant. In Schoenau-Fog, H.; Bruni, L. E.;\nLouchart, S.; and Baceviciute, S., eds., Interactive Story-\ntelling, volume 9445, 81–92. Cham: Springer International\nPublishing. ISBN 978-3-319-27035-7 978-3-319-27036-4.\nSeries Title: Lecture Notes in Computer Science.\nRoven, T. 2014. Tabletop Audio. https://tabletopaudio.com/.\nAccessed: 2023-08-15.\nSamuel, B.; Mateas, M.; and Wardrip-Fruin, N. 2016. The\nDesign of Writing Buddy: A Mixed-Initiative Approach To-\nwards Computational Story Collaboration. In Nack, F.; and\nGordon, A. S., eds., Interactive Storytelling, volume 10045,\n388–396. Cham: Springer International Publishing. ISBN\n978-3-319-48278-1 978-3-319-48279-8. Series Title: Lec-\nture Notes in Computer Science.\nSantiago, J. M., III; Parayno, R. L.; Deja, J. A.; and Sam-\nson, B. P. V . 2023. Rolling the Dice: Imagining Genera-\ntive AI as a Dungeons & Dragons Storytelling Companion.\nArXiv:2304.01860 [cs].\nvan Velsen, M.; Williams, J.; and Verhulsdonck, G. 2009.\nTable-Top Gaming Narratology for Digital Interactive Sto-\nrytelling. In Iurgel, I. A.; Zagalo, N.; and Petta, P., eds., In-\nteractive Storytelling, Lecture Notes in Computer Science,\n109–120. Berlin, Heidelberg: Springer. ISBN 978-3-642-\n10643-9.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. arXiv:1706.03762.\nWizards of the Coast, LLC. 2017. D&D Beyond. https:\n//www.dndbeyond.com/. Accessed: 2023-08-15.\nYang, D.; Zhou, Y .; Zhang, Z.; Jia, T.; Li, J.; and Lc, R. 2022.\nAI as an Active Writer: Interaction strategies with generated\ntext in human-AI collaborative fiction writing. In Joint Pro-\nceedings of the ACM IUI Workshops 2022. Helsinki, Fin-\nland.\nYuan, Y .; Cao, J.; Wang, R.; and Yarosh, S. 2021. Tabletop\nGames in the Age of Remote Collaboration: Design Oppor-\ntunities for a Socially Connected Game Experience. In Pro-\nceedings of the 2021 CHI Conference on Human Factors in\nComputing Systems, 1–14. Yokohama Japan: ACM. ISBN\n978-1-4503-8096-6.\nZhou, P.; Zhu, A.; Hu, J.; Pujara, J.; Ren, X.; Callison-Burch,\nC.; Choi, Y .; and Ammanabrolu, P. 2023. I Cast Detect\nThoughts: Learning to Converse and Guide with Intents and\nTheory-of-Mind in Dungeons and Dragons. In Proceedings\nof the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 11136–11155.\nToronto, Canada: Association for Computational Linguis-\ntics.\nZhu, A.; Aggarwal, K.; Feng, A.; Martin, L.; and Callison-\nBurch, C. 2023. FIREBALL: A Dataset of Dungeons and\nDragons Actual-Play with Structured Game State Informa-\ntion. In Proceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), 4171–4193. Toronto, Canada: Association for Com-\nputational Linguistics.\nZhu, A.; and Wizards of the Coast, LLC. 2016. Avrae. https:\n//avrae.io/. Accessed: 2023-08-15.\n390",
  "topic": "Narrative",
  "concepts": [
    {
      "name": "Narrative",
      "score": 0.7128361463546753
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6702784895896912
    },
    {
      "name": "Agency (philosophy)",
      "score": 0.595344603061676
    },
    {
      "name": "Computer science",
      "score": 0.5756052732467651
    },
    {
      "name": "Formative assessment",
      "score": 0.5108878016471863
    },
    {
      "name": "Brainstorming",
      "score": 0.5003376007080078
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4897020161151886
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4685294032096863
    },
    {
      "name": "Fidelity",
      "score": 0.46655386686325073
    },
    {
      "name": "Augmented reality",
      "score": 0.4622345566749573
    },
    {
      "name": "Video game",
      "score": 0.45409318804740906
    },
    {
      "name": "Presentation (obstetrics)",
      "score": 0.4400786757469177
    },
    {
      "name": "Cognitive science",
      "score": 0.3920994997024536
    },
    {
      "name": "Multimedia",
      "score": 0.3814069926738739
    },
    {
      "name": "Psychology",
      "score": 0.30589592456817627
    },
    {
      "name": "Artificial intelligence",
      "score": 0.22498798370361328
    },
    {
      "name": "Sociology",
      "score": 0.17945259809494019
    },
    {
      "name": "Mathematics education",
      "score": 0.17281436920166016
    },
    {
      "name": "History",
      "score": 0.1566985547542572
    },
    {
      "name": "Art",
      "score": 0.143979012966156
    },
    {
      "name": "Literature",
      "score": 0.1275767982006073
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79272384",
      "name": "University of Maryland, Baltimore County",
      "country": "US"
    }
  ]
}