{
  "title": "Improving Object Grasp Performance via Transformer-Based Sparse Shape Completion",
  "url": "https://openalex.org/W4214627646",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101542024",
      "name": "Wenkai Chen",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A5053592210",
      "name": "Hongzhuo Liang",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A5101705037",
      "name": "Zhaopeng Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055546056",
      "name": "Fuchun Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100326970",
      "name": "Jianwei Zhang",
      "affiliations": [
        "Universität Hamburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2156583822",
    "https://openalex.org/W6733181635",
    "https://openalex.org/W2824754393",
    "https://openalex.org/W2151631165",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W2794663059",
    "https://openalex.org/W2905288042",
    "https://openalex.org/W3119044505",
    "https://openalex.org/W6635231071",
    "https://openalex.org/W3034584726",
    "https://openalex.org/W3004351857",
    "https://openalex.org/W2962736495",
    "https://openalex.org/W2889969363",
    "https://openalex.org/W2997088169",
    "https://openalex.org/W3004167188",
    "https://openalex.org/W2975039016",
    "https://openalex.org/W2600030077",
    "https://openalex.org/W1510186039",
    "https://openalex.org/W2986303149",
    "https://openalex.org/W2962737955",
    "https://openalex.org/W2736534894",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2524140598",
    "https://openalex.org/W2205950488",
    "https://openalex.org/W2969090342",
    "https://openalex.org/W6601076540",
    "https://openalex.org/W2963188159",
    "https://openalex.org/W3109518641",
    "https://openalex.org/W2553307952",
    "https://openalex.org/W2886499109",
    "https://openalex.org/W3105259478",
    "https://openalex.org/W2967068439",
    "https://openalex.org/W3090032229",
    "https://openalex.org/W3131115577"
  ],
  "abstract": "Abstract Currently, robotic grasping methods based on sparse partial point clouds have attained excellent grasping performance on various objects. However, they often generate wrong grasping candidates due to the lack of geometric information on the object. In this work, we propose a novel and robust sparse shape completion model (TransSC). This model has a transformer-based encoder to explore more point-wise features and a manifold-based decoder to exploit more object details using a segmented partial point cloud as input. Quantitative experiments verify the effectiveness of the proposed shape completion network and demonstrate that our network outperforms existing methods. Besides, TransSC is integrated into a grasp evaluation network to generate a set of grasp candidates. The simulation experiment shows that TransSC improves the grasping generation result compared to the existing shape completion baselines. Furthermore, our robotic experiment shows that with TransSC, the robot is more successful in grasping objects of unknown numbers randomly placed on a support surface.",
  "full_text": "https://doi.org/10.1007/s10846-022-01586-4\nREGULARPAPER\nImprovingObjectGraspPerformanceviaTransformer-BasedSparse\nShapeCompletion\nWenkaiChen1 · HongzhuoLiang1 · ZhaopengChen2 · FuchunSun3 · JianweiZhang1\nReceived:28July2021/Accepted:28January2022\n© TheAuthor(s)2022\nAbstract\nCurrently, robotic grasping methods based on sparse partial point clouds have attained excellent grasping performance\non various objects. However, they often generate wrong grasping candidates due to the lack of geometric information\non the object. In this work, we propose a novel and robust sparse shape completion model (TransSC). This model has a\ntransformer-based encoder to explore more point-wise features and a manifold-based decoder to exploit more object details\nusing a segmented partial point cloud as input. Quantitative experiments verify the effectiveness of the proposed shape\ncompletion network and demonstrate that our network outperforms existing methods. Besides, TransSC is integrated into a\ngrasp evaluation network to generate a set of grasp candidates. The simulation experiment shows that TransSC improves the\ngrasping generation result compared to the existing shape completion baselines. Furthermore, our robotic experiment shows\nthat with TransSC, the robot is more successful in grasping objects of unknown numbers randomly placed on a support\nsurface.\nKeywords Robotic grasping · Point cloud · Sparse shape completion · Object segmentation\n1Introduction\nRobotic grasping evaluation is a challenging task due to\nincomplete geometric information from single-view visual\nsensor data [ 28]. Many probabilistic grasp planning models\nhave been proposed to address this problem, such as\nMotel Carlo, Gaussian Process and uncertainty analysis\n[10, 17, 26]. However, these analytic methods are always\ncomputationally expensive. With the development of deep\nlearning techniques, data-driven grasp detection methods\nhave shown great potential [ 4, 15, 22, 31] to solve this\nproblem. They generate lots of grasp candidates and\n/envelopebackHongzhuo Liang\nliang@informatik.uni-hamburg.de\n1 Technical Aspects of Multimodal Systems (TAMS),\nDepartment of Informatics, Universit ¨at Hamburg,\nHamburg, Germany\n2 Agile Robots AG, M¨unchen, Germany\n3 Beijing National Research Center for Information Science\nand Technology (BNRist), State Key Lab on Intelligent\nTechnology and Systems, Department of Computer Science\nand Technology, Tsinghua University, Beijing, China\nestimate the corresponding grasp quality, resulting in a\nbetter grasp performance and generalization. However, as\nmost of these methods still rely on original sensor input like\n2D (image) and 2.5D (depth map), there exists a physical\ngrasping defect when the gripper interacts with real object\nsurfaces or edges because of the incomplete pixel-wise\nand point-wise representations. Otherwise, traditional data-\ndriven grasping algorithms [ 15, 21, 22]are mostly based\non the partial point clouds. Due to the object’s missing\ngeometric and semantic information, these algorithms are\neasily to generate wrong grasp candidates and causing a\nresearch gap.\nTo improve grasp performance, the sparse point cloud\nis necessary to be restored or repaired to generate a\nbetter grasping interaction. Additional sensor input such\nas a tactile sensor is introduced to supplement original\nvision sensing [ 30]. However, object uncertainty still exists\nand extra sensor interference with the object will directly\naffect the final grasping result. Another strategy is to use\nshape completion to infer the original object shape while\ntraditional grasping-based shape completion methods use\na high-resolution voxelized grid as object representation\n[17, 18, 27], causing a high memory cost and information\nloss due to the sparsity of the sensory input. To avoid\nextra sensor cost and obtain complete object information,\n/ Published online: 26 February 2022\nJournal of Intelligent & Robotic Systems (2022) 104: 45\na novel transformer-based shape completion module is\nproposed in this work based on an original sparse point\ncloud. Compared with the traditional convolutional network\nlayer, the transformer has achieved state-of-the-art results\nin visual recognition and segmentation recently [ 11, 25],\nwhich enables our shape completion module to achieve\nbetter performance.\nAs illustrated in Fig. 1, we present a novel grasping\npipeline that uses a sparse point cloud to execute the grasp\ndirectly, without converting it into discrete voxel grids\nduring the shape completion process and then transforming\nit into a mesh in the grasp planning process. The pipeline\nconsists of two sub-modules: The transformer-based shape\ncompletion module and the grasp evaluation module. In\nthe first module, a non-synthetic segmented partial point\ncloud dataset based on YCB objects was constructed.\nNot cropping the object randomly or viewing the object\nin a physical simulator, our dataset contains many real\ncameras and environmental noise, which guarantees an\nimproved grasping interaction in a real robot environment.\nBased on this dataset, we propose a novel point cloud\ncompletion network (TransSC), where the segmented partial\npoint cloud of an object is input, and the complete point\ncloud is output. In the second module, our previous work\n[15] is involved. We use PointNet [ 24] to obtain feature\nrepresentation of the repaired point cloud and build a\ngrasp detection network to generate and evaluate a set of\ngrasp candidates. The grasp with the highest score will\nbe executed in a real robot experiment. The proposed\npipeline is validated in a simulation experiment and robotic\nexperiments, which demonstrate that our shape completion\npipeline can significantly improve grasping performance.\nOur contributions in this paper can be listed as:\n– A large-scale non-synthetic partial point cloud dataset\nis constructed based on the YCB-Video dataset. As the\ndataset is based on 3D point cloud data captured by a\nreal RGB-D camera, the noise that comes from it will\nfacilitate the generalization of our work, especially in\nreal robot environments.\n– A novel point cloud completion network TransSC\nis proposed. The transformer-based encoder and\nFig. 1 Overview of our shape completion based grasp pipeline. The\ntop row shows the shape completion module. In this module, a seg-\nmented partial point cloud ζp with n points is first input into a\ntransformer-based encoder to extract point-wise and self-attention\nfeatures, which outputs a latent vector with m dimensions. Then,\nthe latent vector is concatenated with another latent feature from a\nflat/spatial point seed generator to predict multiple spatial surfaces in\nthe manifold-based decoder. Finally, these surfaces are assembled into\na complete point cloud ζ\nc. The bottom row is the grasp evaluation\nmodule, the complete point cloud ζc is the input of our grasp detec-\ntion pipeline PointNetGPD to compute the grasp quality Qi. The grasp\nwith the highest score Gbest will be sent to calculate a collision-free\ntrajectory and will be executed in a real robot experiment\n45   Page 2 of 14 J Intell Robot Syst (2022) 104: 45\nmanifold-based decoder are introduced into the shape\ncompletion task to improve its performance.\n– Combining our previous work PointNetGPD for grasp\nevaluation and the MoveIt Task Constructor for motion\nplanning, we demonstrate that a robust grasp planning\npipeline using the shape completion result as input can\nachieve a better grasp planning result compared to the\nsingle view without shape completion work.\nThe paper is organized as follows. We first contrast\nrelated approaches for visual grasping, dense point cloud\ncompletion and traditional robotic grasping strategies based\nshape completion in Section 2. Then, we propose our prob-\nlem formulation in Section 3. Furthermore, we explain the\ndifferent components of our grasping evaluation approach:\ndataset construction, transformer-based shape completion\nnetwork architecture and grasping detection module in\nSection 4. After that, we evaluate our method through quan-\ntitative evaluation, simulation grasping experiments and real\nrobotic experiments on single-object and object-occlusion\nscenes in Section 5. Finally, our conclusion and future work\nis drawn in Section 6.\n2RelatedWork\nDeep Visual Robotic Grasping With the development of\ndeep learning, many methods for deep visual grasping have\nbeen proposed. Similar to 2D object recognition, monocular\ncamera images were firstly used to predict the probabil-\nity that the input grasps were successful [ 14]. In [5]a n d\n[26], a single RGB-D image of the target object was used to\ngenerate a 6D-pose grasp and effective end-effector trajec-\ntories. However, this work is not suitable to deal with sparse\n3D object information and spatial grasps. Compared with\nthe 2D feature representations from images, 3D voxel\nor point cloud data could provide robotic grasping with\nmore semantic and spatial information. Given a synthetic\ngrasp dataset, [4] transformed scanning 3D object infor-\nmation into Truncated Signed Distance Function (TSDF)\nrepresentations and passed them into a V olumetric Grasp-\ning Network (VGN) to directly output grasp quality, gripper\norientation and gripper width at each voxel. Wu et al.\n[31] designed a special grasp proposal module that defines\nanchors of grasp centers and related 3D grid corners to pre-\ndict a set of 6D grasps from a partial point cloud. Based\non the scaled point cloud, [ 22] used hand-crafted outline\nfeatures and a CNN-based method to build a grasp qual-\nity evaluation model. In our previous work [ 15], we used\nPointNet [24] to extract raw point cloud features and built\na grasp evaluation network, which performs great in robotic\ngrasping experiments. However, due to the lack of complete\ngeometric information on the object, we found that some\ngrasp candidates are still infeasible and cause a collision\nwith the object.\nDense Point Cloud CompletionThe task of point cloud\ncompletion has been attracting more and more attention in\nthe field of computer vision. Yuan et al. [ 36] firstly used\nMulti-layer Perceptrons (MLP) to extract the local geome-\ntric features of point clouds to accomplish the reconstruc-\ntion. Groueix et al. [ 9] introduced a morphing learning\nstrategy to generate different shapes of 3D surfaces, which\nshows great potential for point cloud and voxel reconstruc-\ntion. Liu et al. [16] combined the above work and proposed\na morphing and sampling network, which shows a higher\nfidelity and quality for the dense point cloud. Furthermore,\n[34] proposed a Gridding Residual Network to restore more\nstructural details, especially for the dense point cloud. How-\never, these methods cannot be applied to robotic research\ndirectly because all trained objects in their datasets are at the\nsame pose and status. This would create dense point cloud\nmodels too complicated to pursue the details of the point\ncloud. It is better to restore a sparse completion of object\nsurfaces for robotic tasks.\nShapeCompletionforRoboticGrasping For robotic grasp-\ning, the critical challenge is recognizing objects in 3D space\nand avoiding potential perception uncertainty. When the\nRGB-D camera captures an object from a particular view-\npoint, the 3D information on the object is incomplete, which\nmeans a lot of semantic and spatial information is missing.\nThe missing of complete 3D object information will lead\nto the grasp generation process generating wrong grasping\nposes.\nRecently, researchers have proposed to use shape com-\npletion to enable robotic grasping. In [ 27], the observed\nobject from 2.5D range sensors was firstly converted to oc-\ncupancy voxel grid data. Then the voxelized data were input\ninto a CNN and formed a high-resolution voxel output. Fur-\nthermore, the completion result was transformed into mesh\nand then loaded into Graspit! [ 20] to generate a grasp.\nLundell et al. [ 17] used dropout layers to modify the net-\nwork, which enabled the prediction of shape samples at run-\ntime. Meanwhile, Monte Carlo Sampling and probabilistic\ngrasp planning were used to generate grasp candidates. As\ntraditional analytic grasping methods are computationally\nexpensive, [18] combined the shape completion of a voxel\ngrid and a data-driven grasping planning strategy (GQCNN)\n[19] to propose a structure called FC-GQCNN, where syn-\nthetic object shapes were obtained from a top-down physics\nsimulator and grasps were generated from depth images.\nTraditional grasp-based shape completion solutions mainly\nconcentrate on completing a single object from different\ncamera views, while they hardly consider the lack of geome-\ntric information caused by occlusion from other objects.\nPage 3 of 14    45J Intell Robot Syst (2022) 104: 45\nIn conclusion, traditional grasp shape completion meth-\nods mainly voxelized the 2.5D data into occupancy grids\nor distance fields to train a CNN. However, these high-\nresolution voxel grids will entail a high memory cost.\nMoreover, detailed semantic information is often lost in\nthe form of occlusions of other objects, which causes\nmeaningful geometric features of objects not to be learned\nfrom the neural network. We propose a transformer-based\nshape completion module to obtain the complete geomet-\nric features and retain original object information. Without\nconverting the observed partial point cloud into the voxel\ngrid and mesh, our completion method segments the sparse\npoint cloud of the target object and outputs a repaired point\ncloud at arbitrary resolution, which outperforms existing\nmethods. Furthermore, PointNet [ 24] is introduced for the\nrepresentation learning of the repaired point cloud and a\ngrasp evaluation network is constructed to generate grasp\ncandidates. Finally, our grasp evaluation pipeline achieves\na better grasping performance than the baseline method\nwithout point cloud completion.\n3ProblemFormulation\nWe consider a setup consisting of a robotic arm with\nparallel-jaw grippers, an RGB-D camera, and objects of\nunknown number that are set on a flat support surface while\nwe define a target object via user input. Meanwhile, we\nassume that the RGB-D camera could capture the depth map\nof objects, where a semantic segmentation network is used\nto extract the mask of the target object and convert it into\na 2.5D partial point cloud\nP ∈ RN×3. For simplicity, all\nspatial quantities are in camera coordinates.\nGiven a gripper configuration C and camera observation\nO, our goal is firstly to extract the target object point\ncloud P using semantic segmentation. Then a point cloud\ncompletion network is used to repair the segmented 2.5D\npartial point cloud P ∈ RN×3, turning it into a complete\n3D point cloud Pc ∈ RN×3. After that, a grasp evaluation\nnetwork based on Pc is used to predict a set of grasp\ncandidates Gi and compute the relative grasp quality Qi.\nThe grasp with the highest score Gbest and highest kinematic\npossible, i.e., a collision-free grasp, will be executed in the\nreal robot experiment.\n4RoboticGraspingEvaluationviaShape\nCompletionandGraspDetection\n4.1DatasetConstruction\nTraditional shape completion models use synthetic CAD\nmodels from the ShapeNet [ 35]o rM o d e l N e t[32] datasets\nto generate partial and corresponding complete point cloud\ndata, while these synthetic data contain no real-world noise.\nAs a result, synthetic data often do not work well in the\nreal world. To tackle this problem, we summarize a shape\ncompletion dataset from the YCB-Video Dataset [ 33]. Non-\nsynthetic RGB-D video images (∼ 133,827 frames) in the\nYCB-Video Dataset are firstly chosen, while most of them\nvary insignificantly. Thus, a preprocessed image dataset\nis obtained by reducing every five frames. Meanwhile, to\ncover distinguishable shapes with different levels of detail,\n18 objects are also chosen from the YCB-Video dataset.\nIn this work, the ground-truth point cloud of 18 objects is\ncreated by the farthest point sampling (FPS) of 2048 points\non each object model. Not randomly sampling or cropping\ncomplete point clouds on the unit sphere to get partial\npoint clouds, RGB-D images and related object label images\nin the preprocessed dataset are loaded to compute the\nmatching partial point clouds using related camera intrinsic\nparameters. To approximate the distribution of point cloud\ndata of real objects and retain the semantic information,\na large number of cameras and environmental noise data\nare kept on, though a small radius is used to remove\npartial outliers. For the convenience of network training, the\npartial point clouds are also unified into the size of 2048\npoints by FPS or replicating points. To enable an accurate\ncomparison with existing baselines, the canonical center of\nthe partial point cloud of each object is transformed into the\ncanonical center of the ground-truth point cloud using pose\ninformation. Finally, more than 70,000 partial point clouds\nare collected in our dataset. Compared to other synthetic\npoint cloud datasets, our dataset also does well at preserving\nthe real point cloud distribution of occluded objects.\n4.2SemanticSegmentation\nAs shown in Fig. 1, the scene of our grasping task is that\nobjects of unknown number are set on a flat support surface.\nTo obtain the target object point cloud, we first build\na semantic segmentation network branch, where different\nYCB objects are assigned a particular semantic label value.\nIt can be seen that the performance of the segmentation\nnetwork is good enough that it can also be deployed in a\ngrasping task of multi-object occlusion.\nOur segmentation network [2] takes an RGB image as\ninput and outputs a binary mask of the expected object.\nThe network has an encoder-decoder architecture based on\nCNN, where the encoder consists of convolutional layers\nwith ReLU activation followed by max-pooling layers. At\nthe same time, the decoder utilizes unpooling operations\nwhereby the pooling indices from the corresponding\nencoder layers are recalled. Convolutional layers again\nfollow this upsampling strategy. Moreover, several data\naugmentation strategies like adjusting brightness, contrast\n45   Page 4 of 14 J Intell Robot Syst (2022) 104: 45\nand saturation are used to make the network generalize\nwell. After getting the expected object mask, the sparse\n2.5D point cloud\nP ∈ RN×3 of the target object\ncould be extracted through the corresponding depth image.\nMeanwhile, we also remove the redundant background\n(support surface) point cloud by setting a threshold value of\nthe z-axis (support surface height).\n4.3Transformer-BasedEncoderModule\nAs shown in Fig. 2, we compare our proposed encoder mod-\nule with several common competitive methods. Multi-layer\nPerception (MLP) is a simple baseline architecture to extract\npoint features. This method maps each point into different\ndimensions and extracts the maximum value from the final\nK dimensions to formulate a latent vector. A simple gen-\neralization for MLP is to combine semantic features from\na low-level dimension with those of a high-level dimen-\nsion. The MSF (Multi-scale Fusion) [13] module inflates the\ndimension of the latent vector from 1024 to 1408 to obtain\nsemantic features from different dimensions. To improve the\nperformance of the feature extractor, L-GAN [ 1] proposed\nto use a Maxpooling layer appropriately. Concatenated Mul-\ntiple Layer Perception (CMLP) [ 12] maxpools the output\nof the last k layers to guarantee that multi-scale feature\nvectors are concatenated directly. An overview of our pro-\nposed Transformer-based multi-layer perception (TMLP)\nmodule is shown in Fig. 2(d). Without an extra skip con-\nnection structure and a maxpooling operation from different\nlayers, the Multi-head Self-attention (MHSA) [ 29] module\nis introduced to replace the traditional convolutional layer\n[128 × 256 × 1].\nMHSA aims to transform (encode) the input point feature\ninto a new feature space, which contains point-wise and\nself-attention features. Figure 2(e) shows a simple MHSA\narchitecture used in TMLP, which includes two sub-layers.\nIn our first layer, the multi-head number is set to 8 and the\ninput feature dimension for each point is 128. Unlike natural\nlanguage processing (NLP) problems, the 128-dimensional\nfeature vector\nAin ∈ R2048×128 will enter into the multi-\nhead attention module directly without positional encoding.\nThis is because each point in the point cloud has its unique\nx − y − z coordinates. The output feature\nZ is formed\nby concatenating the attention of each attention head. A\nresidual structure is also used to add and normalize the\noutput feature\nZ with Ain. This process can be formulated\nas follows:\nAi = SAi(Ain)i = 1, 2, ..., 8( 1 )\nZ = concat( A1, A2, ..., A8) ∗ W0 (2)\nAout = Norm( Ain + Z) (3)\nwhere SAi represents the i-th self-attention layer, each has\nthe same output dimension size with input feature vector\nAin,a n dW0 is the weight of the linear layer. Aout represents\nthe output point-wise features of the first sub-layer.\nThe second sub-layer is called Feed-forward module,\nwhich is a fully connected network. Point-wise features\nAout are processed through two linear transformations and\none ReLU activation. Furthermore, a residual network is\nalso used to fuse and normalize the output features. Finally,\nwe can get the MHSA module output FFout ∈ R2048×128\nas:\nFF = ReLU( Aout ∗ W1 + b1) ∗ W2 + b2 (4)\nFFout = Norm( Aout + FF) (5)\nFig.2 Illustration of various encoder structures for point cloud com-\npletion. ( a) is a simple multiple-layer perception (MLP) structure.\n(b) is a multi-scale fusion (MSF) module, which can fuse features\nfrom different layers directly. (c) is concatenated multiple layer per-\nception (CMLP), which can also concatenate multi-dimensional latent\nfeatures while the max pooling operation is used to extract latent fea-\ntures further. (d) shows our Transformer-based multiple layer percep-\ntion (TMLP) module, which integrates the Multi-head Self-attention\n(MHSA) module into the MLP structure. ( e) depicts the architecture\nof the MHSA module\nPage 5 of 14    45J Intell Robot Syst (2022) 104: 45\nwhere W1, W2 and b1, b2 represent the weight and bias value\nof the corresponding linear transformation, respectively.\n4.4Manifold-BasedDecoderModule\nInspired by the AtlasNet [9], a manifold-based decoder\nmodule is designed to predict a complete point cloud from\npartial point cloud features. As shown in Fig. 3, a complete\npoint cloud could be assumed to consist of multiple sub-\nsurfaces. Therefore, we only concentrate on obtaining each\nsub-surface, then we gather them and make an appropriate\nmontage to form the final complete point cloud. To obtain\neach sub-surface, a point seed generator is used to concate-\nnate with global feature vector\nPg ∈ R2048×1024 output\nfrom the encoder, where point initialization values are com-\nputed from a flat (f )or spatial (g) sampler. As the coordi-\nnate values of the ground-truth point cloud are limited to\nbetween [−1, 1], point initialization values are also limited\nin this range. After that, the concatenated feature vector\nPconcat ∈ R2048×M (M = 1026 or 1027) is input into K\nconvolutional layers, where all sampled 2D or 3D points\nwill be mapped to 3D points on each sub-surface. In our de-\ncoder, the sub-surface number is set to 16. Unlike other\nvoxel-based shape completion methods, our decoder modu-\nle achieves an arbitrary resolution for the completion results.\nEvaluation Metrics To evaluate our shape completion\nresults, we used two permutation-invariant metrics called\nChamfer Distance (CD) and Earth Mover’s Distance (EMD)\nas our evaluation goal [7]. Given two arbitrary point clouds\nS\n1 and S2, CD measures the average distance from each\npoint in one point cloud to its nearest point coordinates in\nthe other point cloud.\ndCD(S1,S 2) = 1\nS1\n∑\nx∈S1\nmin\ny∈S2\n∥x −y∥2\n2 + 1\nS2\n∑\ny∈S2\nmin\nx∈S1\n∥y −x∥2\n2\n(6)\nWhile Earth Mover’s Distance considers two equal point\nsets S1 and S2 and is defined as:\ndEMD (S1,S 2) = min\n∅:S1→S2\n1\nS1\n∑\nx∈S1\n∥x −∅ (x)∥ 2 (7)\nCD has been widely used in most shape completion\ntasks because it is efficient to compute. However, EMD\nis chosen as our completion loss because CD is blind\nto some visual inferiority and ignores details easily [ 1].\nWith ∅:S 1 → S2 being bijective, EMD could solve the\nassignment and transformation problem in which one point\ncloud is mapped into another.\n4.5PointNetGPDBasedGraspingDetectionModule\nGiven the complete point cloud from the previous steps,\nwe put the point cloud into a geometric-based grasp pose\ngeneration algorithm (GPG) [23], which outputs a set of\ngrasp proposals\nGi. We then transform Gi into a gripper\ncoordinate system and use points inside the gripper as\nthe input of PointNetGPD, a data-driven grasp evaluation\nframework. The output grasp will then be sent to the MoveIt\nTask Constructor [8] to plan a feasible trajectory for a pick\nand place task.\nPointNetGPD [15] is trained on a grasp dataset generated\nusing a reconstructed YCB object mesh and evaluates the\ninput grasp quality. The grasp candidates in the grasp dataset\nall proceeded collision-free to the target object. As a result,\nthe grasp evaluation network assumes that all the input grasp\ncandidates are not colliding with the object. If the object\nhas occlusion due to the camera viewpoint, the current\ngeometric-based grasp proposal algorithm will generate\ngrasp candidates that collide with the object. Thus, using a\ncomplete point cloud could ensure that the grasp candidate\ngeneration algorithm generates grasp sets that do not collide\nwith the graspable objects. Figure 4 shows the comparison\nFig. 3 Illustration of the decoder structure for point cloud comple-\ntion. The feature vector with m dimensions from the encoder is firstly\nconcatenated with a latent feature from a special point seed gen-\nerator f or g. Then three convolutional layers as the backbone are\nused to extract features and form different manifold-based surfaces,\nrespectively. Finally, these surfaces are gathered and montaged into a\ncomplete point cloud\n45   Page 6 of 14 J Intell Robot Syst (2022) 104: 45\nFig.4 Comparison of grasp candidates generated using GPG. ( a) RGB image to show the example environment, ( b) grasp generated with partial\npoint cloud, (c) grasp generated with complete point cloud\nof the grasp generation result using GPG [ 23] with and\nwithout point cloud completion, where Fig. 4(b) shows a\ncandidate generated using a partial point cloud and Fig. 4(c)\nshows a grasp candidate generated using a complete point\ncloud. We can see that the grasp in Fig. 4(b) collides with\nthe real object while Fig. 4(c) avoids generating that kind of\ngrasp.\n5Experiments\n5.1QuantitativeEvaluationofProposedShape\nCompletionNetwork\nTraining and Implementation DetailsTo evaluate model\nperformance and reduce training time, eight categories of\ndifferent objects in our dataset are chosen to train the\nshape completion model. The training set and validation\nset are split into 0.8:0.2. We implement our network on\nPyTorch. All the building modules are trained using the\nAdam optimizer with an initial learning rate of 0.0001 and\na batch size of 16. All the parameters of the network are\ninitialized using a Gaussian sampler. Batch Normalization\n(BN) and ReLU activation units are all employed at the\nencoder and decoder module except the final tanh layer\nproducing point coordinates, and Dropout operation is used\nin the MHSA module to suppress model overfitting.\n5.1.1ComparisonwithExistingMethods\nIn this subsection, we compare our method against several\nrepresentative baselines that are also used for point cloud\ncompletion, including AtlasNet [ 9], MSN [16] and GRNet\n[34]. The Oracle method means that we randomly resample\n2048 points from the original surface of different YCB\nobjects. Corresponding EMD and CD distances between\nthe resampled point cloud and the ground-truth point cloud\nprovide an upper bound for the performance. Relative\ncomparison results are shown in Tables 1 and 2. Our method\nis developed into two models based on the different point\nseed generators (f/g) in the decoder module. It can be\nseen that our method outperforms other methods in most\nobjects on both EMD and CD distances. Though for some\nobjects like banana and cracker box, the evaluation metrics\nof Earth Mover’s Distance and Chamfer Distance from\nour both models are bigger than other baselines. However,\nfor other objects in our datatset, our flat/spatial models\nboth achieve a better performance than other baselines.\nTable 1 Comparison of earth mover’s distance with different sparse point cloud completion models for 2048 points and multiplied by 10 3\nModel Cracker box Banana Pitcher base Bleach cleanser Bowl Mug Power drill Scissors Average\nOracle 3.4 1.7 4.6 2.9 1.9 2.0 3.8 1.5 2.7\nAtlasNet [ 9] 9.7 4.9 10.5 10.0 8.8 5.3 15.0 5.2 8.7\nMSN (fusion) [ 16] 10.7 4.6 12.4 14.0 11.5 12.9 23.4 5.3 11.8\nMSN (vanilla) [ 16] 11.0 3.8 9.3 8.3 10.2 3.9 5.9 3.4 7.0\nGRNet (sparse) [ 34] 8.4 4.3 8.8 6.0 6.0 4.3 5.8 4.5 5.3\nOur (flat) 8.5 3.9 9.4 6.7 6.0 3.7 5.2 4.1 4.9\nOur (spatial) 10.1 4.4 8.4 5.8 5.6 3.7 7.0 3.9 6.1\nBold values indicate the best performance\nPage 7 of 14    45J Intell Robot Syst (2022) 104: 45\nTable 2 Comparison of chamfer distance in different sparse point cloud completion models for 2048 points and multiplied by 10 3\nModel Cracker box Banana Pitcher base Bleach cleanser Bowl Mug Power drill Scissors Average\nOracle 0.24 0.52 0.28 0.12 0.10 0.09 0.13 0.38 0.23\nAtlasNet [ 9] 4.51 0.87 4.97 5.61 4.21 1.37 6.18 0.92 3.58\nMSN (fusion) [ 16] 5.59 1.25 5.71 2.77 10.81 1.77 8.34 1.58 4.73\nMSN (vanilla) [ 16]6 . 0 1 0.71 4.01 4.68 7.51 0.76 1.28 0.38 3.17\nGRNet (sparse) [ 34] 2.28 0.97 3.78 1.67 2.85 0.76 1.48 0.88 1.90\nOur (flat) 3.28 0.92 4.09 1.50 2.55 0.66 1.25 0.82 1.88\nOur (spatial) 5.81 0.87 3.19 1.20 2.79 0.69 2.54 0.66 2.22\nBold values indicate the best performance\nMore importantly, the final average evaluation metrics of\nEarth Mover’s Distance and Chamfer Distance of Our(flat)\nmodel are both the best evaluation results. For the same\ncompletion loss function, our (flat) model achieves an\naverage of about 9% improvement in terms of the EMD\ndistance to the latest GRNet model. Since our dataset\ncontains much noise from the camera and the environment,\nwe found that fusing the output completion result with the\noriginal point cloud makes the performance significantly\nworse, which can be seen from the comparison of MSN\n(fusion) and MSN (vanilla). It also implies that our model is\nrobust enough, which is conducive to rapid deployment in\nreal robot experiments. Furthermore, compared with ideal\nresults from the Oracle method, it demonstrates that point\ncloud completion remains an arduous task to solve.\nTo understand the computational complexity of the pro-\nposed transformer-based model, we analyse the floating-\npoint operations(FLOPs) and the number of network para-\nmeters and summarize in Table 3. It can be seen that the\nself-attention module introduced in our transformer-based\nencoder is lighter than traditional convolution layer, redu-\ncing the computational complexity. Moreover, after remov-\ning a large number of redundant convolution layers existing\nin traditional dense shape completion, our FLOPs value is\nalso decreased significantly.\n5.1.2AblationStudies\nThis section provides a series of ablation studies on our\nYCB-based dataset to evaluate our proposed shape comple-\ntion model comprehensively. Accordingly, the effectiveness\nof each particular module in our model is analyzed as\nfollows: We first evaluate our transformer-based encoder\nTable 3 Number of FLOPs and network parameters\nMethod AtlasNet MSN MSN(fusion) GRNet Ours\n# Params (M) 29.46 30.32 33.65 76.71 30.02\n# FLOPs (GMac) 14.36 21.46 —— 25.90 9.87\nmodule with other representative encoder modules under the\nsame setting of convolutional/transformer layer number and\nobject inputs. As shown in Table 4, our encoder has a bet-\nter result overall, though CMLP gets a great result on Mug’s\ncompletion. When the point seed in the decoder is flat, we\nfurther analyze the influence of different point seed distribu-\ntions and surface numbers in Tables 5 and 6. We can see that\nboth Uniform and Gaussian sample methods can achieve\na better result at (0, 1). We choose Uni form(0, 1) in our\nmodel to achieve the best results. Like the weight parame-\nters in the neural network, the initialization value of points\ncannot be close to zero, which predicts the worst result.\nAs illustrated in Table 6, when the sub-surface number\nincreases, the overall model performance improves. How-\never, the improvement of completion results is limited when\nthe number is above 16.\n5.1.3VisualizationAnalysis\nFigure 5 shows the visualized shape completion results\nusing our TransSC. In the visual analysis, each object’s\ninput partial point cloud is first preprocessed to remove\nnoisy data from the camera and the environment. It can\nbe seen that the geometric loss of the input point cloud in\nour dataset comes from the change of the camera viewpoint\nand the occlusion by other objects, which causes a big\nchallenge for our model. The output results of the canonical\npose show that our model works well on all simple and\ncomplex objects. Moreover, our model can generate realistic\nstructures and details like the mug handle, bowl edge and\nbottle mouth. In robotic grasping, as the target object pose\nis randomly put on the support surface, another shape\ncompletion model based on the arbitrary ground-truth pose\nis retrained. This is done by transforming the ground truth\npose to the original pose of the input partial point cloud. The\ncompletion results are also shown in Fig. 5. Arbitrary output\nis not as good as the canonical output while it still restores\nthe overall shape of each object well. It also demonstrates\nthat achieving object completion of arbitrary poses in a real\nenvironment is still a formidable task.\n45   Page 8 of 14 J Intell Robot Syst (2022) 104: 45\nTa\nble 4 Comparison of EMD and CD from different encoder structures\nEarth Mover’s distance (EMD) MLP CMLP MSF TMLP Chamfer distance (CD) MLP CMLP MSF TMLP\nMug 6.01 3.69 9.45 3.69 Mug 2.15 0.65 13.80 0.66\nBleach cleanser 10.51 8.10 11.70 6.70 Bleach cleanser 6.88 2.63 13.89 1.50\nBold values indicate the best performance\nTable 5 Comparison of\naverage EMD and CD from\ndifferent point generators\nSimilarity metrics Uniform distribution: Gaussian distribution: ZERO\n0:1 − 0.5:0.5 − 1:1 0.5,0.5/3 0,0.5 0,1\nAvg EMD 5.94 7.09 6.50 6.34 6.15 6.14 9.88\nAvg CD 1.89 3.25 2.42 2.39 2.38 2.12 6.17\nBold values indicate the best performance\nTable 6 Influence of different surface numbers in the decoder\nEarth Mover’s distance (EMD) n =4n =8n =16 n=32 Chamfer distance (CD) n=4n =8n =16 n=32\nMug 4.71 3.94 3.70 3.61 Mug 9.01 6.70 6.61 6.69\nBleach cleanser 10.10 7.82 6.69 5.94 Bleach cleanser 3.69 1.70 1.51 1.53\nBold values indicate the best performance\nFig.5 Shape completion result\nusing TransSC. The canonical\npose result is trained under a\nfixed point cloud coordinate\nsystem while the arbitrary pose\nresult is trained under the\ncamera perspective. In the robot\nexperiment, the arbitrary pose\ntraining result is used to\ngenerate grasps\nPage 9 of 14    45J Intell Robot Syst (2022) 104: 45\n5.2SimulationGraspExperimentswithComplete\nShape\nExperimental Setup of Simulation Experiments We use\nGraspit! [20] to evaluate the quality of shape completion\nsimilar to [27]. First, the Alpha shapes algorithm [6]i s\nused to implement surface reconstruction of the completion\nobject. The output 3D mesh is then imported into GraspIt!\nSimulator to calculate grasps. To have a fair comparison, we\nalso use a Barrett Hand to generate grasps. After finishing\nthe grasp generation, we remove the completion object\nand import the ground-truth object into the same place.\nMeanwhile, the Barrett Hand is moved back 20 cm along\nthe approach direction and then approaches the object until\nthe gripper detects a collision or reaches the calculated grasp\npose. Furthermore, we adjust the gripper to the calculated\ngrasp joint angles and perform the auto-grasp function in\nGraspIt! to ensure the gripper makes contact with the object\nsurface or reaches the joint limit. The different values of\njoint angles at different positions are then recorded. We use\nfour objects (bleach cleanser, cracker box, pitcher base and\npower drill) from the YCB objects set and calculate 100\ngrasps for each object in our experiment.\nAssuming the grasp pose is the same, we compare\nthe average difference of the joint angle from our shape\ncompletion model to that of Laplacian smoothing in\nMeshlab (Partial), mirroring completion [ 3] (Mirror) and\nvoxel-based completion [27]. Note that we use two different\nmodels, canonical and arbitrary. The canonical model\nmeans all the training is transformed into the same object\ncoordinate system and the arbitrary model means all the\ntraining data are transformed into the camera’s coordinate\nsystem. Although we can see from Fig. 5 that the canonical\nmodel has a better shape completion result, it requires\na 6D pose of the target object if we want to map the\ncomplete point cloud into the real environment. To avoid\nthis complication of adding a 6D pose estimation module\nand achieve real robot experiments, the arbitrary model is\nalso trained. The simulation result is shown in Table 7.I t\ncan be seen that Ours (canonical) gets the best simulation\ngrasping performance, which outperforms other completion\ntypes. Ours (arbitrary) also obtains a great simulation result\nthough its average joint angle is slightly smaller than voxel-\nbased methods. Moreover, the average difference between\nthe two models also demonstrates that a perfect shape\nc o m p l e t i o ni na na r b i t r a r yp o s ei sm u c hh a r d e rt h a ni na\ncanonical pose.\n5.3RoboticExperimentsonSingleObjects\nExperimental Setup of Single Objects To evaluate the\nperformance improvement using a complete point cloud for\nrobotic grasping, we choose six YCB objects to test the\ngrasping success rate. The robot for evaluation is a UR5\nrobot arm equipped with a Robotiq 3-finger gripper. The\nvision sensor is an Industrial 3D camera from Mechmind\n1 to\nacquire a high-quality partial point cloud. The selected six\nobjects are listed in Table 8. We select these objects because\nthey are typical objects that may fail to generate good grasp\ncandidates without shape completion. Other objects such as\na banana or a marker are quite simple and small, so that\nimprovement of shape completion on the grasping result is\nminor. In our robotic experiments, each YCB object is firstly\nplaced on the center of flat table and then moves randomly\nas long as it can appear in the field of the vision sensor and\nwithin the executable range of our UR5 robot arm.\nFor the selected six objects, we perform grasp evaluation\nbased on PointNetGPD [15] on two different methods:\nWithout our shape completion (WOSC) and with our\nshape completion (WSC). We run the robot experiment by\nrandomly putting the object on the table and grasping it\nten times, then calculating the success rate. The experiment\nresult is shown in Table 8. We can see that all six objects’\ngrasp success rates from our grasp pipeline outperform or\nare even with the original method. The low success rate of\nthe power drill for both methods is due to the contact area of\nthe power drill head being too slippery for the robot to grasp.\nThe failures of WOSC with the observed point cloud input\nare mainly due to the limit of the camera viewpoint, and\nGPG generates grasp candidates that sink into the object. An\nexample of this situation is shown in Fig. 4, which is strong\nevidence that our shape completion model can improve the\ngrasp success rate in some particular objects.\n5.4RoboticExperimentsonObjectOcclusion\nExperimental Setup of Object OcclusionWhen there are\ndifferent objects on the flat table, the occlusions from other\nobjects will cause a lack in geometric information on the\ntarget object. To simulate this scene, we choose bleach\ncleanser as the target object and other YCB objects are\npicked as a potential occluder where occluder as foreground\nis placed directly in front of the target object. All objects\nare placed in a natural vertical position while the horizontal\ndistance between the two types of objects is set to 8 cm. The\nexperimental objects and segmentation result of the target\nobject can be seen from Fig. 6. The robot arm and camera\nare the same as in the robotic experiment on the single\nobject. Furthermore, in real experiments, the target object is\nplaced near the center of table to ensure that vision camera\ncould capture it accurately and then we randomly change\nthe 6D pose of target object to grasp ten times.\nAs shown in Fig. 7, we compare the grasping perfor-\nmance of WOSC and WSC when five different YCB objects\n1https://en.mech-mind.net/\n45   Page 10 of 14 J Intell Robot Syst (2022) 104: 45\nTa\nble 7 Comparison of\naverage difference between\ngrasp joints from different\ncompletion types\nError Partial Mirror V oxel-based Ours (canonical) Ours (arbitrary)\nGrasp joint (degree) 10.07 4.42 2.17 1.15 2.02\nBold values indicate the best performance\nTable 8 Robotic grasping\nperformance on a single object Method Cracker box Mug Meat can Pitcher base Bleach cleanser Power drill Average\nWOSC 70% 70% 80% 80% 90% 40% 71.67%\nWSC 80% 100% 100% 80% 90% 50% 83.33%\nFig.6 The target object and segmentation result with different occlusion settings\nFig.7 Grasping performance comparison when target object is behind\ndifferent occluders\nFig. 8 Grasping performance comparison when target object is in\ndifferent occlusion ratio\nPage 11 of 14    45J Intell Robot Syst (2022) 104: 45\nocclude the target object (bleach cleanser). The average suc-\ncessful grasping rate of WSC is 88% while WOSC is 50%.\nIt demonstrates that our shape completion method can sig-\nnificantly increase the successful grasping rate up to 32%\ncomparing original grasping strategy. However, we found\nthat some irregularly shaped objects like the Mustard bottle\nand Power drill will divide the original partial point cloud of\nthe target object into multiple surface parts. Because Point-\nNetGPD [15] cannot understand that these separated point\nclouds are from the same object, WOSC generates more\nwrong grasp candidates without our shape completion. Fur-\nthermore, we explored the effect of the occlusion ratio on\nthe grasp performance through stacking different blocks in\nfront of the target object as an occlusion. Because the tar-\nget object and stacking blocks are all placed on the table\nvertically and the horizontal length of each block is big-\nger than the maximum horizontal width of target object, the\nocclusion ratio is calculated through measuring the verti-\ncal height of stacking blocks (\nHb) and target object ( Ht).\nAs seen from Fig. 7, we conducted six experiments with\nan occlusion rate between 0.2 and 0.9 to compare the two\nmethods. When the occlusion ratio is less than 0.6, the\ngrasping success rate of WSC is significantly improved over\nthat of WOSC. However, because there are few high occlu-\nsion scenes in the YCB video dataset, it is still difficult for\nTransSC to repair the partial point cloud, especially when\nthe occlusion ratio is higher than 0.8. Furthermore, when the\nocclusion ratio is between 0.8 and 1.0, it means that target\nobject has been completely obscured. The vision informa-\ntion of target object is too little, so it’s also much difficult\nto use shape completion to restore complete object informa-\ntion. According to our observation in daily life, we found\n0.2-0.6 is also the most common object occlusion ratio and\nour experiments showed that our shape completion method\ncould improve successful rate within this range (Fig. 8).\n6ConclusionandFutureWork\nWe present a novel transformer-based sparse shape com-\npletion network (TransSC). This network includes a\ntransformer-based encoder and manifold-based decoder that\nwe designed, enabling our model to achieve a great com-\npletion result and outperform other representative methods.\nThe experiments show that our network is robust to sparse\nand noisy point cloud input. Besides, simulation grasp-\ning experiments show our model could achieve a smaller\ngrasp joint error than traditional robotic completion meth-\nods. Finally, when executing real robotic experiments of\nsingle objects and object occlusion, we demonstrate that\nour TransSC can be easily embedded into a existing grasp\nevaluation module and improve grasping performance sig-\nnificantly in both scenes.\nThe lack of object geometric information in our dataset\nis due to the change of the camera viewpoint and the\nocclusion by different objects. Thus, our grasp pipeline\ncan solve both situations occurring in the grasping task\nsuccessfully. However, similar to the research issue of 6\nDoF pose estimation, it is still challenging to achieve shape\ncompletion of an arbitrary object at an arbitrary pose due\nto the limited object categories in our dataset. So the\nmain limitation in this paper is that the object catogories\nin our constructed datatset are still small and they only\nlimited in the YCB objects, which causing that our shape\ncompletion model cannot be generalized into other novel\nobjects. In future work, our goal is to collect more objects\ncategories to achieve a better generalization for unseen but\nsimilarly shaped objects. Furthermore, we will also consider\nmore data augmentation strategies like adding more data\nrepresenting different object 6 DoF pose and different point\ncloud missing ratio as our experiments shown, we think\nour shape completion model could achieve a better grasping\nperformance in the real robotic experiments.\nSupplementary Information The online version contains supplemen-\ntary material available at https://doi.org/10.1007/s10846-022-01586-4.\nAcknowledgements We thank Mech-Mind Robotics Company for\nproviding the 3D camera.\nAuthor Contributions List of all authors: Wenkai Chen, Hongzhuo\nLiang, Zhaopeng Chen, Fucun Sun and Jianwei Zhang.\nAll authors contributed to the conception and design of this\nmanuscript. Technical work was conducted by Wenkai Chen and\nHongzhuo Liang. The manuscript was revised by Zhaopeng Chen,\nFucun Sun and Jianwei Zhang. All authors commented on the\nmanuscript.\nFunding Open Access funding enabled and organized by Projekt\nDEAL. This research was funded by the German Research Foundation\n(DFG) and the National Science Foundation of China (NSFC)\nin project Crossmodal Learning, DFG TRR-169/NSFC, project\nDEXMAN under grant 410816101 and partially supported by\nEuropean projects H2020 Ultracept (778602).\nCode or Data AvailabilityOur code will be released at https://github.\ncom/turbohiro/TransSC.\nDeclarations\nConsent to ParticipateAll participants consented to involve in the\ncreation of this manuscript.\nConﬂictofInterests There are no conflicts of interest.\nConsent for Publication All participants consented to publish this\nmanuscript.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\n45   Page 12 of 14 J Intell Robot Syst (2022) 104: 45\narticle\nare included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning\nrepresentations and generative models for 3D point clouds. In:\nInternational Conference on Machine Learning, pp. 40–49. PMLR\n(2018)\n2. Badrinarayanan, V ., Kendall, A., Cipolla, R.: Segnet: a deep con-\nvolutional encoder-decoder architecture for image segmentation.\nIEEE Trans. Pattern Anal. Mach. Intell. 39(12), 2481–2495 (2017)\n3. Bohg, J., Johnson-Roberson, M., Le ´on, B., Felip, J., Gratal,\nX., Bergstr ¨om, N., Kragic, D., Morales, A.: Mind the gap-\nrobotic grasping under incomplete observation. In: 2011 IEEE\nInternational Conference on Robotics and Automation, pp. 686–\n693. IEEE (2011)\n4. Breyer, M., Chung, J.J., Ott, L., Siegwart, R., Nieto, J.: V olumetric\ngrasping network: real-time 6 dof grasp detection in clutter.\narXiv:2101.01132 (2021)\n5. Chu, F.J., Xu, R., Vela, P.A.: Real-world multiobject, multigrasp\ndetection. IEEE Robot. Autom. Lett. 3(4), 3355–3362 (2018)\n6. Edelsbrunner, H., Kirkpatrick, D., Seidel, R.: On the shape of a\nset of points in the plane. IEEE Trans. Inf. Theory 29(4), 551–559\n(1983)\n7. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d\nobject reconstruction from a single image. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\npp. 605–613 (2017)\n8. G ¨orner, M., Haschke, R., Ritter, H., Zhang, J.: Moveit! Task\nconstructor for task-level motion planning. In: IEEE International\nConference on Robotics and Automation (ICRA), pp. 190–196\n(2019)\n9. Groueix, T., Fisher, M., Kim, V ., Russell, B., Aubry, M.: Atlasnet:\nap a p i e r - mˆach´e approach to learning 3D surface generation.\narXiv:1802.05384 11 (2018)\n10. Gualtieri, M., Platt, R.: Robotic pick-and-place with uncer-\ntain object instance segmentation and shape completion.\narXiv:2101.11605 (2021)\n11. Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.:\nPct: point cloud transformer. arXiv: 2012.09688 (2020)\n12. Huang, Z., Yu, Y ., Xu, J., Ni, F., Le, X.: Pf-net: point fractal\nnetwork for 3d point cloud completion. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 7662–7670 (2020)\n13. Kuang, H., Wang, B., An, J., Zhang, M., Zhang, Z.: V oxel-FPN:\nmulti-scale voxel feature aggregation for 3d object detection from\nlidar point clouds. Sensors 20(3), 704 (2020)\n14. Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J., Quillen, D.:\nLearning hand-eye coordination for robotic grasping with deep\nlearning and large-scale data collection. Int. J. Rob. Res. 37(4-5),\n421–436 (2018)\n15. Liang, H., Ma, X., Li, S., G ¨orner, M., Tang, S., Fang, B., Sun,\nF., Zhang, J.: PointnetGPD: detecting grasp configurations from\npoint sets. In: 2019 International Conference on Robotics and\nAutomation (ICRA), pp. 3629–3635. IEEE (2019)\n16. Liu, M., Sheng, L., Yang, S., Shao, J., Hu, S.M.: Morphing\nand sampling network for dense point cloud completion. In:\nProceedings of the AAAI Conference on Artificial Intelligence,\npp. 11596–11603 (2020)\n17. Lundell, J., Verdoja, F., Kyrki, V .: Robust grasp planning over\nuncertain shape completions. In: 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 1526–\n1532. IEEE (2019)\n18. Lundell, J., Verdoja, F., Kyrki, V .: Beyond Top-Grasps through\nScene Completion. In: 2020 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 545–551. IEEE (2020)\n19. Mahler, J., Liang, J., Niyaz, S., Laskey, M., Doan, R., Liu, X.,\nOjea, J.A., Goldberg, K.: Dex-net 2.0: deep learning to plan robust\ngrasps with synthetic point clouds and analytic grasp metrics.\narXiv:1703.09312 (2017)\n20. Miller, A.T., Allen, P.K.: Graspit! a versatile simulator for robotic\ngrasping. IEEE Robot. Autom. Mag. 11(4), 110–122 (2004)\n21. Mousavian, A., Eppner, C., Fox, D.: 6-Dof graspnet: variational\ngrasp generation for object manipulation. In: IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pp. 2901–2910\n(2019)\n22. ten Pas, A., Gualtieri, M., Saenko, K., Platt, R.: Grasp pose\ndetection in point clouds. Int. J. Rob. Res. 36(13-14), 1455–1473\n(2017)\n23. ten Pas, A., Platt, R.: Using geometry to detect grasp poses in 3d\npoint clouds. In: Robotics Research, pp. 307–324. Springer Inter-\nnational Publishing. https://doi.org/10.1007/978-3-319-51532-\n8\n19 (2018)\n24. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: deep learning\non point sets for 3d classification and segmentation. In:\nProceedings of the IEEE conference on computer vision and\npattern recognition, pp. 652–660 (2017)\n25. Srinivas, A., Lin, T.Y ., Parmar, N., Shlens, J., Abbeel, P.,\nVaswani, A.: Bottleneck transformers for visual recognition.\narXiv:2101.11605 (2021)\n26. Tosun, T., Yang, D., Eisner, B., Isler, V ., Lee, D.: Robotic\ngrasping through combined image-based grasp proposal and 3d\nreconstruction. arXiv:2003.01649 (2020)\n27. Varley, J., DeChant, C., Richardson, A., Ruales, J., Allen,\nP.: Shape completion enabled robotic grasping. In: IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pp. 2442–2447 (2017)\n28. Varley, J., Weisz, J., Weiss, J., Allen, P.: Generating multi-fingered\nrobotic grasps via deep learning. In: 2015 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 4415–\n4420. IEEE (2015)\n29. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need.\narXiv:1706.03762 (2017)\n30. Watkins-Valls, D., Varley, J., Allen, P.: Multi-modal geometric\nlearning for grasping and manipulation. In: 2019 International\nConference on Robotics and Automation (ICRA), pp. 7339–7345.\nIEEE (2019)\n31. Wu, C., Chen, J., Cao, Q., Zhang, J., Tai, Y ., Sun, L., Jia, K.: Grasp\nproposal networks: an end-to-end solution for visual learning of\nrobotic grasps. arXiv: 2009.12606 (2020)\n32. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao,\nJ.: 3d shapenets: a deep representation for volumetric shapes. In:\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1912–1920 (2015)\n33. Xiang, Y ., Schmidt, T., Narayanan, V ., Fox, D.: Posecnn: a\nconvolutional neural network for 6d object pose estimation in\ncluttered scenes. arXiv:1711.00199 (2017)\nPage 13 of 14    45J Intell Robot Syst (2022) 104: 45\n34. Xie, H., Yao, H., Zhou, S., Mao, J., Zhang, S., Sun, W.: Grnet:\ngridding residual network for dense point cloud completion. In:\nEuropean Conference on Computer Vision, pp. 365–381. Springer\n(2020)\n35. Yi, L., Kim, V .G., Ceylan, D., Shen, I.C., Yan, M., Su, H., Lu, C.,\nHuang, Q., Sheffer, A., Guibas, L.: A scalable active framework\nfor region annotation in 3D shape collections. ACM Transactions\non Graphics (ToG) 35(6), 1–12 (2016)\n36. Yuan, W., Khot, T., Held, D., Mertz, C., Hebert, M.: Pcn: point\ncompletion network. In: 2018 International Conference on 3D\nVision (3DV), pp. 728–737. IEEE (2018)\nPublisher’s NoteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional affiliations.\nWenkai Chen received the B.S. degree and M.Eng in Mechanical\nEngineering from Wuhan University and Shanghai Jiao Tong\nUniversity, China. He is currently pursuing a Ph.D. degree from\nthe faculty of informatics, Universit ¨at Hamburg, Germany. His\nresearch interests include 3D computer vision, robotic grasping, and\ncrossmodal learning in the robotic field.\nHongzhuo Liangreceived M.Eng in Mechanical Engineering from\nAnhui University of Technology, China. He is currently pursuing a\nPh.D. degree from the faculty of informatics, Universit ¨at Hamburg,\nGermany. His research interests include crossmodal learning and\nreinforcement learning in robotic grasping and manipulation.\nZhaopeng Chen received B.S. degree in Mechanical Engineering\nand Automation and M.S. degree in Mechatronic Engineering from\nHarbin Institute of Technology, Harbin, China, in 2005 and 2007,\nrespectively. From 2008 to 2010, he was a joint-train Ph.D. candidate\nin the Institute of Robotics and Mechatronics, DLR German Aerospace\nCenter, Oberpfaffenhofen, Germany. Since 2010, he has been with\nDLR German Aerospace Center in Oberpfaffenhofen, Germany, as a\nresearch scientist. Now, he is the CEO of Agile Robots AG.\nFuchun Sun received the B.S. and M.S. degrees from the Naval\nAeronautical Engineering Academy, Yantai, China, in 1986 and 1989,\nrespectively, and the Ph.D. degree from Tsinghua University, Beijing,\nChina, in 1998. He was with the Department of Automatic Control,\nNaval Aeronautical Engineering Academy, for over four years. From\n1998 to 2000, he was a Postdoctoral Fellow with the Department\nof Automation, Tsinghua University. He is currently a Professor\nwith the Department of Computer Science and Technology, Tsinghua\nUniversity. His research interests include intelligent control, neural\nnetworks, fuzzy systems, variable structure control, nonlinear systems,\nand robotics. Dr. Sun was the recipient of the Doctoral Dissertation\nPrize of China in 2000.\nJianwei Zhang (Member, IEEE) received the B.Eng. (Hons.) and\nM.Eng. degrees from the Department of Computer Science, Tsinghua\nUniversity, Beijing, China, in 1986 and 1989, respectively, the Ph.D.\ndegree from the Department of Computer Science, Institute of Real-\nTime Computer Systems and Robotics, University of Karlsruhe,\nKarlsruhe, Germany, in 1994, and the Habilitation degree from the\nFaculty of Technology, Bielefeld University, Bielefeld, Germany, in\n2000., He is currently a Professor and the Head of the TAMS\nGroup, Department of Informatics, University of Hamburg, Hamburg,\nGermany. He has been coordinating numerous collaborative research\nprojects of EU and the German Research Council, including the\nTransregio-SFB TRR 169 “Crossmodal Learning”. He has published\nabout 300 journal articles and conference papers (winning four best\npaper awards), technical reports, 4 book chapters, and 5 research\nmonographs. His current research interests include cognitive robotics,\nsensor fusion, dexterous manipulation, and multimodal robot learning.,\nDr. Zhang is a Life-Long Academician of the Academy of Sciences,\nHamburg. He is the General Chair of the IEEE MFI 2012, the\nIEEE/RSJ IROS 2015, and the IEEE Robotics and Automation Society\nAdCom, from 2013 to 2015.\n45   Page 14 of 14 J Intell Robot Syst (2022) 104: 45",
  "topic": "GRASP",
  "concepts": [
    {
      "name": "GRASP",
      "score": 0.905529260635376
    },
    {
      "name": "Computer science",
      "score": 0.7571192979812622
    },
    {
      "name": "Point cloud",
      "score": 0.732420802116394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6342066526412964
    },
    {
      "name": "Transformer",
      "score": 0.5830011963844299
    },
    {
      "name": "Encoder",
      "score": 0.5577566027641296
    },
    {
      "name": "Computer vision",
      "score": 0.5278946161270142
    },
    {
      "name": "Object (grammar)",
      "score": 0.5276975631713867
    },
    {
      "name": "Robot",
      "score": 0.4941592514514923
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4627950191497803
    },
    {
      "name": "Exploit",
      "score": 0.4135705828666687
    },
    {
      "name": "Engineering",
      "score": 0.10638248920440674
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}