{
  "title": "Transformers and LLMs as the New Benchmark in Early Cancer Detection",
  "url": "https://openalex.org/W4390822968",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3133640098",
      "name": "Yulia Kumar",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2098416816",
      "name": "Kuan Huang",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2062588758",
      "name": "Zachary Gordon",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2649417837",
      "name": "Laís Castro",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A5093071679",
      "name": "Egan Okumu",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2026945012",
      "name": "Patricia Morreale",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2462094061",
      "name": "J . Jenny Li",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A3133640098",
      "name": "Yulia Kumar",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2098416816",
      "name": "Kuan Huang",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2062588758",
      "name": "Zachary Gordon",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2649417837",
      "name": "Laís Castro",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A5093071679",
      "name": "Egan Okumu",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2026945012",
      "name": "Patricia Morreale",
      "affiliations": [
        "Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2462094061",
      "name": "J . Jenny Li",
      "affiliations": [
        "Kean University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6853934039",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384484700",
    "https://openalex.org/W4377043947",
    "https://openalex.org/W4361251463",
    "https://openalex.org/W6908731571",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W4225009244",
    "https://openalex.org/W3166383743",
    "https://openalex.org/W3081427079",
    "https://openalex.org/W4293763841",
    "https://openalex.org/W4291803453",
    "https://openalex.org/W4384695000",
    "https://openalex.org/W4389041498",
    "https://openalex.org/W2962858109"
  ],
  "abstract": "The study explores the transformative capabilities of Transformers and Large Language Models (LLMs) in the early detection of Acute Lymphoblastic Leukaemia (ALL). The researchers benchmark Vision Transformers with Deformable Attention (DAT) and Hierarchical Vision Transformers (Swin) against established Convolutional Neural Networks (CNNs) like ResNet-50 and VGG-16. The findings reveal that transformer models exhibit remarkable accuracy in identifying ALL from original images, demonstrating efficiency in image analysis without necessitating labour-intensive segmentation. A thorough bias analysis is conducted to ensure the robustness and fairness of the models. The promising performance of the transformer models indicates a trajectory towards surpassing CNNs in cancer detection, setting new standards for accuracy. In addition, the study explores the capabilities of LLMs in revolutionising early cancer detection and providing comprehensive support to ALL patients. These models assist in symptom analysis, offer preliminary assessments, and guide individuals seeking information, contributing to a more accessible and informed healthcare journey. The integration of these advanced AI technologies holds the potential to enhance early detection, improve patient outcomes, and reduce healthcare disparities, marking a significant advancement in the fight against ALL.",
  "full_text": " \nTransformers and LLMs as the New Benchmark in Early Cancer \nDetection \nYulia Kumar1, Kuan Huang1, Zachary Gordon1, Lais Castro1, Egan Okumu1, Patricia Morreale1 and J. Jenny Li1   \n \n1Department of Computer Science and Technology, Kean University, Union NJ 07083, USA \nAbstract. The study explores the transformative capabilities of Transformers and Large Language Models \n(LLMs) in the early detection of Acute Lymphoblastic Leukaemia (ALL). The researchers benchmark \nVision Transformers with Deformable Attention (DAT) and Hierarchical Vision Transformers (Swin) \nagainst established Convolutional Neural Networks (CNNs) like ResNet-50 and VGG-16. The findings \nreveal that transformer models exhibit remarkable accuracy in identifying ALL from original images, \ndemonstrating efficiency in image analysis without necessitating labour-intensive segmentation. A thorough \nbias analysis is conducted to ensure the robustness and fairness of the models. The promising performance \nof the transformer models indicates a trajectory towards surpassing CNNs in cancer detection, setting new \nstandards for accuracy. In addition, the study explores the capabilities of LLMs in revolutionising early \ncancer detection and providing comprehensive support to ALL patients. These models assist in symptom \nanalysis, offer preliminary assessments, and guide individuals seeking information, contributing to a more \naccessible and informed healthcare journey. The integration of these advanced AI technologies holds the \npotential to enhance early detection, improve patient outcomes, and reduce healthcare disparities, marking \na significant advancement in the fight against ALL.  \n1 Introduction \nAcute Lymphoblastic Leukaemia ( ALL) predominantly \naffects individuals under 20, necessitating early \ndetection to combat its aggressive nature. Current \ndiagnostic methods are invasive, costly, and time-\nconsuming, with a significant emotional impact on \npatients and families. In 2023, the American Cancer \nSociety estimates 6,540 new ALL cases and 1,390 deaths \nin the U.S., highlighting the urgency for innovative \nsolutions [1]. ALL risk factors vary with age, peaking in \nyoung children and rising again after 50 [2]. Despite \nconstituting less than 0.5% of all U.S. cancers, most \nALL-related deaths occur in adults, underscoring the \nneed for improved diagnostic methods [1, 3]. Artificial \nIntelligence (AI) is revolutionizing this field, with \nmachine learning (ML) models like Vision \nTransformers with Deformable Attention (DAT), \nHierarchical Vision Transformers (Swin), and Larg e \nLanguage Models (LLMs) like ChatGPT, Bing and Bard \nshowing promising results. This study aims to set new \nstandards in accuracy, affordability, and speed of ALL \ndiagnoses, addressing three main research questions: \nRQ1: How do transformer models compare to CNNs in \nearly cancer detection? RQ2: What role can LLMs play \nin ALL detection? RQ3: How can biases in AI models \ntrained on ALL data be identified and mitigated?  The \nresearch aspires to enhance patient well-being and \ncontribute to societal health advancements. \n2 Related Work \nThe adoption of Transformers and Large Language \nModels (LLMs) in healthcare, especially for early \ncancer detection, including Acute Lymphoblastic \nLeukaemia (ALL), represents a significant advancement. \nThis synergy has catalysed research, diagnosis, and \ntreatment innovations, supported by an expanding \nliterature base. Nerella et al. provided a thorough survey \non Transformers in healthcare, highlighting their \nversatility across fields like genomics and patient care \n[4]. Balabin introduced Multimodal Transformers, \nshowcasing their capability to manage complex \nbiomedical data [5]. Thirunavukarasu et al. discussed \nLLMs in medicine, emphasizing the necessity for \nmodels that comprehend complex medical terminology \n[6]. Holmes et al. demonstrated LL Ms’ effectiveness in \nniche areas like radiation oncology physics [7]. Wang et \nal. and Li et al. explored ChatGPT’s integration into \nbiomedical research and healthcare, indicating LLMs’ \nincreasing acceptance in medicine [8-9]. Batzoglou \nfocused on LLMs in Molecular Biology, illustrating \ntheir potential to unravel complex biological language \nand contribute to understanding genetic factors in \ndiseases like cancer [10]. In early cancer detection, \nTransformers have been central. The Kaggle ALL \ndataset authors-researchers highlighted DenseNet-201’s \nperformance with various CNN architectures [11]. \nNewer transformers like DAT [12] and Swin [13], and \nLLMs like ChatGPT-4, have expanded early ALL \ndetection possibilities. Huang et al.’s work on breast \ncancer using transformers adds valuable insights into \nearly detection methods [14-15]. Addressing bias in \nTransformers, crucial across neural networks, has been \n© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0\n(https://creativecommons.org/licenses/by/4.0/). \nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023\nintensively tackled with models like DETR and \nDeformable DETR [16 -18]. Kumar et al.’s testFAILS \nframework guides responsible LLM apps [19]. \n3 Methodology \n3.1 The project dataset \nThe ALL dataset, pivotal for this study, comprises a \nwell-curated collection of medical images, transformed \nand annotated for training AI/ML models. Sourced from \nKaggle [11], it underpins our explo ration of AI in early \nAcute Lymphoblastic Leukaemia (ALL) detection and \nclassification. The dataset encompasses 3,256 \nPeripheral Blood Smear (PBS) images from 89 \nindividuals, 25 healthy and 64 with varying ALL stages, \nprepared by experienced lab personnel.  It categorizes \nimages into benign (non -progressing cancer) and \nmalignant classes, the latter further divided into Early \nPre-B, Pre -B, and Pro -B ALL subtypes, representing \ndifferent ALL stages. Notably, the dataset adheres to a \n1970s classification system (L1, L2, L3), now outdated \nand misaligned with current American Cancer Society \nand WHO guidelines, but still in use at the dataset's \noriginating laboratory outside the US. The images, \ncaptured via microscopy, underwent colour \nthresholding-based segmentatio n to identify cell types \nand subtypes. However, this method showed limitations, \nstruggling with accurate blast cell region definition, \nbackground noise, and segmentation in ALL images \n[11]. Table 1 provides an overview of the dataset. Figure \n1 displays sam ple images and their corresponding \nsegmented versions. \n \nTable 1. The project dataset. \nTypes of \nALL \nThe # of \nimages \nComments \nBenign \nEarly \nPre \nPro \nClassification \n504 \n985 \n963 \n804 \n4 classes \nNoncancerous \nEarly-stage type (L1) \nMiddle stage type (L2) \nLater stage type (L3) \nBenign, Early, Pre, Pro \n \nAs can be seen from Table 1, the Benign ALL type \nis the least dangerous as it is not spreading to the \nneighbouring cells, and in many online resources, it is \ncalled noncancerous, while the other three are cancerous \nand present different types/stages of ALL. The examples \nof ALL images can be seen below: \n \n \nFig. 1. The project ALL dataset (top: original benign, early, \npre, pro images; bottom: corresponding segmented images). \nThe segmented images revealed the need for \nsophisticated AI models capable of classifying original, \nnon-segmented ALL images, marking a departure from \nprevious dataset applications. The ALL dataset [11] thus \nemerges as a valuable resource for advancing AI in ALL \ndiagnosis and classificatio n, highlighting the necessity \nfor innovative, accurate AI techniques to improve early \ncancer detection.  \n3.2 Applying CNNs and Transformers to ALL \nData \nIn this study, the traditional convolutional neural \nnetworks such as VGG-16 and ResNet-50 and advanced \ntransformer models, specifically DAT [12] and Swin \nTransformers [13], were employed on the ALL dataset \nto evaluate their performance in achieving optimal \naccuracy. The hypothesis posited that both transformer \nmodels would be highly compatible with the dataset and \noutperform the convolutional neural networks. The \ndataset’s unique features, such as image presentation, \nsegmentation challenges, and oval cell shapes, were \ndeemed favourable for computer vision tasks. Training \nthe DAT transformer on computer vision tas ks, \nparticularly the ALL dataset, was a complex endeavour. \nAfter the initial 40 epochs, the model achieved a 93.56% \naccuracy on the test set. Figure 2 illustrates the losses: \n \nFig. 2. Initial Training and Validation Losses of the DAT \n \n For comparative purposes, the researchers ran the \nconvolutional models (VGG -16 and ResNet -50) \nalongside the transformers. Table 2 presents the results: \nTable 2. Comparative Results of various models. \nModel Accuracy, % \nBenign  Early Pre Pro Average \nResNet-50 89.11 83.25 98.45 100 92.7025 \nVGG-16 99.01 99.49 100 100 99.6250 \nDAT  89.11 96.95 99.48 100 96.3850 \nSwin  92.08 99.49 99.48 100 97.7625 \n \nAs can be seen from the table DAT and Swin \noutperformed ResNet -50 on initial images across all \nclasses, particularly in early cancer detection, where \nthey achieved 97 -99% accuracy compared to ResNet -\n50’s 83%. Through VGG-16 model performed on early \ncancer detection better. This underscores the new \ngeneration models’ perspectives to bypass the costly and \ntime-intensive segmentatio n step for ALL data. The \nnecessity of the fine-tuning is evident. The experimental \nparameters are detailed in Table 3:  \nThe project ALL dataset (top: original benign, early, \npre, pro images; bottom: corresponding segmented \nimages). \n2\nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023\nTable 3. Experimental Parameters. \nParameter Comment \nInitial \nDataset \nClass \nBatch Size \nInput \nImage \nOptimizer  \n# of Epochs \nPyTorch  \nHardware \n \n \n \n3256 images \n80% for training, 20% for testing at random \n4 classes (Benign, Early, Pre, Pro) \n16  \n \n256× 256 (resized) \nSGD, learning rate 0.001. \n300 \n1.12.1. \nUbuntu 20.04.5 Linux system : AMD EPYC \n7513 32 -Core Processor 2.60GHz, 8 \nNVIDIA GeForce 3090 graphics cards, each \none of 24 Gb \n \nThe Swin and DAT Transformers achieved \naccuracies of 97.76% and 96.385%, respectively, \noutperforming R esNet-50 but also highlighting the \nchallenges in achieving perfect accuracy in early ALL \nstages, even with cutting -edge transformers. Confusion \nmatrices were generated to further analyse performance: \n \n \nFig. 3. Confusion Matrices for VGG-16 (top left), ResNet-50 \n(top right), Swin (left bottom) and DAT (right bottom). \n As can be seen from Figure 3, the Swin Transformer \noutperformed the DAT, contradicting the original DAT \npaper [12] and highlighting the attention architectures’ \nsensitivity to specific data sets and tasks. Training and \ntesting loss curves for DAT, Swin, and ResNet -50 \nprovide additional insights (see Figure 4):  \n \n \nFig. 4. Loss Curves for VGG-16, ResNet-50, DAT, and \nSwin. \n The results contribute to our understanding of ALL \ndetection and pave the way for future research and \ninnovation in applying transformer models to medical \nimaging and diagnosis.  \n3.3 The role of LLMs in early cancer detection  \nThe exploration of Large Language Models (LLMs) in \nthe detection of Acute Lymphoblastic Leukaemia (ALL) \nis a burgeoning field, showing great potential despite its \nexperimental nature. An attempt to utilize ChatGPT-4’s \ncode interpreter for ALL detection in cell images did not \nyield direct results in terms of classification. However, \nthe model demonstrated it s utility by providing a \ncomprehensive metadata analysis of the image and \ngenerating executable code for a pre -trained Faster R -\nCNN model, which was successfully run in Google \nCollab [20]. The multimodal capabilities of LLMs are \nrapidly evolving, with anti cipation building around the \nupcoming ChatGPT -5 model, which is expected to \nhandle a variety of media data types, including video \nand 3D data. Image analysis and generation capabilities \nare already present in models like DALL -E 3 and \nChatGPT-4, showcasing the versatility of these \ntechnologies. \nLLMs have proven their worth as personal assistants, \noffering invaluable support to ALL patients in numerous \nways. A notable development in this area is the \nAssureAIDoctor (AAID) app, a novel tool designed to \nenhance patient care [21], recently upgraded to .NET 8 \nand ChatGPT -4. The prototype’s GUI is displayed in \nFigure 5. \n \n  \nFig. 5. The AssureAIDoctor’s AI assistant for ALL patients.  \n The app has a potential to help ALL patients in \nmany ways. Some of them are provided in Table 4. \nTable 4. Enhancing the Patient Experience with AI Doctor. \nFeature Description \nPersonalized \nPatient \nInteraction \nProvides tailored conversations based on \nsymptoms and medical history, aiding in \nearly detection and guidance for ALL \npatients or those suspecting they might have \nALL. \nSymptom \nAnalysis and \nPreliminary \nDiagnosis \nAnalyses symptoms, compares them with \nknown symptoms of ALL, and provides a \npreliminary assessment to serve as a quick \nand accessible first step for individuals \nseeking information and guidance. \nImage Analysis \nand Assistance \nAssists users in understanding their medical \nimages, such as blood smears, through initial \n3\nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023\nanalysis, helping to demystify complex \nmedical data. \nInformation and \nEducation \nOffers educational content on ALL, \nempowering patients with knowledge to aid \nin informed decision-making. \nEmotional \nSupport and \nMental Health \nProvides emotional support, engages in \nconversations to alleviate anxiety, and \nguides users to professional mental health \nresources if necessary. \nFacilitation of \nDoctor-Patient \nCommunication \nHelps organize medical information, \nsymptoms, and questions for discussions \nwith healthcare providers, ensuring effective \nuse of medical appointments. \nFollow-Up and \nMedication \nReminders \nAssists in follow-up care, providing \nreminders for medication, appointments, and \nnecessary tests to help ALL patients stay on \ntop of their treatment plan. \nCommunity and \nSupport \nConnects ALL patients with support \ncommunities, offering a platform for sharing \nexperiences, advice, and encouragement. \nResearch and \nData Collection \nWith user consent, collects anonymized data \non ALL symptoms and progression, \ncontributing to a broader understanding of \nthe disease and aiding in research efforts. \nAccessibility and \nReducing \nHealthcare \nDisparities \nEnsures individuals from all backgrounds \nhave access to quality healthcare \ninformation, providing immediate access to \ninformation and preliminary assessments, \nand thus helping in reducing healthcare \ndisparities. \n \nChatGPT-4-Turbo, Bing and Bard bots were \nemployed to analyse ALL data directly through their \nmultimodal capacities. Their responses, which include \nimage descriptions and identification of various cell \ntypes, demonstrate their ability to extract information  \nwithout prior training, as shown in Figures 6 and 7. \n \n \nFig. 6. Bing (top) and Bard (bottom) Responses.  \nChatGPT4-Turbo model was fed three images into: one \nnon-cancerous, one with early cancer and one to analyse. \nModel’s response is provided below: \n \n \nFig. 7. ChatGPT-4-Turbo response [22].  \n \n The ChatGPT-4-Turbo model closely analysed the \nimage and even created its threshold, what can be seen \nin the recorded conversation  [22], but  it refused to \nanswer a yes or no question about the presence of early \ncancer in the anonymous image (that was present there).  \nThe integration of LLMs such as ChatGPT, Bard, \nand Bing, alongside innovative tools like the \nAssureAIDoctor, holds great potential in advancing \nALL research and patient care. These technologies are \nbecoming increasingly prevalent across various \ndomains, showcasing their versatility and potential to \nrevolutionize healthcare. The app powered by ChatGPT \nand DALL-E, offers a personalized and comprehensive \napproach to patient care, from symptom analysis to \npreliminary diagnosis, and even image analysis. This \nintegration is setting new standards in accuracy, \nefficiency, and innovation, paving the way for a future \nwhere healthcare is more accessible, personalized, and \neffective for ALL patients. As illustrated a bove, the \napplication of LLMs to ALL data shows promise, but it \nnecessitates further research and validation to ensure \nreliability and accuracy. \n4 Bias Detection in Transformers \nAddressing biases in transformer models, specifically \nwithin the Multilayer Perc eptron (MLP) of the first \ntransformer stage of both DAT and Swin models, was a \ncritical and complex aspect of this study. This section \nprovides an in -depth analysis and discussion of the \nextensive trials and evaluations conducted to understand \nand mitigate  these biases. Figure 8 presents a \ncomparative analysis of the biases of the transformer \nmodels. The Swin Transformer and DAT models \nrequired nearly 100 epochs to reach convergence, \nhighlighting the intricate nature and variability in \ntraining such models. \n \n \n \n \nFig. 8. Bias Visualizations of VGG-16 (the first \nconvolutional layer), and DAT and Swin Models (MLP of \nSecond Transformer Stage).  \n \nThe biases in the Swin transformer exhibited a \nunique dome -like shape; however, the VGG -16 and \nDAT’s biases are more stable. The bias of the Swin \ntransformer shows the hardness of training convergence \nin the Swin transformer. It takes 150 epochs to get stable \nvalues. According to Bard bot this is because the model \nhas learned a bias towards dominant features in the \n4\nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023\ntraining data. This can be a problem for tasks where it is \nimportant to be unbiased, such as for medical diagnosis. \nAccording to the bot the mod el's biases may depend on \nthe training data, meaning that the training data has \nappeared to be biased. There might be an overfit. The \nbiases in the second MLP of the last transformer stage \nof the DAT model stabilized around epochs 45 -50, \nforming almost a straight line, indicating a stabilization \nin the learning process. On the other hand, the Swin \nTransformer required close to 200 epochs for the biases \nto stabilize, emphasizing the model’s complexity and \nthe nuanced interplay of its internal parameters.  \n A noteworthy discovery in this research was the \nbehaviour of the ResNet -50 CNN model, which \nexhibited no biases in its first convolutional layer: \n \n      self.conv1 = nn.Conv2d(3, self.inplanes, \nkernel_size=7, stride=2, padding=3, bias=False)        (1) \n \nThe ResNet -50 CNN model, with its first \nconvolutional layer set to bias=False, demonstrates a \ncommendable ability to learn unbiased features from the \ninput data. This configuration, especially in conjunction \nwith batch normalization, proves advantageous by  \neliminating redundancy and reducing the model's \ncomplexity. The chosen parameters, including a smaller \nfilter size and stride, ensure the capture of finer details, \ncontributing to a more nuanced understanding of the \ninput data. Enough filters further guarantee the model’s \ncapacity to discern a diverse array of patterns, aiding in \nthe mitigation of biases. The careful initialization of \nweights, alongside the application of regularization \ntechniques, solidifies the stability of the learning process, \npreventing overfitting and ensuring a fair and balanced \nrepresentation. This finding is significant as it \ndemonstrates that through careful simulations and \ntuning, biases can be completely mitigated. It also raises \nimportant questions about the inherent difference s in \nhow CNNs and transformer models handle biases.  \n \n \nFig. 9. CAM Results of DL Models.  \n \nClass Activation Map (CAM), a technique \ncommonly used for visualizing and interpreting the \ndecisions made by CNNs in computer vision tasks, was \nemployed in this study [23]. CAM generates a heat map \nhighlighting the significant regions of an input image \nthat contribute to a deep Neural Network’s classification \ndecision. The researchers utilized the Grad -CAM \nmethod to generate CAMs for each classification model \nin the study, including the convolutional block 5 in \nVGG-16 and ResNet-50, and the transformer stage 4 in \nthe Swin Transformer and DAT Transformer.  \nThe CAMs, as shown in Figure 9, reveal the regions of \nimportance for making the final classification decisions. \nThe CNN models (VGG -16 and ResNet -50) tended to \nfocus more on specific features in the original images, \nsuch as the purple dots, which are significant regions \nmarked by the d ataset’s creators. In contrast, the \ntransformer models adopted a different approach.  \nBard bot was asked to analyse the CAM image and \nproduced the following result: \n \n \nFig. 10. Bard bot’s analysis of CAM Results. \nAnalysing the CAMs can aid doctors and pati ents in \nfocusing on the highlighted activations, contributing to \nthe accuracy of diagnosis. It is evident that utilizing \ntransformers in ALL detection can enhance results and \nprovide new perspectives for both doctors and AI \nspecialists. The researchers ack nowledge that there is \npotential for further tuning and optimization of the DAT \nand Swin models to improve their accuracy and reduce \nbias. Techniques such as applying various activation \nfunctions, adjusting neuron weights, and others are \ncurrently under in vestigation. This ongoing effort is \ncrucial for advancing the field and ensuring the \nreliability and fairness of AI models in medical apps. \n5 Conclusion and Future Work \nThis comprehensive study has meticulously addressed \nthe outlined research questions, shed ding light on the \nintegration of transformer models, Large Language \nModels (LLMs), and bias mitigation strategies in Acute \nLymphoblastic Leukaemia (ALL) detection. The \nemployment of transformer models, specifically DAT \nand Swin, yielded impressive accuraci es of 96.385% \nand 97.76%, respectively. However, these models \nrequired a substantial number of epochs to converge, \nwith Swin necessitating up to 200 epochs for bias \nstabilization, and DAT showcasing the complexity of \ntraining transformer models. \nIn compari son, the traditional CNN model \ndemonstrated quicker convergence, with significant loss \nreduction observed between the 10th and 15th epochs. \nLLMs, represented by ChatGPT -4, Bing, and Bard, \nexhibited their potential in image classification and AI \ncode generation, with ChatGPT-4 generating a working \ncode for a pre -trained Faster R -CNN model and Bard \nproviding descriptive image analysis. These capabilities \nunderscore the versatility of LLMs in medical imaging, \nthough they necessitate further research and valida tion \nfor reliability and accuracy assurance. \n5\nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023\nThe study also delved into biases present in the MLP \nof the first transformer stage of both DAT and Swin \nmodels, revealing distinctive dome -like shape in the \nbiases and significant fluctuation in the DAT \nTransformer models. The identification of the point of \nconvergence and the demonstration that biases could be \ncompletely mitigated, as evidenced by the ResNet -50 \nCNN model, underscore the performance of transformer \nmodels, and highlight the promising capabilities  of \nLLMs in medical imaging and explainable AI. \nLooking forward, the integration of transformer \nmodels and LLMs heralds a future of accurate, humane, \naccessible, and patient-centric AI in medical diagnostics, \ncontributing to improved healthcare outcomes an d \nfostering a more inclusive and generalizable research \necosystem. \nReferences \n1. Key Statistics for ALL [Online], available at: \nhttps://www.cancer.org/cancer/types/acute-\nlymphocytic-leukemia/about/key-statistics.html \n(last accessed on 07/30/2023). \n2. Cancer Stat Facts: ALL. [Online], available at: \nhttps://seer.cancer.gov/statfacts/html/alyl.html (last \naccessed on 07/30/2023). \n3. SEER program [Online], available at: \nhttps://seer.cancer.gov/ (last accessed on \n07/30/2023). \n4. Nerella, Subhash, et al. \"Transformers in \nHealthcare: A Survey.\" arXiv preprint \narXiv:2307.00067 (2023). \n5. Balabin, H. (2022). Multimodal Transformers for \nBiomedical Text and Knowledge Graph Data. \n6. Thirunavukarasu, A.J., et al. Large language \nmodels in medicine. Nat Med (2023). \nhttps://doi.org/10.1038/s41591-023-02448-8. \n7. Holmes, J., Liu, Z., et al. (2023). Evaluating large \nlanguage models on a highly specialized topic, \nradiation oncology physics. arXiv preprint \narXiv:2304.01938. \n8. Wang, D. Q., Feng, et al. (2023). Accelerating the \nintegration of ChatGPT and other large‐scale AI \nmodels into biomedical research and healthcare. \nMedComm–Future Medicine, 2(2), e43. \n9. Li, J., Dada, A., Kleesiek, J., & Egger, J. (2023). \nChatGPT in Healthcare: A Taxonomy and \nSystematic Review. medRxiv, 2023-03. \n10. Batzoglou, S. Large Language Models in \nMolecular Biology Deciphering the language of \nbiology, from DNA to cells to human health. \n[Online], available at: \nhttps://towardsdatascience.com/large-language-\nmodels-in-molecular-biology-9eb6b65d8a30 (last \naccessed on 07/30/2023). \n11. M. Aria, et al. ALL image dataset.\" Kaggle, \n(2021). DOI: 10.34740/KAGGLE/DSV/2175623. \n12. Zhuofan Xia, et al. (2022) Vision Transformer \nwith Deformable Attention, \nhttps://doi.org/10.48550/arXiv.2201.00520. \n13. Swin transformer repo [Online], available at: \nhttps://github.com/microsoft/Swin-Transformer \n(last accessed on 07/30/2023). \n14. M. Xu, K. Huang et al. \"Multi-Task Learning with \nContext-Oriented Self-Attention for Breast \nUltrasound Image Classification and \nSegmentation, doi: \n10.1109/ISBI52829.2022.9761685.  \n15. K. Huang et al. \"Shape-Adaptive Convolutional \nOperator for Breast Ultrasound Image \nSegmentation,\" 2021 IEEE International \nConference on Multimedia and Expo (ICME), \nShenzhen, China, 2021, pp. 1-6, doi: \n10.1109/ICME51207.2021.9428287. \n16. Jenny Li, et al. (2021). Evaluating Deep Learning \nBiases Based on Grey-Box Testing Results. \nIntelliSys 2020. Advances in Intelligent Systems \nand Computing, vol 1250. Springer, Cham, \nDOI=https://doi.org/10.1007/978-3-030-55180-\n3_48. \n17. Tellez, N., Serra, J., Kumar. Y., et al. (2023). \nGauging Biases in Various Deep Learning AI \nModels. IntelliSys 2022. Lecture Notes in \nNetworks and Systems, vol 544. Springer, Cham. \nhttps://doi.org/10.1007/978-3-031-16075-2_11. \n18. N. Tellez et al., \"An Assure AI Bot (AAAI bot),\" \nISNCC, Shenzhen, China, 2022, pp. 1-5, doi: \n10.1109/ISNCC55209.2022.9851759. \n19. Kumar Y, et al.  A Testing Framework for AI \nLinguistic Systems (testFAILS). Electronics. \n2023; 12(14):3095. \nhttps://doi.org/10.3390/electronics12143095. \n20. Pre-trained Faster R-CNN (Region-based \nConvolutional Neural Network) code, provided by \nChatGPT-4 code interpreter [Online], available at: \nhttps://colab.research.google.com/drive/1DcdStV-\nxRzLzkCJ9d8eMVRRfTjSSfadH?usp=sharing \n(last accessed on 7/31/2023). \n21. Y. Kumar, at al. (2023) AssureAIDoctor- A Bias-\nFree AI Bot. In proceeding of the 2023 \nInternational Symposium on Networks, Computers \nand Communications (ISNCC): Artificial \nIntelligence and Machine Learning. (ISNCC 2023) \n22. Chat with ChatGPT-4-Turbo [Online], available \nat: https://chat.openai.com/share/4cfc5124-910b-\n49df-9e91-ad5e98fd91f6 (last accessed on \n7/31/2023). \n23. Selvaraju, Ramprasaath R. et al. \"Grad-cam: \nVisual explanations from deep networks via \ngradient-based localization.\" In Proceedings of the \nIEEE international conference on computer vision, \npp. 618-626. 2017. \n6\nITM Web of Conferences 60, 00004 (2024)     https://doi.org/10.1051/itmconf/20246000004\nAISS 2023",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6042219400405884
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5047043561935425
    },
    {
      "name": "Computer science",
      "score": 0.5034930109977722
    },
    {
      "name": "Segmentation",
      "score": 0.49972987174987793
    },
    {
      "name": "Health care",
      "score": 0.4791963994503021
    },
    {
      "name": "Transformative learning",
      "score": 0.47188082337379456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44349122047424316
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4172608256340027
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3883855640888214
    },
    {
      "name": "Machine learning",
      "score": 0.33676013350486755
    },
    {
      "name": "Medicine",
      "score": 0.3015924096107483
    },
    {
      "name": "Psychology",
      "score": 0.2796805500984192
    },
    {
      "name": "Engineering",
      "score": 0.246038556098938
    },
    {
      "name": "Political science",
      "score": 0.1900937855243683
    },
    {
      "name": "Geography",
      "score": 0.1038627028465271
    },
    {
      "name": "Cartography",
      "score": 0.10165202617645264
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ]
}