{
  "title": "A Decomposable Attention Model for Natural Language Inference",
  "url": "https://openalex.org/W2950738719",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5109937799",
      "name": "Ankur P. Parikh",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5087735236",
      "name": "Oscar Täckström",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021055773",
      "name": "Dipanjan Das",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5022416424",
      "name": "Jakob Uszkoreit",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170738476",
    "https://openalex.org/W2068206659",
    "https://openalex.org/W2963542836",
    "https://openalex.org/W2112609142",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2124204950",
    "https://openalex.org/W2149390430",
    "https://openalex.org/W2087451659",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2099884836",
    "https://openalex.org/W2092543775",
    "https://openalex.org/W2952191002",
    "https://openalex.org/W2121495183",
    "https://openalex.org/W2045254372",
    "https://openalex.org/W2131726681",
    "https://openalex.org/W1916559533",
    "https://openalex.org/W2469057590",
    "https://openalex.org/W2211192759",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2172888184",
    "https://openalex.org/W1816599501"
  ],
  "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",
  "full_text": "A Decomposable Attention Model for Natural Language Inference\nAnkur P. Parikh\nGoogle\nNew York, NY\nOscar T¨ackstr¨om\nGoogle\nNew York, NY\nDipanjan Das\nGoogle\nNew York, NY\nJakob Uszkoreit\nGoogle\nMountain View, CA\n{aparikh,oscart,dipanjand,uszkoreit}@google.com\nAbstract\nWe propose a simple neural architecture for nat-\nural language inference. Our approach uses at-\ntention to decompose the problem into subprob-\nlems that can be solved separately, thus making\nit trivially parallelizable. On the Stanford Natu-\nral Language Inference (SNLI) dataset, we ob-\ntain state-of-the-art results with almost an order\nof magnitude fewer parameters than previous\nwork and without relying on any word-order in-\nformation. Adding intra-sentence attention that\ntakes a minimum amount of order into account\nyields further improvements.\n1 Introduction\nNatural language inference (NLI) refers to the prob-\nlem of determining entailment and contradiction re-\nlationships between a premise and a hypothesis. NLI\nis a central problem in language understanding (Katz,\n1972; Bos and Markert, 2005; van Benthem, 2008;\nMacCartney and Manning, 2009) and recently the\nlarge SNLI corpus of 570K sentence pairs was cre-\nated for this task (Bowman et al., 2015). We present\na new model for NLI and leverage this corpus for\ncomparison with prior work.\nA large body of work based on neural networks\nfor text similarity tasks including NLI has been pub-\nlished in recent years (Hu et al., 2014; Rockt¨aschel\net al., 2016; Wang and Jiang, 2016; Yin et al., 2016,\ninter alia). The dominating trend in these models is\nto build complex, deep text representation models,\nfor example, with convolutional networks (LeCun et\nal., 1990, CNNs henceforth) or long short-term mem-\nory networks (Hochreiter and Schmidhuber, 1997,\nLSTMs henceforth) with the goal of deeper sen-\ntence comprehension. While these approaches have\nyielded impressive results, they are often computa-\ntionally very expensive, and result in models having\nmillions of parameters (excluding embeddings).\nHere, we take a different approach, arguing that\nfor natural language inference it can often sufﬁce to\nsimply align bits of local text substructure and then\naggregate this information. For example, consider\nthe following sentences:\n•Bob is in his room, but because of the thunder\nand lightning outside, he cannot sleep.\n•Bob is awake.\n•It is sunny outside.\nThe ﬁrst sentence is complex in structure and it\nis challenging to construct a compact representation\nthat expresses its entire meaning. However, it is fairly\neasy to conclude that the second sentence follows\nfrom the ﬁrst one, by simply aligning Bob with Bob\nand cannot sleep with awake and recognizing that\nthese are synonyms. Similarly, one can conclude\nthat It is sunny outside contradicts the ﬁrst sentence,\nby aligning thunder and lightning with sunny and\nrecognizing that these are most likely incompatible.\nWe leverage this intuition to build a simpler and\nmore lightweight approach to NLI within a neural\nframework; with considerably fewer parameters, our\nmodel outperforms more complex existing neural ar-\nchitectures. In contrast to existing approaches, our\napproach only relies on alignment and is fully com-\nputationally decomposable with respect to the input\ntext. An overview of our approach is given in Fig-\nure 1. Given two sentences, where each word is repre-\narXiv:1606.01933v2  [cs.CL]  25 Sep 2016\nsented by an embedding vector, we ﬁrst create a soft\nalignment matrix using neural attention (Bahdanau\net al., 2015). We then use the (soft) alignment to\ndecompose the task into subproblems that are solved\nseparately. Finally, the results of these subproblems\nare merged to produce the ﬁnal classiﬁcation. In ad-\ndition, we optionally apply intra-sentence attention\n(Cheng et al., 2016) to endow the model with a richer\nencoding of substructures prior to the alignment step.\nAsymptotically our approach does the same total\nwork as a vanilla LSTM encoder, while being triv-\nially parallelizable across sentence length, which can\nallow for considerable speedups in low-latency set-\ntings. Empirical results on the SNLI corpus show that\nour approach achieves state-of-the-art results, while\nusing almost an order of magnitude fewer parameters\ncompared to complex LSTM-based approaches.\n2 Related Work\nOur method is motivated by the central role played by\nalignment in machine translation (Koehn, 2009) and\nprevious approaches to sentence similarity modeling\n(Haghighi et al., 2005; Das and Smith, 2009; Chang\net al., 2010; Fader et al., 2013), natural language\ninference (Marsi and Krahmer, 2005; MacCartney\net al., 2006; Hickl and Bensley, 2007; MacCartney\net al., 2008), and semantic parsing (Andreas et al.,\n2013). The neural counterpart to alignment, atten-\ntion (Bahdanau et al., 2015), which is a key part\nof our approach, was originally proposed and has\nbeen predominantly used in conjunction with LSTMs\n(Rockt¨aschel et al., 2016; Wang and Jiang, 2016) and\nto a lesser extent with CNNs (Yin et al., 2016). In\ncontrast, our use of attention is purely based on word\nembeddings and our method essentially consists of\nfeed-forward networks that operate largely indepen-\ndently of word order.\n3 Approach\nLet a = ( a1,...,a ℓa ) and b = ( b1,...,b ℓb ) be\nthe two input sentences of length ℓa and ℓb, re-\nspectively. We assume that each ai, bj ∈ Rd\nis a word embedding vector of dimension d and\nthat each sentence is prepended with a “NULL”\ntoken. Our training data comes in the form of\nlabeled pairs {a(n),b(n),y(n)}N\nn=1, where y(n) =\n(y(n)\n1 ,...,y (n)\nC ) is an indicator vector encoding the\nlabel and Cis the number of output classes. At test\n     H (                 )+ +…+=ˆy\nin\nthe\npark\nalice\nplays\nsomeone\nplaying\nmusic\noutside\nﬂute\na\nsolo\nG (    ,    )\nG (    ,    )\npark outside\nalice someone\nﬂute+  \nsolo music\n…\nG (    ,    )=\n=\n=\nﬂute music\nF (    ,    )\nFigure 1:Pictoral overview of the approach, showing theAttend\n(left), Compare (center) and Aggregate (right) steps.\ntime, we receive a pair of sentences (a,b) and our\ngoal is to predict the correct label y.\nInput representation. Let ¯a = (¯a1,..., ¯aℓa ) and\n¯b = (¯b1,..., ¯bℓb ) denote the input representation of\neach fragment that is fed to subsequent steps of the\nalgorithm. The vanilla version of our model simply\ndeﬁnes ¯a := a and ¯b := b. With this input rep-\nresentation, our model does not make use of word\norder. However, we discuss an extension using intra-\nsentence attention in Section 3.4 that uses a minimal\namount of sequence information.\nThe core model consists of the following three\ncomponents (see Figure 1), which are trained jointly:\nAttend. First, soft-align the elements of ¯a and ¯b\nusing a variant of neural attention (Bahdanau et al.,\n2015) and decompose the problem into the compari-\nson of aligned subphrases.\nCompare. Second, separately compare each\naligned subphrase to produce a set of vectors\n{v1,i}ℓa\ni=1 for a and {v2,j}ℓb\nj=1 for b. Each v1,i is\na nonlinear combination of ai and its (softly) aligned\nsubphrase in b (and analogously for v2,j).\nAggregate. Finally, aggregate the sets {v1,i}ℓa\ni=1\nand {v2,j}ℓb\nj=1 from the previous step and use the\nresult to predict the label ˆy.\n3.1 Attend\nWe ﬁrst obtain unnormalized attention weights eij,\ncomputed by a function F′, which decomposes as:\neij := F′(¯ai,¯bj) := F(¯ai)T F(¯bj) . (1)\nThis decomposition avoids the quadratic complexity\nthat would be associated with separately applying F′\nℓa ×ℓb times. Instead, only ℓa + ℓb applications of\nF are needed. We take F to be a feed-forward neural\nnetwork with ReLU activations (Glorot et al., 2011).\nThese attention weights are normalized as follows:\nβi :=\nℓb∑\nj=1\nexp(eij)\n∑ℓb\nk=1 exp(eik)\n¯bj ,\nαj :=\nℓa∑\ni=1\nexp(eij)∑ℓa\nk=1 exp(ekj)\n¯ai . (2)\nHere βi is the subphrase in ¯b that is (softly) aligned\nto ¯ai and vice versa for αj.\n3.2 Compare\nNext, we separately compare the aligned phrases\n{(¯ai,βi)}ℓa\ni=1 and {(¯bj,αj)}ℓb\nj=1 using a function G,\nwhich in this work is again a feed-forward network:\nv1,i := G([¯ai,βi]) ∀i∈[1,...,ℓ a] ,\nv2,j := G([¯bj,αj]) ∀j ∈[1,...,ℓ b] . (3)\nwhere the brackets [·,·] denote concatenation. Note\nthat since there are only a linear number of terms in\nthis case, we do not need to apply a decomposition\nas was done in the previous step. Thus Gcan jointly\ntake into account both ¯ai,and βi.\n3.3 Aggregate\nWe now have two sets of comparison vectors\n{v1,i}ℓa\ni=1 and {v2,j}ℓb\nj=1. We ﬁrst aggregate over\neach set by summation:\nv1 =\nℓa∑\ni=1\nv1,i , v2 =\nℓb∑\nj=1\nv2,j . (4)\nand feed the result through a ﬁnal classiﬁer H, that\nis a feed forward network followed by a linear layer:\nˆy = H([v1,v2]) , (5)\nwhere ˆy ∈RC represents the predicted (unnormal-\nized) scores for each class and consequently the pre-\ndicted class is given by ˆy= argmaxiˆyi.\nFor training, we use multi-class cross-entropy loss\nwith dropout regularization (Srivastava et al., 2014):\nL(θF ,θG,θH) = 1\nN\nN∑\nn=1\nC∑\nc=1\ny(n)\nc log exp(ˆyc)∑C\nc′=1 exp(ˆyc′ )\n.\nHere θF ,θG,θH denote the learnable parameters of\nthe functions F, G and H, respectively.\n3.4 Intra-Sentence Attention (Optional)\nIn the above model, the input representations are\nsimple word embeddings. However, we can augment\nthis input representation withintra-sentence attention\nto encode compositional relationships between words\nwithin each sentence, as proposed by Cheng et al.\n(2016). Similar to Eqs. 1 and 2, we deﬁne\nfij := Fintra(ai)T Fintra(aj) , (6)\nwhere Fintra is a feed-forward network. We then cre-\nate the self-aligned phrases\na′\ni :=\nℓa∑\nj=1\nexp(fij + di−j)∑ℓa\nk=1 exp(fik + di−k)\naj . (7)\nThe distance-sensitive bias terms di−j ∈R provides\nthe model with a minimal amount of sequence infor-\nmation, while remaining parallelizable. These terms\nare bucketed such that all distances greater than 10\nwords share the same bias. The input representation\nfor subsequent steps is then deﬁned as ¯ai := [ai,a′\ni]\nand analogously ¯bi := [bi,b′\ni].\n4 Computational Complexity\nWe now discuss the asymptotic complexity of our\napproach and how it offers a higher degree of par-\nallelism than LSTM-based approaches. Recall that\nd denotes embedding dimension and ℓ means sen-\ntence length. For simplicity we assume that all hid-\nden dimensions are d and that the complexity of\nmatrix(d×d)-vector(d×1) multiplication is O(d2).\nA key assumption of our analysis is that ℓ < d,\nwhich we believe is reasonable and is true of the\nSNLI dataset (Bowman et al., 2015) where ℓ <80,\nwhereas recent LSTM-based approaches have used\nd ≥300. This assumption allows us to bound the\ncomplexity of computing the ℓ2 attention weights.\nComplexity of LSTMs. The complexity of an\nLSTM cell is O(d2), resulting in a complexity of\nO(ℓd2) to encode the sentence. Adding attention as\nin Rockt¨aschel et al. (2016) increases this complexity\nto O(ℓd2 + ℓ2d).\nComplexity of our Approach. Application of a\nfeed-forward network requires O(d2) steps. Thus,\nthe Compare and Aggregate steps have complexity\nO(ℓd2) and O(d2) respectively. For the Attend step,\nMethod Train Acc Test Acc #Parameters\nLexicalized Classiﬁer (Bowman et al., 2015) 99.7 78.2 –\n300D LSTM RNN encoders (Bowman et al., 2016) 83.9 80.6 3.0M\n1024D pretrained GRU encoders (Vendrov et al., 2015) 98.8 81.4 15.0M\n300D tree-based CNN encoders (Mou et al., 2015) 83.3 82.1 3.5M\n300D SPINN-PI encoders (Bowman et al., 2016) 89.2 83.2 3.7M\n100D LSTM with attention (Rockt¨aschel et al., 2016) 85.3 83.5 252K\n300D mLSTM (Wang and Jiang, 2016) 92.0 86.1 1.9M\n450D LSTMN with deep attention fusion (Cheng et al., 2016) 88.5 86.3 3.4M\nOur approach (vanilla) 89.5 86.3 382K\nOur approach with intra-sentence attention 90.5 86.8 582K\nTable 1:Train/test accuracies on the SNLI dataset and number of parameters (excluding embeddings) for each approach.\nMethod N E C\nBowman et al. (2016) 80.6 88.2 85.5\nWang and Jiang (2016) 81.6 91.6 87.4\nOur approach (vanilla) 83.6 91.3 85.8\nOur approach w/ intra att. 83.7 92.1 86.7\nTable 2:Breakdown of accuracy with respect to classes on SNLI\ndevelopment set. N=neutral, E=entailment, C=contradiction.\nF is evaluated O(ℓ) times, giving a complexity of\nO(ℓd2). Each attention weight eij requires one dot\nproduct, resulting in a complexity of O(ℓ2d).\nThus the total complexity of the model is O(ℓd2 +\nℓ2d), which is equal to that of an LSTM with atten-\ntion. However, note that with the assumption that\nℓ<d , this becomes O(ℓd2) which is the same com-\nplexity as a regular LSTM. Moreover, unlike the\nLSTM, our approach has the advantage of being par-\nallelizable over ℓ, which can be useful at test time.\n5 Experiments\nWe evaluate our approach on the Stanford Natural\nLanguage Inference (SNLI) dataset (Bowman et al.,\n2015). Given a sentences pair (a,b), the task is to\npredict whether b is entailed by a, b contradicts a,\nor whether their relationship is neutral.\n5.1 Implementation Details\nThe method was implemented in TensorFlow (Abadi\net al., 2015).\nData preprocessing: Following Bowman et al.\n(2015), we remove examples labeled “–” (no gold\nlabel) from the dataset, which leaves 549,367 pairs\nfor training, 9,842 for development, and 9,824 for\ntesting. We use the tokenized sentences from the\nnon-binary parse provided in the dataset and prepend\neach sentence with a “NULL” token. During training,\neach sentence was padded up to the maximum length\nof the batch for efﬁcient training (the padding was\nexplicitly masked out so as not to affect the objec-\ntive/gradients). For efﬁcient batching in TensorFlow,\nwe semi-sorted the training data to ﬁrst contain ex-\namples where both sentences had length less than\n20, followed by those with length less than 50, and\nthen the rest. This ensured that most training batches\ncontained examples of similar length.\nEmbeddings: We use 300 dimensional GloVe\nembeddings (Pennington et al., 2014) to represent\nwords. Each embedding vector was normalized to\nhave ℓ2 norm of 1 and projected down to 200 di-\nmensions, a number determined via hyperparameter\ntuning. Out-of-vocabulary (OOV) words are hashed\nto one of 100 random embeddings each initialized\nto mean 0 and standard deviation 1. All embeddings\nremain ﬁxed during training, but the projection ma-\ntrix is trained. All other parameter weights (hidden\nlayers etc.) were initialized from random Gaussians\nwith mean 0 and standard deviation 0.01.\nEach hyperparameter setting was run on a sin-\ngle machine with 10 asynchronous gradient-update\nthreads, using Adagrad (Duchi et al., 2011) for opti-\nmization with the default initial accumulator value of\n0.1. Dropout regularization (Srivastava et al., 2014)\nwas used for all ReLU layers, but not for the ﬁnal\nlinear layer. We additionally tuned the following\nhyperparameters and present their chosen values in\nID Sentence 1 Sentence 2 DA (vanilla) DA (intra att.) SPINN-PI mLSTM Gold\nA Two kids are standing in the ocean huggingeach other. Two kids enjoy their day at the beach.N N E E N\nB A dancer in costumer performs on stage whilea man watches. the man is captivated N N E E N\nC They are sitting on the edge of a fountain The fountain is splashing the persons seated.N N C C N\nD Two dogs play with tennis ball in ﬁeld. Dogs are watching a tennis match.N C C C C\nE Two kids begin to make a snowman on a sunnywinter day. Two penguins making a snowman.N C C C C\nF The horses pull the carriage, holding peopleand a dog, through the rain. Horses ride in a carriage pulled by a dog.E E C C C\nG A woman closes her eyes as she plays hercello. The woman has her eyes open. E E E E C\nH Two women having drinks and smokingcigarettes at the bar. Three women are at a bar. E E E E C\nI A band playing with fans watching. A band watches the fans playE E E E C\nTable 3:Example wins and losses compared to other approaches. DA (Decomposable Attention) refers to our approach while\nSPINN-PI and mLSTM are previously developed methods (see Table 1).\nparentheses: network size (2-layers, each with 200\nneurons), batch size (4), 1 dropout ratio (0.2) and\nlearning rate (0.05–vanilla, 0.025–intra-attention).\nAll settings were run for 50 million steps (each step\nindicates one batch) but model parameters were saved\nfrequently as training progressed and we chose the\nmodel that did best on the development set.\n5.2 Results\nResults in terms of 3-class accuracy are shown in\nTable 1. Our vanilla approach achieves state-of-the-\nart results with almost an order of magnitude fewer\nparameters than the LSTMN of Cheng et al. (2016).\nAdding intra-sentence attention gives a considerable\nimprovement of 0.5 percentage points over the ex-\nisting state of the art. Table 2 gives a breakdown of\naccuracy on the development set showing that most\nof our gains stem from neutral, while most losses\ncome from contradiction pairs.\nTable 3 shows some wins and losses. Examples A-\nC are cases where both variants of our approach are\ncorrect while both SPINN-PI (Bowman et al., 2016)\nand the mLSTM (Wang and Jiang, 2016) are incor-\nrect. In the ﬁrst two cases, both sentences contain\nphrases that are either identical or highly lexically\nrelated (e.g. “Two kids” and “ocean / beach”) and our\napproach correctly favors neutral in these cases. In\nExample C, it is possible that relying on word-order\nmay confuse SPINN-PI and the mLSTM due to how\n“fountain” is the object of a preposition in the ﬁrst\nsentence but the subject of the second.\nThe second set of examples (D-F) are cases where\n116 or 32 also work well and are a bit more stable.\nour vanilla approach is incorrect but mLSTM and\nSPINN-PI are correct. Example F requires sequen-\ntial information and neither variant of our approach\ncan predict the correct class. Examples D-E are in-\nteresting however, since they don’t require word or-\nder information, yet intra-attention seems to help.\nWe suspect this may be because the word embed-\ndings are not ﬁne-grained enough for the algorithm\nto conclude that “play/watch” is a contradiction, but\nintra-attention, by adding an extra layer of composi-\ntion/nonlinearity to incorporate context, compensates\nfor this.\nFinally, Examples G-I are cases that all methods\nget wrong. The ﬁrst is actually representative of many\nexamples in this category where there is one critical\nword that separates the two sentences (close vs open\nin this case) and goes unnoticed by the algorithms.\nExamples H requires inference about numbers and\nExample I needs sequence information.\n6 Conclusion\nWe presented a simple attention-based approach to\nnatural language inference that is trivially paralleliz-\nable. The approach outperforms considerably more\ncomplex neural methods aiming for text understand-\ning. Our results suggest that, at least for this task,\npairwise comparisons are relatively more important\nthan global sentence-level representations.\nAcknowledgements\nWe thank Slav Petrov, Tom Kwiatkowski, Yoon Kim,\nErick Fonseca, Mark Neumann for useful discussion\nand Sam Bowman and Shuohang Wang for providing\nus their model outputs for error analysis.\nReferences\n[Abadi et al.2015] Mart´ın Abadi, Ashish Agarwal, Paul\nBarham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S Corrado, Andy Davis, Jeffrey Dean, Matthieu\nDevin, et al. 2015. TensorFlow: Large-scale machine\nlearning on heterogeneous systems. Software available\nfrom tensorﬂow. org.\n[Andreas et al.2013] Jacob Andreas, Andreas Vlachos,\nand Stephen Clark. 2013. Semantic parsing as ma-\nchine translation. In Proceedings of ACL.\n[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. 2015. Neural machine trans-\nlation by jointly learning to align and translate. In\nProceedings of ICLR.\n[Bos and Markert2005] Johan Bos and Katja Markert.\n2005. Recognising textual entailment with logical in-\nference. In Proceedings of EMNLP.\n[Bowman et al.2015] Samuel R. Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D. Manning. 2015.\nA large annotated corpus for learning natural language\ninference. In Proceedings of EMNLP.\n[Bowman et al.2016] Samuel R. Bowman, Jon Gauthier,\nAbhinav Rastogi, Raghav Gupta, Christopher D. Man-\nning, and Christopher Potts. 2016. A fast uniﬁed model\nfor parsing and sentence understanding. In Proceedings\nof ACL.\n[Chang et al.2010] Ming-Wei Chang, Dan Goldwasser,\nDan Roth, and Vivek Srikumar. 2010. Discrimina-\ntive learning over constrained latent representations. In\nProceedings of HLT-NAACL.\n[Cheng et al.2016] Jianpeng Cheng, Li Dong, and Mirella\nLapata. 2016. Long short-term memory-networks for\nmachine reading. In Proceedings of EMNLP.\n[Das and Smith2009] Dipanjan Das and Noah A. Smith.\n2009. Paraphrase identiﬁcation as probabilistic quasi-\nsynchronous recognition. In Proceedings of ACL-\nIJCNLP.\n[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram\nSinger. 2011. Adaptive subgradient methods for online\nlearning and stochastic optimization. The Journal of\nMachine Learning Research, 12:2121–2159.\n[Fader et al.2013] Anthony Fader, Luke S Zettlemoyer,\nand Oren Etzioni. 2013. Paraphrase-driven learning\nfor open question answering. In Proceedings of ACL.\n[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and\nYoshua Bengio. 2011. Deep sparse rectiﬁer neural\nnetworks. In Proceedings of AISTATS.\n[Haghighi et al.2005] Aria D. Haghighi, Andrew Y . Ng,\nand Christopher D. Manning. 2005. Robust textual\ninference via graph matching. In Proceedings of HLT-\nNAACL.\n[Hickl and Bensley2007] Andrew Hickl and Jeremy Bens-\nley. 2007. A discourse commitment-based framework\nfor recognizing textual entailment. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment and\nParaphrasing. Association for Computational Linguis-\ntics.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. 1997. Long short-term memory.\nNeural computation, 9(8):1735–1780.\n[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li, and\nQingcai Chen. 2014. Convolutional neural network\narchitectures for matching natural language sentences.\nIn Advances in NIPS.\n[Katz1972] Jerrold J. Katz. 1972. Semantic theory .\nHarper & Row.\n[Koehn2009] Philipp Koehn. 2009. Statistical machine\ntranslation. Cambridge University Press.\n[LeCun et al.1990] Y . LeCun, B. Boser, J.S. Denker,\nD. Henderson, R.E. Howard, W. Hubbard, and L.D.\nJackel. 1990. Handwritten digit recognition with a\nback-propagation network. In Advances in NIPS.\n[MacCartney and Manning2009] Bill MacCartney and\nChristopher D. Manning. 2009. An extended model of\nnatural logic. In Proceedings of the IWCS.\n[MacCartney et al.2006] Bill MacCartney, Trond\nGrenager, Marie-Catherine de Marneffe, Daniel Cer,\nand Christopher D Manning. 2006. Learning to\nrecognize features of valid textual entailments. In\nProceedings of HLT-NAACL.\n[MacCartney et al.2008] Bill MacCartney, Michel Galley,\nand Christopher D Manning. 2008. A phrase-based\nalignment model for natural language inference. In\nProceedings of EMNLP.\n[Marsi and Krahmer2005] Erwin Marsi and Emiel Krah-\nmer. 2005. Classiﬁcation of semantic relations by\nhumans and machines. In Proceedings of the ACL\nworkshop on Empirical Modeling of Semantic Equiva-\nlence and Entailment.\n[Mou et al.2015] Lili Mou, Men Rui, Ge Li, Yan Xu,\nLu Zhang, Rui Yan, and Zhi Jin. 2015. Natural lan-\nguage inference by tree-based convolution and heuristic\nmatching. In Proceedings of ACL (short papers).\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D. Manning. 2014. GloVe:\nGlobal vectors for word representation. In Proceedings\nof EMNLP.\n[Rockt¨aschel et al.2016] Tim Rockt ¨aschel, Edward\nGrefenstette, Karl Moritz Hermann, Tom ´aˇs Koˇcisk`y,\nand Phil Blunsom. 2016. Reasoning about entailment\nwith neural attention. In Proceedings of ICLR.\n[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hinton,\nAlex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. 2014. Dropout: A simple way to prevent neural\nnetworks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958.\n[van Benthem2008] Johan van Benthem. 2008. A brief\nhistory of natural logic. College Publications.\n[Vendrov et al.2015] Ivan Vendrov, Ryan Kiros, Sanja Fi-\ndler, and Raquel Urtasun. 2015. Order-embeddings of\nimages and language. In Proceedings of ICLR.\n[Wang and Jiang2016] Shuohang Wang and Jing Jiang.\n2016. Learning natural language inference with LSTM.\nIn Proceedings of NAACL.\n[Yin et al.2016] Wenpeng Yin, Hinrich Sch¨utze, Bing Xi-\nang, and Bowen Zhou. 2016. ABCNN: Attention-\nbased convolutional neural network for modeling sen-\ntence pairs. In Transactions of the Association of Com-\nputational Linguistics.",
  "topic": "Parallelizable manifold",
  "concepts": [
    {
      "name": "Parallelizable manifold",
      "score": 0.8883856534957886
    },
    {
      "name": "Inference",
      "score": 0.786894679069519
    },
    {
      "name": "Computer science",
      "score": 0.7013899683952332
    },
    {
      "name": "Sentence",
      "score": 0.5919941663742065
    },
    {
      "name": "Natural language",
      "score": 0.5818517208099365
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.535519003868103
    },
    {
      "name": "Word (group theory)",
      "score": 0.5244479775428772
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49395185708999634
    },
    {
      "name": "Order (exchange)",
      "score": 0.4537721574306488
    },
    {
      "name": "Language model",
      "score": 0.43651413917541504
    },
    {
      "name": "Natural language processing",
      "score": 0.4297829568386078
    },
    {
      "name": "Word order",
      "score": 0.4173978567123413
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.41018790006637573
    },
    {
      "name": "Algorithm",
      "score": 0.2557976245880127
    },
    {
      "name": "Mathematics",
      "score": 0.17302057147026062
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 63
}