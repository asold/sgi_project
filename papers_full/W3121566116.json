{
    "title": "Gauge Invariant and Anyonic Symmetric Transformer and RNN Quantum States for Quantum Lattice Models",
    "url": "https://openalex.org/W3121566116",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5031223524",
            "name": "Di Luo",
            "affiliations": [
                "Intel (United States)",
                "Massachusetts Institute of Technology",
                "The NSF AI Institute for Artificial Intelligence and Fundamental Interactions",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5100345072",
            "name": "Zhuo Chen",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "The NSF AI Institute for Artificial Intelligence and Fundamental Interactions",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5082679400",
            "name": "Kaiwen Hu",
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5048820768",
            "name": "Zhizhen Zhao",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5028597140",
            "name": "Vera Mikyoung Hur",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5112482095",
            "name": "Bryan K. Clark",
            "affiliations": [
                "Intel (United States)",
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2912516940",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2916528205",
        "https://openalex.org/W3097229954",
        "https://openalex.org/W3042241440",
        "https://openalex.org/W2077485894",
        "https://openalex.org/W1721005526",
        "https://openalex.org/W2151642301",
        "https://openalex.org/W2949158470",
        "https://openalex.org/W3046391053",
        "https://openalex.org/W2952753050",
        "https://openalex.org/W3006106210",
        "https://openalex.org/W2135273380",
        "https://openalex.org/W2004152702",
        "https://openalex.org/W2108682071",
        "https://openalex.org/W2975801707",
        "https://openalex.org/W2888077409",
        "https://openalex.org/W3101975431",
        "https://openalex.org/W3103822092",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W3012274818",
        "https://openalex.org/W3102618344",
        "https://openalex.org/W571091908",
        "https://openalex.org/W3101214870",
        "https://openalex.org/W1574355293",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W1537243684",
        "https://openalex.org/W2755695124",
        "https://openalex.org/W2953495237",
        "https://openalex.org/W2298547144",
        "https://openalex.org/W3012220381",
        "https://openalex.org/W2604542431",
        "https://openalex.org/W4226267670",
        "https://openalex.org/W3045184637",
        "https://openalex.org/W3168498537",
        "https://openalex.org/W3046694600",
        "https://openalex.org/W2990616205",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3028529071",
        "https://openalex.org/W2997300629",
        "https://openalex.org/W3100128231",
        "https://openalex.org/W2916052864",
        "https://openalex.org/W2152240519",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3104481216",
        "https://openalex.org/W4226506708",
        "https://openalex.org/W2010785223",
        "https://openalex.org/W2995786632",
        "https://openalex.org/W2914465168",
        "https://openalex.org/W2212735930",
        "https://openalex.org/W2119836827",
        "https://openalex.org/W2895363620",
        "https://openalex.org/W3201802401",
        "https://openalex.org/W2902247106",
        "https://openalex.org/W2935584311",
        "https://openalex.org/W2921586812",
        "https://openalex.org/W2883672905",
        "https://openalex.org/W3105828811",
        "https://openalex.org/W2580506952",
        "https://openalex.org/W3099072354",
        "https://openalex.org/W2027330396",
        "https://openalex.org/W3101721439",
        "https://openalex.org/W2994906999",
        "https://openalex.org/W2160049695",
        "https://openalex.org/W2097945906",
        "https://openalex.org/W2041650868",
        "https://openalex.org/W3009104906",
        "https://openalex.org/W2097361916",
        "https://openalex.org/W3103987134",
        "https://openalex.org/W2419175238",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Symmetries such as gauge invariance and anyonic symmetry play a crucial role in quantum many-body physics. We develop a general approach to constructing gauge invariant or anyonic symmetric autoregressive neural network quantum states, including a wide range of architectures such as Transformer and recurrent neural network (RNN), for quantum lattice models. These networks can be efficiently sampled and explicitly obey gauge symmetries or anyonic constraint. We prove that our methods can provide exact representation for the ground and excited states of the 2D and 3D toric codes, and the X-cube fracton model. We variationally optimize our symmetry incorporated autoregressive neural networks for ground states as well as real-time dynamics for a variety of models. We simulate the dynamics and the ground states of the quantum link model of $\\text{U(1)}$ lattice gauge theory, obtain the phase diagram for the 2D $\\mathbb{Z}_2$ gauge theory, determine the phase transition and the central charge of the $\\text{SU(2)}_3$ anyonic chain, and also compute the ground state energy of the SU(2) invariant Heisenberg spin chain. Our approach provides powerful tools for exploring condensed matter physics, high energy physics and quantum information science.",
    "full_text": "Gauge Invariant and Anyonic Symmetric Transformer and RNN Quantum States for\nQuantum Lattice Models\nDi Luo,1, 2, 3, 4,∗ Zhuo Chen,1, 3, 4,∗ Kaiwen Hu,1, 5, 6 Zhizhen Zhao,7 Vera Mikyoung Hur,6 and Bryan K. Clark 1, 2\n1Department of Physics, University of Illinois at Urbana-Champaign, IL 61801, USA\n2IQUIST and Institute for Condensed Matter Theory and NCSA Center for Artificial Intelligence Innovation,\nUniversity of Illinois at Urbana-Champaign, IL 61801, USA\n3The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\n4Center for Theoretical Physics, Massachusetts Institute of Technology, MA, 02139, USA\n5Department of Physics, University of Michigan, Ann Arbor, Michigan 48109, USA\n6Department of Mathematics, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA\n7Department of Electrical and Computer Engineering and CSL,\nUniversity of Illinois at Urbana-Champaign, Urbana, IL 61801, USA\nSymmetries such as gauge invariance and anyonic symmetry play a crucial role in quantum many-\nbody physics. We develop a general approach to constructing gauge invariant or anyonic symmetric\nautoregressive neural network quantum states, including a wide range of architectures such as Trans-\nformer and recurrent neural network (RNN), for quantum lattice models. These networks can be\nefficiently sampled and explicitly obey gauge symmetries or anyonic constraint. We prove that our\nmethods can provide exact representation for the ground and excited states of the 2D and 3D toric\ncodes, and the X-cube fracton model. We variationally optimize our symmetry incorporated au-\ntoregressive neural networks for ground states as well as real-time dynamics for a variety of models.\nWe simulate the dynamics and the ground states of the quantum link model of U(1) lattice gauge\ntheory, obtain the phase diagram for the 2DZ2 gauge theory, determine the phase transition and the\ncentral charge of the SU(2)3 anyonic chain, and also compute the ground state energy of the SU(2)\ninvariant Heisenberg spin chain. Our approach provides powerful tools for exploring condensed\nmatter physics, high energy physics and quantum information science.\nI. INTRODUCTION\nIn recent years, there has been a growing interest\nin machine learning approaches to simulating quantum\nmany-body systems [1–23]. An important step in this\ndirection is the use of neural networks, e.g. restricted\nBoltzmann machines, to represent variational wave func-\ntions. However, many neural networks do not automati-\ncally enforce the symmetries of physical models. A con-\nsiderable amount of work has been devoted to remedy the\ndeficiency for several classes of global symmetries, such\nas translational symmetry [3], discrete rotational symme-\ntry [3], global U(1) symmetry [4], and anti-symmetry [5–\n7].\nIn addition to global symmetries, local symmetries can\nbe encoded through gauge invariance. The notion of\ngauge invariance is crucial in quantum mechanics. In\nhigh energy physics, theory is required to be invariant\nunder the action of gauge symmetry groups [24]. Gauge\ninvariance appears naturally in various condensed matter\nphysics models. For example, topological states of toric\ncode and double semion models arise as the ground states\nof their gauge-invariant Hamiltonians [25, 26]. Also,\nnovel quantum matter such as fracton is the ground\nstate of a Hamiltonian where the subsystem symmetry\nis gauged [27]. In quantum information, various quan-\ntum error correction codes can be viewed as eigenstates\nin a certain gauge-invariant code space [28]. Besides\n∗ Co-first authors.\ngauge symmetries, anyonic symmetry is another impor-\ntant local constraint that arises in exotic phases of mat-\nter [29, 30] and topological quantum computation [31].\nThe study of quantum lattice models with gauge or any-\nonic symmetries is significant to enhance our understand-\ning of high energy physics, condensed matter physics, and\nquantum information science.\nSimulating quantum many-body gauge theory is expo-\nnentially costly. There has been much effort to efficiently\nsimulate quantum lattice gauge theory with both digital\nand analog quantum computers [32], but more effort is\nrequired experimentally to achieve good fidelity. Two\nstandard approaches to simulating gauge theory classi-\ncally are stochastic, integrating an effective Lagrangian\nby sampling, and variational. When simulating gauge\ntheory, the stochastic approach naturally obeys gauge\ninvariance but is plagued with exponential costs associ-\nated with the sign problem in models with finite density\nof fermions or involving quantum dynamics [33]. The\nvariational approach overcomes the difficulty by being\nconstrained to an approximate variational space. Impos-\ning gauge symmetries in the variational approach is par-\nticularly important and challenging as, otherwise, lower\nenergy states can exist in the gauge-violating part of a\nHilbert space. Therefore gauge symmetries must be ex-\nplicitly constrained. While the stochastic approach has\nbeen well studied, there have been limited attempts at\nusing the variational approach for gauge theory. Tensor\nnetworks can be readily applied to gauge theory in one\ndimension and ongoing efforts are required to work with\nchallenges in higher dimensions [32, 34]. A variational\narXiv:2101.07243v4  [cond-mat.str-el]  7 Jun 2024\n2\napproach based on gauge equivariant networks has been\nintroduced very recently [35–37].\nWe develop for the first time, a general approach to\nconstructing gauge invariant or anyonic symmetric (such\nas the fusion rule for anyons) autoregressive neural net-\nworks (AR-NN) for quantum lattice models. Autoregres-\nsive neural networks, such as recurrent neural networks\n(RNN) [38, 39], pixel convolutional neural networks (pix-\nelCNN) [40], and Transformers [41], have revolutionized\nthe fields of computer vision and language translation\nand generation, among many others. Autoregressive neu-\nral networks quantum states have recently been intro-\nduced in quantum many-body physics [4, 42, 43] and\nshown to be capable of representing volume law states\n(as one generically needs in dynamics) with a number of\nparameters that scale sub-linearly [44]. A central fea-\nture of AR-NN is their capability of exactly sampling\nconfigurations from them. This is to be contrasted with\nthe standard approach of sampling configurations by do-\ning a random walk over a Markov chain, which is often\nplagued with long equilibration times and non-ergodic\nbehaviors. We construct gauge invariant AR-NN for the\nquantum link model of U(1) lattice gauge theory [45],\nZN gauge theory, and anyonic symmetric AR-NN for\nSU(2)k anyons. We demonstrate the exact representa-\ntion of gauge invariant AR-NN for the ground and ex-\ncited states of the 2D [26] and 3D [46] toric codes, and\nthe X-cube fracton model [47]. We optimize our sym-\nmetry incorporated AR-NN for the quantum link model,\nthe 2D toric code in a transverse field, the 1D Heisen-\nberg chain with SU(2) symmetry, and the SU(2)3 anyonic\nchain [29, 30, 48], to obtain ground states accurately and\nextract phase diagrams and various dynamic properties.\nII. CONSTRUCTION FOR GAUGE OR\nANYONIC SYMMETRIES\nOur goal in this work is to generate autoregressive neu-\nral networks (AR-NN) which variationally represent wave\nfunctions of quantum lattice models and explicitly obey\ntheir gauge symmetries—i.e. given a set of gauge symme-\ntry operators {Gi} with local support, we would like to\nconstruct a wave function |ψ⟩ such that Gi |ψ⟩ = |ψ⟩ for\neach i. To do this, we will work within the ‘gauge basis’\n{|x⟩} which is diagonal in the gauge, ⟨x|ψ⟩ = ⟨x|Gi|ψ⟩.\nA sufficient condition of gauge invariance of the wave\nfunction is to ensure that the gauge-violating basis ele-\nments |x⟩ have zero amplitude in |ψ⟩. Throughout this\nwork, we will primarily work with gauges Gi which are\nlocal—i.e. Gi |x⟩ only affects a compact range of sites\nwithin the vicinity of site i.\nWhile we would typically want our AR-NN to take\nas input the configuration {x1, x2, . . . , xn} and evaluate\nψ(x1, x2, . . . , xn), we will find it useful to instead evaluate\nψ(ex) where ex ≡ {ex1, ex2, . . . ,exn}, exi ≡ (xi1 , xi2 , . . . , xiv ),\nis a composite particle specifying the configuration of not\nonly site i but also some number of nearby sites. The\nmotivation for working with composite particles is that a\nparticular local gauge constraint Gi might only depend\non composite particle exi (and potentially exi+1), making\nit easier to apply the gauge constraints. Different com-\nposite particles can naturally overlap in physical sites and\nwe will simply augment our gauge constraints to require\nthat the configurations of the composite particles agree\non the state of a physical site—i.e. basis states of com-\nposite particles which map to disagreeing physical states\nshould also have zero amplitude.\nAR-NN perform two functions: sampling and evalua-\ntion. AR-NN can sample configurations ex from |ψ(ex)|2.\nThis is done sequentially (in some pre-determined or-\nder) one composite particle exi at a time; the probabil-\nity to sample exi is equal to a2(exi|ex<i) where a(exi|ex<i)\nis a function which returns the conditional ampli-\ntude. Evaluation of the AR-NN gives a value ψ(ex) =Qn\ni=1 a(exi|ex<i)eiθ(exi|ex<i) where θ(exi|ex<i) is a function\nwhich returns the conditional phase. Both evaluation\nand sampling rely on the existence of a gauge block which\ntakes ex1, . . . ,exk−1 and outputs the possible values {ezi}\nof exk along with their respective amplitudes a(ezi|ex<k)\nand phases θ(ezi|ex<k), ensuring that the amplitude of any\nconfiguration which is going to violate the gauge con-\nstraint is set to zero. To build this gauge block, we start\nwith an autoregressive neural network block which re-\nturns a list of amplitudes which do not constrain the\ngauge (such blocks are standard in autoregressive mod-\nels such as Transformers and RNN); we then zero out\nthose partial configurations which break the gauge (on\nthe already established composite particles) and renor-\nmalize the probabilities in this list (see Fig. 1(a)). Given\nthe gauge block it is then straightforward to both sam-\nple and evaluate (see Fig. 1(b, c)). Note the probability\ninduced by our AR-NN is different from the probabil-\nity induced by the AR-NN with only the autoregressive\nneural network block even if one projects out the gauge-\nbreaking configurations from the latter network.\nIt is worth noticing that the construction is not limited\nto gauge theory, but can be generalized to wave functions\nwith either local or global constraints which are checked\nin the same way as gauge constraints are checked. This\nwill be helpful for describing constraints from certain\nglobal symmetries or special algebraic structure, such as\nthe SU(2) symmetry for the Heisenberg model and the\nSU(2)k fusion rules for non-abelian anyons.\nIII. OPTIMIZATION ALGORITHMS\nWe use AR-NN to calculate both ground states and\nreal-time dynamic properties. In both cases, we need to\noptimize our AR-NN. For ground states, an AR-NN is\noptimized with respect to energy and for real-time dy-\nnamics, we optimize an AR-NN at time-step t + 2τ given\na network at timet. We describe the details of these opti-\nmizations. As these optimization approaches are general,\nwe use x to denote a configuration, but for the context\n3\nAutoregressive\nNeural Network Block\nGauge\nChecking\nNormalize\nRemove gauge-\nbreaking terms\nSelect\nGauge Block\nGauge Block\nGauge Block\nSample Sample Sample\n(b) Evaluation Sampling(c)\nGauge Block Gauge Block\nFIG. 1. Autoregressive parameterization of wave function with n composite particles. (a) Gauge block. The input\n{ex1, ex1, . . . ,exk−1} is processed through the autoregressive neural network block (see Appendix E for details), to output ampli-\ntude and phase parts. The amplitude part goes through gauge checking, which removes the gauge breaking terms. Afterwards,\nthe square of the amplitude is normalized. (b) Evaluation process. The evaluation process can be performed in parallel for all\nthe input sites. Given the input {exk}, the gauge block simultaneously generates amplitudes and phases for all sites. We then\nselect the correct amplitudes and phases based on the input configuration for each site and construct the wave function from\nthe selected amplitudes and phases. (c) Sampling process. The sampling is done sequentially for each site. We begin with\nno input and generate the amplitude and phase for the first site. The configuration of the first site is then sampled from the\nsquare of the amplitude. Afterwards, we feed the first sample into the gauge block to obtain the second sample. This process\ncontinues until we obtain the whole configuration.\nof the paper, x should be viewed as a composite particle\nconfiguration.\nFor the ground state optimization, we stochastically\nminimize the expectation of energy for a Hamiltonian H\nand a wave function |ψθ⟩ as\n⟨ψθ|H|ψθ⟩ ≈1\nN\nNX\nx∼|ψθ|2\nHψθ(x)\nψθ(x) ≡ 1\nN\nNX\nx∼|ψθ|2\nEloc(x),\n(1)\nwhere N is the batch size and the gradient is given by\n∂\n∂θ ⟨ψθ|H|ψθ⟩ ≈2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001a\nEloc(x) ∂\n∂θ log ψ∗\nθ(x)\n\u001b\n.\n(2)\nWe further control the sampling variance [49] by sub-\ntracting from Eloc(x) the average over the batch, Eavg ≡\n1/N P\nx∈batch Eloc(x), and define the stochastic variance\nreduced loss function as\nLg = 2\nN\nNX\nx∼|ψθ|2\nℜ\nn\u0002\nEloc(x) − Eavg\n\u0003\nlog ψ∗\nθ(x)\no\n, (3)\nwhere the gradient is taken on logψ∗\nθ using PyTorch’s [50]\nautomatic differentiation.\nWith this loss function, we also use transfer learn-\ning techniques [51, 52]. We train our neural networks\nin smaller systems and use these parameters as the ini-\ntial starting points for optimizing for larger systems (see\nAppendix E for details).\nFor the dynamics optimization, we use a stochas-\ntic version of the logarithmic forward-backward trape-\nzoid method [53], which can be viewed as a higher or-\nder generalization of IT-SWO [54] and the logarithmic\nversion of the loss functions in Refs. 9 and 42. We\ninitialize two copies of the neural network ψθ(t) and\nψθ(t+2τ). At each time step, we train ψθ(t+2τ) to match\n(1 + iHτ )\n\f\fψθ(t+2τ)\n\u000b\n≡ |Ψθ⟩ and (1 − iHτ )\n\f\fψθ(t)\n\u000b\n≡ |Φ⟩\nby minimizing the negative logarithm of the overlap,\n−log (⟨Ψθ|Φ⟩⟨Φ|Ψθ⟩)/(⟨Ψθ|Ψθ⟩⟨Φ|Φ⟩). (Since we only\ntake the gradient on θ(t + 2τ), for simplicity, we write θ\nfor θ(t+2τ) and neglect θ(t).) The inner products related\nto θ can be evaluated stochastically as\n⟨Ψθ|Φ⟩ ≈1\nN\nNX\nx∼|ψθ|2\nΨ∗\nθ(x)Φ(x)\n|ψθ(x)|2 ≡ 1\nN\nNX\nx∼|ψθ|2\nα(x), (4)\n⟨Ψθ|Ψθ⟩ ≈1\nN\nNX\nx∼|ψθ|2\n|Ψθ(x)|2\n|ψθ(x)|2 ≡ 1\nN\nNX\nx∼|ψθ|2\nβ(x). (5)\nThe gradient of the negative logarithm of the overlap can\n4\nbe evaluated stochastically as\n∂\n∂θ\n\u0012\n−log ⟨Ψθ|Φ⟩⟨Φ|Ψθ⟩\n⟨Ψθ|Ψθ⟩⟨Φ|Φ⟩\n\u0013\n≈ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001a\u0014β(x)\nβavg\n− α(x)\nαavg\n\u0015 ∂\n∂θ log Ψ∗\nθ(x)\n\u001b\n,\n(6)\nwhere αavg and βavg are respectively the average values\nof α(x) and β(x) over the batch of samples. We can then\ndefine the loss function as\nLd ≈ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001a\u0014β(x)\nβavg\n− α(x)\nαavg\n\u0015\nlog Ψ∗\nθ(x)\n\u001b\n, (7)\nwhere the gradient is taken on log Ψ∗\nθ using PyTorch’s [50]\nautomatic differentiation.\nFor both optimizations,ψθ(x) is evaluated as described\nin Fig. 1(a) and x is sampled from |ψθ|2 as described in\nFig. 1(b). The full derivations of the stochastic gradients\nfor both optimizations are in Appendix G.\nIn addition, we extensively use the transfer learning\ntechnique, by training on small system sizes before mov-\ning on to large system sizes. The transfer learning tech-\nnique provides a good initialization for neural networks\nthat are trained on large system sizes. We observe that\nthe transfer learning technique in general significantly\nreduces the number of iterations needed. The details of\nusage of this technique are described in the captions of\neach figures. (See more details in Appendix E).\nIV. APPLICATIONS IN\nQUANTUM LATTICE MODELS\nA. U (1) Quantum Link Model\nThe quantum link model (QLM) of U(1) lattice gauge\ntheory in 1+1 dimensions in the Hamiltonian formulation\nwith staggered fermions [45] is defined as\nHQLM = −\nX\ni\nh\nψ†\ni Ui,i+1ψi+1 + ψ†\ni+1U†\ni,i+1ψi\ni\n+ m\nX\ni\n(−1)iψ†\ni ψi + g2\n2\nX\ni\nE2\ni,i+1,\n(8)\nwhere m is the staggered fermion mass, g is the gauge\ncoupling, i = 1 , 2, . . .labels the lattice site, ψi is the\nfermion operator, Ui,i+1 is the link variable andEi,i+1 the\nelectric flux for the U(1) gauge field on link (i, i+ 1) [45].\nWe denote by |qi⟩ the basis state at site i, and by |ei,i+1⟩\nthe basis at link ( i, i+ 1). Each unit cell is defined to\ninclude two sites and two links. The operators Ei,i+1\nand Ui,i+1 satisfy the following commutation relations:\n[Ei,i+1, Ui,i+1] = Ui,i+1, [ Ei,i+1, U†\ni,i+1] = −U†\ni,i+1 and\n[Ui,i+1, U†\ni,i+1] = 2 Ei,i+1. The gauge constraint is given\nby the Gauss’s law operator eGi = ψ†\ni ψi−Ei,i+1 +Ei−1,i+\n1\n2 [(−1)i − 1] such that the ground state |ψ⟩ satisfies\neGi |ψ⟩ = 0 for each i. The QLM has gained growing\ninterests and been studied in different settings in recent\nyears [32, 34, 55–57]. We focus on the (1+1)D QLM with\nthe S = 1/2 representation for the link operators Ui,i+1\nand Ei,i+1. Under the Jordan-Wigner transformation,\nEq. 8 becomes [58]\nH = −\nX\ni\n\u0002\nS+\ni S+\ni,i+1S−\ni+1 + H.c.\n\u0003\n+ m\nX\ni\n(−1)i(S3\ni + 1\n2) + g2\n2\nX\ni\n1\n4,\n(9)\nwhere S± ≡ S1 ± iS2, S1, S2, S3 are the Heisenberg\nmatrices, and the Gauss’s law operator becomes Gi =\nS3\ni − S3\ni,i+1 + S3\ni−1,i + 1\n2 (−1)i. For the S = 1/2 represen-\ntation, the last term on the right side of Eq. 9 is constant\nand, hence, can be discarded.\n... \n... \n... \n... \nFIG. 2. Composite particles for the quantum link model.\nEach composite particle is defined as |σi⟩ ≡ |qi, ei,i+1⟩. We\ncheck Gauss’s law between |σi⟩ and |σi+1⟩.\nWe define the composite particles of our gauge invari-\nant AR-NN as in Fig. 2 and choose an order from left to\nright. Each composite particle |σi⟩ consists of a fermion\n|qi⟩, which can be either |•⟩ or |◦⟩, and a gauge field in\nthe link |ei,i+1⟩, which can be either |→⟩ or |←⟩. Note\nthat in this case the composite particles do not overlap.\nThe Gauss’s law operator Gi acts on |σi⟩ and |σi+1⟩ to\ndetermine allowed configurations and so can be checked\nin the gauge block which generates the composite parti-\ncle at site i + 1. For example, given |σi⟩ = |• →⟩, |σi+1⟩\ncan only be |• →⟩or |◦ ←⟩if i is even, and |◦ →⟩if i is\nodd.\nWe implement and variationally optimize this AR-NN\nfor the ground state of Eq. 9. Fig. 3 shows the results\nfor 6 unit cells (i.e. 12 particles) which closely match\nthe energy of the exact solution. More importantly, the\ngauge invariant construction guarantees that the solution\nis in the physical space, while the neural network without\ngauge constraint (i.e. removing the gauge-checking from\nthe AR-NN) finds a lower energy but non-physical state.\nWe in addition compute the ground state for 40, 80,\n120 and 160 unit cells with both Transformer and RNN\n(Fig. 4). The average electric fields are compared with\ntensor network (TN) results [59]. We find that our re-\nsults (for matching system sizes) are similar to the TN\nresults for both Transformer and RNN. In addition, we\nextrapolated the ground state energy for the 160 unit\ncell model at m = 0.7 (see Fig. 18 in Appendix. A) by\nlinearly extrapolating in variance vs. energy. We find\nthat the extrapolated ground state energy is −199.7923,\nwhile our lowest energy is −199.7803 ± 0.0005, giving us\na relative error of only 6 × 10−5.\n5\n0 100 200 300 400 500 600 700 800 900 1000\niterations\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nH\nexact\nno Gauss\nGauss\nFIG. 3. Variational ground state optimization for the 6-unit-\ncell (12 sites and 12 links) open-boundary QLM for m = 0\nwith and without gauge invariant construction. The gauge\ninvariant autoregressive neural network reaches an accurate\nground state while the ansatz without gauge constraints ar-\nrives at a non-physical state in the optimization. We use the\nTransformer neural network with 1 layer, 32 hidden dimen-\nsions and the real-imaginary parameterization (see Fig. 24).\nThe neural network is randomly initialized and is trained for\n1000 iterations with 12000 samples in each iteration. The\nneural network architecture and optimization details are dis-\ncussed in Appendix E.\nWe also consider the real-time dynamics for m = 0.1\nand m = 2 .0 for 6 and 12 unit cells starting with an\ninitial product state with |• → ◦ →⟩for each unit cell.\nFig. 5(a) shows the conservation of energy for different\nansatzes. The total energy is −1.2 for m = 0.1, and −24\nfor m = 2.0. We find that our gauge invariant AR-NN\ncaptures the correct electric field oscillation and has a\nlower per step infidelity compared with the non-gauge\nansatz (see Fig 5(b) and (c), and, additionally, the antic-\nipated string inversion of the electric flux for small mass\n(and respectively the static electric flux for large mass)\n(see Fig 6).\nWhile the current work focuses on the S = 1/2 rep-\nresentation, our construction can be generalized to an\narbitrary S representation. For a higher spin S, compos-\nite particles can be defined similarly (see Fig. 2) except\nthat the degree of freedom for each ei,i+1 increases to\n2S + 1 as S increases.\nB. 2D ZN Gauge Theory\nFor the 2D toric code [26], consider an L × L pe-\nriodic square lattice, where each edge has the basis\n{|0⟩, |1⟩}. Let V, P, Edenote the sets of vertices, pla-\nquettes and edges of the lattice, respectively, such that\n|V | = L2, |P| = L2, |E| = 2 L2. Here we consider the\ntoric code with a transverse magnetic field\nH = HTC − h\nX\ne∈E\nσz\ne, (10)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Ei, i + 1 /NL\n(a)\n40-cell TN\n80-cell TN\n120-cell TN\n40-cell Transformer\n80-cell Transformer\n120-cell Transformer\n40-cell RNN\n80-cell RNN\n120-cell RNN\n160-cell RNN\n0.5 0.6 0.7 0.8 0.9 1.0\nm\n1.40\n1.35\n1.30\n1.25\n1.20\nH /Nc\n(b)\n0.6 0.8 1.0\n10 4\n10 3\nVar[H]/Nc\nFIG. 4. Variational ground state optimization for the open-\nboundary QLM of different system sizes and different m’s\nwith gauge invariant construction. (a) The expectation value\nof the electric fields averaged over all links, (b) energy and\n(inset) energy variance per unit cell. We compare our results\nwith the tensor network (TN) results (dashed lines in (a))\nfrom Ref. 59. The Transformer neural network has 1 layer\nand 32 hidden dimensions, whereas the RNN has 2 layers and\n40 hidden dimensions. For both neural networks, we use the\namplitude-phase parameterization (see Fig. 24). The neural\nnetworks are randomly initialized. Then they are trained for\n3000 iterations with 12000 samples on 40 unit cells. Then, we\nuse transfer learning technique and train the Transformer for\n1000 iterations on 80 unit cells and 600 iterations on 120 unit\ncells. The RNN is then trained for 1000 iterations on 80 and\n120 unit cells and 600 iterations on 160 unit cells. The neural\nnetwork architecture and optimization details are discussed\nin Appendix E.\nwhere HTC is the toric code Hamiltonian\nHTC = −\nX\nv∈V\nAv −\nX\np∈P\nBp, (11)\nAv ≡ Q\ne∋v σz\ne (the star operator) , Bp ≡ Q\ne∋p σx\ne , and\nh is the strength of the transverse field. Note that Av\nis the gauge constraint such that the ground state |ψ⟩ of\nEq. A1 and Eq. 11 satisfies Av |ψ⟩ = |ψ⟩ for each v.\nThe composite particle construction is illustrated in\nFig. 7. We order our consecutive particles by an “S”\nshape going up one row and down the next (see Fig. 29(b)\nin Appendix E). Two constraints must be checked in the\ngauge checking process of a gauge block.\nWhen working on the gauge block associated with com-\nposite particle exv, we check that Av |exv⟩ = |exv⟩ (despite\n6\nm = 0.1 6-cell exact\nm = 0.1 6-cell Gauss\nm = 0.1 12-cell no Gauss\nm = 0.1 12-cell Gauss\nm = 2.0 6-cell exact\nm = 2.0 6-cell Gauss\nm = 2.0 12-cell no Gauss\nm = 2.0 12-cell Gauss\n0.2\n0.0\n0.2\n0.4\nH H t = 0\n(a)\n1.0\n0.5\n0.0\n0.5\n1.0\nEi, i + 1 /NL\n(b)\n0 1 2 3 4 5\nt\n10 7\n10 5\n10 3\n1 | | |2\n(c)\nFIG. 5. Dynamics for the 6- and 12-unit-cell (12-24 sites and\n12-24 links) open-boundary QLM for m = 0.1 and m = 2.0\nwith and without gauge invariant construction. The dashed\ncurves are the exact results from the exact diagonalization for\n6 unit cells. (a) The change in the energy during the dynam-\nics. (b) The expectation value of the electric field averaged\nover all links. (c) The per step infidelity measure, where |Ψ⟩\nand |Φ⟩ are defined in Sec. III. We use the Transformer neu-\nral network with 1 layer, 16 hidden dimensions for 6 unit\ncells and 32 hidden dimensions for 12 unit cells, and the real-\nimaginary parameterization (see Fig. 24). The initial state is\n|• → ◦ →⟩for each unit cell and we train the neural network\nusing the forward-backward trapezoid method with the time\nstep τ = 0.005, 600 iterations in each time step, and 12000\nsamples in each iteration. The neural network architecture,\ninitialization and optimization details are discussed in Ap-\npendix E.\nAv acting on an entire state, this can be checked locally\non a single composite particle). In addition, composite\nparticles overlap with their four immediately adjacent\ncomposite particles. The gauge block for the composite\nparticle at v therefore checks consistency of the physi-\ncal sites with the first v − 1 composite particles. For\nFIG. 6. Dynamics of the gauge invariant AR-NN for the 12-\nunit-cell QLM with (a) m = 0.1 and (b) m = 2.0. The ansatz,\ninitialization and optimization are the same as in Fig. 5 and\nare discussed in Appendix E.\n(a) (b) \nFIG. 7. Composite particles for the 2D toric code. (a) Physi-\ncal structure of 2D toric code with red circles specifying com-\nposite particles. Note multiple composite particles share the\nsame physical sites. (b) Composite particles. We define each\nstar as a composite particle (red circle) and check bond consis-\ntency for physical sites shared by adjacent composite particles\n(blue dashed ovals).\nexample, given the configuration\n\f\f\f\f10·\n0\n1\n\u001d\nfor a composite\nparticle, the composite particle to the right can only be\f\f\f\f10·\n0\n1\n\u001d\n,\n\f\f\f\f11·\n0\n0\n\u001d\n,\n\f\f\f\f10·\n1\n0\n\u001d\nor\n\f\f\f\f11·\n1\n1\n\u001d\n, as the |1⟩ on the right of the\nleft particle must also be on the left of the right particle.\nIn the “S” ordering, there always exists valid choices for\neach composite particle. For a composite particle that is\nnot the last one, there is an unchosen site which provides\nfreedom of choices to be valid. For the last composite\nparticle, though all sites are fixed, the fixed configuration\nmust be valid because the product of all the Gauss’s law\nconstraints is 1 and all previous Gauss’s law constraints\nhave been satisfied to be 1.\nWe begin by showing that we can analytically gen-\nerate an AR-NN for the ground state of HTC . One\nground state of Eq. 11 is |ψ⟩ = Q\nv∈V (1 + Av) |+⟩⊗n\nwhere |+⟩ = (|0⟩ + |1⟩)/\n√\n2—i.e. an equal superposition\nof all configurations in the gauge basis which do not vio-\n7\n0 4 8 12 16 20 24 28 32 36 40\nNbreak\n260\n250\n240\n230\n220\n210\n200\n190\n180\n170\n160\nH\nexact\n 11 × 11  2D toric code\n4 × 4 × 4 3D toric code\n4 × 4 × 4 X-cube fracton\nFIG. 8. Energies of the analytical constructions of the ground\nand excited states of the 11 × 11 2D and 4 × 4 × 4 3D toric\ncode, and the 4 × 4 × 4 X-cube fracton model. Here Nbreak\nis the number of Gauss’ law violations. The dashed lines are\nthe exact values for each model. The analytical construction\ngenerates the same values as the exact up to stochastic errors\nfrom sampling.\nlate the gauge constraint. In our construction, if we use\nan autoregressive neural network block which gives equal\nweight to all the configurations (this is straightforward to\narrange by setting the last linear layer’s weight matrices\nto zero and bias vectors to equal amplitudes), we exactly\nachieve this state. Checking the Av does not affect the\nrelative probabilities because it is not conditional involv-\ning only one composite particle. On the other hand, the\n‘gauge constraints’ which verify consistency of the under-\nlying state of the sites leave equal probability between all\nconsistent states. To see this, we examine the effect of the\ngauge constraint on |a(exk|ex<k)|2 for any given k, which\nis the conditional probability of the composite particle\nexk. Due to the conditioning from previous composite\nparticles {ex<k}, some sites of the composite particle exk\nare fixed. For the Gauss’s law gauge constraints to be\n1, the product of all the unchosen site configurations in\nexk must be either 1 or −1, depending on the chosen site\nconfigurations. Let S1 = {b1, . . . , bj|Qj\nr=1 br = 1} and\nS−1 = {c1, . . . , cj|Qj\nr=1 cr = −1} be the two possible\nsets of unchosen site configurations, where br and cr are\nthe configurations of the unchosen sites in exk. Consider\na function f : S1 → S−1 such that f(b1) = −c1 and\nf(br) = cr otherwise. Notice that f is bijective and thus\nS1 and S−1 have the same cardinality, implying that after\nnormalization |a(exk|ex<k)|2 will have the same amplitude\nfor any {ex≤k}. We can also generate excited states by\nchanging the Av for a fixed (even) number of vertices to\nconstrain this local eigenvalue to be −1 instead of 1. We\nprovide a numerical verification of this by computing the\nenergy for an exactly represented tower of ground and\nexcited states in Fig. 8.\nWith a nonzero value of the external fieldh, the ground\nstate of Eq. A1 is no longer exactly representable, and\nwe variationally optimize our AR-NN to compute the\nground state energy. Fig. 9 shows the minimum energy\n210\n209\n208\n207\n206\n205\n204\nH\n(a)\n0.26 0.29 0.32 0.35 0.38\nh\n100\n6 × 10 1\nVar[H]\n0.26 0.28 0.30 0.32 0.34 0.36 0.38\nh\n100\n90\n80\n70\n60\n50\n40\n30\nd H /dh\n(b)\nFIG. 9. (a) Energy, (inset) energy variance and (b) energy\nderivative (computed by the Hellman-Feynman theorem[60]\nas d ⟨H⟩/dh = ⟨dH/dh⟩ = −P\ne∈E ⟨σz\ne⟩) versus h. We use\nthe 2D RNN with 3 layers, 32 hidden dimensions and the\namplitude-phase parameterization (see Fig. 24). We use the\ntransfer learning technique where we first train the neural\nnetwork on a 6 × 6 model for 8000 iterations and then we\ntransfer the neural network to the 10 × 10 model for another\n1000 iterations. In each iteration, we use 12000 samples. The\nneural network architecture, initialization and optimization\ndetails are discussed in Appendix E.\nfor Eq. A1 for different h and the energy derivative, com-\nputed using the Hellman-Feynman theorem [60]. The\ntoric code is expected to exhibit a quantum phase tran-\nsition between the topological and trivial phases at an\nintermediate value of h, and the sharp change of the\nenergy derivative around h = 0 .34 is an indicator of\nthis phase transition, which is consistent with the quan-\ntum Monte Carlo prediction of h = 0 .328474 in the\nthermodynamic limit [62]. We can additionally identify\nthe transition by considering the Wilson loop operator\nWC = Q\ne∈C σx\ne for a closed loop C. It is predicted\nthat the topological order phase follows an area law de-\ncay, ⟨WC⟩ ∼exp(−αAC), and the trivial phase follows a\nperimeter law decay, ⟨WC⟩ ∼exp(−βPC), where AC, PC\nare the enclosed area and perimeter of the loop C [63].\nFig. 10 shows the values of ⟨Wc⟩ using our variationally\noptimized AR-NN. By comparing the respective fits to\nthe area and perimeter laws we again see the transition\nat h = 0.34. Finally, we compare the non-local string cor-\nrelation operators Sγ = Q\ne∈γ σz\ne of our variational states\nwhich could be viewed as a measure of the correlation of\na pair of excited particle and anti-particle along a path\n8\n4 6 8 10 12 14 16 18 20\nPC\n0.1\n0.2\n0.4\n0.8WC\n(a)\nR2 = 0.993\n(a)\nR2 = 0.965\nh = 0.34 fitting\nh = 0.35 fitting\nh = 0.34 data\nh = 0.35 data\n0 5 10 15 20 25\nAC\n0.1\n0.2\n0.4\n0.8WC\n(b)\nR2 = 0.927\n(b)\nR2 = 0.975\nh = 0.34 fitting\nh = 0.35 fitting\nh = 0.34 data\nh = 0.35 data\n0.26 0.28 0.30 0.32 0.34 0.36 0.38\nh\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00R2\n(c)(c)\nPC fitting\nAC fitting\nFIG. 10. Perimeter and area laws for the 10 × 10 2D toric\ncode. The expectation value of the Wilson loop operator with\nrespect to the (a) perimeter and (b) area of the loop in a log-y\nscale for h = 0.34 and 0.35. (c) The fitting of the correlation\ncoefficient R2 for the perimeter and area laws for different h.\nThe ansatz, initialization, and optimization are the same as\nin Fig. 9 and discussed in Appendix E.\nγ. In the topological order phase the non-local string op-\nerators will decay to zero while they will remain constant\nat the trivial phase [64]. In Fig. 11, this is seen clearly\non both sides of the transition.\nAt h = 0 .36, we additionally benchmark our results\nwith the infinite system size iPEPS results [61] and the\ngauge equivariant results [35] (Fig. 12). Here we use an\nimproved ansatz with 180◦ rotation symmetry defined by\nlog ψnew(x) = log\n\u0010\n|ψ(x)|2 + |ψ(R(x))|2\n\u0011\n/2, where R(x)\nrotates configuration x by 180 degrees. We find that our\nresults (at least up to L = 12) are lower in energy density\nthan the gauge equivariant results and the iPEPS results,\nwhich indicates that our approach is very competitive\nwith the state-of-the-art methods.\n0 1 2 3 4 5 6 7\nL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS\n(a)(a) h = 0.34 fitting\nh = 0.35 fitting\nh = 0.34 data\nh = 0.35 data\n0.26 0.28 0.30 0.32 0.34 0.36 0.38\nh\n0.0\n0.1\n0.2\n0.3\n0.4\nS (L = 7.071)\n(b)(b)\nFIG. 11. (a) Non-local string correlation function for the\n10 × 10 2D toric code between a pair of particle and anti-\nparticle with a distance of Ly apart. (b) The correlation of a\npair of particle and anti-particle at a distance of Ly = 5\n√\n2\nfor different h. The ansatz, initialization, and optimization\nare the same as in Fig. 9 and discussed in Appendix E.\nOur approach can be naturally generalized to 2D ZN\ngauge theory, which can be described in the language\nof Kitaev’s D(G) model with group G = ZN (see Ap-\npendix B). In this case, the basis at each edge becomes\na group element in ZN . Similarly to Fig. 7, one can de-\nfine a composite particle over four edges from a vertex\nand impose gauge invariance. We can also extend our\napproach to the (1+1)D ZN lattice quantum electrody-\nnamics (QED) model, which is discussed in Appendix C.\nC. 3D Toric Code and Fracton Model\nWe turn to gauge invariant AR-NN for the ground and\nexcited states of the 3D toric code [46] and the fracton\nmodel [47, 65]. The 3D toric code generalizes the 2D\ntoric code to an L×L×L periodic cube where each edge\nhas the basis {|0⟩, |1⟩}. The Hamiltonian takes the same\nform as the 2D model (see Eq. 11) except that for each\nAv ≡ Q\ne∋v σz\ne there are six edges e associated with each\nvertex v. A ground state of the 3D toric cube similarly\nsatisfies Av |ψ⟩ = Bp |ψ⟩ = |ψ⟩ for each v, p. One of\nthe degenerate ground states can also be expressed as\n|ψ⟩ = Q\nv∈V (1 + Av) |+⟩⊗n. The excited states can be\ngenerated by breaking certain constraints from Av and\nBp as in the 2D case.\n9\n6 7 8 9 10 11 12\nL\n1.0427\n1.0426\n1.0425\n1.0424\n1.0423\n1.0422\n1.0421\n1.0420\n1.0419\nH /(2L2)\niPEPS D = 2 = 40\niPEPS D = 3 = 60\ngauge equivariant\ngauge invariant\nFIG. 12. (a) Energy per site and (b) variance of energy per\nsite for L × L toric code model with h = 0.36. We compare\nour results (blue squares) with the iPEPS results of infinite\nsystem size (dashed lines) from Ref. 61 and the gauge equiv-\nariant neural network results from Ref. 35. Notice that due\nto the difference in the definition of h, the h here is twice\nas large as in Ref. 61. We use the 2D RNN with 3 layers, 32\nhidden dimensions and the amplitude-phase parameterization\n(see Fig. 24). The neural network is randomly initialized and\ntrained for 8000 iterations with 12000 samples on 6 × 6 sys-\ntem. Then, we used the transfer learning technique to train\nthe neural network on 8 × 8 and 10 × 10 systems for another\n8000 iterations and on 12 × 12 system for 4000 iterations.\nThe neural network architecture, initialization and optimiza-\ntion details are discussed in Appendix E.\n  \nFIG. 13. Ai\nv and Bc for the X-cube fracton model.\nThe X-cube fracton model [47] is also defined on an\nL × L × L periodic cube where each edge has the basis\n{|0⟩, |1⟩}. The Hamiltonian takes the following form\nHfracton = −\nX\nv∈V,i\nAi\nv −\nX\nc∈C\nBc, (12)\nwhere Bc ≡ Q\ne∈c σz\ne over the edges in a small cube.\nThe gauge constraint, i.e. Gauss’s law, is Bc|ψ⟩ = |ψ⟩.\nThere are three Ai\nv ≡ Q\nei∋v σx\nei for three choices of\ni = zy, xy, xz, depending on which 2D plane Ai\nv acts on.\nThe operators are illustrated in Fig. 13. A ground state of\nthe X-cube fracton model satisfies Ai\nv |ψ⟩ = Bc |ψ⟩ = |ψ⟩\nfor each i, v, c. One of the ground states can be expressed\nas |ψ⟩ = Q\nc(1 +Bc)|+⟩⊗n. The excited states break some\nconstraints such that Ai\nv |ψ⟩ = −|ψ⟩ or Bc |ψ⟩ = −|ψ⟩\nfor certain Ai\nv, Bc.\n3D toric code\nX-cube fracton\n(a) (b) \n(c) \nFIG. 14. Composite particles of the 3D toric code and X-cube\nfracton model. The colors are to help identify how composite\nparticles are defined (i.e. which parts of (a) map to (b) and\n(c)). (a) Physical structure of the 3D toric code and X-cube\nfracton model. (b) Composite particles of the 3D toric code.\nWe define each star as a composite particle and check bond\nconsistency for adjacent particles similarly to the 2D toric\ncode. (c) Composite particles of the X-cube fracton model.\nWe define each cube as a composite particle and check bond\nconsistency on faces of adjacent particles.\nThe composite particles for the 3D toric code and the\nX-cube fracton model are illustrated in Fig. 14. For the\n3D toric code, a composite particle is made up of six\nparticles associated with a vertex. The ground state can\nbe constructed by initializing the bias of the autoregres-\nsive neural network to be all the |+⟩ state and imposing\ngauge checking on each composite particle to be 1. The\nexcited states can be constructed by forcing even num-\nbers of composite particles to have gauge checking value\n−1. For the X-cube fracton model, a composite parti-\ncle consists of twelve particles on each mini cube. The\nground state comes from initializing all biases of the au-\ntoregressive neural network to be |+⟩ and requiring all\ncomposite particles to have the gauge checking value 1.\nThe excited states break the gauge checking value on sets\nof four nearby composite particles to be −1. We numer-\nically verify the exact representations of ground and ex-\ncited states of the 3D toric code and the X-cube fracton\nmodel in Fig. 8, where the energy is shown to be exactly\nthe same as the theoretical predictions.\nOur approach can be naturally generalized to the\nHaahs code fracton [65] and checkerboard fracton [47]\nmodels. Similarly to 2D ZN gauge theory (see Sec-\ntion IV B), one can consider applying gauge invariant\nAR-NN to study the 3D ZN gauge theory in the context\nof the 3D toric code or the X-cube fracton model [27]\nwith an external field.\nD. SU(2) k Anyonic Chain and SU(2) Symmetry\nNon-abelian anyons play a crucial role in universal\ntopological quantum computation. Here we consider a\nchain of Fibonacci anyons, which can be regarded as an\n10\nSU(2)k=3 deformation of the ordinary quantum spin-1/2\nchain [48]. In this model, there is one type of anyon τ\nand a trivial vacuum state1 for each site. The constraint\nfrom symmetry requires that τ and 1 satisfy the follow-\ning fusion rule: τ ⊗τ = τ ⊕1 , τ ⊗1 = 1 ⊗τ = τ. We work\ndirectly in this basis where each site is either 1 or τ, gen-\nerating an anyonic symmetric AR-NN. We then proceed\nto work out the entire phase diagram of the Fibonacci\nanyons. This can be done particularly efficiently com-\npared with standard Monte Carlo sampling [13] thanks\nto the exact sampling of the AR-NN.\nOur anyonic symmetric AR-NN is constructed so that\nit obeys the anyon fusion rule directly by checking two\nadjacent input configurations and imposing zero ampli-\ntude when both are τ. Each anyon is a composite particle\nand the gauge checking implements the constraint from\nthe anyon fusion rule.\nWe consider the Hamiltonian [30]\nH(θ) = −cos θ\nX\ni\nH(2)\ni − sin θ\nX\ni\nH(3)\ni , (13)\nwhere the two-anyon interactions can be described by the\ngolden chain Hamiltonian [29, 30]\nH(2)\ni = |1 τ1 ⟩⟨1 τ1 | + ϕ−2 |τ1 τ⟩⟨τ1 τ|\n+ ϕ−1 |τττ ⟩⟨τττ |\n+ ϕ−3/2 (|τ1 τ⟩⟨τττ | + H.c.),\n(14)\nand the three-anyon interactions can be described by the\nMajumdar-Gosh chain Hamiltonian [30]\nH(3)\ni = |1 ττ 1 ⟩⟨1 ττ 1 | +\n\u0000\n1 − ϕ−2\u0001\n|ττττ ⟩⟨ττττ |\n+\n\u0000\n1 − ϕ−1\u0001\n(|ττ 1 τ⟩⟨ττ 1 τ| + |τ1 ττ ⟩⟨τ1 ττ |)\n− ϕ−5/2 (|τ1 ττ ⟩⟨ττττ | + |ττ 1 τ⟩⟨ττττ | + H.c.)\n+ ϕ−2 (|ττ 1 τ⟩⟨τ1 ττ | + H.c.),\n(15)\nϕ =\n\u0000√\n5 + 1\n\u0001\n/2 is the golden ratio.\nThis model is predicted to exhibit five phases with re-\nspect to different θ [30]. Fig. 15 shows the optimized en-\nergies of the Hamiltonian in Eq. 13 for differentθ and the\nenergy derivative computed using the Hellman-Feynman\ntheorem [60]. The non-differentiable points of the energy\nderivative indicates the phase transition points, which\nagree with the conformal field theory prediction. In the\nspecial case of θ = 0, the model reduces to the Fibonacci\nanyons in a golden chain, which has a gapless phase [30].\nUsing our optimized AR-NN, we compute the second\nRenyi entropy S2 [4]. Since the second Renyi entropy\nS2 is related to the central charge c under the periodic\nboundary condition as S2 ∼ c\n4 log(L) with system size\nL [66], we then extract the central charge finding a value\nc = 0.703±0.005 very close to the exact result of 0.7 (see\nFig. 16).\nThis can be generalized to the SU(2) k formulation of\nanyon theory, for which there are k + 1 species of anyons\n30\n25\n20\n15\n10\n5\n0\n5\n10\nH\n(a)\n0\n 2\n10 4\n10 2\nVar[H]\n0 /4\n /2\n 3 /4\n 5 /4\n 3 /2\n 7 /4\n 2\n30\n20\n10\n0\n10\n20\n30\nd H /d\nII 3-state Potts 4-\nphase I tricritical\nIsing\n(b)\nFIG. 15. Phase diagram for 40 anyons with the periodic\nboundary condition. (a) Energy, (inset) energy variance and\n(b) energy derivative, computed using the HellmannFeynman\ntheorem [60] as d ⟨H⟩/dθ = ⟨dH/dθ ⟩ = sin θ P\ni ⟨H(2)\ni ⟩ −\ncos θ P\ni ⟨H(3)\ni ⟩, versus θ. Phase transitions occurs when the\nenergy function is not differentiable. Vertical lines in orange\nare located at the exact phase transition points [30]. We use\nthe 1D RNN with 3 layers, 36 hidden dimensions and the\nreal-imaginary parameterization. We use the transfer learn-\ning technique where we first train the neural network on a\n32-anyon model for 3000 iterations and then we transfer the\nneural network to the 40-anyon model for another 3000 iter-\nations. In each iteration, we use 12000 samples. The neural\nnetwork architecture, initialization, and optimization details\nare discussed in Appendix E.\nlabeled by j = 0, 1/2, 1, . . . , k/2 with the fusion rule of\nSU(2)k [29]. The Hamiltonian can be expressed with op-\nerators from the representation of the Temperley-Lieb\nalgebra [29]. To construct an anyonic symmetric au-\ntoregressive neural network for the general SU(2) k any-\nonic chain, one works in the angular momentum basis\n{|. . . , ji−1, ji, ji+1, . . .⟩} where ji ∈ {0, 1/2, 1, . . . , k/2}.\nSince each ji is included as the SU(2) k fusion rule out-\ncome of ji−1 and an extra 1 /2 angular momentum, one\ncan view ji as a composite particle and gauge checking\nis the fusion rule.\nThe Fibonacci anyon is a special case of the SU(2) k=3\nformulation, considering the mapping τ 7→ j = 1 and\n1 7→ j = 0 and applying 3 /2 × j = 3 /2 − j from the\nSU(2)3 fusion rule to the even-number sites [29]. Note\nthat this gives a slightly different AR-NN from what is\ndescribed above. Besides the Fibonacci anyon, one can\nconsider the Yang-Lee anyon, which follows the SU(2) 3\n11\n20 30 40 50 60\nL\n0.85\n0.90\n0.95\n1.00S2\nc = 0.703 ± 0.005\n20 30 40 50 60\nL\n10 3\n6 × 10 4\nVar[H]\nfitting\ndata\nFIG. 16. The second Renyi entropy S2 versus the system\nsize L for the optimized AR-NN for the Fibonacci anyons in\na golden chain ( θ = 0) with the periodic boundary condi-\ntion. The inset shows the variance of energy of the AR-NN.\nThe slope of the fitted line is the central charge c. The AR-\nNN is the 1D RNN with 3 layers, L hidden dimensions, and\nthe amplitude-phase parameterization. The neural network is\ntrained for 8000 iterations with a sample size of 12000. The\nneural network architecture, initialization, and optimization\ndetails are discussed in Appendix E.\nfusion rule [67].\nUsing this framework, one can also consider the Heisen-\nberg spin chain with SU(2) symmetry since it can be con-\nsidered as the SU(2)k deformation of the ordinary quan-\ntum spin-1/2 chain [48] as k → ∞. In Appendix D, we\nprovide the detailed construction of an SU(2) invariant\nautoregressive neural network for the Heisenberg model,\nwhich can be viewed as the case of SU(2)k=∞, and obtain\naccurate results for the 1D Heisenberg model.\nV. CONCLUSION\nWe have provided a general approach to construct-\ning gauge invariant or anyonic symmetric autoregressive\nneural network wave functions for various quantum lat-\ntice models. These wave functions explicitly satisfy the\ngauge or algebraic constraints, allow for perfect sampling\nof configurations, and are capable of explicitly returning\nthe amplitude of a configuration including normalization.\nTo accomplish this, we have upgraded standard AR-NN\nin such a way that the constraints can be autoregressively\nsatisfied.\nWe have given explicit constructions of AR-NN which\nexactly represent the ground and excited states of several\nmodels, including the 2D and 3D toric codes as well as the\nX-cube fracton model. For those models for which exact\nrepresentations are unknown, we variationally optimize\nour symmetry incorporated AR-NN to obtain either high-\nquality ground states or time-dependent wave functions.\nThis has been done for the U(1) quantum link model, ZN\ngauge theory, the SU(2) 3 anyonic chain, and the SU(2)\nquantum spin-1/2 chain. For these systems we are able to\nmeasure dynamical properties, produce phase diagrams,\nand compute observables accurately.\nOur approach opens up the possibility of probing a\nlarger variety of models and the physics associated with\nthem. For example, the higher spin representation S >\n1/2 in the (1+1)D QLM models would allow one to probe\nthe quantum chromodynamics related physics of confine-\nment and string breaking [68]. For the (3+1)D QLM\nmodels, there is the Coulomb phase which manifests in\npyrochlore spin liquids [69]. For ZN gauge theory, it will\nbe interesting to consider the general ZN toric code with\ntransverse field or disorder, with a goal of understanding\nits phase diagram. Recently, there have been proposals\nto understand the ZN X-cube fracton model with non-\ntrivial statistical phases [27]. For non-abelian anyons,\nthe general SU(2) k formulation exhibits rich physics for\ndifferent k and one can study the corresponding topo-\nlogical liquids and edge states [48]. Our approach can\nalso be extended to study the phase diagram for the 2D\nHeisenberg models with SU(2) symmetry.\nBesides exploring various models in condensed matter\nphysics and high energy physics, our approach can also\nbe further applied to quantum information and quantum\ncomputation. Fibonacci anyons are known to support\nuniversal topological quantum computation, which is ro-\nbust to local perturbations [70]. It will be interesting\nto see how well one can approximately simulate topolog-\nical quantum computation or different braiding opera-\ntions with anyonic symmetric autoregressive neural net-\nworks. As toric codes are an important example of quan-\ntum error correction code, our approach can be used to\napproximately study the performance of a toric code un-\nder different noise conditions. With respect to the recent\nefforts on simulating lattice gauge theories with quantum\ncomputation, our approach also provides an alternative\nmethod to compare to and benchmark quantum comput-\ners. In summary, the approach we have developed is ver-\nsatile and powerful for investigating condensed matter\nphysics, high energy physics and quantum information\nscience.\nACKNOWLEDGEMENT\nD.L. is grateful for insightful discussion in high-energy\nphysics with J. Stokes and J. Shen. D.L. acknowledges\nhelpful discussion with L. Yeo, O. Dubinkin, R. Levy, P.\nXiao, R. Sun, and G. Carleo. This work is supported\nby the National Science Foundation under Cooperative\nAgreement No. PHY2019786 (the NSF AI Institute\nfor Artificial Intelligence and Fundamental Interactions\nhttp://iaifi.org/). This work utilizes resources supported\nby the National Science Foundations Major Research In-\nstrumentation program, Grant No. 1725729, as well as\nthe University of Illinois at Urbana-Champaign [71]. The\nauthors acknowledges MIT Satori and MIT SuperCloud\n[72] for providing HPC resources that have contributed\nto the research results reported within this paper. Z.Z.\n12\nis partially supported by NSF DMS-1854791, NSF OAC-\n1934757, and Alfred P. Sloan Foundation. Vera Miky-\noung Hur is partially supported by NSF DMS-1452597\nand DMS-2009981. This work is supported in part by\nthe U.S. Department of Energy, Office of Science, Of-\nfice of High Energy Physics QuantISED program under\nan award for the Fermilab Theory Consortium ”Intersec-\ntions of QIS and Theoretical Particle Physics.\n[1] G. Carleo and M. Troyer, Solving the quan-\ntum many-body problem with artificial neu-\nral networks, Science 355, 602 (2017),\nhttps://science.sciencemag.org/content/355/6325/602.full.pdf.\n[2] X. Han and S. A. Hartnoll, Deep quantum geome-\ntry of matrices, Physical Review X 10, 10.1103/phys-\nrevx.10.011069 (2020).\n[3] K. Choo, T. Neupert, and G. Carleo, Two-dimensional\nfrustrated j1j2 model studied with neural network quan-\ntum states, Physical Review B 100, 10.1103/phys-\nrevb.100.125124 (2019).\n[4] M. Hibat-Allah, M. Ganahl, L. E. Hayward, R. G. Melko,\nand J. Carrasquilla, Recurrent neural network wave func-\ntions, Phys. Rev. Research 2, 023358 (2020).\n[5] D. Luo and B. K. Clark, Backflow transformations\nvia neural networks for quantum many-body wave\nfunctions, Physical Review Letters 122, 10.1103/phys-\nrevlett.122.226401 (2019).\n[6] J. Hermann, Z. Schtzle, and F. No, Deep neural network\nsolution of the electronic schrdinger equation (2019),\narXiv:1909.08423 [physics.comp-ph].\n[7] D. Pfau, J. S. Spencer, A. G. D. G. Matthews, and\nW. M. C. Foulkes, Ab initio solution of the many-electron\nschr¨ odinger equation with deep neural networks, Phys.\nRev. Research 2, 033429 (2020).\n[8] J. Carrasquilla, D. Luo, F. Prez, A. Milsted, B. K.\nClark, M. Volkovs, and L. Aolita, Probabilistic simula-\ntion of quantum circuits with the transformer (2019),\narXiv:1912.11052.\n[9] I. L. Gutirrez and C. B. Mendl, Real time evolution with\nneural-network quantum states (2020), arXiv:1912.08831\n[cond-mat.dis-nn].\n[10] S. Lu, X. Gao, and L.-M. Duan, Efficient representation\nof topologically ordered states with restricted boltzmann\nmachines, Phys. Rev. B 99, 155136 (2019).\n[11] X. Gao and L.-M. Duan, Efficient representation of quan-\ntum many-body states with deep neural networks, Na-\nture Communications 8, 662 (2017).\n[12] I. Glasser, N. Pancotti, M. August, I. D. Rodriguez, and\nJ. I. Cirac, Neural-network quantum states, string-bond\nstates, and chiral topological states, Physical Review X\n8, 10.1103/physrevx.8.011006 (2018).\n[13] T. Vieijra, C. Casert, J. Nys, W. De Neve, J. Haegeman,\nJ. Ryckebusch, and F. Verstraete, Restricted boltzmann\nmachines for quantum states with non-abelian or anyonic\nsymmetries, Physical Review Letters 124, 10.1103/phys-\nrevlett.124.097201 (2020).\n[14] Y. Nomura, A. S. Darmawan, Y. Yamaji, and M. Imada,\nRestricted boltzmann machine learning for solving\nstrongly correlated quantum systems, Physical Review\nB 96, 10.1103/physrevb.96.205152 (2017).\n[15] M. Schmitt and M. Heyl, Quantum many-body dynamics\nin two dimensions with artificial neural networks, Physi-\ncal Review Letters 125, 10.1103/physrevlett.125.100503\n(2020).\n[16] J. Stokes, J. R. Moreno, E. A. Pnevmatikakis, and\nG. Carleo, Phases of two-dimensional spinless lattice\nfermions with first-quantized deep neural-network quan-\ntum states, Physical Review B 102, 10.1103/phys-\nrevb.102.205122 (2020).\n[17] F. Vicentini, A. Biella, N. Regnault, and C. Ciuti,\nVariational neural-network ansatz for steady states in\nopen quantum systems, Physical Review Letters 122,\n10.1103/physrevlett.122.250503 (2019).\n[18] G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer,\nR. Melko, and G. Carleo, Neural-network quantum state\ntomography, Nature Physics 14, 447 (2018).\n[19] K. A. Nicoli, S. Nakajima, N. Strodthoff, W. Samek, K.-\nR. M¨ uller, and P. Kessel, Asymptotically unbiased es-\ntimation of physical observables with neural samplers,\nPhys. Rev. E 101, 023304 (2020).\n[20] K. A. Nicoli, C. J. Anders, L. Funcke, T. Hartung,\nK. Jansen, P. Kessel, S. Nakajima, and P. Stornati, Es-\ntimation of thermodynamic observables in lattice field\ntheories with deep generative models, Phys. Rev. Lett.\n126, 032001 (2021).\n[21] N. Yoshioka and R. Hamazaki, Constructing neural sta-\ntionary states for open quantum many-body systems,\nPhys. Rev. B 99, 214306 (2019).\n[22] M. J. Hartmann and G. Carleo, Neural-network approach\nto dissipative quantum many-body dynamics, Phys. Rev.\nLett. 122, 250502 (2019).\n[23] A. Nagy and V. Savona, Variational quantum monte\ncarlo method with a neural-network ansatz for open\nquantum systems, Phys. Rev. Lett. 122, 250501 (2019).\n[24] J. Kogut and L. Susskind, Hamiltonian formulation of\nwilson’s lattice gauge theories, Physical Review D 11,\n395 (1975).\n[25] M. A. Levin and X.-G. Wen, String-net condensation:a\nphysical mechanism for topological phases, Physical Re-\nview B 71, 10.1103/physrevb.71.045110 (2005).\n[26] A. Kitaev, Fault-tolerant quantum computation by\nanyons, Annals of Physics 303, 230 (2003).\n[27] W. Shirley, K. Slagle, and X. Chen, Foliated fracton order\nfrom gauging subsystem symmetries, SciPost Physics 6,\n10.21468/scipostphys.6.4.041 (2019).\n[28] S. X. Cui, D. Ding, X. Han, G. Penington, D. Ranard,\nB. C. Rayhaun, and Z. Shangnan, Kitaevs quantum dou-\nble model as an error correcting code, Quantum 4, 331\n(2020).\n[29] A. Feiguin, S. Trebst, A. W. W. Ludwig, M. Troyer,\nA. Kitaev, Z. Wang, and M. H. Freedman, Inter-\nacting anyons in topological quantum liquids: The\ngolden chain, Physical Review Letters 98, 10.1103/phys-\nrevlett.98.160409 (2007).\n[30] S. Trebst, E. Ardonne, A. Feiguin, D. A. Huse, A. W. W.\nLudwig, and M. Troyer, Collective states of interact-\ning fibonacci anyons, Physical Review Letters 101,\n10.1103/physrevlett.101.050401 (2008).\n13\n[31] B. Field and T. Simula, Introduction to topological quan-\ntum computation with non-abelian anyons, Quantum\nScience and Technology 3, 045004 (2018).\n[32] M. C. Bauls, R. Blatt, J. Catani, A. Celi, J. I. Cirac,\nM. Dalmonte, L. Fallani, K. Jansen, M. Lewenstein,\nS. Montangero, and et al., Simulating lattice gauge theo-\nries within quantum technologies, The European Physical\nJournal D 74, 10.1140/epjd/e2020-100571-8 (2020).\n[33] C. Rebbi, Lattice gauge theories and Monte Carlo simu-\nlations (World Scientific, 1983).\n[34] G. Magnifico, T. Felser, P. Silvi, and S. Mon-\ntangero, Lattice quantum electrodynamics in (3+1)-\ndimensions at finite density with tensor networks (2020),\narXiv:2011.10658 [hep-lat].\n[35] D. Luo, G. Carleo, B. K. Clark, and J. Stokes, Gauge\nequivariant neural networks for quantum lattice gauge\ntheories (2020), arXiv:2012.05232 [cond-mat.str-el].\n[36] G. Kanwar, M. S. Albergo, D. Boyda, K. Cranmer, D. C.\nHackett, S. Racanire, D. J. Rezende, and P. E. Shana-\nhan, Equivariant flow-based sampling for lattice gauge\ntheory, Physical Review Letters 125, 10.1103/phys-\nrevlett.125.121601 (2020).\n[37] D. Boyda, G. Kanwar, S. Racanire, D. J. Rezende,\nM. S. Albergo, K. Cranmer, D. C. Hackett, and P. E.\nShanahan, Sampling using su(n) gauge equivariant flows\n(2020), arXiv:2008.05456 [hep-lat].\n[38] K. Cho, B. van Merri¨ enboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y. Bengio, Learn-\ning phrase representations using RNN encoder–decoder\nfor statistical machine translation, in Proceedings of the\n2014 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) (Association for Computa-\ntional Linguistics, Doha, Qatar, 2014) pp. 1724–1734.\n[39] S. Hochreiter and J. Schmidhuber, Long short-term mem-\nory, Neural Comput. 9, 17351780 (1997).\n[40] A. Van den Oord, N. Kalchbrenner, L. Espeholt,\nO. Vinyals, A. Graves, et al., Conditional image genera-\ntion with pixelcnn decoders, in Advances in neural infor-\nmation processing systems (2016) pp. 4790–4798.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez,  L. Kaiser, and I. Polosukhin, At-\ntention is all you need, Advances in neural information\nprocessing systems 30, 5998 (2017).\n[42] D. Luo, Z. Chen, J. Carrasquilla, and B. K. Clark, Au-\ntoregressive neural network for simulating open quan-\ntum systems via a probabilistic formulation (2020),\narXiv:2009.05580 [cond-mat.str-el].\n[43] O. Sharir, Y. Levine, N. Wies, G. Carleo, and A. Shashua,\nDeep autoregressive models for the efficient variational\nsimulation of many-body quantum systems, Phys. Rev.\nLett. 124, 020503 (2020).\n[44] Y. Levine, O. Sharir, N. Cohen, and A. Shashua, Quan-\ntum entanglement in deep learning architectures, Physi-\ncal Review Letters 122, 10.1103/physrevlett.122.065301\n(2019).\n[45] J. Kogut and L. Susskind, Hamiltonian formulation of\nwilson’s lattice gauge theories, Phys. Rev. D 11, 395\n(1975).\n[46] A. Hamma, P. Zanardi, and X.-G. Wen, String and mem-\nbrane condensation on three-dimensional lattices, Physi-\ncal Review B 72, 10.1103/physrevb.72.035307 (2005).\n[47] S. Vijay, J. Haah, and L. Fu, Fracton topological order,\ngeneralized lattice gauge theory, and duality, Physical\nReview B 94, 10.1103/physrevb.94.235157 (2016).\n[48] C. Gils, E. Ardonne, S. Trebst, A. W. W. Ludwig,\nM. Troyer, and Z. Wang, Collective states of interact-\ning anyons, edge states, and the nucleation of topologi-\ncal liquids, Physical Review Letters 103, 10.1103/phys-\nrevlett.103.070401 (2009).\n[49] E. Greensmith, P. L. Bartlett, and J. Baxter, Variance\nreduction techniques for gradient estimates in reinforce-\nment learning, J. Mach. Learn. Res. 5, 14711530 (2004).\n[50] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\net al. , Pytorch: An imperative style, high-performance\ndeep learning library, in Advances in neural information\nprocessing systems (2019) pp. 8026–8037.\n[51] C. Roth, Iterative retraining of quantum spin models us-\ning recurrent neural networks (2020), arXiv:2003.06228\n[physics.comp-ph].\n[52] K. Sprague, J. F. Carrasquilla, S. Whitelam, and I. Tam-\nblyn, Watch and learn a generalized approach for trans-\nferrable learning in deep neural networks via physical\nprinciples, Machine Learning: Science and Technology\n10.1088/2632-2153/abc81b (2020).\n[53] A. Iserles, Euler’s method and beyond, in A First Course\nin the Numerical Analysis of Differential Equations ,\nCambridge Texts in Applied Mathematics (Cambridge\nUniversity Press, 2008) p. 813, 2nd ed.\n[54] D. Kochkov and B. K. Clark, Variational optimization in\nthe AI era: Computational graph states and supervised\nwave-function optimization (2018), arXiv:1811.12423\n[cond-mat.str-el].\n[55] Y.-P. Huang, D. Banerjee, and M. Heyl, Dynamical quan-\ntum phase transitions in u(1) quantum link models, Phys-\nical Review Letters 122, 10.1103/physrevlett.122.250401\n(2019).\n[56] P. Karpov, R. Verdel, Y. P. Huang, M. Schmitt,\nand M. Heyl, Disorder-free localization in an inter-\nacting two-dimensional lattice gauge theory (2020),\narXiv:2003.04901 [cond-mat.str-el].\n[57] R. Verdel, M. Schmitt, Y.-P. Huang, P. Karpov, and\nM. Heyl, Variational classical networks for dynamics\nin interacting quantum matter (2020), arXiv:2007.16084\n[cond-mat.str-el].\n[58] D. Luo, J. Shen, M. Highman, B. K. Clark, B. DeMarco,\nA. X. El-Khadra, and B. Gadway, Framework for simu-\nlating gauge theories with dipolar spin systems, Physical\nReview A 102, 10.1103/physreva.102.032617 (2020).\n[59] E. Rico, T. Pichler, M. Dalmonte, P. Zoller, and S. Mon-\ntangero, Tensor networks for lattice gauge theories and\natomic quantum simulation, Physical Review Letters\n112, 10.1103/physrevlett.112.201601 (2014).\n[60] R. P. Feynman, Forces in molecules, Phys. Rev. 56, 340\n(1939).\n[61] S. P. G. Crone and P. Corboz, Detecting a z2 topo-\nlogically ordered phase from unbiased infinite projected\nentangled-pair state simulations, Physical Review B101,\n10.1103/physrevb.101.115143 (2020).\n[62] F. Wu, Y. Deng, and N. Prokof’ev, Phase diagram of the\ntoric code model in a parallel magnetic field, Phys. Rev.\nB 85, 195104 (2012).\n[63] K. Gregor, D. A. Huse, R. Moessner, and S. L. Sondhi,\nDiagnosing deconfinement and topological order, New\nJournal of Physics 13, 025009 (2011).\n[64] M. H. Zarei, Ising order parameter and topological phase\ntransitions: Toric code in a uniform magnetic field, Phys-\nical Review B 100, 10.1103/physrevb.100.125159 (2019).\n14\n[65] J. Haah, Local stabilizer codes in three dimensions with-\nout string logical operators, Physical Review A 83,\n10.1103/physreva.83.042330 (2011).\n[66] A. Bazavov, Y. Meurice, S.-W. Tsai, J. Unmuth-Yockey,\nL.-P. Yang, and J. Zhang, Estimating the central charge\nfrom the rnyi entanglement entropy, Physical Review D\n96, 10.1103/physrevd.96.034514 (2017).\n[67] E. Ardonne, J. Gukelberger, A. W. W. Ludwig, S. Trebst,\nand M. Troyer, Microscopic models of interacting yanglee\nanyons, New Journal of Physics 13, 045006 (2011).\n[68] D. Banerjee, M. Dalmonte, M. M¨ uller, E. Rico, P. Ste-\nbler, U.-J. Wiese, and P. Zoller, Atomic quantum simula-\ntion of dynamical gauge fields coupled to fermionic mat-\nter: From string breaking to evolution after a quench,\nPhys. Rev. Lett. 109, 175302 (2012).\n[69] U. Wiese, Ultracold quantum gases and lattice systems:\nquantum simulation of lattice gauge theories, Annalen\nder Physik 525, 777796 (2013).\n[70] C. Nayak, S. H. Simon, A. Stern, M. Freedman, and\nS. Das Sarma, Non-abelian anyons and topological\nquantum computation, Reviews of Modern Physics 80,\n10831159 (2008).\n[71] V. Kindratenko, D. Mu, Y. Zhan, J. Maloney, S. H.\nHashemi, B. Rabe, K. Xu, R. Campbell, J. Peng, and\nW. Gropp, Hal: Computer system for scalable deep learn-\ning, in Practice and Experience in Advanced Research\nComputing, PEARC ’20 (Association for Computing Ma-\nchinery, New York, NY, USA, 2020) p. 4148.\n[72] A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Ar-\ncand, D. Bestor, B. Bergeron, V. Gadepally, M. Houle,\nM. Hubbell, M. Jones, A. Klein, L. Milechin, J. Mullen,\nA. Prout, A. Rosa, C. Yee, and P. Michaleas, Interactive\nsupercomputing on 40,000 cores for machine learning and\ndata analysis, in 2018 IEEE High Performance extreme\nComputing Conference (HPEC) (IEEE, 2018) pp. 1–6.\n[73] L. Otis and E. Neuscamman, Complementary first and\nsecond derivative methods for ansatz optimization in\nvariational monte carlo, Physical Chemistry Chemical\nPhysics 21, 1449114510 (2019).\n[74] S. Notarnicola, E. Ercolessi, P. Facchi, G. Marmo, S. Pas-\ncazio, and F. V. Pepe, Discrete abelian gauge theories for\nquantum simulations of qed, Journal of Physics A: Math-\nematical and Theoretical 48, 30FT01 (2015).\n[75] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learn-\ning for image recognition, inProceedings of the IEEE con-\nference on computer vision and pattern recognition(2016)\npp. 770–778.\n[76] D. Kingma and J. Ba, Adam: A method for stochastic\noptimization, International Conference on Learning Rep-\nresentations (2014).\n15\nAppendix A: Additional Results for Quantum Link\nModel and Toric Code Model\nm = 0.1 6-cell exact\nm = 0.1 6-cell no Gauss\nm = 0.1 6-cell no Gauss SG\nm = 0.1 6-cell Gauss SG\nm = 2.0 6-cell exact\nm = 2.0 6-cell no Gauss\nm = 2.0 6-cell no Gauss SG\nm = 2.0 6-cell Gauss SG\n0.15\n0.10\n0.05\n0.00\n0.05\nH H t = 0\n(a)\n1.0\n0.5\n0.0\n0.5\n1.0\nEi, i + 1 /NL\n(b)\n0 1 2 3 4 5\nt\n10 7\n10 5\n10 3\n1 | | |2\n(c)\nFIG. 17. Dynamics for the 6-unit-cell (12 sites and 12 links)\nopen-boundary QLM for m = 0.1 and m = 2.0 with and with-\nout gauge invariant construction. The dashed curves are the\nexact results from the exact diagonalization for 6 unit cells.\nThe “6-cell Gauss SG” is the same as the “6-cell Gauss” in\nFig. 5. (a) The change in the energy during the dynamics.\n(b) The expectation value of the electric field averaged over\nall links. (c) The per step infidelity measure, where |Ψ⟩ and\n|Φ⟩ are defined in Sec. III. We use the Transformer neural\nnetwork with 1 layer, 16 hidden dimensions and the real-\nimaginary parameterization (see Fig. 24). The initial state is\n|• → ◦ →⟩for each unit cell and we train the neural network\nusing the forward-backward trapezoid method with the time\nstep τ = 0.005, 600 iterations in each time step, and 12000\nsamples in each iteration. For the results labeled with SG, we\nused the sign gradient (SG) optimizer [73] for 15-30 iterations\n(depending on the resulting fidelity) before switching to the\nregular optimizer. The neural network architecture, initial-\nization and optimization details are discussed in Appendix E.\nIn this section, we present additional results. Fig. 17\n0.0 0.2 0.4 0.6 0.8 1.0\nVar[H]\n199.8\n199.6\n199.4\n199.2\n199.0\nH\nFIG. 18. Ground state energy extrapolation at m = 0 .7\nfor 160 unit cells. The energies and variances are obtained\nfrom the training output of RNN in Fig. 4. We used a\nlinear fit for variances smaller than 0.2 and obtained a y-\nintercept of −199.7923. Our variational ground state energy\nis −199.7803 ± 0.0005. The ansatz, initialization and opti-\nmization are the same as in Fig. 4 and are discussed in Ap-\npendix E.\nis a 6-unit-cell quantum link model dynamics, which we\ncan exactly diagonalize. We observed that the gauge in-\nvariant AR-NN matches the exact results up to t = 4,\nwhile the non-gauge ansatz quickly fails to capture the\nelectric fields even with the sign gradient (SG) optimizer\n[73] (Fig. 17 (b)). In addition, the gauge invariant AR-\nNN in general has a lower per step infidelity (Fig. 17(c)).\nIn Fig. 19, we measure the local observables of the\n12 ×12 toric code model. We show that even though the\nneural network does not automatically preserve transla-\ntional symmetry, the optimization drives the neural net-\nwork to a translationally symmetric state. In addition,\nthe weights of RNN is translationally invariant, which,\nalthough not guaranteed, could be potentially useful for\npreserving translational symmetry.\nFurthermore, we benchmark our method on the follow-\ning model:\nH = −\nX\nv∈V\nAv −\nX\np∈P\nBp −h\nX\ne∈E\nσz\ne −jy\nX\np∈P\nY\ne∈p\nσy\ne , (A1)\nWith the additional P\np∈P\nQ\ne∈p σy\ne , the Hamiltonian\nexhibits a sign problem compared to the original toric\nmodel, which would be challenging for the Monte Carlo\nmethod. We further test our method first on small sys-\ntems (3 × 3 to compare against exact diagonalization)\nand find relative energy differences on the order of 10 −5\nsuggesting good agreement (see Fig. 20). We further ap-\nply our method on a 10 × 10 lattice. Though it’s not\nclear what to benchmark exactly against here, we mea-\nsure the energy difference from a variance extrapolated\nresult, and find our variational answer is close to it within\n∆E ∼ 0.1 (see Fig. 21).\nAppendix B: Kitaev’sD(G) Model and\n16\n0 50 100 150 200 250 300\nsite\n0.010\n0.005\n0.000\n0.005\n0.010\nx\n(a)\n0 50 100 150 200 250 300\nsite\n0.420\n0.425\n0.430\n0.435\n0.440z\n(b)\n0 20 40 60 80 100 120 140\nvertex\n0.990\n0.995\n1.000\n1.005\n1.010Av\n(c)\n0 20 40 60 80 100 120 140\nplaquette\n0.765\n0.770\n0.775\n0.780\n0.785\nBp\n(d)\nFIG. 19. Local observables: (a) Expectation value of ⟨σx⟩,\n(b) ⟨σz⟩, (c) vertex operator ⟨Av⟩ and (d) plaquette operator\n⟨Bp⟩ (defined in Eq. 11) for the 12 ×12 toric code model with\nh = 0.36. The neural network is the same as in Fig. 12. The\nneural network architecture, initialization and optimization\ndetails are discussed in Appendix E.\nExact Representation of Ground State\nWe generalize our gauge invariant autoregressive con-\nstruction for the 2D Z2 toric code. Kitaev’s D(G)\nmodel [26] is defined on an L × L periodic square lat-\ntice where each edge has a basis {|g⟩, g∈ G} for some\ngroup G. Here we focus on finite groups, in particular\n10 5\n( H Egs)/Egs\n(a)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nJy\n10 3\nVar[H]\n(b)\nFIG. 20. (a) Relative error in energy and (b) variance of\nenergy for 3 × 3 toric code model with an additional term\ndiscribed in Eq. A1. Here we choose h = 0.36 and run the\nneural network for different Jy’s. We use the 2D RNN neural\nnetwork with real-imaginary parameterization. The neural\nnetwork is trained using Adam optimizer for 10000 iteraions\nwith 12000 samples in each iteration. The neural network\narchitecture, initialization and optimization details are dis-\ncussed in Appendix E.\nG = ZN for ZN theory. Without loss of generality, we\nattach an upward arrow for each edge in the y-direction\nand a right arrow for each edge in the x-direction. We\nemploy the notation of Sec. IV B and introduce operators\nAg\nv and Bhu,hd,hl,hr\np as in Fig. 22.\nThe Hamiltonian defined on H(G)⊗E is\nH = −\nX\nv∈V\nAv −\nX\np∈P\nBp, (B1)\nwhere Av = 1\n|G|\nP\ng∈G Ag\nv is Gauss’s law and the gauge\nconstraint, and Bp = P\nhuhrhdhl=1 G\nBhu,hr,hd,hl\np .\nLet |+⟩ = 1√\n|G|\nP\ng∈G |g⟩, and |ψ⟩ = Q\np∈P Bp |+⟩⊗E\nis the ground state. This is because |ψ⟩ is a ground state\nfor each Av and Bp. It is easy to verify that Bp |ψ⟩ = |ψ⟩.\nTo see Av |ψ⟩ = |ψ⟩, notice that Av and Bp commute and\nAv |+⟩⊗E = |+⟩⊗E. Similarly to the Z2 toric code, the\nground state can be constructed using gauge invariant\nautoregressive neural networks by defining each star as\na composite particle and checking Gauss’s law and bond\nconsistency.\n17\n0 2 4 6 8 10\nVar[H]\n214.0\n213.8\n213.6\n213.4\n213.2\n213.0\nH\nFIG. 21. Ground state energy extrapolation at h = 0.36 and\nJy = 0.3 for 10 × 10 modified toric code model. The ener-\ngies and variances are obtained from the training output. We\nused a linear fit for variances smaller than 0.2 and obtained a\ny-intercept of −213.938. Our variational ground state energy\nis −213.802±0.003. We use the 2D RNN neural network with\nreal-imaginary parameterization. The neural network is train-\nined using the transfer learning technical for 5000 iterations\nafter the 3 × 3 result with 12000 samples in each iteration.\nThe neural network architecture, initialization and optimiza-\ntion details are discussed in Appendix E.\nFIG. 22. Ag\nv and Bhu,hr,hd,hl\np operators. Ag\nv =\nLg\n+,uLg\n+,rLg\n−,dLg\n−,l and Bhu,hr,hd,hl\np = Thu\n−,uThr\n+,rThd\n+,dThl\n−,l,\nwhere Lg\n+ |z⟩ = |gz⟩, Lg\n− |z⟩ =\n\f\fzg−1\u000b\n, Th\n+ |z⟩ = hδh,z |z⟩ and\nTh\n− |z⟩ = h−1δh,z |z⟩.\nAppendix C: (1+1)DZN Lattice QED Model\nOur approach in Sec. IV A can be applied to the\n(1+1)D ZN lattice quantum electrodynamics (QED)\nmodel, which is a discretization of the Schwinger model\nfor the continuous-space QED in 1+1 dimensions [74].\nThe (1+1)D ZN model takes a similar form as the\n(1+1)D QLM, which has fermions on sites and elec-\ntric fields on links between two sites. Let {|ei,i+1⟩} for\n1 ≤ ei,i+1 ≤ N denote the orthonormal basis on each\nlink (i, i+1). The (1+1)D ZN gauge theory can take the\nfollowing form [74]\nH = −\nX\ni\nh\nψ†\ni Ui,i+1ψi+1 + ψ†\ni+1U†\ni,i+1ψi\ni\n+ m\nX\ni\n(−1)iψ†\ni ψi + g2\n8\nX\ni\n(Vi,i+1 − 1 )(V †\ni,i+1 − 1 ),\n(C1)\nwhere Ui,i+1 |ei,i+1⟩ = |(ei,i+1 + 1)mod N⟩, and\nVi,i+1|ei,i+1⟩ = e−i2πm/N|ei,i+1⟩ for m = ei,i+1. The\nGauss’s law operator Gi of the model can be written as\nGi = e\ni2π\nN (ψ†\ni ψi+ 1\n2 (−1)i−1\n2 )Vi,i+1V †\ni−1,i (C2)\nsuch that Gi |ψ⟩ = |ψ⟩ for each i [74].\nSimilarly to the (1+1)D QLM, one can construct the\ngauge invariant autoregressive neural network as Fig. 2\nand perform gauge checking with Gi in Eq. C2.\nAppendix D: SU(2) Invariant Autoregressive\nNeural Network for Heisenberg Model\nThe 1D Heisenberg Model is described as\nH =\nX\ni\nσx\ni σx\ni+1 + σy\ni σy\ni+1 + σz\ni σz\ni+1. (D1)\nWe work in the angular momentum basis\n{|j1, j2, j3, . . . , jn⟩} similarly to [48], instead of the\nspin basis, to construct an SU(2) invariant autore-\ngressive wave function. Each ji is the total angular\nmomentum quantum number for spins from 1 to i and\njn ≡ J is the total angular momentum quantum number\nfor all spins. For the ground state of the Heisenberg\nmodel, the total angular momentum is zero, so jn = 0.\nWe define the first composite particle as j1 and the\ni’th composite particle as the difference ji − ji−1. Note\nthat this uniquely defines a physical state. We then\nautoregressively enforce ji<n ≥ 0 and jn = 0 as gauge\nchecking, to achieve the SU(2) invariant property.\n22 24 30 40 50 60 80 100\nL\n2 × 10 5\n3 × 10 5\n4 × 10 5\n6 × 10 5\n10 4\n( H E0)/|E0|\nFIG. 23. Relative error of variational ground state energy\nfor the 1D Heisenberg model with SU(2) symmetry for 22,\n24 and 100 spins. We use the 1D RNN with 3 layers, L\nhidden dimensions and the real-imaginary parameterization.\nWe train the neural network for 5000 iterations with 12000\nsamples in each iteration. The exact solutions for 22 spins\nand 24 spins are computed with the exact diagonalization,\nand the exact solution for 100 spins is the DMRG result in\nRef. 4.\nFig. 23 demonstrates the performance of our SU(2) in-\nvariant autoregressive neural network on the Heisenberg\nmodel with SU(2) symmetry.\n18\nAppendix E: Neural Network Architecture\n1. Complex Parameterization\nArgumentLog Modulus\nTransformer\nor 1D/2D RNN\nInput\nEmbedding\nInput \nTanh\nLinear\nTanh\nLinear\nReal Part\nImaginary Part\nLinear\nELU\nLinearLinear\nTransformer\nor 1D/2D RNN\nInput\nEmbedding\nInput \n(a) (b) Amplitude-Phase\nAutoregressive\nNeural Network\nBlock\nReal-Imaginary\nAutoregressive\nNeural Network\nBlock\nAmplitude\nOutput\nPhase\nOutput\nAmplitude\nOutput\nPhase\nOutput\nFIG. 24. Two parameterizations of complex wave functions\nfrom autoregressive neural networks. (a) The amplitude-\nphase parameterization. The raw output is used as the in-\nput of both the amplitude branch and the phase branch. (b)\nThe real-imag parameterization. The raw output is used as\nthe input of both the real branch and the imaginary branch,\nwhich later is converted to the amplitude branch and the\nphase branch.\nWave functions are complex in general but both the\nTransformer network and 1D/2D RNN are real. We use\ntwo approaches (Fig. 24)—(a) amplitude-phase and (b)\nreal-imaginary—to parametrize complex wave functions\nfrom real neural networks. In both parameterizations,\nthe input configuration ex, together with a default config-\nuration ex0, is embedded (i.e. each state of a composite\nparticle is mapped to a unique vector) before fed into the\nTransformer or 1D/2D RNN. Certain gauge blocks in an\nAR-NN take a default state ex0 as opposed to any state\nof the composite particles; the embedded vector of this\ndefault state has arbitrary parameters which are trained\nduring optimization.\n2. Transformer\nRaw Output\nFeed Forward\nMasked\nMulti-head \nAttention\nEmbedded Input\nPositional\nEncoding\nFIG. 25. A single layer Transformer network. The embedded\ninput is fed into the Transformer and the positional encoding\nis added. After a masked multi-head self-attention is applied,\na feed forward layer produces the raw output.\nThe Transformer used in this work (Fig. 25) is the\nsame as the Transformer used in Ref. 8, which can be\nviewed as the standard Transformer encoder with masked\nmulti-head attention from Ref. 41 but without an addi-\ntional add & norm layer. The Transformer consists of a\nstandard positional encoding layer, which uses sinusoidal\nfunctions to encode the positional information of the em-\nbedded input. After positional encoding, the input is fed\ninto the standard masked multi-head attention mecha-\nnism. The mask here is crucial for autoregressiveness, as\nit only allows each site to depend on the previous sites.\nThe output of the attention layer is then passed through\na standard feed forward layer. The detailed explanation\nof the Transformer can be found in Refs. 8 and 41. This\ntransformer is essentially equivalent to the standard Py-\nTorch implementation [50], but was implemented inde-\npendently because that implementation did not exist at\nthe start of our work.\n3. RNN Cells\nσ \nσ \nT anh \n1- \nFIG. 26. The GRU cell [38] on which different RNNs are\nconstructed. This is the same GRU cell as the PyTorch [50]\nimplementation.\nFor all RNNs in this work, we used the gated recurrent\nunit (GRU) cell [38] (Fig. 26) in PyTorch [50], which\ntakes one input vector xk, the hidden input hk−1, and\n19\ncomputes\nr = σ(Wxrxk + bxr + Whrhk−1 + bhr),\nz = σ(Wxzxk + bxz + Whzhk−1 + bhz),\nn = tanh(Wxnxk + bxn + r ⊙ (Whnhk−1 + bhn)),\nhk = (1 − z) ⊙ n + z ⊙ hk−1,\nyk = hk,\n(E1)\nwhere σ is the sigmoid function and ⊙ means element-\nwise product.\nGRU Cell\nAvg\nPoolingGRU Cell\n(a) (b) \nFIG. 27. (a) 1D RNN cell. A ResNet (skip connection) [75] is\nadded between the input xk and output yk. To be noted that\nxk, yk, hold and hnew has the same dimension. (b) 2D RNN\ncell with periodic boundary condition. This cell requires four\nhidden inputs holdi,i=1,2,3,4 and generates one hidden output\nhnew. Skip connections are added for both output yk and\nhidden output hnew. The average pooling reduces the hidden\noutput to have the same dimension as each hidden input. To\nbe noted that the dimension of xk and yk is four times the\ndimension of each holdi and hnew.\nWe then build 1D and periodic 2D RNN cells (Fig. 27)\nbased on the GRU cell. The 1D RNN cell computes\n(yraw, hnew) = GRUcell(xk, hold),\nyk = yraw + xk, (E2)\nwhereas the periodic 2D RNN cell computes\n(yraw, hraw) = GRUcell(xk, [hold1, hold2, hold3, hold4]),\n[hnew1, hnew2, hnew3, hnew4] = hraw + xk,\nhnew = 1\n4 (hnew1 + hnew2 + hnew3 + hnew4) ,\nyk = yraw + xk.\n(E3)\n4. RNNs\nWith the RNN cells, we can build 1D and periodic 2D\nRNNs.\nThe 1D RNN (Fig. 28) has a multi-layer design and\nshares the same structure as the Pytorch [50] GRU [38].\nThe embedded input configuration is fed into the cells\none at a time through multiple layers and produces a\nraw output. In our work, the cells at different layers\nshare the weight matrices and bias vectors.\nThe periodic 2D RNN has a more complicated design\nto capture the most correlations and can be viewed as\n1D RNN\nCell\nRaw Output\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\n1D RNN\nCell\nEmbedded Input\nFIG. 28. 1D RNN built from 1D RNN Cells. The neural\nnetwork has a multi-layer design similar to Pytorch GRU im-\nplementation [38, 50]. The weight matrices and biases are\nshared between different layers.\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n2D RNN\nCell\n(a) (b) \n2D RNN\nCell\nFIG. 29. (a) The hidden information path for 2D RNN with\nperiodic boundary condition for a 3 × 3 system. Blue arrows\nshow the non-boundary information path whereas the green\narrows show the periodic boundary information path. When\nthere are less than four hidden inputs, zero vectors are used\nfor padding. (b) The conditioning and sampling order of the\n2D RNN.\na periodic extension of the 2D RNN in Ref. 4. In each\nlayer, the hidden vector h is passed around according to\nFig. 29(a), where each cell receives a maximum number\nof four hidden vectors and concatenates them according\nto Fig. 27(b). When the number of hidden vectors re-\nceived is less than four, zero vectors are used to pad the\nconcatenated hidden vector to the correct length. The\nconfiguration is evaluated and sampled in a zigzagS path\n(Fig. 29(b)) to ensure autoregressiveness.\nBefore the first layer of the periodic 2D RNN, a special\nconcatenation of the embedded input needs to be per-\nformed. At each location, the concatenation layer takes\nthe four surrounding inputs (periodically) and concate-\nnates them into a single vector. If any (or all) of the\nsurrounding inputs lies later in the conditioning order in\nFig. 29(b), the corresponding input is replaced with a\ndefault input x0. For a 4 × 4 2D input array shown in\nFig. 30(a), some concatenation examples are shown in\nFig. 30(b). After the first layer, the output of a previous\nlayer can be directly fed into the next layer similar to a\nregular RNN without any further process.\nA multi-layer periodic 2D RNN consists of one input\nconcatenation layer and several 2D RNN layers as shown\n20\nCat Cat \nCat Cat \nCat \n(a) (b) \nFIG. 30. 2D RNN input concatenation layer. For (a) a 3 × 3\ninput array with a default input x0, (b) the concatenation\nlayer takes the four input vectors surrounding each site with\nperiodic boundary condition and outputs the concatenated\nvector of the four surrounding vectors. x0 is used when the\nsurrounding inputs appear later in the conditioning order.\nRaw Output\n2D RNN layer\n2D RNN layer\nInput Concatenation\nEmbedded Input\nFIG. 31. The 2D RNN is built from one input concatenation\nlayer and multiple 2D RNN layers. The weight matrices and\nbiases are shared between different layers.\nin Fig. 31.\n5. Initialization and Optimization\nWe use different initialization techniques for different\nmodels. For the QLM model, the initialization is done\nthrough tomography, minimizing −log |ψ(x)|2 for a de-\nsired configuration x (|• → ◦ →⟩for each unit cell in this\ncase) .For the 2D toric code model, we set the weight\nmatrix in the last linear layer to be 0 and the bias\nsuch that the wave function for each composite parti-\ncle is\n√\n0.23\n\f\f\f\f00·\n0\n0\n\u001d\n+\n√\n0.11\n\f\f\f\f11·\n0\n0\n\u001d\n+\n√\n0.11\n\f\f\f\f10·\n1\n0\n\u001d\n+\n√\n0.11\n\f\f\f\f00·\n1\n1\n\u001d\n+\n√\n0.11\n\f\f\f\f01·\n0\n1\n\u001d\n+\n√\n0.11\n\f\f\f\f10·\n0\n1\n\u001d\n+\n√\n0.11\n\f\f\f\f01·\n1\n0\n\u001d\n+\n√\n0.11\n\f\f\f\f11·\n1\n1\n\u001d\n, which\nempirically produces a very low initial energy. We used a\ntransfer learning technique, where we first train our neu-\nral network on a 6 × 6 model before training it on the\n10 × 10 model. When transferred to the larger system,\nthe weight and bias in the last linear layer is dropped and\nreplaced with the initialization scheme described above.\nIn Fig. 32, we show the effect of transfer learning on a\n10 × 10 toric code model. With transfer learning, the\nenergy is clearly lower than the energy without transfer\nlearning.\nFor the anyon model, we set the weight matrix in the\nlast linear layer to be 0 and the bias to be the state of\n1/\n√\n2 |1 ⟩ + 1/\n√\n2 |τ⟩ for each particle. When producing\n0 200 400 600 800 1000\niterations\n2.075\n2.070\n2.065\n2.060\n2.055\nH /(L × L)\n10 × 10 without transfer learning\n10 × 10 with transfer learning\nFIG. 32. Per site energy with and without transfer learning\nduring the training process for the 10 × 10 toric code model.\nThe first 10 iterations are not included as they are too large\nand outside the range of the figure. The energy with trans-\nfer learning is clearly lower than the energy without transfer\nlearning.\nthe phase diagram, we used a transfer learning technique,\nwhere we first train the neural network on 32 anyons\nwith θ = 0 , π/4, π/2, 3π/4, π,5π/4, 3π/2, 7π/4, and 2π\nfor 3000 iterations, and then transfer the model with θ\nthat is closest to the desired value of θ for 40 anyons for\nanother 3000 iterations. Similar to the toric code case,\nwhen transferred to the larger system, the weight and\nbias in the last linear layer is dropped and replaced by\nthe initialization described above. In all models, except\nthe last layer, the weights and biases are initialized using\nPyTorch’s [50] default initialization.\nFor optimization, we used the Adam [76] optimizer\nwith an initial learning rate of 0.01. For the QLM dy-\nnamics, the learning rate is halved at iterations 100, 200,\n270, 350, and 420, for the QLM ground state optimiza-\ntion; the learning rate is halved at iterations 300, 600,\n900, 1200, 1800, 2400, 3000, 4000, 5000, 6000, and 7000;\nand for the ground state optimization of other models,\nthe learning rate is halved at iterations 100, 500, 1000,\n1800, 2500, 4000, and 6000. In addition, for the 6-unit-\ncell cases and 12-unit-cell m = 0 .1 case with Gauss’s\nlaw, we use the sign gradient (SG) optimizer [73] for 15-\n30 iterations (depending on the resulting fidelity) before\nswitching to the regular optimizer, and for the 12-unit-\ncell m = 2.0 case with Gauss’s law, we modified the loss\nfunction by adding an energy penalty term described in\nAppendix F.\n6. Computational Complexity\nIn this section, we explain the computational complex-\nity of the neural networks used in this work.\nIn terms of scaling, the total cost per sweep is\n• Transformer: O(N2h3) (evaluation complexity),\nO(N3h3) (sampling complexity);\n21\n• RNN: O(Nh2)(computational complexity);\nwhere h is the hidden dimension and N is the size of the\nsystem. Note that the memory complexity is bounded by\nthe computational complexity. In the training process,\nwe used 2-4 GPUs depending on the availability of GPUs\nin the cluster and never experienced any memory issue\nwith a total of 64GB GPU memory. To give a sense\nfor computational difficulty, generating Fig. 12 takes six\nGPU days using Tesla V100 GPUs.\nAppendix F: Energy Penalty\nWhile quantum dynamics should exactly preserve the\nenergy, as a practical matter when a variational state\ncan’t fully represent the exact dynamics, there can be\na tension between maximizing fidelity per step and pre-\nserving the energy. In some cases, it may be desirable to\nbetter preserve the total energy at some cost in fidelity\nper step. Toward that end, we show one can introduce\nan additional term into the lost function which acts as a\npenalty toward the drift in energy. We demonstrate this\nfor the m = 2.0 12-unit-cell QLM in Fig. 5. The energy\npenalty term\nLp =\n\f\f\f\f\f\f\n1\nN\nNX\nx∼|ψθ|2\nEloc(x) − E0\n\f\f\f\f\f\f\n2\n, (F1)\nwhere Eloc(x) is defined in Eq. 1 and E0 is the initial\nenergy. This term is added to the dynamics loss function\nLd (Eq. 7) to obtain the total loss function as\nL = Ld + αLp, (F2)\nwith α is a hyperparameter which we choose to be 0.01.\nWe show in Fig. 33 this simulation with and without the\nenergy penalty. We find the dynamics as measured by the\nobservables largely stays the same (and may be better),\nbut the drift in the energy is significantly attenuated.\nAppendix G: Derivation of Stochastic Gradients\nfor Variational and Dynamics Optimization\nIn Sec. III, we presented stochastic gradients of the\nvariational and dynamics optimizations. This section in-\ncludes their derivations.\nThe variational optimization has been widely used and\nderived many times in other works [1, 4]. Here we present\nm = 2.0 6-cell exact\nm = 2.0 6-cell Gauss\nm = 2.0 12-cell Gauss\nm = 2.0 12-cell Gauss EP\n0.8\n0.6\n0.4\n0.2\n0.0\nH H t = 0\n(a)\n0.7\n0.8\n0.9\n1.0\nEi, i + 1 /NL\n(b)\n0 1 2 3 4 5\nt\n10 7\n10 6\n10 5\n10 4\n10 3\n10 2\n1 | | |2\n(c)\nFIG. 33. Dynamics for the 6- and 12-unit-cell (12-24 sites and\n12-24 links) open-boundary QLM for m = 2.0 with and with-\nout energy penalty. The dashed curves are the exact results\nfrom the exact diagonalization for 6 unit cells. The “12-cell\nGauss EP” is the same as the “12-cell Gauss” in Fig. 5. (a)\nThe change in the energy during the dynamics. (b) The ex-\npectation value of the electric field averaged over all links.\n(c) The per step infidelity measure, where |Ψ⟩ and |Φ⟩ are de-\nfined in Sec. III. We use the Transformer neural network with\n1 layer, 16 hidden dimensions for 6 unit cells and 32 hidden\ndimensions for 12 unit cells, and the real-imaginary parame-\nterization (see Fig. 24). The initial state is |• → ◦ →⟩for each\nunit cell and we train the neural network using the forward-\nbackward trapezoid method with the time step τ = 0 .005,\n600 iterations in each time step, and 12000 samples in each\niteration. The neural network architecture, initialization and\noptimization details are discussed in Appendix E.\n22\nthe derivation for the sake of completeness:\n∂ ⟨ψθ|H|ψθ⟩\n∂θ\n=\n\u001c∂ψθ\n∂θ\n\f\f\f\fH\n\f\f\f\fψθ\n\u001d\n+\n\u001c\nψθ\n\f\f\f\fH\n\f\f\f\f\n∂ψθ\n∂θ\n\u001d\n= 2\nX\nx\nℜ\n\u001a∂ψ∗\nθ(x)\n∂θ Hψθ(x)\n\u001b\n= 2\nX\nx\nℜ\n\u001a 1\nψ∗\nθ(x)\n∂ψ∗\nθ(x)\n∂θ ψ∗\nθ(x)Hψθ(x)\n\u001b\n= 2\nX\nx\nℜ\n\u001a\nψ∗\nθ(x)Hψθ(x) ∂\n∂θ log ψ∗\nθ(x)\n\u001b\n≈ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001aHψθ(x)\nψθ(x)\n∂\n∂θ log ψ∗\nθ(x)\n\u001b\n≡ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001a\nEloc(x) ∂\n∂θ log ψ∗\nθ(x)\n\u001b\n,\n(G1)\nwhere the local energy is Eloc(x) ≡ Hψθ(x)/ψθ(x). We\ncan further control the variance by subtracting from the\nEloc(x) the average energy Eavg ≡ PN\nx∼|ψθ|2 Eloc(x)/N\nover the batch of samples [49] as we did in Sec. III and\nuse the stochastic variance reduced gradient as\n2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001ah\nEloc(x) − Eavg\ni ∂\n∂θ log ψ∗\nθ(x)\n\u001b\n, (G2)\nwhich has the same expectation as Eq. G1.\nFor the dynamics optimization gradient, as in Sec. III,\nwe define |Ψθ⟩ = (1 + iHτ )\n\f\fψθ(t+2τ)\n\u000b\nand |Φ⟩ = (1 −\niHτ )\n\f\fψθ(t)\n\u000b\n, and we drop θ(t) and name θ ≡ θ(t + 2τ).\nWe start by splitting the negative log overlap:\n− log ⟨Ψθ|Φ⟩⟨Φ|Ψθ⟩\n⟨Ψθ|Ψθ⟩⟨Φ|Φ⟩\n= −log ⟨Ψθ|Φ⟩ −log ⟨Φ|Ψθ⟩ + log⟨Ψθ|Ψθ⟩ + log⟨Φ|Φ⟩.\n(G3)\nWe then compute the gradient term by term. The first\nterm on the right side of Eq. G3 becomes\n∂\n∂θ log ⟨Ψθ|Φ⟩\n= 1\n⟨Ψθ|Φ⟩\n\u001c∂Ψθ\n∂θ\n\f\f\f\fΦ\n\u001d\n= 1P\nx Ψ∗\nθ(x)Φ(x)\nX\nx\n∂Ψ∗\nθ(x)\n∂θ Φ(x)\n= 1P\nx Ψ∗\nθ(x)Φ(x)\nX\nx\nΨ∗\nθ(x)Φ(x) ∂\n∂θ log Ψ∗\nθ(x)\n≈ 1\nPN\nx∼|ψθ|2\nΨ∗\nθ(x)Φ(x)\n|ψθ(x)|2\nNX\nx∼|ψθ|2\nΨ∗\nθ(x)Φ(x)\n|ψθ(x)|2\n∂\n∂θ log Ψ∗\nθ(x)\n≡ 1\nN\nNX\nx∼|ψθ|2\nα(x)\nαavg\n∂\n∂θ log Ψ∗\nθ(x),\n(G4)\nwhere α(x) = Ψ ∗\nθ(x)Φ(x)/|ψθ(x)|2 and αavg =PN\nx∼|ψθ|2 α(x)/N as in Sec. III. The second term on the\nright side of Eq. G3 is just the complex conjugate of the\nfirst term, whereby\n∂\n∂θ log ⟨Φ|Ψθ⟩ =\n\u0014 ∂\n∂θ log ⟨Ψθ|Φ⟩\n\u0015∗\n≈ 1\nN\nNX\nx∼|ψθ|2\n\u0014α(x)\nαavg\n∂\n∂θ log Ψ∗\nθ(x)\n\u0015∗\n.\n(G5)\nThe third term on the right side of Eq. G3 becomes\n∂\n∂θ log ⟨Ψθ|Ψθ⟩\n= 1\n⟨Ψθ|Ψθ⟩\n\u0012\u001c∂Ψθ\n∂θ\n\f\f\f\fΨθ\n\u001d\n+\n\u001c\nΨθ\n\f\f\f\f\n∂Ψθ\n∂θ\n\u001d\u0013\n= 2P\nx |Ψθ(x)|2\nX\nx\nℜ\n\u001a∂Ψ∗\nθ(x)\n∂θ Ψθ(x)\n\u001b\n= 2P\nx |Ψθ(x)|2\nX\nx\nℜ\n\u001a\n|Ψθ(x)|2 ∂\n∂θ log Ψ∗\nθ(x)\n\u001b\n≈ 2\nPN\nx∼|ψθ|2\n|Ψθ(x)|2\n|ψθ(x)|2\nNX\nx∼|ψθ|2\nℜ\n(\n|Ψθ(x)|2\n|ψθ(x)|2\n∂\n∂θ log Ψ∗\nθ(x)\n)\n≡ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001aβ(x)\nβavg\n∂\n∂θ log Ψ∗\nθ(x)\n\u001b\n,\n(G6)\nwhere β(x) = |Ψθ(x)|2/|ψθ(x)|2 and βavg =PN\nx∼|ψθ|2 β(x)/N as in Sec. III. The last term on the\nright side of Eq. G3 is θ independent such that\n∂\n∂θ log ⟨Φ|Φ⟩ = 0. (G7)\n23\nCombining all the terms together,\n∂\n∂θ\n\u0012\n−log ⟨Ψθ|Φ⟩⟨Φ|Ψθ⟩\n⟨Ψθ|Ψθ⟩⟨Φ|Φ⟩\n\u0013\n≈ 2\nN\nNX\nx∼|ψθ|2\nℜ\n\u001a\u0014β(x)\nβavg\n− α(x)\nαavg\n\u0015 ∂\n∂θ log Ψ∗\nθ(x)\n\u001b\n.\n(G8)"
}