{
  "title": "Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems",
  "url": "https://openalex.org/W3167712016",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4222222672",
      "name": "Lin, Hsien-chin",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A2110037087",
      "name": "Lubis Nurul",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A4287068228",
      "name": "Hu, Songbo",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4222222669",
      "name": "van Niekerk, Carel",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A4222222671",
      "name": "Geishauser, Christian",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A2748510027",
      "name": "Heck, Michael",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A4222222670",
      "name": "Feng, Shutong",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": null,
      "name": "Ga\\v{s}i\\'c, Milica",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3030754432",
    "https://openalex.org/W2970028737",
    "https://openalex.org/W2040123554",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2971159908",
    "https://openalex.org/W2963857397",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2106547558",
    "https://openalex.org/W2035934535",
    "https://openalex.org/W2083205357",
    "https://openalex.org/W2134051188",
    "https://openalex.org/W2571927164",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2804047045",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2168490009",
    "https://openalex.org/W62710299",
    "https://openalex.org/W2062175565",
    "https://openalex.org/W2962682659",
    "https://openalex.org/W200223693",
    "https://openalex.org/W1664253374",
    "https://openalex.org/W2889424988",
    "https://openalex.org/W2952798561",
    "https://openalex.org/W2128965063",
    "https://openalex.org/W1602011256",
    "https://openalex.org/W1518203739",
    "https://openalex.org/W2970828515",
    "https://openalex.org/W2899908862",
    "https://openalex.org/W2891732163",
    "https://openalex.org/W2603612888",
    "https://openalex.org/W2054716580",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4288288848"
  ],
  "abstract": "Dialogue policy optimisation via reinforcement learning requires a large number of training interactions, which makes learning with real users time consuming and expensive. Many set-ups therefore rely on a user simulator instead of humans. These user simulators have their own problems. While hand-coded, rule-based user simulators have been shown to be sufficient in small, simple domains, for complex domains the number of rules quickly becomes intractable. State-of-the-art data-driven user simulators, on the other hand, are still domain-dependent. This means that adaptation to each new domain requires redesigning and retraining. In this work, we propose a domain-independent transformer-based user simulator (TUS). The structure of our TUS is not tied to a specific domain, enabling domain generalisation and learning of cross-domain user behaviour from data. We compare TUS with the state of the art using automatic as well as human evaluations. TUS can compete with rule-based user simulators on pre-defined domains and is able to generalise to unseen domains in a zero-shot fashion.",
  "full_text": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 445–456\nJuly 29–31, 2021. ©2021 Association for Computational Linguistics\n445\nDomain-independent User Simulation with Transformers for\nTask-oriented Dialogue Systems\nHsien-chin Lin1, Nurul Lubis1, Songbo Hu2, Carel van Niekerk1,\nChristian Geishauser1, Michael Heck1, Shutong Feng1, and Milica Gaˇsi´c1\n1Heinrich Heine University Dusseldorf, Germany\n2Department of Computer Science and Technology, University of Cambridge, UK\n1{linh,lubis,niekerk,geishaus,heckmi,shutong.feng,gasic}@hhu.de\n2sh2091@cam.ac.uk\nAbstract\nDialogue policy optimisation via reinforce-\nment learning requires a large number of\ntraining interactions, which makes learning\nwith real users time consuming and expensive.\nMany set-ups therefore rely on a user simula-\ntor instead of humans. These user simulators\nhave their own problems. While hand-coded,\nrule-based user simulators have been shown\nto be sufﬁcient in small, simple domains, for\ncomplex domains the number of rules quickly\nbecomes intractable. State-of-the-art data-\ndriven user simulators, on the other hand, are\nstill domain-dependent. This means that adap-\ntation to each new domain requires redesign-\ning and retraining. In this work, we propose\na domain-independent transformer-based user\nsimulator (TUS). The structure of our TUS is\nnot tied to a speciﬁc domain, enabling domain\ngeneralisation and learning of cross-domain\nuser behaviour from data. We compare TUS\nwith the state of the art using automatic as well\nas human evaluations. TUS can compete with\nrule-based user simulators on pre-deﬁned do-\nmains and is able to generalise to unseen do-\nmains in a zero-shot fashion.\n1 Introduction\nTask-oriented dialogue systems are designed to\nhelp users accomplish speciﬁc goals within a partic-\nular task such as hotel booking or ﬁnding a ﬂight.\nSolving this problem typically requires tracking\nand planning (Young, 2002). In tracking, the sys-\ntem keeps track of information about the user goal\nfrom the beginning of the dialogue until the cur-\nrent dialogue turn. In planning, the dialogue policy\nmakes decisions at each turn to maximise future re-\nwards at the end of the dialogue (Levin and Pierac-\ncini, 1997). The system typically needs thousands\nof interactions to train a usable policy (Schatzmann\net al., 2007; Pietquin et al., 2011; Li et al., 2016; Shi\net al., 2019). The amount of interactions required\nmakes learning from real users time-consuming\nand costly. It is therefore appealing to automati-\ncally generate a large number of dialogues with a\nuser simulator (US)1(Eckert et al., 1997).\nRule-based USs are interpretable and have\nshown success when applied in small, simple do-\nmains. However, expert knowledge is required\nto design their rules and the number of rules\nneeded for complex domains quickly becomes\nintractable (Schatzmann et al., 2007). In addi-\ntion, handcrafted rules are unable to capture hu-\nman behaviour to its fullest extent, leading to sub-\noptimal performance when interacting with real\nusers (Schatzmann et al., 2006).\nData-driven USs on the other hand can learn\nuser behaviour directly from a corpus. However,\nthey are still domain-dependent. This means that\nin order to accommodate an unseen domain one\nneeds to collect and annotate a new dataset, and\nretrain or even re-engineer the simulator.\nWe propose a transformer-based domain-\nindependent user simulator (TUS). Unlike existing\ndata-driven simulators, we design the feature repre-\nsentation to be domain-independent, allowing the\nsimulator to easily generalise to new domains with-\nout modifying or retraining the model. We utilise a\ntransformer architecture (Vaswani et al., 2017) so\nthat the input sequence can have a variable length\nand dynamic order. The dynamic order takes into\naccount the user’s priorities and the varying input\nlength enables the US to incorporate system ac-\ntions in a seamless manner. TUS predicts the value\nof each slot and the domains of the current turn,\nallowing the model to optimise its performance in\nmultiple granularities. By disentangling the user\nbehaviour from the domains, TUS can learn a more\ngeneral user policy to train the dialogue policy.\n1There are approaches that attempt to learn a dialogue\npolicy from direct interaction with humans (Gaˇsi´c et al., 2011).\nEven then, USs are essential for development and evaluation.\n446\nWe compare policies trained with our TUS to\npolicies trained with other USs through indirect and\ndirect evaluation as well as human evaluation. The\nresults show that policies trained with TUS outper-\nform those that are trained with another data-driven\nUS and are on par with policies trained with the\nagenda-based US (ABUS). Moreover, the policy\ngeneralises better when evaluated with a different\nUS. Automatic and human evaluations on our zero-\nshot study show that leave-one-domain-out TUS is\nable to generalise to unseen domains while main-\ntaining a comparable performance to ABUS and\nTUS trained on the full training data.\n2 Related Work\nThe quality of a US has a signiﬁcant impact on\nthe performance of a reinforcement-learning based\ntask-oriented dialogue system (Schatzmann et al.,\n2005). One of the early models include an N-gram\nuser simulator proposed by Eckert et al. (1997).\nIt uses a 2-gram model P(au|am) to predict the\nuser action au according to the system action am.\nSince it only has access to the latest system action,\nits behaviour can be illogical if the goal changes.\nTherefore, models which can take into account a\ngiven user goal were introduced (Georgila et al.,\n2006; Eshky et al., 2012). The Bayesian model\nof Daubigney et al. (2012) predicts the user action\nbased on the user goal, and hidden Markov models\nare used to model the user and the system behaviour\n(Cuay´ahuitl et al., 2005). The graph-based US of\nSchefﬂer and Young (2002) combines all possible\ndialogue paths in a graph. It can generate reason-\nable and consistent behaviour, but is impractical to\nimplement, since extensive domain knowledge is\nrequired.\nThe agenda-based user simulator (ABUS)\n(Schatzmann et al., 2007) models the user state\nas a stack-like agenda, ordered according to the\npriority of the user actions. The probabilities of\nupdating the agenda and choosing user actions are\nset manually or learned from data (Keizer et al.,\n2010). Still, the stacking and popping rules are\ndomain-dependent and need to be designed care-\nfully.\nTo build a data-driven model, the sequence-to-\nsequence (Seq2Seq) model structure is widely used.\nEl Asri et al. (2016) propose a Seq2Seq semantic\nlevel US with an encoder-decoder structure. Each\nturn is fed into the encoder recurrent neural network\n(RNN) and embedded as a context vector. Then\ndomain-independent \ndata-driven \ninterpretable \nTUS \nVHUS \nNUS \nABUS Graph-based \nSeq2Seq \nFigure 1: The difference between USs. We com-\npare to which extent a model is data-driven, domain-\nindependent and interpretable.\nthis context vector is passed to the decoder RNN\nto generate user actions. To add new domains, it is\nnecessary to modify the domain-dependent feature\nrepresentation and retrain the model.\nInstead of generating semantic level output, the\nneural user simulator (NUS) by Kreyssig et al.\n(2018) generates responses in natural language,\nthus requiring less labeling, at the expense of inter-\npretability. However, its feature representation is\nstill domain-dependent.\nA variational hierarchical Seq2Seq user simu-\nlator (VHUS) is proposed by G ¨ur et al. (2018).\nInstead of designing dialogue history features, the\nmodel encodes the user goal and system actions\nwith a vector using an RNN, which alleviates the\nneed of heavy feature engineering. However, the\ninputs are represented as one-hot encodings, which\nare also dependent on the ontology. In addition, the\noutput generator is not constrained by the ontology\nin any way, so it can generate impossible actions.\nAs shown in Fig. 1, ABUS and graph-based\nmodels are domain-dependent and require signif-\nicant design efforts. Data-driven models such as\nSeq2Seq, NUS, and VHUS can learn from data, but\nare constrained by the underlying domain. NUS\ngenerates natural language responses, which re-\nquires less labeling, but comes with reduced inter-\npretability.\nShi et al. (2019) compared different ways to\nbuild a US and indicated that the data-driven mod-\nels suffer from bias in the corpus. If some actions\nare rare in the corpus, the model cannot capture\nthem. Thus, the dialogue policy cannot explore all\npossible paths during training with the data-driven\nUSs. It is important to learn more general human\nbehaviour to reduce the impact of the corpus bias.\n447\n3 Problem Description\nTask-oriented dialogue systems are deﬁned by a\ngiven ontology, which speciﬁes the concepts that\nthe system can handle. The ontology can include\nmultiple domains. In each domain, there are in-\nformable slots, which are the attributes that users\ncan assign values to, and requestable slots, which\nare the attributes that users can query. For example,\nin Fig. 2 the user goal has two domains, “hotel” and\n“restaurant”. The slot Area is an informable slot\nwith the valueNorth in domain “hotel” andAddr\nis a requestable slot in domain “restaurant”. The\nsystem staterecords the slots and values mentioned\nin the dialogue history. A US for task-oriented dia-\nlogue systems needs to provide coherent responses\naccording to a given user goal G = {domain1 :\n[(slot1, value1), (slot2, value2), . . .], . . .}. The\ndomains, slots and values are selected from the\nontology.\nThe user action is composed of user in-\ntents, domains, slots, and values. We con-\nsider user intents that appear in the MultiWOZ\ndataset (Budzianowski et al., 2018). It is of course\npossible to consider arbitrary intents within the\nsame model architecture, as long as they are de-\nﬁned a priori 2. The two possible user intents we\nconsider are Inform and Request. With Inform, the\nuser can provide information, correct the system\nor conﬁrm the system’s recommendations. When a\nuser goal cannot be fulﬁlled, the user can also ran-\ndomly select a value from the ontology and change\nthe goal. With Request, the user can request infor-\nmation about certain slots.\nThe system actionis similar to the user action,\nbut there exist more (system) intents. For example,\nthe system can provide suggestions to users with\nthe intent Recommendation and make reservations\nfor users with the intent Book. More system intents\ncan be found in Appendix A.\nWe view user simulation in a task-oriented dia-\nlogue as a sequence-to-sequence problem. For each\nturn t, we extract the input feature vectorsV t of the\ninput list of slots St = [s1, s2, . . .], which is com-\nposed of the slots from the user goal and the system\naction. The output sequence Ot = [ot\n1, ot\n2, . . .] is\nthen generated by the model, where ot\ni shows how\nthe value for slot si is obtained. The input fea-\nture representation and the output target should be\n2We note that intents are not normally dependent on the do-\nmain but rather on the kind of dialogue that is being modeled,\ne.g. task-oriented or chit-chat.\nUser Goal \nInfo: Hotel-Area=North, Rest-Area=North\nReqt: Hotel-Name, Rest-Addr\nConversation\nTurn 0\nUSR: I want to find a hotel in the north and a nearby restaurant.\n     Inform(Hotel-Area=North, Rest-Area=North)\nSYS: There are some good hotels in the south. Which price range do \n     you prefer? Would you mind providing more information?\n     Recom(Hotel-Area=South), Request(Hotel-Price),\n     general-reqmore()\nTurn 1\nUSR: No, I want one in the north and I don't care about the price range.\n     Inform(Hotel-Area=North, Hotel-Price=dontcare)\nFigure 2: An example dialogue with a multi-domain\ngoal.\ndomain-independent in order to generalise to un-\nseen domains without redesigning and retraining.\nMore details can be found in Sec. 4.\nBy working on the semantic level during train-\ning, we retain interpretability. To interact with\nreal users during human evaluation, we rely on\ntemplate-based natural language generation to con-\nvert the semantic-level actions into utterances, as\nlanguage generation is out of the scope of this\nwork.\n4 Transformer-based\nDomain-independent User Simulator\nThe TUS model structure is shown in Fig. 3.\nFor each turn t, the list of input feature vectors\nV t = [vt\n1, vt\n2 . . . , vt\nnt ] is generated based on the\nsystem actions and the user goal, where vt\ni is the\nfeature vector of slot si and nt is the length of\nthe input list in turn t, V t. We explain the feature\nrepresentation in detail in Sec. 4.1. Inspired by\nABUS, which models the user state as a stack-like\nagenda, the length of input list nt at each turn t\nvaries by taking into account slots mentioned in\nthe system’s action. For example, in Fig. 3 the\ninput list V 0 only contains the slots in the user\ngoal at the ﬁrst turn. Then the system mentions a\nslot not in the user goal, Hotel-Price. So in\nturn 1 the length of input list V 1 is n1 = n0 + 1\nbecause one slot is inserted into the input list\nV 1. The whole input sequence to the model is\nVinput = [vCLS , vt\n1, . . . , vSEP , vt−1\n1 , . . . , vSEP ],\nwhere vCLS is the representation of [CLS] and\nvSEP is the representation of [SEP].\nThe user policy network is a transformer\n(Vaswani et al., 2017; Devlin et al., 2019). We\nchoose this structure because transformers are able\nto handle input sequences of arbitrary lengths and\nto capture the relationship between slots thanks\n448\nUser Policy Network \nTurn Turn \n+\nLinear\n+\nLinear\n+\nLinear\n+\nLinear\n+\nLinear\n... ...\n...\n+\nLinear\nNone?dontcareuser goalsystem staterandom select\ngeneral   domain 1domain 2domain 3domain 4domain 5\nHotel-AreaHotel-Area\n+\nLinear\nRest-Addr\n0 1  0 0 0 0...0 0 1 00 0 0 1 0 0 0 1 1  0 0 0 0 0 1  0 0 1  0 0 0 0 01  0 0 0 ... 0 0\nRest-AddrHotel-Price\n+\nLinear\nPos. Pos. Pos. Pos. Pos. Pos. Pos. Pos.\nLinearLinear LinearLinear\nPos. Pos. Pos. Pos. Pos. Pos. Pos. Pos. x2Transform Layer\nFigure 3: The TUS model structure. The input list starts with a special token, [CLS], and comprises slot lists\nfrom previous turns. The slot lists from each turn are separated by a token, [SEP]. The model predicts an output\nvector for each slot in the last turn. Note that the order of slots in each turn is independent from each other. The\noutput for [CLS] represents which domains should be selected in the current turn. The user goal and dialogue\nhistory are shown in Fig. 2 and here we give the example of the input feature vi for slot Hotel-Area.\nto self-attention. The model structure includes a\nlinear layer and position encoding for inputs, two\ntransformer layers, and one linear layer for outputs.\nThe output list Ot = [ ot\n1, . . . , ot\nnt ] consists\nof one-hot vectors ot\ni which determine the val-\nues of the slots si at turn t. The dimensions of\not\ni ∈{0, 1}6 correspond to “none”, “don’t care”,\n“?”, “from the user goal”, “from the system state”,\nor “randomly selected”. More precisely, “none”\nmeans that this slot is not mentioned in this turn,\n“don’t care” signiﬁes that the US does not care\nabout this slot, “?” means the US wants to request\ninformation about this slot, “from user goal” im-\nplies that the value is the same as in the user goal,\n“from system state” means that the value is as men-\ntioned by the system, and lastly “randomly selected”\nindicates that the US wants to change its goal by\nrandomly selecting a value from the ontology.\nThe loss function for slots measures the differ-\nence between the predicted outputOt and the target\nY t at each turn t from the dataset as computed by\ncross entropy (CE), i.e.,\nlossslots = 1\nnt\nnt∑\ni=1\nCE(ot\ni, yt\ni), (1)\nwhere nt is the number of slots in the input list, ot\ni\nis the output, and yt\ni is the target of slot si in turn t.\n4.1 Domain-independent Input Features\nWe design the input feature representation vt\ni of\neach slot si in turn t consisting of a set of sub-\nvectors, all of which are domain-independent. For\nbetter readability, we drop the slot index i and the\nturn index t, i.e. we write v for vt\ni.\n4.1.1 Basic Information Features\nInspired by the feature representation proposed in\nEl Asri et al. (2016), we use a feature vector vbasic\nthat is composed of binary sub-vectors to represent\nthe basic information for each slot. Each slot has\ntwo value vectors: vsys\nvalue represents the value in the\nsystem state, and vuser\nvalue represents the value in the\nuser goal. Each value vector is a 4-dimensional one-\nhot vector, with coordinates encoding “none”, “?”,\n“don’t care” or “other values”, in this order. For ex-\nample, in turn 1 in Fig. 2, for slot Hotel-Price\nvuser\nvalue = [1, 0, 0, 0], i.e., “none”, because it is not\nin the user goal, and vsys\nvalue = [0, 1, 0, 0], i.e., “?”,\nbecause the system requests it.\nThe slot type vector vtype is a 2-dimensional vec-\ntor which represents whether a slot is in the user\ngoal as a constraint or a request. For example,\n449\nin Fig. 2 for Hotel-Area vtype = [1, 0] (con-\nstraint), while for Hotel-Name vtype = [0, 1]\n(request). A value of [0, 0] means that the slot is\nnot included in the user goal.\nThe state vector vful encodes whether or not a\nconstraint or informable slot has been fulﬁlled. The\nvalue is set to 1 if the constraint has been fulﬁlled,\nand to 0 otherwise. The vector vfirst similarly\nencodes whether a slot is mentioned for the ﬁrst\ntime.\nThe basic information feature vector vbasic is the\nconcatenation of these vectors, i.e.,\nvbasic = vuser\nvalue ⊕vsys\nvalue ⊕vtype ⊕vful ⊕vfirst (2)\n4.1.2 System Action Features\nThe system action feature vector vsystem\naction encodes\nsystem actions in each turn. There are two kinds\nof system actions, general actions and domain-\nspeciﬁc actions. The general actions are com-\nposed only with general intents, such as “reqmore”\nand “bye”. For example,general-reqmore().\nThe feature vector of general actionsvgen is a multi-\nhot encoding of whether or not a general intent\nappears in the dialogue. With a total number of\nngen general intents, for each k ∈{1, . . . , ngen},\nthe k-th entry of vgen is set to 1 if the k-th general\nintent is part of the system act.\nOn the other hand, domain-speciﬁc actions are\ncomposed with domains, slots, values, and domain-\nspeciﬁc intents such as “recommend” and “select”.\nFor example, Recom(Hotel-Area=South).\nEach domain-speciﬁc action vector vspecj with the\ndomain-speciﬁc j-th intent, j ∈ {1, . . . , nspec},\nwhere nspec is the total number of domain-speciﬁc\nintents, is represented by a 3-dimensional one-\nhot encoding that describes whether the value is\n“none”, “?” or “other values”.\nThe ﬁnal action representation vsystem\naction is formed\nby concatenating nspec domain-speciﬁc action rep-\nresentations together with the general action repre-\nsentation, i.e.,\nvsystem\naction = vspec0 ⊕···⊕ vspecnspec ⊕vgen. (3)\nFor the slot Hotel-Area in Fig. 3, we have a\nvector for each intent. For the intent “recommend”\nvspec0 = [0, 0, 1], which means that “other val-\nues” (in this case South) are mentioned. For\nall other domain-speciﬁc intents, the vectors are\n[1, 0, 0] since no value is mentioned. In terms of\nthe general intents, only “reqmore” is mentioned,\nso vgen[1] = 1, as “reqmore” is the ﬁrst general\nintent.\n4.1.3 User Action Features\nThe output vector from the previous turn Ot−1 is\nalso included in the input features of the next turn\nt to take into account what has been mentioned by\nthe US itself, i.e. for slotsi in turn t, the user action\nfeature vuser\naction = ot−1\ni .\n4.1.4 Domain and Slot Index Features\nIn some cases, multiple slots may share the same\nbasic feature vbasic, system action feature vsystem\naction\nand user action feature vuser\naction. This similarity in\nfeatures of different slots makes it difﬁcult for the\nmodel to distinguish one slot from another, despite\nthe positional encoding. In particular, it is challeng-\ning for the model to learn the relationship between\nturns for a given slot because the number and the\norder of slots vary from one turn to the next. This\nmay lead to over-generation: the model selects all\nslots with the same feature vector.\nTo counteract this issue, we introduce the index\nfeature vindex, which consists of the domain index\nfeature vdomain\nindex ∈{0, 1}ld and the slot index fea-\nture vslot\nindex ∈{0, 1}ls , where ld is the maximum\nnumber of domains in a user goal and ls is the\nmaximum number of slots in any given domain3.\nTo make the index feature ontology-independent,\nfor a particular slot, vindex remains consistent\nthroughout a dialogue, but varies between dia-\nlogues. The order of the index in each dialogue\nis determined by the order in the user goal. For\nexample, the “hotel” domain can be the ﬁrst do-\nmain in one user goal of the ﬁrst dialogue, and the\nsecond domain in the next.\nThen for each slot in each turn the input feature\nvector v is formed by concatenating all sub-vectors:\nv = vbasic ⊕vsystem\naction ⊕vuser\naction ⊕vindex. (4)\nAn example of v for slot Hotel-Area is shown\nin Fig. 3 based on the dialogue history in Fig. 2.\nExamples of how the feature representation is con-\nstructed can be seen in Appendix D.\n4.2 Domain Prediction\nInspired by solving downstream tasks using\nBERT (Devlin et al., 2019), we utilise the output\nof [CLS], oCLS , to predict which domains are\nconsidered in turn t as a multi-label classiﬁcation\n3This does not need to be dependent on the number of\ndomains or slots, it can simply be a random identiﬁer assigned\nto each slot during one dialogue.\n450\nproblem. The domain loss lossdomain measures\nthe difference between the output oCLS and the\ntarget yCLS for each turn by binary cross entropy\n(BCE). The ﬁnal loss function is deﬁned as\nloss = lossslots + lossdomain. (5)\n5 Experimental Setup\n5.1 Supervised Training for TUS\nOur model is implemented in PyTorch (Paszke\net al., 2019) and optimised using the Adam op-\ntimiser (Kingma and Ba, 2015) with learning rate\n5 ×10−4. The dimension of the input linear layer\nis 100, the number of the transformer layers is 2,\nand the dimension of the output linear layer is 6.\nThe maximum number of domains ld is 6 and the\nmaximum number of slots in one domain ls is 10.\nDuring training, the dropout rate is 0.1.\nWe train our model 4 on the MultiWOZ 2.1\ndataset (Eric et al., 2020), consisting of dialogues\nbetween two humans, one posing as a user and the\nother as an operator. The dialogues in the dataset\nare complex because there may be more than one\ndomain involved in one dialogue, even in the same\nturn. During training and testing with the dataset,\nthe order of slots in the input list is derived from\nthe data, which means slot si is before slot si+1 if\nthe user mentioned slot si ﬁrst. For inference with-\nout the dataset, such as when using TUS to train\na dialogue policy, the order of slots is randomly\ngenerated.\nWe measure how well a US can ﬁt the dataset by\nprecision, recall, F1 score, and turn accuracy. The\nturn accuracy measures how many model predic-\ntions per turn are identical to the corpus, based on\nthe oracle dialogue history.\n5.2 Training Policies with USs\nUser simulators are designed to train dialogue sys-\ntems, thus a better user simulator should result in\na better dialogue system. We train different di-\nalogue policies by proximal policy optimization\n(PPO) (Schulman et al., 2017), a simple and sta-\nble reinforcement learning algorithm, with ABUS,\nVHUS, and TUS as USs in the ConvLab-2 frame-\nwork (Zhu et al., 2020). The policies are trained\nfor 200 epochs, each of which consists of 1000\ndialogues. The reward function gives a reward of\n80 for a successful dialogue and of -1 for each dia-\nlogue turn, with the maximum number of dialogue\n4https://gitlab.cs.uni-duesseldorf.de/\ngeneral/dsml/tus_public\nturns set to 40. For failed dialogues, an additional\npenalty is set to -40. Each dialogue policy is trained\non 5 random seeds. The dialogue policies are then\nevaluated using all USs by cross-model evaluation\n(Schatztnann et al., 2005) to demonstrate the gen-\neralisation ability of the policy trained with a par-\nticular US when evaluated with a different US.\n5.3 Leave-one-domain-out Training\nTo evaluate the ability of TUS in handling unseen\ndomains, we remove one domain during supervised\nlearning of TUS. The leave-one-domain-out TUSs\nare used to train dialogue policies with all possible\ndomains. For example, TUS-noHotel is trained on\nthe dataset without the “hotel” domain. During\npolicy training, the user goal is generated randomly\nfrom all possible domains.\nSome domains in MultiWOZ may share the\nsame slots, such as “restaurant” and “hotel” do-\nmains which contain property-related slots, e.g.\n“area,” “name,” and “price range.” However, the\ncorpus also includes domains that are quite differ-\nent from the rest, For example, the “train” domain\nwhich contains many time-related slots such as “ar-\nrival time” or “departure time”, as well as unique\nslots such as “price” and “duration.” The different\nproperties of the domains will allow us to study the\nzero-shot transfer capability of the model.\n5.4 Human Evaluation\nFollowing the setting in Kreyssig et al. (2018), we\nselect 2 of the 5 trained versions of each dialogue\npolicy for evaluation in a human trial: the version\nperforming best on ABUS, and the version perform-\ning best in interaction with TUS. The results of the\ntwo versions are averaged. For each version we\ncollect 200 dialogues, which means there are 400\ndialogues for each policy in total. Dialogue policies\ntrained with VHUS signiﬁcantly underperform, so\nwe only consider policies trained with ABUS or\nTUS for the human trial (see Table 1). The best\nand the worst policies in the leave-one-domain-out\nexperiment are also included to see the upper and\nlower bound of the zero-shot domain generalisation\nperformance.\nHuman evaluation is performed via DialCrowd\n(Lee et al., 2018) connected to Amazon Mechanical\nTurk5. Users are provided with a randomly gener-\nated user goal and are required to interact with our\nsystems in natural language.\n5https://www.mturk.com/\n451\nUS for US for evaluation\ntraining ABUS VHUS TUS avg.\nABUS 0.93 0.09 0.58 0.53\nVHUS 0.62 0.11 0.37 0.36\nTUS 0.79 0.10 0.69 0.53\nTable 1: The success rates of policies trained on ABUS,\nVHUS, and TUS when tested on various USs.\n6 Experimental Results\n6.1 Cross-model Evaluation\nThe results of our experiments are shown in Table 1.\nThe policy trained with TUS performs well when\nevaluated with ABUS, with 10% absolute improve-\nment in the success rate over its performance on\nTUS. On the other hand, while a policy trained with\nABUS performs almost perfectly when evaluated\nwith ABUS, the performance drops signiﬁcantly,\nby 35% absolute, when this policy interacts with\nTUS. This signals that, in the case of ABUS, the\npolicy overﬁts to the US used for training, and\nis not able to generalise well to the behaviour of\nother USs. We found that VHUS is neither able\nto train nor to evaluate a multi-domain policy ade-\nquately. This was also observed in the experiments\nby Takanobu et al. (2019). We suspect that this\nis due to the fact that VHUS was designed to op-\nerate on a single domain and does not generalise\nwell to the multi-domain scenario. To the best of\nour knowledge, no other data-driven US has been\ndeveloped for the multi-domain scenario.\nThe success rates of policies trained with ABUS\nand TUS during training, evaluated with both US,\nare shown in Fig. 4. Each of the systems is trained\n5 times on different random seeds. We report the\naverage success rate as well as the standard devia-\ntion. Although the policy trained with TUS is more\nunstable when evaluated on ABUS, it still shows an\nimprovement from the initial policy, converging at\naround 79%. On the other hand, the policy trained\nwith ABUS and evaluated with TUS barely show\nany improvements.\n6.2 Impact of features and loss functions\nWe conduct an ablation study to investigate the\nusefulness of the proposed features and loss func-\ntions. The result is shown in Table. 2. First, we\nmeasure the performance of the basic model which\nuses only the basic information feature vbasic, the\nsystem action feature vsystem\naction , and the user action\nfeature vuser\naction as the input. While this model can\n0 25 50 75 100 125 150 175 200\nEpoch\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nevaluate with\nTUS\nABUS\ntrain with\nTUS\nABUS\nFigure 4: The success rates of policies during training\nwith TUS and ABUS.\nmethod P R F1 ACC LEN\nbasic model 0.11 0.71 0.19 0.11 4.51\n+ index feature 0.17 0.51 0.26 0.44 1.29\n+ domain loss 0.17 0.54 0.26 0.46 1.22\nTable 2: The TUS ablation experiments. We analyse\nthe impact of different settings by measuring precision\nP, recall R, F1 score, turn accuracy ACC, and the av-\nerage slots mentioned in the ﬁrst turn user action LEN.\nHumans, on average, mention 1.5 slots in the ﬁrst turn.\nhave a high recall rate, the precision and the turn\naccuracy are fairly low. We deduce that without the\nindex features the model cannot distinguish the dif-\nference between slots and therefore tends to select\nslots of the same slot type in one turn. For example,\nit provides all constraints in the ﬁrst turn, which\nleads to high recall and over-generation.\nAnalysis of the generated user actions shows that\nthe basic model tends to mention four or more slots\nin the ﬁrst turn. This is unnatural, since human\nusers tend to only mention one or two slots at the\nbeginning of a dialogue. More details about the\naverage slots per turn can be found in Appendix B.\nAfter adding the index feature vindex, the recall\nrate is decreased by 17% absolute, but the turn ac-\ncuracy is increased by 35% absolute, along with\nimprovements on the precision and the F1 score.\nFurthermore, the average number of slots per turn\nis closer to that of a real user. Although the re-\ncall rate with respect to the target in the data is\ndecreased, this is not necessarily a concern since in\ndialogue there are many different plausible actions\nfor a given context. For example, when searching\nfor a restaurant, we may provide the information\nof the area ﬁrst, or the food type. The order of\n452\nUS for removed ABUS TUS mean\ntraining data(%) Attr. Hotel Rest. Taxi Train all Attr. Hotel Rest. Taxi Train all\nTUS-noAttr 32.20 0.69 0.64 0.81 0.65 0.75 0.77 0.71 0.58 0.66 0.61 0.69 0.69 0.73\nTUS-noTaxi 19.60 0.63 0.61 0.81 0.61 0.70 0.74 0.69 0.60 0.69 0.64 0.68 0.69 0.72\nTUS-noRest 45.21 0.62 0.66 0.80 0.56 0.75 0.76 0.71 0.60 0.64 0.65 0.64 0.68 0.72\nTUS-noTrain 36.95 0.64 0.65 0.78 0.67 0.62 0.73 0.67 0.54 0.63 0.64 0.58 0.64 0.68\nTUS-noHotel 40.15 0.59 0.59 0.76 0.61 0.54 0.69 0.64 0.52 0.61 0.61 0.55 0.62 0.66\nTUS 0 0.69 0.68 0.81 0.66 0.77 0.79 0.73 0.59 0.66 0.68 0.64 0.69 0.74\nTable 3: The success rates of dialogue policies trained with leave-one-domain-out TUSs. For example, the TUS-\nnoAttr model is trained without the “attraction” domain. The sum of all removed data is more than 100% because\nsome dialogues have multiple domains. We report results on all domains.\ncommunicating these constraints may vary.\nWhen we include the domain loss lossdomain\nduring training, both the recall rate and the turn ac-\ncuracy improve while a similar average slot length\nper turn is maintained. These results indicate that\nthe proposed ontology-independent index features\ncan help the model to distinguish one slot from the\nother, which solves the over-generation problem of\nthe basic model. The domain loss allows for more\naccurate prediction of the domain at turn level and\nthe value for each slot at the same time.\n6.3 Zero-shot Transfer\nWe test the capability of the model to handle unseen\ndomains in a zero-shot experiment. In a leave-one-\ndomain-out fashion we remove dialogues involving\none particular domain when training the US. The\nshare of each domain in the total dialogue data\nranges from 19.60% to 45.21%. During dialogue\npolicy training we sample the user goal from all\ndomains. As presented in Table 3, removing one\ndomain from the training data when training the\nUS does not dramatically inﬂuence the policy on\nthe corresponding domain. The ﬁnal performance\nof the policies trained with leave-one-domain-out\nTUSs is still reasonably comparable to the policy\ntrained with the full TUS. This is especially note-\nworthy considering the substantial amount of data\nremoved during US training and the difference be-\ntween each domain.\nWe observe that the model is able to learn about\nthe removed domain from the other domains, al-\nthough the removed domain is different from the\nremaining ones. For example, the “train” domain\nis very different from “attraction”, “restaurant”,\nand “hotel”, and it is more complex than “taxi”,\nbut TUS-noTrain still performs reasonably well on\nthe “train” domain. This signals that the model\ncan do zero-shot transfer by leveraging other do-\nUS for success overall\ntraining Attr. Hotel all\nABUS 0.76 0.70 0.83 3.90\nTUS 0.73 0.69 0.83 4.03\nTUS-noAttr 0.75 0.54 0.81 4.01\nTUS-noHotel 0.73 0.55 0.76 3.86\nTable 4: The human evaluation results include success\nrate and overall rating as judged by users.\nmain information. The worst performance on the\n“train” domain happens instead when the “hotel”\ndomain is removed, i.e. the domain with the most\nsubstantial amount of data.\nOur results also show that that some domains are\nmore sensitive to data removal than others, irrespec-\ntive of which domain is removed. This indicates\nthat some domains are more involved and simply re-\nquire more training data. This result demonstrates\nthat TUS has the capability to handle new unseen\ndomains without modifying the feature representa-\ntion or retraining the model. It also shows that our\nmodel is sample-efﬁcient.\n6.4 Human Evaluation\nThe result of the human evaluation is shown in Ta-\nble 4. In total, 156 users participated in the human\nevaluation. The number of interactions per user\nranges from 10 to 80. The success rate measures\nwhether the given goal is fulﬁlled by the system\nand the overall rating grades the system’s perfor-\nmance from 1 star (poor) to 5 stars (excellent). TUS\nis able to achieve a comparable success rate as\nABUS, without domain-speciﬁc information, and\neven scores slightly better in terms of overall rating.\nWe were not able to observe any statistically signif-\nicant differences between ABUS and TUS in the\nhuman evaluation. For leave-one-domain-out mod-\n453\nels, the performance of TUS-noAttr is similar to\nthat one of ABUS and TUS without a statistically\nsigniﬁcant difference. We do however observe a sta-\ntistically signiﬁcant decrease in the success rate of\nTUS-noHotel when compared to TUS and ABUS\n(p < 0.05). This is unsurprising as the hotel do-\nmain accounts for 40.15% of the training data. For\nboth TUS-noAttr and TUS-noHotel, the success\nrate on the domain “attraction” is comparable to\nTUS and ABUS, but the success rate on the do-\nmain “hotel” is relatively low. As observed in the\nsimulation, removing a domain does not decrease\nthe success rate in the corresponding domain as the\nfeature representation is domain agnostic. Instead,\nit impacts domains which need plenty of data to\nlearn.\n7 Conclusion\nWe propose a domain-independent user simula-\ntor with transformers, TUS. We design ontology-\nindependent input and output feature representa-\ntions. TUS outperforms the data-driven VHUS and\nit has a comparable performance to the rule-based\nABUS in cross-model evaluation. Human evalua-\ntion conﬁrms that TUS can compete with ABUS\neven though ABUS is based on carefully designed\ndomain-dependent rules. Our ablation study shows\nthat the proposed features and loss functions are\nessential to model natural user behavior from data.\nLastly, our zero-shot study shows that TUS can\nhandle new domains without feature modiﬁcation\nor model retraining, even with substantially fewer\ntraining samples.\nIn future work, we would like to learn the or-\nder of slots and add output language generation to\nmake the behaviour of TUS more human-like. Ap-\nplying reinforcement learning to this model would\nalso be of interest.\nAcknowledgments\nWe would like to thank Ting-Rui Chiang and Dr.\nMaxine Eskenazi from Carnegie Mellon University\nfor their help with the human trial. This work is a\npart of DYMO project which has received funding\nfrom the European Research Council (ERC) pro-\nvided under the Horizon 2020 research and innova-\ntion programme (Grant agreement No. STG2018\n804636). N. Lubis, C. van Niekerk, M. Heck and\nS. Feng are funded by an Alexander von Hum-\nboldt Sofja Kovalevskaja Award endowed by the\nGerman Federal Ministry of Education and Re-\nsearch. Computational infrastructure and support\nwere provided by the Centre for Information and\nMedia Technology at Heinrich Heine University\nD¨usseldorf. Computing resources were provided\nby Google Cloud.\nReferences\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I ˜nigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga ˇsi´c. 2018. MultiWOZ - a\nlarge-scale multi-domain Wizard-of-Oz dataset for\ntask-oriented dialogue modelling. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5026, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nHeriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and\nHiroshi Shimodaira. 2005. Human-computer dia-\nlogue simulation using hidden markov models. In\nIEEE Workshop on Automatic Speech Recognition\nand Understanding, 2005., pages 290–295. IEEE.\nLucie Daubigney, Matthieu Geist, Senthilkumar Chan-\ndramohan, and Olivier Pietquin. 2012. A compre-\nhensive reinforcement learning framework for dia-\nlogue management optimization. IEEE Journal of\nSelected Topics in Signal Processing, 6(8):891–902.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWieland Eckert, Esther Levin, and Roberto Pierac-\ncini. 1997. User modeling for spoken dialogue sys-\ntem evaluation. In 1997 IEEE Workshop on Auto-\nmatic Speech Recognition and Understanding Pro-\nceedings, pages 80–87. IEEE.\nLayla El Asri, Jing He, and Kaheer Suleman. 2016. A\nsequence-to-sequence model for user simulation in\nspoken dialogue systems. Interspeech 2016, pages\n1151–1155.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar,\nAnuj Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020.\nMultiwoz 2.1: A consolidated multi-domain dia-\nlogue dataset with state corrections and state track-\ning baselines. In Proceedings of The 12th Language\nResources and Evaluation Conference, pages 422–\n428.\nAciel Eshky, Ben Allison, and Mark Steedman. 2012.\nGenerative goal-driven user simulation for dialog\n454\nmanagement. In Proceedings of the 2012 Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning, pages 71–81, Jeju Island, Korea. Associ-\nation for Computational Linguistics.\nMilica Gaˇsi´c, Filip Jur ˇc´ıˇcek, Blaise Thomson, Kai Yu,\nand Steve Young. 2011. On-line policy optimisation\nof spoken dialogue systems via live interaction with\nhuman subjects. In 2011 IEEE Workshop on Auto-\nmatic Speech Recognition & Understanding, pages\n312–317. IEEE.\nKallirroi Georgila, James Henderson, and Oliver\nLemon. 2006. User simulation for spoken dialogue\nsystems: Learning and evaluation. In Ninth Interna-\ntional Conference on Spoken Language Processing.\nIzzeddin G ¨ur, Dilek Hakkani-T ¨ur, Gokhan T ¨ur, and\nPararth Shah. 2018. User modeling for task oriented\ndialogues. In 2018 IEEE Spoken Language Technol-\nogy Workshop (SLT), pages 900–906. IEEE.\nSimon Keizer, Milica Ga ˇsi´c, Filip Jurcicek, Franc ¸ois\nMairesse, Blaise Thomson, Kai Yu, and Steve\nYoung. 2010. Parameter estimation for agenda-\nbased user simulation. In Proceedings of the SIG-\nDIAL 2010 Conference, pages 116–123.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nFlorian Kreyssig, I ˜nigo Casanueva, Paweł\nBudzianowski, and Milica Gaˇsi´c. 2018. Neural user\nsimulation for corpus-based policy optimisation\nof spoken dialogue systems. In Proceedings of\nthe 19th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 60–69, Melbourne, Australia.\nAssociation for Computational Linguistics.\nKyusong Lee, Tiancheng Zhao, Alan W. Black, and\nMaxine Eskenazi. 2018. DialCrowd: A toolkit for\neasy dialog system assessment. In Proceedings of\nthe 19th Annual SIGdial Meeting on Discourse and\nDialogue, pages 245–248, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nEsther Levin and Roberto Pieraccini. 1997. A stochas-\ntic model of computer-human interaction for learn-\ning dialogue strategies. In Fifth European Confer-\nence on Speech Communication and Technology.\nXiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong\nLi, Jianfeng Gao, and Yun-Nung Chen. 2016. A\nuser simulator for task-completion dialogues. arXiv\npreprint arXiv:1612.05688.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nOlivier Pietquin, Matthieu Geist, Senthilkumar Chan-\ndramohan, and Herv ´e Frezza-Buet. 2011. Sample-\nefﬁcient batch reinforcement learning for dialogue\nmanagement optimization. ACM Transactions on\nSpeech and Language Processing (TSLP), 7(3):1–\n21.\nJost Schatzmann, Kallirroi Georgila, and Steve Young.\n2005. Quantitative evaluation of user simulation\ntechniques for spoken dialogue systems. In Proceed-\nings of the 6th SIGdial Workshop on Discourse and\nDialogue, pages 45–54.\nJost Schatzmann, Blaise Thomson, Karl Weilhammer,\nHui Ye, and Steve Young. 2007. Agenda-based\nuser simulation for bootstrapping a POMDP dia-\nlogue system. In Human Language Technologies\n2007: The Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics; Companion Volume, Short Papers, pages 149–\n152, Rochester, New York. Association for Compu-\ntational Linguistics.\nJost Schatzmann, Karl Weilhammer, Matt Stuttle, and\nSteve Young. 2006. A survey of statistical user sim-\nulation techniques for reinforcement-learning of dia-\nlogue management strategies. Knowledge Engineer-\ning Review, 21(2):97–126.\nJost Schatztnann, Matthew N Stuttle, Karl Weilham-\nmer, and Steve Young. 2005. Effects of the user\nmodel on simulation-based learning of dialogue\nstrategies. In IEEE Workshop on Automatic Speech\nRecognition and Understanding, 2005., pages 220–\n225. IEEE.\nKonrad Schefﬂer and Steve Young. 2002. Automatic\nlearning of dialogue strategy using dialogue simula-\ntion and reinforcement learning. In Proceedings of\nthe second international conference on Human Lan-\nguage Technology Research, pages 12–19. Citeseer.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal\npolicy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nWeiyan Shi, Kun Qian, Xuewei Wang, and Zhou Yu.\n2019. How to build user simulators to train rl-based\ndialog systems. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1990–2000.\nRyuichi Takanobu, Hanlin Zhu, and Minlie Huang.\n2019. Guided dialog policy learning: Reward es-\ntimation for multi-domain task-oriented dialog. In\n455\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 100–\n110, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nSteve Young. 2002. Talking to machines (statistically\nspeaking). In Seventh International Conference on\nSpoken Language Processing.\nQi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi\nTakanobu, Jinchao Li, Baolin Peng, Jianfeng Gao,\nXiaoyan Zhu, and Minlie Huang. 2020. ConvLab-\n2: An Open-Source Toolkit for Building, Evaluating,\nand Diagnosing Dialogue Systems. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics.\nA All System Intents\nAll system intents in the MultiWOZ 2.1 dataset are\nlisted in Table 5, including 5 general intents and 9\ndomain-speciﬁc intents.\ntype intents\ngeneral welcome, reqmore, bye, thank, greet\ndomain-\nspeciﬁc\nrecommend, inform, request, select,\nbook, nobook, offerbook, offerbooked,\nnooffer\nTable 5: All system intents in the MultiWOZ 2.1\nB Average Action Length in Each Turn\nThe average number of slots mentioned by TUS\nin each turn when interacting with the rule-based\ndialogue system is shown in Fig. 5. When the index\nfeature vindex and the domain loss lossdomain are\nadded, TUS can deal with the over-generation prob-\nlem and behave more similarly to what is observed\nin the corpus.\nC Success Rates of\nLeave-one-domain-out Training\nThe training success rates of dialogue policies\ntrained with leave-one-domain-out TUSs, which\nare evaluated on TUS, are shown in Fig. 6. In com-\nparison to the full TUS, the leave-one-domain-out\nTUSs are more unstable, but they can achieve a\ncomparable success rate at the end.\nturn\nnumber of slots\n0\n1\n2\n3\n4\n5\n0 1 2 3 4 5 6 7 8 9 10\nbasic model + index-feature + domain-loss real-user\nFigure 5: The average user action length per turn when\ninteracting with the rule-based dialogue system. The\naverage action length of real users in the corpus is also\npresented.\n0 25 50 75 100 125 150 175 200\nEpoch\n0.5\n0.6\n0.7success_rate\nnoHotel\nnoAttraction\nnoRestaurant\nnoTaxi\nnoTrain\nall domain\nFigure 6: The success rates of dialogue policies\ntrained with leave-one-domain-out TUSs during train-\ning, when evaluated on TUS.\nD An example for the input feature\nrepresentation\nThe list of input feature vectors and output se-\nquence are presented on Fig. 7 based on Fig. 2.\nFor turn 0, V 0 only includes 4 vectors from the\nuser goal. For turn 1, the system mentions slot\nHotel-Price, which is not in the user goal,\nso the feature vector of slot Hotel-Price is\ninserted into V 1, where the 1-st dimension of\nvdomain\nslot is 1 because domain Hotel is the ﬁrst do-\nmain in this conversation and the 3-rd dimension\nof vslot\nindex is 1 because it is the third slot in domain\nHotel.\nIn comparison between the feature vec-\ntors of slot Hotel-Area in turn 0, v0\n1,\nand turn 1, v0\n1, the vsys\nvalue and vspec0 are\ndifferent because of the system’s domain-\nspeciﬁc action Recom(Hotel-Area=South).\nThe system also mentioned a general action,\ngeneral-reqmore(), thus vgen is changed. In\n456\n0 0 0 0 0 0...1 0 000 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 01 0 0 0 0 01 0 0 0 ... 0 0\n0 0 0 0 0 0...1 0 000 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 00 1 0 0 0 01 0 0 0 ... 0 0\n0 0 0 0 0 0...1 0 000 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 01 0 0 0 0 00 1 0 0 ... 0 0\n0 0 0 0 0 0...1 0 000 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 00 1 0 0 0 00 1 0 0 ... 0 0\n (Hotel-Area)\n (Rest-Area)\n (Hotel-Name)\n (Rest-Addr)\n0 0 0 1 0 0\n0 0 0 1 0 0\n1 0 0 0 0 0\n1 0 0 0 0 0\n0 1 0 0 0 0...0 0 100 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 01 0 0 0 ... 0 0\n0 1 0 0 0 0...1 0 000 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 01 0 0 0 ... 0 0\n0 1 0 0 0 0...1 0 000 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 01 0 0 0 0 00 1 0 0 ... 0 0\n0 1 0 0 0 0...1 0 000 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 00 1 0 0 0 00 1 0 0 ... 0 0\n (Hotel-Area)\n (Rest-Area)\n (Hotel-Name)\n (Rest-Addr)\n0 0 0 1 0 0\n1 0 0 0 0 0\n1 0 0 0 0 0\n1 0 0 0 0 0\nTurn \nTurn \n0 1 0 0 0 0...1 0 001 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 01 0 0 0 0 00 0 1 0 ... 0 0 0 0 1 0 0 0 (Hotel-Price)\nFigure 7: The input and feature representation according to Fig. 2. vCLS and vsep are ignored in this graph.\naddition, this slot is ﬁrst mentioned at turn 0, so\nvfirst is changed from 0 to 1. Similarly, vuser\naction is\nalso modiﬁed according to the user action. On the\nother hand, vuser\nvalue is the same because the user does\nnot update its goal, vtype is not changed because\nthe slot is still a constraint, and vful is 0 because it\nhas not been fulﬁlled yet. vdomain\nindex and vslot\nindex are\nalso the same through the whole conversation.\nE Example Dialogue Generated by TUS\nAn example dialogue with a multi-domain user\ngoal is shown in Fig. 8. It shows that TUS is able\nto switch between different domains (from turn 2 to\n6), respond to the system’s requests, and generate\nmulti-domain actions (in turn 5).\nTurn 0\nUSR: Inform(Hotel-Area=north, Hotel-Stars=0, \n            Hotel-Parking=yes)\nSYS: Inform(Hotel-Parking=yes)\nTurn 1\nUSR: Request(Hotel-Type)\nSYS: Inform(Hotel-Type=guesthouse)\nTurn 2\nUSR: Inform(Hotel-Stars=0, Hotel-Parking=yes)\nSYS: Inform(Hotel-Stars=0, Hotel-Type=guesthouse, \n            Hotel-Area=north, Hotel-Parking=yes, \n            Hotel-Price=cheap)\nTurn 3\nUSR: Inform(Attr-Type=college)\nSYS: Inform(Attr-Choice=18),\n     Recom(Attr-Name=hughes hall)\nTurn 4\nUSR: Request(Taxi-Phone, Taxi-Car)\nSYS: Request(Taxi-Leave)\nTurn 5\nUSR: Inform(Taxi-Leave=dontcare), \n     Request(Taxi-Phone, Taxi-Car, Attr-Fee)\nSYS: Inform(Taxi-Car=audi, Taxi-Phone=44162528555, \n            Taxi-Car=honda, Taxi-Phone=46793705737, \n            Attr-Fee=free)\nTurn 6\nUSR: Request(Attr-Post)\nSYS: Inform(Attr-Post=cb23bu)\nTurn 7\nUSR: general-bye()\nSYS: general-greet()\nFigure 8: A dialogue generated by TUS when interact-\ning with the rule-based policy.",
  "topic": "Chin",
  "concepts": [
    {
      "name": "Chin",
      "score": 0.8174060583114624
    },
    {
      "name": "Computer science",
      "score": 0.5757573843002319
    },
    {
      "name": "Transformer",
      "score": 0.4980888366699219
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4013383388519287
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.34845656156539917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3434251844882965
    },
    {
      "name": "Natural language processing",
      "score": 0.32238391041755676
    },
    {
      "name": "Engineering",
      "score": 0.1803480088710785
    },
    {
      "name": "Electrical engineering",
      "score": 0.14782288670539856
    },
    {
      "name": "Medicine",
      "score": 0.09806299209594727
    },
    {
      "name": "Voltage",
      "score": 0.07544535398483276
    },
    {
      "name": "Anatomy",
      "score": 0.0
    }
  ]
}